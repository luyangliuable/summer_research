/*

 * Copyright (C) 2016 Samsung Electronics Co.Ltd

 * Authors:

 *	Marek Szyprowski <m.szyprowski@samsung.com>

 *

 * DRM core plane blending related functions

 *

 * Permission to use, copy, modify, distribute, and sell this software and its

 * documentation for any purpose is hereby granted without fee, provided that

 * the above copyright notice appear in all copies and that both that copyright

 * notice and this permission notice appear in supporting documentation, and

 * that the name of the copyright holders not be used in advertising or

 * publicity pertaining to distribution of the software without specific,

 * written prior permission.  The copyright holders make no representations

 * about the suitability of this software for any purpose.  It is provided "as

 * is" without express or implied warranty.

 *

 * THE COPYRIGHT HOLDERS DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,

 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO

 * EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY SPECIAL, INDIRECT OR

 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE,

 * DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER

 * TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE

 * OF THIS SOFTWARE.

/**

 * DOC: overview

 *

 * The basic plane composition model supported by standard plane properties only

 * has a source rectangle (in logical pixels within the &drm_framebuffer), with

 * sub-pixel accuracy, which is scaled up to a pixel-aligned destination

 * rectangle in the visible area of a &drm_crtc. The visible area of a CRTC is

 * defined by the horizontal and vertical visible pixels (stored in @hdisplay

 * and @vdisplay) of the requested mode (stored in &drm_crtc_state.mode). These

 * two rectangles are both stored in the &drm_plane_state.

 *

 * For the atomic ioctl the following standard (atomic) properties on the plane object

 * encode the basic plane composition model:

 *

 * SRC_X:

 * 	X coordinate offset for the source rectangle within the

 * 	&drm_framebuffer, in 16.16 fixed point. Must be positive.

 * SRC_Y:

 * 	Y coordinate offset for the source rectangle within the

 * 	&drm_framebuffer, in 16.16 fixed point. Must be positive.

 * SRC_W:

 * 	Width for the source rectangle within the &drm_framebuffer, in 16.16

 * 	fixed point. SRC_X plus SRC_W must be within the width of the source

 * 	framebuffer. Must be positive.

 * SRC_H:

 * 	Height for the source rectangle within the &drm_framebuffer, in 16.16

 * 	fixed point. SRC_Y plus SRC_H must be within the height of the source

 * 	framebuffer. Must be positive.

 * CRTC_X:

 * 	X coordinate offset for the destination rectangle. Can be negative.

 * CRTC_Y:

 * 	Y coordinate offset for the destination rectangle. Can be negative.

 * CRTC_W:

 * 	Width for the destination rectangle. CRTC_X plus CRTC_W can extend past

 * 	the currently visible horizontal area of the &drm_crtc.

 * CRTC_H:

 * 	Height for the destination rectangle. CRTC_Y plus CRTC_H can extend past

 * 	the currently visible vertical area of the &drm_crtc.

 * FB_ID:

 * 	Mode object ID of the &drm_framebuffer this plane should scan out.

 * CRTC_ID:

 * 	Mode object ID of the &drm_crtc this plane should be connected to.

 *

 * Note that the source rectangle must fully lie within the bounds of the

 * &drm_framebuffer. The destination rectangle can lie outside of the visible

 * area of the current mode of the CRTC. It must be apprpriately clipped by the

 * driver, which can be done by calling drm_plane_helper_check_update(). Drivers

 * are also allowed to round the subpixel sampling positions appropriately, but

 * only to the next full pixel. No pixel outside of the source rectangle may

 * ever be sampled, which is important when applying more sophisticated

 * filtering than just a bilinear one when scaling. The filtering mode when

 * scaling is unspecified.

 *

 * On top of this basic transformation additional properties can be exposed by

 * the driver:

 *

 * alpha:

 * 	Alpha is setup with drm_plane_create_alpha_property(). It controls the

 * 	plane-wide opacity, from transparent (0) to opaque (0xffff). It can be

 * 	combined with pixel alpha.

 *	The pixel values in the framebuffers are expected to not be

 *	pre-multiplied by the global alpha associated to the plane.

 *

 * rotation:

 *	Rotation is set up with drm_plane_create_rotation_property(). It adds a

 *	rotation and reflection step between the source and destination rectangles.

 *	Without this property the rectangle is only scaled, but not rotated or

 *	reflected.

 *

 *	Possbile values:

 *

 *	"rotate-<degrees>":

 *		Signals that a drm plane is rotated <degrees> degrees in counter

 *		clockwise direction.

 *

 *	"reflect-<axis>":

 *		Signals that the contents of a drm plane is reflected along the

 *		<axis> axis, in the same way as mirroring.

 *

 *	reflect-x::

 *

 *			|o |    | o|

 *			|  | -> |  |

 *			| v|    |v |

 *

 *	reflect-y::

 *

 *			|o |    | ^|

 *			|  | -> |  |

 *			| v|    |o |

 *

 * zpos:

 *	Z position is set up with drm_plane_create_zpos_immutable_property() and

 *	drm_plane_create_zpos_property(). It controls the visibility of overlapping

 *	planes. Without this property the primary plane is always below the cursor

 *	plane, and ordering between all other planes is undefined. The positive

 *	Z axis points towards the user, i.e. planes with lower Z position values

 *	are underneath planes with higher Z position values. Two planes with the

 *	same Z position value have undefined ordering. Note that the Z position

 *	value can also be immutable, to inform userspace about the hard-coded

 *	stacking of planes, see drm_plane_create_zpos_immutable_property(). If

 *	any plane has a zpos property (either mutable or immutable), then all

 *	planes shall have a zpos property.

 *

 * pixel blend mode:

 *	Pixel blend mode is set up with drm_plane_create_blend_mode_property().

 *	It adds a blend mode for alpha blending equation selection, describing

 *	how the pixels from the current plane are composited with the

 *	background.

 *

 *	 Three alpha blending equations are defined:

 *

 *	 "None":

 *		 Blend formula that ignores the pixel alpha::

 *

 *			 out.rgb = plane_alpha * fg.rgb +

 *				 (1 - plane_alpha) * bg.rgb

 *

 *	 "Pre-multiplied":

 *		 Blend formula that assumes the pixel color values

 *		 have been already pre-multiplied with the alpha

 *		 channel values::

 *

 *			 out.rgb = plane_alpha * fg.rgb +

 *				 (1 - (plane_alpha * fg.alpha)) * bg.rgb

 *

 *	 "Coverage":

 *		 Blend formula that assumes the pixel color values have not

 *		 been pre-multiplied and will do so when blending them to the

 *		 background color values::

 *

 *			 out.rgb = plane_alpha * fg.alpha * fg.rgb +

 *				 (1 - (plane_alpha * fg.alpha)) * bg.rgb

 *

 *	 Using the following symbols:

 *

 *	 "fg.rgb":

 *		 Each of the RGB component values from the plane's pixel

 *	 "fg.alpha":

 *		 Alpha component value from the plane's pixel. If the plane's

 *		 pixel format has no alpha component, then this is assumed to be

 *		 1.0. In these cases, this property has no effect, as all three

 *		 equations become equivalent.

 *	 "bg.rgb":

 *		 Each of the RGB component values from the background

 *	 "plane_alpha":

 *		 Plane alpha value set by the plane "alpha" property. If the

 *		 plane does not expose the "alpha" property, then this is

 *		 assumed to be 1.0

 *

 * Note that all the property extensions described here apply either to the

 * plane or the CRTC (e.g. for the background color, which currently is not

 * exposed and assumed to be black).

 *

 * SCALING_FILTER:

 *     Indicates scaling filter to be used for plane scaler

 *

 *     The value of this property can be one of the following:

 *

 *     Default:

 *             Driver's default scaling filter

 *     Nearest Neighbor:

 *             Nearest Neighbor scaling filter

 *

 * Drivers can set up this property for a plane by calling

 * drm_plane_create_scaling_filter_property

/**

 * drm_plane_create_alpha_property - create a new alpha property

 * @plane: drm plane

 *

 * This function creates a generic, mutable, alpha property and enables support

 * for it in the DRM core. It is attached to @plane.

 *

 * The alpha property will be allowed to be within the bounds of 0

 * (transparent) to 0xffff (opaque).

 *

 * Returns:

 * 0 on success, negative error code on failure.

/**

 * drm_plane_create_rotation_property - create a new rotation property

 * @plane: drm plane

 * @rotation: initial value of the rotation property

 * @supported_rotations: bitmask of supported rotations and reflections

 *

 * This creates a new property with the selected support for transformations.

 *

 * Since a rotation by 180Â° degress is the same as reflecting both along the x

 * and the y axis the rotation property is somewhat redundant. Drivers can use

 * drm_rotation_simplify() to normalize values of this property.

 *

 * The property exposed to userspace is a bitmask property (see

 * drm_property_create_bitmask()) called "rotation" and has the following

 * bitmask enumaration values:

 *

 * DRM_MODE_ROTATE_0:

 * 	"rotate-0"

 * DRM_MODE_ROTATE_90:

 * 	"rotate-90"

 * DRM_MODE_ROTATE_180:

 * 	"rotate-180"

 * DRM_MODE_ROTATE_270:

 * 	"rotate-270"

 * DRM_MODE_REFLECT_X:

 * 	"reflect-x"

 * DRM_MODE_REFLECT_Y:

 * 	"reflect-y"

 *

 * Rotation is the specified amount in degrees in counter clockwise direction,

 * the X and Y axis are within the source rectangle, i.e.  the X/Y axis before

 * rotation. After reflection, the rotation is applied to the image sampled from

 * the source rectangle, before scaling it to fit the destination rectangle.

/**

 * drm_rotation_simplify() - Try to simplify the rotation

 * @rotation: Rotation to be simplified

 * @supported_rotations: Supported rotations

 *

 * Attempt to simplify the rotation to a form that is supported.

 * Eg. if the hardware supports everything except DRM_MODE_REFLECT_X

 * one could call this function like this:

 *

 * drm_rotation_simplify(rotation, DRM_MODE_ROTATE_0 |

 *                       DRM_MODE_ROTATE_90 | DRM_MODE_ROTATE_180 |

 *                       DRM_MODE_ROTATE_270 | DRM_MODE_REFLECT_Y);

 *

 * to eliminate the DRM_MODE_ROTATE_X flag. Depending on what kind of

 * transforms the hardware supports, this function may not

 * be able to produce a supported transform, so the caller should

 * check the result afterwards.

/**

 * drm_plane_create_zpos_property - create mutable zpos property

 * @plane: drm plane

 * @zpos: initial value of zpos property

 * @min: minimal possible value of zpos property

 * @max: maximal possible value of zpos property

 *

 * This function initializes generic mutable zpos property and enables support

 * for it in drm core. Drivers can then attach this property to planes to enable

 * support for configurable planes arrangement during blending operation.

 * Drivers that attach a mutable zpos property to any plane should call the

 * drm_atomic_normalize_zpos() helper during their implementation of

 * &drm_mode_config_funcs.atomic_check(), which will update the normalized zpos

 * values and store them in &drm_plane_state.normalized_zpos. Usually min

 * should be set to 0 and max to maximal number of planes for given crtc - 1.

 *

 * If zpos of some planes cannot be changed (like fixed background or

 * cursor/topmost planes), drivers shall adjust the min/max values and assign

 * those planes immutable zpos properties with lower or higher values (for more

 * information, see drm_plane_create_zpos_immutable_property() function). In such

 * case drivers shall also assign proper initial zpos values for all planes in

 * its plane_reset() callback, so the planes will be always sorted properly.

 *

 * See also drm_atomic_normalize_zpos().

 *

 * The property exposed to userspace is called "zpos".

 *

 * Returns:

 * Zero on success, negative errno on failure.

/**

 * drm_plane_create_zpos_immutable_property - create immuttable zpos property

 * @plane: drm plane

 * @zpos: value of zpos property

 *

 * This function initializes generic immutable zpos property and enables

 * support for it in drm core. Using this property driver lets userspace

 * to get the arrangement of the planes for blending operation and notifies

 * it that the hardware (or driver) doesn't support changing of the planes'

 * order. For mutable zpos see drm_plane_create_zpos_property().

 *

 * The property exposed to userspace is called "zpos".

 *

 * Returns:

 * Zero on success, negative errno on failure.

	/*

	 * Normalization process might create new states for planes which

	 * normalized_zpos has to be recalculated.

/**

 * drm_atomic_normalize_zpos - calculate normalized zpos values for all crtcs

 * @dev: DRM device

 * @state: atomic state of DRM device

 *

 * This function calculates normalized zpos value for all modified planes in

 * the provided atomic state of DRM device.

 *

 * For every CRTC this function checks new states of all planes assigned to

 * it and calculates normalized zpos value for these planes. Planes are compared

 * first by their zpos values, then by plane id (if zpos is equal). The plane

 * with lowest zpos value is at the bottom. The &drm_plane_state.normalized_zpos

 * is then filled with unique values from 0 to number of active planes in crtc

 * minus one.

 *

 * RETURNS

 * Zero for success or -errno

/**

 * drm_plane_create_blend_mode_property - create a new blend mode property

 * @plane: drm plane

 * @supported_modes: bitmask of supported modes, must include

 *		     BIT(DRM_MODE_BLEND_PREMULTI). Current DRM assumption is

 *		     that alpha is premultiplied, and old userspace can break if

 *		     the property defaults to anything else.

 *

 * This creates a new property describing the blend mode.

 *

 * The property exposed to userspace is an enumeration property (see

 * drm_property_create_enum()) called "pixel blend mode" and has the

 * following enumeration values:

 *

 * "None":

 *	Blend formula that ignores the pixel alpha.

 *

 * "Pre-multiplied":

 *	Blend formula that assumes the pixel color values have been already

 *	pre-multiplied with the alpha channel values.

 *

 * "Coverage":

 *	Blend formula that assumes the pixel color values have not been

 *	pre-multiplied and will do so when blending them to the background color

 *	values.

 *

 * RETURNS:

 * Zero for success or -errno

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright 2021 Google Inc.

 *

 * The DP AUX bus is used for devices that are connected over a DisplayPort

 * AUX bus. The devices on the far side of the bus are referred to as

 * endpoints in this code.

 *

 * Commonly there is only one device connected to the DP AUX bus: a panel.

 * Though historically panels (even DP panels) have been modeled as simple

 * platform devices, putting them under the DP AUX bus allows the panel driver

 * to perform transactions on that bus.

/**

 * dp_aux_ep_match() - The match function for the dp_aux_bus.

 * @dev: The device to match.

 * @drv: The driver to try to match against.

 *

 * At the moment, we just match on device tree.

 *

 * Return: True if this driver matches this device; false otherwise.

/**

 * dp_aux_ep_probe() - The probe function for the dp_aux_bus.

 * @dev: The device to probe.

 *

 * Calls through to the endpoint driver probe.

 *

 * Return: 0 if no error or negative error code.

/**

 * dp_aux_ep_remove() - The remove function for the dp_aux_bus.

 * @dev: The device to remove.

 *

 * Calls through to the endpoint driver remove.

 *

/**

 * dp_aux_ep_shutdown() - The shutdown function for the dp_aux_bus.

 * @dev: The device to shutdown.

 *

 * Calls through to the endpoint driver shutdown.

/**

 * dp_aux_ep_dev_release() - Free memory for the dp_aux_ep device

 * @dev: The device to free.

 *

 * Return: 0 if no error or negative error code.

/**

 * of_dp_aux_ep_destroy() - Destroy an DP AUX endpoint device

 * @dev: The device to destroy.

 * @data: Not used

 *

 * This is just used as a callback by of_dp_aux_depopulate_ep_devices() and

 * is called for _all_ of the child devices of the device providing the AUX bus.

 * We'll only act on those that are of type "dp_aux_bus_type".

 *

 * This function is effectively an inverse of what's in the loop

 * in of_dp_aux_populate_ep_devices().

 *

 * Return: 0 if no error or negative error code.

/**

 * of_dp_aux_depopulate_ep_devices() - Undo of_dp_aux_populate_ep_devices

 * @aux: The AUX channel whose devices we want to depopulate

 *

 * This will destroy all devices that were created

 * by of_dp_aux_populate_ep_devices().

/**

 * of_dp_aux_populate_ep_devices() - Populate the endpoint devices on the DP AUX

 * @aux: The AUX channel whose devices we want to populate. It is required that

 *       drm_dp_aux_init() has already been called for this AUX channel.

 *

 * This will populate all the devices under the "aux-bus" node of the device

 * providing the AUX channel (AKA aux->dev).

 *

 * When this function finishes, it is _possible_ (but not guaranteed) that

 * our sub-devices will have finished probing. It should be noted that if our

 * sub-devices return -EPROBE_DEFER that we will not return any error codes

 * ourselves but our sub-devices will _not_ have actually probed successfully

 * yet. There may be other cases (maybe added in the future?) where sub-devices

 * won't have been probed yet when this function returns, so it's best not to

 * rely on that.

 *

 * If this function succeeds you should later make sure you call

 * of_dp_aux_depopulate_ep_devices() to undo it, or just use the devm version

 * of this function.

 *

 * Return: 0 if no error or negative error code.

 drm_dp_aux_init() should have been called already; warn if not */

			/*

			 * As per docs of device_register(), call this instead

			 * of kfree() directly for error cases.

			/*

			 * Following in the footsteps of of_i2c_register_devices(),

			 * we won't fail the whole function here--we'll just

			 * continue registering any other devices we find.

/**

 * devm_of_dp_aux_populate_ep_devices() - devm wrapper for of_dp_aux_populate_ep_devices()

 * @aux: The AUX channel whose devices we want to populate

 *

 * Handles freeing w/ devm on the device "aux->dev".

 *

 * Return: 0 if no error or negative error code.

/*

 * Copyright (c) 2016 Intel Corporation

 *

 * Permission to use, copy, modify, distribute, and sell this software and its

 * documentation for any purpose is hereby granted without fee, provided that

 * the above copyright notice appear in all copies and that both that copyright

 * notice and this permission notice appear in supporting documentation, and

 * that the name of the copyright holders not be used in advertising or

 * publicity pertaining to distribution of the software without specific,

 * written prior permission.  The copyright holders make no representations

 * about the suitability of this software for any purpose.  It is provided "as

 * is" without express or implied warranty.

 *

 * THE COPYRIGHT HOLDERS DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,

 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO

 * EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY SPECIAL, INDIRECT OR

 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE,

 * DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER

 * TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE

 * OF THIS SOFTWARE.

/**

 * DOC: overview

 *

 * A plane represents an image source that can be blended with or overlaid on

 * top of a CRTC during the scanout process. Planes take their input data from a

 * &drm_framebuffer object. The plane itself specifies the cropping and scaling

 * of that image, and where it is placed on the visible area of a display

 * pipeline, represented by &drm_crtc. A plane can also have additional

 * properties that specify how the pixels are positioned and blended, like

 * rotation or Z-position. All these properties are stored in &drm_plane_state.

 *

 * To create a plane, a KMS drivers allocates and zeroes an instances of

 * &struct drm_plane (possibly as part of a larger structure) and registers it

 * with a call to drm_universal_plane_init().

 *

 * Each plane has a type, see enum drm_plane_type. A plane can be compatible

 * with multiple CRTCs, see &drm_plane.possible_crtcs.

 *

 * Each CRTC must have a unique primary plane userspace can attach to enable

 * the CRTC. In other words, userspace must be able to attach a different

 * primary plane to each CRTC at the same time. Primary planes can still be

 * compatible with multiple CRTCs. There must be exactly as many primary planes

 * as there are CRTCs.

 *

 * Legacy uAPI doesn't expose the primary and cursor planes directly. DRM core

 * relies on the driver to set the primary and optionally the cursor plane used

 * for legacy IOCTLs. This is done by calling drm_crtc_init_with_planes(). All

 * drivers must provide one primary plane per CRTC to avoid surprising legacy

 * userspace too much.

/**

 * DOC: standard plane properties

 *

 * DRM planes have a few standardized properties:

 *

 * type:

 *     Immutable property describing the type of the plane.

 *

 *     For user-space which has enabled the &DRM_CLIENT_CAP_ATOMIC capability,

 *     the plane type is just a hint and is mostly superseded by atomic

 *     test-only commits. The type hint can still be used to come up more

 *     easily with a plane configuration accepted by the driver.

 *

 *     The value of this property can be one of the following:

 *

 *     "Primary":

 *         To light up a CRTC, attaching a primary plane is the most likely to

 *         work if it covers the whole CRTC and doesn't have scaling or

 *         cropping set up.

 *

 *         Drivers may support more features for the primary plane, user-space

 *         can find out with test-only atomic commits.

 *

 *         Some primary planes are implicitly used by the kernel in the legacy

 *         IOCTLs &DRM_IOCTL_MODE_SETCRTC and &DRM_IOCTL_MODE_PAGE_FLIP.

 *         Therefore user-space must not mix explicit usage of any primary

 *         plane (e.g. through an atomic commit) with these legacy IOCTLs.

 *

 *     "Cursor":

 *         To enable this plane, using a framebuffer configured without scaling

 *         or cropping and with the following properties is the most likely to

 *         work:

 *

 *         - If the driver provides the capabilities &DRM_CAP_CURSOR_WIDTH and

 *           &DRM_CAP_CURSOR_HEIGHT, create the framebuffer with this size.

 *           Otherwise, create a framebuffer with the size 64x64.

 *         - If the driver doesn't support modifiers, create a framebuffer with

 *           a linear layout. Otherwise, use the IN_FORMATS plane property.

 *

 *         Drivers may support more features for the cursor plane, user-space

 *         can find out with test-only atomic commits.

 *

 *         Some cursor planes are implicitly used by the kernel in the legacy

 *         IOCTLs &DRM_IOCTL_MODE_CURSOR and &DRM_IOCTL_MODE_CURSOR2.

 *         Therefore user-space must not mix explicit usage of any cursor

 *         plane (e.g. through an atomic commit) with these legacy IOCTLs.

 *

 *         Some drivers may support cursors even if no cursor plane is exposed.

 *         In this case, the legacy cursor IOCTLs can be used to configure the

 *         cursor.

 *

 *     "Overlay":

 *         Neither primary nor cursor.

 *

 *         Overlay planes are the only planes exposed when the

 *         &DRM_CLIENT_CAP_UNIVERSAL_PLANES capability is disabled.

 *

 * IN_FORMATS:

 *     Blob property which contains the set of buffer format and modifier

 *     pairs supported by this plane. The blob is a struct

 *     drm_format_modifier_blob. Without this property the plane doesn't

 *     support buffers with modifiers. Userspace cannot change this property.

 *

 *     Note that userspace can check the &DRM_CAP_ADDFB2_MODIFIERS driver

 *     capability for general modifier support. If this flag is set then every

 *     plane will have the IN_FORMATS property, even when it only supports

 *     DRM_FORMAT_MOD_LINEAR. Before linux kernel release v5.1 there have been

 *     various bugs in this area with inconsistencies between the capability

 *     flag and per-plane properties.

 0 formats are never expected */

	/* Modifiers offset is a pointer to a struct with a 64 bit field so it

	 * should be naturally aligned to 8B.

 If we can't determine support, just bail */

 plane index is used with 32bit bitmasks */

	/*

	 * First driver to need more than 64 formats needs to fix this. Each

	 * format is encoded as a bit and the current code only supports a u64.

 autoset the cap and check for consistency across all planes */

/**

 * drm_universal_plane_init - Initialize a new universal plane object

 * @dev: DRM device

 * @plane: plane object to init

 * @possible_crtcs: bitmask of possible CRTCs

 * @funcs: callbacks for the new plane

 * @formats: array of supported formats (DRM_FORMAT\_\*)

 * @format_count: number of elements in @formats

 * @format_modifiers: array of struct drm_format modifiers terminated by

 *                    DRM_FORMAT_MOD_INVALID

 * @type: type of plane (overlay, primary, cursor)

 * @name: printf style format string for the plane name, or NULL for default name

 *

 * Initializes a plane object of type @type. The &drm_plane_funcs.destroy hook

 * should call drm_plane_cleanup() and kfree() the plane structure. The plane

 * structure should not be allocated with devm_kzalloc().

 *

 * Note: consider using drmm_universal_plane_alloc() instead of

 * drm_universal_plane_init() to let the DRM managed resource infrastructure

 * take care of cleanup and deallocation.

 *

 * Drivers supporting modifiers must set @format_modifiers on all their planes,

 * even those that only support DRM_FORMAT_MOD_LINEAR.

 *

 * Returns:

 * Zero on success, error code on failure.

/**

 * drm_plane_init - Initialize a legacy plane

 * @dev: DRM device

 * @plane: plane object to init

 * @possible_crtcs: bitmask of possible CRTCs

 * @funcs: callbacks for the new plane

 * @formats: array of supported formats (DRM_FORMAT\_\*)

 * @format_count: number of elements in @formats

 * @is_primary: plane type (primary vs overlay)

 *

 * Legacy API to initialize a DRM plane.

 *

 * New drivers should call drm_universal_plane_init() instead.

 *

 * Returns:

 * Zero on success, error code on failure.

/**

 * drm_plane_cleanup - Clean up the core plane usage

 * @plane: plane to cleanup

 *

 * This function cleans up @plane and removes it from the DRM mode setting

 * core. Note that the function does *not* free the plane structure itself,

 * this is the responsibility of the caller.

	/* Note that the plane_list is considered to be static; should we

	 * remove the drm_plane at runtime we would have to decrement all

	 * the indices on the drm_plane after us in the plane_list.

/**

 * drm_plane_from_index - find the registered plane at an index

 * @dev: DRM device

 * @idx: index of registered plane to find for

 *

 * Given a plane index, return the registered plane from DRM device's

 * list of planes with matching index. This is the inverse of drm_plane_index().

/**

 * drm_plane_force_disable - Forcibly disable a plane

 * @plane: plane to disable

 *

 * Forces the plane to be disabled.

 *

 * Used when the plane's current framebuffer is destroyed,

 * and when restoring fbdev mode.

 *

 * Note that this function is not suitable for atomic drivers, since it doesn't

 * wire through the lock acquisition context properly and hence can't handle

 * retries or driver private locks. You probably want to use

 * drm_atomic_helper_disable_plane() or

 * drm_atomic_helper_disable_planes_on_crtc() instead.

 disconnect the plane from the fb and crtc: */

/**

 * drm_mode_plane_set_obj_prop - set the value of a property

 * @plane: drm plane object to set property value for

 * @property: property to set

 * @value: value the property should be set to

 *

 * This functions sets a given property on a given plane object. This function

 * calls the driver's ->set_property callback and changes the software state of

 * the property if the callback succeeds.

 *

 * Returns:

 * Zero on success, error code on failure.

	/*

	 * This ioctl is called twice, once to determine how much space is

	 * needed, and the 2nd time to fill it.

		/*

		 * Unless userspace set the 'universal planes'

		 * capability bit, only advertise overlays.

	/*

	 * This ioctl is called twice, once to determine how much space is

	 * needed, and the 2nd time to fill it.

 Check whether this plane is usable on this CRTC */

 Check whether this plane supports the fb pixel format. */

 Give drivers some help against integer overflows */

/**

 * drm_any_plane_has_format - Check whether any plane supports this format and modifier combination

 * @dev: DRM device

 * @format: pixel format (DRM_FORMAT_*)

 * @modifier: data layout modifier

 *

 * Returns:

 * Whether at least one plane supports the specified format and modifier combination.

/*

 * __setplane_internal - setplane handler for internal callers

 *

 * This function will take a reference on the new fb for the plane

 * on success.

 *

 * src_{x,y,w,h} are provided in 16.16 fixed point format

 src_{x,y,w,h} values are 16.16 fixed point */

 No fb means shut it down */

 No fb means shut it down */

	/*

	 * FIXME: This is redundant with drm_atomic_plane_check(),

	 * but the legacy cursor/"async" .update_plane() tricks

	 * don't call that so we still need this here. Should remove

	 * this when all .update_plane() implementations have been

	 * fixed to call drm_atomic_plane_check().

 src_{x,y,w,h} values are 16.16 fixed point */

	/*

	 * First, find the plane, crtc, and fb objects.  If not available,

	 * we don't bother to call the driver.

	/*

	 * Obtain fb we'll be using (either new or existing) and take an extra

	 * reference to it if fb != null.  setplane will take care of dropping

	 * the reference if the plane update fails.

 Update successful; save new cursor position, if necessary */

	/*

	 * If this crtc has a universal cursor plane, call that plane's update

	 * handler rather than using legacy cursor handlers.

 Turns off the cursor if handle is 0 */

/*

 * Set the cursor configuration based on user request. This implements the 2nd

 * version of the cursor ioctl, which allows userspace to additionally specify

 * the hotspot of the pointer.

	/* Only one of the DRM_MODE_PAGE_FLIP_TARGET_ABSOLUTE/RELATIVE flags

	 * can be specified

		/* The framebuffer is currently unbound, presumably

		 * due to a hotplug event, that userspace has not

		 * yet discovered.

	/*

	 * Only check the FOURCC format code, excluding modifiers. This is

	 * enough for all legacy drivers. Atomic drivers have their own

	 * checks in their ->atomic_check implementation, which will

	 * return -EINVAL if any hw or driver constraint is violated due

	 * to modifier changes.

 Keep the old fb, don't unref it. */

/**

 * DOC: damage tracking

 *

 * FB_DAMAGE_CLIPS is an optional plane property which provides a means to

 * specify a list of damage rectangles on a plane in framebuffer coordinates of

 * the framebuffer attached to the plane. In current context damage is the area

 * of plane framebuffer that has changed since last plane update (also called

 * page-flip), irrespective of whether currently attached framebuffer is same as

 * framebuffer attached during last plane update or not.

 *

 * FB_DAMAGE_CLIPS is a hint to kernel which could be helpful for some drivers

 * to optimize internally especially for virtual devices where each framebuffer

 * change needs to be transmitted over network, usb, etc.

 *

 * Since FB_DAMAGE_CLIPS is a hint so it is an optional property. User-space can

 * ignore damage clips property and in that case driver will do a full plane

 * update. In case damage clips are provided then it is guaranteed that the area

 * inside damage clips will be updated to plane. For efficiency driver can do

 * full update or can update more than specified in damage clips. Since driver

 * is free to read more, user-space must always render the entire visible

 * framebuffer. Otherwise there can be corruptions. Also, if a user-space

 * provides damage clips which doesn't encompass the actual damage to

 * framebuffer (since last plane update) can result in incorrect rendering.

 *

 * FB_DAMAGE_CLIPS is a blob property with the layout of blob data is simply an

 * array of &drm_mode_rect. Unlike plane &drm_plane_state.src coordinates,

 * damage clips are not in 16.16 fixed point. Similar to plane src in

 * framebuffer, damage clips cannot be negative. In damage clip, x1/y1 are

 * inclusive and x2/y2 are exclusive. While kernel does not error for overlapped

 * damage clips, it is strongly discouraged.

 *

 * Drivers that are interested in damage interface for plane should enable

 * FB_DAMAGE_CLIPS property by calling drm_plane_enable_fb_damage_clips().

 * Drivers implementing damage can use drm_atomic_helper_damage_iter_init() and

 * drm_atomic_helper_damage_iter_next() helper iterator function to get damage

 * rectangles clipped to &drm_plane_state.src.

/**

 * drm_plane_enable_fb_damage_clips - Enables plane fb damage clips property.

 * @plane: Plane on which to enable damage clips property.

 *

 * This function lets driver to enable the damage clips property on a plane.

/**

 * drm_plane_get_damage_clips_count - Returns damage clips count.

 * @state: Plane state.

 *

 * Simple helper to get the number of &drm_mode_rect clips set by user-space

 * during plane update.

 *

 * Return: Number of clips in plane fb_damage_clips blob property.

/**

 * drm_plane_get_damage_clips - Returns damage clips.

 * @state: Plane state.

 *

 * Note that this function returns uapi type &drm_mode_rect. Drivers might want

 * to use the helper functions drm_atomic_helper_damage_iter_init() and

 * drm_atomic_helper_damage_iter_next() or drm_atomic_helper_damage_merged() if

 * the driver can only handle a single damage region at most.

 *

 * Return: Damage clips in plane fb_damage_clips blob property.

 check that drm_plane_enable_fb_damage_clips() was called */

/**

 * drm_plane_create_scaling_filter_property - create a new scaling filter

 * property

 *

 * @plane: drm plane

 * @supported_filters: bitmask of supported scaling filters, must include

 *		       BIT(DRM_SCALING_FILTER_DEFAULT).

 *

 * This function lets driver to enable the scaling filter property on a given

 * plane.

 *

 * RETURNS:

 * Zero for success or -errno

 SPDX-License-Identifier: GPL-2.0

/*

 * (C) COPYRIGHT 2016 ARM Limited. All rights reserved.

 * Author: Brian Starkey <brian.starkey@arm.com>

 *

 * This program is free software and is provided to you under the terms of the

 * GNU General Public License version 2 as published by the Free Software

 * Foundation, and any use by you of this program is subject to the terms

 * of such GNU licence.

/**

 * DOC: overview

 *

 * Writeback connectors are used to expose hardware which can write the output

 * from a CRTC to a memory buffer. They are used and act similarly to other

 * types of connectors, with some important differences:

 *

 * * Writeback connectors don't provide a way to output visually to the user.

 *

 * * Writeback connectors are visible to userspace only when the client sets

 *   DRM_CLIENT_CAP_WRITEBACK_CONNECTORS.

 *

 * * Writeback connectors don't have EDID.

 *

 * A framebuffer may only be attached to a writeback connector when the

 * connector is attached to a CRTC. The WRITEBACK_FB_ID property which sets the

 * framebuffer applies only to a single commit (see below). A framebuffer may

 * not be attached while the CRTC is off.

 *

 * Unlike with planes, when a writeback framebuffer is removed by userspace DRM

 * makes no attempt to remove it from active use by the connector. This is

 * because no method is provided to abort a writeback operation, and in any

 * case making a new commit whilst a writeback is ongoing is undefined (see

 * WRITEBACK_OUT_FENCE_PTR below). As soon as the current writeback is finished,

 * the framebuffer will automatically no longer be in active use. As it will

 * also have already been removed from the framebuffer list, there will be no

 * way for any userspace application to retrieve a reference to it in the

 * intervening period.

 *

 * Writeback connectors have some additional properties, which userspace

 * can use to query and control them:

 *

 *  "WRITEBACK_FB_ID":

 *	Write-only object property storing a DRM_MODE_OBJECT_FB: it stores the

 *	framebuffer to be written by the writeback connector. This property is

 *	similar to the FB_ID property on planes, but will always read as zero

 *	and is not preserved across commits.

 *	Userspace must set this property to an output buffer every time it

 *	wishes the buffer to get filled.

 *

 *  "WRITEBACK_PIXEL_FORMATS":

 *	Immutable blob property to store the supported pixel formats table. The

 *	data is an array of u32 DRM_FORMAT_* fourcc values.

 *	Userspace can use this blob to find out what pixel formats are supported

 *	by the connector's writeback engine.

 *

 *  "WRITEBACK_OUT_FENCE_PTR":

 *	Userspace can use this property to provide a pointer for the kernel to

 *	fill with a sync_file file descriptor, which will signal once the

 *	writeback is finished. The value should be the address of a 32-bit

 *	signed integer, cast to a u64.

 *	Userspace should wait for this fence to signal before making another

 *	commit affecting any of the same CRTCs, Planes or Connectors.

 *	**Failure to do so will result in undefined behaviour.**

 *	For this reason it is strongly recommended that all userspace

 *	applications making use of writeback connectors *always* retrieve an

 *	out-fence for the commit and use it appropriately.

 *	From userspace, this property will always read as zero.

/**

 * drm_writeback_connector_init - Initialize a writeback connector and its properties

 * @dev: DRM device

 * @wb_connector: Writeback connector to initialize

 * @con_funcs: Connector funcs vtable

 * @enc_helper_funcs: Encoder helper funcs vtable to be used by the internal encoder

 * @formats: Array of supported pixel formats for the writeback engine

 * @n_formats: Length of the formats array

 *

 * This function creates the writeback-connector-specific properties if they

 * have not been already created, initializes the connector as

 * type DRM_MODE_CONNECTOR_WRITEBACK, and correctly initializes the property

 * values. It will also create an internal encoder associated with the

 * drm_writeback_connector and set it to use the @enc_helper_funcs vtable for

 * the encoder helper.

 *

 * Drivers should always use this function instead of drm_connector_init() to

 * set up writeback connectors.

 *

 * Returns: 0 on success, or a negative error code

/**

 * drm_writeback_queue_job - Queue a writeback job for later signalling

 * @wb_connector: The writeback connector to queue a job on

 * @conn_state: The connector state containing the job to queue

 *

 * This function adds the job contained in @conn_state to the job_queue for a

 * writeback connector. It takes ownership of the writeback job and sets the

 * @conn_state->writeback_job to NULL, and so no access to the job may be

 * performed by the caller after this function returns.

 *

 * Drivers must ensure that for a given writeback connector, jobs are queued in

 * exactly the same order as they will be completed by the hardware (and

 * signaled via drm_writeback_signal_completion).

 *

 * For every call to drm_writeback_queue_job() there must be exactly one call to

 * drm_writeback_signal_completion()

 *

 * See also: drm_writeback_signal_completion()

/*

 * @cleanup_work: deferred cleanup of a writeback job

 *

 * The job cannot be cleaned up directly in drm_writeback_signal_completion,

 * because it may be called in interrupt context. Dropping the framebuffer

 * reference can sleep, and so the cleanup is deferred to a workqueue.

/**

 * drm_writeback_signal_completion - Signal the completion of a writeback job

 * @wb_connector: The writeback connector whose job is complete

 * @status: Status code to set in the writeback out_fence (0 for success)

 *

 * Drivers should call this to signal the completion of a previously queued

 * writeback job. It should be called as soon as possible after the hardware

 * has finished writing, and may be called from interrupt context.

 * It is the driver's responsibility to ensure that for a given connector, the

 * hardware completes writeback jobs in the same order as they are queued.

 *

 * Unless the driver is holding its own reference to the framebuffer, it must

 * not be accessed after calling this function.

 *

 * See also: drm_writeback_queue_job()

/*

 * Copyright (c) 2016 Intel Corporation

 *

 * Permission to use, copy, modify, distribute, and sell this software and its

 * documentation for any purpose is hereby granted without fee, provided that

 * the above copyright notice appear in all copies and that both that copyright

 * notice and this permission notice appear in supporting documentation, and

 * that the name of the copyright holders not be used in advertising or

 * publicity pertaining to distribution of the software without specific,

 * written prior permission.  The copyright holders make no representations

 * about the suitability of this software for any purpose.  It is provided "as

 * is" without express or implied warranty.

 *

 * THE COPYRIGHT HOLDERS DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,

 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO

 * EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY SPECIAL, INDIRECT OR

 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE,

 * DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER

 * TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE

 * OF THIS SOFTWARE.

/*

 * Internal function to assign a slot in the object idr and optionally

 * register the object into the idr.

		/*

		 * Set up the object linking under the protection of the idr

		 * lock so that other users can't see inconsistent state.

/**

 * drm_mode_object_add - allocate a new modeset identifier

 * @dev: DRM device

 * @obj: object pointer, used to generate unique ID

 * @obj_type: object type

 *

 * Create a unique identifier based on @ptr in @dev's identifier space.  Used

 * for tracking modes, CRTCs and connectors.

 *

 * Returns:

 * Zero on success, error code on failure.

/**

 * drm_mode_object_unregister - free a modeset identifier

 * @dev: DRM device

 * @object: object to free

 *

 * Free @id from @dev's unique identifier pool.

 * This function can be called multiple times, and guards against

 * multiple removals.

 * These modeset identifiers are _not_ reference counted. Hence don't use this

 * for reference counted modeset objects like framebuffers.

/**

 * drm_mode_object_lease_required - check types which must be leased to be used

 * @type: type of object

 *

 * Returns whether the provided type of drm_mode_object must

 * be owned or leased to be used by a process.

/**

 * drm_mode_object_find - look up a drm object with static lifetime

 * @dev: drm device

 * @file_priv: drm file

 * @id: id of the mode object

 * @type: type of the mode object

 *

 * This function is used to look up a modeset object. It will acquire a

 * reference for reference counted objects. This reference must be dropped again

 * by callind drm_mode_object_put().

/**

 * drm_mode_object_put - release a mode object reference

 * @obj: DRM mode object

 *

 * This function decrements the object's refcount if it is a refcounted modeset

 * object. It is a no-op on any other object. This is used to drop references

 * acquired with drm_mode_object_get().

/**

 * drm_mode_object_get - acquire a mode object reference

 * @obj: DRM mode object

 *

 * This function increments the object's refcount if it is a refcounted modeset

 * object. It is a no-op on any other object. References should be dropped again

 * by calling drm_mode_object_put().

/**

 * drm_object_attach_property - attach a property to a modeset object

 * @obj: drm modeset object

 * @property: property to attach

 * @init_val: initial value of the property

 *

 * This attaches the given property to the modeset object with the given initial

 * value. Currently this function cannot fail since the properties are stored in

 * a statically sized array.

 *

 * Note that all properties must be attached before the object itself is

 * registered and accessible from userspace.

/**

 * drm_object_property_set_value - set the value of a property

 * @obj: drm mode object to set property value for

 * @property: property to set

 * @val: value the property should be set to

 *

 * This function sets a given property on a given object. This function only

 * changes the software state of the property, it does not call into the

 * driver's ->set_property callback.

 *

 * Note that atomic drivers should not have any need to call this, the core will

 * ensure consistency of values reported back to userspace through the

 * appropriate ->atomic_get_property callback. Only legacy drivers should call

 * this function to update the tracked value (after clamping and other

 * restrictions have been applied).

 *

 * Returns:

 * Zero on success, error code on failure.

	/* read-only properties bypass atomic mechanism and still store

	 * their value in obj->properties->values[].. mostly to avoid

	 * having to deal w/ EDID and similar props in atomic paths:

/**

 * drm_object_property_get_value - retrieve the value of a property

 * @obj: drm mode object to get property value from

 * @property: property to retrieve

 * @val: storage for the property value

 *

 * This function retrieves the softare state of the given property for the given

 * property. Since there is no driver callback to retrieve the current property

 * value this might be out of sync with the hardware, depending upon the driver

 * and property.

 *

 * Atomic drivers should never call this function directly, the core will read

 * out property values through the various ->atomic_get_property callbacks.

 *

 * Returns:

 * Zero on success, error code on failure.

 helper for getconnector and getproperties ioctls */

/**

 * drm_mode_obj_get_properties_ioctl - get the current value of a object's property

 * @dev: DRM device

 * @data: ioctl data

 * @file_priv: DRM file info

 *

 * This function retrieves the current value for an object's property. Compared

 * to the connector specific ioctl this one is extended to also work on crtc and

 * plane objects.

 *

 * Called by the user via ioctl.

 *

 * Returns:

 * Zero on success, negative errno on failure.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * drm gem CMA (contiguous memory allocator) helper functions

 *

 * Copyright (C) 2012 Sascha Hauer, Pengutronix

 *

 * Based on Samsung Exynos code

 *

 * Copyright (c) 2011 Samsung Electronics Co., Ltd.

/**

 * DOC: cma helpers

 *

 * The Contiguous Memory Allocator reserves a pool of memory at early boot

 * that is used to service requests for large blocks of contiguous memory.

 *

 * The DRM GEM/CMA helpers use this allocator as a means to provide buffer

 * objects that are physically contiguous in memory. This is useful for

 * display drivers that are unable to map scattered buffers via an IOMMU.

/**

 * __drm_gem_cma_create - Create a GEM CMA object without allocating memory

 * @drm: DRM device

 * @size: size of the object to allocate

 * @private: true if used for internal purposes

 *

 * This function creates and initializes a GEM CMA object of the given size,

 * but doesn't allocate any memory to back the object.

 *

 * Returns:

 * A struct drm_gem_cma_object * on success or an ERR_PTR()-encoded negative

 * error code on failure.

 Always use writecombine for dma-buf mappings */

/**

 * drm_gem_cma_create - allocate an object with the given size

 * @drm: DRM device

 * @size: size of the object to allocate

 *

 * This function creates a CMA GEM object and allocates a contiguous chunk of

 * memory as backing store.

 *

 * Returns:

 * A struct drm_gem_cma_object * on success or an ERR_PTR()-encoded negative

 * error code on failure.

/**

 * drm_gem_cma_create_with_handle - allocate an object with the given size and

 *     return a GEM handle to it

 * @file_priv: DRM file-private structure to register the handle for

 * @drm: DRM device

 * @size: size of the object to allocate

 * @handle: return location for the GEM handle

 *

 * This function creates a CMA GEM object, allocating a physically contiguous

 * chunk of memory as backing store. The GEM object is then added to the list

 * of object associated with the given file and a handle to it is returned.

 *

 * Returns:

 * A struct drm_gem_cma_object * on success or an ERR_PTR()-encoded negative

 * error code on failure.

	/*

	 * allocate a id of idr table where the obj is registered

	 * and handle has the id what user can see.

 drop reference from allocate - handle holds it now. */

/**

 * drm_gem_cma_free_object - free resources associated with a CMA GEM object

 * @gem_obj: GEM object to free

 *

 * This function frees the backing memory of the CMA GEM object, cleans up the

 * GEM object state and frees the memory used to store the object itself.

 * If the buffer is imported and the virtual address is set, it is released.

 * Drivers using the CMA helpers should set this as their

 * &drm_gem_object_funcs.free callback.

/**

 * drm_gem_cma_dumb_create_internal - create a dumb buffer object

 * @file_priv: DRM file-private structure to create the dumb buffer for

 * @drm: DRM device

 * @args: IOCTL data

 *

 * This aligns the pitch and size arguments to the minimum required. This is

 * an internal helper that can be wrapped by a driver to account for hardware

 * with more specific alignment requirements. It should not be used directly

 * as their &drm_driver.dumb_create callback.

 *

 * Returns:

 * 0 on success or a negative error code on failure.

/**

 * drm_gem_cma_dumb_create - create a dumb buffer object

 * @file_priv: DRM file-private structure to create the dumb buffer for

 * @drm: DRM device

 * @args: IOCTL data

 *

 * This function computes the pitch of the dumb buffer and rounds it up to an

 * integer number of bytes per pixel. Drivers for hardware that doesn't have

 * any additional restrictions on the pitch can directly use this function as

 * their &drm_driver.dumb_create callback.

 *

 * For hardware with additional restrictions, drivers can adjust the fields

 * set up by userspace and pass the IOCTL data along to the

 * drm_gem_cma_dumb_create_internal() function.

 *

 * Returns:

 * 0 on success or a negative error code on failure.

/**

 * drm_gem_cma_get_unmapped_area - propose address for mapping in noMMU cases

 * @filp: file object

 * @addr: memory address

 * @len: buffer size

 * @pgoff: page offset

 * @flags: memory flags

 *

 * This function is used in noMMU platforms to propose address mapping

 * for a given buffer.

 * It's intended to be used as a direct handler for the struct

 * &file_operations.get_unmapped_area operation.

 *

 * Returns:

 * mapping address on success or a negative error code on failure.

		/*

		 * When the object is being freed, after it hits 0-refcnt it

		 * proceeds to tear down the object. In the process it will

		 * attempt to remove the VMA offset and so acquire this

		 * mgr->vm_lock.  Therefore if we find an object with a 0-refcnt

		 * that matches our range, we know it is in the process of being

		 * destroyed and will be freed as soon as we release the lock -

		 * so we have to check for the 0-refcnted object and treat it as

		 * invalid.

/**

 * drm_gem_cma_print_info() - Print &drm_gem_cma_object info for debugfs

 * @p: DRM printer

 * @indent: Tab indentation level

 * @obj: GEM object

 *

 * This function can be used as the &drm_driver->gem_print_info callback.

 * It prints paddr and vaddr for use in e.g. debugfs output.

/**

 * drm_gem_cma_get_sg_table - provide a scatter/gather table of pinned

 *     pages for a CMA GEM object

 * @obj: GEM object

 *

 * This function exports a scatter/gather table by

 * calling the standard DMA mapping API. Drivers using the CMA helpers should

 * set this as their &drm_gem_object_funcs.get_sg_table callback.

 *

 * Returns:

 * A pointer to the scatter/gather table of pinned pages or NULL on failure.

/**

 * drm_gem_cma_prime_import_sg_table - produce a CMA GEM object from another

 *     driver's scatter/gather table of pinned pages

 * @dev: device to import into

 * @attach: DMA-BUF attachment

 * @sgt: scatter/gather table of pinned pages

 *

 * This function imports a scatter/gather table exported via DMA-BUF by

 * another driver. Imported buffers must be physically contiguous in memory

 * (i.e. the scatter/gather table must contain a single entry). Drivers that

 * use the CMA helpers should set this as their

 * &drm_driver.gem_prime_import_sg_table callback.

 *

 * Returns:

 * A pointer to a newly created GEM object or an ERR_PTR-encoded negative

 * error code on failure.

 check if the entries in the sg_table are contiguous */

 Create a CMA GEM buffer. */

/**

 * drm_gem_cma_vmap - map a CMA GEM object into the kernel's virtual

 *     address space

 * @obj: GEM object

 * @map: Returns the kernel virtual address of the CMA GEM object's backing

 *       store.

 *

 * This function maps a buffer into the kernel's

 * virtual address space. Since the CMA buffers are already mapped into the

 * kernel virtual address space this simply returns the cached virtual

 * address. Drivers using the CMA helpers should set this as their DRM

 * driver's &drm_gem_object_funcs.vmap callback.

 *

 * Returns:

 * 0 on success, or a negative error code otherwise.

/**

 * drm_gem_cma_mmap - memory-map an exported CMA GEM object

 * @obj: GEM object

 * @vma: VMA for the area to be mapped

 *

 * This function maps a buffer into a userspace process's address space.

 * In addition to the usual GEM VMA setup it immediately faults in the entire

 * object instead of using on-demand faulting. Drivers that use the CMA

 * helpers should set this as their &drm_gem_object_funcs.mmap callback.

 *

 * Returns:

 * 0 on success or a negative error code on failure.

	/*

	 * Clear the VM_PFNMAP flag that was set by drm_gem_mmap(), and set the

	 * vm_pgoff (used as a fake buffer offset by DRM) to 0 as we want to map

	 * the whole buffer.

/**

 * drm_gem_cma_prime_import_sg_table_vmap - PRIME import another driver's

 *	scatter/gather table and get the virtual address of the buffer

 * @dev: DRM device

 * @attach: DMA-BUF attachment

 * @sgt: Scatter/gather table of pinned pages

 *

 * This function imports a scatter/gather table using

 * drm_gem_cma_prime_import_sg_table() and uses dma_buf_vmap() to get the kernel

 * virtual address. This ensures that a CMA GEM object always has its virtual

 * address set. This address is released when the object is freed.

 *

 * This function can be used as the &drm_driver.gem_prime_import_sg_table

 * callback. The &DRM_GEM_CMA_DRIVER_OPS_VMAP macro provides a shortcut to set

 * the necessary DRM driver operations.

 *

 * Returns:

 * A pointer to a newly created GEM object or an ERR_PTR-encoded negative

 * error code on failure.

 SPDX-License-Identifier: MIT */

/*

 * drm_panel_orientation_quirks.c -- Quirks for non-normal panel orientation

 *

 * Copyright (C) 2017 Hans de Goede <hdegoede@redhat.com>

 *

 * Note the quirks in this file are shared with fbdev/efifb and as such

 * must not depend on other drm code.

/*

 * Some x86 clamshell design devices use portrait tablet screens and a display

 * engine which cannot rotate in hardware, so we need to rotate the fbcon to

 * compensate. Unfortunately these (cheap) devices also typically have quite

 * generic DMI data, so we match on a combination of DMI data, screen resolution

 * and a list of known BIOS dates to avoid false positives.

 Acer One 10 (S1003) */

 Asus T100HA */

 Asus T101HA */

 Asus T103HAF */

 AYA NEO 2021 */

 Chuwi HiBook (CWI514) */

 Above matches are too generic, add bios-date match */

 Chuwi Hi10 Pro (CWI529) */

 GPD MicroPC (generic strings, also match on bios date) */

 GPD MicroPC (later BIOS versions with proper DMI strings) */

	}, {	/*

		 * GPD Pocket, note that the the DMI data is less generic then

		 * it seems, devices with a board-vendor of "AMI Corporation"

		 * are quite rare, as are devices which have both board- *and*

		 * product-id set to "Default String"

 GPD Pocket 2 (generic strings, also match on bios date) */

 GPD Win (same note on DMI match as GPD Pocket) */

 GPD Win 2 (too generic strings, also match on bios date) */

 GPD Win 3 */

 I.T.Works TW891 */

 KD Kurio Smart C15200 2-in-1 */

	}, {	/*

		 * Lenovo Ideapad Miix 310 laptop, only some production batches

		 * have a portrait screen, the resolution checks makes the quirk

		 * apply only to those batches.

 Lenovo Ideapad Miix 320 */

 Lenovo Ideapad D330-10IGM (HD) */

 Lenovo Ideapad D330-10IGM (FHD) */

 OneGX1 Pro */

 Samsung GalaxyBook 10.6 */

 Valve Steam Deck */

 VIOS LTH17 */

/**

 * drm_get_panel_orientation_quirk - Check for panel orientation quirks

 * @width: width in pixels of the panel

 * @height: height in pixels of the panel

 *

 * This function checks for platform specific (e.g. DMI based) quirks

 * providing info on panel_orientation for systems where this cannot be

 * probed from the hard-/firm-ware. To avoid false-positive this function

 * takes the panel resolution as argument and checks that against the

 * resolution expected by the quirk-table entry.

 *

 * Note this function is also used outside of the drm-subsys, by for example

 * the efifb code. Because of this this function gets compiled into its own

 * kernel-module when built as a module.

 *

 * Returns:

 * A DRM_MODE_PANEL_ORIENTATION_* value if there is a quirk for this system,

 * or DRM_MODE_PANEL_ORIENTATION_UNKNOWN if there is no quirk.

 There are no quirks for non x86 devices yet */

/*

 * Copyright (C) 2014 Red Hat

 * Copyright (C) 2014 Intel Corp.

 * Copyright (c) 2020-2021, The Linux Foundation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included in

 * all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR

 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,

 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR

 * OTHER DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 * Rob Clark <robdclark@gmail.com>

 * Daniel Vetter <daniel.vetter@ffwll.ch>

/**

 * drm_crtc_commit_wait - Waits for a commit to complete

 * @commit: &drm_crtc_commit to wait for

 *

 * Waits for a given &drm_crtc_commit to be programmed into the

 * hardware and flipped to.

 *

 * Returns:

 *

 * 0 on success, a negative error code otherwise.

	/*

	 * Currently no support for overwriting flips, hence

	 * stall for previous one to execute completely.

/**

 * drm_atomic_state_default_release -

 * release memory initialized by drm_atomic_state_init

 * @state: atomic state

 *

 * Free all the memory allocated by drm_atomic_state_init.

 * This should only be used by drivers which are still subclassing

 * &drm_atomic_state and haven't switched to &drm_private_state yet.

/**

 * drm_atomic_state_init - init new atomic state

 * @dev: DRM device

 * @state: atomic state

 *

 * Default implementation for filling in a new atomic state.

 * This should only be used by drivers which are still subclassing

 * &drm_atomic_state and haven't switched to &drm_private_state yet.

	/* TODO legacy paths should maybe do a better job about

	 * setting this appropriately?

/**

 * drm_atomic_state_alloc - allocate atomic state

 * @dev: DRM device

 *

 * This allocates an empty atomic state to track updates.

/**

 * drm_atomic_state_default_clear - clear base atomic state

 * @state: atomic state

 *

 * Default implementation for clearing atomic state.

 * This should only be used by drivers which are still subclassing

 * &drm_atomic_state and haven't switched to &drm_private_state yet.

/**

 * drm_atomic_state_clear - clear state object

 * @state: atomic state

 *

 * When the w/w mutex algorithm detects a deadlock we need to back off and drop

 * all locks. So someone else could sneak in and change the current modeset

 * configuration. Which means that all the state assembled in @state is no

 * longer an atomic update to the current state, but to some arbitrary earlier

 * state. Which could break assumptions the driver's

 * &drm_mode_config_funcs.atomic_check likely relies on.

 *

 * Hence we must clear all cached state and completely start over, using this

 * function.

/**

 * __drm_atomic_state_free - free all memory for an atomic state

 * @ref: This atomic state to deallocate

 *

 * This frees all memory associated with an atomic state, including all the

 * per-object state for planes, CRTCs and connectors.

/**

 * drm_atomic_get_crtc_state - get CRTC state

 * @state: global atomic state object

 * @crtc: CRTC to get state object for

 *

 * This function returns the CRTC state for the given CRTC, allocating it if

 * needed. It will also grab the relevant CRTC lock to make sure that the state

 * is consistent.

 *

 * WARNING: Drivers may only add new CRTC states to a @state if

 * drm_atomic_state.allow_modeset is set, or if it's a driver-internal commit

 * not created by userspace through an IOCTL call.

 *

 * Returns:

 *

 * Either the allocated state or the error code encoded into the pointer. When

 * the error is EDEADLK then the w/w mutex code has detected a deadlock and the

 * entire atomic sequence must be restarted. All other errors are fatal.

	/* NOTE: we explicitly don't enforce constraints such as primary

	 * layer covering entire screen, since that is something we want

	 * to allow (on hw that supports it).  For hw that does not, it

	 * should be checked in driver's crtc->atomic_check() vfunc.

	 *

	 * TODO: Add generic modeset state checks once we support those.

	/* The state->enable vs. state->mode_blob checks can be WARN_ON,

	 * as this is a kernel-internal detail that userspace should never

	 * be able to trigger.

	/*

	 * Reject event generation for when a CRTC is off and stays off.

	 * It wouldn't be hard to implement this, but userspace has a track

	 * record of happily burning through 100% cpu (or worse, crash) when the

	 * display pipe is suspended. To avoid all that fun just reject updates

	 * that ask for events since likely that indicates a bug in the

	 * compositor's drawing loop. This is consistent with the vblank IOCTL

	 * and legacy page_flip IOCTL which also reject service on a disabled

	 * pipe.

/**

 * drm_atomic_get_plane_state - get plane state

 * @state: global atomic state object

 * @plane: plane to get state object for

 *

 * This function returns the plane state for the given plane, allocating it if

 * needed. It will also grab the relevant plane lock to make sure that the state

 * is consistent.

 *

 * Returns:

 *

 * Either the allocated state or the error code encoded into the pointer. When

 * the error is EDEADLK then the w/w mutex code has detected a deadlock and the

 * entire atomic sequence must be restarted. All other errors are fatal.

 the legacy pointers should never be set */

	/* This could be refined, but currently there's no helper or driver code

	 * to implement direct switching of active planes nor userspace to take

	 * advantage of more direct plane switching without the intermediate

	 * full OFF state.

/**

 * drm_atomic_plane_check - check plane state

 * @old_plane_state: old plane state to check

 * @new_plane_state: new plane state to check

 *

 * Provides core sanity checks for plane state.

 *

 * RETURNS:

 * Zero on success, error code on failure

 either *both* CRTC and FB must be set, or neither */

 if disabled, we don't care about the rest of the state: */

 Check whether this plane is usable on this CRTC */

 Check whether this plane supports the fb pixel format. */

 Give drivers some help against integer overflows */

 Make sure source coordinates are inside the fb. */

 Make sure damage clips are valid and inside the fb. */

/**

 * DOC: handling driver private state

 *

 * Very often the DRM objects exposed to userspace in the atomic modeset api

 * (&drm_connector, &drm_crtc and &drm_plane) do not map neatly to the

 * underlying hardware. Especially for any kind of shared resources (e.g. shared

 * clocks, scaler units, bandwidth and fifo limits shared among a group of

 * planes or CRTCs, and so on) it makes sense to model these as independent

 * objects. Drivers then need to do similar state tracking and commit ordering for

 * such private (since not exposed to userspace) objects as the atomic core and

 * helpers already provide for connectors, planes and CRTCs.

 *

 * To make this easier on drivers the atomic core provides some support to track

 * driver private state objects using struct &drm_private_obj, with the

 * associated state struct &drm_private_state.

 *

 * Similar to userspace-exposed objects, private state structures can be

 * acquired by calling drm_atomic_get_private_obj_state(). This also takes care

 * of locking, hence drivers should not have a need to call drm_modeset_lock()

 * directly. Sequence of the actual hardware state commit is not handled,

 * drivers might need to keep track of struct drm_crtc_commit within subclassed

 * structure of &drm_private_state as necessary, e.g. similar to

 * &drm_plane_state.commit. See also &drm_atomic_state.fake_commit.

 *

 * All private state structures contained in a &drm_atomic_state update can be

 * iterated using for_each_oldnew_private_obj_in_state(),

 * for_each_new_private_obj_in_state() and for_each_old_private_obj_in_state().

 * Drivers are recommended to wrap these for each type of driver private state

 * object they have, filtering on &drm_private_obj.funcs using for_each_if(), at

 * least if they want to iterate over all objects of a given type.

 *

 * An earlier way to handle driver private state was by subclassing struct

 * &drm_atomic_state. But since that encourages non-standard ways to implement

 * the check/commit split atomic requires (by using e.g. "check and rollback or

 * commit instead" of "duplicate state, check, then either commit or release

 * duplicated state) it is deprecated in favour of using &drm_private_state.

/**

 * drm_atomic_private_obj_init - initialize private object

 * @dev: DRM device this object will be attached to

 * @obj: private object

 * @state: initial private object state

 * @funcs: pointer to the struct of function pointers that identify the object

 * type

 *

 * Initialize the private object, which can be embedded into any

 * driver private object that needs its own atomic state.

/**

 * drm_atomic_private_obj_fini - finalize private object

 * @obj: private object

 *

 * Finalize the private object.

/**

 * drm_atomic_get_private_obj_state - get private object state

 * @state: global atomic state

 * @obj: private object to get the state for

 *

 * This function returns the private object state for the given private object,

 * allocating the state if needed. It will also grab the relevant private

 * object lock to make sure that the state is consistent.

 *

 * RETURNS:

 *

 * Either the allocated state or the error code encoded into a pointer.

/**

 * drm_atomic_get_old_private_obj_state

 * @state: global atomic state object

 * @obj: private_obj to grab

 *

 * This function returns the old private object state for the given private_obj,

 * or NULL if the private_obj is not part of the global atomic state.

/**

 * drm_atomic_get_new_private_obj_state

 * @state: global atomic state object

 * @obj: private_obj to grab

 *

 * This function returns the new private object state for the given private_obj,

 * or NULL if the private_obj is not part of the global atomic state.

/**

 * drm_atomic_get_old_connector_for_encoder - Get old connector for an encoder

 * @state: Atomic state

 * @encoder: The encoder to fetch the connector state for

 *

 * This function finds and returns the connector that was connected to @encoder

 * as specified by the @state.

 *

 * If there is no connector in @state which previously had @encoder connected to

 * it, this function will return NULL. While this may seem like an invalid use

 * case, it is sometimes useful to differentiate commits which had no prior

 * connectors attached to @encoder vs ones that did (and to inspect their

 * state). This is especially true in enable hooks because the pipeline has

 * changed.

 *

 * Returns: The old connector connected to @encoder, or NULL if the encoder is

 * not connected.

/**

 * drm_atomic_get_new_connector_for_encoder - Get new connector for an encoder

 * @state: Atomic state

 * @encoder: The encoder to fetch the connector state for

 *

 * This function finds and returns the connector that will be connected to

 * @encoder as specified by the @state.

 *

 * If there is no connector in @state which will have @encoder connected to it,

 * this function will return NULL. While this may seem like an invalid use case,

 * it is sometimes useful to differentiate commits which have no connectors

 * attached to @encoder vs ones that do (and to inspect their state). This is

 * especially true in disable hooks because the pipeline will change.

 *

 * Returns: The new connector connected to @encoder, or NULL if the encoder is

 * not connected.

/**

 * drm_atomic_get_connector_state - get connector state

 * @state: global atomic state object

 * @connector: connector to get state object for

 *

 * This function returns the connector state for the given connector,

 * allocating it if needed. It will also grab the relevant connector lock to

 * make sure that the state is consistent.

 *

 * Returns:

 *

 * Either the allocated state or the error code encoded into the pointer. When

 * the error is EDEADLK then the w/w mutex code has detected a deadlock and the

 * entire atomic sequence must be restarted. All other errors are fatal.

/**

 * drm_atomic_get_bridge_state - get bridge state

 * @state: global atomic state object

 * @bridge: bridge to get state object for

 *

 * This function returns the bridge state for the given bridge, allocating it

 * if needed. It will also grab the relevant bridge lock to make sure that the

 * state is consistent.

 *

 * Returns:

 *

 * Either the allocated state or the error code encoded into the pointer. When

 * the error is EDEADLK then the w/w mutex code has detected a deadlock and the

 * entire atomic sequence must be restarted.

/**

 * drm_atomic_get_old_bridge_state - get old bridge state, if it exists

 * @state: global atomic state object

 * @bridge: bridge to grab

 *

 * This function returns the old bridge state for the given bridge, or NULL if

 * the bridge is not part of the global atomic state.

/**

 * drm_atomic_get_new_bridge_state - get new bridge state, if it exists

 * @state: global atomic state object

 * @bridge: bridge to grab

 *

 * This function returns the new bridge state for the given bridge, or NULL if

 * the bridge is not part of the global atomic state.

/**

 * drm_atomic_add_encoder_bridges - add bridges attached to an encoder

 * @state: atomic state

 * @encoder: DRM encoder

 *

 * This function adds all bridges attached to @encoder. This is needed to add

 * bridge states to @state and make them available when

 * &drm_bridge_funcs.atomic_check(), &drm_bridge_funcs.atomic_pre_enable(),

 * &drm_bridge_funcs.atomic_enable(),

 * &drm_bridge_funcs.atomic_disable_post_disable() are called.

 *

 * Returns:

 * 0 on success or can fail with -EDEADLK or -ENOMEM. When the error is EDEADLK

 * then the w/w mutex code has detected a deadlock and the entire atomic

 * sequence must be restarted. All other errors are fatal.

 Skip bridges that don't implement the atomic state hooks. */

/**

 * drm_atomic_add_affected_connectors - add connectors for CRTC

 * @state: atomic state

 * @crtc: DRM CRTC

 *

 * This function walks the current configuration and adds all connectors

 * currently using @crtc to the atomic configuration @state. Note that this

 * function must acquire the connection mutex. This can potentially cause

 * unneeded serialization if the update is just for the planes on one CRTC. Hence

 * drivers and helpers should only call this when really needed (e.g. when a

 * full modeset needs to happen due to some change).

 *

 * Returns:

 * 0 on success or can fail with -EDEADLK or -ENOMEM. When the error is EDEADLK

 * then the w/w mutex code has detected a deadlock and the entire atomic

 * sequence must be restarted. All other errors are fatal.

	/*

	 * Changed connectors are already in @state, so only need to look

	 * at the connector_mask in crtc_state.

/**

 * drm_atomic_add_affected_planes - add planes for CRTC

 * @state: atomic state

 * @crtc: DRM CRTC

 *

 * This function walks the current configuration and adds all planes

 * currently used by @crtc to the atomic configuration @state. This is useful

 * when an atomic commit also needs to check all currently enabled plane on

 * @crtc, e.g. when changing the mode. It's also useful when re-enabling a CRTC

 * to avoid special code to force-enable all planes.

 *

 * Since acquiring a plane state will always also acquire the w/w mutex of the

 * current CRTC for that plane (if there is any) adding all the plane states for

 * a CRTC will not reduce parallelism of atomic updates.

 *

 * Returns:

 * 0 on success or can fail with -EDEADLK or -ENOMEM. When the error is EDEADLK

 * then the w/w mutex code has detected a deadlock and the entire atomic

 * sequence must be restarted. All other errors are fatal.

/**

 * drm_atomic_check_only - check whether a given config would work

 * @state: atomic configuration to check

 *

 * Note that this function can return -EDEADLK if the driver needed to acquire

 * more locks but encountered a deadlock. The caller must then do the usual w/w

 * backoff dance and restart. All other errors are fatal.

 *

 * Returns:

 * 0 on success, negative error code on failure.

	/*

	 * For commits that allow modesets drivers can add other CRTCs to the

	 * atomic commit, e.g. when they need to reallocate global resources.

	 * This can cause spurious EBUSY, which robs compositors of a very

	 * effective sanity check for their drawing loop. Therefor only allow

	 * drivers to add unrelated CRTC states for modeset commits.

	 *

	 * FIXME: Should add affected_crtc mask to the ATOMIC IOCTL as an output

	 * so compositors know what's going on.

/**

 * drm_atomic_commit - commit configuration atomically

 * @state: atomic configuration to check

 *

 * Note that this function can return -EDEADLK if the driver needed to acquire

 * more locks but encountered a deadlock. The caller must then do the usual w/w

 * backoff dance and restart. All other errors are fatal.

 *

 * This function will take its own reference on @state.

 * Callers should always release their reference with drm_atomic_state_put().

 *

 * Returns:

 * 0 on success, negative error code on failure.

/**

 * drm_atomic_nonblocking_commit - atomic nonblocking commit

 * @state: atomic configuration to check

 *

 * Note that this function can return -EDEADLK if the driver needed to acquire

 * more locks but encountered a deadlock. The caller must then do the usual w/w

 * backoff dance and restart. All other errors are fatal.

 *

 * This function will take its own reference on @state.

 * Callers should always release their reference with drm_atomic_state_put().

 *

 * Returns:

 * 0 on success, negative error code on failure.

 just used from drm-client and atomic-helper: */

 First disable all connectors on the target crtc. */

 Make sure legacy setCrtc always re-trains */

 Then set all connectors from set->connectors on the target crtc */

		/*

		 * Don't update ->enable for the CRTC in the set_config request,

		 * since a mismatch would indicate a bug in the upper layers.

		 * The actual modeset code later on will catch any

		 * inconsistencies here.

 just used from drm-client and atomic-helper: */

/**

 * drm_atomic_print_new_state - prints drm atomic state

 * @state: atomic configuration to check

 * @p: drm printer

 *

 * This functions prints the drm atomic state snapshot using the drm printer

 * which is passed to it. This snapshot can be used for debugging purposes.

 *

 * Note that this function looks into the new state objects and hence its not

 * safe to be used after the call to drm_atomic_helper_commit_hw_done().

/**

 * drm_state_dump - dump entire device atomic state

 * @dev: the drm device

 * @p: where to print the state to

 *

 * Just for debugging.  Drivers might want an option to dump state

 * to dmesg in case of error irq's.  (Hint, you probably want to

 * ratelimit this!)

 *

 * The caller must wrap this drm_modeset_lock_all_ctx() and

 * drm_modeset_drop_locks(). If this is called from error irq handler, it should

 * not be enabled by default - if you are debugging errors you might

 * not care that this is racey, but calling this without all modeset locks held

 * is inherently unsafe.

 any use in debugfs files to dump individual planes/crtc/etc? */

/*

 * Copyright (C) 2014 Red Hat

 * Copyright (C) 2014 Intel Corp.

 * Copyright (C) 2018 Intel Corp.

 * Copyright (c) 2020, The Linux Foundation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included in

 * all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR

 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,

 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR

 * OTHER DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 * Rob Clark <robdclark@gmail.com>

 * Daniel Vetter <daniel.vetter@ffwll.ch>

/**

 * DOC: overview

 *

 * This file contains the marshalling and demarshalling glue for the atomic UAPI

 * in all its forms: The monster ATOMIC IOCTL itself, code for GET_PROPERTY and

 * SET_PROPERTY IOCTLs. Plus interface functions for compatibility helpers and

 * drivers which have special needs to construct their own atomic updates, e.g.

 * for load detect or similar.

/**

 * drm_atomic_set_mode_for_crtc - set mode for CRTC

 * @state: the CRTC whose incoming state to update

 * @mode: kernel-internal mode to use for the CRTC, or NULL to disable

 *

 * Set a mode (originating from the kernel) on the desired CRTC state and update

 * the enable property.

 *

 * RETURNS:

 * Zero on success, error code on failure. Cannot return -EDEADLK.

 Early return for no change. */

/**

 * drm_atomic_set_mode_prop_for_crtc - set mode for CRTC

 * @state: the CRTC whose incoming state to update

 * @blob: pointer to blob property to use for mode

 *

 * Set a mode (originating from a blob property) on the desired CRTC state.

 * This function will take a reference on the blob property for the CRTC state,

 * and release the reference held on the state's existing mode property, if any

 * was set.

 *

 * RETURNS:

 * Zero on success, error code on failure. Cannot return -EDEADLK.

/**

 * drm_atomic_set_crtc_for_plane - set CRTC for plane

 * @plane_state: the plane whose incoming state to update

 * @crtc: CRTC to use for the plane

 *

 * Changing the assigned CRTC for a plane requires us to grab the lock and state

 * for the new CRTC, as needed. This function takes care of all these details

 * besides updating the pointer in the state object itself.

 *

 * Returns:

 * 0 on success or can fail with -EDEADLK or -ENOMEM. When the error is EDEADLK

 * then the w/w mutex code has detected a deadlock and the entire atomic

 * sequence must be restarted. All other errors are fatal.

 Nothing to do for same crtc*/

/**

 * drm_atomic_set_fb_for_plane - set framebuffer for plane

 * @plane_state: atomic state object for the plane

 * @fb: fb to use for the plane

 *

 * Changing the assigned framebuffer for a plane requires us to grab a reference

 * to the new fb and drop the reference to the old fb, if there is one. This

 * function takes care of all these details besides updating the pointer in the

 * state object itself.

/**

 * drm_atomic_set_fence_for_plane - set fence for plane

 * @plane_state: atomic state object for the plane

 * @fence: dma_fence to use for the plane

 *

 * Helper to setup the plane_state fence in case it is not set yet.

 * By using this drivers doesn't need to worry if the user choose

 * implicit or explicit fencing.

 *

 * This function will not set the fence to the state if it was set

 * via explicit fencing interfaces on the atomic ioctl. In that case it will

 * drop the reference to the fence as we are not storing it anywhere.

 * Otherwise, if &drm_plane_state.fence is not set this function we just set it

 * with the received implicit fence. In both cases this function consumes a

 * reference for @fence.

 *

 * This way explicit fencing can be used to overrule implicit fencing, which is

 * important to make explicit fencing use-cases work: One example is using one

 * buffer for 2 screens with different refresh rates. Implicit fencing will

 * clamp rendering to the refresh rate of the slower screen, whereas explicit

 * fence allows 2 independent render and display loops on a single buffer. If a

 * driver allows obeys both implicit and explicit fences for plane updates, then

 * it will break all the benefits of explicit fencing.

/**

 * drm_atomic_set_crtc_for_connector - set CRTC for connector

 * @conn_state: atomic state object for the connector

 * @crtc: CRTC to use for the connector

 *

 * Changing the assigned CRTC for a connector requires us to grab the lock and

 * state for the new CRTC, as needed. This function takes care of all these

 * details besides updating the pointer in the state object itself.

 *

 * Returns:

 * 0 on success or can fail with -EDEADLK or -ENOMEM. When the error is EDEADLK

 * then the w/w mutex code has detected a deadlock and the entire atomic

 * sequence must be restarted. All other errors are fatal.

		/* setting DPMS property requires special handling, which

		 * is done in legacy setprop path for us.  Disallow (for

		 * now?) atomic writes to DPMS property:

		/* Never downgrade from GOOD to BAD on userspace's request here,

		 * only hw issues can do that.

		 *

		 * For an atomic property the userspace doesn't need to be able

		 * to understand all the properties, but needs to be able to

		 * restore the state it wants on VT switch. So if the userspace

		 * tries to change the link_status from GOOD to BAD, driver

		 * silently rejects it and returns a 0. This prevents userspace

		 * from accidentally breaking  the display when it restores the

		 * state.

 Writeback framebuffer is one-shot, write and forget */

/*

 * The big monster ioctl

/**

 * DOC: explicit fencing properties

 *

 * Explicit fencing allows userspace to control the buffer synchronization

 * between devices. A Fence or a group of fences are transferred to/from

 * userspace using Sync File fds and there are two DRM properties for that.

 * IN_FENCE_FD on each DRM Plane to send fences to the kernel and

 * OUT_FENCE_PTR on each DRM CRTC to receive fences from the kernel.

 *

 * As a contrast, with implicit fencing the kernel keeps track of any

 * ongoing rendering, and automatically ensures that the atomic update waits

 * for any pending rendering to complete. For shared buffers represented with

 * a &struct dma_buf this is tracked in &struct dma_resv.

 * Implicit syncing is how Linux traditionally worked (e.g. DRI2/3 on X.org),

 * whereas explicit fencing is what Android wants.

 *

 * "IN_FENCE_FDâ:

 *	Use this property to pass a fence that DRM should wait on before

 *	proceeding with the Atomic Commit request and show the framebuffer for

 *	the plane on the screen. The fence can be either a normal fence or a

 *	merged one, the sync_file framework will handle both cases and use a

 *	fence_array if a merged fence is received. Passing -1 here means no

 *	fences to wait on.

 *

 *	If the Atomic Commit request has the DRM_MODE_ATOMIC_TEST_ONLY flag

 *	it will only check if the Sync File is a valid one.

 *

 *	On the driver side the fence is stored on the @fence parameter of

 *	&struct drm_plane_state. Drivers which also support implicit fencing

 *	should set the implicit fence using drm_atomic_set_fence_for_plane(),

 *	to make sure there's consistent behaviour between drivers in precedence

 *	of implicit vs. explicit fencing.

 *

 * "OUT_FENCE_PTRâ:

 *	Use this property to pass a file descriptor pointer to DRM. Once the

 *	Atomic Commit request call returns OUT_FENCE_PTR will be filled with

 *	the file descriptor number of a Sync File. This Sync File contains the

 *	CRTC fence that will be signaled when all framebuffers present on the

 *	Atomic Commit * request for that given CRTC are scanned out on the

 *	screen.

 *

 *	The Atomic Commit request fails if a invalid pointer is passed. If the

 *	Atomic Commit request fails for any other reason the out fence fd

 *	returned will be -1. On a Atomic Commit with the

 *	DRM_MODE_ATOMIC_TEST_ONLY flag the out fence will also be set to -1.

 *

 *	Note that out-fences don't have a special interface to drivers and are

 *	internally represented by a &struct drm_pending_vblank_event in struct

 *	&drm_crtc_state, which is also used by the nonblocking atomic commit

 *	helpers and for the DRM event handling for existing userspace.

	/*

	 * Having this flag means user mode pends on event which will never

	 * reach due to lack of at least one CRTC for signaling

		/*

		 * Free the allocated event. drm_atomic_helper_setup_commit

		 * can allocate an event too, so only free it if it's ours

		 * to prevent a double free in drm_atomic_state_clear.

 If this fails log error to the user */

 disallow for drivers not supporting atomic: */

	/* disallow for userspace that has not enabled atomic cap (even

	 * though this may be a bit overkill, since legacy userspace

	 * wouldn't know how to call this ioctl)

 can't test and expect an event at the same time. */

/*

 * Copyright Â© 1997-2003 by The XFree86 Project, Inc.

 * Copyright Â© 2007 Dave Airlie

 * Copyright Â© 2007-2008 Intel Corporation

 *   Jesse Barnes <jesse.barnes@intel.com>

 * Copyright 2005-2006 Luc Verhaegen

 * Copyright (c) 2001, Andy Ritger  aritger@nvidia.com

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included in

 * all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR

 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,

 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR

 * OTHER DEALINGS IN THE SOFTWARE.

 *

 * Except as contained in this notice, the name of the copyright holder(s)

 * and author(s) shall not be used in advertising or otherwise to promote

 * the sale, use or other dealings in this Software without prior written

 * authorization from the copyright holder(s) and author(s).

/**

 * drm_mode_debug_printmodeline - print a mode to dmesg

 * @mode: mode to print

 *

 * Describe @mode using DRM_DEBUG.

/**

 * drm_mode_create - create a new display mode

 * @dev: DRM device

 *

 * Create a new, cleared drm_display_mode with kzalloc, allocate an ID for it

 * and return it.

 *

 * Returns:

 * Pointer to new mode on success, NULL on error.

/**

 * drm_mode_destroy - remove a mode

 * @dev: DRM device

 * @mode: mode to remove

 *

 * Release @mode's unique ID, then free it @mode structure itself using kfree.

/**

 * drm_mode_probed_add - add a mode to a connector's probed_mode list

 * @connector: connector the new mode

 * @mode: mode data

 *

 * Add @mode to @connector's probed_mode list for later use. This list should

 * then in a second step get filtered and all the modes actually supported by

 * the hardware moved to the @connector's modes list.

/**

 * drm_cvt_mode -create a modeline based on the CVT algorithm

 * @dev: drm device

 * @hdisplay: hdisplay size

 * @vdisplay: vdisplay size

 * @vrefresh: vrefresh rate

 * @reduced: whether to use reduced blanking

 * @interlaced: whether to compute an interlaced mode

 * @margins: whether to add margins (borders)

 *

 * This function is called to generate the modeline based on CVT algorithm

 * according to the hdisplay, vdisplay, vrefresh.

 * It is based from the VESA(TM) Coordinated Video Timing Generator by

 * Graham Loveridge April 9, 2003 available at

 * http://www.elo.utfsm.cl/~elo212/docs/CVTd6r1.xls 

 *

 * And it is copied from xf86CVTmode in xserver/hw/xfree86/modes/xf86cvt.c.

 * What I have done is to translate it by using integer calculation.

 *

 * Returns:

 * The modeline based on the CVT algorithm stored in a drm_display_mode object.

 * The display mode object is allocated with drm_mode_create(). Returns NULL

 * when no mode could be allocated.

 1) top/bottom margin size (% of height) - default: 1.8, */

 2) character cell horizontal granularity (pixels) - default 8 */

 3) Minimum vertical porch (lines) - default 3 */

 4) Minimum number of vertical back porch lines - default 6 */

 Pixel Clock step (kHz) */

	/* allocate the drm_display_mode structure. If failure, we will

	 * return directly

 the CVT default refresh rate is 60Hz */

 the required field fresh rate */

 horizontal pixels */

 determine the left&right borders */

 find the total active pixels */

 find the number of lines per field */

 find the top & bottom borders */

 Interlaced */

 Determine VSync Width from aspect ratio */

 custom */

 simplify the GTF calculation */

		/* 4) Minimum time of vertical sync + back porch interval (Âµs)

		 * default 550.0

 3) Nominal HSync width (% of line period) - default 8 */

 estimated the horizontal period */

 9. Find number of lines in sync + backporch */

 10. Find number of lines in back porch */

 5) Definition of Horizontal blanking time limitation */

 Gradient (%/kHz) - default 600 */

 Offset (%) - default 40 */

 Blanking time scaling factor - default 128 */

 Scaling factor weighting - default 20 */

 12. Find ideal blanking duty cycle from formula */

 13. Blanking time */

 14. find the total pixels per line */

 fill the Vsync values */

 Reduced blanking */

 Minimum vertical blanking interval time (Âµs)- default 460 */

 Fixed number of clocks for horizontal sync */

 Fixed number of clocks for horizontal blanking */

 Fixed number of lines for vertical front porch - default 3*/

 8. Estimate Horizontal period. */

 9. Find number of lines in vertical blanking */

 10. Check if vertical blanking is sufficient */

 11. Find total number of lines in vertical field */

 12. Find total number of pixels in a line */

 Fill in HSync values */

 Fill in VSync values */

 15/13. Find pixel clock frequency (kHz for xf86) */

 perform intermediate calcs in u64 */

 18/16. Find actual vertical frame frequency */

 ignore - just set the mode flag for interlaced */

 Fill the mode line name */

/**

 * drm_gtf_mode_complex - create the modeline based on the full GTF algorithm

 * @dev: drm device

 * @hdisplay: hdisplay size

 * @vdisplay: vdisplay size

 * @vrefresh: vrefresh rate.

 * @interlaced: whether to compute an interlaced mode

 * @margins: desired margin (borders) size

 * @GTF_M: extended GTF formula parameters

 * @GTF_2C: extended GTF formula parameters

 * @GTF_K: extended GTF formula parameters

 * @GTF_2J: extended GTF formula parameters

 *

 * GTF feature blocks specify C and J in multiples of 0.5, so we pass them

 * in here multiplied by two.  For a C of 40, pass in 80.

 *

 * Returns:

 * The modeline based on the full GTF algorithm stored in a drm_display_mode object.

 * The display mode object is allocated with drm_mode_create(). Returns NULL

 * when no mode could be allocated.

 1) top/bottom margin size (% of height) - default: 1.8, */

 2) character cell horizontal granularity (pixels) - default 8 */

 3) Minimum vertical porch (lines) - default 3 */

 width of vsync in lines */

 width of hsync as % of total line */

 min time of vsync + back porch (microsec) */

 C' and M' are part of the Blanking Duty Cycle computation */

	/* 1. In order to give correct results, the number of horizontal

	 * pixels requested is first processed to ensure that it is divisible

	 * by the character size, by rounding it to the nearest character

	 * cell boundary:

	/* 2. If interlace is requested, the number of vertical lines assumed

	 * by the calculation must be halved, as the computation calculates

	 * the number of vertical lines per field.

 3. Find the frame rate required: */

 4. Find number of lines in Top margin: */

 5. Find number of lines in bottom margin: */

 6. If interlace is required, then set variable interlace: */

 7. Estimate the Horizontal frequency */

 8. Find the number of lines in V sync + back porch */

 [V SYNC+BP] = RINT(([MIN VSYNC+BP] * hfreq_est / 1000000)) */

  9. Find the number of lines in V back porch alone: */

  10. Find the total number of lines in Vertical field period: */

  11. Estimate the Vertical field frequency: */

  12. Find the actual horizontal period: */

  13. Find the actual Vertical field frequency: */

  14. Find the Vertical frame frequency: */

  15. Find number of pixels in left margin: */

 16.Find number of pixels in right margin: */

 17.Find total number of active pixels in image and left and right */

 18.Find the ideal blanking duty cycle from blanking duty cycle */

	/* 19.Find the number of pixels in the blanking time to the nearest

 20.Find total number of pixels: */

 21.Find pixel clock frequency: */

	/* Stage 1 computations are now complete; I should really pass

	 * the results to another function and do the Stage 2 computations,

	 * but I only need a few more values so I'll just append the

 17. Find the number of pixels in the horizontal sync period: */

 18. Find the number of pixels in horizontal front porch period */

  36. Find the number of lines in the odd front porch period: */

 finally, pack the results in the mode struct */

/**

 * drm_gtf_mode - create the modeline based on the GTF algorithm

 * @dev: drm device

 * @hdisplay: hdisplay size

 * @vdisplay: vdisplay size

 * @vrefresh: vrefresh rate.

 * @interlaced: whether to compute an interlaced mode

 * @margins: desired margin (borders) size

 *

 * return the modeline based on GTF algorithm

 *

 * This function is to create the modeline based on the GTF algorithm.

 * Generalized Timing Formula is derived from:

 *

 *	GTF Spreadsheet by Andy Morrish (1/5/97)

 *	available at https://www.vesa.org

 *

 * And it is copied from the file of xserver/hw/xfree86/modes/xf86gtf.c.

 * What I have done is to translate it by using integer calculation.

 * I also refer to the function of fb_get_mode in the file of

 * drivers/video/fbmon.c

 *

 * Standard GTF parameters::

 *

 *     M = 600

 *     C = 40

 *     K = 128

 *     J = 20

 *

 * Returns:

 * The modeline based on the GTF algorithm stored in a drm_display_mode object.

 * The display mode object is allocated with drm_mode_create(). Returns NULL

 * when no mode could be allocated.

/**

 * drm_display_mode_from_videomode - fill in @dmode using @vm,

 * @vm: videomode structure to use as source

 * @dmode: drm_display_mode structure to use as destination

 *

 * Fills out @dmode using the display mode specified in @vm.

/**

 * drm_display_mode_to_videomode - fill in @vm using @dmode,

 * @dmode: drm_display_mode structure to use as source

 * @vm: videomode structure to use as destination

 *

 * Fills out @vm using the display mode specified in @dmode.

/**

 * drm_bus_flags_from_videomode - extract information about pixelclk and

 * DE polarity from videomode and store it in a separate variable

 * @vm: videomode structure to use

 * @bus_flags: information about pixelclk, sync and DE polarity will be stored

 * here

 *

 * Sets DRM_BUS_FLAG_DE_(LOW|HIGH),  DRM_BUS_FLAG_PIXDATA_DRIVE_(POS|NEG)EDGE

 * and DISPLAY_FLAGS_SYNC_(POS|NEG)EDGE in @bus_flags according to DISPLAY_FLAGS

 * found in @vm

/**

 * of_get_drm_display_mode - get a drm_display_mode from devicetree

 * @np: device_node with the timing specification

 * @dmode: will be set to the return value

 * @bus_flags: information about pixelclk, sync and DE polarity

 * @index: index into the list of display timings in devicetree

 *

 * This function is expensive and should only be used, if only one mode is to be

 * read from DT. To get multiple modes start with of_get_display_timings and

 * work with that instead.

 *

 * Returns:

 * 0 on success, a negative errno code when no of videomode node was found.

 CONFIG_OF */

 CONFIG_VIDEOMODE_HELPERS */

/**

 * drm_mode_set_name - set the name on a mode

 * @mode: name will be set in this mode

 *

 * Set the name of @mode to a standard format which is <hdisplay>x<vdisplay>

 * with an optional 'i' suffix for interlaced modes.

/**

 * drm_mode_vrefresh - get the vrefresh of a mode

 * @mode: mode

 *

 * Returns:

 * @modes's vrefresh rate in Hz, rounded to the nearest integer. Calculates the

 * value first if it is not yet set.

/**

 * drm_mode_get_hv_timing - Fetches hdisplay/vdisplay for given mode

 * @mode: mode to query

 * @hdisplay: hdisplay value to fill in

 * @vdisplay: vdisplay value to fill in

 *

 * The vdisplay value will be doubled if the specified mode is a stereo mode of

 * the appropriate layout.

/**

 * drm_mode_set_crtcinfo - set CRTC modesetting timing parameters

 * @p: mode

 * @adjust_flags: a combination of adjustment flags

 *

 * Setup the CRTC modesetting timing parameters for @p, adjusting if necessary.

 *

 * - The CRTC_INTERLACE_HALVE_V flag can be used to halve vertical timings of

 *   interlaced modes.

 * - The CRTC_STEREO_DOUBLE flag can be used to compute the timings for

 *   buffers containing two eyes (only adjust the timings when needed, eg. for

 *   "frame packing" or "side by side full").

 * - The CRTC_NO_DBLSCAN and CRTC_NO_VSCAN flags request that adjustment *not*

 *   be performed for doublescan and vscan > 1 modes respectively.

/**

 * drm_mode_copy - copy the mode

 * @dst: mode to overwrite

 * @src: mode to copy

 *

 * Copy an existing mode into another mode, preserving the object id and

 * list head of the destination mode.

/**

 * drm_mode_duplicate - allocate and duplicate an existing mode

 * @dev: drm_device to allocate the duplicated mode for

 * @mode: mode to duplicate

 *

 * Just allocate a new mode, copy the existing mode into it, and return

 * a pointer to it.  Used to create new instances of established modes.

 *

 * Returns:

 * Pointer to duplicated mode on success, NULL on error.

	/*

	 * do clock check convert to PICOS

	 * so fb modes get matched the same

/**

 * drm_mode_match - test modes for (partial) equality

 * @mode1: first mode

 * @mode2: second mode

 * @match_flags: which parts need to match (DRM_MODE_MATCH_*)

 *

 * Check to see if @mode1 and @mode2 are equivalent.

 *

 * Returns:

 * True if the modes are (partially) equal, false otherwise.

/**

 * drm_mode_equal - test modes for equality

 * @mode1: first mode

 * @mode2: second mode

 *

 * Check to see if @mode1 and @mode2 are equivalent.

 *

 * Returns:

 * True if the modes are equal, false otherwise.

/**

 * drm_mode_equal_no_clocks - test modes for equality

 * @mode1: first mode

 * @mode2: second mode

 *

 * Check to see if @mode1 and @mode2 are equivalent, but

 * don't check the pixel clocks.

 *

 * Returns:

 * True if the modes are equal, false otherwise.

/**

 * drm_mode_equal_no_clocks_no_stereo - test modes for equality

 * @mode1: first mode

 * @mode2: second mode

 *

 * Check to see if @mode1 and @mode2 are equivalent, but

 * don't check the pixel clocks nor the stereo layout.

 *

 * Returns:

 * True if the modes are equal, false otherwise.

/**

 * drm_mode_validate_driver - make sure the mode is somewhat sane

 * @dev: drm device

 * @mode: mode to check

 *

 * First do basic validation on the mode, and then allow the driver

 * to check for device/driver specific limitations via the optional

 * &drm_mode_config_helper_funcs.mode_valid hook.

 *

 * Returns:

 * The mode status

/**

 * drm_mode_validate_size - make sure modes adhere to size constraints

 * @mode: mode to check

 * @maxX: maximum width

 * @maxY: maximum height

 *

 * This function is a helper which can be used to validate modes against size

 * limitations of the DRM device/connector. If a mode is too big its status

 * member is updated with the appropriate validation failure code. The list

 * itself is not changed.

 *

 * Returns:

 * The mode status

/**

 * drm_mode_validate_ycbcr420 - add 'ycbcr420-only' modes only when allowed

 * @mode: mode to check

 * @connector: drm connector under action

 *

 * This function is a helper which can be used to filter out any YCBCR420

 * only mode, when the source doesn't support it.

 *

 * Returns:

 * The mode status

/**

 * drm_mode_prune_invalid - remove invalid modes from mode list

 * @dev: DRM device

 * @mode_list: list of modes to check

 * @verbose: be verbose about it

 *

 * This helper function can be used to prune a display mode list after

 * validation has been completed. All modes whose status is not MODE_OK will be

 * removed from the list, and if @verbose the status code and mode name is also

 * printed to dmesg.

/**

 * drm_mode_compare - compare modes for favorability

 * @priv: unused

 * @lh_a: list_head for first mode

 * @lh_b: list_head for second mode

 *

 * Compare two modes, given by @lh_a and @lh_b, returning a value indicating

 * which is better.

 *

 * Returns:

 * Negative if @lh_a is better than @lh_b, zero if they're equivalent, or

 * positive if @lh_b is better than @lh_a.

/**

 * drm_mode_sort - sort mode list

 * @mode_list: list of drm_display_mode structures to sort

 *

 * Sort @mode_list by favorability, moving good modes to the head of the list.

/**

 * drm_connector_list_update - update the mode list for the connector

 * @connector: the connector to update

 *

 * This moves the modes from the @connector probed_modes list

 * to the actual mode list. It compares the probed mode against the current

 * list and only adds different/new modes.

 *

 * This is just a helper functions doesn't validate any modes itself and also

 * doesn't prune any invalid modes. Callers need to do that themselves.

 go through current modes checking for the new probed mode */

			/*

			 * If the old matching mode is stale (ie. left over

			 * from a previous probe) just replace it outright.

			 * Otherwise just merge the type bits between all

			 * equal probed modes.

			 *

			 * If two probed modes are considered equal, pick the

			 * actual timings from the one that's marked as

			 * preferred (in case the match isn't 100%). If

			 * multiple or zero preferred modes are present, favor

			 * the mode added to the probed_modes list first.

			/*

			 * Try to pass that to our extras parsing

			 * function to handle the case where the

			 * extras are directly after the resolution

	/*

	 * delim must point to the '=', otherwise it is a syntax error and

	 * if delim points to the terminating zero, then delim + 1 will point

	 * past the end of the string.

 Make sure we have parsed something */

 Make sure there is exactly one rotation defined */

/**

 * drm_mode_parse_command_line_for_connector - parse command line modeline for connector

 * @mode_option: optional per connector mode option

 * @connector: connector to parse modeline for

 * @mode: preallocated drm_cmdline_mode structure to fill out

 *

 * This parses @mode_option command line modeline for modes and options to

 * configure the connector. If @mode_option is NULL the default command line

 * modeline in fb_mode_option will be parsed instead.

 *

 * This uses the same parameters as the fb modedb.c, except for an extra

 * force-enable, force-enable-digital and force-disable bit at the end::

 *

 *	<xres>x<yres>[M][R][-<bpp>][@<refresh>][i][m][eDd]

 *

 * Additionals options can be provided following the mode, using a comma to

 * separate each option. Valid options can be found in

 * Documentation/fb/modedb.rst.

 *

 * The intermediate drm_cmdline_mode structure is required to store additional

 * options from the command line modline like the force-enable/disable flag.

 *

 * Returns:

 * True if a valid modeline has been parsed, false otherwise.

 Try to locate the bpp and refresh specifiers, if any */

 Locate the start of named options */

 Locate the end of the name / resolution, and parse it */

 First check for a named mode */

 named + refresh is invalid */

 No named mode? Check for a normal mode argument, e.g. 1024x768 */

 No mode? Check for freestanding extras and/or options */

 syntax error */

	/*

	 * Locate the end of the bpp / refresh, and parse the extras

	 * if relevant

/**

 * drm_mode_create_from_cmdline_mode - convert a command line modeline into a DRM display mode

 * @dev: DRM device to create the new mode for

 * @cmd: input command line modeline

 *

 * Returns:

 * Pointer to converted mode on success, NULL on error.

 fix up 1368x768: GFT/CVT can't express 1366 width due to alignment */

/**

 * drm_mode_convert_to_umode - convert a drm_display_mode into a modeinfo

 * @out: drm_mode_modeinfo struct to return to the user

 * @in: drm_display_mode to use

 *

 * Convert a drm_display_mode into a drm_mode_modeinfo structure to return to

 * the user.

/**

 * drm_mode_convert_umode - convert a modeinfo into a drm_display_mode

 * @dev: drm device

 * @out: drm_display_mode to return to the user

 * @in: drm_mode_modeinfo to use

 *

 * Convert a drm_mode_modeinfo into a drm_display_mode structure to return to

 * the caller.

 *

 * Returns:

 * Zero on success, negative errno on failure.

	/*

	 * Old xf86-video-vmware (possibly others too) used to

	 * leave 'type' uninitialized. Just ignore any bits we

	 * don't like. It's a just hint after all, and more

	 * useful for the kernel->userspace direction anyway.

	/* Clearing picture aspect ratio bits from out flags,

	 * as the aspect-ratio information is not stored in

	 * flags for kernel-mode, but in picture_aspect_ratio.

/**

 * drm_mode_is_420_only - if a given videomode can be only supported in YCBCR420

 * output format

 *

 * @display: display under action

 * @mode: video mode to be tested.

 *

 * Returns:

 * true if the mode can be supported in YCBCR420 format

 * false if not.

/**

 * drm_mode_is_420_also - if a given videomode can be supported in YCBCR420

 * output format also (along with RGB/YCBCR444/422)

 *

 * @display: display under action.

 * @mode: video mode to be tested.

 *

 * Returns:

 * true if the mode can be support YCBCR420 format

 * false if not.

/**

 * drm_mode_is_420 - if a given videomode can be supported in YCBCR420

 * output format

 *

 * @display: display under action.

 * @mode: video mode to be tested.

 *

 * Returns:

 * true if the mode can be supported in YCBCR420 format

 * false if not.

/*

 * Created: Fri Jan 19 10:48:35 2001 by faith@acm.org

 *

 * Copyright 2001 VA Linux Systems, Inc., Sunnyvale, California.

 * All Rights Reserved.

 *

 * Author Rickard E. (Rik) Faith <faith@valinux.com>

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR

 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,

 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/*

 * If the drm core fails to init for whatever reason,

 * we should prevent any drivers from registering with it.

 * It's best to check this at drm_dev_init(), as some drivers

 * prefer to embed struct drm_device into their own device

 * structure and call drm_dev_init() themselves.

/*

 * DRM Minors

 * A DRM device can provide several char-dev interfaces on the DRM-Major. Each

 * of them is represented by a drm_minor object. Depending on the capabilities

 * of the device-driver, different interfaces are registered.

 *

 * Minors can be accessed via dev->$minor_name. This pointer is either

 * NULL or a valid drm_minor pointer and stays valid as long as the device is

 * valid. This means, DRM minors have the same life-time as the underlying

 * device. However, this doesn't mean that the minor is active. Minors are

 * registered and unregistered dynamically according to device-state.

 replace NULL with @minor so lookups will succeed from now on */

 replace @minor with NULL so lookups will fail from now on */

 safety belt */

/*

 * Looks up the given minor-ID and returns the respective DRM-minor object. The

 * refence-count of the underlying device is increased so you must release this

 * object with drm_minor_release().

 *

 * As long as you hold this minor, it is guaranteed that the object and the

 * minor->dev pointer will stay valid! However, the device may get unplugged and

 * unregistered while you hold the minor.

/**

 * DOC: driver instance overview

 *

 * A device instance for a drm driver is represented by &struct drm_device. This

 * is allocated and initialized with devm_drm_dev_alloc(), usually from

 * bus-specific ->probe() callbacks implemented by the driver. The driver then

 * needs to initialize all the various subsystems for the drm device like memory

 * management, vblank handling, modesetting support and initial output

 * configuration plus obviously initialize all the corresponding hardware bits.

 * Finally when everything is up and running and ready for userspace the device

 * instance can be published using drm_dev_register().

 *

 * There is also deprecated support for initializing device instances using

 * bus-specific helpers and the &drm_driver.load callback. But due to

 * backwards-compatibility needs the device instance have to be published too

 * early, which requires unpretty global locking to make safe and is therefore

 * only support for existing drivers not yet converted to the new scheme.

 *

 * When cleaning up a device instance everything needs to be done in reverse:

 * First unpublish the device instance with drm_dev_unregister(). Then clean up

 * any other resources allocated at device initialization and drop the driver's

 * reference to &drm_device using drm_dev_put().

 *

 * Note that any allocation or resource which is visible to userspace must be

 * released only when the final drm_dev_put() is called, and not when the

 * driver is unbound from the underlying physical struct &device. Best to use

 * &drm_device managed resources with drmm_add_action(), drmm_kmalloc() and

 * related functions.

 *

 * devres managed resources like devm_kmalloc() can only be used for resources

 * directly related to the underlying hardware device, and only used in code

 * paths fully protected by drm_dev_enter() and drm_dev_exit().

 *

 * Display driver example

 * ~~~~~~~~~~~~~~~~~~~~~~

 *

 * The following example shows a typical structure of a DRM display driver.

 * The example focus on the probe() function and the other functions that is

 * almost always present and serves as a demonstration of devm_drm_dev_alloc().

 *

 * .. code-block:: c

 *

 *	struct driver_device {

 *		struct drm_device drm;

 *		void *userspace_facing;

 *		struct clk *pclk;

 *	};

 *

 *	static const struct drm_driver driver_drm_driver = {

 *		[...]

 *	};

 *

 *	static int driver_probe(struct platform_device *pdev)

 *	{

 *		struct driver_device *priv;

 *		struct drm_device *drm;

 *		int ret;

 *

 *		priv = devm_drm_dev_alloc(&pdev->dev, &driver_drm_driver,

 *					  struct driver_device, drm);

 *		if (IS_ERR(priv))

 *			return PTR_ERR(priv);

 *		drm = &priv->drm;

 *

 *		ret = drmm_mode_config_init(drm);

 *		if (ret)

 *			return ret;

 *

 *		priv->userspace_facing = drmm_kzalloc(..., GFP_KERNEL);

 *		if (!priv->userspace_facing)

 *			return -ENOMEM;

 *

 *		priv->pclk = devm_clk_get(dev, "PCLK");

 *		if (IS_ERR(priv->pclk))

 *			return PTR_ERR(priv->pclk);

 *

 *		// Further setup, display pipeline etc

 *

 *		platform_set_drvdata(pdev, drm);

 *

 *		drm_mode_config_reset(drm);

 *

 *		ret = drm_dev_register(drm);

 *		if (ret)

 *			return ret;

 *

 *		drm_fbdev_generic_setup(drm, 32);

 *

 *		return 0;

 *	}

 *

 *	// This function is called before the devm_ resources are released

 *	static int driver_remove(struct platform_device *pdev)

 *	{

 *		struct drm_device *drm = platform_get_drvdata(pdev);

 *

 *		drm_dev_unregister(drm);

 *		drm_atomic_helper_shutdown(drm)

 *

 *		return 0;

 *	}

 *

 *	// This function is called on kernel restart and shutdown

 *	static void driver_shutdown(struct platform_device *pdev)

 *	{

 *		drm_atomic_helper_shutdown(platform_get_drvdata(pdev));

 *	}

 *

 *	static int __maybe_unused driver_pm_suspend(struct device *dev)

 *	{

 *		return drm_mode_config_helper_suspend(dev_get_drvdata(dev));

 *	}

 *

 *	static int __maybe_unused driver_pm_resume(struct device *dev)

 *	{

 *		drm_mode_config_helper_resume(dev_get_drvdata(dev));

 *

 *		return 0;

 *	}

 *

 *	static const struct dev_pm_ops driver_pm_ops = {

 *		SET_SYSTEM_SLEEP_PM_OPS(driver_pm_suspend, driver_pm_resume)

 *	};

 *

 *	static struct platform_driver driver_driver = {

 *		.driver = {

 *			[...]

 *			.pm = &driver_pm_ops,

 *		},

 *		.probe = driver_probe,

 *		.remove = driver_remove,

 *		.shutdown = driver_shutdown,

 *	};

 *	module_platform_driver(driver_driver);

 *

 * Drivers that want to support device unplugging (USB, DT overlay unload) should

 * use drm_dev_unplug() instead of drm_dev_unregister(). The driver must protect

 * regions that is accessing device resources to prevent use after they're

 * released. This is done using drm_dev_enter() and drm_dev_exit(). There is one

 * shortcoming however, drm_dev_unplug() marks the drm_device as unplugged before

 * drm_atomic_helper_shutdown() is called. This means that if the disable code

 * paths are protected, they will not run on regular driver module unload,

 * possibly leaving the hardware enabled.

/**

 * drm_put_dev - Unregister and release a DRM device

 * @dev: DRM device

 *

 * Called at module unload time or when a PCI device is unplugged.

 *

 * Cleans up all DRM device, calling drm_lastclose().

 *

 * Note: Use of this function is deprecated. It will eventually go away

 * completely.  Please use drm_dev_unregister() and drm_dev_put() explicitly

 * instead to make sure that the device isn't userspace accessible any more

 * while teardown is in progress, ensuring that userspace can't access an

 * inconsistent state.

/**

 * drm_dev_enter - Enter device critical section

 * @dev: DRM device

 * @idx: Pointer to index that will be passed to the matching drm_dev_exit()

 *

 * This function marks and protects the beginning of a section that should not

 * be entered after the device has been unplugged. The section end is marked

 * with drm_dev_exit(). Calls to this function can be nested.

 *

 * Returns:

 * True if it is OK to enter the section, false otherwise.

/**

 * drm_dev_exit - Exit device critical section

 * @idx: index returned from drm_dev_enter()

 *

 * This function marks the end of a section that should not be entered after

 * the device has been unplugged.

/**

 * drm_dev_unplug - unplug a DRM device

 * @dev: DRM device

 *

 * This unplugs a hotpluggable DRM device, which makes it inaccessible to

 * userspace operations. Entry-points can use drm_dev_enter() and

 * drm_dev_exit() to protect device resources in a race free manner. This

 * essentially unregisters the device like drm_dev_unregister(), but can be

 * called while there are still open users of @dev.

	/*

	 * After synchronizing any critical read section is guaranteed to see

	 * the new value of ->unplugged, and any critical section which might

	 * still have seen the old value of ->unplugged is guaranteed to have

	 * finished.

 Clear all CPU mappings pointing to this device */

/*

 * DRM internal mount

 * We want to be able to allocate our own "struct address_space" to control

 * memory-mappings in VRAM (or stolen RAM, ...). However, core MM does not allow

 * stand-alone address_space objects, so we need an underlying inode. As there

 * is no way to allocate an independent inode easily, we need a fake internal

 * VFS mount-point.

 *

 * The drm_fs_inode_new() function allocates a new inode, drm_fs_inode_free()

 * frees it again. You are allowed to use iget() and iput() to get references to

 * the inode. But each drm_fs_inode_new() call must be paired with exactly one

 * drm_fs_inode_free() call (which does not have to be the last iput()).

 * We use drm_fs_inode_*() to manage our internal VFS mount-point and share it

 * between multiple inode-users. You could, technically, call

 * iget() + drm_fs_inode_free() directly after alloc and sometime later do an

 * iput(), but this way you'd end up with a new vfsmount for each inode.

/**

 * DOC: component helper usage recommendations

 *

 * DRM drivers that drive hardware where a logical device consists of a pile of

 * independent hardware blocks are recommended to use the :ref:`component helper

 * library<component>`. For consistency and better options for code reuse the

 * following guidelines apply:

 *

 *  - The entire device initialization procedure should be run from the

 *    &component_master_ops.master_bind callback, starting with

 *    devm_drm_dev_alloc(), then binding all components with

 *    component_bind_all() and finishing with drm_dev_register().

 *

 *  - The opaque pointer passed to all components through component_bind_all()

 *    should point at &struct drm_device of the device instance, not some driver

 *    specific private structure.

 *

 *  - The component helper fills the niche where further standardization of

 *    interfaces is not practical. When there already is, or will be, a

 *    standardized interface like &drm_bridge or &drm_panel, providing its own

 *    functions to find such components at driver load time, like

 *    drm_of_find_panel_or_bridge(), then the component helper should not be

 *    used.

	/* Prevent use-after-free in drm_managed_release when debugging is

 no per-device feature limits by default */

/**

 * drm_dev_alloc - Allocate new DRM device

 * @driver: DRM driver to allocate device for

 * @parent: Parent device object

 *

 * This is the deprecated version of devm_drm_dev_alloc(), which does not support

 * subclassing through embedding the struct &drm_device in a driver private

 * structure, and which does not support automatic cleanup through devres.

 *

 * RETURNS:

 * Pointer to new DRM device, or ERR_PTR on failure.

/**

 * drm_dev_get - Take reference of a DRM device

 * @dev: device to take reference of or NULL

 *

 * This increases the ref-count of @dev by one. You *must* already own a

 * reference when calling this. Use drm_dev_put() to drop this reference

 * again.

 *

 * This function never fails. However, this function does not provide *any*

 * guarantee whether the device is alive or running. It only provides a

 * reference to the object and the memory associated with it.

/**

 * drm_dev_put - Drop reference of a DRM device

 * @dev: device to drop reference of or NULL

 *

 * This decreases the ref-count of @dev by one. The device is destroyed if the

 * ref-count drops to zero.

	/*

	 * Some existing userspace out there uses the existing of the controlD*

	 * sysfs files to figure out whether it's a modeset driver. It only does

	 * readdir, hence a symlink is sufficient (and the least confusing

	 * option). Otherwise controlD* is entirely unused.

	 *

	 * Old controlD chardev have been allocated in the range

	 * 64-127.

/**

 * drm_dev_register - Register DRM device

 * @dev: Device to register

 * @flags: Flags passed to the driver's .load() function

 *

 * Register the DRM device @dev with the system, advertise device to user-space

 * and start normal device operation. @dev must be initialized via drm_dev_init()

 * previously.

 *

 * Never call this twice on any device!

 *

 * NOTE: To ensure backward compatibility with existing drivers method this

 * function calls the &drm_driver.load method after registering the device

 * nodes, creating race conditions. Usage of the &drm_driver.load methods is

 * therefore deprecated, drivers must perform all initialization before calling

 * drm_dev_register().

 *

 * RETURNS:

 * 0 on success, negative error code on failure.

/**

 * drm_dev_unregister - Unregister DRM device

 * @dev: Device to unregister

 *

 * Unregister the DRM device from the system. This does the reverse of

 * drm_dev_register() but does not deallocate the device. The caller must call

 * drm_dev_put() to drop their final reference.

 *

 * A special form of unregistering for hotpluggable devices is drm_dev_unplug(),

 * which can be called while there are still open users of @dev.

 *

 * This should be called first in the device teardown code to make sure

 * userspace can't access the device instance any more.

/**

 * drm_dev_set_unique - Set the unique name of a DRM device

 * @dev: device of which to set the unique name

 * @name: unique name

 *

 * Sets the unique name of a DRM device using the specified string. This is

 * already done by drm_dev_init(), drivers should only override the default

 * unique name for backwards compatibility reasons.

 *

 * Return: 0 on success or a negative error code on failure.

/*

 * DRM Core

 * The DRM core module initializes all global DRM objects and makes them

 * available to drivers. Once setup, drivers can probe their respective

 * devices.

 * Currently, core management includes:

 *  - The "DRM-Global" key/value database

 *  - Global ID management for connectors

 *  - DRM major number allocation

 *  - DRM minor management

 *  - DRM sysfs class

 *  - DRM debugfs root

 *

 * Furthermore, the DRM core provides dynamic char-dev lookups. For each

 * interface registered on a DRM device, you can request minor numbers from DRM

 * core. DRM core takes care of major-number management and char-dev

 * registration. A stub ->open() callback forwards any open() requests to the

 * registered minor.

/*

 * drm_irq.c IRQ and vblank support

 *

 * \author Rickard E. (Rik) Faith <faith@valinux.com>

 * \author Gareth Hughes <gareth@valinux.com>

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * VA LINUX SYSTEMS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR

 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,

 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR

 * OTHER DEALINGS IN THE SOFTWARE.

/**

 * DOC: vblank handling

 *

 * From the computer's perspective, every time the monitor displays

 * a new frame the scanout engine has "scanned out" the display image

 * from top to bottom, one row of pixels at a time. The current row

 * of pixels is referred to as the current scanline.

 *

 * In addition to the display's visible area, there's usually a couple of

 * extra scanlines which aren't actually displayed on the screen.

 * These extra scanlines don't contain image data and are occasionally used

 * for features like audio and infoframes. The region made up of these

 * scanlines is referred to as the vertical blanking region, or vblank for

 * short.

 *

 * For historical reference, the vertical blanking period was designed to

 * give the electron gun (on CRTs) enough time to move back to the top of

 * the screen to start scanning out the next frame. Similar for horizontal

 * blanking periods. They were designed to give the electron gun enough

 * time to move back to the other side of the screen to start scanning the

 * next scanline.

 *

 * ::

 *

 *

 *    physical â   â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½

 *    top of      |                                        |

 *    display     |                                        |

 *                |               New frame                |

 *                |                                        |

 *                |ââââââââââââââââââââââââââââââââââââââââ|

 *                |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~| â Scanline,

 *                |ââââââââââââââââââââââââââââââââââââââââ|   updates the

 *                |                                        |   frame as it

 *                |                                        |   travels down

 *                |                                        |   ("scan out")

 *                |               Old frame                |

 *                |                                        |

 *                |                                        |

 *                |                                        |

 *                |                                        |   physical

 *                |                                        |   bottom of

 *    vertical    |â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½| â display

 *    blanking    âxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxâ

 *    region   â  âxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxâ

 *                âxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxâ

 *    start of â   â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½â½

 *    new frame

 *

 * "Physical top of display" is the reference point for the high-precision/

 * corrected timestamp.

 *

 * On a lot of display hardware, programming needs to take effect during the

 * vertical blanking period so that settings like gamma, the image buffer

 * buffer to be scanned out, etc. can safely be changed without showing

 * any visual artifacts on the screen. In some unforgiving hardware, some of

 * this programming has to both start and end in the same vblank. To help

 * with the timing of the hardware programming, an interrupt is usually

 * available to notify the driver when it can start the updating of registers.

 * The interrupt is in this context named the vblank interrupt.

 *

 * The vblank interrupt may be fired at different points depending on the

 * hardware. Some hardware implementations will fire the interrupt when the

 * new frame start, other implementations will fire the interrupt at different

 * points in time.

 *

 * Vertical blanking plays a major role in graphics rendering. To achieve

 * tear-free display, users must synchronize page flips and/or rendering to

 * vertical blanking. The DRM API offers ioctls to perform page flips

 * synchronized to vertical blanking and wait for vertical blanking.

 *

 * The DRM core handles most of the vertical blanking management logic, which

 * involves filtering out spurious interrupts, keeping race-free blanking

 * counters, coping with counter wrap-around and resets and keeping use counts.

 * It relies on the driver to generate vertical blanking interrupts and

 * optionally provide a hardware vertical blanking counter.

 *

 * Drivers must initialize the vertical blanking handling core with a call to

 * drm_vblank_init(). Minimally, a driver needs to implement

 * &drm_crtc_funcs.enable_vblank and &drm_crtc_funcs.disable_vblank plus call

 * drm_crtc_handle_vblank() in its vblank interrupt handler for working vblank

 * support.

 *

 * Vertical blanking interrupts can be enabled by the DRM core or by drivers

 * themselves (for instance to handle page flipping operations).  The DRM core

 * maintains a vertical blanking use count to ensure that the interrupts are not

 * disabled while a user still needs them. To increment the use count, drivers

 * call drm_crtc_vblank_get() and release the vblank reference again with

 * drm_crtc_vblank_put(). In between these two calls vblank interrupts are

 * guaranteed to be enabled.

 *

 * On many hardware disabling the vblank interrupt cannot be done in a race-free

 * manner, see &drm_driver.vblank_disable_immediate and

 * &drm_driver.max_vblank_count. In that case the vblank core only disables the

 * vblanks after a timer has expired, which can be configured through the

 * ``vblankoffdelay`` module parameter.

 *

 * Drivers for hardware without support for vertical-blanking interrupts

 * must not call drm_vblank_init(). For such drivers, atomic helpers will

 * automatically generate fake vblank events as part of the display update.

 * This functionality also can be controlled by the driver by enabling and

 * disabling struct drm_crtc_state.no_vblank.

/* Retry timestamp calculation up to 3 times to satisfy

 * drm_timestamp_precision before giving up.

/* Threshold in nanoseconds for detection of redundant

 * vblank irq in drm_handle_vblank(). 1 msec should be ok.

 Default to 20 usecs. */

 Default to 5000 msecs. */

/*

 * "No hw counter" fallback implementation of .get_vblank_counter() hook,

 * if there is no usable hardware frame counter available.

/*

 * Reset the stored timestamp for the current vblank count to correspond

 * to the last vblank occurred.

 *

 * Only to be called from drm_crtc_vblank_on().

 *

 * Note: caller must hold &drm_device.vbl_lock since this reads & writes

 * device vblank fields.

	/*

	 * sample the current counter to avoid random jumps

	 * when drm_vblank_enable() applies the diff

	/*

	 * Only reinitialize corresponding vblank timestamp if high-precision query

	 * available and didn't fail. Otherwise reinitialize delayed at next vblank

	 * interrupt and assign 0 for now, to mark the vblanktimestamp as invalid.

	/*

	 * +1 to make sure user will never see the same

	 * vblank counter value before and after a modeset

/*

 * Call back into the driver to update the appropriate vblank counter

 * (specified by @pipe).  Deal with wraparound, if it occurred, and

 * update the last read value so we can deal with wraparound on the next

 * call if necessary.

 *

 * Only necessary when going from off->on, to account for frames we

 * didn't get an interrupt for.

 *

 * Note: caller must hold &drm_device.vbl_lock since this reads & writes

 * device vblank fields.

	/*

	 * Interrupts were disabled prior to this call, so deal with counter

	 * wrap if needed.

	 * NOTE!  It's possible we lost a full dev->max_vblank_count + 1 events

	 * here if the register is small or we had vblank interrupts off for

	 * a long time.

	 *

	 * We repeat the hardware vblank counter & timestamp query until

	 * we get consistent results. This to prevent races between gpu

	 * updating its hardware counter while we are retrieving the

	 * corresponding vblank timestamp.

 trust the hw counter when it's around */

		/*

		 * Figure out how many vblanks we've missed based

		 * on the difference in the timestamps and the

		 * frame/field duration.

 some kind of default for drivers w/o accurate vbl timestamping */

	/*

	 * Within a drm_vblank_pre_modeset - drm_vblank_post_modeset

	 * interval? If so then vblank irqs keep running and it will likely

	 * happen that the hardware vblank counter is not trustworthy as it

	 * might reset at some point in that interval and vblank timestamps

	 * are not trustworthy either in that interval. Iow. this can result

	 * in a bogus diff >> 1 which must be avoided as it would cause

	 * random large forward jumps of the software vblank counter.

	/*

	 * Only reinitialize corresponding vblank timestamp if high-precision query

	 * available and didn't fail, or we were called from the vblank interrupt.

	 * Otherwise reinitialize delayed at next vblank interrupt and assign 0

	 * for now, to mark the vblanktimestamp as invalid.

	/*

	 * This read barrier corresponds to the implicit write barrier of the

	 * write seqlock in store_vblank(). Note that this is the only place

	 * where we need an explicit barrier, since all other access goes

	 * through drm_vblank_count_and_time(), which already has the required

	 * read barrier curtesy of the read seqlock.

/**

 * drm_crtc_accurate_vblank_count - retrieve the master vblank counter

 * @crtc: which counter to retrieve

 *

 * This function is similar to drm_crtc_vblank_count() but this function

 * interpolates to handle a race with vblank interrupts using the high precision

 * timestamping support.

 *

 * This is mostly useful for hardware that can obtain the scanout position, but

 * doesn't have a hardware frame counter.

/*

 * Disable vblank irq's on crtc, make sure that last vblank count

 * of hardware and corresponding consistent software vblank counter

 * are preserved, even if there are any spurious vblank irq's after

 * disable.

	/* Prevent vblank irq processing while disabling vblank irqs,

	 * so no updates of timestamps or count can happen after we've

	 * disabled. Needed to prevent races in case of delayed irq's.

	/*

	 * Update vblank count and disable vblank interrupts only if the

	 * interrupts were enabled. This avoids calling the ->disable_vblank()

	 * operation in atomic context with the hardware potentially runtime

	 * suspended.

	/*

	 * Update the count and timestamp to maintain the

	 * appearance that the counter has been ticking all along until

	 * this time. This makes the count account for the entire time

	 * between drm_crtc_vblank_on() and drm_crtc_vblank_off().

/**

 * drm_vblank_init - initialize vblank support

 * @dev: DRM device

 * @num_crtcs: number of CRTCs supported by @dev

 *

 * This function initializes vblank support for @num_crtcs display pipelines.

 * Cleanup is handled automatically through a cleanup function added with

 * drmm_add_action_or_reset().

 *

 * Returns:

 * Zero on success or a negative error code on failure.

/**

 * drm_dev_has_vblank - test if vblanking has been initialized for

 *                      a device

 * @dev: the device

 *

 * Drivers may call this function to test if vblank support is

 * initialized for a device. For most hardware this means that vblanking

 * can also be enabled.

 *

 * Atomic helpers use this function to initialize

 * &drm_crtc_state.no_vblank. See also drm_atomic_helper_check_modeset().

 *

 * Returns:

 * True if vblanking has been initialized for the given device, false

 * otherwise.

/**

 * drm_crtc_vblank_waitqueue - get vblank waitqueue for the CRTC

 * @crtc: which CRTC's vblank waitqueue to retrieve

 *

 * This function returns a pointer to the vblank waitqueue for the CRTC.

 * Drivers can use this to implement vblank waits using wait_event() and related

 * functions.

/**

 * drm_calc_timestamping_constants - calculate vblank timestamp constants

 * @crtc: drm_crtc whose timestamp constants should be updated.

 * @mode: display mode containing the scanout timings

 *

 * Calculate and store various constants which are later needed by vblank and

 * swap-completion timestamping, e.g, by

 * drm_crtc_vblank_helper_get_vblank_timestamp(). They are derived from

 * CRTC's true scanout timing, so they take things like panel scaling or

 * other adjustments into account.

 Valid dotclock? */

		/*

		 * Convert scanline length in pixels and video

		 * dot clock to line duration and frame duration

		 * in nanoseconds:

		/*

		 * Fields of interlaced scanout modes are only half a frame duration.

/**

 * drm_crtc_vblank_helper_get_vblank_timestamp_internal - precise vblank

 *                                                        timestamp helper

 * @crtc: CRTC whose vblank timestamp to retrieve

 * @max_error: Desired maximum allowable error in timestamps (nanosecs)

 *             On return contains true maximum error of timestamp

 * @vblank_time: Pointer to time which should receive the timestamp

 * @in_vblank_irq:

 *     True when called from drm_crtc_handle_vblank().  Some drivers

 *     need to apply some workarounds for gpu-specific vblank irq quirks

 *     if flag is set.

 * @get_scanout_position:

 *     Callback function to retrieve the scanout position. See

 *     @struct drm_crtc_helper_funcs.get_scanout_position.

 *

 * Implements calculation of exact vblank timestamps from given drm_display_mode

 * timings and current video scanout position of a CRTC.

 *

 * The current implementation only handles standard video modes. For double scan

 * and interlaced modes the driver is supposed to adjust the hardware mode

 * (taken from &drm_crtc_state.adjusted mode for atomic modeset drivers) to

 * match the scanout position reported.

 *

 * Note that atomic drivers must call drm_calc_timestamping_constants() before

 * enabling a CRTC. The atomic helpers already take care of that in

 * drm_atomic_helper_calc_timestamping_constants().

 *

 * Returns:

 *

 * Returns true on success, and false on failure, i.e. when no accurate

 * timestamp could be acquired.

 Scanout position query not supported? Should not happen. */

	/* If mode timing undefined, just return as no-op:

	 * Happens during initial modesetting of a crtc.

	/* Get current scanout position with system timestamp.

	 * Repeat query up to DRM_TIMESTAMP_MAXRETRIES times

	 * if single query takes longer than max_error nanoseconds.

	 *

	 * This guarantees a tight bound on maximum error if

	 * code gets preempted or delayed for some reason.

		/*

		 * Get vertical and horizontal scanout position vpos, hpos,

		 * and bounding timestamps stime, etime, pre/post query.

 Return as no-op if scanout query unsupported or failed. */

 Compute uncertainty in timestamp of scanout position query. */

 Accept result with <  max_error nsecs timing uncertainty. */

 Noisy system timing? */

 Return upper bound of timestamp precision error. */

	/* Convert scanout position into elapsed time at raw_time query

	 * since start of scanout at first display scanline. delta_ns

	 * can be negative if start of scanout hasn't happened yet.

	/* Subtract time delta from raw timestamp to get final

	 * vblank_time timestamp for end of vblank.

/**

 * drm_crtc_vblank_helper_get_vblank_timestamp - precise vblank timestamp

 *                                               helper

 * @crtc: CRTC whose vblank timestamp to retrieve

 * @max_error: Desired maximum allowable error in timestamps (nanosecs)

 *             On return contains true maximum error of timestamp

 * @vblank_time: Pointer to time which should receive the timestamp

 * @in_vblank_irq:

 *     True when called from drm_crtc_handle_vblank().  Some drivers

 *     need to apply some workarounds for gpu-specific vblank irq quirks

 *     if flag is set.

 *

 * Implements calculation of exact vblank timestamps from given drm_display_mode

 * timings and current video scanout position of a CRTC. This can be directly

 * used as the &drm_crtc_funcs.get_vblank_timestamp implementation of a kms

 * driver if &drm_crtc_helper_funcs.get_scanout_position is implemented.

 *

 * The current implementation only handles standard video modes. For double scan

 * and interlaced modes the driver is supposed to adjust the hardware mode

 * (taken from &drm_crtc_state.adjusted mode for atomic modeset drivers) to

 * match the scanout position reported.

 *

 * Note that atomic drivers must call drm_calc_timestamping_constants() before

 * enabling a CRTC. The atomic helpers already take care of that in

 * drm_atomic_helper_calc_timestamping_constants().

 *

 * Returns:

 *

 * Returns true on success, and false on failure, i.e. when no accurate

 * timestamp could be acquired.

/**

 * drm_get_last_vbltimestamp - retrieve raw timestamp for the most recent

 *                             vblank interval

 * @dev: DRM device

 * @pipe: index of CRTC whose vblank timestamp to retrieve

 * @tvblank: Pointer to target time which should receive the timestamp

 * @in_vblank_irq:

 *     True when called from drm_crtc_handle_vblank().  Some drivers

 *     need to apply some workarounds for gpu-specific vblank irq quirks

 *     if flag is set.

 *

 * Fetches the system timestamp corresponding to the time of the most recent

 * vblank interval on specified CRTC. May call into kms-driver to

 * compute the timestamp with a high-precision GPU specific method.

 *

 * Returns zero if timestamp originates from uncorrected do_gettimeofday()

 * call, i.e., it isn't very precisely locked to the true vblank.

 *

 * Returns:

 * True if timestamp is considered to be very precise, false otherwise.

 Define requested maximum error on timestamps (nanoseconds). */

 Query driver if possible and precision timestamping enabled. */

	/* GPU high precision timestamp query unsupported or failed.

	 * Return current monotonic/gettimeofday timestamp as best estimate.

/**

 * drm_crtc_vblank_count - retrieve "cooked" vblank counter value

 * @crtc: which counter to retrieve

 *

 * Fetches the "cooked" vblank count value that represents the number of

 * vblank events since the system was booted, including lost events due to

 * modesetting activity. Note that this timer isn't correct against a racing

 * vblank interrupt (since it only reports the software vblank counter), see

 * drm_crtc_accurate_vblank_count() for such use-cases.

 *

 * Note that for a given vblank counter value drm_crtc_handle_vblank()

 * and drm_crtc_vblank_count() or drm_crtc_vblank_count_and_time()

 * provide a barrier: Any writes done before calling

 * drm_crtc_handle_vblank() will be visible to callers of the later

 * functions, if the vblank count is the same or a later one.

 *

 * See also &drm_vblank_crtc.count.

 *

 * Returns:

 * The software vblank counter.

/**

 * drm_vblank_count_and_time - retrieve "cooked" vblank counter value and the

 *     system timestamp corresponding to that vblank counter value.

 * @dev: DRM device

 * @pipe: index of CRTC whose counter to retrieve

 * @vblanktime: Pointer to ktime_t to receive the vblank timestamp.

 *

 * Fetches the "cooked" vblank count value that represents the number of

 * vblank events since the system was booted, including lost events due to

 * modesetting activity. Returns corresponding system timestamp of the time

 * of the vblank interval that corresponds to the current vblank counter value.

 *

 * This is the legacy version of drm_crtc_vblank_count_and_time().

/**

 * drm_crtc_vblank_count_and_time - retrieve "cooked" vblank counter value

 *     and the system timestamp corresponding to that vblank counter value

 * @crtc: which counter to retrieve

 * @vblanktime: Pointer to time to receive the vblank timestamp.

 *

 * Fetches the "cooked" vblank count value that represents the number of

 * vblank events since the system was booted, including lost events due to

 * modesetting activity. Returns corresponding system timestamp of the time

 * of the vblank interval that corresponds to the current vblank counter value.

 *

 * Note that for a given vblank counter value drm_crtc_handle_vblank()

 * and drm_crtc_vblank_count() or drm_crtc_vblank_count_and_time()

 * provide a barrier: Any writes done before calling

 * drm_crtc_handle_vblank() will be visible to callers of the later

 * functions, if the vblank count is the same or a later one.

 *

 * See also &drm_vblank_crtc.count.

		/*

		 * e->event is a user space structure, with hardcoded unsigned

		 * 32-bit seconds/microseconds. This is safe as we always use

		 * monotonic timestamps since linux-4.15

	/*

	 * Use the same timestamp for any associated fence signal to avoid

	 * mismatch in timestamps for vsync & fence events triggered by the

	 * same HW event. Frameworks like SurfaceFlinger in Android expects the

	 * retire-fence timestamp to match exactly with HW vsync as it uses it

	 * for its software vsync modeling.

/**

 * drm_crtc_arm_vblank_event - arm vblank event after pageflip

 * @crtc: the source CRTC of the vblank event

 * @e: the event to send

 *

 * A lot of drivers need to generate vblank events for the very next vblank

 * interrupt. For example when the page flip interrupt happens when the page

 * flip gets armed, but not when it actually executes within the next vblank

 * period. This helper function implements exactly the required vblank arming

 * behaviour.

 *

 * NOTE: Drivers using this to send out the &drm_crtc_state.event as part of an

 * atomic commit must ensure that the next vblank happens at exactly the same

 * time as the atomic commit is committed to the hardware. This function itself

 * does **not** protect against the next vblank interrupt racing with either this

 * function call or the atomic commit operation. A possible sequence could be:

 *

 * 1. Driver commits new hardware state into vblank-synchronized registers.

 * 2. A vblank happens, committing the hardware state. Also the corresponding

 *    vblank interrupt is fired off and fully processed by the interrupt

 *    handler.

 * 3. The atomic commit operation proceeds to call drm_crtc_arm_vblank_event().

 * 4. The event is only send out for the next vblank, which is wrong.

 *

 * An equivalent race can happen when the driver calls

 * drm_crtc_arm_vblank_event() before writing out the new hardware state.

 *

 * The only way to make this work safely is to prevent the vblank from firing

 * (and the hardware from committing anything else) until the entire atomic

 * commit sequence has run to completion. If the hardware does not have such a

 * feature (e.g. using a "go" bit), then it is unsafe to use this functions.

 * Instead drivers need to manually send out the event from their interrupt

 * handler by calling drm_crtc_send_vblank_event() and make sure that there's no

 * possible race with the hardware committing the atomic update.

 *

 * Caller must hold a vblank reference for the event @e acquired by a

 * drm_crtc_vblank_get(), which will be dropped when the next vblank arrives.

/**

 * drm_crtc_send_vblank_event - helper to send vblank event after pageflip

 * @crtc: the source CRTC of the vblank event

 * @e: the event to send

 *

 * Updates sequence # and timestamp on event for the most recently processed

 * vblank, and sends it to userspace.  Caller must hold event lock.

 *

 * See drm_crtc_arm_vblank_event() for a helper which can be used in certain

 * situation, especially to send out events for atomic commit operations.

		/*

		 * Enable vblank irqs under vblank_time_lock protection.

		 * All vblank count & timestamp updates are held off

		 * until we are done reinitializing master counter and

		 * timestamps. Filtercode in drm_handle_vblank() will

		 * prevent double-accounting of same vblank interval.

			/* drm_update_vblank_count() includes a wmb so we just

			 * need to ensure that the compiler emits the write

			 * to mark the vblank as enabled after the call

			 * to drm_update_vblank_count().

 Going from 0->1 means we have to enable interrupts again */

/**

 * drm_crtc_vblank_get - get a reference count on vblank events

 * @crtc: which CRTC to own

 *

 * Acquire a reference count on vblank events to avoid having them disabled

 * while in use.

 *

 * Returns:

 * Zero on success or a negative error code on failure.

 Last user schedules interrupt disable */

/**

 * drm_crtc_vblank_put - give up ownership of vblank events

 * @crtc: which counter to give up

 *

 * Release ownership of a given vblank counter, turning off interrupts

 * if possible. Disable interrupts after drm_vblank_offdelay milliseconds.

/**

 * drm_wait_one_vblank - wait for one vblank

 * @dev: DRM device

 * @pipe: CRTC index

 *

 * This waits for one vblank to pass on @pipe, using the irq driver interfaces.

 * It is a failure to call this when the vblank irq for @pipe is disabled, e.g.

 * due to lack of driver support or because the crtc is off.

 *

 * This is the legacy version of drm_crtc_wait_one_vblank().

/**

 * drm_crtc_wait_one_vblank - wait for one vblank

 * @crtc: DRM crtc

 *

 * This waits for one vblank to pass on @crtc, using the irq driver interfaces.

 * It is a failure to call this when the vblank irq for @crtc is disabled, e.g.

 * due to lack of driver support or because the crtc is off.

/**

 * drm_crtc_vblank_off - disable vblank events on a CRTC

 * @crtc: CRTC in question

 *

 * Drivers can use this function to shut down the vblank interrupt handling when

 * disabling a crtc. This function ensures that the latest vblank frame count is

 * stored so that drm_vblank_on can restore it again.

 *

 * Drivers must use this function when the hardware vblank counter can get

 * reset, e.g. when suspending or disabling the @crtc in general.

	/*

	 * Grab event_lock early to prevent vblank work from being scheduled

	 * while we're in the middle of shutting down vblank interrupts

	/* Avoid redundant vblank disables without previous

	/*

	 * Prevent subsequent drm_vblank_get() from re-enabling

	 * the vblank interrupt by bumping the refcount.

 Send any queued vblank events, lest the natives grow disquiet */

 Cancel any leftover pending vblank work */

	/* Will be reset by the modeset helpers when re-enabling the crtc by

 Wait for any vblank work that's still executing to finish */

/**

 * drm_crtc_vblank_reset - reset vblank state to off on a CRTC

 * @crtc: CRTC in question

 *

 * Drivers can use this function to reset the vblank state to off at load time.

 * Drivers should use this together with the drm_crtc_vblank_off() and

 * drm_crtc_vblank_on() functions. The difference compared to

 * drm_crtc_vblank_off() is that this function doesn't save the vblank counter

 * and hence doesn't need to call any driver hooks.

 *

 * This is useful for recovering driver state e.g. on driver load, or on resume.

	/*

	 * Prevent subsequent drm_vblank_get() from enabling the vblank

	 * interrupt by bumping the refcount.

/**

 * drm_crtc_set_max_vblank_count - configure the hw max vblank counter value

 * @crtc: CRTC in question

 * @max_vblank_count: max hardware vblank counter value

 *

 * Update the maximum hardware vblank counter value for @crtc

 * at runtime. Useful for hardware where the operation of the

 * hardware vblank counter depends on the currently active

 * display configuration.

 *

 * For example, if the hardware vblank counter does not work

 * when a specific connector is active the maximum can be set

 * to zero. And when that specific connector isn't active the

 * maximum can again be set to the appropriate non-zero value.

 *

 * If used, must be called before drm_vblank_on().

/**

 * drm_crtc_vblank_on - enable vblank events on a CRTC

 * @crtc: CRTC in question

 *

 * This functions restores the vblank interrupt state captured with

 * drm_crtc_vblank_off() again and is generally called when enabling @crtc. Note

 * that calls to drm_crtc_vblank_on() and drm_crtc_vblank_off() can be

 * unbalanced and so can also be unconditionally called in driver load code to

 * reflect the current hardware state of the crtc.

 Drop our private "prevent drm_vblank_get" refcount */

	/*

	 * re-enable interrupts if there are users left, or the

	 * user wishes vblank interrupts to be enabled all the time.

/**

 * drm_crtc_vblank_restore - estimate missed vblanks and update vblank count.

 * @crtc: CRTC in question

 *

 * Power manamement features can cause frame counter resets between vblank

 * disable and enable. Drivers can use this function in their

 * &drm_crtc_funcs.enable_vblank implementation to estimate missed vblanks since

 * the last &drm_crtc_funcs.disable_vblank using timestamps and update the

 * vblank counter.

 *

 * Note that drivers must have race-free high-precision timestamping support,

 * i.e.  &drm_crtc_funcs.get_vblank_timestamp must be hooked up and

 * &drm_driver.vblank_disable_immediate must be set to indicate the

 * time-stamping functions are race-free against vblank hardware counter

 * increments.

 vblank is not initialized (IRQ not installed ?), or has been freed */

	/*

	 * To avoid all the problems that might happen if interrupts

	 * were enabled/disabled around or between these calls, we just

	 * have the kernel take a reference on the CRTC (just once though

	 * to avoid corrupting the count if multiple, mismatch calls occur),

	 * so that interrupts remain enabled in the interim.

 vblank is not initialized (IRQ not installed ?), or has been freed */

 If drm_vblank_init() hasn't been called yet, just no-op */

 KMS drivers handle this internally */

	/*

	 * drm_crtc_vblank_off() might have been called after we called

	 * drm_vblank_get(). drm_crtc_vblank_off() holds event_lock around the

	 * vblank disable, so no need for further locking.  The reference from

	 * drm_vblank_get() protects against vblank disable from another source.

 drm_handle_vblank_events will call drm_vblank_put */

/*

 * Widen a 32-bit param to 64-bits.

 *

 * \param narrow 32-bit value (missing upper 32 bits)

 * \param near 64-bit value that should be 'close' to near

 *

 * This function returns a 64-bit value using the lower 32-bits from

 * 'narrow' and constructing the upper 32-bits so that the result is

 * as close as possible to 'near'.

	/*

	 * drm_wait_vblank_reply is a UAPI structure that uses 'long'

	 * to store the seconds. This is safe as we always use monotonic

	 * timestamps since linux-4.15.

 Convert lease-relative crtc index into global crtc index */

	/* If the counter is currently enabled and accurate, short-circuit

	 * queries to return the cached timestamp of the last vblank.

		/* must hold on to the vblank ref until the event fires

		 * drm_vblank_put will be called asynchronously

 timeout */

 interrupted by signal */

/**

 * drm_handle_vblank - handle a vblank event

 * @dev: DRM device

 * @pipe: index of CRTC where this event occurred

 *

 * Drivers should call this routine in their vblank interrupt handlers to

 * update the vblank counter and send any signals that may be pending.

 *

 * This is the legacy version of drm_crtc_handle_vblank().

	/* Need timestamp lock to prevent concurrent execution with

	 * vblank enable/disable, as this would cause inconsistent

	 * or corrupted timestamps and vblank counts.

 Vblank irq handling disabled. Nothing to do. */

	/* With instant-off, we defer disabling the interrupt until after

	 * we finish processing the following vblank after all events have

	 * been signaled. The disable has to be last (after

	 * drm_handle_vblank_events) so that the timestamp is always accurate.

/**

 * drm_crtc_handle_vblank - handle a vblank event

 * @crtc: where this event occurred

 *

 * Drivers should call this routine in their vblank interrupt handlers to

 * update the vblank counter and send any signals that may be pending.

 *

 * This is the native KMS version of drm_handle_vblank().

 *

 * Note that for a given vblank counter value drm_crtc_handle_vblank()

 * and drm_crtc_vblank_count() or drm_crtc_vblank_count_and_time()

 * provide a barrier: Any writes done before calling

 * drm_crtc_handle_vblank() will be visible to callers of the later

 * functions, if the vblank count is the same or a later one.

 *

 * See also &drm_vblank_crtc.count.

 *

 * Returns:

 * True if the event was successfully handled, false on failure.

/*

 * Get crtc VBLANK count.

 *

 * \param dev DRM device

 * \param data user argument, pointing to a drm_crtc_get_sequence structure.

 * \param file_priv drm file private for the user's open file descriptor

/*

 * Queue a event for VBLANK sequence

 *

 * \param dev DRM device

 * \param data user argument, pointing to a drm_crtc_queue_sequence structure.

 * \param file_priv drm file private for the user's open file descriptor

 Check valid flag bits */

	/*

	 * drm_crtc_vblank_off() might have been called after we called

	 * drm_crtc_vblank_get(). drm_crtc_vblank_off() holds event_lock around the

	 * vblank disable, so no need for further locking.  The reference from

	 * drm_crtc_vblank_get() protects against vblank disable from another source.

 drm_handle_vblank_events will call drm_vblank_put */

/*

 * Copyright (c) 2006-2008 Intel Corporation

 * Copyright (c) 2007 Dave Airlie <airlied@linux.ie>

 *

 * DRM core CRTC related functions

 *

 * Permission to use, copy, modify, distribute, and sell this software and its

 * documentation for any purpose is hereby granted without fee, provided that

 * the above copyright notice appear in all copies and that both that copyright

 * notice and this permission notice appear in supporting documentation, and

 * that the name of the copyright holders not be used in advertising or

 * publicity pertaining to distribution of the software without specific,

 * written prior permission.  The copyright holders make no representations

 * about the suitability of this software for any purpose.  It is provided "as

 * is" without express or implied warranty.

 *

 * THE COPYRIGHT HOLDERS DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,

 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO

 * EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY SPECIAL, INDIRECT OR

 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE,

 * DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER

 * TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE

 * OF THIS SOFTWARE.

 *

 * Authors:

 *      Keith Packard

 *	Eric Anholt <eric@anholt.net>

 *      Dave Airlie <airlied@linux.ie>

 *      Jesse Barnes <jesse.barnes@intel.com>

/**

 * DOC: overview

 *

 * The CRTC modeset helper library provides a default set_config implementation

 * in drm_crtc_helper_set_config(). Plus a few other convenience functions using

 * the same callbacks which drivers can use to e.g. restore the modeset

 * configuration on resume with drm_helper_resume_force_mode().

 *

 * Note that this helper library doesn't track the current power state of CRTCs

 * and encoders. It can call callbacks like &drm_encoder_helper_funcs.dpms even

 * though the hardware is already in the desired state. This deficiency has been

 * fixed in the atomic helpers.

 *

 * The driver callbacks are mostly compatible with the atomic modeset helpers,

 * except for the handling of the primary plane: Atomic helpers require that the

 * primary plane is implemented as a real standalone plane and not directly tied

 * to the CRTC state. For easier transition this library provides functions to

 * implement the old semantics required by the CRTC helpers using the new plane

 * and atomic helper callbacks.

 *

 * Drivers are strongly urged to convert to the atomic helpers (by way of first

 * converting to the plane helpers). New drivers must not use these functions

 * but need to implement the atomic interface instead, potentially using the

 * atomic helpers for that.

 *

 * These legacy modeset helpers use the same function table structures as

 * all other modesetting helpers. See the documentation for struct

 * &drm_crtc_helper_funcs, &struct drm_encoder_helper_funcs and struct

 * &drm_connector_helper_funcs.

/**

 * drm_helper_encoder_in_use - check if a given encoder is in use

 * @encoder: encoder to check

 *

 * Checks whether @encoder is with the current mode setting output configuration

 * in use by any connector. This doesn't mean that it is actually enabled since

 * the DPMS state is tracked separately.

 *

 * Returns:

 * True if @encoder is used, false otherwise.

	/*

	 * We can expect this mutex to be locked if we are not panicking.

	 * Locking is currently fubar in the panic handler.

/**

 * drm_helper_crtc_in_use - check if a given CRTC is in a mode_config

 * @crtc: CRTC to check

 *

 * Checks whether @crtc is with the current mode setting output configuration

 * in use by any connector. This doesn't mean that it is actually enabled since

 * the DPMS state is tracked separately.

 *

 * Returns:

 * True if @crtc is used, false otherwise.

	/*

	 * We can expect this mutex to be locked if we are not panicking.

	 * Locking is currently fubar in the panic handler.

 disconnect encoder from any connector */

/**

 * drm_helper_disable_unused_functions - disable unused objects

 * @dev: DRM device

 *

 * This function walks through the entire mode setting configuration of @dev. It

 * will remove any CRTC links of unused encoders and encoder links of

 * disconnected connectors. Then it will disable all unused encoders and CRTCs

 * either by calling their disable callback if available or by calling their

 * dpms callback with DRM_MODE_DPMS_OFF.

 *

 * NOTE:

 *

 * This function is part of the legacy modeset helper library and will cause

 * major confusion with atomic drivers. This is because atomic helpers guarantee

 * to never call ->disable() hooks on a disabled function, or ->enable() hooks

 * on an enabled functions. drm_helper_disable_unused_functions() on the other

 * hand throws such guarantees into the wind and calls disable hooks

 * unconditionally on unused functions.

/*

 * Check the CRTC we're going to map each output to vs. its current

 * CRTC.  If they don't match, we have to disable the output and the CRTC

 * since the driver will have to re-route things.

 Disable unused encoders */

/**

 * drm_crtc_helper_set_mode - internal helper to set a mode

 * @crtc: CRTC to program

 * @mode: mode to use

 * @x: horizontal offset into the surface

 * @y: vertical offset into the surface

 * @old_fb: old framebuffer, for cleanup

 *

 * Try to set @mode on @crtc.  Give @crtc and its associated connectors a chance

 * to fixup or reject the mode prior to trying to set it. This is an internal

 * helper that drivers could e.g. use to update properties that require the

 * entire output pipe to be disabled and re-enabled in a new configuration. For

 * example for changing whether audio is enabled on a hdmi link or for changing

 * panel fitter or dither attributes. It is also called by the

 * drm_crtc_helper_set_config() helper function to drive the mode setting

 * sequence.

 *

 * Returns:

 * True if the mode was set successfully, false otherwise.

	/* Update crtc values up front so the driver can rely on them for mode

	 * setting.

	/* Pass our mode to the connectors and the CRTC to give them a chance to

	 * adjust it according to limitations or connector properties, and also

	 * a chance to reject the mode entirely.

 Prepare the encoders and CRTCs before setting the mode. */

 Disable the encoders as the first thing we do. */

	/* Set up the DPLL and any encoders state that needs to adjust or depend

	 * on the DPLL.

 Now enable the clocks, plane, pipe, and connectors that we set up. */

	/* Calculate and store various constants which

	 * are later needed by vblank and swap-completion

	 * timestamping. They are derived from true hwmode.

 FIXME: add subpixel order */

 Decouple all encoders and their attached connectors from this crtc */

			/*

			 * drm_helper_disable_unused_functions() ought to be

			 * doing this, but since we've decoupled the encoder

			 * from the connector above, the required connection

			 * between them is henceforth no longer available.

 we keep a reference while the encoder is bound */

/*

 * For connectors that support multiple encoders, either the

 * .atomic_best_encoder() or .best_encoder() operation must be implemented.

/**

 * drm_crtc_helper_set_config - set a new config from userspace

 * @set: mode set configuration

 * @ctx: lock acquire context, not used here

 *

 * The drm_crtc_helper_set_config() helper function implements the of

 * &drm_crtc_funcs.set_config callback for drivers using the legacy CRTC

 * helpers.

 *

 * It first tries to locate the best encoder for each connector by calling the

 * connector @drm_connector_helper_funcs.best_encoder helper operation.

 *

 * After locating the appropriate encoders, the helper function will call the

 * mode_fixup encoder and CRTC helper operations to adjust the requested mode,

 * or reject it completely in which case an error will be returned to the

 * application. If the new configuration after mode adjustment is identical to

 * the current configuration the helper function will return without performing

 * any other operation.

 *

 * If the adjusted mode is identical to the current mode but changes to the

 * frame buffer need to be applied, the drm_crtc_helper_set_config() function

 * will call the CRTC &drm_crtc_helper_funcs.mode_set_base helper operation.

 *

 * If the adjusted mode differs from the current mode, or if the

 * ->mode_set_base() helper operation is not provided, the helper function

 * performs a full mode set sequence by calling the ->prepare(), ->mode_set()

 * and ->commit() CRTC and encoder helper operations, in that order.

 * Alternatively it can also use the dpms and disable helper operations. For

 * details see &struct drm_crtc_helper_funcs and struct

 * &drm_encoder_helper_funcs.

 *

 * This function is deprecated.  New drivers must implement atomic modeset

 * support, for which this function is unsuitable. Instead drivers should use

 * drm_atomic_helper_set_config().

 *

 * Returns:

 * Returns 0 on success, negative errno numbers on failure.

 if true do a full mode set */

 if true and !mode_changed just do a flip */

 Enforce sane interface api - has been abused by the fb helper. */

	/*

	 * Allocate space for the backup of all (non-pointer) encoder and

	 * connector data.

	/*

	 * Copy data. Note that driver private data is not affected.

	 * Should anything bad happen only the expected state is

	 * restored, not the drivers personal bookkeeping.

	/* We should be able to check here if the fb has the same properties

 If we have no fb then treat it as a full mode set */

	/* take a reference on all unbound connectors in set, reuse the

	 * already taken reference for bound connectors

 a) traverse passed in connector list and get encoders for them */

				/* if we can't get an encoder for a connector

 don't break so fail path works correct */

			/* If the encoder is reused for another connector, then

			 * the appropriate crtc will be set later.

 Make sure the new CRTC will work with the encoder */

 mode_set_base is not a required function */

 Restore all previous data. */

	/* after fail drop reference on all unbound connectors in set, let

	 * bound connectors keep their reference

 Try to restore the config */

 Helper which handles bridge ordering around encoder dpms */

/**

 * drm_helper_connector_dpms() - connector dpms helper implementation

 * @connector: affected connector

 * @mode: DPMS mode

 *

 * The drm_helper_connector_dpms() helper function implements the

 * &drm_connector_funcs.dpms callback for drivers using the legacy CRTC

 * helpers.

 *

 * This is the main helper function provided by the CRTC helper framework for

 * implementing the DPMS connector attribute. It computes the new desired DPMS

 * state for all encoders and CRTCs in the output mesh and calls the

 * &drm_crtc_helper_funcs.dpms and &drm_encoder_helper_funcs.dpms callbacks

 * provided by the driver.

 *

 * This function is deprecated.  New drivers must implement atomic modeset

 * support, where DPMS is handled in the DRM core.

 *

 * Returns:

 * Always returns 0.

 from off to on, do crtc then encoder */

 from on to off, do encoder then crtc */

/**

 * drm_helper_resume_force_mode - force-restore mode setting configuration

 * @dev: drm_device which should be restored

 *

 * Drivers which use the mode setting helpers can use this function to

 * force-restore the mode setting configuration e.g. on resume or when something

 * else might have trampled over the hw state (like some overzealous old BIOSen

 * tended to do).

 *

 * This helper doesn't provide a error return value since restoring the old

 * config should never fail due to resource allocation issues since the driver

 * has successfully set the restored configuration already. Hence this should

 * boil down to the equivalent of a few dpms on calls, which also don't provide

 * an error code.

 *

 * Drivers where simply restoring an old configuration again might fail (e.g.

 * due to slight differences in allocating shared resources when the

 * configuration is restored in a different order than when userspace set it up)

 * need to use their own restore logic.

 *

 * This function is deprecated. New drivers should implement atomic mode-

 * setting and use the atomic suspend/resume helpers.

 *

 * See also:

 * drm_atomic_helper_suspend(), drm_atomic_helper_resume()

 Restoring the old config should never fail! */

 Turn off outputs that were already powered off */

 disable the unused connectors while restoring the modesetting */

/**

 * drm_helper_force_disable_all - Forcibly turn off all enabled CRTCs

 * @dev: DRM device whose CRTCs to turn off

 *

 * Drivers may want to call this on unload to ensure that all displays are

 * unlit and the GPU is in a consistent, low power state. Takes modeset locks.

 *

 * Note: This should only be used by non-atomic legacy drivers. For an atomic

 * version look at drm_atomic_helper_shutdown().

 *

 * Returns:

 * Zero on success, error code on failure.

/**************************************************************************

 *

 * Copyright 2006 Tungsten Graphics, Inc., Bismarck, ND., USA.

 * Copyright 2016 Intel Corporation

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the

 * "Software"), to deal in the Software without restriction, including

 * without limitation the rights to use, copy, modify, merge, publish,

 * distribute, sub license, and/or sell copies of the Software, and to

 * permit persons to whom the Software is furnished to do so, subject to

 * the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL

 * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,

 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR

 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE

 * USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

 *

/*

 * Generic simple memory manager implementation. Intended to be used as a base

 * class implementation for more advanced memory managers.

 *

 * Note that the algorithm used is quite simple and there might be substantial

 * performance gains if a smarter free list is implemented. Currently it is

 * just an unordered stack of free regions. This could easily be improved if

 * an RB-tree is used instead. At least if we expect heavy fragmentation.

 *

 * Aligned allocations can also see improvement.

 *

 * Authors:

 * Thomas HellstrÃ¶m <thomas-at-tungstengraphics-dot-com>

/**

 * DOC: Overview

 *

 * drm_mm provides a simple range allocator. The drivers are free to use the

 * resource allocator from the linux core if it suits them, the upside of drm_mm

 * is that it's in the DRM core. Which means that it's easier to extend for

 * some of the crazier special purpose needs of gpus.

 *

 * The main data struct is &drm_mm, allocations are tracked in &drm_mm_node.

 * Drivers are free to embed either of them into their own suitable

 * datastructures. drm_mm itself will not do any memory allocations of its own,

 * so if drivers choose not to embed nodes they need to still allocate them

 * themselves.

 *

 * The range allocator also supports reservation of preallocated blocks. This is

 * useful for taking over initial mode setting configurations from the firmware,

 * where an object needs to be created which exactly matches the firmware's

 * scanout target. As long as the range is still free it can be inserted anytime

 * after the allocator is initialized, which helps with avoiding looped

 * dependencies in the driver load sequence.

 *

 * drm_mm maintains a stack of most recently freed holes, which of all

 * simplistic datastructures seems to be a fairly decent approach to clustering

 * allocations and avoiding too much fragmentation. This means free space

 * searches are O(num_holes). Given that all the fancy features drm_mm supports

 * something better would be fairly complex and since gfx thrashing is a fairly

 * steep cliff not a real concern. Removing a node again is O(1).

 *

 * drm_mm supports a few features: Alignment and range restrictions can be

 * supplied. Furthermore every &drm_mm_node has a color value (which is just an

 * opaque unsigned long) which in conjunction with a driver callback can be used

 * to implement sophisticated placement restrictions. The i915 DRM driver uses

 * this to implement guard pages between incompatible caching domains in the

 * graphics TT.

 *

 * Two behaviors are supported for searching and allocating: bottom-up and

 * top-down. The default is bottom-up. Top-down allocation can be used if the

 * memory area has different restrictions, or just to reduce fragmentation.

 *

 * Finally iteration helpers to walk all nodes and all holes are provided as are

 * some basic allocator dumpers for debugging.

 *

 * Note that this range allocator is not thread-safe, drivers need to protect

 * modifications with their own locking. The idea behind this is that for a full

 * memory manager additional data needs to be protected anyway, hence internal

 * locking would be fully redundant.

 May be called under spinlock, so avoid sleeping */

/**

 * DECLARE_NEXT_HOLE_ADDR - macro to declare next hole functions

 * @name: name of function to declare

 * @first: first rb member to traverse (either rb_left or rb_right).

 * @last: last rb member to traverse (either rb_right or rb_left).

 *

 * This macro declares a function to return the next hole of the addr rb tree.

 * While traversing the tree we take the searched size into account and only

 * visit branches with potential big enough holes.

/**

 * drm_mm_reserve_node - insert an pre-initialized node

 * @mm: drm_mm allocator to insert @node into

 * @node: drm_mm_node to insert

 *

 * This functions inserts an already set-up &drm_mm_node into the allocator,

 * meaning that start, size and color must be set by the caller. All other

 * fields must be cleared to 0. This is useful to initialize the allocator with

 * preallocated objects which must be set-up before the range allocator can be

 * set-up, e.g. when taking over a firmware framebuffer.

 *

 * Returns:

 * 0 on success, -ENOSPC if there's no hole where @node is.

 Find the relevant hole to add our node to */

/**

 * drm_mm_insert_node_in_range - ranged search for space and insert @node

 * @mm: drm_mm to allocate from

 * @node: preallocate node to insert

 * @size: size of the allocation

 * @alignment: alignment of the allocation

 * @color: opaque tag value to use for this node

 * @range_start: start of the allowed range for this node

 * @range_end: end of the allowed range for this node

 * @mode: fine-tune the allocation search and placement

 *

 * The preallocated @node must be cleared to 0.

 *

 * Returns:

 * 0 on success, -ENOSPC if there's no suitable hole.

/**

 * drm_mm_remove_node - Remove a memory node from the allocator.

 * @node: drm_mm_node to remove

 *

 * This just removes a node from its drm_mm allocator. The node does not need to

 * be cleared again before it can be re-inserted into this or any other drm_mm

 * allocator. It is a bug to call this function on a unallocated node.

/**

 * drm_mm_replace_node - move an allocation from @old to @new

 * @old: drm_mm_node to remove from the allocator

 * @new: drm_mm_node which should inherit @old's allocation

 *

 * This is useful for when drivers embed the drm_mm_node structure and hence

 * can't move allocations by reassigning pointers. It's a combination of remove

 * and insert with the guarantee that the allocation start will match.

/**

 * DOC: lru scan roster

 *

 * Very often GPUs need to have continuous allocations for a given object. When

 * evicting objects to make space for a new one it is therefore not most

 * efficient when we simply start to select all objects from the tail of an LRU

 * until there's a suitable hole: Especially for big objects or nodes that

 * otherwise have special allocation constraints there's a good chance we evict

 * lots of (smaller) objects unnecessarily.

 *

 * The DRM range allocator supports this use-case through the scanning

 * interfaces. First a scan operation needs to be initialized with

 * drm_mm_scan_init() or drm_mm_scan_init_with_range(). The driver adds

 * objects to the roster, probably by walking an LRU list, but this can be

 * freely implemented. Eviction candidates are added using

 * drm_mm_scan_add_block() until a suitable hole is found or there are no

 * further evictable objects. Eviction roster metadata is tracked in &struct

 * drm_mm_scan.

 *

 * The driver must walk through all objects again in exactly the reverse

 * order to restore the allocator state. Note that while the allocator is used

 * in the scan mode no other operation is allowed.

 *

 * Finally the driver evicts all objects selected (drm_mm_scan_remove_block()

 * reported true) in the scan, and any overlapping nodes after color adjustment

 * (drm_mm_scan_color_evict()). Adding and removing an object is O(1), and

 * since freeing a node is also O(1) the overall complexity is

 * O(scanned_objects). So like the free stack which needs to be walked before a

 * scan operation even begins this is linear in the number of objects. It

 * doesn't seem to hurt too badly.

/**

 * drm_mm_scan_init_with_range - initialize range-restricted lru scanning

 * @scan: scan state

 * @mm: drm_mm to scan

 * @size: size of the allocation

 * @alignment: alignment of the allocation

 * @color: opaque tag value to use for the allocation

 * @start: start of the allowed range for the allocation

 * @end: end of the allowed range for the allocation

 * @mode: fine-tune the allocation search and placement

 *

 * This simply sets up the scanning routines with the parameters for the desired

 * hole.

 *

 * Warning:

 * As long as the scan list is non-empty, no other operations than

 * adding/removing nodes to/from the scan list are allowed.

/**

 * drm_mm_scan_add_block - add a node to the scan list

 * @scan: the active drm_mm scanner

 * @node: drm_mm_node to add

 *

 * Add a node to the scan list that might be freed to make space for the desired

 * hole.

 *

 * Returns:

 * True if a hole has been found, false otherwise.

	/* Remove this block from the node_list so that we enlarge the hole

	 * (distance between the end of our previous node and the start of

	 * or next), without poisoning the link so that we can restore it

	 * later in drm_mm_scan_remove_block().

/**

 * drm_mm_scan_remove_block - remove a node from the scan list

 * @scan: the active drm_mm scanner

 * @node: drm_mm_node to remove

 *

 * Nodes **must** be removed in exactly the reverse order from the scan list as

 * they have been added (e.g. using list_add() as they are added and then

 * list_for_each() over that eviction list to remove), otherwise the internal

 * state of the memory manager will be corrupted.

 *

 * When the scan list is empty, the selected memory nodes can be freed. An

 * immediately following drm_mm_insert_node_in_range_generic() or one of the

 * simpler versions of that function with !DRM_MM_SEARCH_BEST will then return

 * the just freed block (because it's at the top of the free_stack list).

 *

 * Returns:

 * True if this block should be evicted, false otherwise. Will always

 * return false when no hole has been found.

	/* During drm_mm_scan_add_block() we decoupled this node leaving

	 * its pointers intact. Now that the caller is walking back along

	 * the eviction list we can restore this block into its rightful

	 * place on the full node_list. To confirm that the caller is walking

	 * backwards correctly we check that prev_node->next == node->next,

	 * i.e. both believe the same node should be on the other side of the

	 * hole.

/**

 * drm_mm_scan_color_evict - evict overlapping nodes on either side of hole

 * @scan: drm_mm scan with target hole

 *

 * After completing an eviction scan and removing the selected nodes, we may

 * need to remove a few more nodes from either side of the target hole if

 * mm.color_adjust is being used.

 *

 * Returns:

 * A node to evict, or NULL if there are no overlapping nodes.

	/*

	 * The hole found during scanning should ideally be the first element

	 * in the hole_stack list, but due to side-effects in the driver it

	 * may not be.

 We should only be called after we found the hole previously */

/**

 * drm_mm_init - initialize a drm-mm allocator

 * @mm: the drm_mm structure to initialize

 * @start: start of the range managed by @mm

 * @size: end of the range managed by @mm

 *

 * Note that @mm must be cleared to 0 before calling this function.

 Clever trick to avoid a special case in the free hole tracking. */

/**

 * drm_mm_takedown - clean up a drm_mm allocator

 * @mm: drm_mm allocator to clean up

 *

 * Note that it is a bug to call this function on an allocator which is not

 * clean.

/**

 * drm_mm_print - print allocator state

 * @mm: drm_mm allocator to print

 * @p: DRM printer to use

/**************************************************************************

 *

 * Copyright 2006 Tungsten Graphics, Inc., Bismarck, ND. USA.

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the

 * "Software"), to deal in the Software without restriction, including

 * without limitation the rights to use, copy, modify, merge, publish,

 * distribute, sub license, and/or sell copies of the Software, and to

 * permit persons to whom the Software is furnished to do so, subject to

 * the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL

 * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,

 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR

 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE

 * USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

 *

/*

 * Simple open hash tab implementation.

 *

 * Authors:

 * Thomas HellstrÃ¶m <thomas-at-tungstengraphics-dot-com>

/*

 * Just insert an item and return any "bits" bit key that hasn't been

 * used before.

/*

 * Copyright (C) 2016 Red Hat

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included in

 * all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR

 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,

 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR

 * OTHER DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 * Rob Clark <robdclark@gmail.com>

 for pr_debug() */

/*

 * __drm_debug: Enable debug output.

 * Bitmask of DRM_UT_x. See include/drm/drm_print.h for details.

 Copy out the bit of the string that we need */

 Figure out how big the string will be */

 This is the easiest path, we've already advanced beyond the offset */

 Then check if we can directly copy into the target buffer */

	/*

	 * Finally, hit the slow path and make a temporary string to copy over

	 * using _drm_puts_coredump

/**

 * drm_puts - print a const string to a &drm_printer stream

 * @p: the &drm printer

 * @str: const string

 *

 * Allow &drm_printer types that have a constant string

 * option to use it.

/**

 * drm_printf - print to a &drm_printer stream

 * @p: the &drm_printer

 * @f: format string

/**

 * drm_print_bits - print bits to a &drm_printer stream

 *

 * Print bits (in flag fields for example) in human readable form.

 *

 * @p: the &drm_printer

 * @value: field value.

 * @bits: Array with bit names.

 * @nbits: Size of bit names array.

/**

 * drm_print_regset32 - print the contents of registers to a

 * &drm_printer stream.

 *

 * @p: the &drm printer

 * @regset: the list of registers to print.

 *

 * Often in driver debug, it's useful to be able to either capture the

 * contents of registers in the steady state using debugfs or at

 * specific points during operation.  This lets the driver have a

 * single list of registers for both.

/*

 * Copyright (C) 2014 Red Hat

 * Copyright (C) 2014 Intel Corp.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included in

 * all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR

 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,

 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR

 * OTHER DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 * Rob Clark <robdclark@gmail.com>

 * Daniel Vetter <daniel.vetter@ffwll.ch>

/**

 * DOC: overview

 *

 * This helper library provides implementations of check and commit functions on

 * top of the CRTC modeset helper callbacks and the plane helper callbacks. It

 * also provides convenience implementations for the atomic state handling

 * callbacks for drivers which don't need to subclass the drm core structures to

 * add their own additional internal state.

 *

 * This library also provides default implementations for the check callback in

 * drm_atomic_helper_check() and for the commit callback with

 * drm_atomic_helper_commit(). But the individual stages and callbacks are

 * exposed to allow drivers to mix and match and e.g. use the plane helpers only

 * together with a driver private modeset implementation.

 *

 * This library also provides implementations for all the legacy driver

 * interfaces on top of the atomic interface. See drm_atomic_helper_set_config(),

 * drm_atomic_helper_disable_plane(), and the various functions to implement

 * set_property callbacks. New drivers must not implement these functions

 * themselves but must use the provided helpers.

 *

 * The atomic helper uses the same function table structures as all other

 * modesetting helpers. See the documentation for &struct drm_crtc_helper_funcs,

 * struct &drm_encoder_helper_funcs and &struct drm_connector_helper_funcs. It

 * also shares the &struct drm_plane_helper_funcs function table with the plane

 * helpers.

	/*

	 * First loop, find all newly assigned encoders from the connectors

	 * part of the state. If the same encoder is assigned to multiple

	 * connectors bail out.

	/*

	 * Second loop, iterate over all connectors not part of the state.

	 *

	 * If a conflicting encoder is found and disable_conflicting_encoders

	 * is not set, an error is returned. Userspace can provide a solution

	 * through the atomic ioctl.

	 *

	 * If the flag is set conflicting connectors are removed from the CRTC

	 * and the CRTC is disabled if no encoder is left. This preserves

	 * compatibility with the legacy set_config behavior.

 Unset the encoder_mask in the old crtc state. */

		/* A NULL crtc is an error here because we should have

		 * duplicated a NULL best_encoder when crtc was NULL.

		 * As an exception restoring duplicated atomic state

		 * during resume is allowed, so don't warn when

		 * best_encoder is equal to encoder we intend to set.

	/*

	 * For compatibility with legacy users, we want to make sure that

	 * we allow DPMS On->Off modesets on unregistered connectors. Modesets

	 * which would result in anything else must be considered invalid, to

	 * avoid turning on new displays on dead connectors.

	 *

	 * Since the connector can be unregistered at any point during an

	 * atomic check or commit, this is racy. But that's OK: all we care

	 * about is ensuring that userspace can't do anything but shut off the

	 * display on a connector that was destroyed after it's been notified,

	 * not before.

	 *

	 * Additionally, we also want to ignore connector registration when

	 * we're trying to restore an atomic state during system resume since

	 * there's a chance the connector may have been destroyed during the

	 * process, but it's better to ignore that then cause

	 * drm_atomic_helper_resume() to fail.

		/*

		 * Each encoder has at most one connector (since we always steal

		 * it away), so we won't call ->mode_fixup twice.

/**

 * drm_atomic_helper_check_modeset - validate state object for modeset changes

 * @dev: DRM device

 * @state: the driver state object

 *

 * Check the state object to see if the requested state is physically possible.

 * This does all the CRTC and connector related computations for an atomic

 * update and adds any additional connectors needed for full modesets. It calls

 * the various per-object callbacks in the follow order:

 *

 * 1. &drm_connector_helper_funcs.atomic_best_encoder for determining the new encoder.

 * 2. &drm_connector_helper_funcs.atomic_check to validate the connector state.

 * 3. If it's determined a modeset is needed then all connectors on the affected

 *    CRTC are added and &drm_connector_helper_funcs.atomic_check is run on them.

 * 4. &drm_encoder_helper_funcs.mode_valid, &drm_bridge_funcs.mode_valid and

 *    &drm_crtc_helper_funcs.mode_valid are called on the affected components.

 * 5. &drm_bridge_funcs.mode_fixup is called on all encoder bridges.

 * 6. &drm_encoder_helper_funcs.atomic_check is called to validate any encoder state.

 *    This function is only called when the encoder will be part of a configured CRTC,

 *    it must not be used for implementing connector property validation.

 *    If this function is NULL, &drm_atomic_encoder_helper_funcs.mode_fixup is called

 *    instead.

 * 7. &drm_crtc_helper_funcs.mode_fixup is called last, to fix up the mode with CRTC constraints.

 *

 * &drm_crtc_state.mode_changed is set when the input mode is changed.

 * &drm_crtc_state.connectors_changed is set when a connector is added or

 * removed from the CRTC.  &drm_crtc_state.active_changed is set when

 * &drm_crtc_state.active changes, which is used for DPMS.

 * &drm_crtc_state.no_vblank is set from the result of drm_dev_has_vblank().

 * See also: drm_atomic_crtc_needs_modeset()

 *

 * IMPORTANT:

 *

 * Drivers which set &drm_crtc_state.mode_changed (e.g. in their

 * &drm_plane_helper_funcs.atomic_check hooks if a plane update can't be done

 * without a full modeset) _must_ call this function after that change. It is

 * permitted to call this function multiple times for the same update, e.g.

 * when the &drm_crtc_helper_funcs.atomic_check functions depend upon the

 * adjusted dotclock for fifo space allocation and watermark computation.

 *

 * RETURNS:

 * Zero for success or -errno

			/*

			 * For clarity this assignment is done here, but

			 * enable == 0 is only true when there are no

			 * connectors and a NULL mode.

			 *

			 * The other way around is true as well. enable != 0

			 * implies that connectors are attached and a mode is set.

		/*

		 * This only sets crtc->connectors_changed for routing changes,

		 * drivers must set crtc->connectors_changed themselves when

		 * connector properties need to be updated.

	/*

	 * After all the routing has been prepared we need to add in any

	 * connector which is itself unchanged, but whose CRTC changes its

	 * configuration. This must be done before calling mode_fixup in case a

	 * crtc only changed its mode but has the same set of connectors.

	/*

	 * Iterate over all connectors again, to make sure atomic_check()

	 * has been called on them when a modeset is forced.

	/*

	 * Iterate over all connectors again, and add all affected bridges to

	 * the state.

/**

 * drm_atomic_helper_check_plane_state() - Check plane state for validity

 * @plane_state: plane state to check

 * @crtc_state: CRTC state to check

 * @min_scale: minimum @src:@dest scaling factor in 16.16 fixed point

 * @max_scale: maximum @src:@dest scaling factor in 16.16 fixed point

 * @can_position: is it legal to position the plane such that it

 *                doesn't cover the entire CRTC?  This will generally

 *                only be false for primary planes.

 * @can_update_disabled: can the plane be updated while the CRTC

 *                       is disabled?

 *

 * Checks that a desired plane update is valid, and updates various

 * bits of derived state (clipped coordinates etc.). Drivers that provide

 * their own plane handling rather than helper-provided implementations may

 * still wish to call this function to avoid duplication of error checking

 * code.

 *

 * RETURNS:

 * Zero if update appears valid, error code on failure

 crtc should only be NULL when disabling (i.e., !fb) */

 Check scaling */

		/*

		 * Plane isn't visible; some drivers can handle this

		 * so we just return success here.  Drivers that can't

		 * (including those that use the primary plane helper's

		 * update function) will return an error from their

		 * update_plane handler.

/**

 * drm_atomic_helper_check_planes - validate state object for planes changes

 * @dev: DRM device

 * @state: the driver state object

 *

 * Check the state object to see if the requested state is physically possible.

 * This does all the plane update related checks using by calling into the

 * &drm_crtc_helper_funcs.atomic_check and &drm_plane_helper_funcs.atomic_check

 * hooks provided by the driver.

 *

 * It also sets &drm_crtc_state.planes_changed to indicate that a CRTC has

 * updated planes.

 *

 * RETURNS:

 * Zero for success or -errno

/**

 * drm_atomic_helper_check - validate state object

 * @dev: DRM device

 * @state: the driver state object

 *

 * Check the state object to see if the requested state is physically possible.

 * Only CRTCs and planes have check callbacks, so for any additional (global)

 * checking that a driver needs it can simply wrap that around this function.

 * Drivers without such needs can directly use this as their

 * &drm_mode_config_funcs.atomic_check callback.

 *

 * This just wraps the two parts of the state checking for planes and modeset

 * state in the default order: First it calls drm_atomic_helper_check_modeset()

 * and then drm_atomic_helper_check_planes(). The assumption is that the

 * @drm_plane_helper_funcs.atomic_check and @drm_crtc_helper_funcs.atomic_check

 * functions depend upon an updated adjusted_mode.clock to e.g. properly compute

 * watermarks.

 *

 * Note that zpos normalization will add all enable planes to the state which

 * might not desired for some drivers.

 * For example enable/disable of a cursor plane which have fixed zpos value

 * would trigger all other enabled planes to be forced to the state change.

 *

 * RETURNS:

 * Zero for success or -errno

	/*

	 * No new_state means the CRTC is off, so the only criteria is whether

	 * it's currently active or in self refresh mode.

	/*

	 * We need to run through the crtc_funcs->disable() function if the CRTC

	 * is currently on, if it's transitioning to self refresh mode, or if

	 * it's in self refresh mode and needs to be fully disabled.

		/*

		 * Shut down everything that's in the changeset and currently

		 * still on. So need to check the old, saved state.

		/* We shouldn't get this far if we didn't previously have

		 * an encoder.. but WARN_ON() rather than explode.

		/*

		 * Each encoder has at most one connector (since we always steal

		 * it away), so we won't call disable hooks twice.

 Right function depends upon target state. */

 Shut down everything that needs a full modeset. */

 Right function depends upon target state. */

/**

 * drm_atomic_helper_update_legacy_modeset_state - update legacy modeset state

 * @dev: DRM device

 * @old_state: atomic state object with old state structures

 *

 * This function updates all the various legacy modeset state pointers in

 * connectors, encoders and CRTCs.

 *

 * Drivers can use this for building their own atomic commit if they don't have

 * a pure helper-based modeset implementation.

 *

 * Since these updates are not synchronized with lockings, only code paths

 * called from &drm_mode_config_helper_funcs.atomic_commit_tail can look at the

 * legacy state filled out by this helper. Defacto this means this helper and

 * the legacy state pointers are only really useful for transitioning an

 * existing driver to the atomic world.

 clear out existing links and update dpms */

 set new links */

 set legacy state in the crtc structure */

/**

 * drm_atomic_helper_calc_timestamping_constants - update vblank timestamping constants

 * @state: atomic state object

 *

 * Updates the timestamping constants used for precise vblank timestamps

 * by calling drm_calc_timestamping_constants() for all enabled crtcs in @state.

		/*

		 * Each encoder has at most one connector (since we always steal

		 * it away), so we won't call mode_set hooks twice.

/**

 * drm_atomic_helper_commit_modeset_disables - modeset commit to disable outputs

 * @dev: DRM device

 * @old_state: atomic state object with old state structures

 *

 * This function shuts down all the outputs that need to be shut down and

 * prepares them (if required) with the new mode.

 *

 * For compatibility with legacy CRTC helpers this should be called before

 * drm_atomic_helper_commit_planes(), which is what the default commit function

 * does. But drivers with different needs can group the modeset commits together

 * and do the plane commits at the end. This is useful for drivers doing runtime

 * PM since planes updates then only happen when the CRTC is actually enabled.

/**

 * drm_atomic_helper_commit_modeset_enables - modeset commit to enable outputs

 * @dev: DRM device

 * @old_state: atomic state object with old state structures

 *

 * This function enables all the outputs with the new configuration which had to

 * be turned off for the update.

 *

 * For compatibility with legacy CRTC helpers this should be called after

 * drm_atomic_helper_commit_planes(), which is what the default commit function

 * does. But drivers with different needs can group the modeset commits together

 * and do the plane commits at the end. This is useful for drivers doing runtime

 * PM since planes updates then only happen when the CRTC is actually enabled.

 Need to filter out CRTCs where only planes change. */

		/*

		 * Each encoder has at most one connector (since we always steal

		 * it away), so we won't call enable hooks twice.

/**

 * drm_atomic_helper_wait_for_fences - wait for fences stashed in plane state

 * @dev: DRM device

 * @state: atomic state object with old state structures

 * @pre_swap: If true, do an interruptible wait, and @state is the new state.

 *	Otherwise @state is the old state.

 *

 * For implicit sync, driver should fish the exclusive fence out from the

 * incoming fb's and stash it in the drm_plane_state.  This is called after

 * drm_atomic_helper_swap_state() so it uses the current plane state (and

 * just uses the atomic state to find the changed planes)

 *

 * Note that @pre_swap is needed since the point where we block for fences moves

 * around depending upon whether an atomic commit is blocking or

 * non-blocking. For non-blocking commit all waiting needs to happen after

 * drm_atomic_helper_swap_state() is called, but for blocking commits we want

 * to wait **before** we do anything that can't be easily rolled back. That is

 * before we call drm_atomic_helper_swap_state().

 *

 * Returns zero if success or < 0 if dma_fence_wait() fails.

		/*

		 * If waiting for fences pre-swap (ie: nonblock), userspace can

		 * still interrupt the operation. Instead of blocking until the

		 * timer expires, make the wait interruptible.

/**

 * drm_atomic_helper_wait_for_vblanks - wait for vblank on CRTCs

 * @dev: DRM device

 * @old_state: atomic state object with old state structures

 *

 * Helper to, after atomic commit, wait for vblanks on all affected

 * CRTCs (ie. before cleaning up old framebuffers using

 * drm_atomic_helper_cleanup_planes()). It will only wait on CRTCs where the

 * framebuffers have actually changed to optimize for the legacy cursor and

 * plane update use-case.

 *

 * Drivers using the nonblocking commit tracking support initialized by calling

 * drm_atomic_helper_setup_commit() should look at

 * drm_atomic_helper_wait_for_flip_done() as an alternative.

	 /*

	  * Legacy cursor ioctls are completely unsynced, and userspace

	  * relies on that (by doing tons of cursor updates).

/**

 * drm_atomic_helper_wait_for_flip_done - wait for all page flips to be done

 * @dev: DRM device

 * @old_state: atomic state object with old state structures

 *

 * Helper to, after atomic commit, wait for page flips on all affected

 * crtcs (ie. before cleaning up old framebuffers using

 * drm_atomic_helper_cleanup_planes()). Compared to

 * drm_atomic_helper_wait_for_vblanks() this waits for the completion on all

 * CRTCs, assuming that cursors-only updates are signalling their completion

 * immediately (or using a different path).

 *

 * This requires that drivers use the nonblocking commit tracking support

 * initialized using drm_atomic_helper_setup_commit().

/**

 * drm_atomic_helper_commit_tail - commit atomic update to hardware

 * @old_state: atomic state object with old state structures

 *

 * This is the default implementation for the

 * &drm_mode_config_helper_funcs.atomic_commit_tail hook, for drivers

 * that do not support runtime_pm or do not need the CRTC to be

 * enabled to perform a commit. Otherwise, see

 * drm_atomic_helper_commit_tail_rpm().

 *

 * Note that the default ordering of how the various stages are called is to

 * match the legacy modeset helper library closest.

/**

 * drm_atomic_helper_commit_tail_rpm - commit atomic update to hardware

 * @old_state: new modeset state to be committed

 *

 * This is an alternative implementation for the

 * &drm_mode_config_helper_funcs.atomic_commit_tail hook, for drivers

 * that support runtime_pm or need the CRTC to be enabled to perform a

 * commit. Otherwise, one should use the default implementation

 * drm_atomic_helper_commit_tail().

	/*

	 * We're measuring the _entire_ commit, so the time will vary depending

	 * on how many fences and objects are involved. For the purposes of self

	 * refresh, this is desirable since it'll give us an idea of how

	 * congested things are. This will inform our decision on how often we

	 * should enter self refresh after idle.

	 *

	 * These times will be averaged out in the self refresh helpers to avoid

	 * overreacting over one outlier frame

	/*

	 * We cannot safely access new_crtc_state after

	 * drm_atomic_helper_commit_hw_done() so figure out which crtc's have

	 * self-refresh active beforehand:

/**

 * drm_atomic_helper_async_check - check if state can be committed asynchronously

 * @dev: DRM device

 * @state: the driver state object

 *

 * This helper will check if it is possible to commit the state asynchronously.

 * Async commits are not supposed to swap the states like normal sync commits

 * but just do in-place changes on the current state.

 *

 * It will return 0 if the commit can happen in an asynchronous fashion or error

 * if not. Note that error just mean it can't be committed asynchronously, if it

 * fails the commit should be treated like a normal synchronous commit.

 FIXME: we support only single plane updates for now */

	/*

	 * Don't do an async update if there is an outstanding commit modifying

	 * the plane.  This prevents our async update's changes from getting

	 * overridden by a previous synchronous update's state.

/**

 * drm_atomic_helper_async_commit - commit state asynchronously

 * @dev: DRM device

 * @state: the driver state object

 *

 * This function commits a state asynchronously, i.e., not vblank

 * synchronized. It should be used on a state only when

 * drm_atomic_async_check() succeeds. Async commits are not supposed to swap

 * the states like normal sync commits, but just do in-place changes on the

 * current state.

 *

 * TODO: Implement full swap instead of doing in-place changes.

		/*

		 * ->atomic_async_update() is supposed to update the

		 * plane->state in-place, make sure at least common

		 * properties have been properly updated.

		/*

		 * Make sure the FBs have been swapped so that cleanups in the

		 * new_state performs a cleanup in the old FB.

/**

 * drm_atomic_helper_commit - commit validated state object

 * @dev: DRM device

 * @state: the driver state object

 * @nonblock: whether nonblocking behavior is requested.

 *

 * This function commits a with drm_atomic_helper_check() pre-validated state

 * object. This can still fail when e.g. the framebuffer reservation fails. This

 * function implements nonblocking commits, using

 * drm_atomic_helper_setup_commit() and related functions.

 *

 * Committing the actual hardware state is done through the

 * &drm_mode_config_helper_funcs.atomic_commit_tail callback, or its default

 * implementation drm_atomic_helper_commit_tail().

 *

 * RETURNS:

 * Zero for success or -errno.

	/*

	 * This is the point of no return - everything below never fails except

	 * when the hw goes bonghits. Which means we can commit the new state on

	 * the software side now.

	/*

	 * Everything below can be run asynchronously without the need to grab

	 * any modeset locks at all under one condition: It must be guaranteed

	 * that the asynchronous work has either been cancelled (if the driver

	 * supports it, which at least requires that the framebuffers get

	 * cleaned up with drm_atomic_helper_cleanup_planes()) or completed

	 * before the new state gets committed on the software side with

	 * drm_atomic_helper_swap_state().

	 *

	 * This scheme allows new atomic state updates to be prepared and

	 * checked in parallel to the asynchronous completion of the previous

	 * update. Which is important since compositors need to figure out the

	 * composition of the next frame right after having submitted the

	 * current layout.

	 *

	 * NOTE: Commit work has multiple phases, first hardware commit, then

	 * cleanup. We want them to overlap, hence need system_unbound_wq to

	 * make sure work items don't artificially stall on each another.

/**

 * DOC: implementing nonblocking commit

 *

 * Nonblocking atomic commits should use struct &drm_crtc_commit to sequence

 * different operations against each another. Locks, especially struct

 * &drm_modeset_lock, should not be held in worker threads or any other

 * asynchronous context used to commit the hardware state.

 *

 * drm_atomic_helper_commit() implements the recommended sequence for

 * nonblocking commits, using drm_atomic_helper_setup_commit() internally:

 *

 * 1. Run drm_atomic_helper_prepare_planes(). Since this can fail and we

 * need to propagate out of memory/VRAM errors to userspace, it must be called

 * synchronously.

 *

 * 2. Synchronize with any outstanding nonblocking commit worker threads which

 * might be affected by the new state update. This is handled by

 * drm_atomic_helper_setup_commit().

 *

 * Asynchronous workers need to have sufficient parallelism to be able to run

 * different atomic commits on different CRTCs in parallel. The simplest way to

 * achieve this is by running them on the &system_unbound_wq work queue. Note

 * that drivers are not required to split up atomic commits and run an

 * individual commit in parallel - userspace is supposed to do that if it cares.

 * But it might be beneficial to do that for modesets, since those necessarily

 * must be done as one global operation, and enabling or disabling a CRTC can

 * take a long time. But even that is not required.

 *

 * IMPORTANT: A &drm_atomic_state update for multiple CRTCs is sequenced

 * against all CRTCs therein. Therefore for atomic state updates which only flip

 * planes the driver must not get the struct &drm_crtc_state of unrelated CRTCs

 * in its atomic check code: This would prevent committing of atomic updates to

 * multiple CRTCs in parallel. In general, adding additional state structures

 * should be avoided as much as possible, because this reduces parallelism in

 * (nonblocking) commits, both due to locking and due to commit sequencing

 * requirements.

 *

 * 3. The software state is updated synchronously with

 * drm_atomic_helper_swap_state(). Doing this under the protection of all modeset

 * locks means concurrent callers never see inconsistent state. Note that commit

 * workers do not hold any locks; their access is only coordinated through

 * ordering. If workers would access state only through the pointers in the

 * free-standing state objects (currently not the case for any driver) then even

 * multiple pending commits could be in-flight at the same time.

 *

 * 4. Schedule a work item to do all subsequent steps, using the split-out

 * commit helpers: a) pre-plane commit b) plane commit c) post-plane commit and

 * then cleaning up the framebuffers after the old framebuffer is no longer

 * being displayed. The scheduled work should synchronize against other workers

 * using the &drm_crtc_commit infrastructure as needed. See

 * drm_atomic_helper_setup_commit() for more details.

			/*

			 * Userspace is not allowed to get ahead of the previous

			 * commit with nonblocking ones.

	/* We don't want to let commits get ahead of cleanup work too much,

	 * stalling on 2nd previous commit means triple-buffer won't ever stall.

/**

 * drm_atomic_helper_setup_commit - setup possibly nonblocking commit

 * @state: new modeset state to be committed

 * @nonblock: whether nonblocking behavior is requested.

 *

 * This function prepares @state to be used by the atomic helper's support for

 * nonblocking commits. Drivers using the nonblocking commit infrastructure

 * should always call this function from their

 * &drm_mode_config_funcs.atomic_commit hook.

 *

 * Drivers that need to extend the commit setup to private objects can use the

 * &drm_mode_config_helper_funcs.atomic_commit_setup hook.

 *

 * To be able to use this support drivers need to use a few more helper

 * functions. drm_atomic_helper_wait_for_dependencies() must be called before

 * actually committing the hardware state, and for nonblocking commits this call

 * must be placed in the async worker. See also drm_atomic_helper_swap_state()

 * and its stall parameter, for when a driver's commit hooks look at the

 * &drm_crtc.state, &drm_plane.state or &drm_connector.state pointer directly.

 *

 * Completion of the hardware commit step must be signalled using

 * drm_atomic_helper_commit_hw_done(). After this step the driver is not allowed

 * to read or change any permanent software or hardware modeset state. The only

 * exception is state protected by other means than &drm_modeset_lock locks.

 * Only the free standing @state with pointers to the old state structures can

 * be inspected, e.g. to clean up old buffers using

 * drm_atomic_helper_cleanup_planes().

 *

 * At the very end, before cleaning up @state drivers must call

 * drm_atomic_helper_commit_cleanup_done().

 *

 * This is all implemented by in drm_atomic_helper_commit(), giving drivers a

 * complete and easy-to-use default implementation of the atomic_commit() hook.

 *

 * The tracking of asynchronously executed and still pending commits is done

 * using the core structure &drm_crtc_commit.

 *

 * By default there's no need to clean up resources allocated by this function

 * explicitly: drm_atomic_state_default_clear() will take care of that

 * automatically.

 *

 * Returns:

 *

 * 0 on success. -EBUSY when userspace schedules nonblocking commits too fast,

 * -ENOMEM on allocation failures and -EINTR when a signal is pending.

		/*

		 * Drivers only send out events when at least either current or

		 * new CRTC state is active. Complete right away if everything

		 * stays off.

 Legacy cursor updates are fully unsynced. */

		/*

		 * Userspace is not allowed to get ahead of the previous

		 * commit with nonblocking ones.

 Always track connectors explicitly for e.g. link retraining. */

		/*

		 * Userspace is not allowed to get ahead of the previous

		 * commit with nonblocking ones.

 Always track planes explicitly for async pageflip support. */

/**

 * drm_atomic_helper_wait_for_dependencies - wait for required preceeding commits

 * @old_state: atomic state object with old state structures

 *

 * This function waits for all preceeding commits that touch the same CRTC as

 * @old_state to both be committed to the hardware (as signalled by

 * drm_atomic_helper_commit_hw_done()) and executed by the hardware (as signalled

 * by calling drm_crtc_send_vblank_event() on the &drm_crtc_state.event).

 *

 * This is part of the atomic helper support for nonblocking commits, see

 * drm_atomic_helper_setup_commit() for an overview.

/**

 * drm_atomic_helper_fake_vblank - fake VBLANK events if needed

 * @old_state: atomic state object with old state structures

 *

 * This function walks all CRTCs and fakes VBLANK events on those with

 * &drm_crtc_state.no_vblank set to true and &drm_crtc_state.event != NULL.

 * The primary use of this function is writeback connectors working in oneshot

 * mode and faking VBLANK events. In this case they only fake the VBLANK event

 * when a job is queued, and any change to the pipeline that does not touch the

 * connector is leading to timeouts when calling

 * drm_atomic_helper_wait_for_vblanks() or

 * drm_atomic_helper_wait_for_flip_done(). In addition to writeback

 * connectors, this function can also fake VBLANK events for CRTCs without

 * VBLANK interrupt.

 *

 * This is part of the atomic helper support for nonblocking commits, see

 * drm_atomic_helper_setup_commit() for an overview.

/**

 * drm_atomic_helper_commit_hw_done - setup possible nonblocking commit

 * @old_state: atomic state object with old state structures

 *

 * This function is used to signal completion of the hardware commit step. After

 * this step the driver is not allowed to read or change any permanent software

 * or hardware modeset state. The only exception is state protected by other

 * means than &drm_modeset_lock locks.

 *

 * Drivers should try to postpone any expensive or delayed cleanup work after

 * this function is called.

 *

 * This is part of the atomic helper support for nonblocking commits, see

 * drm_atomic_helper_setup_commit() for an overview.

		/*

		 * copy new_crtc_state->commit to old_crtc_state->commit,

		 * it's unsafe to touch new_crtc_state after hw_done,

		 * but we still need to do so in cleanup_done().

 backend must have consumed any event by now */

/**

 * drm_atomic_helper_commit_cleanup_done - signal completion of commit

 * @old_state: atomic state object with old state structures

 *

 * This signals completion of the atomic update @old_state, including any

 * cleanup work. If used, it must be called right before calling

 * drm_atomic_state_put().

 *

 * This is part of the atomic helper support for nonblocking commits, see

 * drm_atomic_helper_setup_commit() for an overview.

/**

 * drm_atomic_helper_prepare_planes - prepare plane resources before commit

 * @dev: DRM device

 * @state: atomic state object with new state structures

 *

 * This function prepares plane state, specifically framebuffers, for the new

 * configuration, by calling &drm_plane_helper_funcs.prepare_fb. If any failure

 * is encountered this function will call &drm_plane_helper_funcs.cleanup_fb on

 * any already successfully prepared framebuffer.

 *

 * Returns:

 * 0 on success, negative error code on failure.

/**

 * drm_atomic_helper_commit_planes - commit plane state

 * @dev: DRM device

 * @old_state: atomic state object with old state structures

 * @flags: flags for committing plane state

 *

 * This function commits the new plane state using the plane and atomic helper

 * functions for planes and CRTCs. It assumes that the atomic state has already

 * been pushed into the relevant object state pointers, since this step can no

 * longer fail.

 *

 * It still requires the global state object @old_state to know which planes and

 * crtcs need to be updated though.

 *

 * Note that this function does all plane updates across all CRTCs in one step.

 * If the hardware can't support this approach look at

 * drm_atomic_helper_commit_planes_on_crtc() instead.

 *

 * Plane parameters can be updated by applications while the associated CRTC is

 * disabled. The DRM/KMS core will store the parameters in the plane state,

 * which will be available to the driver when the CRTC is turned on. As a result

 * most drivers don't need to be immediately notified of plane updates for a

 * disabled CRTC.

 *

 * Unless otherwise needed, drivers are advised to set the ACTIVE_ONLY flag in

 * @flags in order not to receive plane update notifications related to a

 * disabled CRTC. This avoids the need to manually ignore plane updates in

 * driver code when the driver and/or hardware can't or just don't need to deal

 * with updates on disabled CRTCs, for example when supporting runtime PM.

 *

 * Drivers may set the NO_DISABLE_AFTER_MODESET flag in @flags if the relevant

 * display controllers require to disable a CRTC's planes when the CRTC is

 * disabled. This function would skip the &drm_plane_helper_funcs.atomic_disable

 * call for a plane if the CRTC of the old plane state needs a modesetting

 * operation. Of course, the drivers need to disable the planes in their CRTC

 * disable callbacks since no one else would do that.

 *

 * The drm_atomic_helper_commit() default implementation doesn't set the

 * ACTIVE_ONLY flag to most closely match the behaviour of the legacy helpers.

 * This should not be copied blindly by drivers.

			/*

			 * Skip planes related to inactive CRTCs. If the plane

			 * is enabled use the state of the current CRTC. If the

			 * plane is being disabled use the state of the old

			 * CRTC to avoid skipping planes being disabled on an

			 * active CRTC.

		/*

		 * Special-case disabling the plane if drivers support it.

/**

 * drm_atomic_helper_commit_planes_on_crtc - commit plane state for a CRTC

 * @old_crtc_state: atomic state object with the old CRTC state

 *

 * This function commits the new plane state using the plane and atomic helper

 * functions for planes on the specific CRTC. It assumes that the atomic state

 * has already been pushed into the relevant object state pointers, since this

 * step can no longer fail.

 *

 * This function is useful when plane updates should be done CRTC-by-CRTC

 * instead of one global step like drm_atomic_helper_commit_planes() does.

 *

 * This function can only be savely used when planes are not allowed to move

 * between different CRTCs because this function doesn't handle inter-CRTC

 * dependencies. Callers need to ensure that either no such dependencies exist,

 * resolve them through ordering of commit calls or through some other means.

/**

 * drm_atomic_helper_disable_planes_on_crtc - helper to disable CRTC's planes

 * @old_crtc_state: atomic state object with the old CRTC state

 * @atomic: if set, synchronize with CRTC's atomic_begin/flush hooks

 *

 * Disables all planes associated with the given CRTC. This can be

 * used for instance in the CRTC helper atomic_disable callback to disable

 * all planes.

 *

 * If the atomic-parameter is set the function calls the CRTC's

 * atomic_begin hook before and atomic_flush hook after disabling the

 * planes.

 *

 * It is a bug to call this function without having implemented the

 * &drm_plane_helper_funcs.atomic_disable plane hook.

/**

 * drm_atomic_helper_cleanup_planes - cleanup plane resources after commit

 * @dev: DRM device

 * @old_state: atomic state object with old state structures

 *

 * This function cleans up plane state, specifically framebuffers, from the old

 * configuration. Hence the old configuration must be perserved in @old_state to

 * be able to call this function.

 *

 * This function must also be called on the new state when the atomic update

 * fails at any point after calling drm_atomic_helper_prepare_planes().

		/*

		 * This might be called before swapping when commit is aborted,

		 * in which case we have to cleanup the new state.

/**

 * drm_atomic_helper_swap_state - store atomic state into current sw state

 * @state: atomic state

 * @stall: stall for preceding commits

 *

 * This function stores the atomic state into the current state pointers in all

 * driver objects. It should be called after all failing steps have been done

 * and succeeded, but before the actual hardware state is committed.

 *

 * For cleanup and error recovery the current state for all changed objects will

 * be swapped into @state.

 *

 * With that sequence it fits perfectly into the plane prepare/cleanup sequence:

 *

 * 1. Call drm_atomic_helper_prepare_planes() with the staged atomic state.

 *

 * 2. Do any other steps that might fail.

 *

 * 3. Put the staged state into the current state pointers with this function.

 *

 * 4. Actually commit the hardware state.

 *

 * 5. Call drm_atomic_helper_cleanup_planes() with @state, which since step 3

 * contains the old state. Also do any other cleanup required with that state.

 *

 * @stall must be set when nonblocking commits for this driver directly access

 * the &drm_plane.state, &drm_crtc.state or &drm_connector.state pointer. With

 * the current atomic helpers this is almost always the case, since the helpers

 * don't pass the right state structures to the callbacks.

 *

 * Returns:

 *

 * Returns 0 on success. Can return -ERESTARTSYS when @stall is true and the

 * waiting for the previous commits has been interrupted.

		/*

		 * We have to stall for hw_done here before

		 * drm_atomic_helper_wait_for_dependencies() because flip

		 * depth > 1 is not yet supported by all drivers. As long as

		 * obj->state is directly dereferenced anywhere in the drivers

		 * atomic_commit_tail function, then it's unsafe to swap state

		 * before drm_atomic_helper_commit_hw_done() is called.

/**

 * drm_atomic_helper_update_plane - Helper for primary plane update using atomic

 * @plane: plane object to update

 * @crtc: owning CRTC of owning plane

 * @fb: framebuffer to flip onto plane

 * @crtc_x: x offset of primary plane on @crtc

 * @crtc_y: y offset of primary plane on @crtc

 * @crtc_w: width of primary plane rectangle on @crtc

 * @crtc_h: height of primary plane rectangle on @crtc

 * @src_x: x offset of @fb for panning

 * @src_y: y offset of @fb for panning

 * @src_w: width of source rectangle in @fb

 * @src_h: height of source rectangle in @fb

 * @ctx: lock acquire context

 *

 * Provides a default plane update handler using the atomic driver interface.

 *

 * RETURNS:

 * Zero on success, error code on failure

/**

 * drm_atomic_helper_disable_plane - Helper for primary plane disable using * atomic

 * @plane: plane to disable

 * @ctx: lock acquire context

 *

 * Provides a default plane disable handler using the atomic driver interface.

 *

 * RETURNS:

 * Zero on success, error code on failure

/**

 * drm_atomic_helper_set_config - set a new config from userspace

 * @set: mode set configuration

 * @ctx: lock acquisition context

 *

 * Provides a default CRTC set_config handler using the atomic driver interface.

 *

 * NOTE: For backwards compatibility with old userspace this automatically

 * resets the "link-status" property to GOOD, to force any link

 * re-training. The SETCRTC ioctl does not define whether an update does

 * need a full modeset or just a plane update, hence we're allowed to do

 * that. See also drm_connector_set_link_status_property().

 *

 * Returns:

 * Returns 0 on success, negative errno numbers on failure.

/**

 * drm_atomic_helper_disable_all - disable all currently active outputs

 * @dev: DRM device

 * @ctx: lock acquisition context

 *

 * Loops through all connectors, finding those that aren't turned off and then

 * turns them off by setting their DPMS mode to OFF and deactivating the CRTC

 * that they are connected to.

 *

 * This is used for example in suspend/resume to disable all currently active

 * functions when suspending. If you just want to shut down everything at e.g.

 * driver unload, look at drm_atomic_helper_shutdown().

 *

 * Note that if callers haven't already acquired all modeset locks this might

 * return -EDEADLK, which must be handled by calling drm_modeset_backoff().

 *

 * Returns:

 * 0 on success or a negative error code on failure.

 *

 * See also:

 * drm_atomic_helper_suspend(), drm_atomic_helper_resume() and

 * drm_atomic_helper_shutdown().

/**

 * drm_atomic_helper_shutdown - shutdown all CRTC

 * @dev: DRM device

 *

 * This shuts down all CRTC, which is useful for driver unloading. Shutdown on

 * suspend should instead be handled with drm_atomic_helper_suspend(), since

 * that also takes a snapshot of the modeset state to be restored on resume.

 *

 * This is just a convenience wrapper around drm_atomic_helper_disable_all(),

 * and it is the atomic version of drm_crtc_force_disable_all().

/**

 * drm_atomic_helper_duplicate_state - duplicate an atomic state object

 * @dev: DRM device

 * @ctx: lock acquisition context

 *

 * Makes a copy of the current atomic state by looping over all objects and

 * duplicating their respective states. This is used for example by suspend/

 * resume support code to save the state prior to suspend such that it can

 * be restored upon resume.

 *

 * Note that this treats atomic state as persistent between save and restore.

 * Drivers must make sure that this is possible and won't result in confusion

 * or erroneous behaviour.

 *

 * Note that if callers haven't already acquired all modeset locks this might

 * return -EDEADLK, which must be handled by calling drm_modeset_backoff().

 *

 * Returns:

 * A pointer to the copy of the atomic state object on success or an

 * ERR_PTR()-encoded error code on failure.

 *

 * See also:

 * drm_atomic_helper_suspend(), drm_atomic_helper_resume()

 clear the acquire context so that it isn't accidentally reused */

/**

 * drm_atomic_helper_suspend - subsystem-level suspend helper

 * @dev: DRM device

 *

 * Duplicates the current atomic state, disables all active outputs and then

 * returns a pointer to the original atomic state to the caller. Drivers can

 * pass this pointer to the drm_atomic_helper_resume() helper upon resume to

 * restore the output configuration that was active at the time the system

 * entered suspend.

 *

 * Note that it is potentially unsafe to use this. The atomic state object

 * returned by this function is assumed to be persistent. Drivers must ensure

 * that this holds true. Before calling this function, drivers must make sure

 * to suspend fbdev emulation so that nothing can be using the device.

 *

 * Returns:

 * A pointer to a copy of the state before suspend on success or an ERR_PTR()-

 * encoded error code on failure. Drivers should store the returned atomic

 * state object and pass it to the drm_atomic_helper_resume() helper upon

 * resume.

 *

 * See also:

 * drm_atomic_helper_duplicate_state(), drm_atomic_helper_disable_all(),

 * drm_atomic_helper_resume(), drm_atomic_helper_commit_duplicated_state()

 This can never be returned, but it makes the compiler happy */

/**

 * drm_atomic_helper_commit_duplicated_state - commit duplicated state

 * @state: duplicated atomic state to commit

 * @ctx: pointer to acquire_ctx to use for commit.

 *

 * The state returned by drm_atomic_helper_duplicate_state() and

 * drm_atomic_helper_suspend() is partially invalid, and needs to

 * be fixed up before commit.

 *

 * Returns:

 * 0 on success or a negative error code on failure.

 *

 * See also:

 * drm_atomic_helper_suspend()

/**

 * drm_atomic_helper_resume - subsystem-level resume helper

 * @dev: DRM device

 * @state: atomic state to resume to

 *

 * Calls drm_mode_config_reset() to synchronize hardware and software states,

 * grabs all modeset locks and commits the atomic state object. This can be

 * used in conjunction with the drm_atomic_helper_suspend() helper to

 * implement suspend/resume for drivers that support atomic mode-setting.

 *

 * Returns:

 * 0 on success or a negative error code on failure.

 *

 * See also:

 * drm_atomic_helper_suspend()

 Make sure we don't accidentally do a full modeset. */

/**

 * drm_atomic_helper_page_flip - execute a legacy page flip

 * @crtc: DRM CRTC

 * @fb: DRM framebuffer

 * @event: optional DRM event to signal upon completion

 * @flags: flip flags for non-vblank sync'ed updates

 * @ctx: lock acquisition context

 *

 * Provides a default &drm_crtc_funcs.page_flip implementation

 * using the atomic driver interface.

 *

 * Returns:

 * Returns 0 on success, negative errno numbers on failure.

 *

 * See also:

 * drm_atomic_helper_page_flip_target()

/**

 * drm_atomic_helper_page_flip_target - do page flip on target vblank period.

 * @crtc: DRM CRTC

 * @fb: DRM framebuffer

 * @event: optional DRM event to signal upon completion

 * @flags: flip flags for non-vblank sync'ed updates

 * @target: specifying the target vblank period when the flip to take effect

 * @ctx: lock acquisition context

 *

 * Provides a default &drm_crtc_funcs.page_flip_target implementation.

 * Similar to drm_atomic_helper_page_flip() with extra parameter to specify

 * target vblank period to flip.

 *

 * Returns:

 * Returns 0 on success, negative errno numbers on failure.

/**

 * drm_atomic_helper_bridge_propagate_bus_fmt() - Propagate output format to

 *						  the input end of a bridge

 * @bridge: bridge control structure

 * @bridge_state: new bridge state

 * @crtc_state: new CRTC state

 * @conn_state: new connector state

 * @output_fmt: tested output bus format

 * @num_input_fmts: will contain the size of the returned array

 *

 * This helper is a pluggable implementation of the

 * &drm_bridge_funcs.atomic_get_input_bus_fmts operation for bridges that don't

 * modify the bus configuration between their input and their output. It

 * returns an array of input formats with a single element set to @output_fmt.

 *

 * RETURNS:

 * a valid format array of size @num_input_fmts, or NULL if the allocation

 * failed

/*

 * Copyright Â© 2009 Keith Packard

 *

 * Permission to use, copy, modify, distribute, and sell this software and its

 * documentation for any purpose is hereby granted without fee, provided that

 * the above copyright notice appear in all copies and that both that copyright

 * notice and this permission notice appear in supporting documentation, and

 * that the name of the copyright holders not be used in advertising or

 * publicity pertaining to distribution of the software without specific,

 * written prior permission.  The copyright holders make no representations

 * about the suitability of this software for any purpose.  It is provided "as

 * is" without express or implied warranty.

 *

 * THE COPYRIGHT HOLDERS DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,

 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO

 * EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY SPECIAL, INDIRECT OR

 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE,

 * DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER

 * TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE

 * OF THIS SOFTWARE.

/**

 * DOC: dp helpers

 *

 * These functions contain some common logic and helpers at various abstraction

 * levels to deal with Display Port sink devices and related things like DP aux

 * channel transfers, EDID reading over DP aux channels, decoding certain DPCD

 * blocks, ...

 Helpers for DP link training */

 DP 2.0 128b/132b */

 Spec says link_bw = link_rate / 0.27Gbps */

 Spec says link_rate = link_bw * 0.27Gbps */

 us */

/**

 * DOC: dp helpers

 *

 * The DisplayPort AUX channel is an abstraction to allow generic, driver-

 * independent access to AUX functionality. Drivers can take advantage of

 * this by filling in the fields of the drm_dp_aux structure.

 *

 * Transactions are described using a hardware-independent drm_dp_aux_msg

 * structure, which is passed into a driver's .transfer() implementation.

 * Both native and I2C-over-AUX transactions are supported.

	/*

	 * The specification doesn't give any recommendation on how often to

	 * retry native transactions. We used to retry 7 times like for

	 * aux i2c transactions but real world devices this wasn't

	 * sufficient, bump to 32 which makes Dell 4k monitors happier.

		/*

		 * We want the error we return to be the error we received on

		 * the first transaction, since we may get a different error the

		 * next time we retry

/**

 * drm_dp_dpcd_read() - read a series of bytes from the DPCD

 * @aux: DisplayPort AUX channel (SST or MST)

 * @offset: address of the (first) register to read

 * @buffer: buffer to store the register values

 * @size: number of bytes in @buffer

 *

 * Returns the number of bytes transferred on success, or a negative error

 * code on failure. -EIO is returned if the request was NAKed by the sink or

 * if the retry count was exceeded. If not all bytes were transferred, this

 * function returns -EPROTO. Errors from the underlying AUX channel transfer

 * function, with the exception of -EBUSY (which causes the transaction to

 * be retried), are propagated to the caller.

	/*

	 * HP ZR24w corrupts the first DPCD access after entering power save

	 * mode. Eg. on a read, the entire buffer will be filled with the same

	 * byte. Do a throw away read to avoid corrupting anything we care

	 * about. Afterwards things will work correctly until the monitor

	 * gets woken up and subsequently re-enters power save mode.

	 *

	 * The user pressing any button on the monitor is enough to wake it

	 * up, so there is no particularly good place to do the workaround.

	 * We just have to do it before any DPCD access and hope that the

	 * monitor doesn't power down exactly after the throw away read.

/**

 * drm_dp_dpcd_write() - write a series of bytes to the DPCD

 * @aux: DisplayPort AUX channel (SST or MST)

 * @offset: address of the (first) register to write

 * @buffer: buffer containing the values to write

 * @size: number of bytes in @buffer

 *

 * Returns the number of bytes transferred on success, or a negative error

 * code on failure. -EIO is returned if the request was NAKed by the sink or

 * if the retry count was exceeded. If not all bytes were transferred, this

 * function returns -EPROTO. Errors from the underlying AUX channel transfer

 * function, with the exception of -EBUSY (which causes the transaction to

 * be retried), are propagated to the caller.

/**

 * drm_dp_dpcd_read_link_status() - read DPCD link status (bytes 0x202-0x207)

 * @aux: DisplayPort AUX channel

 * @status: buffer to store the link status in (must be at least 6 bytes)

 *

 * Returns the number of bytes transferred on success or a negative error

 * code on failure.

/**

 * drm_dp_dpcd_read_phy_link_status - get the link status information for a DP PHY

 * @aux: DisplayPort AUX channel

 * @dp_phy: the DP PHY to get the link status for

 * @link_status: buffer to return the status in

 *

 * Fetch the AUX DPCD registers for the DPRX or an LTTPR PHY link status. The

 * layout of the returned @link_status matches the DPCD register layout of the

 * DPRX PHY link status.

 *

 * Returns 0 if the information was read successfully or a negative error code

 * on failure.

 Convert the LTTPR to the sink PHY link status layout */

/**

 * drm_dp_downstream_is_type() - is the downstream facing port of certain type?

 * @dpcd: DisplayPort configuration data

 * @port_cap: port capabilities

 * @type: port type to be checked. Can be:

 * 	  %DP_DS_PORT_TYPE_DP, %DP_DS_PORT_TYPE_VGA, %DP_DS_PORT_TYPE_DVI,

 * 	  %DP_DS_PORT_TYPE_HDMI, %DP_DS_PORT_TYPE_NON_EDID,

 *	  %DP_DS_PORT_TYPE_DP_DUALMODE or %DP_DS_PORT_TYPE_WIRELESS.

 *

 * Caveat: Only works with DPCD 1.1+ port caps.

 *

 * Returns: whether the downstream facing port matches the type.

/**

 * drm_dp_downstream_is_tmds() - is the downstream facing port TMDS?

 * @dpcd: DisplayPort configuration data

 * @port_cap: port capabilities

 * @edid: EDID

 *

 * Returns: whether the downstream facing port is TMDS (HDMI/DVI).

/**

 * drm_dp_send_real_edid_checksum() - send back real edid checksum value

 * @aux: DisplayPort AUX channel

 * @real_edid_checksum: real edid checksum for the last block

 *

 * Returns:

 * True on success

 send back checksum for the last edid extension block data */

	/*

	 * Prior to DP1.3 the bit represented by

	 * DP_EXTENDED_RECEIVER_CAP_FIELD_PRESENT was reserved.

	 * If it is set DP_DPCD_REV at 0000h could be at a value less than

	 * the true capability of the panel. The only way to check is to

	 * then compare 0000h and 2200h.

/**

 * drm_dp_read_dpcd_caps() - read DPCD caps and extended DPCD caps if

 * available

 * @aux: DisplayPort AUX channel

 * @dpcd: Buffer to store the resulting DPCD in

 *

 * Attempts to read the base DPCD caps for @aux. Additionally, this function

 * checks for and reads the extended DPRX caps (%DP_DP13_DPCD_REV) if

 * present.

 *

 * Returns: %0 if the DPCD was read successfully, negative error code

 * otherwise.

/**

 * drm_dp_read_downstream_info() - read DPCD downstream port info if available

 * @aux: DisplayPort AUX channel

 * @dpcd: A cached copy of the port's DPCD

 * @downstream_ports: buffer to store the downstream port info in

 *

 * See also:

 * drm_dp_downstream_max_clock()

 * drm_dp_downstream_max_bpc()

 *

 * Returns: 0 if either the downstream port info was read successfully or

 * there was no downstream info to read, or a negative error code otherwise.

 No downstream info to read */

	/* Some branches advertise having 0 downstream ports, despite also advertising they have a

	 * downstream port present. The DP spec isn't clear on if this is allowed or not, but since

	 * some branches do it we need to handle it regardless.

/**

 * drm_dp_downstream_max_dotclock() - extract downstream facing port max dot clock

 * @dpcd: DisplayPort configuration data

 * @port_cap: port capabilities

 *

 * Returns: Downstream facing port max dot clock in kHz on success,

 * or 0 if max clock not defined

/**

 * drm_dp_downstream_max_tmds_clock() - extract downstream facing port max TMDS clock

 * @dpcd: DisplayPort configuration data

 * @port_cap: port capabilities

 * @edid: EDID

 *

 * Returns: HDMI/DVI downstream facing port max TMDS clock in kHz on success,

 * or 0 if max TMDS clock not defined

		/*

		 * It's left up to the driver to check the

		 * DP dual mode adapter's max TMDS clock.

		 *

		 * Unfortunately it looks like branch devices

		 * may not fordward that the DP dual mode i2c

		 * access so we just usually get i2c nak :(

		 /*

		  * We should perhaps assume 165 MHz when detailed cap

		  * info is not available. But looks like many typical

		  * branch devices fall into that category and so we'd

		  * probably end up with users complaining that they can't

		  * get high resolution modes with their favorite dongle.

		  *

		  * So let's limit to 300 MHz instead since DPCD 1.4

		  * HDMI 2.0 DFPs are required to have the detailed cap

		  * info. So it's more likely we're dealing with a HDMI 1.4

		  * compatible* device here.

 FIXME what to do about DVI dual link? */

/**

 * drm_dp_downstream_min_tmds_clock() - extract downstream facing port min TMDS clock

 * @dpcd: DisplayPort configuration data

 * @port_cap: port capabilities

 * @edid: EDID

 *

 * Returns: HDMI/DVI downstream facing port min TMDS clock in kHz on success,

 * or 0 if max TMDS clock not defined

		/*

		 * Unclear whether the protocol converter could

		 * utilize pixel replication. Assume it won't.

/**

 * drm_dp_downstream_max_bpc() - extract downstream facing port max

 *                               bits per component

 * @dpcd: DisplayPort configuration data

 * @port_cap: downstream facing port capabilities

 * @edid: EDID

 *

 * Returns: Max bpc on success or 0 if max bpc not defined

/**

 * drm_dp_downstream_420_passthrough() - determine downstream facing port

 *                                       YCbCr 4:2:0 pass-through capability

 * @dpcd: DisplayPort configuration data

 * @port_cap: downstream facing port capabilities

 *

 * Returns: whether the downstream facing port can pass through YCbCr 4:2:0

/**

 * drm_dp_downstream_444_to_420_conversion() - determine downstream facing port

 *                                             YCbCr 4:4:4->4:2:0 conversion capability

 * @dpcd: DisplayPort configuration data

 * @port_cap: downstream facing port capabilities

 *

 * Returns: whether the downstream facing port can convert YCbCr 4:4:4 to 4:2:0

/**

 * drm_dp_downstream_rgb_to_ycbcr_conversion() - determine downstream facing port

 *                                               RGB->YCbCr conversion capability

 * @dpcd: DisplayPort configuration data

 * @port_cap: downstream facing port capabilities

 * @color_spc: Colorspace for which conversion cap is sought

 *

 * Returns: whether the downstream facing port can convert RGB->YCbCr for a given

 * colorspace.

/**

 * drm_dp_downstream_mode() - return a mode for downstream facing port

 * @dev: DRM device

 * @dpcd: DisplayPort configuration data

 * @port_cap: port capabilities

 *

 * Provides a suitable mode for downstream facing ports without EDID.

 *

 * Returns: A new drm_display_mode on success or NULL on failure

/**

 * drm_dp_downstream_id() - identify branch device

 * @aux: DisplayPort AUX channel

 * @id: DisplayPort branch device id

 *

 * Returns branch device id on success or NULL on failure

/**

 * drm_dp_downstream_debug() - debug DP branch devices

 * @m: pointer for debugfs file

 * @dpcd: DisplayPort configuration data

 * @port_cap: port capabilities

 * @edid: EDID

 * @aux: DisplayPort AUX channel

 *

/**

 * drm_dp_subconnector_type() - get DP branch device type

 * @dpcd: DisplayPort configuration data

 * @port_cap: port capabilities

 DP 1.0 approach */

 Can be HDMI or DVI-D, DVI-D is a safer option */

 Can be VGA or DVI-A, VGA is more popular */

/**

 * drm_dp_set_subconnector_property - set subconnector for DP connector

 * @connector: connector to set property on

 * @status: connector status

 * @dpcd: DisplayPort configuration data

 * @port_cap: port capabilities

 *

 * Called by a driver on every detect event.

/**

 * drm_dp_read_sink_count_cap() - Check whether a given connector has a valid sink

 * count

 * @connector: The DRM connector to check

 * @dpcd: A cached copy of the connector's DPCD RX capabilities

 * @desc: A cached copy of the connector's DP descriptor

 *

 * See also: drm_dp_read_sink_count()

 *

 * Returns: %True if the (e)DP connector has a valid sink count that should

 * be probed, %false otherwise.

 Some eDP panels don't set a valid value for the sink count */

/**

 * drm_dp_read_sink_count() - Retrieve the sink count for a given sink

 * @aux: The DP AUX channel to use

 *

 * See also: drm_dp_read_sink_count_cap()

 *

 * Returns: The current sink count reported by @aux, or a negative error code

 * otherwise.

/*

 * I2C-over-AUX implementation

	/*

	 * In case of i2c defer or short i2c ack reply to a write,

	 * we need to switch to WRITE_STATUS_UPDATE to drain the

	 * rest of the message

 10 to 16 */

 preamble + AUX_SYNC_END */

/*

 * Calculate the duration of the AUX request/reply in usec. Gives the

 * "best" case estimate, ie. successful while as short as possible.

	/*

	 * For read we expect what was asked. For writes there will

	 * be 0 or 1 data bytes. Assume 0 for the "best" case.

 ADDRESS + R/W + ACK/NACK */

 DATA + ACK/NACK */

/*

 * Calculate the length of the i2c transfer in usec, assuming

 * the i2c bus speed is as specified. Gives the the "worst"

 * case estimate, ie. successful while as long as possible.

 * Doesn't account the the "MOT" bit, and instead assumes each

 * message includes a START, ADDRESS and STOP. Neither does it

 * account for additional random variables such as clock stretching.

 AUX bitrate is 1MHz, i2c bitrate as specified */

/*

 * Determine how many retries should be attempted to successfully transfer

 * the specified message, based on the estimated durations of the

 * i2c and AUX transfers.

/*

 * FIXME currently assumes 10 kHz as some real world devices seem

 * to require it. We should query/set the speed via DPCD if supported.

/*

 * Transfer a single I2C-over-AUX message and handle various error conditions,

 * retrying the transaction as appropriate.  It is assumed that the

 * &drm_dp_aux.transfer function does not modify anything in the msg other than the

 * reply field.

 *

 * Returns bytes transferred on success, or a negative error code on failure.

	/*

	 * DP1.2 sections 2.7.7.1.5.6.1 and 2.7.7.1.6.6.1: A DP Source device

	 * is required to retry at least seven times upon receiving AUX_DEFER

	 * before giving up the AUX transaction.

	 *

	 * We also try to account for the i2c bus speed.

			/*

			 * While timeouts can be errors, they're usually normal

			 * behavior (for instance, when a driver tries to

			 * communicate with a non-existent DisplayPort device).

			 * Avoid spamming the kernel log with timeout errors.

			/*

			 * For I2C-over-AUX transactions this isn't enough, we

			 * need to check for the I2C ACK reply.

			/*

			 * We could check for I2C bit rate capabilities and if

			 * available adjust this interval. We could also be

			 * more careful with DP-to-legacy adapters where a

			 * long legacy cable may force very low I2C bit rates.

			 *

			 * For now just defer for long enough to hopefully be

			 * safe for all use-cases.

			/*

			 * Both native ACK and I2C ACK replies received. We

			 * can assume the transfer was successful.

			/* DP Compliance Test 4.2.2.5 Requirement:

			 * Must have at least 7 retries for I2C defers on the

			 * transaction to pass this test

/*

 * Keep retrying drm_dp_i2c_do_msg until all data has been transferred.

 *

 * Returns an error code on failure, or a recommended transfer size on success.

/*

 * Bizlink designed DP->DVI-D Dual Link adapters require the I2C over AUX

 * packets to be as large as possible. If not, the I2C transactions never

 * succeed. Hence the default is maximum.

		/* Send a bare address packet to start the transaction.

		 * Zero sized messages specify an address only (bare

		 * address) transaction.

		/*

		 * Reset msg.request in case in case it got

		 * changed into a WRITE_STATUS_UPDATE.

		/* We want each transaction to be as large as possible, but

		 * we'll go to smaller sizes if the hardware gives us a

		 * short reply.

			/*

			 * Reset msg.request in case in case it got

			 * changed into a WRITE_STATUS_UPDATE.

	/* Send a bare address packet to close out the transaction.

	 * Zero sized messages specify an address only (bare

	 * address) transaction.

 No CRC yet */

	/*

	 * At DP_TEST_CRC_R_CR, there's 6 bytes containing CRC data, 2 bytes

	 * per component (RGB or CrYCb).

/**

 * drm_dp_remote_aux_init() - minimally initialise a remote aux channel

 * @aux: DisplayPort AUX channel

 *

 * Used for remote aux channel in general. Merely initialize the crc work

 * struct.

/**

 * drm_dp_aux_init() - minimally initialise an aux channel

 * @aux: DisplayPort AUX channel

 *

 * If you need to use the drm_dp_aux's i2c adapter prior to registering it with

 * the outside world, call drm_dp_aux_init() first. For drivers which are

 * grandparents to their AUX adapters (e.g. the AUX adapter is parented by a

 * &drm_connector), you must still call drm_dp_aux_register() once the connector

 * has been registered to allow userspace access to the auxiliary DP channel.

 * Likewise, for such drivers you should also assign &drm_dp_aux.drm_dev as

 * early as possible so that the &drm_device that corresponds to the AUX adapter

 * may be mentioned in debugging output from the DRM DP helpers.

 *

 * For devices which use a separate platform device for their AUX adapters, this

 * may be called as early as required by the driver.

 *

/**

 * drm_dp_aux_register() - initialise and register aux channel

 * @aux: DisplayPort AUX channel

 *

 * Automatically calls drm_dp_aux_init() if this hasn't been done yet. This

 * should only be called once the parent of @aux, &drm_dp_aux.dev, is

 * initialized. For devices which are grandparents of their AUX channels,

 * &drm_dp_aux.dev will typically be the &drm_connector &device which

 * corresponds to @aux. For these devices, it's advised to call

 * drm_dp_aux_register() in &drm_connector_funcs.late_register, and likewise to

 * call drm_dp_aux_unregister() in &drm_connector_funcs.early_unregister.

 * Functions which don't follow this will likely Oops when

 * %CONFIG_DRM_DP_AUX_CHARDEV is enabled.

 *

 * For devices where the AUX channel is a device that exists independently of

 * the &drm_device that uses it, such as SoCs and bridge devices, it is

 * recommended to call drm_dp_aux_register() after a &drm_device has been

 * assigned to &drm_dp_aux.drm_dev, and likewise to call

 * drm_dp_aux_unregister() once the &drm_device should no longer be associated

 * with the AUX channel (e.g. on bridge detach).

 *

 * Drivers which need to use the aux channel before either of the two points

 * mentioned above need to call drm_dp_aux_init() in order to use the AUX

 * channel before registration.

 *

 * Returns 0 on success or a negative error code on failure.

/**

 * drm_dp_aux_unregister() - unregister an AUX adapter

 * @aux: DisplayPort AUX channel

/**

 * drm_dp_psr_setup_time() - PSR setup in time usec

 * @psr_cap: PSR capabilities from DPCD

 *

 * Returns:

 * PSR setup time for the panel in microseconds,  negative

 * error code on failure.

/**

 * drm_dp_start_crc() - start capture of frame CRCs

 * @aux: DisplayPort AUX channel

 * @crtc: CRTC displaying the frames whose CRCs are to be captured

 *

 * Returns 0 on success or a negative error code on failure.

/**

 * drm_dp_stop_crc() - stop capture of frame CRCs

 * @aux: DisplayPort AUX channel

 *

 * Returns 0 on success or a negative error code on failure.

 Analogix 7737 needs reduced M and N at HBR2 link rates */

 LG LP140WF6-SPM1 eDP panel */

 Apple panels need some additional handling to support PSR */

 CH7511 seems to leave SINK_COUNT zeroed */

 Synaptics DP1.4 MST hubs can support DSC without virtual DPCD */

 Apple MacBookPro 2017 15 inch eDP Retina panel reports too low DP_MAX_LINK_RATE */

/*

 * Get a bit mask of DPCD quirks for the sink/branch device identified by

 * ident. The quirk data is shared but it's up to the drivers to act on the

 * data.

 *

 * For now, only the OUI (first three bytes) is used, but this may be extended

 * to device identification string and hardware/firmware revisions later.

/**

 * drm_dp_read_desc - read sink/branch descriptor from DPCD

 * @aux: DisplayPort AUX channel

 * @desc: Device descriptor to fill from DPCD

 * @is_branch: true for branch devices, false for sink devices

 *

 * Read DPCD 0x400 (sink) or 0x500 (branch) into @desc. Also debug log the

 * identification.

 *

 * Returns 0 on success or a negative error code on failure.

/**

 * drm_dp_dsc_sink_max_slice_count() - Get the max slice count

 * supported by the DSC sink.

 * @dsc_dpcd: DSC capabilities from DPCD

 * @is_edp: true if its eDP, false for DP

 *

 * Read the slice capabilities DPCD register from DSC sink to get

 * the maximum slice count supported. This is used to populate

 * the DSC parameters in the &struct drm_dsc_config by the driver.

 * Driver creates an infoframe using these parameters to populate

 * &struct drm_dsc_pps_infoframe. These are sent to the sink using DSC

 * infoframe using the helper function drm_dsc_pps_infoframe_pack()

 *

 * Returns:

 * Maximum slice count supported by DSC sink or 0 its invalid

 For eDP, register DSC_SLICE_CAPABILITIES_1 gives slice count */

 For DP, use values from DSC_SLICE_CAP_1 and DSC_SLICE_CAP2 */

/**

 * drm_dp_dsc_sink_line_buf_depth() - Get the line buffer depth in bits

 * @dsc_dpcd: DSC capabilities from DPCD

 *

 * Read the DSC DPCD register to parse the line buffer depth in bits which is

 * number of bits of precision within the decoder line buffer supported by

 * the DSC sink. This is used to populate the DSC parameters in the

 * &struct drm_dsc_config by the driver.

 * Driver creates an infoframe using these parameters to populate

 * &struct drm_dsc_pps_infoframe. These are sent to the sink using DSC

 * infoframe using the helper function drm_dsc_pps_infoframe_pack()

 *

 * Returns:

 * Line buffer depth supported by DSC panel or 0 its invalid

/**

 * drm_dp_dsc_sink_supported_input_bpcs() - Get all the input bits per component

 * values supported by the DSC sink.

 * @dsc_dpcd: DSC capabilities from DPCD

 * @dsc_bpc: An array to be filled by this helper with supported

 *           input bpcs.

 *

 * Read the DSC DPCD from the sink device to parse the supported bits per

 * component values. This is used to populate the DSC parameters

 * in the &struct drm_dsc_config by the driver.

 * Driver creates an infoframe using these parameters to populate

 * &struct drm_dsc_pps_infoframe. These are sent to the sink using DSC

 * infoframe using the helper function drm_dsc_pps_infoframe_pack()

 *

 * Returns:

 * Number of input BPC values parsed from the DPCD

/**

 * drm_dp_read_lttpr_common_caps - read the LTTPR common capabilities

 * @aux: DisplayPort AUX channel

 * @caps: buffer to return the capability info in

 *

 * Read capabilities common to all LTTPRs.

 *

 * Returns 0 on success or a negative error code on failure.

/**

 * drm_dp_read_lttpr_phy_caps - read the capabilities for a given LTTPR PHY

 * @aux: DisplayPort AUX channel

 * @dp_phy: LTTPR PHY to read the capabilities for

 * @caps: buffer to return the capability info in

 *

 * Read the capabilities for the given LTTPR PHY.

 *

 * Returns 0 on success or a negative error code on failure.

/**

 * drm_dp_lttpr_count - get the number of detected LTTPRs

 * @caps: LTTPR common capabilities

 *

 * Get the number of detected LTTPRs from the LTTPR common capabilities info.

 *

 * Returns:

 *   -ERANGE if more than supported number (8) of LTTPRs are detected

 *   -EINVAL if the DP_PHY_REPEATER_CNT register contains an invalid value

 *   otherwise the number of detected LTTPRs

/**

 * drm_dp_lttpr_max_link_rate - get the maximum link rate supported by all LTTPRs

 * @caps: LTTPR common capabilities

 *

 * Returns the maximum link rate supported by all detected LTTPRs.

/**

 * drm_dp_lttpr_max_lane_count - get the maximum lane count supported by all LTTPRs

 * @caps: LTTPR common capabilities

 *

 * Returns the maximum lane count supported by all detected LTTPRs.

/**

 * drm_dp_lttpr_voltage_swing_level_3_supported - check for LTTPR vswing3 support

 * @caps: LTTPR PHY capabilities

 *

 * Returns true if the @caps for an LTTPR TX PHY indicate support for

 * voltage swing level 3.

/**

 * drm_dp_lttpr_pre_emphasis_level_3_supported - check for LTTPR preemph3 support

 * @caps: LTTPR PHY capabilities

 *

 * Returns true if the @caps for an LTTPR TX PHY indicate support for

 * pre-emphasis level 3.

/**

 * drm_dp_get_phy_test_pattern() - get the requested pattern from the sink.

 * @aux: DisplayPort AUX channel

 * @data: DP phy compliance test parameters.

 *

 * Returns 0 on success or a negative error code on failure.

/**

 * drm_dp_set_phy_test_pattern() - set the pattern to the sink.

 * @aux: DisplayPort AUX channel

 * @data: DP phy compliance test parameters.

 * @dp_rev: DP revision to use for compliance testing

 *

 * Returns 0 on success or a negative error code on failure.

 and DP_COLORIMETRY_BT709_YCC */

 and DP_COLORIMETRY_XVYCC_601 */

 and DP_COLORIMETRY_XVYCC_709 */

 and DP_COLORIMETRY_SYCC_601 */

 and DP_COLORIMETRY_OPYCC_601 */

 and DP_COLORIMETRY_BT2020_CYCC */

/**

 * drm_dp_get_pcon_max_frl_bw() - maximum frl supported by PCON

 * @dpcd: DisplayPort configuration data

 * @port_cap: port capabilities

 *

 * Returns maximum frl bandwidth supported by PCON in GBPS,

 * returns 0 if not supported.

/**

 * drm_dp_pcon_frl_prepare() - Prepare PCON for FRL.

 * @aux: DisplayPort AUX channel

 * @enable_frl_ready_hpd: Configure DP_PCON_ENABLE_HPD_READY.

 *

 * Returns 0 if success, else returns negative error code.

/**

 * drm_dp_pcon_is_frl_ready() - Is PCON ready for FRL

 * @aux: DisplayPort AUX channel

 *

 * Returns true if success, else returns false.

/**

 * drm_dp_pcon_frl_configure_1() - Set HDMI LINK Configuration-Step1

 * @aux: DisplayPort AUX channel

 * @max_frl_gbps: maximum frl bw to be configured between PCON and HDMI sink

 * @frl_mode: FRL Training mode, it can be either Concurrent or Sequential.

 * In Concurrent Mode, the FRL link bring up can be done along with

 * DP Link training. In Sequential mode, the FRL link bring up is done prior to

 * the DP Link training.

 *

 * Returns 0 if success, else returns negative error code.

/**

 * drm_dp_pcon_frl_configure_2() - Set HDMI Link configuration Step-2

 * @aux: DisplayPort AUX channel

 * @max_frl_mask : Max FRL BW to be tried by the PCON with HDMI Sink

 * @frl_type : FRL training type, can be Extended, or Normal.

 * In Normal FRL training, the PCON tries each frl bw from the max_frl_mask

 * starting from min, and stops when link training is successful. In Extended

 * FRL training, all frl bw selected in the mask are trained by the PCON.

 *

 * Returns 0 if success, else returns negative error code.

/**

 * drm_dp_pcon_reset_frl_config() - Re-Set HDMI Link configuration.

 * @aux: DisplayPort AUX channel

 *

 * Returns 0 if success, else returns negative error code.

/**

 * drm_dp_pcon_frl_enable() - Enable HDMI link through FRL

 * @aux: DisplayPort AUX channel

 *

 * Returns 0 if success, else returns negative error code.

/**

 * drm_dp_pcon_hdmi_link_active() - check if the PCON HDMI LINK status is active.

 * @aux: DisplayPort AUX channel

 *

 * Returns true if link is active else returns false.

/**

 * drm_dp_pcon_hdmi_link_mode() - get the PCON HDMI LINK MODE

 * @aux: DisplayPort AUX channel

 * @frl_trained_mask: pointer to store bitmask of the trained bw configuration.

 * Valid only if the MODE returned is FRL. For Normal Link training mode

 * only 1 of the bits will be set, but in case of Extended mode, more than

 * one bits can be set.

 *

 * Returns the link mode : TMDS or FRL on success, else returns negative error

 * code.

/**

 * drm_dp_pcon_hdmi_frl_link_error_count() - print the error count per lane

 * during link failure between PCON and HDMI sink

 * @aux: DisplayPort AUX channel

 * @connector: DRM connector

 * code.

/*

 * drm_dp_pcon_enc_is_dsc_1_2 - Does PCON Encoder supports DSC 1.2

 * @pcon_dsc_dpcd: DSC capabilities of the PCON DSC Encoder

 *

 * Returns true is PCON encoder is DSC 1.2 else returns false.

/*

 * drm_dp_pcon_dsc_max_slices - Get max slices supported by PCON DSC Encoder

 * @pcon_dsc_dpcd: DSC capabilities of the PCON DSC Encoder

 *

 * Returns maximum no. of slices supported by the PCON DSC Encoder.

/*

 * drm_dp_pcon_dsc_max_slice_width() - Get max slice width for Pcon DSC encoder

 * @pcon_dsc_dpcd: DSC capabilities of the PCON DSC Encoder

 *

 * Returns maximum width of the slices in pixel width i.e. no. of pixels x 320.

/*

 * drm_dp_pcon_dsc_bpp_incr() - Get bits per pixel increment for PCON DSC encoder

 * @pcon_dsc_dpcd: DSC capabilities of the PCON DSC Encoder

 *

 * Returns the bpp precision supported by the PCON encoder.

/**

 * drm_dp_pcon_pps_default() - Let PCON fill the default pps parameters

 * for DSC1.2 between PCON & HDMI2.1 sink

 * @aux: DisplayPort AUX channel

 *

 * Returns 0 on success, else returns negative error code.

/**

 * drm_dp_pcon_pps_override_buf() - Configure PPS encoder override buffer for

 * HDMI sink

 * @aux: DisplayPort AUX channel

 * @pps_buf: 128 bytes to be written into PPS buffer for HDMI sink by PCON.

 *

 * Returns 0 on success, else returns negative error code.

/*

 * drm_dp_pcon_pps_override_param() - Write PPS parameters to DSC encoder

 * override registers

 * @aux: DisplayPort AUX channel

 * @pps_param: 3 Parameters (2 Bytes each) : Slice Width, Slice Height,

 * bits_per_pixel.

 *

 * Returns 0 on success, else returns negative error code.

/*

 * drm_dp_pcon_convert_rgb_to_ycbcr() - Configure the PCon to convert RGB to Ycbcr

 * @aux: displayPort AUX channel

 * @color_spc: Color-space/s for which conversion is to be enabled, 0 for disable.

 *

 * Returns 0 on success, else returns negative error code.

/**

 * drm_edp_backlight_set_level() - Set the backlight level of an eDP panel via AUX

 * @aux: The DP AUX channel to use

 * @bl: Backlight capability info from drm_edp_backlight_init()

 * @level: The brightness level to set

 *

 * Sets the brightness level of an eDP panel's backlight. Note that the panel's backlight must

 * already have been enabled by the driver by calling drm_edp_backlight_enable().

 *

 * Returns: %0 on success, negative error code on failure

 The panel uses something other then DPCD for enabling its backlight */

/**

 * drm_edp_backlight_enable() - Enable an eDP panel's backlight using DPCD

 * @aux: The DP AUX channel to use

 * @bl: Backlight capability info from drm_edp_backlight_init()

 * @level: The initial backlight level to set via AUX, if there is one

 *

 * This function handles enabling DPCD backlight controls on a panel over DPCD, while additionally

 * restoring any important backlight state such as the given backlight level, the brightness byte

 * count, backlight frequency, etc.

 *

 * Note that certain panels, while supporting brightness level controls over DPCD, may not support

 * having their backlights enabled via the standard %DP_EDP_DISPLAY_CONTROL_REGISTER. On such panels

 * &drm_edp_backlight_info.aux_enable will be set to %false, this function will skip the step of

 * programming the %DP_EDP_DISPLAY_CONTROL_REGISTER, and the driver must perform the required

 * implementation specific step for enabling the backlight after calling this function.

 *

 * Returns: %0 on success, negative error code on failure.

/**

 * drm_edp_backlight_disable() - Disable an eDP backlight using DPCD, if supported

 * @aux: The DP AUX channel to use

 * @bl: Backlight capability info from drm_edp_backlight_init()

 *

 * This function handles disabling DPCD backlight controls on a panel over AUX. Note that some

 * panels have backlights that are enabled/disabled by other means, despite having their brightness

 * values controlled through DPCD. On such panels &drm_edp_backlight_info.aux_enable will be set to

 * %false, this function will become a no-op (and we will skip updating

 * %DP_EDP_DISPLAY_CONTROL_REGISTER), and the driver must take care to perform it's own

 * implementation specific step for disabling the backlight.

 *

 * Returns: %0 on success or no-op, negative error code on failure.

	/*

	 * Set PWM Frequency divider to match desired frequency provided by the driver.

	 * The PWM Frequency is calculated as 27Mhz / (F x P).

	 * - Where F = PWM Frequency Pre-Divider value programmed by field 7:0 of the

	 *             EDP_BACKLIGHT_FREQ_SET register (DPCD Address 00728h)

	 * - Where P = 2^Pn, where Pn is the value programmed by field 4:0 of the

	 *             EDP_PWMGEN_BIT_COUNT register (DPCD Address 00724h)

	/* Find desired value of (F x P)

	 * Note that, if F x P is out of supported range, the maximum value or minimum value will

	 * applied automatically. So no need to check that.

	/* Use highest possible value of Pn for more granularity of brightness adjustment while

	 * satisfying the conditions below.

	 * - Pn is in the range of Pn_min and Pn_max

	 * - F is in the range of 1 and 255

	 * - FxP is within 25% of desired value.

	 *   Note: 25% is arbitrary value and may need some tweak.

 Ensure frequency is within 25% of desired value */

	/*

	 * If we're not in DPCD control mode yet, the programmed brightness value is meaningless and

	 * the driver should assume max brightness

/**

 * drm_edp_backlight_init() - Probe a display panel's TCON using the standard VESA eDP backlight

 * interface.

 * @aux: The DP aux device to use for probing

 * @bl: The &drm_edp_backlight_info struct to fill out with information on the backlight

 * @driver_pwm_freq_hz: Optional PWM frequency from the driver in hz

 * @edp_dpcd: A cached copy of the eDP DPCD

 * @current_level: Where to store the probed brightness level

 * @current_mode: Where to store the currently set backlight control mode

 *

 * Initializes a &drm_edp_backlight_info struct by probing @aux for it's backlight capabilities,

 * along with also probing the current and maximum supported brightness levels.

 *

 * If @driver_pwm_freq_hz is non-zero, this will be used as the backlight frequency. Otherwise, the

 * default frequency from the panel is used.

 *

 * Returns: %0 on success, negative error code on failure.

/**

 * drm_panel_dp_aux_backlight - create and use DP AUX backlight

 * @panel: DRM panel

 * @aux: The DP AUX channel to use

 *

 * Use this function to create and handle backlight if your panel

 * supports backlight control over DP AUX channel using DPCD

 * registers as per VESA's standard backlight control interface.

 *

 * When the panel is enabled backlight will be enabled after a

 * successful call to &drm_panel_funcs.enable()

 *

 * When the panel is disabled backlight will be disabled before the

 * call to &drm_panel_funcs.disable().

 *

 * A typical implementation for a panel driver supporting backlight

 * control over DP AUX will call this function at probe time.

 * Backlight will then be handled transparently without requiring

 * any intervention from the driver.

 *

 * drm_panel_dp_aux_backlight() must be called after the call to drm_panel_init().

 *

 * Return: 0 on success or a negative error code on failure.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * drm kms/fb cma (contiguous memory allocator) helper functions

 *

 * Copyright (C) 2012 Analog Devices Inc.

 *   Author: Lars-Peter Clausen <lars@metafoo.de>

 *

 * Based on udl_fbdev.c

 *  Copyright (C) 2012 Red Hat

/**

 * DOC: framebuffer cma helper functions

 *

 * Provides helper functions for creating a cma (contiguous memory allocator)

 * backed framebuffer.

 *

 * drm_gem_fb_create() is used in the &drm_mode_config_funcs.fb_create

 * callback function to create a cma backed framebuffer.

/**

 * drm_fb_cma_get_gem_obj() - Get CMA GEM object for framebuffer

 * @fb: The framebuffer

 * @plane: Which plane

 *

 * Return the CMA GEM object for given framebuffer.

 *

 * This function will usually be called from the CRTC callback functions.

/**

 * drm_fb_cma_get_gem_addr() - Get physical address for framebuffer, for pixel

 * formats where values are grouped in blocks this will get you the beginning of

 * the block

 * @fb: The framebuffer

 * @state: Which state of drm plane

 * @plane: Which plane

 * Return the CMA GEM address for given framebuffer.

 *

 * This function will usually be called from the PLANE callback functions.

/**

 * drm_fb_cma_sync_non_coherent - Sync GEM object to non-coherent backing

 *	memory

 * @drm: DRM device

 * @old_state: Old plane state

 * @state: New plane state

 *

 * This function can be used by drivers that use damage clips and have

 * CMA GEM objects backed by non-coherent memory. Calling this function

 * in a plane's .atomic_update ensures that all the data in the backing

 * memory have been written to RAM.

 Ignore x1/x2 values, invalidate complete lines */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * drm_sysfs.c - Modifications to drm_sysfs_class.c to support

 *               extra sysfs attribute from DRM. Normal drm_sysfs_class

 *               does not allow adding attributes.

 *

 * Copyright (c) 2004 Jon Smirl <jonsmirl@gmail.com>

 * Copyright (c) 2003-2004 Greg Kroah-Hartman <greg@kroah.com>

 * Copyright (c) 2003-2004 IBM Corp.

/**

 * DOC: overview

 *

 * DRM provides very little additional support to drivers for sysfs

 * interactions, beyond just all the standard stuff. Drivers who want to expose

 * additional sysfs properties and property groups can attach them at either

 * &drm_device.dev or &drm_connector.kdev.

 *

 * Registration is automatically handled when calling drm_dev_register(), or

 * drm_connector_register() in case of hot-plugged connectors. Unregistration is

 * also automatically handled by drm_dev_unregister() and

 * drm_connector_unregister().

/**

 * drm_sysfs_init - initialize sysfs helpers

 *

 * This is used to create the DRM class, which is the implicit parent of any

 * other top-level DRM sysfs objects.

 *

 * You must call drm_sysfs_destroy() to release the allocated resources.

 *

 * Return: 0 on success, negative error code on failure.

/**

 * drm_sysfs_destroy - destroys DRM class

 *

 * Destroy the DRM device class.

/*

 * Connector properties

/**

 * drm_sysfs_hotplug_event - generate a DRM uevent

 * @dev: DRM device

 *

 * Send a uevent for the DRM device specified by @dev.  Currently we only

 * set HOTPLUG=1 in the uevent environment, but this could be expanded to

 * deal with other types of events.

 *

 * Any new uapi should be using the drm_sysfs_connector_status_event()

 * for uevents on connector status change.

/**

 * drm_sysfs_connector_status_event - generate a DRM uevent for connector

 * property status change

 * @connector: connector on which property status changed

 * @property: connector property whose status changed.

 *

 * Send a uevent for the DRM device specified by @dev.  Currently we

 * set HOTPLUG=1 and connector id along with the attached property id

 * related to the status change.

/**

 * drm_class_device_register - register new device with the DRM sysfs class

 * @dev: device to register

 *

 * Registers a new &struct device within the DRM sysfs class. Essentially only

 * used by ttm to have a place for its global settings. Drivers should never use

 * this.

/**

 * drm_class_device_unregister - unregister device with the DRM sysfs class

 * @dev: device to unregister

 *

 * Unregisters a &struct device from the DRM sysfs class. Essentially only used

 * by ttm to have a place for its global settings. Drivers should never use

 * this.

/*

 * Copyright (c) 2016 Laurent Pinchart <laurent.pinchart@ideasonboard.com>

 *

 * DRM core format related functions

 *

 * Permission to use, copy, modify, distribute, and sell this software and its

 * documentation for any purpose is hereby granted without fee, provided that

 * the above copyright notice appear in all copies and that both that copyright

 * notice and this permission notice appear in supporting documentation, and

 * that the name of the copyright holders not be used in advertising or

 * publicity pertaining to distribution of the software without specific,

 * written prior permission.  The copyright holders make no representations

 * about the suitability of this software for any purpose.  It is provided "as

 * is" without express or implied warranty.

 *

 * THE COPYRIGHT HOLDERS DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,

 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO

 * EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY SPECIAL, INDIRECT OR

 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE,

 * DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER

 * TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE

 * OF THIS SOFTWARE.

/**

 * drm_mode_legacy_fb_format - compute drm fourcc code from legacy description

 * @bpp: bits per pixels

 * @depth: bit depth per pixel

 *

 * Computes a drm fourcc pixel format code for the given @bpp/@depth values.

 * Useful in fbdev emulation code, since that deals in those values.

/**

 * drm_driver_legacy_fb_format - compute drm fourcc code from legacy description

 * @dev: DRM device

 * @bpp: bits per pixels

 * @depth: bit depth per pixel

 *

 * Computes a drm fourcc pixel format code for the given @bpp/@depth values.

 * Unlike drm_mode_legacy_fb_format() this looks at the drivers mode_config,

 * and depending on the &drm_mode_config.quirk_addfb_prefer_host_byte_order flag

 * it returns little endian byte order or host byte order framebuffer formats.

/*

 * Internal function to query information for a given format. See

 * drm_format_info() for the public API.

/**

 * drm_format_info - query information for a given format

 * @format: pixel format (DRM_FORMAT_*)

 *

 * The caller should only pass a supported pixel format to this function.

 * Unsupported pixel formats will generate a warning in the kernel log.

 *

 * Returns:

 * The instance of struct drm_format_info that describes the pixel format, or

 * NULL if the format is unsupported.

/**

 * drm_get_format_info - query information for a given framebuffer configuration

 * @dev: DRM device

 * @mode_cmd: metadata from the userspace fb creation request

 *

 * Returns:

 * The instance of struct drm_format_info that describes the pixel format, or

 * NULL if the format is unsupported.

/**

 * drm_format_info_block_width - width in pixels of block.

 * @info: pixel format info

 * @plane: plane index

 *

 * Returns:

 * The width in pixels of a block, depending on the plane index.

/**

 * drm_format_info_block_height - height in pixels of a block

 * @info: pixel format info

 * @plane: plane index

 *

 * Returns:

 * The height in pixels of a block, depending on the plane index.

/**

 * drm_format_info_min_pitch - computes the minimum required pitch in bytes

 * @info: pixel format info

 * @plane: plane index

 * @buffer_width: buffer width in pixels

 *

 * Returns:

 * The minimum required pitch in bytes for a buffer by taking into consideration

 * the pixel format information and the buffer width.

/*

 * Copyright Â© 2015 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Rafael Antognolli <rafael.antognolli@intel.com>

 *

 Backward compatibility for drm_kms_helper.edid_firmware */

 Call exit functions from specific kms helpers here */

/*

 * Copyright (C) 2013, NVIDIA Corporation.  All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sub license,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/**

 * DOC: drm panel

 *

 * The DRM panel helpers allow drivers to register panel objects with a

 * central registry and provide functions to retrieve those panels in display

 * drivers.

 *

 * For easy integration into drivers using the &drm_bridge infrastructure please

 * take look at drm_panel_bridge_add() and devm_drm_panel_bridge_add().

/**

 * drm_panel_init - initialize a panel

 * @panel: DRM panel

 * @dev: parent device of the panel

 * @funcs: panel operations

 * @connector_type: the connector type (DRM_MODE_CONNECTOR_*) corresponding to

 *	the panel interface

 *

 * Initialize the panel structure for subsequent registration with

 * drm_panel_add().

/**

 * drm_panel_add - add a panel to the global registry

 * @panel: panel to add

 *

 * Add a panel to the global registry so that it can be looked up by display

 * drivers.

/**

 * drm_panel_remove - remove a panel from the global registry

 * @panel: DRM panel

 *

 * Removes a panel from the global registry.

/**

 * drm_panel_prepare - power on a panel

 * @panel: DRM panel

 *

 * Calling this function will enable power and deassert any reset signals to

 * the panel. After this has completed it is possible to communicate with any

 * integrated circuitry via a command bus.

 *

 * Return: 0 on success or a negative error code on failure.

/**

 * drm_panel_unprepare - power off a panel

 * @panel: DRM panel

 *

 * Calling this function will completely power off a panel (assert the panel's

 * reset, turn off power supplies, ...). After this function has completed, it

 * is usually no longer possible to communicate with the panel until another

 * call to drm_panel_prepare().

 *

 * Return: 0 on success or a negative error code on failure.

/**

 * drm_panel_enable - enable a panel

 * @panel: DRM panel

 *

 * Calling this function will cause the panel display drivers to be turned on

 * and the backlight to be enabled. Content will be visible on screen after

 * this call completes.

 *

 * Return: 0 on success or a negative error code on failure.

/**

 * drm_panel_disable - disable a panel

 * @panel: DRM panel

 *

 * This will typically turn off the panel's backlight or disable the display

 * drivers. For smart panels it should still be possible to communicate with

 * the integrated circuitry via any command bus after this call.

 *

 * Return: 0 on success or a negative error code on failure.

/**

 * drm_panel_get_modes - probe the available display modes of a panel

 * @panel: DRM panel

 * @connector: DRM connector

 *

 * The modes probed from the panel are automatically added to the connector

 * that the panel is attached to.

 *

 * Return: The number of modes available from the panel on success or a

 * negative error code on failure.

/**

 * of_drm_find_panel - look up a panel using a device tree node

 * @np: device tree node of the panel

 *

 * Searches the set of registered panels for one that matches the given device

 * tree node. If a matching panel is found, return a pointer to it.

 *

 * Return: A pointer to the panel registered for the specified device tree

 * node or an ERR_PTR() if no panel matching the device tree node can be found.

 *

 * Possible error codes returned by this function:

 *

 * - EPROBE_DEFER: the panel device has not been probed yet, and the caller

 *   should retry later

 * - ENODEV: the device is not available (status != "okay" or "ok")

/**

 * of_drm_get_panel_orientation - look up the orientation of the panel through

 * the "rotation" binding from a device tree node

 * @np: device tree node of the panel

 * @orientation: orientation enum to be filled in

 *

 * Looks up the rotation of a panel in the device tree. The orientation of the

 * panel is expressed as a property name "rotation" in the device tree. The

 * rotation in the device tree is counter clockwise.

 *

 * Return: 0 when a valid rotation value (0, 90, 180, or 270) is read or the

 * rotation property doesn't exist. Return a negative error code on failure.

 Don't return an error if there's no rotation property. */

/**

 * drm_panel_of_backlight - use backlight device node for backlight

 * @panel: DRM panel

 *

 * Use this function to enable backlight handling if your panel

 * uses device tree and has a backlight phandle.

 *

 * When the panel is enabled backlight will be enabled after a

 * successful call to &drm_panel_funcs.enable()

 *

 * When the panel is disabled backlight will be disabled before the

 * call to &drm_panel_funcs.disable().

 *

 * A typical implementation for a panel driver supporting device tree

 * will call this function at probe time. Backlight will then be handled

 * transparently without requiring any intervention from the driver.

 * drm_panel_of_backlight() must be called after the call to drm_panel_init().

 *

 * Return: 0 on success or a negative error code on failure.

/*

 * Copyright (c) 2006-2009 Red Hat Inc.

 * Copyright (c) 2006-2008 Intel Corporation

 * Copyright (c) 2007 Dave Airlie <airlied@linux.ie>

 *

 * DRM framebuffer helper functions

 *

 * Permission to use, copy, modify, distribute, and sell this software and its

 * documentation for any purpose is hereby granted without fee, provided that

 * the above copyright notice appear in all copies and that both that copyright

 * notice and this permission notice appear in supporting documentation, and

 * that the name of the copyright holders not be used in advertising or

 * publicity pertaining to distribution of the software without specific,

 * written prior permission.  The copyright holders make no representations

 * about the suitability of this software for any purpose.  It is provided "as

 * is" without express or implied warranty.

 *

 * THE COPYRIGHT HOLDERS DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,

 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO

 * EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY SPECIAL, INDIRECT OR

 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE,

 * DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER

 * TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE

 * OF THIS SOFTWARE.

 *

 * Authors:

 *      Dave Airlie <airlied@linux.ie>

 *      Jesse Barnes <jesse.barnes@intel.com>

/*

 * In order to keep user-space compatibility, we want in certain use-cases

 * to keep leaking the fbdev physical address to the user-space program

 * handling the fbdev buffer.

 * This is a bad habit essentially kept into closed source opengl driver

 * that should really be moved into open-source upstream projects instead

 * of using legacy physical addresses in user space to communicate with

 * other out-of-tree kernel modules.

 *

 * This module_param *should* be removed as soon as possible and be

 * considered as a broken and legacy behaviour from a modern fbdev device.

/**

 * DOC: fbdev helpers

 *

 * The fb helper functions are useful to provide an fbdev on top of a drm kernel

 * mode setting driver. They can be used mostly independently from the crtc

 * helper functions used by many drivers to implement the kernel mode setting

 * interfaces.

 *

 * Drivers that support a dumb buffer with a virtual address and mmap support,

 * should try out the generic fbdev emulation using drm_fbdev_generic_setup().

 * It will automatically set up deferred I/O if the driver requires a shadow

 * buffer.

 *

 * At runtime drivers should restore the fbdev console by using

 * drm_fb_helper_lastclose() as their &drm_driver.lastclose callback.

 * They should also notify the fb helper code from updates to the output

 * configuration by using drm_fb_helper_output_poll_changed() as their

 * &drm_mode_config_funcs.output_poll_changed callback.

 *

 * For suspend/resume consider using drm_mode_config_helper_suspend() and

 * drm_mode_config_helper_resume() which takes care of fbdev as well.

 *

 * All other functions exported by the fb helper library can be used to

 * implement the fbdev driver interface by the driver.

 *

 * It is possible, though perhaps somewhat tricky, to implement race-free

 * hotplug detection using the fbdev helpers. The drm_fb_helper_prepare()

 * helper must be called first to initialize the minimum required to make

 * hotplug detection work. Drivers also need to make sure to properly set up

 * the &drm_mode_config.funcs member. After calling drm_kms_helper_poll_init()

 * it is safe to enable interrupts and start processing hotplug events. At the

 * same time, drivers should initialize all modeset objects such as CRTCs,

 * encoders and connectors. To finish up the fbdev helper initialization, the

 * drm_fb_helper_init() function is called. To probe for all attached displays

 * and set up an initial configuration using the detected hardware, drivers

 * should call drm_fb_helper_initial_config().

 *

 * If &drm_framebuffer_funcs.dirty is set, the

 * drm_fb_helper_{cfb,sys}_{write,fillrect,copyarea,imageblit} functions will

 * accumulate changes and schedule &drm_fb_helper.dirty_work to run right

 * away. This worker then calls the dirty() function ensuring that it will

 * always run in process context since the fb_*() function could be running in

 * atomic context. If drm_fb_helper_deferred_io() is used as the deferred_io

 * callback it will also schedule dirty_work with the damage collected from the

 * mmap page writes.

 *

 * Deferred I/O is not compatible with SHMEM. Such drivers should request an

 * fbdev shadow buffer and call drm_fbdev_generic_setup() instead.

/**

 * drm_fb_helper_debug_enter - implementation for &fb_ops.fb_debug_enter

 * @info: fbdev registered by the helper

/**

 * drm_fb_helper_debug_leave - implementation for &fb_ops.fb_debug_leave

 * @info: fbdev registered by the helper

		/*

		 * Yes this is the _locked version which expects the master lock

		 * to be held. But for forced restores we're intentionally

		 * racing here, see drm_fb_helper_set_par().

/**

 * drm_fb_helper_restore_fbdev_mode_unlocked - restore fbdev configuration

 * @fb_helper: driver-allocated fbdev helper, can be NULL

 *

 * This should be called from driver's drm &drm_driver.lastclose callback

 * when implementing an fbcon on top of kms using this helper. This ensures that

 * the user isn't greeted with a black screen when e.g. X dies.

 *

 * RETURNS:

 * Zero if everything went ok, negative error code otherwise.

 emergency restore, don't bother with error reporting */

/**

 * drm_fb_helper_blank - implementation for &fb_ops.fb_blank

 * @blank: desired blanking state

 * @info: fbdev registered by the helper

 Display: On; HSync: On, VSync: On */

 Display: Off; HSync: On, VSync: On */

 Display: Off; HSync: Off, VSync: On */

 Display: Off; HSync: On, VSync: Off */

 Display: Off; HSync: Off, VSync: Off */

 go to first pixel within clip rect */

	/*

	 * We have to pin the client buffer to its current location while

	 * flushing the shadow buffer. In the general case, concurrent

	 * modesetting operations could try to move the buffer and would

	 * fail. The modeset has to be serialized by acquiring the reservation

	 * object of the underlying BO here.

	 *

	 * For fbdev emulation, we only have to protect against fbdev modeset

	 * operations. Nothing else will involve the client buffer's BO. So it

	 * is sufficient to acquire struct drm_fb_helper.lock here.

 Call damage handlers only if necessary */

	/*

	 * Restore damage clip rectangle on errors. The next run

	 * of the damage worker will perform the update.

/**

 * drm_fb_helper_prepare - setup a drm_fb_helper structure

 * @dev: DRM device

 * @helper: driver-allocated fbdev helper structure to set up

 * @funcs: pointer to structure of functions associate with this helper

 *

 * Sets up the bare minimum to make the framebuffer helper usable. This is

 * useful to implement race-free initialization of the polling helpers.

/**

 * drm_fb_helper_init - initialize a &struct drm_fb_helper

 * @dev: drm device

 * @fb_helper: driver-allocated fbdev helper structure to initialize

 *

 * This allocates the structures for the fbdev helper with the given limits.

 * Note that this won't yet touch the hardware (through the driver interfaces)

 * nor register the fbdev. This is only done in drm_fb_helper_initial_config()

 * to allow driver writes more control over the exact init sequence.

 *

 * Drivers must call drm_fb_helper_prepare() before calling this function.

 *

 * RETURNS:

 * Zero if everything went ok, nonzero otherwise.

	/*

	 * If this is not the generic fbdev client, initialize a drm_client

	 * without callbacks so we can use the modesets.

/**

 * drm_fb_helper_alloc_fbi - allocate fb_info and some of its members

 * @fb_helper: driver-allocated fbdev helper

 *

 * A helper to alloc fb_info and the members cmap and apertures. Called

 * by the driver within the fb_probe fb_helper callback function. Drivers do not

 * need to release the allocated fb_info structure themselves, this is

 * automatically done when calling drm_fb_helper_fini().

 *

 * RETURNS:

 * fb_info pointer if things went okay, pointer containing error code

 * otherwise

	/*

	 * TODO: We really should be smarter here and alloc an aperture

	 * for each IORESOURCE_MEM resource helper->dev->dev has and also

	 * init the ranges of the appertures based on the resources.

	 * Note some drivers currently count on there being only 1 empty

	 * aperture and fill this themselves, these will need to be dealt

	 * with somehow when fixing this.

/**

 * drm_fb_helper_unregister_fbi - unregister fb_info framebuffer device

 * @fb_helper: driver-allocated fbdev helper, can be NULL

 *

 * A wrapper around unregister_framebuffer, to release the fb_info

 * framebuffer device. This must be called before releasing all resources for

 * @fb_helper by calling drm_fb_helper_fini().

/**

 * drm_fb_helper_fini - finialize a &struct drm_fb_helper

 * @fb_helper: driver-allocated fbdev helper, can be NULL

 *

 * This cleans up all remaining resources associated with @fb_helper.

/**

 * drm_fb_helper_deferred_io() - fbdev deferred_io callback function

 * @info: fb_info struct pointer

 * @pagelist: list of mmap framebuffer pages that have to be flushed

 *

 * This function is used as the &fb_deferred_io.deferred_io

 * callback function for flushing the fbdev mmap writes.

/**

 * drm_fb_helper_sys_read - wrapper around fb_sys_read

 * @info: fb_info struct pointer

 * @buf: userspace buffer to read from framebuffer memory

 * @count: number of bytes to read from framebuffer memory

 * @ppos: read offset within framebuffer memory

 *

 * A wrapper around fb_sys_read implemented by fbdev core

/**

 * drm_fb_helper_sys_write - wrapper around fb_sys_write

 * @info: fb_info struct pointer

 * @buf: userspace buffer to write to framebuffer memory

 * @count: number of bytes to write to framebuffer memory

 * @ppos: write offset within framebuffer memory

 *

 * A wrapper around fb_sys_write implemented by fbdev core

/**

 * drm_fb_helper_sys_fillrect - wrapper around sys_fillrect

 * @info: fbdev registered by the helper

 * @rect: info about rectangle to fill

 *

 * A wrapper around sys_fillrect implemented by fbdev core

/**

 * drm_fb_helper_sys_copyarea - wrapper around sys_copyarea

 * @info: fbdev registered by the helper

 * @area: info about area to copy

 *

 * A wrapper around sys_copyarea implemented by fbdev core

/**

 * drm_fb_helper_sys_imageblit - wrapper around sys_imageblit

 * @info: fbdev registered by the helper

 * @image: info about image to blit

 *

 * A wrapper around sys_imageblit implemented by fbdev core

/**

 * drm_fb_helper_cfb_fillrect - wrapper around cfb_fillrect

 * @info: fbdev registered by the helper

 * @rect: info about rectangle to fill

 *

 * A wrapper around cfb_fillrect implemented by fbdev core

/**

 * drm_fb_helper_cfb_copyarea - wrapper around cfb_copyarea

 * @info: fbdev registered by the helper

 * @area: info about area to copy

 *

 * A wrapper around cfb_copyarea implemented by fbdev core

/**

 * drm_fb_helper_cfb_imageblit - wrapper around cfb_imageblit

 * @info: fbdev registered by the helper

 * @image: info about image to blit

 *

 * A wrapper around cfb_imageblit implemented by fbdev core

/**

 * drm_fb_helper_set_suspend - wrapper around fb_set_suspend

 * @fb_helper: driver-allocated fbdev helper, can be NULL

 * @suspend: whether to suspend or resume

 *

 * A wrapper around fb_set_suspend implemented by fbdev core.

 * Use drm_fb_helper_set_suspend_unlocked() if you don't need to take

 * the lock yourself

/**

 * drm_fb_helper_set_suspend_unlocked - wrapper around fb_set_suspend that also

 *                                      takes the console lock

 * @fb_helper: driver-allocated fbdev helper, can be NULL

 * @suspend: whether to suspend or resume

 *

 * A wrapper around fb_set_suspend() that takes the console lock. If the lock

 * isn't available on resume, a worker is tasked with waiting for the lock

 * to become available. The console lock can be pretty contented on resume

 * due to all the printk activity.

 *

 * This function can be called multiple times with the same state since

 * &fb_info.state is checked to see if fbdev is running or not before locking.

 *

 * Use drm_fb_helper_set_suspend() if you need to take the lock yourself.

 make sure there's no pending/ongoing resume */

		/*

		 * FIXME: This always uses gamma_lut. Some HW have only

		 * degamma_lut, in which case we should reset gamma_lut and set

		 * degamma_lut. See drm_crtc_legacy_gamma_set().

/**

 * drm_fb_helper_setcmap - implementation for &fb_ops.fb_setcmap

 * @cmap: cmap to set

 * @info: fbdev registered by the helper

/**

 * drm_fb_helper_ioctl - legacy ioctl implementation

 * @info: fbdev registered by the helper

 * @cmd: ioctl command

 * @arg: ioctl argument

 *

 * A helper to implement the standard fbdev ioctl. Only

 * FBIO_WAITFORVSYNC is implemented for now.

		/*

		 * Only consider the first CRTC.

		 *

		 * This ioctl is supposed to take the CRTC number as

		 * an argument, but in fbdev times, what that number

		 * was supposed to be was quite unclear, different

		 * drivers were passing that argument differently

		 * (some by reference, some by value), and most of the

		 * userspace applications were just hardcoding 0 as an

		 * argument.

		 *

		 * The first CRTC should be the integrated panel on

		 * most drivers, so this is the best choice we can

		 * make. If we're not smart enough here, one should

		 * just consider switch the userspace to KMS.

		/*

		 * Only wait for a vblank event if the CRTC is

		 * enabled, otherwise just don't do anythintg,

		 * not even report an error.

 8bit DAC */

/**

 * drm_fb_helper_check_var - implementation for &fb_ops.fb_check_var

 * @var: screeninfo to check

 * @info: fbdev registered by the helper

	/*

	 * Changes struct fb_var_screeninfo are currently not pushed back

	 * to KMS, hence fail if different settings are requested.

	/*

	 * Workaround for SDL 1.2, which is known to be setting all pixel format

	 * fields values to zero in some cases. We treat this situation as a

	 * kind of "use some reasonable autodetected values".

	/*

	 * Likewise, bits_per_pixel should be rounded up to a supported value.

	/*

	 * drm fbdev emulation doesn't support changing the pixel format at all,

	 * so reject all pixel format changing requests.

/**

 * drm_fb_helper_set_par - implementation for &fb_ops.fb_set_par

 * @info: fbdev registered by the helper

 *

 * This will let fbcon do the mode init and is called at initialization time by

 * the fbdev core when registering the driver, and later on through the hotplug

 * callback.

	/*

	 * Normally we want to make sure that a kms master takes precedence over

	 * fbdev, to avoid fbdev flickering and occasionally stealing the

	 * display status. But Xorg first sets the vt back to text mode using

	 * the KDSET IOCTL with KD_TEXT, and only after that drops the master

	 * status when exiting.

	 *

	 * In the past this was caught by drm_fb_helper_lastclose(), but on

	 * modern systems where logind always keeps a drm fd open to orchestrate

	 * the vt switching, this doesn't work.

	 *

	 * To not break the userspace ABI we have this special case here, which

	 * is only used for the above case. Everything else uses the normal

	 * commit function, which ensures that we never steal the display from

	 * an active drm master.

/**

 * drm_fb_helper_pan_display - implementation for &fb_ops.fb_pan_display

 * @var: updated screen information

 * @info: fbdev registered by the helper

/*

 * Allocates the backing storage and sets up the fbdev info structure through

 * the ->fb_probe callback.

	/*

	 * If driver picks 8 or 16 by default use that for both depth/bpp

	 * to begin with

	/*

	 * If we run into a situation where, for example, the primary plane

	 * supports RGBA5551 (16 bpp, depth 15) but not RGB565 (16 bpp, depth

	 * 16) we need to scale down the depth of the sizes we request.

			/*

			 * Do not consider YUV or other complicated formats

			 * for framebuffers. This means only legacy formats

			 * are supported (fmt->depth is a legacy field) but

			 * the framebuffer emulation can only deal with such

			 * formats, specifically RGB/BGA formats.

 We found a perfect fit, great */

 Skip depths above what we're looking for */

 Best depth found so far */

 first up get a count of crtcs now in use and new min/maxes width/heights */

		/* in case of tile group, are we the last tile vert or horiz?

		 * If no tile group you are always the last one both vertically

		 * and horizontally

 cloning to multiple tiles is just crazy-talk, so: */

 First time: disable all crtc's.. */

 Handle our overallocation */

 push down into drivers */

 doing it in hw */

 doing it in hw */

/**

 * drm_fb_helper_fill_info - initializes fbdev information

 * @info: fbdev instance to set up

 * @fb_helper: fb helper instance to use as template

 * @sizes: describes fbdev size and scanout surface size

 *

 * Sets up the variable and fixed fbdev metainformation from the given fb helper

 * instance and the drm framebuffer allocated in &drm_fb_helper.fb.

 *

 * Drivers should call this (or their equivalent setup code) from their

 * &drm_fb_helper_funcs.fb_probe callback after having allocated the fbdev

 * backing storage framebuffer.

/*

 * This is a continuation of drm_setup_crtcs() that sets up anything related

 * to the framebuffer. During initialization, drm_setup_crtcs() is called before

 * the framebuffer has been allocated (fb_helper->fb and fb_helper->fbdev).

 * So, any setup that touches those fields needs to be done here instead of in

 * drm_setup_crtcs().

 Rotating in hardware, fbcon should not rotate */

 use first connected connector for the physical dimensions */

		/*

		 * Multiple bits are set / multiple rotations requested

		 * fbcon cannot handle separate rotation settings per

		 * output, so fallback to unrotated.

 Note: Drops fb_helper->lock before returning. */

 Shamelessly allow physical address leaking to userspace */

 don't leak any physical addresses to userspace */

	/* Need to drop locks to avoid recursive deadlock in

	 * register_framebuffer. This is ok because the only thing left to do is

/**

 * drm_fb_helper_initial_config - setup a sane initial connector configuration

 * @fb_helper: fb_helper device struct

 * @bpp_sel: bpp value to use for the framebuffer configuration

 *

 * Scans the CRTCs and connectors and tries to put together an initial setup.

 * At the moment, this is a cloned configuration across all heads with

 * a new framebuffer object as the backing store.

 *

 * Note that this also registers the fbdev and so allows userspace to call into

 * the driver through the fbdev interfaces.

 *

 * This function will call down into the &drm_fb_helper_funcs.fb_probe callback

 * to let the driver allocate and initialize the fbdev info structure and the

 * drm framebuffer used to back the fbdev. drm_fb_helper_fill_info() is provided

 * as a helper to setup simple default values for the fbdev info structure.

 *

 * HANG DEBUGGING:

 *

 * When you have fbcon support built-in or already loaded, this function will do

 * a full modeset to setup the fbdev console. Due to locking misdesign in the

 * VT/fbdev subsystem that entire modeset sequence has to be done while holding

 * console_lock. Until console_unlock is called no dmesg lines will be sent out

 * to consoles, not even serial console. This means when your driver crashes,

 * you will see absolutely nothing else but a system stuck in this function,

 * with no further output. Any kind of printk() you place within your own driver

 * or in the drm core modeset code will also never show up.

 *

 * Standard debug practice is to run the fbcon setup without taking the

 * console_lock as a hack, to be able to see backtraces and crashes on the

 * serial line. This can be done by setting the fb.lockless_register_fb=1 kernel

 * cmdline option.

 *

 * The other option is to just disable fbdev emulation since very likely the

 * first modeset from userspace will crash in the same way, and is even easier

 * to debug. This can be done by setting the drm_kms_helper.fbdev_emulation=0

 * kernel cmdline option.

 *

 * RETURNS:

 * Zero if everything went ok, nonzero otherwise.

/**

 * drm_fb_helper_hotplug_event - respond to a hotplug notification by

 *                               probing all the outputs attached to the fb

 * @fb_helper: driver-allocated fbdev helper, can be NULL

 *

 * Scan the connectors attached to the fb_helper and try to put together a

 * setup after notification of a change in output configuration.

 *

 * Called at runtime, takes the mode config locks to be able to check/change the

 * modeset configuration. Must be run from process context (which usually means

 * either the output polling work or a work item launched from the driver's

 * hotplug interrupt).

 *

 * Note that drivers may call this even before calling

 * drm_fb_helper_initial_config but only after drm_fb_helper_init. This allows

 * for a race-free fbcon setup and will make sure that the fbdev emulation will

 * not miss any hotplug events.

 *

 * RETURNS:

 * 0 on success and a non-zero error code otherwise.

/**

 * drm_fb_helper_lastclose - DRM driver lastclose helper for fbdev emulation

 * @dev: DRM device

 *

 * This function can be used as the &drm_driver->lastclose callback for drivers

 * that only need to call drm_fb_helper_restore_fbdev_mode_unlocked().

/**

 * drm_fb_helper_output_poll_changed - DRM mode config \.output_poll_changed

 *                                     helper for fbdev emulation

 * @dev: DRM device

 *

 * This function can be used as the

 * &drm_mode_config_funcs.output_poll_changed callback for drivers that only

 * need to call drm_fb_helper_hotplug_event().

 @user: 1=userspace, 0=fbcon */

 No need to take a ref for fbcon because it unbinds on unregister */

/*

 * fb_ops.fb_destroy is called by the last put_fb_info() call at the end of

 * unregister_framebuffer() or fb_release().

	/*

	 * Copy to framebuffer even if we already logged an error. Emulates

	 * the behavior of the original fbdev implementation.

/*

 * This function uses the client API to create a framebuffer backed by a dumb buffer.

 *

 * The _sys_ versions are used for &fb_ops.fb_read, fb_write, fb_fillrect,

 * fb_copyarea, fb_imageblit.

 buffer is mapped for HW framebuffer */

		/*

		 * Shamelessly leak the physical address to user-space. As

		 * page_to_phys() is undefined for I/O memory, warn in this

		 * case.

 drm_fbdev_fb_destroy() takes care of cleanup */

 Setup is not retried if it has failed */

/**

 * drm_fbdev_generic_setup() - Setup generic fbdev emulation

 * @dev: DRM device

 * @preferred_bpp: Preferred bits per pixel for the device.

 *                 @dev->mode_config.preferred_depth is used if this is zero.

 *

 * This function sets up generic fbdev emulation for drivers that supports

 * dumb buffers with a virtual address and that can be mmap'ed.

 * drm_fbdev_generic_setup() shall be called after the DRM driver registered

 * the new DRM device with drm_dev_register().

 *

 * Restore, hotplug events and teardown are all taken care of. Drivers that do

 * suspend/resume need to call drm_fb_helper_set_suspend_unlocked() themselves.

 * Simple drivers might use drm_mode_config_helper_suspend().

 *

 * Drivers that set the dirty callback on their framebuffer will get a shadow

 * fbdev buffer that is blitted onto the real buffer. This is done in order to

 * make deferred I/O work with all kinds of buffers. A shadow buffer can be

 * requested explicitly by setting struct drm_mode_config.prefer_shadow or

 * struct drm_mode_config.prefer_shadow_fbdev to true beforehand. This is

 * required to use generic fbdev emulation with SHMEM helpers.

 *

 * This function is safe to call even when there are no connectors present.

 * Setup will be retried on the next hotplug event.

 *

 * The fbdev is destroyed by drm_dev_unregister().

	/*

	 * FIXME: This mixes up depth with bpp, which results in a glorious

	 * mess, resulting in some drivers picking wrong fbdev defaults and

	 * others wrong preferred_depth defaults.

/*

 * Copyright 2003 JosÃ© Fonseca.

 * Copyright 2003 Leif Delgass.

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE

 * AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION

 * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 List of devices hanging off drivers with stealth attach. */

	/* For historical reasons, drm_get_pci_domain() is busticated

	 * on most archs and has to remain so for userspace interface

	 * < 1.4, except on alpha which was right from the beginning

 __alpha__ */

/**

 * drm_legacy_irq_by_busid - Get interrupt from bus ID

 * @dev: DRM device

 * @data: IOCTL parameter pointing to a drm_irq_busid structure

 * @file_priv: DRM file private.

 *

 * Finds the PCI device with the specified bus id and gets its IRQ number.

 * This IOCTL is deprecated, and will now return EINVAL for any busid not equal

 * to that of the device that this DRM instance attached to.

 *

 * Return: 0 on success or a negative error code on failure.

 UMS was only ever support on PCI devices. */

/**

 * drm_legacy_pci_init - shadow-attach a legacy DRM PCI driver

 * @driver: DRM device driver

 * @pdriver: PCI device driver

 *

 * This is only used by legacy dri1 drivers and deprecated.

 *

 * Return: 0 on success or a negative error code on failure.

 If not using KMS, fall back to stealth mode manual scanning. */

		/* Loop around setting up a DRM device for each PCI device

		 * matching our ID and device class.  If we had the internal

		 * function that pci_get_subsys and pci_get_class used, we'd

		 * be able to just pass pid in instead of doing a two-stage

		 * thing.

 stealth mode requires a manual probe */

/**

 * drm_legacy_pci_exit - unregister shadow-attach legacy DRM driver

 * @driver: DRM device driver

 * @pdriver: PCI device driver

 *

 * Unregister a DRM driver shadow-attached through drm_legacy_pci_init(). This

 * is deprecated and only used by dri1 drivers.

/*

 * \file drm_scatter.c

 * IOCTLs to manage scatter/gather memory

 *

 * \author Gareth Hughes <gareth@valinux.com>

/*

 * Created: Mon Dec 18 23:20:54 2000 by gareth@valinux.com

 *

 * Copyright 2000 VA Linux Systems, Inc., Sunnyvale, California.

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR

 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,

 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

	/* This also forces the mapping of COW pages, so our page list

	 * will be valid.  Please don't remove it...

	/* Verify that each page points to its virtual address, and vice

	 * versa.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 Intel Corporation.

 *

 * Authors:

 * Ramalingam C <ramalingam.c@intel.com>

	/*

	 * When vrls are not valid, ksvs are not considered.

	 * Hence SRM will be discarded.

 Length of the all vrls combined */

 Length of the all vrls combined */

/**

 * drm_hdcp_check_ksvs_revoked - Check the revoked status of the IDs

 *

 * @drm_dev: drm_device for which HDCP revocation check is requested

 * @ksvs: List of KSVs (HDCP receiver IDs)

 * @ksv_count: KSV count passed in through @ksvs

 *

 * This function reads the HDCP System renewability Message(SRM Table)

 * from userspace as a firmware and parses it for the revoked HDCP

 * KSVs(Receiver IDs) detected by DCP LLC. Once the revoked KSVs are known,

 * revoked state of the KSVs in the list passed in by display drivers are

 * decided and response is sent.

 *

 * SRM should be presented in the name of "display_hdcp_srm.bin".

 *

 * Format of the SRM table, that userspace needs to write into the binary file,

 * is defined at:

 * 1. Renewability chapter on 55th page of HDCP 1.4 specification

 * https://www.digital-cp.com/sites/default/files/specifications/HDCP%20Specification%20Rev1_4_Secure.pdf

 * 2. Renewability chapter on 63rd page of HDCP 2.2 specification

 * https://www.digital-cp.com/sites/default/files/specifications/HDCP%20on%20HDMI%20Specification%20Rev2_2_Final1.pdf

 *

 * Returns:

 * Count of the revoked KSVs or -ve error number in case of the failure.

 revoked_ksv_cnt will be zero when above function failed */

/**

 * drm_connector_attach_content_protection_property - attach content protection

 * property

 *

 * @connector: connector to attach CP property on.

 * @hdcp_content_type: is HDCP Content Type property needed for connector

 *

 * This is used to add support for content protection on select connectors.

 * Content Protection is intentionally vague to allow for different underlying

 * technologies, however it is most implemented by HDCP.

 *

 * When hdcp_content_type is true enum property called HDCP Content Type is

 * created (if it is not already) and attached to the connector.

 *

 * This property is used for sending the protected content's stream type

 * from userspace to kernel on selected connectors. Protected content provider

 * will decide their type of their content and declare the same to kernel.

 *

 * Content type will be used during the HDCP 2.2 authentication.

 * Content type will be set to &drm_connector_state.hdcp_content_type.

 *

 * The content protection will be set to &drm_connector_state.content_protection

 *

 * When kernel triggered content protection state change like DESIRED->ENABLED

 * and ENABLED->DESIRED, will use drm_hdcp_update_content_protection() to update

 * the content protection state of a connector.

 *

 * Returns:

 * Zero on success, negative errno on failure.

/**

 * drm_hdcp_update_content_protection - Updates the content protection state

 * of a connector

 *

 * @connector: drm_connector on which content protection state needs an update

 * @val: New state of the content protection property

 *

 * This function can be used by display drivers, to update the kernel triggered

 * content protection state changes of a drm_connector such as DESIRED->ENABLED

 * and ENABLED->DESIRED. No uevent for DESIRED->UNDESIRED or ENABLED->UNDESIRED,

 * as userspace is triggering such state change and kernel performs it without

 * fail.This function update the new state of the property into the connector's

 * state and generate an uevent to notify the userspace.

 SPDX-License-Identifier: GPL-2.0

 Copyright 2019 Collabora Ltd */

 Map the perfcnt buf in the address space attached to file_priv. */

	/*

	 * Invalidate the cache and clear the counters to start from a fresh

	 * state.

	/*

	 * Bifrost GPUs have 2 set of counters, but we're only interested by

	 * the first one for now.

	/*

	 * Due to PRLAM-8186 we need to disable the Tiler before we enable HW

	 * counters.

 The BO ref is retained by the mapping. */

 Only Bifrost GPUs have 2 set of counters. */

		/*

		 * TODO: define a macro to extract the number of l2 caches from

		 * mem_features.

		/*

		 * shader_present might be sparse, but the counters layout

		 * forces to dump unused regions too, hence the fls64() call

		 * instead of hweight64().

		/*

		 * There's always one JM and one Tiler block, hence the '+ 2'

		 * here.

 Start with everything disabled. */

 Disable everything before leaving. */

 SPDX-License-Identifier: GPL-2.0

 Copyright 2019 Linaro, Ltd, Rob Herring <robh@kernel.org> */

 Copyright 2019 Collabora ltd. */

 panfrost seqno for signaled() test */

	/* JS0: fragment jobs.

	 * JS1: vertex/tiler jobs

	 * JS2: compute jobs

 Not exposed to userspace yet */

	/*

	 * Use all cores for now.

	 * Eventually we may need to support tiler only jobs and h/w with

	 * multiple (2) coherent core groups

	/* start MMU, medium priority, cache clean/flush on end, clean/flush on

 GO ! */

 Don't queue the job if a reset is in progress */

 panfrost always uses write mode in its current uapi */

 put by scheduler job completion */

	/* Nothing to execute: can happen if the job has finished while

	 * we were resetting the GPU.

 Update the job head so we can resume */

 The job will be resumed, don't signal the fence */

 Job has been hard-stopped, flag it as canceled */

		/* We might want to provide finer-grained error code based on

		 * the exception type, but unconditionally setting to EINVAL

		 * is good enough for now.

	/* Set ->jc to 0 to avoid re-submitting an already finished job (can

	 * happen when we receive the DONE interrupt while doing a GPU reset).

 First we collect all failed/done jobs. */

				/* Cancel the next submission. Will be submitted

				 * after we're done handling this failure if

				 * there's no reset pending.

		/* JS_STATE is sampled when JOB_INT_CLEAR is written.

		 * For each BIT(slot) or BIT(slot + 16) bit written to

		 * JOB_INT_CLEAR, the corresponding bits in JS_STATE

		 * (BIT(slot) and BIT(slot + 16)) are updated, but this

		 * is racy. If we only have one job done at the time we

		 * read JOB_INT_RAWSTAT but the second job fails before we

		 * clear the status, we end up with a status containing

		 * only the DONE bit and consider both jobs as DONE since

		 * JS_STATE reports both NEXT and CURRENT as inactive.

		 * To prevent that, let's repeat this clear+read steps

		 * until status is 0.

 Then we handle the dequeued jobs. */

			/* When the current job doesn't fail, the JM dequeues

			 * the next job without waiting for an ACK, this means

			 * we can have 2 jobs dequeued and only catch the

			 * interrupt when the second one is done. If both slots

			 * are inactive, but one job remains in pfdev->jobs[j],

			 * consider it done. Of course that doesn't apply if a

			 * failure happened since we cancelled execution of the

			 * job in _NEXT (see above).

	/* And finally we requeue jobs that were waiting in the second slot

	 * and have been stopped if we detected a failure on the first slot.

 The job was cancelled, signal the fence now */

 Requeue the job we removed if no reset is pending */

	/* Stop the schedulers.

	 *

	 * FIXME: We temporarily get out of the dma_fence_signalling section

	 * because the cleanup path generate lockdep splats when taking locks

	 * to release job resources. We should rework the code to follow this

	 * pattern:

	 *

	 *	try_lock

	 *	if (locked)

	 *		release

	 *	else

	 *		schedule_work_to_release_later

	/* Mask job interrupts and synchronize to make sure we won't be

	 * interrupted during our reset.

 Cancel the next job and soft-stop the running job. */

 Wait at most 10ms for soft-stops to complete */

 Handle the remaining interrupts before we reset. */

	/* Remaining interrupts have been handled, but we might still have

	 * stuck jobs. Let's make sure the PM counters stay balanced by

	 * manually calling pm_runtime_put_noidle() and

	 * panfrost_devfreq_record_idle() for each stuck job.

 Proceed with reset now. */

	/* panfrost_device_reset() unmasks job interrupts, but we want to

	 * keep them masked a bit longer.

 GPU has been reset, we can clear the reset pending bit. */

	/* Now resubmit jobs that were previously queued but didn't have a

	 * chance to finish.

	 * FIXME: We temporarily get out of the DMA fence signalling section

	 * while resubmitting jobs because the job submission logic will

	 * allocate memory with the GFP_KERNEL flag which can trigger memory

	 * reclaim and exposes a lock ordering issue.

 Restart the schedulers */

 Re-enable job interrupts now that everything has been restarted. */

	/*

	 * If the GPU managed to complete this jobs fence, the timeout is

	 * spurious. Bail out.

	/* All GPUs have two entries per queue, but without jobchain

	 * disambiguation stopping the right job in the close path is tricky,

	 * so let's just advertise one entry in that case.

 Kill in-flight jobs */

 Try to cancel the job before it starts */

				/* Reset the job head so it doesn't get restarted if

				 * the job in the first slot failed.

 If there are any jobs in the HW queue, we're not idle */

 SPDX-License-Identifier: GPL-2.0

 Copyright 2018 Marty E. Plummer <hanetzer@startmail.com> */

 Copyright 2019 Linaro, Ltd., Rob Herring <robh@kernel.org> */

 Copyright 2019 Collabora ltd. */

	/*

	 * The Amlogic integrated Mali-T820, Mali-G31 & Mali-G52 needs

	 * these undocumented bits in GPU_PWR_OVERRIDE1 to be set in order

	 * to operate correctly.

 T60x, T62x, T72x */

 T76x, T8xx */

 Set tiler clock gate override if required */

 Limit read & write ID width for AXI */

 Here goes platform specific quirks */

 T60x has an oddball version */

	/* The T60x has an oddball ID value. Fix it up to the standard Midgard

	 * format so we (and userspace) don't have to special case it.

 Just turn on everything for now */

 Flush reduction only makes sense when the GPU is kept powered on between jobs */

 SPDX-License-Identifier:	GPL-2.0

 Copyright 2019 Linaro, Ltd, Rob Herring <robh@kernel.org> */

	/* Wait for the MMU status to indicate there is no active command, in

 The GPU hung, let's trigger a reset */

 write AS_COMMAND when MMU is ready to accept another command */

	/*

	 * The locked region is a naturally aligned power of 2 block encoded as

	 * log2 minus(1).

	 * Calculate the desired start/end and look for the highest bit which

	 * differs. The smallest naturally aligned block must include this bit

	 * change, the desired region starts with this bit (and subsequent bits)

	 * zeroed and ends with the bit (and subsequent bits) set to one.

	/*

	 * Mask off the low bits of region_start (which would be ignored by

	 * the hardware anyway)

 Lock the region that needs to be updated */

 Run the MMU operation */

 Wait for the flush to complete */

	/* Need to revisit mem attrs.

	 * NC is the default, Mali driver is inner WT.

		/*

		 * AS can be retained by active jobs or a perfcnt context,

		 * hence the '+ 1' here.

			/* Unhandled pagefault on this AS, the MMU was

			 * disabled. We need to re-enable the MMU after

			 * clearing+unmasking the AS interrupts.

 Check for a free AS */

 Assign the free or reclaimed AS to the FD */

 Flush the PTs only if we're already awake */

struct panfrost_mmu *mmu = cookie;

 TODO: Wait 1000 GPU cycles for HW_ISSUE_6367/T60X

 Assume 2MB alignment and size multiple */

 Pages are already mapped, bail out. */

 Executable buffers can't start or end on a 4GB boundary */

 4G enough for now. can be 48-bit */

 decode the fault status */

 Page fault only */

 terminal fault, print info about the fault */

			/* Ignore MMU interrupts on this AS until it's been

			 * re-enabled.

 Disable the MMU to kill jobs on this AS. */

 If we received new MMU interrupts, process them before returning. */

 SPDX-License-Identifier: GPL-2.0

 Copyright 2019 Collabora ltd. */

 ~3 frames */

		/*

		 * GPUs with more than 1 supply require platform-specific handling:

		 * continue without devfreq

 Continue if the optional regulator is missing */

 Optional, continue without devfreq */

	/*

	 * Setup default thresholds for the simple_ondemand governor.

	 * The values are chosen based on experiments.

 SPDX-License-Identifier: GPL-2.0

 Copyright 2019 Linaro, Ltd, Rob Herring <robh@kernel.org> */

/* Called DRM core on the last userspace/kernel unreference of the

 * BO.

	/*

	 * Make sure the BO is no longer inserted in the shrinker list before

	 * taking care of the destruction itself. If we don't do that we have a

	 * race condition between this function and what's done in

	 * panfrost_gem_shrinker_scan().

	/*

	 * If we still have mappings attached to the BO, there's a problem in

	 * our refcounting.

	/*

	 * Executable buffers cannot cross a 16MB boundary as the program

	 * counter is 24-bits. We assume executable buffers will be less than

	 * 16MB and aligning executable buffers to their size will avoid

	 * crossing a 16MB boundary.

/**

 * panfrost_gem_create_object - Implementation of driver->gem_create_object.

 * @dev: DRM device

 * @size: Size in bytes of the memory the object will reference

 *

 * This lets the GEM helpers allocate object structs for us, and keep

 * our BO stats correct.

 Round up heap allocations to 2MB to keep fault handling simple */

	/*

	 * Allocate an id of idr table where the obj is registered

	 * and handle has the id what user can see.

 drop reference from allocate - handle holds it now. */

 SPDX-License-Identifier: GPL-2.0

 Copyright 2018 Marty E. Plummer <hanetzer@startmail.com> */

 Copyright 2019 Linaro, Ltd, Rob Herring <robh@kernel.org> */

	/*

	 * Single domain is handled by the core, and, if only a single power

	 * the power domain is requested, the property is optional.

 OPP will handle regulators */

	/* Right now, none of the GPU we support need a reset, but this

	 * might change.

 SPDX-License-Identifier: GPL-2.0

 Copyright 2018 Marty E. Plummer <hanetzer@startmail.com> */

 Copyright 2019 Linaro, Ltd., Rob Herring <robh@kernel.org> */

 Copyright 2019 Collabora ltd. */

 Heaps should never be executable */

/**

 * panfrost_lookup_bos() - Sets up job->bo[] with the GEM objects

 * referenced by the job.

 * @dev: DRM device

 * @file_priv: DRM file for this fd

 * @args: IOCTL args

 * @job: job being set up

 *

 * Resolve handles from userspace to BOs and attach them to job.

 *

 * Note that this function doesn't need to unreference the BOs on

 * failure, because that will happen at panfrost_job_cleanup() time.

/**

 * panfrost_copy_in_sync() - Sets up job->deps with the sync objects

 * referenced by the job.

 * @dev: DRM device

 * @file_priv: DRM file for this fd

 * @args: IOCTL args

 * @job: job being set up

 *

 * Resolve syncobjs from userspace to fences and attach them to job.

 *

 * Note that this function doesn't need to unreference the fences on

 * failure, because that will happen at panfrost_job_cleanup() time.

 Update the return sync object for the job */

 Don't allow mmapping of heap objects as pages are not pinned. */

		/*

		 * If we want to mark the BO purgeable, there must be only one

		 * user: the caller FD.

		 * We could do something smarter and mark the BO purgeable only

		 * when all its users have marked it purgeable, but globally

		 * visible/shared BOs are likely to never be marked purgeable

		 * anyway, so let's not bother.

/*

 * Panfrost driver version:

 * - 1.0 - initial interface

 * - 1.1 - adds HEAP and NOEXEC flags for CREATE_BO

 * - 1.2 - adds AFBC_FEATURES query

 Allocate and initialze the DRM device. */

 ~3 frames */

	/*

	 * Register the DRM device with the core and the connectors with

	 * sysfs

 optional */

 Set first to probe before the generic compatibles */

 SPDX-License-Identifier: GPL-2.0 */

/* Copyright (C) 2019 Arm Ltd.

 *

 * Based on msm_gem_freedreno.c:

 * Copyright (C) 2016 Red Hat

 * Author: Rob Clark <robdclark@gmail.com>

/**

 * panfrost_gem_shrinker_init - Initialize panfrost shrinker

 * @dev: DRM device

 *

 * This function registers and sets up the panfrost shrinker.

/**

 * panfrost_gem_shrinker_cleanup - Clean up panfrost shrinker

 * @dev: DRM device

 *

 * This function unregisters the panfrost shrinker.

 SPDX-License-Identifier: GPL-2.0+

		/* dropping the reference we acquired in

		 * vkms_primary_plane_update()

 must be set to NULL here */

 for now primary plane must be visible and full screen */

 SPDX-License-Identifier: GPL-2.0+

 SPDX-License-Identifier: GPL-2.0+

 SPDX-License-Identifier: GPL-2.0+

		/* update frame_start only if a queued vkms_composer_worker()

		 * has read the data

	/*

	 * To prevent races we roll the hrtimer forward before we do any

	 * interrupt processing - this is how real hw works (the interrupt is

	 * only generated after all the vblank registers are updated) and what

	 * the vblank core expects. Therefore we need to always correct the

	 * timestampe by one frame.

	/* This lock is held across the atomic commit to block vblank timer

	 * from scheduling vkms_composer_worker until the composer is updated

 SPDX-License-Identifier: GPL-2.0+

/**

 * DOC: vkms (Virtual Kernel Modesetting)

 *

 * VKMS is a software-only model of a KMS driver that is useful for testing

 * and for running X (or similar) on headless machines. VKMS aims to enable

 * a virtual display with no need of a hardware display capability, releasing

 * the GPU in DRM API tests.

	/* FIXME: There's a confusion between bpp and depth between this and

	 * fbdev helpers. We have to go with 0, meaning "pick the default",

 SPDX-License-Identifier: GPL-2.0+

/**

 * compute_crc - Compute CRC value on output frame

 *

 * @vaddr: address to final framebuffer

 * @composer: framebuffer's metadata

 *

 * returns CRC value computed using crc32 on the visible portion of

 * the final framebuffer at vaddr_out

 Faster div by 255 */

/**

 * alpha_blend - alpha blending equation

 * @argb_src: src pixel on premultiplied alpha mode

 * @argb_dst: dst pixel completely opaque

 *

 * blend pixels using premultiplied blend formula. The current DRM assumption

 * is that pixel color values have been already pre-multiplied with the alpha

 * channel values. See more drm_plane_create_blend_mode_property(). Also, this

 * formula assumes a completely opaque background.

/**

 * x_blend - blending equation that ignores the pixel alpha

 *

 * overwrites RGB color value from src pixel to dst pixel.

/**

 * blend - blend value at vaddr_src with value at vaddr_dst

 * @vaddr_dst: destination address

 * @vaddr_src: source address

 * @dst_composer: destination framebuffer's metadata

 * @src_composer: source framebuffer's metadata

 * @pixel_blend: blending equation based on plane format

 *

 * Blend the vaddr_src value with the vaddr_dst value using a pixel blend

 * equation according to the supported plane formats DRM_FORMAT_(A/XRGB8888)

 * and clearing alpha channel to an completely opaque background. This function

 * uses buffer's metadata to locate the new composite values at vaddr_dst.

 *

 * TODO: completely clear the primary plane (a = 0xff) before starting to blend

 * pixel color values

 clearing alpha channel (0xff)*/

	/* If there are other planes besides primary, we consider the active

	 * planes should be in z-order and compose them associatively:

	 * ((primary <- overlay) <- cursor)

/**

 * vkms_composer_worker - ordered work_struct to compute CRC

 *

 * @work: work_struct

 *

 * Work handler for composing and computing CRCs. work_struct scheduled in

 * an ordered workqueue that's periodically scheduled to run by

 * _vblank_handle() and flushed at vkms_atomic_crtc_destroy_state().

	/*

	 * We raced with the vblank hrtimer and previous work already computed

	 * the crc, nothing to do.

	/*

	 * The worker can fall behind the vblank hrtimer, make sure we catch up.

 SPDX-License-Identifier: GPL-2.0+

/*

 * shmob_drm_crtc.c  --  SH Mobile DRM CRTCs

 *

 * Copyright (C) 2012 Renesas Electronics Corporation

 *

 * Laurent Pinchart (laurent.pinchart@ideasonboard.com)

/*

 * TODO: panel support

/* -----------------------------------------------------------------------------

 * Clock management

/* -----------------------------------------------------------------------------

 * CRTC

 Setup SYS bus. */

 HDCN */

 HTCN */

 HSYNW */

 HSYNP */

 VDLN */

 VTLN */

 VSYNW */

 VSYNP */

 Wait until power is applied/stopped. */

 Stop the dot clock. */

/*

 * shmob_drm_crtc_start - Configure and start the LCDC

 * @scrtc: the SH Mobile CRTC

 *

 * Configure and start the LCDC device. External devices (clocks, MERAM, panels,

 * ...) are not touched by this function.

 Enable clocks before accessing the hardware. */

 Reset and enable the LCDC. */

 Stop the LCDC first and disable all interrupts. */

 Configure power supply, dot clocks and start them. */

		/* FIXME: sh7724 can only use 42, 48, 54 and 60 for the divider

		 * denominator.

 TODO: Setup SYS panel */

 Setup geometry, format, frame buffer memory and operation mode. */

 TODO: Handle YUV colorspaces. Hardcode REC709 for now. */

 Word and long word swap. */

 Setup planes. */

 Enable the display output. */

 Stop the LCDC. */

 Disable the display output. */

 Stop clocks. */

 Be careful not to acknowledge any pending interrupt. */

/* -----------------------------------------------------------------------------

 * Encoder

 The flat panel mode is fixed, just copy it to the adjusted mode. */

 No-op, everything is handled in the CRTC code. */

 No-op, everything is handled in the CRTC code. */

 No-op, everything is handled in the CRTC code. */

/* -----------------------------------------------------------------------------

 * Connector

 SPDX-License-Identifier: GPL-2.0+

/*

 * shmob_drm_backlight.c  --  SH Mobile DRM Backlight

 *

 * Copyright (C) 2012 Renesas Electronics Corporation

 *

 * Laurent Pinchart (laurent.pinchart@ideasonboard.com)

 SPDX-License-Identifier: GPL-2.0+

/*

 * shmob_drm_drv.c  --  SH Mobile DRM driver

 *

 * Copyright (C) 2012 Renesas Electronics Corporation

 *

 * Laurent Pinchart (laurent.pinchart@ideasonboard.com)

/* -----------------------------------------------------------------------------

 * Hardware initialization

/* -----------------------------------------------------------------------------

 * DRM operations

	/* Acknowledge interrupts. Putting interrupt enable and interrupt flag

	 * bits in the same register is really brain-dead design and requires

	 * taking a spinlock.

/* -----------------------------------------------------------------------------

 * Power management

/* -----------------------------------------------------------------------------

 * Platform driver

	/*

	 * Allocate and initialize the driver private data, I/O resources and

	 * clocks.

 Allocate and initialize the DRM device. */

	/*

	 * Register the DRM device with the core and the connectors with

	 * sysfs.

 SPDX-License-Identifier: GPL-2.0+

/*

 * shmob_drm_kms.c  --  SH Mobile DRM Mode Setting

 *

 * Copyright (C) 2012 Renesas Electronics Corporation

 *

 * Laurent Pinchart (laurent.pinchart@ideasonboard.com)

/* -----------------------------------------------------------------------------

 * Format helpers

/* -----------------------------------------------------------------------------

 * Frame buffer

 SPDX-License-Identifier: GPL-2.0+

/*

 * shmob_drm_plane.c  --  SH Mobile DRM Planes

 *

 * Copyright (C) 2012 Renesas Electronics Corporation

 *

 * Laurent Pinchart (laurent.pinchart@ideasonboard.com)

 TODO: Support ROP3 mode */

 SPDX-License-Identifier: GPL-2.0-only

/**************************************************************************

 * Copyright (c) 2011, Intel Corporation.

 * All Rights Reserved.

 *

/*

 *	Provide the low level interfaces for the Moorestown backlight

 10000000 */

 Percentage 1-100% being valid */

 Calculate and set the brightness value */

		/* Adjust the backlight level with the percent in

		 * dev_priv->blc_adj1;

		/* Adjust the backlight level with the percent in

		 * dev_priv->blc_adj2;

 force PWM bit on */

 return locally cached var instead of HW read (due to DPST etc.) */

	/* FIXME: ideally return actual value in case firmware fiddled with

 this needs to be set elsewhere */

/*

 *	Provide the Moorestown specific chip logic and low level methods

 *	for power management

/**

 *	oaktrail_save_display_registers	-	save registers lost on suspend

 *	@dev: our DRM device

 *

 *	Save the state we need in order to be able to restore the interface

 *	upon resume from suspend

 Display arbitration control + watermarks */

 Pipe & plane A info */

 Save cursor regs */

 Save palette (gamma) */

 Save performance state */

 LVDS state */

 HW overlay */

 DPST registers */

 Shut down the panel */

 Turn off the plane */

 Trigger the plane disable */

 Wait ~4 ticks */

 Turn off pipe */

 Wait ~8 ticks */

 Turn off PLLs */

/**

 *	oaktrail_restore_display_registers	-	restore lost register state

 *	@dev: our DRM device

 *

 *	Restore register state that was lost during suspend and resume.

 Display arbitration + watermarks */

 Make sure VGA plane is off. it initializes to on after reset!*/

 set the plls */

 Actually enable it */

 Restore mode */

 Restore performance mode*/

 Enable the pipe*/

 Set up the plane*/

 Enable the plane */

 Enable Cursor A */

 Restore palette (gamma) */

port 61180h*/

 Wait for cycle delay */

 Wait for panel power up */

 Restore HW overlay */

 DPST registers */

/**

 *	oaktrail_power_down	-	power down the display island

 *	@dev: our DRM device

 *

 *	Power down the display interface of our device

/*

 * oaktrail_power_up

 *

 * Restore power to the specified island(s) (powergating)

 Oaktrail */

 Now pull the BIOS data */

/*

 * Copyright (c) 2002-2010, Intel Corporation.

 * Copyright (c) 2014 ATRON electronic GmbH

 *   Author: Jan Safrata <jan.nikitenko@gmail.com>

 *

 * Permission is hereby granted, free of charge, to any person obtaining a copy

 * of this software and associated documentation files (the "Software"), to deal

 * in the Software without restriction, including without limitation the rights

 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell

 * copies of the Software, and to permit persons to whom the Software is

 * furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included in

 * all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE

 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN

 * THE SOFTWARE.

 *

/*

 * LPC GPIO based I2C bus for LVDS of Atom E6xx

/*-----------------------------------------------------------------------------

 * LPC Register Offsets. Used for LVDS GPIO Bit Bashing. Registers are part

 * Atom E6xx [D31:F0]

/* The LVDS GPIO clock lines are GPIOSUS[3]

 * The LVDS GPIO data lines are GPIOSUS[4]

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright Â© 2006-2007 Intel Corporation

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 *	Dave Airlie <airlied@linux.ie>

 *	Jesse Barnes <jesse.barnes@intel.com>

/*

 * LVDS I2C backlight control macros

	/*

	 * Saved LVDO output states

/*

 * Returns the maximum level of the backlight duty cycle field.

 Powered off, use the saved value */

 Top 15bits hold the frequency mask */

 Return a 16bit range as needed for setting */

/*

 * Set LVDS backlight level by I2C command

 *

 * FIXME: at some point we need to both track this for PM and also

 * disable runtime pm on MRST if the brightness is nil (ie blanked)

BLC_PWM_CTL Should be initiated while backlight device init*/

/*

 * Set LVDS backlight level either by I2C or PWM

/*

 * Sets the backlight level.

 *

 * level: backlight level, from 0 to psb_intel_lvds_get_max_backlight().

/*

 * Sets the power state for the panel.

 XXX: We never power down the LVDS pairs. */

lvds_priv->savePP_DIVISOR = REG_READ(PP_DIVISOR);*/

TODO: move backlight_duty_cycle to psb_intel_lvds_priv*/

	/*

	 * If the light is off at server startup,

	 * just make it full brightness

REG_WRITE(PP_DIVISOR, lvds_priv->savePP_DIVISOR);*/

 just in case */

 just in case */

 PSB requires the LVDS is on pipe B, MRST has only one pipe anyway */

 Should never happen!! */

	/*

	 * If we have timings from the BIOS for the panel, put them in

	 * to the adjusted mode.  The CRTC will be set up for this mode,

	 * with the panel scaling set up to source from the H/VDisplay

	 * of the original mode.

	/*

	 * XXX: It would be nice to support lower refresh rates on the

	 * panels to reduce power consumption, and perhaps match the

	 * user's requested refresh rate.

	/*

	 * The LVDS pin pair will already have been turned on in the

	 * psb_intel_crtc_mode_set since it has a large impact on the DPLL

	 * settings.

	/*

	 * Enable automatic panel scaling so that non-native modes fill the

	 * screen.  Should be enabled before the pipe is enabled, according to

	 * register description and PRM.

/*

 * Return the list of DDC modes if available, or the BIOS fixed mode otherwise.

/**

 * psb_intel_lvds_destroy - unregister and free LVDS structures

 * @connector: connector to free

 *

 * Unregister the DDC bus for this connector then free the driver private

 * structure.

/**

 * psb_intel_lvds_init - setup LVDS connectors on this device

 * @dev: drm device

 * @mode_dev: mode device

 *

 * Create the connector, register the LVDS DDC bus, and try to figure out what

 * modes we can display on the LVDS panel (if present).

 *modes, *bios_mode; */

Attach connector properties*/

	/*

	 * Set up I2C bus

	 * FIXME: distroy i2c_bus when exit

	/*

	 * LVDS discovery:

	 * 1) check for EDID on DDC

	 * 2) check for VBT data

	 * 3) check to see if LVDS is already on

	 *    if none of the above, no panel

	 * 4) make sure lid is open

	 *    if closed, act like it's not there for now

 Set up the DDC bus. */

	/*

	 * Attempt to get the fixed panel mode from DDC.  Assume that the

	 * preferred mode is the right one.

 FIXME: check for quirks */

 Failed to get EDID, what about VBT? do we need this? */

	/*

	 * If we didn't get EDID, try checking if the panel is already turned

	 * on.	If so, assume that whatever is currently programmed is the

	 * correct mode.

 FIXME: check for quirks */

 If we still don't have a mode after all that, give up. */

	/*

	 * Blacklist machines with BIOSes that list an LVDS panel without

	 * actually having one.

 SPDX-License-Identifier: GPL-2.0-only

/**************************************************************************

 * Copyright (c) 2011, Intel Corporation.

 * All Rights Reserved.

 *

/* TODO

 * - Split functions by vbt type

 * - Make them all take drm_device

 * - Check ioremap failures

 FB_MIPI_DISABLE doesn't mean LVDS on with Medfield */

 Prevent runtime suspend at start*/

/*

 *	Get the revison ID, B0:D2:F0;0x08

 The same for r0 and r1 */

 Get the address of the platform config vbt */

 get the virtual address of the vbt */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright Â© 2006-2009 Intel Corporation

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 *	Dave Airlie <airlied@linux.ie>

 *	Jesse Barnes <jesse.barnes@intel.com>

 The max/min PWM frequency in BPCR[31:17] - */

/* The smallest number is 1 (not 0) that can fit in the

/* shifts to the left by one bit to get the actual 16-bit

/*

 * Sets the power state for the panel.

 XXX: We never power down the LVDS pairs. */

	/*

	 * The LVDS pin pair will already have been turned on in the

	 * psb_intel_crtc_mode_set since it has a large impact on the DPLL

	 * settings.

	/* If the firmware says dither on Moorestown, or the BIOS does

 Find the connector we're trying to set up */

(v == DRM_MODE_SCALE_FULLSCREEN)*/

 Returns the panel fixed mode from configuration. */

 Use the firmware provided data on Moorestown */

 Use the BIOS VBT mode if available */

 Then try the LVDS VBT mode */

 If we still got no mode then bail */

/**

 * oaktrail_lvds_init - setup LVDS connectors on this device

 * @dev: drm device

 * @mode_dev: PSB mode device

 *

 * Create the connector, register the LVDS DDC bus, and try to figure out what

 * modes we can display on the LVDS panel (if present).

 *modes, *bios_mode; */

	/*

	 * LVDS discovery:

	 * 1) check for EDID on DDC

	 * 2) check for VBT data

	 * 3) check to see if LVDS is already on

	 *    if none of the above, no panel

	 * 4) make sure lid is open

	 *    if closed, act like it's not there for now

	/*

	 * Attempt to get the fixed panel mode from DDC.  Assume that the

	 * preferred mode is the right one.

 FIXME: check for quirks */

	/*

	 * If we didn't get EDID, try geting panel timing

	 * from configuration data

 FIXME: check for quirks */

 If we still don't have a mode after all that, give up. */

 failed_ddc: */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright Â© 2006-2011 Intel Corporation

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 CDV_SINGLE_LVDS_96MHz */

 CDV_SINGLE_LVDS_100MHz */

	 /* The single-channel range is 25-112Mhz, and dual-channel

	  * is 80-224Mhz.  Prefer single channel as much as possible.

 CDV_DAC_HDMI_27MHz */

 CDV_DAC_HDMI_96MHz */

 CDV_DP_27MHz */

 CDV_DP_100MHz */

/* Reset the DPIO configuration register.  The BIOS does this at every

 * mode set.

/* Unlike most Intel display engines, on Cedarview the DPLL registers

 * are behind this sideband bus.  They must be programmed while the

 * DPLL reference clock is on in the DPLL control register, but before

 * the DPLL is enabled in the DPLL control register.

 Follow the BIOS and write the REF/SFR Register. Hardcoded value */

	/* We don't know what the other fields of these regs are, so

	 * leave them in place.

	/*

	 * The BIT 14:13 of 0x8010/0x8030 is used to select the ref clk

	 * for the pipe A/B. Display spec 1.06 has wrong definition.

	 * Correct definition is like below:

	 *

	 * refclka mean use clock from same PLL

	 *

	 * if DPLLA sets 01 and DPLLB sets 01, they use clock from their pll

	 *

	 * if DPLLA sets 01 and DPLLB sets 02, both use clk from DPLLA

	 *

 use DPLL_A for pipeB on CRT/HDMI */

 Follow the BIOS to program the N_DIVIDER REG */

		/*

		 * Now only single-channel LVDS is supported on CDV. If it is

		 * incorrect, please add the dual-channel LVDS.

 m1 is reserved as 0 in CDV, n is a ring counter */

 Disable self-refresh before adjust WM */

		/* Cedarview workaround to write ovelay plane, which force to leave

		 * MAX_FIFO state.

dev_priv->ovl_offset*/);

 Is only one pipe enabled? */

 ignore FW4 */

 Is pipe b lvds ? */

 enable self-refresh for single pipe active */

 HW team suggested values... */

/*

 * Return the pipe currently connected to the panel fitter,

 * or -1 if the panel fitter is not present or not in use

 See if the panel fitter is in use */

 low-end sku, 96/100 mhz */

 high-end sku, 27/100 mhz */

		/*

		 * Based on the spec the low-end SKU has only CRT/LVDS. So it is

		 * unnecessary to consider it for DP/eDP.

		 * On the high-end SKU, it will use the 27/100M reference clk

		 * for DP/eDP. When using SSC clock, the ref clk is 100MHz.Otherwise

		 * it will be 27MHz. From the VBIOS code it seems that the pipe A choose

		 * 27MHz for DP/eDP while the Pipe B chooses the 100MHz.

/*	if (is_lvds)

		dpll |= DPLLB_MODE_LVDS;

	else

 dpll |= (2 << 11); */

 setup pipeconf */

 the BPC will be 6 if it is 18-bit LVDS panel */

 Set up the display plane register */

	/* The LVDS pin pair needs to be on before the DPLLs are enabled.

	 * This is an exception to the general rule that mode_set doesn't turn

	 * things on.

		/* Set the B0-B3 data pairs corresponding to

		 * whether we're going to

		 * set the DPLLs for dual-channel mode or not.

		/* It would be nice to set 24 vs 18-bit mode (LVDS_A3_POWER_UP)

		 * appropriately here, but we need to look more

		 * thoroughly into how panels behave in the two modes.

 Disable the panel fitter if it was on our pipe */

 Wait for the clocks to stabilize. */

 42 usec w/o calibration, 110 with.  rounded up. */

	/* pipesrc and dspsize control the size that is scaled from,

	 * which should always be the user's requested size.

 Flush the plane changes */

* Derive the pixel clock for the given refclk and divisors for 8xx chips. */

 FIXME: why are we using this, should it be cdv_ in this tree ? */

 Returns the clock of the currently programmed mode of the given pipe. */

 XXX: might not be 66MHz */

	/* XXX: It would be nice to validate the clocks, but we can't reuse

	 * i830PllIsValid() because it relies on the xf86_config connector

	 * configuration being accurate, which it isn't necessarily.

* Returns the currently programmed mode of the given pipe. */

 SPDX-License-Identifier: GPL-2.0-only

/**************************************************************************

 * Copyright (c) 2007-2011, Intel Corporation.

 * All Rights Reserved.

 *

	/*

	 * If this is a GEM object then info->screen_base is the virtual

	 * kernel remapping of the object. FIXME: Review if this is

	 * suitable for our mmap work

/**

 *	psb_framebuffer_init	-	initialize a framebuffer

 *	@dev: our DRM device

 *	@fb: framebuffer to set up

 *	@mode_cmd: mode description

 *	@obj: backing object

 *

 *	Configure and fill in the boilerplate for our frame buffer. Return

 *	0 on success or an error code if we fail.

	/*

	 * Reject unknown formats, YUV formats, and formats with more than

	 * 4 bytes per pixel.

/**

 *	psb_framebuffer_create	-	create a framebuffer backed by gt

 *	@dev: our DRM device

 *	@mode_cmd: the description of the requested mode

 *	@obj: the backing object

 *

 *	Create a framebuffer object backed by the gt, and fill in the

 *	boilerplate required

 *

 *	TODO: review object references

/**

 *	psbfb_alloc		-	allocate frame buffer memory

 *	@dev: the DRM device

 *	@aligned_size: space needed

 *

 *	Allocate the frame buffer. In the usual case we get a GTT range that

 *	is stolen memory backed and life is simple. If there isn't sufficient

 *	we fail as we don't have the virtual mapping space to really vmap it

 *	and the kernel console code can't handle non linear framebuffers.

 *

 *	Re-address this as and if the framebuffer layer grows this ability.

 Begin by trying to use stolen memory backing */

/**

 *	psbfb_create		-	create a framebuffer

 *	@fb_helper: the framebuffer helper

 *	@sizes: specification of the layout

 *

 *	Create a framebuffer to the specifications provided

 No 24bit packed */

 Allocate the framebuffer in the GTT with stolen page backing */

 Accessed stolen memory directly */

 Use default scratch pixmap (info->pixmap.flags = FB_PIXMAP_SYSTEM) */

/**

 *	psb_user_framebuffer_create	-	create framebuffer

 *	@dev: our DRM device

 *	@filp: client file

 *	@cmd: mode request

 *

 *	Create a new framebuffer backed by a userspace GEM object

	/*

	 *	Find the GEM object and thus the gtt range object that is

	 *	to back this space

 Let the core code do all the work */

 no 24bit packed */

	/* If the mode will not fit in 32bit then switch to 16bit to get

	   a console on full resolution. The X mode setting server will

 disable all the possible outputs/crtcs before entering KMS mode */

 It is ok for this to fail - we just don't get backlight control */

 valid crtcs */

 set memory base */

 Oaktrail and Poulsbo should use BAR 2*/

 num pipes is 2 for PSB but 1 for Mrst */

/*

 * Copyright Â© 2010 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Li Peng <peng.li@intel.com>

	/*

	 * 1024 x 768  new_crtc_htotal = 0x1024;

	 * 1280 x 1024 new_crtc_htotal = 0x0c34;

	/*

	 * 1024 x 768  np = 1; nr = 0x26; nf = 0x0fd8000;

	 * 1280 x 1024 np = 1; nr = 0x17; nf = 0x1034000;

 scu processing time is in few u secods */

 break if scu doesn't reset busy bit after huge retry */

/*

 *	You don't want to know, you really really don't want to know....

 *

 *	This is magic. However it's safe magic because of the way the platform

 *	works and it is necessary magic.

 scu ipc: assert hdmi controller reset */

 scu ipc: de-assert hdmi controller reset */

 Disable the VGA plane that we never use */

 Disable dpll if necessary */

 Reset controller */

 program and enable dpll */

 Set the DPLL */

 configure HDMI */

 Flush the plane changes */

 Set up the display plane register */

 setup pipeconf */

 Disable plane */

 Flush the plane changes */

 Disable pipe B */

 Disable LNW Pipes, etc */

 wait for pipe off */

 Disable dpll */

 wait for dpll off */

 Enable dpll */

 wait for dpll warm up */

 Enable pipe B */

 Enable LNW Pipe B */

 Enable plane */

 Flush the plane changes */

 DSPARB */

 FW1 */

 FW2 */

 FW4 */

 FW5 */

 LNC Chicken Bits - Squawk! */

	/*

	 *	FIXME: We need to figure this lot out. In theory we can

	 *	read the EDID somehow but I've yet to find working reference

	 *	code.

 FIXME ? edid = drm_get_edid(connector, i2c_adap); */

 Initialize i2c controller */

 save HDMI register state */

 dpll */

 pipe B */

 plane */

 cursor B */

 save palette */

 restore HDMI register state */

 dpll */

 pipe */

 plane */

 cursor B */

 restore palette */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * GMA500 Backlight Interface

 *

 * Copyright (c) 2009-2011, Intel Corporation.

 *

 * Authors: Eric Knopp

 SPDX-License-Identifier: GPL-2.0-only

/**************************************************************************

 * Copyright (c) 2011, Intel Corporation.

 * All Rights Reserved.

 *

 These bits indicate HDMI not SDVO on CDV */

/*

 *	Cedartrail Backlght Interfaces

		/* i915 does this, I believe which means that we should not

 Percentage 1-100% being valid */

/*

 *	Provide the Cedarview specific chip logic and low level methods

 *	for power management

 *

 *	FIXME: we need to implement the apm/ospm base management bits

 *	for this and the MID devices.

 Power status */

 Enable the GPU */

 Wait for the GPU power */

	/* Disable bonus launch.

	 *	CPU and GPU competes for memory and display misses updates and

	 *	flickers. Worst with dual core, dual displays.

	 *

	 *	Fixes were done to Win 7 gfx driver to disable a feature called

	 *	Bonus Launch to work around the issue, by degrading

	 *	performance.

/**

 *	cdv_save_display_registers	-	save registers lost on suspend

 *	@dev: our DRM device

 *

 *	Save the state we need in order to be able to restore the interface

 *	upon resume from suspend

/**

 *	cdv_restore_display_registers	-	restore lost register state

 *	@dev: our DRM device

 *

 *	Restore register state that was lost during suspend and resume.

 *

 *	FIXME: review

 BIOS does below anyway */

 Fix arbitration bug */

 Resume the modeset for every activated CRTC */

 Just fire off a uevent and let userspace tell us what to do */

/* The core driver has received a hotplug IRQ. We are in IRQ context

 Cedarview */

 CDV is much like Poulsbo but has MID like SGX offsets and PM */

/*

 * Copyright (c) 2006 Dave Airlie <airlied@linux.ie>

 * Copyright Â© 2006-2008,2010 Intel Corporation

 *   Jesse Barnes <jesse.barnes@intel.com>

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 *	Chris Wilson <chris@chris-wilson.co.uk>

 Intel GPIO access functions */

 When using bit bashing for I2C, this bit needs to be set to 1 */

	/* FIXME: We are never Pineview, right?



	u32 val;



	if (!IS_PINEVIEW(dev_priv->dev))

		return;



	val = REG_READ(DSPCLK_GATE_D);

	if (enable)

		val |= DPCUNIT_CLOCK_GATE_DISABLE;

	else

		val &= ~DPCUNIT_CLOCK_GATE_DISABLE;

	REG_WRITE(DSPCLK_GATE_D, val);



	return;

 On most chips, these bits must be preserved in software. */

 Posting */

	/* Toggle the Software Clear Interrupt bit. This has the effect

	 * of resetting the GMBUS controller and so clearing the

	 * BUS_ERROR raised by the slave's NAK.

	/* Mark the GMBUS interface as disabled. We will re-enable it at the

	 * start of the next xfer, till then let it sleep.

 Hardware may not support GMBUS over these pins? Try GPIO bitbanging instead. */

 I2C_FUNC_10BIT_ADDR | */

/**

 * gma_intel_setup_gmbus() - instantiate all Intel i2c GMBuses

 * @dev: DRM device

 By default use a conservative clock rate */

 XXX force bit banging until GMBUS is fully debugged */

	/* speed:

	 * 0x0 = 100 KHz

	 * 0x1 = 50 KHz

	 * 0x2 = 400 KHz

	 * 0x3 = 1000 Khz

 iounmap is done in driver_unload */

 SPDX-License-Identifier: GPL-2.0-only

/**************************************************************************

 * Copyright (c) 2007-2011, Intel Corporation.

 * All Rights Reserved.

 * Copyright (c) 2008, Tungsten Graphics, Inc. Cedar Park, TX., USA.

 * All Rights Reserved.

 *

/*

 * The table below contains a mapping of the PCI vendor ID and the PCI Device ID

 * to the different groups of PowerVR 5-series chip designs

 *

 * 0x8086 = Intel Corporation

 *

 * PowerVR SGX535    - Poulsbo    - Intel GMA 500, Intel Atom Z5xx

 * PowerVR SGX535    - Moorestown - Intel GMA 600

 * PowerVR SGX535    - Oaktrail   - Intel GMA 600, Intel Atom Z6xx, E6xx

 * PowerVR SGX545    - Cedartrail - Intel GMA 3600, Intel Atom D2500, N2600

 * PowerVR SGX545    - Cedartrail - Intel GMA 3650, Intel Atom D2550, D2700,

 *                                  N2800

 Poulsbo */

 Oak Trail */

 Cedar Trail */

/*

 * Standard IOCTLs.

/**

 *	psb_spank		-	reset the 2D engine

 *	@dev_priv: our PSB DRM device

 *

 *	Soft reset the graphics engine and then reload the necessary registers.

 Do not bypass any MMU access, let them pagefault instead */

 mmu_gatt ?? */

 Post */

 TODO: Kill vblank etc here */

 Destroy VBT data */

 initializing driver private data */

 Couldn't find the aux vdc so map to primary vdc */

 Init OSPM support */

 Add stolen memory to SGX MMU */

 Setup vertical blanking handling */

	/*

	 * Install interrupt handlers prior to powering off SGX or else we will

	 * crash.

 only 24 bits of frame count */

 Only add backlight support if we have LVDS output */

 Enable runtime pm at last */

 FIXME: do we need to wrap the other side of this */

/**************************************************************************

 * Copyright (c) 2009-2011, Intel Corporation.

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Benjamin Defnet <benjamin.r.defnet@intel.com>

 *    Rajesh Poornachandran <rajesh.poornachandran@intel.com>

 * Massively reworked

 *    Alan Cox <alan@linux.intel.com>

 Serialize power ops */

 Serialize power claim */

/**

 *	gma_power_init		-	initialise power manager

 *	@dev: our device

 *

 *	Set up for power management tracking of our hardware.

 FIXME: Move APM/OSPM base into relevant device code */

 We start active */

 Currently no users */

 And not suspended */

/**

 *	gma_power_uninit	-	end power manager

 *	@dev: device to end for

 *

 *	Undo the effects of gma_power_init

/**

 *	gma_suspend_display	-	suspend the display logic

 *	@dev: our DRM device

 *

 *	Suspend the display logic of the graphics interface

/**

 *	gma_resume_display	-	resume display side logic

 *	@pdev: PCI device

 *

 *	Resume the display hardware restoring state and enabling

 *	as necessary.

 turn on the display power island */

 Rebuild our GTT mappings */

/**

 *	gma_suspend_pci		-	suspend PCI side

 *	@pdev: PCI device

 *

 *	Perform the suspend processing on our PCI device state

/**

 *	gma_resume_pci		-	resume helper

 *	@pdev: our PCI device

 *

 *	Perform the resume processing on our PCI device state - rewrite

 *	register state and re-enable the PCI device

 restoring MSI address and data in PCIx space */

/**

 *	gma_power_suspend		-	bus callback for suspend

 *	@_dev: our device

 *

 *	Called back by the PCI layer during a suspend of the system. We

 *	perform the necessary shut down steps and save enough state that

 *	we can undo this when resume is called.

/**

 *	gma_power_resume		-	resume power

 *	@_dev: our device

 *

 *	Resume the PCI side of the graphics and then the displays

/**

 *	gma_power_is_on		-	returne true if power is on

 *	@dev: our DRM device

 *

 *	Returns true if the display island power is on at this moment

/**

 *	gma_power_begin		-	begin requiring power

 *	@dev: our DRM device

 *	@force_on: true to force power on

 *

 *	Begin an action that requires the display power island is enabled.

 *	We refcount the islands.

 Power already on ? */

 Ok power up needed */

/**

 *	gma_power_end		-	end use of power

 *	@dev: Our DRM device

 *

 *	Indicate that one of our gma_power_begin() requested periods when

 *	the diplay island power is needed has completed.

 SPDX-License-Identifier: GPL-2.0-only

/**************************************************************************

 * Copyright (c) 2011, Intel Corporation.

 * All Rights Reserved.

 *

/*

 *	Poulsbo Backlight Interfaces

 10000000 */

 return locally cached var instead of HW read (due to DPST etc.) */

	/* FIXME: ideally return actual value in case firmware fiddled with

 u32 bl_max_freq; */

 unsigned long value; */

 get bl_max_freq and pol from dev_priv*/

 Percentage 1-100% being valid */

 This must occur after the backlight is properly initialised */

/*

 *	Provide the Poulsbo specific chip logic and low level methods

 *	for power management

 Disable 2D clock gating */

/**

 *	psb_save_display_registers	-	save registers lost on suspend

 *	@dev: our DRM device

 *

 *	Save the state we need in order to be able to restore the interface

 *	upon resume from suspend

 Display arbitration control + watermarks */

 Save crtc and output state */

/**

 *	psb_restore_display_registers	-	restore lost register state

 *	@dev: our DRM device

 *

 *	Restore register state that was lost during suspend and resume.

 Display arbitration + watermarks */

make sure VGA plane is off. it initializes to on after reset!*/

 Poulsbo */

/*

 * Copyright Â© 2012 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Keith Packard <keithp@keithp.com>

 *

/**

 * struct i2c_algo_dp_aux_data - driver interface structure for i2c over dp

 * 				 aux algorithm

 * @running: set by the algo indicating whether an i2c is ongoing or whether

 * 	     the i2c bus is quiescent

 * @address: i2c target address for the currently ongoing transfer

 * @aux_ch: driver callback to transfer a single byte of the i2c payload

 Run a single AUX_CH I2C transaction, writing/reading data as necessary */

/*

 * I2C over AUX CH

/*

 * Send the address. If the I2C link is running, this 'restarts'

 * the connection with the new address, this is used for doing

 * a write followed by a read (as needed for DDC)

/*

 * Stop the I2C transaction. This closes out the link, sending

 * a bare address packet with the MOT bit turned off

/*

 * Write a single byte to the current I2C address, the

 * the I2C link must be running or this returns -EIO

/*

 * Read a single byte from the current I2C address, the

 * I2C link must be running or this returns -EIO

/*

 * FIXME: This is the old dp aux helper, gma500 is the last driver that needs to

 * be ported over to the new helper code in drm_dp_helper.c like i915 or radeon.

 for eDP */

/**

 * is_edp - is the given port attached to an eDP panel (either CPU or PCH)

 * @encoder: GMA encoder struct

 *

 * If a CPU or PCH DP output is attached to an eDP panel, this function

 * will return true, and false otherwise.

 Returns true if the panel was already on when called */

 ILK workaround: disable reset around power sequence */

	/*

	 * If we enable the backlight right away following a panel power

	 * on, we may see slight flicker as the panel syncs with the eDP

	 * link.  So delay a bit to make sure the image is solid before

	 * allowing it to appear.

	/* only refuse the mode on non eDP since we have seen some weird eDP panels

	/* The clock divider is based off the hrawclk,

	 * and would like to run at 2MHz. So, take the

	 * hrawclk value and divide by 2 and use that

	 * On CDV platform it uses 200MHz as hrawclk.

	 *

 Must try at least 3 times according to DP spec */

 Load the send data into the aux channel data registers */

 Send the command and wait for it to complete */

 Clear done status and any errors */

	/* Check for timeout or receive error.

	 * Timeouts occur when the sink is not connected

	/* Timeouts occur when the device isn't connected, so they're

 Unload any bytes sent back from the other side */

 Write data to the aux channel in native mode */

 Write a single byte to the aux channel in native mode */

 read bytes from a native aux channel */

 Set up the command byte */

			/* I2C-over-AUX Reply field is only valid

			 * when paired with AUX ACK.

 okay we failed just pick the highest */

	/*

	while (*num > 0xffffff || *den > 0xffffff) {

		*num >>= 1;

		*den >>= 1;

	/*

	 * Find the lane count in the intel_encoder private

	/*

	 * Compute the GMCH and Link ratios. The '3' here is

	 * the number of bytes_per_pixel post-LUT, which we always

	 * set up for 8-bits of R/G/B, or 3 bytes total.

	/*

	 * Check for DPCD version > 1.1 and enhanced framing support

 CPT DP's pipe select is decided in TRANS_DP_CTL */

 If the sink supports it, try to set the power state appropriately */

 Should have a valid DPCD by this point */

		/*

		 * When turning on, we need to retry for 1ms to give the sink

		 * time to wake up.

 Wake up the sink first */

/*

 * Native read with retry for link status and receiver capability reads for

 * cases where the sink may still be asleep.

	/*

	 * Sinks are *supposed* to come up within 1ms from an off state,

	 * but we're also supposed to retry 3 times per the spec.

/*

 * Fetch AUX CH registers 0x202 - 0x207 which contain

 * link status information

 Check for clock recovery is done on all channels */

 Check to see if channel eq is done on all channels */

return ;

	/* ;Swing voltage programming

 ;gfx_dpio_set_reg(0x8154, 0x43406055) */

	/* ;gfx_dpio_set_reg(0x8148, 0x55338954)

	 * The VSwing_PreEmph table is also considered based on the vswing/premp

 ;gfx_dpio_set_reg(0x814c, 0x40802040) */

 ;gfx_dpio_set_reg(0x8150, 0x2b405555) */

 cdv_sb_write(dev, ddi_reg->VSwing4, 0x2b405555); */

 ;gfx_dpio_set_reg(0x8154, 0xc3406055) */

	/* ;Pre emphasis programming

	 * ;gfx_dpio_set_reg(0xc02c, 0x1f030040)

 ;gfx_dpio_set_reg(0x8124, 0x00004000) */

 Enable corresponding port and start training pattern 1 */

 Enable output, wait for it to become active */

 Write the link configuration data */

 Use intel_dp->train_set[0] to set the voltage and pre emphasis values */

 Set training pattern 1 */

 Check to see if we've tried the max voltage */

 Check to see if we've tried the same voltage 5 times */

 Compute new intel_dp->train_set as requested by target */

 channel equalization */

 channel eq pattern */

 Use intel_dp->train_set[0] to set the voltage and pre emphasis values */

 Make sure clock is still ok */

 Try 5 times, then try clock recovery if that fails */

 Compute new intel_dp->train_set as requested by target */

/*

 * Uses CRT_HOTPLUG_EN and CRT_HOTPLUG_STAT to detect DP connection.

 *

 * \return true if DP port is connected.

 * \return false if DP port is disconnected.

	cdv_intel_panel_destroy_backlight(connector->dev); */

 check the VBT to see whether the eDP is on DP-D port */

/* Cedarview display clock gating



   We need this disable dot get correct behaviour while enabling

   DP/eDP. TODO - investigate if we can turn it back to normality

 Set up the DDC bus. */

 FIXME:fail check */

 Pull timing values out of registers */

 if this fails, presume the device is a ghost */

		/* The CDV reference driver moves pnale backlight setup into the displays that

		   have a backlight: this is a good idea and one we should probably adopt, however

cdv_intel_panel_setup_backlight(dev); */

/*

 * Copyright Â© 2010 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Li Peng <peng.li@intel.com>

 Enable I2C transaction */

	/*

	 * XXX: i2c write seems isn't useful for EDID probe, don't do anything

 Enable i2c unit */

 Enable irq */

 next message */

 Disable irq */

 clearing read buffer full intr */

 continue read transaction */

 clear transaction done intr */

/*

 * choose alternate function 2 of GPIO pin 52, 53,

 * which is used by HDMI I2C logic

 Enable HDMI I2C function on gpio */

 request irq */

 Adapter registration */

/*

 * Copyright Â© 2006-2011 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	jim liu <jim.liu@intel.com>

 hdmi control bits */

 hdmi-b control bits */

 Should set this when detect hotplug */

 for control functions */

/*

 * Return the list of HDMI DDC modes if available.

 just in case */

 just in case */

/*

 * Copyright 2006 Dave Airlie <airlied@linux.ie>

 * Copyright Â© 2006-2007 Intel Corporation

 *   Jesse Barnes <jesse.barnes@intel.com>

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 Register for the SDVO device: SDVOB or SDVOC */

 Active outputs controlled by this SDVO output */

	/*

	 * Capabilities of the SDVO device returned by

	 * i830_sdvo_get_capabilities()

 Pixel clock limitations reported by the SDVO device, in kHz */

	/*

	* For multiple function SDVO device,

	* this is for current attached outputs.

	/**

	 * This is used to select the color range of RBG outputs in HDMI mode.

	 * It is only valid when using TMDS encoding and 8 bit per color mode.

	/**

	 * This is set if we're going to treat the device as TV-out.

	 *

	 * While we have these nice friendly flags for output types that ought

	 * to decide this for us, the S-Video output on our HDMI+S-Video card

	 * shows up as RGB1 (VGA).

 This is for current tv format name */

	/**

	 * This is set if we treat the device as HDMI, instead of DVI.

	/**

	 * This is set if we detect output of sdvo device as LVDS and

	 * have a valid fixed mode to use with the panel.

	/**

	 * This is sdvo fixed panel mode pointer

 DDC bus used by this SDVO encoder */

 Input timings for adjusted_mode */

 Saved SDVO output states */

 Can be SDVOB or SDVOC depending on sdvo_reg */

 Mark the type of connector */

 This contains all current supported TV format */

 add the property for the SDVO-TV */

 add the property for the SDVO-TV/LVDS */

 Add variable to record current setting for the above property */

 this is to get the range of margin.*/

/*

 * Writes the SDVOB or SDVOC with the given value, but always writes both

 * SDVOB and SDVOC to work around apparent hardware issues (according to

 * comments in the BIOS).

		/*

		* Write the registers twice for luck. Sometimes,

		* writing them only once doesn't appear to 'stick'.

		* The BIOS does this too. Yay, magic

* Mapping of command numbers to names, for debug output */

 Add the op code for SDVO enhancements */

 HDMI op code */

 the following two are to read the response */

 failure in I2C transfer */

	/*

	 * The documentation states that all commands will be

	 * processed within 15Âµs, and that we need only poll

	 * the status byte a maximum of 3 times in order for the

	 * command to be complete.

	 *

	 * Check 5 times in case the hardware failed to read the docs.

 Read the command response */

 This must be the immediately preceding write before the i2c xfer */

/*

 * Return whether each input is trained.

 *

 * This function is making an assumption about the layout of the response,

 * which should be checked against the docs.

 Convert the values from units of 10 kHz to kHz. */

 do some mode translations */

 Reset the input timing to the screen. Assume always input 0. */

	/* We need to construct preferred input timings based on our

	 * output timings.  To do that, we have to set the output

	 * timings, even though this isn't really the right place in

	 * the sequence to do it. Oh well.

	/* Make the CRTC code factor in the SDVO pixel multiplier.  The

	 * SDVO device will factor out the multiplier during mode_set.

	/* First, set the input mapping for the first input to our controlled

	 * output. This is only correct if we're a single-input device, in

	 * which case the first input is the output from the appropriate SDVO

	 * channel on the motherboard.  In a two-input device, the first input

	 * will be SDVOB and the second SDVOC.

 Set the output timings to the screen */

	/* We have tried to get input timing in mode_fixup, and filled into

	 * adjusted_mode.

 Set the output timing to the screen */

 Set the input timing to the screen. Assume always input 0. */

 Set the SDVO control regs. */

	/* FIXME: Check if this is needed for PSB

	sdvox |= (pixel_multiplier - 1) << SDVO_PORT_MULTIPLY_SHIFT;

		/* Warn if the device reported failure to sync.

		 * A lot of SDVO devices fail to notify of sync, but it's

		 * a given it the status is a success, we succeeded.

 Is there more than one type of output? */

 Mac mini hack -- use the same DDC as the analog connector */

		/*

		 * Don't use the 1 as the argument of DDC bus switch to get

		 * the EDID. It is used for SDVO SPD ROM.

		/*

		 * If we found the EDID on the other bus,

		 * assume that is the correct DDC bus.

	/*

	 * When there is no edid and no monitor is connected with VGA

	 * port, try to use the CRT ddc to read the EDID for DVI-connector.

 DDC bus is shared, match EDID to connector type */

 add 30ms delay when the output type might be TV */

 if we have an edid check it matches the connection */

 May update encoder flag for like clock for SDVO TV, etc.*/

 set the bus switch and get the modes */

	/*

	 * Mac mini hack.  On this device, the DVI-I connector shares one DDC

	 * link between analog and digital outputs. So, if the regular SDVO

	 * DDC fails, check to see if the analog output is disconnected, in

	 * which case we'll look there for the digital DDC data.

/*

 * Set of SDVO TV modes.

 * Note!  This is in reply order (see loop in get_tv_modes).

 * XXX: all 60Hz refresh?

	/* Read the list of supported input resolutions for the selected TV

	 * format.

	/*

	 * Attempt to get the mode list from DDC.

	 * Assume that the preferred modes are

	 * arranged in priority order.

 Fetch modes from VBT */

 Guarantee the mode is preferred */

 unknown property */

	/* Force a full mode set on the crtc. We're supposed to have the

	/* FIXME: At the moment, ddc_bus = 2 is the only thing that works.

	 * We need to figure out if this is true for all available poulsbo

	 * hardware, or if we need to fiddle with the guessing code above.

	/* Make a mask of outputs less than or equal to our own priority in the

	 * list.

 Count bits to find what number we are in the priority list. */

 If more than 3 outputs, default to DDC bus 3 for now. */

 Corresponds to SDVO_CONTROL_BUS_DDCx */

/*

 * Choose the appropriate DDC bus for control bus switch command for this

 * SDVO output based on the controlled output.

 *

 * DDC bus number assignment is in a priority order of RGB outputs, then TMDS

 * outputs, then LVDS outputs.

 If the BIOS described our SDVO device, take advantage of it. */

	/* If the BIOS only described a different SDVO device, use the

	 * address that it isn't using.

	/* No SDVO device info is found for another DVO port,

	 * so use mapping assumption we had before BIOS parsing.

	/* FIXME: We don't support HDMI at the moment

	struct drm_device *dev = connector->base.base.dev;



	intel_attach_force_audio_property(&connector->base.base);

	intel_attach_broadcast_rgb_property(&connector->base.base);

 connector->polled = DRM_CONNECTOR_POLL_CONNECT | DRM_CONNECTOR_POLL_DISCONNECT;

 SDVO requires XXX1 function may not exist unless it has XXX0 function.*/

 TV has no XXX1 function block */

 when horizontal overscan is supported, Add the left/right  property */

 encoder type will be decided later */

 Read the regs to test if we can talk to the device */

 In default case sdvo lvds is false */

 Set the input timing to the screen. Assume always input 0. */

 check currently supported outputs */

 SPDX-License-Identifier: GPL-2.0-only

/**************************************************************************

 * Copyright (c) 2011, Intel Corporation.

 * All Rights Reserved.

 *

pci_write_config_dword(pci_root, 0xD4, 0x00C32004);*/

pci_write_config_dword(pci_root, 0xD0, 0xE0033000);*/

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  psb GEM interface

 *

 * Copyright (c) 2011, Intel Corporation.

 *

 * Authors: Alan Cox

 *

 * TODO:

 *	-	we need to work out if the MMU is relevant (eg for

 *		accelerated operations on a GEM object)

 Remove the list map if one is present */

 This must occur last as it frees up the memory of the GEM object */

/**

 *	psb_gem_create		-	create a mappable object

 *	@file: the DRM file of the client

 *	@dev: our device

 *	@size: the size requested

 *	@handlep: returned handle (opaque number)

 *	@stolen: unused

 *	@align: unused

 *

 *	Create a GEM object, fill in the boilerplate and attach a handle to

 *	it so that userspace can speak about it. This does the core work

 *	for the various methods that do/will create GEM objects for things

	/* Allocate our object - for now a direct gtt range which is not

 Initialize the extra goodies GEM needs to do all the hard work */

 GEM doesn't give an error code so use -ENOMEM */

 Limit the object to 32bit mappings */

 Give the object a handle so we can carry it more easily */

 We have the initial and handle reference but need only one now */

/**

 *	psb_gem_dumb_create	-	create a dumb buffer

 *	@file: our client file

 *	@dev: our device

 *	@args: the requested arguments copied from userspace

 *

 *	Allocate a buffer suitable for use for a frame buffer of the

 *	form described by user space. Give userspace a handle by which

 *	to reference it.

/**

 *	psb_gem_fault		-	pagefault handler for GEM objects

 *	@vmf: fault detail

 *

 *	Invoked when a fault occurs on an mmap of a GEM managed area. GEM

 *	does most of the work for us including the actual map/unmap calls

 *	but we need to do the actual page work.

 *

 *	This code eventually needs to handle faulting objects in and out

 *	of the GTT and repacking it when we run out of space. We can put

 *	that off for now and for our simple uses

 *

 *	The VMA was set up by GEM. In doing so it also ensured that the

 *	vma->vm_private_data points to the GEM object that is backing this

 *	mapping.

 GEM object */

 Get the gtt range */

	/* Make sure we don't parallel update on a fault, nor move or remove

	/* For now the mmap pins the object and it stays pinned. As things

	/* Page relative to the VMA start - we must calculate this ourselves

 CPU view of the page, don't go via the GART for CPU writes */

 SPDX-License-Identifier: GPL-2.0-only

/**************************************************************************

 * Copyright (c) 2007, Intel Corporation.

 *

 * Authors: Thomas Hellstrom <thomas-at-tungstengraphics-dot-com>

lid state is open*/

FIXME: should be backlight level before*/

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2006 Intel Corporation

 *

 * Authors:

 *    Eric Anholt <eric@anholt.net>

 skip to first section */

 walk the sections looking for section_id */

 Get the eDP sequencing and link info */

 Some VBTs have bogus h/vtotal values */

 Try to find integrated panel data */

 Defaults if we can't find VBT info */

 Try to find sdvo panel data */

 Set sensible defaults in case we can't find the general block */

	/* judge whether the size of child device meets the requirements.

	 * If the child device size obtained from general definition block

	 * is different with sizeof(struct child_device_config), skip the

	 * parsing of sdvo device info

 different child dev size . Ignore it */

 get the block size of general definitions */

 get the number of child device */

 skip the device block if device type is invalid */

			/*

			 * If the slave address is neither 0x70 nor 0x72,

			 * it is not a SDVO device. Skip it.

 skip the incorrect SDVO port */

 Maybe this is a SDVO device with multiple inputs */

 And the mapping info is not added */

 No SDVO device info is found */

 This bit means to use 96Mhz for DPLL_A or not */

	/* judge whether the size of child device meets the requirements.

	 * If the child device size obtained from general definition block

	 * is different with sizeof(struct child_device_config), skip the

	 * parsing of sdvo device info

 different child dev size . Ignore it */

 get the block size of general definitions */

 get the number of child device */

 get the number of child devices that are present */

 skip the device block if device type is invalid */

 skip the device block if device type is invalid */

/**

 * psb_intel_init_bios - initialize VBIOS settings & find VBT

 * @dev: DRM device

 *

 * Loads the Video BIOS and checks that the VBT exists.  Sets scratch registers

 * to appropriate values.

 *

 * VBT existence is a sanity check that is relied on by other i830_bios.c code.

 * Note that it would be better to use a BIOS call to get the VBT, as BIOSes may

 * feed an updated VBT back through that, compared to what we'll fetch using

 * this method of groping around in the BIOS data.

 *

 * Returns 0 on success, nonzero on failure.

 XXX Should this validation be moved to intel_opregion.c? */

 Scour memory looking for the VBT signature */

 Grab useful general dxefinitions */

/*

 * Destroy and free VBT data

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2007 Intel Corporation

 *

 * Authers: Jesse Barnes <jesse.barnes@intel.com>

/**

 * psb_intel_ddc_probe

 * @adapter:   Associated I2C adaptor

/**

 * psb_intel_ddc_get_modes - get modelist from monitor

 * @connector: DRM connector device to use

 * @adapter:   Associated I2C adaptor

 *

 * Fetch the EDID information from @connector using the DDC bus.

/*

 * Copyright 2011 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 current lid state indicator */

 current docking state indicator */

 OpRegion mailbox #1: public ACPI methods */

 driver readiness */

 notification status */

 current event */

 supported display devices ID list */

 currently presented display list */

 currently active display list */

 next active devices list */

 ASL sleep time-out */

 toggle table index */

 current hotplug enable indicator */

 current lid state*/

 current docking state */

 Sx state resume */

 ASL supported events */

 current OS notification */

 driver status */

 OpRegion mailbox #2: SWSCI */

FIXME: add it later*/

 OpRegion mailbox #3: ASLE */

 driver readiness */

 ASLE interrupt command */

 technology enabled indicator */

 current ALS illuminance reading */

 backlight brightness to set */

 panel fitting state */

 current brightness level */

 backlight level duty cycle mapping table */

 current panel fitting mode */

 enabled panel fitting modes */

 panel LUT and identifier */

 PWM freq and min brightness */

 ASLE irq request bits */

 response bits of ASLE irq request */

 ASLE backlight brightness to set */

 ASLE panel fitting request */

 response bits of ASLE irq request */

 ASLE backlight brightness to set */

 ASLE panel fitting request */

 PWM frequency and minimum brightness */

		/* Don't do this on Medfield or other non PC like devices, they

	/* The only video events relevant to opregion are 0x80. These indicate

	   either a docking event, lid switch or display switch request. In

	   Linux, these are handled by the dock, button and video drivers.

	   We might want to fix the video driver to be opregion-aware in

	   future, but right now we just indicate to the firmware that the

		/* Notify BIOS we are ready to handle ACPI video ext notifs.

		 * Right now, all the events are handled by the ACPI video

 just clear all opregion memory pointers now */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright Â© 2006-2011 Intel Corporation

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 *	Dave Airlie <airlied@linux.ie>

 *	Jesse Barnes <jesse.barnes@intel.com>

/*

 * LVDS I2C backlight control macros

	/**

	 * Saved LVDO output states

/*

 * Returns the maximum level of the backlight duty cycle field.

/*

 * Sets the backlight level.

 *

 * level backlight level, from 0 to cdv_intel_lvds_get_max_backlight().

/*

 * Sets the power state for the panel.

 XXX: We never power down the LVDS pairs. */

 just in case */

 just in case */

 Should never happen!! */

	/*

	 * If we have timings from the BIOS for the panel, put them in

	 * to the adjusted mode.  The CRTC will be set up for this mode,

	 * with the panel scaling set up to source from the H/VDisplay

	 * of the original mode.

	/*

	 * XXX: It would be nice to support lower refresh rates on the

	 * panels to reduce power consumption, and perhaps match the

	 * user's requested refresh rate.

	/*

	 * The LVDS pin pair will already have been turned on in the

	 * cdv_intel_crtc_mode_set since it has a large impact on the DPLL

	 * settings.

	/*

	 * Enable automatic panel scaling so that non-native modes fill the

	 * screen.  Should be enabled before the pipe is enabled, according to

	 * register description and PRM.

/*

 * Return the list of DDC modes if available, or the BIOS fixed mode otherwise.

/**

 * cdv_intel_lvds_destroy - unregister and free LVDS structures

 * @connector: connector to free

 *

 * Unregister the DDC bus for this connector then free the driver private

 * structure.

/*

 * Enumerate the child dev array parsed from VBT to check whether

 * the LVDS is present.

 * If it is present, return 1.

 * If it is not present, return false.

 * If no child dev is parsed from VBT, it assumes that the LVDS is present.

		/* If the device type is not LFP, continue.

		 * We have to check both the new identifiers as well as the

		 * old for compatibility with some BIOSes.

		/* However, we cannot trust the BIOS writers to populate

		 * the VBT correctly.  Since LVDS requires additional

		 * information from AIM blocks, a non-zero addin offset is

		 * a good indicator that the LVDS is actually present.

		/* But even then some BIOS writers perform some black magic

		 * and instantiate the device without reference to any

		 * additional data.  Trust that if the VBT was written into

		 * the OpRegion then they have validated the LVDS's existence.

/**

 * cdv_intel_lvds_init - setup LVDS connectors on this device

 * @dev: drm device

 * @mode_dev: PSB mode device

 *

 * Create the connector, register the LVDS DDC bus, and try to figure out what

 * modes we can display on the LVDS panel (if present).

Attach connector properties*/

	/**

	 * Set up I2C bus

	 * FIXME: distroy i2c_bus when exit

	/*

	 * LVDS discovery:

	 * 1) check for EDID on DDC

	 * 2) check for VBT data

	 * 3) check to see if LVDS is already on

	 *    if none of the above, no panel

	 * 4) make sure lid is open

	 *    if closed, act like it's not there for now

 Set up the DDC bus. */

	/*

	 * Attempt to get the fixed panel mode from DDC.  Assume that the

	 * preferred mode is the right one.

 FIXME: check for quirks */

 Failed to get EDID, what about VBT? do we need this?*/

 FIXME: check for quirks */

	/*

	 * If we didn't get EDID, try checking if the panel is already turned

	 * on.	If so, assume that whatever is currently programmed is the

	 * correct mode.

 FIXME: check for quirks */

 If we still don't have a mode after all that, give up. */

 setup PWM */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright Â© 2006-2011 Intel Corporation

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 *	Patrik Jakobsson <patrik.r.jakobsson@gmail.com>

/*

 * Returns whether any output on the specified pipe is of the specified type

 Wait for 20ms, i.e. one cycle at 50hz. */

 no fb bound */

	/* We are displaying this buffer, make sure it is actually loaded

	/* FIXME: Investigate whether this really is the base for psb and why

		  the linear offset is named base for the other chips. map->surf

 If there was a previous display we can now unpin it */

 Loads the palette/gamma unit for the CRTC with the prepared values */

 The clocks have to be on to load the palette. */

 FIXME: Why pipe[0] and not pipe[..._crtc->pipe]? */

/*

 * Sets the power management mode of the pipe and plane.

 *

 * This code should probably grow support for turning the cursor off and back

 * on appropriately at the same time as we're turning the pipe off/on.

	/* XXX: When our outputs are all unaware of DPMS modes other than off

	 * and on, we should map those modes to DRM_MODE_DPMS_OFF in the CRTC.

 Enable the DPLL */

 Wait for the clocks to stabilize. */

 Wait for the clocks to stabilize. */

 Wait for the clocks to stabilize. */

 Enable the plane */

 Flush the plane changes */

 Enable the pipe */

		/* Give the overlay scaler a chance to enable

 psb_intel_crtc_dpms_video(crtc, true); TODO */

		/* Give the overlay scaler a chance to disable

 psb_intel_crtc_dpms_video(crtc, FALSE); TODO */

 Disable the VGA plane that we never use */

 Turn off vblank interrupts */

 Wait for vblank for the disable to take effect */

 Disable plane */

 Flush the plane changes */

 Disable pipe */

 Wait for vblank for the disable to take effect. */

 Disable DPLL */

 Wait for the clocks to turn off. */

 Set FIFO watermarks */

 If we didn't get a handle then turn the cursor off */

 Unpin the old GEM object */

 Currently we only support 64x64 cursors */

 Pin the memory into the GTT */

 Prevent overflow */

 Copy the cursor to cursor mem */

 set the pipe for the cursor */

 unpin the old bo */

 Using mode_set_base requires the new fb to be set already. */

 Call this locked if we want an event at vblank interrupt. */

 Restore previous fb in case of failure. */

/*

 * Save HW states of given crtc

 NOTE: DSPSIZE DSPPOS only for psb */

/*

 * Restore HW states of given crtc

 lvds has its own version of prepare see psb_intel_lvds_prepare */

 lvds has its own version of commit see psb_intel_lvds_commit */

 Currently there is only a 1:1 mapping of encoders and connectors */

 DRM_ERROR(s); */ return false; }

 On CDV m1 is always 0 */

	/* XXX: We may need to be checking "Dot clock"

	 * depending on the multiplier, connector, etc.,

	 * rather than just a single range.

		/*

		 * For LVDS, if the panel is on, just rely on its current

		 * settings for dual-channel.  We haven't figured out how to

		 * reliably set up different single/dual channel state, if we

		 * even can.

 m1 is always 0 on CDV so the outmost loop will run just once */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2007, Intel Corporation.

 * All Rights Reserved.

 *

 * Authors: Thomas Hellstrom <thomas-at-tungstengraphics.com>

 *	    Alan Cox <alan@linux.intel.com>

/*

 *	GTT resource allocator - manage page mappings in GTT space

/**

 *	psb_gtt_mask_pte	-	generate GTT pte entry

 *	@pfn: page number to encode

 *	@type: type of memory in the GTT

 *

 *	Set the GTT entry for the appropriate memory type.

	/* Ensure we explode rather than put an invalid low mapping of

/**

 *	psb_gtt_entry		-	find the GTT entries for a gtt_range

 *	@dev: our DRM device

 *	@r: our GTT range

 *

 *	Given a gtt_range object return the GTT offset of the page table

 *	entries for this gtt_range

/**

 *	psb_gtt_insert	-	put an object into the GTT

 *	@dev: our DRM device

 *	@r: our GTT range

 *	@resume: on resume

 *

 *	Take our preallocated GTT range and insert the GEM object into

 *	the GTT. This is protected via the gtt mutex which the caller

 *	must hold.

 refcount these maybe ? */

 Make sure changes are visible to the GPU */

 Write our page entries into the GTT itself */

 Make sure all the entries are set before we return */

/**

 *	psb_gtt_remove	-	remove an object from the GTT

 *	@dev: our DRM device

 *	@r: our GTT range

 *

 *	Remove a preallocated GTT range from the GTT. Overwrite all the

 *	page table entries with the dummy page. This is protected via the gtt

 *	mutex which the caller must hold.

/**

 *	psb_gtt_attach_pages	-	attach and pin GEM pages

 *	@gt: the gtt range

 *

 *	Pin and build an in kernel list of the pages that back our GEM object.

 *	While we hold this the pages cannot be swapped out. This is protected

 *	via the gtt mutex which the caller must hold.

/**

 *	psb_gtt_detach_pages	-	attach and pin GEM pages

 *	@gt: the gtt range

 *

 *	Undo the effect of psb_gtt_attach_pages. At this point the pages

 *	must have been removed from the GTT as they could now be paged out

 *	and move bus address. This is protected via the gtt mutex which the

 *	caller must hold.

/**

 *	psb_gtt_pin		-	pin pages into the GTT

 *	@gt: range to pin

 *

 *	Pin a set of pages into the GTT. The pins are refcounted so that

 *	multiple pins need multiple unpins to undo.

 *

 *	Non GEM backed objects treat this as a no-op as they are always GTT

 *	backed objects.

/**

 *	psb_gtt_unpin		-	Drop a GTT pin requirement

 *	@gt: range to pin

 *

 *	Undoes the effect of psb_gtt_pin. On the last drop the GEM object

 *	will be removed from the GTT which will also drop the page references

 *	and allow the VM to clean up or page stuff.

 *

 *	Non GEM backed objects treat this as a no-op as they are always GTT

 *	backed objects.

/*

 *	GTT resource allocator - allocate and manage GTT address space

/**

 *	psb_gtt_alloc_range	-	allocate GTT address space

 *	@dev: Our DRM device

 *	@len: length (bytes) of address space required

 *	@name: resource name

 *	@backed: resource should be backed by stolen pages

 *	@align: requested alignment

 *

 *	Ask the kernel core to find us a suitable range of addresses

 *	to use for a GTT mapping.

 *

 *	Returns a gtt_range structure describing the object, or NULL on

 *	error. On successful return the resource is both allocated and marked

 *	as in use.

 The start of the GTT is the stolen pages */

 The rest we will use for GEM backed objects */

 Ensure this is set for non GEM objects */

/**

 *	psb_gtt_free_range	-	release GTT address space

 *	@dev: our DRM device

 *	@gt: a mapping created with psb_gtt_alloc_range

 *

 *	Release a resource that was allocated with psb_gtt_alloc_range. If the

 *	object has been pinned by mmap users we clean this up here currently.

 Undo the mmap pin if we are destroying the object */

 Enable the GTT */

 The root resource we allocate address space from */

	/*

	 *	The video mmu has a hw bug when accessing 0x0D0000000.

	 *	Make gatt start at 0x0e000,0000. This doesn't actually

	 *	matter for us but may do if the video acceleration ever

	 *	gets opened up.

 CDV doesn't report this. In which case the system has 64 gtt pages */

 Preferably peppermint */

		/* This can occur on CDV systems. Fudge it in this case.

		   We really don't care what imaginary space is being allocated

		/* This is a little confusing but in fact the GTT is providing

		   a view from the GPU into memory and not vice versa. As such

		   this is really allocating space that is not the same as the

	/*

	 *	Map the GTT and the stolen memory area

	/*

	 * Insert vram stolen pages into the GTT

	/*

	 * Init rest of GTT to the scratch page to avoid accidents or scribbles

 On resume, the gtt_mutex is already initialized */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright ÃÂ© 2006-2011 Intel Corporation

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 INTEL_LIMIT_I9XX_SDVO_DAC */

 INTEL_LIMIT_I9XX_LVDS */

	 /* The single-channel range is 25-112Mhz, and dual-channel

	  * is 80-224Mhz.  Prefer single channel as much as possible.

/*

 * Return the pipe currently connected to the panel fitter,

 * or -1 if the panel fitter is not present or not in use

 See if the panel fitter is in use */

 Must be on PIPE 1 for PSB */

 No scan out no play */

 compute bitmask from p1 value */

 XXX: just matching BIOS for now */

	dpll |= PLL_REF_INPUT_TVCLKINBC; */

 setup pipeconf */

 Set up the display plane register */

 Disable the panel fitter if it was on our pipe */

	/* The LVDS pin pair needs to be on before the DPLLs are enabled.

	 * This is an exception to the general rule that mode_set doesn't turn

	 * things on.

		/* Set the B0-B3 data pairs corresponding to

		 * whether we're going to

		 * set the DPLLs for dual-channel mode or not.

		/* It would be nice to set 24 vs 18-bit mode (LVDS_A3_POWER_UP)

		 * appropriately here, but we need to look more

		 * thoroughly into how panels behave in the two modes.

 Wait for the clocks to stabilize. */

 write it again -- the BIOS does, after all */

 Wait for the clocks to stabilize. */

	/* pipesrc and dspsize control the size that is scaled from,

	 * which should always be the user's requested size.

 Flush the plane changes */

 Returns the clock of the currently programmed mode of the given pipe. */

 XXX: might not be 66MHz */

	/* XXX: It would be nice to validate the clocks, but we can't reuse

	 * i830PllIsValid() because it relies on the xf86_config connector

	 * configuration being accurate, which it isn't necessarily.

* Returns the currently programmed mode of the given pipe. */

/*

 * Set the default value of cursor control and base register

 * to zero. This is a workaround for h/w defect on Oaktrail

		/* Allocate 4 pages of stolen mem for a hardware cursor. That

		 * is enough for the 64 x 64 ARGB cursors we support.

	/* We allocate a extra array of drm_connector pointers

 Set the CRTC operations from the chip specific data */

 Set the CRTC clock functions from chip specific data */

 Setup the array of drm_connector pointer array */

 Set to true so that the pipe is forced off on initial config. */

 SPDX-License-Identifier: GPL-2.0-only

/**************************************************************************

 * Copyright (c) 2007, Intel Corporation.

 * All Rights Reserved.

 *

 * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to

 * develop this driver.

 *

/*

 * inline functions

 Enable the interrupt, clear any pending status */

/*

 * Display controller interrupt handler for pipe event.

	/* Clear the 2nd level interrupt status bits

/*

 * Display controller interrupt handler.

/*

 * SGX interrupt handler

 Clear bits */

	/* Note: this bit has other meanings on some devices, so we will

 Revisit this area - want per device masks ? */

 This register is safe even if display island is off */

 Enable 2D and MMU fault interrupts */

 Post */

 This register is safe even if display island is off */

 PCI devices require shared interrupts. */

 These two registers are safe even if display island is off */

 This register is safe even if display island is off */

/*

 * It is used to enable VBLANK interrupt

/*

 * It is used to disable VBLANK interrupt

/* Called from drm generic code, passed a 'crtc', which

 * we use as a pipe index

	/*

	 * High & low register fields aren't synchronized, so make sure

	 * we get a low value that's stable across two reads of the high

	 * register.

/*

 * Copyright Â© 2006-2007 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 The lowest clock for CDV is 20000KHz */

 The max clock for CDV is 355 instead of 400 */

	/*

	 * Disable separate mode multiplier used when cloning SDVO to CRT

	 * XXX this needs to be adjusted when we really are cloning

/*

 * Uses CRT_HOTPLUG_EN and CRT_HOTPLUG_STAT to detect CRT presence.

 *

 * \return true if CRT is connected.

 * \return false if CRT is disconnected.

	/*

	 * On a CDV thep, CRT detect sequence need to be done twice

	 * to get a reliable result.

 turn on the FORCE_DETECT */

 wait for FORCE_DETECT to go off */

 clear the interrupt we just generated, if any */

 and put the bits back */

/*

 * Routines for controlling stuff on the analog port

 Set up the DDC bus. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright Â© 2006-2007 Intel Corporation

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

/*

 * Intel GPIO access functions

 On most chips, these bits must be preserved in software. */

 wait for the line to change state */

 On most chips, these bits must be preserved in software. */

 wait for the line to change state */

/**

 * psb_intel_i2c_create - instantiate an Intel i2c bus using the specified GPIO reg

 * @dev: DRM device

 * @reg: GPIO reg to use

 * @name: name for this bus

 *

 * Creates and registers a new i2c bus with the Linux i2c layer, for use

 * in output probing and control (e.g. DDC or SDVO control functions).

 *

 * Possible values for @reg include:

 *   %GPIOA

 *   %GPIOB

 *   %GPIOC

 *   %GPIOD

 *   %GPIOE

 *   %GPIOF

 *   %GPIOG

 *   %GPIOH

 * see PRM for details on how these different busses are used.

 JJJ:  raise SCL and SDA? */

/**

 * psb_intel_i2c_destroy - unregister and free i2c bus resources

 * @chan: channel to free

 *

 * Unregister the adapter from the i2c layer, then free the structure.

 SPDX-License-Identifier: GPL-2.0-only

/**************************************************************************

 * Copyright (c) 2007, Intel Corporation.

 *

/*

 * Code for the SGX MMU:

/*

 * clflush on one processor only:

 * clflush should apparently flush the cache line on all processors in an

 * SMP system.

/*

 * kmap atomic:

 * The usage of the slots must be completely encapsulated within a spinlock, and

 * no other functions that may be using the locks for other purposed may be

 * called from within the locked region.

 * Since the slots are per processor, this will guarantee that we are the only

 * user.

/*

 * TODO: Inserting ptes from an interrupt handler:

 * This may be desirable for some SGX functionality where the GPU can fault in

 * needed pages. For that, we need to make an atomic insert_pages function, that

 * may fail.

 * If it fails, the caller need to insert the page using a workqueue function,

 * but on average it should be fast.

 Make sure data cache is turned off before enabling it */

	/* Make sure data cache is turned off and MMU is flushed before

	/* Should take the spinlock here, but we don't need to do that

		/*

		 * clflush size is determined at kernel setup for x86_64 but not

		 * for i386. We have to do it here.

 Make sure we only need to flush this processor's cache */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright Â© 2009 Intel Corporation

 MRST_LIMIT_LVDS_100L */

 MRST_LIMIT_LVDS_83L */

 MRST_LIMIT_LVDS_100 */

 MRST_LIMIT_SDVO */

* Derive the pixel clock for the given refclk and divisors for 8xx chips. */

 p2 value always stored in p2_slow on SDVO */

 VCO will increase at this point so break */

					/* freq_error will start to decrease at

/*

 * Returns a set of divisors for the desired target clock with the given refclk,

 * or FALSE.  Divisor values are the actual divisors for

/*

 * Sets the power management mode of the pipe and plane.

 *

 * This code should probably grow support for turning the cursor off and back

 * on appropriately at the same time as we're turning the pipe off/on.

	/* XXX: When our outputs are all unaware of DPMS modes other than off

	 * and on, we should map those modes to DRM_MODE_DPMS_OFF in the CRTC.

 Enable the DPLL */

 Wait for the clocks to stabilize. */

 Wait for the clocks to stabilize. */

 Wait for the clocks to stabilize. */

 Enable the pipe */

 Enable the plane */

 Flush the plane changes */

		/* Give the overlay scaler a chance to enable

 psb_intel_crtc_dpms_video(crtc, true); TODO */

		/* Give the overlay scaler a chance to disable

 psb_intel_crtc_dpms_video(crtc, FALSE); TODO */

 Disable the VGA plane that we never use */

 Disable display plane */

 Flush the plane changes */

 Next, disable display pipes */

 Wait for for the pipe disable to take effect. */

 Wait for the clocks to turn off. */

 Set FIFO Watermarks (values taken from EMGD) */

/*

 * Return the pipe currently connected to the panel fitter,

 * or -1 if the panel fitter is not present or not in use

 See if the panel fitter is in use */

 Disable the VGA plane that we never use */

 Disable the panel fitter if it was on our pipe */

		/* Moorestown doesn't have register support for centering so

		 * we need to mess with the h/vblank and h/vsync start and

 Flush the plane changes */

 setup pipeconf */

 Set up the display plane register */

BIT16 = 0 for 100MHz reference */

 Convert calculated values to register values */

 compute bitmask from p1 value */

 dpll |= (1 << (clock.p1 - 1)) << 16;

 Check the DPLLA lock bit PIPEACONF[29] */

 Wait for the clocks to stabilize. */

 write it again -- the BIOS does, after all */

 Wait for the clocks to stabilize. */

 no fb bound */

 Not used yet */

 SPDX-License-Identifier: GPL-2.0+

/*

 * DRM driver for display panels connected to a Sitronix ST7715R or ST7735R

 * display controller in SPI mode.

 *

 * Copyright 2017 David Lechner <david@lechnology.com>

 * Copyright (C) 2019 Glider bvba

 RGB (vs. BGR) */

 Must be first for .release() */

 Cannot read from Adafruit 1.8" display via SPI */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * DRM driver for Ilitek ILI9225 panels

 *

 * Copyright 2017 David Lechner <david@lechnology.com>

 *

 * Some code copied from mipi-dbi.c

 * Copyright 2016 Noralf TrÃ¸nnes

	/*

	 * There don't seem to be two example init sequences that match, so

	 * using the one from the popular Arduino library for this display.

	 * https://github.com/Nkawu/TFT_22_ILI9225/blob/master/src/TFT_22_ILI9225.cpp

	/*

	 * This callback is not protected by drm_dev_enter/exit since we want to

	 * turn off the display on regular driver unload. It's highly unlikely

	 * that the underlying SPI controller is gone should this be called after

	 * unplug.

 override the command function set in  mipi_dbi_spi_init() */

 SPDX-License-Identifier: GPL-2.0-or-later

 ---------------------------------------------------------------------- */

 ---------------------------------------------------------------------- */

 hw */

 mode */

 drm */

 ---------------------------------------------------------------------- */

 vga register offset */)

 check header to detect whenever edid support is enabled in qemu */

 mmio bar with vga and bochs registers present */

 TODO: shot down existing vram mappings */

 discard ar_flip_flop */

 blank or unblank; we need only update index and set 0x20 */

 should not happen */

 ---------------------------------------------------------------------- */

 Bug: we didn't pin the BO to VRAM in prepare_fb. */

 ---------------------------------------------------------------------- */

 drm interface                                                          */

 ---------------------------------------------------------------------- */

 pm interface                                                           */

 ---------------------------------------------------------------------- */

 pci interface                                                          */

 end of list */ }

 ---------------------------------------------------------------------- */

 module init/exit                                                       */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ARC PGU DRM driver.

 *

 * Copyright (C) 2016 Synopsys, Inc. (www.synopsys.com)

 +-0.5% allowed by HDMI spec */

 Get the optional framebuffer memory resource */

	/*

	 * There is only one output port inside each device. It is linked with

	 * encoder endpoint.

 Locate drm bridge from the hdmi encoder DT node */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * DRM driver for Pervasive Displays RePaper branded e-ink panels

 *

 * Copyright 2013-2017 Pervasive Displays, Inc.

 * Copyright 2017 Noralf TrÃ¸nnes

 *

 * The driver supports:

 * Material Film: Aurora Mb (V231)

 * Driver IC: G2 (eTC)

 *

 * The controller code was taken from the userspace driver:

 * https://github.com/repaper/gratis

 0 is reserved to avoid clashing with NULL */

 Image pixel -> Display pixel */

 B -> W, W -> B (Current Image) */

 B -> N, W -> W (Current Image) */

 B -> N, W -> B (New Image) */

 B -> B, W -> W (New Image) */

 Stack allocated tx? */

 pixels on display are numbered from 1 so even is actually bits 1,3,5,... */

 B -> W, W -> B (Current) */

 B -> N, W -> W (Current) */

 B -> N, W -> B (New) */

 B -> B, W -> W (New) */

 pixels on display are numbered from 1 so odd is actually bits 0,2,4,... */

 B -> W, W -> B (Current) */

 B -> N, W -> W (Current) */

 B -> N, W -> B (New) */

 B -> B, W -> W (New) */

 interleave bits: (byte)76543210 -> (16 bit).7.6.5.4.3.2.1 */

 pixels on display are numbered from 1 */

 B -> W, W -> B (Current) */

 B -> N, W -> W (Current) */

 B -> N, W -> B (New) */

 B -> B, W -> W (New) */

 output one line of scan and data bytes to the display */

 data bytes */

 scan line */

 data bytes */

		/*

		 * even scan line, but as lines on display are numbered from 1,

		 * line: 1,3,5,...

 data bytes */

		/*

		 * odd scan line, but as lines on display are numbered from 1,

		 * line: 0,2,4,6,...

 Output data to panel */

 repaper can't do partial updates */

 Clear display (anything -> white) */

 Assuming a clear (white) screen output an image */

	/*

	 * An extra frame write is needed if pixels are set in the bottom line,

	 * or else grey lines rises up from the pixels

 Turn off power and all signals */

 Ensure SPI MOSI and CLOCK are Low before CS Low */

 Discharge pulse */

 Power up sequence */

	/*

	 * This delay comes from the repaper.org userspace driver, it's not

	 * mentioned in the datasheet.

 Wait for COG to become ready */

 Disable OE */

 Power saving mode */

 Channel select */

 High power mode osc */

 Power setting */

 Vcom level */

 Power setting */

 Driver latch on */

 Driver latch off */

 Start chargepump */

 Charge pump positive voltage on - VGH/VDL on */

 Charge pump negative voltage on - VGL/VDL on */

 Charge pump Vcom on - Vcom driver on */

 check DC/DC */

	/*

	 * Output enable to disable

	 * The userspace driver sets this to 0x04, but the datasheet says 0x06

	/*

	 * This callback is not protected by drm_dev_enter/exit since we want to

	 * turn off the display on regular driver unload. It's highly unlikely

	 * that the underlying SPI controller is gone should this be called after

	 * unplug.

 Nothing frame */

 2.7" */

 Dummy line */

 Border dummy line */

 not described in datasheet */

 Latch reset turn on */

 Power off charge pump Vcom */

 Power off charge pump neg voltage */

 Discharge internal */

 turn off all charge pumps */

 Turn off osc */

 The SPI device is used to allocate dma memory */

 data-scan-data */

 scan-data-scan */

 data-scan-data */

 data-scan-data */

 SPDX-License-Identifier: GPL-2.0+

/*

 * DRM driver for Ilitek ILI9486 panels

 *

 * Copyright 2020 Kamlesh Gurudasani <kamlesh.gurudasani@gmail.com>

/*

 * The PiScreen/waveshare rpi-lcd-35 has a SPI to 16-bit parallel bus converter

 * in front of the  display controller. This means that 8-bit values have to be

 * transferred as 16-bit.

	/*

	 * The displays are Raspberry Pi HATs and connected to the 8-bit only

	 * SPI controller, so 16-bit command and parameters need byte swapping

	 * before being transferred as 8-bit on the big endian SPI bus.

	 * Pixel data bytes have already been swapped before this function is

	 * called.

 8-bit configuration data, not 16-bit pixel data */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright 2019 Hans de Goede <hdegoede@redhat.com>

/*

 * The DLP has an actual width of 854 pixels, but that is not a multiple

 * of 8, breaking things left and right, so we export a width of 848.

 Windows driver does once every second, with arg d = 1, other args 0 */

 Windows driver does this on init, with arg a, b = 0, c = 0xa0, d = 4 */

 Send request */

 Read value */

 cmd_buf[0] now contains the read value, which we don't use */

 Read status */

 TODO: Use mapping abstraction properly */

 Send data command to device */

 Send data block to device */

 Read status */

 Send draw command to device */

 Read status */

	/*

	 * We must draw a frame every 2s otherwise the projector

	 * switches back to showing its logo.

 Do not log errors caused by module unload or device unplug */

 set */,

 ------------------------------------------------------------------ */

 gm12u320 connector						      */

/*

 *Â We use fake EDID info so that userspace know that it is dealing with

 * an Acer projector, rather then listing this as an "unknown" monitor.

 * Note this assumes this driver is only ever used with the Acer C120, if we

 * add support for other devices the vendor and model should be parameterized.

 "ACR" */

 C120h */

 EDID 1.3 */

 EDID 1.3 */

 Analog input */

 Pref timing in DTD 1 */

 hactive = 848, hblank = 256 */

 vactive = 480, vblank = 28 */

 hsync offset 40 pw 128, vsync offset 1 pw 4 */

 Digital separate syncs, hsync+, vsync+ */

 Monitor ranges */

 40 MHz */

 Model string */

 Unspecified text / padding */

 ------------------------------------------------------------------ */

 gm12u320 (simple) display pipe				      */

/*

 * FIXME: Dma-buf sharing requires DMA support by the importing device.

 *        This function is a workaround to make USB devices work as well.

 *        See todo.rst for how to fix the issue in the dma-buf framework.

	/*

	 * The gm12u320 presents itself to the system as 2 usb mass-storage

	 * interfaces, we only care about / need the first one.

 not an error */

 SPDX-License-Identifier: GPL-2.0 */

/*

 * Copyright 2012-2019 Red Hat

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License version 2. See the file COPYING in the main

 * directory of this archive for more details.

 *

 * Authors: Matthew Garrett

 *	    Dave Airlie

 *	    Gerd Hoffmann

 *

 * Portions of this code derived from cirrusfb.c:

 * drivers/video/cirrusfb.c - driver for Cirrus Logic chipsets

 *

 * Copyright 1999-2001 Jeff Garzik <jgarzik@pobox.com>

 (4096 - 1) & ~111b bytes */

 4 MB */

 ------------------------------------------------------------------ */

/*

 * The meat of this driver. The core passes us a mode and we have to program

 * it. The modesetting here is the bare minimum required to satisfy the qemu

 * emulation of this hardware, and running this against a real device is

 * likely to result in an inadequately programmed mode. We've already had

 * the opportunity to modify the mode, so whatever we receive here should

 * be something that can be correctly programmed and displayed

 convert from XR24 to RG24 */

 convert from XR24 to RG16 */

	/*

	 * Overflow bits for values that don't fit in the standard registers

 More overflow bits */

 Disable Hercules/CGA compatibility */

 Program the pitch */

 Enable extended blanking and pitch bits, and enable full memory */

 Enable high-colour modes */

 And set graphics mode */

 Unblank (needed on S3 resume, vgabios doesn't do it then) */

 TODO: Use mapping abstraction properly */

 ------------------------------------------------------------------ */

 cirrus connector						      */

 ------------------------------------------------------------------ */

 cirrus (simple) display pipe					      */

 ------------------------------------------------------------------ */

 cirrus framebuffers & mode config				      */

 ------------------------------------------------------------------ */

 only bind to the cirrus chip in qemu */

 end if list */ }

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * DRM driver for Sitronix ST7586 panels

 *

 * Copyright 2017 David Lechner <david@lechnology.com>

 controller-specific commands */

/*

 * The ST7586 controller has an unusual pixel format where 2bpp grayscale is

 * packed 3 pixels per byte with the first two pixels using 3 bits and the 3rd

 * pixel using only 2 bits.

 *

 * |  D7  |  D6  |  D5  ||      |      || 2bpp |

 * | (D4) | (D3) | (D2) ||  D1  |  D0  || GRAY |

 * +------+------+------++------+------++------+

 * |  1   |  1   |  1   ||  1   |  1   || 0  0 | black

 * |  1   |  0   |  0   ||  1   |  0   || 0  1 | dark gray

 * |  0   |  1   |  0   ||  0   |  1   || 1  0 | light gray

 * |  0   |  0   |  0   ||  0   |  0   || 1  1 | white

 3 pixels per byte, so grow clip to nearest multiple of 3 */

 Pixels are packed 3 per byte */

	/*

	 * This callback is not protected by drm_dev_enter/exit since we want to

	 * turn off the display on regular driver unload. It's highly unlikely

	 * that the underlying SPI controller is gone should this be called after

	 * unplug.

 Cannot read from this controller via SPI */

	/*

	 * we are using 8-bit data, so we are not actually swapping anything,

	 * but setting mipi->swap_bytes makes mipi_dbi_typec3_command() do the

	 * right thing and not use 16-bit transfers (which results in swapped

	 * bytes on little-endian systems and causes out of order data to be

	 * sent to the display).

 SPDX-License-Identifier: GPL-2.0+

/*

 * DRM driver for Ilitek ILI9341 panels

 *

 * Copyright 2018 David Lechner <david@lechnology.com>

 *

 * Based on mi0283qt.c:

 * Copyright 2016 Noralf TrÃ¸nnes

 Power Control */

 VCOM */

 Memory Access Control */

 Frame Rate */

 Gamma */

 DDRAM */

 Display */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * DRM driver for Multi-Inno MI0283QT panels

 *

 * Copyright 2016 Noralf TrÃ¸nnes

 Power Control */

 VCOM */

 Memory Access Control */

 Frame Rate */

 Gamma */

 DDRAM */

 Display */

	/* The PiTFT (ili9340) has a hardware reset circuit that

	 * resets only on power-on and not on each reboot through

	 * a gpio like the rpi-display does.

	 * As a result, we need to always apply the rotation value

	 * regardless of the display "on/off" state.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Assume a monitor resolution of 96 dpi to

 * get a somewhat reasonable screen size.

/*

 * Helpers for simplefb

/*

 * Simple Framebuffer device

 clocks */

 regulators */

 simplefb settings */

 memory management */

 modesetting */

/*

 * Hardware

/*

 * Clock handling code.

 *

 * Here we handle the clocks property of our "simple-framebuffer" dt node.

 * This is necessary so that we can make sure that any clocks needed by

 * the display engine that the bootloader set up for us (and for which it

 * provided a simplefb dt node), stay up, for the life of the simplefb

 * driver.

 *

 * When the driver unloads, we cleanly disable, and then release the clocks.

 *

 * We only complain about errors here, no action is taken as the most likely

 * error can only happen due to a mismatch between the bootloader which set

 * up simplefb, and the clock definitions in the device tree. Chances are

 * that there are no adverse effects, and if there are, a clean teardown of

 * the fb probe will not help us much either. So just complain and carry on,

 * and hope that the user actually gets a working fb at the end of things.

/*

 * Regulator handling code.

 *

 * Here we handle the num-supplies and vin*-supply properties of our

 * "simple-framebuffer" dt node. This is necessary so that we can make sure

 * that any regulators needed by the display hardware that the bootloader

 * set up for us (and for which it provided a simplefb dt node), stay up,

 * for the life of the simplefb driver.

 *

 * When the driver unloads, we cleanly disable, and then release the

 * regulators.

 *

 * We only complain about errors here, no action is taken as the most likely

 * error can only happen due to a mismatch between the bootloader which set

 * up simplefb, and the regulator definitions in the device tree. Chances are

 * that there are no adverse effects, and if there are, a clean teardown of

 * the fb probe will not help us much either. So just complain and carry on,

 * and hope that the user actually gets a working fb at the end of things.

 Count the number of regulator supplies */

 32 is max size of property name */

/*

 *  Simplefb settings

 Hz */ * mode.hdisplay * mode.vdisplay;

/*

 * Memory management

/*

 * Modesetting

/*

 * Support all formats of simplefb and maybe more; in order

 * of preference. The display's update function will do any

 * conversion necessary.

 *

 * TODO: Add blit helpers for remaining formats and uncomment

 *       constants.

DRM_FORMAT_XRGB1555,

DRM_FORMAT_ARGB1555,

DRM_FORMAT_XRGB2101010,

DRM_FORMAT_ARGB2101010,

 TODO: Use mapping abstraction */

 Clear screen to black if disabled */

 TODO: Use mapping abstraction */

 don't rebuild list on recurring calls */

 native format goes first */

 default formats go second */

 native format already went first */

	/*

	 * TODO: The simpledrm driver converts framebuffers to the native

	 * format when copying them to device memory. If there are more

	 * formats listed than supported by the driver, the native format

	 * is not supported by the conversion helpers. Therefore *only*

	 * support the native format and add a conversion helper ASAP.

/*

 * Init / Cleanup

/*

 * DRM driver

/*

 * Platform driver

 connect to sysfb */

 SPDX-License-Identifier: GPL-2.0+

/*

 * DRM driver for the HX8357D LCD controller

 *

 * Copyright 2018 Broadcom

 * Copyright 2018 David Lechner <david@lechnology.com>

 * Copyright 2016 Noralf TrÃ¸nnes

 * Copyright (C) 2015 Adafruit Industries

 * Copyright (C) 2013 Christian Vogelgsang

 setextc */

 setRGB which also enables SDO */

 -1.52V */

 Normal mode 70Hz, Idle mode 55 Hz */

 Set Panel - BGR, Gate direction swapped */

 Not deep standby */

 BT */

 VSPR */

 VSNR */

 AP */

 FS */

 OPON normal */

 OPON idle */

 STBA */

 STBA */

 STBA */

 GEN */

 NW 0x02 */

 RTN */

 DIV */

 DUM */

 DUM */

 GDON */

 GDOFF */

 16 bit */

 TE off */

 tear line */

 Exit Sleep */

 display on */

/*

 * Copyright 2015 Advanced Micro Devices, Inc.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included in

 * all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR

 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,

 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR

 * OTHER DEALINGS IN THE SOFTWARE.

 *

/**

 * drm_sched_entity_init - Init a context entity used by scheduler when

 * submit to HW ring.

 *

 * @entity: scheduler entity to init

 * @priority: priority of the entity

 * @sched_list: the list of drm scheds on which jobs from this

 *           entity can be submitted

 * @num_sched_list: number of drm sched in sched_list

 * @guilty: atomic_t set to 1 when a job on this queue

 *          is found to be guilty causing a timeout

 *

 * Note that the &sched_list must have at least one element to schedule the entity.

 *

 * For changing @priority later on at runtime see

 * drm_sched_entity_set_priority(). For changing the set of schedulers

 * @sched_list at runtime see drm_sched_entity_modify_sched().

 *

 * An entity is cleaned up by callind drm_sched_entity_fini(). See also

 * drm_sched_entity_destroy().

 *

 * Returns 0 on success or a negative error code on failure.

 We start in an idle state. */

/**

 * drm_sched_entity_modify_sched - Modify sched of an entity

 * @entity: scheduler entity to init

 * @sched_list: the list of new drm scheds which will replace

 *		 existing entity->sched_list

 * @num_sched_list: number of drm sched in sched_list

 *

 * Note that this must be called under the same common lock for @entity as

 * drm_sched_job_arm() and drm_sched_entity_push_job(), or the driver needs to

 * guarantee through some other means that this is never called while new jobs

 * can be pushed to @entity.

 for list_empty to work without lock */

 Return true if entity could provide a job. */

/**

 * drm_sched_entity_flush - Flush a context entity

 *

 * @entity: scheduler entity

 * @timeout: time to wait in for Q to become empty in jiffies.

 *

 * Splitting drm_sched_entity_fini() into two functions, The first one does the

 * waiting, removes the entity from the runqueue and returns an error when the

 * process was killed.

 *

 * Returns the remaining time in jiffies left from the input timeout

	/**

	 * The client will not queue more IBs during this fini, consume existing

	 * queued IBs or discard them on SIGKILL

 For killed process disable any more IBs enqueue right now */

 Signal the scheduler finished fence when the entity in question is killed. */

 Wait for all dependencies to avoid data corruptions */

		/*

		 * When pipe is hanged by older entity, new entity might

		 * not even have chance to submit it's first job to HW

		 * and so entity->last_scheduled will remain NULL

/**

 * drm_sched_entity_fini - Destroy a context entity

 *

 * @entity: scheduler entity

 *

 * Cleanups up @entity which has been initialized by drm_sched_entity_init().

 *

 * If there are potentially job still in flight or getting newly queued

 * drm_sched_entity_flush() must be called first. This function then goes over

 * the entity and signals all jobs with an error code if the process was killed.

	/* Consumption of existing IBs wasn't completed. Forcefully

	 * remove them here.

			/*

			 * Wait for thread to idle to make sure it isn't processing

			 * this entity.

/**

 * drm_sched_entity_destroy - Destroy a context entity

 * @entity: scheduler entity

 *

 * Calls drm_sched_entity_flush() and drm_sched_entity_fini() as a

 * convenience wrapper.

 drm_sched_entity_clear_dep - callback to clear the entities dependency */

/*

 * drm_sched_entity_clear_dep - callback to clear the entities dependency and

 * wake up scheduler

/**

 * drm_sched_entity_set_priority - Sets priority of the entity

 *

 * @entity: scheduler entity

 * @priority: scheduler priority

 *

 * Update the priority of runqueus used for the entity.

/*

 * Add a callback to the current dependency of the entity to wake up the

 * scheduler when the entity becomes available.

		/*

		 * Fence is a scheduled/finished fence from a job

		 * which belongs to the same entity, we can ignore

		 * fences from ourself

		/*

		 * Fence is from the same scheduler, only need to wait for

		 * it to be scheduled

 Ignore it when it is already scheduled */

 skip jobs from entity that marked guilty */

	/*

	 * If the queue is empty we allow drm_sched_entity_select_rq() to

	 * locklessly access ->last_scheduled. This only works if we set the

	 * pointer before we dequeue and if we a write barrier here.

 single possible engine and already selected */

 queue non-empty, stay on the same engine */

	/*

	 * Only when the queue is empty are we guaranteed that the scheduler

	 * thread cannot change ->last_scheduled. To enforce ordering we need

	 * a read barrier here. See drm_sched_entity_pop_job() for the other

	 * side.

 stay on the same engine if the previous job hasn't finished */

/**

 * drm_sched_entity_push_job - Submit a job to the entity's job queue

 * @sched_job: job to submit

 *

 * Note: To guarantee that the order of insertion to queue matches the job's

 * fence sequence number this function should be called with drm_sched_job_arm()

 * under common lock for the struct drm_sched_entity that was set up for

 * @sched_job in drm_sched_job_init().

 *

 * Returns 0 for success, negative error code otherwise.

 first job wakes up scheduler */

 Add the entity to the run queue */

/*

 * Copyright 2015 Advanced Micro Devices, Inc.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included in

 * all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR

 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,

 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR

 * OTHER DEALINGS IN THE SOFTWARE.

 *

/**

 * DOC: Overview

 *

 * The GPU scheduler provides entities which allow userspace to push jobs

 * into software queues which are then scheduled on a hardware run queue.

 * The software queues have a priority among them. The scheduler selects the entities

 * from the run queue using a FIFO. The scheduler provides dependency handling

 * features among jobs. The driver is supposed to provide callback functions for

 * backend operations to the scheduler like submitting a job to hardware run queue,

 * returning the dependencies of a job etc.

 *

 * The organisation of the scheduler is the following:

 *

 * 1. Each hw run queue has one scheduler

 * 2. Each scheduler has multiple run queues with different priorities

 *    (e.g., HIGH_HW,HIGH_SW, KERNEL, NORMAL)

 * 3. Each scheduler run queue has a queue of entities to schedule

 * 4. Entities themselves maintain a queue of jobs that will be scheduled on

 *    the hardware.

 *

 * The jobs in a entity are always scheduled in the order that they were pushed.

/**

 * drm_sched_rq_init - initialize a given run queue struct

 *

 * @sched: scheduler instance to associate with this run queue

 * @rq: scheduler run queue

 *

 * Initializes a scheduler runqueue.

/**

 * drm_sched_rq_add_entity - add an entity

 *

 * @rq: scheduler run queue

 * @entity: scheduler entity

 *

 * Adds a scheduler entity to the run queue.

/**

 * drm_sched_rq_remove_entity - remove an entity

 *

 * @rq: scheduler run queue

 * @entity: scheduler entity

 *

 * Removes a scheduler entity from the run queue.

/**

 * drm_sched_rq_select_entity - Select an entity which could provide a job to run

 *

 * @rq: scheduler run queue to check.

 *

 * Try to find a ready entity, returns NULL if none found.

/**

 * drm_sched_job_done - complete a job

 * @s_job: pointer to the job which is done

 *

 * Finish the job's fence and wake up the worker thread.

/**

 * drm_sched_job_done_cb - the callback for a done job

 * @f: fence

 * @cb: fence callbacks

/**

 * drm_sched_dependency_optimized

 *

 * @fence: the dependency fence

 * @entity: the entity which depends on the above fence

 *

 * Returns true if the dependency can be optimized and false otherwise

/**

 * drm_sched_start_timeout - start timeout for reset worker

 *

 * @sched: scheduler instance to start the worker for

 *

 * Start the timeout for the given scheduler.

/**

 * drm_sched_fault - immediately start timeout handler

 *

 * @sched: scheduler where the timeout handling should be started.

 *

 * Start timeout handling immediately when the driver detects a hardware fault.

/**

 * drm_sched_suspend_timeout - Suspend scheduler job timeout

 *

 * @sched: scheduler instance for which to suspend the timeout

 *

 * Suspend the delayed work timeout for the scheduler. This is done by

 * modifying the delayed work timeout to an arbitrary large value,

 * MAX_SCHEDULE_TIMEOUT in this case.

 *

 * Returns the timeout remaining

 *

	/*

	 * Modify the timeout to an arbitrarily large value. This also prevents

	 * the timeout to be restarted when new submissions arrive

/**

 * drm_sched_resume_timeout - Resume scheduler job timeout

 *

 * @sched: scheduler instance for which to resume the timeout

 * @remaining: remaining timeout

 *

 * Resume the delayed work timeout for the scheduler.

 Protects against concurrent deletion in drm_sched_get_cleanup_job */

		/*

		 * Remove the bad job so it cannot be freed by concurrent

		 * drm_sched_cleanup_jobs. It will be reinserted back after sched->thread

		 * is parked at which point it's safe.

		/*

		 * Guilty job did complete and hence needs to be manually removed

		 * See drm_sched_stop doc.

 /**

  * drm_sched_increase_karma - Update sched_entity guilty flag

  *

  * @bad: The job guilty of time out

  *

  * Increment on every hang caused by the 'bad' job. If this exceeds the hang

  * limit of the scheduler then the respective sched entity is marked guilty and

  * jobs from it will not be scheduled further

/**

 * drm_sched_stop - stop the scheduler

 *

 * @sched: scheduler instance

 * @bad: job which caused the time out

 *

 * Stop the scheduler and also removes and frees all completed jobs.

 * Note: bad job will not be freed as it might be used later and so it's

 * callers responsibility to release it manually if it's not part of the

 * pending list any more.

 *

	/*

	 * Reinsert back the bad job here - now it's safe as

	 * drm_sched_get_cleanup_job cannot race against us and release the

	 * bad job at this point - we parked (waited for) any in progress

	 * (earlier) cleanups and drm_sched_get_cleanup_job will not be called

	 * now until the scheduler thread is unparked.

		/*

		 * Add at the head of the queue to reflect it was the earliest

		 * job extracted.

	/*

	 * Iterate the job list from later to  earlier one and either deactive

	 * their HW callbacks or remove them from pending list if they already

	 * signaled.

	 * This iteration is thread safe as sched thread is stopped.

			/*

			 * remove job from pending_list.

			 * Locking here is for concurrent resume timeout

			/*

			 * Wait for job's HW fence callback to finish using s_job

			 * before releasing it.

			 *

			 * Job is still alive so fence refcount at least 1

			/*

			 * We must keep bad job alive for later use during

			 * recovery by some of the drivers but leave a hint

			 * that the guilty job must be released.

	/*

	 * Stop pending timer in flight as we rearm it in  drm_sched_start. This

	 * avoids the pending timeout work in progress to fire right away after

	 * this TDR finished and before the newly restarted jobs had a

	 * chance to complete.

/**

 * drm_sched_start - recover jobs after a reset

 *

 * @sched: scheduler instance

 * @full_recovery: proceed with complete sched restart

 *

	/*

	 * Locking the list is not required here as the sched thread is parked

	 * so no new jobs are being inserted or removed. Also concurrent

	 * GPU recovers can't run in parallel.

/**

 * drm_sched_resubmit_jobs - helper to relaunch jobs from the pending list

 *

 * @sched: scheduler instance

 *

/**

 * drm_sched_resubmit_jobs_ext - helper to relunch certain number of jobs from mirror ring list

 *

 * @sched: scheduler instance

 * @max: job numbers to relaunch

 *

/**

 * drm_sched_job_init - init a scheduler job

 * @job: scheduler job to init

 * @entity: scheduler entity to use

 * @owner: job owner for debugging

 *

 * Refer to drm_sched_entity_push_job() documentation

 * for locking considerations.

 *

 * Drivers must make sure drm_sched_job_cleanup() if this function returns

 * successfully, even when @job is aborted before drm_sched_job_arm() is called.

 *

 * WARNING: amdgpu abuses &drm_sched.ready to signal when the hardware

 * has died, which can mean that there's no valid runqueue for a @entity.

 * This function returns -ENOENT in this case (which probably should be -EIO as

 * a more meanigful return value).

 *

 * Returns 0 for success, negative error code otherwise.

/**

 * drm_sched_job_arm - arm a scheduler job for execution

 * @job: scheduler job to arm

 *

 * This arms a scheduler job for execution. Specifically it initializes the

 * &drm_sched_job.s_fence of @job, so that it can be attached to struct dma_resv

 * or other places that need to track the completion of this job.

 *

 * Refer to drm_sched_entity_push_job() documentation for locking

 * considerations.

 *

 * This can only be called if drm_sched_job_init() succeeded.

/**

 * drm_sched_job_add_dependency - adds the fence as a job dependency

 * @job: scheduler job to add the dependencies to

 * @fence: the dma_fence to add to the list of dependencies.

 *

 * Note that @fence is consumed in both the success and error cases.

 *

 * Returns:

 * 0 on success, or an error on failing to expand the array.

	/* Deduplicate if we already depend on a fence from the same context.

	 * This lets the size of the array of deps scale with the number of

	 * engines involved, rather than the number of BOs.

/**

 * drm_sched_job_add_implicit_dependencies - adds implicit dependencies as job

 *   dependencies

 * @job: scheduler job to add the dependencies to

 * @obj: the gem object to add new dependencies from.

 * @write: whether the job might write the object (so we need to depend on

 * shared fences in the reservation object).

 *

 * This should be called after drm_gem_lock_reservations() on your array of

 * GEM objects used in the job but before updating the reservations with your

 * own fences.

 *

 * Returns:

 * 0 on success, or an error on failing to expand the array.

 Make sure to grab an additional ref on the added fence */

/**

 * drm_sched_job_cleanup - clean up scheduler job resources

 * @job: scheduler job to clean up

 *

 * Cleans up the resources allocated with drm_sched_job_init().

 *

 * Drivers should call this from their error unwind code if @job is aborted

 * before drm_sched_job_arm() is called.

 *

 * After that point of no return @job is committed to be executed by the

 * scheduler, and this function should be called from the

 * &drm_sched_backend_ops.free_job callback.

 drm_sched_job_arm() has been called */

 aborted job before committing to run it */

/**

 * drm_sched_ready - is the scheduler ready

 *

 * @sched: scheduler instance

 *

 * Return true if we can push more jobs to the hw, otherwise false.

/**

 * drm_sched_wakeup - Wake up the scheduler when it is ready

 *

 * @sched: scheduler instance

 *

/**

 * drm_sched_select_entity - Select next entity to process

 *

 * @sched: scheduler instance

 *

 * Returns the entity to process or NULL if none are found.

 Kernel run queue has higher priority than normal run queue*/

/**

 * drm_sched_get_cleanup_job - fetch the next finished job to be destroyed

 *

 * @sched: scheduler instance

 *

 * Returns the next finished job from the pending list (if there is one)

 * ready for it to be destroyed.

 remove job from pending_list */

 cancel this job's TO timer */

 make the scheduled timestamp more accurate */

 start TO timer for next job */

/**

 * drm_sched_pick_best - Get a drm sched from a sched_list with the least load

 * @sched_list: list of drm_gpu_schedulers

 * @num_sched_list: number of drm_gpu_schedulers in the sched_list

 *

 * Returns pointer of the sched with the least load or NULL if none of the

 * drm_gpu_schedulers are ready

/**

 * drm_sched_blocked - check if the scheduler is blocked

 *

 * @sched: scheduler instance

 *

 * Returns true if blocked, otherwise false.

/**

 * drm_sched_main - main scheduler thread

 *

 * @param: scheduler instance

 *

 * Returns 0.

/**

 * drm_sched_init - Init a gpu scheduler instance

 *

 * @sched: scheduler instance

 * @ops: backend operations for this scheduler

 * @hw_submission: number of hw submissions that can be in flight

 * @hang_limit: number of times to allow a job to hang before dropping it

 * @timeout: timeout value in jiffies for the scheduler

 * @timeout_wq: workqueue to use for timeout work. If NULL, the system_wq is

 *		used

 * @score: optional score atomic shared with other schedulers

 * @name: name used for debugging

 *

 * Return 0 on success, otherwise error code.

 Each scheduler will run on a seperate kernel thread */

/**

 * drm_sched_fini - Destroy a gpu scheduler

 *

 * @sched: scheduler instance

 *

 * Tears down and cleans up the scheduler.

			/*

			 * Prevents reinsertion and marks job_queue as idle,

			 * it will removed from rq in drm_sched_entity_fini

			 * eventually

 Wakeup everyone stuck in drm_sched_entity_flush for this scheduler */

 Confirm no work left behind accessing device structures */

/**

 * drm_sched_increase_karma_ext - Update sched_entity guilty flag

 *

 * @bad: The job guilty of time out

 * @type: type for increase/reset karma

 *

	/* don't change @bad's karma if it's from KERNEL RQ,

	 * because sometimes GPU hang would cause kernel jobs (like VM updating jobs)

	 * corrupt but keep in mind that kernel jobs always considered good.

/*

 * Copyright 2015 Advanced Micro Devices, Inc.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included in

 * all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR

 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,

 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR

 * OTHER DEALINGS IN THE SOFTWARE.

 *

/**

 * drm_sched_fence_free - free up an uninitialized fence

 *

 * @fence: fence to free

 *

 * Free up the fence memory. Should only be used if drm_sched_fence_init()

 * has not been called yet.

 This function should not be called if the fence has been initialized. */

/**

 * drm_sched_fence_release_scheduled - callback that fence can be freed

 *

 * @f: fence

 *

 * This function is called when the reference count becomes zero.

 * It just RCU schedules freeing up the fence.

/**

 * drm_sched_fence_release_finished - drop extra reference

 *

 * @f: fence

 *

 * Drop the extra reference from the scheduled fence to the base fence.

 SPDX-License-Identifier: GPL-2.0



 Ingenic JZ47xx KMS driver



 Copyright (C) 2019, Paul Cercueil <paul@crapouillou.net>

	/*

	 * f1 (aka. foreground1) is our primary plane, on top of which

	 * f0 (aka. foreground0) can be overlayed. Z-order is fixed in

	 * hardware and cannot be changed.

	/*

	 * clk_mutex is used to synchronize the pixel clock rate update with

	 * the VBLANK. When the pixel clock's parent clock needs to be updated,

	 * clock_nb's notifier function will lock the mutex, then wait until the

	 * next VBLANK. At that point, the parent clock's rate can be updated,

	 * and the mutex is then unlocked. If an atomic commit happens in the

	 * meantime, it will lock on the mutex, effectively waiting until the

	 * clock update process finishes. Finally, the pixel clock's rate will

	 * be recomputed when the mutex has been released, in the pending atomic

	 * commit, or a future one.

	/*

	 * IPU restart - specify how much time the LCDC will wait before

	 * transferring a new frame from the IPU. The value is the one

	 * suggested in the programming manual.

 IPU and F1 planes cannot be enabled at the same time. */

 If all the planes are disabled, we won't get a VBLANK IRQ */

		/*

		 * If IPU plane is enabled, enable IPU as source for the F1

		 * plane; otherwise use regular DMA.

	/*

	 * If OSD is not available, check that the width/height match.

	 * Note that state->src_* are in 16.16 fixed-point format.

	/*

	 * Require full modeset if enabling or disabling a plane, or changing

	 * its position, size or depth.

		/*

		 * The LCD controller expects timing values in dot-clock ticks,

		 * which is 3x the timing values in pixels when using a 3x8-bit

		 * display; but it will count the display area size in pixels

		 * either way. Go figure.

 Configure DMA hwdesc for foreground0 plane */

 Configure DMA hwdesc for foreground1 plane */

 Configure DMA hwdesc for palette */

 we're done */

		/* LCD Device clock must be 3x the pixel clock for STN panels,

		 * or 1.5x the pixel clock for TFT panels. To avoid having to

		 * check for the LCD device clock everytime we do a mode change,

		 * we set the LCD device clock to the highest rate possible.

 Set address of our DMA descriptor chain */

 Enable OSD if available */

 IPU is at port address 8 */

 JZ4740 has only one plane */

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0



 Ingenic JZ47xx IPU driver



 Copyright (C) 2020, Paul Cercueil <paul@crapouillou.net>

 Copyright (C) 2020, Daniel Silsby <dansilsby@gmail.com>

 Signed 15.16 fixed-point math (for bicubic scaling coefficients) */

/*

 * Apply conventional cubic convolution kernel. Both parameters

 *  and return value are 15.16 signed fixed-point.

 *

 *  @f_a: Sharpness factor, typically in range [-4.0, -0.25].

 *        A larger magnitude increases perceived sharpness, but going past

 *        -2.0 might cause ringing artifacts to outweigh any improvement.

 *        Nice values on a 320x240 LCD are between -0.75 and -2.0.

 *

 *  @f_x: Absolute distance in pixels from 'pixel 0' sample position

 *        along horizontal (or vertical) source axis. Range is [0, +2.0].

 *

 *  returns: Weight of this pixel within 4-pixel sample group. Range is

 *           [-2.0, +2.0]. For moderate (i.e. > -3.0) sharpness factors,

 *           range is within [-1.0, +1.0].

/*

 * On entry, "weight" is a coefficient suitable for bilinear mode,

 *  which is converted to a set of four suitable for bicubic mode.

 *

 * "weight 512" means all of pixel 0;

 * "weight 256" means half of pixel 0 and half of pixel 1;

 * "weight 0" means all of pixel 1;

 *

 * "offset" is increment to next source pixel sample location.

 Pixel weights at X (or Y) offsets -1,0,1,2 */

		/*

		 *  When sharpness setting is 0, emulate nearest-neighbor.

		 *  When sharpness setting is 1, emulate bilinear.

 Round up 0.5 */

		/*

		 * Note that always rounding towards +infinity here is intended.

		 * The resulting coefficients match a round-to-nearest-int

		 * double floating-point implementation.

	/*

	 * Force nearest-neighbor scaling and use simple math when upscaling

	 * by an integer ratio. It looks better, and fixes a few problem cases.

 Begin programming the LUT */

 The scaling table has only 31 entries */

 Reset all the registers if needed */

 Enable the chip */

 New addresses will be committed in vblank handler... */

 Or right here if we're doing a full modeset. */

 Set the input height/width/strides */

 Fix output to RGB888 */

 Set pixel format */

 Set the output height/width/stride */

		/*

		 * Offsets for Chroma/Luma.

		 * y = source Y - LUMA,

		 * u = source Cb - CHROMA,

		 * v = source Cr - CHROMA

		/*

		 * YUV422 to RGB conversion table.

		 * R = C0 / 0x400 * y + C1 / 0x400 * v

		 * G = C0 / 0x400 * y - C2 / 0x400 * u - C3 / 0x400 * v

		 * B = C0 / 0x400 * y + C4 / 0x400 * u

	/*

	 * Must set ZOOM_SEL before programming bicubic LUTs.

	 * If the IPU supports bicubic, we enable it unconditionally, since it

	 * can do anything bilinear can and more.

 Set the LUT index register */

 Clear STATUS register */

 Start IPU */

 Request a full modeset if we are enabling or disabling the IPU. */

 Plane must be fully visible */

 Minimum size is 4x4 */

 Input and output lines must have an even number of pixels. */

	/*

	 * Increase the scaled image's theorical width/height until we find a

	 * configuration that has valid scaling coefficients, up to 102% of the

	 * screen's resolution. This makes sure that we can scale from almost

	 * every resolution possible at the cost of a very small distorsion.

	 * The CRTC_W / CRTC_H are not modified.

 dummy read allows CPU to reconfigure IPU */

 ACK interrupt */

 Set previously cached addresses */

 Run IPU for the new frame */

	/*

	 * Sharpness settings range is [0,32]

	 * 0       : nearest-neighbor

	 * 1       : bilinear

	 * 2 .. 32 : bicubic (translated to sharpness factor -0.25 .. -4.0)

 Default sharpness factor: -0.125 * 8 = -1.0 */

	/*

	 * While officially supported, packed YUV 4:2:2 formats can cause

	 * random hardware crashes on JZ4725B under certain circumstances.

	 * It seems to happen with some specific resize ratios.

	 * Until a proper workaround or fix is found, disable these formats.

	DRM_FORMAT_YUYV,

	DRM_FORMAT_YVYU,

	DRM_FORMAT_UYVY,

	DRM_FORMAT_VYUY,

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0

/*

 * rcar_lvds.c  --  R-Car LVDS Encoder

 *

 * Copyright (C) 2013-2018 Renesas Electronics Corporation

 *

 * Contact: Laurent Pinchart (laurent.pinchart@ideasonboard.com)

 Keep in sync with the LVDCR0.LVMD hardware register values. */

 LVDS lanes 1 and 3 inverted */

 LVEN bit needs to be set on R8A77970/R8A7799x */

 PWD bit available (all of Gen3 but E3) */

 Has extended PLL */

 Supports dual-link operation */

 CPG module clock */

 External clock */

 External DU clocks */

/* -----------------------------------------------------------------------------

 * PLL Setup

	/*

	 * The LVDS PLL is made of a pre-divider and a multiplier (strangely

	 * enough called M and N respectively), followed by a post-divider E.

	 *

	 *         ,-----.         ,-----.     ,-----.         ,-----.

	 * Fin --> | 1/M | -Fpdf-> | PFD | --> | VCO | -Fvco-> | 1/E | --> Fout

	 *         `-----'     ,-> |     |     `-----'   |     `-----'

	 *                     |   `-----'               |

	 *                     |         ,-----.         |

	 *                     `-------- | 1/N | <-------'

	 *                               `-----'

	 *

	 * The clock output by the PLL is then further divided by a programmable

	 * divider DIV to achieve the desired target frequency. Finally, an

	 * optional fixed /7 divider is used to convert the bit clock to a pixel

	 * clock (as LVDS transmits 7 bits per lane per clock sample).

	 *

	 *          ,-------.     ,-----.     |\

	 * Fout --> | 1/DIV | --> | 1/7 | --> | |

	 *          `-------'  |  `-----'     | | --> dot clock

	 *                     `------------> | |

	 *                                    |/

	 *

	 * The /7 divider is optional, it is enabled when the LVDS PLL is used

	 * to drive the LVDS encoder, and disabled when  used to generate a dot

	 * clock for the DU RGB output, without using the LVDS encoder.

	 *

	 * The PLL allowed input frequency range is 12 MHz to 192 MHz.

	/*

	 * The comparison frequency range is 12 MHz to 24 MHz, which limits the

	 * allowed values for the pre-divider M (normal range 1-8).

	 *

	 * Fpfd = Fin / M

		/*

		 * The VCO operating range is 900 Mhz to 1800 MHz, which limits

		 * the allowed values for the multiplier N (normal range

		 * 60-120).

		 *

		 * Fvco = Fin * N / M

			/*

			 * The output frequency is limited to 1039.5 MHz,

			 * limiting again the allowed values for the

			 * post-divider E (normal value 1, 2 or 4).

			 *

			 * Fout = Fvco / E

				/*

				 * Finally we have a programable divider after

				 * the PLL, followed by a an optional fixed /7

				 * divider.

		/*

		 * The DIVRESET bit is a misnomer, setting it to 1 deasserts the

		 * divisor reset.

/* -----------------------------------------------------------------------------

 * Clock - D3/E3 only

/* -----------------------------------------------------------------------------

 * Bridge

	/*

	 * There is no API yet to retrieve LVDS mode from a bridge, only panels

	 * are supported.

 Enable the companion LVDS encoder in dual-link mode. */

	/*

	 * Hardcode the channels and control signals routing for now.

	 *

	 * HSYNC -> CTRL0

	 * VSYNC -> CTRL1

	 * DISP  -> CTRL2

	 * 0     -> CTRL3

			/*

			 * By default we generate even pixels from the primary

			 * encoder and odd pixels from the companion encoder.

			 * Swap pixels around if the sink requires odd pixels

			 * from the primary encoder and even pixels from the

			 * companion encoder.

			/*

			 * Configure vertical stripe since we are dealing with

			 * an LVDS dual-link connection.

			 *

			 * ST_SWAP is reserved for the companion encoder, only

			 * set it in the primary encoder.

	/*

	 * PLL clock configuration on all instances but the companion in

	 * dual-link mode.

 Set the LVDS mode and select the input. */

 Turn all the channels on. */

 Enable LVDS operation and turn the bias circuitry on. */

		/*

		 * Turn the PLL on (simple PLL only, extended PLL is fully

		 * controlled through LVDPLLCR).

 Set LVDS normal mode. */

		/*

		 * Turn on the LVDS PHY. On D3, the LVEN and LVRES bit must be

		 * set at the same time, so don't write the register yet.

 Wait for the PLL startup delay (simple PLL only). */

 Turn the output on. */

 Disable the companion LVDS encoder in dual-link mode. */

	/*

	 * The internal LVDS encoder has a restricted clock frequency operating

	 * range, from 5MHz to 148.5MHz on D3 and E3, and from 31MHz to

	 * 148.5MHz on all other platforms. Clamp the clock accordingly.

/* -----------------------------------------------------------------------------

 * Probe & Remove

 Locate the companion LVDS encoder for dual-link operation, if any. */

	/*

	 * Sanity check: the companion encoder must have the same compatible

	 * string.

	/*

	 * We need to work out if the sink is expecting us to function in

	 * dual-link mode. We do this by looking at the DT port nodes we are

	 * connected to, if they are marked as expecting even pixels and

	 * odd pixels than we need to enable vertical stripe output.

		/*

		 * Early dual-link bridge specific implementations populate the

		 * timings field of drm_bridge. If the flag is set, we assume

		 * that we are expected to generate even pixels from the primary

		 * encoder, and odd pixels from the companion encoder.

	/*

	 * FIXME: We should not be messing with the companion encoder private

	 * data from the primary encoder, we should rather let the companion

	 * encoder work things out on its own. However, the companion encoder

	 * doesn't hold a reference to the primary encoder, and

	 * drm_of_lvds_get_dual_link_pixel_order needs to be given references

	 * to the output ports of both encoders, therefore leave it like this

	 * for the time being.

	/*

	 * On D3/E3 the LVDS encoder provides a clock to the DU, which can be

	 * used for the DPAD output even when the LVDS output is not connected.

	 * Don't fail probe in that case as the DU will need the bridge to

	 * control the clock.

	/*

	 * LVDS encoders without an extended PLL have no external clock inputs.

 At least one input to the PLL must be available. */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0+

/*

 * rcar_du_crtc.c  --  R-Car Display Unit CRTCs

 *

 * Copyright (C) 2013-2015 Renesas Electronics Corporation

 *

 * Contact: Laurent Pinchart (laurent.pinchart@ideasonboard.com)

/* -----------------------------------------------------------------------------

 * Hardware Setup

	/*

	 *   fin                                 fvco        fout       fclkout

	 * in --> [1/M] --> |PD| -> [LPF] -> [VCO] -> [1/P] -+-> [1/FDPLL] -> out

	 *              +-> |  |                             |

	 *              |                                    |

	 *              +---------------- [1/N] <------------+

	 *

	 *	fclkout = fvco / P / FDPLL -- (1)

	 *

	 * fin/M = fvco/P/N

	 *

	 *	fvco = fin * P *  N / M -- (2)

	 *

	 * (1) + (2) indicates

	 *

	 *	fclkout = fin * N / M / FDPLL

	 *

	 * NOTES

	 *	N	: (n + 1)

	 *	M	: (m + 1)

	 *	FDPLL	: (fdpll + 1)

	 *	P	: 2

	 *	2kHz < fvco < 4096MHz

	 *

	 * To minimize the jitter,

	 * N : as large as possible

	 * M : as small as possible

			/*

			 * This code only runs on 64-bit architectures, the

			 * unsigned long type can thus be used for 64-bit

			 * computation. It will still compile without any

			 * warning on 32-bit architectures.

			 *

			 * To optimize calculations, use fout instead of fvco

			 * to verify the VCO frequency constraint.

	/*

	 * If the target rate has already been achieved perfectly we can't do

	 * better.

	/*

	 * Compute the input clock rate and internal divisor values to obtain

	 * the clock rate closest to the target frequency.

	/*

	 * Store the parameters if the resulting frequency is better than any

	 * previously calculated value.

 sentinel */ }

		/*

		 * DU channels that have a display PLL can't use the internal

		 * system clock, and have no internal clock divider.

		/*

		 * The H3 ES1.x exhibits dot clock duty cycle stability issues.

		 * We can work around them by configuring the DPLL to twice the

		 * desired frequency, coupled with a /2 post-divider. Restrict

		 * the workaround to H3 ES1.x as ES2.0 and all other SoCs have

		 * no post-divider when a display PLL is present (as shown by

		 * the workaround breaking HDMI output on M3-W during testing).

		/*

		 * Use the LVDS PLL output as the dot clock when outputting to

		 * the LVDS encoder on an SoC that supports this clock routing

		 * option. We use the clock directly in that case, without any

		 * additional divider.

 Signal polarities */

 Display timings */

 Insert the plane in the sorted planes array. */

 If VSP+DU integration is enabled the plane assignment is fixed. */

	/*

	 * Update the planes to display timing and dot clock generator

	 * associations.

	 *

	 * Updating the DPTSR register requires restarting the CRTC group,

	 * resulting in visible flicker. To mitigate the issue only update the

	 * association if needed by enabled planes. Planes being disabled will

	 * keep their current association.

 Restart the group if plane sources have changed. */

/* -----------------------------------------------------------------------------

 * Page Flip

/* -----------------------------------------------------------------------------

 * Color Management Module (CMM)

 We only accept fully populated LUT tables. */

/* -----------------------------------------------------------------------------

 * Start/Stop and Suspend/Resume

 Set display off and background to black */

 Configure display timings and output routing */

 Start with all planes disabled. */

 Enable the VSP compositor. */

 Turn vertical blanking interrupt reporting on. */

	/*

	 * Guard against double-get, as the function is called from both the

	 * .atomic_enable() and .atomic_begin() handlers.

	/*

	 * Select master sync mode. This enables display operation in master

	 * sync mode (with the HSYNC and VSYNC signals configured as outputs and

	 * actively driven).

 Make sure vblank interrupts are enabled. */

	/*

	 * Disable planes and calculate how many vertical blanking interrupts we

	 * have to wait for. If a vertical blanking interrupt has been triggered

	 * but not processed yet, we don't know whether it occurred before or

	 * after the planes got disabled. We thus have to wait for two vblank

	 * interrupts in that case.

	/*

	 * Disable all planes and wait for the change to take effect. This is

	 * required as the plane enable registers are updated on vblank, and no

	 * vblank will occur once the CRTC is stopped. Disabling planes when

	 * starting the CRTC thus wouldn't be enough as it would start scanning

	 * out immediately from old frame buffers until the next vblank.

	 *

	 * This increases the CRTC stop delay, especially when multiple CRTCs

	 * are stopped in one operation as we now wait for one vblank per CRTC.

	 * Whether this can be improved needs to be researched.

	/*

	 * Disable vertical blanking interrupt reporting. We first need to wait

	 * for page flip completion before stopping the CRTC as userspace

	 * expects page flips to eventually complete.

 Disable the VSP compositor. */

	/*

	 * Select switch sync mode. This stops display operation and configures

	 * the HSYNC and VSYNC signals as inputs.

	 *

	 * TODO: Find another way to stop the display for DUs that don't support

	 * TVM sync.

/* -----------------------------------------------------------------------------

 * CRTC Functions

 Store the routes from the CRTC output to the DU outputs. */

 Skip the writeback encoder. */

	/*

	 * On D3/E3 the dot clock is provided by the LVDS encoder attached to

	 * the DU channel. We need to enable its clock output explicitly if

	 * the LVDS output is disabled.

	/*

	 * TODO: The chip manual indicates that CMM tables should be written

	 * after the DU channel has been activated. Investigate the impact

	 * of this restriction on the first displayed frame.

		/*

		 * Disable the LVDS clock output, see

		 * rcar_du_crtc_atomic_enable().

	/*

	 * If a mode set is in progress we can be called with the CRTC disabled.

	 * We thus need to first get and setup the CRTC in order to configure

	 * planes. We must *not* put the CRTC in .atomic_flush(), as it must be

	 * kept awake until the .atomic_enable() call that will follow. The get

	 * operation in .atomic_enable() will in that case be a no-op, and the

	 * CRTC will be put later in .atomic_disable().

	 *

	 * If a mode set is not in progress the CRTC is enabled, and the

	 * following get call will be a no-op. There is thus no need to balance

	 * it in .atomic_flush() either.

 If the active state changed, we let .atomic_enable handle CMM. */

	/*

	 * The hardware requires a minimum combined horizontal sync and back

	 * porch of 20 pixels and a minimum vertical back porch of 3 lines.

 CRC available only on Gen3 HW. */

 Reserve 1 for "auto" source. */

	/*

	 * Parse the source name. Supported values are "plane%u" to compute the

	 * CRC on an input plane (%u is the plane ID), and "auto" to compute the

	 * CRC on the composer (VSP) output.

 Perform an atomic commit to set the CRC source. */

/* -----------------------------------------------------------------------------

 * Interrupt Handling

		/*

		 * Wake up the vblank wait if the counter reaches 0. This must

		 * be protected by the vblank_lock to avoid races in

		 * rcar_du_crtc_disable_planes().

/* -----------------------------------------------------------------------------

 * Initialization

 Get the CRTC clock and the optional external clock. */

		/*

		 * DU channels that have a display PLL can't use the internal

		 * system clock and thus require an external clock.

 CMM might be disabled for this CRTC. */

 Register the interrupt handler. */

 The IRQ's are associated with the CRTC (sw)index. */

 SPDX-License-Identifier: GPL-2.0+

/*

 * rcar_du_encoder.c  --  R-Car Display Unit Encoder

 *

 * Copyright (C) 2013-2014 Renesas Electronics Corporation

 *

 * Contact: Laurent Pinchart (laurent.pinchart@ideasonboard.com)

/* -----------------------------------------------------------------------------

 * Encoder

	/*

	 * Locate the DRM bridge from the DT node. For the DPAD outputs, if the

	 * DT node has a single port, assume that it describes a panel and

	 * create a panel bridge.

	/*

	 * Create and initialize the encoder. On Gen3, skip the LVDS1 output if

	 * the LVDS1 encoder is used as a companion for LVDS0 in dual-link

	 * mode, or any LVDS output if it isn't connected. The latter may happen

	 * on D3 or E3 as the LVDS encoders are needed to provide the pixel

	 * clock to the DU, even when the LVDS outputs are not used.

 Attach the bridge to the encoder. */

 Create the connector for the chain of bridges. */

 SPDX-License-Identifier: GPL-2.0+

/*

 * rcar_du_group.c  --  R-Car Display Unit Channels Pair

 *

 * Copyright (C) 2013-2015 Renesas Electronics Corporation

 *

 * Contact: Laurent Pinchart (laurent.pinchart@ideasonboard.com)

/*

 * The R8A7779 DU is split in per-CRTC resources (scan-out engine, blending

 * unit, timings generator, ...) and device-global resources (start/stop

 * control, planes, ...) shared between the two CRTCs.

 *

 * The R8A7790 introduced a third CRTC with its own set of global resources.

 * This would be modeled as two separate DU device instances if it wasn't for

 * a handful or resources that are shared between the three CRTCs (mostly

 * related to input and output routing). For this reason the R8A7790 DU must be

 * modeled as a single device with three CRTCs, two sets of "semi-global"

 * resources, and a few device-global resources.

 *

 * The rcar_du_group object is a driver specific object, without any real

 * counterpart in the DU documentation, that models those semi-global resources.

		/*

		 * On Gen2 the DEFR8 register for the first group also controls

		 * RGB output routing to DPAD0 and VSPD1 routing to DU0/1/2 for

		 * DU instances that support it.

		/*

		 * On Gen3 VSPD routing can't be configured, and DPAD routing

		 * is set in the group corresponding to the DPAD output (no Gen3

		 * SoC has multiple DPAD sources belonging to separate groups).

	/*

	 * Configure input dot clock routing with a hardcoded configuration. If

	 * the DU channel can use the LVDS encoder output clock as the dot

	 * clock, do so. Otherwise route DU_DOTCLKINn signal to DUn.

	 *

	 * Each channel can then select between the dot clock configured here

	 * and the clock provided by the CPG through the ESCR register.

		/*

		 * On Gen2 a single register in the first group controls dot

		 * clock selection for all channels.

		/*

		 * On Gen3 dot clocks are setup through per-group registers,

		 * only available when the group has two channels.

 Enable extended features */

	/*

	 * TODO: Handle routing of the DU output to CMM dynamically, as we

	 * should bypass CMM completely when no color management feature is

	 * used.

	/*

	 * Use DS1PR and DS2PR to configure planes priorities and connects the

	 * superposition 0 to DU0 pins. DU1 pins will be configured dynamically.

 Apply planes to CRTCs association. */

/*

 * rcar_du_group_get - Acquire a reference to the DU channels group

 *

 * Acquiring the first reference setups core registers. A reference must be held

 * before accessing any hardware registers.

 *

 * This function must be called with the DRM mode_config lock held.

 *

 * Return 0 in case of success or a negative error code otherwise.

/*

 * rcar_du_group_put - Release a reference to the DU

 *

 * This function must be called with the DRM mode_config lock held.

	/*

	 * Group start/stop is controlled by the DRES and DEN bits of DSYSR0

	 * for the first group and DSYSR2 for the second group. On most DU

	 * instances, this maps to the first CRTC of the group, and we can just

	 * use rcar_du_crtc_dsysr_clr_set() to access the correct DSYSR. On

	 * M3-N, however, DU2 doesn't exist, but DSYSR2 does. We thus need to

	 * access the register directly using group read/write.

	/*

	 * Many of the configuration bits are only updated when the display

	 * reset (DRES) bit in DSYSR is set to 1, disabling *both* CRTCs. Some

	 * of those bits could be pre-configured, but others (especially the

	 * bits related to plane assignment to display timing controllers) need

	 * to be modified at runtime.

	 *

	 * Restart the display controller if a start is requested. Sorry for the

	 * flicker. It should be possible to move most of the "DRES-update" bits

	 * setup to driver initialization time and minimize the number of cases

	 * when the display controller will have to be restarted.

	/*

	 * RGB output routing to DPAD0 and VSP1D routing to DU0/1/2 are

	 * configured in the DEFR8 register of the first group on Gen2 and the

	 * last group on Gen3. As this function can be called with the DU

	 * channels of the corresponding CRTCs disabled, we need to enable the

	 * group clock before accessing the register.

	/*

	 * The DPAD outputs can't be controlled directly. However, the parallel

	 * output of the DU channels routed to DPAD can be set to fixed levels

	 * through the DOFLR group register. Use this to turn the DPAD on or off

	 * by driving fixed low-level signals at the output of any DU channel

	 * not routed to a DPAD output. This doesn't affect the DU output

	 * signals going to other outputs, such as the internal LVDS and HDMI

	 * encoders.

	/*

	 * Set the DPAD1 pins sources. Select CRTC 0 if explicitly requested and

	 * CRTC 1 in all other cases to avoid cloning CRTC 0 to DPAD0 and DPAD1

	 * by default.

 SPDX-License-Identifier: GPL-2.0+

/*

 * rcar_du_plane.c  --  R-Car Display Unit Planes

 *

 * Copyright (C) 2013-2015 Renesas Electronics Corporation

 *

 * Contact: Laurent Pinchart (laurent.pinchart@ideasonboard.com)

/* -----------------------------------------------------------------------------

 * Atomic hardware plane allocator

 *

 * The hardware plane allocator is solely based on the atomic plane states

 * without keeping any external state to avoid races between .atomic_check()

 * and .atomic_commit().

 *

 * The core idea is to avoid using a free planes bitmask that would need to be

 * shared between check and commit handlers with a collective knowledge based on

 * the allocated hardware plane(s) for each KMS plane. The allocator then loops

 * over all plane states to compute the free planes bitmask, allocates hardware

 * planes based on that bitmask, and stores the result back in the plane states.

 *

 * For this to work we need to access the current state of planes not touched by

 * the atomic update. To ensure that it won't be modified, we need to lock all

 * planes using drm_atomic_get_plane_state(). This effectively serializes atomic

 * updates from .atomic_check() up to completion (when swapping the states if

 * the check step has succeeded) or rollback (when freeing the states if the

 * check step has failed).

 *

 * Allocation is performed in the .atomic_check() handler and applied

 * automatically when the core swaps the old and new states.

	/*

	 * Lowering the number of planes doesn't strictly require reallocation

	 * as the extra hardware plane will be freed when committing, but doing

	 * so could lead to more fragmentation.

 Reallocate hardware planes if the source has changed. */

/*

 * The R8A7790 DU can source frames directly from the VSP1 devices VSPD0 and

 * VSPD1. VSPD0 feeds DU0/1 plane 0, and VSPD1 feeds either DU2 plane 0 or

 * DU0/1 plane 1.

 *

 * Allocate the correct fixed plane when sourcing frames from VSPD0 or VSPD1,

 * and allocate planes in reverse index order otherwise to ensure maximum

 * availability of planes 0 and 1.

 *

 * The caller is responsible for ensuring that the requested source is

 * compatible with the DU revision.

 VSPD0 feeds plane 0 on DU0/1. */

 VSPD1 feeds plane 1 on DU0/1 or plane 0 on DU2. */

 Check if hardware planes need to be reallocated. */

		/*

		 * If the plane is being disabled we don't need to go through

		 * the full reallocation procedure. Just mark the hardware

		 * plane(s) as freed.

		/*

		 * If the plane needs to be reallocated mark it as such, and

		 * mark the hardware plane(s) as free.

	/*

	 * Grab all plane states for the groups that need reallocation to ensure

	 * locking and avoid racy updates. This serializes the update operation,

	 * but there's not much we can do about it as that's the hardware

	 * design.

	 *

	 * Compute the used planes mask for each group at the same time to avoid

	 * looping over the planes separately later.

			/*

			 * If the plane has been freed in the above loop its

			 * hardware planes must not be added to the used planes

			 * bitmask. However, the current state doesn't reflect

			 * the free state yet, as we've modified the new state

			 * above. Use the local freed planes list to check for

			 * that condition instead.

 Reallocate hardware planes for each plane that needs it. */

		/*

		 * Skip planes that are being disabled or don't need to be

		 * reallocated.

		/*

		 * Try to allocate the plane from the free planes currently

		 * associated with the target CRTC to avoid restarting the CRTC

		 * group and thus minimize flicker. If it fails fall back to

		 * allocating from all free planes.

/* -----------------------------------------------------------------------------

 * Plane Setup

	/*

	 * Memory pitch (expressed in pixels). Must be doubled for interlaced

	 * operation with 32bpp formats.

	/*

	 * The Y position is expressed in raster line units and must be doubled

	 * for 32bpp formats, according to the R8A7790 datasheet. No mention of

	 * doubling the Y position is found in the R8A7779 datasheet, but the

	 * rule seems to apply there as well.

	 *

	 * Despite not being documented, doubling seem not to be needed when

	 * operating in interlaced mode.

	 *

	 * Similarly, for the second plane, NV12 and NV21 formats seem to

	 * require a halved Y position value, in both progressive and interlaced

	 * modes.

	/*

	 * The PnALPHAR register controls alpha-blending in 16bpp formats

	 * (ARGB1555 and XRGB1555).

	 *

	 * For ARGB, set the alpha value to 0, and enable alpha-blending when

	 * the A bit is 0. This maps A=0 to alpha=0 and A=1 to alpha=255.

	 *

	 * For XRGB, set the alpha value to the plane-wide alpha value and

	 * enable alpha-blending regardless of the X bit value.

	/*

	 * Disable color keying when requested. YUV formats have the

	 * PnMR_SPIM_TP_OFF bit set in their pnmr field, disabling color keying

	 * automatically.

 For packed YUV formats we need to select the U/V order. */

	/*

	 * Data format

	 *

	 * The data format is selected by the DDDF field in PnMR and the EDF

	 * field in DDCR4.

 Destination position and size */

 Wrap-around and blinking, disabled */

		/*

		 * The visible field is not reset by the DRM core but only

		 * updated by drm_plane_helper_check_state(), set it manually.

	/*

	 * Check whether the source has changed from memory to live source or

	 * from live source to memory. The source has been configured by the

	 * VSPS bit in the PnDDCR4 register. Although the datasheet states that

	 * the bit is updated during vertical blanking, it seems that updates

	 * only occur when the DU group is held in reset through the DSYSR.DRES

	 * bit. We thus need to restart the group if the source changes.

	 /*

	  * Create one primary plane per CRTC in this group and seven overlay

	  * planes.

 SPDX-License-Identifier: GPL-2.0+

/*

 * rcar_du_drv.c  --  R-Car Display Unit DRM driver

 *

 * Copyright (C) 2013-2015 Renesas Electronics Corporation

 *

 * Contact: Laurent Pinchart (laurent.pinchart@ideasonboard.com)

/* -----------------------------------------------------------------------------

 * Device Information

		/*

		 * R8A774[34] has one RGB output and one LVDS output

		/*

		 * R8A7745 has two RGB outputs

		/*

		 * R8A77470 has two RGB outputs, one LVDS output, and

		 * one (currently unsupported) analog video output

		/*

		 * R8A774A1 has one RGB output, one LVDS output and one HDMI

		 * output.

		/*

		 * R8A774B1 has one RGB output, one LVDS output and one HDMI

		 * output.

		/*

		 * R8A774C0 has one RGB output and two LVDS outputs

		/*

		 * R8A774E1 has one RGB output, one LVDS output and one HDMI

		 * output.

		/*

		 * R8A7779 has two RGB outputs and one (currently unsupported)

		 * TCON output.

		/*

		 * R8A7742 and R8A7790 each have one RGB output and two LVDS

		 * outputs. Additionally R8A7790 supports one TCON output

		 * (currently unsupported by the driver).

 M2-W (r8a7791) and M2-N (r8a7793) are identical */

		/*

		 * R8A779[13] has one RGB output, one LVDS output and one

		 * (currently unsupported) TCON output.

 R8A7792 has two RGB outputs. */

		/*

		 * R8A7794 has two RGB outputs and one (currently unsupported)

		 * TCON output.

		/*

		 * R8A7795 has one RGB output, two HDMI outputs and one

		 * LVDS output.

		/*

		 * R8A7796 has one RGB output, one LVDS output and one HDMI

		 * output.

		/*

		 * R8A77965 has one RGB output, one LVDS output and one HDMI

		 * output.

		/*

		 * R8A77970 and R8A77980 have one RGB output and one LVDS

		 * output.

		/*

		 * R8A77990 and R8A77995 have one RGB output and two LVDS

		 * outputs.

 R8A779A0 has two MIPI DSI outputs. */

/* -----------------------------------------------------------------------------

 * DRM operations

/* -----------------------------------------------------------------------------

 * Power management

/* -----------------------------------------------------------------------------

 * Platform driver

 Allocate and initialize the R-Car device structure. */

 I/O resources */

	/*

	 * Set the DMA coherent mask to reflect the DU 32-bit DMA address space

	 * limitations. When sourcing frames from a VSP the DU doesn't perform

	 * any memory access so set the mask to 40 bits to accept all buffers.

 DRM/KMS objects */

	/*

	 * Register the DRM device with the core and the connectors with

	 * sysfs.

 SPDX-License-Identifier: GPL-2.0+

/*

 * rcar_du_vsp.h  --  R-Car Display Unit VSP-Based Compositor

 *

 * Copyright (C) 2015 Renesas Electronics Corporation

 *

 * Contact: Laurent Pinchart (laurent.pinchart@ideasonboard.com)

	/*

	 * Ensure that the plane source configuration takes effect by requesting

	 * a restart of the group. See rcar_du_plane_atomic_update() for a more

	 * detailed explanation.

	 *

	 * TODO: Check whether this is still needed on Gen3.

			/*

			 * If the GEM buffer has a scatter gather table, it has

			 * been imported from a dma-buf and has no physical

			 * address as it might not be physically contiguous.

			 * Copy the original scatter gather table to map it to

			 * the VSP.

	/*

	 * There's no need to prepare (and unprepare) the framebuffer when the

	 * plane is not visible, as it will not be displayed.

 Find the VSP device and initialize it. */

	 /*

	  * The VSP2D (Gen3) has 5 RPFs, but the VSP1D (Gen2) is limited to

	  * 4 RPFs.

 SPDX-License-Identifier: GPL-2.0+

/*

 * rcar_cmm.c -- R-Car Display Unit Color Management Module

 *

 * Copyright (C) 2019 Jacopo Mondi <jacopo+renesas@jmondi.org>

	/*

	 * @lut:		1D-LUT state

	 * @lut.enabled:	1D-LUT enabled flag

/*

 * rcar_cmm_lut_write() - Scale the DRM LUT table entries to hardware precision

 *			  and write to the CMM registers

 * @rcmm: Pointer to the CMM device

 * @drm_lut: Pointer to the DRM LUT table

/*

 * rcar_cmm_setup() - Configure the CMM unit

 * @pdev: The platform device associated with the CMM instance

 * @config: The CMM unit configuration

 *

 * Configure the CMM unit with the given configuration. Currently enabling,

 * disabling and programming of the 1-D LUT unit is supported.

 *

 * As rcar_cmm_setup() accesses the CMM registers the unit should be powered

 * and its functional clock enabled. To guarantee this, before any call to

 * this function is made, the CMM unit has to be enabled by calling

 * rcar_cmm_enable() first.

 *

 * TODO: Add support for LUT double buffer operations to avoid updating the

 * LUT table entries while a frame is being displayed.

 Disable LUT if no table is provided. */

 Enable LUT and program the new gamma table values. */

/*

 * rcar_cmm_enable() - Enable the CMM unit

 * @pdev: The platform device associated with the CMM instance

 *

 * When the output of the corresponding DU channel is routed to the CMM unit,

 * the unit shall be enabled before the DU channel is started, and remain

 * enabled until the channel is stopped. The CMM unit shall be disabled with

 * rcar_cmm_disable().

 *

 * Calls to rcar_cmm_enable() and rcar_cmm_disable() are not reference-counted.

 * It is an error to attempt to enable an already enabled CMM unit, or to

 * attempt to disable a disabled unit.

/*

 * rcar_cmm_disable() - Disable the CMM unit

 * @pdev: The platform device associated with the CMM instance

 *

 * See rcar_cmm_enable() for usage information.

 *

 * Disabling the CMM unit disable all the internal processing blocks. The CMM

 * state shall thus be restored with rcar_cmm_setup() when re-enabling the CMM

 * unit after the next rcar_cmm_enable() call.

/*

 * rcar_cmm_init() - Initialize the CMM unit

 * @pdev: The platform device associated with the CMM instance

 *

 * Return: 0 on success, -EPROBE_DEFER if the CMM is not available yet,

 *         -ENODEV if the DRM_RCAR_CMM config option is disabled

 SPDX-License-Identifier: GPL-2.0+

/*

 * rcar_du_kms.c  --  R-Car Display Unit Mode Setting

 *

 * Copyright (C) 2013-2015 Renesas Electronics Corporation

 *

 * Contact: Laurent Pinchart (laurent.pinchart@ideasonboard.com)

/* -----------------------------------------------------------------------------

 * Format helpers

	/*

	 * The following formats are not supported on Gen2 and thus have no

	 * associated .pnmr or .edf settings.

/* -----------------------------------------------------------------------------

 * Frame buffer

 Create a CMA GEM buffer. */

	/*

	 * The R8A7779 DU requires a 16 pixels pitch alignment as documented,

	 * but the R8A7790 DU seems to require a 128 bytes pitch alignment.

		/*

		 * On Gen2 the DU limits the pitch to 4095 pixels and requires

		 * buffers to be aligned to a 16 pixels boundary (or 128 bytes

		 * on some platforms).

		/*

		 * On Gen3 the memory interface is handled by the VSP that

		 * limits the pitch to 65535 bytes and has no alignment

		 * constraint.

	/*

	 * Calculate the chroma plane(s) pitch using the horizontal subsampling

	 * factor. For semi-planar formats, the U and V planes are combined, the

	 * pitch must thus be doubled.

/* -----------------------------------------------------------------------------

 * Atomic Check and Update

	/*

	 * Store RGB routing to DPAD0 and DPAD1, the hardware will be configured

	 * when starting the CRTCs.

 Apply the atomic update. */

/* -----------------------------------------------------------------------------

 * Initialization

 Locate the connected entity and initialize the encoder. */

	/*

	 * Iterate over the endpoints and create one encoder for each output

	 * pipeline.

 Find the output route corresponding to the port number. */

 Process the output pipeline. */

	/*

	 * The color key is expressed as an RGB888 triplet stored in a 32-bit

	 * integer in XRGB8888 format. Bit 24 is used as a flag to disable (0)

	 * or enable source color keying (1).

	/*

	 * First parse the DT vsps property to populate the list of VSPs. Each

	 * entry contains a pointer to the VSP DT node and a bitmask of the

	 * connected DU CRTCs.

 Backward compatibility with old DTBs. */

		/*

		 * Add the VSP to the list or update the corresponding existing

		 * entry if the VSP has already been added.

		/*

		 * Store the VSP pointer and pipe index in the CRTC. If the

		 * second cell of the 'renesas,vsps' specifier isn't present,

		 * default to 0 to remain compatible with older DT bindings.

	/*

	 * Then initialize all the VSPs from the node pointers and CRTCs bitmask

	 * computed previously.

 It's fine to have a phandle to a non-enabled CMM. */

		/*

		 * -ENODEV is used to report that the CMM config option is

		 * disabled: return 0 and let the DU continue probing.

		/*

		 * Enforce suspend/resume ordering by making the CMM a provider

		 * of the DU: CMM is suspended after and resumed before the DU.

		/*

		 * The Gen3 DU uses the VSP1 for memory access, and is limited

		 * to frame sizes of 8190x8190.

	/*

	 * Initialize vertical blanking interrupts handling. Start with vblank

	 * disabled for all CRTCs.

 Initialize the groups. */

 Extract the channel mask for this group only. */

		/*

		 * If we have more than one CRTCs in this group pre-associate

		 * the low-order planes with CRTC 0 and the high-order planes

		 * with CRTC 1 to minimize flicker occurring when the

		 * association is changed.

 Initialize the compositors. */

 Initialize the Color Management Modules. */

 Create the CRTCs. */

 Skip unpopulated DU channels. */

 Initialize the encoders. */

	/*

	 * Set the possible CRTCs and possible clones. There's always at least

	 * one way for all encoders to clone each other, set all bits in the

	 * possible clones field.

 Create the writeback connectors. */

	/*

	 * Initialize the default DPAD0 source to the index of the first DU

	 * channel that can be connected to DPAD0. The exact value doesn't

	 * matter as it should be overwritten by mode setting for the RGB

	 * output, but it is nonetheless required to ensure a valid initial

	 * hardware configuration on Gen3 where DU0 can't always be connected to

	 * DPAD0.

 SPDX-License-Identifier: GPL-2.0+

/*

 * R-Car Gen3 HDMI PHY

 *

 * Copyright (C) 2016 Renesas Electronics Corporation

 *

 * Contact: Laurent Pinchart (laurent.pinchart@ideasonboard.com)

 Mode of operation and PLL dividers */

 PLL current and Gmp (conductance) */

 PLL dividers */

 Mode of operation and PLL dividers */

 PLL current and Gmp (conductance) */

 PLL dividers */

	/*

	 * The maximum supported clock frequency is 297 MHz, as shown in the PHY

	 * parameters table.

 Sentinel */ },

 SPDX-License-Identifier: GPL-2.0

/*

 * rcar_du_writeback.c  --  R-Car Display Unit Writeback Support

 *

 * Copyright (C) 2019 Laurent Pinchart <laurent.pinchart@ideasonboard.com>

/**

 * struct rcar_du_wb_conn_state - Driver-specific writeback connector state

 * @state: base DRM connector state

 * @format: format of the writeback framebuffer

/**

 * struct rcar_du_wb_job - Driver-private data for writeback jobs

 * @sg_tables: scatter-gather tables for the framebuffer memory

 Map the framebuffer to the VSP. */

	/*

	 * Verify that the framebuffer format is supported and that its size

	 * matches the current mode.

/*

 * Only RGB formats are currently supported as the VSP outputs RGB to the DU

 * and can't convert to YUV separately for writeback.

 SPDX-License-Identifier: GPL-2.0

/*

 * rcar_du_of.c - Legacy DT bindings compatibility

 *

 * Copyright (C) 2018 Laurent Pinchart <laurent.pinchart@ideasonboard.com>

 *

 * Based on work from Jyri Sarha <jsarha@ti.com>

 * Copyright (C) 2015 Texas Instruments

/* -----------------------------------------------------------------------------

 * Generic Overlay Handling

/* -----------------------------------------------------------------------------

 * LVDS Overlays

 Sentinel */ },

	/*

	 * Set the LVDS clocks property. This can't be performed by the overlay

	 * as the structure of the clock specifier has changed over time, and we

	 * don't know at compile time which binding version the system we will

	 * run on uses.

	/*

	 * Insert the node in the OF graph: patch the LVDS ports remote-endpoint

	 * properties to point to the endpoints of the sibling nodes in the

	 * graph. This can't be performed by the overlay: on the input side the

	 * overlay would contain a phandle for the DU LVDS output port that

	 * would clash with the system DT, and on the output side the connection

	 * is board-specific.

 Get the DU node and exit if not present or disabled. */

	/*

	 * Skip if the LVDS nodes already exists.

	 *

	 * The nodes are searched based on the compatible string, which we

	 * construct from the SoC name found in the DU compatible string. As a

	 * match has been found we know the compatible string matches the

	 * expected format and can thus skip some of the string manipulation

	 * normal safety checks.

	/*

	 * Parse the DU node and store the register specifier, the clock

	 * specifier and the local and remote endpoint of the LVDS link for

	 * later use.

 Parse and apply the overlay. This will resolve phandles. */

 Patch the newly created LVDS encoder nodes. */

 Locate the lvds_data entry based on the resource start. */

 Patch the LVDS encoder. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 * Copyright (C) 2015 Amlogic, Inc. All rights reserved.

/**

 * DOC: Video Clocks

 *

 * VCLK is the "Pixel Clock" frequency generator from a dedicated PLL.

 * We handle the following encodings :

 *

 * - CVBS 27MHz generator via the VCLK2 to the VENCI and VDAC blocks

 * - HDMI Pixel Clocks generation

 *

 * What is missing :

 *

 * - Genenate Pixel clocks for 2K/4K 10bit formats

 *

 * Clock generator scheme :

 *

 * .. code::

 *

 *    __________   _________            _____

 *   |          | |         |          |     |--ENCI

 *   | HDMI PLL |-| PLL_DIV |--- VCLK--|     |--ENCL

 *   |__________| |_________| \        | MUX |--ENCP

 *                             --VCLK2-|     |--VDAC

 *                                     |_____|--HDMI-TX

 *

 * Final clocks can take input for either VCLK or VCLK2, but

 * VCLK is the preferred path for HDMI clocking and VCLK2 is the

 * preferred path for CVBS VDAC clocking.

 *

 * VCLK and VCLK2 have fixed divided clocks paths for /1, /2, /4, /6 or /12.

 *

 * The PLL_DIV can achieve an additional fractional dividing like

 * 1.5, 3.5, 3.75... to generate special 2K and 4K 10bit clocks.

 HHI Registers */

 0x68 offset in data sheet */

 0x4a offset in data sheet */

 0x4b offset in data sheet */

 0x59 offset in data sheet */

 0x5f offset in data sheet */

 0x65 offset in data sheet */

 0x73 offset in data sheet */

 0xbd offset in data sheet */

 0xbe offset in data sheet */

 0xc8 offset in data sheet */

 0xc9 offset in data sheet */

 0xca offset in data sheet */

 0xcb offset in data sheet */

 0xcc offset in data sheet */

 0xcd offset in data sheet */

 0xce offset in data sheet */

 VID PLL Dividers */

 Disable vid_pll output clock */

 Enable vid_pll bypass to HDMI pll */

 Disable Bypass */

 Clear sel */

 Setup sel and val */

 Enable the vid_pll output clock */

/*

 * Setup VCLK2 for 27MHz, and enable clocks for ENCI and VDAC

 *

 * TOFIX: Refactor into table to also handle HDMI frequency and paths

 Setup PLL to output 1.485GHz */

 Poll for lock bit */

 Reset PLL */

 Poll for lock bit */

 Poll for lock bit */

 Disable VCLK2 */

 Setup vid_pll to /1 */

 Setup the VCLK2 divider value to achieve 27MHz */

 select vid_pll for vclk2 */

 enable vclk2 gate */

 select vclk_div1 for enci */

 select vclk_div1 for vdac */

 release vclk2_div_reset and enable vclk2_div */

 enable vclk2_div1 gate */

 reset vclk2 */

 enable enci_clk */

 enable vdac_clk */

 PLL	O1 O2 O3 VP DV     EN TX */

 4320 /4 /4 /1 /5 /1  => /2 /2 */

 4320 /4 /4 /1 /5 /1  => /1 /2 */

 2970 /4 /1 /1 /5 /1  => /1 /2 */

 2970 /2 /2 /2 /5 /1  => /1 /1 */

 2970 /1 /2 /2 /5 /1  => /1 /1 */

 2970 /1 /1 /1 /5 /2  => /1 /1 */

 5940 /1 /1 /2 /5 /1  => /1 /1 */

 2970 /1 /1 /1 /5 /1  => /1 /2 */

 sentinel */ },

 Invalid */

 Enable and unreset */

 Poll for lock bit */

 Reset PLL */

 Poll for lock bit */

 Enable and reset */

 TODO: add specific macro for g12a here */

 G12A HDMI PLL Needs specific parameters for 5.4GHz */

 Reset PLL */

 UN-Reset PLL */

 Poll for lock bits */

 The GXBB PLL has a /2 pre-multiplier */

 The GXBB PLL has a /2 pre-multiplier and a larger FRAC width */

 We can have a perfect match !*/

 Empiric supported min/max dividers */

 Empiric supported min/max dividers */

 Empiric supported min/max dividers */

 Cycle from /16 to /2 */

 pll_freq is the frequency after the OD dividers */

 In DMT mode, path after PLL is always /10 */

 Check against soc revision/package limits */

 pll_freq is the frequency after the OD dividers */

 OD2 goes to the PHY, and needs to be *10, so keep OD3=1 */

 Check against soc revision/package limits */

 Match strict frequency */

 Match 1000/1001 variant */

 Set HDMI-TX sys clock */

 Set HDMI PLL rate */

 Setup vid_pll divider */

 Set VCLK div */

 Set HDMI-TX source */

 enable vclk_div1 gate */

 select vclk_div1 for HDMI-TX */

 enable vclk_div2 gate */

 select vclk_div2 for HDMI-TX */

 enable vclk_div4 gate */

 select vclk_div4 for HDMI-TX */

 enable vclk_div6 gate */

 select vclk_div6 for HDMI-TX */

 enable vclk_div12 gate */

 select vclk_div12 for HDMI-TX */

 Set ENCI/ENCP Source */

 enable vclk_div1 gate */

 select vclk_div1 for enci */

 select vclk_div1 for encp */

 enable vclk_div2 gate */

 select vclk_div2 for enci */

 select vclk_div2 for encp */

 enable vclk_div4 gate */

 select vclk_div4 for enci */

 select vclk_div4 for encp */

 enable vclk_div6 gate */

 select vclk_div6 for enci */

 select vclk_div6 for encp */

 enable vclk_div12 gate */

 select vclk_div12 for enci */

 select vclk_div12 for encp */

 Enable ENCI clock gate */

 Enable ENCP clock gate */

		/*

		 * The DMT clock path is fixed after the PLL:

		 * - automatic PLL freq + OD management

		 * - vid_pll_div = VID_PLL_DIV_5

		 * - vclk_div = 2

		 * - hdmi_tx_div = 1

		 * - venc_div = 1

		 * - encp encoder

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 * Copyright (C) 2015 Amlogic, Inc. All rights reserved.

/**

 * DOC: HDMI Output

 *

 * HDMI Output is composed of :

 *

 * - A Synopsys DesignWare HDMI Controller IP

 * - A TOP control block controlling the Clocks and PHY

 * - A custom HDMI PHY in order convert video to TMDS signal

 *

 * .. code::

 *

 *    ___________________________________

 *   |            HDMI TOP               |<= HPD

 *   |___________________________________|

 *   |                  |                |

 *   |  Synopsys HDMI   |   HDMI PHY     |=> TMDS

 *   |    Controller    |________________|

 *   |___________________________________|<=> DDC

 *

 *

 * The HDMI TOP block only supports HPD sensing.

 * The Synopsys HDMI Controller interrupt is routed

 * through the TOP Block interrupt.

 * Communication to the TOP Block and the Synopsys

 * HDMI Controller is done a pair of addr+read/write

 * registers.

 * The HDMI PHY is configured by registers in the

 * HHI register block.

 *

 * Pixel data arrives in 4:4:4 format from the VENC

 * block and the VPU HDMI mux selects either the ENCI

 * encoder for the 576i or 480i formats or the ENCP

 * encoder for all the other formats including

 * interlaced HD formats.

 * The VENC uses a DVI encoder on top of the ENCI

 * or ENCP encoders to generate DVI timings for the

 * HDMI controller.

 *

 * GXBB, GXL and GXM embeds the Synopsys DesignWare

 * HDMI TX IP version 2.01a with HDCP and I2C & S/PDIF

 * audio source interfaces.

 *

 * We handle the following features :

 *

 * - HPD Rise & Fall interrupt

 * - HDMI Controller Interrupt

 * - HDMI PHY Init for 480i to 1080p60

 * - VENC & HDMI Clock setup for 480i to 1080p60

 * - VENC Mode setup for 480i to 1080p60

 *

 * What is missing :

 *

 * - PHY, Clock and Mode setup for 2k && 4k modes

 * - SDDC Scrambling mode for HDMI 2.0a

 * - HDCP Setup

 * - CEC Management

 TOP Block Communication Channel */

 Controller Communication Channel */

 HHI Registers */

 0x40 */

 0x73 */

 0xe8 */

 0xe9 */

 0xea */

 0xeb */

 0xec */

 0xed */

 PHY (via TOP bridge) and Controller dedicated register interface */

 ADDR must be written twice */

 Read needs a second DATA read */

 ADDR must be written twice */

 Write needs single DATA write */

 Helper to change specific bits in PHY registers */

 ADDR must be written twice */

 Read needs a second DATA read */

 ADDR must be written twice */

 Write needs single DATA write */

 Helper to change specific bits in controller registers */

 Bridge */

 Setup PHY bandwidth modes */

 For 420, pixel clock is half unlike venc clock */

 5.94Gbps, 3.7125Gbps */

 2.97Gbps */

 1.485Gbps */

 742.5Mbps, and below */

 5.94Gbps, 3.7125Gbps */

 2.97Gbps */

 1.485Gbps, and below */

 5.94Gbps, 3.7125Gbps */

 2.97Gbps */

 1.485Gbps, and below */

 Enable and software reset */

 Enable and unreset */

 For 420, pixel clock is half unlike venc clock */

 TMDS clock is pixel_clock * 10 */

 480i/576i needs global pixel doubling */

 VENC double pixels for 1080i, 720p and YUV420 modes */

 Enable clocks */

 Bring HDMITX MEM output of power down */

 Bring out of reset */

 Enable internal pixclk, tmds_clk, spdif_clk, i2s_clk, cecclk */

 Enable cec_clk and hdcp22_tmdsclk_en */

 Enable normal output to PHY */

 TMDS pattern setup */

 Load TMDS pattern */

 Setup PHY parameters */

 Setup PHY */

 BIT_INVERT */

 Disable clock, fifo, fifo_wr */

 Reset PHY 3 times in a row */

 Temporary Disable VENC video stream */

 Temporary Disable HDMI video stream to HDMI-TX */

 Re-Enable VENC video stream */

 Push back HDMI clock settings */

 Enable and Select HDMI video source for HDMI-TX */

 Setup HPD Filter */

 Clear interrupts */

 Unmask interrupts */

 HPD Events, handle in the threaded interrupt handler */

 HDMI Controller Interrupt */

 TOFIX Handle HDCP Interrupts */

 Threaded interrupt handler to manage HPD events */

 HPD Events */

 If sink does not support 540MHz, reject the non-420 HDMI2 modes */

 Check against non-VIC supported modes */

 Check against supported VIC modes */

 For 420, pixel clock is half unlike venc clock */

 TMDS clock is pixel_clock * 10 */

 480i/576i needs global pixel doubling */

 VENC double pixels for 1080i, 720p and YUV420 modes */

 Encoder */

 VENC + VENC-DVI Mode setup */

 VCLK Set clock */

 Setup YUV420 to HDMI-TX, no 10bit diphering */

 Setup YUV444 to HDMI-TX, no 10bit diphering */

 DW HDMI Regmap */

 HDMI Connector is on the second port, first endpoint */

 If the endpoint node exists, consider it enabled */

 Enable clocks */

 Bring HDMITX MEM output of power down */

 Reset HDMITX APB & TX & PHY */

 Enable APB3 fail on error */

 Bring out of reset */

 Enable HDMI-TX Interrupt */

 Encoder */

 Bridge / Connector */

 Reset TOP */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 * Copyright (C) 2015 Amlogic, Inc. All rights reserved.

/**

 * DOC: Video Encoder

 *

 * VENC Handle the pixels encoding to the output formats.

 * We handle the following encodings :

 *

 * - CVBS Encoding via the ENCI encoder and VDAC digital to analog converter

 * - TMDS/HDMI Encoding via ENCI_DIV and ENCP

 * - Setup of more clock rates for HDMI modes

 *

 * What is missing :

 *

 * - LCD Panel encoding via ENCL

 * - TV Panel encoding via ENCT

 *

 * VENC paths :

 *

 * .. code::

 *

 *          _____   _____   ____________________

 *   vd1---|     |-|     | | VENC     /---------|----VDAC

 *   vd2---| VIU |-| VPP |-|-----ENCI/-ENCI_DVI-|-|

 *   osd1--|     |-|     | | \                  | X--HDMI-TX

 *   osd2--|_____|-|_____| |  |\-ENCP--ENCP_DVI-|-|

 *                         |  |                 |

 *                         |  \--ENCL-----------|----LVDS

 *                         |____________________|

 *

 * The ENCI is designed for PAl or NTSC encoding and can go through the VDAC

 * directly for CVBS encoding or through the ENCI_DVI encoder for HDMI.

 * The ENCP is designed for Progressive encoding but can also generate

 * 1080i interlaced pixels, and was initially designed to encode pixels for

 * VDAC to output RGB ou YUV analog outputs.

 * It's output is only used through the ENCP_DVI encoder for HDMI.

 * The ENCL LVDS encoder is not implemented.

 *

 * The ENCI and ENCP encoders needs specially defined parameters for each

 * supported mode and thus cannot be determined from standard video timings.

 *

 * The ENCI end ENCP DVI encoders are more generic and can generate any timings

 * from the pixel data generated by ENCI or ENCP, so can use the standard video

 * timings are source for HW parameters.

 HHI Registers */

 0x52 offset in data sheet */

 0xbd offset in data sheet */

 0xbb offset in data sheet */

 0xbe offset in data sheet */

 0xbc offset in data sheet */

 0xe8 offset in data sheet */

 video_yc_dly */

 video_rgb_ctrl */

 video_ofld_voav_ofst */

 eqpuls_begin */

 eqpuls_end */

 eqpuls_bline */

 eqpuls_eline */

 vso_eline */

 video_yc_dly */

 video_rgb_ctrl */

 video_ofld_voav_ofst */

 eqpuls_begin */

 eqpuls_end */

 eqpuls_bline */

 eqpuls_eline */

 vso_eline */

 video_prog_mode */

 video_sync_mode */

 video_yc_dly */

 video_rgb_ctrl */

 video_filt_ctrl */

 video_ofld_voav_ofst */

 eqpuls_begin */

 eqpuls_end */

 eqpuls_bline */

 eqpuls_eline */

 sy_val */

 sy2_val */

 video_rgb_ctrl */

 video_filt_ctrl */

 video_ofld_voav_ofst */

 eqpuls_begin */

 eqpuls_end */

 eqpuls_bline */

 eqpuls_eline */

 sy_val */

 sy2_val */

 video_yc_dly */

 video_rgb_ctrl */

 video_filt_ctrl */

 sy_val */

 sy2_val */

 video_yc_dly */

 video_rgb_ctrl */

 video_filt_ctrl */

 sy_val */

 sy2_val */

 video_ofld_voav_ofst */

 eqpuls_begin */

 eqpuls_end */

 sy_val */

 sy2_val */

 video_sync_mode */

 video_yc_dly */

 video_rgb_ctrl */

 video_ofld_voav_ofst */

 eqpuls_begin */

 eqpuls_end */

 eqpuls_bline */

 eqpuls_eline */

 sy_val */

 sy2_val */

 video_filt_ctrl */

 video_ofld_voav_ofst */

 eqpuls_begin */

 eqpuls_end */

 sy_val */

 sy2_val */

 video_sync_mode */

 video_yc_dly */

 video_rgb_ctrl */

 video_ofld_voav_ofst */

 eqpuls_begin */

 eqpuls_end */

 eqpuls_bline */

 eqpuls_eline */

 sy_val */

 sy2_val */

 video_sync_mode */

 video_yc_dly */

 video_rgb_ctrl */

 video_ofld_voav_ofst */

 eqpuls_begin */

 eqpuls_end */

 eqpuls_bline */

 eqpuls_eline */

 sy_val */

 sy2_val */

 video_sync_mode */

 video_yc_dly */

 video_rgb_ctrl */

 video_ofld_voav_ofst */

 eqpuls_begin */

 eqpuls_end */

 eqpuls_bline */

 eqpuls_eline */

 sy_val */

 sy2_val */

 video_sync_mode */

 video_yc_dly */

 video_rgb_ctrl */

 video_ofld_voav_ofst */

 eqpuls_begin */

 eqpuls_end */

 eqpuls_bline */

 eqpuls_eline */

 sy_val */

 sy2_val */

 sentinel */

 Repeat VENC pixels for 480/576i/p, 720p50/60 and 1080p50/60 */

 480i */

 576i */

 576p */

 480p */

 720p60 */

 720p50 */

 1080i60 */

 1080i50 */

 Use VENCI for 480i and 576i and double HDMI pixels */

 Repeat VENC pixels for 480/576i/p, 720p50/60 and 1080p50/60 */

 Disable VDACs */

 CVBS Filter settings */

 Digital Video Select : Interlace, clk27 clk, external */

 Reset Video Mode */

 Horizontal sync signal output */

 Vertical Sync lines */

 Macrovision max amplitude change */

 Video mode */

		/*

		 * Advanced Video Mode :

		 * Demux shifting 0x2

		 * Blank line end at line17/22

		 * High bandwidth Luma Filter

		 * Low bandwidth Chroma Filter

		 * Bypass luma low pass filter

		 * No macrovision on CSYNC

 Sync mode : MASTER Master mode, free run, send HSO/VSO out */

 UNreset Interlaced TV Encoder */

		/*

		 * Enable Vfifo2vd and set Y_Cb_Y_Cr:

		 * Corresponding value:

		 * Y  => 00 or 10

		 * Cb => 01

		 * Cr => 11

		 * Ex: 0x4e => 01001110 would mean Cb/Y/Cr/Y

 Timings */

 Select ENCI for VIU */

 Interlace video enable */

 Program Hsync timing */

 Program Vsync timing for even field */

 Program Vsync timing for odd field */

 Set DE signalâs polarity is active high */

 Program DE timing */

 Program DE timing for even field */

 Program DE timing for odd field if needed */

 Program Hsync timing */

 Program Vsync timing for even field */

 Program Vsync timing for odd field if needed */

 Select ENCP for VIU */

 Set VPU HDMI setting */

 Select ENCP or ENCI data to HDMI */

 Invert polarity of HSYNC from VENC */

 Invert polarity of VSYNC from VENC */

 Output data format */

	/*

	 * Write rate to the async FIFO between VENC and HDMI.

	 * One write every 2 wr_clk.

	/*

	 * Read rate to the async FIFO between VENC and HDMI.

	 * One read every 2 wr_clk.

 CVBS Filter settings */

 Digital Video Select : Interlace, clk27 clk, external */

 Reset Video Mode */

 Horizontal sync signal output */

 Vertical Sync lines */

 Macrovision max amplitude change */

 Video mode */

	/*

	 * Advanced Video Mode :

	 * Demux shifting 0x2

	 * Blank line end at line17/22

	 * High bandwidth Luma Filter

	 * Low bandwidth Chroma Filter

	 * Bypass luma low pass filter

	 * No macrovision on CSYNC

 Sync mode : MASTER Master mode, free run, send HSO/VSO out */

 0x3 Y, C, and Component Y delay */

 Timings */

 Internal Venc, Internal VIU Sync, Internal Vencoder */

 UNreset Interlaced TV Encoder */

	/*

	 * Enable Vfifo2vd and set Y_Cb_Y_Cr:

	 * Corresponding value:

	 * Y  => 00 or 10

	 * Cb => 01

	 * Cr => 11

	 * Ex: 0x4e => 01001110 would mean Cb/Y/Cr/Y

 Power UP Dacs */

 Video Upsampling */

	/*

	 * CTRL0, CTRL1 and CTRL2:

	 * Filter0: input data sample every 2 cloks

	 * Filter1: filtering and upsample enable

	/*

	 * Upsample CTRL0:

	 * Interlace High Bandwidth Luma

	/*

	 * Upsample CTRL1:

	 * Interlace Pb

	/*

	 * Upsample CTRL2:

	 * Interlace R

 Select Interlace Y DACs */

 Select ENCI for VIU */

 Enable ENCI FIFO */

 Select ENCI DACs 0, 1, 4, and 5 */

 Interlace video enable */

 Configure Video Saturation / Contrast / Brightness / Hue */

 Enable DAC0 Filter */

 0 in Macrovision register 0 */

 Analog Synchronization and color burst value adjust */

 Returns the current ENCI field polarity */

 Disable CVBS VDAC */

 Power Down Dacs */

 Disable HDMI PHY */

 Disable HDMI */

 Disable all encoders */

 Disable VSync IRQ */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 * Copyright (C) 2015 Amlogic, Inc. All rights reserved.

 * Copyright (C) 2014 Endless Mobile

/**

 * DOC: Video Input Unit

 *

 * VIU Handles the Pixel scanout and the basic Colorspace conversions

 * We handle the following features :

 *

 * - OSD1 RGB565/RGB888/xRGB8888 scanout

 * - RGB conversion to x/cb/cr

 * - Progressive or Interlace buffer scanout

 * - OSD1 Commit on Vsync

 * - HDR OSD matrix for GXL/GXM

 *

 * What is missing :

 *

 * - BGR888/xBGR8888/BGRx8888/BGRx8888 modes

 * - YUV4:2:2 Y0CbY1Cr scanout

 * - Conversion to YUV 4:4:4 from 4:2:2 input

 * - Colorkey Alpha matching

 * - Big endian scanout

 * - X/Y reverse scanout

 * - Global alpha setup

 * - OSD2 support, would need interlace switching on vsync

 * - OSD1 full scaling to support TV overscan

 OSD csc defines */

 pre offset */

 10'/11'/12' */

 20'/21'/22' */

 offset */

 mode, right_shift, clip_en */

  eotf matrix: bypass */

 right shift */

 VPP WRAP OSD1 matrix */

 osd matrix, VIU_MATRIX_0 */

 23 reserved for clipping control */

 osd eotf matrix, VIU_MATRIX_OSD_EOTF */

 eotf lut: linear */

 osd oetf lut: linear */

 eotf lut bypass */

 R */

 G */

 B */

 eotf matrix bypass */

 oetf lut bypass */

 R */

 G */

 B */

 osd matrix RGB709 to YUV709 limit */

 VIU OSD1 Reset as workaround for GXL+ Alpha OSD Bug */

 Save these 2 registers state */

 Reset OSD1 */

 Rewrite these registers state lost in the reset */

 Reload the conversion matrix */

 Enable Mali AFBC Unpack */

 Setup RGBA Reordering */

 Select AFBCD path for OSD1 */

 Disable AFBCD path for OSD1 */

 Disable AFBCD unpack */

 Disable OSDs */

 On GXL/GXM, Use the 10bit HDR conversion matrix */

 fix green/pink color distortion from vendor u-boot */

 Initialize OSD1 fifo control register */

 fifo_depth_val: 32*8=256 */

 4 words in 1 burst */

 fifo_lim: 2*16=32 */

 Set OSD alpha replace value */

 Disable VD1 AFBC */

 di_mif0_en=0 mif0_to_vpp_en=0 di_mad_en=0 and afbc vd1 set=0*/

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2019 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

/*

 * The VPU embeds a "Register DMA" that can write a sequence of registers

 * on the VPU AHB bus, either manually or triggered by an internal IRQ

 * event like VSYNC or a line input counter.

 * The initial implementation handles a single channel (over 8), triggered

 * by the VSYNC irq and does not handle the RDMA irq.

 Allocate a PAGE buffer */

 Channel 1: Write Flag, No Address Increment */

 Stop Channel 1 */

/*

 * This will add the register to the RDMA buffer and write it to the

 * hardware at the same time.

 * When meson_rdma_flush is called, the RDMA will replay the register

 * writes in order.

 Start of Channel 1 register writes buffer */

 Last byte on Channel 1 register writes buffer */

 Trigger Channel 1 on VSYNC event */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 * Copyright (C) 2015 Amlogic, Inc. All rights reserved.

 * Copyright (C) 2014 Endless Mobile

 *

 * Written by:

 *     Jasper St. Pierre <jstpierre@mecheye.net>

 CRTC definition */

 CRTC */

 VD1 Preblend vertical start/end */

 Setup Blender */

 Enable VPP Postblend */

 VD1 Preblend vertical start/end */

 Disable VPP Postblend */

 Update the OSD registers */

 Enable OSD1 */

 Update the VD1 registers */

 Enable VD1 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 * Copyright (C) 2015 Amlogic, Inc. All rights reserved.

 * Copyright (C) 2014 Endless Mobile

/**

 * DOC: Video Post Processing

 *

 * VPP Handles all the Post Processing after the Scanout from the VIU

 * We handle the following post processings :

 *

 * - Postblend, Blends the OSD1 only

 *	We exclude OSD2, VS1, VS1 and Preblend output

 * - Vertical OSD Scaler for OSD1 only, we disable vertical scaler and

 *	use it only for interlace scanout

 * - Intermediate FIFO with default Amlogic values

 *

 * What is missing :

 *

 * - Preblend for video overlay pre-scaling

 * - OSD2 support for cursor framebuffer

 * - Video pre-scaling before postblend

 * - Full Vertical/Horizontal OSD scaling to support TV overscan

 * - HDR conversion

 set dummy data default YUV black */

 Initialize vpu fifo control registers */

 Turn off preblend */

 Turn off POSTBLEND */

 Force all planes off */

 Setup default VD settings */

 Disable Scalers */

 Set horizontal/vertical bank length and enable video scale out */

 Enable minus black level for vadj1 */

 Write in the proper filter coefficients. */

 Write the VD proper filter coefficients. */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2019 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

/*

 * DOC: Driver for the ARM FrameBuffer Compression Decoders

 *

 * The Amlogic GXM and G12A SoC families embeds an AFBC Decoder,

 * to decode compressed buffers generated by the ARM Mali GPU.

 *

 * For the GXM Family, Amlogic designed their own Decoder, named in

 * the vendor source as "MESON_AFBC", and a single decoder is available

 * for the 2 OSD planes.

 * This decoder is compatible with the AFBC 1.0 specifications and the

 * Mali T820 GPU capabilities.

 * It supports :

 * - basic AFBC buffer for RGB32 only, thus YTR feature is mandatory

 * - SPARSE layout and SPLIT layout

 * - only 16x16 superblock

 *

 * The decoder reads the data from the SDRAM, decodes and sends the

 * decoded pixel stream to the OSD1 Plane pixel composer.

 *

 * For the G12A Family, Amlogic integrated an ARM AFBC Decoder, named

 * in the vendor source as "MALI_AFBC", and the decoder can decode up

 * to 4 surfaces, one for each of the 4 available OSDs.

 * This decoder is compatible with the AFBC 1.2 specifications for the

 * Mali G31 and G52 GPUs.

 * Is supports :

 * - basic AFBC buffer for multiple RGB and YUV pixel formats

 * - SPARSE layout and SPLIT layout

 * - 16x16 and 32x8 "wideblk" superblocks

 * - Tiled header

 *

 * The ARM AFBC Decoder independent from the VPU Pixel Pipeline, so

 * the ARM AFBC Decoder reads the data from the SDRAM then decodes

 * into a private internal physical address where the OSD1 Plane pixel

 * composer unpacks the decoded data.

 Amlogic AFBC Decoder for GXM Family */

 TOFIX support mode formats */

 TOFIX: bits 31:24 are not documented, nor the meaning of 0xe4 */

 ARM AFBC Decoder for G12A Family */

 Amlogic G12A Mali AFBC Decoder supported formats */

 YTR is forbidden for non XBGR formats */

 YTR is forbidden for non XBGR formats */

 YTR is forbidden for non XBGR formats */

 TOFIX support mode formats */

 TOFIX support mode formats */

 TOFIX support mode formats */

 Handle AFBC Decoder reset manually */

 This will enable the RDMA replaying the register writes on vsync */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 * Copyright (C) 2015 Amlogic, Inc. All rights reserved.

 * Copyright (C) 2014 Endless Mobile

 *

 * Written by:

 *     Jasper St. Pierre <jstpierre@mecheye.net>

 HHI VDAC Registers */

 0xbd offset in data sheet */

 0xbd offset in data sheet */

 0xbe offset in data sheet */

 0xbe offset in data sheet */

 Supported Modes */

 PAL */

 NTSC */

 Connector */

 FIXME: Add load-detect or jack-detect if possible */

 Validate the modes added in get_modes */

 Encoder */

 Disable CVBS VDAC */

 VDAC0 source is not from ATV */

 Setup 27MHz vclk2 for ENCI and VDAC */

 Connector */

 Encoder */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2018 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 * Copyright (C) 2015 Amlogic, Inc. All rights reserved.

 VD1_IF0_GEN_REG */

 VD1_IF0_CANVAS0 */

 VD1_IF0_LUMA_X0 VD1_IF0_CHROMA_X0 */

 VD1_IF0_LUMA_Y0 VD1_IF0_CHROMA_Y0 */

 VD1_IF0_GEN_REG2 */

 VIU_VD1_FMT_CTRL */

 VPP_POSTBLEND_VD1_H_START_END */

 VPP_POSTBLEND_VD1_V_START_END */

 VPP_BLEND_VD2_V_START_END */

 VIU_VD1_FMT_W */

 VPP_HSC_REGION12_STARTP VPP_HSC_REGION34_STARTP */

 AFBC_ENABLE */

 AFBC_MODE */

 AFBC_SIZE_IN */

 AFBC_DEC_DEF_COLOR */

 AFBC_CONV_CTRL */

 AFBC_LBUF_DEPTH */

 AFBC_OUT_XSCOPE/AFBC_SIZE_OUT */

 AFBC_OUT_YSCOPE */

 AFBC_VD_CFMT_CTRL */

 AFBC_VD_CFMT_W */

 AFBC_MIF_HOR_SCOPE */

 AFBC_MIF_VER_SCOPE */

 AFBC_PIXEL_HOR_SCOPE */

 AFBC_PIXEL_VER_SCOPE */

 AFBC_VD_CFMT_H */

 Takes a fixed 16.16 number and converts it to integer. */

 Vertical */

	/*

	 * TOFIX: Input frames are handled and scaled like progressive frames,

	 * proper handling of interlaced field input frames need to be figured

	 * out using the proper framebuffer flags set by userspace.

 Horizontal */

 420: horizontal / 2, vertical / 4 */

 AFBC Only formats */

 Setup scaler params */

 Default values for RGB888/YUV444 */

 None will match for AFBC Only formats */

 TOFIX DRM_FORMAT_RGB888 should be supported */

 /2 */

 /2 */

 /2 */

 /4 */

 /2 */

 /2 */

 /2 */

 /4 */

 /4 */

 /2 */

 /4 */

 /4 */

 Update Canvas with buffer address */

			/*

			 * In Scatter mode, the header contains the physical

			 * body content layout, thus the body content

			 * size isn't needed.

 Default mode is 4k per superblock */

 8bit mem saving mode is 3072bytes per superblock */

 Header is after body content */

 Disable VD1 */

 Amlogic FBC Only */

 Amlogic FBC Only */

 For now, VD Overlay plane is always on the back */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 * Copyright (C) 2014 Endless Mobile

 *

 * Written by:

 *     Jasper St. Pierre <jstpierre@mecheye.net>

/**

 * DOC: Video Processing Unit

 *

 * VPU Handles the Global Video Processing, it includes management of the

 * clocks gates, blocks reset lines and power domains.

 *

 * What is missing :

 *

 * - Full reset of entire video processing HW blocks

 * - Scaling and setup of the VPU clock

 * - Bus clock gates

 * - Powering up video processing HW blocks

 * - Powering Up HDMI controller and PHY

	/*

	 * We need 64bytes aligned stride, and PAGE aligned size

 CMA Ops */

 Misc */

 Parses each endpoint and check if remote exists */

 If the endpoint node exists, consider it enabled */

	/*

	 * Slave dc0 and dc5 connected to master port 1.

	 * By default other slaves are connected to master port 0.

 Slave dc0 connected to master port 1 */

 Slave dc4 and dc7 connected to master port 1 */

 Slave dc1 connected to master port 1 */

 S805X/S805Y HDMI PLL won't lock for HDMI PHY freq > 1,65GHz */

 sentinel */ },

 Checks if an output connector is available */

 Simply ioremap since it may be a shared register zone */

 Assign limits per soc revision/package */

	/*

	 * Remove early framebuffers (ie. simplefb). The framebuffer can be

	 * located anywhere in RAM

 Hardware Initialization */

 Encoder Initialization */

 Possible connectors nodes to ignore */

 If node is a connector, return and do not add to match table */

 Ignore parent endpoint */

 If some endpoints were found, initialize the nodes */

 If no output endpoints were available, simply bail out */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 * Copyright (C) 2015 Amlogic, Inc. All rights reserved.

 * Copyright (C) 2014 Endless Mobile

 *

 * Written by:

 *     Jasper St. Pierre <jstpierre@mecheye.net>

 OSD_SCI_WH_M1 */

 OSD_SCO_H_START_END */

 OSD_SCO_V_START_END */

 OSD_SC_CTRL0 */

 OSD_VSC_CTRL0 */

 OSD_VSC_INI_PHASE */

 OSD_HSC_CTRL0 */

 VPP_OSD_VSC_PHASE_STEP */

 VPP_OSD_HSC_PHASE_STEP */

	/*

	 * Only allow :

	 * - Upscaling up to 5x, vertical and horizontal

	 * - Final coordinates must match crtc size

 Takes a fixed 16.16 number and converts it to integer. */

	/*

	 * Update Coordinates

	 * Update Formats

	 * Update Buffer

	 * Enable Plane

 Check if AFBC decoder is required for this buffer */

 Enable OSD and BLK0, set max global alpha */

 Set up BLK0 to point to the right canvas */

 This is the internal decoding memory address */

 On GXBB, Use the old non-HDR RGB2YUV converter */

 For XRGB, replace the pixel's alpha by 0xFF */

 For ARGB, use the pixel's alpha */

 Default scaler parameters */

	/*

	 * When the output is interlaced, the OSD must switch between

	 * each field using the INTERLACE_SEL_ODD (0) of VIU_OSD1_BLK0_CFG_W0

	 * at each vsync.

	 * But the vertical scaler can provide such funtionnality if

	 * is configured for 2:1 scaling with interlace options enabled.

 In interlaced mode, scaler is always active */

 Enable OSD Scaler */

 In interlaced mode, vertical scaler is always active */

 Horizontal scaler is only used if width does not match */

	/*

	 * The format of these registers is (x2 << 16 | x1),

	 * where x2 is exclusive.

	 * e.g. +30x1920 would be (1919 << 16) | 30

 Update Canvas with buffer address */

 Calculate decoder write stride */

 Reset OSD1 before enabling it on GXL+ SoCs */

 Disable OSD1 */

 SPLIT mandates SPARSE, RGB modes mandates YTR */

	/*

	 * - TOFIX Support AFBC modifiers for YUV formats (16x16 + TILED)

	 * - SPLIT is mandatory for performances reasons when in 16x16

	 *   block size

	 * - 32x8 block size + SPLIT is mandatory with 4K frame size

	 *   for performances reasons

 For now, OSD Primary plane is always on the front */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 int param */

 support boolean values too */

 unsigned int param */

 support boolean values too */

 char * param */

 add a subdirectory with files for each i915 param */

	/*

	 * Note: We could create files for params needing special handling

	 * here. Set mode in params to 0 to skip the generic create file, or

	 * just let the generic create file fail silently with -EEXIST.

/*

 * SPDX-License-Identifier: MIT

 *

 * (C) Copyright 2016 Intel Corporation

 after WQ_FLAG_* for safety */

 flush the change in state before reallocation */

 0 -> -1 [done] */

	/*

	 * To prevent unbounded recursion as we traverse the graph of

	 * i915_sw_fences, we move the entry list from this, the next ready

	 * fence, to the tail of the original fence's entry list

	 * (and so added to the list to be woken).

	/*

	 * It is only safe to add a new await to the fence while it has

	 * not yet been signaled (i.e. there are still existing signalers).

 The dependency graph must be acyclic. */

 fence already signaled */

 fence already signaled */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

 For the funcs/ops export only */

/**

 * DOC: TTM support structure

 *

 * The code in this file deals with setting up memory managers for TTM

 * LMEM and MOCK regions and converting the output from

 * the managers to struct sg_table, Basically providing the mapping from

 * i915 GEM regions to TTM memory types and resource managers.

/**

 * intel_region_ttm_device_init - Initialize a TTM device

 * @dev_priv: Pointer to an i915 device private structure.

 *

 * Return: 0 on success, negative error code on failure.

/**

 * intel_region_ttm_device_fini - Finalize a TTM device

 * @dev_priv: Pointer to an i915 device private structure.

/*

 * Map the i915 memory regions to TTM memory types. We use the

 * driver-private types for now, reserving TTM_PL_VRAM for stolen

 * memory and TTM_PL_TT for GGTT use if decided to implement this.

/**

 * intel_region_ttm_init - Initialize a memory region for TTM.

 * @mem: The region to initialize.

 *

 * This function initializes a suitable TTM resource manager for the

 * region, and if it's a LMEM region type, attaches it to the TTM

 * device. MOCK regions are NOT attached to the TTM device, since we don't

 * have one for the mock selftests.

 *

 * Return: 0 on success, negative error code on failure.

/**

 * intel_region_ttm_fini - Finalize a TTM region.

 * @mem: The memory region

 *

 * This functions takes down the TTM resource manager associated with the

 * memory region, and if it was registered with the TTM device,

 * removes that registration.

/**

 * intel_region_ttm_resource_to_st - Convert an opaque TTM resource manager resource

 * to an sg_table.

 * @mem: The memory region.

 * @res: The resource manager resource obtained from the TTM resource manager.

 *

 * The gem backends typically use sg-tables for operations on the underlying

 * io_memory. So provide a way for the backends to translate the

 * nodes they are handed from TTM to sg-tables.

 *

 * Return: A malloced sg_table on success, an error pointer on failure.

/**

 * intel_region_ttm_resource_alloc - Allocate memory resources from a region

 * @mem: The memory region,

 * @size: The requested size in bytes

 * @flags: Allocation flags

 *

 * This functionality is provided only for callers that need to allocate

 * memory from standalone TTM range managers, without the TTM eviction

 * functionality. Don't use if you are not completely sure that's the

 * case. The returned opaque node can be converted to an sg_table using

 * intel_region_ttm_resource_to_st(), and can be freed using

 * intel_region_ttm_resource_free().

 *

 * Return: A valid pointer on success, an error pointer on failure.

/**

 * intel_region_ttm_resource_free - Free a resource allocated from a resource manager

 * @mem: The region the resource was allocated from.

 * @res: The opaque resource representing an allocation.

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

/**

 * i915_memcpy_from_wc: perform an accelerated *aligned* read from WC

 * @dst: destination pointer

 * @src: source pointer

 * @len: how many bytes to copy

 *

 * i915_memcpy_from_wc copies @len bytes from @src to @dst using

 * non-temporal instructions where available. Note that all arguments

 * (@src, @dst) must be aligned to 16 bytes and @len must be a multiple

 * of 16.

 *

 * To test whether accelerated reads from WC are supported, use

 * i915_memcpy_from_wc(NULL, NULL, 0);

 *

 * Returns true if the copy was successful, false if the preconditions

 * are not met.

/**

 * i915_unaligned_memcpy_from_wc: perform a mostly accelerated read from WC

 * @dst: destination pointer

 * @src: source pointer

 * @len: how many bytes to copy

 *

 * Like i915_memcpy_from_wc(), the unaligned variant copies @len bytes from

 * @src to @dst using * non-temporal instructions where available, but

 * accepts that its arguments may not be aligned, but are valid for the

 * potential 16-byte read past the end.

	/*

	 * Some hypervisors (e.g. KVM) don't support VEX-prefix instructions

	 * emulation. So don't enable movntdqa in hypervisor guest.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2017-2019 Intel Corporation

/**

 * DOC: WOPCM Layout

 *

 * The layout of the WOPCM will be fixed after writing to GuC WOPCM size and

 * offset registers whose values are calculated and determined by HuC/GuC

 * firmware size and set of hardware requirements/restrictions as shown below:

 *

 * ::

 *

 *    +=========> +====================+ <== WOPCM Top

 *    ^           |  HW contexts RSVD  |

 *    |     +===> +====================+ <== GuC WOPCM Top

 *    |     ^     |                    |

 *    |     |     |                    |

 *    |     |     |                    |

 *    |    GuC    |                    |

 *    |   WOPCM   |                    |

 *    |    Size   +--------------------+

 *  WOPCM   |     |    GuC FW RSVD     |

 *    |     |     +--------------------+

 *    |     |     |   GuC Stack RSVD   |

 *    |     |     +------------------- +

 *    |     v     |   GuC WOPCM RSVD   |

 *    |     +===> +====================+ <== GuC WOPCM base

 *    |           |     WOPCM RSVD     |

 *    |           +------------------- + <== HuC Firmware Top

 *    v           |      HuC FW        |

 *    +=========> +====================+ <== WOPCM Base

 *

 * GuC accessible WOPCM starts at GuC WOPCM base and ends at GuC WOPCM top.

 * The top part of the WOPCM is reserved for hardware contexts (e.g. RC6

 * context).

 Default WOPCM size is 2MB from Gen11, 1MB on previous platforms */

 16KB WOPCM (RSVD WOPCM) is reserved from HuC firmware top. */

 16KB reserved at the beginning of GuC WOPCM. */

 8KB from GUC_WOPCM_RESERVED is reserved for GuC stack. */

 GuC WOPCM Offset value needs to be aligned to 16KB. */

 24KB at the end of WOPCM is reserved for RC6 CTX on BXT. */

 36KB WOPCM reserved at the end of WOPCM on ICL. */

 128KB from GUC_WOPCM_RESERVED is reserved for FW on Gen9. */

/**

 * intel_wopcm_init_early() - Early initialization of the WOPCM.

 * @wopcm: pointer to intel_wopcm.

 *

 * Setup the size of WOPCM which will be used by later on WOPCM partitioning.

	/*

	 * GuC WOPCM size shall be at least a dword larger than the offset from

	 * WOPCM base (GuC WOPCM offset from WOPCM base + GEN9_GUC_WOPCM_OFFSET)

	 * due to hardware limitation on Gen9.

	/*

	 * On Gen9, hardware requires the total available GuC WOPCM

	 * size to be larger than or equal to HuC firmware size. Otherwise,

	 * firmware uploading would fail.

/**

 * intel_wopcm_init() - Initialize the WOPCM structure.

 * @wopcm: pointer to intel_wopcm.

 *

 * This function will partition WOPCM space based on GuC and HuC firmware sizes

 * and will allocate max remaining for use by GuC. This function will also

 * enforce platform dependent hardware restrictions on GuC WOPCM offset and

 * size. It will fail the WOPCM init if any of these checks fail, so that the

 * following WOPCM registers setup and GuC firmware uploading would be aborted.

	/*

	 * Aligned value of guc_wopcm_base will determine available WOPCM space

	 * for HuC firmware and mandatory reserved area.

	/*

	 * Need to clamp guc_wopcm_base now to make sure the following math is

	 * correct. Formal check of whole WOPCM layout will be done below.

 Aligned remainings of usable WOPCM space can be assigned to GuC. */

/*

 * Copyright Â© 2012-2014 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Eugeni Dodonov <eugeni.dodonov@intel.com>

 *    Daniel Vetter <daniel.vetter@ffwll.ch>

 *

/**

 * DOC: runtime pm

 *

 * The i915 driver supports dynamic enabling and disabling of entire hardware

 * blocks at runtime. This is especially important on the display side where

 * software is supposed to control many power gates manually on recent hardware,

 * since on the GT side a lot of the power management is done by the hardware.

 * But even there some manual control at the device level is required.

 *

 * Since i915 supports a diverse set of platforms with a unified codebase and

 * hardware engineers just love to shuffle functionality around between power

 * domains there's a sizeable amount of indirection required. This file provides

 * generic functions to the driver for grabbing and releasing references for

 * abstract power domains. It then maps those to the actual power wells

 * present for a given platform.

/**

 * intel_runtime_pm_get_raw - grab a raw runtime pm reference

 * @rpm: the intel_runtime_pm structure

 *

 * This is the unlocked version of intel_display_power_is_enabled() and should

 * only be used from error capture and recovery code where deadlocks are

 * possible.

 * This function grabs a device-level runtime pm reference (mostly used for

 * asynchronous PM management from display code) and ensures that it is powered

 * up. Raw references are not considered during wakelock assert checks.

 *

 * Any runtime pm reference obtained by this function must have a symmetric

 * call to intel_runtime_pm_put_raw() to release the reference again.

 *

 * Returns: the wakeref cookie to pass to intel_runtime_pm_put_raw(), evaluates

 * as True if the wakeref was acquired, or False otherwise.

/**

 * intel_runtime_pm_get - grab a runtime pm reference

 * @rpm: the intel_runtime_pm structure

 *

 * This function grabs a device-level runtime pm reference (mostly used for GEM

 * code to ensure the GTT or GT is on) and ensures that it is powered up.

 *

 * Any runtime pm reference obtained by this function must have a symmetric

 * call to intel_runtime_pm_put() to release the reference again.

 *

 * Returns: the wakeref cookie to pass to intel_runtime_pm_put()

/**

 * __intel_runtime_pm_get_if_active - grab a runtime pm reference if device is active

 * @rpm: the intel_runtime_pm structure

 * @ignore_usecount: get a ref even if dev->power.usage_count is 0

 *

 * This function grabs a device-level runtime pm reference if the device is

 * already active and ensures that it is powered up. It is illegal to try

 * and access the HW should intel_runtime_pm_get_if_active() report failure.

 *

 * If @ignore_usecount is true, a reference will be acquired even if there is no

 * user requiring the device to be powered up (dev->power.usage_count == 0).

 * If the function returns false in this case then it's guaranteed that the

 * device's runtime suspend hook has been called already or that it will be

 * called (and hence it's also guaranteed that the device's runtime resume

 * hook will be called eventually).

 *

 * Any runtime pm reference obtained by this function must have a symmetric

 * call to intel_runtime_pm_put() to release the reference again.

 *

 * Returns: the wakeref cookie to pass to intel_runtime_pm_put(), evaluates

 * as True if the wakeref was acquired, or False otherwise.

		/*

		 * In cases runtime PM is disabled by the RPM core and we get

		 * an -EINVAL return value we are not supposed to call this

		 * function, since the power state is undefined. This applies

		 * atm to the late/early system suspend/resume handlers.

/**

 * intel_runtime_pm_get_noresume - grab a runtime pm reference

 * @rpm: the intel_runtime_pm structure

 *

 * This function grabs a device-level runtime pm reference (mostly used for GEM

 * code to ensure the GTT or GT is on).

 *

 * It will _not_ power up the device but instead only check that it's powered

 * on.  Therefore it is only valid to call this functions from contexts where

 * the device is known to be powered up and where trying to power it up would

 * result in hilarity and deadlocks. That pretty much means only the system

 * suspend/resume code where this is used to grab runtime pm references for

 * delayed setup down in work items.

 *

 * Any runtime pm reference obtained by this function must have a symmetric

 * call to intel_runtime_pm_put() to release the reference again.

 *

 * Returns: the wakeref cookie to pass to intel_runtime_pm_put()

/**

 * intel_runtime_pm_put_raw - release a raw runtime pm reference

 * @rpm: the intel_runtime_pm structure

 * @wref: wakeref acquired for the reference that is being released

 *

 * This function drops the device-level runtime pm reference obtained by

 * intel_runtime_pm_get_raw() and might power down the corresponding

 * hardware block right away if this is the last reference.

/**

 * intel_runtime_pm_put_unchecked - release an unchecked runtime pm reference

 * @rpm: the intel_runtime_pm structure

 *

 * This function drops the device-level runtime pm reference obtained by

 * intel_runtime_pm_get() and might power down the corresponding

 * hardware block right away if this is the last reference.

 *

 * This function exists only for historical reasons and should be avoided in

 * new code, as the correctness of its use cannot be checked. Always use

 * intel_runtime_pm_put() instead.

/**

 * intel_runtime_pm_put - release a runtime pm reference

 * @rpm: the intel_runtime_pm structure

 * @wref: wakeref acquired for the reference that is being released

 *

 * This function drops the device-level runtime pm reference obtained by

 * intel_runtime_pm_get() and might power down the corresponding

 * hardware block right away if this is the last reference.

/**

 * intel_runtime_pm_enable - enable runtime pm

 * @rpm: the intel_runtime_pm structure

 *

 * This function enables runtime pm at the end of the driver load sequence.

 *

 * Note that this function does currently not enable runtime pm for the

 * subordinate display power domains. That is done by

 * intel_power_domains_enable().

	/*

	 * Disable the system suspend direct complete optimization, which can

	 * leave the device suspended skipping the driver's suspend handlers

	 * if the device was already runtime suspended. This is needed due to

	 * the difference in our runtime and system suspend sequence and

	 * becaue the HDA driver may require us to enable the audio power

	 * domain during system suspend.

 10s */

	/*

	 * Take a permanent reference to disable the RPM functionality and drop

	 * it only when unloading the driver. Use the low level get/put helpers,

	 * so the driver's own RPM reference tracking asserts also work on

	 * platforms without RPM support.

	/*

	 * The core calls the driver load handler with an RPM reference held.

	 * We drop that here and will reacquire it during unloading in

	 * intel_power_domains_fini().

 Transfer rpm ownership back to core */

/*

 * Copyright Â© 2015-2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *   Robert Bragg <robert@sixbynine.org>

/**

 * DOC: i915 Perf Overview

 *

 * Gen graphics supports a large number of performance counters that can help

 * driver and application developers understand and optimize their use of the

 * GPU.

 *

 * This i915 perf interface enables userspace to configure and open a file

 * descriptor representing a stream of GPU metrics which can then be read() as

 * a stream of sample records.

 *

 * The interface is particularly suited to exposing buffered metrics that are

 * captured by DMA from the GPU, unsynchronized with and unrelated to the CPU.

 *

 * Streams representing a single context are accessible to applications with a

 * corresponding drm file descriptor, such that OpenGL can use the interface

 * without special privileges. Access to system-wide metrics requires root

 * privileges by default, unless changed via the dev.i915.perf_event_paranoid

 * sysctl option.

 *

/**

 * DOC: i915 Perf History and Comparison with Core Perf

 *

 * The interface was initially inspired by the core Perf infrastructure but

 * some notable differences are:

 *

 * i915 perf file descriptors represent a "stream" instead of an "event"; where

 * a perf event primarily corresponds to a single 64bit value, while a stream

 * might sample sets of tightly-coupled counters, depending on the

 * configuration.  For example the Gen OA unit isn't designed to support

 * orthogonal configurations of individual counters; it's configured for a set

 * of related counters. Samples for an i915 perf stream capturing OA metrics

 * will include a set of counter values packed in a compact HW specific format.

 * The OA unit supports a number of different packing formats which can be

 * selected by the user opening the stream. Perf has support for grouping

 * events, but each event in the group is configured, validated and

 * authenticated individually with separate system calls.

 *

 * i915 perf stream configurations are provided as an array of u64 (key,value)

 * pairs, instead of a fixed struct with multiple miscellaneous config members,

 * interleaved with event-type specific members.

 *

 * i915 perf doesn't support exposing metrics via an mmap'd circular buffer.

 * The supported metrics are being written to memory by the GPU unsynchronized

 * with the CPU, using HW specific packing formats for counter sets. Sometimes

 * the constraints on HW configuration require reports to be filtered before it

 * would be acceptable to expose them to unprivileged applications - to hide

 * the metrics of other processes/contexts. For these use cases a read() based

 * interface is a good fit, and provides an opportunity to filter data as it

 * gets copied from the GPU mapped buffers to userspace buffers.

 *

 *

 * Issues hit with first prototype based on Core Perf

 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 *

 * The first prototype of this driver was based on the core perf

 * infrastructure, and while we did make that mostly work, with some changes to

 * perf, we found we were breaking or working around too many assumptions baked

 * into perf's currently cpu centric design.

 *

 * In the end we didn't see a clear benefit to making perf's implementation and

 * interface more complex by changing design assumptions while we knew we still

 * wouldn't be able to use any existing perf based userspace tools.

 *

 * Also considering the Gen specific nature of the Observability hardware and

 * how userspace will sometimes need to combine i915 perf OA metrics with

 * side-band OA data captured via MI_REPORT_PERF_COUNT commands; we're

 * expecting the interface to be used by a platform specific userspace such as

 * OpenGL or tools. This is to say; we aren't inherently missing out on having

 * a standard vendor/architecture agnostic interface by not using perf.

 *

 *

 * For posterity, in case we might re-visit trying to adapt core perf to be

 * better suited to exposing i915 metrics these were the main pain points we

 * hit:

 *

 * - The perf based OA PMU driver broke some significant design assumptions:

 *

 *   Existing perf pmus are used for profiling work on a cpu and we were

 *   introducing the idea of _IS_DEVICE pmus with different security

 *   implications, the need to fake cpu-related data (such as user/kernel

 *   registers) to fit with perf's current design, and adding _DEVICE records

 *   as a way to forward device-specific status records.

 *

 *   The OA unit writes reports of counters into a circular buffer, without

 *   involvement from the CPU, making our PMU driver the first of a kind.

 *

 *   Given the way we were periodically forward data from the GPU-mapped, OA

 *   buffer to perf's buffer, those bursts of sample writes looked to perf like

 *   we were sampling too fast and so we had to subvert its throttling checks.

 *

 *   Perf supports groups of counters and allows those to be read via

 *   transactions internally but transactions currently seem designed to be

 *   explicitly initiated from the cpu (say in response to a userspace read())

 *   and while we could pull a report out of the OA buffer we can't

 *   trigger a report from the cpu on demand.

 *

 *   Related to being report based; the OA counters are configured in HW as a

 *   set while perf generally expects counter configurations to be orthogonal.

 *   Although counters can be associated with a group leader as they are

 *   opened, there's no clear precedent for being able to provide group-wide

 *   configuration attributes (for example we want to let userspace choose the

 *   OA unit report format used to capture all counters in a set, or specify a

 *   GPU context to filter metrics on). We avoided using perf's grouping

 *   feature and forwarded OA reports to userspace via perf's 'raw' sample

 *   field. This suited our userspace well considering how coupled the counters

 *   are when dealing with normalizing. It would be inconvenient to split

 *   counters up into separate events, only to require userspace to recombine

 *   them. For Mesa it's also convenient to be forwarded raw, periodic reports

 *   for combining with the side-band raw reports it captures using

 *   MI_REPORT_PERF_COUNT commands.

 *

 *   - As a side note on perf's grouping feature; there was also some concern

 *     that using PERF_FORMAT_GROUP as a way to pack together counter values

 *     would quite drastically inflate our sample sizes, which would likely

 *     lower the effective sampling resolutions we could use when the available

 *     memory bandwidth is limited.

 *

 *     With the OA unit's report formats, counters are packed together as 32

 *     or 40bit values, with the largest report size being 256 bytes.

 *

 *     PERF_FORMAT_GROUP values are 64bit, but there doesn't appear to be a

 *     documented ordering to the values, implying PERF_FORMAT_ID must also be

 *     used to add a 64bit ID before each value; giving 16 bytes per counter.

 *

 *   Related to counter orthogonality; we can't time share the OA unit, while

 *   event scheduling is a central design idea within perf for allowing

 *   userspace to open + enable more events than can be configured in HW at any

 *   one time.  The OA unit is not designed to allow re-configuration while in

 *   use. We can't reconfigure the OA unit without losing internal OA unit

 *   state which we can't access explicitly to save and restore. Reconfiguring

 *   the OA unit is also relatively slow, involving ~100 register writes. From

 *   userspace Mesa also depends on a stable OA configuration when emitting

 *   MI_REPORT_PERF_COUNT commands and importantly the OA unit can't be

 *   disabled while there are outstanding MI_RPC commands lest we hang the

 *   command streamer.

 *

 *   The contents of sample records aren't extensible by device drivers (i.e.

 *   the sample_type bits). As an example; Sourab Gupta had been looking to

 *   attach GPU timestamps to our OA samples. We were shoehorning OA reports

 *   into sample records by using the 'raw' field, but it's tricky to pack more

 *   than one thing into this field because events/core.c currently only lets a

 *   pmu give a single raw data pointer plus len which will be copied into the

 *   ring buffer. To include more than the OA report we'd have to copy the

 *   report into an intermediate larger buffer. I'd been considering allowing a

 *   vector of data+len values to be specified for copying the raw data, but

 *   it felt like a kludge to being using the raw field for this purpose.

 *

 * - It felt like our perf based PMU was making some technical compromises

 *   just for the sake of using perf:

 *

 *   perf_event_open() requires events to either relate to a pid or a specific

 *   cpu core, while our device pmu related to neither.  Events opened with a

 *   pid will be automatically enabled/disabled according to the scheduling of

 *   that process - so not appropriate for us. When an event is related to a

 *   cpu id, perf ensures pmu methods will be invoked via an inter process

 *   interrupt on that core. To avoid invasive changes our userspace opened OA

 *   perf events for a specific cpu. This was workable but it meant the

 *   majority of the OA driver ran in atomic context, including all OA report

 *   forwarding, which wasn't really necessary in our case and seems to make

 *   our locking requirements somewhat complex as we handled the interaction

 *   with the rest of the i915 driver.

/* HW requires this to be a power of two, between 128k and 16M, though driver

 * is currently generally designed assuming the largest 16M size is used such

 * that the overflow cases are unlikely in normal operation.

/**

 * DOC: OA Tail Pointer Race

 *

 * There's a HW race condition between OA unit tail pointer register updates and

 * writes to memory whereby the tail pointer can sometimes get ahead of what's

 * been written out to the OA buffer so far (in terms of what's visible to the

 * CPU).

 *

 * Although this can be observed explicitly while copying reports to userspace

 * by checking for a zeroed report-id field in tail reports, we want to account

 * for this earlier, as part of the oa_buffer_check_unlocked to avoid lots of

 * redundant read() attempts.

 *

 * We workaround this issue in oa_buffer_check_unlocked() by reading the reports

 * in the OA buffer, starting from the tail reported by the HW until we find a

 * report with its first 2 dwords not 0 meaning its previous report is

 * completely in memory and ready to be read. Those dwords are also set to 0

 * once read and the whole buffer is cleared upon OA buffer initialization. The

 * first dword is the reason for this report while the second is the timestamp,

 * making the chances of having those 2 fields at 0 fairly unlikely. A more

 * detailed explanation is available in oa_buffer_check_unlocked().

 *

 * Most of the implementation details for this workaround are in

 * oa_buffer_check_unlocked() and _append_oa_reports()

 *

 * Note for posterity: previously the driver used to define an effective tail

 * pointer that lagged the real pointer by a 'tail margin' measured in bytes

 * derived from %OA_TAIL_MARGIN_NSEC and the configured sampling frequency.

 * This was flawed considering that the OA unit may also automatically generate

 * non-periodic reports (such as on context switch) or the OA unit may be

 * enabled without any periodic sampling.

/* The default frequency for checking whether the OA unit has written new

 * reports to the circular OA buffer...

 for sysctl proc_dointvec_minmax of dev.i915.perf_stream_paranoid */

/* The maximum exponent the hardware accepts is 63 (essentially it selects one

 * of the 64bit timestamp bits to trigger reports from) but there's currently

 * no known use case for sampling as infrequently as once per 47 thousand years.

 *

 * Since the timestamps included in OA reports are only 32bits it seems

 * reasonable to limit the OA exponent where it's still possible to account for

 * overflow in OA report timestamps.

 On Gen8+ automatically triggered OA reports include a 'reason' field... */

/* For sysctl proc_dointvec_minmax of i915_oa_max_sample_rate

 *

 * The highest sampling frequency we can theoretically program the OA unit

 * with is always half the timestamp frequency: E.g. 6.25Mhz for Haswell.

 *

 * Initialized just before we register the sysctl parameter.

/* Theoretically we can program the OA unit to sample every 160ns but don't

 * allow that by default unless root...

 *

 * The default threshold of 100000Hz is based on perf's similar

 * kernel.perf_event_max_sample_rate sysctl parameter.

/* XXX: beware if future OA HW adds new report formats that the current

 * code assumes all reports have a power-of-two size and ~(size - 1) can

 * be used as a mask to align the OA tail pointer.

 A29_B8_C8 Disallowed as 192 bytes doesn't factor into buffer size */

/**

 * struct perf_open_properties - for validated properties given to open a stream

 * @sample_flags: `DRM_I915_PERF_PROP_SAMPLE_*` properties are tracked as flags

 * @single_context: Whether a single or all gpu contexts should be monitored

 * @hold_preemption: Whether the preemption is disabled for the filtered

 *                   context

 * @ctx_handle: A gem ctx handle for use with @single_context

 * @metrics_set: An ID for an OA unit metric set advertised via sysfs

 * @oa_format: An OA unit HW report format

 * @oa_periodic: Whether to enable periodic OA unit sampling

 * @oa_period_exponent: The OA unit sampling period is derived from this

 * @engine: The engine (typically rcs0) being monitored by the OA unit

 * @has_sseu: Whether @sseu was specified by userspace

 * @sseu: internal SSEU configuration computed either from the userspace

 *        specified configuration in the opening parameters or a default value

 *        (see get_default_sseu_config())

 * @poll_oa_period: The period in nanoseconds at which the CPU will check for OA

 * data availability

 *

 * As read_properties_unlocked() enumerates and validates the properties given

 * to open a stream of metrics the configuration is built up in the structure

 * which starts out zero initialized.

 OA sampling state */

/**

 * oa_buffer_check_unlocked - check for data and update tail ptr state

 * @stream: i915 stream instance

 *

 * This is either called via fops (for blocking reads in user ctx) or the poll

 * check hrtimer (atomic ctx) to check the OA buffer tail pointer and check

 * if there is data available for userspace to read.

 *

 * This function is central to providing a workaround for the OA unit tail

 * pointer having a race with respect to what data is visible to the CPU.

 * It is responsible for reading tail pointers from the hardware and giving

 * the pointers time to 'age' before they are made available for reading.

 * (See description of OA_TAIL_MARGIN_NSEC above for further details.)

 *

 * Besides returning true when there is data available to read() this function

 * also updates the tail, aging_tail and aging_timestamp in the oa_buffer

 * object.

 *

 * Note: It's safe to read OA config state here unlocked, assuming that this is

 * only called while the stream is enabled, while the global OA configuration

 * can't be modified.

 *

 * Returns: %true if the OA buffer contains data, else %false

	/* We have to consider the (unlikely) possibility that read() errors

	 * could result in an OA buffer reset which might reset the head and

	 * tail state.

	/* The tail pointer increases in 64 byte increments,

	 * not in report_size steps...

		/* If the HW tail hasn't move since the last check and the HW

		 * tail has been aging for long enough, declare it the new

		 * tail.

		/* NB: The head we observe here might effectively be a little

		 * out of date. If a read() is in progress, the head could be

		 * anywhere between this head and stream->oa_buffer.tail.

		/* Walk the stream backward until we find a report with dword 0

		 * & 1 not at 0. Since the circular buffer pointers progress by

		 * increments of 64 bytes and that reports can be up to 256

		 * bytes long, we can't tell whether a report has fully landed

		 * in memory before the first 2 dwords of the following report

		 * have effectively landed.

		 *

		 * This is assuming that the writes of the OA unit land in

		 * memory in the order they were written to.

		 * If not : (â¯Â°â¡Â°ï¼â¯ï¸µ â»ââ»

/**

 * append_oa_status - Appends a status record to a userspace read() buffer.

 * @stream: An i915-perf stream opened for OA metrics

 * @buf: destination buffer given by userspace

 * @count: the number of bytes userspace wants to read

 * @offset: (inout): the current position for writing into @buf

 * @type: The kind of status to report to userspace

 *

 * Writes a status record (such as `DRM_I915_PERF_RECORD_OA_REPORT_LOST`)

 * into the userspace read() buffer.

 *

 * The @buf @offset will only be updated on success.

 *

 * Returns: 0 on success, negative error code on failure.

/**

 * append_oa_sample - Copies single OA report into userspace read() buffer.

 * @stream: An i915-perf stream opened for OA metrics

 * @buf: destination buffer given by userspace

 * @count: the number of bytes userspace wants to read

 * @offset: (inout): the current position for writing into @buf

 * @report: A single OA report to (optionally) include as part of the sample

 *

 * The contents of a sample are configured through `DRM_I915_PERF_PROP_SAMPLE_*`

 * properties when opening a stream, tracked as `stream->sample_flags`. This

 * function copies the requested components of a single sample to the given

 * read() @buf.

 *

 * The @buf @offset will only be updated on success.

 *

 * Returns: 0 on success, negative error code on failure.

/**

 * gen8_append_oa_reports - Copies all buffered OA reports into

 *			    userspace read() buffer.

 * @stream: An i915-perf stream opened for OA metrics

 * @buf: destination buffer given by userspace

 * @count: the number of bytes userspace wants to read

 * @offset: (inout): the current position for writing into @buf

 *

 * Notably any error condition resulting in a short read (-%ENOSPC or

 * -%EFAULT) will be returned even though one or more records may

 * have been successfully copied. In this case it's up to the caller

 * to decide if the error should be squashed before returning to

 * userspace.

 *

 * Note: reports are consumed from the head, and appended to the

 * tail, so the tail chases the head?... If you think that's mad

 * and back-to-front you're not alone, but this follows the

 * Gen PRM naming convention.

 *

 * Returns: 0 on success, negative error code on failure.

	/*

	 * NB: oa_buffer.head/tail include the gtt_offset which we don't want

	 * while indexing relative to oa_buf_base.

	/*

	 * An out of bounds or misaligned head or tail pointer implies a driver

	 * bug since we validate + align the tail pointers we read from the

	 * hardware and we are in full control of the head pointer which should

	 * only be incremented by multiples of the report size (notably also

	 * all a power of two).

 none */;

		/*

		 * All the report sizes factor neatly into the buffer

		 * size so we never expect to see a report split

		 * between the beginning and end of the buffer.

		 *

		 * Given the initial alignment check a misalignment

		 * here would imply a driver bug that would result

		 * in an overrun.

		/*

		 * The reason field includes flags identifying what

		 * triggered this specific report (mostly timer

		 * triggered or e.g. due to a context switch).

		 *

		 * This field is never expected to be zero so we can

		 * check that the report isn't invalid before copying

		 * it to userspace...

		/*

		 * Squash whatever is in the CTX_ID field if it's marked as

		 * invalid to be sure we avoid false-positive, single-context

		 * filtering below...

		 *

		 * Note: that we don't clear the valid_ctx_bit so userspace can

		 * understand that the ID has been squashed by the kernel.

		/*

		 * NB: For Gen 8 the OA unit no longer supports clock gating

		 * off for a specific context and the kernel can't securely

		 * stop the counters from updating as system-wide / global

		 * values.

		 *

		 * Automatic reports now include a context ID so reports can be

		 * filtered on the cpu but it's not worth trying to

		 * automatically subtract/hide counter progress for other

		 * contexts while filtering since we can't stop userspace

		 * issuing MI_REPORT_PERF_COUNT commands which would still

		 * provide a side-band view of the real values.

		 *

		 * To allow userspace (such as Mesa/GL_INTEL_performance_query)

		 * to normalize counters for a single filtered context then it

		 * needs be forwarded bookend context-switch reports so that it

		 * can track switches in between MI_REPORT_PERF_COUNT commands

		 * and can itself subtract/ignore the progress of counters

		 * associated with other contexts. Note that the hardware

		 * automatically triggers reports when switching to a new

		 * context which are tagged with the ID of the newly active

		 * context. To avoid the complexity (and likely fragility) of

		 * reading ahead while parsing reports to try and minimize

		 * forwarding redundant context switch reports (i.e. between

		 * other, unrelated contexts) we simply elect to forward them

		 * all.

		 *

		 * We don't rely solely on the reason field to identify context

		 * switches since it's not-uncommon for periodic samples to

		 * identify a switch before any 'context switch' report.

			/*

			 * While filtering for a single context we avoid

			 * leaking the IDs of other contexts.

		/*

		 * Clear out the first 2 dword as a mean to detect unlanded

		 * reports.

		/*

		 * We removed the gtt_offset for the copy loop above, indexing

		 * relative to oa_buf_base so put back here...

/**

 * gen8_oa_read - copy status records then buffered OA reports

 * @stream: An i915-perf stream opened for OA metrics

 * @buf: destination buffer given by userspace

 * @count: the number of bytes userspace wants to read

 * @offset: (inout): the current position for writing into @buf

 *

 * Checks OA unit status registers and if necessary appends corresponding

 * status records for userspace (such as for a buffer full condition) and then

 * initiate appending any buffered OA reports.

 *

 * Updates @offset according to the number of bytes successfully copied into

 * the userspace buffer.

 *

 * NB: some data may be successfully copied to the userspace buffer

 * even if an error is returned, and this is reflected in the

 * updated @offset.

 *

 * Returns: zero on success or a negative error code

	/*

	 * We treat OABUFFER_OVERFLOW as a significant error:

	 *

	 * Although theoretically we could handle this more gracefully

	 * sometimes, some Gens don't correctly suppress certain

	 * automatically triggered reports in this condition and so we

	 * have to assume that old reports are now being trampled

	 * over.

	 *

	 * Considering how we don't currently give userspace control

	 * over the OA buffer size and always configure a large 16MB

	 * buffer, then a buffer overflow does anyway likely indicate

	 * that something has gone quite badly wrong.

		/*

		 * Note: .oa_enable() is expected to re-init the oabuffer and

		 * reset GEN8_OASTATUS for us

/**

 * gen7_append_oa_reports - Copies all buffered OA reports into

 *			    userspace read() buffer.

 * @stream: An i915-perf stream opened for OA metrics

 * @buf: destination buffer given by userspace

 * @count: the number of bytes userspace wants to read

 * @offset: (inout): the current position for writing into @buf

 *

 * Notably any error condition resulting in a short read (-%ENOSPC or

 * -%EFAULT) will be returned even though one or more records may

 * have been successfully copied. In this case it's up to the caller

 * to decide if the error should be squashed before returning to

 * userspace.

 *

 * Note: reports are consumed from the head, and appended to the

 * tail, so the tail chases the head?... If you think that's mad

 * and back-to-front you're not alone, but this follows the

 * Gen PRM naming convention.

 *

 * Returns: 0 on success, negative error code on failure.

	/* NB: oa_buffer.head/tail include the gtt_offset which we don't want

	 * while indexing relative to oa_buf_base.

	/* An out of bounds or misaligned head or tail pointer implies a driver

	 * bug since we validate + align the tail pointers we read from the

	 * hardware and we are in full control of the head pointer which should

	 * only be incremented by multiples of the report size (notably also

	 * all a power of two).

 none */;

		/* All the report sizes factor neatly into the buffer

		 * size so we never expect to see a report split

		 * between the beginning and end of the buffer.

		 *

		 * Given the initial alignment check a misalignment

		 * here would imply a driver bug that would result

		 * in an overrun.

		/* The report-ID field for periodic samples includes

		 * some undocumented flags related to what triggered

		 * the report and is never expected to be zero so we

		 * can check that the report isn't invalid before

		 * copying it to userspace...

		/* Clear out the first 2 dwords as a mean to detect unlanded

		 * reports.

		/* We removed the gtt_offset for the copy loop above, indexing

		 * relative to oa_buf_base so put back here...

/**

 * gen7_oa_read - copy status records then buffered OA reports

 * @stream: An i915-perf stream opened for OA metrics

 * @buf: destination buffer given by userspace

 * @count: the number of bytes userspace wants to read

 * @offset: (inout): the current position for writing into @buf

 *

 * Checks Gen 7 specific OA unit status registers and if necessary appends

 * corresponding status records for userspace (such as for a buffer full

 * condition) and then initiate appending any buffered OA reports.

 *

 * Updates @offset according to the number of bytes successfully copied into

 * the userspace buffer.

 *

 * Returns: zero on success or a negative error code

	/* XXX: On Haswell we don't have a safe way to clear oastatus1

	 * bits while the OA unit is enabled (while the tail pointer

	 * may be updated asynchronously) so we ignore status bits

	 * that have already been reported to userspace.

	/* We treat OABUFFER_OVERFLOW as a significant error:

	 *

	 * - The status can be interpreted to mean that the buffer is

	 *   currently full (with a higher precedence than OA_TAKEN()

	 *   which will start to report a near-empty buffer after an

	 *   overflow) but it's awkward that we can't clear the status

	 *   on Haswell, so without a reset we won't be able to catch

	 *   the state again.

	 *

	 * - Since it also implies the HW has started overwriting old

	 *   reports it may also affect our sanity checks for invalid

	 *   reports when copying to userspace that assume new reports

	 *   are being written to cleared memory.

	 *

	 * - In the future we may want to introduce a flight recorder

	 *   mode where the driver will automatically maintain a safe

	 *   guard band between head/tail, avoiding this overflow

	 *   condition, but we avoid the added driver complexity for

	 *   now.

/**

 * i915_oa_wait_unlocked - handles blocking IO until OA data available

 * @stream: An i915-perf stream opened for OA metrics

 *

 * Called when userspace tries to read() from a blocking stream FD opened

 * for OA metrics. It waits until the hrtimer callback finds a non-empty

 * OA buffer and wakes us.

 *

 * Note: it's acceptable to have this return with some false positives

 * since any subsequent read handling will return -EAGAIN if there isn't

 * really data ready for userspace yet.

 *

 * Returns: zero on success or a negative error code

 We would wait indefinitely if periodic sampling is not enabled */

/**

 * i915_oa_poll_wait - call poll_wait() for an OA stream poll()

 * @stream: An i915-perf stream opened for OA metrics

 * @file: An i915 perf stream file

 * @wait: poll() state table

 *

 * For handling userspace polling on an i915 perf stream opened for OA metrics,

 * this starts a poll_wait with the wait queue that our hrtimer callback wakes

 * when it sees data ready to read in the circular OA buffer.

/**

 * i915_oa_read - just calls through to &i915_oa_ops->read

 * @stream: An i915-perf stream opened for OA metrics

 * @buf: destination buffer given by userspace

 * @count: the number of bytes userspace wants to read

 * @offset: (inout): the current position for writing into @buf

 *

 * Updates @offset according to the number of bytes successfully copied into

 * the userspace buffer.

 *

 * Returns: zero on success or a negative error code

 first match! */

	/*

	 * As the ID is the gtt offset of the context's vma we

	 * pin the vma to ensure the ID remains fixed.

/**

 * oa_get_render_ctx_id - determine and hold ctx hw id

 * @stream: An i915-perf stream opened for OA metrics

 *

 * Determine the render context hw id, and ensure it remains fixed for the

 * lifetime of the stream. This ensures that we don't have to worry about

 * updating the context ID in OACONTROL on the fly.

 *

 * Returns: zero on success or a negative error code

		/*

		 * On Haswell we don't do any post processing of the reports

		 * and don't need to use the mask.

			/*

			 * When using GuC, the context descriptor we write in

			 * i915 is read by GuC and rewritten before it's

			 * actually written into the hardware. The LRCA is

			 * what is put into the context id field of the

			 * context descriptor by GuC. Because it's aligned to

			 * a page, the lower 12bits are always at 0 and

			 * dropped by GuC. They won't be part of the context

			 * ID in the OA reports, so squash those lower bits.

			/*

			 * GuC uses the top bit to signal proxy submission, so

			 * ignore that bit.

			/*

			 * Pick an unused context id

			 * 0 - BITS_PER_LONG are used by other contexts

			 * GEN12_MAX_CONTEXT_HW_ID (0x7ff) is used by idle context

/**

 * oa_put_render_ctx_id - counterpart to oa_get_render_ctx_id releases hold

 * @stream: An i915-perf stream opened for OA metrics

 *

 * In case anything needed doing to ensure the context HW ID would remain valid

 * for the lifetime of the stream, then that can be undone here.

 recomputed on next submission after parking */

	/*

	 * Unset exclusive_stream first, it will be checked while disabling

	 * the metric set on gen8+.

	 *

	 * See i915_oa_init_reg_state() and lrc_configure_all_contexts()

	/* Pre-DevBDW: OABUFFER must be set with counters off,

	 * before OASTATUS1, but after OASTATUS2

 head */

 tail */

 Mark that we need updated tail pointers to read from... */

	/* On Haswell we have to track which OASTATUS1 flags we've

	 * already seen since they can't be cleared while periodic

	 * sampling is enabled.

	/* NB: although the OA buffer will initially be allocated

	 * zeroed via shmfs (and so this memset is redundant when

	 * first allocating), we may re-init the OA buffer, either

	 * when re-enabling a stream or in error/reset paths.

	 *

	 * The reason we clear the buffer for each re-init is for the

	 * sanity check in gen7_append_oa_reports() that looks at the

	 * report-id field to make sure it's non-zero which relies on

	 * the assumption that new reports are being written to zeroed

	 * memory...

	/*

	 * PRM says:

	 *

	 *  "This MMIO must be set before the OATAILPTR

	 *  register and after the OAHEADPTR register. This is

	 *  to enable proper functionality of the overflow

	 *  bit."

 Mark that we need updated tail pointers to read from... */

	/*

	 * Reset state used to recognise context switches, affecting which

	 * reports we will forward to userspace while filtering for a single

	 * context.

	/*

	 * NB: although the OA buffer will initially be allocated

	 * zeroed via shmfs (and so this memset is redundant when

	 * first allocating), we may re-init the OA buffer, either

	 * when re-enabling a stream or in error/reset paths.

	 *

	 * The reason we clear the buffer for each re-init is for the

	 * sanity check in gen8_append_oa_reports() that looks at the

	 * reason field to make sure it's non-zero which relies on

	 * the assumption that new reports are being written to zeroed

	 * memory...

	/*

	 * PRM says:

	 *

	 *  "This MMIO must be set before the OATAILPTR

	 *  register and after the OAHEADPTR register. This is

	 *  to enable proper functionality of the overflow

	 *  bit."

 Mark that we need updated tail pointers to read from... */

	/*

	 * Reset state used to recognise context switches, affecting which

	 * reports we will forward to userspace while filtering for a single

	 * context.

	/*

	 * NB: although the OA buffer will initially be allocated

	 * zeroed via shmfs (and so this memset is redundant when

	 * first allocating), we may re-init the OA buffer, either

	 * when re-enabling a stream or in error/reset paths.

	 *

	 * The reason we clear the buffer for each re-init is for the

	 * sanity check in gen8_append_oa_reports() that looks at the

	 * reason field to make sure it's non-zero which relies on

	 * the assumption that new reports are being written to zeroed

	 * memory...

 PreHSW required 512K alignment, HSW requires 16M */

	/*

	 * We pin in GGTT because we jump into this buffer now because

	 * multiple OA config BOs will have a jump to this address and it

	 * needs to be fixed during the lifetime of the i915/perf stream.

 Save registers. */

 save */, CS_GPR(i),

 save */, MI_PREDICATE_RESULT_1,

 First timestamp snapshot location. */

	/*

	 * Initial snapshot of the timestamp register to implement the wait.

	 * We work with 32b values, so clear out the top 32b bits of the

	 * register because the ALU works 64bits.

	/*

	 * This is the location we're going to jump back into until the

	 * required amount of time has passed.

	/*

	 * Take another snapshot of the timestamp register. Take care to clear

	 * up the top 32bits of CS_GPR(1) as we're using it for other

	 * operations below.

	/*

	 * Do a diff between the 2 timestamps and store the result back into

	 * CS_GPR(1).

	/*

	 * Transfer the carry flag (set to 1 if ts1 < ts0, meaning the

	 * timestamp have rolled over the 32bits) into the predicate register

	 * to be used for the predicated jump.

 Restart from the beginning if we had timestamps roll over. */

	/*

	 * Now add the diff between to previous timestamps and add it to :

	 *      (((1 * << 64) - 1) - delay_ns)

	 *

	 * When the Carry Flag contains 1 this means the elapsed time is

	 * longer than the expected delay, and we can exit the wait loop.

	/*

	 * Transfer the result into the predicate register to be used for the

	 * predicated jump.

 Predicate the jump.  */

 Restore registers. */

 restore */, CS_GPR(i),

 restore */, MI_PREDICATE_RESULT_1,

 And return to the ring. */

 MI_BATCH_BUFFER_START */

 Jump into the active wait. */

	/*

	 * Look for the buffer in the already allocated BOs attached

	 * to the stream.

 After all individual context modifications */

	/*

	 * PRM:

	 *

	 * OA unit is using âcrclkâ for its functionality. When trunk

	 * level clock gating takes place, OA clock would be gated,

	 * unable to count the events from non-render clock domain.

	 * Render clock gating must be disabled when OA is enabled to

	 * count the events from non-render domain. Unit level clock

	 * gating for RCS should also be disabled.

	/*

	 * This arbitrary default will select the 'EU FPU0 Pipeline

	 * Active' event. In the future it's anticipated that there

	 * will be an explicit 'No Event' we can select, but not yet...

/*

 * NB: It must always remain pointer safe to run this even if the OA unit

 * has been disabled.

 *

 * It's fine to put out-of-date values into these per-context registers

 * in the case that the OA unit has been disabled.

 The MMIO offsets for Flex EU registers aren't contiguous */

 Serialise with the remote context */

 Otherwise OA settings will be set upon first use */

	/* Offsets in regs_lri are not used since this configuration is only

	 * applied using LRI. Initialize the correct offsets for posterity.

 Modify the context image of pinned context with regs_context*/

 Apply regs_lri using LRI with pinned context */

/*

 * Manages updating the per-context aspects of the OA stream

 * configuration across all contexts.

 *

 * The awkward consideration here is that OACTXCONTROL controls the

 * exponent for periodic sampling which is primarily used for system

 * wide profiling where we'd like a consistent sampling period even in

 * the face of context switches.

 *

 * Our approach of updating the register state context (as opposed to

 * say using a workaround batch buffer) ensures that the hardware

 * won't automatically reload an out-of-date timer exponent even

 * transiently before a WA BB could be parsed.

 *

 * This function needs to:

 * - Ensure the currently running context's per-context OA state is

 *   updated

 * - Ensure that all existing contexts will have the correct per-context

 *   OA state if they are scheduled for use.

 * - Ensure any new contexts will be initialized with the correct

 *   per-context OA state.

 *

 * Note: it's only the RCS/Render context that has any OA state.

 * Note: the first flex register passed must always be R_PWR_CLK_STATE

	/*

	 * The OA register config is setup through the context image. This image

	 * might be written to by the GPU on context switch (in particular on

	 * lite-restore). This means we can't safely update a context's image,

	 * if this context is scheduled/submitted to run on the GPU.

	 *

	 * We could emit the OA register config through the batch buffer but

	 * this might leave small interval of time where the OA unit is

	 * configured at an invalid sampling period.

	 *

	 * Note that since we emit all requests from a single ring, there

	 * is still an implicit global barrier here that may cause a high

	 * priority context to wait for an otherwise independent low priority

	 * context. Contexts idle at the time of reconfiguration are not

	 * trapped behind the barrier.

	/*

	 * After updating all other contexts, we need to modify ourselves.

	 * If we don't modify the kernel_context, we do not get events while

	 * idle.

 The MMIO offsets for Flex EU registers aren't contiguous */

	/*

	 * We disable slice/unslice clock ratio change reports on SKL since

	 * they are too noisy. The HW generates a lot of redundant reports

	 * where the ratio hasn't really changed causing a lot of redundant

	 * work to processes and increasing the chances we'll hit buffer

	 * overruns.

	 *

	 * Although we don't currently use the 'disable overrun' OABUFFER

	 * feature it's worth noting that clock ratio reports have to be

	 * disabled before considering to use that feature since the HW doesn't

	 * correctly block these reports.

	 *

	 * Currently none of the high-level metrics we have depend on knowing

	 * this ratio to normalize.

	 *

	 * Note: This register is not power context saved and restored, but

	 * that's OK considering that we disable RC6 while the OA unit is

	 * enabled.

	 *

	 * The _INCLUDE_CLK_RATIO bit allows the slice/unslice frequency to

	 * be read back from automatically triggered reports, as part of the

	 * RPT_ID field.

	/*

	 * Update all contexts prior writing the mux configurations as we need

	 * to make sure all slices/subslices are ON before writing to NOA

	 * registers.

 Disable clk ratio reports, like previous Gens. */

			   /*

			    * If the user didn't require OA reports, instruct

			    * the hardware not to emit ctx switch reports.

	/*

	 * Update all contexts prior writing the mux configurations as we need

	 * to make sure all slices/subslices are ON before writing to NOA

	 * registers.

	/*

	 * For Gen12, performance counters are context

	 * saved/restored. Only enable it for the context that

	 * requested this.

 Reset all contexts' slices/subslices configurations. */

 Reset all contexts' slices/subslices configurations. */

 Make sure we disable noa to save power. */

 Reset all contexts' slices/subslices configurations. */

 disable the context save/restore or OAR counters */

 Make sure we disable noa to save power. */

	/*

	 * Reset buf pointers so we don't forward reports from before now.

	 *

	 * Think carefully if considering trying to avoid this, since it

	 * also ensures status flags and the buffer itself are cleared

	 * in error paths, and we have checks for invalid reports based

	 * on the assumption that certain fields are written to zeroed

	 * memory which this helps maintains.

	/*

	 * Reset buf pointers so we don't forward reports from before now.

	 *

	 * Think carefully if considering trying to avoid this, since it

	 * also ensures status flags and the buffer itself are cleared

	 * in error paths, and we have checks for invalid reports based

	 * on the assumption that certain fields are written to zeroed

	 * memory which this helps maintains.

	/*

	 * Note: we don't rely on the hardware to perform single context

	 * filtering and instead filter on the cpu based on the context-id

	 * field of reports

	/*

	 * If we don't want OA reports from the OA buffer, then we don't even

	 * need to program the OAG unit.

/**

 * i915_oa_stream_enable - handle `I915_PERF_IOCTL_ENABLE` for OA stream

 * @stream: An i915 perf stream opened for OA metrics

 *

 * [Re]enables hardware periodic sampling according to the period configured

 * when opening the stream. This also starts a hrtimer that will periodically

 * check for data in the circular OA buffer for notifying userspace (e.g.

 * during a read() or poll()).

/**

 * i915_oa_stream_disable - handle `I915_PERF_IOCTL_DISABLE` for OA stream

 * @stream: An i915 perf stream opened for OA metrics

 *

 * Stops the OA unit from periodically writing counter reports into the

 * circular OA buffer. This also stops the hrtimer that periodically checks for

 * data in the circular OA buffer, for notifying userspace.

		/*

		 * We only need subslice count so it doesn't matter which ones

		 * we select - just turn off low bits in the amount of half of

		 * all available subslices per slice.

/**

 * i915_oa_stream_init - validate combined props for OA stream and init

 * @stream: An i915 perf stream

 * @param: The open parameters passed to `DRM_I915_PERF_OPEN`

 * @props: The property state that configures stream (individually validated)

 *

 * While read_properties_unlocked() validates properties in isolation it

 * doesn't ensure that the combination necessarily makes sense.

 *

 * At this point it has been determined that userspace wants a stream of

 * OA metrics, but still we need to further validate the combined

 * properties are OK.

 *

 * If the configuration makes sense then we can allocate memory for

 * a circular OA buffer and apply the requested metric set configuration.

 *

 * Returns: zero on success or a negative error code.

	/*

	 * If the sysfs metrics/ directory wasn't registered for some

	 * reason then don't let userspace try their luck with config

	 * IDs

	/*

	 * To avoid the complexity of having to accurately filter

	 * counter reports and marshal to the appropriate client

	 * we currently only allow exclusive access

	/* PRM - observability performance counters:

	 *

	 *   OACONTROL, performance counter enable, note:

	 *

	 *   "When this bit is set, in order to have coherent counts,

	 *   RC6 power state and trunk clock gating must be disabled.

	 *   This can be achieved by programming MMIO registers as

	 *   0xA094=0 and 0xA090[31]=1"

	 *

	 *   In our case we are expecting that taking pm + FORCEWAKE

	 *   references will effectively disable RC6.

 perf.exclusive_stream serialised by lrc_configure_all_contexts() */

/**

 * i915_perf_read - handles read() FOP for i915 perf stream FDs

 * @file: An i915 perf stream file

 * @buf: destination buffer given by userspace

 * @count: the number of bytes userspace wants to read

 * @ppos: (inout) file seek position (unused)

 *

 * The entry point for handling a read() on a stream file descriptor from

 * userspace. Most of the work is left to the i915_perf_read_locked() and

 * &i915_perf_stream_ops->read but to save having stream implementations (of

 * which we might have multiple later) we handle blocking read here.

 *

 * We can also consistently treat trying to read from a disabled stream

 * as an IO error so implementations can assume the stream is enabled

 * while reading.

 *

 * Returns: The number of bytes copied or a negative error code on failure.

	/* To ensure it's handled consistently we simply treat all reads of a

	 * disabled stream as an error. In particular it might otherwise lead

	 * to a deadlock for blocking file descriptors...

		/* There's the small chance of false positives from

		 * stream->ops->wait_unlocked.

		 *

		 * E.g. with single context filtering since we only wait until

		 * oabuffer has >= 1 report we don't immediately know whether

		 * any reports really belong to the current context

	/* We allow the poll checking to sometimes report false positive EPOLLIN

	 * events where we might actually report EAGAIN on read() if there's

	 * not really any data available. In this situation though we don't

	 * want to enter a busy loop between poll() reporting a EPOLLIN event

	 * and read() returning -EAGAIN. Clearing the oa.pollin state here

	 * effectively ensures we back off until the next hrtimer callback

	 * before reporting another EPOLLIN event.

	 * The exception to this is if ops->read() returned -ENOSPC which means

	 * that more OA data is available than could fit in the user provided

	 * buffer. In this case we want the next poll() call to not block.

 Possible values for ret are 0, -EFAULT, -ENOSPC, -EIO, ... */

/**

 * i915_perf_poll_locked - poll_wait() with a suitable wait queue for stream

 * @stream: An i915 perf stream

 * @file: An i915 perf stream file

 * @wait: poll() state table

 *

 * For handling userspace polling on an i915 perf stream, this calls through to

 * &i915_perf_stream_ops->poll_wait to call poll_wait() with a wait queue that

 * will be woken for new stream data.

 *

 * Note: The &perf->lock mutex has been taken to serialize

 * with any non-file-operation driver hooks.

 *

 * Returns: any poll events that are ready without sleeping

	/* Note: we don't explicitly check whether there's something to read

	 * here since this path may be very hot depending on what else

	 * userspace is polling, or on the timeout in use. We rely solely on

	 * the hrtimer/oa_poll_check_timer_cb to notify us when there are

	 * samples to read.

/**

 * i915_perf_poll - call poll_wait() with a suitable wait queue for stream

 * @file: An i915 perf stream file

 * @wait: poll() state table

 *

 * For handling userspace polling on an i915 perf stream, this ensures

 * poll_wait() gets called with a wait queue that will be woken for new stream

 * data.

 *

 * Note: Implementation deferred to i915_perf_poll_locked()

 *

 * Returns: any poll events that are ready without sleeping

/**

 * i915_perf_enable_locked - handle `I915_PERF_IOCTL_ENABLE` ioctl

 * @stream: A disabled i915 perf stream

 *

 * [Re]enables the associated capture of data for this stream.

 *

 * If a stream was previously enabled then there's currently no intention

 * to provide userspace any guarantee about the preservation of previously

 * buffered data.

 Allow stream->ops->enable() to refer to this */

/**

 * i915_perf_disable_locked - handle `I915_PERF_IOCTL_DISABLE` ioctl

 * @stream: An enabled i915 perf stream

 *

 * Disables the associated capture of data for this stream.

 *

 * The intention is that disabling an re-enabling a stream will ideally be

 * cheaper than destroying and re-opening a stream with the same configuration,

 * though there are no formal guarantees about what state or buffered data

 * must be retained between disabling and re-enabling a stream.

 *

 * Note: while a stream is disabled it's considered an error for userspace

 * to attempt to read from the stream (-EIO).

 Allow stream->ops->disable() to refer to this */

		/*

		 * If OA is bound to a specific context, emit the

		 * reconfiguration inline from that context. The update

		 * will then be ordered with respect to submission on that

		 * context.

		 *

		 * When set globally, we use a low priority kernel context,

		 * so it will effectively take effect when idle.

/**

 * i915_perf_ioctl_locked - support ioctl() usage with i915 perf stream FDs

 * @stream: An i915 perf stream

 * @cmd: the ioctl request

 * @arg: the ioctl data

 *

 * Note: The &perf->lock mutex has been taken to serialize

 * with any non-file-operation driver hooks.

 *

 * Returns: zero on success or a negative error code. Returns -EINVAL for

 * an unknown ioctl request.

/**

 * i915_perf_ioctl - support ioctl() usage with i915 perf stream FDs

 * @file: An i915 perf stream file

 * @cmd: the ioctl request

 * @arg: the ioctl data

 *

 * Implementation deferred to i915_perf_ioctl_locked().

 *

 * Returns: zero on success or a negative error code. Returns -EINVAL for

 * an unknown ioctl request.

/**

 * i915_perf_destroy_locked - destroy an i915 perf stream

 * @stream: An i915 perf stream

 *

 * Frees all resources associated with the given i915 perf @stream, disabling

 * any associated data capture in the process.

 *

 * Note: The &perf->lock mutex has been taken to serialize

 * with any non-file-operation driver hooks.

/**

 * i915_perf_release - handles userspace close() of a stream file

 * @inode: anonymous inode associated with file

 * @file: An i915 perf stream file

 *

 * Cleans up any resources associated with an open i915 perf stream file.

 *

 * NB: close() can't really fail from the userspace point of view.

 *

 * Returns: zero on success or a negative error code.

 Release the reference the perf stream kept on the driver. */

	/* Our ioctl have no arguments, so it's safe to use the same function

	 * to handle 32bits compatibility.

/**

 * i915_perf_open_ioctl_locked - DRM ioctl() for userspace to open a stream FD

 * @perf: i915 perf instance

 * @param: The open parameters passed to 'DRM_I915_PERF_OPEN`

 * @props: individually validated u64 property value pairs

 * @file: drm file

 *

 * See i915_perf_ioctl_open() for interface details.

 *

 * Implements further stream config validation and stream initialization on

 * behalf of i915_perf_open_ioctl() with the &perf->lock mutex

 * taken to serialize with any non-file-operation driver hooks.

 *

 * Note: at this point the @props have only been validated in isolation and

 * it's still necessary to validate that the combination of properties makes

 * sense.

 *

 * In the case where userspace is interested in OA unit metrics then further

 * config validation and stream initialization details will be handled by

 * i915_oa_stream_init(). The code here should only validate config state that

 * will be relevant to all stream types / backends.

 *

 * Returns: zero on success or a negative error code.

	/*

	 * On Haswell the OA unit supports clock gating off for a specific

	 * context and in this mode there's no visibility of metrics for the

	 * rest of the system, which we consider acceptable for a

	 * non-privileged client.

	 *

	 * For Gen8->11 the OA unit no longer supports clock gating off for a

	 * specific context and the kernel can't securely stop the counters

	 * from updating as system-wide / global values. Even though we can

	 * filter reports based on the included context ID we can't block

	 * clients from seeing the raw / global counter values via

	 * MI_REPORT_PERF_COUNT commands and so consider it a privileged op to

	 * enable the OA unit by default.

	 *

	 * For Gen12+ we gain a new OAR unit that only monitors the RCS on a

	 * per context basis. So we can relax requirements there if the user

	 * doesn't request global stream access (i.e. query based sampling

	 * using MI_RECORD_PERF_COUNT.

	/*

	 * Asking for SSEU configuration is a priviliged operation.

	/* Similar to perf's kernel.perf_paranoid_cpu sysctl option

	 * we check a dev.i915.perf_stream_paranoid sysctl option

	 * to determine if it's ok to access system wide OA counters

	 * without CAP_PERFMON or CAP_SYS_ADMIN privileges.

	/* we avoid simply assigning stream->sample_flags = props->sample_flags

	 * to have _stream_init check the combination of sample flags more

	 * thoroughly, but still this is the expected result at this point.

	/* Take a reference on the driver that will be kept with stream_fd

	 * until its release.

/**

 * read_properties_unlocked - validate + copy userspace stream open properties

 * @perf: i915 perf instance

 * @uprops: The array of u64 key value pairs given by userspace

 * @n_props: The number of key value pairs expected in @uprops

 * @props: The stream configuration built up while validating properties

 *

 * Note this function only validates properties in isolation it doesn't

 * validate that the combination of properties makes sense or that all

 * properties necessary for a particular kind of stream have been set.

 *

 * Note that there currently aren't any ordering requirements for properties so

 * we shouldn't validate or assume anything about ordering here. This doesn't

 * rule out defining new properties with ordering requirements in the future.

 At the moment we only support using i915-perf on the RCS. */

	/* Considering that ID = 0 is reserved and assuming that we don't

	 * (currently) expect any configurations to ever specify duplicate

	 * values for a particular property ID then the last _PROP_MAX value is

	 * one greater than the maximum number of properties we expect to get

	 * from userspace.

			/* Theoretically we can program the OA unit to sample

			 * e.g. every 160ns for HSW, 167ns for BDW/SKL or 104ns

			 * for BXT. We don't allow such high sampling

			 * frequencies by default unless root.

			/* This check is primarily to ensure that oa_period <=

			 * UINT32_MAX (before passing to do_div which only

			 * accepts a u32 denominator), but we can also skip

			 * checking anything < 1Hz which implicitly can't be

			 * limited via an integer oa_max_sample_rate.

 100us */) {

/**

 * i915_perf_open_ioctl - DRM ioctl() for userspace to open a stream FD

 * @dev: drm device

 * @data: ioctl data copied from userspace (unvalidated)

 * @file: drm file

 *

 * Validates the stream open parameters given by userspace including flags

 * and an array of u64 key, value pair properties.

 *

 * Very little is assumed up front about the nature of the stream being

 * opened (for instance we don't assume it's for periodic OA unit metrics). An

 * i915-perf stream is expected to be a suitable interface for other forms of

 * buffered data written by the GPU besides periodic OA metrics.

 *

 * Note we copy the properties from userspace outside of the i915 perf

 * mutex to avoid an awkward lockdep with mmap_lock.

 *

 * Most of the implementation details are handled by

 * i915_perf_open_ioctl_locked() after taking the &perf->lock

 * mutex for serializing with any non-file-operation driver hooks.

 *

 * Return: A newly opened i915 Perf stream file descriptor or negative

 * error code on failure.

/**

 * i915_perf_register - exposes i915-perf to userspace

 * @i915: i915 device instance

 *

 * In particular OA metric sets are advertised under a sysfs metrics/

 * directory allowing userspace to enumerate valid IDs that can be

 * used to open an i915-perf stream.

	/* To be sure we're synchronized with an attempted

	 * i915_perf_open_ioctl(); considering that we register after

	 * being exposed to userspace.

/**

 * i915_perf_unregister - hide i915-perf from userspace

 * @i915: i915 device instance

 *

 * i915-perf state cleanup is split up into an 'unregister' and

 * 'deinit' phase where the interface is first hidden from

 * userspace by i915_perf_unregister() before cleaning up

 * remaining state in i915_perf_fini().

	/* HALF_SLICE_CHICKEN2 is programmed with a the

	 * WaDisableSTUnitPowerOptimization workaround. Make sure the value

	 * programmed by userspace doesn't change this.

	/* WAIT_FOR_RC6_EXIT has only one bit fullfilling the function

	 * indicated by its name and a bunch of selection fields used by OA

	 * configs.

 No is_valid function means we're not allowing any register to be programmed. */

/**

 * i915_perf_add_config_ioctl - DRM ioctl() for userspace to add a new OA config

 * @dev: drm device

 * @data: ioctl data (pointer to struct drm_i915_perf_oa_config) copied from

 *        userspace (unvalidated)

 * @file: drm file

 *

 * Validates the submitted OA register to be saved into a new OA config that

 * can then be used for programming the OA unit and its NOA network.

 *

 * Returns: A new allocated config number to be used with the perf open ioctl

 * or a negative error code on failure.

	/* Last character in oa_config->uuid will be 0 because oa_config is

	 * kzalloc.

	/* We shouldn't have too many configs, so this iteration shouldn't be

	 * too costly.

 Config id 0 is invalid, id 1 for kernel stored test config. */

/**

 * i915_perf_remove_config_ioctl - DRM ioctl() for userspace to remove an OA config

 * @dev: drm device

 * @data: ioctl data (pointer to u64 integer) copied from userspace

 * @file: drm file

 *

 * Configs can be removed while being used, the will stop appearing in sysfs

 * and their content will be freed when the stream using the config is closed.

 *

 * Returns: 0 on success or a negative error code on failure.

/**

 * i915_perf_init - initialize i915-perf state on module bind

 * @i915: i915 device instance

 *

 * Initializes i915-perf state without exposing anything to userspace.

 *

 * Note: i915-perf initialization is split into an 'init' and 'register'

 * phase with the i915_perf_register() exposing state to userspace.

 XXX const struct i915_perf_ops! */

		/* Note: that although we could theoretically also support the

		 * legacy ringbuffer mode on BDW (and earlier iterations of

		 * this driver, before upstreaming did this) it didn't seem

		 * worth the complexity to maintain now that BDW+ enable

		 * execlist mode by default.

 Choose a representative limit */

		/* We set up some ratelimit state to potentially throttle any

		 * _NOTES about spurious, invalid OA reports which we don't

		 * forward to userspace.

		 *

		 * We print a _NOTE about any throttling when closing the

		 * stream instead of waiting until driver _fini which no one

		 * would ever see.

		 *

		 * Using the same limiting factors as printk_ratelimit()

		/* Since we use a DRM_NOTE for spurious reports it would be

		 * inconsistent to let __ratelimit() automatically print a

		 * warning for throttling.

 500us */);

/**

 * i915_perf_fini - Counter part to i915_perf_init()

 * @i915: i915 device instance

/**

 * i915_perf_ioctl_version - Version of the i915-perf subsystem

 *

 * This version number is used by userspace to detect available features.

	/*

	 * 1: Initial version

	 *   I915_PERF_IOCTL_ENABLE

	 *   I915_PERF_IOCTL_DISABLE

	 *

	 * 2: Added runtime modification of OA config.

	 *   I915_PERF_IOCTL_CONFIG

	 *

	 * 3: Add DRM_I915_PERF_PROP_HOLD_PREEMPTION parameter to hold

	 *    preemption on a particular context so that performance data is

	 *    accessible from a delta of MI_RPC reports without looking at the

	 *    OA buffer.

	 *

	 * 4: Add DRM_I915_PERF_PROP_ALLOWED_SSEU to limit what contexts can

	 *    be run for the duration of the performance recording based on

	 *    their SSEU configuration.

	 *

	 * 5: Add DRM_I915_PERF_PROP_POLL_OA_PERIOD parameter that controls the

	 *    interval for the hrtimer used to check for OA data.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 XXX: consider maybe converting to an rb tree at some point */

/**

 * intel_memory_region_reserve - Reserve a memory range

 * @mem: The region for which we want to reserve a range.

 * @offset: Start of the range to reserve.

 * @size: The size of the range to reserve.

 *

 * Return: 0 on success, negative error code on failure.

 Global memory region registration -- only slight layer inversions! */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 GAM */

 MBC */

 GCP */

 GPM */

 Display 1 CZ domain */

 GT SA CZ domain */

 Display 2 CZ domain */

/*

 * Save all Gunit registers that may be lost after a D3 and a subsequent

 * S0i[R123] transition. The list of registers needing a save/restore is

 * defined in the VLV2_S0IXRegs document. This documents marks all Gunit

 * registers in the following way:

 * - Driver: saved/restored by the driver

 * - Punit : saved/restored by the Punit firmware

 * - No, w/o marking: no need to save/restore, since the register is R/O or

 *                    used internally by the HW in a way that doesn't depend

 *                    keeping the content across a suspend/resume.

 * - Debug : used for debugging

 *

 * We save/restore all registers marked with 'Driver', with the following

 * exceptions:

 * - Registers out of use, including also registers marked with 'Debug'.

 *   These have no effect on the driver's operation, so we don't save/restore

 *   them to reduce the overhead.

 * - Registers that are fully setup by an initialization function called from

 *   the resume path. For example many clock gating and RPS/RC6 registers.

 * - Registers that provide the right functionality with their reset defaults.

 *

 * TODO: Except for registers that based on the above 3 criteria can be safely

 * ignored, we save/restore all others, practically treating the HW context as

 * a black-box for the driver. Further investigation is needed to reduce the

 * saved/restored registers even further, by following the same 3 criteria.

 GAM 0x4000-0x4770 */

 MBC 0x9024-0x91D0, 0x8500 */

 GCP 0x9400-0x9424, 0x8100-0x810C */

 GPM 0xA000-0xAA84, 0x8000-0x80FC */

 Display CZ domain, 0x4400C-0x4402C, 0x4F000-0x4F11F */

 GT SA CZ domain, 0x100000-0x138124 */

 Gunit-Display CZ domain, 0x182028-0x1821CF */

	/*

	 * Not saving any of:

	 * DFT,		0x9800-0x9EC0

	 * SARB,	0xB000-0xB1FC

	 * GAC,		0x5208-0x524C, 0x14000-0x14C000

	 * PCI CFG

 GAM 0x4000-0x4770 */

 MBC 0x9024-0x91D0, 0x8500 */

 GCP 0x9400-0x9424, 0x8100-0x810C */

 GPM 0xA000-0xAA84, 0x8000-0x80FC */

 Display CZ domain, 0x4400C-0x4402C, 0x4F000-0x4F11F */

 GT SA CZ domain, 0x100000-0x138124 */

	/*

	 * Preserve the GT allow wake and GFX force clock bit, they are not

	 * be restored, as they are used to control the s0ix suspend/resume

	 * sequence by the caller.

 Gunit-Display CZ domain, 0x182028-0x1821CF */

	/* The HW does not like us polling for PW_STATUS frequently, so

	 * use the sleeping loop rather than risk the busy spin within

	 * intel_wait_for_register().

	 *

	 * Transitioning between RC6 states should be at most 2ms (see

	 * valleyview_enable_rps) so use a 3ms timeout.

 just trace the final value */

	/*

	 * RC6 transitioning can be delayed up to 2 msec (see

	 * valleyview_enable_rps), use 3 msec for safety.

	 *

	 * This can fail to turn off the rc6 if the GPU is stuck after a failed

	 * reset and we are trying to force the machine to sleep.

	/*

	 * Bspec defines the following GT well on flags as debug only, so

	 * don't treat them as hard failures.

 For safety always re-enable waking and disable gfx clock forcing */

	/*

	 * If any of the steps fail just try to continue, that's the best we

	 * can do at this point. Return the first error code (which will also

	 * leave RPM permanently disabled).

 we write all the values in the struct, so no need to zero it out */

/* i915_irq.c -- IRQ support for the I915 -*- linux-c -*-

/*

 * Copyright 2003 Tungsten Graphics, Inc., Cedar Park, Texas.

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the

 * "Software"), to deal in the Software without restriction, including

 * without limitation the rights to use, copy, modify, merge, publish,

 * distribute, sub license, and/or sell copies of the Software, and to

 * permit persons to whom the Software is furnished to do so, subject to

 * the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.

 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR

 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,

 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE

 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

/**

 * DOC: interrupt handling

 *

 * These functions provide the basic support for enabling and disabling the

 * interrupt handling support. There's a lot more functionality in i915_irq.c

 * and related files, but that will be described in separate chapters.

/*

 * Interrupt statistic for PMU. Increments the counter only if the

 * interrupt originated from the the GPU so interrupts from a device which

 * shares the interrupt line are not accounted.

	/*

	 * A clever compiler translates that into INC. A not so clever one

	 * should at least prevent store tearing.

 IIR can theoretically queue up two events. Be paranoid. */

 IIR can theoretically queue up two events. Be paranoid. */

/*

 * We should clear IMR at preinstall/uninstall, and just check at postinstall.

 For display hotplug interrupt */

/**

 * i915_hotplug_interrupt_update - update hotplug interrupt enable

 * @dev_priv: driver private

 * @mask: bits to update

 * @bits: bits to enable

 * NOTE: the HPD enable bits are modified both inside and outside

 * of an interrupt context. To avoid that read-modify-write cycles

 * interfer, these bits are protected by a spinlock. Since this

 * function is usually not called from a context where the lock is

 * held already, this function acquires the lock itself. A non-locking

 * version is also available.

/**

 * ilk_update_display_irq - update DEIMR

 * @dev_priv: driver private

 * @interrupt_mask: mask of interrupt bits to update

 * @enabled_irq_mask: mask of interrupt bits to enable

/**

 * bdw_update_port_irq - update DE port interrupt

 * @dev_priv: driver private

 * @interrupt_mask: mask of interrupt bits to update

 * @enabled_irq_mask: mask of interrupt bits to enable

/**

 * bdw_update_pipe_irq - update DE pipe interrupt

 * @dev_priv: driver private

 * @pipe: pipe whose interrupt to update

 * @interrupt_mask: mask of interrupt bits to update

 * @enabled_irq_mask: mask of interrupt bits to enable

/**

 * ibx_display_interrupt_update - update SDEIMR

 * @dev_priv: driver private

 * @interrupt_mask: mask of interrupt bits to update

 * @enabled_irq_mask: mask of interrupt bits to enable

	/*

	 * On pipe A we don't support the PSR interrupt yet,

	 * on pipe B and C the same bit MBZ.

	/*

	 * On pipe B and C we don't support the PSR interrupt yet, on pipe

	 * A the same bit is for perf counters which we don't use either.

/**

 * i915_enable_asle_pipestat - enable ASLE pipestat for OpRegion

 * @dev_priv: i915 device private

/*

 * This timing diagram depicts the video signal in and

 * around the vertical blanking period.

 *

 * Assumptions about the fictitious mode used in this example:

 *  vblank_start >= 3

 *  vsync_start = vblank_start + 1

 *  vsync_end = vblank_start + 2

 *  vtotal = vblank_start + 3

 *

 *           start of vblank:

 *           latch double buffered registers

 *           increment frame counter (ctg+)

 *           generate start of vblank interrupt (gen4+)

 *           |

 *           |          frame start:

 *           |          generate frame start interrupt (aka. vblank interrupt) (gmch)

 *           |          may be shifted forward 1-3 extra lines via PIPECONF

 *           |          |

 *           |          |  start of vsync:

 *           |          |  generate vsync interrupt

 *           |          |  |

 * ___xxxx___    ___xxxx___    ___xxxx___    ___xxxx___    ___xxxx___    ___xxxx

 *       .   \hs/   .      \hs/          \hs/          \hs/   .      \hs/

 * ----va---> <-----------------vb--------------------> <--------va-------------

 *       |          |       <----vs----->                     |

 * -vbs-----> <---vbs+1---> <---vbs+2---> <-----0-----> <-----1-----> <-----2--- (scanline counter gen2)

 * -vbs-2---> <---vbs-1---> <---vbs-----> <---vbs+1---> <---vbs+2---> <-----0--- (scanline counter gen3+)

 * -vbs-2---> <---vbs-2---> <---vbs-1---> <---vbs-----> <---vbs+1---> <---vbs+2- (scanline counter hsw+ hdmi)

 *       |          |                                         |

 *       last visible pixel                                   first visible pixel

 *                  |                                         increment frame counter (gen3/4)

 *                  pixel counter = vblank_start * htotal     pixel counter = 0 (gen3/4)

 *

 * x  = horizontal active

 * _  = horizontal blanking

 * hs = horizontal sync

 * va = vertical active

 * vb = vertical blanking

 * vs = vertical sync

 * vbs = vblank_start (number)

 *

 * Summary:

 * - most events happen at the start of horizontal sync

 * - frame start happens at the start of horizontal blank, 1-4 lines

 *   (depending on PIPECONF settings) after the start of vblank

 * - gen3/4 pixel and frame counter are synchronized with the start

 *   of horizontal active on the first line of vertical active

/* Called from drm generic code, passed a 'crtc', which

 * we use as a pipe index

	/*

	 * On i965gm TV output the frame counter only works up to

	 * the point when we enable the TV encoder. After that the

	 * frame counter ceases to work and reads zero. We need a

	 * vblank wait before enabling the TV encoder and so we

	 * have to enable vblank interrupts while the frame counter

	 * is still in a working state. However the core vblank code

	 * does not like us returning non-zero frame counter values

	 * when we've told it that we don't have a working frame

	 * counter. Thus we must stop non-zero values leaking out.

 Convert to pixel count */

 Start of vblank event occurs at start of hsync */

	/*

	 * High & low register fields aren't synchronized, so make sure

	 * we get a low value that's stable across two reads of the high

	 * register.

	/*

	 * The frame counter increments at beginning of active.

	 * Cook up a vblank counter by also checking the pixel

	 * counter against vblank start.

	/*

	 * To avoid the race condition where we might cross into the

	 * next vblank just between the PIPE_FRMTMSTMP and TIMESTAMP_CTR

	 * reads. We make sure we read PIPE_FRMTMSTMP and TIMESTAMP_CTR

	 * during the same frame.

		/*

		 * This field provides read back of the display

		 * pipe frame time stamp. The time stamp value

		 * is sampled at every start of vertical blank.

		/*

		 * The TIMESTAMP_CTR register has the current

		 * time stamp value.

/*

 * On certain encoders on certain platforms, pipe

 * scanline register will not work to get the scanline,

 * since the timings are driven from the PORT or issues

 * with scanline register updates.

 * This function will use Framestamp and current

 * timestamp registers to calculate the scanline.

/*

 * intel_de_read_fw(), only for fast reads of display block, no need for

 * forcewake etc.

	/*

	 * On HSW, the DSL reg (0x70000) appears to return 0 if we

	 * read it just before the start of vblank.  So try it again

	 * so we don't accidentally end up spanning a vblank frame

	 * increment, causing the pipe_update_end() code to squak at us.

	 *

	 * The nature of this problem means we can't simply check the ISR

	 * bit and return the vblank start value; nor can we use the scanline

	 * debug register in the transcoder as it appears to have the same

	 * problem.  We may need to extend this to include other platforms,

	 * but so far testing only shows the problem on HSW.

	/*

	 * See update_scanline_offset() for the details on the

	 * scanline_offset adjustment.

	/*

	 * Lock uncore.lock, as we will do multiple timing critical raw

	 * register reads, potentially with preemption disabled, so the

	 * following code must not block on uncore.lock.

 preempt_disable_rt() should go right here in PREEMPT_RT patchset. */

 Get optional system timestamp before query. */

		/*

		 * Already exiting vblank? If so, shift our position

		 * so it looks like we're already apporaching the full

		 * vblank end. This should make the generated timestamp

		 * more or less match when the active portion will start.

		/* No obvious pixelcount register. Only query vertical

		 * scanout position from Display scan line register.

		/* Have access to pixelcount since start of frame.

		 * We can split this into vertical and horizontal

		 * scanout position.

 convert to pixel counts */

		/*

		 * In interlaced modes, the pixel counter counts all pixels,

		 * so one field will have htotal more pixels. In order to avoid

		 * the reported position from jumping backwards when the pixel

		 * counter is beyond the length of the shorter field, just

		 * clamp the position the length of the shorter field. This

		 * matches how the scanline counter based position works since

		 * the scanline counter doesn't count the two half lines.

		/*

		 * Start of vblank interrupt is triggered at start of hsync,

		 * just prior to the first active line of vblank. However we

		 * consider lines to start at the leading edge of horizontal

		 * active. So, should we get here before we've crossed into

		 * the horizontal active of the first line in vblank, we would

		 * not set the DRM_SCANOUTPOS_INVBL flag. In order to fix that,

		 * always add htotal-hsync_start to the current pixel position.

 Get optional system timestamp after query. */

 preempt_enable_rt() should go right here in PREEMPT_RT patchset. */

	/*

	 * While in vblank, position will be negative

	 * counting up towards 0 at vbl_end. And outside

	 * vblank, position will be positive counting

	 * up since vbl_end.

/**

 * ivb_parity_work - Workqueue called when a parity error interrupt

 * occurred.

 * @work: workqueue struct

 *

 * Doesn't actually do anything except notify userspace. As a consequence of

 * this event, userspace should try to remap the bad rows since statistically

 * it is likely the same row is more likely to go bad again.

	/* We must turn off DOP level clock gating to access the L3 registers.

	 * In order to prevent a get/put style interface, acquire struct mutex

	 * any time we access those registers.

 If we've screwed up tracking, just let the interrupt fire again */

/*

 * Get a bit mask of pins that have triggered, and which ones may be long.

 * This can be called multiple times with the same masks to accumulate

 * hotplug detection results from several registers.

 *

 * Note that the caller is expected to zero out the masks initially.

	/*

	 * For some not yet identified reason, the first CRC is

	 * bonkers. So let's just wait for the next vblank and read

	 * out the buggy result.

	 *

	 * On GEN8+ sometimes the second CRC is bonkers as well, so

	 * don't trust that one either.

		/*

		 * PIPESTAT bits get signalled even when the interrupt is

		 * disabled with the mask bits, and some of the status bits do

		 * not generate interrupts at all (like the underrun bit). Hence

		 * we need to be careful that we only handle what we want to

		 * handle.

 fifo underruns are filterered in the underrun handler. */

		/*

		 * Clear the PIPE*STAT regs before the IIR

		 *

		 * Toggle the enable bits to make sure we get an

		 * edge in the ISR pipe event bit if we don't clear

		 * all the enabled status bits. Otherwise the edge

		 * triggered IIR on i965/g4x wouldn't notice that

		 * an interrupt is still pending.

	/*

	 * We absolutely have to clear all the pending interrupt

	 * bits in PORT_HOTPLUG_STAT. Otherwise the ISR port

	 * interrupt bit won't have an edge, and the i965/g4x

	 * edge triggered IIR will not notice that an interrupt

	 * is still pending. We can't use PORT_HOTPLUG_EN to

	 * guarantee the edge as the act of toggling the enable

	 * bits can itself generate a new hotplug interrupt :(

 IRQs are synced during runtime_suspend, we don't require a wakeref */

		/*

		 * Theory on interrupt generation, based on empirical evidence:

		 *

		 * x = ((VLV_IIR & VLV_IER) ||

		 *      (((GT_IIR & GT_IER) || (GEN6_PMIIR & GEN6_PMIER)) &&

		 *       (VLV_MASTER_IER & MASTER_INTERRUPT_ENABLE)));

		 *

		 * A CPU interrupt will only be raised when 'x' has a 0->1 edge.

		 * Hence we clear MASTER_INTERRUPT_ENABLE and VLV_IER to

		 * guarantee the CPU interrupt will be raised again even if we

		 * don't end up clearing all the VLV_IIR, GT_IIR, GEN6_PMIIR

		 * bits this time around.

		/* Call regardless, as some status bits might not be

		/*

		 * VLV_IIR is single buffered, and reflects the level

		 * from PIPESTAT/PORT_HOTPLUG_STAT, hence clear it last.

 IRQs are synced during runtime_suspend, we don't require a wakeref */

		/*

		 * Theory on interrupt generation, based on empirical evidence:

		 *

		 * x = ((VLV_IIR & VLV_IER) ||

		 *      ((GEN8_MASTER_IRQ & ~GEN8_MASTER_IRQ_CONTROL) &&

		 *       (GEN8_MASTER_IRQ & GEN8_MASTER_IRQ_CONTROL)));

		 *

		 * A CPU interrupt will only be raised when 'x' has a 0->1 edge.

		 * Hence we clear GEN8_MASTER_IRQ_CONTROL and VLV_IER to

		 * guarantee the CPU interrupt will be raised again even if we

		 * don't end up clearing all the VLV_IIR and GEN8_MASTER_IRQ_CONTROL

		 * bits this time around.

		/* Call regardless, as some status bits might not be

		/*

		 * VLV_IIR is single buffered, and reflects the level

		 * from PIPESTAT/PORT_HOTPLUG_STAT, hence clear it last.

	/*

	 * Somehow the PCH doesn't seem to really ack the interrupt to the CPU

	 * unless we touch the hotplug register, even if hotplug_trigger is

	 * zero. Not acking leads to "The master control interrupt lied (SDE)!"

	 * errors.

 check event from PCH */

 should clear PCH hotplug event before clear CPU irq */

 check event from PCH */

 clear PCH hotplug event before clear CPU irq */

/*

 * To handle irqs with the minimum potential races with fresh interrupts, we:

 * 1 - Disable Master Interrupt Control.

 * 2 - Find the source(s) of the interrupt.

 * 3 - Clear the Interrupt Identity bits (IIR).

 * 4 - Process the interrupt(s) that had bits set in the IIRs.

 * 5 - Re-enable Master Interrupt Control.

 IRQs are synced during runtime_suspend, we don't require a wakeref */

 disable master interrupt before clearing iir  */

	/* Disable south interrupts. We'll only write to SDEIIR once, so further

	 * interrupts will will be stored on its back queue, and then we'll be

	 * able to process them after we restore SDEIER (as soon as we restore

	 * it, we'll get an interrupt if SDEIIR still has something to process

 Find, clear, then process each source of interrupt */

 IRQs are synced during runtime_suspend, we don't require a wakeref */

 prior GEN12 only have one EDP PSR */

	/*

	 * Incase of dual link, TE comes from DSI_1

	 * this is to check if dual link is enabled

	/*

	 * if dual link is enabled, then read DSI_0

	 * transcoder registers

 Check if DSI configured in command mode */

 Get PIPE for handling VBLANK event */

 clear TE in dsi IIR */

		/*

		 * FIXME(BDW): Assume for now that the new interrupt handling

		 * scheme also closed the SDE interrupt handling race we've seen

		 * on older pch-split platforms. But this needs testing.

			/*

			 * Like on previous PCH there seems to be something

			 * fishy going on with forwarding PCH interrupts.

	/*

	 * Now with master disabled, get a sample of level indications

	 * for this interrupt. Indications will be cleared on related acks.

	 * New indications can and will light up during processing,

	 * and will generate new interrupt after enabling master.

 Find, queue (onto bottom-halves), then clear each source */

 IRQs are synced during runtime_suspend, we don't require a wakeref */

	/*

	 * Now with master disabled, get a sample of level indications

	 * for this interrupt. Indications will be cleared on related acks.

	 * New indications can and will light up during processing,

	 * and will generate new interrupt after enabling master.

	/*

	 * GEN11_DISPLAY_INT_CTL has same format as GEN8_MASTER_IRQ

	 * for the display related bits.

 Find, queue (onto bottom-halves), then clear each source */

 IRQs are synced during runtime_suspend, we don't require a wakeref */

 First disable interrupts */

 Get the indication levels and ack the master unit */

 FIXME: we only support tile 0 for now. */

/* Called from drm generic code, passed 'crtc' which

 * we use as a pipe index

	/*

	 * Vblank interrupts fail to wake the device up from C2+.

	 * Disabling render clock gating during C-states avoids

	 * the problem. There is a small power cost so we do this

	 * only when vblank interrupts are actually enabled.

	/* Even though there is no DMC, frame counter can get stuck when

	 * PSR is active as no frames are generated.

 for dual link cases we consider TE from slave */

	/* Even if there is no DMC, frame counter can get stuck when

	 * PSR is active as no frames are generated, so check only for PSR.

/* Called from drm generic code, passed 'crtc' which

 * we use as a pipe index

/* drm_dma.h hooks

 make sure we're done processing display irqs */

		/*

		 * When CPU and PCH are on the same package, port A

		 * HPD must be enabled in both north and south.

	/*

	 * Enable digital hotplug on the PCH, and configure the DP short pulse

	 * duration to 2ms (which is the minimum in the Display Port spec).

	 * The pulse duration bits are reserved on LPT+.

 Display WA #1179 WaHardHangonHotPlug: cnp */

 Enable digital hotplug on the PCH */

	/*

	 * Enable digital hotplug on the CPU, and configure the DP short pulse

	 * duration to 2ms (which is the minimum in the Display Port spec)

	 * The pulse duration bits are reserved on HSW+.

/*

 * SDEIER is also touched by the interrupt handler to work around missed PCH

 * interrupts. Hence we can't update it after the interrupt handler is enabled -

 * instead we unconditionally enable all PCH interrupt sources here, but then

 * only unmask them as needed with SDEIMR.

 *

 * Note that we currently do this after installing the interrupt handler,

 * but before we enable the master interrupt. That should be sufficient

 * to avoid races with the irq handler, assuming we have MSI. Shared legacy

 * interrupts could still race.

 Unmask the interrupts that we always want on. */

	/* Interrupt setup is already guaranteed to be single-threaded, this is

	/*

	 * Toggle all EMR bits to make sure we get an edge

	 * in the ISR master error bit if we don't clear

	 * all the EIR bits. Otherwise the edge triggered

	 * IIR on i965/g4x wouldn't notice that an interrupt

	 * is still pending. Also some EIR bits can't be

	 * cleared except by handling the underlying error

	 * (or by a GPU reset) so we mask any bit that

	 * remains set.

	/*

	 * Toggle all EMR bits to make sure we get an edge

	 * in the ISR master error bit if we don't clear

	 * all the EIR bits. Otherwise the edge triggered

	 * IIR on i965/g4x wouldn't notice that an interrupt

	 * is still pending. Also some EIR bits can't be

	 * cleared except by handling the underlying error

	 * (or by a GPU reset) so we mask any bit that

	 * remains set.

 IRQs are synced during runtime_suspend, we don't require a wakeref */

		/* Call regardless, as some status bits might not be

 Unmask the interrupts that we always want on. */

 Enable in IER... */

 and unmask in IMR */

	/* Interrupt setup is already guaranteed to be single-threaded, this is

 IRQs are synced during runtime_suspend, we don't require a wakeref */

		/* Call regardless, as some status bits might not be

	/*

	 * Enable some error detection, note the instruction error mask

	 * bit is reserved, so we leave it masked.

 Unmask the interrupts that we always want on. */

	/* Interrupt setup is already guaranteed to be single-threaded, this is

 Note HDMI and DP share hotplug bits */

 enable bits are the same for all generations */

	/* Programming the CRT detection parameters tends

	   to generate a spurious hotplug event about three

	   seconds later.  So just do it once.

 Ignore TV since it's buggy */

 IRQs are synced during runtime_suspend, we don't require a wakeref */

		/* Call regardless, as some status bits might not be

/**

 * intel_irq_init - initializes irq support

 * @dev_priv: i915 device instance

 *

 * This function initializes all the irq support including work items, timers

 * and all the vtables. It does not setup the interrupt itself though.

 pre-gen11 the guc irqs bits are in the upper 16 bits of the pm reg */

	/* Most platforms treat the display irq block as an always-on

	 * power domain. vlv/chv can disable it at runtime and need

	 * special care to avoid writing any of the display block registers

	 * outside of the power domain. We defer setting up the display irqs

	 * in this case to the runtime pm.

	/* If we have MST support, we want to avoid doing short HPD IRQ storm

	 * detection, as short HPD storms will occur as a natural part of

	 * sideband messaging with MST.

	 * On older platforms however, IRQ storms can occur with both long and

	 * short pulses, as seen on some G4x systems.

/**

 * intel_irq_fini - deinitializes IRQ support

 * @i915: i915 device instance

 *

 * This function deinitializes all the IRQ support.

/**

 * intel_irq_install - enables the hardware interrupt

 * @dev_priv: i915 device instance

 *

 * This function enables the hardware interrupt handling, but leaves the hotplug

 * handling still disabled. It is called after intel_irq_init().

 *

 * In the driver load and resume code we need working interrupts in a few places

 * but don't want to deal with the hassle of concurrent probe and hotplug

 * workers. Hence the split into this two-stage approach.

	/*

	 * We enable some interrupt sources in our postinstall hooks, so mark

	 * interrupts as enabled _before_ actually enabling them to avoid

	 * special cases in our ordering checks.

/**

 * intel_irq_uninstall - finilizes all irq handling

 * @dev_priv: i915 device instance

 *

 * This stops interrupt and hotplug handling and unregisters and frees all

 * resources acquired in the init functions.

	/*

	 * FIXME we can get called twice during driver probe

	 * error handling as well as during driver remove due to

	 * intel_modeset_driver_remove() calling us out of sequence.

	 * Would be nice if it didn't do that...

/**

 * intel_runtime_pm_disable_interrupts - runtime interrupt disabling

 * @dev_priv: i915 device instance

 *

 * This function is used to disable interrupts at runtime, both in the runtime

 * pm and the system suspend/resume code.

/**

 * intel_runtime_pm_enable_interrupts - runtime interrupt enabling

 * @dev_priv: i915 device instance

 *

 * This function is used to enable interrupts at runtime, both in the runtime

 * pm and the system suspend/resume code.

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

/**

 * DOC: Intel GVT-g host support

 *

 * Intel GVT-g is a graphics virtualization technology which shares the

 * GPU among multiple virtual machines on a time-sharing basis. Each

 * virtual machine is presented a virtual GPU (vGPU), which has equivalent

 * features as the underlying physical GPU (pGPU), so i915 driver can run

 * seamlessly in a virtual machine.

 *

 * To virtualize GPU resources GVT-g driver depends on hypervisor technology

 * e.g KVM/VFIO/mdev, Xen, etc. to provide resource access trapping capability

 * and be virtualized within GVT-g device module. More architectural design

 * doc is available on https://01.org/group/2230/documentation-list.

/**

 * intel_gvt_sanitize_options - sanitize GVT related options

 * @dev_priv: drm i915 private data

 *

 * This function is called at the i915 options sanitize stage.

/**

 * intel_gvt_init - initialize GVT components

 * @dev_priv: drm i915 private data

 *

 * This function is called at the initialization stage to create a GVT device.

 *

 * Returns:

 * Zero on success, negative error code if failed.

 *

/**

 * intel_gvt_driver_remove - cleanup GVT components when i915 driver is

 *			     unbinding

 * @dev_priv: drm i915 private *

 *

 * This function is called at the i915 driver unloading stage, to shutdown

 * GVT components and release the related resources.

/**

 * intel_gvt_resume - GVT resume routine wapper

 *

 * @dev_priv: drm i915 private *

 *

 * This function is called at the i915 driver resume stage to restore required

 * HW status for GVT so that vGPU can continue running after resumed.

/*

 * Copyright Â© 2008-2015 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

	/*

	 * The timeline struct (as part of the ppgtt underneath a context)

	 * may be freed when the request is no longer in use by the GPU.

	 * We could extend the life of a context to beyond that of all

	 * fences, possibly keeping the hw resource around indefinitely,

	 * or we just give them a false name. Since

	 * dma_fence_ops.get_timeline_name is a debug feature, the occasional

	 * lie seems justifiable.

	/*

	 * The request is put onto a RCU freelist (i.e. the address

	 * is immediately reused), mark the fences as being freed now.

	 * Otherwise the debugobjects for the fences are only marked as

	 * freed when the slab cache itself is freed, and so we would get

	 * caught trying to reuse dead objects.

	/*

	 * Keep one request on each engine for reserved use under mempressure,

	 * do not use with virtual engines as this really is only needed for

	 * kernel contexts.

/**

 * i915_request_active_engine

 * @rq: request to inspect

 * @active: pointer in which to return the active engine

 *

 * Fills the currently active engine to the @active pointer if the request

 * is active and still not completed.

 *

 * Returns true if request was active or false otherwise.

	/*

	 * Serialise with __i915_request_submit() so that it sees

	 * is-banned?, or we know the request is already inflight.

	 *

	 * Note that rq->engine is unstable, and so we double

	 * check that we have acquired the lock on the final engine.

	/*

	 * We know the GPU must have read the request to have

	 * sent us the seqno + interrupt, so use the position

	 * of tail of the request to update the last known position

	 * of the GPU head.

	 *

	 * Note this requires that we are always called in request

	 * completion order.

 Poison before we release our space in the ring */

	/*

	 * We only loosely track inflight requests across preemption,

	 * and so we may find ourselves attempting to retire a _completed_

	 * request that we have removed from the HW and put back on a run

	 * queue.

	 *

	 * As we set I915_FENCE_FLAG_ACTIVE on the request, this should be

	 * after removing the breadcrumb and signaling it, so that we do not

	 * inadvertently attach the breadcrumb to a completed request.

 poison neither prev/next (RCU walks) */

	/*

	 * Even if we have unwound the request, it may still be on

	 * the GPU (preempt-to-busy). If that request is inside an

	 * unpreemptible critical section, it will not be removed. Some

	 * GPU functions may even be stuck waiting for the paired request

	 * (__await_execution) to be submitted and cannot be preempted

	 * until the bond is executing.

	 *

	 * As we know that there are always preemption points between

	 * requests, we know that only the currently executing request

	 * may be still active even though we have cleared the flag.

	 * However, we can't rely on our tracking of ELSP[0] to know

	 * which request is currently active and so maybe stuck, as

	 * the tracking maybe an event behind. Instead assume that

	 * if the context is still inflight, then it is still active

	 * even if the active flag has been cleared.

	 *

	 * To further complicate matters, if there a pending promotion, the HW

	 * may either perform a context switch to the second inflight execlists,

	 * or it may switch to the pending set of execlists. In the case of the

	 * latter, it may send the ACK and we process the event copying the

	 * pending[] over top of inflight[], _overwriting_ our *active. Since

	 * this implies the HW is arbitrating and not struck in *active, we do

	 * not worry about complete accuracy, but we do require no read/write

	 * tearing of the pointer [the read of the pointer must be valid, even

	 * as the array is being overwritten, for which we require the writes

	 * to avoid tearing.]

	 *

	 * Note that the read of *execlists->active may race with the promotion

	 * of execlists->pending[] to execlists->inflight[], overwritting

	 * the value at *execlists->active. This is fine. The promotion implies

	 * that we received an ACK from the HW, and so the context is not

	 * stuck -- if we do not see ourselves in *active, the inflight status

	 * is valid. If instead we see ourselves being copied into *active,

	 * we are inflight and may signal the callback.

 may race with promotion of pending[] */

	/*

	 * Register the callback first, then see if the signaler is already

	 * active. This ensures that if we race with the

	 * __notify_execute_cb from i915_request_submit() and we are not

	 * included in that list, we get a second bite of the cherry and

	 * execute it ourselves. After this point, a future

	 * i915_request_submit() will notify us.

	 *

	 * In i915_request_retire() we set the ACTIVE bit on a completed

	 * request (then flush the execute_cb). So by registering the

	 * callback first, then checking the ACTIVE bit, we serialise with

	 * the completed/retired request.

 not an error! */

 innocent victim of a GT reset (__i915_request_reset) */

 waiting for Godot (timer_i915_sw_fence_wake) */

	/*

	 * As this request likely depends on state from the lost

	 * context, clear out all the user operations leaving the

	 * breadcrumb at the end (so we get the fence notifications).

 As soon as the request is completed, it may be retired */

	/*

	 * With the advent of preempt-to-busy, we frequently encounter

	 * requests that we have unsubmitted from HW, but left running

	 * until the next ack and so have completed in the meantime. On

	 * resubmission of that completed request, we can skip

	 * updating the payload, and execlists can even skip submitting

	 * the request.

	 *

	 * We must remove the request from the caller's priority queue,

	 * and the caller must only call us when the request is in their

	 * priority queue, under the sched_engine->lock. This ensures that the

	 * request has *not* yet been retired and we can safely move

	 * the request into the engine->active.list where it will be

	 * dropped upon retiring. (Otherwise if resubmit a *retired*

	 * request, this would be a horrible use-after-free.)

	/*

	 * Are we using semaphores when the gpu is already saturated?

	 *

	 * Using semaphores incurs a cost in having the GPU poll a

	 * memory location, busywaiting for it to change. The continual

	 * memory reads can have a noticeable impact on the rest of the

	 * system with the extra bus traffic, stalling the cpu as it too

	 * tries to access memory across the bus (perf stat -e bus-cycles).

	 *

	 * If we installed a semaphore on this request and we only submit

	 * the request after the signaler completed, that indicates the

	 * system is overloaded and using semaphores at this time only

	 * increases the amount of work we are doing. If so, we disable

	 * further use of semaphores until we are idle again, whence we

	 * optimistically try again.

	/*

	 * XXX Rollback bonded-execution on __i915_request_unsubmit()?

	 *

	 * In the future, perhaps when we have an active time-slicing scheduler,

	 * it will be interesting to unsubmit parallel execution and remove

	 * busywaits from the GPU until their master is restarted. This is

	 * quite hairy, we have to carefully rollback the fence and do a

	 * preempt-to-idle cycle on the target engine, all the while the

	 * master execute_cb may refire.

 We may be recursing from the signal callback of another i915 fence */

 Will be called from irq-context when using foreign fences. */

	/*

	 * Only unwind in reverse order, required so that the per-context list

	 * is kept in seqno/ring order.

	/*

	 * Before we remove this breadcrumb from the signal list, we have

	 * to ensure that a concurrent dma_fence_enable_signaling() does not

	 * attach itself. We first mark the request as no longer active and

	 * make sure that is visible to other cores, and then remove the

	 * breadcrumb if attached.

 We've already spun, don't charge on resubmitting. */

	/*

	 * We don't need to wake_up any waiters on request->execute, they

	 * will get woken by any other event or us re-adding this request

	 * to the engine timeline (__i915_request_submit()). The waiters

	 * should be quite adapt at finding that the request now has a new

	 * global_seqno to the one they went to sleep on.

 Will be called from irq-context when using foreign fences. */

		/*

		 * We need to serialize use of the submit_request() callback

		 * with its hotplugging performed during an emergency

		 * i915_gem_set_wedged().  We use the RCU mechanism to mark the

		 * critical section in order to force i915_gem_set_wedged() to

		 * wait until the submit_request() is completed before

		 * proceeding.

 If we cannot wait, dip into our reserves */

 Use the normal failure path for one final WARN */

 Move our oldest request to the slab-cache (if not in use!) */

 Ratelimit ourselves to prevent oom from malicious clients */

 Retire our old requests in the hope that we free some */

 Check that the caller provided an already pinned context */

	/*

	 * Beware: Dragons be flying overhead.

	 *

	 * We use RCU to look up requests in flight. The lookups may

	 * race with the request being allocated from the slab freelist.

	 * That is the request we are writing to here, may be in the process

	 * of being read by __i915_active_request_get_rcu(). As such,

	 * we have to be very careful when overwriting the contents. During

	 * the RCU lookup, we change chase the request->engine pointer,

	 * read the request->global_seqno and increment the reference count.

	 *

	 * The reference count is incremented atomically. If it is zero,

	 * the lookup knows the request is unallocated and complete. Otherwise,

	 * it is either still in use, or has been reallocated and reset

	 * with dma_fence_init(). This increment is safe for release as we

	 * check that the request we have a reference to and matches the active

	 * request.

	 *

	 * Before we increment the refcount, we chase the request->engine

	 * pointer. We must not call kmem_cache_zalloc() or else we set

	 * that pointer to NULL and cause a crash during the lookup. If

	 * we see the request is completed (based on the value of the

	 * old engine and seqno), the lookup is complete and reports NULL.

	 * If we decide the request is not completed (new engine or seqno),

	 * then we grab a reference and double check that it is still the

	 * active request - which it won't be and restart the lookup.

	 *

	 * Do not use kmem_cache_zalloc() here!

	/*

	 * Hold a reference to the intel_context over life of an i915_request.

	 * Without this an i915_request can exist after the context has been

	 * destroyed (e.g. request retired, context closed, but user space holds

	 * a reference to the request from an out fence). In the case of GuC

	 * submission + virtual engine, the engine that the request references

	 * is also destroyed which can trigger bad pointer dref in fence ops

	 * (e.g. i915_fence_get_driver_name). We could likely change these

	 * functions to avoid touching the engine but let's just be safe and

	 * hold the intel_context reference. In execlist mode the request always

	 * eventually points to a physical engine so this isn't an issue.

 acts as smp_mb() */

 We bump the ref for the fence chain */

 No zalloc, everything must be cleared after use */

	/*

	 * Reserve space in the ring buffer for all the commands required to

	 * eventually emit this request. This is to guarantee that the

	 * i915_request_add() call can't fail. Note that the reserve may need

	 * to be redone if the request is not actually submitted straight

	 * away, e.g. because a GPU scheduler has deferred it.

	 *

	 * Note that due to how we add reserved_space to intel_ring_begin()

	 * we need to double our request to ensure that if we need to wrap

	 * around inside i915_request_add() there is sufficient space at

	 * the beginning of the ring as well.

	/*

	 * Record the position of the start of the request so that

	 * should we detect the updated seqno part-way through the

	 * GPU processing the request, we never over-estimate the

	 * position of the head.

 end of header; start of user payload */

 Make sure we didn't add ourselves to external state before freeing */

 Move our oldest request to the slab-cache (if not in use!) */

 active reference transferred to request */

 Check that we do not interrupt ourselves with a new request */

	/*

	 * The caller holds a reference on @signal, but we do not serialise

	 * against it being retired and removed from the lists.

	 *

	 * We do not hold a reference to the request before @signal, and

	 * so must be very careful to ensure that it is not _recycled_ as

	 * we follow the link backwards.

 Confirm signal has not been retired, the link is valid */

 Is signal the earliest request on its timeline? */

		/*

		 * Peek at the request before us in the timeline. That

		 * request will only be valid before it is retired, so

		 * after acquiring a reference to it, confirm that it is

		 * still part of the signaler's timeline.

 After the strong barrier, confirm prev is still attached */

	/*

	 * Polling a semaphore causes bus traffic, delaying other users of

	 * both the GPU and CPU. We want to limit the impact on others,

	 * while taking advantage of early submission to reduce GPU

	 * latency. Therefore we restrict ourselves to not using more

	 * than one semaphore from each source, and not using a semaphore

	 * if we have detected the engine is saturated (i.e. would not be

	 * submitted early and cause bus traffic reading an already passed

	 * semaphore).

	 *

	 * See the are-we-too-late? check in __i915_request_submit().

 We need to pin the signaler's HWSP until we are finished reading. */

	/*

	 * Using greater-than-or-equal here means we have to worry

	 * about seqno wraparound. To side step that issue, we swap

	 * the timeline HWSP upon wrapping, so that everyone listening

	 * for the old (pre-wrap) values do not see the much smaller

	 * (post-wrap) values than they were expecting (and so wait

	 * forever).

	/*

	 * If this or its dependents are waiting on an external fence

	 * that may fail catastrophically, then we want to avoid using

	 * sempahores as they bypass the fence signaling metadata, and we

	 * lose the fence->error propagation.

 Just emit the first semaphore we see as request space is limited. */

 Only submit our spinner after the signaler is running! */

 Submit both requests at the same time */

 Squash repeated depenendices to the same timelines */

	/*

	 * Wait until the start of this request.

	 *

	 * The execution cb fires when we submit the request to HW. But in

	 * many cases this may be long before the request itself is ready to

	 * run (consider that we submit 2 requests for the same context, where

	 * the request of interest is behind an indefinite spinner). So we hook

	 * up to both to reduce our queues and keep the execution lag minimised

	 * in the worst case, though we hope that the await_start is elided.

	/*

	 * Ensure both start together [after all semaphores in signal]

	 *

	 * Now that we are queued to the HW at roughly the same time (thanks

	 * to the execute cb) and are ready to run at roughly the same time

	 * (thanks to the await start), our signaler may still be indefinitely

	 * delayed by waiting on a semaphore from a remote engine. If our

	 * signaler depends on a semaphore, so indirectly do we, and we do not

	 * want to start our payload until our signaler also starts theirs.

	 * So we wait.

	 *

	 * However, there is also a second condition for which we need to wait

	 * for the precise start of the signaler. Consider that the signaler

	 * was submitted in a chain of requests following another context

	 * (with just an ordinary intra-engine fence dependency between the

	 * two). In this case the signaler is queued to HW, but not for

	 * immediate execution, and so we must wait until it reaches the

	 * active slot.

 Couple the dependency tree for PI on this exposed to->fence */

	/*

	 * The downside of using semaphores is that we lose metadata passing

	 * along the signaling chain. This is particularly nasty when we

	 * need to pass along a fatal error such as EFAULT or EDEADLK. For

	 * fatal errors we want to scrub the request before it is executed,

	 * which means that we cannot preload the request onto HW and have

	 * it wait upon a semaphore.

 XXX Error for signal-on-any fence arrays */

		/*

		 * We don't squash repeated fence dependencies here as we

		 * want to run our callback in all cases.

	/*

	 * If we are waiting on a virtual engine, then it may be

	 * constrained to execute on a single engine *prior* to submission.

	 * When it is submitted, it will be first submitted to the virtual

	 * engine and then passed to the physical engine. We cannot allow

	 * the waiter to be submitted immediately to the physical engine

	 * as it may then bypass the virtual request.

	/*

	 * Note that if the fence-array was created in signal-on-any mode,

	 * we should *not* decompose it into its individual fences. However,

	 * we don't currently store which mode the fence-array is operating

	 * in. Fortunately, the only user of signal-on-any is private to

	 * amdgpu and we should not see any incoming fence-array from

	 * sync-file being in signal-on-any mode.

		/*

		 * Requests on the same timeline are explicitly ordered, along

		 * with their dependencies, by i915_request_add() which ensures

		 * that requests are submitted in-order through each ring.

 Squash repeated waits to the same timelines */

 Record the latest fence used against each timeline */

/**

 * i915_request_await_object - set this request to (async) wait upon a bo

 * @to: request we are wishing to use

 * @obj: object which may be in use on another ring.

 * @write: whether the wait is on behalf of a writer

 *

 * This code is meant to abstract object synchronization with the GPU.

 * Conceptually we serialise writes between engines inside the GPU.

 * We only allow one engine to write into a buffer at any time, but

 * multiple readers. To ensure each has a coherent view of memory, we must:

 *

 * - If there is an outstanding write request to the object, the new

 *   request must wait for it to complete (either CPU or in hw, requests

 *   on the same ring will be naturally ordered).

 *

 * - If we are a write request (pending_write_domain is set), the new

 *   request must wait for outstanding read requests to complete.

 *

 * Returns 0 if successful, else propagates up the lower layer error.

		/*

		 * The requests are supposed to be kept in order. However,

		 * we need to be wary in case the timeline->last_request

		 * is used as a barrier for external modification to this

		 * context.

	/*

	 * Dependency tracking and request ordering along the timeline

	 * is special cased so that we can eliminate redundant ordering

	 * operations while building the request (we know that the timeline

	 * itself is ordered, and here we guarantee it).

	 *

	 * As we know we will need to emit tracking along the timeline,

	 * we embed the hooks into our request struct -- at the cost of

	 * having to have specialised no-allocation interfaces (which will

	 * be beneficial elsewhere).

	 *

	 * A second benefit to open-coding i915_request_await_request is

	 * that we can apply a slight variant of the rules specialised

	 * for timelines that jump between engines (such as virtual engines).

	 * If we consider the case of virtual engine, we must emit a dma-fence

	 * to prevent scheduling of the second request until the first is

	 * complete (to maximise our greedy late load balancing) and this

	 * precludes optimising to use semaphores serialisation of a single

	 * timeline across engines.

	 *

	 * We do not order parallel submission requests on the timeline as each

	 * parallel submission context has its own timeline and the ordering

	 * rules for parallel requests are that they must be submitted in the

	 * order received from the execbuf IOCTL. So rather than using the

	 * timeline we store a pointer to last request submitted in the

	 * relationship in the gem context and insert a submission fence

	 * between that request and request passed into this function or

	 * alternatively we use completion fence if gem context has a single

	 * timeline and this is the first submission of an execbuf IOCTL.

	/*

	 * Make sure that no request gazumped us - if it was allocated after

	 * our i915_request_alloc() and called __i915_request_add() before

	 * us, the timeline will hold its seqno which is later than ours.

/*

 * NB: This function is not allowed to fail. Doing so would mean the the

 * request is not being tracked for completion but the work itself is

 * going to happen on the hardware. This would be a Bad Thing(tm).

	/*

	 * To ensure that this call will not fail, space for its emissions

	 * should already have been reserved in the ring buffer. Let the ring

	 * know that it is time to use that space up.

	/*

	 * Record the position of the start of the breadcrumb so that

	 * should we detect the updated seqno part-way through the

	 * GPU processing the request, we never over-estimate the

	 * position of the ring's HEAD.

	/*

	 * Let the backend know a new request has arrived that may need

	 * to adjust the existing execution schedule due to a high priority

	 * request - i.e. we may want to preempt the current request in order

	 * to run a high priority dependency chain *before* we can execute this

	 * request.

	 *

	 * This is called before the request is ready to run so that we can

	 * decide whether to preempt the entire chain so that it is ready to

	 * run at the earliest possible convenience.

 kick tasklets */

 XXX placeholder for selftests */

	/*

	 * Cheaply and approximately convert from nanoseconds to microseconds.

	 * The result and subsequent calculations are also defined in the same

	 * approximate microseconds units. The principal source of timing

	 * error here is from the simple truncation.

	 *

	 * Note that local_clock() is only defined wrt to the current CPU;

	 * the comparisons are no longer valid if we switch CPUs. Instead of

	 * blocking preemption for the entire busywait, we can detect the CPU

	 * switch and use that as indicator of system load and a reason to

	 * stop busywaiting, see busywait_stop().

	/*

	 * Only wait for the request if we know it is likely to complete.

	 *

	 * We don't track the timestamps around requests, nor the average

	 * request length, so we do not have a good indicator that this

	 * request will complete within the timeout. What we do know is the

	 * order in which requests are executed by the context and so we can

	 * tell if the request has been started. If the request is not even

	 * running yet, it is a fair assumption that it will not complete

	 * within our relatively short timeout.

	/*

	 * When waiting for high frequency requests, e.g. during synchronous

	 * rendering split between the CPU and GPU, the finite amount of time

	 * required to set up the irq and wait upon it limits the response

	 * rate. By busywaiting on the request completion for a short while we

	 * can service the high frequency waits as quick as possible. However,

	 * if it is a slow request, we want to sleep as quickly as possible.

	 * The tradeoff between waiting and sleeping is roughly the time it

	 * takes to sleep on a request, on the order of a microsecond.

/**

 * i915_request_wait - wait until execution of request has finished

 * @rq: the request to wait upon

 * @flags: how to wait

 * @timeout: how long to wait in jiffies

 *

 * i915_request_wait() waits for the request to be completed, for a

 * maximum of @timeout jiffies (with MAX_SCHEDULE_TIMEOUT implying an

 * unbounded wait).

 *

 * Returns the remaining time (in jiffies) if the request completed, which may

 * be zero or -ETIME if the request is unfinished after the timeout expires.

 * May return -EINTR is called with I915_WAIT_INTERRUPTIBLE and a signal is

 * pending before the request completes.

	/*

	 * We must never wait on the GPU while holding a lock as we

	 * may need to perform a GPU reset. So while we don't need to

	 * serialise wait/reset with an explicit lock, we do want

	 * lockdep to detect potential dependency cycles.

	/*

	 * Optimistic spin before touching IRQs.

	 *

	 * We may use a rather large value here to offset the penalty of

	 * switching away from the active task. Frequently, the client will

	 * wait upon an old swapbuffer to throttle itself to remain within a

	 * frame of the gpu. If the client is running in lockstep with the gpu,

	 * then it should not be waiting long at all, and a sleep now will incur

	 * extra scheduler latency in producing the next frame. To try to

	 * avoid adding the cost of enabling/disabling the interrupt to the

	 * short wait, we first spin to see if the request would have completed

	 * in the time taken to setup the interrupt.

	 *

	 * We need upto 5us to enable the irq, and upto 20us to hide the

	 * scheduler latency of a context switch, ignoring the secondary

	 * impacts from a context switch such as cache eviction.

	 *

	 * The scheme used for low-latency IO is called "hybrid interrupt

	 * polling". The suggestion there is to sleep until just before you

	 * expect to be woken by the device interrupt and then poll for its

	 * completion. That requires having a good predictor for the request

	 * duration, which we currently lack.

	/*

	 * This client is about to stall waiting for the GPU. In many cases

	 * this is undesirable and limits the throughput of the system, as

	 * many clients cannot continue processing user input/output whilst

	 * blocked. RPS autotuning may take tens of milliseconds to respond

	 * to the GPU load and thus incurs additional latency for the client.

	 * We can circumvent that by promoting the GPU frequency to maximum

	 * before we sleep. This makes the GPU throttle up much more quickly

	 * (good for benchmarks and user experience, e.g. window animations),

	 * but at a cost of spending more power processing the workload

	 * (bad for battery).

	/*

	 * Flush the submission tasklet, but only if it may help this request.

	 *

	 * We sometimes experience some latency between the HW interrupts and

	 * tasklet execution (mostly due to ksoftirqd latency, but it can also

	 * be due to lazy CS events), so lets run the tasklet manually if there

	 * is a chance it may submit this request. If the request is not ready

	 * to run, as it is waiting for other fences to be signaled, flushing

	 * the tasklet is busy work without any advantage for this client.

	 *

	 * If the HW is being lazy, this is the last chance before we go to

	 * sleep to catch any pending events. We will check periodically in

	 * the heartbeat to flush the submission tasklets as a last resort

	 * for unhappy HW.

	/*

	 * The prefix is used to show the queue status, for which we use

	 * the following flags:

	 *

	 *  U [Unready]

	 *    - initial status upon being submitted by the user

	 *

	 *    - the request is not ready for execution as it is waiting

	 *      for external fences

	 *

	 *  R [Ready]

	 *    - all fences the request was waiting on have been signaled,

	 *      and the request is now ready for execution and will be

	 *      in a backend queue

	 *

	 *    - a ready request may still need to wait on semaphores

	 *      [internal fences]

	 *

	 *  V [Ready/virtual]

	 *    - same as ready, but queued over multiple backends

	 *

	 *  E [Executing]

	 *    - the request has been transferred from the backend queue and

	 *      submitted for execution on HW

	 *

	 *    - a completed request may still be regarded as executing, its

	 *      status may not be updated until it is retired and removed

	 *      from the lists

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2013-2021 Intel Corporation

/*

 * IOSF sideband, see VLV2_SidebandMsg_HAS.docx and

 * VLV_VLV2_PUNIT_HAS_0.8.docx

 Standard MMIO read, non-posted */

 Standard MMIO write, non-posted */

 Private register read, double-word addressing, non-posted */

 Private register write, double-word addressing, non-posted */

	/*

	 * Prevent the cpu from sleeping while we use this sideband, otherwise

	 * the punit may cause a machine hang. The issue appears to be isolated

	 * with changing the power state of the CPU package while changing

	 * the power state via the punit, and we have only observed it

	 * reliably on 4-core Baytail systems suggesting the issue is in the

	 * power delivery mechanism and likely to be board/function

	 * specific. Hence we presume the workaround needs only be applied

	 * to the Valleyview P-unit and not all sideband communications.

 Flush the previous comms, just in case it failed last time. */

	/*

	 * IOSF_PORT_DPIO: VLV x2 PHY (DP/HDMI B and C), CHV x1 PHY (DP/HDMI D)

	 * IOSF_PORT_DPIO_2: CHV x2 PHY (DP/HDMI B and C)

	/*

	 * FIXME: There might be some registers where all 1's is a valid value,

	 * so ideally we should check the register offset instead...

/* i915_drv.c -- i830,i845,i855,i865,i915 driver -*- linux-c -*-

/*

 *

 * Copyright 2003 Tungsten Graphics, Inc., Cedar Park, Texas.

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the

 * "Software"), to deal in the Software without restriction, including

 * without limitation the rights to use, copy, modify, merge, publish,

 * distribute, sub license, and/or sell copies of the Software, and to

 * permit persons to whom the Software is furnished to do so, subject to

 * the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.

 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR

 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,

 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE

 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

 Allocate space for the MCH regs if needed, return nonzero on error */

 If ACPI doesn't have it, assume we need to allocate it ourselves */

 Get some space for it */

 Setup MCHBAR if possible, return true if we should disable it again */

 If it's already enabled, don't have to do anything */

 Space is allocated or reserved, so enable it. */

	/*

	 * The i915 workqueue is primarily used for batched retirement of

	 * requests (and thus managing bo) once the task has been completed

	 * by the GPU. i915_retire_requests() is called directly when we

	 * need high-priority retirement, such as waiting for an explicit

	 * bo.

	 *

	 * It is also used for periodic low-priority events, such as

	 * idle-timers and recording error state.

	 *

	 * All tasks on the workqueue are expected to acquire the dev mutex

	 * so there is no point in running more than one instance of the

	 * workqueue at any time.  Use an ordered one.

/*

 * We don't keep the workarounds for pre-production hardware, so we expect our

 * driver to fail on these machines in one way or another. A little warning on

 * dmesg may help both the user and the bug triagers.

 *

 * Our policy for removing pre-production workarounds is to keep the

 * current gen workarounds as a guide to the bring-up of the next gen

 * (workarounds have a habit of persisting!). Anything older than that

 * should be removed along with the complications they introduce.

/**

 * i915_driver_early_probe - setup state not requiring device access

 * @dev_priv: device private

 *

 * Initialize everything that is a "SW-only" state, that is state not

 * requiring accessing the device or exposing the driver via kernel internal

 * or userspace interfaces. Example steps belonging here: lock initialization,

 * system memory allocation, setting up device specific attributes and

 * function hooks not requiring accessing the device.

 This must be called before any calls to HAS_PCH_* */

/**

 * i915_driver_late_release - cleanup the setup done in

 *			       i915_driver_early_probe()

 * @dev_priv: device private

/**

 * i915_driver_mmio_probe - setup device MMIO

 * @dev_priv: device private

 *

 * Setup minimal device state necessary for MMIO accesses later in the

 * initialization sequence. The setup here should avoid any other device-wide

 * side effects or exposing the driver via kernel internal or user space

 * interfaces.

 Try to make sure MCHBAR is enabled before poking at it */

 As early as possible, scrub existing GPU state before clobbering */

/**

 * i915_driver_mmio_release - cleanup the setup done in i915_driver_mmio_probe()

 * @dev_priv: device private

/**

 * i915_set_dma_info - set all relevant PCI dma info as configured for the

 * platform

 * @i915: valid i915 instance

 *

 * Set the dma max segment size, device and coherent masks.  The dma mask set

 * needs to occur before i915_ggtt_probe_hw.

 *

 * A couple of platforms have special needs.  Address them as well.

 *

	/*

	 * We don't have a max segment size, so set it to the max so sg's

	 * debugging layer doesn't complain

 overlay on gen2 is broken and can't address above 1G */

	/*

	 * 965GM sometimes incorrectly writes to hardware status page (HWS)

	 * using 32bit addressing, overwriting memory if HWS is located

	 * above 4GB.

	 *

	 * The documentation also mentions an issue with undefined

	 * behaviour if any general state is accessed within a page above 4GB,

	 * which also needs to be handled carefully.

/**

 * i915_driver_hw_probe - setup state requiring device access

 * @dev_priv: device private

 *

 * Setup state that requires accessing the device, but doesn't require

 * exposing the driver via kernel internal or userspace interfaces.

		/*

		 * Older GVT emulation depends upon intercepting CSB mmio,

		 * which we no longer use, preferring to use the HWSP cache

		 * instead.

 needs to be done before ggtt probe */

	/* On the 945G/GM, the chipset reports the MSI capability on the

	 * integrated graphics even though the support isn't actually there

	 * according to the published specs.  It doesn't appear to function

	 * correctly in testing on 945G.

	 * This may be a side effect of MSI having been made available for PEG

	 * and the registers being closely associated.

	 *

	 * According to chipset errata, on the 965GM, MSI interrupts may

	 * be lost or delayed, and was defeatured. MSI interrupts seem to

	 * get lost on g4x as well, and interrupt delivery seems to stay

	 * properly dead afterwards. So we'll just disable them for all

	 * pre-gen5 chipsets.

	 *

	 * dp aux and gmbus irq on gen4 seems to be able to generate legacy

	 * interrupts even when in MSI mode. This results in spurious

	 * interrupt warnings if the legacy irq no. is shared with another

	 * device. The kernel then disables that interrupt source and so

	 * prevents the other device from working properly.

	/*

	 * Fill the dram structure to get the system dram info. This will be

	 * used for memory latency calculation.

/**

 * i915_driver_hw_remove - cleanup the setup done in i915_driver_hw_probe()

 * @dev_priv: device private

/**

 * i915_driver_register - register the driver with the rest of the system

 * @dev_priv: device private

 *

 * Perform any steps necessary to make the driver available via kernel

 * internal or userspace interfaces.

 Reveal our presence to userspace */

 Depends on sysfs having been initialized */

/**

 * i915_driver_unregister - cleanup the registration done in i915_driver_regiser()

 * @dev_priv: device private

 Device parameters start as a copy of module parameters. */

 Setup the write-once "constant" device info */

/**

 * i915_driver_probe - setup chip and create an initial config

 * @pdev: PCI device

 * @ent: matching PCI ID entry

 *

 * The driver probe routine has to do several things:

 *   - drive output discovery via intel_modeset_init()

 *   - initialize the memory manager

 *   - allocate initial config memory

 *   - setup the DRM framebuffer with the allocated memory

 Disable nuclear pageflip by default on pre-ILK */

	/*

	 * Check if we support fake LMEM -- for now we only unleash this for

	 * the live selftests(test-and-exit).

 FIXME clean up the error path */

 Flush any external code that still may be under the RCU lock */

/**

 * i915_driver_lastclose - clean up after all DRM clients have exited

 * @dev: DRM device

 *

 * Take care of cleaning up after all DRM clients have exited.  In the

 * mode setting case, we want to restore the kernel's initial mode (just

 * in case the last client left us in a bad state).

 *

 * Additionally, in the non-mode setting case, we'll tear down the GTT

 * and DMA structures, since the kernel won't be using them, and clea

 * up any GEM state.

 Catch up with all the deferred frees from "this" client */

	/*

	 * The only requirement is to reboot with display DC states disabled,

	 * for now leaving all display power wells in the INIT power domain

	 * enabled.

	 *

	 * TODO:

	 * - unify the pci_driver::shutdown sequence here with the

	 *   pci_driver.driver.pm.poweroff,poweroff_late sequence.

	 * - unify the driver remove and system/runtime suspend sequences with

	 *   the above unified shutdown/poweroff sequence.

	/*

	 * NB intel_display_suspend() may issue new requests after we've

	 * ostensibly marked the GPU as ready-to-sleep here. We need to

	 * split out that work and pull it forward so that after point,

	 * the GPU is not woken again.

	/* We do a lot of poking in a lot of registers, make sure they work

	/*

	 * During hibernation on some platforms the BIOS may try to access

	 * the device even though it's already in D3 and hang the machine. So

	 * leave the device in D0 on those platforms and hope the BIOS will

	 * power down the device properly. The issue was seen on multiple old

	 * GENs with different BIOS vendors, so having an explicit blacklist

	 * is inpractical; apply the workaround on everything pre GEN6. The

	 * platforms where the issue was seen:

	 * Lenovo Thinkpad X301, X61s, X60, T60, X41

	 * Fujitsu FSC S7110

	 * Acer Aspire 1830T

	/*

	 * Interrupts have to be enabled before any batches are run. If not the

	 * GPU will hang. i915_gem_init_hw() will initiate batches to

	 * update/restore the context.

	 *

	 * drm_mode_config_reset() needs AUX interrupts.

	 *

	 * Modeset enabling in intel_modeset_init_hw() also needs working

	 * interrupts.

 MST sideband requires HPD interrupts enabled */

	/*

	 * We have a resume ordering issue with the snd-hda driver also

	 * requiring our device to be power up. Due to the lack of a

	 * parent/child relationship we currently solve this with an early

	 * resume hook.

	 *

	 * FIXME: This should be solved with a special hdmi sink device or

	 * similar so that power domains can be employed.

	/*

	 * Note that we need to set the power state explicitly, since we

	 * powered off the device during freeze and the PCI core won't power

	 * it back up for us during thaw. Powering off the device during

	 * freeze is not a hard requirement though, and during the

	 * suspend/resume phases the PCI core makes sure we get here with the

	 * device powered on. So in case we change our freeze logic and keep

	 * the device powered we can also remove the following set power state

	 * call.

	/*

	 * Note that pci_enable_device() first enables any parent bridge

	 * device and only then sets the power state for this device. The

	 * bridge enabling is a nop though, since bridge devices are resumed

	 * first. The order of enabling power and enabling the device is

	 * imposed by the PCI core as described above, so here we preserve the

	 * same order for the freeze/thaw phases.

	 *

	 * TODO: eventually we should remove pci_disable_device() /

	 * pci_enable_enable_device() from suspend/resume. Due to how they

	 * depend on the device enable refcount we can't anyway depend on them

	 * disabling/enabling the device.

	/*

	 * We have a suspend ordering issue with the snd-hda driver also

	 * requiring our device to be power up. Due to the lack of a

	 * parent/child relationship we currently solve this with an late

	 * suspend hook.

	 *

	 * FIXME: This should be solved with a special hdmi sink device or

	 * similar so that power domains can be employed.

 freeze: before creating the hibernation_image */

 thaw: called after creating the hibernation image, but before turning off. */

 restore: called after loading the hibernation image. */

	/*

	 * We are safe here against re-faults, since the fault handler takes

	 * an RPM reference.

	/*

	 * FIXME: We really should find a document that references the arguments

	 * used below!

		/*

		 * On Broadwell, if we use PCI_D1 the PCH DDI ports will stop

		 * being detected, and the call we do at intel_runtime_resume()

		 * won't be able to restore them. Since PCI_D3hot matches the

		 * actual specification and appears to be working, use it.

		/*

		 * current versions of firmware which depend on this opregion

		 * notification have repurposed the D1 definition to mean

		 * "runtime suspended" vs. what you would normally expect (D3)

		 * to distinguish it from notifications that might be sent via

		 * the suspend path.

	/*

	 * No point of rolling back things in case of an error, as the best

	 * we can do is to hope that things will still work (and disable RPM).

	/*

	 * On VLV/CHV display interrupts are part of the display

	 * power well, so hpd is reinitialized from there. For

	 * everyone else do it here.

	/*

	 * S0ix (via system suspend) and S3 event handlers [PMSG_SUSPEND,

	 * PMSG_RESUME]

	/*

	 * S4 event handlers

	 * @freeze, @freeze_late    : called (1) before creating the

	 *                            hibernation image [PMSG_FREEZE] and

	 *                            (2) after rebooting, before restoring

	 *                            the image [PMSG_QUIESCE]

	 * @thaw, @thaw_early       : called (1) after creating the hibernation

	 *                            image, before writing it [PMSG_THAW]

	 *                            and (2) after failing to create or

	 *                            restore the image [PMSG_RECOVER]

	 * @poweroff, @poweroff_late: called after writing the hibernation

	 *                            image, before rebooting [PMSG_HIBERNATE]

	 * @restore, @restore_early : called after rebooting and restoring the

	 *                            hibernation image [PMSG_RESTORE]

 S0ix (via runtime suspend) event handlers */

	/* Don't use MTRRs here; the Xserver or userspace app should

	 * deal with them for Intel hardware.

/*

 * Copyright Â© 2008 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Eric Anholt <eric@anholt.net>

 *    Keith Packard <keithp@keithp.com>

 *

 Bounce buffer required because of kernfs __user API convenience. */

 On BDW+, swizzling is not used. See detect_bit_6_swizzle() */

	/*

	 * This would lead to infinite waits as we're doing timestamp

	 * difference on the CS with only 32bits.

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2017-2018 Intel Corporation

 Frequency for the sampling timer for events which need it. */

		/*

		 * Events that do not require sampling, or tracking state

		 * transitions between enabled and disabled can be ignored.

	/*

	 * Only some counters need the sampling timer.

	 *

	 * We start with a bitmask of all currently enabled events.

	/*

	 * Mask out all the ones which do not need the timer, or in

	 * other words keep all the ones that could need the timer.

	/*

	 * When the GPU is idle per-engine counters do not need to be

	 * running so clear those bits out.

	/*

	 * Also there is software busyness tracking available we do not

	 * need the timer for I915_SAMPLE_BUSY counter.

	/*

	 * If some bits remain it means we need the sampling timer running.

		/*

		 * We think we are runtime suspended.

		 *

		 * Report the delta from when the device was suspended to now,

		 * on top of the last known real value, as the approximated RC6

		 * counter value.

	/*

	 * Signal sampling timer to stop if only engine events are enabled and

	 * GPU went idle.

	/*

	 * Re-enable sampling timer when GPU goes active.

	/*

	 * We have to avoid concurrent mmio cache line access on gen7 or

	 * risk a machine hang. For a fun history lesson dig out the old

	 * userspace intel_gpu_top and run it on Ivybridge or Haswell!

 powerwell off => engine idle */

 No need to sample when busy stats are supported. */

	/*

	 * While waiting on a semaphore or event, MI_MODE reports the

	 * ring as idle. However, previously using the seqno, and with

	 * execlists sampling, we account for the ring waiting as the

	 * engine being busy. Therefore, we record the sample as being

	 * busy if either waiting or !idle.

 Report 0/0 (actual/requested) frequency while parked. */

		/*

		 * We take a quick peek here without using forcewake

		 * so that we don't perturb the system under observation

		 * (forcewake => !rc6 => increased power use). We expect

		 * that if the read fails because it is outside of the

		 * mmio power well, then it will return 0 -- in which

		 * case we assume the system is running at the intended

		 * frequency. Fortunately, the read should rarely fail!

	/*

	 * Strictly speaking the passed in period may not be 100% accurate for

	 * all internal calculation, since some amount of time can be spent on

	 * grabbing the forcewake. However the potential error from timer call-

	 * back delay greatly dominates this so we keep it simple.

 Requires a mutex for sampling! */

 unsupported modes and filters */

 no sampling */

 only allow running on one cpu at a time */

 Do nothing */

 to MHz */);

 to MHz */);

	/*

	 * Update the bitmask of enabled events and increment

	 * the event reference counter.

	/*

	 * Start the sampling timer if needed and not already enabled.

	/*

	 * For per-engine events the bitmask and reference counting

	 * is stored per engine.

	/*

	 * Store the current counter value so we can report the correct delta

	 * for all listeners. Even when the event was already enabled and has

	 * an existing non-zero value.

		/*

		 * Decrement the reference count and clear the enabled

		 * bitmask when the last listener on an event goes away.

	/*

	 * Decrement the reference count and clear the enabled

	 * bitmask when the last listener on an event goes away.

 Count how many counters we will be exposing. */

 Allocate attribute objects and table. */

 Max one pointer of each attribute type plus a termination entry. */

 Initialize supported non-engine counters. */

 Initialize supported engine counters. */

 Select the first online CPU as a designated reader. */

	/*

	 * Unregistering an instance generates a CPU offline event which we must

	 * ignore to avoid incorrectly modifying the shared i915_pmu_cpumask.

 Migrate events if there is a valid target */

 IGP is 0000:00:02.0 */

 tools/perf reserves colons as special. */

	/*

	 * "Disconnect" the PMU callbacks - since all are atomic synchronize_rcu

	 * ensures all currently executing ones will have exited before we

	 * proceed with unregistration.

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the

 * "Software"), to deal in the Software without restriction, including

 * without limitation the rights to use, copy, modify, merge, publish,

 * distribute, sub license, and/or sell copies of the Software, and to

 * permit persons to whom the Software is furnished to do so, subject to

 * the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

/*

 * Note: As a rule, keep module parameter sysfs permissions read-only

 * 0400. Runtime changes are only supported through i915 debugfs.

 *

 * For any exceptions requiring write access and runtime changes through module

 * parameter sysfs, prevent debugfs file creation by setting the parameter's

 * debugfs mode to 0.

 Special case writable file */

 WA to get away with the default setting in VBT for early platforms.Will be removed */

/**

 * i915_params_dump - dump i915 modparams

 * @params: i915 modparams

 * @p: the &drm_printer

 *

 * Pretty printer for i915 modparams.

 free the allocated members, *not* the passed in params itself */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2013-2021 Intel Corporation

 *

 * LPT/WPT IOSF sideband.

 SBI access */

/*

 * Copyright Â© 2013 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 for shadow reg list */

 Save and disable mmio debugging for the user bypass */

	/*

	 * We don't really know if the powerwell for the forcewake domain we are

	 * trying to reset here does exist at this point (engines could be fused

	 * off in ICL+), so no waiting for acks

 WaRsClearFWBitsAtReset:bdw,skl */

 CI now unreliable */

	/*

	 * There is a possibility of driver's wake request colliding

	 * with hardware's own wake requests and that can cause

	 * hardware to not deliver the driver's ack message.

	 *

	 * Use a fallback bit toggle to kick the gpu state machine

	 * in the hope that the original ack will be delivered along with

	 * the fallback ack.

	 *

	 * This workaround is described in HSDES #1604254524 and it's known as:

	 * WaRsForcewakeAddDelayForAck:skl,bxt,kbl,glk,cfl,cnl,icl

	 * although the name is a bit misleading.

 Give gt some time to relax before the polling frenzy */

 CI now unreliable */

	/*

	 * w/a for a sporadic read returning 0 by waiting for the GT

	 * thread to wake up.

 WaRsForcewakeWaitTC0:snb,ivb,hsw,bdw,vlv */

	/* On VLV, FIFO will be shared by both SW and HW.

 Note callers must have acquired the PUNIT->PMIC bus, before calling this. */

	/* Hold uncore.lock across reset to prevent any register access

	 * with forcewake not set correctly. Wait until all pending

	 * timers are run before holding.

 track the lost user forcewake domains */

	/*

	 * Bugs in PCI programming (or failing hardware) can occasionally cause

	 * us to lose access to the MMIO BAR.  When this happens, register

	 * reads will come back with 0xFFFFFFFF for every register and things

	 * go bad very quickly.  Let's try to detect that special case and at

	 * least try to print a more informative message about what has

	 * happened.

	 *

	 * During normal operation the FPGA_DBG register has several unused

	 * bits that will always read back as 0's so we can use them as canaries

	 * to recognize when MMIO accesses are just busted.

 WaDisableShadowRegForCpd:chv */

/**

 * intel_uncore_forcewake_get - grab forcewake domain references

 * @uncore: the intel_uncore structure

 * @fw_domains: forcewake domains to get reference on

 *

 * This function can be used get GT's forcewake domain references.

 * Normal register access will handle the forcewake domains automatically.

 * However if some sequence requires the GT to not power down a particular

 * forcewake domains this function should be called at the beginning of the

 * sequence. And subsequently the reference should be dropped by symmetric

 * call to intel_unforce_forcewake_put(). Usually caller wants all the domains

 * to be kept awake so the @fw_domains would be then FORCEWAKE_ALL.

/**

 * intel_uncore_forcewake_user_get - claim forcewake on behalf of userspace

 * @uncore: the intel_uncore structure

 *

 * This function is a wrapper around intel_uncore_forcewake_get() to acquire

 * the GT powerwell and in the process disable our debugging for the

 * duration of userspace's bypass.

/**

 * intel_uncore_forcewake_user_put - release forcewake on behalf of userspace

 * @uncore: the intel_uncore structure

 *

 * This function complements intel_uncore_forcewake_user_get() and releases

 * the GT powerwell taken on behalf of the userspace bypass.

/**

 * intel_uncore_forcewake_get__locked - grab forcewake domain references

 * @uncore: the intel_uncore structure

 * @fw_domains: forcewake domains to get reference on

 *

 * See intel_uncore_forcewake_get(). This variant places the onus

 * on the caller to explicitly handle the dev_priv->uncore.lock spinlock.

/**

 * intel_uncore_forcewake_put - release a forcewake domain reference

 * @uncore: the intel_uncore structure

 * @fw_domains: forcewake domains to put references

 *

 * This function drops the device-level forcewakes for specified

 * domains obtained by intel_uncore_forcewake_get().

/**

 * intel_uncore_forcewake_flush - flush the delayed release

 * @uncore: the intel_uncore structure

 * @fw_domains: forcewake domains to flush

/**

 * intel_uncore_forcewake_put__locked - grab forcewake domain references

 * @uncore: the intel_uncore structure

 * @fw_domains: forcewake domains to get reference on

 *

 * See intel_uncore_forcewake_put(). This variant places the onus

 * on the caller to explicitly handle the dev_priv->uncore.lock spinlock.

	/*

	 * Check that the caller has an explicit wakeref and we don't mistake

	 * it for the auto wakeref.

 pending automatic release */

 We give fast paths for the really cool registers */

 Copied and "macroized" from lib/bsearch.c */

	/*

	 * The list of FW domains depends on the SKU in gen11+ so we

	 * can't determine it statically. We use FORCEWAKE_ALL and

	 * translate it here to the list of available domains.

 *Must* be sorted by offset ranges! See intel_fw_table_check(). */

 *Must* be sorted by offset! See intel_shadow_table_check(). */

 TODO: Other registers are not yet used */

	/*

	 * The rest of these ranges are specific to Xe_HP and beyond, but

	 * are reserved/unused ranges on earlier gen12 platforms, so they can

	 * be safely added to the gen12 table.

 *Must* be sorted by offset ranges! See intel_fw_table_check(). */

 *Must* be sorted by offset ranges! See intel_fw_table_check(). */

 uncore range */

 *Must* be sorted by offset ranges! See intel_fw_table_check(). */

 uncore range */

/*

 * *Must* be sorted by offset ranges! See intel_fw_table_check().

 *

 * Note that the spec lists several reserved/unused ranges that don't

 * actually contain any registers.  In the table below we'll combine those

 * reserved ranges with either the preceding or following range to keep the

 * table small and lookups fast.

	GEN_FW_RANGE(0x0, 0x1fff, 0), /*

		0x0   -  0xaff: reserved

	GEN_FW_RANGE(0x4000, 0x51ff, FORCEWAKE_GT), /*

		0x4000 - 0x48ff: gt

	GEN_FW_RANGE(0x5200, 0x7fff, FORCEWAKE_RENDER), /*

		0x5200 - 0x53ff: render

		0x5400 - 0x54ff: reserved

	GEN_FW_RANGE(0x8160, 0x81ff, 0), /*

		0x8160 - 0x817f: reserved

	GEN_FW_RANGE(0x8500, 0x94cf, FORCEWAKE_GT), /*

		0x8500 - 0x87ff: gt

		0x8800 - 0x8fff: reserved

		0x9000 - 0x947f: gt

	GEN_FW_RANGE(0x9560, 0x97ff, 0), /*

		0x9560 - 0x95ff: always on

	GEN_FW_RANGE(0xb400, 0xcfff, FORCEWAKE_GT), /*

		0xb400 - 0xbf7f: gt

		0xb480 - 0xbfff: reserved

	GEN_FW_RANGE(0xdc00, 0xefff, FORCEWAKE_RENDER), /*

		0xdc00 - 0xddff: render

		0xde00 - 0xde7f: reserved

		0xde80 - 0xe8ff: render

	GEN_FW_RANGE(0xf000, 0x147ff, FORCEWAKE_GT), /*

		 0xf000 - 0xffff: gt

	GEN_FW_RANGE(0x14800, 0x1ffff, FORCEWAKE_RENDER), /*

		0x14800 - 0x14fff: render

		0x15000 - 0x16dff: reserved

		0x16e00 - 0x1bfff: render

	GEN_FW_RANGE(0x24000, 0x2417f, 0), /*

		0x24000 - 0x2407f: always on

	GEN_FW_RANGE(0x24180, 0x249ff, FORCEWAKE_GT), /*

		0x24180 - 0x241ff: gt

	GEN_FW_RANGE(0x24a00, 0x251ff, FORCEWAKE_RENDER), /*

		0x24a00 - 0x24a7f: render

	GEN_FW_RANGE(0x25200, 0x255ff, FORCEWAKE_GT), /*

		0x25200 - 0x252ff: gt

	GEN_FW_RANGE(0x25680, 0x259ff, FORCEWAKE_MEDIA_VDBOX2), /*

		0x25680 - 0x256ff: VD2

	GEN_FW_RANGE(0x25a80, 0x2ffff, FORCEWAKE_MEDIA_VDBOX2), /*

		0x25a80 - 0x25aff: VD2

	GEN_FW_RANGE(0x1c0000, 0x1c3fff, FORCEWAKE_MEDIA_VDBOX0), /*

		0x1c0000 - 0x1c2bff: VD0

		0x1c2c00 - 0x1c2cff: reserved

		0x1c2d00 - 0x1c2dff: VD0

		0x1c2e00 - 0x1c3eff: reserved

	GEN_FW_RANGE(0x1c8000, 0x1cbfff, FORCEWAKE_MEDIA_VEBOX0), /*

		0x1c8000 - 0x1ca0ff: VE0

		0x1ca100 - 0x1cbeff: reserved

	GEN_FW_RANGE(0x1cc000, 0x1cffff, FORCEWAKE_MEDIA_VDBOX0), /*

		0x1cc000 - 0x1ccfff: VD0

	GEN_FW_RANGE(0x1d0000, 0x1d3fff, FORCEWAKE_MEDIA_VDBOX2), /*

		0x1d0000 - 0x1d2bff: VD2

		0x1d2c00 - 0x1d2cff: reserved

		0x1d2d00 - 0x1d2dff: VD2

		0x1d2e00 - 0x1d3eff: reserved

/*

 * Graphics IP version 12.55 brings a slight change to the 0xd800 range,

 * switching it from the GT domain to the render domain.

 *

 * *Must* be sorted by offset ranges! See intel_fw_table_check().

	GEN_FW_RANGE(0x0, 0x1fff, 0), /*					\

		  0x0 -  0xaff: reserved					\

	GEN_FW_RANGE(0x4b00, 0x51ff, 0), /*					\

		0x4b00 - 0x4fff: reserved					\

	GEN_FW_RANGE(0x8160, 0x81ff, 0), /*					\

		0x8160 - 0x817f: reserved					\

	GEN_FW_RANGE(0x8500, 0x8cff, FORCEWAKE_GT), /*				\

		0x8500 - 0x87ff: gt						\

		0x8800 - 0x8c7f: reserved					\

	GEN_FW_RANGE(0x8d00, 0x8fff, FORCEWAKE_RENDER), /*			\

		0x8d00 - 0x8dff: render (DG2 only)				\

	GEN_FW_RANGE(0x9000, 0x94cf, FORCEWAKE_GT), /*				\

		0x9000 - 0x947f: gt						\

	GEN_FW_RANGE(0x9560, 0x967f, 0), /*					\

		0x9560 - 0x95ff: always on					\

	GEN_FW_RANGE(0x9680, 0x97ff, FORCEWAKE_RENDER), /*			\

		0x9680 - 0x96ff: render (DG2 only)				\

	GEN_FW_RANGE(0x9800, 0xcfff, FORCEWAKE_GT), /*				\

		0x9800 - 0xb4ff: gt						\

		0xb500 - 0xbfff: reserved					\

	GEN_FW_RANGE(0xdd00, 0xde7f, FORCEWAKE_GT), /*				\

		0xdd00 - 0xddff: gt						\

	GEN_FW_RANGE(0xde80, 0xe8ff, FORCEWAKE_RENDER), /*			\

		0xde80 - 0xdfff: render						\

		0xe000 - 0xe0ff: reserved					\

	GEN_FW_RANGE(0xe900, 0xffff, FORCEWAKE_GT), /*				\

		0xe900 - 0xe9ff: gt						\

		0xea00 - 0xefff: reserved					\

	GEN_FW_RANGE(0x10000, 0x12fff, 0), /*					\

		0x10000 - 0x11fff: reserved					\

		0x12000 - 0x127ff: always on					\

 DG2 only */	\

	GEN_FW_RANGE(0x13200, 0x13fff, FORCEWAKE_MEDIA_VDBOX2), /*		\

		0x13200 - 0x133ff: VD2 (DG2 only)				\

 XEHPSDV only */	\

 XEHPSDV only */	\

 XEHPSDV only */	\

 XEHPSDV only */	\

	GEN_FW_RANGE(0x15000, 0x16dff, FORCEWAKE_GT), /*			\

		0x15000 - 0x15fff: gt (DG2 only)				\

	GEN_FW_RANGE(0x20000, 0x21fff, FORCEWAKE_MEDIA_VDBOX0), /*		\

		0x20000 - 0x20fff: VD0 (XEHPSDV only)				\

	GEN_FW_RANGE(0x24000, 0x2417f, 0), /*					\

		0x24000 - 0x2407f: always on					\

	GEN_FW_RANGE(0x24180, 0x249ff, FORCEWAKE_GT), /*			\

		0x24180 - 0x241ff: gt						\

	GEN_FW_RANGE(0x24a00, 0x251ff, FORCEWAKE_RENDER), /*			\

		0x24a00 - 0x24a7f: render					\

	GEN_FW_RANGE(0x25200, 0x25fff, FORCEWAKE_GT), /*			\

		0x25200 - 0x252ff: gt						\

	GEN_FW_RANGE(0x26000, 0x2ffff, FORCEWAKE_RENDER), /*			\

		0x26000 - 0x27fff: render					\

		0x28000 - 0x29fff: reserved					\

	GEN_FW_RANGE(0x1c0000, 0x1c3fff, FORCEWAKE_MEDIA_VDBOX0), /*		\

		0x1c0000 - 0x1c2bff: VD0					\

		0x1c2c00 - 0x1c2cff: reserved					\

		0x1c2d00 - 0x1c2dff: VD0					\

		0x1c2e00 - 0x1c3eff: VD0 (DG2 only)				\

	GEN_FW_RANGE(0x1c4000, 0x1c7fff, FORCEWAKE_MEDIA_VDBOX1), /*		\

		0x1c4000 - 0x1c6bff: VD1					\

		0x1c6c00 - 0x1c6cff: reserved					\

		0x1c6d00 - 0x1c6dff: VD1					\

	GEN_FW_RANGE(0x1c8000, 0x1cbfff, FORCEWAKE_MEDIA_VEBOX0), /*		\

		0x1c8000 - 0x1ca0ff: VE0					\

	GEN_FW_RANGE(0x1d0000, 0x1d3fff, FORCEWAKE_MEDIA_VDBOX2), /*		\

		0x1d0000 - 0x1d2bff: VD2					\

		0x1d2c00 - 0x1d2cff: reserved					\

		0x1d2d00 - 0x1d2dff: VD2					\

		0x1d2e00 - 0x1d3dff: VD2 (DG2 only)				\

		0x1d3e00 - 0x1d3eff: reserved					\

	GEN_FW_RANGE(0x1d4000, 0x1d7fff, FORCEWAKE_MEDIA_VDBOX3), /*		\

		0x1d4000 - 0x1d6bff: VD3					\

		0x1d6c00 - 0x1d6cff: reserved					\

		0x1d6d00 - 0x1d6dff: VD3					\

	GEN_FW_RANGE(0x1d8000, 0x1dffff, FORCEWAKE_MEDIA_VEBOX1), /*		\

		0x1d8000 - 0x1da0ff: VE1					\

	GEN_FW_RANGE(0x1e0000, 0x1e3fff, FORCEWAKE_MEDIA_VDBOX4), /*		\

		0x1e0000 - 0x1e2bff: VD4					\

		0x1e2c00 - 0x1e2cff: reserved					\

		0x1e2d00 - 0x1e2dff: VD4					\

		0x1e2e00 - 0x1e3eff: reserved					\

	GEN_FW_RANGE(0x1e4000, 0x1e7fff, FORCEWAKE_MEDIA_VDBOX5), /*		\

		0x1e4000 - 0x1e6bff: VD5					\

		0x1e6c00 - 0x1e6cff: reserved					\

		0x1e6d00 - 0x1e6dff: VD5					\

	GEN_FW_RANGE(0x1e8000, 0x1effff, FORCEWAKE_MEDIA_VEBOX2), /*		\

		0x1e8000 - 0x1ea0ff: VE2					\

	GEN_FW_RANGE(0x1f0000, 0x1f3fff, FORCEWAKE_MEDIA_VDBOX6), /*		\

		0x1f0000 - 0x1f2bff: VD6					\

		0x1f2c00 - 0x1f2cff: reserved					\

		0x1f2d00 - 0x1f2dff: VD6					\

		0x1f2e00 - 0x1f3eff: reserved					\

	GEN_FW_RANGE(0x1f4000, 0x1f7fff, FORCEWAKE_MEDIA_VDBOX7), /*		\

		0x1f4000 - 0x1f6bff: VD7					\

		0x1f6c00 - 0x1f6cff: reserved					\

		0x1f6d00 - 0x1f6dff: VD7					\

	/* WaIssueDummyWriteToWakeupFromRC6:ilk Issue a dummy write to wake up

	 * the chip from rc6 before touching it for real. MI_MODE is masked,

 Only report the first N failures */

 interrupts are disabled and re-enabled around uncore->lock usage */

 Turn on all requested but inactive supported forcewake domains. */

 we'll prune the domains of missing engines later */

 IVB configs may use multi-threaded forcewake */

		/* A small trick here - if the bios hasn't configured

		 * MT forcewake, and if the device is in RC6, then

		 * force_wake_mt_get will not wake the device and the

		 * ECOBUS read will return zero. Which will be

		 * (correctly) interpreted by the test below as MT

		 * forcewake being disabled.

		/* We need to init first for ECOBUS access and then

		 * determine later if we want to reinit, in case of MT access is

		 * not working. In this stage we don't know which flavour this

		 * ivb is, so it is better to reset also the gen6 fw registers

		 * before the ecobus check.

 All future platforms are expected to require complex power gating */

		/*

		 * forcewake all now to make sure that we don't need to do a

		 * forcewake later which on systems where this notifier gets

		 * called requires the punit to access to the shared pmic i2c

		 * bus, which will be busy after this notification, leading to:

		 * "render: timed out waiting for forcewake ack request."

		 * errors.

		 *

		 * The notifier is unregistered during intel_runtime_suspend(),

		 * so it's ok to access the HW here without holding a RPM

		 * wake reference -> disable wakeref asserts for the time of

		 * the access.

	/*

	 * Before gen4, the registers and the GTT are behind different BARs.

	 * However, from gen4 onwards, the registers and the GTT are shared

	 * in the same BAR, so we want to restrict this ioremap from

	 * clobbering the GTT which we want ioremap_wc instead. Fortunately,

	 * the register BAR remains the same size for all the earlier

	 * generations up to Ironlake.

	 * For dgfx chips register range is expanded to 4MB.

	/*

	 * The boot firmware initializes local memory and assesses its health.

	 * If memory training fails, the punit will have been instructed to

	 * keep the GT powered down; we won't be able to communicate with it

	 * and we should not continue with driver initialization.

 make sure fw funcs are set if and only if we have fw*/

 clear out unclaimed reg detection bit */

/*

 * We might have detected that some engines are fused off after we initialized

 * the forcewake domains. Prune them, to make sure they only reference existing

 * engines.

		/*

		 * Starting with XeHP, the power well for an even-numbered

		 * VDBOX is also used for shared units within the

		 * media slice such as SFC.  So even if the engine

		 * itself is fused off, we still need to initialize

		 * the forcewake domain if any of the other engines

		 * in the same media slice are present.

/**

 * __intel_wait_for_register_fw - wait until register matches expected state

 * @uncore: the struct intel_uncore

 * @reg: the register to read

 * @mask: mask to apply to register value

 * @value: expected value

 * @fast_timeout_us: fast timeout in microsecond for atomic/tight wait

 * @slow_timeout_ms: slow timeout in millisecond

 * @out_value: optional placeholder to hold registry value

 *

 * This routine waits until the target register @reg contains the expected

 * @value after applying the @mask, i.e. it waits until ::

 *

 *     (intel_uncore_read_fw(uncore, reg) & mask) == value

 *

 * Otherwise, the wait will timeout after @slow_timeout_ms milliseconds.

 * For atomic context @slow_timeout_ms must be zero and @fast_timeout_us

 * must be not larger than 20,0000 microseconds.

 *

 * Note that this routine assumes the caller holds forcewake asserted, it is

 * not suitable for very long waits. See intel_wait_for_register() if you

 * wish to wait without holding forcewake for the duration (i.e. you expect

 * the wait to be slow).

 *

 * Return: 0 if the register matches the desired condition, or -ETIMEDOUT.

 Catch any overuse of this function */

/**

 * __intel_wait_for_register - wait until register matches expected state

 * @uncore: the struct intel_uncore

 * @reg: the register to read

 * @mask: mask to apply to register value

 * @value: expected value

 * @fast_timeout_us: fast timeout in microsecond for atomic/tight wait

 * @slow_timeout_ms: slow timeout in millisecond

 * @out_value: optional placeholder to hold registry value

 *

 * This routine waits until the target register @reg contains the expected

 * @value after applying the @mask, i.e. it waits until ::

 *

 *     (intel_uncore_read(uncore, reg) & mask) == value

 *

 * Otherwise, the wait will timeout after @timeout_ms milliseconds.

 *

 * Return: 0 if the register matches the desired condition, or -ETIMEDOUT.

 just trace the final value */

/**

 * intel_uncore_forcewake_for_reg - which forcewake domains are needed to access

 * 				    a register

 * @uncore: pointer to struct intel_uncore

 * @reg: register in question

 * @op: operation bitmask of FW_REG_READ and/or FW_REG_WRITE

 *

 * Returns a set of forcewake domains required to be taken with for example

 * intel_uncore_forcewake_get for the specified register to be accessible in the

 * specified mode (read, write or read/write) with raw mmio accessors.

 *

 * NOTE: On Gen6 and Gen7 write forcewake domain (FORCEWAKE_RENDER) requires the

 * callers to do FIFO management on their own or risk losing writes.

 SPDX-License-Identifier: MIT

/*

 * Copyright 2019 Intel Corporation.

 Map PCH device id to PCH type, or PCH_NONE if unknown. */

 PantherPoint is CPT compatible */

 WildcatPoint is LPT compatible */

 WildcatPoint is LPT compatible */

 KBP is SPT compatible */

 CometPoint is CNP Compatible */

 Comet Lake V PCH is based on KBP, which is SPT compatible */

	/*

	 * In a virtualized passthrough environment we can be in a

	 * setup where the ISA bridge is not able to be passed through.

	 * In this case, a south bridge can be emulated and we have to

	 * make an educated guess as to which PCH is really there.

 Sanity check virtual PCH id */

 DG1 has south engine display on the same PCI device */

	/*

	 * The reason to probe ISA bridge instead of Dev31:Fun0 is to

	 * make graphics device passthrough work easy for VMM, that only

	 * need to expose ISA bridge to let driver know the real hardware

	 * underneath. This is a requirement from virtualization team.

	 *

	 * In some virtualized environments (e.g. XEN), there is irrelevant

	 * ISA bridge in the system. To work reliably, we should scan trhough

	 * all the ISA bridge devices and check for the first match, instead

	 * of only checking the first one.

	/*

	 * Use PCH_NOP (PCH but no South Display) for PCH platforms without

	 * display.

/*

 * Copyright Â© 2008-2015 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Eric Anholt <eric@anholt.net>

 *

	/*

	 * As some machines use ACPI to handle runtime-resume callbacks, and

	 * ACPI is quite kmalloc happy, we cannot resume beneath the vm->mutex

	 * as they are required by the shrinker. Ergo, we wake the device up

	 * first just in case.

 Prevent vma being freed by i915_vma_parked as we unbind */

 flush the i915_vm_release() */

 We can use the cpu mem copy function because this is X86. */

 NOWARN */ |

		/* Operation in this page

		 *

		 * page_base = page offset within aperture

		 * page_offset = offset within page

		 * page_length = bytes to copy for this page

/**

 * Reads data from the object referenced by handle.

 * @dev: drm device pointer

 * @data: ioctl data blob

 * @file: drm file pointer

 *

 * On error, the contents of *data are undefined.

	/* PREAD is disallowed for all platforms after TGL-LP.  This also

	 * covers all platforms with local memory.

 Bounds check source.  */

/* This is the fast write path which cannot handle

 * page faults in the source data

 We can use the cpu mem copy function because this is X86. */

/**

 * This is the fast pwrite path, where we copy the data directly from the

 * user into the GTT, uncached.

 * @obj: i915 GEM object

 * @args: pwrite arguments structure

		/*

		 * Avoid waking the device up if we can fallback, as

		 * waking/resuming is very slow (worst-case 10-100 ms

		 * depending on PCI sleeps and our own resume time).

		 * This easily dwarfs any performance advantage from

		 * using the cache bypass of indirect GGTT access.

 No backing pages, no fallback, we must force GGTT access */

		/* Operation in this page

		 *

		 * page_base = page offset within aperture

		 * page_offset = offset within page

		 * page_length = bytes to copy for this page

 flush the write before we modify the GGTT */

 flush modifications to the GGTT (insert_page) */

		/* If we get a fault while copying data, then (presumably) our

		 * source page isn't available.  Return the error and we'll

		 * retry in the slow path.

		 * If the object is non-shmem backed, we retry again with the

		 * path that handles page fault.

/* Per-page copy function for the shmem pwrite fastpath.

 * Flushes invalid cachelines before writing to the target if

 * needs_clflush_before is set and flushes out any written cachelines after

 * writing if needs_clflush is set.

	/* If we don't overwrite a cacheline completely we need to be

	 * careful to have up-to-date data by first clflushing. Don't

	 * overcomplicate things and flush the entire patch.

/**

 * Writes data to the object referenced by handle.

 * @dev: drm device

 * @data: ioctl data blob

 * @file: drm file

 *

 * On error, the contents of the buffer that were to be modified are undefined.

	/* PWRITE is disallowed for all platforms after TGL-LP.  This also

	 * covers all platforms with local memory.

 Bounds check destination. */

 Writes not allowed into this read-only object */

	/* We can only do the GTT pwrite on untiled buffers, as otherwise

	 * it would end up going through the fenced access, and we'll get

	 * different detiling behavior between reading and writing.

	 * pread/pwrite currently are reading and writing from the CPU

	 * perspective, requiring manual detiling by the client.

		/* Note that the gtt paths might fail with non-page-backed user

		 * pointers (e.g. gtt mappings when moving data between

		 * textures). Fallback to the shmem path in that case.

/**

 * Called when user space has done writes to this buffer

 * @dev: drm device

 * @data: ioctl data blob

 * @file: drm file

	/*

	 * Proxy objects are barred from CPU access, so there is no

	 * need to ban sw_finish as it is a nop.

 Pinned buffers may be scanout, so flush the cache */

	/*

	 * Only called during RPM suspend. All users of the userfault_list

	 * must be holding an RPM wakeref to ensure that this can not

	 * run concurrently with themselves (and use the struct_mutex for

	 * protection between themselves).

	/*

	 * The fence will be lost when the device powers down. If any were

	 * in use by hardware (i.e. they are pinned), we should not be powering

	 * down! All other fences will be reacquired by the user upon waking.

		/*

		 * Ideally we want to assert that the fence register is not

		 * live at this point (i.e. that no piece of code will be

		 * trying to write through fence + GTT, as that both violates

		 * our tracking of activity and associated locking/barriers,

		 * but also is illegal given that the hw is powered down).

		 *

		 * Previously we used reg->pin_count as a "liveness" indicator.

		 * That is not sufficient, and we need a more fine-grained

		 * tool if we want to have a sanity check here.

		/*

		 * If the required space is larger than the available

		 * aperture, we will not able to find a slot for the

		 * object and unbinding the object now will be in

		 * vain. Worse, doing so may cause us to ping-pong

		 * the object in and out of the Global GTT and

		 * waste a lot of cycles under the mutex.

		/*

		 * If NONBLOCK is set the caller is optimistically

		 * trying to cache the full object within the mappable

		 * aperture, and *must* have a fallback in place for

		 * situations where we cannot bind the object. We

		 * can be a little more lax here and use the fallback

		 * more often to avoid costly migrations of ourselves

		 * and other objects within the aperture.

		 *

		 * Half-the-aperture is used as a simple heuristic.

		 * More interesting would to do search for a free

		 * block prior to making the commitment to unbind.

		 * That caters for the self-harm case, and with a

		 * little more heuristics (e.g. NOFAULT, NOEVICT)

		 * we could try to minimise harm to others.

 if the object is no longer attached, discard its backing storage */

 We need to fallback to 4K pages if host doesn't support huge gtt. */

	/*

	 * Despite its name intel_init_clock_gating applies both display

	 * clock gating workarounds; GT mmio workarounds and the occasional

	 * GT power context workaround. Worse, sometimes it includes a context

	 * register workaround which we need to apply before we record the

	 * default HW state for all contexts.

	 *

	 * FIXME: break up the workarounds and apply them at the right time!

	/*

	 * Unwinding is complicated by that we want to handle -EIO to mean

	 * disable GPU submission but keep KMS alive. We want to mark the

	 * HW as irrevisibly wedged, but keep enough state around that the

	 * driver doesn't explode during runtime.

		/*

		 * Allow engines or uC initialisation to fail by marking the GPU

		 * as wedged. But we only want to do this when the GPU is angry,

		 * for all other failure, such as an allocation failure, bail.

 Minimal basic recovery for KMS */

 Flush any outstanding unpin_work. */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2013-2021 Intel Corporation

	/*

	 * GEN6_PCODE_* are outside of the forcewake domain, we can use

	 * intel_uncore_read/write_fw variants to reduce the amount of work

	 * required when reading/writing.

/**

 * skl_pcode_request - send PCODE request until acknowledgment

 * @i915: device private

 * @mbox: PCODE mailbox ID the request is targeted for

 * @request: request ID

 * @reply_mask: mask used to check for request acknowledgment

 * @reply: value used to check for request acknowledgment

 * @timeout_base_ms: timeout for polling with preemption enabled

 *

 * Keep resending the @request to @mbox until PCODE acknowledges it, PCODE

 * reports an error or an overall timeout of @timeout_base_ms+50 ms expires.

 * The request is acknowledged once the PCODE reply dword equals @reply after

 * applying @reply_mask. Polling is first attempted with preemption enabled

 * for @timeout_base_ms and if this times out for another 50 ms with

 * preemption disabled.

 *

 * Returns 0 on success, %-ETIMEDOUT in case of a timeout, <0 in case of some

 * other error as reported by PCODE.

	/*

	 * Prime the PCODE by doing a request first. Normally it guarantees

	 * that a subsequent request, at most @timeout_base_ms later, succeeds.

	 * _wait_for() doesn't guarantee when its passed condition is evaluated

	 * first, so send the first request explicitly.

	/*

	 * The above can time out if the number of requests was low (2 in the

	 * worst case) _and_ PCODE was busy for some reason even after a

	 * (queued) request and @timeout_base_ms delay. As a workaround retry

	 * the poll with preemption disabled to maximize the number of

	 * requests. Increase the timeout from @timeout_base_ms to 50ms to

	 * account for interrupts that could reduce the number of these

	 * requests, and for any quirks of the PCODE firmware that delays

	 * the request completion.

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 Keep in gen based order, and chronological order within a gen */

 ilk does support rc6, but we do not implement [power] contexts */ \

 legal, last one wins */

 RC6p removed-by HSW */, \

	/* According to the device ID those devices are GT3, they were

	 * previously treated as not GT3, keep it like that.

 4 blocks for bypass path allocation */ \

 4 blocks for bypass path allocation */

 4 blocks for bypass path allocation */

 Wa_16011227922 */

/*

 * Make sure any device matches here are from most specific to most

 * general.  For example, since the Quanta match is based on the subsystem

 * and subvendor IDs, we need it to come before the more general IVB

 * PCI ID matches, otherwise we'll use the wrong info struct above.

 must be first IVB */

 driver load aborted, nothing to cleanup */

 is device_id present in comma separated list of ids */

 match everything */

	/* Only bind to function 0 of the device. Early generations

	 * used function 1 as a placeholder for multi-head. This causes

	 * us confusion instead, especially on the systems where both

	 * functions have the same PCI-ID!

	/*

	 * apple-gmux is needed on dual GPU MacBook Pro

	 * to probe the panel if we're the inactive GPU.

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2019 Intel Corporation

/*

 * Active refs memory management

 *

 * To be more economical with memory, we reap all the i915_active trees as

 * they idle (when we know the active requests are inactive) and allocate the

 * nodes from a local slab cache to hopefully reduce the fragmentation.

 before the first inc */

 after the last dec */

 return the unused nodes to our slabcache -- flushing the allocator */

 Even if we have not used the cache, we may still have a barrier */

 Keep the MRU cached node for reuse */

 Discard all other nodes in the tree */

 Rebuild the tree with only the cached node */

 Make the cached node available for reuse with any timeline */

 needs cmpxchg(u64) */

 After the final retire, the entire struct may be freed */

 ... except if you wait on it, you must manage your own references! */

 Finally free the discarded timeline tree  */

 0 is the unordered timeline, rsvd for cache */

	/*

	 * We track the most recently used timeline to skip a rbtree search

	 * for the common case, under typical loads we never need the rbtree

	 * at all. We can reuse the last slot if it is empty, that is

	 * after the previous activity has been retired, or if it matches the

	 * current timeline.

 Once claimed, this slot will only belong to this idx */

		/*

		 * An unclaimed cache [.timeline=0] can only be claimed once.

		 *

		 * If the value is already non-zero, some other thread has

		 * claimed the cache and we know that is does not match our

		 * idx. If, and only if, the timeline is currently zero is it

		 * worth competing to claim it atomically for ourselves (for

		 * only the winner of that race will cmpxchg return the old

		 * value of 0).

 While active, the tree can only be built; not destroyed */

 NB: If the tree rotated beneath us, we may miss our target. */

	/*

	 * XXX: We should preallocate this before i915_active_ref() is ever

	 *  called, but we cannot call into fs_reclaim() anyway, so use GFP_ATOMIC.

	/*

	 * Rebuild the llist excluding our node. We may perform this

	 * outside of the kernel_context timeline mutex and so someone

	 * else may be manipulating the engine->barrier_tasks, in

	 * which case either we or they will be upset :)

	 *

	 * A second __active_del_barrier() will report failure to claim

	 * the active_node and the caller will just shrug and know not to

	 * claim ownership of its node.

	 *

	 * A concurrent i915_request_add_active_barriers() will miss adding

	 * any of the tasks, but we will try again on the next -- and since

	 * we are actively using the barrier, we know that there will be

	 * at least another opportunity when we idle.

 proto-node used by our idle barrier? */

	/*

	 * This request is on the kernel_context timeline, and so

	 * we can use it to substitute for the pending idle-barrer

	 * request that we want to emit on the kernel_context.

 Prevent reaping in case we malloc/wait while building the tree */

 Contention with parallel tree builders! */

 slot must be preallocated */

 Only valid while active, see i915_active_acquire_for_context() */

 We expect the caller to manage the exclusive timeline ordering */

 __active_retire() */

 return with active ref */

 serialise with add_active_barriers */

 unconnected idle barrier? */

 Any fence added after the wait begins will not be auto-signaled */

	/*

	 * After the wait is complete, the caller may free the active.

	 * We have to flush any concurrent retirement before returning.

 XXX flush the barrier? */

	/*

	 * Try to reuse any existing barrier nodes already allocated for this

	 * i915_active, due to overlapping active phases there is likely a

	 * node kept alive (as we reuse before parking). We prefer to reuse

	 * completely idle barriers (less hassle in manipulating the llists),

	 * but otherwise any will do.

	/*

	 * No quick match, but we did find the leftmost rb_node for the

	 * kernel_context. Walk the rb_tree in-order to see if there were

	 * any idle-barriers on this timeline that we missed, or just use

	 * the first pending barrier.

		/*

		 * The list of pending barriers is protected by the

		 * kernel_context timeline, which notably we do not hold

		 * here. i915_request_add_active_barriers() may consume

		 * the barrier before we claim it, so we have to check

		 * for success.

 serialise with add_active_barriers */

 Hide from waits and sibling allocations */

 Wait until the previous preallocation is completed */

	/*

	 * Preallocate a node for each physical engine supporting the target

	 * engine (remember virtual engines have more than one sibling).

	 * We can then use the preallocated nodes in

	 * i915_active_acquire_barrier()

			/*

			 * Mark this as being *our* unconnected proto-node.

			 *

			 * Since this node is not in any list, and we have

			 * decoupled it from the rbtree, we can reuse the

			 * request to indicate this is an idle-barrier node

			 * and then we can use the rb_node and list pointers

			 * for our tracking of the pending barrier.

	/*

	 * Transfer the list of preallocated barriers into the

	 * i915_active rbtree, but only as proto-nodes. They will be

	 * populated by i915_request_add_active_barriers() to point to the

	 * request that will eventually release them.

	/*

	 * Attach the list of proto-fences to the in-flight request such

	 * that the parent i915_active will be released when this request

	 * is retired.

 serialise with reuse_idle_barrier */

/*

 * __i915_active_fence_set: Update the last active fence along its timeline

 * @active: the active tracker

 * @fence: the new fence (under construction)

 *

 * Records the new @fence as the last active fence along its timeline in

 * this active tracker, moving the tracking callbacks from the previous

 * fence onto this one. Returns the previous fence (if not already completed),

 * which the caller must ensure is executed before the new fence. To ensure

 * that the order of fences within the timeline of the i915_active_fence is

 * understood, it should be locked by the caller.

	/*

	 * Consider that we have two threads arriving (A and B), with

	 * C already resident as the active->fence.

	 *

	 * A does the xchg first, and so it sees C or NULL depending

	 * on the timing of the interrupt handler. If it is NULL, the

	 * previous fence must have been signaled and we know that

	 * we are first on the timeline. If it is still present,

	 * we acquire the lock on that fence and serialise with the interrupt

	 * handler, in the process removing it from any future interrupt

	 * callback. A will then wait on C before executing (if present).

	 *

	 * As B is second, it sees A as the previous fence and so waits for

	 * it to complete its transition and takes over the occupancy for

	 * itself -- remembering that it needs to wait on A before executing.

	 *

	 * Note the strong ordering of the timeline also provides consistent

	 * nesting rules for the fence->lock; the inner lock is always the

	 * older lock.

 serialise with prev->cb_list */

 Must maintain timeline ordering wrt previous active requests */

 but the previous fence may not belong to that timeline! */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2018 Intel Corporation

 most positive priority is scheduled first, equal priorities fifo */

 Convert an allocation failure to a priority bump */

 recurses just once */

			/* To maintain ordering with all rendering, after an

			 * allocation failure we have to disable all scheduling.

			 * Requests will then be executed in fifo, and schedule

			 * will ensure that dependencies are emitted in fifo.

			 * There will be still some reordering with existing

			 * requests, so if userspace lied about their

			 * dependencies that reordering may be visible.

	/*

	 * Virtual engines complicate acquiring the engine timeline lock,

	 * as their rq->engine pointer is not stable until under that

	 * engine lock. The simple ploy we use is to take the lock then

	 * check that the rq still belongs to the newly locked engine.

 Needed in order to use the temporary link inside i915_dependency */

	/*

	 * Recursively bump all dependent priorities to match the new request.

	 *

	 * A naive approach would be to use recursion:

	 * static void update_priorities(struct i915_sched_node *node, prio) {

	 *	list_for_each_entry(dep, &node->signalers_list, signal_link)

	 *		update_priorities(dep->signal, prio)

	 *	queue_request(node);

	 * }

	 * but that may have unlimited recursion depth and so runs a very

	 * real risk of overunning the kernel stack. Instead, we build

	 * a flat list of all dependencies starting with the current request.

	 * As we walk the list of dependencies, we add all of its dependencies

	 * to the end of the list (this may include an already visited

	 * request) and continue to walk onwards onto the new dependencies. The

	 * end result is a topological list of requests in reverse order, the

	 * last element in the list is the request we must execute first.

 If we are already flying, we know we have no signalers */

		/*

		 * Within an engine, there can be no cycle, but we may

		 * refer to the same dependency chain multiple times

		 * (redundant dependencies are not eliminated) and across

		 * engines.

 no cycles! */

	/*

	 * If we didn't need to bump any existing priorities, and we haven't

	 * yet submitted this request (i.e. there is no potential race with

	 * execlists_submit_request()), we can set our own priority and skip

	 * acquiring the engine locks.

 Fifo and depth-first replacement ensure our deps execute before us */

 Recheck after acquiring the engine->timeline.lock */

 Must be called before changing the nodes priority */

		/*

		 * Once the request is ready, it will be placed into the

		 * priority lists and then onto the HW runlist. Before the

		 * request is ready, it does not contribute to our preemption

		 * decisions and we can safely ignore it, as it will, and

		 * any preemption required, be dealt with upon submission.

		 * See engine->submit_request()

 Defer (tasklet) submission until after all of our updates. */

 All set, now publish. Beware the lockless walkers. */

 Propagate the chains */

	/*

	 * Everyone we depended upon (the fences we wait to be signaled)

	 * should retire before us and remove themselves from our list.

	 * However, retirement is run independently on each timeline and

	 * so we may be called out-of-order.

 Remove ourselves from everyone who depends upon us */

 Dependencies along the same timeline are expected. */

 flush the callback */

	/*

	 * Due to an interesting quirk in lockdep's internal debug tracking,

	 * after setting a subclass we must ensure the lock is used. Otherwise,

	 * nr_unused_locks is incremented once too often.

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 The aliasing_ppgtt should never be used directly! */

 Declare ourselves safe for use inside shrinkers */

 overflow */

		/*

		 * If the view already exists in the tree, another thread

		 * already created a matching vma, so return the older instance

		 * and dispose of ours.

		/*

		 * We put the GGTT vma at the start of the vma-list, followed

		 * by the ppGGTT vma. This allows us to break early when

		 * iterating over only the GGTT vma for an object, see

		 * for_each_ggtt_vma()

/**

 * i915_vma_instance - return the singleton instance of the VMA

 * @obj: parent &struct drm_i915_gem_object to be mapped

 * @vm: address space in which the mapping is located

 * @view: additional mapping requirements

 *

 * i915_vma_instance() looks up an existing VMA of the @obj in the @vm with

 * the same @view characteristics. If a match is not found, one is created.

 * Once created, the VMA is kept until either the object is freed, or the

 * address space is closed.

 *

 * Returns the vma, or an error pointer.

 vma_create() will resolve the race if another creates the vma */

 disable the worker by default */

/**

 * i915_vma_bind - Sets up PTEs for an VMA in it's corresponding address space.

 * @vma: VMA to map

 * @cache_level: mapping cache level

 * @flags: flags like global or local mapping

 * @work: preallocated worker for allocating and binding the PTE

 *

 * DMA addresses are taken from the scatter-gather table of this object (or of

 * this VMA in case of non-default GGTT views) and PTE entries set up.

 * Note that DMA addresses are also the only part of the SG table we care about.

		/*

		 * Note we only want to chain up to the migration fence on

		 * the pages (not the object itself). As we don't track that,

		 * yet, we have to use the exclusive fence instead.

		 *

		 * Also note that we do not want to track the async vma as

		 * part of the obj->resv->excl_fence as it only affects

		 * execution and not content or object's backing store lifetime.

 enable the queue_work() */

		/*

		 * TODO: consider just using i915_gem_object_pin_map() for lmem

		 * instead, which already supports mapping non-contiguous chunks

		 * of pages, that way we can also drop the

		 * I915_BO_ALLOC_CONTIGUOUS when allocating the object.

 NB Access through the GTT requires the device to be awake. */

	/*

	 * On some machines we have to be careful when putting differing types

	 * of snoopable memory together to avoid the prefetcher crossing memory

	 * domains and dying. During vm initialisation, we decide whether or not

	 * these constraints apply and set the drm_mm.color_adjust

	 * appropriately.

 Only valid to be called on an already inserted vma */

/**

 * i915_vma_insert - finds a slot for the vma in its address space

 * @vma: the vma

 * @size: requested size in bytes (can be larger than the VMA)

 * @alignment: required alignment

 * @flags: mask of PIN_* flags to use

 *

 * First we try to allocate some free space that meets the requirements for

 * the VMA. Failiing that, if the flags permit, it will evict an old VMA,

 * preferrably the oldest idle entry to make room for the new VMA.

 *

 * Returns:

 * 0 on success, negative error code otherwise.

	/* If binding the object/GGTT view requires more space than the entire

	 * aperture has, reject it early before evicting everything in a vain

	 * attempt to find space.

		/*

		 * We only support huge gtt pages through the 48b PPGTT,

		 * however we also don't want to force any alignment for

		 * objects which need to be tightly packed into the low 32bits.

		 *

		 * Note that we assume that GGTT are limited to 4GiB for the

		 * forseeable future. See also i915_ggtt_offset().

			/*

			 * We can't mix 64K and 4K PTEs in the same page-table

			 * (2M block), and so to avoid the ugliness and

			 * complexity of coloring we opt for just aligning 64K

			 * objects to 2M.

			/*

			 * Check we don't expand for the limited Global GTT

			 * (mappable aperture is even more precious!). This

			 * also checks that we exclude the aliasing-ppgtt.

	/*

	 * And finally now the object is completely decoupled from this

	 * vma, we can drop its hold on the backing storage and allow

	 * it to be reaped by the shrinker.

	/*

	 * If pin_count==0, but we are bound, check under the lock to avoid

	 * racing with a concurrent i915_vma_unbind().

 Allocations ahoy! */

 We allocate under vma_get_pages, so beware the shrinker */

 The upper portion of pages_count is the number of bindings */

 First try and grab the pin without rebinding the vma */

 lock VM */

 Allocate enough page directories to used PTE */

	/*

	 * Differentiate between user/kernel vma inside the aliasing-ppgtt.

	 *

	 * We conflate the Global GTT with the user's vma when using the

	 * aliasing-ppgtt, but it is still vitally important to try and

	 * keep the use cases distinct. For example, userptr objects are

	 * not allowed inside the Global GTT as that will cause lock

	 * inversions when we have to evict them the mmu_notifier callbacks -

	 * but they are allowed to be part of the user ppGTT which can never

	 * be mapped. As such we try to give the distinct users of the same

	 * mutex, distinct lockclasses [equivalent to how we keep i915_ggtt

	 * and i915_ppgtt separate].

	 *

	 * NB this may cause us to mask real lock inversions -- while the

	 * code is safe today, lockdep may not be able to spot future

	 * transgressions.

 No more allocations allowed now we hold vm->mutex */

 pins are meant to be fairly temporary */

 There should only be at most 2 active bindings (user, global) */

 Unlike i915_vma_pin, we don't take no for an answer! */

	/*

	 * We defer actually closing, unbinding and destroying the VMA until

	 * the next idle point, or if the object is freed in the meantime. By

	 * postponing the unbind, we allow for it to be resurrected by the

	 * client, avoiding the work required to rebind the VMA. This is

	 * advantageous for DRI, where the client/server pass objects

	 * between themselves, temporarily opening a local VMA to the

	 * object, and then closing it again. The same object is then reused

	 * on the next frame (or two, depending on the depth of the swap queue)

	 * causing us to rebind the VMA once more. This ends up being a lot

	 * of wasted work for the steady state.

 XXX All to avoid keeping a reference on i915_vma itself */

 As the GT is held idle, no vma can be reopened as we destroy them */

 Wait for the vma to be bound before we start! */

 Force a pagefault for domain tracking on next user access */

		/*

		 * Check that we have flushed all writes through the GGTT

		 * before the unbind, other due to non-strict nature of those

		 * indirect writes they may end up referencing the GGTT PTE

		 * after the unbind.

		 *

		 * Note that we may be concurrently poking at the GGTT_WRITE

		 * bit from set-domain, as we mark all GGTT vma associated

		 * with an object. We know this is for another vma, as we

		 * are currently unbinding this one -- so if this vma will be

		 * reused, it will be refaulted and have its dirty bit set

		 * before the next write.

 release the fence reg _after_ flushing */

	/*

	 * After confirming that no one else is pinning this vma, wait for

	 * any laggards who may have crept in during the wait (through

	 * a residual pin skipping the vm->mutex) to complete.

 pairs with i915_vma_release() */

 Optimistic wait before taking the mutex */

 XXX not always required: nop_clear_range */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2021 Intel Corporation

	/*

	 * Enable KMS by default, unless explicitly overriden by

	 * either the i915.modeset prarameter or by the

	 * vga_text_mode_force boot option.

 Silently fail loading to not upset userspace. */

			/*

			 * Early-exit success is reserved for things which

			 * don't have an exit() function because we have no

			 * idea how far they got or how to partially tear

			 * them down.

/*

 *

 * Copyright 2008 (c) Intel Corporation

 *   Jesse Barnes <jbarnes@virtuousgeek.org>

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the

 * "Software"), to deal in the Software without restriction, including

 * without limitation the rights to use, copy, modify, merge, publish,

 * distribute, sub license, and/or sell copies of the Software, and to

 * permit persons to whom the Software is furnished to do so, subject to

 * the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.

 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR

 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,

 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE

 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 Scratch space */

 Scratch space */

 Display arbitration control */

 Display arbitration */

 only restore FBC info on the platform that supports FBC*/

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2010 Daniel Vetter

 * Copyright Â© 2020 Intel Corporation

 fault-inject.h is not standalone! */

		/*

		 * If the DMA remap fails, one cause can be that we have

		 * too many objects pinned in a small remapping table,

		 * such as swiotlb. Incrementally purge all other objects and

		 * try again - if there are no more pages to remove from

		 * the DMA remapper, i915_gem_shrink will return 0.

 XXX This does not prevent more requests being submitted! */

 Wait a bit, in the hope it avoids the hang */

/**

 * i915_gem_gtt_reserve - reserve a node in an address_space (GTT)

 * @vm: the &struct i915_address_space

 * @node: the &struct drm_mm_node (typically i915_vma.mode)

 * @size: how much space to allocate inside the GTT,

 *        must be #I915_GTT_PAGE_SIZE aligned

 * @offset: where to insert inside the GTT,

 *          must be #I915_GTT_MIN_ALIGNMENT aligned, and the node

 *          (@offset + @size) must fit within the address space

 * @color: color to apply to node, if this node is not from a VMA,

 *         color must be #I915_COLOR_UNEVICTABLE

 * @flags: control search and eviction behaviour

 *

 * i915_gem_gtt_reserve() tries to insert the @node at the exact @offset inside

 * the address space (using @size and @color). If the @node does not fit, it

 * tries to evict any overlapping nodes from the GTT, including any

 * neighbouring nodes if the colors do not match (to ensure guard pages between

 * differing domains). See i915_gem_evict_for_node() for the gory details

 * on the eviction algorithm. #PIN_NONBLOCK may used to prevent waiting on

 * evicting active overlapping objects, and any overlapping node that is pinned

 * or marked as unevictable will also result in failure.

 *

 * Returns: 0 on success, -ENOSPC if no suitable hole is found, -EINTR if

 * asked to wait for eviction and interrupted.

/**

 * i915_gem_gtt_insert - insert a node into an address_space (GTT)

 * @vm: the &struct i915_address_space

 * @node: the &struct drm_mm_node (typically i915_vma.node)

 * @size: how much space to allocate inside the GTT,

 *        must be #I915_GTT_PAGE_SIZE aligned

 * @alignment: required alignment of starting offset, may be 0 but

 *             if specified, this must be a power-of-two and at least

 *             #I915_GTT_MIN_ALIGNMENT

 * @color: color to apply to node

 * @start: start of any range restriction inside GTT (0 for all),

 *         must be #I915_GTT_PAGE_SIZE aligned

 * @end: end of any range restriction inside GTT (U64_MAX for all),

 *       must be #I915_GTT_PAGE_SIZE aligned if not U64_MAX

 * @flags: control search and eviction behaviour

 *

 * i915_gem_gtt_insert() first searches for an available hole into which

 * is can insert the node. The hole address is aligned to @alignment and

 * its @size must then fit entirely within the [@start, @end] bounds. The

 * nodes on either side of the hole must match @color, or else a guard page

 * will be inserted between the two nodes (or the node evicted). If no

 * suitable hole is found, first a victim is randomly selected and tested

 * for eviction, otherwise then the LRU list of objects within the GTT

 * is scanned to find the first set of replacement nodes to create the hole.

 * Those old overlapping nodes are evicted from the GTT (and so must be

 * rebound before any future use). Any node that is currently pinned cannot

 * be evicted (see i915_vma_pin()). Similar if the node's VMA is currently

 * active and #PIN_NONBLOCK is specified, that node is also skipped when

 * searching for an eviction candidate. See i915_gem_evict_something() for

 * the gory details on the eviction algorithm.

 *

 * Returns: 0 on success, -ENOSPC if no suitable hole is found, -EINTR if

 * asked to wait for eviction and interrupted.

	/* We only allocate in PAGE_SIZE/GTT_PAGE_SIZE (4096) chunks,

	 * so we know that we always have a minimum alignment of 4096.

	 * The drm_mm range manager is optimised to return results

	 * with zero alignment, so where possible use the optimal

	 * path.

	/*

	 * No free space, pick a slot at random.

	 *

	 * There is a pathological case here using a GTT shared between

	 * mmap and GPU (i.e. ggtt/aliasing_ppgtt but not full-ppgtt):

	 *

	 *    |<-- 256 MiB aperture -->||<-- 1792 MiB unmappable -->|

	 *         (64k objects)             (448k objects)

	 *

	 * Now imagine that the eviction LRU is ordered top-down (just because

	 * pathology meets real life), and that we need to evict an object to

	 * make room inside the aperture. The eviction scan then has to walk

	 * the 448k list before it finds one within range. And now imagine that

	 * it has to search for a new hole between every byte inside the memcpy,

	 * for several simultaneous clients.

	 *

	 * On a full-ppgtt system, if we have run out of available space, there

	 * will be lots and lots of objects in the eviction list! Again,

	 * searching that LRU list may be slow if we are also applying any

	 * range restrictions (e.g. restriction to low 4GiB) and so, for

	 * simplicity and similarilty between different GTT, try the single

	 * random replacement first.

 Randomly selected placement is pinned, do a search */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020,2021 Intel Corporation

/*

 * Some platforms have unusual ways of mapping PCI revision ID to GT/display

 * steppings.  E.g., in some cases a higher PCI revision may translate to a

 * lower stepping of the GT and/or display IP.  This file provides lookup

 * tables to map the PCI revision into a standard set of stepping values that

 * can be compared numerically.

 *

 * Also note that some revisions/steppings may have been set aside as

 * placeholders but never materialized in real hardware; in those cases there

 * may be jumps in the revision IDs or stepping values in the tables below.

/*

 * Some platforms always have the same stepping value for GT and display;

 * use a macro to define these to make it easier to identify the platforms

 * where the two steppings can deviate.

 Same GT stepping between tgl_uy_revids and tgl_revids don't mean the same HW */

 Not using the stepping scheme for the platform yet. */

		/*

		 * If we hit a gap in the revid array, use the information for

		 * the next revid.

		 *

		 * This may be wrong in all sorts of ways, especially if the

		 * steppings in the array are not monotonically increasing, but

		 * it's better than defaulting to 0.

/*

 * Copyright(c) 2011-2015 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

/**

 * DOC: Intel GVT-g guest support

 *

 * Intel GVT-g is a graphics virtualization technology which shares the

 * GPU among multiple virtual machines on a time-sharing basis. Each

 * virtual machine is presented a virtual GPU (vGPU), which has equivalent

 * features as the underlying physical GPU (pGPU), so i915 driver can run

 * seamlessly in a virtual machine. This file provides vGPU specific

 * optimizations when running in a virtual machine, to reduce the complexity

 * of vGPU emulation and to improve the overall performance.

 *

 * A primary function introduced here is so-called "address space ballooning"

 * technique. Intel GVT-g partitions global graphics memory among multiple VMs,

 * so each VM can directly access a portion of the memory without hypervisor's

 * intervention, e.g. filling textures or queuing commands. However with the

 * partitioning an unmodified i915 driver would assume a smaller graphics

 * memory starting from address ZERO, then requires vGPU emulation module to

 * translate the graphics address between 'guest view' and 'host view', for

 * all registers and command opcodes which contain a graphics memory address.

 * To reduce the complexity, Intel GVT-g introduces "address space ballooning",

 * by telling the exact partitioning knowledge to each guest i915 driver, which

 * then reserves and prevents non-allocated portions from allocation. Thus vGPU

 * emulation module only needs to scan and validate graphics addresses without

 * complexity of address translation.

 *

/**

 * intel_vgpu_detect - detect virtual GPU

 * @dev_priv: i915 device private

 *

 * This function is called at the initialization stage, to detect whether

 * running on a vGPU.

	/*

	 * This is called before we setup the main MMIO BAR mappings used via

	 * the uncore structure, so we need to access the BAR directly. Since

	 * we do not support VGT on older gens, return early so we don't have

	 * to consider differently numbered or sized MMIO bars

	/*

	 * Notify a valid surface after modesetting, when running inside a VM.

	/*

	 * There are up to 2 regions per mappable/unmappable graphic

	 * memory that might be ballooned. Here, index 0/1 is for mappable

	 * graphic memory, 2/3 for unmappable graphic memory.

/**

 * intel_vgt_deballoon - deballoon reserved graphics address trunks

 * @ggtt: the global GGTT from which we reserved earlier

 *

 * This function is called to deallocate the ballooned-out graphic memory, when

 * driver is unloaded or when ballooning fails.

/**

 * intel_vgt_balloon - balloon out reserved graphics address trunks

 * @ggtt: the global GGTT from which to reserve

 *

 * This function is called at the initialization stage, to balloon out the

 * graphic address space allocated to other vGPUs, by marking these spaces as

 * reserved. The ballooning related knowledge(starting address and size of

 * the mappable/unmappable graphic memory) is described in the vgt_if structure

 * in a reserved mmio range.

 *

 * To give an example, the drawing below depicts one typical scenario after

 * ballooning. Here the vGPU1 has 2 pieces of graphic address spaces ballooned

 * out each for the mappable and the non-mappable part. From the vGPU1 point of

 * view, the total size is the same as the physical one, with the start address

 * of its graphic space being zero. Yet there are some portions ballooned out(

 * the shadow part, which are marked as reserved by drm allocator). From the

 * host point of view, the graphic address space is partitioned by multiple

 * vGPUs in different VMs. ::

 *

 *                         vGPU1 view         Host view

 *              0 ------> +-----------+     +-----------+

 *                ^       |###########|     |   vGPU3   |

 *                |       |###########|     +-----------+

 *                |       |###########|     |   vGPU2   |

 *                |       +-----------+     +-----------+

 *         mappable GM    | available | ==> |   vGPU1   |

 *                |       +-----------+     +-----------+

 *                |       |###########|     |           |

 *                v       |###########|     |   Host    |

 *                +=======+===========+     +===========+

 *                ^       |###########|     |   vGPU3   |

 *                |       |###########|     +-----------+

 *                |       |###########|     |   vGPU2   |

 *                |       +-----------+     +-----------+

 *       unmappable GM    | available | ==> |   vGPU1   |

 *                |       +-----------+     +-----------+

 *                |       |###########|     |           |

 *                |       |###########|     |   Host    |

 *                v       |###########|     |           |

 *  total GM size ------> +-----------+     +-----------+

 *

 * Returns:

 * zero on success, non-zero if configuration invalid or ballooning failed

 Unmappable graphic memory ballooning */

 Mappable graphic memory ballooning */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 Returns total Gb for the whole DIMM */

 Returns total Gb for the whole DIMM */

 Convert total Gb to Gb per DRAM device */

 Returns Gb per DRAM device */

	/*

	 * Size in register is Gb per DRAM device. Convert to total

	 * Gb to match the way we report this for non-LP platforms.

	/*

	 * Now read each DUNIT8/9/10/11 to check the rank of each dimms.

	/*

	 * Assume level 0 watermark latency adjustment is needed until proven

	 * otherwise, this w/a is not needed by bxt/glk.

 NB: We can't write IDICR yet because we don't have gt funcs set up */

	/*

	 * The needed capability bits for size calculation are not there with

	 * pre gen9 so return 128MB always.

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2016 Intel Corporation

 Should walk exactly nents and hit the end */

/**

 * i915_sg_from_mm_node - Create an sg_table from a struct drm_mm_node

 * @node: The drm_mm_node.

 * @region_start: An offset to add to the dma addresses of the sg list.

 *

 * Create a struct sg_table, initializing it from a struct drm_mm_node,

 * taking a maximum segment length into account, splitting into segments

 * if necessary.

 *

 * Return: A pointer to a kmalloced struct sg_table on success, negative

 * error code cast to an error pointer on failure.

 Do we have a limit on this? */

/**

 * i915_sg_from_buddy_resource - Create an sg_table from a struct

 * i915_buddy_block list

 * @res: The struct i915_ttm_buddy_resource.

 * @region_start: An offset to add to the dma addresses of the sg list.

 *

 * Create a struct sg_table, initializing it from struct i915_buddy_block list,

 * taking a maximum segment length into account, splitting into segments

 * if necessary.

 *

 * Return: A pointer to a kmalloced struct sg_table on success, negative

 * error code cast to an error pointer on failure.

/*

 * Copyright (c) 2008 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Eric Anholt <eric@anholt.net>

 *    Keith Packard <keithp@keithp.com>

 *    Mika Kuoppala <mika.kuoppala@intel.com>

 *

 single threaded page allocator with a reserved stash for emergencies */

 Fallback to uncompressed if we increase size? */

 more space requested */

 any error */

			/*

			 * SFC_DONE resides in the VD forcewake domain, so it

			 * only exists if the corresponding VCS engine is

			 * present.

 worstcase zlib growth */

 XXX: gen8 returns to sanity */

 reference held while active */

	/*

	 * We need to copy these to an anonymous buffer

	 * as the simplest method to avoid being overwritten

	 * by userspace.

		/*

		 * Getting here with GuC enabled means it is a forced error capture

		 * with no actual hang. So, no need to attempt the execlist search.

 Refill our page pool before entering atomic section */

	/* Non-default firmware paths will be specified by the modparam.

	 * As modparams are generally accesible from the userspace make

	 * explicit copies of the firmware paths.

 Capture all registers which don't fit into another category. */

	/*

	 * General organization

	 * 1. Registers specific to a single generation

	 * 2. Registers which belong to multiple generations

	 * 3. Feature specific registers.

	 * 4. Everything else

	 * Please try to follow the order.

 1: Registers specific to a single generation */

 2: Registers which belong to multiple generations */

 3: Feature specific registers */

			/*

			 * SFC_DONE resides in the VD forcewake domain, so it

			 * only exists if the corresponding VCS engine is

			 * present.

 4: Everything else */

/*

 * Generate a semi-unique error code. The code is not meant to have meaning, The

 * code's only purpose is to try to prevent false duplicated bug reports by

 * grossly estimating a GPU error state.

 *

 * TODO Ideally, hashing the batchbuffer would be a very nice way to determine

 * the hang if we could strip the GTT offset information from it.

 *

 * It's only a small step better than a random number in its current form.

	/*

	 * IPEHR would be an ideal way to detect errors, as it's the gross

	 * measure of "the command that hung." However, has some very common

	 * synchronization commands which almost always appear in the case

	 * strictly a client bug. Use instdone to differentiate those some.

 Just show the first executing process, more is confusing */

 Check if GPU capture has been disabled */

gitlab.freedesktop.org/drm/intel/issues/new.\n");

gitlab.freedesktop.org/drm/intel/-/wikis/How-to-file-i915-bugs for details.\n");

/**

 * i915_capture_error_state - capture an error record for later analysis

 * @gt: intel_gt which originated the hang

 * @engine_mask: hung engines

 *

 *

 * Should be called when an error is detected (either a hang or an error

 * interrupt) to capture error state from the time of the error.  Fills

 * out a structure which becomes available in debugfs for user level tools

 * to pick up.

 if disabled, always disabled */

/*

 * Copyright Â© 2012 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Ben Widawsky <ben@bwidawsk.net>

 *

 NB: We defer the remapping until we switch to the context */

	/*

	 * TODO: Ideally we really want a GPU reset here to make sure errors

	 * aren't propagated. Since I cannot find a stable way to reset the GPU

	 * at this point it is left as a TODO.

 Validate against (static) hardware limits */

 For now we have a static number of RP states */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

	/*

	 * Split into power-of-two blocks, in case we are given a size that is

	 * not itself a power-of-two.

/*

 * Allocate power-of-two block. The order value here translates to:

 *

 *   0 = 2^0 * mm->chunk_size

 *   1 = 2^1 * mm->chunk_size

 *   2 = 2^2 * mm->chunk_size

 *   ...

 Go low */

/*

 * Allocate range. Note that it's safe to chain together multiple alloc_ranges

 * with the same blocks list.

 *

 * Intended for pre-allocating portions of the address space, for example to

 * reserve a block for the initial framebuffer or similar, hence the expectation

 * here is that i915_buddy_alloc() is still the main vehicle for

 * allocations, so if that's not the case then the drm_mm range allocator is

 * probably a much better fit, and so you should probably go use that instead.

	/*

	 * We really don't want to leave around a bunch of split blocks, since

	 * bigger is better, so make sure we merge everything back before we

	 * free the allocated blocks.

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 Special PTE are not associated with any struct page */

 Special PTE are not associated with any struct page */

 track insertions in case we need to unwind later */

/**

 * remap_io_mapping - remap an IO mapping to userspace

 * @vma: user vma to map to

 * @addr: target user address to start at

 * @pfn: physical address of kernel memory

 * @size: size of map area

 * @iomap: the source io_mapping

 *

 *  Note: this is only safe if the mm semaphore is held when called.

 We rely on prevalidation of the io-mapping to skip track_pfn(). */

/**

 * remap_io_sg - remap an IO mapping to userspace

 * @vma: user vma to map to

 * @addr: target user address to start at

 * @size: size of map area

 * @sgl: Start sg entry

 * @iobase: Use stored dma address offset by this address or pfn if -1

 *

 *  Note: this is only safe if the mm semaphore is held when called.

 We rely on prevalidation of the io-mapping to skip track_pfn(). */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

		/*

		 * Ask the user to file a bug report for the error, except

		 * if they may have caused the bug by fiddling with unsafe

		 * module parameters.

 Failures that occur during fault injection testing are expected */

	/*

	 * Paranoia to make sure the compiler computes the timeout before

	 * loading 'jiffies' as jiffies is volatile and may be updated in

	 * the background by a timer tick. All to reduce the complexity

	 * of the addition and reduce the risk of losing a jiffie.

 Keep t->expires = 0 reserved to indicate a canceled timer. */

/*

 * Copyright Â© 2008-2010 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Eric Anholt <eric@anholt.net>

 *    Chris Wilson <chris@chris-wilson.co.uuk>

 *

	/*

	 * Not everything in the GGTT is tracked via vma (otherwise we

	 * could evict as required with minimal stalling) so we are forced

	 * to idle the GPU and explicitly retire outstanding requests in

	 * the hopes that we can then remove contexts and the like only

	 * bound by their active reference.

/**

 * i915_gem_evict_something - Evict vmas to make room for binding a new one

 * @vm: address space to evict from

 * @min_size: size of the desired free space

 * @alignment: alignment constraint of the desired free space

 * @color: color for the desired space

 * @start: start (inclusive) of the range from which to evict objects

 * @end: end (exclusive) of the range from which to evict objects

 * @flags: additional flags to control the eviction algorithm

 *

 * This function will try to evict vmas until a free space satisfying the

 * requirements is found. Callers must check first whether any such hole exists

 * already before calling this function.

 *

 * This function is used by the object/vma binding code.

 *

 * Since this function is only used to free up virtual address space it only

 * ignores pinned vmas, and not object where the backing storage itself is

 * pinned. Hence obj->pages_pin_count does not protect against eviction.

 *

 * To clarify: This is for freeing up virtual address space, not for freeing

 * memory in e.g. the shrinker.

	/*

	 * The goal is to evict objects and amalgamate space in rough LRU order.

	 * Since both active and inactive objects reside on the same list,

	 * in a mix of creation and last scanned order, as we process the list

	 * we sort it into inactive/active, which keeps the active portion

	 * in a rough MRU order.

	 *

	 * The retirement sequence is thus:

	 *   1. Inactive objects (already retired, random order)

	 *   2. Active objects (will stall on unbinding, oldest scanned first)

 now seen this vma twice */

		/*

		 * We keep this list in a rough least-recently scanned order

		 * of active elements (inactive elements are cheap to reap).

		 * New entries are added to the end, and we move anything we

		 * scan to the end. The assumption is that the working set

		 * of applications is either steady state (and thanks to the

		 * userspace bo cache it almost always is) or volatile and

		 * frequently replaced after a frame, which are self-evicting!

		 * Given that assumption, the MRU order of the scan list is

		 * fairly static, and keeping it in least-recently scan order

		 * is suitable.

		 *

		 * To notice when we complete one full cycle, we record the

		 * first active element seen, before moving it to the tail.

 Nothing found, clean up and bail out! */

	/*

	 * Can we unpin some objects such as idle hw contents,

	 * or pending flips? But since only the GGTT has global entries

	 * such as scanouts, rinbuffers and contexts, we can skip the

	 * purge when inspecting per-process local address spaces.

	/*

	 * Not everything in the GGTT is tracked via VMA using

	 * i915_vma_move_to_active(), otherwise we could evict as required

	 * with minimal stalling. Instead we are forced to idle the GPU and

	 * explicitly retire outstanding requests which will then remove

	 * the pinning for active objects such as contexts and ring,

	 * enabling us to evict them on the next iteration.

	 *

	 * To ensure that all user contexts are evictable, we perform

	 * a switch to the perma-pinned kernel context. This all also gives

	 * us a termination condition, when the last retired context is

	 * the kernel's there is no more we can evict.

	/* drm_mm doesn't allow any other other operations while

	 * scanning, therefore store to-be-evicted objects on a

	 * temporary list and take a reference for all before

	 * calling unbind (which may remove the active reference

	 * of any of our objects, thus corrupting the list).

 Unbinding will emit any required flushes */

 If we find any non-objects (!vma), we cannot evict them */

 XXX search failed, try again? */

/**

 * i915_gem_evict_for_node - Evict vmas to make room for binding a new one

 * @vm: address space to evict from

 * @target: range (and color) to evict for

 * @flags: additional flags to control the eviction algorithm

 *

 * This function will try to evict vmas that overlap the target node.

 *

 * To clarify: This is for freeing up virtual address space, not for freeing

 * memory in e.g. the shrinker.

	/*

	 * Retire before we search the active list. Although we have

	 * reasonable accuracy in our retirement lists, we may have

	 * a stray pin (preventing eviction) that can only be resolved by

	 * retiring.

 Expand search to cover neighbouring guard pages (or lack!) */

 Always look at the page afterwards to avoid the end-of-GTT */

 If we find any non-objects (!vma), we cannot evict them */

		/*

		 * If we are using coloring to insert guard pages between

		 * different cache domains within the address space, we have

		 * to check whether the objects on either side of our range

		 * abutt and conflict. If they are in conflict, then we evict

		 * those as well to make room for our guard pages.

		/*

		 * Never show fear in the face of dragons!

		 *

		 * We cannot directly remove this node from within this

		 * iterator and as with i915_gem_evict_something() we employ

		 * the vma pin_count in order to prevent the action of

		 * unbinding one vma from freeing (by dropping its active

		 * reference) another in our eviction list.

/**

 * i915_gem_evict_vm - Evict all idle vmas from a vm

 * @vm: Address space to cleanse

 *

 * This function evicts all vmas from a vm.

 *

 * This is used by the execbuf code as a last-ditch effort to defragment the

 * address space.

 *

 * To clarify: This is for freeing up virtual address space, not for freeing

 * memory in e.g. the shrinker.

	/* Switch back to the default context in order to unpin

	 * the existing context objects. However, such objects only

	 * pin themselves inside the global GTT and performing the

	 * switch otherwise is ineffective.

 "Get me out of here!" */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2018 Intel Corporation

	/*

	 * We'll just put the number of registers, and won't copy the

	 * register.

 reserved for test_config */

 Only write the length back to userspace if they differ. */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

/**

 * i915_ttm_buddy_man_init - Setup buddy allocator based ttm manager

 * @bdev: The ttm device

 * @type: Memory type we want to manage

 * @use_tt: Set use_tt for the manager

 * @size: The size in bytes to manage

 * @default_page_size: The default minimum page size in bytes for allocations,

 * this must be at least as large as @chunk_size, and can be overridden by

 * setting the BO page_alignment, to be larger or smaller as needed.

 * @chunk_size: The minimum page size in bytes for our allocations i.e

 * order-zero

 *

 * Note that the starting address is assumed to be zero here, since this

 * simplifies keeping the property where allocated blocks having natural

 * power-of-two alignment. So long as the real starting address is some large

 * power-of-two, or naturally start from zero, then this should be fine.  Also

 * the &i915_ttm_buddy_man_reserve interface can be used to preserve alignment

 * if say there is some unusable range from the start of the region. We can

 * revisit this in the future and make the interface accept an actual starting

 * offset and let it take care of the rest.

 *

 * Note that if the @size is not aligned to the @chunk_size then we perform the

 * required rounding to get the usable size. The final size in pages can be

 * taken from &ttm_resource_manager.size.

 *

 * Return: 0 on success, negative error code on failure.

/**

 * i915_ttm_buddy_man_fini - Destroy the buddy allocator ttm manager

 * @bdev: The ttm device

 * @type: Memory type we want to manage

 *

 * Note that if we reserved anything with &i915_ttm_buddy_man_reserve, this will

 * also be freed for us here.

 *

 * Return: 0 on success, negative error code on failure.

/**

 * i915_ttm_buddy_man_reserve - Reserve address range

 * @man: The buddy allocator ttm manager

 * @start: The offset in bytes, where the region start is assumed to be zero

 * @size: The size in bytes

 *

 * Note that the starting address for the region is always assumed to be zero.

 *

 * Return: 0 on success, negative error code on failure.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 i915 resume handler doesn't set to D0 */

	/*

	 * FIXME: open_count is protected by drm_global_mutex but that would lead to

	 * locking inversion with the driver load path. And the access here is

	 * completely racy anyway. So don't bother with locking for now.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

/*

 * Copyright Â© 2012 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Eugeni Dodonov <eugeni.dodonov@intel.com>

 *

 Stores plane specific WM parameters */

 used in computing the new watermarks state */

		/*

		 * WaCompressedResourceDisplayNewHashMode:skl,kbl

		 * Display WA #0390: skl,kbl

		 *

		 * Must match Sampler, Pixel Back End, and Media. See

		 * WaCompressedResourceSamplerPbeMediaNewHashMode.

		/*

		 * "Plane N strech max must be programmed to 11b (x1)

		 *  when Async flips are enabled on that plane."

 See Bspec note for PSR2_CTL bit 31, Wa#828:skl,bxt,kbl,cfl */

 WaEnableChickenDCPR:skl,bxt,kbl,glk,cfl */

	/*

	 * WaFbcWakeMemOn:skl,bxt,kbl,glk,cfl

	 * Display WA #0859: skl,bxt,kbl,glk,cfl

 WaDisableSDEUnitClockGating:bxt */

	/*

	 * FIXME:

	 * GEN8_HDCUNIT_CLOCK_GATE_DISABLE_HDCREQ applies on 3x6 GT SKUs only.

	/*

	 * Wa: Backlight PWM may stop in the asserted state, causing backlight

	 * to stay fully on.

	/*

	 * Lower the display internal timeout.

	 * This is needed to avoid any hard hangs when DSI port PLL

	 * is off and a MMIO access is attempted by any privilege

	 * application, using batch buffers or any other means.

	/*

	 * WaFbcTurnOffFbcWatermark:bxt

	 * Display WA #0562: bxt

	/*

	 * WaFbcHighMemBwCorruptionAvoidance:bxt

	 * Display WA #0883: bxt

	/*

	 * WaDisablePWMClockGating:glk

	 * Backlight PWM may stop in the asserted state, causing backlight

	 * to stay fully on.

 133*4 */

 200*4 */

 167*4 */

 100*4 */

 detect pineview DDR3 setting */

 DDR2-400 SC */

 DDR2-667 SC */

 DDR2-800 SC */

 DDR3-667 SC */

 DDR3-800 SC */

 DDR2-400 SC */

 DDR2-667 SC */

 DDR2-800 SC */

 DDR3-667 SC */

 DDR3-800 SC */

 DDR2-400 SC */

 DDR2-667 SC */

 DDR2-800 SC */

 DDR3-667 SC */

 DDR3-800 SC */

 DDR2-400 SC */

 DDR2-667 SC */

 DDR2-800 SC */

 DDR3-667 SC */

 DDR3-800 SC */

 DDR2-400 SC */

 DDR2-667 SC */

 DDR2-800 SC */

 DDR3-667 SC */

 DDR3-800 SC */

 DDR2-400 SC */

 DDR2-667 SC */

 DDR2-800 SC */

 DDR3-667 SC */

 DDR3-800 SC */

		/*

		 * FIXME can't find a bit like this for 915G, and

		 * and yet it does have the related watermark in

		 * FW_BLC_SELF. What's going on?

/**

 * intel_set_memory_cxsr - Configure CxSR state

 * @dev_priv: i915 device

 * @enable: Allow vs. disallow CxSR

 *

 * Allow or disallow the system to enter a special CxSR

 * (C-state self refresh) state. What typically happens in CxSR mode

 * is that several display FIFOs may get combined into a single larger

 * FIFO for a particular plane (so called max FIFO mode) to allow the

 * system to defer memory fetches longer, and the memory will enter

 * self refresh.

 *

 * Note that enabling CxSR does not guarantee that the system enter

 * this special mode, nor does it guarantee that the system stays

 * in that mode once entered. So this just allows/disallows the system

 * to autonomously utilize the CxSR mode. Other factors such as core

 * C-states will affect when/if the system actually enters/exits the

 * CxSR mode.

 *

 * Note that on VLV/CHV this actually only controls the max FIFO mode,

 * and the system is free to enter/exit memory self refresh at any time

 * even when the use of CxSR has been disallowed.

 *

 * While the system is actually in the CxSR/max FIFO mode, some plane

 * control registers will not get latched on vblank. Thus in order to

 * guarantee the system will respond to changes in the plane registers

 * we must always disallow CxSR prior to making changes to those registers.

 * Unfortunately the system will re-evaluate the CxSR conditions at

 * frame start which happens after vblank start (which is when the plane

 * registers would get latched), so we can't proceed with the plane update

 * during the same frame where we disallowed CxSR.

 *

 * Certain platforms also have a deeper HPLL SR mode. Fortunately the

 * HPLL SR mode depends on CxSR itself, so we don't have to hand hold

 * the hardware w.r.t. HPLL SR when writing to plane registers.

 * Disallowing just CxSR is sufficient.

/*

 * Latency for FIFO fetches is dependent on several factors:

 *   - memory configuration (speed, channels)

 *   - chipset

 *   - current MCH state

 * It can be fairly high in some situations, so here we assume a fairly

 * pessimal value.  It's a tradeoff between extra memory fetches (if we

 * set this value too high, the FIFO will fetch frequently to stay full)

 * and power consumption (set it too low to save power and we might see

 * FIFO underruns and display "flicker").

 *

 * A value of 5us seems to be a good balance; safe for very low end

 * platforms but not overly aggressive on lower latency configs.

 Convert to cachelines */

 Convert to cachelines */

 Pineview has different values for various configs */

/**

 * intel_wm_method1 - Method 1 / "small buffer" watermark formula

 * @pixel_rate: Pipe pixel rate in kHz

 * @cpp: Plane bytes per pixel

 * @latency: Memory wakeup latency in 0.1us units

 *

 * Compute the watermark using the method 1 or "small buffer"

 * formula. The caller may additonally add extra cachelines

 * to account for TLB misses and clock crossings.

 *

 * This method is concerned with the short term drain rate

 * of the FIFO, ie. it does not account for blanking periods

 * which would effectively reduce the average drain rate across

 * a longer period. The name "small" refers to the fact the

 * FIFO is relatively small compared to the amount of data

 * fetched.

 *

 * The FIFO level vs. time graph might look something like:

 *

 *   |\   |\

 *   | \  | \

 * __---__---__ (- plane active, _ blanking)

 * -> time

 *

 * or perhaps like this:

 *

 *   |\|\  |\|\

 * __----__----__ (- plane active, _ blanking)

 * -> time

 *

 * Returns:

 * The watermark in bytes

/**

 * intel_wm_method2 - Method 2 / "large buffer" watermark formula

 * @pixel_rate: Pipe pixel rate in kHz

 * @htotal: Pipe horizontal total

 * @width: Plane width in pixels

 * @cpp: Plane bytes per pixel

 * @latency: Memory wakeup latency in 0.1us units

 *

 * Compute the watermark using the method 2 or "large buffer"

 * formula. The caller may additonally add extra cachelines

 * to account for TLB misses and clock crossings.

 *

 * This method is concerned with the long term drain rate

 * of the FIFO, ie. it does account for blanking periods

 * which effectively reduce the average drain rate across

 * a longer period. The name "large" refers to the fact the

 * FIFO is relatively large compared to the amount of data

 * fetched.

 *

 * The FIFO level vs. time graph might look something like:

 *

 *    |\___       |\___

 *    |    \___   |    \___

 *    |        \  |        \

 * __ --__--__--__--__--__--__ (- plane active, _ blanking)

 * -> time

 *

 * Returns:

 * The watermark in bytes

	/*

	 * FIXME remove once all users are computing

	 * watermarks in the correct place.

/**

 * intel_calculate_wm - calculate watermark level

 * @pixel_rate: pixel clock

 * @wm: chip FIFO params

 * @fifo_size: size of the FIFO buffer

 * @cpp: bytes per pixel

 * @latency_ns: memory latency for the platform

 *

 * Calculate the watermark level (the level at which the display plane will

 * start fetching from memory again).  Each chip has a different display

 * FIFO size and allocation, so the caller needs to figure that out and pass

 * in the correct intel_watermark_params structure.

 *

 * As the pixel clock runs, the FIFO will be drained at a rate that depends

 * on the pixel size.  When it reaches the watermark level, it'll start

 * fetching FIFO line sized based chunks from memory until the FIFO fills

 * past the watermark point.  If the FIFO drains completely, a FIFO underrun

 * will occur, and a display engine hang could result.

	/*

	 * Note: we need to make sure we don't overflow for various clock &

	 * latency values.

	 * clocks go from a few thousand to several hundred thousand.

	 * latency is usually a few thousand

 Don't promote wm_size to unsigned... */

	/*

	 * Bspec seems to indicate that the value shouldn't be lower than

	 * 'burst size + 1'. Certainly 830 is quite unhappy with low values.

	 * Lets go for 8 which is the burst size since certain platforms

	 * already use a hardcoded 8 (which is what the spec says should be

	 * done).

 FIXME check the 'enable' instead */

	/*

	 * Treat cursor with fb as always visible since cursor updates

	 * can happen faster than the vrefresh rate, and the current

	 * watermark code doesn't handle that correctly. Cursor updates

	 * which set/clear the fb or change the cursor size are going

	 * to get throttled by intel_legacy_cursor_update() to work

	 * around this problem with the watermark code.

	/* Be paranoid as we can arrive here with only partial

	 * state retrieved from the hardware during setup.

	 *

	 * We can ditch the adjusted_mode.crtc_clock check as soon

	 * as Haswell has gained clock readout/fastboot support.

	 *

	 * We can ditch the crtc->primary->state->fb check as soon as we can

	 * properly reconstruct framebuffers.

	 *

	 * FIXME: The intel_crtc->active here should be switched to

	 * crtc->state->active once we have proper CRTC states wired up

	 * for atomic.

 Display SR */

 cursor SR */

 Display HPLL off SR */

 cursor HPLL off SR */

/*

 * Documentation says:

 * "If the line size is small, the TLB fetches can get in the way of the

 *  data fetches, causing some lag in the pixel data return which is not

 *  accounted for in the above formulas. The following adjustment only

 *  needs to be applied if eight whole lines fit in the buffer at once.

 *  The WM is adjusted upwards by the difference between the FIFO size

 *  and the size of 8 whole lines. This adjustment is always performed

 *  in the actual pixel depth regardless of whether FBC is enabled or not."

	/*

	 * Zero the (unused) WM1 watermarks, and also clear all the

	 * high order bits so that there are no out of bounds values

	 * present in the registers during the reprogramming.

 all latencies in usec */

	/*

	 * DSPCNTR[13] supposedly controls whether the

	 * primary plane can use the FIFO space otherwise

	 * reserved for the sprite plane. It's not 100% clear

	 * what the actual FIFO size is, but it looks like we

	 * can happily set both primary and sprite watermarks

	 * up to 127 cachelines. So that would seem to mean

	 * that either DSPCNTR[13] doesn't do anything, or that

	 * the total FIFO is >= 256 cachelines in size. Either

	 * way, we don't seem to have to worry about this

	 * repartitioning as the maximum watermark value the

	 * register can hold for each plane is lower than the

	 * minimum FIFO size.

	/*

	 * WaUse32BppForSRWM:ctg,elk

	 *

	 * The spec fails to list this restriction for the

	 * HPLL watermark, which seems a little strange.

	 * Let's use 32bpp for the HPLL watermark as well.

 NORMAL level doesn't have an FBC watermark */

		/*

		 * FBC wm is not mandatory as we

		 * can always just disable its use.

 mark watermarks as invalid */

 mark all levels starting from 'level' as invalid */

 invalidate the higher levels */

	/*

	 * Determine if the FBC watermark(s) can be used. IF

	 * this isn't the case we prefer to disable the FBC

	 * watermark(s) rather than disable the SR/HPLL

	 * level(s) entirely. 'level-1' is the highest valid

	 * level here.

	/*

	 * If our intermediate WM are identical to the final WM, then we can

	 * omit the post-vblank programming; only update if it's different.

 latency must be in 0.1us units. */

 all latencies in usec */

		/*

		 * FIXME the formula gives values that are

		 * too big for the cursor FIFO, and hence we

		 * would never be able to use cursors. For

		 * now just hardcode the watermark.

	/*

	 * When enabling sprite0 after sprite1 has already been enabled

	 * we tend to get an underrun unless sprite0 already has some

	 * FIFO space allcoated. Hence we always allocate at least one

	 * cacheline for sprite0 whenever sprite1 is enabled.

	 *

	 * All other plane enable sequences appear immune to this problem.

 spread the remainder evenly */

 give it all to the first plane if none are active */

 mark all levels starting from 'level' as invalid */

/*

 * Starting from 'level' set all higher

 * levels to 'value' in the "raw" watermarks.

 mark all higher levels as invalid */

	/*

	 * DSPARB registers may have been reset due to the

	 * power well being turned off. Make sure we restore

	 * them to a consistent state even if no primary/sprite

	 * planes are initially active.

 cursor changes don't warrant a FIFO recompute */

 initially allow all levels */

	/*

	 * Note that enabling cxsr with no primary/sprite planes

	 * enabled can wedge the pipe. Hence we only allow cxsr

	 * with exactly one enabled primary/sprite plane.

 limit to only levels we can actually handle */

 invalidate the higher levels */

	/*

	 * uncore.lock serves a double purpose here. It allows us to

	 * use the less expensive I915_{READ,WRITE}_FW() functions, and

	 * it protects the DSPARB registers from getting clobbered by

	 * parallel updates from multiple pipes.

	 *

	 * intel_pipe_update_start() has already disabled interrupts

	 * for us, so a plain spin_lock() is sufficient here.

	/*

	 * If our intermediate WM are identical to the final WM, then we can

	 * omit the post-vblank programming; only update if it's different.

 Calc sr entries for one plane configs */

 self-refresh has much higher latency */

 Turn off self refresh if both pipes are enabled */

 965 has limitations... */

 update cursor SR watermark */

 self-refresh seems busted with untiled */

	/*

	 * Overlay gets an aggressive default since video jitter is bad.

 Play safe and disable self-refresh before adjusting watermarks. */

 Calc sr entries for one plane configs */

 self-refresh has much higher latency */

 Set request length to 8 cachelines per fetch */

 latency must be in 0.1us units. */

 latency must be in 0.1us units. */

	/*

	 * Neither of these should be possible since this function shouldn't be

	 * called if the CRTC is off or the plane is invisible.  But let's be

	 * extra paranoid to avoid a potential divide-by-zero if we screw up

	 * elsewhere in the driver.

/*

 * For both WM_PIPE and WM_LP.

 * mem_value must be in 0.1us units.

/*

 * For both WM_PIPE and WM_LP.

 * mem_value must be in 0.1us units.

/*

 * For both WM_PIPE and WM_LP.

 * mem_value must be in 0.1us units.

 Only for WM_LP. */

 BDW primary/sprite plane watermarks */

 IVB/HSW primary/sprite plane watermarks */

 ILK/SNB primary plane watermarks */

 ILK/SNB sprite plane watermarks */

 Calculate the maximum primary/sprite plane watermark */

 if sprites aren't enabled, sprites get nothing */

 HSW allows LP1+ watermarks even with multiple pipes */

		/*

		 * For some reason the non self refresh

		 * FIFO size is only half of the self

		 * refresh FIFO size on ILK/SNB.

 level 0 is always calculated with 1:1 split */

 clamp to max that the registers can hold */

 Calculate the maximum cursor plane watermark */

 HSW LP1+ watermarks w/ multiple pipes */

 otherwise just report max that registers can hold */

 already determined to be invalid? */

	/*

	 * HACK until we can pre-compute everything,

	 * and thus fail gracefully if LP0 watermarks

	 * are exceeded...

 WM1+ latency values stored in 0.5us units */

 read the first set of memory latencies[0:3] */

 data0 to be programmed to 0 for first set */

 read the second set of memory latencies[4:7] */

 data0 to be programmed to 1 for second set */

		/*

		 * If a level n (n > 1) has a 0us latency, all levels m (m >= n)

		 * need to be disabled. We make sure to sanitize the values out

		 * of the punit to satisfy this requirement.

		/*

		 * WaWmMemoryReadLatency

		 *

		 * punit doesn't take into account the read latency so we need

		 * to add proper adjustement to each valid level we retrieve

		 * from the punit when level 0 response data is 0us.

		/*

		 * WA Level-0 adjustment for 16GB DIMMs: SKL+

		 * If we could not get dimm info enable this WA to prevent from

		 * any underrun. If not able to get Dimm info assume 16GB dimm

		 * to avoid any underrun.

 ILK primary LP0 latency is 700 ns */

 ILK sprite LP0 latency is 1300 ns */

 ILK cursor LP0 latency is 1300 ns */

 how many WM levels are we expecting */

		/*

		 * - latencies are in us on gen9.

		 * - before then, WM1+ latency values are in 0.5us units

	/*

	 * The BIOS provided WM memory latency values are often

	 * inadequate for high resolution displays. Adjust them.

	/*

	 * On some SNB machines (Thinkpad X220 Tablet at least)

	 * LP3 usage can cause vblank interrupts to be lost.

	 * The DEIIR bit will go high but it looks like the CPU

	 * never gets interrupted.

	 *

	 * It's not clear whether other interrupt source could

	 * be affected or if this is somehow limited to vblank

	 * interrupts only. To play it safe we disable LP3

	 * watermarks entirely.

 LP0 watermark maximums depend on this pipe alone */

 LP0 watermarks always use 1/2 DDB partitioning */

 At least LP0 must be valid */

 Compute new watermarks for the pipe */

 ILK/SNB: LP2+ watermarks only w/o sprites */

 ILK/SNB/IVB: LP1+ watermarks only w/o scaling */

		/*

		 * Disable any watermark level that exceeds the

		 * register maximums since such watermarks are

		 * always invalid.

/*

 * Build a set of 'intermediate' watermark values that satisfy both the old

 * state and the new state.  These can be programmed to the hardware

 * immediately.

	/*

	 * Start with the final, target watermarks, then combine with the

	 * currently active watermarks to get values that are safe both before

	 * and after the vblank.

	/*

	 * We need to make sure that these merged watermark values are

	 * actually a valid configuration themselves.  If they're not,

	 * there's no safe way to transition from the old state to

	 * the new state, so we need to fail the atomic transaction.

	/*

	 * If our intermediate WM are identical to the final WM, then we can

	 * omit the post-vblank programming; only update if it's different.

/*

 * Merge the watermarks from all active pipes for a specific level.

		/*

		 * The watermark values may have been used in the past,

		 * so we must maintain them in the registers for some

		 * time even if the level is now disabled.

/*

 * Merge all low power watermarks for all active pipes.

 ILK/SNB/IVB: LP1+ watermarks only w/ single pipe */

 ILK: FBC WM must be disabled always */

 merge each WM1+ level */

 make sure all following levels get disabled */

		/*

		 * The spec says it is preferred to disable

		 * FBC WMs instead of disabling a WM level.

 ILK: LP2+ must be disabled when FBC WM is disabled but FBC enabled */

	/*

	 * FIXME this is racy. FBC might get enabled later.

	 * What we should check here is whether FBC can be

	 * enabled sometime later.

 LP1,LP2,LP3 levels are either 1,2,3 or 1,3,4 */

 The value we need to program into the WM_LPx latency field */

 LP1+ register values */

		/*

		 * Maintain the watermark values even if the level is

		 * disabled. Doing otherwise could cause underruns.

		/*

		 * Always set WM1S_LP_EN when spr_val != 0, even if the

		 * level is disabled. Doing otherwise could cause underruns.

 LP0 register values */

/* Find the result with the highest level enabled. Check for enable_fbc_wm in

 dirty bits used to track which watermarks need changes */

 Must disable LP1+ watermarks too */

 Must disable LP1+ watermarks too */

 Must disable LP1+ watermarks too */

 LP1+ watermarks already deemed dirty, no need to continue */

 Find the lowest numbered LP1+ watermark in need of an update... */

 ...and mark it and all higher numbered LP1+ watermarks as dirty */

	/*

	 * Don't touch WM1S_LP_EN here.

	 * Doing so could cause underruns.

/*

 * The spec says we shouldn't write when we don't need, because every write

 * causes WMs to be re-evaluated, expending some power.

/*

 * FIXME: We still don't have the proper code detect if we need to apply the WA,

 * so assume we'll always need it in order to avoid underruns.

 Default to an unusable block time */

/*

 * SAGV dynamically adjusts the system agent voltage and clock frequencies

 * depending on power and performance requirements. The display engine access

 * to system memory is blocked during the adjustment time. Because of the

 * blocking time, having this enabled can cause full system hangs and/or pipe

 * underruns if we don't meet all of the following requirements:

 *

 *  - <= 1 pipe enabled

 *  - All planes can enable watermarks for latencies >= SAGV engine block time

 *  - We're not using an interlaced display configuration

 We don't need to wait for SAGV when enabling */

	/*

	 * Some skl systems, pre-release machines in particular,

	 * don't actually have SAGV.

 bspec says to keep retrying for at least 1 ms */

	/*

	 * Some skl systems, pre-release machines in particular,

	 * don't actually have SAGV.

	/*

	 * Just return if we can't control SAGV or don't have it.

	 * This is different from situation when we have SAGV but just can't

	 * afford it due to DBuf limitation - in case if SAGV is completely

	 * disabled in a BIOS, we are not even allowed to send a PCode request,

	 * as it will throw an error. So have to check it here.

	/*

	 * Nothing to mask

	/*

	 * If new mask is zero - means there is nothing to mask,

	 * we can only unmask, which should be done in unmask.

	/*

	 * Restrict required qgv points before updating the configuration.

	 * According to BSpec we can't mask and unmask qgv points at the same

	 * time. Also masking should be done before updating the configuration

	 * and unmasking afterwards.

	/*

	 * Just return if we can't control SAGV or don't have it.

	 * This is different from situation when we have SAGV but just can't

	 * afford it due to DBuf limitation - in case if SAGV is completely

	 * disabled in a BIOS, we are not even allowed to send a PCode request,

	 * as it will throw an error. So have to check it here.

	/*

	 * Nothing to unmask

	/*

	 * Allow required qgv points after updating the configuration.

	 * According to BSpec we can't mask and unmask qgv points at the same

	 * time. Also masking should be done before updating the configuration

	 * and unmasking afterwards.

 Skip this plane if it's not enabled */

 Find the highest enabled wm level for this plane */

 Highest common enabled wm level for all planes */

 No enabled planes? */

		/*

		 * All enabled planes must have enabled a common wm level that

		 * can tolerate memory latencies higher than sagv_block_time_us

		/*

		 * We store use_sagv_wm in the crtc state rather than relying on

		 * that bw state since we have no convenient way to get at the

		 * latter from the plane commit hooks (especially in the legacy

		 * cursor case)

	/*

	 * Per plane DDB entry can in a really worst case be on multiple slices

	 * but single entry is anyway contigious.

	/*

	 * Watermark/ddb requirement highly depends upon width of the

	 * framebuffer, So instead of allocating DDB equally among pipes

	 * distribute DDB based on resolution/width of the display.

		/*

		 * Do not account pipes using other slice sets

		 * luckily as of current BSpec slice sets do not partially

		 * intersect(pipes share either same one slice or same slice set

		 * i.e no partial intersection), so it is enough to check for

		 * equality for now.

	/*

	 * Used for checking overlaps, so we need absolute

	 * offsets instead of MBUS relative offsets.

 out */);

 Cursor doesn't support NV12/planar, so no extra calculation needed */

 No DDB allocated for disabled planes */

/*

 * Determines the downscale amount of a plane for the purposes of watermark calculations.

 * The bspec defines downscale amount as:

 *

 * """

 * Horizontal down scale amount = maximum[1, Horizontal source size /

 *                                           Horizontal destination size]

 * Vertical down scale amount = maximum[1, Vertical source size /

 *                                         Vertical destination size]

 * Total down scale amount = Horizontal down scale amount *

 *                           Vertical down scale amount

 * """

 *

 * Return value is provided in 16.16 fixed point form to retain fractional part.

 * Caller should take care of dividing & rounding off the value.

	/*

	 * Src coordinates are already rotated by 270 degrees for

	 * the 90/270 degree plane rotation cases (to match the

	 * GTT mapping), hence no need to account for rotation here.

	 *

	 * n.b., src is 16.16 fixed point, dst is whole integer.

/*

 * Table taken from Bspec 12716

 * Pipes do have some preferred DBuf slice affinity,

 * plus there are some hardcoded requirements on how

 * those should be distributed for multipipe scenarios.

 * For more DBuf slices algorithm can get even more messy

 * and less readable, so decided to use a table almost

 * as is from BSpec itself - that way it is at least easier

 * to compare, change and check.

 Autogenerated with igt/tools/intel_dbuf_map tool: */

/*

 * Table taken from Bspec 49255

 * Pipes do have some preferred DBuf slice affinity,

 * plus there are some hardcoded requirements on how

 * those should be distributed for multipipe scenarios.

 * For more DBuf slices algorithm can get even more messy

 * and less readable, so decided to use a table almost

 * as is from BSpec itself - that way it is at least easier

 * to compare, change and check.

 Autogenerated with igt/tools/intel_dbuf_map tool: */

/*

 * This function finds an entry with same enabled pipe configuration and

 * returns correspondent DBuf slice mask as stated in BSpec for particular

 * platform.

	/*

	 * FIXME: For ICL this is still a bit unclear as prev BSpec revision

	 * required calculating "pipe ratio" in order to determine

	 * if one or two slices can be used for single pipe configurations

	 * as additional constraint to the existing table.

	 * However based on recent info, it should be not "pipe ratio"

	 * but rather ratio between pixel_rate and cdclk with additional

	 * constants, so for now we are using only table until this is

	 * clarified. Also this is the reason why crtc_state param is

	 * still here - we will need it once those additional constraints

	 * pop up.

	/*

	 * For anything else just return one slice yet.

	 * Should be extended for other platforms.

	/*

	 * Src coordinates are already rotated by 270 degrees for

	 * the 90/270 degree plane rotation cases (to match the

	 * GTT mapping), hence no need to account for rotation here.

 UV plane does 1/2 pixel sub-sampling */

 Calculate and cache data rate for each plane */

 packed/y */

 uv-plane */

 Calculate and cache data rate for each plane */

			/*

			 * The slave plane might not iterate in

			 * intel_atomic_crtc_state_for_each_plane_state(),

			 * and needs the master plane state which may be

			 * NULL if we try get_new_plane_state(), so we

			 * always calculate from the master.

 Y plane rate is calculated on the slave */

/*

 * We only disable the watermarks for each plane if

 * they exceed the ddb allocation of said plane. This

 * is done so that we don't end up touching cursor

 * watermarks needlessly when some other plane reduces

 * our max possible watermark level.

 *

 * Bspec has this to say about the PLANE_WM enable bit:

 * "All the watermarks at this level for all enabled

 *  planes must be enabled before the level will be used."

 * So this is actually safe to do.

 Clear the partitioning for disabled planes. */

 Allocate fixed number of blocks for cursor. */

	/*

	 * Find the highest watermark level for which we can satisfy the block

	 * requirement of active planes.

	/*

	 * Grant each plane the blocks it requires at the highest achievable

	 * watermark level, plus an extra share of the leftover blocks

	 * proportional to its relative data rate.

		/*

		 * We've accounted for all active planes; remaining planes are

		 * all disabled.

 Set the actual DDB start/end points for each plane */

 Gen11+ uses a separate plane for UV watermarks */

 Leave disabled planes at (0,0) */

	/*

	 * When we calculated watermark values we didn't know how high

	 * of a level we'd actually be able to hit, so we just marked

	 * all levels as "enabled."  Go back now and disable the ones

	 * that aren't actually possible.

			/*

			 * Wa_1408961008:icl, ehl

			 * Underruns with WM1+ disabled

	/*

	 * Go back and disable the transition and SAGV watermarks

	 * if it turns out we don't have enough DDB blocks for them.

/*

 * The max latency should be 257 (max the punit can code is 255 and we add 2us

 * for the read latency) and cpp should always be <= 8, so that

 * should allow pixel_rate up to ~2 GHz which seems sufficient since max

 * 2xcdclk is 1350 MHz and the pixel rate should never exceed that.

 only planar format has two planes */

	/*

	 * Src coordinates are already rotated by 270 degrees for

	 * the 90/270 degree plane rotation cases (to match the

	 * GTT mapping), hence no need to account for rotation here.

 The number of lines are ignored for the level 0 watermark. */

 out */)

 reject it */

	/*

	 * WaIncreaseLatencyIPCEnabled: kbl,cfl

	 * Display WA #1141: kbl,cfl

 Display WA #1125: skl,bxt,kbl */

 Display WA #1126: skl,bxt,kbl */

			/*

			 * Make sure result blocks for higher latency levels are

			 * atleast as high as level below the current level.

			 * Assumption in DDB algorithm optimization for special

			 * cases. Also covers Display WA #1125 for RC.

 reject it */

	/*

	 * If lines is valid, assume we can use this watermark level

	 * for now.  We'll come back and disable it after we calculate the

	 * DDB allocation if it turns out we don't actually have enough

	 * blocks to satisfy it.

 Bspec says: value >= plane ddb allocation -> invalid, hence the +1 here */

 Transition WM don't make any sense if ipc is disabled */

	/*

	 * WaDisableTWM:skl,kbl,cfl,bxt

	 * Transition WM are not recommended by HW team for GEN9

 Display WA #1140: glk,cnl */

 This is configurable amount */

	/*

	 * The spec asks for Selected Result Blocks for wm0 (the real value),

	 * not Result Blocks (the integer value). Pay attention to the capital

	 * letters. The value wm_l0->blocks is actually Result Blocks, but

	 * since Result Blocks is the ceiling of Selected Result Blocks plus 1,

	 * and since we later will have to get the ceiling of the sum in the

	 * transition watermarks calculation, we can just pretend Selected

	 * Result Blocks is Result Blocks minus 1 and it should work for the

	 * current platforms.

	/*

	 * Just assume we can enable the transition watermark.  After

	 * computing the DDB we'll come back and disable it if that

	 * assumption turns out to be false.

 uv plane watermarks must also be validated for NV12/Planar */

 Watermarks calculated in master */

		/*

		 * FIXME should perhaps check {old,new}_plane_crtc->hw.crtc

		 * instead but we don't populate that correctly for NV12 Y

		 * planes so for now hack this.

		/*

		 * We don't check uv_wm as the hardware doesn't actually

		 * use it. It only gets used for calculating the required

		 * ddb allocation.

	/*

	 * FIXME: For now we always enable slice S1 as per

	 * the Bspec display initialization sequence.

 TODO: Implement vblank synchronized MBUS joining changes */

		/*

		 * We don't check uv_wm as the hardware doesn't actually

		 * use it. It only gets used for calculating the required

		 * ddb allocation.

/*

 * To make sure the cursor watermark registers are always consistent

 * with our computed state the following scenario needs special

 * treatment:

 *

 * 1. enable cursor

 * 2. move cursor entirely offscreen

 * 3. disable cursor

 *

 * Step 2. does call .disable_plane() but does not zero the watermarks

 * (since we consider an offscreen cursor still active for the purposes

 * of watermarks). Step 3. would not normally call .disable_plane()

 * because the actual plane visibility isn't changing, and we don't

 * deallocate the cursor ddb until the pipe gets disabled. So we must

 * force step 3. to call .disable_plane() to update the watermark

 * registers properly.

 *

 * Other planes do not suffer from this issues as their watermarks are

 * calculated based on the actual plane visibility. The only time this

 * can trigger for the other planes is during the initial readout as the

 * default value of the watermarks registers is not zero.

		/*

		 * Force a full wm update for every plane on modeset.

		 * Required because the reset value of the wm registers

		 * is non-zero, whereas we want all disabled planes to

		 * have zero watermarks. So if we turn off the relevant

		 * power well the hardware state will go out of sync

		 * with the software state.

	/*

	 * skl_compute_ddb() will have adjusted the final watermarks

	 * based on how much ddb is available. Now we can actually

	 * check if the final watermarks changed.

 Compute the currently _active_ config */

 5/6 split only in single pipe config on IVB+ */

		/*

		 * Used for checking overlaps, so we need absolute

		 * offsets instead of MBUS relative offsets.

		/*

		 * For active pipes LP0 watermark is marked as

		 * enabled, and LP1+ watermaks as disabled since

		 * we can't really reverse compute them in case

		 * multiple pipes are active.

		/*

		 * For inactive pipes, all watermark levels

		 * should be marked as enabled but zeroed,

		 * which is what we'd compute them to.

		/*

		 * If DDR DVFS is disabled in the BIOS, Punit

		 * will never ack the request. So if that happens

		 * assume we don't have to enable/disable DDR DVFS

		 * dynamically. To test that just set the REQ_ACK

		 * bit to poke the Punit, but don't change the

		 * HIGH/LOW bits so that we don't actually change

		 * the current state.

/*

 * FIXME should probably kill this and improve

 * the real watermark readout/sanitation instead

	/*

	 * Don't touch WM1S_LP_EN here.

	 * Doing so could cause underruns.

 Display WA #0477 WaDisableIPC: skl */

 Display WA #1141: SKL:all KBL:all CFL */

	/*

	 * On Ibex Peak and Cougar Point, we need to disable clock

	 * gating for the panel power sequencer or it will fail to

	 * start up when no ports are active.

	/*

	 * Required for FBC

	 * WaFbcDisableDpfcClockGating:ilk

	/*

	 * According to the spec the following bits should be set in

	 * order to enable memory self-refresh

	 * The bit 22/21 of 0x42004

	 * The bit 5 of 0x42020

	 * The bit 15 of 0x45000

	/*

	 * Based on the document from hardware guys the following bits

	 * should be set unconditionally in order to enable FBC.

	 * The bit 22 of 0x42000

	 * The bit 22 of 0x42004

	 * The bit 7,8,9 of 0x42020.

 WaFbcAsynchFlipDisableFbcQueue:ilk */

	/*

	 * On Ibex Peak and Cougar Point, we need to disable clock

	 * gating for the panel power sequencer or it will fail to

	 * start up when no ports are active.

	/* The below fixes the weird display corruption, a few pixels shifted

	 * downward, on (only) LVDS of some HP laptops with IVY.

 WADP0ClockGatingDisable */

	/* According to the BSpec vol1g, bit 12 (RCPBUNIT) clock

	 * gating disable must be set.  Failure to set it results in

	 * flickering pixels due to Z write ordering failures after

	 * some amount of runtime in the Mesa "fire" demo, and Unigine

	 * Sanctuary and Tropics, and apparently anything else with

	 * alpha test or pixel discard.

	 *

	 * According to the spec, bit 11 (RCCUNIT) must also be set,

	 * but we didn't debug actual testcases to find it out.

	 *

	 * WaDisableRCCUnitClockGating:snb

	 * WaDisableRCPBUnitClockGating:snb

	/*

	 * According to the spec the following bits should be

	 * set in order to enable memory self-refresh and fbc:

	 * The bit21 and bit22 of 0x42000

	 * The bit21 and bit22 of 0x42004

	 * The bit5 and bit7 of 0x42020

	 * The bit14 of 0x70180

	 * The bit14 of 0x71180

	 *

	 * WaFbcAsynchFlipDisableFbcQueue:snb

	/*

	 * TODO: this bit should only be enabled when really needed, then

	 * disabled when not needed anymore in order to save power.

 WADPOClockGatingDisable:hsw */

 WaTempDisableDOPClkGating:bdw */

	/*

	 * Wait at least 100 clocks before re-enabling clock gating.

	 * See the definition of L3SQCREG1 in BSpec.

 Wa_1409120013:icl,ehl */

Wa_14010594013:icl, ehl */

 Wa_1409120013:tgl,rkl,adl-s,dg1 */

 Wa_1409825376:tgl (pre-prod)*/

 Wa_14013723622:tgl,rkl,dg1,adl-s */

 Wa_22011091694:adlp */

 Wa_1409836686:dg1[a0] */

 Display WA #1181 WaSouthDisplayDisablePWMCGEGating: cnp */

 WAC6entrylatency:cfl */

	/*

	 * WaFbcTurnOffFbcWatermark:cfl

	 * Display WA #0562: cfl

	/*

	 * WaFbcNukeOnHostModify:cfl

	 * Display WA #0873: cfl

 WAC6entrylatency:kbl */

 WaDisableSDEUnitClockGating:kbl */

 WaDisableGamClockGating:kbl */

	/*

	 * WaFbcTurnOffFbcWatermark:kbl

	 * Display WA #0562: kbl

	/*

	 * WaFbcNukeOnHostModify:kbl

	 * Display WA #0873: kbl

 WaDisableDopClockGating:skl */

 WAC6entrylatency:skl */

	/*

	 * WaFbcTurnOffFbcWatermark:skl

	 * Display WA #0562: skl

	/*

	 * WaFbcNukeOnHostModify:skl

	 * Display WA #0873: skl

	/*

	 * WaFbcHighMemBwCorruptionAvoidance:skl

	 * Display WA #0883: skl

 WaFbcAsynchFlipDisableFbcQueue:hsw,bdw */

 WaSwitchSolVfFArbitrationPriority:bdw */

 WaPsrDPAMaskVBlankInSRD:bdw */

 WaPsrDPRSUnmaskVBlankInSRD:bdw */

 WaVSRefCountFullforceMissDisable:bdw */

 WaDSRefCountFullforceMissDisable:bdw */

 WaDisableSDEUnitClockGating:bdw */

 WaProgramL3SqcReg1Default:bdw */

 WaKVMNotificationOnConfigChange:bdw */

	/* WaDisableDopClockGating:bdw

	 *

	 * Also see the CHICKEN2 write in bdw_init_workarounds() to disable DOP

	 * clock gating.

 WaFbcAsynchFlipDisableFbcQueue:hsw,bdw */

 This is required by WaCatErrorRejectionIssue:hsw */

 WaSwitchSolVfFArbitrationPriority:hsw */

 WaFbcAsynchFlipDisableFbcQueue:ivb */

 WaDisableBackToBackFlipFix:ivb */

 must write both registers */

	/*

	 * According to the spec, bit 13 (RCZUNIT) must be set on IVB.

	 * This implements the WaDisableRCZUnitClockGating:ivb workaround.

 This is required by WaCatErrorRejectionIssue:ivb */

 WaDisableBackToBackFlipFix:vlv */

 WaDisableDopClockGating:vlv */

 This is required by WaCatErrorRejectionIssue:vlv */

	/*

	 * According to the spec, bit 13 (RCZUNIT) must be set on IVB.

	 * This implements the WaDisableRCZUnitClockGating:vlv workaround.

	/* WaDisableL3Bank2xClockGate:vlv

	 * Disabling L3 clock gating- MMIO 940c[25] = 1

	/*

	 * WaDisableVLVClockGating_VBIIssue:vlv

	 * Disable clock gating on th GCFG unit to prevent a delay

	 * in the reporting of vblank events.

 WaVSRefCountFullforceMissDisable:chv */

 WaDSRefCountFullforceMissDisable:chv */

 WaDisableSemaphoreAndSyncFlipWait:chv */

 WaDisableCSUnitClockGating:chv */

 WaDisableSDEUnitClockGating:chv */

	/*

	 * WaProgramL3SqcReg1Default:chv

	 * See gfxspecs/Related Documents/Performance Guide/

	 * LSQC Setting Recommendations.

 IIR "flip pending" means done if this bit is set */

 interrupts should cause a wake up from C3 */

 On GEN3 we really need to make sure the ARB C3 LP bit is set */

 interrupts should cause a wake up from C3 */

	/*

	 * Have FBC ignore 3D activity since we use software

	 * render tracking, and otherwise a pure 3D workload

	 * (even if it just renders a single frame and then does

	 * abosultely nothing) would not allow FBC to recompress

	 * until a 2D blit occurs.

/**

 * intel_init_clock_gating_hooks - setup the clock gating hooks

 * @dev_priv: device private

 *

 * Setup the hooks that configure which clocks of a given platform can be

 * gated and also apply various GT and display specific workarounds for these

 * platforms. Note that some GT specific workarounds are applied separately

 * when GPU contexts or batchbuffers start their execution.

 Set up chip specific power management-related functions */

 For cxsr */

 For FIFO watermark updates */

 Disable CxSR and never update its watermark again */

/*

 * Configure MBUS_CTL and all DBUF_CTL_S of each slice to join_mbus state before

 * update the request state of all DBUS slices.

	/*

	 * TODO: Implement vblank synchronized MBUS joining changes.

	 * Must be properly coordinated with dbuf reprogramming.

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 Make sure IS_<platform> checks are working. */

 Find and mark subplatform bits based on the PCI device id. */

 ULX machines are also considered ULT. */

/**

 * intel_device_info_runtime_init - initialize runtime info

 * @dev_priv: the i915 device

 *

 * Determine various intel_device_info fields at runtime.

 *

 * Use it when either:

 *   - it's judged too laborious to fill n static structures with the limit

 *     when a simple if statement does the job,

 *   - run-time checks (eg read fuse/strap registers) are needed.

 *

 * This function needs to be called:

 *   - after the MMIO has been setup as we are reading registers,

 *   - after the PCH has been detected,

 *   - before the first usage of the fields it can tweak.

 Wa_14011765242: adl-s A0,A1 */

		/*

		 * Skylake and Broxton currently don't expose the topmost plane as its

		 * use is exclusive with the legacy cursor and we only want to expose

		 * one of those, not both. Until we can safely expose the topmost plane

		 * as a DRM_PLANE_TYPE_CURSOR with all the features exposed/supported,

		 * we don't expose the topmost plane at all to prevent ABI breakage

		 * down the line.

		/*

		 * SFUSE_STRAP is supposed to have a bit signalling the display

		 * is fused off. Unfortunately it seems that, at least in

		 * certain cases, fused off display means that PCH display

		 * reads don't land anywhere. In that case, we read 0s.

		 *

		 * On CPT/PPT, we can detect this case as SFUSE_STRAP_FUSE_LOCK

		 * should be set when taking over after the firmware.

/*

 * Copyright Â© 2017 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 GEM_BUG_ON() */

/*

 * struct i915_syncmap is a layer of a radixtree that maps a u64 fence

 * context id to the last u32 fence seqno waited upon from that context.

 * Unlike lib/radixtree it uses a parent pointer that allows traversal back to

 * the root. This allows us to access the whole tree via a single pointer

 * to the most recently used layer. We expect fence contexts to be dense

 * and most reuse to be on the same i915_gem_context but on neighbouring

 * engines (i.e. on adjacent contexts) and reuse the same leaf, a very

 * effective lookup cache. If the new lookup is not on the same leaf, we

 * expect it to be on the neighbouring branch.

 *

 * A leaf holds an array of u32 seqno, and has height 0. The bitmap field

 * allows us to store whether a particular seqno is valid (i.e. allows us

 * to distinguish unset from 0).

 *

 * A branch holds an array of layer pointers, and has height > 0, and always

 * has at least 2 layers (either branches or leaves) below it.

 *

 * For example,

 *	for x in

 *	  0 1 2 0x10 0x11 0x200 0x201

 *	  0x500000 0x500001 0x503000 0x503001

 *	  0xE<<60:

 *		i915_syncmap_set(&sync, x, lower_32_bits(x));

 * will build a tree like:

 *	0xXXXXXXXXXXXXXXXX

 *	0-> 0x0000000000XXXXXX

 *	|   0-> 0x0000000000000XXX

 *	|   |   0-> 0x00000000000000XX

 *	|   |   |   0-> 0x000000000000000X 0:0, 1:1, 2:2

 *	|   |   |   1-> 0x000000000000001X 0:10, 1:11

 *	|   |   2-> 0x000000000000020X 0:200, 1:201

 *	|   5-> 0x000000000050XXXX

 *	|       0-> 0x000000000050000X 0:500000, 1:500001

 *	|       3-> 0x000000000050300X 0:503000, 1:503001

 *	e-> 0xe00000000000000X e:e

	/*

	 * Following this header is an array of either seqno or child pointers:

	 * union {

	 *	u32 seqno[KSYNCMAP];

	 *	struct i915_syncmap *child[KSYNCMAP];

	 * };

/**

 * i915_syncmap_init -- initialise the #i915_syncmap

 * @root: pointer to the #i915_syncmap

/**

 * i915_syncmap_is_later -- compare against the last know sync point

 * @root: pointer to the #i915_syncmap

 * @id: the context id (other timeline) we are synchronising to

 * @seqno: the sequence number along the other timeline

 *

 * If we have already synchronised this @root timeline with another (@id) then

 * we can omit any repeated or earlier synchronisation requests. If the two

 * timelines are already coupled, we can also omit the dependency between the

 * two as that is already known via the timeline.

 *

 * Returns true if the two timelines are already synchronised wrt to @seqno,

 * false if not and the synchronisation must be emitted.

 First climb the tree back to a parent branch */

 And then descend again until we find our leaf */

 Caller handled the likely cached case */

 Climb back up the tree until we find a common prefix */

	/*

	 * No shortcut, we have to descend the tree to find the right layer

	 * containing this fence.

	 *

	 * Each layer in the tree holds 16 (KSYNCMAP) pointers, either fences

	 * or lower layers. Leaf nodes (height = 0) contain the fences, all

	 * other nodes (height > 0) are internal layers that point to a lower

	 * node. Each internal layer has at least 2 descendents.

	 *

	 * Starting at the top, we check whether the current prefix matches. If

	 * it doesn't, we have gone past our target and need to insert a join

	 * into the tree, and a new leaf node for the target as a descendent

	 * of the join, as well as the original layer.

	 *

	 * The matching prefix means we are still following the right branch

	 * of the tree. If it has height 0, we have found our leaf and just

	 * need to replace the fence slot with ourselves. If the height is

	 * not zero, our slot contains the next layer in the tree (unless

	 * it is empty, in which case we can add ourselves as a new leaf).

	 * As descend the tree the prefix grows (and height decreases).

 Insert a join above the current layer */

 Compute the height at which these two diverge */

 Insert the join into the parent */

 Compute the idx of the other branch, not our id! */

 Ascend to the join */

 Descend into the next layer */

/**

 * i915_syncmap_set -- mark the most recent syncpoint between contexts

 * @root: pointer to the #i915_syncmap

 * @id: the context id (other timeline) we have synchronised to

 * @seqno: the sequence number along the other timeline

 *

 * When we synchronise this @root timeline with another (@id), we also know

 * that we have synchronized with all previous seqno along that timeline. If

 * we then have a request to synchronise with the same seqno or older, we can

 * omit it, see i915_syncmap_is_later()

 *

 * Returns 0 on success, or a negative error code.

	/*

	 * We expect to be called in sequence following is_later(id), which

	 * should have preloaded the root for us.

/**

 * i915_syncmap_free -- free all memory associated with the syncmap

 * @root: pointer to the #i915_syncmap

 *

 * Either when the timeline is to be freed and we no longer need the sync

 * point tracking, or when the fences are all known to be signaled and the

 * sync point tracking is redundant, we can free the #i915_syncmap to recover

 * its allocations.

 *

 * Will reinitialise the @root pointer so that the #i915_syncmap is ready for

 * reuse.

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2018 Intel Corporation

 recursion vs useful flexibility */

/*

 * SPDX-License-Identifier: MIT

 Reject all old ums/dri params. */

		/* Though we've started our numbering from 1, and so class all

		 * earlier versions as 0, in effect their value is undefined as

		 * the ioctl will report EINVAL for the unknown param!

 Remember to bump this if the version changes! */

 depends on GEM */

		/* For the time being all of these are always true;

		 * if some supported hardware does not have one of these

		 * features this value needs to be provided from

		 * INTEL_INFO(), a feature macro, or similar.

 Only copy bits from the first slice */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright Â© 2009 Intel Corporation

 *

 * Authors:

 *    Chris Wilson <chris@chris-wilson.co.uk>

/*

 * 32-bit ioctl compatibility routines for the i915 DRM.

 *

 * Copyright (C) Paul Mackerras 2005

 * Copyright (C) Alan Hourihane 2005

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,

 * WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Author: Alan Hourihane <alanh@fairlite.demon.co.uk>

	/*

	 * We screwed up the generic ioctl struct here and used a variable-sized

	 * pointer. Use u32 in the compat struct to match the 32bit pointer

	 * userspace expects.

/**

 * i915_ioc32_compat_ioctl - handle the mistakes of the past

 * @filp: the file pointer

 * @cmd: the ioctl command (and encoded flags)

 * @arg: the ioctl argument (from userspace)

 *

 * Called whenever a 32-bit process running under a 64-bit kernel

 * performs an ioctl on /dev/dri/card<n>.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

 Be tolerant of leading/trailing whitespace */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2019 Intel Corporation

	/*

	 * Treat get/put as different subclasses, as we may need to run

	 * the put callback from under the shrinker and do not want to

	 * cross-contanimate that callback with any extra work performed

	 * upon acquiring the wakeref.

 release wf->count */

 ops->put() must reschedule its own release on error/deferral */

 Assume we are not in process context and so cannot sleep. */

 Our mission is that we only extend an already active wakeref */

	/*

	 * If we extend a pending timer, we will only get a single timer

	 * callback and so need to cancel the local inc by running the

	 * elided callback to keep the wf->count balanced.

/*

 * Copyright Â© 2013 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Brad Volkin <bradley.d.volkin@intel.com>

 *

/**

 * DOC: batch buffer command parser

 *

 * Motivation:

 * Certain OpenGL features (e.g. transform feedback, performance monitoring)

 * require userspace code to submit batches containing commands such as

 * MI_LOAD_REGISTER_IMM to access various registers. Unfortunately, some

 * generations of the hardware will noop these commands in "unsecure" batches

 * (which includes all userspace batches submitted via i915) even though the

 * commands may be safe and represent the intended programming model of the

 * device.

 *

 * The software command parser is similar in operation to the command parsing

 * done in hardware for unsecure batches. However, the software parser allows

 * some operations that would be noop'd by hardware, if the parser determines

 * the operation is safe, and submits the batch as "secure" to prevent hardware

 * parsing.

 *

 * Threats:

 * At a high level, the hardware (and software) checks attempt to prevent

 * granting userspace undue privileges. There are three categories of privilege.

 *

 * First, commands which are explicitly defined as privileged or which should

 * only be used by the kernel driver. The parser rejects such commands

 *

 * Second, commands which access registers. To support correct/enhanced

 * userspace functionality, particularly certain OpenGL extensions, the parser

 * provides a whitelist of registers which userspace may safely access

 *

 * Third, commands which access privileged memory (i.e. GGTT, HWS page, etc).

 * The parser always rejects such commands.

 *

 * The majority of the problematic commands fall in the MI_* range, with only a

 * few specific commands on each engine (e.g. PIPE_CONTROL and MI_FLUSH_DW).

 *

 * Implementation:

 * Each engine maintains tables of commands and registers which the parser

 * uses in scanning batch buffers submitted to that engine.

 *

 * Since the set of commands that the parser must check for is significantly

 * smaller than the number of commands supported, the parser tables contain only

 * those commands required by the parser. This generally works because command

 * opcode ranges have standard command length encodings. So for commands that

 * the parser does not need to check, it can easily skip them. This is

 * implemented via a per-engine length decoding vfunc.

 *

 * Unfortunately, there are a number of commands that do not follow the standard

 * length encoding for their opcode range, primarily amongst the MI_* commands.

 * To handle this, the parser provides a way to define explicit "skip" entries

 * in the per-engine command tables.

 *

 * Other command table entries map fairly directly to high level categories

 * mentioned above: rejected, register whitelist. The parser implements a number

 * of checks, including the privileged memory checks, via a general bitmasking

 * mechanism.

/*

 * A command that requires special handling by the command parser.

	/*

	 * Flags describing how the command parser processes the command.

	 *

	 * CMD_DESC_FIXED: The command has a fixed length if this is set,

	 *                 a length mask if not set

	 * CMD_DESC_SKIP: The command is allowed but does not follow the

	 *                standard length encoding for the opcode range in

	 *                which it falls

	 * CMD_DESC_REJECT: The command is never allowed

	 * CMD_DESC_REGISTER: The command should be checked against the

	 *                    register whitelist for the appropriate ring

	/*

	 * The command's unique identification bits and the bitmask to get them.

	 * This isn't strictly the opcode field as defined in the spec and may

	 * also include type, subtype, and/or subop fields.

	/*

	 * The command's length. The command is either fixed length (i.e. does

	 * not include a length field) or has a length field mask. The flag

	 * CMD_DESC_FIXED indicates a fixed length. Otherwise, the command has

	 * a length mask. All command entries in a command table must include

	 * length information.

	/*

	 * Describes where to find a register address in the command to check

	 * against the ring's register whitelist. Only valid if flags has the

	 * CMD_DESC_REGISTER bit set.

	 *

	 * A non-zero step value implies that the command may access multiple

	 * registers in sequence (e.g. LRI), in that case step gives the

	 * distance in dwords between individual offset fields.

	/*

	 * Describes command checks where a particular dword is masked and

	 * compared against an expected value. If the command does not match

	 * the expected value, the parser rejects it. Only valid if flags has

	 * the CMD_DESC_BITMASK bit set. Only entries where mask is non-zero

	 * are valid.

	 *

	 * If the check specifies a non-zero condition_mask then the parser

	 * only performs the check when the bits specified by condition_mask

	 * are non-zero.

/*

 * A table of commands requiring special handling by the command parser.

 *

 * Each engine has an array of tables. Each table consists of an array of

 * command descriptors, which must be sorted with command opcodes in

 * ascending order.

 Convenience macros to compress the tables */

/*            Command                          Mask   Fixed Len   Action

	/*

	 * MI_BATCH_BUFFER_START requires some special handling. It's not

	 * really a 'skip' action but it doesn't seem like it's worth adding

	 * a new action. See intel_engine_cmd_parser().

	/*

	 * MFX_WAIT doesn't fit the way we handle length for most commands.

	 * It has a length field but it uses a non-standard length bias.

	 * It is always 1 dword though, so just treat it as fixed length.

/*

 * For Gen9 we can still rely on the h/w to enforce cmd security, and only

 * need to re-enforce the register access checks. We therefore only need to

 * teach the cmdparser how to find the end of each command, and identify

 * register accesses. The table doesn't need to reject any commands, and so

 * the only commands listed here are:

 *   1) Those that touch registers

 *   2) Those that do not have the default 8-bit length

 *

 * Note that the default MI length mask chosen for this table is 0xFF, not

 * the 0x3F used on older devices. This is because the vast majority of MI

 * cmds on Gen9 use a standard 8-bit Length field.

 * All the Gen9 blitter instructions are standard 0xFF length mask, and

 * none allow access to non-general registers, so in fact no BLT cmds are

 * included in the table at all.

 *

	/*

	 * We allow BB_START but apply further checks. We just sanitize the

	 * basic fields here.

/*

 * Register whitelists, sorted by increasing register offset.

/*

 * An individual whitelist entry granting access to register addr.  If

 * mask is non-zero the argument of immediate register writes will be

 * AND-ed with mask, and the command will be rejected if the result

 * doesn't match value.

 *

 * Registers with non-zero mask are only allowed to be written using

 * LRI.

 Convenience macro for adding 32-bit registers. */

/*

 * Convenience macro for adding 64-bit registers.

 *

 * Some registers that userspace accesses are 64 bits. The register

 * access commands only allow 32-bit accesses. Hence, we have to include

 * entries for both halves of the 64-bit registers.

/*

 * Different command ranges have different numbers of bits for the opcode. For

 * example, MI commands use bits 31:23 while 3D commands use bits 31:16. The

 * problem is that, for example, MI commands use bits 22:16 for other fields

 * such as GGTT vs PPGTT bits. If we include those bits in the mask then when

 * we mask a command from a batch it could hash to the wrong bucket due to

 * non-opcode bits being set. But if we don't include those bits, some 3D

 * commands may hash to the same bucket due to not including opcode bits that

 * make the command unique. For now, we will risk hashing to the same bucket.

/**

 * intel_engine_init_cmd_parser() - set cmd parser related fields for an engine

 * @engine: the engine to initialize

 *

 * Optionally initializes fields related to batch buffer command parsing in the

 * struct intel_engine_cs based on whether the platform requires software

 * command parsing.

 BCS Engine unsafe without parser */

 VECS can use the same length_mask function as VCS */

/**

 * intel_engine_cleanup_cmd_parser() - clean up cmd parser related fields

 * @engine: the engine to clean up

 *

 * Releases any resources related to command parsing that may have been

 * initialized for the specified engine.

/*

 * Returns a pointer to a descriptor for the command specified by cmd_header.

 *

 * The caller must supply space for a default descriptor via the default_desc

 * parameter. If no descriptor for the specified command exists in the engine's

 * command parser tables, this function fills in default_desc based on the

 * engine's default length encoding and returns default_desc.

 Returns a vmap'd pointer to dst_obj, which the caller must unmap */

		/*

		 * We can avoid clflushing partial cachelines before the write

		 * if we only every write full cache-lines. Since we know that

		 * both the source and destination are in multiples of

		 * PAGE_SIZE, we can simply round up to the next cacheline.

		 * We don't care about copying too much here as we only

		 * validate up to the end of the batch.

 dst_obj is returned with vmap pinned */

		/*

		 * Get the distance between individual register offset

		 * fields if the command can perform more than one

		 * access at a time.

			/*

			 * Check the value written to the register against the

			 * allowed mask/value pair given in the whitelist entry.

 For igt compatibility on older platforms */

	/*

	 * Any underflow of jump_target is guaranteed to be outside the range

	 * of a u32, so >= test catches both too large and too small

	/*

	 * This cannot overflow a u32 because we already checked jump_offset

	 * is within the BB, and the batch_length is a u32

	/*

	 * We expect batch_length to be less than 256KiB for known users,

	 * i.e. we need at most an 8KiB bitmap allocation which should be

	 * reasonably cheap due to kmalloc caches.

 Prefer to report transient allocation failure rather than hit oom */

/**

 * intel_engine_cmd_parser() - parse a batch buffer for privilege violations

 * @engine: the engine on which the batch is to execute

 * @batch: the batch buffer in question

 * @batch_offset: byte offset in the batch at which execution starts

 * @batch_length: length of the commands in batch_obj

 * @shadow: validated copy of the batch buffer in question

 * @trampoline: true if we need to trampoline into privileged execution

 *

 * Parses the specified batch buffer looking for privilege violations as

 * described in the overview.

 *

 * Return: non-zero if the parser finds violations or otherwise fails; -EACCES

 * if the batch appears legal but should use hardware parsing

 Defer failure until attempted use */

	/*

	 * We use the batch length as size because the shadow object is as

	 * large or larger and copy_batch() will write MI_NOPs to the extra

	 * space. Parsing should be faster in some cases this way.

		/*

		 * With the trampoline, the shadow is executed twice.

		 *

		 *   1 - starting at offset 0, in privileged mode

		 *   2 - starting at offset batch_len, as non-privileged

		 *

		 * Only if the batch is valid and safe to execute, do we

		 * allow the first privileged execution to proceed. If not,

		 * we terminate the first batch and use the second batchbuffer

		 * entry to chain to the original unsafe non-privileged batch,

		 * leaving it to the HW to validate.

 Batch unsafe to execute with privileges, cancel! */

 If batch is unsafe but valid, jump to the original */

 allow execution */

/**

 * i915_cmd_parser_get_version() - get the cmd parser version number

 * @dev_priv: i915 device private

 *

 * The cmd parser maintains a simple increasing integer version number suitable

 * for passing to userspace clients to determine what operations are permitted.

 *

 * Return: the current version number of the cmd parser

 If the command parser is not enabled, report 0 - unsupported */

	/*

	 * Command parser version history

	 *

	 * 1. Initial version. Checks batches and reports violations, but leaves

	 *    hardware parsing enabled (so does not allow new use cases).

	 * 2. Allow access to the MI_PREDICATE_SRC0 and

	 *    MI_PREDICATE_SRC1 registers.

	 * 3. Allow access to the GPGPU_THREADS_DISPATCHED register.

	 * 4. L3 atomic chicken bits of HSW_SCRATCH1 and HSW_ROW_CHICKEN3.

	 * 5. GPGPU dispatch compute indirect registers.

	 * 6. TIMESTAMP register and Haswell CS GPR registers

	 * 7. Allow MI_LOAD_REGISTER_REG between whitelisted registers.

	 * 8. Don't report cmd_check() failures as EINVAL errors to userspace;

	 *    rely on the HW to NOOP disallowed commands as it would without

	 *    the parser enabled.

	 * 9. Don't whitelist or handle oacontrol specially, as ownership

	 *    for oacontrol state is moving to i915-perf.

	 * 10. Support for Gen9 BCS Parsing

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2008-2015 Intel Corporation

 Consider only shrinkable ojects. */

	/*

	 * We can only return physical pages to the system if we can either

	 * discard the contents (because the user has marked them as being

	 * purgeable) or if we can move their contents out to swap.

/**

 * i915_gem_shrink - Shrink buffer object caches

 * @ww: i915 gem ww acquire ctx, or NULL

 * @i915: i915 device

 * @target: amount of memory to make available, in pages

 * @nr_scanned: optional output for number of pages scanned (incremental)

 * @shrink: control flags for selecting cache types

 *

 * This function is the main interface to the shrinker. It will try to release

 * up to @target pages of main memory backing storage from buffer objects.

 * Selection of the specific caches can be done with @flags. This is e.g. useful

 * when purgeable objects should be removed from caches preferentially.

 *

 * Note that it's not guaranteed that released amount is actually available as

 * free system memory - the pages might still be in-used to due to other reasons

 * (like cpu mmaps) or the mm core has reused them before we could grab them.

 * Therefore code that needs to explicitly shrink buffer objects caches (e.g. to

 * avoid deadlocks in memory reclaim) must fall back to i915_gem_shrink_all().

 *

 * Also note that any kind of pinning (both per-vma address space pins and

 * backing storage pins at the buffer object level) result in the shrinker code

 * having to skip the object.

 *

 * Returns:

 * The number of pages of backing storage actually released.

 CHV + VTD workaround use stop_machine(); need to trylock vm->mutex */

	/*

	 * Unbinding of objects will require HW access; Let us not wake the

	 * device just to recover a little memory. If absolutely necessary,

	 * we will force the wake during oom-notifier.

	/*

	 * When shrinking the active list, we should also consider active

	 * contexts. Active contexts are pinned until they are retired, and

	 * so can not be simply unbound to retire and unpin their pages. To

	 * shrink the contexts, we must wait until the gpu is idle and

	 * completed its switch to the kernel context. In short, we do

	 * not have a good mechanism for idling a specific context, but

	 * what we can do is give them a kick so that we do not keep idle

	 * contexts around longer than is necessary.

 Retire requests to unpin all idle contexts */

	/*

	 * As we may completely rewrite the (un)bound list whilst unbinding

	 * (due to retiring requests) we have to strictly process only

	 * one element of the list at the time, and recheck the list

	 * on every iteration.

	 *

	 * In particular, we must hold a reference whilst removing the

	 * object as we may end up waiting for and/or retiring the objects.

	 * This might release the final reference (held by the active list)

	 * and result in the object being freed from under us. This is

	 * similar to the precautions the eviction code must take whilst

	 * removing objects.

	 *

	 * Also note that although these lists do not hold a reference to

	 * the object we can safely grab one here: The final object

	 * unreferencing and the bound_list are both protected by the

	 * dev->struct_mutex and so we won't ever be able to observe an

	 * object on the bound_list with a reference count equals 0.

		/*

		 * We serialize our access to unreferenced objects through

		 * the use of the struct_mutex. While the objects are not

		 * yet freed (due to RCU then a workqueue) we still want

		 * to be able to shrink their pages, so they remain on

		 * the unbound/bound list until actually freed.

 May arrive from get_pages on another bo */

/**

 * i915_gem_shrink_all - Shrink buffer object caches completely

 * @i915: i915 device

 *

 * This is a simple wraper around i915_gem_shrink() to aggressively shrink all

 * caches completely. It also first waits for and retires all outstanding

 * requests to also be able to release backing storage for active objects.

 *

 * This should only be used in code to intentionally quiescent the gpu or as a

 * last-ditch effort when memory seems to have run out.

 *

 * Returns:

 * The number of pages of backing storage actually released.

	/*

	 * Update our preferred vmscan batch size for the next pass.

	 * Our rough guess for an effective batch size is roughly 2

	 * available GEM objects worth of pages. That is we don't want

	 * the shrinker to fire, until it is worth the cost of freeing an

	 * entire GEM object.

 default SHRINK_BATCH */);

	/* Because we may be allocating inside our own driver, we cannot

	 * assert that there are no objects with pinned pages that are not

	 * being pointed to by hardware.

 We also want to clear any cached iomaps as they wrap vmap */

	/*

	 * We can only be called while the pages are pinned or when

	 * the pages are released. If pinned, we should only be called

	 * from a single caller under controlled conditions; and on release

	 * only one caller may release us. Neither the two may cross.

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2008 Intel Corporation

/**

 * DOC: buffer object tiling

 *

 * i915_gem_set_tiling_ioctl() and i915_gem_get_tiling_ioctl() is the userspace

 * interface to declare fence register requirements.

 *

 * In principle GEM doesn't care at all about the internal data layout of an

 * object, and hence it also doesn't care about tiling or swizzling. There's two

 * exceptions:

 *

 * - For X and Y tiling the hardware provides detilers for CPU access, so called

 *   fences. Since there's only a limited amount of them the kernel must manage

 *   these, and therefore userspace must tell the kernel the object tiling if it

 *   wants to use fences for detiling.

 * - On gen3 and gen4 platforms have a swizzling pattern for tiled objects which

 *   depends upon the physical page frame number. When swapping such objects the

 *   page frame number might change and the kernel must be able to fix this up

 *   and hence now the tiling. Note that on a subset of platforms with

 *   asymmetric memory channel population the swizzling pattern changes in an

 *   unknown way, and for those the kernel simply forbids swapping completely.

 *

 * Since neither of this applies for new tiling layouts on modern platforms like

 * W, Ys and Yf tiling GEM only allows object tiling to be set to X or Y tiled.

 * Anything else can be handled in userspace entirely without the kernel's

 * invovlement.

/**

 * i915_gem_fence_size - required global GTT size for a fence

 * @i915: i915 device

 * @size: object size

 * @tiling: tiling mode

 * @stride: tiling stride

 *

 * Return the required global GTT size for a fence (view of a tiled object),

 * taking into account potential fence register mapping.

 Previous chips need a power-of-two fence region when tiling */

/**

 * i915_gem_fence_alignment - required global GTT alignment for a fence

 * @i915: i915 device

 * @size: object size

 * @tiling: tiling mode

 * @stride: tiling stride

 *

 * Return the required global GTT alignment for a fence (a view of a tiled

 * object), taking into account potential fence register mapping.

	/*

	 * Minimum alignment is 4k (GTT page size), but might be greater

	 * if a fence register is needed for the object.

	/*

	 * Previous chips need to be aligned to the size of the smallest

	 * fence register that can contain the object.

 Check pitch constriants for all chips & tiling formats */

 Linear is always fine */

 check maximum stride & object size */

	/* i965+ stores the end address of the gtt mapping in the fence

 Make the current GTT allocation valid for the change in tiling. */

 Restore the remaining vma on an error */

 Make sure we don't cross-contaminate obj->tiling_and_stride */

	/* We need to rebind the object if its current allocation

	 * no longer meets the alignment restrictions for its new

	 * tiling mode. Otherwise we can just leave it alone, but

	 * need to ensure that any fence register is updated before

	 * the next fenced (either through the GTT or by the BLT unit

	 * on older GPUs) access.

	 *

	 * After updating the tiling parameters, we then flag whether

	 * we need to update an associated fence register. Note this

	 * has to also include the unfenced register the GPU uses

	 * whilst executing a fenced command for an untiled object.

	/* If the memory has unknown (i.e. varying) swizzling, we pin the

	 * pages to prevent them being swapped out and causing corruption

	 * due to the change in swizzling.

 Force the fence to be reacquired for GTT access */

 Try to preallocate memory required to save swizzling on put-pages */

/**

 * i915_gem_set_tiling_ioctl - IOCTL handler to set tiling mode

 * @dev: DRM device

 * @data: data pointer for the ioctl

 * @file: DRM file for the ioctl call

 *

 * Sets the tiling mode of an object, returning the required swizzling of

 * bit 6 of addresses in the object.

 *

 * Called by the user via ioctl.

 *

 * Returns:

 * Zero on success, negative errno on failure.

	/*

	 * The tiling mode of proxy objects is handled by its generator, and

	 * not allowed to be changed by userspace.

		/* Hide bit 17 swizzling from the user.  This prevents old Mesa

		 * from aborting the application on sw fallbacks to bit 17,

		 * and we use the pread/pwrite bit17 paths to swizzle for it.

		 * If there was a user that was relying on the swizzle

		 * information for drm_intel_bo_map()ed reads/writes this would

		 * break it, but we don't have any of those.

 If we can't handle the swizzling, make it untiled. */

 We have to maintain this existing ABI... */

/**

 * i915_gem_get_tiling_ioctl - IOCTL handler to get tiling mode

 * @dev: DRM device

 * @data: data pointer for the ioctl

 * @file: DRM file for the ioctl call

 *

 * Returns the current tiling mode and required bit 6 swizzling for the object.

 *

 * Called by the user via ioctl.

 *

 * Returns:

 * Zero on success, negative errno on failure.

 Hide bit 17 from the user -- see comment in i915_gem_set_tiling */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2016 Intel Corporation

		/*

		 * If both shared fences and an exclusive fence exist,

		 * then by construction the shared fences must be later

		 * than the exclusive fence. If we successfully wait for

		 * all the shared fences, we know that the exclusive fence

		 * must all be signaled. If all the shared fences are

		 * signaled, we can prune the array and recover the

		 * floating references on the fences/requests.

	/*

	 * Opportunistically prune the fences iff we know they have *all* been

	 * signaled.

 RCU serialisation for set-wedged protection */

 Recurse once into a fence-array */

 The chain is ordered; if we boost the last, we boost all */

 kick the tasklets if queues were reprioritised */

/**

 * Waits for rendering to the object to be completed

 * @obj: i915 gem object

 * @flags: how to wait (under a lock, for all rendering or just for writes etc)

 * @timeout: how long to wait

 nsecs_to_jiffies64() does not guard against overflow */

/**

 * i915_gem_wait_ioctl - implements DRM_IOCTL_I915_GEM_WAIT

 * @dev: drm device pointer

 * @data: ioctl data blob

 * @file: drm file pointer

 *

 * Returns 0 if successful, else an error is returned with the remaining time in

 * the timeout parameter.

 *  -ETIME: object is still busy after timeout

 *  -ERESTARTSYS: signal interrupted the wait

 *  -ENONENT: object doesn't exist

 * Also possible, but rare:

 *  -EAGAIN: incomplete, restart syscall

 *  -ENOMEM: damn

 *  -ENODEV: Internal IRQ fail

 *  -E?: The add request failed

 *

 * The wait ioctl with a timeout of 0 reimplements the busy ioctl. With any

 * non-zero timeout parameter the wait ioctl will wait for the given number of

 * nanoseconds on an object becoming unbusy. Since the wait itself does so

 * without holding struct_mutex the object may become re-busied before this

 * function completes. A similar but shorter * race condition exists in the busy

 * ioctl

		/*

		 * Apparently ktime isn't accurate enough and occasionally has a

		 * bit of mismatch in the jiffies<->nsecs<->ktime loop. So patch

		 * things up to make the test happy. We allow up to 1 jiffy.

		 *

		 * This is a regression from the timespec->ktime conversion.

 Asked to wait beyond the jiffie/scheduler precision? */

/**

 * i915_gem_object_wait_migration - Sync an accelerated migration operation

 * @obj: The migrating object.

 * @flags: waiting flags. Currently supports only I915_WAIT_INTERRUPTIBLE.

 *

 * Wait for any pending async migration operation on the object,

 * whether it's explicitly (i915_gem_object_migrate()) or implicitly

 * (swapin, initial clearing) initiated.

 *

 * Return: 0 if successful, -ERESTARTSYS if a signal was hit during waiting.

 NOP for now. */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2019 Intel Corporation

	/*

	 * We have to flush all the executing contexts to main memory so

	 * that they can saved in the hibernation image. To ensure the last

	 * context image is coherent, we have to switch away from it. That

	 * leaves the i915->kernel_context still active when

	 * we actually suspend, and its image in memory may not match the GPU

	 * state. Fortunately, the kernel_context is disposable and we do

	 * not rely on its state.

 Opportunistically try to evict unpinned objects */

	/*

	 * More objects may have become unpinned as requests were

	 * retired. Now try to evict again. The gt may be wedged here

	 * in which case we automatically fall back to memcpy.

	 * We allow also backing up pinned objects that have not been

	 * marked for early recover, and that may contain, for example,

	 * page-tables for the migrate context.

	/*

	 * Remaining objects are backed up using memcpy once we've stopped

	 * using the migrate context.

	/*

	 * Neither the BIOS, ourselves or any other kernel

	 * expects the system to be in execlists mode on startup,

	 * so we need to reset the GPU back to legacy mode. And the only

	 * known way to disable logical contexts is through a GPU reset.

	 *

	 * So in order to leave the system in a known default configuration,

	 * always reset the GPU upon unload and suspend. Afterwards we then

	 * clean up the GEM state tracking, flushing off the requests and

	 * leaving the system in a known idle state.

	 *

	 * Note that is of the upmost importance that the GPU is idle and

	 * all stray writes are flushed *before* we dismantle the backing

	 * storage for the pinned objects.

	 *

	 * However, since we are uncertain that resetting the GPU on older

	 * machines is a good idea, we don't - just in case it leaves the

	 * machine in an unusable condition.

 presume auto-hibernate */

	/* Discard all purgeable objects, let userspace recover those as

	 * required after resuming.

	/*

	 * Called just before we write the hibernation image.

	 *

	 * We need to update the domain tracking to reflect that the CPU

	 * will be accessing all the pages to create and restore from the

	 * hibernation, and so upon restoration those pages will be in the

	 * CPU domain.

	 *

	 * To make sure the hibernation image contains the latest state,

	 * we update that state just before writing out the image.

	 *

	 * To try and reduce the hibernation image, we manually shrink

	 * the objects as well, see i915_gem_freeze()

	/*

	 * As we didn't flush the kernel context before suspend, we cannot

	 * guarantee that the context image is complete. So let's just reset

	 * it and start again.

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2011-2012 Intel Corporation

/*

 * This file implements HW context support. On gen5+ a HW context consists of an

 * opaque GPU object which is referenced at times of context saves and restores.

 * With RC6 enabled, the context is also referenced as the GPU enters and exists

 * from RC6 (GPU has it's own internal power context, except on gen5). Though

 * something like a context does exist for the media ring, the code only

 * supports contexts for the render ring.

 *

 * In software, there is a distinction between contexts created by the user,

 * and the default HW context. The default HW context is used by GPU clients

 * that do not request setup of their own hardware context. The default

 * context's state is never restored to help prevent programming errors. This

 * would happen if a client ran and piggy-backed off another clients GPU state.

 * The default context only exists to give the GPU some offset to load as the

 * current to invoke a save of the context we actually care about. In fact, the

 * code could likely be constructed, albeit in a more complicated fashion, to

 * never use the default context, though that limits the driver's ability to

 * swap out, and/or destroy other contexts.

 *

 * All other contexts are created as a request by the GPU client. These contexts

 * store GPU state, and thus allow GPU clients to not re-emit state (and

 * potentially query certain state) at any time. The kernel driver makes

 * certain that the appropriate commands are inserted.

 *

 * The context life cycle is semi-complicated in that context BOs may live

 * longer than the context itself because of the way the hardware, and object

 * tracking works. Below is a very crude representation of the state machine

 * describing the context life.

 *                                         refcount     pincount     active

 * S0: initial state                          0            0           0

 * S1: context created                        1            0           0

 * S2: context is currently running           2            1           X

 * S3: GPU referenced, but not current        2            0           1

 * S4: context is current, but destroyed      1            1           0

 * S5: like S3, but destroyed                 1            0           1

 *

 * The most common (but not all) transitions:

 * S0->S1: client creates a context

 * S1->S2: client submits execbuf with context

 * S2->S3: other clients submits execbuf with context

 * S3->S1: context object was retired

 * S3->S2: clients submits another execbuf

 * S2->S4: context destroy called with current context

 * S3->S5->S0: destroy path

 * S4->S5->S0: destroy path on current context

 *

 * There are two confusing terms used above:

 *  The "current context" means the context which is currently running on the

 *  GPU. The GPU has loaded its state already and has stored away the gtt

 *  offset of the BO. The GPU is not actively referencing the data at this

 *  offset, but it will on the next context switch. The only way to avoid this

 *  is to do a GPU reset.

 *

 *  An "active context' is one which was previously the "current context" and is

 *  on the active list waiting for the next context switch to occur. Until this

 *  happens, the object must remain at the same gtt offset. It is therefore

 *  possible to destroy a context, but it is still active.

 *

		/*

		 * Only contexts that are short-lived [that will expire or be

		 * reset] are allowed to survive past termination. We require

		 * hangcheck to ensure that the persistent requests are healthy.

 To cancel a context we use "preempt-to-idle" */

		/*

		 * If the cancel fails, we then need to reset, cleanly!

		 *

		 * If the per-engine reset fails, all hope is lost! We resort

		 * to a full GPU reset in that unlikely case, but realistically

		 * if the engine could not reset, the full reset does not fare

		 * much better. The damage has been done.

		 *

		 * However, if we cannot reset an engine by itself, we cannot

		 * cleanup a hanging persistent context without causing

		 * colateral damage, and we should not pretend we can by

		 * exposing the interface.

		/*

		 * protected context usage requires the PXP session to be up,

		 * which in turn requires the device to be active.

 FIXME: This is NIY for execlists */

 Create contexts / engines */

 RING_MASK has no shift so we can use it directly here */

 Only render engine supports RPCS configuration. */

 Only render engine supports RPCS configuration. */

 There is only one render engine */

 A valid SSEU has no zero fields */

		/*

		 * XXX: Must be done after calling intel_context_set_gem as that

		 * function changes the ring size. The ring is allocated when

		 * the context is pinned. If the ring size is changed after

		 * allocation we have a mismatch of the ring size and will cause

		 * the context to hang. Presumably with a bit of reordering we

		 * could move the perma-pin step to the backend function

		 * intel_engine_create_parallel.

	/*

	 * Send a "high priority pulse" down the engine to cause the

	 * current request to be momentarily preempted. (If it fails to

	 * be preempted, it will be reset). As we have marked our context

	 * as banned, any incomplete request, including any running, will

	 * be skipped following the preemption.

	 *

	 * If there is no hangchecking (one of the reasons why we try to

	 * cancel the context) and no forced preemption, there may be no

	 * means by which we reset the GPU and evict the persistent hog.

	 * Ergo if we are unable to inject a preemptive pulse that can

	 * kill the banned context, we fallback to doing a local reset

	 * instead.

	/*

	 * rq->link is only SLAB_TYPESAFE_BY_RCU, we need to hold a reference

	 * to the request to prevent it being transferred to a new timeline

	 * (and onto a new timeline->requests list).

 timeline is already completed upto this point? */

 Check with the backend if the request is inflight */

	/*

	 * Map the user's engine back to the actual engines; one virtual

	 * engine will be mapped to multiple engines, and using ctx->engine[]

	 * the same engine may be have multiple instances in the user's map.

	 * However, we only care about pending requests, so only include

	 * engines on which there are incomplete requests.

		/*

		 * Check the current active state of this context; if we

		 * are currently executing on the GPU we need to evict

		 * ourselves. On the other hand, if we haven't yet been

		 * submitted to the GPU or if everything is complete,

		 * we have nothing to do.

 First attempt to gracefully cancel the context */

			/*

			 * If we are unable to send a preemptive pulse to bump

			 * the context from the GPU, we have to resort to a full

			 * reset. We hope the collateral damage is worth it.

 decouple from FENCE_COMPLETE */

 serialises with execbuf */

 Wait until context is finally scheduled out and retired */

 raced, already closed */

 Replace '[]' with '<>' to indicate closed in debug prints */

 Flush any concurrent set_engines() */

		/* i915_vm_close drops the final reference, which is a bit too

		 * early and could result in surprises with concurrent

		 * operations racing with thist ctx close. Keep a full reference

		 * until the end.

	/*

	 * The LUT uses the VMA as a backpointer to unref the object,

	 * so we need to clear the LUT before we close all the VMA (inside

	 * the ppgtt).

	/*

	 * If the user has disabled hangchecking, we can not be sure that

	 * the batches will ever complete after the context is closed,

	 * keeping the context and all resources pinned forever. So in this

	 * case we opt to forcibly kill off all remaining requests on

	 * context close.

		/*

		 * Only contexts that are short-lived [that will expire or be

		 * reset] are allowed to survive past termination. We require

		 * hangcheck to ensure that the persistent requests are healthy.

 To cancel a context we use "preempt-to-idle" */

		/*

		 * If the cancel fails, we then need to reset, cleanly!

		 *

		 * If the per-engine reset fails, all hope is lost! We resort

		 * to a full GPU reset in that unlikely case, but realistically

		 * if the engine could not reset, the full reset does not fare

		 * much better. The damage has been done.

		 *

		 * However, if we cannot reset an engine by itself, we cannot

		 * cleanup a hanging persistent context without causing

		 * colateral damage, and we should not pretend we can by

		 * exposing the interface.

 i915_vm_open() takes a reference */

	/* NB: Mark all slices as needing a remap so that when the context first

	 * loads it will restore whatever remap state already exists. If there

 And finally expose ourselves to userspace via the idr */

 0 reserved for the default context */

 0 reserved for invalid/unassigned ppgtt */

 reserved for invalid/unassigned ppgtt */

 reserved for invalid/unassigned ppgtt */

 No zeros in any field. */

 Max > min. */

	/*

	 * Some future proofing on the types since the uAPI is wider than the

	 * current internal implementation.

 Check validity against hardware. */

 Part specific restrictions. */

		/*

		 * Only full subslice enablement is possible if more than one

		 * slice is turned on.

		/*

		 * If more than four (SScount bitfield limit) subslices are

		 * requested then the number has to be even.

		/*

		 * If only one slice is enabled and subslice count is below the

		 * device full enablement, it must be at most half of the all

		 * available subslices.

 ABI restriction - VME use case only. */

 All slices or one slice only. */

		/*

		 * Half subslices or full enablement only when one slice is

		 * enabled.

 No EU configuration changes. */

 Only render engine supports RPCS configuration. */

 can't clear this for protected contexts */

 can't set this for protected contexts */

 One for the xarray and one for the caller */

 Try one more time under the lock */

 Get ourselves a context ID */

	/* We need to hold the proto-context lock here to prevent races

	 * with finalize_create_context_locked().

 serialises with set_sseu */

			/* Contexts should be finalized inside

			 * GEM_CONTEXT_CREATE starting with graphics

			 * version 13.

	/*

	 * We opt for unserialised reads here. This may result in tearing

	 * in the extremely unlikely event of a GPU hang on this context

	 * as we are querying them. If we need that extra layer of protection,

	 * we should wrap the hangstats with a seqlock.

 GEM context-engines iterator: for_each_gem_engine() */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2008,2010 Intel Corporation

* This vma's place in the execbuf reservation list */

 choose one of the above! */

 __EXEC_OBJECT_NO_RESERVE is BIT(31), defined in i915_vma.h */

 all of the above + */

 Catch emission of unexpected errors for CI! */

/**

 * DOC: User command execution

 *

 * Userspace submits commands to be executed on the GPU as an instruction

 * stream within a GEM object we call a batchbuffer. This instructions may

 * refer to other GEM objects containing auxiliary state such as kernels,

 * samplers, render targets and even secondary batchbuffers. Userspace does

 * not know where in the GPU memory these objects reside and so before the

 * batchbuffer is passed to the GPU for execution, those addresses in the

 * batchbuffer and auxiliary objects are updated. This is known as relocation,

 * or patching. To try and avoid having to relocate each object on the next

 * execution, userspace is told the location of those objects in this pass,

 * but this remains just a hint as the kernel may choose a new location for

 * any object in the future.

 *

 * At the level of talking to the hardware, submitting a batchbuffer for the

 * GPU to execute is to add content to a buffer from which the HW

 * command streamer is reading.

 *

 * 1. Add a command to load the HW context. For Logical Ring Contexts, i.e.

 *    Execlists, this command is not placed on the same buffer as the

 *    remaining items.

 *

 * 2. Add a command to invalidate caches to the buffer.

 *

 * 3. Add a batchbuffer start command to the buffer; the start command is

 *    essentially a token together with the GPU address of the batchbuffer

 *    to be executed.

 *

 * 4. Add a pipeline flush to the buffer.

 *

 * 5. Add a memory write command to the buffer to record when the GPU

 *    is done executing the batchbuffer. The memory write writes the

 *    global sequence number of the request, ``i915_request::global_seqno``;

 *    the i915 driver uses the current value in the register to determine

 *    if the GPU has completed the batchbuffer.

 *

 * 6. Add a user interrupt command to the buffer. This command instructs

 *    the GPU to issue an interrupt when the command, pipeline flush and

 *    memory write are completed.

 *

 * 7. Inform the hardware of the additional commands added to the buffer

 *    (by updating the tail pointer).

 *

 * Processing an execbuf ioctl is conceptually split up into a few phases.

 *

 * 1. Validation - Ensure all the pointers, handles and flags are valid.

 * 2. Reservation - Assign GPU address space for every object

 * 3. Relocation - Update any addresses to point to the final locations

 * 4. Serialisation - Order the request with respect to its dependencies

 * 5. Construction - Construct a request to execute the batchbuffer

 * 6. Submission (at some point in the future execution)

 *

 * Reserving resources for the execbuf is the most complicated phase. We

 * neither want to have to migrate the object in the address space, nor do

 * we want to have to update any relocations pointing to this object. Ideally,

 * we want to leave the object where it is and for all the existing relocations

 * to match. If the object is given a new address, or if userspace thinks the

 * object is elsewhere, we have to parse all the relocation entries and update

 * the addresses. Userspace can set the I915_EXEC_NORELOC flag to hint that

 * all the target addresses in all of its objects match the value in the

 * relocation entries and that they all match the presumed offsets given by the

 * list of execbuffer objects. Using this knowledge, we know that if we haven't

 * moved any buffers, all the relocation entries are valid and we can skip

 * the update. (If userspace is wrong, the likely outcome is an impromptu GPU

 * hang.) The requirement for using I915_EXEC_NO_RELOC are:

 *

 *      The addresses written in the objects must match the corresponding

 *      reloc.presumed_offset which in turn must match the corresponding

 *      execobject.offset.

 *

 *      Any render targets written to in the batch must be flagged with

 *      EXEC_OBJECT_WRITE.

 *

 *      To avoid stalling, execobject.offset should match the current

 *      address of that object within the active context.

 *

 * The reservation is done is multiple phases. First we try and keep any

 * object already bound in its current location - so as long as meets the

 * constraints imposed by the new execbuffer. Any object left unbound after the

 * first pass is then fitted into any available idle space. If an object does

 * not fit, all objects are removed from the reservation and the process rerun

 * after sorting the objects into a priority order (more difficult to fit

 * objects are tried first). Failing that, the entire VM is cleared and we try

 * to fit the execbuf once last time before concluding that it simply will not

 * fit.

 *

 * A small complication to all of this is that we allow userspace not only to

 * specify an alignment and a size for the object in the address space, but

 * we also allow userspace to specify the exact offset. This objects are

 * simpler to place (the location is known a priori) all we have to do is make

 * sure the space is available.

 *

 * Once all the objects are in place, patching up the buried pointers to point

 * to the final locations is a fairly simple job of walking over the relocation

 * entry arrays, looking up the right address and rewriting the value into

 * the object. Simple! ... The relocation entries are stored in user memory

 * and so to access them we have to copy them into a local buffer. That copy

 * has to avoid taking any pagefaults as they may lead back to a GEM object

 * requiring the struct_mutex (i.e. recursive deadlock). So once again we split

 * the relocation into multiple passes. First we try to do everything within an

 * atomic context (avoid the pagefaults) which requires that we never wait. If

 * we detect that we may wait, or if we need to fault, then we have to fallback

 * to a slower path. The slowpath has to drop the mutex. (Can you hear alarm

 * bells yet?) Dropping the mutex means that we lose all the state we have

 * built up so far for the execbuf and we must reset any global data. However,

 * we do leave the objects pinned in their final locations - which is a

 * potential issue for concurrent execbufs. Once we have left the mutex, we can

 * allocate and copy all the relocation entries into a large array at our

 * leisure, reacquire the mutex, reclaim all the objects and other state and

 * then proceed to update any incorrect addresses with the objects.

 *

 * As we process the relocation entries, we maintain a record of whether the

 * object is being written to. Using NORELOC, we expect userspace to provide

 * this information instead. We also check whether we can skip the relocation

 * by comparing the expected value inside the relocation entry with the target's

 * final address. If they differ, we have to map the current object and rewrite

 * the 4 or 8 byte pointer within.

 *

 * Serialising an execbuf is quite simple according to the rules of the GEM

 * ABI. Execution within each context is ordered by the order of submission.

 * Writes to any GEM object are in order of submission and are exclusive. Reads

 * from a GEM object are unordered with respect to other reads, but ordered by

 * writes. A write submitted after a read cannot occur before the read, and

 * similarly any read submitted after a write cannot occur before the write.

 * Writes are ordered between engines such that only one write occurs at any

 * time (completing any reads beforehand) - using semaphores where available

 * and CPU serialisation otherwise. Other GEM access obey the same rules, any

 * write (either via mmaps using set-domain, or via pwrite) must flush all GPU

 * reads before starting, and any read (either using set-domain or pread) must

 * flush all GPU writes before starting. (Note we only employ a barrier before,

 * we currently rely on userspace not concurrently starting a new execution

 * whilst reading or writing to an object. This may be an advantage or not

 * depending on how much you trust userspace not to shoot themselves in the

 * foot.) Serialisation may just result in the request being inserted into

 * a DAG awaiting its turn, but most simple is to wait on the CPU until

 * all dependencies are resolved.

 *

 * After all of that, is just a matter of closing the request and handing it to

 * the hardware (well, leaving it in a queue to be executed). However, we also

 * offer the ability for batchbuffers to be run with elevated privileges so

 * that they access otherwise hidden registers. (Used to adjust L3 cache etc.)

 * Before any batch is given extra privileges we first must check that it

 * contains no nefarious instructions, we check that each instruction is from

 * our whitelist and all registers are also from an allowed list. We first

 * copy the user's batchbuffer to a shadow (so that the user doesn't have

 * access to it, either by the CPU or GPU as we scan it) and then parse each

 * instruction. If everything is ok, we set a flag telling the hardware to run

 * the batchbuffer in trusted mode, otherwise the ioctl is rejected.

 Use with ptr_mask_bits() */

* i915 backpointer */

* per-file lookup tables and limits */

* ioctl parameters */

* ioctl execobj[] */

 gt for the execbuf */

 logical state for the request */

* caller's context */

* our requests to build */

* identity of the batch obj/vma */

* trampoline used for chaining */

* used for excl fence in dma_resv objects when > 1 BB submitted */

* actual size of execobj[] as we may extend it for the cmdparser */

 number of batches in execbuf IOCTL */

* list of vma not yet bound during reservation phase */

* list of vma that have execobj.relocation_count */

	/**

	 * Track the most recently used object for relocations, as we

	 * frequently have to perform multiple relocations within the same

	 * obj/page

* temporary GTT binding */

* Current kmap address */

* Currently mapped page index */

* Cached value of GRAPHICS_VER */

* Set of execobj.flags that are invalid */

* Length of batch within object */

* Location within object of batch */

* Flags composed for emit_bb_start() */

* pool node for batch buffer */

	/**

	 * Indicate either the size of the hastable used to resolve

	 * relocation handles, or if negative that we are using a direct

	 * index into the execobj[].

* ht for relocation handles */

		/*

		 * Without a 1:1 association between relocation handles and

		 * the execobject[] index, we instead create a hashtable.

		 * We size it dynamically based on available memory, starting

		 * first with 1:1 assocative hash and scaling back until

		 * the allocation succeeds.

		 *

		 * Later on we use a positive lut_size to indicate we are

		 * using this hashtable, and a negative value to indicate a

		 * direct lookup.

			/* While we can still reduce the allocation size, don't

			 * raise a warning and allow the allocation to fail.

			 * On the last pass though, we want to try as hard

			 * as possible to perform the allocation and warn

			 * if it fails.

	/*

	 * Wa32bitGeneralStateOffset & Wa32bitInstructionBaseOffset,

	 * limit address to the first 4GBs for unflagged objects.

 Attempt to reuse the current location if available */

 Failing that pick any _free_ space if suitable */

	/* Relocations are disallowed for all platforms after TGL-LP.  This

	 * also covers all platforms with local memory.

	/*

	 * Offset can be used as input (EXEC_OBJECT_PINNED), reject

	 * any non-page-aligned or non-canonical addresses.

 pad_to_size was once a reserved field, so sanitize it */

	/*

	 * From drm_mm perspective address space is continuous,

	 * so from this point we're always using non-canonical

	 * form internally.

	/*

	 * SNA is doing fancy tricks with compressing batch buffers, which leads

	 * to negative relocation deltas. Usually that works out ok since the

	 * relocate address is still positive, except when the batch is placed

	 * very low in the GTT. Ensure this doesn't happen.

	 *

	 * Note that actual hangs have only been observed on gen7, but for

	 * paranoia do it everywhere.

 impossible! */

	/*

	 * Attempt to pin all of the buffers into the GTT.

	 * This is done in 3 phases:

	 *

	 * 1a. Unbind all objects that do not match the GTT constraints for

	 *     the execbuffer (fenceable, mappable, alignment etc).

	 * 1b. Increment pin count for already bound objects.

	 * 2.  Bind new objects.

	 * 3.  Decrement pin count.

	 *

	 * This avoid unnecessary unbinding of later objects in order to make

	 * room for the earlier objects *unless* we need to defragment.

 Resort *all* the objects into priority order */

 Pinned must have their slot */

 Map require the lowest 256MiB (aperture) */

 Prioritise 4GiB region for restricted bo */

 Too fragmented, unbind everything and retry */

 Check that the context hasn't been closed in the meantime */

 And nor has this handle */

		/*

		 * If the user has opted-in for protected-object tracking, make

		 * sure the object encryption can be used.

		 * We only need to do this when the object is first used with

		 * this context, because the context itself will be banned when

		 * the protected objects become invalid.

					/*

					 * Execbuffer code expects last vma entry to be NULL,

					 * since we already initialized this entry,

					 * set the next value to NULL or we mess up

					 * cleanup handling.

 Must be a variable in the struct to allow GCC to unroll. */

 after CLFLUSH_FLAGS */

 NOWARN */ |

 no inactive aperture space, use cpu reloc */

		/*

		 * Writes to the same cacheline are serialised by the CPU

		 * (including clflush). On the write path, we only require

		 * that it hits memory in an orderly fashion and place

		 * mb barriers at the start and end of the relocation phase

		 * to ensure ordering of clflush wrt to the system.

 we've already hold a reference to all valid objects */

 Validate that the target is in a valid r/w GPU domain */

		/*

		 * Sandybridge PPGTT errata: We need a global gtt mapping

		 * for MI and pipe_control writes because the gpu doesn't

		 * properly redirect them through the ppgtt for non_secure

		 * batchbuffers.

	/*

	 * If the relocation already has the right value in it, no

	 * more work needs to be done.

 Check that the relocation address is valid... */

	/*

	 * If we write into the object, we need to force the synchronisation

	 * barrier, either with an asynchronous clflush or if we executed the

	 * patching using the GPU (though that should be serialised by the

	 * timeline). To be completely sure, and since we are required to

	 * do relocations we are already stalling, disable the user's opt

	 * out of our synchronisation.

 and update the user's relocation entry */

	/*

	 * We must check that the entire relocation array is safe

	 * to read. However, if the array is not writable the user loses

	 * the updated relocation values.

		/*

		 * This is the fast path and we cannot handle a pagefault

		 * whilst holding the struct mutex lest the user pass in the

		 * relocations contained within a mmaped bo. For in such a case

		 * we, the page fault handler would call i915_gem_fault() and

		 * we would try to acquire the struct mutex again. Obviously

		 * this is bad and so lockdep complains vehemently.

				/*

				 * Note that reporting an error now

				 * leaves everything in an inconsistent

				 * state as we have *already* changed

				 * the relocation value inside the

				 * object. As we have not changed the

				 * reloc.presumed_offset or will not

				 * change the execobject.offset, on the

				 * call we may not rewrite the value

				 * inside the object, leaving it

				 * dangling and causing a GPU hang. Unless

				 * userspace dynamically rebuilds the

				 * relocations on each execbuf rather than

				 * presume a static tree.

				 *

				 * We did previously check if the relocations

				 * were writable (access_ok), an error now

				 * would be a strange race with mprotect,

				 * having already demonstrated that we

				 * can read from this userspace address.

 copy_from_user is limited to < 4GiB */

		/*

		 * As we do not update the known relocation offsets after

		 * relocating (due to the complexities in lock handling),

		 * we need to mark them as invalid now so that we force the

		 * relocation processing next time. Just in case the target

		 * object is evicted and then rebound into its old

		 * presumed_offset before the next execbuffer - if that

		 * happened we would make the mistake of assuming that the

		 * relocations were valid.

 We may process another execbuffer during the unlock... */

	/*

	 * We take 3 passes through the slowpatch.

	 *

	 * 1 - we try to just prefault all the user relocation entries and

	 * then attempt to reuse the atomic pagefault disabled fast path again.

	 *

	 * 2 - we copy the user entries to a local buffer here outside of the

	 * local and allow ourselves to wait upon any rendering before

	 * relocations

	 *

	 * 3 - we already have a local copy of the relocation entries, but

	 * were interrupted (EAGAIN) whilst waiting for the objects, try again.

 reacquire the objects */

 as last step, parse the command buffer */

	/*

	 * Leave the user relocations as are, this is the painfully slow path,

	 * and we want to avoid the complication of dropping the lock whilst

	 * having buffers reserved in the aperture and so causing spurious

	 * ENOSPC for random operations.

 only throttle once, even if we didn't need to throttle */

 The objects are in their final locations, apply the relocations. */

		/*

		 * If the user expects the execobject.offset and

		 * reloc.presumed_offset to be an exact match,

		 * as for using NO_RELOC, then we cannot update

		 * the execobject.offset until we have completed

		 * relocation.

/*

 * Using two helper loops for the order of which requests / batches are created

 * and added the to backend. Requests are created in order from the parent to

 * the last child. Requests are added in the reverse order, from the last child

 * to parent. This is done for locking reasons as the timeline lock is acquired

 * during request creation and released when the request is added to the

 * backend. To make lockdep happy (see intel_context_timeline_lock) this must be

 * the ordering.

		/*

		 * If the GPU is not _reading_ through the CPU cache, we need

		 * to make sure that any writes (both previous GPU writes from

		 * before a change in snooping levels and normal CPU writes)

		 * caught in that cache are flushed to main memory.

		 *

		 * We want to say

		 *   obj->cache_dirty &&

		 *   !(obj->cache_coherent & I915_BO_CACHE_COHERENT_FOR_READ)

		 * but gcc's optimiser doesn't handle that as well and emits

		 * two jumps instead of one. Maybe one day...

		 *

		 * FIXME: There is also sync flushing in set_pages(), which

		 * serves a different purpose(some of the time at least).

		 *

		 * We should consider:

		 *

		 *   1. Rip out the async flush code.

		 *

		 *   2. Or make the sync flushing use the async clflush path

		 *   using mandatory fences underneath. Currently the below

		 *   async flush happens after we bind the object.

 We only need to await on the first request */

		/*

		 * count is always at least 1, otherwise __EXEC_USERPTR_USED

		 * could not have been set

 Unconditionally flush any chipset caches (for streaming writes). */

 Kernel clipping was a DRI1 misfeature */

	/*

	 * snb/ivb/vlv conflate the "batch in ppgtt" bit with the "non-secure

	 * batch" bit. Hence we need to pin secure batches into the global gtt.

		/*

		 * ppGTT backed shadow buffers must be mapped RO, to prevent

		 * post-scan tampering

 last paranoid check of overflow */

	/*

	 * After we completed waiting for other engines (using HW semaphores)

	 * then we can signal that this request/batch is ready to run. This

	 * allows us to determine if the batch is still waiting on the GPU

	 * or actually running by checking the breadcrumb.

/*

 * Find one BSD ring to dispatch the corresponding BSD command.

 * The engine index is returned.

 Check whether the file_priv has already selected one ring. */

	/*

	 * Completely unscientific finger-in-the-air estimates for suitable

	 * maximum user request size (to avoid blocking) and then backoff.

	/*

	 * Find a request that after waiting upon, there will be at least half

	 * the ring available. The hysteresis allows us to compete for the

	 * shared ring and should mean that we sleep less often prior to

	 * claiming our resources, but not so long that the ring completely

	 * drains before we can submit our next request.

 weird, we will check again later for real */

	/*

	 * Take a local wakeref for preparing to dispatch the execbuf as

	 * we expect to access the hardware fairly frequently in the

	 * process, and require the engine to be kept awake between accesses.

	 * Upon dispatch, we acquire another prolonged wakeref that we hold

	 * until the timeline is idle, which in turn releases the wakeref

	 * taken on the engine, and the parent device.

	/*

	 * Pinning the contexts may generate requests in order to acquire

	 * GGTT space, so do this first before we reserve a seqno for

	 * ourselves.

 perma-pinned should incr a counter */

	/*

	 * ABI: Before userspace accesses the GPU (e.g. execbuffer), report

	 * EIO if the GPU is already wedged.

	/*

	 * Make sure engine pool stays alive even if we call intel_context_put

	 * during ww handling. The pool is destroyed when last pm reference

	 * is dropped, which breaks our -EDEADLK handling.

 Check multiplication overflow for access_ok() and kvmalloc_array() */

		/*

		 * A point might have been signaled already and

		 * garbage collected from the timeline. In this case

		 * just ignore the point and carry on.

		/*

		 * For timeline syncobjs we need to preallocate chains for

		 * later signaling.

			/*

			 * Waiting and signaling the same point (when point !=

			 * 0) would break the timeline.

 Check multiplication overflow for access_ok() and kvmalloc_array() */

			/*

			 * The chain's ownership is transferred to the

			 * timeline.

 Check that the context wasn't destroyed before submission */

 Serialise with context_close via the add_to_timeline */

 override any transient errors */

 Try to clean up the client's timeline after submitting the request */

	/*

	 * We iterate in reverse order of creation to release timeline mutexes in

	 * same order.

	/* The execbuf2 extension mechanism reuses cliprects_ptr. So we cannot

	 * have another flag also using it at the same time.

 Move ownership to the dma_fence_array created above */

 sync_file now owns fence_arry, drop creation ref */

 Allocate a request for this batch buffer nice and early. */

		/*

		 * Only the first request added (committed to backend) has to

		 * take the in fences into account as all subsequent requests

		 * will have fences inserted inbetween them.

		/*

		 * Whilst this request exists, batch_obj will be on the

		 * active_list, and so will hold the active reference. Only when

		 * this request is retired will the batch_obj be moved onto

		 * the inactive_list and lose its active reference. Hence we do

		 * not need to explicitly hold another reference here.

 Return -EPERM to trigger fallback code on old binaries. */

		/*

		 * If the user expects the execobject.offset and

		 * reloc.presumed_offset to be an exact match,

		 * as for using NO_RELOC, then we cannot update

		 * the execobject.offset until we have completed

		 * relocation.

 keep in-fence */

	/*

	 * When using LUT_HANDLE, we impose a limit of INT_MAX for the lookup

	 * array size (see eb_create()). Otherwise, we can accept an array as

	 * large as can be addressed (though use large arrays at your peril)!

 Allocate extra slots for use by the command parser */

	/*

	 * Now that we have begun execution of the batchbuffer, we ignore

	 * any new error after this point. Also given that we have already

	 * updated the associated relocations, we try to write out the current

	 * object locations irrespective of any error.

 Copy the new buffer offsets back to the user's exec list. */

		/*

		 * Note: count * sizeof(*user_exec_list) does not overflow,

		 * because we checked 'count' in check_buffer_count().

		 *

		 * And this range already got effectively checked earlier

		 * when we did the "copy_from_user()" above.

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2014-2016 Intel Corporation

 Make the pages coherent with the GPU (flushing any swapin). */

	/*

	 * Calculate the supported page-sizes which fit into the given

	 * sg_page_sizes. This will give us the page-sizes which we may be able

	 * to use opportunistically when later inserting into the GTT. For

	 * example if phys=2G, then in theory we should be able to use 1G, 2M,

	 * 64K or 4K pages, although in practice this will depend on a number of

	 * other factors.

/* Ensure that the associated pages are gathered from the backing storage

 * and pinned into our object. i915_gem_object_pin_pages() may be called

 * multiple times before they are released by a single call to

 * i915_gem_object_unpin_pages() - once the pages are no longer referenced

 * either as a result of memory pressure (reaping pages under the shrinker)

 * or as the object is itself released.

 Immediately discard the backing storage */

 Try to discard unwanted pages */

 May be called by shrinker from within get_pages() (on another bo) */

	/*

	 * ->put_pages might need to allocate memory for the bit17 swizzle

	 * array, hence protect them from being reaped by removing them from gtt

	 * lists early.

	/*

	 * XXX Temporary hijinx to avoid updating all backends to handle

	 * NULL pages. In the future, when we have more asynchronous

	 * get_pages backends we should be better able to handle the

	 * cancellation of the async task in a more uniform manner.

 The 'mapping' part of i915_gem_object_pin_map() below */

 to use PAGE_KERNEL anyway */

		/*

		 * On 32b, highmem using a finite set of indirect PTE (i.e.

		 * vmap) to provide virtual mappings of the high pages.

		 * As these are finite, map_new_virtual() must wait for some

		 * other kmap() to finish when it runs out. If we map a large

		 * number of objects, there is no method for it to tell us

		 * to release the mappings, and we deadlock.

		 *

		 * However, if we make an explicit vmap of the page, that

		 * uses a larger vmalloc arena, and also has the ability

		 * to tell us to release unwanted mappings. Most importantly,

		 * it will fail and propagate an error instead of waiting

		 * forever.

		 *

		 * So if the page is beyond the 32b boundary, make an explicit

		 * vmap.

 Too big for stack -- allocate temporary array instead */

 Too big for stack -- allocate temporary array instead */

 get, pin, and map the pages of the object into kernel space */

	/*

	 * For discrete our CPU mappings needs to be consistent in order to

	 * function correctly on !x86. When mapping things through TTM, we use

	 * the same rules to determine the caching type.

	 *

	 * The caching rules, starting from DG1:

	 *

	 *	- If the object can be placed in device local-memory, then the

	 *	  pages should be allocated and mapped as write-combined only.

	 *

	 *	- Everything else is always allocated and mapped as write-back,

	 *	  with the guarantee that everything is also coherent with the

	 *	  GPU.

	 *

	 * Internal users of lmem are already expected to get this right, so no

	 * fudging needed there.

 let all previous writes be visible to coherent partners */

	/*

	 * We allow removing the mapping from underneath pinned pages!

	 *

	 * Furthermore, since this is an unsafe operation reserved only

	 * for construction time manipulation, we ignore locking prudence.

	/* As we iterate forward through the sg, we record each entry in a

	 * radixtree for quick repeated (backwards) lookups. If we have seen

	 * this index previously, we will have an entry for it.

	 *

	 * Initial lookup is O(N), but this is amortized to O(1) for

	 * sequential page access (where each new request is consecutive

	 * to the previous one). Repeated lookups are O(lg(obj->base.size)),

	 * i.e. O(1) with a large constant!

	/* We prefer to reuse the last sg so that repeated lookup of this

	 * (or the subsequent) sg are fast - comparing against the last

	 * sg is faster than going through the radixtree.

		/* If we cannot allocate and insert this entry, or the

		 * individual pages from this range, cancel updating the

		 * sg_idx so that on this lookup we are forced to linearly

		 * scan onwards, but on future lookups we will try the

		 * insertion again (in which case we need to be careful of

		 * the error return reporting that we have already inserted

		 * this index).

 insertion completed by another thread */

	/* In case we failed to insert the entry into the radixtree, we need

	 * to look beyond the current sg.

	/* If this index is in the middle of multi-page sg entry,

	 * the radix tree will contain a value entry that points

	 * to the start of that range. We will return the pointer to

	 * the base page and the offset of this page within the

	 * sg entry's range.

 Like i915_gem_object_get_page(), but mark the returned page dirty */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2012-2014 Intel Corporation

 *

  * Based on amdgpu_mn, which bears the following notice:

 *

 * Copyright 2014 Advanced Micro Devices, Inc.

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the

 * "Software"), to deal in the Software without restriction, including

 * without limitation the rights to use, copy, modify, merge, publish,

 * distribute, sub license, and/or sell copies of the Software, and to

 * permit persons to whom the Software is furnished to do so, subject to

 * the following conditions:

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL

 * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,

 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR

 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE

 * USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

/*

 * Authors:

 *    Christian KÃ¶nig <christian.koenig@amd.com>

/**

 * i915_gem_userptr_invalidate - callback to notify about mm change

 *

 * @mni: the range (mm) is about to update

 * @range: details on the invalidation

 * @cur_seq: Value to pass to mmu_interval_set_seq()

 *

 * Block for operations on BOs to finish and mark pages as accessed and

 * potentially dirty.

	/*

	 * We don't wait when the process is exiting. This is valid

	 * because the object will be cleaned up anyway.

	 *

	 * This is also temporarily required as a hack, because we

	 * cannot currently force non-consistent batch buffers to preempt

	 * and reschedule by waiting on it, hanging processes on exit.

 we will unbind on next submission, still have userptr pins */

	/*

	 * We always mark objects as dirty when they are used by the GPU,

	 * just in case. However, if we set the vma as being read-only we know

	 * that the object will never have been written to.

			/*

			 * As this may not be anonymous memory (e.g. shmem)

			 * but exist on a real mapping, we have to lock

			 * the page in order to dirty it -- holding

			 * the page reference is not sufficient to

			 * prevent the inode from being truncated.

			 * Play safe and take the lock.

			 *

			 * However...!

			 *

			 * The mmu-notifier can be invalidated for a

			 * migrate_page, that is alreadying holding the lock

			 * on the page. Such a try_to_unmap() will result

			 * in us calling put_pages() and so recursively try

			 * to lock the page. We avoid that deadlock with

			 * a trylock_page() and in exchange we risk missing

			 * some page dirtying.

 We collided with the mmu notifier, need to retry */

		/*

		 * Since we only check validity, not use the pages,

		 * it doesn't matter if we collide with the mmu notifier,

		 * and -EAGAIN handling is not required.

 Check for holes, note that we also update the addr below */

/*

 * Creates a new mm object that wraps some normal memory from the process

 * context - user memory.

 *

 * We impose several restrictions upon the memory being mapped

 * into the GPU.

 * 1. It must be page aligned (both start/end addresses, i.e ptr and size).

 * 2. It must be normal system memory, not a pointer into another map of IO

 *    space (e.g. it must not be a GTT mmapping of another object).

 * 3. We only allow a bo as large as we could in theory map into the GTT,

 *    that is we limit the size to the total size of the GTT.

 * 4. The bo is marked as being snoopable. The backing pages are left

 *    accessible directly by the CPU, but reads and writes by the GPU may

 *    incur the cost of a snoop (unless you have an LLC architecture).

 *

 * Synchronisation between multiple users and the GPU is left to userspace

 * through the normal set-domain-ioctl. The kernel will enforce that the

 * GPU relinquishes the VMA before it is returned back to the system

 * i.e. upon free(), munmap() or process termination. However, the userspace

 * malloc() library may not immediately relinquish the VMA after free() and

 * instead reuse it whilst the GPU is still reading and writing to the VMA.

 * Caveat emptor.

 *

 * Also note, that the object created here is not currently a "first class"

 * object, in that several ioctls are banned. These are the CPU access

 * ioctls: mmap(), pwrite and pread. In practice, you are expected to use

 * direct access via your pointer rather than use those ioctls. Another

 * restriction is that we do not allow userptr surfaces to be pinned to the

 * hardware and so we reject any attempt to create a framebuffer out of a

 * userptr.

 *

 * If you think this is a good interface to use to pass GPU memory between

 * drivers, please use dma-buf instead. In fact, wherever possible use

 * dma-buf instead.

		/* We cannot support coherent userptr objects on hw without

		 * LLC and broken snooping.

		/*

		 * On almost all of the older hw, we cannot tell the GPU that

		 * a page is readonly.

		/*

		 * Check that the range pointed to represents real struct

		 * pages and not iomappings (at this moment in time!)

	/* And keep a pointer to the current->mm for resolving the user pages

	 * at binding. This means that we need to hook into the mmu_notifier

	 * in order to detect if the mmu is destroyed.

 drop reference from allocate - handle holds it now */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2014-2016 Intel Corporation

/**

 * i915_gem_mmap_ioctl - Maps the contents of an object, returning the address

 *			 it is mapped to.

 * @dev: drm device

 * @data: ioctl data blob

 * @file: drm file

 *

 * While the mapping holds a reference on the contents of the object, it doesn't

 * imply a ref on the object itself.

 *

 * IMPORTANT:

 *

 * DRM driver writers who look a this function as an example for how to do GEM

 * mmap support, please don't implement mmap support like here. The modern way

 * to implement DRM mmap support is with an mmap offset ioctl (like

 * i915_gem_mmap_gtt) and then using the mmap syscall on the DRM fd directly.

 * That way debug tooling like valgrind will understand what's going on, hiding

 * the mmap call in a driver private ioctl will break that. The i915 driver only

 * does cpu mmaps this way because we didn't know better.

	/*

	 * mmap ioctl is disallowed for all discrete platforms,

	 * and for all platforms with GRAPHICS_VER > 12.

	/* prime objects have no backing filp to GEM mmap

	 * pages from.

/**

 * i915_gem_mmap_gtt_version - report the current feature set for GTT mmaps

 *

 * A history of the GTT mmap interface:

 *

 * 0 - Everything had to fit into the GTT. Both parties of a memcpy had to

 *     aligned and suitable for fencing, and still fit into the available

 *     mappable space left by the pinned display objects. A classic problem

 *     we called the page-fault-of-doom where we would ping-pong between

 *     two objects that could not fit inside the GTT and so the memcpy

 *     would page one object in at the expense of the other between every

 *     single byte.

 *

 * 1 - Objects can be any size, and have any compatible fencing (X Y, or none

 *     as set via i915_gem_set_tiling() [DRM_I915_GEM_SET_TILING]). If the

 *     object is too large for the available space (or simply too large

 *     for the mappable aperture!), a view is created instead and faulted

 *     into userspace. (This view is aligned and sized appropriately for

 *     fenced access.)

 *

 * 2 - Recognise WC as a separate cache domain so that we can flush the

 *     delayed writes via GTT before performing direct access via WC.

 *

 * 3 - Remove implicit set-domain(GTT) and synchronisation on initial

 *     pagefault; swapin remains transparent.

 *

 * 4 - Support multiple fault handlers per object depending on object's

 *     backing storage (a.k.a. MMAP_OFFSET).

 *

 * Restrictions:

 *

 *  * snoopable objects cannot be accessed via the GTT. It can cause machine

 *    hangs on some architectures, corruption on others. An attempt to service

 *    a GTT page fault from a snoopable object will generate a SIGBUS.

 *

 *  * the object must be able to fit into RAM (physical memory, though no

 *    limited to the mappable aperture).

 *

 *

 * Caveats:

 *

 *  * a new GTT page fault will synchronize rendering from the GPU and flush

 *    all data to system memory. Subsequent access will not be synchronized.

 *

 *  * all mappings are revoked on runtime device suspend.

 *

 *  * there are only 8, 16 or 32 fence registers to share between all users

 *    (older machines require fence register for display and blitter access

 *    as well). Contention of the fence registers will cause the previous users

 *    to be unmapped and any new access will generate new page faults.

 *

 *  * running out of memory while servicing a fault may generate a SIGBUS,

 *    rather than the expected SIGSEGV.

 If the partial covers the entire object, just create a normal VMA. */

 shmemfs failure from swap device */

 purged object */

 bad object, how did you get here! */

 unable to access backing store (on device) */

 our allocation failure */

 transient failure to evict? */

		/*

		 * EBUSY is ok: this just means that another thread

		 * already did the job.

 Sanity check that we allow writing into this object */

 PTEs are revoked in obj->ops->put_pages() */

 We don't use vmf->pgoff since that has the fake offset */

 Sanity check that we allow writing into this object */

 Now pin it into the GTT as needed */

 NOWARN */ |

 Use a partial view if it is bigger than available space */

 avoid warnings for pinned */

		/*

		 * Userspace is now writing through an untracked VMA, abandon

		 * all hope that the hardware is able to track future writes.

 The entire mappable GGTT is pinned? Unexpected! */

 Access to snoopable pages through the GTT is incoherent. */

 Finally, remap it using the new GTT offset */

 Mark as being mmapped into userspace for later revocation */

 Track the mmo associated with the fenced vma */

 As this is primarily for debugging, let's focus on simplicity */

/*

 * It is vital that we remove the page mapping if we have mapped a tiled

 * object through the GTT and then lose the fence register due to

 * resource pressure. Similarly if the object has been moved out of the

 * aperture, than pages mapped into userspace must be revoked. Removing the

 * mapping will then trigger a page fault on the next user access, allowing

 * fixup by vm_fault_gtt().

	/*

	 * Serialisation between user GTT access and our code depends upon

	 * revoking the CPU's PTE whilst the mutex is held. The next user

	 * pagefault then has to wait until we release the mutex.

	 *

	 * Note that RPM complicates somewhat by adding an additional

	 * requirement that operations to the GGTT be made holding the RPM

	 * wakeref.

	/*

	 * Ensure that the CPU's PTE are revoked and there are not outstanding

	 * memory transactions from userspace before we return. The TLB

	 * flushing implied above by changing the PTE above *should* be

	 * sufficient, an extra barrier here just provides us with a bit

	 * of paranoid documentation about our requirement to serialise

	 * memory writes before touching registers / GSM.

		/*

		 * vma_node_unmap for GTT mmaps handled already in

		 * __i915_gem_object_release_mmap_gtt

 Attempt to reap some mmap space from dead objects */

/**

 * i915_gem_mmap_offset_ioctl - prepare an object for GTT mmap'ing

 * @dev: DRM device

 * @data: GTT mapping ioctl data

 * @file: GEM object info

 *

 * Simply returns the fake offset to userspace so it can mmap it.

 * The mmap call will end up in drm_gem_mmap(), which will set things

 * up so we can get faults in the handler above.

 *

 * The fault handler will take care of binding the object into the GTT

 * (since it may have been evicted to make room for something), allocating

 * a fence register, and mapping the appropriate aperture address into

 * userspace.

	/*

	 * Historically we failed to check args.pad and args.offset

	 * and so we cannot use those fields for user input and we cannot

	 * add -EINVAL for them as the ABI is fixed, i.e. old userspace

	 * may be feeding in garbage in those fields.

	 *

	 * if (args->pad) return -EINVAL; is verbotten!

 Everyone shares a single global address space */

/*

 * This overcomes the limitation in drm_gem_mmap's assignment of a

 * drm_gem_object as the vma->vm_private_data. Since we need to

 * be able to resolve multiple mmap offsets which could be tied

 * to a single gem object.

		/*

		 * Skip 0-refcnted objects as it is in the process of being

		 * destroyed and will be invalid when the vma manager lock

		 * is released.

	/*

	 * We keep the ref on mmo->obj, not vm_file, but we require

	 * vma->vm_file->f_mapping, see vma_link(), for later revocation.

	 * Our userspace is accustomed to having per-file resource cleanup

	 * (i.e. contexts, objects and requests) on their close(fd), which

	 * requires avoiding extraneous references to their filp, hence why

	 * we prefer to use an anonymous file for their mmaps.

 Drop the initial creation reference, the vma is now holding one. */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2008-2012 Intel Corporation

/*

 * The BIOS typically reserves some of the system's memory for the exclusive

 * use of the integrated graphics. This memory is no longer available for

 * use by the OS and so the user finds that his system has less memory

 * available than he put in. We refer to this memory as stolen.

 *

 * The BIOS will allocate its framebuffer from the stolen memory. Our

 * goal is try to reuse that object for our own fbcon which must always

 * be available for panics. Anything else we can reuse the stolen memory

 * for is a boon.

 WaSkipStolenMemoryFirstPage:bdw+ */

	/*

	 * TODO: We have yet too encounter the case where the GTT wasn't at the

	 * end of stolen. With that assumption we could simplify this.

 Make sure we don't clobber the GTT if it's within stolen memory */

 Pick the larger of the two chunks */

	/*

	 * With stolen lmem, we don't need to check if the address range

	 * overlaps with the non-stolen system memory range, since lmem is local

	 * to the gpu.

	/*

	 * Verify that nothing else uses this physical address. Stolen

	 * memory should be reserved by the BIOS and hidden from the

	 * kernel. So if the region is already marked as busy, something

	 * is seriously wrong.

		/*

		 * One more attempt but this time requesting region from

		 * start + 1, as we have seen that this resolves the region

		 * conflict with the PCI Bus.

		 * This is a BIOS w/a: Some BIOS wrap stolen in the root

		 * PCI bus, but have an off-by-one error. Hence retry the

		 * reservation starting from 1 instead of 0.

		 * There's also BIOS with off-by-one on the other end.

		/*

		 * GEN3 firmware likes to smash pci bridges into the stolen

		 * range. Apparently this works.

	/*

	 * Whether ILK really reuses the ELK register for this is unclear.

	 * Let's see if we catch anyone with this supposedly enabled on ILK.

	/*

	 * On vlv, the ADDR_MASK portion is left as 0 and HW deduces the

	 * reserved location as (top - size).

	/*

	 * Our expectation is that the reserved space is at the top of the

	 * stolen region and *never* at the bottom. If we see !reserved_base,

	 * it likely means we failed to read the registers correctly.

	/* It is possible for the reserved area to end before the end of stolen

 Basic memrange allocator for stolen space. */

 beware stop_machine() inversion */

	/* We hide that we have no struct page backing our stolen object

	 * by wrapping the contiguous physical allocation with a fake

	 * dma mapping in a single scatterlist.

 Should only be called from i915_gem_object_release_stolen() */

	/*

	 * Stolen objects are always physically contiguous since we just

	 * allocate one big block underneath using the drm_mm range allocator.

	/*

	 * Initialise stolen early so that we may reserve preallocated

	 * objects for the BIOS to KMS transition.

	/*

	 * TODO: For stolen lmem we mostly just care about populating the dsm

	 * related bits and setting up the drm_mm allocator for the range.

	 * Perhaps split up i915_gem_init_stolen() for this.

	/*

	 * TODO: consider creating common helper to just print all the

	 * interesting stuff from intel_memory_region, which we can use for all

	 * our probed regions.

 KISS and expect everything to be page-aligned */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

	/*

	 * For the common case of one memory region, skip storing an

	 * allocated array and just point at the region directly.

 drop reference from allocate - handle holds it now */

 For most of the ABI (e.g. mmap) we think in system pages */

	/*

	 * I915_BO_ALLOC_USER will make sure the object is cleared before

	 * any user access.

 Add any flag set by create_ext options */

/**

 * Creates a new object using the same path as DRM_I915_GEM_CREATE_EXT

 * @i915: i915 private

 * @size: size of the buffer, in bytes

 * @placements: possible placement regions, in priority order

 * @n_placements: number of possible placement regions

 *

 * This function is exposed primarily for selftests and does very little

 * error checking.  It is assumed that the set of placement regions has

 * already been verified to be valid.

 have to work out size/pitch and return them */

 align stride to page size so that we can remap */

/**

 * Creates a new mm object and returns a handle to it.

 * @dev: drm device pointer

 * @data: ioctl data blob

 * @file: drm file pointer

/**

 * Creates a new mm object and returns a handle to it.

 * @dev: drm device pointer

 * @data: ioctl data blob

 * @file: drm file pointer

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

/**

 * i915_gem_object_is_lmem - Whether the object is resident in

 * lmem

 * @obj: The object to check.

 *

 * Even if an object is allowed to migrate and change memory region,

 * this function checks whether it will always be present in lmem when

 * valid *or* if that's not the case, whether it's currently resident in lmem.

 * For migratable and evictable objects, the latter only makes sense when

 * the object is locked.

 *

 * Return: Whether the object migratable but resident in lmem, or not

 * migratable and will be present in lmem when valid.

/**

 * __i915_gem_object_is_lmem - Whether the object is resident in

 * lmem while in the fence signaling critical path.

 * @obj: The object to check.

 *

 * This function is intended to be called from within the fence signaling

 * path where the fence, or a pin, keeps the object from being migrated. For

 * example during gpu reset or similar.

 *

 * Return: Whether the object is resident in lmem.

/**

 * __i915_gem_object_create_lmem_with_ps - Create lmem object and force the

 * minimum page size for the backing pages.

 * @i915: The i915 instance.

 * @size: The size in bytes for the object. Note that we need to round the size

 * up depending on the @page_size. The final object size can be fished out from

 * the drm GEM object.

 * @page_size: The requested minimum page size in bytes for this object. This is

 * useful if we need something bigger than the regions min_page_size due to some

 * hw restriction, or in some very specialised cases where it needs to be

 * smaller, where the internal fragmentation cost is too great when rounding up

 * the object size.

 * @flags: The optional BO allocation flags.

 *

 * Note that this interface assumes you know what you are doing when forcing the

 * @page_size. If this is smaller than the regions min_page_size then it can

 * never be inserted into any GTT, otherwise it might lead to undefined

 * behaviour.

 *

 * Return: The object pointer, which might be an ERR_PTR in the case of failure.

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2014-2016 Intel Corporation

/*

 * Move pages to appropriate lru and release the pagevec, decrementing the

 * ref count of those pages.

 suppress gcc warning */

	/*

	 * Assert that the object is not currently in any GPU domain. As it

	 * wasn't in the GTT, there shouldn't be any way it could have been in

	 * a GPU cache

	/*

	 * If there's no chance of allocating enough pages for the whole

	 * object, bail early.

	/*

	 * Get the list of pages out of our struct file.  They'll be pinned

	 * at this point until we release them.

	 *

	 * Fail silently without starting the shrinker

			/*

			 * We've tried hard to allocate the memory by reaping

			 * our own buffer, now let the real VM do its job and

			 * go down in flames if truly OOM.

			 *

			 * However, since graphics tend to be disposable,

			 * defer the oom here by reporting the ENOMEM back

			 * to userspace.

 reclaim and warn, but no oom */

				/*

				 * Our bo are always dirty and so we require

				 * kswapd to reclaim our pages (direct reclaim

				 * does not effectively begin pageout of our

				 * buffers on its own). However, direct reclaim

				 * only waits for kswapd when under allocation

				 * congestion. So as a result __GFP_RECLAIM is

				 * unreliable and fails to actually reclaim our

				 * dirty pages -- unless you try over and over

				 * again with !__GFP_NORETRY. However, we still

				 * want to fail this allocation rather than

				 * trigger the out-of-memory killer and for

				 * this we want __GFP_RETRY_MAYFAIL.

 Check that the i965g/gm workaround works. */

 loop terminated early; short sg table */

 Trim unused sg entries to avoid wasting memory. */

		/*

		 * DMA remapping failed? One possible cause is that

		 * it could not reserve enough large entries, asking

		 * for PAGE_SIZE chunks instead may be helpful.

	/*

	 * shmemfs first checks if there is enough memory to allocate the page

	 * and reports ENOSPC should there be insufficient, along with the usual

	 * ENOMEM for a genuine allocation failure.

	 *

	 * We use ENOSPC in our driver to mean that we have run out of aperture

	 * space and so want to translate the error from shmemfs back to our

	 * usual understanding of ENOMEM.

	/*

	 * Our goal here is to return as much of the memory as

	 * is possible back to the system as we are called from OOM.

	 * To do this we must instruct the shmfs to drop all of its

	 * backing pages, *now*.

	/*

	 * Leave mmapings intact (GTT will have been revoked on unbinding,

	 * leaving only CPU mmapings around) and add those pages to the LRU

	 * instead of invoking writeback so they are aged and paged out

	 * as normal.

 Begin writeback on each dirty page */

	/*

	 * On non-LLC platforms, force the flush-on-acquire if this is ever

	 * swapped-in. Our async flush path is not trust worthy enough yet(and

	 * happens in the wrong order), and with some tricks it's conceivable

	 * for userspace to change the cache-level to I915_CACHE_NONE after the

	 * pages are swapped-in, and since execbuf binds the object before doing

	 * the async flush, we have a race window.

 Caller already validated user args */

	/*

	 * Before we instantiate/pin the backing store for our use, we

	 * can prepopulate the shmemfs filp efficiently using a write into

	 * the pagecache. We avoid the penalty of instantiating all the

	 * pages, important if the user is just writing to a few and never

	 * uses the object on the GPU, and using a direct write into shmemfs

	 * allows it to avoid the cost of retrieving a page (either swapin

	 * or clearing-before-use) before it is overwritten.

	/*

	 * Before the pages are instantiated the object is treated as being

	 * in the CPU domain. The pages will be clflushed as required before

	 * use, and we can freely write into the pages directly. If userspace

	 * races pwrite with any other operation; corruption will ensue -

	 * that is userspace's prerogative!

 Prefault the user page to reduce potential recursion */

 We don't handle -EFAULT, leave it to the caller to check */

 965gm cannot relocate objects above 4GiB. */

		/* On some devices, we can have the GPU use the LLC (the CPU

		 * cache) for about a 10% performance improvement

		 * compared to uncached.  Graphics requests other than

		 * display scanout are coherent with the CPU in

		 * accessing this cache.  This means in this mode we

		 * don't need to clflush on the CPU side, and on the

		 * GPU side we only need to flush internal caches to

		 * get data visible to the CPU.

		 *

		 * However, we maintain the display planes as UC, and so

		 * need to rebind when first used as such.

 Allocate a new GEM object and fill it with the supplied data */

 Don't error, we can simply fallback to the kernel mnt */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2014-2016 Intel Corporation

 965gm cannot relocate objects above 4GiB. */

 Limit subsequent allocations as well */

 Failed to dma-map try again with single page sg segments */

/**

 * i915_gem_object_create_internal: create an object with volatile pages

 * @i915: the i915 device

 * @size: the size in bytes of backing storage to allocate for the object

 *

 * Creates a new object that wraps some internal memory for private use.

 * This object is not backed by swappable storage, and as such its contents

 * are volatile and only valid whilst pinned. If the object is reaped by the

 * shrinker, its pages and data will be discarded. Equally, it is not a full

 * GEM object and so not valid for access from userspace. This makes it useful

 * for hardware interfaces like ringbuffers (which are pinned from the time

 * the request is written to the time the hardware stops accessing it), but

 * not for contexts (which need to be preserved when not active for later

 * reuse). Note that it is not cleared upon allocation.

	/*

	 * Mark the object as volatile, such that the pages are marked as

	 * dontneed whilst they are still pinned. As soon as they are unpinned

	 * they are allowed to be reaped by the shrinker, and the caller is

	 * expected to repopulate - the contents of this object are only valid

	 * whilst active and pinned.

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2017 Intel Corporation

	/*

	 * By creating our own shmemfs mountpoint, we can pass in

	 * mount flags that better match our usecase.

	 *

	 * One example, although it is probably better with a per-file

	 * control, is selecting huge page allocations ("huge=within_size").

	 * However, we only do so to offset the overhead of iommu lookups

	 * due to bandwidth issues (slow reads) on Broadwell+.

 r/w */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

	/*

	 * NB: Our use of resource_size_t for the size stems from using struct

	 * resource for the mem->region. We might need to revisit this in the

	 * future.

/**

 * i915_gem_process_region - Iterate over all objects of a region using ops

 * to process and optionally skip objects

 * @mr: The memory region

 * @apply: ops and private data

 *

 * This function can be used to iterate over the regions object list,

 * checking whether to skip objects, and, if not, lock the objects and

 * process them using the supplied ops. Note that this function temporarily

 * removes objects from the region list while iterating, so that if run

 * concurrently with itself may not iterate over all objects.

 *

 * Return: 0 if successful, negative error code on failure.

	/*

	 * In the future, a non-NULL apply->ww could mean the caller is

	 * already in a locking transaction and provides its own context.

		/*

		 * Note: Someone else might be migrating the object at this

		 * point. The object's region is not stable until we lock

		 * the object.

 Implicit object unlock */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2014-2016 Intel Corporation

	/*

	 * The uABI guarantees an active writer is also amongst the read

	 * engines. This would be true if we accessed the activity tracking

	 * under the lock, but as we perform the lookup of the object and

	 * its activity locklessly we can not guarantee that the last_write

	 * being active implies that we have set the same engine flag from

	 * last_read - hence we always set both read and write busy for

	 * last_write.

	/*

	 * We have to check the current hw status of the fence as the uABI

	 * guarantees forward progress. We could rely on the idle worker

	 * to eventually flush us, but to minimise latency just ask the

	 * hardware.

	 *

	 * Note we only report on the status of native fences and we currently

	 * have two native fences:

	 *

	 * 1. A composite fence (dma_fence_array) constructed of i915 requests

	 * created during a parallel submission. In this case we deconstruct the

	 * composite fence into individual i915 requests and check the status of

	 * each request.

	 *

	 * 2. A single i915 request.

 Not an i915 fence, can't be busy per above */

 All requests in array complete, not busy */

 Beware type-expansion follies! */

	/*

	 * A discrepancy here is that we do not report the status of

	 * non-i915 fences, i.e. even though we may report the object as idle,

	 * a call to set-domain may still stall waiting for foreign rendering.

	 * This also means that wait-ioctl may report an object as busy,

	 * where busy-ioctl considers it idle.

	 *

	 * We trade the ability to warn of foreign fences to report on which

	 * i915 engines are active for the object.

	 *

	 * Alternatively, we can trade that extra information on read/write

	 * activity with

	 *	args->busy =

	 *		!dma_resv_test_signaled(obj->resv, true);

	 * to report the overall busyness. This is what the wait-ioctl does.

	 *

 Translate the exclusive fence to the READ *and* WRITE engine */

 Translate shared fences to READ set of engines */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright 2012 Red Hat Inc

 Copy sg so that we make an independent mapping */

 XXX: consider doing a vmap flush or something */

 is this one of own objects? */

 is it from our device? */

			/*

			 * Importing dmabuf exported from out own gem increases

			 * refcount on gem itself instead of f_count of dmabuf.

 need to attach */

	/* We use GTT as shorthand for a coherent domain, one that is

	 * neither in the GPU cache nor in the CPU cache, where all

	 * writes are immediately visible in memory. (That's not strictly

	 * true, but it's close! There are internal buffers such as the

	 * write-combined buffer or a delay through the chipset for GTT

	 * writes that do require us to treat GTT as a separate cache domain.)

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2016 Intel Corporation

 obj <-> clflush cycle */

	/*

	 * Stolen memory is always coherent with the GPU as it is explicitly

	 * marked as wc by the system, or the system is cache-coherent.

	 * Similarly, we only access struct pages through the CPU cache, so

	 * anything not backed by physical memory we consider to be always

	 * coherent and not need clflushing.

	/* If the GPU is snooping the contents of the CPU cache,

	 * we do not need to manually clear the CPU cache lines.  However,

	 * the caches are only snooped when the render cache is

	 * flushed/invalidated.  As we always have to emit invalidations

	 * and flushes when moving into and out of the RENDER domain, correct

	 * snooping behaviour occurs naturally as the result of our domain

	 * tracking.

/*

 * Copyright Â© 2017 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

	/*

	 * A gem object is embedded both in a struct ttm_buffer_object :/ and

	 * in a drm_i915_gem_object. Make sure they are aliased.

/**

 * i915_gem_object_fini - Clean up a GEM object initialization

 * @obj: The gem object to cleanup

 *

 * This function cleans up gem object fields that are set up by

 * drm_gem_private_object_init() and i915_gem_object_init().

 * It's primarily intended as a helper for backends that need to

 * clean up the gem object in separate steps.

/**

 * Mark up the object's coherency levels for a given cache_level

 * @obj: #drm_i915_gem_object

 * @cache_level: cache level

	/*

	 * This is purely from a security perspective, so we simply don't care

	 * about non-userspace objects being able to bypass the LLC.

	/*

	 * EHL and JSL add the 'Bypass LLC' MOCS entry, which should make it

	 * possible for userspace to bypass the GTT caching bits set by the

	 * kernel, as per the given object cache_level. This is troublesome

	 * since the heavy flush we apply when first gathering the pages is

	 * skipped if the kernel thinks the object is coherent with the GPU. As

	 * a result it might be possible to bypass the cache and read the

	 * contents of the page directly, which could be stale data. If it's

	 * just a case of userspace shooting themselves in the foot then so be

	 * it, but since i915 takes the stance of always zeroing memory before

	 * handing it to userspace, we need to prevent this.

 Break long locks, and carefully continue on from this spot */

		/*

		 * We allow the process to have multiple handles to the same

		 * vma, in the same fd namespace, by virtue of flink/open.

 Skip serialisation and waking the device if known to be not used. */

/**

 * __i915_gem_object_pages_fini - Clean up pages use of a gem object

 * @obj: The gem object to clean up

 *

 * This function cleans up usage of the object mm.pages member. It

 * is intended for backends that need to clean up a gem object in

 * separate steps and needs to be called when the object is idle before

 * the object's backing memory is freed.

		/*

		 * Note that the vma keeps an object reference while

		 * it is active, so it *should* not sleep while we

		 * destroy it. Our debug code errs insits it *might*.

		 * For the moment, play along.

 But keep the pointer alive for RCU-protected lookups */

	/*

	 * Before we free the object, make sure any pure RCU-only

	 * read-side critical sections are complete, e.g.

	 * i915_gem_busy_ioctl(). For the corresponding synchronized

	 * lookup see i915_gem_object_lookup_rcu().

	/*

	 * This serializes freeing with the shrinker. Since the free

	 * is delayed, first by RCU then by the workqueue, we want the

	 * shrinker to be able to free pages of unreferenced objects,

	 * or else we may oom whilst there are plenty of deferred

	 * freed objects.

	/*

	 * Since we require blocking on struct_mutex to unbind the freed

	 * object from the GPU before releasing resources back to the

	 * system, we can not do that directly from the RCU callback (which may

	 * be a softirq context), but must instead then defer that work onto a

	 * kthread. We use the RCU callback rather than move the freed object

	 * directly onto the work queue so that we can mix between using the

	 * worker and performing frees directly from subsequent allocations for

	 * crude but effective memory throttling.

/**

 * i915_gem_object_read_from_page - read data from the page of a GEM object

 * @obj: GEM object to read from

 * @offset: offset within the object

 * @dst: buffer to store the read data

 * @size: size to read

 *

 * Reads data from @obj at the specified offset. The requested region to read

 * from can't cross a page boundary. The caller must ensure that @obj pages

 * are pinned and that @obj is synced wrt. any related writes.

 *

 * Returns 0 on success or -ENODEV if the type of @obj's backing store is

 * unsupported.

/**

 * i915_gem_object_evictable - Whether object is likely evictable after unbind.

 * @obj: The object to check

 *

 * This function checks whether the object is likely unvictable after unbind.

 * If the object is not locked when checking, the result is only advisory.

 * If the object is locked when checking, and the function returns true,

 * then an eviction should indeed be possible. But since unlocked vma

 * unpinning and unbinding is currently possible, the object can actually

 * become evictable even if this function returns false.

 *

 * Return: true if the object may be evictable. False otherwise.

/**

 * i915_gem_object_migratable - Whether the object is migratable out of the

 * current region.

 * @obj: Pointer to the object.

 *

 * Return: Whether the object is allowed to be resident in other

 * regions than the current while pages are present.

/**

 * i915_gem_object_has_struct_page - Whether the object is page-backed

 * @obj: The object to query.

 *

 * This function should only be called while the object is locked or pinned,

 * otherwise the page backing may change under the caller.

 *

 * Return: True if page-backed, false otherwise.

/**

 * i915_gem_object_has_iomem - Whether the object is iomem-backed

 * @obj: The object to query.

 *

 * This function should only be called while the object is locked or pinned,

 * otherwise the iomem backing may change under the caller.

 *

 * Return: True if iomem-backed, false otherwise.

/**

 * i915_gem_object_can_migrate - Whether an object likely can be migrated

 *

 * @obj: The object to migrate

 * @id: The region intended to migrate to

 *

 * Check whether the object backend supports migration to the

 * given region. Note that pinning may affect the ability to migrate as

 * returned by this function.

 *

 * This function is primarily intended as a helper for checking the

 * possibility to migrate objects and might be slightly less permissive

 * than i915_gem_object_migrate() when it comes to objects with the

 * I915_BO_ALLOC_USER flag set.

 *

 * Return: true if migration is possible, false otherwise.

/**

 * i915_gem_object_migrate - Migrate an object to the desired region id

 * @obj: The object to migrate.

 * @ww: An optional struct i915_gem_ww_ctx. If NULL, the backend may

 * not be successful in evicting other objects to make room for this object.

 * @id: The region id to migrate to.

 *

 * Attempt to migrate the object to the desired memory region. The

 * object backend must support migration and the object may not be

 * pinned, (explicitly pinned pages or pinned vmas). The object must

 * be locked.

 * On successful completion, the object will have pages pointing to

 * memory in the new region, but an async migration task may not have

 * completed yet, and to accomplish that, i915_gem_object_wait_migration()

 * must be called.

 *

 * Note: the @ww parameter is not used yet, but included to make sure

 * callers put some effort into obtaining a valid ww ctx if one is

 * available.

 *

 * Return: 0 on success. Negative error code on failure. In particular may

 * return -ENXIO on lack of region space, -EDEADLK for deadlock avoidance

 * if @ww is set, -EINTR or -ERESTARTSYS if signal pending, and

 * -EBUSY if the object is pinned.

/**

 * i915_gem_object_placement_possible - Check whether the object can be

 * placed at certain memory type

 * @obj: Pointer to the object

 * @type: The memory type to check

 *

 * Return: True if the object can be placed in @type. False otherwise.

 Ignore stolen for now */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2014-2016 Intel Corporation

	/*

	 * We manually flush the CPU domain so that we can override and

	 * force the flush for the display, and perform it asyncrhonously.

/**

 * Moves a single object to the WC read, and possibly write domain.

 * @obj: object to act on

 * @write: ask for write access or read only

 *

 * This function returns when the move is complete, including waiting on

 * flushes to occur.

	/* Flush and acquire obj->pages so that we are coherent through

	 * direct access in memory with previous cached writes through

	 * shmemfs and that our cache domain tracking remains valid.

	 * For example, if the obj->filp was moved to swap without us

	 * being notified and releasing the pages, we would mistakenly

	 * continue to assume that the obj remained out of the CPU cached

	 * domain.

	/* Serialise direct access to this object with the barriers for

	 * coherent writes from the GPU, by effectively invalidating the

	 * WC domain upon first access.

	/* It should now be out of any other write domains, and we can update

	 * the domain values for our changes.

/**

 * Moves a single object to the GTT read, and possibly write domain.

 * @obj: object to act on

 * @write: ask for write access or read only

 *

 * This function returns when the move is complete, including waiting on

 * flushes to occur.

	/* Flush and acquire obj->pages so that we are coherent through

	 * direct access in memory with previous cached writes through

	 * shmemfs and that our cache domain tracking remains valid.

	 * For example, if the obj->filp was moved to swap without us

	 * being notified and releasing the pages, we would mistakenly

	 * continue to assume that the obj remained out of the CPU cached

	 * domain.

	/* Serialise direct access to this object with the barriers for

	 * coherent writes from the GPU, by effectively invalidating the

	 * GTT domain upon first access.

	/* It should now be out of any other write domains, and we can update

	 * the domain values for our changes.

/**

 * Changes the cache-level of an object across all VMA.

 * @obj: object to act on

 * @cache_level: new cache level to set for the object

 *

 * After this function returns, the object will be in the new cache-level

 * across all GTT and the contents of the backing storage will be coherent,

 * with respect to the new cache-level. In order to keep the backing storage

 * coherent for all users, we only allow a single cache level to be set

 * globally on the object and prevent it from being changed whilst the

 * hardware is reading from the object. That is if the object is currently

 * on the scanout it will be set to uncached (or equivalent display

 * cache coherency) and all non-MOCS GPU access will also be uncached so

 * that all direct access to the scanout remains coherent.

 Always invalidate stale cachelines */

 The cache-level will be applied when each vma is rebound. */

		/*

		 * Due to a HW issue on BXT A stepping, GPU stores via a

		 * snooped mapping may leave stale data in a corresponding CPU

		 * cacheline, whereas normally such cachelines would get

		 * invalidated.

	/*

	 * The caching mode of proxy object is handled by its generator, and

	 * not allowed to be changed by userspace.

		/*

		 * Silently allow cached for userptr; the vulkan driver

		 * sets all objects to cached

/*

 * Prepare buffer for display plane (scanout, cursors, etc). Can be called from

 * an uninterruptible phase (modesetting) and allows any flushes to be pipelined

 * (for pageflips). We only flush the caches while preparing the buffer for

 * display, the callers are responsible for frontbuffer flush.

 Frame buffer must be in LMEM */

	/*

	 * The display engine is not coherent with the LLC cache on gen6.  As

	 * a result, we make sure that the pinning that is about to occur is

	 * done with uncached PTEs. This is lowest common denominator for all

	 * chipsets.

	 *

	 * However for gen6+, we could do better by using the GFDT bit instead

	 * of uncaching, which would allow us to flush all the LLC-cached data

	 * with that bit in the PTE to main memory with just one PIPE_CONTROL.

	/*

	 * As the user may map the buffer once pinned in the display plane

	 * (e.g. libkms for the bootup splash), we have to ensure that we

	 * always use map_and_fenceable for all scanout buffers. However,

	 * it may simply be too big to fit into mappable, in which case

	 * put it anyway and hope that userspace can cope (but always first

	 * try to preserve the existing ABI).

/**

 * Moves a single object to the CPU read, and possibly write domain.

 * @obj: object to act on

 * @write: requesting write or read-only access

 *

 * This function returns when the move is complete, including waiting on

 * flushes to occur.

 Flush the CPU cache if it's still invalid. */

	/* It should now be out of any other write domains, and we can update

	 * the domain values for our changes.

	/* If we're writing through the CPU, then the GPU read domains will

	 * need to be invalidated at next use.

/**

 * Called when user space prepares to use an object with the CPU, either

 * through the mmap ioctl's mapping or a GTT mapping.

 * @dev: drm device

 * @data: ioctl data blob

 * @file: drm file

 Only handle setting domains to types used by the CPU. */

	/*

	 * Having something in the write domain implies it's in the read

	 * domain, and only that read domain.  Enforce that in the request.

	/*

	 * Try to flush the object off the GPU without holding the lock.

	 * We will repeat the flush holding the lock in the normal manner

	 * to catch cases where we are gazumped.

		/*

		 * Try to grab userptr pages, iris uses set_domain to check

		 * userptr validity

	/*

	 * Proxy objects do not control access to the backing storage, ergo

	 * they cannot be used as a means to manipulate the cache domain

	 * tracking for that backing storage. The proxy object is always

	 * considered to be outside of any cache domain.

	/*

	 * Flush and acquire obj->pages so that we are coherent through

	 * direct access in memory with previous cached writes through

	 * shmemfs and that our cache domain tracking remains valid.

	 * For example, if the obj->filp was moved to swap without us

	 * being notified and releasing the pages, we would mistakenly

	 * continue to assume that the obj remained out of the CPU cached

	 * domain.

	/*

	 * Already in the desired write domain? Nothing for us to do!

	 *

	 * We apply a little bit of cunning here to catch a broader set of

	 * no-ops. If obj->write_domain is set, we must be in the same

	 * obj->read_domains, and only that domain. Therefore, if that

	 * obj->write_domain matches the request read_domains, we are

	 * already in the same read/write domain and can skip the operation,

	 * without having to further check the requested write_domain.

/*

 * Pins the specified object's pages and synchronizes the object with

 * GPU accesses. Sets needs_clflush to non-zero if the caller should

 * flush the object from the CPU cache.

	/* If we're not in the cpu read domain, set ourself into the gtt

	 * read domain and manually flush cachelines (if required). This

	 * optimizes for the case when the gpu will dirty the data

	 * anyway again before the next pread happens.

 return with the pages pinned */

	/* If we're not in the cpu write domain, set ourself into the

	 * gtt write domain and manually flush cachelines (as required).

	 * This optimizes for the case when the gpu will use the data

	 * right away and we therefore have to clflush anyway.

		/*

		 * Same trick applies to invalidate partially written

		 * cachelines read before writing.

 return with the pages pinned */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2014-2016 Intel Corporation

	/*

	 * Always aligning to the object size, allows a single allocation

	 * to handle all possible callers, and given typical object sizes,

	 * the alignment of the buddy allocation will naturally match.

 We're no longer struct page backed */

	/*

	 * We manually control the domain here and pretend that it

	 * remains coherent i.e. in the GTT domain, like shmem_pwrite.

 Perma-pin (until release) the physical set of pages */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

/*

 * Size of struct ttm_place vector in on-stack struct ttm_placement allocs

/**

 * struct i915_ttm_tt - TTM page vector with additional private information

 * @ttm: The base TTM page vector.

 * @dev: The struct device used for dma mapping and unmapping.

 * @cached_st: The cached scatter-gather table.

 *

 * Note that DMA may be going on right up to the point where the page-

 * vector is unpopulated in delayed destroy. Hence keep the

 * scatter-gather table mapped and cached up to that point. This is

 * different from the cached gem object io scatter-gather table which

 * doesn't have an associated dma mapping.

/**

 * i915_ttm_sys_placement - Return the struct ttm_placement to be

 * used for an object in system memory.

 *

 * Rather than making the struct extern, use this

 * function.

 *

 * Return: A pointer to a static variable for sys placement.

 Fastpath */

		/*

		 * TTM likes to convert -EDEADLK to -EBUSY, and wants us to

		 * restart the operation, since we don't record the contending

		 * lock. We use -EAGAIN to restart.

		/*

		 * Memory type / region is full, and we can't evict.

		 * Except possibly system, that returns -ENOMEM;

 Once / if we support GGTT, this is also false for cached ttm_tts */

	/*

	 * Objects only allowed in system get cached cpu-mappings.

	 * Other objects get WC mapping for now. Even if in system.

 Cache this on object? */

 Will do for now. Our pinned objects are still on TTM's LRU lists */

	/*

	 * If object was moved to an allowable region, update the object

	 * region to consider it migrated. Note that if it's currently not

	 * in an allowable region, it's evicted and we don't update the

	 * object region.

 TTM's purge interface. Note that we might be reentering. */

 There's some room for optimization here... */

	/*

	 * If CPU mapping differs, we need to add the ttm_tt pages to

	 * the resulting st. Might make sense for GGTT.

 Sync for now. We could do the actual copy async. */

 Populate ttm with pages if needed. Typically system memory. */

/**

 * i915_ttm_driver - Return a pointer to the TTM device funcs

 *

 * Return: Pointer to statically allocated TTM device funcs.

 First try only the requested placement. No eviction. */

		/*

		 * Anything that wants to restart the operation gets to

		 * do that.

		/*

		 * If the initial attempt fails, allow all accepted placements,

		 * evicting if necessary.

 Object either has a page vector or is an iomem object */

 Move to the requested placement. */

/**

 * DOC: Migration vs eviction

 *

 * GEM migration may not be the same as TTM migration / eviction. If

 * the TTM core decides to evict an object it may be evicted to a

 * TTM memory type that is not in the object's allowable GEM regions, or

 * in fact theoretically to a TTM memory type that doesn't correspond to

 * a GEM memory region. In that case the object's GEM region is not

 * updated, and the data is migrated back to the GEM region at

 * get_pages time. TTM may however set up CPU ptes to the object even

 * when it is evicted.

 * Gem forced migration using the i915_ttm_migrate() op, is allowed even

 * to regions that are not in the object's list of allowable placements.

	/*

	 * Reinitialize the region bindings. This is primarily

	 * required for objects where the new region is not in

	 * its allowable placements.

	/*

	 * We're currently not called from a shrinker, so put_pages()

	 * typically means the object is about to destroyed, or called

	 * from move_notify(). So just avoid doing much for now.

	 * If the object is not destroyed next, The TTM eviction logic

	 * and shrinkers will move it out if needed.

	/*

	 * Don't manipulate the TTM LRUs while in TTM bo destruction.

	 * We're called through i915_ttm_delete_mem_notify().

	/*

	 * Put on the correct LRU list depending on the MADV status

/*

 * TTM-backed gem object destruction requires some clarification.

 * Basically we have two possibilities here. We can either rely on the

 * i915 delayed destruction and put the TTM object when the object

 * is idle. This would be detected by TTM which would bypass the

 * TTM delayed destroy handling. The other approach is to put the TTM

 * object early and rely on the TTM destroyed handling, and then free

 * the leftover parts of the GEM object once TTM's destroyed list handling is

 * complete. For now, we rely on the latter for two reasons:

 * a) TTM can evict an object even when it's on the delayed destroy list,

 * which in theory allows for complete eviction.

 * b) There is work going on in TTM to allow freeing an object even when

 * it's not idle, and using the TTM destroyed list handling could help us

 * benefit from that.

 Sanity check that we allow writing into this object */

 The ttm_bo must be allocated with I915_BO_ALLOC_USER */

 This releases all gem object bindings to the backend. */

/**

 * __i915_gem_ttm_object_init - Initialize a ttm-backed i915 gem object

 * @mem: The initial memory region for the object.

 * @obj: The gem object.

 * @size: Object size in bytes.

 * @flags: gem object flags.

 *

 * Return: 0 on success, negative error code on failure.

 Don't put on a region list until we're either locked or fully initialized. */

 Forcing the page size is kernel internal only */

	/*

	 * If this function fails, it will call the destructor, but

	 * our caller still owns the object. So no freeing in the

	 * destructor until obj->ttm.created is true.

	 * Similarly, in delayed_destroy, we can't call ttm_bo_put()

	 * until successful initialization.

/**

 * i915_gem_obj_copy_ttm - Copy the contents of one ttm-based gem object to

 * another

 * @dst: The destination object

 * @src: The source object

 * @allow_accel: Allow using the blitter. Otherwise TTM memcpy is used.

 * @intr: Whether to perform waits interruptible:

 *

 * Note: The caller is responsible for assuring that the underlying

 * TTM objects are populated if needed and locked.

 *

 * Return: Zero on success. Negative error code on error. If @intr == true,

 * then it may return -ERESTARTSYS or -EINTR.

	/*

	 * Sync for now. This will change with async moves.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

/**

 * i915_ttm_backup_free - Free any backup attached to this object

 * @obj: The object whose backup is to be freed.

/**

 * struct i915_gem_ttm_pm_apply - Apply-to-region subclass for restore

 * @base: The i915_gem_apply_to_region we derive from.

 * @allow_gpu: Whether using the gpu blitter is allowed.

 * @backup_pinned: On backup, backup also pinned objects.

/**

 * i915_ttm_recover_region - Free the backup of all objects of a region

 * @mr: The memory region

 *

 * Checks all objects of a region if there is backup attached and if so

 * frees that backup. Typically this is called to recover after a partially

 * performed backup.

/**

 * i915_ttm_backup_region - Back up all objects of a region to smem.

 * @mr: The memory region

 * @allow_gpu: Whether to allow the gpu blitter for this backup.

 * @backup_pinned: Backup also pinned objects.

 *

 * Loops over all objects of a region and either evicts them if they are

 * evictable or backs them up using a backup object if they are pinned.

 *

 * Return: Zero on success. Negative error code on error.

 Content may have been swapped. */

/**

 * i915_ttm_restore_region - Restore backed-up objects of a region from smem.

 * @mr: The memory region

 * @allow_gpu: Whether to allow the gpu blitter to recover.

 *

 * Loops over all objects of a region and if they are backed-up, restores

 * them from smem.

 *

 * Return: Zero on success. Negative error code on error.

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2014-2016 Intel Corporation

/*

 * 20ms is a fairly arbitrary limit (greater than the average frame time)

 * chosen to prevent the CPU getting more than a frame ahead of the GPU

 * (when using lax throttling for the frontbuffer). We also use it to

 * offer free GPU waitboosts for severely congested workloads.

/*

 * Throttle our rendering by waiting until the ring has completed our requests

 * emitted over 20 msec ago.

 *

 * Note that if we were to use the current jiffies each time around the loop,

 * we wouldn't escape the function with any frames outstanding if the time to

 * render a frame was over 20ms.

 *

 * This should get us reasonable parallelism between CPU and GPU but also

 * relatively low latency when blocking on a particular request to finish.

 ABI: return -EIO if already wedged */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2018 Intel Corporation

	/*

	 * Pinning the contexts may generate requests in order to acquire

	 * GGTT space, so do this first before we reserve a seqno for

	 * ourselves.

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2017 Intel Corporation

	/*

	 * We repeatedly write, overwrite and read from a sequence of

	 * cachelines in order to try and detect incoherency (unflushed writes

	 * from either the CPU or GPU). Each setter/getter uses our cache

	 * domain API which should prevent incoherency.

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2017 Intel Corporation

	/*

	 * Create as many contexts as we can feasibly get away with

	 * and check we can switch between them rapidly.

	 *

	 * Serves as very simple stress test for submission and HW switching

	 * between contexts.

 Force submission order */

				/*

				 * This space is left intentionally blank.

				 *

				 * We do not actually want to perform any

				 * action with this request, we just want

				 * to measure the latency in allocation

				 * and submission of our breadcrumbs -

				 * ensuring that the bare request is sufficient

				 * for the system to work (i.e. proper HEAD

				 * tracking of the rings, interrupt handling,

				 * etc). It also gives us the lowest bounds

				 * for latency.

	/*

	 * Check we can process switches on all engines simultaneously.

 Use the first context as our template for the engines */

 Clone the same set of engines into the other contexts */

 start all threads before we kthread_stop() */

	/*

	 * Within the GTT the huge objects maps every page onto

	 * its 1024 real pages (using phys_pfn = dma_pfn % 1024).

	 * We set the nth dword within the page using the nth

	 * mapping via the GTT - this should exercise the GTT mapping

	 * whilst checking that each context provides a unique view

	 * into the object.

 tie the object to the drm_file for easy reaping */

 Keep in GEM's good graces */

	/*

	 * Create a few different contexts (with different mm) and write

	 * through each ctx/mm using the GPU making sure those writes end

	 * up in the expected pages of our obj.

 No logical context support in HW */

	/*

	 * Create a few different contexts with the same mm and write

	 * through each ctx using the GPU making sure those writes end

	 * up in the expected pages of our obj.

 not full-ppgtt; nothing to share */

		/*

		 * Gen11 VME friendly power-gated configuration with

		 * half enabled sub-slices.

 First set the default mask. */

 Then set a power-gated configuration. */

 Back to defaults. */

 One last power-gated configuration for the road. */

	/*

	 * Create a few read-only objects (with the occasional writable object)

	 * and try to write into these object checking that the GPU discards

	 * any write to a read-only object.

 hsw: register access even to 3DPRIM! is protected */

	/*

	 * The simple goal here is that a write into one context is not

	 * observed in a second (separate page tables and scratch).

 We can only test vm isolation, if the vm are distinct */

 Read the initial state of the scratch page */

 Not all engines have their own GPR! */

 Leave enough space at offset 0 for the batch */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 room to maneuver */

 Use scratch to fill objects */

 We want to check position invariant tiling across GTT eviction */

 Reposition so that we overlap the old addresses, and slightly off */

 Test requires explicit BLT tiling controls */

 Requires sane (sub-page) swizzling */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2016 Intel Corporation

 largest tile row x2 */

	/* We want to check the page mapping and fencing of a large object

	 * mmapped through the GTT. The object we create is larger than can

	 * possibly be mmaped as a whole, and so we must use partial GGTT vma.

	 * We then check that a write through each partial GGTT vma ends up

	 * in the right set of pages within the object, and with the expected

	 * tiling, which we verify by manual swizzling.

			/*

			 * The swizzling pattern is actually unknown as it

			 * varies based on physical address of each page.

			 * See i915_gem_detect_bit_6_swizzle().

 largest tile row x2 */

	/*

	 * igt_partial_tiling() does an exhastive check of partial tiling

	 * chunking, but will undoubtably run out of time. Here, we do a

	 * randomised search and hope over many runs of 1s with different

	 * seeds we will do a thorough check.

	 *

	 * Remember to look at the st_seed if we see a flip-flop in BAT!

 leave it only alive via its active ref */

 Disable background reaper */

 Trim the device mmap space to only a page */

 PAGE_SIZE units */

 Just fits! */

 Too large */

 Fill the hole, further allocation attempts should then fail */

 Now fill with busy dead objects that we expect to reap */

	/*

	 * Verify that the mmap access into the backing store aligns with

	 * that of the GPU, i.e. that mmap is indeed writing into the same

	 * page as being read by the GPU.

	/*

	 * After unbinding the object from the GGTT, its address may be reused

	 * for other objects. Ergo we have to revoke the previous mmap PTE

	 * access as it no longer points to the same object.

 ttm allows access to evicted regions by design */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2016 Intel Corporation

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2016 Intel Corporation

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2016 Intel Corporation

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2016 Intel Corporation

	/*

	 * We expect an import of an LMEM-only object to fail with

	 * -EOPNOTSUPP because it can't be migrated to SMEM.

	/*

	 * If the exported object is not in system memory, something

	 * weird is going on. TODO: When p2p is supported, this is no

	 * longer considered weird.

 Now try a fake an importer */

 Can not yet map dmabuf */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2016 Intel Corporation

 Basic test to ensure we can create an object */

 just to be awkward */

 Basic sanitycheck of our huge fake object allocation */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020-2021 Intel Corporation

 Switch object backing-store on create */

 From LMEM to shmem and back again */

 Initial GPU fill, sync, CPU initialization. */

	/*

	 * Migrate to and from smem without explicitly syncing.

	 * Finalize with data in smem for fast readout.

 Finally sync migration and check content. */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2016 Intel Corporation

	/* Create an object and bind it to a contiguous set of physical pages,

	 * i.e. exercise the i915_gem_object_phys API.

 Make the object dirty so that put_pages must do copy back the data */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2017 Intel Corporation

	/*

	 * Our goal here is simple, we want to greedily fill the object from

	 * largest to smallest page-size, while ensuring that we use *every*

	 * page-size as per the given page-mask.

 Use optimal page sized chunks to fill in the sg table */

 We have to wait for the async bind to complete before our asserts */

	/*

	 * The dma-api is like a box of chocolates when it comes to the

	 * alignment of dma addresses, however for LMEM we have total control

	 * and so can guarantee alignment, likewise when we allocate our blocks

	 * they should appear in descending order, and if we know that we align

	 * to the largest page size for the GTT address, we should be able to

	 * assert that if we see 2M physical pages then we should also get 2M

	 * GTT pages. If we don't then something might be wrong in our

	 * construction of the backing pages.

	 *

	 * Maintaining alignment is required to utilise huge pages in the ppGGT.

	/*

	 * Sanity check creating objects with every valid page support

	 * combination for our mock device.

 Required for ppGTT */

	/*

	 * Sanity check dma misalignment for huge pages -- the dma addresses we

	 * insert into the paging structures need to always respect the page

	 * size alignment.

 Force the page size for this object */

		/*

		 * Try all the other valid offsets until the next

		 * boundary -- should always fall back to using 4K

		 * pages.

		/*

		 * Figure out the expected gtt page size knowing that we go from

		 * largest to smallest page size sg chunks, and that we align to

		 * the largest page size.

 Cases with forced padding/alignment */

 Try without any forced padding/alignment */

	/*

	 * Sanity check some of the trickiness with 64K pages -- either we can

	 * safely mark the whole page-table(2M block) as 64K, or we have to

	 * always fallback to 4K.

			/*

			 * Disable 2M pages -- We only want to use 64K/4K pages

			 * for this test.

		/*

		 * The ggtt may have some pages reserved so

		 * refrain from erroring out.

	/*

	 * To keep things interesting when alternating between engines in our

	 * randomized order, lets also make feeding to the same engine a few

	 * times in succession a possibility by enlarging the permutation array.

	/*

	 * Try various offsets in an ascending/descending fashion until we

	 * timeout -- we want to avoid issues hidden by effectively always using

	 * offset = 0.

		/*

		 * In order to utilize 64K pages we need to both pad the vma

		 * size and ensure the vma offset is at the start of the pt

		 * boundary, however to improve coverage we opt for testing both

		 * aligned and unaligned offsets.

	/*

	 * Sanity check that the HW uses huge pages correctly through our

	 * various backends -- ensure that our writes land in the right place.

	/*

	 * Sanity check that the HW behaves with a limited set of combinations.

	 * We already have a bunch of randomised testing, which should give us

	 * a decent amount of variation between runs, however we should keep

	 * this to limit the chances of introducing a temporary regression, by

	 * testing the most obvious cases that might make something blow up.

	/*

	 * Make sure that we don't burst into a ball of flames upon falling back

	 * to tmpfs, which we rely on if on the off-chance we encouter a failure

	 * when setting up gemfs.

	/*

	 * Sanity check shrinking huge-paged object -- make sure nothing blows

	 * up.

	/*

	 * Nuke everything *before* we unpin the pages so we can be reasonably

	 * sure that when later checking get_nr_swap_pages() that some random

	 * leftover object doesn't steal the remaining swap space.

	/*

	 * Now that the pages are *unpinned* shrinking should invoke

	 * shmem to truncate our pages, if we have available swap.

 Pretend to be a device which supports the 48b PPGTT */

 If we were ever hit this then it's time to mock the 64K scratch */

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Eddie Dong <eddie.dong@intel.com>

 *    Kevin Tian <kevin.tian@intel.com>

 *

 * Contributors:

 *    Ping Gao <ping.a.gao@intel.com>

 *    Zhi Wang <zhi.a.wang@intel.com>

 *    Bing Niu <bing.niu@intel.com>

 *

 setup the ballooning information */

	/* A vGPU with a weight of 8 will get twice as much GPU as a vGPU

	 * with a weight of 4 on a contended host, different vGPU type has

	 * different weight set. Legal weights range from 1 to 16.

 Fixed vGPU type table */

/**

 * intel_gvt_init_vgpu_types - initialize vGPU type list

 * @gvt : GVT device

 *

 * Initialize vGPU type list based on available resource.

 *

	/* vGPU type name is defined as GVTg_Vx_y which contains

	 * physical GPU generation type (e.g V4 as BDW server, V5 as

	 * SKL server).

	 *

	 * Depend on physical SKU resource, might see vGPU types like

	 * GVTg_V4_8, GVTg_V4_4, GVTg_V4_2, etc. We can create

	 * different types of vGPU on same physical GPU depending on

	 * available resource. Each vGPU type will have "avail_instance"

	 * to indicate how many vGPU instance can be created for this

	 * type.

	 *

	/* Need to depend on maxium hw resource size but keep on

	 * static config for now.

/**

 * intel_gvt_active_vgpu - activate a virtual GPU

 * @vgpu: virtual GPU

 *

 * This function is called when user wants to activate a virtual GPU.

 *

/**

 * intel_gvt_deactive_vgpu - deactivate a virtual GPU

 * @vgpu: virtual GPU

 *

 * This function is called when user wants to deactivate a virtual GPU.

 * The virtual GPU will be stopped.

 *

/**

 * intel_gvt_release_vgpu - release a virtual GPU

 * @vgpu: virtual GPU

 *

 * This function is called when user wants to release a virtual GPU.

 * The virtual GPU will be stopped and all runtime information will be

 * destroyed.

 *

/**

 * intel_gvt_destroy_vgpu - destroy a virtual GPU

 * @vgpu: virtual GPU

 *

 * This function is called when user wants to destroy a virtual GPU.

 *

	/*

	 * remove idr first so later clean can judge if need to stop

	 * service if no active vgpu.

/**

 * intel_gvt_create_idle_vgpu - create an idle virtual GPU

 * @gvt: GVT device

 *

 * This function is called when user wants to create an idle virtual GPU.

 *

 * Returns:

 * pointer to intel_vgpu, error pointer if failed.

/**

 * intel_gvt_destroy_vgpu - destroy an idle virtual GPU

 * @vgpu: virtual GPU

 *

 * This function is called when user wants to destroy an idle virtual GPU.

 *

/**

 * intel_gvt_create_vgpu - create a virtual GPU

 * @gvt: GVT device

 * @type: type of the vGPU to create

 *

 * This function is called when user wants to create a virtual GPU.

 *

 * Returns:

 * pointer to intel_vgpu, error pointer if failed.

 XXX current param based on MB */

 calculate left instance change for types */

/**

 * intel_gvt_reset_vgpu_locked - reset a virtual GPU by DMLR or GT reset

 * @vgpu: virtual GPU

 * @dmlr: vGPU Device Model Level Reset or GT Reset

 * @engine_mask: engines to reset for GT reset

 *

 * This function is called when user wants to reset a virtual GPU through

 * device model reset or GT reset. The caller should hold the vgpu lock.

 *

 * vGPU Device Model Level Reset (DMLR) simulates the PCI level reset to reset

 * the whole vGPU to default state as when it is created. This vGPU function

 * is required both for functionary and security concerns.The ultimate goal

 * of vGPU FLR is that reuse a vGPU instance by virtual machines. When we

 * assign a vGPU to a virtual machine we must isse such reset first.

 *

 * Full GT Reset and Per-Engine GT Reset are soft reset flow for GPU engines

 * (Render, Blitter, Video, Video Enhancement). It is defined by GPU Spec.

 * Unlike the FLR, GT reset only reset particular resource of a vGPU per

 * the reset request. Guest driver can issue a GT reset by programming the

 * virtual GDRST register to reset specific virtual GPU engine or all

 * engines.

 *

 * The parameter dev_level is to identify if we will do DMLR or GT reset.

 * The parameter engine_mask is to specific the engines that need to be

 * resetted. If value ALL_ENGINES is given for engine_mask, it means

 * the caller requests a full GT reset that we will reset all virtual

 * GPU engines. For FLR, engine_mask is ignored.

	/*

	 * The current_vgpu will set to NULL after stopping the

	 * scheduler when the reset is triggered by current vgpu.

 full GPU reset or device model level reset */

fence will not be reset during virtual reset */

 only reset the failsafe mode when dmlr reset */

			/*

			 * PCI_D0 is set before dmlr, so reset d3_entered here

			 * after done using.

/**

 * intel_gvt_reset_vgpu - reset a virtual GPU (Function Level)

 * @vgpu: virtual GPU

 *

 * This function is called when user wants to reset a virtual GPU.

 *

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Kevin Tian <kevin.tian@intel.com>

 *

 * Contributors:

 *    Bing Niu <bing.niu@intel.com>

 *    Xu Han <xu.han@intel.com>

 *    Ping Gao <ping.a.gao@intel.com>

 *    Xiaoguang Chen <xiaoguang.chen@intel.com>

 *    Yang Liu <yang2.liu@intel.com>

 *    Tina Zhang <tina.zhang@intel.com>

 *

 Pixel format in DRM definition */

 Bits per pixel, 0 indicates invalid */

 The description */

 non-supported format has bpp default to 0 */

 non-supported format has bpp default to 0 */

/**

 * intel_vgpu_decode_primary_plane - Decode primary plane

 * @vgpu: input vgpu

 * @plane: primary plane to save decoded info

 * This function is called for decoding plane

 *

 * Returns:

 * 0 on success, non-zero if failed.

 raw height is one minus the real value */

 Pixel format in DRM definition */

 Bits per pixel; 0 indicates invalid */

 In pixel */

 In lines */

 The description */

 non-supported format has bpp default to 0 */

/**

 * intel_vgpu_decode_cursor_plane - Decode sprite plane

 * @vgpu: input vgpu

 * @plane: cursor plane to save decoded info

 * This function is called for decoding plane

 *

 * Returns:

 * 0 on success, non-zero if failed.

/**

 * intel_vgpu_decode_sprite_plane - Decode sprite plane

 * @vgpu: input vgpu

 * @plane: sprite plane to save decoded info

 * This function is called for decoding plane

 *

 * Returns:

 * 0 on success, non-zero if failed.

	/* Order of RGB values in an RGBxxx buffer may be ordered RGB or

	 * BGR depending on the state of the color_order field

 yuv_order has only 2 bits */

 raw height is one minus the real value */

 raw width is one minus the real value */

/*

 * Copyright(c) 2011-2017 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

/**

 * intel_vgpu_find_page_track - find page track rcord of guest page

 * @vgpu: a vGPU

 * @gfn: the gfn of guest page

 *

 * Returns:

 * A pointer to struct intel_vgpu_page_track if found, else NULL returned.

/**

 * intel_vgpu_register_page_track - register a guest page to be tacked

 * @vgpu: a vGPU

 * @gfn: the gfn of guest page

 * @handler: page track handler

 * @priv: tracker private

 *

 * Returns:

 * zero on success, negative error code if failed.

/**

 * intel_vgpu_unregister_page_track - unregister the tracked guest page

 * @vgpu: a vGPU

 * @gfn: the gfn of guest page

 *

/**

 * intel_vgpu_enable_page_track - set write-protection on guest page

 * @vgpu: a vGPU

 * @gfn: the gfn of guest page

 *

 * Returns:

 * zero on success, negative error code if failed.

/**

 * intel_vgpu_enable_page_track - cancel write-protection on guest page

 * @vgpu: a vGPU

 * @gfn: the gfn of guest page

 *

 * Returns:

 * zero on success, negative error code if failed.

/**

 * intel_vgpu_page_track_handler - called when write to write-protected page

 * @vgpu: a vGPU

 * @gpa: the gpa of this write

 * @data: the writed data

 * @bytes: the length of this write

 *

 * Returns:

 * zero on success, negative error code if failed.

 Remove write protection to prevent furture traps. */

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Ke Yu

 *    Zhiyuan Lv <zhiyuan.lv@intel.com>

 *

 * Contributors:

 *    Terrence Xu <terrence.xu@intel.com>

 *    Changbin Du <changbin.du@intel.com>

 *    Bing Niu <bing.niu@intel.com>

 *    Zhi Wang <zhi.a.wang@intel.com>

 *

 EDID with 1024x768 as its resolution */

Header*/

 Vendor & Product Identification */

 Version & Revision */

 Basic Display Parameters & Features */

 Color Characteristics */

 Established Timings: maximum resolution is 1024x768 */

 Standard Timings. All invalid */

 18 Byte Data Blocks 1: invalid */

 18 Byte Data Blocks 2: invalid */

 18 Byte Data Blocks 3: invalid */

 18 Byte Data Blocks 4: invalid */

 Extension Block Count */

 Checksum */

 EDID with 1920x1200 as its resolution */

Header*/

 Vendor & Product Identification */

 Version & Revision */

 Basic Display Parameters & Features */

 Color Characteristics */

 Established Timings: maximum resolution is 1024x768 */

		/*

		 * Standard Timings.

		 * below new resolutions can be supported:

		 * 1920x1080, 1280x720, 1280x960, 1280x1024,

		 * 1440x900, 1600x1200, 1680x1050

 18 Byte Data Blocks 1: max resolution is 1920x1200 */

 18 Byte Data Blocks 2: invalid */

 18 Byte Data Blocks 3: invalid */

 18 Byte Data Blocks 4: invalid */

 Extension Block Count */

 Checksum */

 let the virtual display supports DP1.2 */

 Clear PIPE, DDI, PHY, HPD before setting new */

 No hpd_invert set in vgpu vbt, need to clear invert mask */

		/*

		 * Only 1 PIPE enabled in current vGPU display and PIPE_A is

		 *  tied to TRANSCODER_A in HW, so it's safe to assume PIPE_A,

		 *   TRANSCODER_A can be enabled. PORT_x depends on the input of

		 *   setup_virtual_dp_monitor.

		/*

		 * Golden M/N are calculated based on:

		 *   24 bpp, 4 lanes, 154000 pixel clk (from virtual EDID),

		 *   DP link clk 1620 MHz and non-constant_n.

		 * TODO: calculate DP link symbol clk and stream clk m/n.

 Enable per-DDI/PORT vreg */

		/*

		 * Only 1 PIPE enabled in current vGPU display and PIPE_A is

		 *  tied to TRANSCODER_A in HW, so it's safe to assume PIPE_A,

		 *   TRANSCODER_A can be enabled. PORT_x depends on the input of

		 *   setup_virtual_dp_monitor, we can bind DPLL0 to any PORT_x

		 *   so we fixed to DPLL0 here.

		 * Setup DPLL0: DP link clk 1620 MHz, non SSC, DP Mode

		/*

		 * Golden M/N are calculated based on:

		 *   24 bpp, 4 lanes, 154000 pixel clk (from virtual EDID),

		 *   DP link clk 1620 MHz and non-constant_n.

		 * TODO: calculate DP link symbol clk and stream clk m/n.

 Clear host CRT status, so guest couldn't detect this host CRT. */

 Disable Primary/Sprite/Cursor plane */

 Set vblank emulation request per-vGPU bit */

 Init hrtimer based on default refresh rate */

/**

 * vgpu_update_vblank_emulation - Update per-vGPU vblank_timer

 * @vgpu: vGPU operated

 * @turnon: Turn ON/OFF vblank_timer

 *

 * This function is used to turn on/off or update the per-vGPU vblank_timer

 * when PIPECONF is enabled or disabled. vblank_timer period is also updated

 * if guest changed the refresh rate.

 *

		/*

		 * Skip the re-enable if already active and vrefresh unchanged.

		 * Otherwise, stop timer if already active and restart with new

		 *   period.

 Stop timer before start with new period if active */

 Make sure new refresh rate updated to timer period */

 Caller request to stop vblank */

/**

 * intel_vgpu_emulate_hotplug - trigger hotplug event for vGPU

 * @vgpu: a vGPU

 * @connected: link state

 *

 * This function is used to trigger hotplug interrupt for vGPU

 *

 TODO: add more platforms support */

/**

 * intel_vgpu_clean_display - clean vGPU virtual display emulation

 * @vgpu: a vGPU

 *

 * This function is used to clean vGPU virtual display emulation stuffs

 *

/**

 * intel_vgpu_init_display- initialize vGPU virtual display emulation

 * @vgpu: a vGPU

 * @resolution: resolution index for intel_vgpu_edid

 *

 * This function is used to initialize vGPU virtual display emulation stuffs

 *

 * Returns:

 * Zero on success, negative error code if failed.

 *

/**

 * intel_vgpu_reset_display- reset vGPU virtual display emulation

 * @vgpu: a vGPU

 *

 * This function is used to reset vGPU virtual display emulation stuffs

 *

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Anhua Xu

 *    Kevin Tian <kevin.tian@intel.com>

 *

 * Contributors:

 *    Min He <min.he@intel.com>

 *    Bing Niu <bing.niu@intel.com>

 *    Zhi Wang <zhi.a.wang@intel.com>

 *

 We give 2 seconds higher prio for vGPU during start */

	/* The timeslice accumulation reset at stage 0, which is

	 * allocated again without adding previous debt.

			/* timeslice for next 100ms should add the left/debt

			 * slice of previous stages.

	/* no need to schedule if next_vgpu is the same with current_vgpu,

	 * let scheduler chose next_vgpu again by setting it to NULL.

	/*

	 * after the flag is set, workload dispatch thread will

	 * stop dispatching workload for current vgpu

 still have uncompleted workload? */

 switch current vgpu */

 wake up workload dispatch thread */

 search a vgpu with pending workload */

 Return the vGPU only if it has time slice left */

 in nanosecond */

 no active vgpu or has already had a target */

 Move the last used vGPU to the tail of lru_list */

 this vgpu id has been removed */

/* for per-vgpu scheduler policy, there are 2 per-vgpu data:

 * sched_data, and sched_ctl. We see these 2 data as part of

 * the global scheduler which are proteced by gvt->sched_lock.

 * Caller should make their decision if the vgpu_lock should

 * be hold outside.

 stop workload dispatching */

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Kevin Tian <kevin.tian@intel.com>

 *    Zhi Wang <zhi.a.wang@intel.com>

 *

 * Contributors:

 *    Min he <min.he@intel.com>

 *

 common offset among interrupt control registers */

/**

 * intel_vgpu_reg_imr_handler - Generic IMR register emulation write handler

 * @vgpu: a vGPU

 * @reg: register offset written by guest

 * @p_data: register data written by guest

 * @bytes: register data length

 *

 * This function is used to emulate the generic IMR register bit change

 * behavior.

 *

 * Returns:

 * Zero on success, negative error code if failed.

 *

/**

 * intel_vgpu_reg_master_irq_handler - master IRQ write emulation handler

 * @vgpu: a vGPU

 * @reg: register offset written by guest

 * @p_data: register data written by guest

 * @bytes: register data length

 *

 * This function is used to emulate the master IRQ register on gen8+.

 *

 * Returns:

 * Zero on success, negative error code if failed.

 *

	/*

	 * GEN8_MASTER_IRQ is a special irq register,

	 * only bit 31 is allowed to be modified

	 * and treated as an IER bit.

/**

 * intel_vgpu_reg_ier_handler - Generic IER write emulation handler

 * @vgpu: a vGPU

 * @reg: register offset written by guest

 * @p_data: register data written by guest

 * @bytes: register data length

 *

 * This function is used to emulate the generic IER register behavior.

 *

 * Returns:

 * Zero on success, negative error code if failed.

 *

/**

 * intel_vgpu_reg_iir_handler - Generic IIR write emulation handler

 * @vgpu: a vGPU

 * @reg: register offset written by guest

 * @p_data: register data written by guest

 * @bytes: register data length

 *

 * This function is used to emulate the generic IIR register behavior.

 *

 * Returns:

 * Zero on success, negative error code if failed.

 *

 =======================vEvent injection===================== */

 =======================vEvent Handlers===================== */

 =====================GEN specific logic======================= */

 GEN8 interrupt routines. */

 GEN8 level 2 interrupts. */

 GEN8 interrupt GT0 events */

 GEN8 interrupt GT1 events */

 GEN8 interrupt GT3 events */

 GEN8 interrupt DE PORT events */

 GEN8 interrupt DE MISC events */

 PCH events */

 GEN8 interrupt PCU events */

/**

 * intel_vgpu_trigger_virtual_event - Trigger a virtual event for a vGPU

 * @vgpu: a vGPU

 * @event: interrupt event

 *

 * This function is used to trigger a virtual interrupt event for vGPU.

 * The caller provides the event to be triggered, the framework itself

 * will emulate the IRQ register bit change.

 *

/**

 * intel_gvt_init_irq - initialize GVT-g IRQ emulation subsystem

 * @gvt: a GVT device

 *

 * This function is called at driver loading stage, to initialize the GVT-g IRQ

 * emulation subsystem.

 *

 * Returns:

 * Zero on success, negative error code if failed.

 common event initialization */

 gen specific initialization */

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Zhiyuan Lv <zhiyuan.lv@intel.com>

 *    Zhi Wang <zhi.a.wang@intel.com>

 *

 * Contributors:

 *    Min He <min.he@intel.com>

 *    Bing Niu <bing.niu@intel.com>

 *    Ping Gao <ping.a.gao@intel.com>

 *    Tina Zhang <tina.zhang@intel.com>

 *

 Update the CSB and CSB write pointer in HWSP */

 ctx1 is valid, ctx0/ctx is scheduled-out -> element switch */

		/*

		 * ctx1 is not valid, ctx == ctx0

		 * ctx1 is valid, ctx1 == ctx

		 *	--> last element is finished

		 * emulate:

		 *	active-to-idle if there is *no* pending execlist

		 *	context-complete if there *is* pending execlist

	/*

	 * no running execlist, make this write bundle as running execlist

	 * -> idle-to-active

	/*

	 * already has an running execlist

	 *	a. running ctx1 is valid,

	 *	   ctx0 is finished, and running ctx1 == new execlist ctx[0]

	 *	b. running ctx1 is not valid,

	 *	   ctx0 == new execlist ctx[0]

	 * ----> lite-restore + preempted

 condition a */

 condition b */

		/*

		 * otherwise

		 * --> emulate pending execlist exist + but no preemption case

 submit workload */

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Ke Yu

 *    Kevin Tian <kevin.tian@intel.com>

 *    Dexuan Cui

 *

 * Contributors:

 *    Tina Zhang <tina.zhang@intel.com>

 *    Min He <min.he@intel.com>

 *    Niu Bing <bing.niu@intel.com>

 *    Zhi Wang <zhi.a.wang@intel.com>

 *

/**

 * intel_vgpu_gpa_to_mmio_offset - translate a GPA to MMIO offset

 * @vgpu: a vGPU

 * @gpa: guest physical address

 *

 * Returns:

 * Zero on success, negative error code if failed

/**

 * intel_vgpu_emulate_mmio_read - emulate MMIO read

 * @vgpu: a vGPU

 * @pa: guest physical address

 * @p_data: data return buffer

 * @bytes: access data length

 *

 * Returns:

 * Zero on success, negative error code if failed

/**

 * intel_vgpu_emulate_mmio_write - emulate MMIO write

 * @vgpu: a vGPU

 * @pa: guest physical address

 * @p_data: write data buffer

 * @bytes: access data length

 *

 * Returns:

 * Zero on success, negative error code if failed

/**

 * intel_vgpu_reset_mmio - reset virtual MMIO space

 * @vgpu: a vGPU

 * @dmlr: whether this is device model level reset

 set the bit 0:2(Core C-State ) to C0 */

 uc reset hw expect GS_MIA_IN_RESET */

		/* only reset the engine related, so starting with 0x44200

		 * interrupt include DE,display mmio related will not be

		 * touched

/**

 * intel_vgpu_init_mmio - init MMIO  space

 * @vgpu: a vGPU

 *

 * Returns:

 * Zero on success, negative error code if failed

/**

 * intel_vgpu_clean_mmio - clean MMIO space

 * @vgpu: a vGPU

 *

/*

 * KVMGT - the implementation of Intel mediated pass-through framework for KVM

 *

 * Copyright(c) 2014-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Kevin Tian <kevin.tian@intel.com>

 *    Jike Song <jike.song@intel.com>

 *    Xiaoguang Chen <xiaoguang.chen@intel.com>

 helper macros copied from vfio-pci */

	/*

	 * Two caches are used to avoid mapping duplicated pages (eg.

	 * scratch pages). This help to reduce dma setup overhead.

 Pin a normal or compound guest page for dma. */

	/*

	 * We pin the pages one-by-one to avoid allocating a big arrary

	 * on stack to hold pfns.

 Setup DMA mapping. */

 gfn_cache maps gfn to struct gvt_dma. */

 dma_addr_cache maps dma addr to struct gvt_dma. */

 read-only regs */

	/* Each vgpu has its own opregion, although VFIO would create another

	 * one later. This one is used to expose opregion to VFIO. And the

	 * other one created by VFIO later, is used by guest actually.

 TODO: Add multi-port and EDID extension block support */

 the only action we care about */

	/* Take a module reference as mdev core doesn't take

	 * a reference for vendor driver.

 dereference module reference taken at open */

 1M mem BAR treated as 32-bit BAR */

 mem unknown type treated as 32-bit BAR */

 Only allow MMIO GGTT entry access */

 Only support GGTT entry 8 bytes read */

 Only support GGTT entry 8 bytes write */

 XXX Need masking support exported */

	/*

	 * When guest is poweroff, msi_trigger is set to NULL, but vgpu's

	 * config and mmio register isn't restored to default during guest

	 * poweroff. If this vgpu is still used in next vm, this vgpu's pipe

	 * may be enabled, then once this vgpu is active, it will get inject

	 * vblank interrupt request. But msi_trigger is null until msi is

	 * enabled by guest. so if msi_trigger is null, success is still

	 * returned and don't inject interrupt into guest.

 the same gfn with different size: unmap and re-map */

/*

 * Copyright(c) 2011-2017 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 Compare two diff_mmio items. */

 Show the all the different values of tracked mmio. */

 Recognize all the diff mmios to list. */

 In an ascending order by mmio offset. */

/*

 * set/unset bit engine_id of vgpu->scan_nonprivbb to turn on/off scanning

 * of non-privileged batch buffer. e.g.

 * if vgpu->scan_nonprivbb=3, then it will scan non-privileged batch buffer

 * on engine 0 and 1.

/**

 * intel_gvt_debugfs_add_vgpu - register debugfs entries for a vGPU

 * @vgpu: a vGPU

/**

 * intel_gvt_debugfs_remove_vgpu - remove debugfs entries of a vGPU

 * @vgpu: a vGPU

/**

 * intel_gvt_debugfs_init - register gvt debugfs root entry

 * @gvt: GVT device

/**

 * intel_gvt_debugfs_clean - remove debugfs entries

 * @gvt: GVT device

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Zhi Wang <zhi.a.wang@intel.com>

 *

 * Contributors:

 *    Changbin Du <changbin.du@intel.com>

 *

 protect the data after this field */

 offset in the file */

 offset in the file */

 Take a snapshot of hw mmio registers. */

/**

 * intel_gvt_free_firmware - free GVT firmware

 * @gvt: intel gvt device

 *

/**

 * intel_gvt_load_firmware - load GVT firmware

 * @gvt: intel gvt device

 *

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

/*

 * Note: Only for GVT-g virtual VBT generation, other usage must

 * not do like this.

 device handle */

 data size */

/* For supporting windows guest with opregion, here hardcode the emulated

 * bdb header version as '186', and the corresponding child_device_config

 * length should be '33' but not '38'.

 158 */

 158 */

 169 */

 204 */

 161 */

 161 */

 198 */

 198 */

 202 */

 198 */

  202 */

 for add-in card */

 for add-in card */

 158 */

 184 */

 192 */

 196 */

 BXT 196 */

 158 */

 171 */

 header->bdb_offset point to bdb_header offset */

 there's features depending on version! */

 child_dev_size = 33 */

 general features */

 child device */

 each port has one child */

 size will include child devices */

 portA */

 portB */

 portC */

 portD */

 driver features */

/**

 * intel_vgpu_init_opregion - initialize the stuff used to emulate opregion

 * @vgpu: a vGPU

 *

 * Returns:

 * Zero on success, negative error code if failed.

 emulated opregion with VBT mailbox only */

	/* for unknown reason, the value in LID field is incorrect

	 * which block the windows guest, so workaround it by force

	 * setting it to "OPEN"

 emulated vbt from virt vbt generation */

/**

 * intel_vgpu_opregion_base_write_handler - Opregion base register write handler

 *

 * @vgpu: a vGPU

 * @gpa: guest physical address of opregion

 *

 * Returns:

 * Zero on success, negative error code if failed.

		/**

		 * Wins guest on Xengt will write this register twice: xen

		 * hvmloader and windows graphic driver.

/**

 * intel_vgpu_clean_opregion - clean the stuff used to emulate opregion

 * @vgpu: a vGPU

 *

 Guest opregion is released by VFIO */

/**

 * intel_vgpu_emulate_opregion_request - emulating OpRegion request

 * @vgpu: a vGPU

 * @swsci: SWSCI request

 *

 * Returns:

 * Zero on success, negative error code if failed

 ignore non 0->1 trasitions */

		/*

		 * emulate exit status of function call, '0' means

		 * "failure, generic, unsupported or unknown cause"

/*

 * GTT virtualization

 *

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Zhi Wang <zhi.a.wang@intel.com>

 *    Zhenyu Wang <zhenyuw@linux.intel.com>

 *    Xiao Zheng <xiao.zheng@intel.com>

 *

 * Contributors:

 *    Min He <min.he@intel.com>

 *    Bing Niu <bing.niu@intel.com>

 *

/*

 * validate a gm address and related range size,

 * translate it to host gm address

 translate a guest gmadr to host gmadr */

 translate a host gmadr to guest gmadr */

/*

 * Mappings between GTT_TYPE* enumerations.

 * Following information can be found according to the given type:

 * - type of next level page table

 * - type of entry inside this level page table

 * - type of entry with PSE set

 *

 * If the given type doesn't have such a kind of information,

 * e.g. give a l4 root entry type, then request to get its PSE type,

 * give a PTE page table type, then request to get its next level page

 * table type, as we know l4 root entry doesn't have a PSE bit,

 * and a PTE page table doesn't have a next level page table type,

 * GTT_TYPE_INVALID will be returned. This is useful when traversing a

 * page table.

 We take IPS bit as 'PSE' for PTE level. */

 splited 64K gtt entry */

	/*

	 * i915 writes PDP root pointer registers without present bit,

	 * it also works, so we need to treat root pointer entry

	 * specifically.

/*

 * Per-platform GMA routines.

 Update entry type per pse and ips bit. */

/*

 * MM helpers.

/*

 * PPGTT shadow page table helpers.

 Find a spt by guest gfn. */

 Find the spt by shadow page mfn. */

 Allocate shadow page table without guest page. */

	/*

	 * Init shadow_page.

 Allocate shadow page table associated with specific gfn. */

	/*

	 * Init guest_page.

 Uninitialized spte or unshadowed spte. */

 We don't setup 64K shadow entry so far. */

 64K paging only controlled by IPS bit in PTE now. */

 Because we always split 64KB pages, so clear IPS in shadow PDE. */

/**

 * Check if can do 2M page

 * @vgpu: target vgpu

 * @entry: target pfn's gtt entry

 *

 * Return 1 if 2MB huge gtt shadowing is possible, 0 if miscondition,

 * negative if found err.

 Copy the PAT field from PDE. */

 Clear dirty field. */

		/*

		 * The layout of 64K page is special, the page size is

		 * controlled by uper PDE. To be simple, we always split

		 * 64K page to smaller 4K pages in shadow PT.

 direct shadow */

 We don't setup 64K shadow entry so far. */

/**

 * intel_vgpu_sync_oos_pages - sync all the out-of-synced shadow for vGPU

 * @vgpu: a vGPU

 *

 * This function is called before submitting a guest workload to host,

 * to sync all the out-of-synced shadow for vGPU

 *

 * Returns:

 * Zero on success, negative error code if failed.

/*

 * The heart of PPGTT shadow page table.

	/*

	 * Adding the new entry first and then removing the old one, that can

	 * guarantee the ppgtt table is validated during the window between

	 * adding and removal.

 For 64KB splited entries, we need clear them all. */

/**

 * intel_vgpu_flush_post_shadow - flush the post shadow transactions

 * @vgpu: a vGPU

 *

 * This function is called before submitting a guest workload to host,

 * to flush all the post shadows for a vGPU.

 *

 * Returns:

 * Zero on success, negative error code if failed.

	/*

	 * For page table which has 64K gtt entry, only PTE#0, PTE#16,

	 * PTE#32, ... PTE#496 are used. Unused PTEs update should be

	 * ignored.

/**

 * intel_vgpu_create_ppgtt_mm - create a ppgtt mm object for a vGPU

 * @vgpu: a vGPU

 * @root_entry_type: ppgtt root entry type

 * @pdps: guest pdps.

 *

 * This function is used to create a ppgtt mm object for a vGPU.

 *

 * Returns:

 * Zero on success, negative error code in pointer if failed.

/**

 * _intel_vgpu_mm_release - destroy a mm object

 * @mm_ref: a kref object

 *

 * This function is used to destroy a mm object for vGPU

 *

/**

 * intel_vgpu_unpin_mm - decrease the pin count of a vGPU mm object

 * @mm: a vGPU mm object

 *

 * This function is called when user doesn't want to use a vGPU mm object

/**

 * intel_vgpu_pin_mm - increase the pin count of a vGPU mm object

 * @mm: target vgpu mm

 *

 * This function is called when user wants to use a vGPU mm object. If this

 * mm object hasn't been shadowed yet, the shadow will be populated at this

 * time.

 *

 * Returns:

 * Zero on success, negative error code if failed.

/*

 * GMA translation APIs.

/**

 * intel_vgpu_gma_to_gpa - translate a gma to GPA

 * @mm: mm object. could be a PPGTT or GGTT mm object

 * @gma: graphics memory address in this mm object

 *

 * This function is used to translate a graphics memory address in specific

 * graphics memory space to guest physical address.

 *

 * Returns:

 * Guest physical address on success, INTEL_GVT_INVALID_ADDR if failed.

 walk the shadow page table and get gpa from guest entry */

/**

 * intel_vgpu_emulate_gtt_mmio_read - emulate GTT MMIO register read

 * @vgpu: a vGPU

 * @off: register offset

 * @p_data: data will be returned to guest

 * @bytes: data length

 *

 * This function is used to emulate the GTT MMIO register read

 *

 * Returns:

 * Zero on success, error code if failed.

 the VM may configure the whole GM space when ballooning is used */

	/* If ggtt entry size is 8 bytes, and it's split into two 4 bytes

	 * write, save the first 4 bytes in a list and update virtual

	 * PTE. Only update shadow PTE when the second 4 bytes comes.

 the second partial part*/

 update of the first partial part */

 the first partial part */

		/* one PTE update may be issued in multiple writes and the

		 * first write may not construct a valid gfn

			/* guest driver may read/write the entry when partial

			 * update the entry in this situation p2m will fail

			 * settting the shadow entry to point to a scratch page

/*

 * intel_vgpu_emulate_ggtt_mmio_write - emulate GTT MMIO register write

 * @vgpu: a vGPU

 * @off: register offset

 * @p_data: data from guest write

 * @bytes: data length

 *

 * This function is used to emulate the GTT MMIO register write

 *

 * Returns:

 * Zero on success, error code if failed.

	/* if ggtt of last submitted context is written,

	 * that context is probably got unpinned.

	 * Set last shadowed ctx to invalid.

	/* Build the tree by full filled the scratch pt with the entries which

	 * point to the next level scratch pt or scratch page. The

	 * scratch_pt[type] indicate the scratch pt/scratch page used by the

	 * 'type' pt.

	 * e.g. scratch_pt[GTT_TYPE_PPGTT_PDE_PT] is used by

	 * GTT_TYPE_PPGTT_PDE_PT level pt, that means this scratch_pt it self

	 * is GTT_TYPE_PPGTT_PTE_PT, and full filled by scratch page mfn.

		/* The entry parameters like present/writeable/cache type

		 * set to the same as i915's scratch page tree.

/**

 * intel_vgpu_init_gtt - initialize per-vGPU graphics memory virulization

 * @vgpu: a vGPU

 *

 * This function is used to initialize per-vGPU graphics memory virtualization

 * components.

 *

 * Returns:

 * Zero on success, error code if failed.

/**

 * intel_vgpu_clean_gtt - clean up per-vGPU graphics memory virulization

 * @vgpu: a vGPU

 *

 * This function is used to clean up per-vGPU graphics memory virtualization

 * components.

 *

 * Returns:

 * Zero on success, error code if failed.

/**

 * intel_vgpu_find_ppgtt_mm - find a PPGTT mm object

 * @vgpu: a vGPU

 * @pdps: pdp root array

 *

 * This function is used to find a PPGTT mm object from mm object pool

 *

 * Returns:

 * pointer to mm object on success, NULL if failed.

/**

 * intel_vgpu_get_ppgtt_mm - get or create a PPGTT mm object.

 * @vgpu: a vGPU

 * @root_entry_type: ppgtt root entry type

 * @pdps: guest pdps

 *

 * This function is used to find or create a PPGTT mm object from a guest.

 *

 * Returns:

 * Zero on success, negative error code if failed.

/**

 * intel_vgpu_put_ppgtt_mm - find and put a PPGTT mm object.

 * @vgpu: a vGPU

 * @pdps: guest pdps

 *

 * This function is used to find a PPGTT mm object from a guest and destroy it.

 *

 * Returns:

 * Zero on success, negative error code if failed.

/**

 * intel_gvt_init_gtt - initialize mm components of a GVT device

 * @gvt: GVT device

 *

 * This function is called at the initialization stage, to initialize

 * the mm components of a GVT device.

 *

 * Returns:

 * zero on success, negative error code if failed.

/**

 * intel_gvt_clean_gtt - clean up mm components of a GVT device

 * @gvt: GVT device

 *

 * This function is called at the driver unloading stage, to clean up the

 * the mm components of a GVT device.

 *

/**

 * intel_vgpu_invalidate_ppgtt - invalidate PPGTT instances

 * @vgpu: a vGPU

 *

 * This function is called when invalidate all PPGTT instances of a vGPU.

 *

/**

 * intel_vgpu_reset_ggtt - reset the GGTT entry

 * @vgpu: a vGPU

 * @invalidate_old: invalidate old entries

 *

 * This function is called at the vGPU create stage

 * to reset all the GGTT entries.

 *

/**

 * intel_vgpu_reset_gtt - reset the all GTT related status

 * @vgpu: a vGPU

 *

 * This function is called from vfio core to reset reset all

 * GTT related status, including GGTT, PPGTT, scratch page.

 *

	/* Shadow pages are only created when there is no page

	 * table tracking data, so remove page tracking data after

	 * removing the shadow pages.

/**

 * intel_gvt_restore_ggtt - restore all vGPU's ggtt entries

 * @gvt: intel gvt device

 *

 * This function is called at driver resume stage to restore

 * GGTT entries of every vGPU.

 *

 Restore dirty host ggtt for all vGPUs */

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Eddie Dong <eddie.dong@intel.com>

 *    Jike Song <jike.song@intel.com>

 *

 * Contributors:

 *    Zhi Wang <zhi.a.wang@intel.com>

 *    Min He <min.he@intel.com>

 *    Bing Niu <bing.niu@intel.com>

 *

/* bitmap for writable bits (RW or RW1C bits, but cannot co-exist in one

 * byte) byte by byte in standard pci configuration space. (not the full

 * 256 bytes.)

 the only one RW1C byte */

/**

 * vgpu_pci_cfg_mem_write - write virtual cfg space memory

 * @vgpu: target vgpu

 * @off: offset

 * @src: src ptr to write

 * @bytes: number of bytes

 *

 * Use this function to write virtual cfg space memory.

 * For standard cfg space, only RW bits can be changed,

 * and we emulates the RW1C behavior of PCI_STATUS register.

		/**

		 * The PCI_STATUS high byte has RW1C bits, here

		 * emulates clear by writing 1 for these bits.

		 * Writing a 0b to RW1C bits has no effect.

 For other configuration space directly copy as it is. */

/**

 * intel_vgpu_emulate_cfg_read - emulate vGPU configuration space read

 * @vgpu: target vgpu

 * @offset: offset

 * @p_data: return data ptr

 * @bytes: number of bytes to read

 *

 * Returns:

 * Zero on success, negative error code if failed.

 We don't have rom, return size of 0. */

	/*

	 * Power-up software can determine how much address

	 * space the device requires by writing a value of

	 * all 1's to the register and then reading the value

	 * back. The device will return 0's in all don't-care

	 * address bits.

			/*

			 * Untrap the BAR, since guest hasn't configured a

			 * valid GPA

 Unimplemented BARs */

			/*

			 * Untrap the old BAR first, since guest has

			 * re-configured the BAR

/**

 * intel_vgpu_emulate_cfg_read - emulate vGPU configuration space write

 * @vgpu: target vgpu

 * @offset: offset

 * @p_data: write data ptr

 * @bytes: number of bytes to write

 *

 * Returns:

 * Zero on success, negative error code if failed.

 First check if it's PCI_COMMAND */

/**

 * intel_vgpu_init_cfg_space - init vGPU configuration space when create vGPU

 *

 * @vgpu: a vGPU

 * @primary: is the vGPU presented as primary

 *

 Show guest that there isn't any stolen memory.*/

	/*

	 * Clear the bar upper 32bit and let guest to assign the new value

 PM Support */

/**

 * intel_vgpu_reset_cfg_space - reset vGPU configuration space

 *

 * @vgpu: a vGPU

 *

	/**

	 * Currently we only do such reset when vGPU is not

	 * owned by any VM, so we simply restore entire cfg

	 * space to default value.

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Eddie Dong <eddie.dong@intel.com>

 *    Kevin Tian <kevin.tian@intel.com>

 *

 * Contributors:

 *    Zhi Wang <zhi.a.wang@intel.com>

 *    Changbin Du <changbin.du@intel.com>

 *    Zhenyu Wang <zhenyuw@linux.intel.com>

 *    Tina Zhang <tina.zhang@intel.com>

 *    Bing Niu <bing.niu@intel.com>

 *

 Raw offset is appened to each line for convenience. */

 0x229c */

 0x2248 */

 0x2098 */

 0x20c0 */

 0x24d0 */

 0x24d4 */

 0x24d8 */

 0x24dc */

 0x24e0 */

 0x24e4 */

 0x24e8 */

 0x24ec */

 0x24f0 */

 0x24f4 */

 0x24f8 */

 0x24fc */

 0x7004 */

 0x7008 */

 0x7000 */

 0x7010 */

 0x7300 */

 0x83a4 */

 0x2229c */

 0x2209c */

 0x220c0 */

 0x22098 */

 0x22028 */

 Terminated */

 0x229c */

 0x2248 */

 0x2098 */

 0x20c0 */

 0x24d0 */

 0x24d4 */

 0x24d8 */

 0x24dc */

 0x24e0 */

 0x24e4 */

 0x24e8 */

 0x24ec */

 0x24f0 */

 0x24f4 */

 0x24f8 */

 0x24fc */

 0x7004 */

 0x7008 */

 0x7000 */

 0x7010 */

 0x7300 */

 0x83a4 */

 0x40e0 */

 0x40e4 */

 0x2580 */

 0x7014 */

 0x20ec */

 0xb118 */

 0xb11c */

 0xb008 */

 0xe100 */

 0xe180 */

 0xe184 */

 0xe188 */

 0xe194 */

 0xe4f0 */

 0x4de0 */

 0x4de4 */

 0x4de8 */

 0x4dec */

 0x4df0 */

 0x4df4 */

 0x2229c */

 0x2209c */

 0x220c0 */

 0x22098 */

 0x22028 */

 0x1c028 */

 0x1a028 */

 0x7304 */

 0x2248 */

 0x940c */

 0x4ab8 */

 0x4ab0 */

 0x20d4 */

 0x20d8 */

 0xb004 */

 0x20a0 */

 0x20e4 */

 Terminated */

 Platform doesn't have mocs mmios. */

/*

 * Use lri command to initialize the mmio which is in context state image for

 * inhibit context, it contains tracked engine mmio, render_mocs and

 * render_mocs_l3cc.

 no MOCS register in context except render engine */

	/* WaForceWakeRenderDuringMmioTLBInvalidate:skl

	 * we need to put a forcewake when invalidating RCS TLB caches,

	 * otherwise device can go to RC6 state and interrupt invalidation

	 * process

 Switch ring mmio values (context). */

		/*

		 * No need to do save or restore of the mmio which is in context

		 * state image on gen9, it's initialized by lri command and

		 * save or restore with context together.

 save

 restore

			/*

			 * No need to restore the mmio which is in context state

			 * image if it's not inhibit context, it will restore

			 * itself.

/**

 * intel_gvt_switch_render_mmio - switch mmio context of specific engine

 * @pre: the last vGPU that own the engine

 * @next: the vGPU to switch to

 * @engine: the engine

 *

 * If pre is null indicates that host own the engine. If next is null

 * indicates that we are switching to host workload.

	/**

	 * We are using raw mmio access wrapper to improve the

	 * performace for batch mmio read/write, so we need

	 * handle forcewake mannually.

/**

 * intel_gvt_init_engine_mmio_context - Initiate the engine mmio list

 * @gvt: GVT device

 *

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Jike Song <jike.song@intel.com>

 *

 * Contributors:

 *    Zhi Wang <zhi.a.wang@intel.com>

 *

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Kevin Tian <kevin.tian@intel.com>

 *    Eddie Dong <eddie.dong@intel.com>

 *    Zhiyuan Lv <zhiyuan.lv@intel.com>

 *

 * Contributors:

 *    Min He <min.he@intel.com>

 *    Tina Zhang <tina.zhang@intel.com>

 *    Pei Zhang <pei.zhang@intel.com>

 *    Niu Bing <bing.niu@intel.com>

 *    Ping Gao <ping.a.gao@intel.com>

 *    Zhi Wang <zhi.a.wang@intel.com>

 *



 XXX FIXME i915 has changed PP_XXX definition */

			/* We return -EEXIST here to make GVT-g load fail.

			 * So duplicated MMIO can be found as soon as

			 * possible.

/**

 * intel_gvt_render_mmio_to_engine - convert a mmio offset into the engine

 * @gvt: a GVT device

 * @offset: register offset

 *

 * Returns:

 * The engine containing the offset within its mmio page.

		/* When guest access oob fence regs without access

		 * pv_info first, we treat guest not supporting GVT,

		 * and we will let vgpu enter failsafe mode.

			/* All engines must be enabled together for vGPU,

			 * since we don't know which engine the ppgtt will

			 * bind to when shadowing.

should not hit here*/

 vgpu_lock already hold by emulate mmio r/w */

 sw will wait for the device to ack the reset request */

/*

 * Only PIPE_A is enabled in current vGPU display and PIPE_A is tied to

 *   TRANSCODER_A in HW. DDI/PORT could be PORT_x depends on

 *   setup_virtual_dp_monitor().

 * emulate_monitor_status_change() set up PLL for PORT_x as the initial enabled

 *   DPLL. Later guest driver may setup a different DPLLx when setting mode.

 * So the correct sequence to find DP stream clock is:

 *   Check TRANS_DDI_FUNC_CTL on TRANSCODER_A to get PORT_x.

 *   Check correct PLLx for PORT_x to get PLL frequency and DP bitrate.

 * Then Refresh rate then can be calculated based on follow equations:

 *   Pixel clock = h_total * v_total * refresh_rate

 *   stream clock = Pixel clock

 *   ls_clk = DP bitrate

 *   Link M/N = strm_clk / ls_clk

 Port to PHY mapping is fixed, see bxt_ddi_phy_info{} */

 Find the enabled DPLL for the DDI/PORT */

 Find PLL output frequency from correct DPLL, and get bir rate */

 Find DDI/PORT assigned to TRANSCODER_A, expect B or D */

 Calculate DP bitrate from PLL */

 Get DP link symbol clock M/N */

 Get H/V total from transcoder timing */

 Calcuate pixel clock by (ls_clk * M / N) */

 Calcuate refresh rate by (pixel_clk / (h_total * v_total)) */

 sorted in ascending order */

_MMIO(0x20ec)

_MMIO(0x2248)

_MMIO(0x2340)

_MMIO(0x2348)

_MMIO(0x2350)

_MMIO(0x2580)

_MMIO(0x7010)

_MMIO(0x7300)

_MMIO(0x7304)

_MMIO(0xb118)

 a simple bsearch */

 If imr bit has been masked */

 mark transaction done */

 message size */

 training pattern 1 for CR */

 set LANE0_CR_DONE, LANE1_CR_DONE */

 set LANE2_CR_DONE, LANE3_CR_DONE */

 training pattern 2 for EQ */

 Set CHANNEL_EQ_DONE and  SYMBOL_LOCKED for Lane0_1 */

 Set CHANNEL_EQ_DONE and  SYMBOL_LOCKED for Lane2_3 */

 set INTERLANE_ALIGN_DONE */

 finish link training */

 set sink status as synchronized */

 SKL DPB/C/D aux ctl register changed */

 write to the data registers */

 just want to clear the sticky bits */

 read out message from DATA1 register */

			/*

			 * Write request exceeds what we supported,

			 * DCPD spec: When a Source Device is writing a DPCD

			 * address not supported by the Sink Device, the Sink

			 * Device shall reply with AUX NACK and âMâ equal to

			 * zero.

 NAK the write */

		/*

		 * Write request format: Headr (command + address + size) occupies

		 * 4 bytes, followed by (len + 1) bytes of data. See details at

		 * intel_dp_aux_transfer().

 unpack data from vreg to buf */

 write to virtual DPCD */

 check for link training */

 ACK the write */

			/*

			 * read request exceeds what we supported

			 * DPCD spec: A Sink Device receiving a Native AUX CH

			 * read request for an unsupported DPCD address must

			 * reply with an AUX ACK and read data set equal to

			 * zero instead of replying with AUX NACK.

 ACK the READ*/

 clear the data registers */

		/*

		 * Read reply format: ACK (1 byte) plus (len + 1) bytes of data.

 read from virtual DPCD to vreg */

 first 4 bytes: [ACK][addr][addr+1][addr+2] */

 i2c transaction starts */

 vgt_caps */

 Remove this in guest driver. */

 add xhot and yhot to handled list to avoid error log */

			/**

			 * "Read memory latency" command on gen9.

			 * Below memory latency values are read

			 * from skylake platform.

			/**

			 * "Read memory latency" command on gen9.

			 * Below memory latency values are read

			 * from Broxton MRB.

	/**

	 * PCODE_READY clear means ready for pcode read/write,

	 * PCODE_ERROR_MASK clear means no error happened. In GVT-g we

	 * always emulate as pcode read/write success and ready for access

	 * anytime, since we don't touch real physical registers here.

	/*

	 * Need to emulate all the HWSP register write to ensure host can

	 * update the VM CSB status correctly. Here listed registers can

	 * support BDW, SKL or other platforms with same HWSP registers.

 other bits are MBZ. */

/*

 * FixMe:

 * If guest fills non-priv batch buffer on ApolloLake/Broxton as Mesa i965 did:

 * 717e7539124d (i965: Use a WC map and memcpy for the batch instead of pwrite.)

 * Due to the missing flush of bb filled by VM vCPU, host GPU hangs on executing

 * these MI_BATCH_BUFFER.

 * Temporarily workaround this by setting SNOOP bit for PAT3 used by PPGTT

 * PML4 PTE: PAT(0) PCD(1) PWT(1).

 * The performance is still expected to be low, will need further improvement.

 keep MIA_IN_RESET before clearing */

	/**

	 * Read HW reg in following case

	 * a. the offset isn't a ring mmio

	 * b. the offset's ring is running on hw.

	 * c. the offset is ring time stamp mmio

	/*

	 * Due to d3_entered is used to indicate skipping PPGTT invalidation on

	 * vGPU reset, it's set on D0->D3 on PCI config write, and cleared after

	 * vGPU reset if in resuming.

	 * In S0ix exit, the device power state also transite from D3 to D0 as

	 * S3 resume, but no vGPU reset (triggered by QEMU devic model). After

	 * S0ix exit, all engines continue to work. However the d3_entered

	 * remains set which will break next vGPU reset logic (miss the expected

	 * PPGTT invalidation).

	 * Engines can only work in D0. Thus the 1st elsp write gives GVT a

	 * chance to clear d3_entered.

	/* when PPGTT mode enabled, we will check if guest has called

	 * pvinfo, if not, we will treat this guest as non-gvtg-aware

	 * guest, and stop emulating its cfg space, mmio, gtt, etc.

 RING MODE */

 display */

 TRTT */

/**

 * intel_gvt_clean_mmio_info - clean up MMIO information table for GVT device

 * @gvt: GVT device

 *

 * This function is called at the driver unloading stage, to clean up the MMIO

 * information table of GVT device

 *

/* Special MMIO blocks. registers in MMIO block ranges should not be command

 * accessible (should have no F_CMD_ACCESS flag).

 * otherwise, need to update cmd_reg_handler in cmd_parser.c

/**

 * intel_gvt_setup_mmio_info - setup MMIO information table for GVT device

 * @gvt: GVT device

 *

 * This function is called at the initialization stage, to setup the MMIO

 * information table for GVT device

 *

 * Returns:

 * zero on success, negative if failed.

/**

 * intel_gvt_for_each_tracked_mmio - iterate each tracked mmio

 * @gvt: a GVT device

 * @handler: the handler

 * @data: private data given to handler

 *

 * Returns:

 * Zero on success, negative error code if failed.

 pvinfo data doesn't come from hw mmio */

/**

 * intel_vgpu_default_mmio_read - default MMIO read handler

 * @vgpu: a vGPU

 * @offset: access offset

 * @p_data: data return buffer

 * @bytes: access data length

 *

 * Returns:

 * Zero on success, negative error code if failed.

/**

 * intel_t_default_mmio_write - default MMIO write handler

 * @vgpu: a vGPU

 * @offset: access offset

 * @p_data: write data buffer

 * @bytes: access data length

 *

 * Returns:

 * Zero on success, negative error code if failed.

/**

 * intel_vgpu_mask_mmio_write - write mask register

 * @vgpu: a vGPU

 * @offset: access offset

 * @p_data: write data buffer

 * @bytes: access data length

 *

 * Returns:

 * Zero on success, negative error code if failed.

/**

 * intel_gvt_in_force_nonpriv_whitelist - if a mmio is in whitelist to be

 * force-nopriv register

 *

 * @gvt: a GVT device

 * @offset: register offset

 *

 * Returns:

 * True if the register is in force-nonpriv whitelist;

 * False if outside;

/**

 * intel_vgpu_mmio_reg_rw - emulate tracked mmio registers

 * @vgpu: a vGPU

 * @offset: register offset

 * @pdata: data buffer

 * @bytes: data length

 * @is_read: read or write

 *

 * Returns:

 * Zero on success, negative error code if failed.

	/*

	 * Handle special MMIO blocks.

	/*

	 * Normal tracked MMIOs.

 keep the RO bits in the virtual register */

 higher 16bits of mode ctl regs are mask bits for change */

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Ke Yu

 *    Kevin Tian <kevin.tian@intel.com>

 *    Zhiyuan Lv <zhiyuan.lv@intel.com>

 *

 * Contributors:

 *    Min He <min.he@intel.com>

 *    Ping Gao <ping.a.gao@intel.com>

 *    Tina Zhang <tina.zhang@intel.com>

 *    Yulei Zhang <yulei.zhang@intel.com>

 *    Zhi Wang <zhi.a.wang@intel.com>

 *

 Render Command Map */

 MI_* command Opcode (28:23) */

 HSW+ */

 HSW+ */

 HSW+ */

 IVB+ */

 IVB+ */

 IVB+ */

 HSW+ */

 HSW+ */

 BDW+ */

 BDW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 BDW+ */

 BDW+ */

 Bit definition for dword 0 */

 2D command: Opcode (28:22) */

 3D/Media Command: Pipeline Type(28:27) Opcode(26:24) Sub Opcode(23:16) */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 HSW+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 HSW+ */

 BDW+ */

 BDW+ */

 BDW+ */

 BDW+ */

 BDW+ */

 BDW+ */

 BDW+ */

 BDW+ */

 BDW+ */

 BDW+ */

 SKL+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 HSW+ */

 HSW+ */

 HSW+ */

 VCCP Command Parser */

/*

 * Below MFX and VBE cmd definition is from vaapi intel driver project (BSD License)

 * git://anongit.freedesktop.org/vaapi/intel-driver

 * src/i965_defines.h

 *

 ALL */

 ALL */

 ALL */

 ALL */

 ALL */

 ALL */

 ALL */

 IVB+ */

 IVB+ */

 IVB+ */

 IVB+ */

 ALL */

 IVB+ */

 ALL */

 ALL */

 ALL */

 ALL */

 ALL */

 ALL */

 HSW+ */

 IVB+ */

 IVB+ */

 ALL */

 ALL */

 ALL */

 ALL */

 IVB+ */

 IVB+ */

 ALL */

 ALL */

 ALL */

 ALL */

 ALL */

 ALL */

 IVB+ */

 IVB+ */

 IVB+ */

 which DWords need address fix */

 value is const although LEN maybe variable */

/*

 * command has its own ip advance logic

 * e.g. MI_BATCH_START, MI_BATCH_END

 rings that support this cmd: BLT/RCS/VCS/VECS */

 devices that support this cmd: SNB/IVB/HSW/... */

	/* which DWords are address that need fix up.

	 * bit 0 means a 32-bit non address operand in command

	 * bit 1 means address operand, which could be 32-bit

	 * or 64-bit depending on different architectures.(

	 * defined by "gmadr_bytes_in_cmd" in intel_gvt.

	 * No matter the address length, each address only takes

	 * one bit in the bitmap.

	/* flag == F_LEN_CONST : command length

	 * flag == F_LEN_VAR : length bias bits

	 * Note: length is in DWord

 valid length in DWord */

 batch buffer address type */

 graphics memory address of ring buffer start */

 instruction graphics memory address */

 mapped va of the instr_gma */

 next instruction when return from  batch buffer to ring buffer */

 next instruction when return from 2nd batch buffer to batch buffer */

	/* batch buffer address type (GTT or PPGTT)

	 * used when ret from 2nd level batch buffer

 ring ALL, type = 0 */

 ring RCS, command type 2 */

 ring RCS, command type 3 */

 ring VCS, command type 3 */

 ring VECS, command type 3 */

 shadow batch buffer */

 do not remove this, some platform may need clflush here */

		/* Currently all guests use PML4 table and now can't

		 * have a guest with 3-level table but uses LRI for

 TODO: add LRI POST logic here */

 below are all lri handlers */

 Writing to HW VGT_PVINFO_PAGE offset will be discarded */

 only patch cmd. restore vreg value if changed in mmio write handler*/

	/* TODO

	 * In order to let workload with inhibit context to generate

	 * correct image data into memory, vregs values will be loaded to

	 * hw via LRIs in the workload with inhibit context. But as

	 * indirect context is loaded prior to LRIs in workload, we don't

	 * want reg values specified in indirect context overwritten by

	 * LRIs in workloads. So, when scanning an indirect context, we

	 * update reg values in it into vregs, so LRIs in workload with

	 * inhibit context will restore with correct values

 check inhibit context */

 LRI post sync */

 post sync */

 check ggtt*/

 Store Data Index */

 Flip Type == Stereo 3D Flip */

 check ppggt */

 check if QWORD */

 check inline data */

 Check again for Qword */

 Check post-sync and ppgtt bit */

 Store Data Index */

 Check notify bit */

/*

 * Check whether a batch buffer needs to be scanned. Currently

 * the only criteria is based on privilege.

 Decide privilege based on address space */

 get the start gm address of the batch buffer */

 chained batch buffer */

 get the start gm address of the batch buffer */

	/* the start_offset stores the batch buffer's start gma's

	 * offset relative to page boundary. so for non-privileged batch

	 * buffer, the shadowed gem object holds exactly the same page

	 * layout as original gem object. This is for the convience of

	 * replacing the whole non-privilged batch buffer page to this

	 * shadowed one in PPGTT at the same gma address. (this replacing

	 * action is not implemented yet now, but may be necessary in

	 * future).

	 * for prileged batch buffer, we just change start gma address to

	 * that of shadowed page.

	/*

	 * ip_va saves the virtual address of the shadow batch buffer, while

	 * ip_gma saves the graphics address of the original batch buffer.

	 * As the shadow batch buffer is just a copy from the originial one,

	 * it should be right to use shadow batch buffer'va and original batch

	 * buffer's gma in pair. After all, we don't want to pin the shadow

	 * buffer here (too early).

 emulate a batch buffer end to do return right */

 call the cmd handler, and advance ip */

 fastpath for MI_NOOP */

/* Keep the consistent return type, e.g EBADRQC for unknown

 * cmd, EFAULT for invalid address, EPERM for nonpriv. later

 * works as the input of VM healthy status.

 ring base is page aligned */

 ring base is page aligned */

 calculate workload ring buffer size */

 realloc the new ring buffer if needed */

 get shadow ring buffer va */

 head > tail --> copy head <-> top */

 copy head or start <-> tail */

 get the va of the shadow batch buffer */

/* generate dummy contexts by sending empty requests to HW, and let

 * the HW to fill Engine Contexts. This dummy contexts are used for

 * initialization purpose (update reg whitelist), so referred to as

 * init context here

 scan init ctx to update cmd accessible list */

 skipping the first RING_CTX_SIZE(0x50) dwords */

	/* Only ring contxt is loaded to HW for inhibit context, no need to

	 * scan engine context

	/* don't scan the first RING_CTX_SIZE(0x50) dwords, as it's ring

	 * context

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Kevin Tian <kevin.tian@intel.com>

 *    Eddie Dong <eddie.dong@intel.com>

 *

 * Contributors:

 *    Niu Bing <bing.niu@intel.com>

 *    Zhi Wang <zhi.a.wang@intel.com>

 *

/**

 * intel_gvt_clean_device - clean a GVT device

 * @i915: i915 private

 *

 * This function is called at the driver unloading stage, to free the

 * resources owned by a GVT device.

 *

/**

 * intel_gvt_init_device - initialize a GVT device

 * @i915: drm i915 private data

 *

 * This function is called at the initialization stage, to initialize

 * necessary GVT components.

 *

 * Returns:

 * Zero on success, negative error code if failed.

 *

 Get a reference for device model module */

/*

 * Copyright 2017 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *    Zhiyuan Lv <zhiyuan.lv@intel.com>

 *

 * Contributors:

 *    Xiaoguang Chen

 *    Tina Zhang <tina.zhang@intel.com>

 Free the orphan dmabuf_objs here */

 vgpu is NULL, as it has been removed already */

 If exists, pick up the exposed dmabuf_obj */

		/* This buffer may be released between query_plane ioctl and

		 * get_dmabuf ioctl. Add the refcount to make sure it won't

		 * be released between the two ioctls.

 Need to allocate a new one*/

 ENODEV means plane isn't ready, which might be a normal case. */

 To associate an exposed dmabuf with the dmabuf_obj */

 dmabuf_obj might be freed in dmabuf_obj_put */

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Zhi Wang <zhi.a.wang@intel.com>

 *

 * Contributors:

 *    Ping Gao <ping.a.gao@intel.com>

 *    Tina Zhang <tina.zhang@intel.com>

 *    Chanbin Du <changbin.du@intel.com>

 *    Min He <min.he@intel.com>

 *    Bing Niu <bing.niu@intel.com>

 *    Zhenyu Wang <zhenyuw@linux.intel.com>

 *

/*

 * when populating shadow ctx from guest, we should not overrride oa related

 * registers, so that they will not be overlapped by guest oa configs. Thus

 * made it possible to capture oa data from host for both host and guests.

 first gpa of consecutive GPAs */

 size of consecutive GPAs */

	/* don't copy Ring Context (the first 0x50 dwords),

	 * only copy the Engine Context part from guest

	/* only need to ensure this context is not pinned/unpinned during the

	 * period from last submission to this this submission.

	 * Upon reaching this function, the currently submitted context is not

	 * supposed to get unpinned. If a misbehaving guest driver ever does

	 * this, it would corrupt itself.

	/* find consecutive GPAs from gma until the first inconsecutive GPA.

	 * read from the continuous GPAs into dst virtual address

 Switch ring from vGPU to host. */

 Switch ring from host to vGPU or vGPU to vGPU. */

	/*

	 * Update bits 0-11 of the context descriptor which includes flags

	 * like GEN8_CTX_* cached in desc_template

	/*

	 * To track whether a request has started on HW, we can emit a

	 * breadcrumb at the beginning of the request and check its

	 * timeline's HWSP to see if the breadcrumb has advanced past the

	 * start of this request. Actually, the request must have the

	 * init_breadcrumb if its timeline set has_init_bread_crumb, or the

	 * scheduler might get a wrong state of it during reset. Since the

	 * requests from gvt always set the has_init_breadcrumb flag, here

	 * need to do the emit_init_breadcrumb for all the requests.

 allocate shadow ring buffer */

 get shadow ring buffer va */

 This is not a good idea */

			/* skip now as current i915 ppgtt alloc won't allocate

			   top level pdp for non 4-level table, won't impact

/**

 * intel_gvt_scan_and_shadow_workload - audit the workload by scanning and

 * shadow it as well, include ringbuffer,wa_ctx and ctx.

 * @workload: an abstract entity for each execlist submission.

 *

 * This function is called before the workload submitting to i915, to make

 * sure the content of the workload is valid.

		/* For privilge batch buffer and not wa_ctx, the bb_start_cmd_va

		 * is only updated into ring_scan_buffer, not real ring address

		 * allocated in later copy_workload_to_ring_buffer. pls be noted

		 * shadow_ring_buffer_va is now pointed to real ring buffer va

		 * in copy_workload_to_ring_buffer.

		/*

		 * For non-priv bb, scan&shadow is only for

		 * debugging purpose, so the content of shadow bb

		 * is the same as original bb. Therefore,

		 * here, rather than switch to shadow bb's gma

		 * address, we directly use original batch buffer's

		 * gma address, and send original bb to hardware

		 * directly

 relocate shadow batch buffer */

 No one is going to touch shadow bb from now on. */

	/* FIXME: we are not tracking our pinned VMA leaving it

	 * up to the core to fix up the stray pin_count upon

	 * free.

		/* We might still need to add request with

		 * clean ctx to retire it properly..

	/*

	 * no current vgpu / will be scheduled out / no workload

	 * bail out

	/*

	 * still have current workload, maybe the workload disptacher

	 * fail to submit it for some reason, resubmit it.

	/*

	 * pick a workload as current workload

	 * once current workload is set, schedule policy routines

	 * will wait the current workload is finished when trying to

	 * schedule out a vgpu.

 see comment in LRI handler in cmd_parser.c */

 first gpa of consecutive GPAs */

 size of consecutive GPAs*/

	/* find consecutive GPAs from gma until the first inconsecutive GPA.

	 * write to the consecutive GPAs from src virtual address

 free the unsubmited workloads in the queues. */

	/* For the workload w/ request, needs to wait for the context

	 * switch to make sure request is completed.

	 * For the workload w/o request, directly complete the workload.

		/* If this request caused GPU hang, req->fence.error will

		 * be set to -EIO. Use -EIO to set workload status so

		 * that when this request caused GPU hang, didn't trigger

		 * context switch interrupt to guest.

		/* if workload->status is not successful means HW GPU

		 * has occurred GPU hang or something wrong with i915/GVT,

		 * and GVT won't inject context switch interrupt to guest.

		 * So this error is a vGPU hang actually to the guest.

		 * According to this we should emunlate a vGPU hang. If

		 * there are pending workloads which are already submitted

		 * from guest, we should clean them up like HW GPU does.

		 *

		 * if it is in middle of engine resetting, the pending

		 * workloads won't be submitted to HW GPU and will be

		 * cleaned up during the resetting process later, so doing

		 * the workload clean up here doesn't have any impact.

		/*

		 * Update the vReg of the vGPU which submitted this

		 * workload. The vGPU may use these registers for checking

		 * the context state. The value comes from GPU commands

		 * in this workload.

/**

 * intel_vgpu_clean_submission - free submission-related resource for vGPU

 * @vgpu: a vGPU

 *

 * This function is called when a vGPU is being destroyed.

 *

/**

 * intel_vgpu_reset_submission - reset submission-related resource for vGPU

 * @vgpu: a vGPU

 * @engine_mask: engines expected to be reset

 *

 * This function is called when a vGPU is being destroyed.

 *

/**

 * intel_vgpu_setup_submission - setup submission-related resource for vGPU

 * @vgpu: a vGPU

 *

 * This function is called when a vGPU is being created.

 *

 * Returns:

 * Zero on success, negative error code if failed.

 *

 Max ring buffer size */

/**

 * intel_vgpu_select_submission_ops - select virtual submission interface

 * @vgpu: a vGPU

 * @engine_mask: either ALL_ENGINES or target engine mask

 * @interface: expected vGPU virtual submission interface

 *

 * This function is called when guest configures submission interface.

 *

 * Returns:

 * Zero on success, negative error code if failed.

 *

/**

 * intel_vgpu_destroy_workload - destroy a vGPU workload

 * @workload: workload to destroy

 *

 * This function is called when destroy a vGPU workload.

 *

 legacy 32-bit */

 legacy 64-bit */

/**

 * intel_vgpu_create_workload - create a vGPU workload

 * @vgpu: a vGPU

 * @engine: the engine

 * @desc: a guest context descriptor

 *

 * This function is called when creating a vGPU workload.

 *

 * Returns:

 * struct intel_vgpu_workload * on success, negative error code in

 * pointer if failed.

 *

			/*

			 * cannot use guest context head pointer here,

			 * as it might not be updated at this time

 record some ring buffer register values for scan and shadow */

	/* Only scan and shadow the first workload in the queue

	 * as there is only one pre-allocated buf-obj for shadow.

/**

 * intel_vgpu_queue_workload - Qeue a vGPU workload

 * @workload: the workload to queue in

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Kevin Tian <kevin.tian@intel.com>

 *    Dexuan Cui

 *

 * Contributors:

 *    Pei Zhang <pei.zhang@intel.com>

 *    Min He <min.he@intel.com>

 *    Niu Bing <bing.niu@intel.com>

 *    Yulei Zhang <yulei.zhang@intel.com>

 *    Zhenyu Wang <zhenyuw@linux.intel.com>

 *    Zhi Wang <zhi.a.wang@intel.com>

 *

/**

 * intel_vgpu_write_fence - write fence registers owned by a vGPU

 * @vgpu: vGPU instance

 * @fence: vGPU fence register number

 * @value: Fence register value to be written

 *

 * This function is used to write fence registers owned by a vGPU. The vGPU

 * fence register number will be translated into HW fence register number.

 *

 Request fences from host */

 Return fences to host, if fail */

/**

 * inte_gvt_free_vgpu_resource - free HW resource owned by a vGPU

 * @vgpu: a vGPU

 *

 * This function is used to free the HW resource owned by a vGPU.

 *

/**

 * intel_vgpu_reset_resource - reset resource state owned by a vGPU

 * @vgpu: a vGPU

 *

 * This function is used to reset resource state owned by a vGPU.

 *

/**

 * intel_alloc_vgpu_resource - allocate HW resource for a vGPU

 * @vgpu: vGPU

 * @param: vGPU creation params

 *

 * This function is used to allocate HW resource for a vGPU. User specifies

 * the resource configuration through the creation params.

 *

 * Returns:

 * zero on success, negative error code if failed.

 *

/*

 * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Ke Yu

 *    Zhiyuan Lv <zhiyuan.lv@intel.com>

 *

 * Contributors:

 *    Terrence Xu <terrence.xu@intel.com>

 *    Changbin Du <changbin.du@intel.com>

 *    Bing Niu <bing.niu@intel.com>

 *    Zhi Wang <zhi.a.wang@intel.com>

 *

 GMBUS0 bits definitions */

 GMBUS0 */

		/*

		 * TODO: "This bit is cleared to zero when an event

		 * causes the HW_RDY bit transition to occur "

		/*

		 * per bspec setting this bit can cause:

		 * 1) INT status bit cleared

		 * 2) HW_RDY bit asserted

		/* For virtualization, we suppose that HW is always ready,

		 * so GMBUS_SW_RDY should always be cleared

 vgpu gmbus only support EDID */

			/* From spec:

			 * This can only cause a STOP to be generated

			 * if a GMBUS cycle is generated, the GMBUS is

			 * currently in a data/wait/idle phase, or it is in a

			 * WAIT phase

				/* After the 'stop' cycle, hw state would become

				 * 'stop phase' and then 'idle phase' after a

				 * few milliseconds. In emulation, we just set

				 * it as 'idle phase' ('stop phase' is not

				 * visible in gmbus interface)

			/* From hw spec the GMBUS phase

			 * transition like this:

			 * START (-->INDEX) -->DATA

		/*

		 * From hw spec the WAIT state will be

		 * cleared:

		 * (1) in a new GMBUS cycle

		 * (2) by generating a stop

 Data can only be recevied if previous settings correct */

		/*

		 * Read GMBUS3 during send operation,

		 * return the latest written value

 All other bits are read-only */

/**

 * intel_gvt_i2c_handle_gmbus_read - emulate gmbus register mmio read

 * @vgpu: a vGPU

 * @offset: reg offset

 * @p_data: data return buffer

 * @bytes: access data length

 *

 * This function is used to emulate gmbus register mmio read

 *

 * Returns:

 * Zero on success, negative error code if failed.

 *

/**

 * intel_gvt_i2c_handle_gmbus_write - emulate gmbus register mmio write

 * @vgpu: a vGPU

 * @offset: reg offset

 * @p_data: data return buffer

 * @bytes: access data length

 *

 * This function is used to emulate gmbus register mmio write

 *

 * Returns:

 * Zero on success, negative error code if failed.

 *

/**

 * intel_gvt_i2c_handle_aux_ch_write - emulate AUX channel register write

 * @vgpu: a vGPU

 * @port_idx: port index

 * @offset: reg offset

 * @p_data: write ptr

 *

 * This function is used to emulate AUX channel register write

 *

 check the msg in DATA register.

 The ctl write to clear some states */

 Always set the wanted value for vms. */

 stop */

 start or restart */

 reset the address */

		/* TODO

		 * We only support EDID reading from I2C_over_AUX. And

		 * we do not expect the index mode to be used. Right now

		 * the WRITE operation is ignored. It is good enough to

		 * support the gfx driver to do EDID access.

	/* write the return value in AUX_CH_DATA reg which includes:

	 * ACK of I2C_WRITE

	 * returned byte if it is READ

/**

 * intel_vgpu_init_i2c_edid - initialize vGPU i2c edid emulation

 * @vgpu: a vGPU

 *

 * This function is used to initialize vGPU i2c edid emulation stuffs

 *

/*

 * Copyright Â© 2006-2010 Intel Corporation

 * Copyright (c) 2006 Dave Airlie <airlied@linux.ie>

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 *      Dave Airlie <airlied@linux.ie>

 *      Jesse Barnes <jesse.barnes@intel.com>

 *      Chris Wilson <chris@chris-wilson.co.uk>

	/*

	 * We don't want to lie too much to the user about the refresh

	 * rate they're going to get. But we have to allow a bit of latitude

	 * for Xorg since it likes to automagically cook up modes with slightly

	 * off refresh rates.

		/*

		 * If one mode has the same resolution with the fixed_panel

		 * mode while they have the different refresh rate, it means

		 * that the reduced downclock is found. In such

		 * case we can set the different FPx0/1 to dynamically select

		 * between low and high frequency.

			/*

			 * The downclock is already found. But we

			 * expect to find the lower downclock.

 prefer fixed mode from EDID if available */

 adjusted_mode has been preset to be the panel's fixed mode */

 Native modes don't need fitting */

 Scale but preserve the aspect ratio */

 pillar */

 letter */

 keep the hsync and hblank widths constant */

 make the border even */

 keep the vsync and vblank widths constant */

	/*

	 * Floating point operation is not supported. So the FACTOR

	 * is defined, which can avoid the floating point computation

	 * when calculating the panel ratio.

 965+ is easy, it does everything in hw */

	/*

	 * For earlier chips we have to calculate the scaling

	 * ratio by hand and program it into the

	 * PFIT_PGM_RATIO register

 pillar */

 letter */

 Aspects match, Let hw scale both directions */

 Native modes don't need fitting */

		/*

		 * For centered modes, we have to calculate border widths &

		 * heights and modify the values programmed into the CRTC.

 Scale but preserve the aspect ratio */

		/*

		 * Full scaling, even if it changes the aspect ratio.

		 * Fortunately this is all done for us in hw.

 965+ wants fuzzy fitting */

 FIXME: handle multiple panels by failing gracefully */

 Make sure pre-965 set dither correctly for 18bpp panels. */

/*

 * Copyright Â© 2006-2017 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/**

 * DOC: CDCLK / RAWCLK

 *

 * The display engine uses several different clocks to do its work. There

 * are two main clocks involved that aren't directly related to the actual

 * pixel clock or any symbol/bit clock of the actual output port. These

 * are the core display clock (CDCLK) and RAWCLK.

 *

 * CDCLK clocks most of the display pipe logic, and thus its frequency

 * must be high enough to support the rate at which pixels are flowing

 * through the pipes. Downscaling must also be accounted as that increases

 * the effective pixel rate.

 *

 * On several platforms the CDCLK frequency can be changed dynamically

 * to minimize power consumption for a given display configuration.

 * Typically changes to the CDCLK frequency require all the display pipes

 * to be shut down while the frequency is being changed.

 *

 * On SKL+ the DMC will toggle the CDCLK off/on during DC5/6 entry/exit.

 * DMC will not change the active CDCLK frequency however, so that part

 * will still be performed by the driver directly.

 *

 * RAWCLK is a fixed frequency clock, often used by various auxiliary

 * blocks such as AUX CH or backlight PWM. Hence the only thing we

 * really need to know about RAWCLK is its frequency so that various

 * dividers can be programmed correctly.

	/*

	 * 852GM/852GMV only supports 133 MHz and the HPLLCC

	 * encoding is different :(

	 * FIXME is this the right way to detect 852GM/852GMV?

	/* Assume that the hardware is in the high speed state.  This

	 * should be the default.

 FIXME other chipsets? */

	/*

	 * We seem to get an unstable or solid color picture at 200MHz.

	 * Not sure what's wrong. For now use 200MHz only when all pipes

	 * are off.

 jump to highest voltage for 400MHz too */

		/*

		 * Specs are full of misinformation, but testing on actual

		 * hardware has shown that we just need to write the desired

		 * CCK divider into the Punit register.

 CHV suggested value is 31 or 63 */

	/*

	 * WA - write default credits before re-programming

	 * FIXME: should we also set the resend bit here?

	/*

	 * FIXME is this guaranteed to clear

	 * immediately or should we poll for it?

	/* There are cases where we can end up here with power domains

	 * off and a CDCLK frequency other than the minimum, like when

	 * issuing a modeset without actually changing any display after

	 * a system suspend.  So grab the display core domain, which covers

	 * the HW blocks needed for the following programming.

 adjust cdclk divider */

 adjust self-refresh exit latency value */

	/*

	 * For high bandwidth configs, we set a higher latency in the bunit

	 * so that the core display fetch happens in time to avoid underruns.

 4.5 usec */

 3.0 usec */

	/* There are cases where we can end up here with power domains

	 * off and a CDCLK frequency other than the minimum, like when

	 * issuing a modeset without actually changing any display after

	 * a system suspend.  So grab the display core domain, which covers

	 * the HW blocks needed for the following programming.

	/*

	 * Can't read this out :( Let's assume it's

	 * at least what the CDCLK frequency requires.

	/*

	 * According to the spec, it should be enough to poll for this 1 us.

	 * However, extensive testing shows that this can take longer.

	/*

	 * Can't read this out :( Let's assume it's

	 * at least what the CDCLK frequency requires.

 convert from kHz to .1 fixpoint MHz with -1MHz offset */

	/*

	 * We always enable DPLL0 with the lowest link rate possible, but still

	 * taking into account the VCO required to operate the eDP panel at the

	 * desired frequency. The usual DP link rates operate with a VCO of

	 * 8100 while the eDP 1.4 alternate link rates need a VCO of 8640.

	 * The modeset code is responsible for the selection of the exact link

	 * rate later on, with the constraint of choosing a frequency that

	 * works with vco.

 We'll want to keep using the current vco from now on. */

	/*

	 * Based on WA#1183 CDCLK rates 308 and 617MHz CDCLK rates are

	 * unsupported on SKL. In theory this should never happen since only

	 * the eDP1.4 2.16 and 4.32Gbps rates require it, but eDP1.4 is not

	 * supported on SKL either, see the above WA. WARN whenever trying to

	 * use the corresponding VCO freq as that always leads to using the

	 * minimum 308MHz CDCLK.

 Wa Display #1183: skl,kbl,cfl */

 Wa Display #1183: skl,kbl,cfl */

 Wa Display #1183: skl,kbl,cfl */

 Wa Display #1183: skl,kbl,cfl */

 inform PCU of the change */

	/*

	 * check if the pre-os initialized the display

	 * There is SWF18 scratchpad register defined which is set by the

	 * pre-os which can be used by the OS drivers to check the status

 Is PLL enabled and locked ? */

	/* DPLL okay; verify the cdclock

	 *

	 * Noticed in some instances that the freq selection is correct but

	 * decimal part is programmed wrong from BIOS where pre-os does not

	 * enable display. Verify the same as well.

 All well; nothing to sanitize */

 force cdclk programming */

 force full PLL disable + enable */

		/*

		 * Use the current vco as our initial

		 * guess as to what the preferred vco is.

		/*

		 * CDCLK PLL is disabled, the VCO/ratio doesn't matter, but

		 * setting it to zero is a way to signal that.

	/*

	 * DISPLAY_VER >= 11 have the ratio directly in the PLL enable register,

	 * gen9lp had it in a separate PLL control register.

	/*

	 * Can't read this out :( Let's assume it's

	 * at least what the CDCLK frequency requires.

 Timeout 200us */

 Timeout 200us */

 Timeout 200us */

 Timeout 200us */

 Write PLL ratio without disabling */

 Submit freq change request */

 Timeout 200us */

 cdclk = vco / 2 / div{1,1.5,2,4} */

 Inform power controller of upcoming frequency change. */

		/*

		 * BSpec requires us to wait up to 150usec, but that leads to

		 * timeouts; the 2ms used here is based on experiment.

	/*

	 * Disable SSA Precharge when CD clock frequency < 500 MHz,

	 * enable otherwise.

		/*

		 * The timeout isn't specified, the 2ms used here is based on

		 * experiment.

		 * FIXME: Waiting for the request completion could be delayed

		 * until the next PCODE request based on BSpec.

		/*

		 * Can't read out the voltage level :(

		 * Let's just assume everything is as expected.

	/* DPLL okay; verify the cdclock

	 *

	 * Some BIOS versions leave an incorrect decimal frequency value and

	 * set reserved MBZ bits in CDCLK_CTL at least during exiting from S4,

	 * so sanitize this register.

	/*

	 * Let's ignore the pipe field, since BIOS could have configured the

	 * dividers both synching to an active pipe, or asynchronously

	 * (PIPE_NONE).

 Make sure this is a legal cdclk value for the platform */

 Make sure the VCO is correct for the cdclk */

 Figure out what CD2X divider we should be using for this cdclk */

	/*

	 * Disable SSA Precharge when CD clock frequency < 500 MHz,

	 * enable otherwise.

 All well; nothing to sanitize */

 force cdclk programming */

 force full PLL disable + enable */

	/*

	 * FIXME:

	 * - The initial CDCLK needs to be read from VBT.

	 *   Need to make this change after VBT has changes for BXT.

/**

 * intel_cdclk_init_hw - Initialize CDCLK hardware

 * @i915: i915 device

 *

 * Initialize CDCLK. This consists mainly of initializing dev_priv->cdclk.hw and

 * sanitizing the state of the hardware if needed. This is generally done only

 * during the display core initialization sequence, after which the DMC will

 * take care of turning CDCLK off/on as needed.

/**

 * intel_cdclk_uninit_hw - Uninitialize CDCLK hardware

 * @i915: i915 device

 *

 * Uninitialize CDCLK. This is done only during the display core

 * uninitialization sequence.

	/*

	 * The vco and cd2x divider will change independently

	 * from each, so we disallow cd2x change when crawling.

/**

 * intel_cdclk_needs_modeset - Determine if changong between the CDCLK

 *                             configurations requires a modeset on all pipes

 * @a: first CDCLK configuration

 * @b: second CDCLK configuration

 *

 * Returns:

 * True if changing between the two CDCLK configurations

 * requires all pipes to be off, false if not.

/**

 * intel_cdclk_can_cd2x_update - Determine if changing between the two CDCLK

 *                               configurations requires only a cd2x divider update

 * @dev_priv: i915 device

 * @a: first CDCLK configuration

 * @b: second CDCLK configuration

 *

 * Returns:

 * True if changing between the two CDCLK configurations

 * can be done with just a cd2x divider update, false if not.

 Older hw doesn't have the capability */

/**

 * intel_cdclk_changed - Determine if two CDCLK configurations are different

 * @a: first CDCLK configuration

 * @b: second CDCLK configuration

 *

 * Returns:

 * True if the CDCLK configurations don't match, false if they do.

/**

 * intel_set_cdclk - Push the CDCLK configuration to the hardware

 * @dev_priv: i915 device

 * @cdclk_config: new CDCLK configuration

 * @pipe: pipe with which to synchronize the update

 *

 * Program the hardware based on the passed in CDCLK state,

 * if necessary.

	/*

	 * Lock aux/gmbus while we change cdclk in case those

	 * functions use cdclk. Not all platforms/ports do,

	 * but we'll lock them all for simplicity.

/**

 * intel_set_cdclk_pre_plane_update - Push the CDCLK state to the hardware

 * @state: intel atomic state

 *

 * Program the hardware before updating the HW plane state based on the

 * new CDCLK state, if necessary.

/**

 * intel_set_cdclk_post_plane_update - Push the CDCLK state to the hardware

 * @state: intel atomic state

 *

 * Program the hardware after updating the HW plane state based on the

 * new CDCLK state, if necessary.

 pixel rate mustn't exceed 95% of cdclk with IPS on BDW */

	/* BSpec says "Do not use DisplayPort with CDCLK less than 432 MHz,

	 * audio enabled, port width x4, and link rate HBR2 (5.4 GHz), or else

	 * there may be audio corruption or screen corruption." This cdclk

	 * restriction for GLK is 316.8 MHz.

 Display WA #1145: glk */

 Display WA #1144: skl,bxt */

	/*

	 * According to BSpec, "The CD clock frequency must be at least twice

	 * the frequency of the Azalia BCLK." and BCLK is 96 MHz by default.

	/*

	 * "For DP audio configuration, cdclk frequency shall be set to

	 *  meet the following requirements:

	 *  DP Link Frequency(MHz) | Cdclk frequency(MHz)

	 *  270                    | 320 or higher

	 *  162                    | 200 or higher"

	/*

	 * On Valleyview some DSI panels lose (v|h)sync when the clock is lower

	 * than 320000KHz.

	/*

	 * On Geminilake once the CDCLK gets as low as 79200

	 * picture gets unstable, despite that values are

	 * correct for DSI PLL and DE PLL.

 Account for additional needs from the planes */

	/*

	 * When we decide to use only one VDSC engine, since

	 * each VDSC operates with 1 ppc throughput, pixel clock

	 * cannot be higher than the VDSC clock (cdclk)

	/*

	 * HACK. Currently for TGL platforms we calculate

	 * min_cdclk initially based on pixel_rate divided

	 * by 2, accounting for also plane requirements,

	 * however in some cases the lowest possible CDCLK

	 * doesn't work and causing the underruns.

	 * Explicitly stating here that this seems to be currently

	 * rather a Hack, than final solution.

		/*

		 * Clamp to max_cdclk_freq in case pixel rate is higher,

		 * in order not to break an 8K, but still leave W/A at place.

/*

 * Account for port clock min voltage level requirements.

 * This only really does something on DISPLA_VER >= 11 but can be

 * called on earlier platforms as well.

 *

 * Note that this functions assumes that 0 is

 * the lowest voltage value, and higher values

 * correspond to increasingly higher voltages.

 *

 * Should that relationship no longer hold on

 * future platforms this code will need to be

 * adjusted.

	/*

	 * FIXME should also account for plane ratio

	 * once 64bpp pixel formats are supported.

		/*

		 * DPLL0 VCO may need to be adjusted to get the correct

		 * clock for eDP. This will affect cdclk as well.

	/*

	 * FIXME should also account for plane ratio

	 * once 64bpp pixel formats are supported.

	/*

	 * We can't change the cdclk frequency, but we still want to

	 * check that the required minimum frequency doesn't exceed

	 * the actual cdclk frequency.

		/*

		 * Also serialize commits across all crtcs

		 * if the actual hw needs to be poked.

 All pipes must be switched off while we change the cdclk. */

/**

 * intel_update_max_cdclk - Determine the maximum support CDCLK frequency

 * @dev_priv: i915 device

 *

 * Determine the maximum CDCLK frequency the platform supports, and also

 * derive the maximum dot clock frequency the maximum CDCLK frequency

 * allows.

		/*

		 * Use the lower (vco 8640) cdclk values as a

		 * first guess. skl_calc_cdclk() will correct it

		 * if the preferred vco is 8100 instead.

		/*

		 * FIXME with extra cooling we can allow

		 * 540 MHz for ULX and 675 Mhz for ULT.

		 * How can we know if extra cooling is

		 * available? PCI ID, VTB, something else?

 otherwise assume cdclk is fixed */

/**

 * intel_update_cdclk - Determine the current CDCLK frequency

 * @dev_priv: i915 device

 *

 * Determine the current CDCLK frequency.

	/*

	 * 9:0 CMBUS [sic] CDCLK frequency (cdfreq):

	 * Programmng [sic] note: bit[9:2] should be programmed to the number

	 * of cdclk that generates 4MHz reference clock freq which is used to

	 * generate GMBus clock. This will vary with the cdclk freq.

	/*

	 * DG1 always uses a 38.4 MHz rawclk.  The bspec tells us

	 * "Program Numerator=2, Denominator=4, Divider=37 decimal."

 24 MHz */

 19.2 MHz */

 RAWCLK_FREQ_VLV register updated from power well code */

	/*

	 * hrawclock is 1/4 the FSB frequency

	 *

	 * Note that this only reads the state of the FSB

	 * straps, not the actual FSB frequency. Some BIOSen

	 * let you configure each independently. Ideally we'd

	 * read out the actual FSB frequency but sadly we

	 * don't know which registers have that information,

	 * and all the relevant docs have gone to bit heaven :(

/**

 * intel_read_rawclk - Determine the current RAWCLK frequency

 * @dev_priv: i915 device

 *

 * Determine the current RAWCLK frequency. RAWCLK is a fixed

 * frequency clock so this needs to done only once.

 no rawclk on other platforms, or no need to know it */

 SNB, IVB, 965G, 945G */

 G45 uses G33 */

 i965G uses fixed 400 */

 i945G uses fixed 400 */

/**

 * intel_init_cdclk_hooks - Initialize CDCLK related modesetting hooks

 * @dev_priv: i915 device

 Wa_22011320316:adl-p[a0] */

/*

 * Copyright Â© 2013 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Shobhit Kumar <shobhit.kumar@intel.com>

 *	Yogesh Mohan Marimuthu <yogesh.mohan.marimuthu@intel.com>

 62 - 70 */

 71 - 80 */

 81 - 90 */

 91 - 100 */

 Get DSI clock from pixel clock */

	/* DSI data rate = pixel clock * bits per pixel / lane count

 target_dsi_clk is expected in kHz */

			/*

			 * Find the optimal m and p divisors with minimal delta

			 * +/- the required clock

 register has log2(N1), this works fine for powers of two */

/*

 * XXX: The muxing and gating is hard coded for now. Need to add support for

 * sharing PLLs with two DSI outputs.

	/* wait at least 0.5 us after ungating before enabling VCO,

	 * allow hrtimer subsystem optimization by relaxing timing

	/*

	 * Dividers must be programmed with valid values. As per BSEPC, for

	 * GEMINLAKE only PORT A divider values are checked while for BXT

	 * both divider values are validated. Check this here for

	 * paranoia, since BIOS is known to misconfigure PLLs in this way at

	 * times, and since accessing DSI registers with invalid dividers

	 * causes a system hang.

	/*

	 * PLL lock should deassert within 200us.

	 * Wait up to 1ms before timing out.

 mask out other bits and extract the P1 divisor */

 N1 divisor */

 register has log2(N1) */

 mask out the other bits and extract the M1 divisor */

 Variable divider value */

 Calculate TXESC1 divider */

 Calculate TXESC2 divider */

 Program BXT Mipi clocks and dividers */

 Clear old configurations */

 Get the current DSI rate(actual) */

	/*

	 * tx clock should be <= 20MHz and the div value must be

	 * subtracted by 1 as per bspec

	/*

	 * rx clock should be <= 150MHz and the div value must be

	 * subtracted by 1 as per bspec

	/*

	 * rx divider value needs to be updated in the

	 * two differnt bit fields in the register hence splitting the

	 * rx divider value accordingly

	/*

	 * From clock diagram, to get PLL ratio divider, divide double of DSI

	 * link rate (i.e., 2*8x=16x frequency value) by ref clock. Make sure to

	 * round 'up' the result

	/*

	 * Program DSI ratio and Select MIPIC and MIPIA PLL output as 8x

	 * Spec says both have to be programmed, even if one is not getting

	 * used. Configure MIPI_CLOCK_CTL dividers in modeset

	/* As per recommendation from hardware team,

	 * Prog PVD ratio =1 if dsi ratio <= 50

 Configure PLL vales */

 Program TX, RX, Dphy clocks */

 Enable DSI PLL */

 Timeout and fail if PLL not locked */

 Clear old configurations */

/*

 * Copyright Â© 2007 Dave Mueller

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Dave Mueller <dave.mueller@gmx.ch>

 *

 register definitions according to the TFP410 data sheet */

 Ti TFP410 driver for chip on i2c bus */

 this will detect the tfp410 chip on the specified i2c bus */

	/* As long as the basics are set up, since we don't have clock dependencies

	* in the mode setup, we can just leave the registers alone and everything

	* will work fine.

 don't do much */

 set the tfp410 power state */

/*

 * Copyright (c) 2007 Dave Airlie <airlied@linux.ie>

 * Copyright (c) 2007, 2010 Intel Corporation

 *   Jesse Barnes <jesse.barnes@intel.com>

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

	/*

	 * Allocate enough memory to hold intel_digital_connector_state,

	 * This might be a few bytes too many, but for connectors that don't

	 * need it we'll free the state and allocate a smaller one on the first

	 * successful commit anyway.

/*

 * Free the bits allocated by intel_connector_alloc.

 * This should only be used after intel_connector_alloc has returned

 * successfully, and before drm_connector_init returns successfully.

 * Otherwise the destroy callbacks for the connector and the state should

 * take care of proper cleanup/free (see intel_connector_destroy).

/*

 * Connector type independent destroy hook for drm_connector_funcs.

/*

 * Simple connector->get_hw_state implementation for encoders that support only

 * one connector and no cloning and hence the encoder state determines the state

 * of the connector.

/**

 * intel_connector_update_modes - update connector from edid

 * @connector: DRM connector device to use

 * @edid: previously read EDID information

/**

 * intel_ddc_get_modes - get modelist from monitor

 * @connector: DRM connector device to use

 * @adapter: i2c adapter

 *

 * Fetch the EDID information from @connector using the DDC bus.

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/**

 * DOC: atomic plane helpers

 *

 * The functions here are used by the atomic plane helper functions to

 * implement legacy plane updates (i.e., drm_plane->update_plane() and

 * drm_plane->disable_plane()).  This allows plane updates to use the

 * atomic state infrastructure and perform plane updates as separate

 * prepare/check/commit/cleanup steps.

/**

 * intel_plane_duplicate_state - duplicate plane state

 * @plane: drm plane

 *

 * Allocates and returns a copy of the plane state (both common and

 * Intel-specific) for the specified plane.

 *

 * Returns: The newly allocated plane state, or NULL on failure.

 add reference to fb */

/**

 * intel_plane_destroy_state - destroy plane state

 * @plane: drm plane

 * @state: state object to destroy

 *

 * Destroys the plane state (both common and Intel-specific) for the

 * specified plane.

 Downscaling limits the maximum pixel rate */

	/*

	 * Note we don't check for plane visibility here as

	 * we want to use this when calculating the cursor

	 * watermarks even if the cursor is fully offscreen.

	 * That depends on the src/dst rectangles being

	 * correctly populated whenever the watermark code

	 * considers the cursor to be visible, whether or not

	 * it is actually visible.

	 *

	 * See: intel_wm_plane_visible() and intel_check_cursor()

	/*

	 * Based on HSD#:1408715493

	 * NV12 cpp == 4, P010 cpp == 8

	 *

	 * FIXME what is the logic behind this?

	/*

	 * No need to check against the cdclk state if

	 * the min cdclk for the plane doesn't increase.

	 *

	 * Ie. we only ever increase the cdclk due to plane

	 * requirements. This can reduce back and forth

	 * display blinking due to constant cdclk changes.

	/*

	 * No need to recalculate the cdclk state if

	 * the min cdclk for the pipe doesn't increase.

	 *

	 * Ie. we only ever increase the cdclk due to plane

	 * requirements. This can reduce back and forth

	 * display blinking due to constant cdclk changes.

	/*

	 * For the bigjoiner slave uapi.crtc will point at

	 * the master crtc. So we explicitly assign the right

	 * slave crtc to hw.crtc. uapi.crtc!=NULL simply indicates

	 * the plane is logically enabled on the uapi level.

 FIXME pre-g4x don't work like this */

 should never happen */

 Check scaling */

 right side of the image is on the slave crtc, adjust dst to match */

	/*

	 * FIXME: This might need further adjustment for seamless scaling

	 * with phase information, for the 2p2 and 2p1 scenarios.

	/*

	 * If we missed the vblank, but the request is already running it

	 * is reasonable to assume that it will complete before the next

	 * vblank without our intervention, so leave RPS alone.

/**

 * intel_prepare_plane_fb - Prepare fb for usage on plane

 * @_plane: drm plane to prepare for

 * @_new_plane_state: the plane state being prepared

 *

 * Prepares a framebuffer for usage on a display plane.  Generally this

 * involves pinning the underlying object and updating the frontbuffer tracking

 * bits.  Some older platforms need special physical address handling for

 * cursor planes.

 *

 * Returns 0 on success, negative error code on failure.

		/* Big Hammer, we also need to ensure that any pending

		 * MI_WAIT_FOR_EVENT inside a user batch buffer on the

		 * current scanout is retired before unpinning the old

		 * framebuffer. Note that we rely on userspace rendering

		 * into the buffer attached to the pipe they are waiting

		 * on. If not, userspace generates a GPU hang with IPEHR

		 * point to the MI_WAIT_FOR_EVENT.

		 *

		 * This should only fail upon a hung GPU, in which case we

		 * can safely continue.

 explicit fencing */

 implicit fencing */

	/*

	 * We declare pageflips to be interactive and so merit a small bias

	 * towards upclocking to deliver the frame on time. By only changing

	 * the RPS thresholds to sample more regularly and aim for higher

	 * clocks we can hopefully deliver low power workloads (like kodi)

	 * that are not quite steady state without resorting to forcing

	 * maximum clocks following a vblank miss (see do_rps_boost()).

/**

 * intel_cleanup_plane_fb - Cleans up an fb after plane use

 * @plane: drm plane to clean up for

 * @_old_plane_state: the state from the previous modeset

 *

 * Cleans up a framebuffer that has just been removed from a plane.

 Should only be called after a successful intel_prepare_plane_fb()! */

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

/*

 * ILK+ csc matrix:

 *

 * |R/Cr|   | c0 c1 c2 |   ( |R/Cr|   |preoff0| )   |postoff0|

 * |G/Y | = | c3 c4 c5 | x ( |G/Y | + |preoff1| ) + |postoff1|

 * |B/Cb|   | c6 c7 c8 |   ( |B/Cb|   |preoff2| )   |postoff2|

 *

 * ILK/SNB don't have explicit post offsets, and instead

 * CSC_MODE_YUV_TO_RGB and CSC_BLACK_SCREEN_OFFSET are used:

 *  CSC_MODE_YUV_TO_RGB=0 + CSC_BLACK_SCREEN_OFFSET=0 -> 1/2, 0, 1/2

 *  CSC_MODE_YUV_TO_RGB=0 + CSC_BLACK_SCREEN_OFFSET=1 -> 1/2, 1/16, 1/2

 *  CSC_MODE_YUV_TO_RGB=1 + CSC_BLACK_SCREEN_OFFSET=0 -> 0, 0, 0

 *  CSC_MODE_YUV_TO_RGB=1 + CSC_BLACK_SCREEN_OFFSET=1 -> 1/16, 1/16, 1/16

/*

 * Extract the CSC coefficient from a CTM coefficient (in U32.32 fixed point

 * format). This macro takes the coefficient we want transformed and the

 * number of fractional bits.

 *

 * We only have a 9 bits precision window which slides depending on the value

 * of the CTM coefficient and we write the value from bit 3. We also round the

 * value.

 Nop pre/post offsets */

 Identity matrix */

 Limited range RGB post offsets */

 Full range RGB -> limited range RGB matrix */

 BT.709 full range RGB -> limited range YCbCr matrix */

 Limited range YCbCr post offsets */

/*

 * When using limited range, multiply the matrix given by userspace by

 * the matrix that we would use for the limited range.

		/*

		 * By scaling every co-efficient with limited range (16-235)

		 * vs full range (0-255) the final o/p will be scaled down to

		 * fit in the limited range supported by the panel.

	/*

	 * FIXME if there's a gamma LUT after the CSC, we should

	 * do the range compression using the gamma LUT instead.

	/*

	 * Convert fixed point S31.32 input to format supported by the

	 * hardware.

		/*

		 * Clamp input value to min/max supported by

		 * hardware.

 sign bit */

		/*

		 * On GLK both pipe CSC and degamma LUT are controlled

		 * by csc_enable. Hence for the cases where the degama

		 * LUT is needed but CSC is not we need to load an

		 * identity matrix.

 Round coefficient. */

 Clamp to hardware limits. */

 Write coefficients in S3.12 format. */

 convert hw value with given bit_precision to lut property val */

 i965+ "10.6" bit interpolated format "even DW" (low 8 bits) */

 i965+ "10.6" interpolated format "odd DW" (high 8 bits) */

 PIPEGCMAX is 11.6, clamp to 10.6 */

	/*

	 * We don't (yet) allow userspace to control the pipe background color,

	 * so force it to black, but apply pipe gamma and CSC appropriately

	 * so that its handling will match how we program our planes.

/*

 * IVB/HSW Bspec / PAL_PREC_INDEX:

 * "Restriction : Index auto increment mode is not

 *  supported and must not be enabled."

 We discard half the user entries in split gamma mode */

	/*

	 * Reset the index, otherwise it prevents the legacy palette to be

	 * written properly.

 On BDW+ the index auto increment mode actually works */

 We discard half the user entries in split gamma mode */

	/*

	 * Reset the index, otherwise it prevents the legacy palette to be

	 * written properly.

 Program the max register to clamp values > 1.0. */

	/*

	 * Program the gc max 2 register to clamp values > 1.0.

	 * ToDo: Extend the ABI to be able to program values

	 * from 3.0 to 7.0

	/*

	 * When setting the auto-increment bit, the hardware seems to

	 * ignore the index bits, so we need to reset it to index 0

	 * separately.

		/*

		 * First 33 entries represent range from 0 to 1.0

		 * 34th and 35th entry will represent extended range

		 * inputs 3.0 and 7.0 respectively, currently clamped

		 * at 1.0. Since the precision is 16bit, the user

		 * value can be directly filled to register.

		 * The pipe degamma table in GLK+ onwards doesn't

		 * support different values per channel, so this just

		 * programs green value which will be equal to Red and

		 * Blue into the lut registers.

		 * ToDo: Extend to max 7.0. Enable 32 bit input value

		 * as compared to just 16 to achieve this.

 Clamp values > 1.0. */

	/*

	 * When setting the auto-increment bit, the hardware seems to

	 * ignore the index bits, so we need to reset it to index 0

	 * separately.

 Clamp values > 1.0. */

	/*

	 * On GLK+ both pipe CSC and degamma LUT are controlled

	 * by csc_enable. Hence for the cases where the CSC is

	 * needed but degamma LUT is not we need to load a

	 * linear degamma LUT. In fact we'll just always load

	 * the degama LUT so that we don't have to reload

	 * it every time the pipe CSC is being enabled.

 ilk+ "12.4" interpolated format (high 10 bits) */

 ilk+ "12.4" interpolated format (low 6 bits) */

 FIXME LUT entries are 16 bit only, so we can prog 0xFFFF max */

	/*

	 * Program Super Fine segment (let's call it seg1)...

	 *

	 * Super Fine segment's step is 1/(8 * 128 * 256) and it has

	 * 9 entries, corresponding to values 0, 1/(8 * 128 * 256),

	 * 2/(8 * 128 * 256) ... 8/(8 * 128 * 256).

	/*

	 * Program Fine segment (let's call it seg2)...

	 *

	 * Fine segment's step is 1/(128 * 256) i.e. 1/(128 * 256), 2/(128 * 256)

	 * ... 256/(128 * 256). So in order to program fine segment of LUT we

	 * need to pick every 8th entry in the LUT, and program 256 indexes.

	 *

	 * PAL_PREC_INDEX[0] and PAL_PREC_INDEX[1] map to seg2[1],

	 * seg2[0] being unused by the hardware.

	/*

	 * Program Coarse segment (let's call it seg3)...

	 *

	 * Coarse segment starts from index 0 and it's step is 1/256 ie 0,

	 * 1/256, 2/256 ... 256/256. As per the description of each entry in LUT

	 * above, we need to pick every (8 * 128)th entry in LUT, and

	 * program 256 of those.

	 *

	 * Spec is not very clear about if entries seg3[0] and seg3[1] are

	 * being used or not, but we still need to program these to advance

	 * the index.

 The last entry in the LUT is to be programmed in GCMAX */

	/*

	 * CGM_PIPE_MODE is itself single buffered. We'd have to

	 * somehow split it out from chv_load_luts() if we wanted

	 * the ability to preload the CGM LUTs/CSC without tearing.

	/*

	 * The hardware degamma is active whenever the pipe

	 * CSC is active. Thus even if the old state has no

	 * software degamma we need to avoid clobbering the

	 * linear hardware degamma mid scanout.

	/*

	 * On pre-SKL the pipe gamma enable and pipe csc enable for

	 * the pipe bottom color are configured via the primary plane.

	 * We have to reconfigure that even if the plane is inactive.

 Always allow legacy gamma LUT with no further checking. */

 C8 relies on its palette being stored in the legacy LUT */

 i965+ only */

/*

 * CHV color pipeline:

 * u0.10 -> CGM degamma -> u0.14 -> CGM csc -> u0.14 -> CGM gamma ->

 * u0.10 -> WGC csc -> u0.10 -> pipe gamma -> u0.10

 *

 * We always bypass the WGC csc and use the CGM csc

 * instead since it has degamma and better precision.

	/*

	 * Pipe gamma will be used only for the legacy LUT.

	 * Otherwise we bypass it and use the CGM gamma instead.

	/*

	 * CSC comes after the LUT in RGB->YCbCr mode.

	 * RGB->YCbCr needs the limited range offsets added to

	 * the output. RGB limited range output is handled by

	 * the hw automagically elsewhere.

	/*

	 * We don't expose the ctm on ilk/snb currently, also RGB

	 * limited range output is handled by the hw automagically.

	/*

	 * CSC comes after the LUT in degamma, RGB->YCbCr,

	 * and RGB full->limited range mode.

 On GLK+ degamma LUT is controlled by csc_enable */

 check sw and hw lut size */

 check sw and hw lut entry to be equal */

 On BDW+ the index auto increment mode actually works */

	/*

	 * FIXME readouts from PAL_PREC_DATA register aren't giving

	 * correct values in the case of fine and coarse segments.

	 * Restricting readouts only for super fine segment as of now.

 SPDX-License-Identifier: MIT */

/*

 * Copyright Â© 2019 Intel Corporation

/**

 * __intel_display_power_is_enabled - unlocked check for a power domain

 * @dev_priv: i915 device instance

 * @domain: power domain to check

 *

 * This is the unlocked version of intel_display_power_is_enabled() and should

 * only be used from error capture and recovery code where deadlocks are

 * possible.

 *

 * Returns:

 * True when the power domain is enabled, false otherwise.

/**

 * intel_display_power_is_enabled - check for a power domain

 * @dev_priv: i915 device instance

 * @domain: power domain to check

 *

 * This function can be used to check the hw power domain state. It is mostly

 * used in hardware state readout functions. Everywhere else code should rely

 * upon explicit power domain reference counting to ensure that the hardware

 * block is powered up before accessing it.

 *

 * Callers must hold the relevant modesetting locks to ensure that concurrent

 * threads can't disable the power well while the caller tries to read a few

 * registers.

 *

 * Returns:

 * True when the power domain is enabled, false otherwise.

/*

 * Starting with Haswell, we have a "Power Down Well" that can be turned off

 * when not needed anymore. We have 4 registers that can request the power well

 * to be enabled, and it will only be disabled if none of the registers is

 * requesting it to be enabled.

 We'll check the MST primary port */

	/*

	 * For some power wells we're not supposed to watch the status bit for

	 * an ack, but rather just wait a fixed amount of time and then

	 * proceed.  This is only used on DG2.

 Timeout for PW1:10 us, AUX:not specified, other PWs:20 us. */

	/*

	 * Bspec doesn't require waiting for PWs to get disabled, but still do

	 * this for paranoia. The known cases where a PW will be forced on:

	 * - a KVMR request on any power well via the KVMR request register

	 * - a DMC request on PW1 and MISC_IO power wells via the BIOS and

	 *   DEBUG request registers

	 * Skip the wait in case any of the request bits are set and print a

	 * diagnostic message.

 Timeout 5us for PG#0, for other PGs 1us */

		/*

		 * For PW1 we have to wait both for the PW0/PG0 fuse state

		 * before enabling the power well and PW1/PG1's own fuse

		 * state after the enabling. For all other power wells with

		 * fuses we only have to wait for that PW/PG's fuse state

		 * after the enabling.

 Display WA #1178: icl */

 Bypass the check if all references are released asynchronously */

 Spec states that TC cold exit can take up to 1ms to complete */

 TODO: turn failure into a error as soon i915 CI updates ICL IFWI */

	/*

	 * An AUX timeout is expected if the TBT DP tunnel is down,

	 * or need to enable AUX on a legacy TypeC port as part of the TC-cold

	 * exit sequence.

/*

 * We should only use the power well if we explicitly asked the hardware to

 * enable it, so check if it's enabled and also check if we've requested it to

 * be enabled.

	/*

	 * On GEN9 big core due to a DMC bug the driver's request bits for PW1

	 * and the MISC_IO PW will be not restored, so check instead for the

	 * BIOS's own request bits, which are forced-on for these power wells

	 * when exiting DC5/6.

	 /*

	  * TODO: check for the following to verify the conditions to enter DC9

	  * state are satisfied:

	  * 1] Check relevant display engine registers to verify if mode set

	  * disable sequence was followed.

	  * 2] Check if display uninitialize sequence is initialized.

	 /*

	  * TODO: check for the following to verify DC9 state was indeed

	  * entered before programming to disable it:

	  * 1] Check relevant display engine registers to verify if mode

	  *  set disable sequence was followed.

	  * 2] Check if display uninitialize sequence is initialized.

	/* It has been observed that disabling the dc6 state sometimes

	 * doesn't stick and dmc keeps returning old value. Make sure

	 * the write really sticks enough times and also force rewrite until

	 * we are confident that state is exactly what we want.

 Most of the times we need one retry, avoid spam */

/**

 * gen9_set_dc_state - set target display C power state

 * @dev_priv: i915 device instance

 * @state: target DC power state

 * - DC_STATE_DISABLE

 * - DC_STATE_EN_UPTO_DC5

 * - DC_STATE_EN_UPTO_DC6

 * - DC_STATE_EN_DC9

 *

 * Signal to DMC firmware/HW the target DC power state passed in @state.

 * DMC/HW can turn off individual display clocks and power rails when entering

 * a deeper DC power state (higher in number) and turns these back when exiting

 * that state to a shallower power state (lower in number). The HW will decide

 * when to actually enter a given state on an on-demand basis, for instance

 * depending on the active state of display pipes. The state of display

 * registers backed by affected power rails are saved/restored as needed.

 *

 * Based on the above enabling a deeper DC power state is asynchronous wrt.

 * enabling it. Disabling a deeper power state is synchronous: for instance

 * setting %DC_STATE_DISABLE won't complete until all HW resources are turned

 * back on and register state is restored. This is guaranteed by the MMIO write

 * to DC_STATE_EN blocking until the state is restored.

 Check if DMC is ignoring our DC state requests */

	/*

	 * Delay of 200us DC3CO Exit time B.Spec 49196

	/*

	 * Power sequencer reset is not needed on

	 * platforms with South Display Engine on PCH,

	 * because PPS registers are always on.

	/*

	 * It's not feasible to add error checking code to the callers since

	 * this condition really shouldn't happen and it doesn't even make sense

	 * to abort things like display initialization sequences. Just return

	 * the first power well and hope the WARN gets reported so we can fix

	 * our driver.

/**

 * intel_display_power_set_target_dc_state - Set target dc state.

 * @dev_priv: i915 device

 * @state: state which needs to be set as target_dc_state.

 *

 * This function set the "DC off" power well target_dc_state,

 * based upon this target_dc_stste, "DC off" power well will

 * enable desired DC state.

	/*

	 * If DC off power well is disabled, need to enable and disable the

	 * DC off power well to effect target DC state.

 Power wells at this level and above must be disabled for DC5 entry */

 Wa Display #1183: skl,kbl,cfl */

 Wa Display #1183: skl,kbl,cfl */

 Take over the request bit if set by BIOS. */

 Can't read out voltage_level so can't use intel_cdclk_changed() */

		/*

		 * DMC retains HW context only for port A, the other combo

		 * PHY's HW context for port B is lost after DC transitions,

		 * so we need to restore it manually.

	/*

	 * We only ever set the power-on and power-gate states, anything

	 * else is unexpected.

	/*

	 * A transient state at this point would mean some unexpected party

	 * is poking at the power controls too.

	/*

	 * On driver load, a pipe may be active and driving a DSI display.

	 * Preserve DPOUNIT_CLOCK_GATE_DISABLE to avoid the pipe getting stuck

	 * (and never recovering) in this case. intel_dsi_post_disable() will

	 * clear it when we turn off the display.

	/*

	 * Disable trickle feed and enable pnd deadline calculation

	/*

	 * Enable the CRI clock source so we can get at the

	 * display and the reference clock for VGA

	 * hotplug / manual detection. Supposedly DSI also

	 * needs the ref clock up and running.

	 *

	 * CHV DPLL B/C have some issues if VGA mode is enabled.

	/*

	 * During driver initialization/resume we can avoid restoring the

	 * part of the HW/SW state that will be inited anyway explicitly.

 Re-enable the ADPA, if we have one */

 make sure we're done processing display irqs */

 Prevent us from re-enabling polling on accident in late suspend */

 since ref/cri clock was enabled */

 >10ns for cmnreset, >0ns for sidereset */

	/*

	 * From VLV2A0_DP_eDP_DPIO_driver_vbios_notes_10.docx -

	 *  6.	De-assert cmn_reset/side_reset. Same as VLV X0.

	 *   a.	GUnit 0x2110 bit[0] set to 1 (def 0)

	 *   b.	The other bits such as sfr settings / modesel may all

	 *	be set to 0.

	 *

	 * This should only be done on init and resume from S3 with

	 * both PLLs disabled, or we risk losing DPIO and PLL

	 * synchronization.

 Assert common reset */

	/*

	 * The BIOS can leave the PHY is some weird state

	 * where it doesn't fully power down some parts.

	 * Disable the asserts until the PHY has been fully

	 * reset (ie. the power well has been disabled at

	 * least once).

 this assumes override is only used to enable lanes */

 CL1 is on whenever anything is on in either channel */

		/*

		 * The DPLLB check accounts for the pipe B + port A usage

		 * with CL2 powered up but all the lanes in the second channel

		 * powered down.

 this assumes override is only used to enable lanes */

	/*

	 * The PHY may be busy with some initial calibration and whatnot,

	 * so the power state can take a while to actually change.

 since ref/cri clock was enabled */

 >10ns for cmnreset, >0ns for sidereset */

 Poll for phypwrgood signal */

 Enable dynamic power down */

		/*

		 * Force the non-existing CL2 off. BXT does this

		 * too, so maybe it saves some power even though

		 * CL2 doesn't exist?

 PHY is fully reset now, so we can enable the PHY state asserts */

	/*

	 * The BIOS can leave the PHY is some weird state

	 * where it doesn't fully power down some parts.

	 * Disable the asserts until the PHY has been fully

	 * reset (ie. the power well has been disabled at

	 * least once).

	/*

	 * This assumes !override is only used when the port is disabled.

	 * All lanes should power down even without the override when

	 * the port is disabled.

		/*

		 * If CH1 common lane is not active anymore

		 * (eg. for pipe B DPLL) the entire channel will

		 * shut down, which causes the common lane registers

		 * to read as 0. That means we can't actually check

		 * the lane power down status bits, but as the entire

		 * register reads as 0 it's a good indication that the

		 * channel is indeed entirely powered down.

	/*

	 * We only ever set the power-on and power-gate states, anything

	 * else is unexpected.

	/*

	 * A transient state at this point would mean some unexpected party

	 * is poking at the power controls too.

 CONFIG_DRM_I915_DEBUG_RUNTIME_PM */

/**

 * intel_display_power_get - grab a power domain reference

 * @dev_priv: i915 device instance

 * @domain: power domain to reference

 *

 * This function grabs a power domain reference for @domain and ensures that the

 * power domain and all its parents are powered up. Therefore users should only

 * grab a reference to the innermost power domain they need.

 *

 * Any power domain reference obtained by this function must have a symmetric

 * call to intel_display_power_put() to release the reference again.

/**

 * intel_display_power_get_if_enabled - grab a reference for an enabled display power domain

 * @dev_priv: i915 device instance

 * @domain: power domain to reference

 *

 * This function grabs a power domain reference for @domain and ensures that the

 * power domain and all its parents are powered up. Therefore users should only

 * grab a reference to the innermost power domain they need.

 *

 * Any power domain reference obtained by this function must have a symmetric

 * call to intel_display_power_put() to release the reference again.

	/*

	 * The caller must hold already raw wakeref, upgrade that to a proper

	 * wakeref to make the state checker happy about the HW access during

	 * power well disabling.

 Clear before put, so put's sanity check is happy. */

	/*

	 * Bail out if all the domain refs pending to be released were grabbed

	 * by subsequent gets or a flush_work.

 Requeue the work if more domains were async put meanwhile. */

		/*

		 * Cancel the work that got queued after this one got dequeued,

		 * since here we released the corresponding async-put reference.

/**

 * intel_display_power_put_async - release a power domain reference asynchronously

 * @i915: i915 device instance

 * @domain: power domain to reference

 * @wakeref: wakeref acquired for the reference that is being released

 *

 * This function drops the power domain reference obtained by

 * intel_display_power_get*() and schedules a work to power down the

 * corresponding hardware block if this is the last reference.

 Let a pending work requeue itself or queue a new one. */

/**

 * intel_display_power_flush_work - flushes the async display power disabling work

 * @i915: i915 device instance

 *

 * Flushes any pending work that was scheduled by a preceding

 * intel_display_power_put_async() call, completing the disabling of the

 * corresponding power domains.

 *

 * Note that the work handler function may still be running after this

 * function returns; to ensure that the work handler isn't running use

 * intel_display_power_flush_work_sync() instead.

/**

 * intel_display_power_flush_work_sync - flushes and syncs the async display power disabling work

 * @i915: i915 device instance

 *

 * Like intel_display_power_flush_work(), but also ensure that the work

 * handler function is not running any more when this function returns.

/**

 * intel_display_power_put - release a power domain reference

 * @dev_priv: i915 device instance

 * @domain: power domain to reference

 * @wakeref: wakeref acquired for the reference that is being released

 *

 * This function drops the power domain reference obtained by

 * intel_display_power_get() and might power down the corresponding hardware

 * block right away if this is the last reference.

/**

 * intel_display_power_put_unchecked - release an unchecked power domain reference

 * @dev_priv: i915 device instance

 * @domain: power domain to reference

 *

 * This function drops the power domain reference obtained by

 * intel_display_power_get() and might power down the corresponding hardware

 * block right away if this is the last reference.

 *

 * This function is only for the power domain code's internal use to suppress wakeref

 * tracking when the correspondig debug kconfig option is disabled, should not

 * be used otherwise.

 DDI E */	\

 DDI E */	\

/*

 * ICL PW_0/PG_0 domains (HW/DMC control):

 * - PCI

 * - clocks except port PLL

 * - central power except FBC

 * - shared functions except pipe interrupts, pipe MBUS, DBUF registers

 * ICL PW_1/PG_1 domains (HW/DMC control):

 * - DBUF function

 * - PIPE_A and its planes, except VGA

 * - transcoder EDP + PSR

 * - transcoder DSI

 * - DDI_A

 * - FBC

 VDSC/joining */

	/*

	 * - transcoder WD

	 * - KVMR (HW control)

	/*

	 * - KVMR (HW control)

/*

 * There is no PW_2/PG_2 on RKL.

 *

 * RKL PW_1/PG_1 domains (under HW/DMC control):

 * - DBUF function (note: registers are in PW0)

 * - PIPE_A and its planes and VDSC/joining, except VGA

 * - transcoder A

 * - DDI_A and DDI_B

 * - FBC

 *

 * RKL PW_0/PG_0 domains (under HW/DMC control):

 * - PCI

 * - clocks except port PLL

 * - shared functions:

 *     * interrupts except pipe interrupts

 *     * MBus except PIPE_MBUS_DBOX_CTL

 *     * DBUF registers

 * - central power except FBC

 * - top-level GTC (DDI-level GTC is in the well associated with the DDI)

/*

 * DG1 onwards Audio MMIO/VERBS lies in PG0 power well.

/*

 * XE_LPD Power Domains

 *

 * Previous platforms required that PG(n-1) be enabled before PG(n).  That

 * dependency chain turns into a dependency tree on XE_LPD:

 *

 *       PG0

 *        |

 *     --PG1--

 *    /       \

 *  PGA     --PG2--

 *         /   |   \

 *       PGB  PGC  PGD

 *

 * Power wells must be enabled from top to bottom and disabled from bottom

 * to top.  This allows pipes to be power gated independently.

/*

 * XELPD PW_1/PG_1 domains (under HW/DMC control):

 *  - DBUF function (registers are in PW0)

 *  - Transcoder A

 *  - DDI_A and DDI_B

 *

 * XELPD PW_0/PW_1 domains (under HW/DMC control):

 *  - PCI

 *  - Clocks except port PLL

 *  - Shared functions:

 *     * interrupts except pipe interrupts

 *     * MBus except PIPE_MBUS_DBOX_CTL

 *     * DBUF registers

 *  - Central power except FBC

 *  - Top-level GTC (DDI-level GTC is in the well associated with the DDI)

		/*

		 * Pipe A power well is the new disp2d well. Pipe B and C

		 * power wells don't actually exist. Pipe A power well is

		 * required for any pipe to work.

 Handled by the DMC firmware */

 Handled by the DMC firmware */

 Handled by the DMC firmware */

 Handled by the DMC firmware */

 Handled by the DMC firmware */

		/*

		 * Spec states that we should timeout the request after 200us

		 * but the function below will timeout after 500us

	/*

	 * Not the correctly implementation but there is no way to just read it

	 * from PCODE, so returning count to avoid state mismatch errors

 Handled by the DMC firmware */

 Handled by the DMC firmware */

 Handled by the DMC firmware */

 Handled by the DMC firmware */

	/*

	 * DC9 has a separate HW flow from the rest of the DC states,

	 * not depending on the DMC firmware. It's needed by system

	 * suspend/resume, so allow it unconditionally.

/**

 * intel_power_domains_init - initializes the power domain structures

 * @dev_priv: i915 device instance

 *

 * Initializes the power domain structures for @dev_priv depending upon the

 * supported platform.

	/*

	 * The enabling order will be from lower to higher indexed wells,

	 * the disabling order is reversed.

/**

 * intel_power_domains_cleanup - clean up power domains resources

 * @dev_priv: i915 device instance

 *

 * Release any resources acquired by intel_power_domains_init()

	/*

	 * Might be running this in parallel to gen9_dc_off_power_well_enable

	 * being called from intel_dp_detect for instance,

	 * which causes assertion triggered by race condition,

	 * as gen9_assert_dbuf_enabled might preempt this when registers

	 * were already updated, while dev_priv was not.

	/*

	 * Just power up at least 1 slice, we will

	 * figure out later which slices we have and what we need.

	/*

	 * gen12 platforms that use abox1 and abox2 for pixel data reads still

	 * expect us to program the abox_ctl0 register as well, even though

	 * we don't have to program other instance-0 registers like BW_BUDDY.

	/*

	 * The LCPLL register should be turned on by the BIOS. For now

	 * let's just check its state and print errors in case

	 * something is wrong.  Don't even try to turn it on.

	/*

	 * In theory we can still leave IRQs enabled, as long as only the HPD

	 * interrupts remain enabled. We used to check for that, but since it's

	 * gen-specific and since we only disable LCPLL after we fully disable

	 * the interrupts, the check below should be enough.

/*

 * This function implements pieces of two sequences from BSpec:

 * - Sequence for display software to disable LCPLL

 * - Sequence for display software to allow package C8+

 * The steps implemented here are just the steps that actually touch the LCPLL

 * register. Callers should take care of disabling all the display engine

 * functions, doing the mode unset, fixing interrupts, etc.

/*

 * Fully restores LCPLL, disallowing power down and switching back to LCPLL

 * source.

	/*

	 * Make sure we're not on PC8 state before disabling PC8, otherwise

	 * we'll hang the machine. To prevent PC8 state, just enable force_wake.

/*

 * Package states C8 and deeper are really deep PC states that can only be

 * reached when all the devices on the system allow it, so even if the graphics

 * device allows PC8+, it doesn't mean the system will actually get to these

 * states. Our driver only allows PC8+ when going into runtime PM.

 *

 * The requirements for PC8+ are that all the outputs are disabled, the power

 * well is disabled and most interrupts are disabled, and these are also

 * requirements for runtime PM. When these conditions are met, we manually do

 * the other conditions: disable the interrupts, clocks and switch LCPLL refclk

 * to Fclk. If we're in PC8+ and we get an non-hotplug interrupt, we can hard

 * hang the machine.

 *

 * When we really reach PC8 or deeper states (not just when we allow it) we lose

 * the state of some registers, so when we come back from PC8+ we need to

 * restore this state. We don't get into PC8+ if we're not in RC6, so we don't

 * need to take care of the registers kept by RC6. Notice that this happens even

 * if we don't put the device in PCI D3 state (which is what currently happens

 * because of the runtime PM support).

 *

 * For more, read "Display Sequences for Package C8" on the hardware

 * documentation.

 enable PCH reset handshake */

 enable PG1 and Misc I/O */

 The spec doesn't call for removing the reset handshake flag */

 disable PG1 and Misc I/O */

	/*

	 * BSpec says to keep the MISC IO power well enabled here, only

	 * remove our request for power well 1.

	 * Note that even though the driver's request is removed power well 1

	 * may stay enabled after this due to DMC's own request on it.

 10 us delay per Bspec */

	/*

	 * NDE_RSTWRN_OPT RST PCH Handshake En must always be 0b on BXT

	 * or else the reset will hang because there is no PCH to respond.

	 * Move the handshake programming to initialization sequence.

	 * Previously was left up to BIOS.

 Enable PG1 */

 The spec doesn't call for removing the reset handshake flag */

	/*

	 * Disable PW1 (PG1).

	 * Note that even though the driver's request is removed power well 1

	 * may stay enabled after this due to DMC's own request on it.

 10 us delay per Bspec */

 BW_BUDDY registers are not used on dgpu's beyond DG1 */

 Wa_1409767108:tgl,dg1,adl-s */

 Wa_22010178259:tgl,dg1,rkl,adl-s */

 Wa_14011294188:ehl,jsl,tgl,rkl,adl-s */

 1. Enable PCH reset handshake. */

 2. Initialize all combo phys */

	/*

	 * 3. Enable Power Well 1 (PG1).

	 *    The AUX IO power wells will be enabled on demand.

 4. Enable CDCLK. */

 5. Enable DBUF. */

 6. Setup MBUS. */

 7. Program arbiter BW_BUDDY registers */

 8. Ensure PHYs have completed calibration and adaptation */

 Wa_14011508470:tgl,dg1,rkl,adl-s,adl-p */

 Wa_14011503030:xelpd */

 1. Disable all display engine functions -> aready done */

 2. Disable DBUF */

 3. Disable CD clock */

	/*

	 * 4. Disable Power Well 1 (PG1).

	 *    The AUX IO power wells are toggled on demand, so they are already

	 *    disabled at this point.

 5. */

	/*

	 * DISPLAY_PHY_CONTROL can get corrupted if read. As a

	 * workaround never ever read DISPLAY_PHY_CONTROL, and

	 * instead maintain a shadow copy ourselves. Use the actual

	 * power well state and lane status to reconstruct the

	 * expected initial value.

	/*

	 * If all lanes are disabled we leave the override disabled

	 * with all power down bits cleared to match the state we

	 * would use after disabling the port. Otherwise enable the

	 * override and set the lane powerdown bits accding to the

	 * current lane status.

 Defer application of initial phy_control to enabling the powerwell */

 If the display might be already active skip this */

 cmnlane needs DPLL registers */

	/*

	 * From VLV2A0_DP_eDP_HDMI_DPIO_driver_vbios_notes_11.docx:

	 * Need to assert and de-assert PHY SB reset by gating the

	 * common lane power, then un-gating it.

	 * Simply ungating isn't enough to reset the PHY enough to get

	 * ports and lanes running.

/**

 * intel_power_domains_init_hw - initialize hardware power domain state

 * @i915: i915 device instance

 * @resume: Called from resume code paths or not

 *

 * This function initializes the hardware power domain state and enables all

 * power wells belonging to the INIT power domain. Power wells in other

 * domains (and not in the INIT domain) are referenced or disabled by

 * intel_modeset_readout_hw_state(). After that the reference count of each

 * power well must match its HW enabled state, see

 * intel_power_domains_verify_state().

 *

 * It will return with power domains disabled (to be enabled later by

 * intel_power_domains_enable()) and must be paired with

 * intel_power_domains_driver_remove().

	/*

	 * Keep all power wells enabled for any dependent HW access during

	 * initialization and to make sure we keep BIOS enabled display HW

	 * resources powered until display HW readout is complete. We drop

	 * this reference in intel_power_domains_enable().

 Disable power support if the user asked so. */

/**

 * intel_power_domains_driver_remove - deinitialize hw power domain state

 * @i915: i915 device instance

 *

 * De-initializes the display power domain HW state. It also ensures that the

 * device stays powered up so that the driver can be reloaded.

 *

 * It must be called with power domains already disabled (after a call to

 * intel_power_domains_disable()) and must be paired with

 * intel_power_domains_init_hw().

 Remove the refcount we took to keep power well support disabled. */

 Keep the power well enabled, but cancel its rpm wakeref. */

/**

 * intel_power_domains_enable - enable toggling of display power wells

 * @i915: i915 device instance

 *

 * Enable the ondemand enabling/disabling of the display power wells. Note that

 * power wells not belonging to POWER_DOMAIN_INIT are allowed to be toggled

 * only at specific points of the display modeset sequence, thus they are not

 * affected by the intel_power_domains_enable()/disable() calls. The purpose

 * of these function is to keep the rest of power wells enabled until the end

 * of display HW readout (which will acquire the power references reflecting

 * the current HW state).

/**

 * intel_power_domains_disable - disable toggling of display power wells

 * @i915: i915 device instance

 *

 * Disable the ondemand enabling/disabling of the display power wells. See

 * intel_power_domains_enable() for which power wells this call controls.

/**

 * intel_power_domains_suspend - suspend power domain state

 * @i915: i915 device instance

 * @suspend_mode: specifies the target suspend state (idle, mem, hibernation)

 *

 * This function prepares the hardware power domain state before entering

 * system suspend.

 *

 * It must be called with power domains already disabled (after a call to

 * intel_power_domains_disable()) and paired with intel_power_domains_resume().

	/*

	 * In case of suspend-to-idle (aka S0ix) on a DMC platform without DC9

	 * support don't manually deinit the power domains. This also means the

	 * DMC firmware will stay active, it will power down any HW

	 * resources as required and also enable deeper system power states

	 * that would be blocked if the firmware was inactive.

	/*

	 * Even if power well support was disabled we still want to disable

	 * power wells if power domains must be deinitialized for suspend.

/**

 * intel_power_domains_resume - resume power domain state

 * @i915: i915 device instance

 *

 * This function resume the hardware power domain state during system resume.

 *

 * It will return with power domain support disabled (to be enabled later by

 * intel_power_domains_enable()) and must be paired with

 * intel_power_domains_suspend().

/**

 * intel_power_domains_verify_state - verify the HW/SW state for all power wells

 * @i915: i915 device instance

 *

 * Verify if the reference count of each power well matches its HW enabled

 * state and the total refcount of the domains it belongs to. This must be

 * called after modeset HW state sanitization, which is responsible for

 * acquiring reference counts for any power wells in use and disabling the

 * ones left on by BIOS but not required by any active output.

 Tweaked Wa_14010685332:cnp,icp,jsp,mcc,tgp,adp */

 Tweaked Wa_14010685332:cnp,icp,jsp,mcc,tgp,adp */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

/*

 * The hardware phase 0.0 refers to the center of the pixel.

 * We want to start from the top/left edge which is phase

 * -0.5. That matches how the hardware calculates the scaling

 * factors (from top-left of the first pixel to bottom-right

 * of the last pixel, as opposed to the pixel centers).

 *

 * For 4:2:0 subsampled chroma planes we obviously have to

 * adjust that so that the chroma sample position lands in

 * the right spot.

 *

 * Note that for packed YCbCr 4:2:2 formats there is no way to

 * control chroma siting. The hardware simply replicates the

 * chroma samples for both of the luma samples, and thus we don't

 * actually get the expected MPEG2 chroma siting convention :(

 * The same behaviour is observed on pre-SKL platforms as well.

 *

 * Theory behind the formula (note that we ignore sub-pixel

 * source coordinates):

 * s = source sample position

 * d = destination sample position

 *

 * Downscaling 4:1:

 * -0.5

 * | 0.0

 * | |     1.5 (initial phase)

 * | |     |

 * v v     v

 * | s | s | s | s |

 * |       d       |

 *

 * Upscaling 1:4:

 * -0.5

 * | -0.375 (initial phase)

 * | |     0.0

 * | |     |

 * v v     v

 * |       s       |

 * | d | d | d | d |

	/*

	 * Hardware initial phase limited to [-0.5:1.5].

	 * Since the max hardware scale factor is 3.0, we

	 * should never actually excdeed 1.0 here.

	/*

	 * Src coordinates are already rotated by 270 degrees for

	 * the 90/270 degree plane rotation cases (to match the

	 * GTT mapping), hence no need to account for rotation here.

	/*

	 * Scaling/fitting not supported in IF-ID mode in GEN9+

	 * TODO: Interlace fetch mode doesn't support YUV420 planar formats.

	 * Once NV12 is enabled, handle it here while allocating scaler

	 * for NV12.

	/*

	 * if plane is being disabled or scaler is no more required or force detach

	 *  - free scaler binded to this plane/crtc

	 *  - in order to do this, update crtc->scaler_usage

	 *

	 * Here scaler state in crtc_state is set free so that

	 * scaler can be assigned to other user. Actual register

	 * update to free the scaler is done in plane/panel-fit programming.

	 * For this purpose crtc/plane_state->scaler_id isn't reset here.

 range checks */

 mark this plane as a scaler user in crtc_state */

/**

 * skl_update_scaler_plane - Stages update to scaler state for a given plane.

 * @crtc_state: crtc's scaler state

 * @plane_state: atomic plane state to update

 *

 * Return

 *     0 - scaler_usage updated successfully

 *    error - requested scaling cannot be supported or other error condition

 Pre-gen11 and SDR planes always need a scaler for planar formats. */

 check colorkey */

 Check src format */

/*

 *  Theory behind setting nearest-neighbor integer scaling:

 *

 *  17 phase of 7 taps requires 119 coefficients in 60 dwords per set.

 *  The letter represents the filter tap (D is the center tap) and the number

 *  represents the coefficient set for a phase (0-16).

 *

 *         +------------+------------------------+------------------------+

 *         |Index value | Data value coeffient 1 | Data value coeffient 2 |

 *         +------------+------------------------+------------------------+

 *         |   00h      |          B0            |          A0            |

 *         +------------+------------------------+------------------------+

 *         |   01h      |          D0            |          C0            |

 *         +------------+------------------------+------------------------+

 *         |   02h      |          F0            |          E0            |

 *         +------------+------------------------+------------------------+

 *         |   03h      |          A1            |          G0            |

 *         +------------+------------------------+------------------------+

 *         |   04h      |          C1            |          B1            |

 *         +------------+------------------------+------------------------+

 *         |   ...      |          ...           |          ...           |

 *         +------------+------------------------+------------------------+

 *         |   38h      |          B16           |          A16           |

 *         +------------+------------------------+------------------------+

 *         |   39h      |          D16           |          C16           |

 *         +------------+------------------------+------------------------+

 *         |   3Ah      |          F16           |          C16           |

 *         +------------+------------------------+------------------------+

 *         |   3Bh      |        Reserved        |          G16           |

 *         +------------+------------------------+------------------------+

 *

 *  To enable nearest-neighbor scaling:  program scaler coefficents with

 *  the center tap (Dxx) values set to 1 and all other values set to 0 as per

 *  SCALER_COEFFICIENT_FORMAT

 *

 TODO: handle sub-pixel coordinates */

 MPEG2 chroma siting convention */

 not used */

/*

 * This function detaches (aka. unbinds) unused scalers in hardware

 loop through and disable scalers that aren't in use */

/*

 *

 * Copyright (c) 2012 Gilles Dartiguelongue, Thomas Richter

 *

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the

 * "Software"), to deal in the Software without restriction, including

 * without limitation the rights to use, copy, modify, merge, publish,

 * distribute, sub license, and/or sell copies of the Software, and to

 * permit persons to whom the Software is furnished to do so, subject to

 * the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.

 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR

 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,

 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE

 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

/*

 * The following registers are not part of the official datasheet

 * and are the result of reverse engineering.

/*

 * Register c0 controls how the DVO synchronizes with

 * its input.

 enable the DVO sync in general */

 synchronize horizontal with input */

 synchronize vertical with input */

 reset the synchronization flip/flops */

/*

 * Register 41 is somehow related to the sync register and sync

 * configuration. It should be 0x32 whenever regC0 is 0x05 (hsync off)

 * and 0x00 otherwise.

/*

 * this register controls the dithering of the DVO

 * One bit enables it, the other define the dithering depth.

 * The higher the value, the lower the dithering depth.

 if set, dithering is enabled */

 controls the dither depth */

 shifts the dither mask */

/*

 * PLL configuration register. This is a pair of registers,

 * one single byte register at 1B, and a pair at 1C,1D.

 * These registers are counters/dividers.

 one byte PLL control register */

 low-part of the second register */

 high-part of the second register */

/*

 * Scaler control registers. Horizontal at b8,b9,

 * vertical at 10,11. The scale factor is computed as

 * 2^16/control-value. The low-byte comes first.

 low-byte vertical scaler */

 high-byte vertical scaler */

 low-byte horizontal scaler */

 high-byte horizontal scaler */

/*

 * Display window definition. This consists of four registers

 * per dimension. One register pair defines the start of the

 * display, one the end.

 * As far as I understand, this defines the window within which

 * the scaler samples the input.

 low-byte horizontal display start */

 high-byte horizontal display start */

 low-byte horizontal display stop */

 high-byte horizontal display stop */

 low-byte vertical display start */

 high-byte vertical display start */

 low-byte vertical display stop */

 high-byte vertical display stop */

/*

 * The following register pair seems to define the start of

 * the vertical sync. If automatic syncing is enabled, and the

 * register value defines a sync pulse that is later than the

 * incoming sync, then the register value is ignored and the

 * external hsync triggers the synchronization.

 low-byte vsync-start */

 high-byte vsync-start */

/*

 * The following register pair seems to define the total number

 * of lines created at the output side of the scaler.

 * This is again a low-high register pair.

 output display height, low byte */

 output display height, high byte */

/*

 * The following registers define the end of the front-porch

 * in horizontal and vertical position and hence allow to shift

 * the image left/right or up/down.

 horizontal start of display + 256, low */

 horizontal start of display + 256, high */

 vertical start of the display, low byte */

 vertical start of the display, high byte */

/*

 * The following register pair control the function of the

 * backlight and the DVO output. To enable the corresponding

 * function, the corresponding bit must be set in both registers.

 DVO enable functions, first register */

 DVO enable functions, second register */

 enable DVO output */

 enable backlight */

/*

 * Registers 9C and 9D define the vertical output offset

 * of the visible region.

/*

 * The register 9F defines the dithering. This requires the

 * scaler to be ON. Bit 0 enables dithering, the remaining

 * bits control the depth of the dither. The higher the value,

 * the LOWER the dithering amplitude. A good value seems to be

 * 15 (total register value).

 enable dithering */

 dither masking */

 upshift of the dither mask */

/*

 * The following structure keeps the complete configuration of

 * the DVO, given a specific output configuration.

 * This is pretty much guess-work from reverse-engineering, so

 * read all this with a grain of salt.

 configuration of the C0 register */

 configuration register 8 */

 configuration register 41 */

 configuration of the dithering */

 PLL configuration, register A, 1B */

 PLL configuration, register B, 1C/1D */

 horizontal start, registers C1/C2 */

 horizontal total, registers C3/C4 */

 vertical start, registers C5/C6 */

 vertical total, registers C7/C8 */

 manual vertical sync start, 80/81 */

 number of lines generated, 82/83 */

 horizontal position + 256, 98/99  */

 vertical position, 8e/8f */

 vertical output offset, 9c/9d */

 horizontal scaling factor, b8/b9 */

 vertical scaling factor, 10/11 */

/*

 * DVO configuration values, partially based on what the BIOS

 * of the Fujitsu Lifebook S6010 writes into registers,

 * partially found by manual tweaking. These configurations assume

 * a 1024x768 panel.

 actually, ignored with this config */

/*

 * Other configuration values left by the BIOS of the

 * Fujitsu S6010 in the DVO control registers. Their

 * value does not depend on the BIOS and their meaning

 * is unknown.

 08 is mode specific */

 10,11 are part of the mode specific configuration */

 PLL?, ignored */

 1b,1c,1d are part of the mode specific configuration */

 80-84 are part of the mode-specific configuration */

 8e,8f are part of the mode-specific configuration */

 98,99 are part of the mode-specific configuration */

 9c,9d are part of the mode-specific configuration */

 register 0xa4 is mode specific, but 0x80..0x84 works always */

 0xa9 to 0xab are mode specific, but have no visible effect */

 b8,b9 are part of the mode-specific configuration */

 f8 is mode specific, but the value does not matter */

/*

** Read a register from the ns2501.

** Returns true if successful, false otherwise.

** If it returns false, it might be wise to enable the

** DVO with the above function.

/*

** Write a register to the ns2501.

** Returns true if successful, false otherwise.

** If it returns false, it might be wise to enable the

** DVO with the above function.

/* National Semiconductor 2501 driver for chip on i2c bus

 * scan for the chip on the bus.

 * Hope the VBIOS initialized the PLL correctly so we can

 * talk to it. If not, it will not be seen and not detected.

 * Bummer!

 this will detect the NS2501 chip on the specified i2c bus */

	/*

	 * This is a Laptop display, it doesn't have hotplugging.

	 * Even if not, the detection bit of the 2501 is unreliable as

	 * it only works for some display types.

	 * It is even more unreliable as the PLL must be active for

	 * allowing reading from the chiop.

	/*

	 * Currently, these are all the modes I have data from.

	 * More might exist. Unclear how to find the native resolution

	 * of the panel in here so we could always accept it

	 * by disabling the scaler.

 Is this a reasonable error? */

 Hopefully doing it every time won't hurt... */

 Write the mode-agnostic values */

 Write now the mode-specific configuration */

 set the NS2501 power state */

 set the NS2501 power state */

/*

 * Copyright Â© 2006 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *    Eric Anholt <eric@anholt.net>

 *    Thomas Richter <thor@math.tu-berlin.de>

 *

 * Minor modifications (Dithering enable):

 *    Thomas Richter <thor@math.tu-berlin.de>

 *

/*

 * register definitions for the i82807aa.

 *

 * Documentation on this chipset can be found in datasheet #29069001 at

 * intel.com.

/*

 * VCH Revision & GMBus Base Addr

/*

 * Functionality Enable

/*

 * Enable the panel fitter

/*

 * Enables the LCD display.

 *

 * This must not be set while VR01_DVO_BYPASS_ENABLE is set.

 Enables the DVO repeater. */

 Enables the DVO clock */

 Enable dithering for 18bpp panels. Not documented. */

/*

 * LCD Interface Format

 Enables LVDS output instead of CMOS */

 Enables 18-bit LVDS output. */

 Enables 24-bit LVDS or CMOS output */

 Enables 2x18-bit LVDS or CMOS output. */

 Enables 2x24-bit LVDS output */

 Mask that defines the depth of the pipeline */

/*

 * VR20 LCD Horizontal Display Size

/*

 * LCD Vertical Display Size

/*

 * Panel power down status

 Read only bit indicating that the panel is not in a safe poweroff state. */

/*

 * Panel Fitting Vertical Ratio

 * (((image_height - 1) << 16) / ((panel_height - 1))) >> 2

/*

 * Panel Fitting Horizontal Ratio

 * (((image_width - 1) << 16) / ((panel_width - 1))) >> 2

/*

 * Horizontal Image Size

/* VR80 GPIO 0

/* VR88 GPIO 8

/* Graphics BIOS scratch 0

/* Graphics BIOS scratch 1

/* Some Bios implementations do not restore the DVO state upon

 * resume from standby. Thus, this driver has to handle it

 * instead. The following list contains all registers that

 * require saving.

 this must come last */

 Register backup */

/*

 * Reads a register on the ivch.

 *

 * Each of the 256 registers are 16 bits long.

 Writes a 16-bit register on the ivch */

 Probes the given bus and slave address for an ivch */

	/* Since the identification bits are probably zeroes, which doesn't seem

	 * very unique, check that the value in the base address field matches

	 * the address it's responding on.

	/* Make a backup of the registers to be able to restore them

	 * upon suspend.

/* Restore the DVO registers after a resume

 * from RAM. Registers have been saved during

 * the initialization.

 Sets the power state of the panel connected to the ivch */

 Set the new power state of the panel. */

 Wait for the panel to make its state transition */

 wait some more; vch may fail to resync sometimes without this */

 Set the new power state of the panel. */

 Enable dithering for 18 bpp pipelines */

 GPIO registers */

 Scratch register 0 - AIM Panel type */

 Scratch register 1 - Status register */

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Daniel Vetter <daniel.vetter@ffwll.ch>

/**

 * DOC: frontbuffer tracking

 *

 * Many features require us to track changes to the currently active

 * frontbuffer, especially rendering targeted at the frontbuffer.

 *

 * To be able to do so we track frontbuffers using a bitmask for all possible

 * frontbuffer slots through intel_frontbuffer_track(). The functions in this

 * file are then called when the contents of the frontbuffer are invalidated,

 * when frontbuffer rendering has stopped again to flush out all the changes

 * and when the frontbuffer is exchanged with a flip. Subsystems interested in

 * frontbuffer changes (e.g. PSR, FBC, DRRS) should directly put their callbacks

 * into the relevant places and filter for the frontbuffer slots that they are

 * interested int.

 *

 * On a high level there are two types of powersaving features. The first one

 * work like a special cache (FBC and PSR) and are interested when they should

 * stop caching and when to restart caching. This is done by placing callbacks

 * into the invalidate and the flush functions: At invalidate the caching must

 * be stopped and at flush time it can be restarted. And maybe they need to know

 * when the frontbuffer changes (e.g. when the hw doesn't initiate an invalidate

 * and flush on its own) which can be achieved with placing callbacks into the

 * flip functions.

 *

 * The other type of display power saving feature only cares about busyness

 * (e.g. DRRS). In that case all three (invalidate, flush and flip) indicate

 * busyness. There is no direct way to detect idleness. Instead an idle timer

 * work delayed work should be started from the flush and flip functions and

 * cancelled as soon as busyness is detected.

/**

 * frontbuffer_flush - flush frontbuffer

 * @i915: i915 device

 * @frontbuffer_bits: frontbuffer plane tracking bits

 * @origin: which operation caused the flush

 *

 * This function gets called every time rendering on the given planes has

 * completed and frontbuffer caching can be started again. Flushes will get

 * delayed if they're blocked by some outstanding asynchronous rendering.

 *

 * Can be called without any locks held.

 Delay flushing when rings are still busy.*/

/**

 * intel_frontbuffer_flip_prepare - prepare asynchronous frontbuffer flip

 * @i915: i915 device

 * @frontbuffer_bits: frontbuffer plane tracking bits

 *

 * This function gets called after scheduling a flip on @obj. The actual

 * frontbuffer flushing will be delayed until completion is signalled with

 * intel_frontbuffer_flip_complete. If an invalidate happens in between this

 * flush will be cancelled.

 *

 * Can be called without any locks held.

 Remove stale busy bits due to the old buffer. */

/**

 * intel_frontbuffer_flip_complete - complete asynchronous frontbuffer flip

 * @i915: i915 device

 * @frontbuffer_bits: frontbuffer plane tracking bits

 *

 * This function gets called after the flip has been latched and will complete

 * on the next vblank. It will execute the flush if it hasn't been cancelled yet.

 *

 * Can be called without any locks held.

 Mask any cancelled flips. */

/**

 * intel_frontbuffer_flip - synchronous frontbuffer flip

 * @i915: i915 device

 * @frontbuffer_bits: frontbuffer plane tracking bits

 *

 * This function gets called after scheduling a flip on @obj. This is for

 * synchronous plane updates which will happen on the next vblank and which will

 * not get delayed by pending gpu rendering.

 *

 * Can be called without any locks held.

 Remove stale busy bits due to the old buffer. */

 Filter out new bits since rendering started. */

/**

 * intel_frontbuffer_track - update frontbuffer tracking

 * @old: current buffer for the frontbuffer slots

 * @new: new buffer for the frontbuffer slots

 * @frontbuffer_bits: bitmask of frontbuffer slots

 *

 * This updates the frontbuffer tracking bits @frontbuffer_bits by clearing them

 * from @old and setting them in @new. Both @old and @new can be NULL.

	/*

	 * Control of individual bits within the mask are guarded by

	 * the owning plane->mutex, i.e. we can never see concurrent

	 * manipulation of individual bits. But since the bitfield as a whole

	 * is updated using RMW, we need to use atomics in order to update

	 * the bits.

/*

 * Copyright Â© 2015 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/**

 * DOC: atomic modeset support

 *

 * The functions here implement the state management and hardware programming

 * dispatch required by the atomic modeset infrastructure.

 * See intel_atomic_plane.c for the plane-specific atomic functionality.

/**

 * intel_digital_connector_atomic_get_property - hook for connector->atomic_get_property.

 * @connector: Connector to get the property for.

 * @state: Connector state to retrieve the property from.

 * @property: Property to retrieve.

 * @val: Return value for the property.

 *

 * Returns the atomic property value for a digital connector.

/**

 * intel_digital_connector_atomic_set_property - hook for connector->atomic_set_property.

 * @connector: Connector to set the property for.

 * @state: Connector state to set the property on.

 * @property: Property to set.

 * @val: New value for the property.

 *

 * Sets the atomic property value for a digital connector.

	/*

	 * These properties are handled by fastset, and might not end

	 * up in a modeset.

/**

 * intel_digital_connector_duplicate_state - duplicate connector state

 * @connector: digital connector

 *

 * Allocates and returns a copy of the connector state (both common and

 * digital connector specific) for the specified connector.

 *

 * Returns: The newly allocated connector state, or NULL on failure.

/**

 * intel_connector_needs_modeset - check if connector needs a modeset

 * @state: the atomic state corresponding to this modeset

 * @connector: the connector

/**

 * intel_any_crtc_needs_modeset - check if any CRTC needs a modeset

 * @state: the atomic state corresponding to this modeset

 *

 * Returns true if any CRTC in @state needs a modeset.

/**

 * intel_crtc_duplicate_state - duplicate crtc state

 * @crtc: drm crtc

 *

 * Allocates and returns a copy of the crtc state (both common and

 * Intel-specific) for the specified crtc.

 *

 * Returns: The newly allocated crtc state, or NULL on failure.

 copy color blobs */

/**

 * intel_crtc_destroy_state - destroy crtc state

 * @crtc: drm crtc

 * @state: the state to destroy

 *

 * Destroys the crtc state (both common and Intel-specific) for the

 * specified crtc.

 find a free scaler */

 set scaler mode */

			/*

			 * On gen11+'s HDR planes we only use the scaler for

			 * scaling. They have a dedicated chroma upsampler, so

			 * we don't need the scaler to upsample the UV plane.

		/*

		 * when only 1 scaler is in use on a pipe with 2 scalers

		 * scaler 0 operates in high quality (HQ) mode.

		 * In this case use scaler 0 to take advantage of HQ mode

/**

 * intel_atomic_setup_scalers() - setup scalers for crtc per staged requests

 * @dev_priv: i915 device

 * @intel_crtc: intel crtc

 * @crtc_state: incoming crtc_state to validate and setup scalers

 *

 * This function sets up scalers based on staged scaling requests for

 * a @crtc and its planes. It is called from crtc level check path. If request

 * is a supportable request, it attaches scalers to requested planes and crtc.

 *

 * This function takes into account the current scaler(s) in use by any planes

 * not being part of this atomic state

 *

 *  Returns:

 *         0 - scalers were setup succesfully

 *         error code - otherwise

	/*

	 * High level flow:

	 * - staged scaler requests are already in scaler_state->scaler_users

	 * - check whether staged scaling requests can be supported

	 * - add planes using scalers that aren't in current transaction

	 * - assign scalers to requested users

	 * - as part of plane commit, scalers will be committed

	 *   (i.e., either attached or detached) to respective planes in hw

	 * - as part of crtc_commit, scaler will be either attached or detached

	 *   to crtc in hw

 fail if required scalers > available scalers */

 walkthrough scaler_users bits and start assigning scalers */

 skip if scaler not required */

 panel fitter case: assign as a crtc scaler */

 plane scaler case: assign as a plane scaler */

 find the plane that set the bit as scaler_user */

			/*

			 * to enable/disable hq mode, add planes that are using scaler

			 * into this transaction

				/*

				 * GLK+ scalers don't have a HQ mode so it

				 * isn't necessary to change between HQ and dyn mode

				 * on those platforms.

 plane on different crtc cannot be a scaler user of this crtc */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 Pineview's Ncounter is a ring counter */

 Pineview only has one combined m divider, which we treat as m2. */

/* Ironlake / Sandybridge

 *

 * We calculate clock using (register_value + 2) for N/M1/M2, so here

 * the range value for them is (actual_value - 2).

 LVDS 100mhz refclk limits. */

	 /*

	  * These are the data rate limits (measured in fast clocks)

	  * since those are the strictest limits we have. The fast

	  * clock and actual rate limits are more relaxed, so checking

	  * them would make no difference.

 slow=min, fast=max */

	/*

	 * These are the data rate limits (measured in fast clocks)

	 * since those are the strictest limits we have.  The fast

	 * clock and actual rate limits are more relaxed, so checking

	 * them would make no difference.

 FIXME: find real dot limits */

 FIXME: find real m2 limits */

/*

 * Platform specific helpers to calculate the port PLL loopback- (clock.m),

 * and post-divider (clock.p) values, pre- (clock.vco) and post-divided fast

 * (clock.dot) clock rates. This fast dot clock is fed to the port's IO logic.

 * The helpers' return value is the rate of the clock that is fed to the

 * display engine's pipe which can be the above fast dot clock rate or a

 * divided-down version of it.

 m1 is reserved as 0 in Pineview, n is a ring counter */

/*

 * Returns whether the given set of divisors are valid for a given refclk with

 * the given connectors.

	/* XXX: We may need to be checking "Dot clock" depending on the multiplier,

	 * connector, etc., rather than just a single range.

		/*

		 * For LVDS just rely on its current settings for dual-channel.

		 * We haven't figured out how to reliably set up different

		 * single/dual channel state, if we even can.

/*

 * Returns a set of divisors for the desired target clock with the given

 * refclk, or FALSE.  The returned values represent the clock equation:

 * reflck * (5 * (m1 + 2) + (m2 + 2)) / (n + 2) / p1 / p2.

 *

 * Target and reference clocks are specified in kHz.

 *

 * If match_clock is provided, then best_clock P divider must match the P

 * divider from @match_clock used for LVDS downclocking.

/*

 * Returns a set of divisors for the desired target clock with the given

 * refclk, or FALSE.  The returned values represent the clock equation:

 * reflck * (5 * (m1 + 2) + (m2 + 2)) / (n + 2) / p1 / p2.

 *

 * Target and reference clocks are specified in kHz.

 *

 * If match_clock is provided, then best_clock P divider must match the P

 * divider from @match_clock used for LVDS downclocking.

/*

 * Returns a set of divisors for the desired target clock with the given

 * refclk, or FALSE.  The returned values represent the clock equation:

 * reflck * (5 * (m1 + 2) + (m2 + 2)) / (n + 2) / p1 / p2.

 *

 * Target and reference clocks are specified in kHz.

 *

 * If match_clock is provided, then best_clock P divider must match the P

 * divider from @match_clock used for LVDS downclocking.

 approximately equals target * 0.00585 */

 based on hardware requirement, prefer smaller n to precision */

 based on hardware requirement, prefere larger m1,m2 */

/*

 * Check if the calculated PLL configuration is more optimal compared to the

 * best configuration and error found so far. Return the calculated error.

	/*

	 * For CHV ignore the error and consider only the P value.

	 * Prefer a bigger P value based on HW requirements.

	/*

	 * Prefer a better P value over a better (smaller) error if the error

	 * is small. Ensure this preference for future configurations too by

	 * setting the error to 0.

/*

 * Returns a set of divisors for the desired target clock with the given

 * refclk, or FALSE.  The returned values represent the clock equation:

 * reflck * (5 * (m1 + 2) + (m2 + 2)) / (n + 2) / p1 / p2.

 min update 19.2 MHz */

 fast clock */

 based on hardware requirement, prefer smaller n to precision */

 based on hardware requirement, prefer bigger m1,m2 values */

/*

 * Returns a set of divisors for the desired target clock with the given

 * refclk, or FALSE.  The returned values represent the clock equation:

 * reflck * (5 * (m1 + 2) + (m2 + 2)) / (n + 2) / p1 / p2.

	/*

	 * Based on hardware doc, the n always set to 1, and m1 always

	 * set to 2.  If requires to support 200Mhz refclk, we need to

	 * revisit this because n may not 1 anymore.

 fast clock */

 compute bitmask from p1 value */

	/*

	 * Bspec:

	 * "[Almador Errata}: For the correct operation of the muxed DVO pins

	 *  (GDEVSELB/I2Cdata, GIRDBY/I2CClk) and (GFRAMEB/DVI_Data,

	 *  GTRDYB/DVI_Clk): Bit 31 (DPLL VCO Enable) and Bit 30 (2X Clock

	 *  Enable) must be set to â1â in both the DPLL A Control Register

	 *  (06014h-06017h) and DPLL B Control Register (06018h-0601Bh)."

	 *

	 * For simplicity We simply keep both bits always enabled in

	 * both DPLLS. The spec says we should disable the DVO 2X clock

	 * when not needed, but this seems to work fine in practice.

 Enable autotuning of the PLL clock (if permissible) */

	/*

	 * The high speed IO clock is only really required for

	 * SDVO/HDMI/DP, but we also enable it for CRT to make it

	 * possible to share the DPLL between CRT and HDMI. Enabling

	 * the clock needlessly does no real harm, except use up a

	 * bit of power potentially.

	 *

	 * We'll limit this to IVB with 3 pipes, since it has only two

	 * DPLLs and so DPLL sharing is the only way to get three pipes

	 * driving PCH ports at the same time. On SNB we could do this,

	 * and potentially avoid enabling the second DPLL, but it's not

	 * clear if it''s a win or loss power wise. No point in doing

	 * this on ILK at all since it has a fixed DPLL<->pipe mapping.

 compute bitmask from p1 value */

 also FPA1 */

 CPU eDP is the only output that doesn't need a PCH PLL of its own. */

 DPLL not used with DSI, but still need the rest set up */

 DPLL not used with DSI, but still need the rest set up */

 The option is for other outputs */

 PLL is protected by panel, make sure we can write it */

	/*

	 * Apparently we need to have VGA mode enabled prior to changing

	 * the P1/P2 dividers. Otherwise the DPLL will keep using the old

	 * dividers, even though the register value does change.

 Wait for the clocks to stabilize. */

		/* The pixel multiplier can only be updated once the

		 * DPLL is enabled and the clocks are stable.

		 *

		 * So write it again.

 We do this three times for luck */

 wait for warmup */

	/*

	 * PLLB opamp always calibrates to max value of 0x3f, force enable it

	 * and set it to a reasonable value instead.

 See eDP HDMI DPIO driver vbios notes doc */

 PLL B needs special handling */

 Set up Tx target for periodic Rcomp update */

 Disable target IRef on PLL */

 Disable fast lock */

 Set idtafcrecal before PLL is enabled */

	/*

	 * Post divider depends on pixel clock rate, DAC vs digital (and LVDS,

	 * but we don't support that).

	 * Note: don't use the DAC post divider as it seems unstable.

 Set HBR and RBR LPF coefficients */

 Use SSC source */

 HDMI or VGA */

 Use bend source */

 PLL is protected by panel, make sure we can write it */

 Enable Refclk */

 p1 and p2 divider */

 Feedback post-divider - m2 */

 Feedback refclk divider - n and m1 */

 M2 fraction division */

 M2 fraction division enable */

 Program digital lock detect threshold */

 Loop filter */

 Not supported. Apply the same limits as in the max case */

 AFC Recal */

 Enable back the 10bit clock to display controller */

	/*

	 * Need to wait > 100ns between dclkp clock enable bit and PLL enable.

 Enable PLL */

 Check PLL is locked */

 PLL is protected by panel, make sure we can write it */

 Enable Refclk and SSC */

		/*

		 * WaPixelRepeatModeFixForC0:chv

		 *

		 * DPLLCMD is AWOL. Use chicken bits to propagate

		 * the value from DPLLBMD to either pipe B or C.

		/*

		 * DPLLB VGA mode also seems to cause problems.

		 * We should always have it disabled.

/**

 * vlv_force_pll_on - forcibly enable just the PLL

 * @dev_priv: i915 private structure

 * @pipe: pipe PLL to enable

 * @dpll: PLL configuration

 *

 * Enable the PLL for @pipe using the supplied @dpll config. To be used

 * in cases where we need the PLL enabled even when @pipe is not going to

 * be enabled.

 Make sure the pipe isn't still relying on us */

 Make sure the pipe isn't still relying on us */

 Disable 10bit clock to display controller */

 Don't disable pipe or pipe PLLs if needed */

 Make sure the pipe isn't still relying on us */

/**

 * vlv_force_pll_off - forcibly disable just the PLL

 * @dev_priv: i915 private structure

 * @pipe: pipe PLL to disable

 *

 * Disable the PLL for @pipe. To be used in cases where we need

 * the PLL enabled even when @pipe is not going to be enabled.

 Only for pre-ILK configs */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

	/*

	 * If the FB is too big, just don't use it since fbdev is not very

	 * important and we should probably use that space with FBC or other

	 * features.

	/*

	 * Mark it WT ahead of time to avoid changing the

	 * cache_level during fbdev initialization. The

	 * unbind there would get stuck waiting for rcu.

	/*

	 * TODO:

	 *   Disable planes if get_initial_plane_config() failed.

	 *   Make sure things work if the surface base is not page aligned.

	/*

	 * Failed to alloc the obj, check to see if we should share

	 * an fb with another CRTC instead

	/*

	 * We've failed to reconstruct the BIOS FB.  Current display state

	 * indicates that the primary plane is visible, but has a NULL FB,

	 * which will lead to problems later if we don't fix it up.  The

	 * simplest solution is to just disable the primary plane now and

	 * pretend the BIOS never had it enabled.

 We may only have the stub and not a full framebuffer */

	/*

	 * Note that reserving the BIOS fb up front prevents us

	 * from stuffing other stolen allocations like the ring

	 * on top.  This prevents some ugliness at boot time, and

	 * can even allow for smooth boot transitions if the BIOS

	 * fb is large enough for the active pipe configuration.

	/*

	 * If the fb is shared between multiple heads, we'll

	 * just get the first one.

/*

 * Copyright 2006 Dave Airlie <airlied@linux.ie>

 * Copyright Â© 2006-2009 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 *	Jesse Barnes <jesse.barnes@intel.com>

 clear DIP data offset */

 Write every possible data byte to force correct ECC calculation. */

 clear DIP data offset */

 clear DIP data offset */

 Write every possible data byte to force correct ECC calculation. */

 clear DIP data offset */

 clear DIP data offset */

	/* The DIP control register spec says that we need to update the AVI

 Write every possible data byte to force correct ECC calculation. */

 clear DIP data offset */

 clear DIP data offset */

 Write every possible data byte to force correct ECC calculation. */

 clear DIP data offset */

 Write every possible data byte to force correct ECC calculation. */

 Wa_14013475917 */

 map from hardware bits to dip idx */

/*

 * The data we write to the DIP data buffer registers is 1 byte bigger than the

 * HDMI infoframe size because of an ECC/reserved byte at position 3 (starting

 * at 0). It's also a byte used by DisplayPort so the same DIP registers can be

 * used for both technologies.

 *

 * DW0: Reserved/ECC/DP | HB2 | HB1 | HB0

 * DW1:       DB3       | DB2 | DB1 | DB0

 * DW2:       DB7       | DB6 | DB5 | DB4

 * DW3: ...

 *

 * (HB is Header Byte, DB is Data Byte)

 *

 * The hdmi pack() functions don't know about that hardware specific hole so we

 * trick them by giving an offset into the buffer and moving back the header

 * bytes by one.

 see comment above for the reason for this offset */

 Insert the 'hole' (see big comment above) at position 3 */

 Fill the 'hole' (see big comment above) at position 3 */

 see comment above for the reason for this offset */

 nonsense combination */

 TODO: handle pixel repetition for YCBCR420 outputs */

	/* If the registers were not initialized yet, they might be zeroes,

	 * which means we're selecting the AVI DIP and we're setting its

	 * frequency to once. This seems to really confuse the HW and make

	 * things stop working (the register spec says the AVI always needs to

	 * be sent every VSync). So here we avoid writing to the register more

	 * than we need and also explicitly select the AVI DIP and explicitly

	 * set its frequency to every VSync. Avoiding to write it twice seems to

	 * be enough to solve the problem, but being defensive shouldn't hurt us

/*

 * Determine if default_phase=1 can be indicated in the GCP infoframe.

 *

 * From HDMI specification 1.4a:

 * - The first pixel of each Video Data Period shall always have a pixel packing phase of 0

 * - The first pixel following each Video Data Period shall have a pixel packing phase of 0

 * - The PP bits shall be constant for all GCPs and will be equal to the last packing phase

 * - The first pixel following every transition of HSYNC or VSYNC shall have a pixel packing

 *   phase of 0

 4 pixels in 5 clocks */

 2 pixels in 3 clocks */

 1 pixel in 2 clocks */

 phase information not relevant for 8bpc */

 Indicate color indication for deep color mode */

 Enable default_phase whenever the display mode is suitably aligned */

 See the big comment in g4x_set_infoframes() */

 See the big comment in g4x_set_infoframes() */

 Set both together, unset both together: see the spec. */

 See the big comment in g4x_set_infoframes() */

 Bspec says >= 6us */

	/*

	 * WA: To fix incorrect positioning of the window of

	 * opportunity and enc_en signalling in KABYLAKE.

 Wait for Ri prime match */

	/*

	 * Available msg size should be equal to or lesser than the

	 * available buffer.

	/*

	 * Re-auth request and Link Integrity Failures are represented by

	 * same bit. i.e reauth_req.

 GLK DPLL can't generate 446-480 MHz */

 BXT/GLK DPLL can't generate 223-240 MHz */

 CHV DPLL can't generate 216-240 MHz */

	/*

	 * SNPS PHYs' MPLLB table-based programming can only handle a fixed

	 * set of link rates.

	 *

	 * FIXME: We will hopefully get an algorithmic way of programming

	 * the MPLLB for HDMI in the future.

	/*

	 * Need to adjust the port link by:

	 *  1.5x for 12bpc

	 *  1.25x for 10bpc

 check if we can do 8bpc */

 if we can't do 8bpc we may still be able to do 12bpc */

 if we can't do 8,12bpc we may still be able to do 10bpc */

	/*

	 * HDMI deep color affects the clocks, so it's only possible

	 * when not cloning with other encoder types.

 Display Wa_1405510057:icl,ehl */

 YCBCR420 TMDS rate requirement is half the pixel clock */

	/*

	 * pipe_bpp could already be below 8bpc due to

	 * FDI bandwidth constraints. We shouldn't bump it

	 * back up to 8bpc in that case.

	/*

	 * Our YCbCr output is always limited range.

	 * crtc_state->limited_color_range only applies to RGB,

	 * and it must never be set for YCbCr or we risk setting

	 * some conflicting bits in PIPECONF which will mess up

	 * the colors on the monitor.

 See CEA-861-E - 5.1 Default Encoding Parameters */

	/*

	 * Give a hand to buggy BIOSen which forget to turn

	 * the TMDS output buffers back on after a reboot.

	/*

	 * Type 1 DVI adaptors are not required to implement any

	 * registers, so we can't always detect their presence.

	 * Ideally we should be able to check the state of the

	 * CONFIG1 pin, but no such luck on our hardware.

	 *

	 * The only method left to us is to check the VBT to see

	 * if the port is a dual mode capable DP port. But let's

	 * only do that when we sucesfully read the EDID, to avoid

	 * confusing log messages about DP dual mode adaptors when

	 * there's nothing connected to the port.

		/* An overridden EDID imply that we want this port for testing.

		 * Make sure not to set limits for that port.

	/*

	 * Make sure the refs for power wells enabled during detect are

	 * dropped to avoid a new detect cycle triggered by HPD polling.

/*

 * intel_hdmi_handle_sink_scrambling: handle sink scrambling/clock ratio setup

 * @encoder: intel_encoder

 * @connector: drm_connector

 * @high_tmds_clock_ratio = bool to indicate if the function needs to set

 *  or reset the high tmds clock ratio for scrambling

 * @scrambling: bool to Indicate if the function needs to set or reset

 *  sink scrambling

 *

 * This function handles scrambling on HDMI 2.0 capable sinks.

 * If required clock rate is > 340 Mhz && scrambling is supported by sink

 * it enables scrambling. This should be called before enabling the HDMI

 * 2.0 port, as the sink can choose to disable the scrambling if it doesn't

 * detect a scrambled clock within 100 ms.

 *

 * Returns:

 * True on success, false on failure.

 Set TMDS bit clock ratio to 1/40 or 1/10, and enable/disable scrambling */

	/*

	 * Pin mapping for RKL depends on which PCH is present.  With TGP, the

	 * final two outputs use type-c pins, even though they're actually

	 * combo outputs.  With CMP, the traditional DDI A-D pins are used for

	 * all outputs.

	/*

	 * Pin mapping for GEN9 BC depends on which PCH is present.  With TGP,

	 * final two outputs use type-c pins, even though they're actually

	 * combo outputs.  With CMP, the traditional DDI A-D pins are used for

	 * all outputs.

	/*

	 * Pin mapping for ADL-S requires TC pins for all combo phy outputs

	 * except first combo output.

	/* For G4X desktop chip, PEG_BAND_GAP_DATA 3:0 must first be written

	 * 0xd.  Failure to do so will result in spurious interrupts being

	 * generated on the port when a cable is not attached.

/*

 * intel_hdmi_dsc_get_slice_height - get the dsc slice_height

 * @vactive: Vactive of a display mode

 *

 * @return: appropriate dsc slice height for a given mode.

	/*

	 * Slice Height determination : HDMI2.1 Section 7.7.5.2

	 * Select smallest slice height >=96, that results in a valid PPS and

	 * requires minimum padding lines required for final slice.

	 *

	 * Assumption : Vactive is even.

/*

 * intel_hdmi_dsc_get_num_slices - get no. of dsc slices based on dsc encoder

 * and dsc decoder capabilities

 *

 * @crtc_state: intel crtc_state

 * @src_max_slices: maximum slices supported by the DSC encoder

 * @src_max_slice_width: maximum slice width supported by DSC encoder

 * @hdmi_max_slices: maximum slices supported by sink DSC decoder

 * @hdmi_throughput: maximum clock per slice (MHz) supported by HDMI sink

 *

 * @return: num of dsc slices that can be supported by the dsc encoder

 * and decoder.

 Pixel rates in KPixels/sec */

/*

 * Rates at which the source and sink are required to process pixels in each

 * slice, can be two levels: either atleast 340000KHz or atleast 40000KHz.

 Spec limits the slice width to 2720 pixels */

 max clock freq. in khz per slice */

	/*

	 * Slice Width determination : HDMI2.1 Section 7.7.5.1

	 * kslice_adjust factor for 4:2:0, and 4:2:2 formats is 0.5, where as

	 * for 4:4:4 is 1.0. Multiplying these factors by 10 and later

	 * dividing adjusted clock value by 10.

	/*

	 * As per spec, the rate at which the source and the sink process

	 * the pixels per slice are at two levels: atleast 340Mhz or 400Mhz.

	 * This depends upon the pixel clock rate and output formats

	 * (kslice adjust).

	 * If pixel clock * kslice adjust >= 2720MHz slices can be processed

	 * at max 340MHz, otherwise they can be processed at max 400MHz.

	/*

	 * Taking into account the sink's capability for maximum

	 * clock per slice (in MHz) as read from HF-VSDB.

	/*

	 * Keep on increasing the num of slices/line, starting from min_slices

	 * per line till we get such a number, for which the slice_width is

	 * just less than max_slice_width. The slices/line selected should be

	 * less than or equal to the max horizontal slices that the combination

	 * of PCON encoder and HDMI decoder can support.

/*

 * intel_hdmi_dsc_get_bpp - get the appropriate compressed bits_per_pixel based on

 * source and sink capabilities.

 *

 * @src_fraction_bpp: fractional bpp supported by the source

 * @slice_width: dsc slice width supported by the source and sink

 * @num_slices: num of slices supported by the source and sink

 * @output_format: video output format

 * @hdmi_all_bpp: sink supports decoding of 1/16th bpp setting

 * @hdmi_max_chunk_bytes: max bytes in a line of chunks supported by sink

 *

 * @return: compressed bits_per_pixel in step of 1/16 of bits_per_pixel

	/*

	 * Get min bpp and max bpp as per Table 7.23, in HDMI2.1 spec

	 * Start with the max bpp and keep on decrementing with

	 * fractional bpp, if supported by PCON DSC encoder

	 *

	 * for each bpp we check if no of bytes can be supported by HDMI sink

 Assuming: bpc as 8*/

 3*bpc/2 */

 3*bpc */

 Assuming 4:2:2 encoding */

 2*bpc */

	/*

	 * Taking into account if all dsc_all_bpp supported by HDMI2.1 sink

	 * Section 7.7.34 : Source shall not enable compressed Video

	 * Transport with bpp_target settings above 12 bpp unless

	 * DSC_all_bpp is set to 1.

	/*

	 * The Sink has a limit of compressed data in bytes for a scanline,

	 * as described in max_chunk_bytes field in HFVSDB block of edid.

	 * The no. of bytes depend on the target bits per pixel that the

	 * source configures. So we start with the max_bpp and calculate

	 * the target_chunk_bytes. We keep on decrementing the target_bpp,

	 * till we get the target_chunk_bytes just less than what the sink's

	 * max_chunk_bytes, or else till we reach the min_dsc_bpp.

	 *

	 * The decrement is according to the fractional support from PCON DSC

	 * encoder. For fractional BPP we use bpp_target as a multiple of 16.

	 *

	 * bpp_target_x16 = bpp_target * 16

	 * So we need to decrement by {1, 2, 4, 8, 16} for fractional bpps

	 * {1/16, 1/8, 1/4, 1/2, 1} respectively.

 src does not support fractional bpp implies decrement by 16 for bppx16 */

/*

 * Copyright Â© 2013 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Author: Jani Nikula <jani.nikula@intel.com>

 return pixels in terms of txbyteclkhs */

 return pixels equvalent to txbyteclkhs */

 It just so happens the VBT matches register contents. */

 note: this is never true for reads */

 ->rx_len is set only for reads */

 XXX: fix for reads and writes */

/*

 * send a video mode command

 *

 * XXX: commands with data in MIPI_DPI_DATA?

 XXX: pipe, hs */

 clear bit */

 XXX: old code skips write if control unchanged */

 DSI uses short packets for sync events, so clear mode flags for DSI */

 Enable Frame time stamp based scanline reporting */

 Dual link goes to DSI transcoder A. */

	/* Set the MIPI mode

	 * If MIPI_Mode is off, then writing to LP_Wake bit is not reflecting.

	 * Power ON MIPI IO first and then write into IO reset and LP wake bits

 Put the IO into reset */

 Program LP Wake */

 Wait for Pwr ACK */

 Check for cold boot scenario */

 Wait for MIPI PHY status bit to set */

 Get IO out of reset */

 Get IO out of Low power state*/

 Enter ULPS */

 Wait for ULPS active */

 Exit ULPS */

 Enter Normal Mode */

 Wait for Stop state */

 Wait for AFE LATCH */

 Enable MIPI PHY transparent latch */

 Clear ULPS and set device ready */

	/* program rcomp for compliance, reduce from 50 ohms to 45 ohms

 bandgap reset is needed after everytime we do power gate */

		/* Enable MIPI PHY transparent latch

		 * Common bit for both MIPI Port A & MIPI Port C

		 * No similar bit in MIPI Port C reg

 Enter ULPS */

 Wait for MIPI PHY status bit to unset */

 Wait for Pwr ACK bit to unset */

 Put the IO into reset */

 Wait for MIPI PHY status bit to unset */

 Clear MIPI mode */

 Common bit for both MIPI Port A & MIPI Port C on VLV/CHV */

		/*

		 * On VLV/CHV, wait till Clock lanes are in LP-00 state for MIPI

		 * Port A only. MIPI Port C has no similar bit for checking.

 Disable MIPI PHY transparent latch */

 assert ip_tg_enable signal */

 de-assert ip_tg_enable signal */

/*

 * Panel enable/disable sequences from the VBT spec.

 *

 * Note the spec has AssertReset / DeassertReset swapped from their

 * usual naming. We use the normal names to avoid confusion (so below

 * they are swapped compared to the spec).

 *

 * Steps starting with MIPI refer to VBT sequences, note that for v2

 * VBTs several steps which have a VBT in v2 are expected to be handled

 * directly by the driver, by directly driving gpios for example.

 *

 * v2 video mode seq         v3 video mode seq         command mode seq

 * - power on                - MIPIPanelPowerOn        - power on

 * - wait t1+t2                                        - wait t1+t2

 * - MIPIDeassertResetPin    - MIPIDeassertResetPin    - MIPIDeassertResetPin

 * - io lines to lp-11       - io lines to lp-11       - io lines to lp-11

 * - MIPISendInitialDcsCmds  - MIPISendInitialDcsCmds  - MIPISendInitialDcsCmds

 *                                                     - MIPITearOn

 *                                                     - MIPIDisplayOn

 * - turn on DPI             - turn on DPI             - set pipe to dsr mode

 * - MIPIDisplayOn           - MIPIDisplayOn

 * - wait t5                                           - wait t5

 * - backlight on            - MIPIBacklightOn         - backlight on

 * ...                       ...                       ... issue mem cmds ...

 * - backlight off           - MIPIBacklightOff        - backlight off

 * - wait t6                                           - wait t6

 * - MIPIDisplayOff

 * - turn off DPI            - turn off DPI            - disable pipe dsr mode

 *                                                     - MIPITearOff

 *                           - MIPIDisplayOff          - MIPIDisplayOff

 * - io lines to lp-00       - io lines to lp-00       - io lines to lp-00

 * - MIPIAssertResetPin      - MIPIAssertResetPin      - MIPIAssertResetPin

 * - wait t3                                           - wait t3

 * - power off               - MIPIPanelPowerOff       - power off

 * - wait t4                                           - wait t4

/*

 * DSI port enable has to be done before pipe and plane enable, so we do it in

 * the pre_enable hook instead of the enable hook.

	/*

	 * The BIOS may leave the PLL in a wonky state where it doesn't

	 * lock. It needs to be fully powered down to fix it.

 Add MIPI IO reset programming for modeset */

 Power up DSI regulator */

 Disable DPOunit clock gating, can stall pipe */

	/*

	 * Give the panel time to power-on and then deassert its reset.

	 * Depending on the VBT MIPI sequences version the deassert-seq

	 * may contain the necessary delay, intel_dsi_msleep() will skip

	 * the delay in that case. If there is no deassert-seq, then an

	 * unconditional msleep is used to give the panel time to power-on.

 Prepare port in cold boot(s3/s4) scenario */

 Put device in ready state (LP-11) */

 Prepare port in normal boot scenario */

 Send initialization commands in LP mode */

	/*

	 * Enable port in pre-enable phase itself because as per hw team

	 * recommendation, port should be enabled before plane & pipe

 XXX */

/*

 * DSI port disable has to be done after pipe and plane disable, so we do it in

 * the post_disable hook.

	/*

	 * According to the spec we should send SHUTDOWN before

	 * MIPI_SEQ_DISPLAY_OFF only for v3+ VBTs, but field testing

	 * has shown that the v3 sequence works for v2 VBTs too

 Send Shutdown command to the panel in LP mode */

	/*

	 * if disable packets are sent before sending shutdown packet then in

	 * some next enable sequence send turn on packet error is observed

 Transition to LP-00 */

 Power down DSI regulator to save power */

 Add MIPI IO reset programming for modeset */

 Assert reset */

	/*

	 * On Broxton the PLL needs to be enabled with a valid divider

	 * configuration, otherwise accessing DSI registers will hang the

	 * machine. See BSpec North Display Engine registers/MIPI[BXT].

 XXX: this only works for one DSI output */

		/*

		 * Due to some hardware limitations on VLV/CHV, the DPI enable

		 * bit in port C control register does not get set. As a

		 * workaround, check pipe B conf instead.

 Try command mode if video mode not enabled */

 FIXME: hw readout should not depend on SW state */

	/*

	 * Atleast one port is active as encoder->get_config called only if

	 * encoder->get_hw_state() returns true.

 Enable Frame time stamo based scanline reporting */

 In terms of pixels */

	/*

	 * Meaningful for video mode non-burst sync pulse mode only,

	 * can be zero for non-burst sync events and burst modes

 harizontal values are in terms of high speed byte clock */

 vertical values are in terms of lines */

	/*

	 * In BXT DSI there is no regs programmed with few horizontal timings

	 * in Pixels but txbyteclkhs.. So retrieval process adds some

	 * ROUND_UP ERRORS in the process of PIXELS<==>txbyteclkhs.

	 * Actually here for the given adjusted_mode, we are calculating the

	 * value programmed to the port and then back to the horizontal timing

	 * param in pixels. This is the expected value, including roundup errors

	 * And if that is same as retrieved value from port, then

	 * (HW state) adjusted_mode's horizontal timings are corrected to

	 * match with SW state to nullify the errors.

 Calculating the value programmed to the Port register */

 Reverse calculating the adjusted mode parameters from port reg vals*/

 return txclkesc cycles in terms of divider and duration in us */

 horizontal values are in terms of high speed byte clock */

			/*

			 * Program hdisplay and vdisplay on MIPI transcoder.

			 * This is different from calculated hactive and

			 * vactive, as they are calculated per channel basis,

			 * whereas these values should be based on resolution.

		/* meaningful for video mode non-burst sync pulse mode only,

 vertical values are in terms of lines */

			/*

			 * escape clock divider, 20MHz, shared for A and C.

			 * device ready must be off when doing this! txclkesc?

 read request priority is per pipe */

 XXX: why here, why like this? handling in irq handler?! */

 XXX */

		/* timeouts for recovery. one frame IIUC. if counter expires,

		/*

		 * In burst mode, value greater than one DPI line Time in byte

		 * clock (txbyteclkhs) To timeout this timer 1+ of the above

		 * said value is recommended.

		 *

		 * In non-burst mode, Value greater than one DPI frame time in

		 * byte clock(txbyteclkhs) To timeout this timer 1+ of the above

		 * said value is recommended.

		 *

		 * In DBI only mode, value greater than one DBI frame time in

		 * byte clock(txbyteclkhs) To timeout this timer 1+ of the above

		 * said value is recommended.

 dphy stuff */

 in terms of low power clock */

			/*

			 * BXT spec says write MIPI_INIT_COUNT for

			 * both the ports, even if only one is

			 * getting used. So write the other port

			 * if not in dual link mode.

 recovery disables */

 in terms of low power clock */

		/* in terms of txbyteclkhs. actual high to low switch +

		 * MIPI_STOP_STATE_STALL * MIPI_LP_BYTECLK.

		 *

		 * XXX: write MIPI_STOP_STATE_STALL?

		/* XXX: low power clock equivalence in terms of byte clock.

		 * the number of byte clocks occupied in one low power clock.

		 * based on txbyteclkhs and txclkesc.

		 * txclkesc time / txbyteclk time * (105 + MIPI_STOP_STATE_STALL

		 * ) / 105.???

 Shadow of DPHY reg */

		/* the bw essential for transmitting 16 long packets containing

		 * 252 bytes meant for dcs write memory command is programmed in

		 * this register in terms of byte clocks. based on dsi transfer

		 * rate and the number of lanes configured the time taken to

			/* Some panels might have resolution which is not a

			 * multiple of 64 like 1366 x 768. Enable RANDOM

 Panel commands can be sent when clock is in LP11 */

 in Kbps */

	/*

	 * B060

	 * LP byte clock = TLPX/ (8UI)

	/* DDR clock period = 2 * UI

	 * UI(sec) = 1/(bitrate * 10^3) (bitrate is in KHZ)

	 * UI(nsec) = 10^6 / bitrate

	 * DDR clock period (nsec) = 2 * UI = (2 * 10^6)/ bitrate

	 * DDR clock count  = ns_value / DDR clock period

	 *

	 * For GEMINILAKE dphy_param_reg will be programmed in terms of

	 * HS byte clock count for other platform in HS ddr clock count

 prepare count */

 exit zero count */

	/*

	 * Exit zero is unified val ths_zero and ths_exit

	 * minimum value for ths_exit = 110ns

	 * min (exit_zero_cnt * 2) = 110/UI

	 * exit_zero_cnt = 55/UI

 clk zero count */

 trail count */

 B080 */

	/*

	 * LP to HS switch count = 4TLPX + PREP_COUNT * mul + EXIT_ZERO_COUNT *

	 *					mul + 10UI + Extra Byte Count

	 *

	 * HS to LP switch count = THS-TRAIL + 2TLPX + Extra Byte Count

	 * Extra Byte Count is calculated according to number of lanes.

	 * High Low Switch Count is the Max of LP to HS and

	 * HS to LP switch count

	 *

 B044 */

	/* FIXME:

 B088 */

	/* LP -> HS for clock lanes

	 * LP clk sync + LP11 + LP01 + tclk_prepare + tclk_zero +

	 *						extra byte count

	 * 2TPLX + 1TLPX + 1 TPLX(in ns) + prepare_cnt * 2 + clk_zero_cnt *

	 *					2(in UI) + extra byte count

	 * In byteclks = (4TLPX + prepare_cnt * 2 + clk_zero_cnt *2 (in UI)) /

	 *					8 + extra byte count

	/* HS->LP for Clock Lanes

	 * Low Power clock synchronisations + 1Tx byteclk + tclk_trail +

	 *						Extra byte count

	 * 2TLPX + 8UI + (trail_count*2)(in UI) + Extra byte count

	 * In byteclks = (2*TLpx(in UI) + trail_count*2 +8)(in UI)/8 +

	 *						Extra byte count

 There is no detection method for MIPI so rely on VBT */

	/*

	 * On BYT/CHV, pipe A maps to MIPI DSI port A, pipe B maps to MIPI DSI

	 * port C. BXT isn't limited like this.

 Create a DSI host (and a device) for each port. */

 Use clock read-back from current hw-state for fastboot */

XXX*/

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 *

 LSPCON OUI Vendor ID(signatures) */

 AUX addresses to write MCA AVI IF */

 AUX addresses to write Parade AVI IF */

 Lets probe the adaptor and check its type */

 Yay ... got a LSPCON device */

	/*

	 * In the SW state machine, lets Put LSPCON in PCON mode only.

	 * In this way, it will work with both HDMI 1.4 sinks as well as HDMI

	 * 2.0 sinks.

 Check if LSPCON FW is ready for data */

		/*

		 * Once a block of data is written, we have to inform the FW

		 * about this by writing into avi infoframe control register:

		 * - set the kickoff bit[7] to 1

		 * - write the block no. to bits[1:0]

	/*

	 * Parade's frames contains 32 bytes of data, divided

	 * into 4 frames:

	 *	Token byte (first byte of first frame, must be non-zero)

	 *	HB0 to HB2	 from AVI IF (3 bytes header)

	 *	PB0 to PB27 from AVI IF (28 bytes data)

	 * So it should look like this

	 *	first block: | <token> <HB0-HB2> <DB0-DB3> |

	 *	next 3 blocks: |<DB4-DB11>|<DB12-DB19>|<DB20-DB28>|

 DPCD write for AVI IF can fail on a slow FW day, so retry */

 Indicate LSPCON chip about infoframe, clear bit 1 and set bit 0 */

 It uses the legacy hsw implementation for the same */

 FIXME implement for AVI Infoframe as well */

 FIXME precompute infoframes */

	/*

	 * Currently there is no interface defined to

	 * check user preference between RGB/YCBCR444

	 * or YCBCR420. So the only possible case for

	 * YCBCR444 usage is driving YCBCR420 output

	 * with LSPCON, when pipe is configured for

	 * YCBCR444 output and LSPCON takes care of

	 * downsampling it.

 Set the Colorspace as per the HDMI spec */

 nonsense combination */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

		/*

		 * If the new state wasn't modified (and properly

		 * locked for write access) we throw it away.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 *

/**

 * DOC: DSB

 *

 * A DSB (Display State Buffer) is a queue of MMIO instructions in the memory

 * which can be offloaded to DSB HW in Display Controller. DSB HW is a DMA

 * engine that can be programmed to download the DSB from memory.

 * It allows driver to batch submit display HW programming. This helps to

 * reduce loading time and CPU activity, thereby making the context switch

 * faster. DSB Support added from Gen12 Intel graphics based platform.

 *

 * DSB's can access only the pipe, plane, and transcoder Data Island Packet

 * registers.

 *

 * DSB HW can support only register writes (both indexed and direct MMIO

 * writes). There are no registers reads possible with DSB HW engine.

 DSB opcodes. */

/**

 * intel_dsb_indexed_reg_write() -Write to the DSB context for auto

 * increment register.

 * @crtc_state: intel_crtc_state structure

 * @reg: register address.

 * @val: value.

 *

 * This function is used for writing register-value pair in command

 * buffer of DSB for auto-increment register. During command buffer overflow,

 * a warning is thrown and rest all erroneous condition register programming

 * is done through mmio write.

	/*

	 * For example the buffer will look like below for 3 dwords for auto

	 * increment register:

	 * +--------------------------------------------------------+

	 * | size = 3 | offset &| value1 | value2 | value3 | zero   |

	 * |          | opcode  |        |        |        |        |

	 * +--------------------------------------------------------+

	 * +          +         +        +        +        +        +

	 * 0          4         8        12       16       20       24

	 * Byte

	 *

	 * As every instruction is 8 byte aligned the index of dsb instruction

	 * will start always from even number while dealing with u32 array. If

	 * we are writing odd no of dwords, Zeros will be added in the end for

	 * padding.

 Every instruction should be 8 byte aligned. */

 Update the size. */

 Update the opcode and reg. */

 Update the value. */

 Update the new value. */

 Update the size. */

 if number of data words is odd, then the last dword should be 0.*/

/**

 * intel_dsb_reg_write() -Write to the DSB context for normal

 * register.

 * @crtc_state: intel_crtc_state structure

 * @reg: register address.

 * @val: value.

 *

 * This function is used for writing register-value pair in command

 * buffer of DSB. During command buffer overflow, a warning  is thrown

 * and rest all erroneous condition register programming is done

 * through mmio write.

/**

 * intel_dsb_commit() - Trigger workload execution of DSB.

 * @crtc_state: intel_crtc_state structure

 *

 * This function is used to do actual write to hardware using DSB.

 * On errors, fall back to MMIO. Also this function help to reset the context.

/**

 * intel_dsb_prepare() - Allocate, pin and map the DSB command buffer.

 * @crtc_state: intel_crtc_state structure to prepare associated dsb instance.

 *

 * This function prepare the command buffer which is used to store dsb

 * instructions with data.

/**

 * intel_dsb_cleanup() - To cleanup DSB context.

 * @crtc_state: intel_crtc_state structure to cleanup associated dsb instance.

 *

 * This function cleanup the DSB context by unpinning and releasing

 * the VMA object associated with it.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

/**

 * scale - scale values from one range to another

 * @source_val: value in range [@source_min..@source_max]

 * @source_min: minimum legal value for @source_val

 * @source_max: maximum legal value for @source_val

 * @target_min: corresponding target value for @source_min

 * @target_max: corresponding target value for @source_max

 *

 * Return @source_val in range [@source_min..@source_max] scaled to range

 * [@target_min..@target_max].

 defensive */

 avoid overflows */

/*

 * Scale user_level in range [0..user_max] to [0..hw_max], clamping the result

 * to [hw_min..hw_max].

 Scale hw_level in range [hw_min..hw_max] to [0..user_max]. */

/* set backlight brightness to level in range [0..max], assuming hw min is

 * respected.

	/*

	 * Lack of crtc may occur during driver init because

	 * connection_mutex isn't held across the entire backlight

	 * setup + modeset readout, and the BIOS can issue the

	 * requests at any time.

	/*

	 * Although we don't support or enable CPU PWM with LPT/SPT based

	 * systems, it may have been enabled prior to loading the

	 * driver. Disable to avoid warnings on LCPLL disable.

	 *

	 * This needs rework if we need to add support for CPU PWM on PCH split

	 * platforms.

	/*

	 * Do not disable backlight on the vga_switcheroo path. When switching

	 * away from i915, the other client may depend on i915 to handle the

	 * backlight. This will leave the backlight on unnecessarily when

	 * another client is not activated.

 After LPT, override is the default. */

 This won't stick until the above enable. */

 This won't stick until the above enable. */

 XXX: combine this into above write? */

	/*

	 * Needed to enable backlight on some 855gm models. BLC_HIST_CTL is

	 * 855gm only, but checking for gen2 is safe, as 855gm is the only gen2

	 * that has backlight.

 XXX: combine this into above write? */

 Controller 1 uses the utility pin. */

 Scale user_level in range [0..user_max] to [hw_min..hw_max]. */

 set backlight brightness to level in range [0..max], scaling wrt hw min */

	/*

	 * Allow flipping bl_power as a sub-state of enabled. Sadly the

	 * backlight class device does not make it easy to differentiate

	 * between callbacks for brightness and bl_power, so our backlight_power

	 * callback needs to take this into account.

	/*

	 * Note: Everything should work even if the backlight device max

	 * presented to the userspace is arbitrarily chosen.

	/*

	 * Using the same name independent of the drm device or connector

	 * prevents registration of multiple backlight devices in the

	 * driver. However, we need to use the default name for backward

	 * compatibility. Use unique names for subsequent backlight devices as a

	 * fallback when the default name already exists.

 CONFIG_BACKLIGHT_CLASS_DEVICE */

/*

 * CNP: PWM clock frequency is 19.2 MHz or 24 MHz.

 *      PWM increment = 1

/*

 * BXT: PWM clock frequency = 19.2 MHz.

/*

 * SPT: This value represents the period of the PWM stream in clock periods

 * multiplied by 16 (default increment) or 128 (alternate increment selected in

 * SCHICKEN_1 bit 0). PWM clock is 24 MHz.

/*

 * LPT: This value represents the period of the PWM stream in clock periods

 * multiplied by 128 (default increment) or 16 (alternate increment, selected in

 * LPT SOUTH_CHICKEN2 register bit 5).

 LPT:H */

 LPT:LP */

/*

 * ILK/SNB/IVB: This value represents the period of the PWM stream in PCH

 * display raw clocks multiplied by 128.

/*

 * Gen2: This field determines the number of time base events (display core

 * clock frequency/32) in total for a complete cycle of modulated backlight

 * control.

 *

 * Gen3: A time base event equals the display core clock ([DevPNV] HRAW clock)

 * divided by 32.

/*

 * Gen4: This value represents the period of the PWM stream in display core

 * clocks ([DevCTG] HRAW clocks) multiplied by 128.

 *

/*

 * VLV: This value represents the period of the PWM stream in display core

 * clocks ([DevCTG] 200MHz HRAW clocks) multiplied by 128 or 25MHz S0IX clocks

 * multiplied by 16. CHV uses a 19.2MHz S0IX clock.

/*

 * Note: The setup hooks can't assume pipe is set!

	/*

	 * XXX: If the vbt value is 255, it makes min equal to max, which leads

	 * to problems. There are such machines out there. Either our

	 * interpretation is wrong or the vbt has bogus data. Or both. Safeguard

	 * against this by letting the minimum be at most (arbitrarily chosen)

	 * 25% of the max.

 vbt value is a coefficient in range [0..255] */

 Write converted CPU PWM value to PCH override register */

 Controller 1 uses the utility pin. */

	/*

	 * CNP has the BXT implementation of backlight, but with only one

	 * controller. TODO: ICP has multiple controllers but we only use

	 * controller 0 for now.

 Get the right PWM chip for DSI backlight according to VBT */

 100% */

 PWM is already enabled, use existing settings */

 Set period from VBT frequency, leave other settings at 0. */

 ensure intel_panel has been initialized first */

 set level and max in panel struct */

 dispose of the pwm */

 Set up chip specific backlight functions */

 We're using a standard PWM backlight interface */

/*

 * Copyright 2006 Dave Airlie <airlied@linux.ie>

 * Copyright Â© 2006-2007 Intel Corporation

 *   Jesse Barnes <jesse.barnes@intel.com>

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 Register for the SDVO device: SDVOB or SDVOC */

 Active outputs controlled by this SDVO output */

	/*

	 * Capabilities of the SDVO device returned by

	 * intel_sdvo_get_capabilities()

 Pixel clock limitations reported by the SDVO device, in kHz */

	/*

	* For multiple function SDVO device,

	* this is for current attached outputs.

	/*

	 * Hotplug activation bits for this device

 DDC bus used by this SDVO encoder */

	/*

	 * the sdvo flag gets lost in round trip: dtd->adjusted_mode->dtd

 Mark the type of connector */

 This contains all current supported TV format */

 add the property for the SDVO-TV */

 add the property for the SDVO-TV/LVDS */

 this is to get the range of margin.*/

	/**

	 * This is set if we treat the device as HDMI, instead of DVI.

 base.base: tv.saturation/contrast/hue/brightness */

/*

 * Writes the SDVOB or SDVOC with the given value, but always writes both

 * SDVOB and SDVOC to work around apparent hardware issues (according to

 * comments in the BIOS).

		/*

		 * HW workaround, need to write this twice for issue

		 * that may result in first write getting masked.

	/*

	 * Write the registers twice for luck. Sometimes,

	 * writing them only once doesn't appear to 'stick'.

	 * The BIOS does this too. Yay, magic

* Mapping of command numbers to names, for debug output */

 Add the op code for SDVO enhancements */

 HDMI op code */

 Would be simpler to allocate both in one go ? */

 the following two are to read the response */

 failure in I2C transfer */

 5 quick checks, followed by 10 long checks */

	/*

	 * The documentation states that all commands will be

	 * processed within 15Âµs, and that we need only poll

	 * the status byte a maximum of 3 times in order for the

	 * command to be complete.

	 *

	 * Check 5 times in case the hardware failed to read the docs.

	 *

	 * Also beware that the first response by many devices is to

	 * reply PENDING and stall for time. TVs are notorious for

	 * requiring longer than specified to complete their replies.

	 * Originally (in the DDX long ago), the delay was only ever 15ms

	 * with an additional delay of 30ms applied for TVs added later after

	 * many experiments. To accommodate both sets of delays, we do a

	 * sequence of slow checks if the device is falling behind and fails

	 * to reply within 5*15Âµs.

 Read the command response */

 This must be the immediately preceding write before the i2c xfer */

/*

 * Return whether each input is trained.

 *

 * This function is making an assumption about the layout of the response,

 * which should be checked against the docs.

 Convert the values from units of 10 kHz to kHz. */

 do some mode translations */

 Buffer size is 0 based, hooray! However zero means zero. */

/*

 * Asks the sdvo controller for the preferred input mode given the output mode.

 * Unfortunately we have to set up the full output mode to do that.

 Reset the input timing to the screen. Assume always input 0. */

	/*

	 * SDVO TV has fixed PLL values depend on its clock range,

	 * this mirrors vbios setting.

	/*

	 * We need to construct preferred input timings based on our

	 * output timings.  To do that, we have to set the output

	 * timings, even though this isn't really the right place in

	 * the sequence to do it. Oh well.

	/*

	 * Make the CRTC code factor in the SDVO pixel multiplier.  The

	 * SDVO device will factor out the multiplier during mode_set.

 Clock computation needs to happen after pixel multiplier. */

	/*

	 * First, set the input mapping for the first input to our controlled

	 * output. This is only correct if we're a single-input device, in

	 * which case the first input is the output from the appropriate SDVO

	 * channel on the motherboard.  In a two-input device, the first input

	 * will be SDVOB and the second SDVOC.

 Set the output timings to the screen */

 lvds has a special fixed output timing. */

 Set the input timing to the screen. Assume always input 0. */

 Set the SDVO control regs. */

		/* The real mode polarity is set by the SDVO commands, using

 done in crtc_mode_set as the dpll_md reg must be written early */

 done in crtc_mode_set as it lives inside the dpll register */

 asserts want to know the pipe even if the port is disabled */

		/*

		 * Some sdvo encoders are not spec compliant and don't

		 * implement the mandatory get_timings function.

	/*

	 * pixel multiplier readout is tricky: Only on i915g/gm it is stored in

	 * the sdvo port register, on all other platforms it is part of the dpll

	 * state. Since the general pipe state readout happens before the

	 * encoder->get_config we so already have a valid pixel multplier on all

	 * other platfroms.

 Cross check the port pixel multiplier with the sdvo encoder state. */

	/*

	 * HW workaround for IBX, we need to move the port

	 * to transcoder A after disabling it to allow the

	 * matching DP port to be enabled on transcoder A.

		/*

		 * We get CPU/PCH FIFO underruns on the other pipe when

		 * doing the workaround. Sweep them under the rug.

	/*

	 * Warn if the device reported failure to sync.

	 *

	 * A lot of SDVO devices fail to notify of sync, but it's

	 * a given it the status is a success, we succeeded.

	/*

	 * HW Erratum: SDVO Hotplug is broken on all i945G chips, there's noise

	 * on the line.

 Is there more than one type of output? */

 Mac mini hack -- use the same DDC as the analog connector */

		/*

		 * Don't use the 1 as the argument of DDC bus switch to get

		 * the EDID. It is used for SDVO SPD ROM.

		/*

		 * If we found the EDID on the other bus,

		 * assume that is the correct DDC bus.

	/*

	 * When there is no edid and no monitor is connected with VGA

	 * port, try to use the CRT ddc to read the EDID for DVI-connector.

 DDC bus is shared, match EDID to connector type */

 if we have an edid check it matches the connection */

 set the bus switch and get the modes */

	/*

	 * Mac mini hack.  On this device, the DVI-I connector shares one DDC

	 * link between analog and digital outputs. So, if the regular SDVO

	 * DDC fails, check to see if the analog output is disconnected, in

	 * which case we'll look there for the digital DDC data.

/*

 * Set of SDVO TV modes.

 * Note!  This is in reply order (see loop in get_tv_modes).

 * XXX: all 60Hz refresh?

	/*

	 * Read the list of supported input resolutions for the selected TV

	 * format.

	/*

	 * Fetch modes from VBT. For SDVO prefer the VBT mode since some

	 * SDVO->LVDS transcoders can't cope with the EDID mode.

 Guarantee the mode is preferred */

	/*

	 * Attempt to get the mode list from DDC.

	 * Assume that the preferred modes are

	 * arranged in priority order.

 Cannot set these independent from each other */

 Cannot set these independent from each other */

	/*

	 * Make a mask of outputs less than or equal to our own priority in the

	 * list.

 Count bits to find what number we are in the priority list. */

 If more than 3 outputs, default to DDC bus 3 for now. */

 Corresponds to SDVO_CONTROL_BUS_DDCx */

/*

 * Choose the appropriate DDC bus for control bus switch command for this

 * SDVO output based on the controlled output.

 *

 * DDC bus number assignment is in a priority order of RGB outputs, then TMDS

 * outputs, then LVDS outputs.

	/*

	 * With gmbus we should be able to drive sdvo i2c at 2MHz, but somehow

	 * our code totally fails once we start using gmbus. Hence fall back to

	 * bit banging for now.

 undo any changes intel_sdvo_select_i2c_bus() did to sdvo->i2c */

 If the BIOS described our SDVO device, take advantage of it. */

	/*

	 * If the BIOS only described a different SDVO device, use the

	 * address that it isn't using.

	/*

	 * No SDVO device info is found for another DVO port,

	 * so use mapping assumption we had before BIOS parsing.

		/*

		 * Some SDVO devices have one-shot hotplug interrupts.

		 * Ensure that they get re-enabled when an interrupt happens.

 SDVO requires XXX1 function may not exist unless it has XXX0 function.*/

 TV has no XXX1 function block */

 when horizontal overscan is supported, Add the left/right property */

 encoder type will be decided later */

 Read the regs to test if we can talk to the device */

 In default case sdvo lvds is false */

 Output_setup can leave behind connectors! */

	/*

	 * Only enable the hotplug irq if we need it, to work around noisy

	 * hotplug lines.

	/*

	 * Cloning SDVO with anything is often impossible, since the SDVO

	 * encoder can request a special input timing mode. And even if that's

	 * not the case we have evidence that cloning a plain unscaled mode with

	 * VGA doesn't really work. Furthermore the cloning flags are way too

	 * simplistic anyway to express such constraints, so just give up on

	 * cloning for SDVO encoders.

 Set the input timing to the screen. Assume always input 0. */

 check currently supported outputs */

/*

 * Copyright (c) 2006 Dave Airlie <airlied@linux.ie>

 * Copyright Â© 2006-2008,2010 Intel Corporation

 *   Jesse Barnes <jesse.barnes@intel.com>

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 *	Chris Wilson <chris@chris-wilson.co.uk>

 Map gmbus pin pairs to names and registers. */

 pin is expected to be valid */

 Intel GPIO access functions */

 When using bit bashing for I2C, this bit needs to be set to 1 */

 On most chips, these bits must be preserved in software. */

	/* Important: The hw handles only the first bit, so set only one! Since

	 * we also need to check for NAKs besides the hw ready/idle signal, we

	 * need to wake up periodically and check that ourselves.

 Important: The hw handles only the first bit, so set only one! */

		/*

		 * As per HW Spec, for 512Bytes need to read extra Byte and

		 * Ignore the extra byte read.

 Reset the override bit */

/*

 * HW spec says that 512Bytes in Burst read need special treatment.

 * But it doesn't talk about other multiple of 256Bytes. And couldn't locate

 * an I2C slave, which supports such a lengthy burst read too for experiments.

 *

 * So until things get clarified on HW support, to avoid the burst read length

 * in fold of 256Bytes except 512, max burst read length is fixed at 767Bytes.

/*

 * The gmbus controller can combine a 1 or 2 byte write with another read/write

 * that immediately follows it by using an "INDEX" cycle.

 GMBUS5 holds 16-bit index */

 Clear GMBUS5 after each index transfer */

 Display WA #0868: skl,bxt,kbl,cfl,glk */

 an index transmission is two msgs */

	/* Generate a STOP condition on the bus. Note that gmbus can't generata

	 * a STOP on the very first cycle. To simplify the code we

	 * unconditionally generate the STOP condition with an additional gmbus

	/* Mark the GMBUS interface as disabled after waiting for idle.

	 * We will re-enable it at the start of the next xfer,

	 * till then let it sleep.

	/*

	 * Wait for bus to IDLE before clearing NAK.

	 * If we clear the NAK while bus is still active, then it will stay

	 * active and the next transaction may fail.

	 *

	 * If no ACK is received during the address phase of a transaction, the

	 * adapter must report -ENXIO. It is not clear what to return if no ACK

	 * is received at other times. But we have to be careful to not return

	 * spurious -ENXIO because that will prevent i2c and drm edid functions

	 * from retrying. So return -ENXIO only when gmbus properly quiescents -

	 * timing out seems to happen when there _is_ a ddc chip present, but

	 * it's slow responding and only answers on the 2nd retry.

	/* Toggle the Software Clear Interrupt bit. This has the effect

	 * of resetting the GMBUS controller and so clearing the

	 * BUS_ERROR raised by the slave's NAK.

	/*

	 * Passive adapters sometimes NAK the first probe. Retry the first

	 * message once on -ENXIO for GMBUS transfers; the bit banging algorithm

	 * has retries internally. See also the retry loop in

	 * drm_do_probe_ddc_edid, which bails out on the first -ENXIO.

	/*

	 * Hardware may not support GMBUS over these pins? Try GPIO bitbanging

	 * instead. Use EAGAIN to have i2c core retry.

 Display WA #0868: skl,bxt,kbl,cfl,glk */

	/*

	 * In order to output Aksv to the receiver, use an indexed write to

	 * pass the i2c command, and tell GMBUS to use the HW-provided value

	 * instead of sourcing GMBUS3 for the data.

 I2C_FUNC_10BIT_ADDR | */

/**

 * intel_gmbus_setup - instantiate all Intel i2c GMBuses

 * @dev_priv: i915 device private

		/*

		 * Broxton uses the same PCH offsets for South Display Engine,

		 * even though it doesn't have a PCH.

		/*

		 * We wish to retry with bit banging

		 * after a timed out GMBUS attempt.

 By default use a conservative clock rate */

 gmbus seems to be broken on i830 */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

		/*

		 * DDI does not have a specific FDI_TX register.

		 *

		 * FDI is never fed from EDP transcoder

		 * so pipe->transcoder cast is fine here.

 ILK FDI PLL is always enabled */

 On Haswell, DDI ports are responsible for the FDI PLL setup */

 units of 100MHz */

 Ivybridge 3 pipe is really complicated */

 SPLL */

	/* FDI is a binary signal running at ~2.7GHz, encoding

	 * each output octet as 10 bits. The actual frequency

	 * is stored as a divider into a 100MHz clock, and the

	 * mode pixel clock is stored in units of 1KHz.

	 * Hence the bw of each lane in terms of the mode signal

	 * is:

 enable normal train */

 wait one idle pattern time */

 IVB wants error correction enabled */

 The FDI link training functions for ILK/Ibexpeak. */

	/*

	 * Write the TU size bits before fdi link training, so that error

	 * detection works.

 FDI needs bits from pipe first */

	/* Train 1: umask FDI RX Interrupt symbol_lock and bit_lock bit

 enable CPU FDI TX and PCH FDI RX */

 Ironlake workaround, enable clock pointer after FDI enable*/

 Train 2 */

 The FDI link training functions for SNB/Cougarpoint. */

	/*

	 * Write the TU size bits before fdi link training, so that error

	 * detection works.

	/* Train 1: umask FDI RX Interrupt symbol_lock and bit_lock bit

 enable CPU FDI TX and PCH FDI RX */

 SNB-B */

 Train 2 */

 SNB-B */

 Manual link training for Ivy Bridge A0 parts */

	/*

	 * Write the TU size bits before fdi link training, so that error

	 * detection works.

	/* Train 1: umask FDI RX Interrupt symbol_lock and bit_lock bit

 Try each vswing and preemphasis setting twice before moving on */

 disable first in case we need to retry */

 enable CPU FDI TX and PCH FDI RX */

 should be 0.5us */

 should be 0.5us */

 Train 2 */

 should be 1.5us */

 should be 1.5us */

/* Starting with Haswell, different DDI ports can work in FDI mode for

 * connection to the PCH-located connectors. For this, it is necessary to train

 * both the DDI port and PCH receiver for the desired DDI buffer settings.

 *

 * The recommended port to work in FDI mode is DDI E, which we use here. Also,

 * please note that when FDI mode is active on DDI E, it shares 2 lines with

 * DDI A (which is used for eDP)

	/* Set the FDI_RX_MISC pwrdn lanes and the 2 workarounds listed at the

	 * mode set "sequence for CRT port" document:

	 * - TP1 to TP2 time with the default value

	 * - FDI delay to 90h

	 *

	 * WaFDIAutoLinkSetTimingOverrride:hsw

 Enable the PCH Receiver FDI PLL */

 Switch from Rawclk to PCDclk */

 Configure Port Clock Select */

	/* Start the training iterating through available voltages and emphasis,

 Configure DP_TP_CTL with auto-training */

		/* Configure and enable DDI_BUF_CTL for DDI E with next voltage.

		 * DDI E does not support port reversal, the functionality is

		 * achieved on the PCH side in FDI_RX_CTL, so no need to set the

 Program PCH FDI Receiver TU */

 Enable PCH FDI Receiver with auto-training */

 Wait for FDI receiver lane calibration */

 Unset FDI_RX_MISC pwrdn lanes */

 Wait for FDI auto training time */

		/*

		 * Leave things enabled even if we failed to train FDI.

		 * Results in less fireworks from the state checker.

 Disable DP_TP_CTL and FDI_RX_CTL and retry */

 Reset FDI_RX_MISC pwrdn lanes */

 Enable normal pixel sending for FDI */

 enable PCH FDI RX PLL, wait warmup plus DMI latency */

 Switch from Rawclk to PCDclk */

 Enable CPU FDI TX PLL, always on for Ironlake */

 Switch from PCDclk to Rawclk */

 Disable CPU FDI TX PLL */

 Wait for the clocks to turn off. */

 disable CPU FDI tx and PCH FDI rx */

 Ironlake workaround, disable clock pointer after downing FDI */

 still set train pattern 1 */

 BPC in FDI rx is consistent with that in PIPECONF */

 WaMPhyProgramming:hsw */

 FIXME: detect B0+ stepping and use auto training */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 Cursor formats */

	/*

	 * Put the final coordinates back so that the src

	 * coordinate checks will see the right values.

 ILK+ do this automagically in hardware */

 Use the unclipped src/dst rectangles, which we program to hw */

	/*

	 * 845g/865g are only limited by the width of their cursors,

	 * the height is arbitrary up to the precision of the register.

 if we want to turn off the cursor ignore width and height */

 Check for which cursor types we support */

	/* On these chipsets we can only modify the base/size/stride

	 * whilst the cursor is disabled.

 Wa_22012358565:adl-p */

 Cursor width is limited to a few power-of-two sizes */

	/*

	 * IVB+ have CUR_FBC_CTL which allows an arbitrary cursor

	 * height from 8 lines up to the cursor width, when the

	 * cursor is not rotated. Everything else requires square

	 * cursors.

 if we want to turn off the cursor ignore width and height */

 Check for which cursor types we support */

	/*

	 * There's something wrong with the cursor on CHV pipe C.

	 * If it straddles the left edge of the screen then

	 * moving it away from the edge or disabling it often

	 * results in a pipe underrun, and often that can lead to

	 * dead pipe (constant underrun reported, and it scans

	 * out just a solid color). To recover from that, the

	 * display power well must be turned off and on again.

	 * Refuse the put the cursor into that compromised position.

	/*

	 * On some platforms writing CURCNTR first will also

	 * cause CURPOS to be armed by the CURBASE write.

	 * Without the CURCNTR write the CURPOS write would

	 * arm itself. Thus we always update CURCNTR before

	 * CURPOS.

	 *

	 * On other platforms CURPOS always requires the

	 * CURBASE write to arm the update. Additonally

	 * a write to any of the cursor register will cancel

	 * an already armed cursor update. Thus leaving out

	 * the CURBASE write after CURPOS could lead to a

	 * cursor that doesn't appear to move, or even change

	 * shape. Thus we always write CURBASE.

	 *

	 * The other registers are armed by the CURBASE write

	 * except when the plane is getting enabled at which time

	 * the CURCNTR write arms the update.

	/*

	 * Not 100% correct for planes that can move between pipes,

	 * but that's only the case for gen2-3 which don't have any

	 * display power wells.

	/*

	 * When crtc is inactive or there is a modeset pending,

	 * wait for it to complete in the slowpath.

	 * PSR2 selective fetch also requires the slow path as

	 * PSR2 plane and transcoder registers can only be updated during

	 * vblank.

	 *

	 * FIXME bigjoiner fastpath would be good

	/*

	 * Don't do an async update if there is an outstanding commit modifying

	 * the plane.  This prevents our async update's changes from getting

	 * overridden by a previous synchronous update's state.

	/*

	 * If any parameters change that may affect watermarks,

	 * take the slowpath. Only changing fb or position should be

	 * in the fastpath.

 Swap plane state */

	/*

	 * We cannot swap crtc_state as it may be in use by an atomic commit or

	 * page flip that's running simultaneously. If we swap crtc_state and

	 * destroy the old state, we will cause a use-after-free there.

	 *

	 * Only update active_planes, which is needed for our internal

	 * bookkeeping. Either value will do the right thing when updating

	 * planes atomically. If the cursor was part of the atomic update then

	 * we would have taken the slowpath.

/*

 * Copyright Â© 2009

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Daniel Vetter <daniel@ffwll.ch>

 *

 * Derived from Xorg ddx, xf86-video-intel, src/i830_video.c

/* Limits for overlay size. According to intel doc, the real limits are:

 * Y width: 4095, UV width (planar): 2047, Y height: 2047,

 * UV width (planar): * 1023. But the xorg thinks 2048 for height and width. Use

 2 * 1023 */

 on 830 and 845 these large limits result in the card hanging */

 overlay register definitions */

 OCMD register */

 zero for YUYV or FOURCC YUY2 */

 YVYU */

 UYVY or FOURCC UYVY */

 VYUY */

 not in i965 Intel docs */

 not in i965 Intel docs */

 not in i965 Intel docs */

 not in i965 Intel docs */

 also 411 */

 OCONFIG register */

 DCLRKM (dst-key) register */

 overlay flip addr flag */

 polyphase filter coefficients */

 memory bufferd overlay registers */

 0x6C */

 0xA0 */

 0xA4 */

 0xA8 - 0x1FC */

 0x200 */

 0x300 */

 0x500 */

 0x600 */

 shifted-point number, (1<<12) == 1.0 */

 register access */

 flip handling */

 WA_OVERLAY_CLKGATE:alm */

 WA_DISABLE_L2CACHE_CLOCK_GATING:alm */

 overlay needs to be disable in OCMD reg */

 overlay needs to be enabled in OCMD reg */

 check for underruns */

 overlay needs to be disabled in OCMD reg */

	/* According to intel docs the overlay hw may hang (when switching

	 * off) without loading the filter coeffs. It is however unclear whether

	 * this applies to the disabling of the overlay or to the switching off

 wait for overlay to go idle */

 turn overlay off */

/* recover from an interruption due to a signal

/* Wait for pending overlay flip and release old frame.

 * Needs to be called before the overlay register are changed

 * via intel_overlay_(un)map_regs

	/*

	 * Only wait if there is actually an old frame to release to

	 * guarantee forward progress.

 return 6; not implemented */

 fixed point with a 12 bit shift */

if (params->format & I915_OVERLAY_YUV_PLANAR) {*/

 make the Y scale to UV scale ratio an exact multiply */

	/*} else {

	  xscale_UV = 0;

	  yscale_UV = 0;

 YUV packed */

 can't use the overlay with double wide pipe */

	/* XXX: This is not the same logic as in the xorg driver, but more in

	 * line with the intel documentation for the i965

 on i965 use the PGM reg to read out the autoscaler values */

 downscaling limit is 8.0 */

 check src dimensions */

 better safe than sorry, use 4 as the maximal subsampling ratio */

 check alignment constraints */

 not implemented */

 ignore UV planes */

 check pixel alignment */

 no offset restrictions for planar formats */

 stride checking */

 check buffer dimensions */

 always 4 Y values per depth pixels */

 line too wide, i.e. one-line-mode */

 shifting right rounds downwards, so add 1 */

 Check scaling after src size to prevent a divide-by-zero. */

	/*

	 * The bo's should be free'd by the generic code already.

	 * Furthermore modesetting teardown happens beforehand so the

	 * hardware should be off already.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 Primary plane formats for gen <= 3 */

 Primary plane formats for ivb (no fp16 due to hw issue) */

 Primary plane formats for gen >= 4, except ivb */

 Primary plane formats for vlv/chv */

 tied to pipe A */

 Undocumented hardware limit on i965/g4x/vlv/chv */

	/*

	 * When using an X-tiled surface the plane starts to

	 * misbehave if the x offset + width exceeds the stride.

	 * hsw/bdw: underrun galore

	 * ilk/snb/ivb: wrap to the next tile row mid scanout

	 * i965/g4x: so far appear immune to this

	 * vlv/chv: TODO check

	 *

	 * Linear surfaces seem to work just fine, even on hsw/bdw

	 * despite them not using the linear offset anymore.

	/*

	 * Put the final coordinates back so that the src

	 * coordinate checks will see the right values.

 HSW/BDW do this automagically in hardware */

	/*

	 * g4x bspec says 64bpp pixel rate can't exceed 80%

	 * of cdclk when the sprite plane is enabled on the

	 * same pipe. ilk/snb bspec says 64bpp pixel rate is

	 * never allowed to exceed 80% of cdclk. Let's just go

	 * with the ilk/snb limit always.

	/*

	 * Note that crtc_state->pixel_rate accounts for both

	 * horizontal and vertical panel fitter downscaling factors.

	 * Pre-HSW bspec tells us to only consider the horizontal

	 * downscaling factor here. We ignore that and just consider

	 * both for simplicity.

 two pixels per clock with double wide pipe */

		/*

		 * PLANE_A doesn't actually have a full window

		 * generator but let's assume we still need to

		 * program whatever is there.

	/*

	 * The control register self-arms if the plane was previously

	 * disabled. Try to make the plane enable atomic by writing

	 * the control register just before the surface register.

	/*

	 * DSPCNTR pipe gamma enable on g4x+ and pipe csc

	 * enable on ilk+ affect the pipe bottom color as

	 * well, so we must configure them even if the plane

	 * is disabled.

	 *

	 * On pre-g4x there is no way to gamma correct the

	 * pipe bottom color but we'll keep on doing this

	 * anyway so that the crtc state readout works correctly.

	/*

	 * Not 100% correct for planes that can move between pipes,

	 * but that's only the case for gen2-4 which don't have any

	 * display power wells.

 Limit to 8k pixels to guarantee OFFSET.x doesn't get too big. */

 Limit to 4k pixels to guarantee TILEOFF.x doesn't get too big. */

 Limit to 4k pixels to guarantee TILEOFF.x doesn't get too big. */

	/*

	 * On gen2/3 only plane A can do FBC, but the panel fitter and LVDS

	 * port is hooked to pipe B. Hence we want plane A feeding pipe B.

		/*

		 * WaFP16GammaEnabling:ivb

		 * "Workaround : When using the 64-bit format, the plane

		 *  output on each color channel has one quarter amplitude.

		 *  It can be brought up to full amplitude by using pipe

		 *  gamma correction or pipe color space conversion to

		 *  multiply the plane output by four."

		 *

		 * There is no dedicated plane gamma for the primary plane,

		 * and using the pipe gamma/csc could conflict with other

		 * planes, so we choose not to expose fp16 on IVB primary

		 * planes. HSW primary planes no longer have this problem.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

	/*

	 * On ADL-P the AUX stride must align with a power-of-two aligned main

	 * surface stride. The stride of the allocated main surface object can

	 * be less than this POT stride, which is then autopadded to the POT

	 * size.

/*

 * Return the tile dimensions in pixel units, based on the (2 or 4 kbyte) GTT

 * page tile size.

/*

 * Return the tile dimensions in pixel units, based on the tile block size.

 * The block covers the full GTT page sized tile on all tiled surfaces and

 * it's a 64 byte portion of the tile on TGL+ CCS surfaces.

 AUX_DIST needs only 4K alignment */

		/*

		 * TODO: cross-check wrt. the bspec stride in bytes * 64 bytes

		 * alignment for linear UV planes on all platforms.

	/*

	 * TODO: Deduct the subsampling from the char block for all CCS

	 * formats and planes.

	/*

	 * The min stride check in the core framebuffer_check() function

	 * assumes that format->hsub applies to every plane except for the

	 * first plane. That's incorrect for the CCS AUX plane of the first

	 * plane, but for the above check to pass we must define the block

	 * width with that subsampling applied to it. Adjust the width here

	 * accordingly, so we can calculate the actual subsampling factor.

	/*

	 * On ADL-P the CCS AUX surface layout always aligns with the

	 * power-of-two aligned main surface stride. The main surface

	 * stride in the allocated FB object may not be power-of-two

	 * sized, in which case it is auto-padded to the POT size.

 minimize x in case it got needlessly big */

/*

 * Adjust the tile offset by moving the difference into

 * the x/y offsets.

/*

 * Computes the aligned offset to the base tile and adjusts

 * x, y. bytes per pixel is assumed to be a power-of-two.

 *

 * In the 90/270 rotated case, x and y are assumed

 * to be already rotated to match the rotated GTT view, and

 * pitch is the tile_height aligned framebuffer height.

 *

 * This function is used when computing the derived information

 * under intel_framebuffer, so using any of that information

 * here is not allowed. Anything under drm_framebuffer can be

 * used. This is why the user has to pass in the pitch since it

 * is specified in the rotated orientation.

 Convert the fb->offset[] into x/y offsets */

 Catch potential overflows early */

	/*

	 * While all the tile dimensions are based on a 2k or 4k GTT page size

	 * here the main and CCS coordinates must match only within a (64 byte

	 * on TGL+) block inside the tile.

	/*

	 * CCS doesn't have its own x/y offset register, so the intra CCS tile

	 * x/y offsets must match between CCS and the main surface.

 We don't want to deal with remapping with cursors */

	/*

	 * The display engine limits already match/exceed the

	 * render engine limits, so not much point in remapping.

	 * Would also need to deal with the fence POT alignment

	 * and gen2 2KiB GTT tile size.

	/*

	 * The new CCS hash mode isn't compatible with remapping as

	 * the virtual address of the pages affects the compressed data.

 Linear needs a page aligned stride for remapping */

	/*

	 * No remapping for invisible planes since we don't have

	 * an actual source viewport to remap.

	/*

	 * FIXME: aux plane limits on gen9+ are

	 * unclear in Bspec, for now no checking.

	/*

	 * The fence (if used) is aligned to the start of the object

	 * so having the framebuffer wrap around across the edge of the

	 * fenced region doesn't really work. We have no API to configure

	 * the fence start offset within the object (nor could we probably

	 * on gen2/3). So it's just easier if we just require that the

	 * fb layout agrees with the fence layout. We already check that the

	 * fb stride matches the fence stride elsewhere.

		/*

		 * ADL_P, the only platform needing a POT stride has a minimum

		 * of 8 main surface and 2 CCS AUX stride tiles.

 rotate the x/y offsets to match the GTT view */

 rotate the tile dimensions to match the GTT view */

	/*

	 * We only keep the x/y offsets, so push all of the gtt offset into

	 * the x/y offsets.  x,y will hold the first pixel of the framebuffer

	 * plane from the start of the remapped/rotated gtt mapping.

 Return number of tiles @color_plane needs. */

		/*

		 * If the plane isn't horizontally tile aligned,

		 * we need one more tile.

		/*

		 * Plane 2 of Render Compression with Clear Color fb modifier

		 * is consumed by the driver and not passed to DE. Skip the

		 * arithmetic related to alignment and offset calculation.

		/*

		 * First pixel of the framebuffer from

		 * the start of the normal gtt mapping.

 how many tiles in total needed in the bo */

 Make src coordinates relative to the viewport */

 Rotate src coordinates to match rotated GTT view */

		/*

		 * First pixel of the src viewport from the

		 * start of the normal gtt mapping.

	/*

	 * Arbitrary limit for gen4+ chosen to match the

	 * render engine max stride.

	 *

	 * The new CCS hash mode makes remapping impossible

		/*

		 * To make remapping with linear generally feasible

		 * we need the stride to be page aligned.

		/*

		 * On ADL-P the stride must be either 8 tiles or a stride

		 * that is aligned to 16 tiles, required by the 16 tiles =

		 * 64 kbyte CCS AUX PTE granularity, allowing CCS FBs to be

		 * remapped.

		/*

		 * On TGL the surface stride must be 4 tile aligned, mapped by

		 * one 64 byte cacheline on the CCS AUX surface.

		/*

		 * Display WA #0531: skl,bxt,kbl,glk

		 *

		 * Render decompression and plane width > 3840

		 * combined with horizontal panning requires the

		 * plane stride to be a multiple of 4. We'll just

		 * require the entire fb to accommodate that to avoid

		 * potential runtime errors at plane configuration time.

	/*

	 * We ignore stride for all invisible planes that

	 * can be remapped. Otherwise we could end up

	 * with a false positive when the remapping didn't

	 * kick in due the plane being invisible.

 FIXME other color planes? */

		/*

		 * Sometimes even remapping can't overcome

		 * the stride limitations :( Can happen with

		 * big plane sizes and suitably misaligned

		 * offsets.

 Rotate src coordinates to match rotated GTT view */

		/*

		 * If there's a fence, enforce that

		 * the fb modifier and tiling mode match.

	/*

	 * gen2/3 display engine uses the fence if present,

	 * so the tiling mode must match the fb modifier exactly.

	/*

	 * If there's a fence, enforce that

	 * the fb pitch and fence stride match.

 FIXME need to adjust LINOFF/TILEOFF accordingly. */

 object is backed with LMEM for discrete */

 object is "remote", not in local memory */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

/**

 * DOC: Synopsis PHY support

 *

 * Synopsis PHYs are primarily programmed by looking up magic register values

 * in tables rather than calculating the necessary values at runtime.

 *

 * Of special note is that the SNPS PHYs include a dedicated port PLL, known as

 * an "MPLLB."  The MPLLB replaces the shared DPLL functionality used on other

 * platforms and must be programming directly during the modeset sequence

 * since it is not handled by the shared DPLL framework as on other platforms.

/*

 * Basic DP link rates with 100 MHz reference clock.

	/*

	 * SSC will be enabled, DP UHBR has a minimum SSC requirement.

	/*

	 * SSC will be enabled, DP UHBR has a minimum SSC requirement.

/*

 * Basic DP link rates with 38.4 MHz reference clock.

	/*

	 * SSC will be enabled, DP UHBR has a minimum SSC requirement.

	/*

	 * SSC will be enabled, DP UHBR has a minimum SSC requirement.

/*

 * eDP link rates with 100 MHz reference clock.

/*

 * HDMI link rates with 100 MHz reference clock.

		/*

		 * FIXME: Initially we're just enabling the "combo" outputs on

		 * port A-D.  The MPLLB for those ports takes an input from the

		 * "Display Filter PLL" which always has an output frequency

		 * of 100 MHz, hence the use of the _100 tables below.

		 *

		 * Once we enable port TC1 it will either use the same 100 MHz

		 * "Display Filter PLL" (when strapped to support a native

		 * display connection) or different 38.4 MHz "Filter PLL" when

		 * strapped to support a USB connection, so we'll need to check

		 * that to determine which table to use.

			/*

			 * FIXME: Can only support fixed HDMI frequencies

			 * until we have a proper algorithm under a valid

			 * license.

	/*

	 * 3. Software programs the following PLL registers for the desired

	 * frequency.

	/*

	 * 4. If the frequency will result in a change to the voltage

	 * requirement, follow the Display Voltage Frequency Switching -

	 * Sequence Before Frequency Change.

	 *

	 * We handle this step in bxt_set_cdclk().

 5. Software sets DPLL_ENABLE [PLL Enable] to "1". */

	/*

	 * 9. Software sets SNPS_PHY_MPLLB_DIV dp_mpllb_force_en to "1". This

	 * will keep the PLL running during the DDI lane programming and any

	 * typeC DP cable disconnect. Do not set the force before enabling the

	 * PLL because that will start the PLL before it has sampled the

	 * divider values.

	/*

	 * 10. Software polls on register DPLL_ENABLE [PLL Lock] to confirm PLL

	 * is locked at new settings. This register bit is sampling PHY

	 * dp_mpllb_state interface signal.

	/*

	 * 11. If the frequency will result in a change to the voltage

	 * requirement, follow the Display Voltage Frequency Switching -

	 * Sequence After Frequency Change.

	 *

	 * We handle this step in bxt_set_cdclk().

	/*

	 * 1. If the frequency will result in a change to the voltage

	 * requirement, follow the Display Voltage Frequency Switching -

	 * Sequence Before Frequency Change.

	 *

	 * We handle this step in bxt_set_cdclk().

 2. Software programs DPLL_ENABLE [PLL Enable] to "0" */

	/*

	 * 4. Software programs SNPS_PHY_MPLLB_DIV dp_mpllb_force_en to "0".

	 * This will allow the PLL to stop running.

	/*

	 * 5. Software polls DPLL_ENABLE [PLL Lock] for PHY acknowledgment

	 * (dp_txX_ack) that the new transmitter setting request is completed.

	/*

	 * 6. If the frequency will result in a change to the voltage

	 * requirement, follow the Display Voltage Frequency Switching -

	 * Sequence After Frequency Change.

	 *

	 * We handle this step in bxt_set_cdclk().

	/*

	 * REF_CONTROL is under firmware control and never programmed by the

	 * driver; we read it only for sanity checking purposes.  The bspec

	 * only tells us the expected value for one field in this register,

	 * so we'll only read out those specific bits here.

	/*

	 * MPLLB_DIV is programmed twice, once with the software-computed

	 * state, then again with the MPLLB_FORCE_EN bit added.  Drop that

	 * extra bit during readout so that we return the actual expected

	 * software state.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 Disable the VGA plane that we never use */

 WaEnableVGAAccessThroughIOPort:ctg,elk,ilk,snb,ivb,vlv,hsw */

	/*

	 * This function can be called both from intel_modeset_setup_hw_state or

	 * at a very early point in our resume sequence, where the power well

	 * structures are not yet restored. Since this function is at a very

	 * paranoid "someone might have enabled VGA while we were not looking"

	 * level, just check if the power well is enabled instead of trying to

	 * follow the "don't touch the power well if we don't need it" policy

	 * the rest of the driver uses.

	/*

	 * After we re-enable the power well, if we touch VGA register 0x3d5

	 * we'll get unclaimed register interrupts. This stops after we write

	 * anything to the VGA MSR register. The vgacon module uses this

	 * register all the time, so if we unbind our driver and, as a

	 * consequence, bind vgacon, we'll get stuck in an infinite loop at

	 * console_unlock(). So make here we touch the VGA MSR register, making

	 * sure vgacon can keep working normally without triggering interrupts

	 * and error messages.

	/*

	 * If we have > 1 VGA cards, then we need to arbitrate access to the

	 * common VGA resources.

	 *

	 * If we are a secondary display controller (!PCI_DISPLAY_CLASS_VGA),

	 * then we do not take part in VGA arbitration and the

	 * vga_client_register() fails with -ENODEV.

/*

 * Copyright Â© 2018 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *   Madhav Chauhan <madhav.chauhan@intel.com>

 *   Jani Nikula <jani.nikula@intel.com>

 wait for header/payload credits to be released */

 send nop DCS command */

 wait for header credits to be released */

 wait for LP TX in progress bit to be cleared */

 payload queue can accept *256 bytes*, check limit */

	/*

	 * case 1 also covers dual link

	 * In case of dual link, frame update should be set on

	 * DSI_0

		/*

		 * Program voltage swing and pre-emphasis level values as per

		 * table in BSPEC under DDI buffer programing

 Bspec: must not use GRP register for write */

 Interleave */

 aka DSI 8X clock */

 Step 4b(i) set loadgen select for transmit and aux lanes */

 Step 4b(ii) set latency optimization for transmit and aux lanes */

 For EHL, TGL, set latency optimization for PCS_DW1 lanes */

 clear common keeper enable bit */

	/*

	 * Set SUS Clock Config bitfield to 11b

	 * Note: loadgen select program is done

	 * as part of lane phy sequence configuration

 Clear training enable to change swing values */

 Program swing and de-emphasis */

 Set training enable to trigger update */

 Program T-INIT master registers */

 Program DPHY clock lanes timings */

 shadow register inside display core */

 Program DPHY data lanes timings */

 shadow register inside display core */

	/*

	 * If DSI link operating at or below an 800 MHz,

	 * TA_SURE should be override and programmed to

	 * a value '0' inside TA_PARAM_REGISTERS otherwise

	 * leave all fields at HW default values.

 shadow register inside display core */

 enable link calibration if freq > 1.5Gbps */

 configure continuous clock */

 configure buffer threshold limit to minimum */

 set virtual channel to '0' */

 program BGR transmission */

 select pixel format */

 program DSI operation mode */

			/*

			 * FIXME: Retrieve this info from VBT.

			 * As per the spec when dsi transcoder is operating

			 * in TE GATE mode, TE comes from GPIO

			 * which is UTIL PIN for DSI 0.

			 * Also this GPIO would not be used for other

			 * purposes is an assumption.

 enable port sync mode if dual link */

 configure stream splitting */

 select data lane width */

 select input pipe */

 enable DDI buffer */

 wait for link ready */

 horizontal timings */

 vertical timings */

	/*

	 * Adjust horizontal timings (htotal, hsync_start, hsync_end) to account

	 * for slower link speed if DSC is enabled.

	 *

	 * The compression frequency ratio is the ratio between compressed and

	 * non-compressed link speeds, and simplifies down to the ratio between

	 * compressed and non-compressed bpp.

 minimum hactive as per bspec: 256 pixels */

 if RGB666 format, then hactive must be multiple of 4 pixels */

 program TRANS_HTOTAL register */

 TRANS_HSYNC register to be programmed only for video mode */

 BSPEC: hsync size should be atleast 16 pixels */

 program TRANS_VTOTAL register */

		/*

		 * FIXME: Programing this by assuming progressive mode, since

		 * non-interlaced info from VBT is not saved inside

		 * struct drm_display_mode.

		 * For interlace mode: program required pixel minus 2

 program TRANS_VSYNC register for video mode only */

	/*

	 * FIXME: It has to be programmed only for video modes and interlaced

	 * modes. Put the check condition here once interlaced

	 * info available as described above.

	 * program TRANS_VSYNCSHIFT register

 program TRANS_VBLANK register, should be same as vtotal programmed */

 wait for transcoder to be enabled */

	/*

	 * escape clock count calculation:

	 * BYTE_CLK_COUNT = TIME_NS/(8 * UI)

	 * UI (nsec) = (10^6)/Bitrate

	 * TIME_NS = (BYTE_CLK_COUNT * 8 * 10^6)/ Bitrate

	 * ESCAPE_CLK_COUNT  = TIME_NS/ESC_CLK_NS

 program hst_tx_timeout */

 FIXME: DSI_CALIB_TO */

 program lp_rx_host timeout */

 FIXME: DSI_PWAIT_TO */

 program turn around timeout */

	/*

	 * used as TE i/p for DSI0,

	 * for dual link/DSI1 TE is from slave DSI1

	 * through GPIO.

 step 4a: power up all lanes of the DDI used by DSI */

 step 4b: configure lane sequencing of the Combo-PHY transmitters */

 step 4c: configure voltage swing and skew */

 enable DDI buffer */

 setup D-PHY timings */

 Since transcoder is configured to take events from GPIO */

 step 4h: setup DSI protocol timeouts */

 Step (4h, 4i, 4j, 4k): Configure transcoder */

 Step 4l: Gate DDI clocks */

 set maximum return packet size */

		/*

		 * FIXME: This uses the number of DW's currently in the payload

		 * receive queue. This is probably not what we want here.

 multiply "Number Rx Payload DW" by 4 to get max value */

 panel power on related mipi dsi vbt sequences */

 ensure all panel commands dispatched before enabling transcoder */

 step2: enable IO power */

 step3: enable DSI PLL */

 step3b */

 step4: enable DSI port and DPHY */

 step5: program and powerup panel */

 step6c: configure transcoder timings */

/*

 * Wa_1409054076:icl,jsl,ehl

 * When pipe A is disabled and MIPI DSI is enabled on pipe B,

 * the AMT KVMR feature will incorrectly see pipe A as enabled.

 * Set 0x42080 bit 23=1 before enabling DSI on pipe B and leave

 * it set while DSI is enabled on pipe B

/*

 * Wa_16012360555:adl-p

 * SW will have to program the "LP to HS Wakeup Guardband"

 * to account for the repeaters on the HS Request/Ready

 * PPI signaling between the Display engine and the DPHY.

 Wa_1409054076:icl,jsl,ehl */

 Wa_16012360555:adl-p */

 step6d: enable dsi transcoder */

 step7: enable backlight */

 disable transcoder */

 wait for transcoder to be disabled */

 ensure cmds dispatched to panel */

 disable periodic update mode */

 put dsi link in ULPS */

 disable ddi function */

 disable port sync mode if dual link */

 set mode to DDI */

 step1: turn off backlight */

 step2d,e: disable transcoder and wait */

 Wa_1409054076:icl,jsl,ehl */

 step2f,g: powerdown panel */

 step2h,i,j: deconfig trancoder */

 step3: disable port */

 step4: disable IO power */

 FIXME: DSC? */

 Get the details on which TE should be enabled */

 wa verify 1409054076:icl,jsl,ehl */

 FIXME: split only when necessary */

 FIXME: initialize from VBT */

 DSI specific sanity checks on the common code */

 Dual link goes to trancoder DSI'0' */

	/*

	 * In case of TE GATE cmd mode, we

	 * receive TE from the slave if

	 * dual link is enabled

 only long packet contains payload */

 send packet header */

TODO: add payload receive code if needed

	/*

	 * prepare cnt in escape clocks

	 * this field represents a hexadecimal value with a precision

	 * of 1.2 â i.e. the most significant bit is the integer

	 * and the least significant 2 bits are fraction bits.

	 * so, the field can represent a range of 0.25 to 1.75

 clk zero count in escape clocks */

 trail cnt in escape clocks*/

 tclk pre count in escape clocks */

 tclk post count in escape clocks */

 hs zero cnt in escape clocks */

 hs exit zero cnt in escape clocks */

 clock lane dphy timings */

 data lanes dphy timings */

 register DSI encoder with DRM subsystem */

 register DSI connector with DRM subsystem */

 attach connector to encoder */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 Parameters for Qclk Geyserville (QGV) */

 clock in multiples of 16.6666 MHz */

 6 * 16.666 MHz = 100 MHz */

 8 * 16.666 MHz = 133 MHz */

 bspec says to keep retrying for at least 1 ms */

 multiples of 16.666MHz (100/6) */

	/*

	 * clk is multiples of 16.666MHz (100/6)

	 * According to BSpec PSF GV bandwidth is

	 * calculated as BW = 64 * clk * 16.666Mhz

 GB/s */

 GB/s */

 GB/s */

 GB/s */

 GB/s */

 assume y tile may be used */

 60% */

			/*

			 * Max row cycle time

			 *

			 * FIXME what is the logic behind the

			 * assumed burst length?

	/*

	 * In case if SAGV is disabled in BIOS, we always get 1

	 * SAGV point, but we can't send PCode commands to restrict it

	 * as it will fail and pointless anyway.

	/*

	 * DG2 doesn't have SAGV or QGV points, just a constant max bandwidth

	 * that doesn't depend on the number of planes enabled.  Create a

	 * single dummy QGV point to reflect that.  DG2-G10 platforms have a

	 * constant 50 GB/s bandwidth, whereas DG2-G11 platforms have 38 GB/s.

	/*

	 * Let's return max bw for 0 planes

		/*

		 * Pcode will not expose all QGV points when

		 * SAGV is forced to off/min/med/max.

	/*

	 * We assume cursors are small enough

	 * to not not cause bandwidth problems.

		/*

		 * We assume cursors are small enough

		 * to not not cause bandwidth problems.

			/*

			 * FIXME: To calculate that more properly we probably

			 * need to to split per plane data_rate into data_rate_y

			 * and data_rate_uv for multiplanar formats in order not

			 * to get accounted those twice if they happen to reside

			 * on different slices.

			 * However for pre-icl this would work anyway because

			 * we have only single slice and for icl+ uv plane has

			 * non-zero data rate.

			 * So in worst case those calculation are a bit

			 * pessimistic, which shouldn't pose any significant

			 * problem anyway.

			/*

			 * Current experimental observations show that contrary

			 * to BSpec we get underruns once we exceed 64 * CDCLK

			 * for slices in total.

			 * As a temporary measure in order not to keep CDCLK

			 * bumped up all the time we calculate CDCLK according

			 * to this formula for  overall bw consumed by slices.

 FIXME earlier gens need some checks too */

	/*

	 * We can _not_ use the whole ADLS_QGV_PT_MASK here, as PCode rejects

	 * it with failure if we try masking any unadvertised points.

	 * So need to operate only with those returned from PCode.

		/*

		 * Avoid locking the bw state when

		 * nothing significant has changed.

		/*

		 * We need to know which qgv point gives us

		 * maximum bandwidth in order to disable SAGV

		 * if we find that we exceed SAGV block time

		 * with watermarks. By that moment we already

		 * have those, as it is calculated earlier in

		 * intel_atomic_check,

	/*

	 * BSpec states that we always should have at least one allowed point

	 * left, so if we couldn't - simply reject the configuration for obvious

	 * reasons.

	/*

	 * Leave only single point with highest bandwidth, if

	 * we can't enable SAGV due to the increased memory latency it may

	 * cause.

	/*

	 * We store the ones which need to be masked as that is what PCode

	 * actually accepts as a parameter.

	/*

	 * If the actual mask had changed we need to make sure that

	 * the commits are serialized(in case this is a nomodeset, nonblocking)

/*

 * Copyright Â© 2006-2007 Intel Corporation

 * Copyright (c) 2006 Dave Airlie <airlied@linux.ie>

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 *      Dave Airlie <airlied@linux.ie>

 *      Jesse Barnes <jesse.barnes@intel.com>

 Private structure for the integrated LVDS support */

 100us units */

 asserts want to know the pipe even if the port is disabled */

 gen2/3 store dither state in pfit control, needs to match */

	/*

	 * Remove the BSpec specified +1 (100ms) offset that accounts for a

	 * too short power-cycle delay due to the asynchronous programming of

	 * the register.

 Convert from 100ms to 100us units */

 Set T2 to 40ms and T5 to 200ms in 100 usec units */

 Set T3 to 35ms and Tx to 200ms in 100 usec units */

 set the corresponsding LVDS_BORDER bit */

	/*

	 * Set the B0-B3 data pairs corresponding to whether we're going to

	 * set the DPLLs for dual-channel mode or not.

	/*

	 * It would be nice to set 24 vs 18-bit mode (LVDS_A3_POWER_UP)

	 * appropriately here, but we need to look more thoroughly into how

	 * panels behave in the two modes. For now, let's just maintain the

	 * value we got from the BIOS.

	/*

	 * Set the dithering flag on LVDS as needed, note that there is no

	 * special lvds dither control bit on pch-split platforms, dithering is

	 * only controlled through the PIPECONF reg.

		/*

		 * Bspec wording suggests that LVDS port dithering only exists

		 * for 18bpp panels.

/*

 * Sets the power state for the panel.

 Should never happen!! */

	/*

	 * We have timings from the BIOS for the panel, put them in

	 * to the adjusted mode.  The CRTC will be set up for this mode,

	 * with the panel scaling set up to source from the H/VDisplay

	 * of the original mode.

	/*

	 * XXX: It would be nice to support lower refresh rates on the

	 * panels to reduce power consumption, and perhaps match the

	 * user's requested refresh rate.

/*

 * Return the list of DDC modes if available, or the BIOS fixed mode otherwise.

 use cached edid if we have one */

 These systems claim to have LVDS, but really don't */

 terminating entry */

 terminating entry */

 use the module option value if specified */

 single channel LVDS is limited to 112 MHz */

	/*

	 * BIOS should set the proper LVDS register value at boot, but

	 * in reality, it doesn't set the value when the lid is closed;

	 * we need to check "the value to be set" in VBT when LVDS

	 * register is uninitialized.

/**

 * intel_lvds_init - setup LVDS connectors on this device

 * @dev_priv: i915 device

 *

 * Create the connector, register the LVDS DDC bus, and try to figure out what

 * modes we can display on the LVDS panel (if present).

 Skip init on machines we know falsely report LVDS */

 create the scaling mode property */

	/*

	 * LVDS discovery:

	 * 1) check for EDID on DDC

	 * 2) check for VBT data

	 * 3) check to see if LVDS is already on

	 *    if none of the above, no panel

	/*

	 * Attempt to get the fixed panel mode from DDC.  Assume that the

	 * preferred mode is the right one.

 Failed to get EDID, what about VBT? */

	/*

	 * If we didn't get EDID, try checking if the panel is already turned

	 * on.  If so, assume that whatever is currently programmed is the

	 * correct mode.

 If we still don't have a mode after all that, give up. */

 SPDX-License-Identifier: MIT */

/*

 * Copyright (C) 2017 Google, Inc.

 * Copyright _ 2017-2019, Intel Corporation.

 *

 * Authors:

 * Sean Paul <seanpaul@chromium.org>

 * Ramalingam C <ramalingam.c@intel.com>

 For HDMI this is forced to be 0x0. For DP SST also this is 0x0. */

/*

 * intel_hdcp_required_content_stream selects the most highest common possible HDCP

 * content_type for all streams in DP MST topology because security f/w doesn't

 * have any provision to mark content_type for each stream separately, it marks

 * all available streams with the content_type proivided at the time of port

 * authentication. This may prohibit the userspace to use type1 content on

 * HDCP 2.2 capable sink because of other sink are not capable of HDCP 2.2 in

 * DP MST topology. Though it is not compulsory, security fw should change its

 * policy to mark different content_types for different streams.

 if there is only one active stream */

	/*

	 * Apply common protection level across all streams in DP MST Topology.

	 * Use highest supported content type for all streams in DP MST Topology.

 KSV has 20 1's and 20 0's */

 HDCP spec states that we must retry the bksv if it is invalid */

 Is HDCP1.4 capable on Platform and Sink */

 Is HDCP2.2 capable on Platform and Sink */

 I915 support for HDCP2.2 */

 MEI interface is solid */

 Sink's capability for HDCP2.2 */

 Poll for ksv list ready (spec says max time allowed is 5s) */

	/*

	 * On HSW and BDW, Display HW loads the Key as soon as Display resumes.

	 * On all BXT+, SW can load the keys only when the PW#1 is turned on.

 PG1 (power well #1) needs to be enabled */

	/*

	 * Another req for hdcp key loadability is enabled state of pll for

	 * cdclk. Without active crtc we wont land here. So we are assuming that

	 * cdclk is already on.

	/*

	 * On HSW and BDW HW loads the HDCP1.4 Key when Display comes

	 * out of reset. So if Key is not already loaded, its an error state.

	/*

	 * Initiate loading the HDCP key from fuses.

	 *

	 * BXT+ platforms, HDCP key needs to be loaded by SW. Only display

	 * version 9 platforms (minus BXT) differ in the key load trigger

	 * process from other platforms. These platforms use the GT Driver

	 * Mailbox interface.

 Wait for the keys to load (500us) */

 Send Aksv over to PCH display for use in authentication */

 Returns updated SHA-1 index */

 Process V' values from the receiver */

	/*

	 * We need to write the concatenation of all device KSVs, BINFO (DP) ||

	 * BSTATUS (HDMI), and M0 (which is added via HDCP_REP_CTL). This byte

	 * stream is written via the HDCP_SHA_TEXT register in 32-bit

	 * increments. Every 64 bytes, we need to write HDCP_REP_CTL again. This

	 * index will keep track of our progress through the 64 bytes as well as

	 * helping us work the 40-bit KSVs through our 32-bit register.

	 *

	 * NOTE: data passed via HDCP_SHA_TEXT should be big-endian

 Fill up the empty slots in sha_text and write it out */

 Programming guide writes this every 64 bytes */

 Store the leftover bytes from the ksv in sha_text */

		/*

		 * If we still have room in sha_text for more data, continue.

		 * Otherwise, write it out immediately.

	/*

	 * We need to write BINFO/BSTATUS, and M0 now. Depending on how many

	 * bytes are leftover from the last ksv, we might be able to fit them

	 * all in sha_text (first 2 cases), or we might need to split them up

	 * into 2 writes (last 2 cases).

 Write 16 bits of text, 16 bits of M0 */

 Write 32 bits of M0 */

 Write 16 bits of M0 */

 Write 24 bits of text, 8 bits of M0 */

 Only 24-bits of data, must be in the LSB */

 Write 32 bits of M0 */

 Write 24 bits of M0 */

 Write 32 bits of text */

 Write 64 bits of M0 */

		/*

		 * Terminate the SHA-1 stream by hand. For the other leftover

		 * cases this is appended by the hardware.

 Write 32 bits of text (filled from LSB) */

 Write 8 bits of text (filled from LSB), 24 bits of M0 */

 Write 32 bits of M0 */

 Write 8 bits of M0 */

 Fill up to 64-4 bytes with zeros (leave the last write for length) */

	/*

	 * Last write gets the length of the concatenation in bits. That is:

	 *  - 5 bytes per device

	 *  - 10 bytes for BINFO/BSTATUS(2), M0(8)

 Tell the HW we're done with the hash and wait for it to ACK */

 Implements Part 2 of the HDCP authorization procedure */

	/*

	 * When repeater reports 0 device count, HDCP1.4 spec allows disabling

	 * the HDCP encryption. That implies that repeater can't have its own

	 * display. As there is no consumption of encrypted content in the

	 * repeater with 0 downstream devices, we are failing the

	 * authentication.

	/*

	 * When V prime mismatches, DP Spec mandates re-read of

	 * V prime atleast twice.

 Implements Part 1 of the HDCP authorization procedure */

	/*

	 * Detects whether the display is HDCP capable. Although we check for

	 * valid Bksv below, the HDCP over DP spec requires that we check

	 * whether the display supports HDCP before we write An. For HDMI

	 * displays, this is not necessary.

 Initialize An with 2 random values and acquire it */

 Wait for An to be acquired */

 Wait for R0 ready */

	/*

	 * Wait for R0' to become available. The spec says 100ms from Aksv, but

	 * some monitors can take longer than this. We'll set the timeout at

	 * 300ms just to be sure.

	 *

	 * On DP, there's an R0_READY bit available but no such bit

	 * exists on HDMI. Since the upper-bound is the same, we'll just do

	 * the stupid thing instead of polling on one and not the other.

	/*

	 * DP HDCP Spec mandates the two more reattempt to read R0, incase

	 * of R0 mismatch.

 Wait for Ri prime match */

 Wait for encryption confirmation */

 DP MST Auth Part 1 Step 2.a and Step 2.b */

		/*

		 * If there are other connectors on this port using HDCP,

		 * don't disable it until it disabled HDCP encryption for

		 * all connectors in MST topology.

 Incase of authentication failures, HDCP spec expects reauth. */

 Ensuring HDCP encryption and signalling are stopped. */

 Implements Part 3 of the HDCP authorization procedure */

 Check_link valid only when HDCP1.4 is enabled */

	/*

	 * This worker is only used to flip between ENABLED/DESIRED. Either of

	 * those to UNDESIRED is handled by core. If value == UNDESIRED,

	 * we're running just after hdcp has been disabled, so just exit

 Authentication flow starts from here */

 Init for seq_num */

	/*

	 * Here msgs.no_stored_km will hold msgs corresponding to the km

	 * stored also.

 Pairing is required */

 Prepare RepeaterAuth_Stream_Manage msg */

 Send it to Repeater */

	/*

	 * MST topology is not Type 1 capable if it contains a downstream

	 * device that is only HDCP 1.x or Legacy HDCP 2.0/2.1 compliant.

 Converting and Storing the seq_num_v to local variable as DWORD */

 Roll over of the seq_num_v from repeater. Reauthenticate. */

 Link is Authenticated. Now set for Encryption */

 Lets restart the auth incase of seq_num_m roll over */

 Clearing the mei hdcp session */

		/*

		 * Ensuring the required 200mSec min time interval between

		 * Session Key Exchange and encryption.

 Implements the Link Integrity Check for HDCP2.2 */

 hdcp2_check_link is expected only when HDCP2.2 is Enabled */

 eDP, DSI TRANSCODERS are non HDCP capable */

		/*

		 * As per ME FW API expectation, for GEN 12+, fw_ddi is filled

		 * with zero(INVALID PORT index).

	/*

	 * As associated transcoder is set and modified at modeset, here fw_tc

	 * is initialized to zero (invalid transcoder index). This will be

	 * retained for <Gen12 forever.

 For SST */

	/*

	 * Considering that HDCP2.2 is more secure than HDCP1.4, If the setup

	 * is capable of HDCP2.2, it is preferred to use HDCP2.2.

	/*

	 * When HDCP2.2 fails and Content Type is not Type1, HDCP1.4 will

	 * be attempted.

	/*

	 * During the HDCP encryption session if Type change is requested,

	 * disable the HDCP and reenable it with new TYPE value.

	/*

	 * Mark the hdcp state as DESIRED after the hdcp disable of type

	 * change procedure.

 Avoid enabling hdcp, if it already ENABLED */

		/*

		 * If HDCP already ENABLED and CP property is DESIRED, schedule

		 * prop_work to update correct CP property to user space.

	/*

	 * If the connector is registered, it's possible userspace could kick

	 * off another HDCP enable, which would re-spawn the workers.

	/*

	 * Now that the connector is not registered, check_work won't be run,

	 * but cancel any outstanding instances of it

	/*

	 * We don't cancel prop_work in the same way as check_work since it

	 * requires connection_mutex which could be held while calling this

	 * function. Instead, we rely on the connector references grabbed before

	 * scheduling prop_work to ensure the connector is alive when prop_work

	 * is run. So if we're in the destroy path (which is where this

	 * function should be called), we're "guaranteed" that prop_work is not

	 * active (tl;dr This Should Never Happen).

		/*

		 * If the connector is being disabled with CP enabled, mark it

		 * desired so it's re-enabled when the connector is brought back

	/*

	 * Fix the HDCP uapi content protection state in case of modeset.

	 * FIXME: As per HDCP content protection property uapi doc, an uevent()

	 * need to be sent if there is transition from ENABLED->DESIRED.

	/*

	 * Nothing to do if the state didn't change, or HDCP was activated since

	 * the last commit. And also no change in hdcp content type.

 Handles the CP_IRQ raised from the DP HDCP sink */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2018 Intel Corporation

 *

 * Author: Gaurav K Singh <gaurav.k.singh@intel.com>

 *         Manasi Navare <manasi.d.navare@intel.com>

 From DSC_v1.11 spec, rc_parameter_Set syntax element typically constant */

/*

 * Selected Rate Control Related Parameter Recommended Values

 * from DSC_v1.11 spec & C Model release: DSC_model_20161212

 6BPP/8BPC */

 6BPP/10BPC */

 6BPP/12BPC */

 6BPP/14BPC */

 6BPP/16BPC */

 8BPP/8BPC */

 8BPP/10BPC */

 8BPP/12BPC */

 8BPP/14BPC */

 8BPP/16BPC */

 10BPP/8BPC */

 10BPP/10BPC */

 10BPP/12BPC */

 10BPP/14BPC */

 10BPP/16BPC */

 12BPP/8BPC */

 12BPP/10BPC */

 12BPP/12BPC */

 12BPP/14BPC */

 12BPP/16BPC */

 15BPP/8BPC */

 15BPP/10BPC */

 15BPP/12BPC */

 15BPP/14BPC */

 15BPP/16BPC */

 On TGL, DSC is supported on all Pipes */

 There's no pipe A DSC engine on ICL */

 Our hw supports only 444 modes as of today */

 initial_xmit_delay = rc_model_size/2/compression_bpp */

 Read range_minqp and range_max_qp from qp tables */

 Calculate range_bgp_offset */

 Gen 11 does not support YCbCr */

 Gen 11 does not support VBR */

 Gen 11 only supports integral values of bpp */

		/*

		 * six 0s are appended to the lsb of each threshold value

		 * internally in h/w.

		 * Only 8 bits are allowed for programming RcBufThreshold

	/*

	 * For 6bpp, RC Buffer threshold 12 and 13 need a different value

	 * as per C Model

	/*

	 * From XE_LPD onwards we supports compression bpps in steps of 1

	 * upto uncompressed bpp-1, hence add calculations for all the rc

	 * parameters

		/*

		 * Range BPG Offset uses 2's complement and is only a 6 bits. So

		 * mask it to get only 6 bits.

	/*

	 * BitsPerComponent value determines mux_word_size:

	 * When BitsPerComponent is less than or 10bpc, muxWordSize will be equal to

	 * 48 bits otherwise 64

 InitialScaleValue is a 6 bit value with 3 fractional bits (U3.3) */

	/*

	 * VDSC/joining uses a separate power well, PW2, and requires

	 * POWER_DOMAIN_TRANSCODER_VDSC_PW2 power domain in two cases:

	 *

	 *  - ICL eDP/DSI transcoder

	 *  - Display version 12 (except RKL) pipe A

	 *

	 * For any other pipe, VDSC/joining uses the power well associated with

	 * the pipe in use. Hence another reference on the pipe power domain

	 * will suffice. (Except no VDSC/joining on ICL pipe A.)

 Populate PICTURE_PARAMETER_SET_0 registers */

		/*

		 * If 2 VDSC instances are needed, configure PPS for second

		 * VDSC

 Populate PICTURE_PARAMETER_SET_1 registers */

		/*

		 * If 2 VDSC instances are needed, configure PPS for second

		 * VDSC

 Populate PICTURE_PARAMETER_SET_2 registers */

		/*

		 * If 2 VDSC instances are needed, configure PPS for second

		 * VDSC

 Populate PICTURE_PARAMETER_SET_3 registers */

		/*

		 * If 2 VDSC instances are needed, configure PPS for second

		 * VDSC

 Populate PICTURE_PARAMETER_SET_4 registers */

		/*

		 * If 2 VDSC instances are needed, configure PPS for second

		 * VDSC

 Populate PICTURE_PARAMETER_SET_5 registers */

		/*

		 * If 2 VDSC instances are needed, configure PPS for second

		 * VDSC

 Populate PICTURE_PARAMETER_SET_6 registers */

		/*

		 * If 2 VDSC instances are needed, configure PPS for second

		 * VDSC

 Populate PICTURE_PARAMETER_SET_7 registers */

		/*

		 * If 2 VDSC instances are needed, configure PPS for second

		 * VDSC

 Populate PICTURE_PARAMETER_SET_8 registers */

		/*

		 * If 2 VDSC instances are needed, configure PPS for second

		 * VDSC

 Populate PICTURE_PARAMETER_SET_9 registers */

		/*

		 * If 2 VDSC instances are needed, configure PPS for second

		 * VDSC

 Populate PICTURE_PARAMETER_SET_10 registers */

		/*

		 * If 2 VDSC instances are needed, configure PPS for second

		 * VDSC

 Populate Picture parameter set 16 */

		/*

		 * If 2 VDSC instances are needed, configure PPS for second

		 * VDSC

 Populate the RC_BUF_THRESH registers */

 Populate the RC_RANGE_PARAMETERS registers */

 Prepare DP SDP PPS header as per DP 1.4 spec, Table 2-123 */

 Fill the PPS payload bytes as per DSC spec 1.2 Table 4-1 */

 Disable only if either of them is enabled */

 FIXME: add more state readout as needed */

 PPS1 */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

/**

 * DOC: display pinning helpers

	/* Note that the w/a also requires 64 PTE of padding following the

	 * bo. We currently fill all unused PTE with the shadow page and so

	 * we should always have valid PTE following the scanout preventing

	 * the VT-d warning.

	/*

	 * Global gtt pte registers are special registers which actually forward

	 * writes to a chunk of system memory. Which means that there is no risk

	 * that the register values disappear as soon as we call

	 * intel_runtime_pm_put(), so it is correct to wrap only the

	 * pin/unpin/fence and not more.

	/*

	 * Valleyview is definitely limited to scanning out the first

	 * 512MiB. Lets presume this behaviour was inherited from the

	 * g4x display engine and that all earlier gen are similarly

	 * limited. Testing suggests that it is a little more

	 * complicated than this. For example, Cherryview appears quite

	 * happy to scanout from anywhere within its global aperture.

 TODO: Do we need to sync when migration becomes async? */

		/*

		 * Install a fence for tiled scan-out. Pre-i965 always needs a

		 * fence, whereas 965+ only requires a fence if using

		 * framebuffer compression.  For simplicity, we always, when

		 * possible, install a fence as the cost is not that onerous.

		 *

		 * If we fail to fence the tiled scanout, then either the

		 * modeset will reject the change (which is highly unlikely as

		 * the affected systems, all but one, do not have unmappable

		 * space) or we will not be able to enable full powersaving

		 * techniques (also likely not to apply due to various limits

		 * FBC and the like impose on the size of the buffer, which

		 * presumably we violated anyway with this unmappable buffer).

		 * Anyway, it is presumably better to stumble onwards with

		 * something and try to run the system in a "less than optimal"

		 * mode that matches the user configuration.

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Author: Deepak M <m.deepak at intel.com>

/**************************************************************************



Copyright Â© 2006 Dave Airlie



All Rights Reserved.



Permission is hereby granted, free of charge, to any person obtaining a

copy of this software and associated documentation files (the

"Software"), to deal in the Software without restriction, including

without limitation the rights to use, copy, modify, merge, publish,

distribute, sub license, and/or sell copies of the Software, and to

permit persons to whom the Software is furnished to do so, subject to

the following conditions:



The above copyright notice and this permission notice (including the

next paragraph) shall be included in all copies or substantial portions

of the Software.



THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.

IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR

ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,

TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE

SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.



I2CDevRec d;

 Silicon Image 164 driver for chip on i2c bus */

 this will detect the SIL164 chip on the specified i2c bus */

	/* As long as the basics are set up, since we don't have clock

	 * dependencies in the mode setup, we can just leave the

	 * registers alone and everything will work fine.

 recommended programming sequence from doc */

	/*sil164_writeb(sil, 0x08, 0x30);

	  sil164_writeb(sil, 0x09, 0x00);

	  sil164_writeb(sil, 0x0a, 0x90);

	  sil164_writeb(sil, 0x0c, 0x89);

 don't do much */

 set the SIL164 power state */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

/**

 * DOC: Display Refresh Rate Switching (DRRS)

 *

 * Display Refresh Rate Switching (DRRS) is a power conservation feature

 * which enables swtching between low and high refresh rates,

 * dynamically, based on the usage scenario. This feature is applicable

 * for internal panels.

 *

 * Indication that the panel supports DRRS is given by the panel EDID, which

 * would list multiple refresh rates for one resolution.

 *

 * DRRS is of 2 types - static and seamless.

 * Static DRRS involves changing refresh rate (RR) by doing a full modeset

 * (may appear as a blink on screen) and is used in dock-undock scenario.

 * Seamless DRRS involves changing RR without any visual effect to the user

 * and can be used during normal system usage. This is done by programming

 * certain registers.

 *

 * Support for static/seamless DRRS may be indicated in the VBT based on

 * inputs from the panel spec.

 *

 * DRRS saves power by switching to low RR based on usage scenarios.

 *

 * The implementation is based on frontbuffer tracking implementation.  When

 * there is a disturbance on the screen triggered by user activity or a periodic

 * system activity, DRRS is disabled (RR is changed to high RR).  When there is

 * no movement on screen, after a timeout of 1 second, a switch to low RR is

 * made.

 *

 * For integration with frontbuffer tracking code, intel_drrs_invalidate()

 * and intel_drrs_flush() are called.

 *

 * DRRS can be further extended to support other internal panels and also

 * the scenario of video playback wherein RR is set based on the rate

 * requested by userspace.

	/*

	 * DRRS and PSR can't be enable together, so giving preference to PSR

	 * as it allows more power-savings by complete shutting down display,

	 * so to guarantee this, intel_drrs_compute_config() must be called

	 * after intel_psr_compute_config().

 FIXME: abstract this better */

/**

 * intel_drrs_enable - init drrs struct if supported

 * @intel_dp: DP struct

 * @crtc_state: A pointer to the active crtc state.

 *

 * Initializes frontbuffer_bits and drrs.dp

/**

 * intel_drrs_disable - Disable DRRS

 * @intel_dp: DP struct

 * @old_crtc_state: Pointer to old crtc_state.

 *

/**

 * intel_drrs_update - Update DRRS state

 * @intel_dp: Intel DP

 * @crtc_state: new CRTC state

 *

 * This function will update DRRS states, disabling or enabling DRRS when

 * executing fastsets. For full modeset, intel_drrs_disable() and

 * intel_drrs_enable() should be called instead.

 New state matches current one? */

	/*

	 * The delayed work can race with an invalidate hence we need to

	 * recheck.

 flush/invalidate means busy screen hence upclock */

	/*

	 * flush also means no more activity hence schedule downclock, if all

	 * other fbs are quiescent too

/**

 * intel_drrs_invalidate - Disable Idleness DRRS

 * @dev_priv: i915 device

 * @frontbuffer_bits: frontbuffer plane tracking bits

 *

 * This function gets called everytime rendering on the given planes start.

 * Hence DRRS needs to be Upclocked, i.e. (LOW_RR -> HIGH_RR).

 *

 * Dirty frontbuffers relevant to DRRS are tracked in busy_frontbuffer_bits.

/**

 * intel_drrs_flush - Restart Idleness DRRS

 * @dev_priv: i915 device

 * @frontbuffer_bits: frontbuffer plane tracking bits

 *

 * This function gets called every time rendering on the given planes has

 * completed or flip on a crtc is completed. So DRRS should be upclocked

 * (LOW_RR -> HIGH_RR). And also Idleness detection should be started again,

 * if no other planes are dirty.

 *

 * Dirty frontbuffers relevant to DRRS are tracked in busy_frontbuffer_bits.

/**

 * intel_drrs_init - Init basic DRRS work and mutex.

 * @connector: eDP connector

 * @fixed_mode: preferred mode of panel

 *

 * This function is  called only once at driver load to initialize basic

 * DRRS stuff.

 *

 * Returns:

 * Downclock mode if panel supports it, else return NULL.

 * DRRS support is determined by the presence of downclock mode (apart

 * from VBT setting).

/**************************************************************************



Copyright Â© 2006 Dave Airlie



All Rights Reserved.



Permission is hereby granted, free of charge, to any person obtaining a

copy of this software and associated documentation files (the

"Software"), to deal in the Software without restriction, including

without limitation the rights to use, copy, modify, merge, publish,

distribute, sub license, and/or sell copies of the Software, and to

permit persons to whom the Software is furnished to do so, subject to

the following conditions:



The above copyright notice and this permission notice (including the

next paragraph) shall be included in all copies or substantial portions

of the Software.



THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.

IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR

ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,

TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE

SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.



 7010 as well */

/** @file

 * driver for the Chrontel 7xxx DVI chip over DVO.

* Reads an 8 bit register */

* Writes an 8 bit register */

 this will detect the CH7xxx chip on the specified i2c bus */

 set the CH7xxx power state */

/*

 * Copyright Â© 2012 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Eugeni Dodonov <eugeni.dodonov@intel.com>

 *

/*

 * Starting with Haswell, DDI port buffers must be programmed with correct

 * values in advance. This function programs the correct values for

 * DP/eDP/FDI use cases.

 If we're boosting the current, set bit 31 of trans1 */

/*

 * Starting with Haswell, DDI port buffers must be programmed with correct

 * values in advance. This function programs the correct values for

 * HDMI/DVI use cases.

 If we're boosting the current, set bit 31 of trans1 */

 Entry 9 is for HDMI: */

 Wait > 518 usecs for DDI_BUF_CTL to be non idle */

		/*

		 * DPLL_ID_ICL_DPLL0 and DPLL_ID_ICL_DPLL1 should not be used

		 * here, so do warn if this get passed in

 DDI_BUF_CTL_ENABLE will be set by intel_ddi_prepare_link_retrain() later */

 nonsense combination */

	/*

	 * As per DP 1.2 spec section 2.3.4.3 while sending

	 * YCBCR 444 signals we should program MSA MISC1/0 fields with

	 * colorspace information.

	/*

	 * As per DP 1.4a spec section 2.2.4.3 [MSA Field for Indication

	 * of Color Encoding Format and Content Color Gamut] while sending

	 * YCBCR 420, HDR BT.2020 signals we should program MSA MISC1 fields

	 * which indicate VSC SDP for the Pixel Encoding/Colorimetry Format.

/*

 * Returns the TRANS_DDI_FUNC_CTL value based on CRTC state.

 *

 * Only intended to be used by intel_ddi_enable_transcoder_func() and

 * intel_ddi_config_transcoder_func().

 Enable TRANS_DDI_FUNC_CTL for the pipe to work in HDMI mode */

			/* On Haswell, can only use the always-on power well for

			 * eDP when not using the panel fitter, and when not

			 * using motion blur mitigation (which we don't

/*

 * Same as intel_ddi_enable_transcoder_func(), but it does not set the enable

 * bit.

 Quirk time at 100ms for reliable operation */

		/* if the transcoder is in MST state then

 128b/132b */

 FDI */

	/* ICL+ HW requires corresponding AUX IOs to be powered up for PSR with

	 * DC states enabled at the same time, while for driver initiated AUX

	 * transfers we need the same AUX IOs to be powered but with DC states

	 * disabled. Accordingly use the AUX power domain here which leaves DC

	 * states enabled.

	 * However, for non-A AUX ports the corresponding non-EDP transcoders

	 * would have already enabled power well 2 and DC_OFF. This means we can

	 * acquire a wider POWER_DOMAIN_AUX_{B,C,D,F} reference instead of a

	 * specific AUX_IO reference without powering up any extra wells.

	 * Note that PSR is enabled only on Port A even though this function

	 * returns the correct domain for other ports too.

	/*

	 * TODO: Add support for MST encoders. Atm, the following should never

	 * happen since fake-MST encoders don't set their get_power_domains()

	 * hook.

	/*

	 * AUX power is only needed for (e)DP mode, and for HDMI mode on TC

	 * ports.

 Make sure that the requested I_boost is valid */

/*

 * We assume that the full set of pre-emphasis values can be

 * used on all DDI platforms. Should that change we need to

 * rethink this code.

 Set PORT_TX_DW5 */

 Program PORT_TX_DW2 */

 Program Rcomp scalar for every table entry */

 Program PORT_TX_DW4 */

 We cannot write to GRP. It would overwrite individual loadgen. */

 Program PORT_TX_DW7 */

	/*

	 * 1. If port type is eDP or DP,

	 * set PORT_PCS_DW1 cmnkeeper_enable to 1b,

	 * else clear to 0b.

 2. Program loadgen select */

	/*

	 * Program PORT_TX_DW4 depending on Bit rate and used lanes

	 * <= 6 GHz and 4 lanes (LN0=0, LN1=1, LN2=1, LN3=1)

	 * <= 6 GHz and 1,2 lanes (LN0=0, LN1=1, LN2=1, LN3=0)

	 * > 6 GHz (LN0=0, LN1=0, LN2=0, LN3=0)

 3. Set PORT_CL_DW5 SUS Clock Config to 11b */

 4. Clear training enable to change swing values */

 5. Program swing and de-emphasis */

 6. Set training enable to trigger update */

 Set MG_TX_LINK_PARAMS cri_use_fs32 to 0. */

 Program MG_TX_SWINGCTRL with values from vswing table */

 Program MG_TX_DRVCTRL with values from vswing table */

 FIXME: Program CRI_LOADGEN_SEL after the spec is updated */

	/*

	 * Program MG_CLKHUB<LN, port being used> with value from frequency table

	 * In case of Legacy mode on MG PHY, both TX1 and TX2 enabled so use the

	 * values from table for which TX1 and TX2 enabled.

 Program the MG_TX_DCC<LN, port being used> based on the link frequency */

 Program MG_TX_PISO_READLOAD with values from vswing table */

 All the registers are RMW */

 HDMI ignores the rest */

	/*

	 * "This step and the step before must be

	 *  done with separate register writes."

	/*

	 * If we fail this, something went very wrong: first 2 PLLs should be

	 * used by first 2 phys and last 2 PLLs by last phys

	/*

	 * _DG1_DPCLKA0_CFGCR0 maps between DPLL 0 and 1 with one bit for phy A

	 * and B while _DG1_DPCLKA1_CFGCR0 maps between DPLL 2 and 3 with one

	 * bit for phy C and D.

	/*

	 * "For DDIC and DDID, program DDI_CLK_SEL to map the MG clock to the port.

	 *  MG does not exist, but the programming is required to ungate DDIC and DDID."

	/*

	 * FIXME Not sure if the override affects both

	 * the PLL selection and the CLK_OFF bit.

	/*

	 * FIXME Not sure if the override affects both

	 * the PLL selection and the CLK_OFF bit.

	/*

	 * In case of DP MST, we sanitize the primary encoder only, not the

	 * virtual ones.

		/*

		 * In the unlikely case that BIOS enables DP in MST mode, just

		 * warn since our MST HW readout is incomplete.

		/*

		 * Sanity check that we haven't incorrectly registered another

		 * encoder using any of the ports of this DSI encoder.

		/*

		 * For DSI we keep the ddi clocks gated

		 * except during enable/disable sequence.

 DPPATC */

 Splitter enable for eDP MSO is limited to certain pipes. */

	/*

	 * We only configure what the register value will be here.  Actual

	 * enabling happens during link training farther down.

	/*

	 * 1. Enable Power Wells

	 *

	 * This was handled at the beginning of intel_atomic_commit_tail(),

	 * before we called down into this function.

 2. Enable Panel Power if PPS is required */

	/*

	 * 3. Enable the port PLL.

 4. Enable IO power */

	/*

	 * 5. The rest of the below are substeps under the bspec's "Enable and

	 * Train Display Port" step.  Note that steps that are specific to

	 * MST will be handled by intel_mst_pre_enable_dp() before/after it

	 * calls into this function.  Also intel_mst_pre_enable_dp() only calls

	 * us when active_mst_links==0, so any steps designated for "single

	 * stream or multi-stream master transcoder" can just be performed

	 * unconditionally here.

	/*

	 * 5.a Configure Transcoder Clock Select to direct the Port clock to the

	 * Transcoder.

 5.b Configure transcoder for DP 2.0 128b/132b */

	/*

	 * 5.c Configure TRANS_DDI_FUNC_CTL DDI Select, DDI Mode Select & MST

	 * Transport Select

	/*

	 * 5.d Configure & enable DP_TP_CTL with link training pattern 1

	 * selected

	 *

	 * This will be handled by the intel_dp_start_link_train() farther

	 * down this function.

 5.e Configure voltage swing and related IO settings */

	/*

	 * DDI FEC: "anticipates enabling FEC encoding sets the FEC_READY bit

	 * in the FEC_CONFIGURATION register to 1 before initiating link

	 * training

	/*

	 * 5.h Follow DisplayPort specification training sequence (see notes for

	 *     failure handling)

	 * 5.i If DisplayPort multi-stream - Set DP_TP_CTL link training to Idle

	 *     Pattern, wait for 5 idle patterns (DP_TP_STATUS Min_Idles_Sent)

	 *     (timeout after 800 us)

 5.j Set DP_TP_CTL link training to Normal */

 5.k Configure and enable FEC if needed */

	/*

	 * We only configure what the register value will be here.  Actual

	 * enabling happens during link training farther down.

	/*

	 * 1. Enable Power Wells

	 *

	 * This was handled at the beginning of intel_atomic_commit_tail(),

	 * before we called down into this function.

 2. Enable Panel Power if PPS is required */

	/*

	 * 3. For non-TBT Type-C ports, set FIA lane count

	 * (DFLEXDPSP.DPX4TXLATC)

	 *

	 * This was done before tgl_ddi_pre_enable_dp by

	 * hsw_crtc_enable()->intel_encoders_pre_pll_enable().

	/*

	 * 4. Enable the port PLL.

	 *

	 * The PLL enabling itself was already done before this function by

	 * hsw_crtc_enable()->intel_enable_shared_dpll().  We need only

	 * configure the PLL to port mapping here.

 5. If IO power is controlled through PWR_WELL_CTL, Enable IO Power */

 6. Program DP_MODE */

	/*

	 * 7. The rest of the below are substeps under the bspec's "Enable and

	 * Train Display Port" step.  Note that steps that are specific to

	 * MST will be handled by intel_mst_pre_enable_dp() before/after it

	 * calls into this function.  Also intel_mst_pre_enable_dp() only calls

	 * us when active_mst_links==0, so any steps designated for "single

	 * stream or multi-stream master transcoder" can just be performed

	 * unconditionally here.

	/*

	 * 7.a Configure Transcoder Clock Select to direct the Port clock to the

	 * Transcoder.

	/*

	 * 7.b Configure TRANS_DDI_FUNC_CTL DDI Select, DDI Mode Select & MST

	 * Transport Select

	/*

	 * 7.c Configure & enable DP_TP_CTL with link training pattern 1

	 * selected

	 *

	 * This will be handled by the intel_dp_start_link_train() farther

	 * down this function.

 7.e Configure voltage swing and related IO settings */

	/*

	 * 7.f Combo PHY: Configure PORT_CL_DW10 Static Power Down to power up

	 * the used lanes of the DDI.

	/*

	 * 7.g Program CoG/MSO configuration bits in DSS_CTL1 if selected.

	/*

	 * DDI FEC: "anticipates enabling FEC encoding sets the FEC_READY bit

	 * in the FEC_CONFIGURATION register to 1 before initiating link

	 * training

	/*

	 * 7.i Follow DisplayPort specification training sequence (see notes for

	 *     failure handling)

	 * 7.j If DisplayPort multi-stream - Set DP_TP_CTL link training to Idle

	 *     Pattern, wait for 5 idle patterns (DP_TP_STATUS Min_Idles_Sent)

	 *     (timeout after 800 us)

 7.k Set DP_TP_CTL link training to Normal */

 7.l Configure and enable FEC if needed */

	/*

	 * We only configure what the register value will be here.  Actual

	 * enabling happens during link training farther down.

	/* MST will call a setting of MSA after an allocating of Virtual Channel

	 * from MST encoder pre_enable callback.

	/*

	 * When called from DP MST code:

	 * - conn_state will be NULL

	 * - encoder will be the main encoder (ie. mst->primary)

	 * - the main connector associated with this port

	 *   won't be active or linked to a crtc

	 * - crtc_state will be the state of the first stream to

	 *   be activated on this port, and it may not be the same

	 *   stream that will be deactivated last, but each stream

	 *   should have a state that is identical when it comes to

	 *   the DP link parameteres

 FIXME precompute everything properly */

 FIXME how do we turn infoframes off again? */

 Disable FEC in DP Sink */

	/*

	 * Power down sink before disabling the port, otherwise we end

	 * up getting interrupts from the sink on detecting link loss.

	/*

	 * From TGL spec: "If single stream or multi-stream master transcoder:

	 * Configure Transcoder Clock select to direct no clock to the

	 * transcoder"

	/*

	 * When called from DP MST code:

	 * - old_conn_state will be NULL

	 * - encoder will be the main encoder (ie. mst->primary)

	 * - the main connector associated with this port

	 *   won't be active or linked to a crtc

	 * - old_crtc_state will be the state of the last stream to

	 *   be deactivated on this port, and it may not be the same

	 *   stream that was activated last, but each stream

	 *   should have a state that is identical when it comes to

	 *   the DP link parameteres

	/*

	 * Bspec lists this as both step 13 (before DDI_BUF_CTL disable)

	 * and step 18 (after clearing PORT_CLK_SEL). Based on a BUN,

	 * step 13 is the correct place for it. Step 18 is where it was

	 * originally before the BUN.

 Display WA #1143: skl,kbl,cfl */

		/*

		 * For some reason these chicken bits have been

		 * stuffed into a transcoder register, event though

		 * the bits affect a specific DDI port rather than

		 * a specific transcoder.

	/* In HDMI/DVI mode, the port width, and swing/emphasis values

	 * are ignored so nothing special needs to be done besides

	 * enabling the port.

	 *

	 * On ADL_P the PHY link rate and lane count must be programmed but

	 * these are both 0 for HDMI.

 Enable hdcp if it's desired */

 Disable the decompression in DP Sink */

 Disable Ignore_MSA bit in DP Sink */

		/*

		 * Program the lane count for static/dynamic connections on

		 * Type-C ports.  Skip this step for TBT.

	/*

	 * Until TGL on PORT_A we can have only eDP in SST mode. There the only

	 * reason we need to set idle transmission mode is to work around a HW

	 * issue where we enable the pipe while not in idle link-training mode.

	 * In this case there is requirement to wait for a minimum number of

	 * idle patterns to be sent.

 FDI */

 128b/132b */

 XXX: DSI transcoder paranoia */

 read out pipe settings from master */

 Our own transcoder needs to be disabled when reading it in intel_ddi_read_func_ctl() */

		/*

		 * This is a big fat ugly hack.

		 *

		 * Some machines in UEFI boot mode provide us a VBT that has 18

		 * bpp and 1.62 GHz link bandwidth for eDP, which for reasons

		 * unknown we fail to light up. Yet the same BIOS boots up with

		 * 24 bpp and 2.7 GHz link. Use the same bpp as the BIOS uses as

		 * max, not what it tells us to use.

		 *

		 * Note: This will still be broken if the eDP panel is not lit

		 * up by the BIOS, and thus we can't get the mode at module

		 * load.

 we want an exact match */

	/*

	 * We don't enable port sync on BDW due to missing w/as and

	 * due to not having adjusted the modeset sequence appropriately.

	/*

	 * EDP Transcoders cannot be ensalved

	 * make them a master always when present

	/*

	 * HDMI 2.0 says that one should not send scrambled data

	 * prior to configuring the sink scrambling, and that

	 * TMDS clock/data transmission should be suspended when

	 * changing the TMDS clock rate in the sink. So let's

	 * just do a full modeset here, even though some sinks

	 * would be perfectly happy if were to just reconfigure

	 * the SCDC settings on the fly.

 just do the PHY test and nothing else */

	/*

	 * Unpowered type-c dongles can take some time to boot and be

	 * responsible, so here giving some time to those dongles to power up

	 * and then retrying the probe.

	 *

	 * On many platforms the HDMI live state signal is known to be

	 * unreliable, so we can't use it to detect if a sink is connected or

	 * not. Instead we detect if it's connected based on whether we can

	 * read the EDID or not. That in turn has a problem during disconnect,

	 * since the HPD interrupt may be raised before the DDC lines get

	 * disconnected (due to how the required length of DDC vs. HPD

	 * connector pins are specified) and so we'll still be able to get a

	 * valid EDID. To solve this schedule another detection cycle if this

	 * time around we didn't detect any change in the sink's connection

	 * status.

	 *

	 * Type-c connectors which get their HPD signal deasserted then

	 * reasserted, without unplugging/replugging the sink from the

	 * connector, introduce a delay until the AUX channel communication

	 * becomes functional. Retry the detection for 5 seconds on type-c

	 * connectors to account for this delay.

	/* Broxton/Geminilake: Bspec says that DDI_A_4_LANES is the only

	 *                     supported configuration

 Both A and E share 2 lanes */

	/*

	 * Some BIOS might fail to set this bit on port A if eDP

	 * wasn't lit up at boot.  Force this bit set when needed

	 * so we use the proper lane count for our calculations.

	/*

	 * On platforms with HTI (aka HDPORT), if it's enabled at boot it may

	 * have taken over some of the PHYs and made them unavailable to the

	 * driver.  In that case we should skip initializing the corresponding

	 * outputs.

		/*

		 * Lspcon device needs to be driven with DP connector

		 * with special detection sequence. So make sure DP

		 * is initialized before lspcon.

 BXT/GLK have fixed PLL->port mapping */

	/* In theory we don't need the encoder->type check, but leave it just in

/*

 * Copyright Â© 2008-2015 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

	/*

	 * Detecting LTTPRs must be avoided on platforms with an AUX timeout

	 * period < 3.2ms. (see DP Standard v2.0, 2.11.2, 3.6.6.1).

 The minimum value of LT_TUNABLE_PHY_REPEATER_FIELD_DATA_STRUCTURE_REV is 1.4 */

	/*

	 * Prevent setting LTTPR transparent mode explicitly if no LTTPRs are

	 * detected as this breaks link training at least on the Dell WD19TB

	 * dock.

	/*

	 * See DP Standard v2.0 3.6.6.1. about the explicit disabling of

	 * non-transparent mode and the disable->enable non-transparent mode

	 * sequence.

	/*

	 * In case of unsupported number of LTTPRs or failing to switch to

	 * non-transparent mode fall-back to transparent link training mode,

	 * still taking into account any LTTPR common lane- rate/count limits.

/**

 * intel_dp_init_lttpr_and_dprx_caps - detect LTTPR and DPRX caps, init the LTTPR link training mode

 * @intel_dp: Intel DP struct

 *

 * Read the LTTPR common and DPRX capabilities and switch to non-transparent

 * link training mode if any is detected and read the PHY capabilities for all

 * detected LTTPRs. In case of an LTTPR detection error or if the number of

 * LTTPRs is more than is supported (8), fall back to the no-LTTPR,

 * transparent mode link training mode.

 *

 * Returns:

 *   >0  if LTTPRs were detected and the non-transparent LT mode was set. The

 *       DPRX capabilities are read out.

 *    0  if no LTTPRs or more than 8 LTTPRs were detected or in case of a

 *       detection failure and the transparent LT mode was set. The DPRX

 *       capabilities are read out.

 *   <0  Reading out the DPRX capabilities failed.

 The DPTX shall read the DPRX caps after LTTPR detection. */

	/*

	 * Get voltage_max from the DPTX_PHY (source or LTTPR) upstream from

	 * the DPRX_PHY we train.

	/*

	 * Get preemph_max from the DPTX_PHY (source or LTTPR) upstream from

	 * the DPRX_PHY we train.

 128b/132b */

 8b/10b */

 DP_TRAINING_LANEx_SET follow DP_TRAINING_PATTERN_SET */

 128b/132b */

/*

 * 8b/10b

 *

 * FIXME: The DP spec is very confusing here, also the Link CTS spec seems to

 * have self contradicting tests around this area.

 *

 * In lieu of better ideas let's just stop when we've reached the max supported

 * vswing with its max pre-emphasis, which is either 2+1 or 3+0 depending on

 * whether vswing level 3 is supported or not.

/*

 * Prepare link training by configuring the link parameters. On DDI platforms

 * also enable the port here.

 Write the link configuration data */

 eDP 1.4 rate select method. */

/*

 * Perform the link training clock recovery phase on the given DP PHY using

 * training pattern 1.

 clock recovery */

	/*

	 * The DP 1.4 spec defines the max clock recovery retries value

	 * as 10 but for pre-DP 1.4 devices we set a very tolerant

	 * retry limit of 80 (4 voltage levels x 4 preemphasis levels x

	 * x 5 identical voltage retries). Since the previous specs didn't

	 * define a limit and created the possibility of an infinite loop

	 * we want to prevent any sync from triggering that corner case.

 Update training set as requested by target */

/*

 * Pick Training Pattern Sequence (TPS) for channel equalization. 128b/132b TPS2

 * for UHBR+, TPS4 for HBR3 or for 1.4 devices that support it, TPS3 for HBR2 or

 * 1.2 devices that support it, TPS2 otherwise.

 UHBR+ use separate 128b/132b TPS2 */

	/*

	 * TPS4 support is mandatory for all downstream devices that

	 * support HBR3. There are no known eDP panels that support

	 * TPS4 as of Feb 2018 as per VESA eDP_v1.4b_E1 specification.

	 * LTTPRs must support TPS4.

	/*

	 * TPS3 support is mandatory for downstream devices that

	 * support HBR2. However, not all sinks follow the spec.

/*

 * Perform the link training channel equalization phase on the given DP PHY

 * using one of training pattern 2, 3 or 4 depending on the source and

 * sink capabilities.

 Scrambling is disabled for TPS2/3 and enabled for TPS4 */

 channel equalization */

 Make sure clock is still ok */

 Update training set as requested by target */

 Try 5 times, else fail and try at lower BW */

/**

 * intel_dp_stop_link_train - stop link training

 * @intel_dp: DP struct

 * @crtc_state: state for CRTC attached to the encoder

 *

 * Stop the link training of the @intel_dp port, disabling the training

 * pattern in the sink's DPCD, and disabling the test pattern symbol

 * generation on the port.

 *

 * What symbols are output on the port after this point is

 * platform specific: On DDI/VLV/CHV platforms it will be the idle pattern

 * with the pipe being disabled, on older platforms it's HW specific if/how an

 * idle pattern is generated, as the pipe is already enabled here for those.

 *

 * This function must be called after intel_dp_start_link_train().

 Schedule a Hotplug Uevent to userspace to start modeset */

 Perform the link training on all LTTPRs and the DPRX on a link. */

/**

 * intel_dp_start_link_train - start link training

 * @intel_dp: DP struct

 * @crtc_state: state for CRTC attached to the encoder

 *

 * Start the link training of the @intel_dp port, scheduling a fallback

 * retraining with reduced link rate/lane parameters if the link training

 * fails.

 * After calling this function intel_dp_stop_link_train() must be called.

	/*

	 * TODO: Reiniting LTTPRs here won't be needed once proper connector

	 * HW state readout is added.

 Still continue with enabling the port and link training. */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 *

 * HDMI support for G4x,ILK,SNB,IVB,VLV,CHV (HSW+ handled by the DDI code).

	/*

	 * HW workaround, need to write this twice for issue

	 * that may result in first write getting masked.

	/*

	 * HW workaround, need to toggle enable bit off and on

	 * for 12bpc with pixel repeat.

	 *

	 * FIXME: BSpec says this should be done at the end of

	 * the modeset sequence, so not sure if this isn't too soon.

		/*

		 * HW workaround, need to write this twice for issue

		 * that may result in first write getting masked.

	/*

	 * WaEnableHDMI8bpcBefore12bpc:snb,ivb

	 *

	 * The procedure for 12bpc is as follows:

	 * 1. disable HDMI clock gating

	 * 2. enable HDMI with 8bpc

	 * 3. enable HDMI with 12bpc

	 * 4. enable HDMI clock gating

	/*

	 * HW workaround for IBX, we need to move the port

	 * to transcoder A after disabling it to allow the

	 * matching DP port to be enabled on transcoder A.

		/*

		 * We get CPU/PCH FIFO underruns on the other pipe when

		 * doing the workaround. Sweep them under the rug.

		/*

		 * HW workaround, need to write this twice for issue

		 * that may result in first write getting masked.

 HDMI 1.0V-2dB */

 Reset lanes to avoid HDMI flicker (VLV w/a) */

 Assert data lane reset */

 FIXME: Program the support xxx V-dB */

 Use 800mV-0dB */

 Second common lane will stay alive on its own now */

	/*

	 * On many platforms the HDMI live state signal is known to be

	 * unreliable, so we can't use it to detect if a sink is connected or

	 * not. Instead we detect if it's connected based on whether we can

	 * read the EDID or not. That in turn has a problem during disconnect,

	 * since the HPD interrupt may be raised before the DDC lines get

	 * disconnected (due to how the required length of DDC vs. HPD

	 * connector pins are specified) and so we'll still be able to get a

	 * valid EDID. To solve this schedule another detection cycle if this

	 * time around we didn't detect any change in the sink's connection

	 * status.

	/*

	 * BSpec is unclear about HDMI+HDMI cloning on g4x, but it seems

	 * to work on real hardware. And since g4x can send infoframes to

	 * only one port anyway, nothing is lost by allowing it.

/*

 * Copyright Â© 2006-2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/**

 * DOC: Display PLLs

 *

 * Display PLLs used for driving outputs vary by platform. While some have

 * per-pipe or per-encoder dedicated PLLs, others allow the use of any PLL

 * from a pool. In the latter scenario, it is possible that multiple pipes

 * share a PLL if their configurations match.

 *

 * This file provides an abstraction over display PLLs. The function

 * intel_shared_dpll_init() initializes the PLLs for the given platform.  The

 * users of a PLL are tracked and that tracking is integrated with the atomic

 * modset interface. During an atomic operation, required PLLs can be reserved

 * for a given CRTC and encoder configuration by calling

 * intel_reserve_shared_dplls() and previously reserved PLLs can be released

 * with intel_release_shared_dplls().

 * Changes to the users are first staged in the atomic state, and then made

 * effective by calling intel_shared_dpll_swap_state() during the atomic

 * commit phase.

 Copy shared dpll state */

/**

 * intel_get_shared_dpll_by_id - get a DPLL given its id

 * @dev_priv: i915 device instance

 * @id: pll id

 *

 * Returns:

 * A pointer to the DPLL with @id

/**

 * intel_get_shared_dpll_id - get the id of a DPLL

 * @dev_priv: i915 device instance

 * @pll: the DPLL

 *

 * Returns:

 * The id of @pll

 For ILK+ */

/**

 * intel_enable_shared_dpll - enable a CRTC's shared DPLL

 * @crtc_state: CRTC, and its state, which has a shared DPLL

 *

 * Enable the shared DPLL used by @crtc.

/**

 * intel_disable_shared_dpll - disable a CRTC's shared DPLL

 * @crtc_state: CRTC, and its state, which has a shared DPLL

 *

 * Disable the shared DPLL used by @crtc.

 PCH only available on ILK+ */

 Only want to check enabled timings first */

 Ok no matching timings, maybe there's a free one? */

/**

 * intel_shared_dpll_swap_state - make atomic DPLL configuration effective

 * @state: atomic state

 *

 * This is the dpll version of drm_atomic_helper_swap_state() since the

 * helper does not handle driver-specific global state.

 *

 * For consistency with atomic helpers this function does a complete swap,

 * i.e. it also puts the current state into @state, even though there is no

 * need for that at this moment.

 PCH refclock must be enabled first */

 Wait for the clocks to stabilize. */

	/* The pixel multiplier can only be updated once the

	 * DPLL is enabled and the clocks are stable.

	 *

	 * So write it again.

 Ironlake PCH has a fixed PLL->PCH pipe mapping. */

 reference the pll */

	/*

	 * Try to set up the PCH reference clock once all DPLLs

	 * that depend on it have been shut down.

	/*

	 * Try to set up the PCH reference clock once all DPLLs

	 * that depend on it have been shut down.

 Constraints for PLL good behavior */

 No best (r,n,p) yet */

	/*

	 * Output clock is (LC_FREQ_2K / 2000) * N / (P * R), which compares to

	 * freq2k.

	 *

	 * delta = 1e6 *

	 *	   abs(freq2k - (LC_FREQ_2K * n2/(p * r2))) /

	 *	   freq2k;

	 *

	 * and we would like delta <= budget.

	 *

	 * If the discrepancy is above the PPM-based budget, always prefer to

	 * improve upon the previous solution.  However, if you're within the

	 * budget, try to maximize Ref * VCO, that is N / (P * R^2).

 If both are above the budget, pick the closer */

 If A is below the threshold but B is above it?  Update. */

 Both are below the limit, so pick the higher n2/(r2*r2) */

 Otherwise a < c && b >= d, do nothing */

 in Hz */,

	/* Special case handling for 540 pixel clock: bypass WR PLL entirely

	/*

	 * Ref = LC_FREQ / R, where Ref is the actual reference input seen by

	 * the WR PLL.

	 *

	 * We want R so that REF_MIN <= Ref <= REF_MAX.

	 * Injecting R2 = 2 * R gives:

	 *   REF_MAX * r2 > LC_FREQ * 2 and

	 *   REF_MIN * r2 < LC_FREQ * 2

	 *

	 * Which means the desired boundaries for r2 are:

	 *  LC_FREQ * 2 / REF_MAX < r2 < LC_FREQ * 2 / REF_MIN

	 *

		/*

		 * VCO = N * Ref, that is: VCO = N * LC_FREQ / R

		 *

		 * Once again we want VCO_MIN <= VCO <= VCO_MAX.

		 * Injecting R2 = 2 * R and N2 = 2 * N, we get:

		 *   VCO_MAX * r2 > n2 * LC_FREQ and

		 *   VCO_MIN * r2 < n2 * LC_FREQ)

		 *

		 * Which means the desired boundaries for n2 are:

		 * VCO_MIN * r2 / LC_FREQ < n2 < VCO_MAX * r2 / LC_FREQ

 Muxed-SSC for BDW, non-SSC for non-ULT HSW. */

		/*

		 * We could calculate spread here, but our checking

		 * code only cares about 5% accuracy, and spread is a max of

		 * 0.5% downspread.

 Convert to KHz, p & r have a fixed point portion */

 Non-SSC is only used on non-ULT HSW. */

 this array is indexed by the *shared* pll id */

 DPLL 0 */

 DPLL 0 doesn't support HDMI mode */

 DPLL 1 */

 DPLL 2 */

 DPLL 3 */

 the enable bit is always bit 31 */

 the enable bit is always bit 31 */

 avoid reading back stale values if HDMI mode is not enabled */

 DPLL0 is always enabled since it drives CDCLK */

 current minimal deviation */

 chosen central freq */

 chosen dco freq */

 chosen divider */

 DCO freq must be within +1%/-6%  of the DCO central freq */

 positive deviation */

 negative deviation */

 out */,

 out */,

 out */)

 even dividers */

 3, 5, 7, 9, 15, 21, 35 */

	/*

	 * Intermediate values are in Hz.

	 * Divide by MHz to match bsepc

 in Hz */,

 AFE Clock is 5x Pixel clock */

				/*

				 * Skip the remaining dividers if we're sure to

				 * have found the definitive divider, we can't

				 * improve a 0 deviation.

		/*

		 * If a solution is found with an even divider, prefer

		 * this one.

	/*

	 * gcc incorrectly analyses that these can be used without being

	 * initialized. To be fair, it's hard to guess.

	/*

	 * See comment in intel_dpll_hw_state to understand why we always use 0

	 * as the DPLL id in this function.

		/*

		 * Incorrect ASUS-Z170M BIOS setting, the HW seems to ignore bit#0,

		 * handling it the same way as PDIV_7.

	/*

	 * See comment in intel_dpll_hw_state to understand why we always use 0

	 * as the DPLL id in this function.

 eDP 1.4 rates */

	/*

	 * ctrl1 register is already shifted for each pll, just use 0 to get

	 * the internal shift for each field

 No SSC ref */

 1:1 port->PLL mapping */

 Non-SSC reference */

 Disable 10 bit clock */

 Write P1 & P2 */

 Write M2 integer */

 Write N */

 Write M2 fraction */

 Write M2 fraction enable */

 Write coeff */

 Write calibration val */

 Recalibrate with new settings */

 Enable PLL */

	/*

	 * While we write to the group register to program all lanes at once we

	 * can read only lane registers and we pick lanes 0/1 for that.

 1:1 port->PLL mapping */

 1:1 port->PLL mapping */

	/*

	 * While we write to the group register to program all lanes at once we

	 * can read only lane registers. We configure all lanes the same way, so

	 * here just read out lanes 0/1 and output a note if lanes 2/3 differ.

 bxt clock parameters */

 pre-calculated values for DP linkrates */

 Calculate HDMI div */

	/*

	 * FIXME: tie the following calculation into

	 * i9xx_crtc_compute_clock

 1:1 mapping between ports and PLLs */

 DSI non-SSC ref 19.2MHz */

 even dividers */

 9, 15, 21 */

/*

 * Display WA #22010492432: ehl, tgl, adl-p

 * Program half of the nominal DCO divider fraction value.

/*

 * These values alrea already adjusted: they're the bits we write to the

 * registers, not the logical values.

 [0]: 5.4 */

 3 */, .kdiv = 1, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 [1]: 2.7 */

 3 */, .kdiv = 2, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 [2]: 1.62 */

 5 */, .kdiv = 2, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 [3]: 3.24 */

 5 */, .kdiv = 1, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 [4]: 2.16 */

 2 */, .kdiv = 2, .qdiv_mode = 1, .qdiv_ratio = 2, }, },

 [5]: 4.32 */

 2 */, .kdiv = 2, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 [6]: 6.48 */

 3 */, .kdiv = 1, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 [7]: 8.1 */

 2 */, .kdiv = 1, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 Also used for 38.4 MHz values. */

 [0]: 5.4 */

 3 */, .kdiv = 1, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 [1]: 2.7 */

 3 */, .kdiv = 2, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 [2]: 1.62 */

 5 */, .kdiv = 2, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 [3]: 3.24 */

 5 */, .kdiv = 1, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 [4]: 2.16 */

 2 */, .kdiv = 2, .qdiv_mode = 1, .qdiv_ratio = 2, }, },

 [5]: 4.32 */

 2 */, .kdiv = 2, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 [6]: 6.48 */

 3 */, .kdiv = 1, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 [7]: 8.1 */

 2 */, .kdiv = 1, .qdiv_mode = 0, .qdiv_ratio = 0, }, },

 5 */, .kdiv = 1, .qdiv_mode = 0, .qdiv_ratio = 0,

 5 */, .kdiv = 1, .qdiv_mode = 0, .qdiv_ratio = 0,

 the following params are unused */

 the following params are unused */

	/*

	 * The PLL outputs multiple frequencies at the same time, selection is

	 * made at DDI clock mux level.

	/*

	 * For ICL+, the spec states: if reference frequency is 38.4,

	 * use 19.2 because the DPLL automatically divides that by 2.

 Spec meaning of 999999 MHz */

				/*

				 * Note: a_divratio not matching TGL BSpec

				 * algorithm but matching hardcoded values and

				 * working on HW for DP alt-mode at least

/*

 * The specification for this function uses real numbers, so the math had to be

 * adapted to integer-only calculation, that's why it looks so different.

	/*

	 * tdc_res = 0.000003

	 * tdc_targetcnt = int(2 / (tdc_res * 8 * 50 * 1.1) / refclk_mhz + 0.5)

	 *

	 * The multiplication by 1000 is due to refclk MHz to KHz conversion. It

	 * was supposed to be a division, but we rearranged the operations of

	 * the formula to avoid early divisions so we don't multiply the

	 * rounding errors.

	 *

	 * 0.000003 * 8 * 50 * 1.1 = 0.00132, also known as 132 / 100000, which

	 * we also rearrange to work with integers.

	 *

	 * The 0.5 transformed to 5 results in a multiplication by 10 and the

	 * last division by 10.

	/*

	 * Here we divide dco_khz by 10 in order to allow the dividend to fit in

	 * 32 bits. That's not a problem since we round the division down

	 * anyway.

 write pll_state calculations */

 div2 value of 0 is same as 1 means no div */

	/*

	 * Adjust the original formula to delay the division by 2^22 in order to

	 * minimize possible rounding errors.

/**

 * icl_set_active_port_dpll - select the active port DPLL for a given CRTC

 * @crtc_state: state for the CRTC to select the DPLL for

 * @port_dpll_id: the active @port_dpll_id to select

 *

 * Select the given @port_dpll_id instance from the DPLLs reserved for the

 * CRTC.

 Eliminate DPLLs from consideration if reserved by HTI */

	/*

	 * All registers read here have the same HIP_INDEX_REG even though

	 * they are on different building blocks

	/*

	 * Some of the following registers have reserved fields, so program

	 * these with RMW based on a mask. The mask can be fixed or generated

	 * during the calc/readout phase if the mask depends on some other HW

	 * state like refclk, see icl_calc_mg_pll_state().

	/*

	 * All registers programmed here have the same HIP_INDEX_REG even

	 * though on different building block

 All the registers are RMW */

	/*

	 * The spec says we need to "wait" but it also says it should be

	 * immediate.

 Timeout is actually 600us. */

	/*

	 * Wa_16011069516:adl-p[a0]

	 *

	 * All CMTG regs are unreliable until CMTG clock gating is disabled,

	 * so we can only assume the default TRANS_CMTG_CHICKEN reg value and

	 * sanity check this assumption with a double read, which presumably

	 * returns the correct value even with clock gating on.

	 *

	 * Instead of the usual place for workarounds we apply this one here,

	 * since TRANS_CMTG_CHICKEN is only accessible while DPLL0 is enabled.

		/*

		 * We need to disable DC states when this DPLL is enabled.

		 * This can be done by taking a reference on DPLL4 power

		 * domain.

	/*

	 * DVFS pre sequence would be here, but in our driver the cdclk code

	 * paths should already be setting the appropriate voltage, hence we do

	 * nothing here.

 DVFS post sequence would be here. See the comment above. */

	/*

	 * DVFS pre sequence would be here, but in our driver the cdclk code

	 * paths should already be setting the appropriate voltage, hence we do

	 * nothing here.

 DVFS post sequence would be here. See the comment above. */

	/*

	 * DVFS pre sequence would be here, but in our driver the cdclk code

	 * paths should already be setting the appropriate voltage, hence we do

	 * nothing here.

 DVFS post sequence would be here. See the comment above. */

 The first steps are done by intel_ddi_post_disable(). */

	/*

	 * DVFS pre sequence would be here, but in our driver the cdclk code

	 * paths should already be setting the appropriate voltage, hence we do

	 * nothing here.

 Timeout is actually 1us. */

 DVFS post sequence would be here. See the comment above. */

	/*

	 * The spec says we need to "wait" but it also says it should be

	 * immediate.

 No SSC ref */

/**

 * intel_shared_dpll_init - Initialize shared DPLLs

 * @dev: drm device

 *

 * Initialize shared DPLLs for @dev.

 No shared DPLLs on DG2; port PLLs are part of the PHY */

/**

 * intel_reserve_shared_dplls - reserve DPLLs for CRTC and encoder combination

 * @state: atomic state

 * @crtc: CRTC to reserve DPLLs for

 * @encoder: encoder

 *

 * This function reserves all required DPLLs for the given CRTC and encoder

 * combination in the current atomic commit @state and the new @crtc atomic

 * state.

 *

 * The new configuration in the atomic commit @state is made effective by

 * calling intel_shared_dpll_swap_state().

 *

 * The reserved DPLLs should be released by calling

 * intel_release_shared_dplls().

 *

 * Returns:

 * True if all required DPLLs were successfully reserved.

/**

 * intel_release_shared_dplls - end use of DPLLs by CRTC in atomic state

 * @state: atomic state

 * @crtc: crtc from which the DPLLs are to be released

 *

 * This function releases all DPLLs reserved by intel_reserve_shared_dplls()

 * from the current atomic commit @state and the old @crtc atomic state.

 *

 * The new configuration in the atomic commit @state is made effective by

 * calling intel_shared_dpll_swap_state().

	/*

	 * FIXME: this function is called for every platform having a

	 * compute_clock hook, even though the platform doesn't yet support

	 * the shared DPLL framework and intel_reserve_shared_dplls() is not

	 * called on those.

/**

 * intel_update_active_dpll - update the active DPLL for a CRTC/encoder

 * @state: atomic state

 * @crtc: the CRTC for which to update the active DPLL

 * @encoder: encoder determining the type of port DPLL

 *

 * Update the active DPLL for the given @crtc/@encoder in @crtc's atomic state,

 * from the port DPLLs reserved previously by intel_reserve_shared_dplls(). The

 * DPLL selected will be based on the current mode of the encoder's port.

/**

 * intel_dpll_get_freq - calculate the DPLL's output frequency

 * @i915: i915 device

 * @pll: DPLL for which to calculate the output frequency

 * @pll_state: DPLL state from which to calculate the output frequency

 *

 * Return the output frequency corresponding to @pll's passed in @pll_state.

/**

 * intel_dpll_get_hw_state - readout the DPLL's hardware state

 * @i915: i915 device

 * @pll: DPLL for which to calculate the output frequency

 * @hw_state: DPLL's hardware state

 *

 * Read out @pll's hardware state into @hw_state.

/**

 * intel_dpll_dump_hw_state - write hw_state to dmesg

 * @dev_priv: i915 drm device

 * @hw_state: hw state to be written to the log

 *

 * Write the relevant values in @hw_state to dmesg using drm_dbg_kms.

		/* fallback for platforms that don't use the shared dpll

		 * infrastructure

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2018 Intel Corporation

	/*

	 * We should call mipi_dsi_host_register(&host->base) here, but we don't

	 * have a host->dev, and we don't have OF stuff either. So just use the

	 * dsi framework as a library and hope for the best. Create the dsi

	 * devices by ourselves here too. Need to be careful though, because we

	 * don't initialize any of the driver model devices here.

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/**

 * DOC: Frame Buffer Compression (FBC)

 *

 * FBC tries to save memory bandwidth (and so power consumption) by

 * compressing the amount of memory used by the display. It is total

 * transparent to user space and completely handled in the kernel.

 *

 * The benefits of FBC are mostly visible with solid backgrounds and

 * variation-less patterns. It comes from keeping the memory footprint small

 * and having fewer memory pages opened and accessed for refreshing the display.

 *

 * i915 is responsible to reserve stolen memory for FBC and configure its

 * offset on proper registers. The hardware takes care of all

 * compress/decompress. However there are many known cases where we have to

 * forcibly disable it to allow proper screen updates.

/*

 * For SKL+, the plane source size used by the hardware is based on the value we

 * write to the PLANE_SIZE register. For BDW-, the hardware looks at the value

 * we wrote to PIPESRC.

 plane stride in pixels */

 plane stride based cfb stride in bytes, assuming 1:1 compression limit */

 FBC always 4 bytes per pixel */

 minimum acceptable cfb stride in bytes, assuming 1:1 compression limit */

 1:4 compression limit is the worst case */

 FBC always 4 bytes per pixel */

 FBC segment is 4 lines */

 minimum segment stride we can use */

	/*

	 * Wa_16011863758: icl+

	 * Avoid some hardware segment address miscalculation.

	/*

	 * At least some of the platforms require each 4 line segment to

	 * be 512 byte aligned. Just do it always for simplicity.

 convert back to single line equivalent with 1:1 compression limit */

 properly aligned cfb stride in bytes, assuming 1:1 compression limit */

	/*

	 * At least some of the platforms require each 4 line segment to

	 * be 512 byte aligned. Aligning each line to 512 bytes guarantees

	 * that regardless of the compression limit we choose later.

 Disable compression */

 Wait for compressing bit to clear */

 FBC_CTL wants 32B or 64B units */

 Clear old tags */

 Set it up... */

 enable it... */

 945 needs special SR handling */

 enable it... */

 Disable compression */

 This function forces a CFB recompression through the nuke operation. */

 enable it... */

 Disable compression */

 Display WA #0529: skl, kbl, bxt. */

/**

 * intel_fbc_is_active - Is FBC active?

 * @dev_priv: i915 device instance

 *

 * This function is used to verify the current state of FBC.

 *

 * FIXME: This should be tracked in the plane config eventually

 * instead of queried at runtime for most callers.

	/* The FBC hardware for BDW/SKL doesn't have access to the stolen

	 * reserved range size, so it always assumes the maximum (8mb) is used.

	 * If we enable FBC using a CFB on that memory range we'll get FIFO

 WaFbcOnly1to1Ratio:ctg */

	/*

	 * FBC2 can only do 1:1, 1:2, 1:4, we limit

	 * FBC1 to the same out of convenience.

 Try to over-allocate to reduce reallocations and fragmentation. */

 This should have been caught earlier. */

 Below are the additional FBC restrictions. */

 Display WA #1105: skl,bxt,kbl,cfl,glk */

 16bpp not supported on gen2 */

 WaFbcOnly1to1Ratio:ctg */

/*

 * For some reason, the hardware tracking starts looking at whatever we

 * programmed as the display plane base address register. It does not look at

 * the X and Y offset registers. That's why we include the src x/y offsets

 * instead of just looking at the plane size.

	/*

	 * Src coordinates are already rotated by 270 degrees for

	 * the 90/270 degree plane rotation cases (to match the

	 * GTT mapping), hence no need to account for rotation here.

 FBC1 compression interval: arbitrary choice of 1 second */

	/*

	 * Override stride in 64 byte units per 4 line segment.

	 *

	 * Gen9 hw miscalculates cfb stride for linear as

	 * PLANE_STRIDE*512 instead of PLANE_STRIDE*64, so

	 * we always need to use the override there.

	/* We don't need to use a state cache here since this information is

	 * global for all CRTC.

	/* The use of a CPU fence is one of two ways to detect writes by the

	 * CPU to the scanout and trigger updates to the FBC.

	 *

	 * The other method is by software tracking (see

	 * intel_fbc_invalidate/flush()), it will manually notify FBC and nuke

	 * the current compressed buffer and recompress it.

	 *

	 * Note that is possible for a tiled surface to be unmappable (and

	 * so have no fence associated with it) due to aperture constraints

	 * at the time of pinning.

	 *

	 * FIXME with 90/270 degree rotation we should use the fence on

	 * the normal GTT view (the rotated view doesn't even have a

	 * fence). Would need changes to the FBC fence Y offset as well.

	 * For now this will effectively disable FBC with 90/270 degree

	 * rotation.

 WaFbcExceedCdClockThreshold:hsw,bdw */

	/* It is possible for the required CFB size change without a

	 * crtc->disable + crtc->enable since it is possible to change the

	 * stride without triggering a full modeset. Since we try to

	 * over-allocate the CFB, there's a chance we may keep FBC enabled even

	 * if this happens, but if we exceed the current CFB size we'll have to

	 * disable FBC. Notice that it would be possible to disable FBC, wait

	 * for a frame, free the stolen node, then try to reenable FBC in case

	 * we didn't get any invalidate/deactivate calls, but this would require

	 * a lot of tracking just for a specific case. If we conclude it's an

	/*

	 * Work around a problem on GEN9+ HW, where enabling FBC on a plane

	 * having a Y offset that isn't divisible by 4 causes FIFO underrun

	 * and screen flicker.

 Wa_22010751166: icl, ehl, tgl, dg1, rkl */

	/*

	 * Display 12+ is not supporting FBC with PSR2.

	 * Recommendation is to keep this combination disabled

	 * Bspec: 50422 HSD: 14010260002

	/* Since all our fields are integer types, use memset here so the

	 * comparison function can rely on memcmp because the padding will be

		/*

		 * Display WA #1198: glk+

		 * Need an extra vblank wait between FBC disable and most plane

		 * updates. Bspec says this is only needed for plane disable, but

		 * that is not true. Touching most plane registers will cause the

		 * corruption to appear. Also SKL/derivatives do not seem to be

		 * affected.

		 *

		 * TODO: could optimize this a bit by sampling the frame

		 * counter when we disable FBC (if it was already done earlier)

		 * and skipping the extra vblank wait before the plane update

		 * if at least one frame has already passed.

/**

 * __intel_fbc_disable - disable FBC

 * @dev_priv: i915 device instance

 *

 * This is the low level function that actually disables FBC. Callers should

 * grab the FBC lock.

/**

 * intel_fbc_choose_crtc - select a CRTC to enable FBC on

 * @dev_priv: i915 device instance

 * @state: the atomic state structure

 *

 * This function looks at the proposed state for CRTCs and planes, then chooses

 * which pipe is going to have FBC by setting intel_crtc_state->enable_fbc to

 * true.

 *

 * Later, intel_fbc_enable is going to look for state->enable_fbc and then maybe

 * enable FBC for the chosen CRTC. If it does, it will set dev_priv->fbc.crtc.

 Does this atomic commit involve the CRTC currently tied to FBC? */

	/* Simply choose the first CRTC that is compatible and has a visible

	 * plane. We could go for fancier schemes such as checking the plane

	 * size, but this would just affect the few platforms that don't tie FBC

/**

 * intel_fbc_enable: tries to enable FBC on the CRTC

 * @crtc: the CRTC

 * @state: corresponding &drm_crtc_state for @crtc

 *

 * This function checks if the given CRTC was chosen for FBC, then enables it if

 * possible. Notice that it doesn't activate FBC. It is valid to call

 * intel_fbc_enable multiple times for the same pipe without an

 * intel_fbc_disable in the middle, as long as it is deactivated.

 FIXME crtc_state->enable_fbc lies :( */

/**

 * intel_fbc_disable - disable FBC if it's associated with crtc

 * @crtc: the CRTC

 *

 * This function disables FBC if it's associated with the provided CRTC.

/**

 * intel_fbc_update: enable/disable FBC on the CRTC

 * @state: atomic state

 * @crtc: the CRTC

 *

 * This function checks if the given CRTC was chosen for FBC, then enables it if

 * possible. Notice that it doesn't activate FBC. It is valid to call

 * intel_fbc_update multiple times for the same pipe without an

 * intel_fbc_disable in the middle.

/**

 * intel_fbc_global_disable - globally disable FBC

 * @dev_priv: i915 device instance

 *

 * This function disables FBC regardless of which CRTC is associated with it.

 Maybe we were scheduled twice. */

/*

 * intel_fbc_reset_underrun - reset FBC fifo underrun status.

 * @dev_priv: i915 device instance

 *

 * See intel_fbc_handle_fifo_underrun_irq(). For automated testing we

 * want to re-enable FBC after an underrun to increase test coverage.

/**

 * intel_fbc_handle_fifo_underrun_irq - disable FBC when we get a FIFO underrun

 * @dev_priv: i915 device instance

 *

 * Without FBC, most underruns are harmless and don't really cause too many

 * problems, except for an annoying message on dmesg. With FBC, underruns can

 * become black screens or even worse, especially when paired with bad

 * watermarks. So in order for us to be on the safe side, completely disable FBC

 * in case we ever detect a FIFO underrun on any pipe. An underrun on any pipe

 * already suggests that watermarks may be bad, so try to be as safe as

 * possible.

 *

 * This function is called from the IRQ handler.

	/* There's no guarantee that underrun_detected won't be set to true

	 * right after this check and before the work is scheduled, but that's

	 * not a problem since we'll check it again under the work function

	 * while FBC is locked. This check here is just to prevent us from

	 * unnecessarily scheduling the work, and it relies on the fact that we

/*

 * The DDX driver changes its behavior depending on the value it reads from

 * i915.enable_fbc, so sanitize it by translating the default value into either

 * 0 or 1 in order to allow it to know what's going on.

 *

 * Notice that this is done at driver initialization and we still allow user

 * space to change the value during runtime without sanitizing it again. IGT

 * relies on being able to change i915.enable_fbc at runtime.

 WaFbcTurnOffFbcWhenHyperVisorIsUsed:skl,bxt */

/**

 * intel_fbc_init - Initialize FBC

 * @dev_priv: the i915 device

 *

 * This function might be called during PM init process.

	/* We still don't have any sort of hardware state readout for FBC, so

	 * deactivate it in case the BIOS activated it to make sure software

/*

 * Copyright Â© 2007 David Airlie

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *     David Airlie

 we don't do packed 24bpp */

		/*

		 * If the FB is too big, just don't use it since fbdev is not very

		 * important and we should probably use that space with FBC or other

		 * features.

	/* Pin the GGTT vma for our access via info->screen_base.

	 * This also validates that any existing fb inherited from the

	 * BIOS is suitable for own access.

 setup aperture base/size for vesafb takeover */

 Use fbdev's framebuffer from lmem for discrete */

 Our framebuffer is the entirety of fbdev's system memory */

	/* If the object is shmemfs backed, it will have given us zeroed pages.

	 * If the object is stolen however, it will be full of whatever

	 * garbage was left in there.

 Use default scratch pixmap (info->pixmap.flags = FB_PIXMAP_SYSTEM) */

	/* We rely on the object-free to release the VMA pinning for

	 * the info->screen_base mmaping. Leaking the VMA is simpler than

	 * trying to rectify all the possible error paths leading here.

/*

 * Build an intel_fbdev struct using a BIOS allocated framebuffer, if possible.

 * The core display code will have read out the current plane configuration,

 * so we use that to figure out if there's an object for us to use as the

 * fb, and if so, we re-use it for the fbdev configuration.

 *

 * Note we only support a single fb shared across pipes for boot (mostly for

 * fbcon), so we just find the biggest and use that.

 Find the largest fb */

 Now make sure all the pipes will fit into it */

		/*

		 * See if the plane fb we found above will fit on this

		 * pipe.  Note we need to use the selected fb's pitch and bpp

		 * rather than the current pipe's, since they differ.

 Final pass to check if any active pipes don't have fbs */

 Due to peculiar init order wrt to hpd handling this is separate. */

 Only serialises with all preceding async calls, hence +1 */

/* Suspends/resumes fbdev processing of incoming HPD events. When resuming HPD

 * processing, fbdev will perform a full connector reprobe if a hotplug event

 * was received while HPD was suspended.

		/* Flush any pending work to turn the console on, and then

		 * wait to turn it off. It must be synchronous as we are

		 * about to suspend or unload the driver.

		 *

		 * Note that from within the work-handler, we cannot flush

		 * ourselves, so only flush outstanding work upon suspend!

		/*

		 * The console lock can be pretty contented on resume due

		 * to all the printk activity.  Try to keep it out of the hot

		 * path of resume if possible.

			/* Don't block our own workqueue as this can

			 * be run in parallel with other i915.ko tasks.

	/* On resume from hibernation: If the object is shmemfs backed, it has

	 * been restored from swap. If the object is stolen however, it will be

	 * full of whatever garbage was left in there.

/*

 * Copyright Â© 2015 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

/*

 * Laptops with Intel GPUs which have panels that support controlling the

 * backlight through DP AUX can actually use two different interfaces: Intel's

 * proprietary DP AUX backlight interface, and the standard VESA backlight

 * interface. Unfortunately, at the time of writing this a lot of laptops will

 * advertise support for the standard VESA backlight interface when they

 * don't properly support it. However, on these systems the Intel backlight

 * interface generally does work properly. Additionally, these systems will

 * usually just indicate that they use PWM backlight controls in their VBIOS

 * for some reason.

/* TODO:

 * Implement HDR, right now we just implement the bare minimum to bring us back into SDR mode so we

 * can make people's backlights work in the mean time

/*

 * DP AUX registers for Intel's proprietary HDR backlight interface. We define

 * them here since we'll likely be the only driver to ever use these.

 Pre-TGL+ */

 Bit 6 is reserved */

 Pre-TGL+ */

 Intel EDP backlight callbacks */

 Assume 100% brightness if backlight controls aren't enabled yet */

 Nothing to do for AUX based backlight controls */

 Note we want the actual pwm_level to be 0, regardless of pwm_min */

 VESA backlight callbacks */

	/* TODO: We currently only support AUX only backlight configurations, not backlights which

	 * require a mix of PWM and AUX controls to work. In the mean time, these machines typically

	 * work just fine using normal PWM controls anyway.

	/* Check the VBT and user's module parameters to figure out which

	 * interfaces to probe

	/*

	 * A lot of eDP panels in the wild will report supporting both the

	 * Intel proprietary backlight control interface, and the VESA

	 * backlight control interface. Many of these panels are liars though,

	 * and will only work with the Intel interface. So, always probe for

	 * that first.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

	/*

	 * From Gen 11, In case of dsi cmd mode, frame counter wouldnt

	 * have updated at the beginning of TE, if we want to use

	 * the hw counter, then we would find it updated in only

	 * the next TE, hence switching to sw counter.

	/*

	 * On i965gm the hardware frame counter reads

	 * zero when the TV encoder is enabled :(

 full 32 bit counter */

 only 24 bits of frame count */

 Gen2 doesn't have a hardware frame counter */

	/*

	 * Should really happen exactly when we enable the pipe

	 * but we want the frame counters in the trace, and that

	 * requires vblank support on some platforms/outputs.

	/*

	 * Should really happen exactly when we disable the pipe

	 * but we want the frame counters in the trace, and that

	 * requires vblank support on some platforms/outputs.

 no hw vblank counter */

 paranoia */

/**

 * intel_pipe_update_start() - start update of a set of display registers

 * @new_crtc_state: the new crtc state

 *

 * Mark the start of an update to pipe registers that should be updated

 * atomically regarding vblank. If the next vblank will happens within

 * the next 100 us, this function waits until the vblank passes.

 *

 * After a successful call to this function, interrupts will be disabled

 * until a subsequent call to intel_pipe_update_end(). That is done to

 * avoid random delays.

 FIXME needs to be calibrated sensibly */

	/*

	 * Wait for psr to idle out after enabling the VBL interrupts

	 * VBL interrupts will start the PSR exit and prevent a PSR

	 * re-entry as well.

		/*

		 * prepare_to_wait() has a memory barrier, which guarantees

		 * other CPUs can see the task state update by the time we

		 * read the scanline.

	/*

	 * On VLV/CHV DSI the scanline counter would appear to

	 * increment approx. 1/3 of a scanline before start of vblank.

	 * The registers still get latched at start of vblank however.

	 * This means we must not write any registers on the first

	 * line of vblank (since not the whole line is actually in

	 * vblank). And unfortunately we can't use the interrupt to

	 * wait here since it will fire too soon. We could use the

	 * frame start interrupt instead since it will fire after the

	 * critical scanline, but that would require more changes

	 * in the interrupt code. So for now we'll just do the nasty

	 * thing and poll for the bad scanline to pass us by.

	 *

	 * FIXME figure out if BXT+ DSI suffers from this as well

/**

 * intel_pipe_update_end() - end update of a set of display registers

 * @new_crtc_state: the new crtc state

 *

 * Mark the end of an update started with intel_pipe_update_start(). This

 * re-enables interrupts and verifies the update was actually completed

 * before a vblank.

	/*

	 * Incase of mipi dsi command mode, we need to set frame update

	 * request for every commit.

	/* We're still in the vblank-evade critical section, this can't race.

	 * Would be slightly nice to just grab the vblank count and arm the

	 * event outside of the critical section - the spinlock might spin for a

 Send VRR Push to terminate Vblank */

/*

 * Copyright Â© 2014-2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/**

 * DOC: DPIO

 *

 * VLV, CHV and BXT have slightly peculiar display PHYs for driving DP/HDMI

 * ports. DPIO is the name given to such a display PHY. These PHYs

 * don't follow the standard programming model using direct MMIO

 * registers, and instead their registers must be accessed trough IOSF

 * sideband. VLV has one such PHY for driving ports B and C, and CHV

 * adds another PHY for driving port D. Each PHY responds to specific

 * IOSF-SB port.

 *

 * Each display PHY is made up of one or two channels. Each channel

 * houses a common lane part which contains the PLL and other common

 * logic. CH0 common lane also contains the IOSF-SB logic for the

 * Common Register Interface (CRI) ie. the DPIO registers. CRI clock

 * must be running when any DPIO registers are accessed.

 *

 * In addition to having their own registers, the PHYs are also

 * controlled through some dedicated signals from the display

 * controller. These include PLL reference clock enable, PLL enable,

 * and CRI clock selection, for example.

 *

 * Eeach channel also has two splines (also called data lanes), and

 * each spline is made up of one Physical Access Coding Sub-Layer

 * (PCS) block and two TX lanes. So each channel has two PCS blocks

 * and four TX lanes. The TX lanes are used as DP lanes or TMDS

 * data/clock pairs depending on the output type.

 *

 * Additionally the PHY also contains an AUX lane with AUX blocks

 * for each channel. This is used for DP AUX communication, but

 * this fact isn't really relevant for the driver since AUX is

 * controlled from the display controller side. No DPIO registers

 * need to be accessed during AUX communication,

 *

 * Generally on VLV/CHV the common lane corresponds to the pipe and

 * the spline (PCS/TX) corresponds to the port.

 *

 * For dual channel PHY (VLV/CHV):

 *

 *  pipe A == CMN/PLL/REF CH0

 *

 *  pipe B == CMN/PLL/REF CH1

 *

 *  port B == PCS/TX CH0

 *

 *  port C == PCS/TX CH1

 *

 * This is especially important when we cross the streams

 * ie. drive port B with pipe B, or port C with pipe A.

 *

 * For single channel PHY (CHV):

 *

 *  pipe C == CMN/PLL/REF CH0

 *

 *  port D == PCS/TX CH0

 *

 * On BXT the entire PHY channel corresponds to the port. That means

 * the PLL is also now associated with the port rather than the pipe,

 * and so the clock needs to be routed to the appropriate transcoder.

 * Port A PLL is directly connected to transcoder EDP and port B/C

 * PLLs can be routed to any transcoder A/B/C.

 *

 * Note: DDI0 is digital port B, DD1 is digital port C, and DDI2 is

 * digital port D (CHV) or port A (BXT). ::

 *

 *

 *     Dual channel PHY (VLV/CHV/BXT)

 *     ---------------------------------

 *     |      CH0      |      CH1      |

 *     |  CMN/PLL/REF  |  CMN/PLL/REF  |

 *     |---------------|---------------| Display PHY

 *     | PCS01 | PCS23 | PCS01 | PCS23 |

 *     |-------|-------|-------|-------|

 *     |TX0|TX1|TX2|TX3|TX0|TX1|TX2|TX3|

 *     ---------------------------------

 *     |     DDI0      |     DDI1      | DP/HDMI ports

 *     ---------------------------------

 *

 *     Single channel PHY (CHV/BXT)

 *     -----------------

 *     |      CH0      |

 *     |  CMN/PLL/REF  |

 *     |---------------| Display PHY

 *     | PCS01 | PCS23 |

 *     |-------|-------|

 *     |TX0|TX1|TX2|TX3|

 *     -----------------

 *     |     DDI2      | DP/HDMI port

 *     -----------------

/**

 * struct bxt_ddi_phy_info - Hold info for a broxton DDI phy

	/**

	 * @dual_channel: true if this phy has a second channel.

	/**

	 * @rcomp_phy: If -1, indicates this phy has its own rcomp resistor.

	 * Otherwise the GRC value will be copied from the phy indicated by

	 * this field.

	/**

	 * @reset_delay: delay in us to wait before setting the common reset

	 * bit in BXT_PHY_CTL_FAMILY, which effectively enables the phy.

	/**

	 * @pwron_mask: Mask with the appropriate bit set that would cause the

	 * punit to power this phy if written to BXT_P_CR_GT_DISP_PWRON.

	/**

	 * @channel: struct containing per channel information.

		/**

		 * @channel.port: which port maps to this channel.

	/*

	 * While we write to the group register to program all lanes at once we

	 * can read only lane registers and we pick lanes 0/1 for that.

 Still read out the GRC value for state verification */

	/*

	 * The PHY registers start out inaccessible and respond to reads with

	 * all 1s.  Eventually they become accessible as they power up, then

	 * the reserved bit will give the default 0.  Poll on the reserved bit

	 * becoming 0 to find when the PHY is accessible.

	 * The flag should get set in 100us according to the HW team, but

	 * use 1ms due to occasional timeouts observed with that.

 Program PLL Rcomp code offset */

 Program power gating */

		/*

		 * PHY0 isn't connected to an RCOMP resistor so copy over

		 * the corresponding calibrated value from PHY1, and disable

		 * the automatic calibration on PHY0.

	/*

	 * We need to copy the GRC calibration value from rcomp_phy,

	 * so make sure it's powered up.

 PLL Rcomp code offset */

 Power gating */

		/*

		 * Note that on CHV this flag is called UPAR, but has

		 * the same function.

 Clear calc init */

 Program swing deemph */

 Program swing margin */

		/*

		 * Supposedly this value shouldn't matter when unique transition

		 * scale is disabled, but in fact it does matter. Let's just

		 * always program the same value and hope it's OK.

	/*

	 * The document said it needs to set bit 27 for ch0 and bit 26

	 * for ch1. Might be a typo in the doc.

	 * For now, for this unique transition scale selection, set bit

	 * 27 for ch0 and ch1.

 Start swing calculation */

	/*

	 * Must trick the second common lane into life.

	 * Otherwise we can't even access the PLL.

 Assert data lane reset */

 program left/right clock distribution */

 program clock channel usage */

	/*

	 * This a a bit weird since generally CL

	 * matches the pipe, but here we need to

	 * pick the CL based on the port.

 allow hardware to manage TX FIFO reset source */

 Program Tx lane latency optimal setting*/

 Set the upar bit */

 Data lane stagger programming */

 Deassert data lane reset */

 disable left/right clock distribution */

	/*

	 * Leave the power down bit cleared for at least one

	 * lane so that chv_powergate_phy_ch() will power

	 * on something when the channel is otherwise unused.

	 * When the port is off and the override is removed

	 * the lanes power down anyway, so otherwise it doesn't

	 * really matter what the state of power down bits is

	 * after this.

 Program Tx lane resets to default */

 Fix up inter-pair skew failure */

 Enable clock channels for this port */

 Program lane clock */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

 from BPP 6 to 24 in steps of 0.5 */

 from BPP 6 to 30 in steps of 0.5 */

 from BPP 6 to 36 in steps of 0.5 */

/*

 * These qp tables are as per the C model

 * and it has the rows pointing to bpps which increment

 * in steps of 0.5

 * We do not support fractional bpps as of today,

 * hence we would skip the fractional bpps during

 * our references for qp calclulations.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 *

	/*

	 * DP Sink is capable of VRR video timings if

	 * Ignore MSA bit is set in DPCD.

	 * EDID monitor range also should be atleast 10 for reasonable

	 * Adaptive Sync or Variable Refresh Rate end user experience.

/*

 * Without VRR registers get latched at:

 *  vblank_start

 *

 * With VRR the earliest registers can get latched is:

 *  intel_vrr_vmin_vblank_start(), which if we want to maintain

 *  the correct min vtotal is >=vblank_start+1

 *

 * The latest point registers can get latched is the vmax decision boundary:

 *  intel_vrr_vmax_vblank_start()

 *

 * Between those two points the vblank exit starts (and hence registers get

 * latched) ASAP after a push is sent.

 *

 * framestart_delay is programmable 0-3.

 The hw imposes the extra scanline before frame start */

 Min vblank actually determined by flipline that is always >=vmin+1 */

	/*

	 * flipline determines the min vblank length the hardware will

	 * generate, and flipline>=vmin+1, hence we reduce vmin by one

	 * to make sure we can get the actual min vblank length.

	/*

	 * For XE_LPD+, we use guardband and pipeline override

	 * is deprecated.

		/*

		 * FIXME: s/4/framestart_delay+1/ to get consistent

		 * earliest/latest points for register latching regardless

		 * of the framestart_delay used?

		 *

		 * FIXME: this really needs the extra scanline to provide consistent

		 * behaviour for all framestart_delay values. Otherwise with

		 * framestart_delay==3 we will end up extending the min vblank by

		 * one extra line.

/*

 * Copyright Â© 2015 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

/**

 * DOC: Hotplug

 *

 * Simply put, hotplug occurs when a display is connected to or disconnected

 * from the system. However, there may be adapters and docking stations and

 * Display Port short pulses and MST devices involved, complicating matters.

 *

 * Hotplug in i915 is handled in many different levels of abstraction.

 *

 * The platform dependent interrupt handling code in i915_irq.c enables,

 * disables, and does preliminary handling of the interrupts. The interrupt

 * handlers gather the hotplug detect (HPD) information from relevant registers

 * into a platform independent mask of hotplug pins that have fired.

 *

 * The platform independent interrupt handler intel_hpd_irq_handler() in

 * intel_hotplug.c does hotplug irq storm detection and mitigation, and passes

 * further processing to appropriate bottom halves (Display Port specific and

 * regular hotplug).

 *

 * The Display Port work function i915_digport_work_func() calls into

 * intel_dp_hpd_pulse() via hooks, which handles DP short pulses and DP MST long

 * pulses, with failures and non-MST long pulses triggering regular hotplug

 * processing on the connector.

 *

 * The regular hotplug work function i915_hotplug_work_func() calls connector

 * detect hooks, and, if connector status changes, triggers sending of hotplug

 * uevent to userspace via drm_kms_helper_hotplug_event().

 *

 * Finally, the userspace is responsible for triggering a modeset upon receiving

 * the hotplug uevent, disabling or enabling the crtc as needed.

 *

 * The hotplug interrupt storm detection and mitigation code keeps track of the

 * number of interrupts per hotplug pin per a period of time, and if the number

 * of interrupts exceeds a certain threshold, the interrupt is disabled for a

 * while before being re-enabled. The intention is to mitigate issues raising

 * from broken hardware triggering massive amounts of interrupts and grinding

 * the system to a halt.

 *

 * Current implementation expects that hotplug interrupt storm will not be

 * seen when display port sink is connected, hence on platforms whose DP

 * callback is handled by i915_digport_work_func reenabling of hpd is not

 * performed (it was never expected to be disabled in the first place ;) )

 * this is specific to DP sinks handled by this routine and any other display

 * such as HDMI or DVI enabled on the same port will have proper logic since

 * it will use i915_hotplug_work_func where this logic is handled.

/**

 * intel_hpd_pin_default - return default pin associated with certain port.

 * @dev_priv: private driver data pointer

 * @port: the hpd port to get associated pin

 *

 * It is only valid and used by digital port encoder.

 *

 * Return pin that is associatade with @port.

	/*

	 * MST connectors get their encoder attached dynamically

	 * so need to make sure we have an encoder here. But since

	 * MST encoders have their hpd_pin set to HPD_NONE we don't

	 * have to special case them beyond that.

/**

 * intel_hpd_irq_storm_detect - gather stats and detect HPD IRQ storm on a pin

 * @dev_priv: private driver data pointer

 * @pin: the pin to gather stats on

 * @long_hpd: whether the HPD IRQ was long or short

 *

 * Gather stats about HPD IRQs from the specified @pin, and detect IRQ

 * storms. Only the pin specific stats and state are changed, the caller is

 * responsible for further action.

 *

 * The number of IRQs that are allowed within @HPD_STORM_DETECT_PERIOD is

 * stored in @dev_priv->hotplug.hpd_storm_threshold which defaults to

 * @HPD_STORM_DEFAULT_THRESHOLD. Long IRQs count as +10 to this threshold, and

 * short IRQs count as +1. If this threshold is exceeded, it's considered an

 * IRQ storm and the IRQ state is set to @HPD_MARK_DISABLED.

 *

 * By default, most systems will only count long IRQs towards

 * &dev_priv->hotplug.hpd_storm_threshold. However, some older systems also

 * suffer from short IRQ storms and must also track these. Because short IRQ

 * storms are naturally caused by sideband interactions with DP MST devices,

 * short IRQ detection is only enabled for systems without DP MST support.

 * Systems which are new enough to support DP MST are far less likely to

 * suffer from IRQ storms at all, so this is fine.

 *

 * The HPD threshold can be controlled through i915_hpd_storm_ctl in debugfs,

 * and should only be adjusted for automated hotplug testing.

 *

 * Return true if an IRQ storm was detected on @pin.

 Enable polling and queue hotplug re-enabling. */

 fall back to old school hpd */

/**

 * intel_hpd_trigger_irq - trigger an hpd irq event for a port

 * @dig_port: digital port

 *

 * Trigger an HPD interrupt event for the given port, emulating a short pulse

 * generated by the sink, and schedule the dig port work to handle it.

/*

 * Handle hotplug events outside the interrupt handler proper.

 Enable polling for connectors which had HPD IRQ storms */

 Remove shared HPD pins that have changed */

/**

 * intel_hpd_irq_handler - main hotplug irq handler

 * @dev_priv: drm_i915_private

 * @pin_mask: a mask of hpd pins that have triggered the irq

 * @long_mask: a mask of hpd pins that may be long hpd pulses

 *

 * This is the main hotplug irq handler for all platforms. The platform specific

 * irq handlers call the platform specific hotplug irq handlers, which read and

 * decode the appropriate registers into bitmasks about hpd pins that have

 * triggered (@pin_mask), and which of those pins may be long pulses

 * (@long_mask). The @long_mask is ignored if the port corresponding to the pin

 * is not a digital port.

 *

 * Here, we do hotplug irq storm detection and mitigation, and pass further

 * processing to appropriate bottom halves.

	/*

	 * Determine whether ->hpd_pulse() exists for each pin, and

	 * whether we have a short or a long pulse. This is needed

	 * as each pin may have up to two encoders (HDMI and DP) and

	 * only the one of them (DP) will have ->hpd_pulse().

 Now process each pin just once */

			/*

			 * On GMCH platforms the interrupt mask bits only

			 * prevent irq generation, not the setting of the

			 * hotplug bits itself. So only WARN about unexpected

			 * interrupts on saner platforms.

		/*

		 * Delegate to ->hpd_pulse() if one of the encoders for this

		 * pin has it, otherwise let the hotplug_work deal with this

		 * pin directly.

	/*

	 * Disable any IRQs that storms were detected on. Polling enablement

	 * happens later in our hotplug work.

	/*

	 * Our hotplug handler can grab modeset locks (by calling down into the

	 * fb helpers). Hence it must not be run on our own dev-priv->wq work

	 * queue for otherwise the flush_work in the pageflip code will

	 * deadlock.

/**

 * intel_hpd_init - initializes and enables hpd support

 * @dev_priv: i915 device instance

 *

 * This function enables the hotplug support. It requires that interrupts have

 * already been enabled with intel_irq_init_hw(). From this point on hotplug and

 * poll request can run concurrently to other code, so locking rules must be

 * obeyed.

 *

 * This is a separate step from interrupt enabling to simplify the locking rules

 * in the driver load and resume code.

 *

 * Also see: intel_hpd_poll_enable() and intel_hpd_poll_disable().

	/*

	 * Interrupt setup is already guaranteed to be single-threaded, this is

	 * just to make the assert_spin_locked checks happy.

	/*

	 * We might have missed any hotplugs that happened while we were

	 * in the middle of disabling polling

/**

 * intel_hpd_poll_enable - enable polling for connectors with hpd

 * @dev_priv: i915 device instance

 *

 * This function enables polling for all connectors which support HPD.

 * Under certain conditions HPD may not be functional. On most Intel GPUs,

 * this happens when we enter runtime suspend.

 * On Valleyview and Cherryview systems, this also happens when we shut off all

 * of the powerwells.

 *

 * Since this function can get called in contexts where we're already holding

 * dev->mode_config.mutex, we do the actual hotplug enabling in a seperate

 * worker.

 *

 * Also see: intel_hpd_init() and intel_hpd_poll_disable().

	/*

	 * We might already be holding dev->mode_config.mutex, so do this in a

	 * seperate worker

	 * As well, there's no issue if we race here since we always reschedule

	 * this worker anyway

/**

 * intel_hpd_poll_disable - disable polling for connectors with hpd

 * @dev_priv: i915 device instance

 *

 * This function disables polling for all connectors which support HPD.

 * Under certain conditions HPD may not be functional. On most Intel GPUs,

 * this happens when we enter runtime suspend.

 * On Valleyview and Cherryview systems, this also happens when we shut off all

 * of the powerwells.

 *

 * Since this function can get called in contexts where we're already holding

 * dev->mode_config.mutex, we do the actual hotplug enabling in a seperate

 * worker.

 *

 * Also used during driver init to initialize connector->polled

 * appropriately for all connectors.

 *

 * Also see: intel_hpd_init() and intel_hpd_poll_enable().

/*

 * Copyright Â© 2006 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *    Eric Anholt <eric@anholt.net>

 *

/**

 * DOC: Video BIOS Table (VBT)

 *

 * The Video BIOS Table, or VBT, provides platform and board specific

 * configuration information to the driver that is not discoverable or available

 * through other means. The configuration is mostly related to display

 * hardware. The VBT is available via the ACPI OpRegion or, on older systems, in

 * the PCI ROM.

 *

 * The VBT consists of a VBT Header (defined as &struct vbt_header), a BDB

 * Header (&struct bdb_header), and a number of BIOS Data Blocks (BDB) that

 * contain the actual configuration information. The VBT Header, and thus the

 * VBT, begins with "$VBT" signature. The VBT Header contains the offset of the

 * BDB Header. The data blocks are concatenated after the BDB Header. The data

 * blocks have a 1-byte Block ID, 2-byte Block Size, and Block Size bytes of

 * data. (Block 53, the MIPI Sequence Block is an exception.)

 *

 * The driver parses the VBT during load. The relevant information is stored in

 * driver private data for ease of use, and the actual VBT is not read after

 * that.

 Wrapper for VBT child device config */

 Get BDB block size given a pointer to Block ID. */

 The MIPI Sequence Block v3+ has a separate size field. */

 Get BDB block size give a pointer to data after Block ID and Block Size. */

 skip to first section */

 walk the sections looking for section_id */

 Some VBTs have bogus h/vtotal values */

	/*

	 * the size of fp_timing varies on the different platform.

	 * So calculate the DVO timing relative offset in LVDS data

	 * entry to get the DVO timing entry

/* get lvds_fp_timing entry

 * this function may return NULL if the corresponding entry is invalid

 stored in header */

 Parse general panel options */

	/*

	 * VBT has static DRRS = 0 and seamless DRRS = 2.

	 * The below piece of code is required to adjust vbt.drrs_type

	 * to match the enum drrs_support_type.

 Try to find integrated panel timing data */

 check the resolution, just to be sure */

 DTD has unknown fields, but keep going */

	/*

	 * Older VBTs provided provided DTD information for internal displays

	 * through the "LFP panel DTD" block (42).  As of VBT revision 229,

	 * that block is now deprecated and DTD information should be provided

	 * via a newer "generic DTD" block (58).  Just to be safe, we'll

	 * try the new generic DTD block first on VBT >= 229, but still fall

	 * back to trying the old LFP block if that fails.

 Try to find sdvo panel data */

 int_crt_support can't be trusted on earlier platforms */

	/*

	 * Only parse SDVO mappings on gens that could have SDVO. This isn't

	 * accurate and doesn't have to be, as long as it's not too strict.

			/*

			 * If the slave address is neither 0x70 nor 0x72,

			 * it is not a SDVO device. Skip it.

 skip the incorrect SDVO port */

 Maybe this is a SDVO device with multiple inputs */

 And the mapping info is not added */

 No SDVO device info is found */

		/*

		 * Note that we consider BDB_DRIVER_FEATURE_INT_SDVO_LVDS

		 * to mean "eDP". The VBT spec doesn't agree with that

		 * interpretation, but real world VBTs seem to.

		/*

		 * FIXME it's not clear which BDB version has the LVDS config

		 * bits defined. Revision history in the VBT spec says:

		 * "0.92 | Add two definitions for VBT value of LVDS Active

		 *  Config (00b and 11b values defined) | 06/13/2005"

		 * but does not the specify the BDB version.

		 *

		 * So far version 134 (on i945gm) is the oldest VBT observed

		 * in the wild with the bits correctly populated. Version

		 * 108 (on i85x) does not have the bits correctly populated.

		/*

		 * If DRRS is not supported, drrs_type has to be set to 0.

		 * This is because, VBT is configured in such a way that

		 * static DRRS is 0 and DRRS not supported is represented by

		 * driver->drrs_enabled=false

	/*

	 * If DRRS is not supported, drrs_type has to be set to 0.

	 * This is because, VBT is configured in such a way that

	 * static DRRS is 0 and DRRS not supported is represented by

	 * power->drrs & BIT(panel_type)=false

 Get the eDP sequencing and link info */

 Don't read from VBT if module parameter has valid value*/

 Allowed VBT values goes from 0 to 15 */

	/*

	 * New psr options 0=500us, 1=100us, 2=2500us, 3=0us

	 * Old decimal value is wake up time in multiples of 100 us.

 Reusing PSR1 wakeup time for PSR2 in older VBTs */

 parse MIPI blocks only if LFP type is MIPI */

 Initialize this to undefined indicating no generic MIPI support */

	/* Block #40 is already parsed and panel_fixed_mode is

	 * stored in i915->lfp_lvds_vbt_mode

	 * resuse this when needed

	/* Parse #52 for panel index used from panel_type already

	 * parsed

	/*

	 * get hold of the correct configuration block and pps data as per

	 * the panel_type as index

 store as of now full data. Trim when we realise all is not needed */

 FIXME is the 90 vs. 270 correct? */

		/*

		 * Most (all?) VBTs claim 0 degrees despite having

		 * an upside down panel, thus we do not trust this.

 We have mandatory mipi config blocks. Initialize as generic panel */

 Find the sequence block and size for the given panel. */

 skip new block size */

 Skip Sequence Byte. */

	/*

	 * Could skip sequence based on Size of Sequence alone, but also do some

	 * checking on the structure.

 Skip Sequence Byte. */

	/*

	 * Size of Sequence. Excludes the Sequence Byte and the size itself,

	 * includes MIPI_SEQ_ELEM_END byte, excludes the final MIPI_SEQ_END

	 * byte.

		/*

		 * FIXME: Would be nice to check elements like for v1/v2 in

		 * goto_next_sequence() above.

/*

 * Get len of pre-fixed deassert fragment from a v1 init OTP sequence,

 * skip all delay + gpio operands and stop at the first DSI packet op.

 index = 1 to skip sequence byte */

 1 byte for operand + uint32 */

 1 byte for op, 1 for gpio_nr, 1 for value */

/*

 * Some v1 VBT MIPI sequences do the deassert in the init OTP sequence.

 * The deassert must be done before calling intel_dsi_device_ready, so for

 * these devices we split the init OTP sequence into a deassert sequence and

 * the actual init OTP part.

 Limit this to VLV for now. */

 Limit this to v1 vid-mode sequences */

 Only do this if there are otp and assert seqs and no deassert seq */

 The deassert-sequence ends at the first DSI packet */

 Copy the fragment, update seq byte and terminate it */

 Use the copy for deassert */

 Replace the last byte of the fragment with init OTP seq byte */

 And make MIPI_MIPI_SEQ_INIT_OTP point to it */

 Only our generic panel driver uses the sequence block. */

 Fail gracefully for forward incompatible sequence block. */

 Parse the sequences, store pointers to each sequence. */

 Log about presence of sequences we won't run. */

 Sanity checks */

 See VBT spec */

 N/A */

 sic */

 sic */

 Assuming direct map */

	/*

	 * If we have multiple ports supposedly sharing the pin, then dvi/hdmi

	 * couldn't exist on the shared port. Otherwise they share the same ddc

	 * pin and system couldn't communicate with them separately.

	 *

	 * Give inverse child device order the priority, last one wins. Yes,

	 * there are real machines (eg. Asrock B250M-HDV) where VBT has both

	 * port A and port E with the same AUX ch and we must pick port E :(

	/*

	 * If we have multiple ports supposedly sharing the aux channel, then DP

	 * couldn't exist on the shared port. Otherwise they share the same aux

	 * channel and system couldn't communicate with them separately.

	 *

	 * Give inverse child device order the priority, last one wins. Yes,

	 * there are real machines (eg. Asrock B250M-HDV) where VBT has both

	 * port A and port E with the same AUX ch and we must pick port E :(

	/*

	 * Each DDI port can have more than one value on the "DVO Port" field,

	 * so look for all the possible values for each port.

	/*

	 * RKL VBT uses PHY based mapping. Combo PHYs A,B,C,D

	 * map to DDI A,B,TC1,TC2 respectively.

	/*

	 * Alderlake S ports used in the driver are PORT_A, PORT_D, PORT_E,

	 * PORT_F and PORT_G, we need to map that to correct VBT sections.

	/*

	 * On some ICL SKUs port F is not present, but broken VBTs mark

	 * the port as present. Only try to initialize port F for the

	 * SKUs that may actually have it.

 I_boost config for SKL and above */

 Flag an error for unexpected size, but continue anyway. */

 The legacy sized child device config is the minimum we need. */

 get the number of child device */

		/*

		 * Copy as much as we know (sizeof) and is available

		 * (child_dev_size) of the child device config. Accessing the

		 * data must depend on VBT version.

 Common defaults which may be overridden by VBT. */

 Default to having backlight */

 LFP panel data */

 SDVO panel data */

 general features */

 driver features */

 Default to using SSC */

	/*

	 * Core/SandyBridge/IvyBridge use alternative (120MHz) reference

	 * clock for LVDS.

 Defaults to initialize only if there is no VBT. */

		/*

		 * VBT has the TypeC mode (native,TBT/USB) and we don't want

		 * to detect it.

 Create fake child device config */

 Bypass some minimum baseline VBT version checks */

/**

 * intel_bios_is_valid_vbt - does the given buffer contain a valid VBT

 * @buf:	pointer to a buffer to validate

 * @size:	size of the buffer

 *

 * Returns true on valid VBT.

 Scour memory looking for the VBT signature. */

 The rest will be validated by intel_bios_is_valid_vbt() */

/**

 * intel_bios_init - find VBT and initialize settings from the BIOS

 * @i915: i915 device instance

 *

 * Parse and initialize settings from the Video BIOS Tables (VBT). If the VBT

 * was not found in ACPI OpRegion, try to find it in PCI ROM first. Also

 * initialize some defaults if the VBT is not present at all.

 If the OpRegion does not have VBT, look in PCI ROM. */

 Grab useful general definitions */

 Depends on child device list */

 Further processing on pre-parsed or generated child device data */

/**

 * intel_bios_driver_remove - Free any resources allocated by intel_bios_init()

 * @i915: i915 device instance

/**

 * intel_bios_is_tv_present - is integrated TV present in VBT

 * @i915: i915 device instance

 *

 * Return true if TV is present. If no child devices were parsed from VBT,

 * assume TV is present.

		/*

		 * If the device type is not TV, continue.

		/* Only when the addin_offset is non-zero, it is regarded

		 * as present.

/**

 * intel_bios_is_lvds_present - is LVDS present in VBT

 * @i915:	i915 device instance

 * @i2c_pin:	i2c pin for LVDS if present

 *

 * Return true if LVDS is present. If no child devices were parsed from VBT,

 * assume LVDS is present.

		/* If the device type is not LFP, continue.

		 * We have to check both the new identifiers as well as the

		 * old for compatibility with some BIOSes.

		/* However, we cannot trust the BIOS writers to populate

		 * the VBT correctly.  Since LVDS requires additional

		 * information from AIM blocks, a non-zero addin offset is

		 * a good indicator that the LVDS is actually present.

		/* But even then some BIOS writers perform some black magic

		 * and instantiate the device without reference to any

		 * additional data.  Trust that if the VBT was written into

		 * the OpRegion then they have validated the LVDS's existence.

/**

 * intel_bios_is_port_present - is the specified digital port present

 * @i915:	i915 device instance

 * @port:	port to check

 *

 * Return true if the device in %port is present.

 FIXME maybe deal with port A as well? */

/**

 * intel_bios_is_port_edp - is the device in given port eDP

 * @i915:	i915 device instance

 * @port:	port to check

 *

 * Return true if the device in %port is eDP.

 Only accept a HDMI dvo_port as DP++ if it has an AUX channel */

		/*

		 * Buggy VBTs may declare DP ports as having

		 * HDMI type dvo_port :( So let's check both.

/**

 * intel_bios_is_dsi_present - is DSI present in VBT

 * @i915:	i915 device instance

 * @port:	port for DSI if present

 *

 * Return true if DSI is present, and return the port in %port.

	/*

	 * FIXME: This is ugly, and slice count should take DSC engine

	 * throughput etc. into account.

	 *

	 * Also, per spec DSI supports 1, 2, 3 or 4 horizontal slices.

 FIXME */

	/*

	 * The VBT rc_buffer_block_size and rc_buffer_size definitions

	 * correspond to DP 1.4 DPCD offsets 0x62 and 0x63.

 FIXME: DSI spec says bpc + 1 for this one */

 FIXME: initially DSI specific */

/**

 * intel_bios_is_port_hpd_inverted - is HPD inverted for %port

 * @i915:	i915 device instance

 * @port:	port to check

 *

 * Return true if HPD should be inverted for %port.

/**

 * intel_bios_is_lspcon_present - if LSPCON is attached on %port

 * @i915:	i915 device instance

 * @port:	port to check

 *

 * Return true if LSPCON is present on this port

/**

 * intel_bios_is_lane_reversal_needed - if lane reversal needed on port

 * @i915:       i915 device instance

 * @port:       port to check

 *

 * Return true if port requires lane reversal

	/*

	 * RKL/DG1 VBT uses PHY based mapping. Combo PHYs A,B,C,D

	 * map to DDI A,B,TC1,TC2 respectively.

	 *

	 * ADL-S VBT uses PHY based mapping. Combo PHYs A,B,C,D,E

	 * map to DDI A,TC1,TC2,TC3,TC4 respectively.

 This is an index in the HDMI/DVI DDI buffer translation table, or -1 */

/*

 * Copyright Â© 2006-2007 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 Here's the desired hotplug mode */

	/* DPMS state is stored in the connector, which we need in the

 asserts want to know the pipe even if the port is disabled */

/* Note: The caller is required to filter out dpms modes not supported by the

 For CPT allow 3 pipe config, for others just use A or B */

 Those bits don't exist here */

		/*

		 * 270 MHz due to current DPLL limits,

		 * DAC limit supposedly 355 MHz.

 The FDI receiver on LPT only supports 8bpc and only has 2 lanes. */

 HSW/BDW FDI limited to 4k */

 HSW/BDW FDI limited to 4k */

 LPT FDI RX only supports 8bpc. */

 FDI must always be 2.7 GHz */

 The first time through, trigger an explicit detection cycle */

 Check the status to see if both blue and green are on now */

	/*

	 * Doing a force trigger causes a hpd interrupt to get sent, which can

	 * get us stuck in a loop if we're polling:

	 *  - We enable power wells and reset the ADPA

	 *  - output_poll_exec does force probe on VGA, triggering a hpd

	 *  - HPD handler waits for poll to unlock dev->mode_config.mutex

	 *  - output_poll_exec shuts off the ADPA, unlocks

	 *    dev->mode_config.mutex

	 *  - HPD handler runs, resets ADPA and brings us back to the start

	 *

	 * Just disable HPD interrupts here to prevent this

 Check the status to see if both blue and green are on now */

	/*

	 * On 4 series desktop, CRT detect sequence need to be done twice

	 * to get a reliable result.

 turn on the FORCE_DETECT */

 wait for FORCE_DETECT to go off */

 clear the interrupt we just generated, if any */

 local version of intel_ddc_get_modes() to use intel_crt_get_edid() */

		/*

		 * This may be a DVI-I connector with a shared DDC

		 * link between analog and digital outputs, so we

		 * have to check the EDID input spec of the attached device.

 Set the border color to purple. */

		/* Wait for next Vblank to substitue

		/*

		* If there isn't any border, add some.

		* Yes, this will flicker

 sample in the vertical border, selecting the larger one */

		/*

		 * Wait for the border to be displayed

		/*

		 * Watch ST00 for an entire scanline

 Read the ST00 VGA status register */

 restore vblank if necessary */

		/*

		 * If more than 3/4 of the scanline detected a monitor,

		 * then it is assumed to be present. This works even on i830,

		 * where there isn't any way to force the border color across

		 * the screen

 Restore previous settings */

 Skip machines without VGA that falsely report hotplug events */

		/* We can not rely on the HPD pin always being correctly wired

		 * up, for example many KVM do not pass it through, and so

		 * only trust an assertion that the monitor is connected.

	/* Load detection is broken on HPD capable machines. Whoever wants a

	 * broken monitor (without edid) to work behind a broken kvm (that fails

	 * to have the right resistors for HP detection) needs to fix this up.

 for pre-945g platforms use load detect */

	/*

	 * Make sure the refs for power wells enabled during detect are

	 * dropped to avoid a new detect cycle triggered by HPD polling.

 Try to probe digital port for output in DVI-I -> VGA mode. */

/*

 * Routines for controlling stuff on the analog port

		/*

		 * On some machines (some IVB at least) CRT can be

		 * fused off, but there's no known fuse bit to

		 * indicate that. On these machine the ADPA register

		 * works normally, except the DAC enable bit won't

		 * take. So the only way to tell is attempt to enable

		 * it and see what happens.

	/*

	 * TODO: find a proper way to discover whether we need to set the the

	 * polarity and link reversal bits or not, instead of relying on the

	 * BIOS.

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/**

 * DOC: High Definition Audio over HDMI and Display Port

 *

 * The graphics and audio drivers together support High Definition Audio over

 * HDMI and Display Port. The audio programming sequences are divided into audio

 * codec and controller enable and disable sequences. The graphics driver

 * handles the audio codec sequences, while the audio driver handles the audio

 * controller sequences.

 *

 * The disable sequences must be performed before disabling the transcoder or

 * port. The enable sequences may only be performed after enabling the

 * transcoder and port, and after completed link training. Therefore the audio

 * enable/disable sequences are part of the modeset sequence.

 *

 * The codec and controller sequences could be done either parallel or serial,

 * but generally the ELDV/PD change in the codec sequence indicates to the audio

 * driver that the controller sequence should start. Indeed, most of the

 * co-operation between the graphics and audio drivers is handled via audio

 * related registers. (The notable exception is the power management, not

 * covered here.)

 *

 * The struct &i915_audio_component is used to interact between the graphics

 * and audio drivers. The struct &i915_audio_component_ops @ops in it is

 * defined in graphics driver and called in audio driver. The

 * struct &i915_audio_component_audio_ops @audio_ops is called from i915 driver.

 DP N/M table */

 Values according to DP 1.4 Table 2-104 */

 default per bspec */

 HDMI N/CTS table */

 Appendix C - N & CTS values for deep color from HDMI 2.0 spec*/

 HDMI N/CTS table for 10 bit deep color(30 bpp)*/

 HDMI N/CTS table for 12 bit deep color(36 bpp)*/

 get AUD_CONFIG_PIXEL_CLOCK_HDMI_* value for mode */

 Invalidate ELD */

 ELD buffer size */

	/*

	 * Let's disable "Enable CTS or M Prog bit"

	 * and let HW calculate the value

 Disable timestamps */

 Invalidate ELD */

 fec= 0.972261, using rounding multiplier of 1000000 */

 Get hblank early enable value required */

 Get samples room value required */

 Program 0 i.e "All Samples available in buffer" */

 Enable Audio WA for 4k DSC usecases */

 Enable audio presence detect, invalidate ELD */

	/*

	 * FIXME: We're supposed to wait for vblank here, but we have vblanks

	 * disabled during the mode set. The proper fix would be to push the

	 * rest of the setup into a vblank work item, queued here, but the

	 * infrastructure is not there yet.

 Reset ELD write address */

 Up to 84 bytes of hw ELD buffer */

 ELD valid */

 Enable timestamps */

 Disable timestamps */

 Invalidate ELD */

	/*

	 * FIXME: We're supposed to wait for vblank here, but we have vblanks

	 * disabled during the mode set. The proper fix would be to push the

	 * rest of the setup into a vblank work item, queued here, but the

	 * infrastructure is not there yet.

 Invalidate ELD */

 Reset ELD write address */

 Up to 84 bytes of hw ELD buffer */

 ELD valid */

 Enable timestamps */

/**

 * intel_audio_codec_enable - Enable the audio codec for HD audio

 * @encoder: encoder on which to enable audio

 * @crtc_state: pointer to the current crtc state.

 * @conn_state: pointer to the current connector state.

 *

 * The enable sequences may only be performed after enabling the transcoder and

 * port, and after completed link training.

 FIXME precompute the ELD in .compute_config() */

 referred in audio callbacks */

 audio drivers expect pipe = -1 to indicate Non-MST cases */

/**

 * intel_audio_codec_disable - Disable the audio codec for HD audio

 * @encoder: encoder on which to disable audio

 * @old_crtc_state: pointer to the old crtc state.

 * @old_conn_state: pointer to the old connector state.

 *

 * The disable sequences must be performed before disabling the transcoder or

 * port.

 audio drivers expect pipe = -1 to indicate Non-MST cases */

/**

 * intel_init_audio_hooks - Set up chip specific audio hooks

 * @dev_priv: device private

 need to hold at least one crtc lock for the global state */

 Catch potential impedance mismatches before they occur! */

 Force CDCLK to 2*BCLK as long as we need audio powered. */

 Stop forcing CDCLK to 2*BCLK if no need for audio to be powered. */

	/*

	 * Enable/disable generating the codec wake signal, overriding the

	 * internal logic to generate the codec wake to controller.

 Get CDCLK in kHz  */

/*

 * get the intel_encoder according to the parameter port and pipe

 * intel_encoder is saved by the index of pipe

 * MST & (pipe >= 0): return the av_enc_map[pipe],

 *   when port is matched

 * MST & (pipe < 0): this is invalid

 * Non-MST & (pipe >= 0): only pipe = 0 (the first device entry)

 *   will get the right intel_encoder with port matched

 * Non-MST & (pipe < 0): get the right intel_encoder with port matched

 MST */

		/*

		 * when bootup, audio driver may not know it is

		 * MST or not. So it will poll all the port & pipe

		 * combinations

 Non-MST */

 1. get the pipe */

 port must be valid now, otherwise the pipe will be invalid */

/**

 * i915_audio_component_init - initialize and register the audio component

 * @dev_priv: i915 device instance

 *

 * This will register with the component framework a child component which

 * will bind dynamically to the snd_hda_intel driver's corresponding master

 * component when the latter is registered. During binding the child

 * initializes an instance of struct i915_audio_component which it receives

 * from the master. The master can then start to use the interface defined by

 * this struct. Each side can break the binding at any point by deregistering

 * its own component after which each side's component unbind callback is

 * called.

 *

 * We ignore any error during registration and continue with reduced

 * functionality (i.e. without HDMI audio).

 continue with reduced functionality */

 use BIOS provided value for TGL and RKL unless it is a known bad value */

/**

 * i915_audio_component_cleanup - deregister the audio component

 * @dev_priv: i915 device instance

 *

 * Deregisters the audio component, breaking any existing binding to the

 * corresponding snd_hda_intel driver's master component.

/**

 * intel_audio_init() - Initialize the audio driver either using

 * component framework or using lpe audio bridge

 * @dev_priv: the i915 drm device private data

 *

/**

 * intel_audio_deinit() - deinitialize the audio driver

 * @dev_priv: the i915 drm device private data

 *

/*

 * Copyright 2006 Dave Airlie <airlied@linux.ie>

 * Copyright Â© 2006-2007 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

 For some ch7010 */

 Might also be 0x44, 0x84, 0xc4 */

 XXX: Validate clock range */

	/*

	 * If we have timings from the BIOS for the panel, put them in

	 * to the adjusted mode.  The CRTC will be set up for this mode,

	 * with the panel scaling set up to source from the H/VDisplay

	 * of the original mode.

 Save the data order, since I don't know what it should be set to. */

	/*

	 * We should probably have an i2c driver get_modes function for those

	 * devices which will have a fixed set of modes determined by the chip

	 * (TV-out, for example), but for now with just TMDS and LVDS,

	 * that's not the case.

/*

 * Attempts to get a fixed panel timing for LVDS (currently only the i830).

 *

 * Other chips with DVO LVDS will need to extend this to deal with the LVDS

 * chip being on DVOB/C and having multiple pipes.

 Now, try to find a controller */

		/*

		 * Allow the I2C driver info to specify the GPIO to be used in

		 * special cases, but otherwise default to what's defined

		 * in the spec.

		/*

		 * Set up the I2C bus necessary for the chip we're probing.

		 * It appears that everything is on GPIOE except for panels

		 * on i830 laptops, which are on GPIOB (DVOA).

		/*

		 * GMBUS NAK handling seems to be unstable, hence let the

		 * transmitter detection run in bit banging mode for now.

		/*

		 * ns2501 requires the DVO 2x clock before it will

		 * respond to i2c accesses, so make sure we have

		 * have the clock enabled before we attempt to

		 * initialize the device.

 restore the DVO 2x clock state to original */

			/*

			 * For our LVDS chipsets, we should hopefully be able

			 * to dig the fixed panel mode out of the BIOS data.

			 * However, it's in a different format from the BIOS

			 * data on chipsets with integrated LVDS (stored in AIM

			 * headers, likely), so for now, just get the current

			 * mode being output through DVO.

/*

 * Copyright Â© 2013 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Author: Damien Lespiau <damien.lespiau@intel.com>

 *

	/*

	 * When the pipe CRC tap point is after the transcoders we need

	 * to tweak symbol-level features to produce a deterministic series of

	 * symbols for a given frame. We need to reset those features only once

	 * a frame (instead of every nth symbol):

	 *   - DC-balance: used to ensure a better clock recovery from the data

	 *     link (SDVO)

	 *   - DisplayPort scrambling: used for EMI reduction

		/*

		 * The DP CRC source doesn't work on g4x.

		 * It can be made to work to some degree by selecting

		 * the correct CRC source before the port is enabled,

		 * and not touching the CRC source bits again until

		 * the port is disabled. But even then the bits

		 * eventually get stuck and a reboot is needed to get

		 * working CRCs on the pipe again. Let's simply

		 * refuse to use DP CRCs on g4x.

 shut up gcc */

 Don't need pipe_crc->lock here, IRQs are not generated. */

 Swallow crc's until we stop generating them. */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020-2021 Intel Corporation

 just trace the final value */

	/*

	 * The clock divider is based off the hrawclk, and would like to run at

	 * 2MHz.  So, take the hrawclk value and divide by 2000 and use that

	/*

	 * The clock divider is based off the cdclk or PCH rawclk, and would

	 * like to run at 2MHz.  So, take the cdclk or PCH rawclk value and

	 * divide by 2000 and use that

 Workaround for non-ULT HSW */

	/*

	 * SKL doesn't need us to program the AUX clock divider (Hardware will

	 * derive the clock from CDCLK automatically). We still implement the

	 * get_aux_clock_divider vfunc to plug-in into the existing code.

 Max timeout value on G4x-BDW: 1.6ms */

	/*

	 * Max timeout values:

	 * SKL-GLK: 1.6ms

	 * ICL+: 4ms

	/*

	 * We will be called with VDD already enabled for dpcd/edid/oui reads.

	 * In such cases we want to leave VDD enabled and it's up to upper layers

	 * to turn it off. But for eg. i2c-dev access we need to turn it on/off

	 * ourselves.

	/*

	 * dp aux is extremely sensitive to irq latency, hence request the

	 * lowest possible wakeup latency and so prevent the cpu from going into

	 * deep sleep states.

 Try to wait for any previous AUX channel activity */

 just trace the final value */

 Only 5 data registers! */

 Must try at least 3 times according to DP spec */

 Load the send data into the aux channel data registers */

 Send the command and wait for it to complete */

 Clear done status and any errors */

			/*

			 * DP CTS 1.2 Core Rev 1.1, 4.2.1.1 & 4.2.1.2

			 *   400us delay required for errors and timeouts

			 *   Timeout errors from the HW already meet this

			 *   requirement so skip to next iteration

	/*

	 * Check for timeout or receive error. Timeouts occur when the sink is

	 * not connected.

	/*

	 * Timeouts occur when the device isn't connected, so they're "normal"

	 * -- don't fill the kernel log with these

 Unload any bytes sent back from the other side */

	/*

	 * By BSpec: "Message sizes of 0 or >20 are not allowed."

	 * We have no idea of what happened so we return -EBUSY so

	 * drm layer takes care for the necessary retries.

	/*

	 * If we're trying to send the HDCP Aksv, we need to set a the Aksv

	 * select bit to inform the hardware to send the Aksv after our header

	 * since we can't access that data from software.

 0 or 1 data bytes */

 Number of bytes written in a short write. */

 Return payload size. */

			/*

			 * Assume happy day, and copy the data. The caller is

			 * expected to check msg->reply before touching it.

			 *

			 * Return payload size.

 aka AUX_CH_D_XELPD */

 aka AUX_CH_E_XELPD */

 aka AUX_CH_D_XELPD */

 aka AUX_CH_E_XELPD */

 Failure to allocate our preferred name is not critical */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

	/*

	 * wakeref == -1, means some error happened saving save_depot_stack but

	 * power should still be put down and 0 is a invalid save_depot_stack

	 * id so can be used to skip it for non TC legacy ports.

 If live status mismatches the VBT flag, trust the live status. */

 The sink can be connected only in a single mode. */

	/*

	 * On ADL-P HW/FW will wake from TCCOLD to complete the read access of

	 * registers in IOM. Note that this doesn't apply to PHY and FIA

	 * registers.

 The sink can be connected only in a single mode. */

/*

 * Return the PHY status complete flag indicating that display can acquire the

 * PHY ownership. The IOM firmware sets this flag when a DP-alt or legacy sink

 * is connected and it's ready to switch the ownership to display. The flag

 * will be left cleared when a TBT-alt sink is connected, where the PHY is

 * owned by the TBT subsystem and so switching the ownership to display is not

 * required.

/*

 * Return the PHY status complete flag indicating that display can acquire the

 * PHY ownership. The IOM firmware sets this flag when it's ready to switch

 * the ownership to display, regardless of what sink is connected (TBT-alt,

 * DP-alt, legacy or nothing). For TBT-alt sinks the PHY is owned by the TBT

 * subsystem and so switching the ownership to display is not required.

/*

 * This function implements the first part of the Connect Flow described by our

 * specification, Gen11 TypeC Programming chapter. The rest of the flow (reading

 * lanes, EDID, etc) is done as needed in the typical places.

 *

 * Unlike the other ports, type-C ports are not available to use as soon as we

 * get a hotplug. The type-C PHYs can be shared between multiple controllers:

 * display, USB, etc. As a result, handshaking through FIA is required around

 * connect and disconnect to cleanly transfer ownership with the controller and

 * set the type-C power state.

	/*

	 * Now we have to re-check the live state, in case the port recently

	 * became disconnected. Not necessary for legacy mode.

/*

 * See the comment at the connect function. This implements the Disconnect

 * Flow.

 On ADL-P the PHY complete flag is set in TBT mode as well. */

 Get power domain required to check the hotplug live status. */

 Get power domain required for resetting the mode. */

 Get power domain matching the new mode after reset. */

/*

 * The type-C ports are different because even when they are connected, they may

 * not be available/usable by the graphics driver: see the comment on

 * icl_tc_phy_connect(). So in our driver instead of adding the additional

 * concept of "usable" and make everything check for "connected and usable" we

 * define a port as "connected" when it is not only connected, but also when it

 * is usable by the rest of the driver. That maintains the old assumption that

 * connected ports are usable, and avoids exposing to the users objects they

 * can't really use.

/**

 * intel_tc_port_disconnect_phy_work: disconnect TypeC PHY from display port

 * @dig_port: digital port

 *

 * Disconnect the given digital port from its TypeC PHY (handing back the

 * control of the PHY to the TypeC subsystem). This will happen in a delayed

 * manner after each aux transactions and modeset disables.

/**

 * intel_tc_port_flush_work: flush the work disconnecting the PHY

 * @dig_port: digital port

 *

 * Flush the delayed work disconnecting an idle PHY.

	/*

	 * Disconnecting the PHY after the PHY's PLL gets disabled may

	 * hang the system on ADL-P, so disconnect the PHY here synchronously.

	 * TODO: remove this once the root cause of the ordering requirement

	 * is found/fixed.

	/*

	 * Each Modular FIA instance houses 2 TC ports. In SOC that has more

	 * than two TC ports, there are multiple instances of Modular FIA.

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

/**

 * DOC: DMC Firmware Support

 *

 * From gen9 onwards we have newly added DMC (Display microcontroller) in display

 * engine to save and restore the state of display engine when it enter into

 * low-power state and comes back to normal.

 0x09 for DMC */

 Includes the DMC specific header in dwords */

 always value would be 0x10000 */

 Not used */

 Not used */

 in YYYYMMDD format */

 Size in dwords (CSS_Headerlen + PackageHeaderLen + dmc FWsLen)/4 */

 Not used */

 Not used */

 Not used */

 Not used */

 Major Minor */

 Not used */

 Not used */

 reserved on package_header version 1, must be 0 on version 2 */

 Stepping (A, B, C, ..., *). * is a wildcard */

 Sub-stepping (0, 1, ..., *). * is a wildcard */

 DMC container header length in dwords */

 0x01, 0x02 */

 Number of valid entries in the FWInfo array below */

 always value would be 0x40403E3E */

 DMC binary header length */

 0x01 */

 Reserved */

 Major, Minor */

 Firmware program size (excluding header) in dwords */

 Major Minor version */

 Number of valid MMIO cycles present. */

 MMIO address */

 MMIO data */

 FW filename  */

 DMC RAM start MMIO address */

 FW filename */

 Number of valid MMIO cycles present. */

 MMIO address */

 MMIO data */

 The below bit doesn't need to be cleared ever afterwards */

/**

 * intel_dmc_load_program() - write the firmware from memory to register.

 * @dev_priv: i915 drm device.

 *

 * DMC firmware is read from a .bin file and kept in internal memory one time.

 * Everytime display comes back from low power state this function is called to

 * copy the firmware from internal memory to registers.

	    /*

	     * If we don't find a more specific one from above two checks, we

	     * then check for the generic one to be sure to work even with

	     * "broken firmware"

/*

 * Search fw_info table for dmc_offset to find firmware binary: num_entries is

 * already sanitized.

		/* More specific versions come first, so we don't even have to

		 * check for the stepping since we already found a previous FW

		 * for this id.

	/*

	 * Check if we can access common fields, we will checkc again below

	 * after we have read the version

 Cope with small differences between v1 and v3 */

 header_len is in dwords */

 Cache the dmc header info. */

 fw_size is in dwords, so multiplied by 4 to convert into bytes. */

	/*

	 * We should always have space for max_entries,

	 * even if not all are used

 dmc_offset is in dwords */

 Return number of bytes parsed or 0 on error */

 Extract CSS Header information */

 Extract Package Header information */

/**

 * intel_dmc_ucode_init() - initialize the firmware loading.

 * @dev_priv: i915 drm device.

 *

 * This function is called at the time of loading the display driver to read

 * firmware from a .bin file and copied into a internal memory.

	/*

	 * Obtain a runtime pm reference, until DMC is loaded, to avoid entering

	 * runtime-suspend.

	 *

	 * On error, we return with the rpm wakeref held to prevent runtime

	 * suspend as runtime suspend *requires* a working DMC for whatever

	 * reason.

 Bypass version check for firmware override. */

/**

 * intel_dmc_ucode_suspend() - prepare DMC firmware before system suspend

 * @dev_priv: i915 drm device

 *

 * Prepare the DMC firmware before entering system suspend. This includes

 * flushing pending work items and releasing any resources acquired during

 * init.

 Drop the reference held in case DMC isn't loaded. */

/**

 * intel_dmc_ucode_resume() - init DMC firmware during system resume

 * @dev_priv: i915 drm device

 *

 * Reinitialize the DMC firmware during system resume, reacquiring any

 * resources released in intel_dmc_ucode_suspend().

	/*

	 * Reacquire the reference to keep RPM disabled in case DMC isn't

	 * loaded.

/**

 * intel_dmc_ucode_fini() - unload the DMC firmware.

 * @dev_priv: i915 drm device.

 *

 * Firmmware unloading includes freeing the internal memory and reset the

 * firmware loading status.

/*

 * Copyright Â© 2006-2008 Intel Corporation

 *   Jesse Barnes <jesse.barnes@intel.com>

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *    Eric Anholt <eric@anholt.net>

 *

/** @file

 * Integrated TV-out support for the 915GM and 945GM.

/*

 * Color conversion values have 3 separate fixed point formats:

 *

 * 10 bit fields (ay, au)

 *   1.9 fixed point (b.bbbbbbbbb)

 * 11 bit fields (ry, by, ru, gu, gv)

 *   exp.mantissa (ee.mmmmmmmmm)

 *   ee = 00 = 10^-1 (0.mmmmmmmmm)

 *   ee = 01 = 10^-2 (0.0mmmmmmmmm)

 *   ee = 10 = 10^-3 (0.00mmmmmmmmm)

 *   ee = 11 = 10^-4 (0.000mmmmmmmmm)

 * 12 bit fields (gy, rv, bu)

 *   exp.mantissa (eee.mmmmmmmmm)

 *   eee = 000 = 10^-1 (0.mmmmmmmmm)

 *   eee = 001 = 10^-2 (0.0mmmmmmmmm)

 *   eee = 010 = 10^-3 (0.00mmmmmmmmm)

 *   eee = 011 = 10^-4 (0.000mmmmmmmmm)

 *   eee = 100 = reserved

 *   eee = 101 = reserved

 *   eee = 110 = reserved

 *   eee = 111 = 10^0 (m.mmmmmmmm) (only usable for 1.0 representation)

 *

 * Saturation and contrast are 8 bits, with their own representation:

 * 8 bit field (saturation, contrast)

 *   exp.mantissa (ee.mmmmmm)

 *   ee = 00 = 10^-1 (0.mmmmmm)

 *   ee = 01 = 10^0 (m.mmmmm)

 *   ee = 10 = 10^1 (mm.mmmm)

 *   ee = 11 = 10^2 (mmm.mmm)

 *

 * Simple conversion function:

 *

 * static u32

 * float_to_csc_11(float f)

 * {

 *     u32 exp;

 *     u32 mant;

 *     u32 ret;

 *

 *     if (f < 0)

 *         f = -f;

 *

 *     if (f >= 1) {

 *         exp = 0x7;

 *	   mant = 1 << 8;

 *     } else {

 *         for (exp = 0; exp < 3 && f < 0.5; exp++)

 *	   f *= 2.0;

 *         mant = (f * (1 << 9) + 0.5);

 *         if (mant >= (1 << 9))

 *             mant = (1 << 9) - 1;

 *     }

 *     ret = (exp << 9) | mant;

 *     return ret;

 * }

/*

 * Behold, magic numbers!  If we plant them they might grow a big

 * s-video cable to the sky... or something.

 *

 * Pre-converted to appropriate hex value.

/*

 * PAL & NTSC values for composite & s-video connections

/*

 * Component connections

 in millihertz (for precision) */

	/*

	 * subcarrier programming

	/*

	 * blank/black levels

/*

 * Sub carrier DDA

 *

 *  I think this works as follows:

 *

 *  subcarrier freq = pixel_clock * (dda1_inc + dda2_inc / dda2_size) / 4096

 *

 * Presumably, when dda3 is added in, it gets to adjust the dda2_inc value

 *

 * So,

 *  dda1_ideal = subcarrier/pixel * 4096

 *  dda1_inc = floor (dda1_ideal)

 *  dda2 = dda1_ideal - dda1_inc

 *

 *  then pick a ratio for dda2 that gives the closest approximation. If

 *  you can't get close enough, you can play with dda3 as well. This

 *  seems likely to happen when dda2 is small as the jumps would be larger

 *

 * To invert this,

 *

 *  pixel_clock = subcarrier * 4096 / (dda1_inc + dda2_inc / dda2_size)

 *

 * The constants below were all computed using a 107.520MHz clock

/*

 * Register programming values for TV modes.

 *

 * These values account for -1s required.

 525 Lines, 60 Fields, 15.734KHz line, Sub-Carrier 3.580MHz */

 desired 3.5800000 actual 3.5800000 clock 107.52 */

 525 Lines, 60 Fields, 15.734KHz line, Sub-Carrier 4.43MHz */

 desired 4.4336180 actual 4.4336180 clock 107.52 */

 525 Lines, 60 Fields, 15.734KHz line, Sub-Carrier 3.580MHz */

 desired 3.5800000 actual 3.5800000 clock 107.52 */

 525 Lines, 60 Fields, 15.734KHz line, Sub-Carrier 3.580MHz */

 desired 3.5800000 actual 3.5800000 clock 107.52 */

 625 Lines, 50 Fields, 15.625KHz line, Sub-Carrier 4.434MHz */

 desired 4.4336180 actual 4.4336180 clock 107.52 */

 625 Lines, 50 Fields, 15.625KHz line, Sub-Carrier 4.434MHz */

 desired 4.4336180 actual 4.4336180 clock 107.52 */

	/*

	 * May need to override the user margins for

	 * gen3 >1024 wide source vertical centering.

 Prevents vblank waits from timing out in intel_tv_detect_type() */

 Ensure TV refresh is close to desired refresh */

	/*

	 * tv_mode horizontal timings:

	 *

	 * hsync_end

	 *    | hblank_end

	 *    |    | hblank_start

	 *    |    |       | htotal

	 *    |     _______    |

	 *     ____/       \___

	 * \__/                \

	/*

	 * tv_mode vertical timings:

	 *

	 * vsync_start

	 *    | vsync_end

	 *    |  | vi_end nbr_end

	 *    |  |    |       |

	 *    |  |     _______

	 * \__    ____/       \

	 *    \__/

 TV has it's own notion of sync and other mode flags, so clear them. */

 pixel counter doesn't work on i965gm TV output */

 Need to turn off the vertical filter and center the image */

 Attempt to maintain the relative sizes of the margins */

	/*

	 * The pipe scanline counter behaviour looks as follows when

	 * using the TV encoder:

	 *

	 * time ->

	 *

	 * dsl=vtotal-1       |             |

	 *                   ||            ||

	 *               ___| |        ___| |

	 *              /     |       /     |

	 *             /      |      /      |

	 * dsl=0   ___/       |_____/       |

	 *        | | |  |  | |

	 *         ^ ^ ^   ^ ^

	 *         | | |   | pipe vblank/first part of tv vblank

	 *         | | |   bottom margin

	 *         | | active

	 *         | top margin

	 *         remainder of tv vblank

	 *

	 * When the TV encoder is used the pipe wants to run faster

	 * than expected rate. During the active portion the TV

	 * encoder stalls the pipe every few lines to keep it in

	 * check. When the TV encoder reaches the bottom margin the

	 * pipe simply stops. Once we reach the TV vblank the pipe is

	 * no longer stalled and it runs at the max rate (apparently

	 * oversample clock on gen3, cdclk on gen4). Once the pipe

	 * reaches the pipe vtotal the pipe stops for the remainder

	 * of the TV vblank/top margin. The pipe starts up again when

	 * the TV encoder exits the top margin.

	 *

	 * To avoid huge hassles for vblank timestamping we scale

	 * the pipe timings as if the pipe always runs at the average

	 * rate it maintains during the active period. This also

	 * gives us a reasonable guesstimate as to the pixel rate.

	 * Due to the variation in the actual pipe speed the scanline

	 * counter will give us slightly erroneous results during the

	 * TV vblank/margins. But since vtotal was selected such that

	 * it matches the average rate of the pipe during the active

	 * portion the error shouldn't cause any serious grief to

	 * vblank timestamps.

	 *

	 * For posterity here is the empirically derived formula

	 * that gives us the maximum length of the pipe vblank

	 * we can use without causing display corruption. Following

	 * this would allow us to have a ticking scanline counter

	 * everywhere except during the bottom margin (there the

	 * pipe always stops). Ie. this would eliminate the second

	 * flat portion of the above graph. However this would also

	 * complicate vblank timestamping as the pipe vtotal would

	 * no longer match the average rate the pipe runs at during

	 * the active portion. Hence following this formula seems

	 * more trouble that it's worth.

	 *

	 * if (GRAPHICS_VER(dev_priv) == 4) {

	 *	num = cdclk * (tv_mode->oversample >> !tv_mode->progressive);

	 *	den = tv_mode->clock;

	 * } else {

	 *	num = tv_mode->oversample >> !tv_mode->progressive;

	 *	den = 1;

	 * }

	 * max_pipe_vblank_len ~=

	 *	(num * tv_htotal * (tv_vblank_len + top_margin)) /

	 *	(den * pipe_htotal);

 pixel counter doesn't work on i965gm TV output */

 can't happen (mode_prepare prevents this) */

 Enable two fixes for the chips that need them. */

 Filter ctl must be set before TV_WIN_SIZE */

 Disable TV interrupts around load detect or we'll recurse */

 Poll for TV detection */

	/*

	 * The TV sense state should be cleared to zero on cantiga platform. Otherwise

	 * the TV is misdetected. This is hardware requirement.

	/*

	 *  A B C

	 *  0 1 1 Composite

	 *  1 0 X svideo

	 *  0 0 0 Component

 For unknown reasons the hw barfs if we don't do this vblank wait. */

 Restore interrupt config */

/*

 * Here we set accurate tv format according to connector type

 * i.e Component TV should not be assigned by NTSC or PAL

 Component supports everything so we can keep the current mode */

 If the current mode is fine don't change it */

 Choose preferred mode according to line number of TV format */

 prefer 480 line modes for all SD TV modes */

 no vertical scaling with wide sources on gen3 */

		/*

		 * We take the TV mode and scale it to look

		 * like it had the expected h/vdisplay. This

		 * provides the most information to userspace

		 * about the actual timings of the mode. We

		 * do ignore the margins though.

 Force a modeset. */

	/*

	 * Sanity check the TV output by checking to see if the

	 * DAC register holds a value

	/*

	 * If the register does not hold the state change enable

	 * bit, (either as a 0 or a 1), assume it doesn't really

	 * exist

	/*

	 * The documentation, for the older chipsets at least, recommend

	 * using a polling method rather than hotplug detection for TVs.

	 * This is because in order to perform the hotplug detection, the PLLs

	 * for the TV must be kept alive increasing power drain and starving

	 * bandwidth from other encoders. Notably for instance, it causes

	 * pipe underruns on Crestline when this encoder is supposedly idle.

	 *

	 * More recent chipsets favour HDMI rather than integrated S-Video.

 BIOS margin values */

 Create TV properties then attach current values */

 1080p50/1080p60 not supported on gen3 */

/*

 * Copyright 2008 Intel Corporation <hong.liu@intel.com>

 * Copyright 2008 Red Hat <mjg@redhat.com>

 *

 * Permission is hereby granted, free of charge, to any person obtaining

 * a copy of this software and associated documentation files (the

 * "Software"), to deal in the Software without restriction, including

 * without limitation the rights to use, copy, modify, merge, publish,

 * distribute, sub license, and/or sell copies of the Software, and to

 * permit persons to whom the Software is furnished to do so, subject to

 * the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial

 * portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NON-INFRINGEMENT.  IN NO EVENT SHALL INTEL AND/OR ITS SUPPLIERS BE

 * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 current lid state indicator */

 current docking state indicator */

 OpRegion mailbox #1: public ACPI methods */

 driver readiness */

 notification status */

 current event */

 supported display devices ID list */

 currently presented display list */

 currently active display list */

 next active devices list */

 ASL sleep time-out */

 toggle table index */

 current hotplug enable indicator */

 current lid state*/

 current docking state */

 Sx state resume */

 ASL supported events */

 current OS notification */

 driver status */

 extended supported display devices ID list */

 extended attached display devices list */

 OpRegion mailbox #2: SWSCI */

 SWSCI command|status|data */

 command parameters */

 driver sleep time-out */

 OpRegion mailbox #3: ASLE */

 driver readiness */

 ASLE interrupt command */

 technology enabled indicator */

 current ALS illuminance reading */

 backlight brightness to set */

 panel fitting state */

 current brightness level */

 backlight level duty cycle mapping table */

 current panel fitting mode */

 enabled panel fitting modes */

 panel LUT and identifier */

 PWM freq and min brightness */

 color correction default values */

 power conservation features */

 supported rotation angles */

 IUER events */

	u64 rvda;	/* Physical (2.0) or relative from opregion (2.1+)

 Size of raw vbt data */

 OpRegion mailbox #5: ASLE ext */

 Panel Header */

 Panel EDID */

 Driver readiness indicator */

 ASLE Interrupt Command (ASLC) bits */

 response bits */

 Technology enabled indicator */

 ASLE backlight brightness to set */

 ASLE panel fitting request */

 PWM frequency and minimum brightness */

 IUER */

 Software System Control Interrupt (SWSCI) */

 SWSCI: Get BIOS Data (GBDA) */

 SWSCI: System BIOS Callbacks (SBCB) */

 Check if we can call the function. See swsci_setup for details. */

 Driver sleep timeout in ms. */

		/* The spec says 2ms should be the default, but it's too small

 Hey bios, trust must be earned. */

 The spec tells us to do this, but we are the only user... */

 Ensure SCI event is selected and event trigger is cleared. */

 Use event trigger to tell bios to check the mail. */

 Poll for the result. */

 Note: scic == 0 is an error! */

 don't care about old stuff for now */

	/*

	 * Update backlight on all connectors that support backlight (usually

	 * only one).

	/* alsi is the current ALS reading in lux. 0 indicates below sensor

	/* Panel fitting is currently controlled by the X code, so this is a

/*

 * The only video events relevant to opregion are 0x80. These indicate either a

 * docking event, lid switch or display switch request. In Linux, these are

 * handled by the dock, button and video drivers.

/*

 * Initialise the DIDL field in opregion. This passes a list of devices to

 * the firmware. Values are defined by section B.4.2 of the ACPI specification

 * (version 3)

	/*

	 * In theory, did2, the extended didl, gets added at opregion version

	 * 3.0. In practice, however, we're supposed to set it for earlier

	 * versions as well, since a BIOS that doesn't understand did2 should

	 * not look at it anyway. Use a variable so we can tweak this if a need

	 * arises later.

 If fewer than max outputs, the list must be null terminated */

	/*

	 * Initialize the CADL field from the connector device ids. This is

	 * essentially the same as copying from the DIDL. Technically, this is

	 * not always correct as display outputs may exist, but not active. This

	 * initialization is necessary for some Clevo laptops that check this

	 * field before processing the brightness and display switching hotkeys.

	 *

	 * Note that internal panels should be at the front of the connector

	 * list already, ensuring they're not left out.

 If fewer than 8 active devices, the list must be null terminated */

 Sub-function code 0 is okay, let's allow them. */

 We use GBDA to ask for supported GBDA calls. */

 make the bits match the sub-function codes */

	/*

	 * We also use GBDA to ask for _requested_ SBCB callbacks. The driver

	 * must not call interfaces that are not specifically requested by the

	 * bios.

 here, the bits already match sub-function codes */

	/*

	 * But we use SBCB to ask for _supported_ SBCB calls. This does not mean

	 * the callback is _requested_. But we still can't call interfaces that

	 * are not requested.

 make the bits match the sub-function codes */

 bit 11 is reserved */

 best guess what to do with supported wrt requested */

 XXX: for now, trust the requested callbacks */

 opregion->swsci_sbcb_sub_functions &= tmp; */

		/*

		 * Indicate we handle monitor hotplug events ourselves so we do

		 * not need ACPI notifications for them. Disabling these avoids

		 * triggering the AML code doing the notifation, which may be

		 * broken as Windows also seems to disable these.

		/*

		 * opregion 2.0: rvda is the physical VBT address.

		 *

		 * opregion 2.1+: rvda is unsigned, relative offset from

		 * opregion base, and should never point within opregion.

	/*

	 * The VBT specification says that if the ASLE ext mailbox is not used

	 * its area is reserved, but on some CHT boards the VBT extends into the

	 * ASLE ext area. Allow this even though it is against the spec, so we

	 * do not end up rejecting the VBT on those boards (and end up not

	 * finding the LCD panel because of this).

 fall back to VBT panel type? */

	/*

	 * So far we know that some machined must use it, others must not use it.

	 * There doesn't seem to be any way to determine which way to go, except

	 * via a quirk list :(

		/*

		 * Notify BIOS we are ready to handle ACPI video ext notifs.

		 * Right now, all the events are handled by the ACPI video

		 * module. We don't actually need to do anything with them.

 Some platforms abuse the _DSM to enable MUX */

 just clear all opregion memory pointers now */

 SPDX-License-Identifier: MIT */

/*

 * Copyright (C) 2020 Google, Inc.

 *

 * Authors:

 * Sean Paul <seanpaul@chromium.org>

 Output An first, that's easy */

	/*

	 * Since Aksv is Oh-So-Secret, we can't access it in software. So we

	 * send an empty buffer of the correct length through the DP helpers. On

	 * the other side, in the transfer hook, we'll generate a flag based on

	 * the destination address which will tickle the hardware to output the

	 * Aksv on our behalf after the header is sent.

	/*

	 * For some reason the HDMI and DP HDCP specs call this register

	 * definition by different names. In the HDMI spec, it's called BSTATUS,

	 * but in DP it's called BINFO.

 KSV list is read via 15 byte window (3 entries @ 5 bytes each) */

 Not used for single stream DisplayPort setups */

 Added for non_paired situation */

 Timeout to read entire msg */

 local define to shovel this through the write_2_2 interface */

	/*

	 * There is no way to detect the CERT, LPRIME and STREAM_READY

	 * availability. So Wait for timeout and read the msg.

		/*

		 * As we want to check the msg availability at timeout, Ignoring

		 * the timeout at wait for CP_IRQ.

 No msg_id in DP HDCP2.2 msgs */

 DP adaptation msgs has no msg_id */

 Entire msg read timeout since initiate of msg read */

	/*

	 * Errata for DP: As Stream type is used for encryption, Receiver

	 * should be communicated with stream type for the decryption of the

	 * content.

	 * Repeater will be communicated with stream type as a part of it's

	 * auth later in time.

 Wait for encryption confirmation */

 Wait for encryption confirmation */

	/*

	 * We do need to do the Link Check only for the connector involved with

	 * HDCP port authentication and encryption.

	 * We can re-use the hdcp->is_repeater flag to know that the connector

	 * involved with HDCP port authentication and encryption.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

	/*

	 * Note that we ignore PTE_READ_ONLY here. The caller must be careful

	 * not to allow the user to override access to a read only page.

 Applicable to VLV (gen8+ do not support RO in the GGTT) */

	/*

	 * Without aliasing PPGTT there's no difference between

	 * GLOBAL/LOCAL_BIND, it's all the same ptes. Hence unconditionally

	 * upgrade to both bound if we bind either to avoid double-binding.

/*

 * Copyright Â© 2008 Intel Corporation

 *             2014 Red Hat Inc.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

	/*

	 * for MST we always configure max link bw - the spec doesn't

	 * seem to suggest we should do otherwise.

	/*

	 * FIXME: If all the streams can't fit into the link with

	 * their current pipe_bpp we should reduce pipe_bpp across

	 * the board until things start to fit. Until then we

	 * limit to <= 8bpc since that's what was hardcoded for all

	 * MST streams previously. This hack should be removed once

	 * we have the proper retry logic in place.

/*

 * Iterate over all connectors and return a mask of

 * all CPU transcoders streaming over the same DP link.

 lowest numbered transcoder will be designated master */

/*

 * If one of the connectors in a MST stream needs a modeset, mark all CRTCs

 * that shares the same MST stream as mode changed,

 * intel_modeset_pipe_config()+intel_crtc_check_fastset() will take care to do

 * a fastset when possible.

	/* We only want to free VCPI if this state disables the CRTC on this

	 * connector

	/*

	 * Power down mst path before disabling the port, otherwise we end

	 * up getting interrupts from the sink upon detecting link loss.

	/*

	 * BSpec 4287: disable DIP after the transcoder is disabled and before

	 * the transcoder clock select is set to none.

	/*

	 * From TGL spec: "If multi-stream slave transcoder: Configure

	 * Transcoder Clock Select to direct no clock to the transcoder"

	 *

	 * From older GENs spec: "Configure Transcoder Clock Select to direct

	 * no clock to the transcoder"

	/* MST encoders are bound to a crtc, not to a connector,

	 * force the mapping here for get_hw_state.

	/*

	 * Before Gen 12 this is not done as part of

	 * dig_port->base.pre_enable() and should be done here. For

	 * Gen 12+ the step in which this should be done is different for the

	 * first MST stream, so it's done on the DDI for the first stream and

	 * here for the following ones.

 Enable hdcp if it's desired */

	/*

	 * Reuse the prop from the SST connector because we're

	 * not allowed to create new props after device registration.

	/*

	 * This is wrong, but broken userspace uses the intersection

	 * of possible_crtcs of all the encoders of a given connector

	 * to figure out which crtcs can drive said connector. What

	 * should be used instead is the union of possible_crtcs.

	 * To keep such userspace functioning we must misconfigure

	 * this to make sure the intersection is not empty :(

 create encoders */

 encoders will get killed by normal cleanup */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 two pixels per clock */

 two pixels per clock */

		/*

		 * Validated limit is 4k, but has 5k should

		 * work apart from the following features:

		 * - Ytile (already limited to 4k)

		 * - FP16 (already limited to 4k)

		 * - render compression (already limited to 4k)

		 * - KVMR sprite and cursor (don't care)

		 * - horizontal panning (TODO verify this)

		 * - pipe and plane scaling (TODO verify this)

 FIXME AUX plane? */

 FIXME AUX plane? */

 Wa_14011264657, Wa_14011050563: gen11+ */

		/*

		 * The stride in bytes must not exceed of the size

		 * of 128K bytes. For pixel formats of 64bpp will allow

		 * for a 16K pixel surface.

		/*

		 * "The stride in bytes must not exceed the

		 * of the size of 8K pixels and 32K bytes."

 Preoffset values for YUV to RGB Conversion */

/*

 * Programs the input color space conversion stage for ICL HDR planes.

 * Note that it is assumed that this stage always happens after YUV

 * range correction. Thus, the input to this stage is assumed to be

 * in full-range YCbCr.

		/*

		 * BT.601 full range YCbCr -> full range RGB

		 * The matrix required is :

		 * [1.000, 0.000, 1.371,

		 *  1.000, -0.336, -0.698,

		 *  1.000, 1.732, 0.0000]

		/*

		 * BT.709 full range YCbCr -> full range RGB

		 * The matrix required is :

		 * [1.000, 0.000, 1.574,

		 *  1.000, -0.187, -0.468,

		 *  1.000, 1.855, 0.0000]

		/*

		 * BT.2020 full range YCbCr -> full range RGB

		 * The matrix required is :

		 * [1.000, 0.000, 1.474,

		 *  1.000, -0.1645, -0.5713,

		 *  1.000, 1.8814, 0.0000]

	/*

	 * The stride is either expressed as a multiple of 64 bytes chunks for

	 * linear buffers or in number of tiles for tiled buffers.

	/*

	 * DRM_MODE_ROTATE_ is counter clockwise to stay compatible with Xrandr

	 * while i915 HW rotation is clockwise, thats why this swapping.

 Wa_22012358565:adl-p */

		/*

		 * The DPT object contains only one vma, so the VMA's offset

		 * within the DPT is always 0.

 Sizes are 0 based */

 The scaler will handle the output position */

	/*

	 * Enable the scaler before the plane so that we don't

	 * get a catastrophic underrun even if the two operations

	 * end up happening in two different frames.

	/*

	 * The control register self-arms if the plane was previously

	 * disabled. Try to make the plane enable atomic by writing

	 * the control register just before the surface register.

	/*

	 * FIXME: pxp session invalidation can hit any time even at time of commit

	 * or after the commit, display content will be garbage.

 Program the UV plane on planar master */

		/*

		 * 90/270 is not allowed with RGB64 16:16:16:16 and

		 * Indexed 8-bit. RGB 16-bit 5:6:5 is allowed gen11 onwards.

 Y-tiling is not supported in IF-ID Interlace mode */

 Wa_1606054188:tgl,adl-s */

	/*

	 * Display WA #1175: glk

	 * Planes other than the cursor may cause FIFO underflow and display

	 * corruption if starting less than 4 pixels from the right edge of

	 * the screen.

	 * Besides the above WA fix the similar problem, where planes other

	 * than the cursor ending less than 4 pixels from the left edge of the

	 * screen may cause FIFO underflow and display corruption.

 Display WA #1106 */

	/*

	 * We don't yet know the final source width nor

	 * whether we can use the HQ scaler mode. Assume

	 * the best case.

	 * FIXME need to properly check this later.

	/*

	 * AUX surface offset is specified as the distance from the

	 * main surface offset, and it must be non-negative. Make

	 * sure that is what we will get.

	/*

	 * When using an X-tiled surface, the plane blows up

	 * if the x offset + width exceed the stride.

	 *

	 * TODO: linear and Y-tiled seem fine, Yf untested,

	/*

	 * CCS AUX surface doesn't have its own x/y offsets, we must make sure

	 * they match with the main surface x/y offsets.

	/*

	 * Put the final coordinates back so that the src

	 * coordinate checks will see the right values.

 FIXME not quite sure how/if these apply to the chroma plane */

	/*

	 * Handle the AUX surface first since the main surface setup depends on

	 * it.

 use scaler when colorkey is not required */

 HW only has 8 bits pixel precision, disable plane if invisible */

 Enable and use MPEG-2 chroma siting */

 Display WA #0870: skl, bxt */

 Wa_14010477008:tgl[a0..c0],rkl[all],dg1[all] */

 Wa_22011186057 */

 Wa_22011186057 */

 Wa_22011186057 */

	/*

	 * DRM_MODE_ROTATE_ is counter clockwise to stay compatible with Xrandr

	 * while i915 HW rotation is clockwise, thats why this swapping.

 90/270 degree rotation would require extra work */

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Author: Shobhit Kumar <shobhit.kumar@intel.com>

 *

 base offsets for gpio pads */

 ICL DSI Display GPIO Pins */

	/* For DSI single link on Port A & C, the seq_port value which is

	 * parsed from Sequence Block#53 of VBT has been set to 0

	 * Now, read/write of packets for the DSI single link on Port A and

	 * Port C will based on the DVO port from VBT block 2.

 XXX: this assumes vlv_gpio_table only has NC GPIOs. */

 FIXME: remove constant below */

 XXX: it's unclear whether 255->57 is part of SE. */

 XXX: The spec is unclear about CHV GPIO on seq v2 */

 XXX: this table is a quick ugly hack. */

 gpio source in sequence v2 only */

 pull up/down */

 byte 0 aka PMIC Flag is reserved */

/*

 * MIPI Sequence from VBT #53 parsing logic

 * We have already separated each seqence during bios parsing

 * Following is generic execution function for any sequence

 Skip Sequence Byte. */

 Skip Size of Sequence. */

 Size of Operation. */

 Consistency check if we have size. */

 We have size, skip. */

 No size, can't skip without parsing. */

 For v3 VBTs in vid-mode the delays are part of the VBT sequences */

 Starting point, adjusted depending on dual link and burst mode */

 In dual link mode each port needs half of pixel clock */

		/* we can enable pixel_overlap if needed by panel. In this

		 * case we need to increase the pixelclock for extra pixels

	/* Burst Mode Ratio

	 * Target ddr frequency from VBT / non burst ddr freq

	 * multiply by 100 to preserve remainder

			/*

			 * Sometimes the VBT contains a slightly lower clock,

			 * then the bitrate we have calculated, in this case

			 * just replace it with the calculated bitrate.

	/* delays in VBT are in unit of 100us, so need to convert

	 * here in ms

 a regular driver would get the device in probe */

/*

 * On some BYT/CHT devs some sequences are incomplete and we need to manually

 * control some GPIOs. We need to add a GPIO lookup table before we get these.

 * If the GOP did not initialize the panel (HDMI inserted) we may need to also

 * change the pinmux for the SoC's PWM0 pin from GPIO to PWM.

 Intel GFX is consumer */

 Panel EN/DISABLE */

 Ensure PWM0 pin is muxed as PWM instead of GPIO */

/*

 * Copyright Â© 2006 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *    Eric Anholt <eric@anholt.net>

 *

* Enables the TV output path. */

* Powers down the TV out block, and DAC0-3 */

*< Low bits of horizontal active pixel input */

* High bits of horizontal active pixel input */

* High bits of vertical active line output */

*< Low bits of vertical active line output */

*< Low bits of horizontal active pixel output */

* High bits of horizontal active pixel output */

* Enables the LVDS power down state transition */

* Enables the LVDS upscaler */

* Enables the LVDS panel output path */

* Enables the LVDS panel backlight */

* Probes for a CH7017 on the given bus and slave address. */

 LVDS PLL settings from page 75 of 7017-7017ds.pdf*/

 XXX: dual channel panel detection.  Assume yes for now. */

 Turn the LVDS back on with new settings. */

 set the CH7017 power state */

 Turn off TV/VGA, and never turn it on since we don't support it. */

 Turn on the LVDS */

 Turn off the LVDS */

 XXX: Should actually wait for update power status somehow */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2018 Intel Corporation

/*

 * Some machines (Lenovo U160) do not work with SSC on LVDS for some reason

/*

 * A machine (e.g. Acer Aspire 5734Z) may need to invert the panel backlight

 * brightness value

 Some VBT's incorrectly indicate no backlight is present */

/* Toshiba Satellite P50-C-18C requires T12 delay to be min 800ms

 * which is 300 ms greater than eDP spec T12 min.

/*

 * GeminiLake NUC HDMI outputs require additional off time

 * this allows the onboard retimer to correctly sync to signal

 For systems that don't have a meaningful PCI subdevice/subvendor ID */

 DMI strings are too generic, also match on BIOS date */

 terminating entry */

 Lenovo U160 cannot use SSC on LVDS */

 Sony Vaio Y cannot use SSC on LVDS */

 Acer Aspire 5734Z must invert backlight brightness */

 Acer/eMachines G725 */

 Acer/eMachines e725 */

 Acer/Packard Bell NCL20 */

 Acer Aspire 4736Z */

 Acer Aspire 5336 */

 Acer C720 and C720P Chromebooks (Celeron 2955U) have backlights */

 Acer C720 Chromebook (Core i3 4005U) */

 Apple Macbook 2,1 (Core 2 T7400) */

 Apple Macbook 4,1 */

 Toshiba CB35 Chromebook (Celeron 2955U) */

 HP Chromebook 14 (Celeron 2955U) */

 Dell Chromebook 11 */

 Dell Chromebook 11 (2015 version) */

 Toshiba Satellite P50-C-18C */

 GeminiLake NUC */

 ASRock ITX*/

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/**

 * DOC: Panel Self Refresh (PSR/SRD)

 *

 * Since Haswell Display controller supports Panel Self-Refresh on display

 * panels witch have a remote frame buffer (RFB) implemented according to PSR

 * spec in eDP1.3. PSR feature allows the display to go to lower standby states

 * when system is idle but display is on as it eliminates display refresh

 * request to DDR memory completely as long as the frame buffer for that

 * display is unchanged.

 *

 * Panel Self Refresh must be supported by both Hardware (source) and

 * Panel (sink).

 *

 * PSR saves power by caching the framebuffer in the panel RFB, which allows us

 * to power down the link and memory controller. For DSI panels the same idea

 * is called "manual mode".

 *

 * The implementation uses the hardware-based PSR support which automatically

 * enters/exits self-refresh mode. The hardware takes care of sending the

 * required DP aux message and could even retrain the link (that part isn't

 * enabled yet though). The hardware also keeps track of any frontbuffer

 * changes to know when to exit self-refresh mode again. Unfortunately that

 * part doesn't work too well, hence why the i915 PSR support uses the

 * software frontbuffer tracking to make sure it doesn't miss a screen

 * update. For this integration intel_psr_invalidate() and intel_psr_flush()

 * get called by the frontbuffer tracking code. Note that because of locking

 * issues the self-refresh re-enable code is done from a work queue, which

 * must be correctly synchronized/cancelled when shutting down the pipe."

 *

 * DC3CO (DC3 clock off)

 *

 * On top of PSR2, GEN12 adds a intermediate power savings state that turns

 * clock off automatically during PSR2 idle state.

 * The smaller overhead of DC3co entry/exit vs. the overhead of PSR2 deep sleep

 * entry/exit allows the HW to enter a low-power state even when page flipping

 * periodically (for instance a 30fps video playback scenario).

 *

 * Every time a flips occurs PSR2 will get out of deep sleep state(if it was),

 * so DC3CO is enabled and tgl_dc3co_disable_work is schedule to run after 6

 * frames, if no other flip occurs and the function above is executed, DC3CO is

 * disabled and PSR2 is configured to enter deep sleep, resetting again in case

 * of another flip.

 * Front buffer modifications do not trigger DC3CO activation on purpose as it

 * would bring a lot of complexity and most of the moderns systems will only

 * use page flips.

	/*

	 * gen12+ has registers relative to transcoder and one per transcoder

	 * using the same bit definition: handle it as TRANSCODER_EDP to force

	 * 0 shift in bit definition

 Warning: it is masking/setting reserved bits too */

		/*

		 * If this interruption is not masked it will keep

		 * interrupting so fast that it prevents the scheduled

		 * work to run.

		 * Also after a PSR error, we don't want to arm PSR

		 * again so we don't care about unmask the interruption

		 * or unset irq_aux_error.

 assume the worst if we can't read the value */

 If sink don't have specific granularity requirements set legacy ones */

 As PSR2 HW sends full lines, we do not care about x granularity */

	/*

	 * Spec says that if the value read is 0 the default granularity should

	 * be used instead.

		/*

		 * All panels that supports PSR version 03h (PSR2 +

		 * Y-coordinate) can handle Y-coordinates in VSC but we are

		 * only sure that it is going to be used when required by the

		 * panel. This way panel is capable to do selective update

		 * without a aux frame sync.

		 *

		 * To support PSR version 02h and PSR version 03h without

		 * Y-coordinate requirement panels we would need to enable

		 * GTC first.

 Enable ALPM at sink for psr2 */

	/* Let's use 6 as the minimum to cover all known cases including the

	 * off-by-one issue that HW has in some cases.

 Wa_22012278275:adl-p */

 5 lines */

 6 lines */

 7 lines */

 8 lines */

 9 lines */

 10 lines */

 11 lines */

 12 lines */

		/*

		 * Still using the default IO_BUFFER_WAKE and FAST_WAKE, see

		 * comments bellow for more information

		/*

		 * TODO: 7 lines of IO_BUFFER_WAKE and FAST_WAKE are default

		 * values from BSpec. In order to setting an optimal power

		 * consumption, lower than 4k resoluition mode needs to decrese

		 * IO_BUFFER_WAKE and FAST_WAKE. And higher than 4K resolution

		 * mode needs to increase IO_BUFFER_WAKE and FAST_WAKE.

 Wa_1408330847 */

	/*

	 * PSR2 HW is incorrectly using EDP_PSR_TP1_TP3_SEL and BSpec is

	 * recommending keep this bit unset while PSR2 is enabled.

 If delayed work is pending, it is not idle */

 Before PSR2 exit disallow dc3co*/

	/*

	 * FIXME: Due to the changed sequence of activating/deactivating DC3CO,

	 * disable DC3CO until the changed dc3co activating/deactivating sequence

	 * is applied. B.Specs:49196

	/*

	 * DMC's DC3CO exit mechanism has an issue with Selective Fecth

	 * TODO: when the issue is addressed, this restriction should be removed.

 Wa_16011303918:adl-p */

	/*

	 * DC3CO Exit time 200us B.Spec 49196

	 * PSR2 transcoder Early Exit scanlines = ROUNDUP(200 / line time) + 1

 Wa_14010254185 Wa_14010103792 */

 PSR2 HW only send full lines so we only need to validate the width */

 HW tracking is only aligned to 4 lines */

	/*

	 * adl_p has 1 line granularity. For other platforms with SW tracking we

	 * can adjust the y coordinates to match sink requirement if multiple of

	 * 4.

 From spec: (72 / number of lanes) * 1000 / symbol clock frequency MHz */

 JSL and EHL only supports eDP 1.3 */

 Wa_16011181250 */

	/*

	 * DSC and PSR2 cannot be enabled simultaneously. If a requested

	 * resolution requires DSC to be enabled, priority is given to DSC

	 * over PSR2.

 Wa_2209313811 */

 Wa_16011303918:adl-p */

	/*

	 * Current PSR panels dont work reliably with VRR enabled

	 * So if VRR is enabled, do not enable PSR.

	/*

	 * Not possible to read EDP_PSR/PSR2_CTL registers as it is

	 * enabled/disabled because of frontbuffer tracking and others.

 psr1 and psr2 are mutually exclusive.*/

	/*

	 * Wa_16014451276:adlp

	 * All supported adlp panels have 1-based X granularity, this may

	 * cause issues if non-supported panels are used.

	/*

	 * Per Spec: Avoid continuous PSR exit by masking MEMUP and HPD also

	 * mask LPSP to avoid dependency on other drivers that might block

	 * runtime_pm besides preventing  other hw tracking issues now we

	 * can rely on frontbuffer tracking.

		/*

		 * TODO: if future platforms supports DC3CO in more than one

		 * transcoder, EXITLINE will need to be unset when disabling PSR

 Wa_16011168373:adl-p */

 Wa_16012604467:adlp */

	/*

	 * If a PSR error happened and the driver is reloaded, the EDP_PSR_IIR

	 * will still keep the error set even after the reset done in the

	 * irq_preinstall and irq_uninstall hooks.

	 * And enabling in this situation cause the screen to freeze in the

	 * first time that PSR HW tries to activate so lets keep PSR disabled

	 * to avoid any rendering problems.

 DC5/DC6 requires at least 6 idle frames */

 Wait till PSR is idle */

 Wa_1408330847 */

 Wa_16011168373:adl-p */

 Wa_16012604467:adlp */

 Disable PSR on Sink */

/**

 * intel_psr_disable - Disable PSR

 * @intel_dp: Intel DP

 * @old_crtc_state: old CRTC state

 *

 * This function needs to be called before disabling pipe.

/**

 * intel_psr_pause - Pause PSR

 * @intel_dp: Intel DP

 *

 * This function need to be called after enabling psr.

/**

 * intel_psr_resume - Resume PSR

 * @intel_dp: Intel DP

 *

 * This function need to be called after pausing psr.

	/*

	 * Display WA #0884: skl+

	 * This documented WA for bxt can be safely applied

	 * broadly so we can force HW tracking to exit PSR

	 * instead of disabling and re-enabling.

	 * Workaround tells us to write 0 to CUR_SURFLIVE_A,

	 * but it makes more sense write to the current active

	 * pipe.

	 *

	 * This workaround do not exist for platforms with display 10 or newer

	 * but testing proved that it works for up display 13, for newer

	 * than that testing will be needed.

 TODO: consider auxiliary surfaces */

 Sizes are 0 based */

		/*

		 * Not applying Wa_14014971508:adlp as we do not support the

		 * feature that requires this workaround.

/*

 * TODO: Not clear how to handle planes with negative position,

 * also planes are not updated if they have a negative X

 * position so for now doing a full update in this cases

 *

 * TODO: We are missing multi-planar formats handling, until it is

 * implemented it will send full frame updates.

 *

 * Plane scaling and rotation is not supported by selective fetch and both

 * properties can change without a modeset, so need to be check at every

 * atomic commmit.

/*

 * Check for pipe properties that is not supported by selective fetch.

 *

 * TODO: pipe scaling causes a modeset but skl_update_scaler_crtc() is executed

 * after intel_psr_compute_config(), so for now keeping PSR2 selective fetch

 * enabled and going to the full update path.

	/*

	 * Calculate minimal selective fetch area of each plane and calculate

	 * the pipe damaged area.

	 * In the next loop the plane selective fetch area will actually be set

	 * using whole pipe damaged area.

		/*

		 * If visibility or plane moved, mark the whole plane area as

		 * damaged as it needs to be complete redraw in the new and old

		 * position.

 If alpha changed mark the whole plane area as damaged */

	/*

	 * Now that we have the pipe damaged area check if it intersect with

	 * every plane, if it does set the plane selective fetch area.

		/*

		 * Reasons to disable:

		 * - PSR disabled in new state

		 * - All planes will go inactive

		 * - Changing between PSR versions

 Only enable if there is active planes */

 Force a PSR exit when enabling CRC to avoid CRC timeouts */

/**

 * psr_wait_for_idle - wait for PSR1 to idle

 * @intel_dp: Intel DP

 * @out_value: PSR status in case of failure

 *

 * Returns: 0 on success or -ETIMEOUT if PSR status does not idle.

 *

	/*

	 * From bspec: Panel Self Refresh (BDW+)

	 * Max. time for PSR to idle = Inverse of the refresh rate + 6 ms of

	 * exit training time + 1.5 ms of aux channel handshake. 50 ms is

	 * defensive enough to cover everything.

/**

 * intel_psr_wait_for_idle - wait for PSR1 to idle

 * @new_crtc_state: new CRTC state

 *

 * This function is expected to be called from pipe_update_start() where it is

 * not expected to race with PSR enable or disable.

 when the PSR1 is enabled */

 After the unlocked wait, verify that PSR is still wanted! */

 Mark mode as changed to trigger a pipe->update() */

	/*

	 * Do it right away if it's already enabled, otherwise it will be done

	 * when enabling the source.

 let's make sure that sink is awaken */

	/*

	 * We have to make sure PSR is ready for re-enable

	 * otherwise it keeps disabled until next full enable/disable cycle.

	 * PSR might take some time to get fully disabled

	 * and be ready for re-enable.

	/*

	 * The delayed work can race with an invalidate hence we need to

	 * recheck. Since psr_flush first clears this and then reschedules we

	 * won't ever miss a flush when bailing out here.

/**

 * intel_psr_invalidate - Invalidade PSR

 * @dev_priv: i915 device

 * @frontbuffer_bits: frontbuffer plane tracking bits

 * @origin: which operation caused the invalidate

 *

 * Since the hardware frontbuffer tracking has gaps we need to integrate

 * with the software frontbuffer tracking. This function gets called every

 * time frontbuffer rendering starts and a buffer gets dirtied. PSR must be

 * disabled if the frontbuffer mask contains a buffer relevant to PSR.

 *

 * Dirty frontbuffers relevant to PSR are tracked in busy_frontbuffer_bits."

/*

 * When we will be completely rely on PSR2 S/W tracking in future,

 * intel_psr_flush() will invalidate and flush the PSR for ORIGIN_FLIP

 * event also therefore tgl_dc3co_flush_locked() require to be changed

 * accordingly in future.

	/*

	 * At every frontbuffer flush flip event modified delay of delayed work,

	 * when delayed work schedules that means display has been idle.

/**

 * intel_psr_flush - Flush PSR

 * @dev_priv: i915 device

 * @frontbuffer_bits: frontbuffer plane tracking bits

 * @origin: which operation caused the flush

 *

 * Since the hardware frontbuffer tracking has gaps we need to integrate

 * with the software frontbuffer tracking. This function gets called every

 * time frontbuffer rendering has completed and flushed out to memory. PSR

 * can be enabled again if no other frontbuffer relevant to PSR is dirty.

 *

 * Dirty frontbuffers relevant to PSR are tracked in busy_frontbuffer_bits.

		/*

		 * If the PSR is paused by an explicit intel_psr_paused() call,

		 * we have to ensure that the PSR is not activated until

		 * intel_psr_resume() is called.

 By definition flush = invalidate + flush */

/**

 * intel_psr_init - Init basic PSR work and mutex.

 * @intel_dp: Intel DP

 *

 * This function is called after the initializing connector.

 * (the initializing of connector treats the handling of connector capabilities)

 * And it initializes basic PSR stuff for each DP Encoder.

	/*

	 * HSW spec explicitly says PSR is tied to port A.

	 * BDW+ platforms have a instance of PSR registers per transcoder but

	 * BDW, GEN9 and GEN11 are not validated by HW team in other transcoder

	 * than eDP one.

	 * For now it only supports one instance of PSR for BDW, GEN9 and GEN11.

	 * So lets keep it hardcoded to PORT_A for BDW, GEN9 and GEN11.

	 * But GEN12 supports a instance of PSR registers per transcoder.

 Set link_standby x link_off defaults */

 For new platforms up to TGL let's respect VBT back again */

 Clearing error */

 Clearing it */

 clear status register */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

/* HDMI/DVI modes ignore everything but the last 2 items. So we share

 * them for both DP and FDI transports, allowing those ports to

 * automatically adapt to HDMI connections as well

 Idx	NT mV d	T mV d	db	*/

 0:	400	400	0	*/

 1:	400	500	2	*/

 2:	400	600	3.5	*/

 3:	600	600	0	*/

 4:	600	750	2	*/

 5:	600	900	3.5	*/

 6:	800	800	0	*/

 7:	800	1000	2	*/

 8:	850	850	0	*/

 9:	900	900	0	*/

 10:	950	950	0	*/

 11:	1000	1000	0	*/

 Idx	NT mV d	T mV df	db	*/

 0:	400	400	0	*/

 1:	400	600	3.5	*/

 2:	400	800	6	*/

 3:	450	450	0	*/

 4:	600	600	0	*/

 5:	600	800	2.5	*/

 6:	600	1000	4.5	*/

 7:	800	800	0	*/

 8:	800	1000	2	*/

 9:	1000	1000	0	*/

 Skylake H and S */

 Skylake U */

 Skylake Y */

 Kabylake H and S */

 Kabylake U */

 Kabylake Y */

/*

 * Skylake/Kabylake H and S

 * eDP 1.4 low vswing translation parameters

/*

 * Skylake/Kabylake U

 * eDP 1.4 low vswing translation parameters

/*

 * Skylake/Kabylake Y

 * eDP 1.4 low vswing translation parameters

 Skylake/Kabylake U, H and S */

 Default */

 Skylake/Kabylake Y */

 Default */

 Idx	NT mV diff	db  */

 0:	400		0   */

 1:	400		3.5 */

 2:	400		6   */

 3:	400		9.5 */

 4:	600		0   */

 5:	600		3.5 */

 6:	600		6   */

 7:	800		0   */

 8:	800		3.5 */

 9:	1200		0   */

 Idx	NT mV diff	db  */

 0:	200		0   */

 1:	200		1.5 */

 2:	200		4   */

 3:	200		6   */

 4:	250		0   */

 5:	250		1.5 */

 6:	250		4   */

 7:	300		0   */

 8:	300		1.5 */

 9:	300		0   */

/* BSpec has 2 recommended values - entries 0 and 8.

 * Using the entry with higher vswing.

 Idx	NT mV diff	db  */

 0:	400		0   */

 1:	400		3.5 */

 2:	400		6   */

 3:	400		9.5 */

 4:	600		0   */

 5:	600		3.5 */

 6:	600		6   */

 7:	800		0   */

 8:	800		3.5 */

 9:	1200		0   */

 icl_combo_phy_trans */

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

 NT mV Trans mV db    */

 200   200      0.0   */

 200   250      1.9   */

 200   300      3.5   */

 200   350      4.9   */

 250   250      0.0   */

 250   300      1.6   */

 250   350      2.9   */

 300   300      0.0   */

 300   350      1.3   */

 350   350      0.0   */

 NT mV Trans mV db    */

 450   450      0.0   */

 450   650      3.2   */

 450   850      5.5   */

 650   650      0.0   ALS */

 650   850      2.3   */

 850   850      0.0   */

 600   850      3.0   */

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

 NT mV Trans mV db    */

 200   200      0.0   */

 200   250      1.9   */

 200   300      3.5   */

 200   350      4.9   */

 250   250      0.0   */

 250   300      1.6   */

 250   350      2.9   */

 300   300      0.0   */

 300   350      1.3   */

 350   350      0.0   */

 NT mV Trans mV db    */

 200   200      0.0   */

 200   250      1.9   */

 200   300      3.5   */

 200   350      4.9   */

 250   250      0.0   */

 250   300      1.6   */

 250   350      2.9   */

 300   300      0.0   */

 300   350      1.3   */

 350   350      0.0   */

 NT mV Trans mV db    */

 200   200      0.0   */

 200   250      1.9   */

 200   300      3.5   */

 200   350      4.9   */

 250   250      0.0   */

 250   300      1.6   */

 250   350      2.9   */

 300   300      0.0   */

 300   350      1.3   */

 350   350      0.0   */

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

 Voltage swing  pre-emphasis */

 0              0   */

 0              1   */

 0              2   */

 0              3   */

 1              0   */

 1              1   */

 1              2   */

 2              0   */

 2              1   */

 3              0   */

 Voltage swing  pre-emphasis */

 0              0   */

 0              1   */

 0              2   */

 0              3   */

 1              0   */

 1              1   */

 1              2   */

 2              0   */

 2              1   */

 3              0   */

 HDMI Preset	VS	Pre-emph */

 1		400mV	0dB */

 2		500mV	0dB */

 3		650mV	0dB */

 4		800mV	0dB */

 5		1000mV	0dB */

 6		Full	-1.5 dB */

 7		Full	-1.8 dB */

 8		Full	-2 dB */

 9		Full	-2.5 dB */

 10		Full	-3 dB */

 VS	pre-emp	Non-trans mV	Pre-emph dB */

 0	0	400mV		0 dB */

 0	1	400mV		3.5 dB */

 0	2	400mV		6 dB */

 0	3	400mV		9.5 dB */

 1	0	600mV		0 dB */

 1	1	600mV		3.5 dB */

 1	2	600mV		6 dB */

 2	0	800mV		0 dB */

 2	1	800mV		3.5 dB */

 3	0	1200mV		0 dB HDMI default */

 VS	pre-emp	Non-trans mV	Pre-emph dB */

 0	0	400mV		0 dB */

 0	1	400mV		3.5 dB */

 0	2	400mV		6 dB */

 0	3	400mV		9.5 dB */

 1	0	600mV		0 dB */

 1	1	600mV		3.5 dB */

 1	2	600mV		6 dB */

 2	0	800mV		0 dB */

 2	1	800mV		3.5 dB */

 3	0	1200mV		0 dB HDMI default */

 HDMI Preset	VS	Pre-emph */

 1		400mV	0dB */

 2		500mV	0dB */

 3		650mV	0dB */

 4		800mV	0dB */

 5		1000mV	0dB */

 6		Full	-1.5 dB */

 7		Full	-1.8 dB */

 8		Full	-2 dB */

 9		Full	-2.5 dB */

 10		Full	-3 dB */

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

/*

 * Cloned the HOBL entry to comply with the voltage and pre-emphasis entries

 * that DisplayPort specification requires

 VS	pre-emp	*/

 0	0	*/

 0	1	*/

 0	2	*/

 0	3	*/

 1	0	*/

 1	1	*/

 1	2	*/

 2	0	*/

 2	1	*/

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

 NT mV Trans mV db    */

 200   200      0.0   */

 200   250      1.9   */

 200   300      3.5   */

 200   350      4.9   */

 250   250      0.0   */

 250   300      1.6   */

 250   350      2.9   */

 300   300      0.0   */

 300   350      1.3   */

 350   350      0.0   */

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

 NT mV Trans mV    db   */

  400    400      0.0 */

  500    500      0.0 */

  650    650      0.0 ALS */

  800    800      0.0 */

 1000   1000      0.0 Re-timer */

 Full    Red     -1.5 */

 Full    Red     -1.8 */

 Full    Red     -2.0 CRLS */

 Full    Red     -2.5 */

 Full    Red     -3.0 */

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

 NT mV Trans mV db    */

 350   350      0.0   */

 350   500      3.1   */

 350   700      6.0   */

 350   900      8.2   */

 500   500      0.0   */

 500   700      2.9   */

 500   900      5.1   */

 650   700      0.6   */

 600   900      3.5   */

 900   900      0.0   */

 VS	pre-emp	Non-trans mV	Pre-emph dB */

 0	0	400mV		0 dB */

 0	1	400mV		3.5 dB */

 0	2	400mV		6 dB */

 0	3	400mV		9.5 dB */

 1	0	600mV		0 dB */

 1	1	600mV		3.5 dB */

 1	2	600mV		6 dB */

 2	0	800mV		0 dB */

 2	1	800mV		3.5 dB */

 3	0	1200mV		0 dB */

 VS	pre-emp	Non-trans mV	Pre-emph dB */

 0	0	400mV		0 dB */

 0	1	400mV		3.5 dB */

 0	2	400mV		6 dB */

 0	3	400mV		9.5 dB */

 1	0	600mV		0 dB */

 1	1	600mV		3.5 dB */

 1	2	600mV		6 dB */

 2	0	800mV		0 dB */

 2	1	800mV		3.5 dB */

 3	0	1200mV		0 dB */

 VS 0, pre-emph 0 */

 VS 0, pre-emph 1 */

 VS 0, pre-emph 2 */

 VS 0, pre-emph 3 */

 VS 1, pre-emph 0 */

 VS 1, pre-emph 1 */

 VS 1, pre-emph 2 */

 VS 2, pre-emph 0 */

 VS 2, pre-emph 1 */

 VS 3, pre-emph 0 */

 preset 0 */

 preset 1 */

 preset 2 */

 preset 3 */

 preset 4 */

 preset 5 */

 preset 6 */

 preset 7 */

 preset 8 */

 preset 9 */

 preset 10 */

 preset 11 */

 preset 12 */

 preset 13 */

 preset 14 */

 preset 15 */

 Only DDIA and DDIE can select the 10th register with DP */

/*

 * Copyright Â© 2006-2007 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Eric Anholt <eric@anholt.net>

/**

 * intel_update_watermarks - update FIFO watermark values based on current modes

 * @dev_priv: i915 device

 *

 * Calculate watermark values for the various WM regs based on current mode

 * and plane configuration.

 *

 * There are several cases to deal with here:

 *   - normal (i.e. non-self-refresh)

 *   - self-refresh (SR) mode

 *   - lines are large relative to FIFO size (buffer can hold up to 2)

 *   - lines are small relative to FIFO size (buffer can hold more than 2

 *     lines), so need to account for TLB latency

 *

 *   The normal calculation is:

 *     watermark = dotclock * bytes per pixel * latency

 *   where latency is platform & configuration dependent (we assume pessimal

 *   values here).

 *

 *   The SR calculation is:

 *     watermark = (trunc(latency/line time)+1) * surface width *

 *       bytes per pixel

 *   where

 *     line time = htotal / dotclock

 *     surface width = hdisplay for normal plane and 64 for cursor

 *   and latency is assumed to be high, as above.

 *

 * The final value programmed to the register should always be rounded up,

 * and include an extra 2 entries to account for clock crossings.

 *

 * We don't use the sprite, so we can ignore that.  And on Crestline we have

 * to set the non-SR watermarks to 8.

 returns HPLL frequency in kHz */

 Obtain SKU information */

 WA Display #0827: Gen9:all */

 Wa_2006604312:icl,ehl */

 Wa_1604331009:icl,jsl,ehl */

 Wait for the display line to settle/start moving */

 Wait for the Pipe State to go off */

 we keep both pipes enabled on 830 */

 PCH SDVOB multiplex with HDMIB */

 Make sure PCH DPLL is enabled */

 FDI must be feeding us bits for PCH ports */

		/*

		 * Workaround: Set the timing override bit

		 * before enabling the pch transcoder.

 Configure frame start delay to match the CPU */

 Configure frame start delay to match the CPU */

		/*

		 * Make the BPC in transcoder be consistent with

		 * that in pipeconf reg. For HDMI we must use 8bpc

		 * here for both 8bpc and 12bpc.

 FDI must be feeding us bits for PCH ports */

 Workaround: set timing override bit. */

 Configure frame start delay to match the CPU */

 FDI relies on the transcoder */

 Ports must be off as well */

 wait for PCH transcoder off, transcoder state */

 Workaround: Clear the timing override chicken bit again. */

 wait for PCH transcoder off, transcoder state */

 Workaround: clear timing override bit. */

	/*

	 * A pipe without a PLL won't actually be able to drive bits from

	 * a plane.  On ILK+ the pipe PLLs are integrated, so we don't

	 * need the check.

 if driving the PCH, we need FDI enabled */

 FIXME: assert CPU port conditions for SNB+ */

 Wa_22012358565:adl-p */

 we keep both pipes enabled on 830 */

	/*

	 * Until the pipe starts PIPEDSL reads will return a stale value,

	 * which causes an apparent vblank timestamp jump when PIPEDSL

	 * resets to its proper value. That also messes up the frame count

	 * when it's derived from the timestamps. So let's wait for the

	 * pipe to start properly before we call drm_crtc_vblank_on()

	/*

	 * Make sure planes won't keep trying to pump pixels to us,

	 * or we might hang the display.

	/*

	 * Double wide has implications for planes

	 * so best keep it disabled when not needed.

 Don't disable pipe or pipe PLLs if needed */

/*

 * Convert the x/y offsets into a linear offset.

 * Only valid with 0/180 degree rotation, which is fine since linear

 * offset is only used with linear buffers on pre-hsw and tiled buffers

 * with gen2/3, and 90/270 degree rotations isn't supported on any of them.

/*

 * Add the x/y offsets derived from fb->offsets[] to the user

 * specified plane src x/y offsets. The resulting x/y offsets

 * specify the start of scanout from the beginning of the gtt mapping.

/*

 * From the Sky Lake PRM:

 * "The Color Control Surface (CCS) contains the compression status of

 *  the cache-line pairs. The compression state of the cache-line pair

 *  is specified by 2 bits in the CCS. Each CCS cache-line represents

 *  an area on the main surface of 16 x16 sets of 128 byte Y-tiled

 *  cache-line-pairs. CCS is always Y tiled."

 *

 * Since cache line pairs refers to horizontally adjacent cache lines,

 * each cache line in the CCS corresponds to an area of 32x16 cache

 * lines on the main surface. Since each pixel is 4 bytes, this gives

 * us a ratio of one byte in the CCS for each 8x16 pixels in the

 * main surface.

/*

 * Gen-12 compression uses 4 bits of CCS data for each cache line pair in the

 * main surface. And each 64B CCS cache line represents an area of 4x1 Y-tiles

 * in the main surface. With 4 byte pixels and each Y-tile having dimensions of

 * 32x32 pixels, the ratio turns out to 1B in the CCS for every 2x32 pixels in

 * the main surface.

/*

 * Same as gen12_ccs_formats[] above, but with additional surface used

 * to pass Clear Color information in plane 2 with 64 bits of data.

	/*

	 * We assume the primary plane for pipe A has

	 * the highest stride limits of them all,

	 * if in case pipe A is disabled, use the first pipe from pipe_mask.

	/*

	 * Active_planes aliases if multiple "primary" or cursor planes

	 * have been used on the same (or wrong) pipe. plane_mask uses

	 * unique ids, hence we can use that to reconstruct active_planes.

	/*

	 * Vblank time updates from the shadow to live plane control register

	 * are blocked if the memory self-refresh mode is active at that

	 * moment. So to make sure the plane gets truly disabled, disable

	 * first the self-refresh mode. The self-refresh enable bit in turn

	 * will be checked/applied by the HW only at the next frame start

	 * event which is after the vblank start event, so we need to have a

	 * wait-for-vblank between disabling the plane and the pipe.

	/*

	 * Gen2 reports pipe underruns whenever all planes are disabled.

	 * So disable underrun reporting before all the planes get disabled.

	/*

	 * We've duplicated the state, pointers to the old state are invalid.

	 *

	 * Don't attempt to use the old state until we commit the duplicated state.

		/*

		 * Force recalculation even if we restore

		 * current state. With fast modeset this may not result

		 * in a modeset when the state is compatible.

 ignore any reset values/BIOS leftovers in the WM registers */

 reset doesn't touch the display */

 We have a modeset vs reset deadlock, defensively unbreak it. */

	/*

	 * Need mode_config.mutex so that we don't

	 * trample ongoing ->detect() and whatnot.

	/*

	 * Disabling the crtcs gracefully seems nicer. Also the

	 * g33 docs say we should at least disable all the planes.

 reset doesn't touch the display */

 reset doesn't touch the display */

 for testing only restore the display */

		/*

		 * The display has been reset as well,

		 * so need a full re-initialization.

	/*

	 * Display WA #1153: icl

	 * enable hardware to bypass the alpha math

	 * and rounding for per-pixel values 00 and 0xff

	/*

	 * Display WA # 1605353570: icl

	 * Set the pixel rounding bit to 1 for allowing

	 * passthrough of Frame buffer pixels unmodified

	 * across pipe

		/*

		 * Underrun recovery must always be disabled on DG2.  However

		 * the chicken bit meaning is inverted compared to other

		 * platforms.

 Program iCLKIP clock to the desired frequency */

	/* The iCLK virtual clock root frequency is in MHz,

	 * but the adjusted_mode->crtc_clock in in KHz. To get the

	 * divisors, it is necessary to divide one by another, so we

	 * convert the virtual clock precision to KHz here for higher

	 * precision.

		/*

		 * Near 20MHz is a corner case which is

		 * out of range for the 7-bit divisor

 This should not happen with any sane values */

 Program SSCDIVINTPHASE6 */

 Program SSCAUXDIV */

 Enable modulator and associated divider */

 Wait for initialization time */

/*

 * Finds the encoder associated with the given CRTC. This can only be

 * used when we know that the CRTC isn't feeding multiple encoders!

/*

 * Enable PCH resources required for PCH ports:

 *   - PCH PLLs

 *   - FDI training & RX/TX

 *   - update transcoder timings

 *   - DP transcoding bits

 *   - transcoder

 For PCH output, training FDI link */

	/* We need to program the right clock selection before writing the pixel

	/* XXX: pch pll's can be enabled any time before we enable the PCH

	 * transcoder, and we actually should do this to not upset any PCH

	 * transcoder that already use the clock when we share it.

	 *

	 * Note that enable_shared_dpll tries to do the right thing, but

	 * get_shared_dpll unconditionally resets the pll - we need that to have

 set transcoder timing, panel must allow it */

 For PCH DP, enable TRANS_DP_CTL */

 same format but at 11:9 */

 Set transcoder timing. */

	/* Force use of hard-coded filter coefficients

	 * as some pre-programmed values are broken,

	 * e.g. x201.

	/*

	 * We can only enable IPS after we enable a plane and wait for a vblank

	 * This function is called from post_plane_update, which is run after

	 * a vblank wait.

		/* Quoting Art Runyan: "its not safe to expect any particular

		 * value in IPS_CTL bit 31 after enabling IPS through the

		 * mailbox." Moreover, the mailbox may return a bogus state,

		 * so we need to just enable it and continue on.

		/* The bit only becomes 1 in the next vblank, so this wait here

		 * is essentially intel_wait_for_vblank. If we don't have this

		 * and don't wait for vblanks until the end of crtc_enable, then

		 * the HW state readout code will complain that the expected

		/*

		 * Wait for PCODE to finish disabling IPS. The BSpec specified

		 * 42ms timeout value leads to occasional timeouts so use 100ms

		 * instead.

 We need to wait for a vblank before we can disable the plane. */

	/* Let userspace switch the overlay on again. In most cases userspace

	 * has to recompute where to put it anyway.

	/*

	 * Workaround : Do not read or write the pipe palette/gamma data while

	 * GAMMA_MODE is configured for split gamma and IPS_CTL has IPS enabled.

	 *

	 * Disable IPS before we program the LUT.

	/*

	 * Workaround : Do not read or write the pipe palette/gamma data while

	 * GAMMA_MODE is configured for split gamma and IPS_CTL has IPS enabled.

	 *

	 * Re-enable IPS after the LUT has been programmed.

	/*

	 * We can't read out IPS on broadwell, assume the worst and

	 * forcibly enable IPS on the first fastset.

 WA Display #0827: Gen9:all */

 Wa_2006604312:icl,ehl */

 Wa_1604331009:icl,jsl,ehl */

		/*

		 * "Plane N strech max must be programmed to 11b (x1)

		 *  when Async flips are enabled on that plane."

 Also needed on HSW/BDW albeit undocumented */

			/*

			 * Apart from the async flip bit we want to

			 * preserve the old state for the plane.

 Display WA 827 */

 Wa_2006604312:icl,ehl */

 Wa_1604331009:icl,jsl,ehl */

	/*

	 * Vblank time updates from the shadow to live plane control register

	 * are blocked if the memory self-refresh mode is active at that

	 * moment. So to make sure the plane gets truly disabled, disable

	 * first the self-refresh mode. The self-refresh enable bit in turn

	 * will be checked/applied by the HW only at the next frame start

	 * event which is after the vblank start event, so we need to have a

	 * wait-for-vblank between disabling the plane and the pipe.

	/*

	 * IVB workaround: must disable low power watermarks for at least

	 * one frame before enabling scaling.  LP watermarks can be re-enabled

	 * when scaling is disabled.

	 *

	 * WaCxSRDisabledForSpriteScaling:ivb

	/*

	 * If we're doing a modeset we don't need to do any

	 * pre-vblank watermark programming here.

		/*

		 * For platforms that support atomic watermarks, program the

		 * 'intermediate' watermarks immediately.  On pre-gen9 platforms, these

		 * will be the intermediate values that are safe for both pre- and

		 * post- vblank; when vblank happens, the 'active' values will be set

		 * to the final 'target' values and we'll do this again to get the

		 * optimal watermarks.  For gen9+ platforms, the values we program here

		 * will be the final target values which will get automatically latched

		 * at vblank time; no further programming will be necessary.

		 *

		 * If a platform hasn't been transitioned to atomic watermarks yet,

		 * we'll continue to update watermarks the old way, if flags tell

		 * us to.

	/*

	 * Gen2 reports pipe underruns whenever all planes are disabled.

	 * So disable underrun reporting before all the planes get disabled.

	 *

	 * We do this after .initial_watermarks() so that we have a

	 * chance of catching underruns with the intermediate watermarks

	 * vs. the old plane configuration.

	/*

	 * WA for platforms where async address update enable bit

	 * is double buffered and only latched at start of vblank.

/*

 * intel_connector_primary_encoder - get the primary encoder for a connector

 * @connector: connector for which to return the encoder

 *

 * Returns the primary encoder for a connector. There is a 1:1 mapping from

 * all connectors to their encoder, except for DP-MST connectors which have

 * both a virtual and a primary encoder. These DP-MST primary encoders can be

 * pointed to by as many DP-MST connectors as there are pipes.

	/*

	 * Sometimes spurious CPU pipe underruns happen during FDI

	 * training, at least with VGA+HDMI cloning. Suppress them.

	 *

	 * On ILK we get an occasional spurious CPU pipe underruns

	 * between eDP port A enable and vdd enable. Also PCH port

	 * enable seems to result in the occasional CPU pipe underrun.

	 *

	 * Spurious PCH underruns also occur during PCH enabling.

		/* Note: FDI PLL enabling _must_ be done before we enable the

		 * cpu pipes, hence this is separate from all the other fdi/pch

	/*

	 * On ILK+ LUT must be loaded before the pipe is running but with

	 * clocks enabled

 update DSPCNTR to configure gamma for pipe bottom color */

	/*

	 * Must wait for vblank to avoid spurious PCH FIFO underruns.

	 * And a second vblank wait is needed at least on ILK with

	 * some interlaced HDMI modes. Let's do the double wait always

	 * in case there are more corner cases we don't know about.

 IPS only exists on ULT machines and is tied to pipe A. */

 Wa_22010947358:adl-p */

 need to enable VDSC, which we skipped in pre-enable */

		/*

		 * Enable sequence steps 1-7 on bigjoiner master

 and DSC on slave */

 Display WA #1180: WaDisableScalarClockGating: glk */

	/*

	 * On ILK+ LUT must be loaded before the pipe is running but with

	 * clocks enabled

 update DSPCNTR to configure gamma/csc for pipe bottom color */

	/* If we change the relative order between pipe/planes enabling, we need

	/* To avoid upsetting the power well on haswell only disable the pfit if

	/*

	 * Sometimes spurious CPU pipe underruns happen when the

	 * pipe is already disabled, but FDI RX/TX is still enabled.

	 * Happens at least with VGA+HDMI cloning. Suppress them.

 disable TRANS_DP_CTL */

 disable DPLL_SEL */

	/*

	 * FIXME collapse everything to one hook.

	 * Need care with mst->ddi interactions.

	/*

	 * The panel fitter should only be adjusted whilst the pipe is disabled,

	 * according to register description and PRM.

	/* Border color in case we don't scale up to the full screen. Black by

		/*

		 * DG2 outputs labelled as "combo PHY" in the bspec use

		 * SNPS PHYs with completely different programming,

		 * hence we always return false here.

 DG2's "TC1" output uses a SNPS PHY */

		/*

		 * All four "combo" ports and the TC1 port (PHY E) use

		 * Synopsis PHYs.

/*

 * Converts aux_ch to power_domain without caring about TBT ports for that use

 * intel_aux_power_domain()

 update DSPCNTR to configure gamma for pipe bottom color */

 update DSPCNTR to configure gamma for pipe bottom color */

 prevents spurious underruns */

	/*

	 * On gen2 planes are double buffered but the pipe isn't, so we must

	 * wait for planes to fully turn off before disabling the pipe.

 clock the pipe down to 640x480@60 to potentially save power */

 Everything's already locked, -EDEADLK can't happen. */

/*

 * turn all crtc's off, but do not adjust state

 * This has to be paired with a call to intel_modeset_setup_hw_state.

/* Cross check the actual hw state with our own modeset state tracking (and it's

 IPS only exists on ULT machines and is tied to pipe A. */

	/*

	 * We compare against max which means we must take

	 * the increased cdclk requirement into account when

	 * calculating the new cdclk.

	 *

	 * Should measure whether using a lower cdclk w/o IPS

	/*

	 * When IPS gets enabled, the pipe CRC changes. Since IPS gets

	 * enabled and disabled dynamically based on package C states,

	 * user space can't make reliable use of the CRCs, so let's just

	 * completely disable it.

 IPS should be fine as long as at least one plane is enabled. */

 pixel rate mustn't exceed 95% of cdclk with IPS on BDW */

 GDG double wide on either pipe, otherwise pipe A only */

	/*

	 * We only use IF-ID interlacing. If we ever use

	 * PF-ID we'll need to adjust the pixel_rate here.

 FIXME calculate proper pipe pixel rate for GMCH pfit */

		/*

		 * transcoder is programmed to the full mode,

		 * but pipe timings are half of the transcoder mode

		/*

		 * eDP MSO uses segment timings from EDID for transcoder

		 * timings, but full mode for everything else.

		 *

		 * h_full = (h_segment - pixel_overlap) * link_count

 Adjust pipe_mode for bigjoiner, with half the horizontal mode */

		/*

		 * Enable double wide mode when the dot clock

		 * is > 90% of the (display) core speed.

	/*

	 * Pipe horizontal size must be even in:

	 * - DVO ganged mode

	 * - LVDS dual channel mode

	 * - Double wide pipe

	/*

	 * Several DP dongles in particular seem to be fussy about

	 * too large link M/N values. Give N value as 0x8000 that

	 * should be acceptable by specific devices. 0x8000 is the

	 * specified fixed N value for asynchronous clock mode,

	 * which the devices expect also in synchronous clock mode.

	/*

	 * There may be no VBT; and if the BIOS enabled SSC we can

	 * just keep using it to avoid unnecessary flicker.  Whereas if the

	 * BIOS isn't using it, don't assume it will work even if the VBT

	 * indicates as much.

	/*

	 * Strictly speaking some registers are available before

	 * gen7, but we only support DRRS on gen7+

		/*

		 *  M2_N2 registers are set only if DRRS is supported

		 * (to make sure the registers are not unnecessarily accessed).

		/*

		 * M2_N2 registers are not supported. Hence m2_n2 divider value

		 * needs to be programmed into M1_N1.

	/* We need to be careful not to changed the adjusted mode, for otherwise

 the chip adds 2 halflines automatically */

	/* Workaround: when the EDP input selection is B, the VTOTAL_B must be

	 * programmed with the VTOTAL_EDP value. Same for VTOTAL_C. This is

	 * documented on the DDI_FUNC_CTL register description, EDP Input Select

	/* pipesrc controls the size that is scaled from, which should

	 * always be the user's requested size.

 we keep both pipes enabled on 830 */

 only g4x and later have fancy bpc/dither controls */

 Bspec claims that we can't use dithering for 30bpp pipes. */

 Case prevented by intel_choose_pipe_bpp_dither. */

 Check whether the pfit is attached to our pipe. */

 In case of DSI, DPLL will not be used */

 In case of DSI, DPLL will not be used */

 We support 4:2:0 in full blend mode only */

 No way to read it out on pipes B and C */

		/* Note that on i915G/GM the pixel multiplier is in the sdvo

		 * port and will be fixed up in the encoder->get_config

 Mask out read-only status bits. */

	/*

	 * Normally the dotclock is filled in by the encoder .get_config()

	 * but in case the pipe is enabled w/o any ports we need a sane

	 * default.

 We need to take the global config into account */

 Check if any DPLLs are using the SSC source */

	/* Ironlake: try to setup display ref clock before DPLL

	 * enabling. This is only under driver's control after

	 * PCH B stepping, previous chipset stepping should be

	 * ignoring this setting.

	/* As we must carefully and slowly disable/enable each source in turn,

	 * compute the final state we want first and check if we need to

	 * make any changes at all.

 Always enable nonspread source */

 SSC must be turned on before enabling the CPU output  */

 Get SSC going before enabling the outputs */

 Enable CPU source on CPU attached eDP */

 Turn off CPU output */

 Turn off the SSC source */

 Turn off SSC1 */

/* Implements 3 different sequences from BSpec chapter "Display iCLK

 * Programming" based on the parameters passed:

 * - Sequence to enable CLKOUT_DP

 * - Sequence to enable CLKOUT_DP without spread

 * - Sequence to enable CLKOUT_DP for FDI usage and configure PCH FDI I/O

 Sequence to disable CLKOUT_DP */

/*

 * Bend CLKOUT_DP

 * steps -50 to 50 inclusive, in steps of 5

 * < 0 slow down the clock, > 0 speed up the clock, 0 == no bend (135MHz)

 * change in clock period = -(steps / 10) * 5.787 ps

	/*

	 * The BIOS may have decided to use the PCH SSC

	 * reference so we must not disable it until the

	 * relevant PLLs have stopped relying on it. We'll

	 * just leave the PCH SSC reference enabled in case

	 * any active PLL is using it. It will get disabled

	 * after runtime suspend if we don't have FDI.

	 *

	 * TODO: Move the whole reference clock handling

	 * to the modeset sequence proper so that we can

	 * actually enable/disable/reconfigure these things

	 * safely. To do that we need to introduce a real

	 * clock hierarchy. That would also allow us to do

	 * clock bending finally.

/*

 * Initialize reference clocks when the driver loads

 Case prevented by intel_choose_pipe_bpp_dither. */

	/*

	 * This would end up with an odd purple hue over

	 * the entire display. Make sure we don't do it.

 Port output 12BPC defined for ADLP+ */

	/*

	 * PORT OUTPUT 12 BPC defined for ADLP+.

	 *

	 * TODO:

	 * For previous platforms with DSI interface, bits 5:7

	 * are used for storing pipe_bpp irrespective of dithering.

	 * Since the value of 12 BPC is not defined for these bits

	 * on older platforms, need to find a workaround for 12 BPC

	 * MIPI DSI HW readout.

	/*

	 * Account for spread spectrum to avoid

	 * oversubscribing the link. Max center spread

	 * is 2.5%; use 5% for safety's sake.

 find scaler attached to this pipe */

	/*

	 * We currently do not free assignements of panel fitters on

	 * ivb/hsw (since we don't use the higher upscaling modes which

	 * differentiates them) so just WARN about this case for now.

			/*

			 * The pipe->pch transcoder and pch transcoder->pll

			 * mapping is fixed.

	/*

	 * XXX: Do intel_display_power_get_if_enabled before reading this (for

	 * consistency and less surprising code; it's in always on power).

 Only one type of transcoder please */

 Only DSI transcoders can be ganged */

	/*

	 * With the exception of DSI we should only ever have

	 * a single enabled transcoder. With DSI let's just

	 * pick the first one.

		/*

		 * The PLL needs to be enabled with a valid divider

		 * configuration, otherwise accessing DSI registers will hang

		 * the machine. See BSpec North Display Engine

		 * registers/MIPI[BXT]. We can break out here early, since we

		 * need the same DSI PLL to be enabled for both DSI ports.

 XXX: this works for video mode only */

	/*

	 * Haswell has only FDI/PCH transcoder A. It is which is connected to

	 * DDI E. So just check whether this pipe is wired to DDI E and whether

	 * the PCH transcoder is on.

 bigjoiner slave doesn't enable transcoder */

 we cannot read out most state, so don't bother.. */

			/*

			 * We cannot readout IPS state on broadwell, set to

			 * true so we can set it to a defined state on first

			 * commit.

 Cannot be read out as a slave, set to 0. */

 VESA 640x480x72Hz mode to set on the pipe */

	/*

	 * Algorithm gets a little messy:

	 *

	 *   - if the connector already has an assigned crtc, use it (but make

	 *     sure it's on first)

	 *

	 *   - try to find the first unused crtc that can drive this connector,

	 *     and use that if we find one

 See if we already have a CRTC for this connector */

 Make sure the crtc and connector are running */

 Find an unused one (if possible) */

	/*

	 * If we didn't find an unused CRTC, don't use any.

 let the connector get through one full cycle before testing */

 Returns the clock of the currently programmed mode of the given pipe. */

	/*

	 * This value includes pixel_multiplier. We will use

	 * port_clock to compute adjusted_mode.crtc_clock in the

	 * encoder's get_config() function.

	/*

	 * The calculation for the data clock is:

	 * pixel_clock = ((m/n)*(link_clock * nr_lanes))/bpp

	 * But we want to avoid losing precison if possible, so:

	 * pixel_clock = ((m * link_clock * nr_lanes)/(n*bpp))

	 *

	 * and the link clock is simpler:

	 * link_clock = (m * link_clock) / n

 read out port_clock from the DPLL */

	/*

	 * In case there is an active pipe without active ports,

	 * we may need some idea for the dotclock anyway.

	 * Calculate one based on the FDI configuration.

 Returns the currently programmed mode of the given encoder. */

/**

 * intel_wm_need_update - Check whether watermarks need updating

 * @cur: current plane state

 * @new: new plane state

 *

 * Check current plane state versus the new one to determine whether

 * watermarks need to be recalculated.

 *

 * Returns true or false.

 Update watermarks on tiling or size changes. */

	/*

	 * Visibility is calculated as if the crtc was on, but

	 * after scaler setup everything depends on it being off

	 * when the crtc isn't active.

	 *

	 * FIXME this is wrong for watermarks. Watermarks should also

	 * be computed as if the pipe would be active. Perhaps move

	 * per-plane wm computation to the .check_plane() hook, and

	 * only combine the results from all planes in the current place?

 must disable cxsr around plane enable/disable */

 must disable cxsr around plane enable/disable */

 FIXME bollocks */

	/*

	 * ILK/SNB DVSACNTR/Sprite Enable

	 * IVB SPR_CTL/Sprite Enable

	 * "When in Self Refresh Big FIFO mode, a write to enable the

	 *  plane will be internally buffered and delayed while Big FIFO

	 *  mode is exiting."

	 *

	 * Which means that enabling the sprite can take an extra frame

	 * when we start in big FIFO mode (LP1+). Thus we need to drop

	 * down to LP0 and wait for vblank in order to make sure the

	 * sprite gets enabled on the next vblank after the register write.

	 * Doing otherwise would risk enabling the sprite one frame after

	 * we've already signalled flip completion. We can resume LP1+

	 * once the sprite has been enabled.

	 *

	 *

	 * WaCxSRDisabledForSpriteScaling:ivb

	 * IVB SPR_SCALE/Scaling Enable

	 * "Low Power watermarks must be disabled for at least one

	 *  frame before enabling sprite scaling, and kept disabled

	 *  until sprite scaling is disabled."

	 *

	 * ILK/SNB DVSASCALE/Scaling Enable

	 * "When in Self Refresh Big FIFO mode, scaling enable will be

	 *  masked off while Big FIFO mode is exiting."

	 *

	 * Despite the w/a only being listed for IVB we assume that

	 * the ILK/SNB note has similar ramifications, hence we apply

	 * the w/a on all three platforms.

	 *

	 * With experimental results seems this is needed also for primary

	 * plane, not only sprite plane.

 masks could be asymmetric, so check both ways */

	/*

	 * Destroy all old plane links and make the slave plane invisible

	 * in the crtc_state->active_planes mask.

 Copy parameters to slave plane */

 Display WA #1135: BXT:ALL GLK:ALL */

	/*

	 * May need to update pipe gamma enable bits

	 * when C8 planes are getting enabled/disabled.

	/*

	 * Calculate 'intermediate' watermarks that satisfy both the

	 * old state and the new state.  We can program these

	 * immediately.

 Clamp display bpp to connector max bpp */

	/*

	 * We're going to peek into connector->state,

	 * hence connection_mutex must be held.

	/*

	 * Walk the connector list instead of the encoder

	 * list to detect the problem on ddi platforms

	 * where there's just one encoder per digital port.

 the same port mustn't appear more than once */

 can't mix MST and SST/HDMI on the same port */

 No need to copy state if the master state is unchanged */

 copy color blobs to uapi */

 Re-init hw state */

 Some fixups */

 free the old crtc_state->hw members */

	/* FIXME: before the switch to atomic started, a new pipe_config was

	 * kzalloc'd. Code that depends on any field being zero should be

	 * fixed, so that the crtc_state can be safely duplicated. For now,

	/*

	 * Sanitize sync polarity flags based on requested ones. If neither

	 * positive or negative polarity is requested, treat this as meaning

	 * negative polarity.

	/*

	 * Determine the real pipe dimensions. Note that stereo modes can

	 * increase the actual pipe size due to the frame doubling and

	 * insertion of additional space for blanks between the frame. This

	 * is stored in the crtc timings. We use the requested mode to do this

	 * computation to clearly distinguish it from the adjusted mode, which

	 * can be changed by the connectors in the below retry loop.

		/*

		 * Determine output_types before calling the .compute_config()

		 * hooks so that the hooks can use this information safely.

 Ensure the port clock defaults are reset when retrying. */

 Fill in default crtc timings, allow encoders to overwrite them. */

	/* Pass our mode to the connectors and the CRTC to give them a chance to

	 * adjust it according to limitations or connector properties, and also

	 * a chance to reject the mode entirely.

	/* Set default port clock if not overwritten by the encoder. Needs to be

	/* Dithering seems to not pass-through bits correctly when it should, so

	 * only enable it on 6bpc panels and when its not a compliance

	 * test requesting 6bpc video pattern.

 Enable fastboot by default on Skylake and newer */

 Enable fastboot by default on VLV and CHV */

 Disabled by default on all others */

/*

 * Checks state where we only read out the enabling, but not the entire

 * state itself (like full infoframes or ELD for audio). These states

 * require a full modeset on bootup to fix up.

/* This is required for BDW+ where there is only one set of registers for

 * switching between high and low RR.

 * This macro can be used whenever a comparison has to be made between one

 * hw state and multiple sw state variables.

 FIXME do the readout properly and get rid of this quirk */

 FIXME do the readout properly and get rid of this quirk */

 pfit ratios are autocomputed by the hw on gen4+ */

	/*

	 * Changing the EDP transcoder input mux

	 * (A_ONOFF vs. A_ON) requires a full modeset.

 FIXME do the readout properly and get rid of this quirk */

 FIXME do the readout properly and get rid of this quirk */

		/*

		 * FDI already provided one idea for the dotclock.

		 * Yell if the encoder disagrees.

 Watermarks */

 DDB */

 we keep both pipes enabled on 830 */

 No PLLs set for slave */

	/*

	 * ref_control is handled by the hardware/firemware and never

	 * programmed by the software, but the proper values are supplied

	 * in the bspec for verification purposes.

	/*

	 * Add all pipes to the state, and force

	 * a modeset on all the active ones.

	/*

	 * The scanline counter increments at the leading edge of hsync.

	 *

	 * On most platforms it starts counting from vtotal-1 on the

	 * first active line. That means the scanline counter value is

	 * always one less than what we would expect. Ie. just after

	 * start of vblank, which also occurs at start of hsync (on the

	 * last active line), the scanline counter will read vblank_start-1.

	 *

	 * On gen2 the scanline counter starts counting from 1 instead

	 * of vtotal-1, so we have to subtract one (or rather add vtotal-1

	 * to keep the value positive), instead of adding one.

	 *

	 * On HSW+ the behaviour of the scanline counter depends on the output

	 * type. For DP ports it behaves like most other platforms, but on HDMI

	 * there's an extra 1 line difference. So we need to add two instead of

	 * one to the value.

	 *

	 * On VLV/CHV DSI the scanline counter would appear to increment

	 * approx. 1/3 of a scanline before start of vblank. Unfortunately

	 * that means we can't tell whether we're in vblank or not while

	 * we're on that particular line. We must still set scanline_offset

	 * to 1 so that the vblank timestamps come out correct when we query

	 * the scanline counter from within the vblank interrupt handler.

	 * However if queried just before the start of vblank we'll get an

	 * answer that's slightly in the future.

/*

 * This implements the workaround described in the "notes" section of the mode

 * set sequence documentation. When going from no pipes or single pipe to

 * multiple pipes, and planes are enabled after the pipe, we need to wait at

 * least 2 vblanks on the first pipe before enabling planes on the second pipe.

 look at all crtc's that are going to be enabled in during modeset */

 No workaround needed? */

 w/a possibly needed, check how many crtc's are already enabled. */

 2 or more enabled crtcs means no need for w/a */

	/*

	 * If we're not doing the full modeset we want to

	 * keep the current M/N values as they may be

	 * sufficiently different to the computed values

	 * to cause problems.

	 *

	 * FIXME: should really copy more fuzzy state here

 See {hsw,vlv,ivb}_plane_ratio() */

		/*

		 * On some platforms the number of active planes affects

		 * the planes' minimum cdclk calculation. Add such planes

		 * to the state before we compute the minimum cdclk.

	/*

	 * active_planes bitmask has been updated, and potentially

	 * affected planes are part of the state. We can now

	 * compute the minimum cdclk for each plane.

		/*

		 * Currently do this change only if we need to increase

 slave being enabled, is master is still claiming this crtc? */

 master being enabled, slave was already configured? */

/**

 * DOC: asynchronous flip implementation

 *

 * Asynchronous page flip is the implementation for the DRM_MODE_PAGE_FLIP_ASYNC

 * flag. Currently async flip is only supported via the drmModePageFlip IOCTL.

 * Correspondingly, support is currently added for primary plane only.

 *

 * Async flip can only change the plane surface address, so anything else

 * changing is rejected from the intel_atomic_check_async() function.

 * Once this check is cleared, flip done interrupt is enabled using

 * the intel_crtc_enable_flip_done() function.

 *

 * As soon as the surface address register is written, flip done interrupt is

 * generated and the requested events are sent to the usersapce in the interrupt

 * handler itself. The timestamp and sequence sent during the flip done event

 * correspond to the last vblank and have no relation to the actual time when

 * the flip done event was sent.

		/*

		 * TODO: Async flip is only supported through the page flip IOCTL

		 * as of now. So support currently added for primary plane only.

		 * Support for other planes on platforms on which supports

		 * this(vlv/chv and icl+) should be added when async flip is

		 * enabled in the atomic IOCTL path.

		/*

		 * FIXME: This check is kept generic for all platforms.

		 * Need to verify this for all gen9 platforms to enable

		 * this selectively if required.

 plane decryption is allow to change only in synchronous flips */

 Kill old bigjoiner link, we may re-establish afterwards */

/**

 * intel_atomic_check - validate state object

 * @dev: drm device

 * @_state: state to validate

 Light copy */

	/**

	 * Check if fastset is allowed by external dependencies like other

	 * pipes and transcoders.

	 *

	 * Right now it only forces a fullmodeset when the MST master

	 * transcoder did not changed but the pipe of the master transcoder

	 * needs a fullmodeset so all slaves also needs to do a fullmodeset or

	 * in case of port synced crtcs, if one of the synced crtcs

	 * needs a full modeset, all other synced crtcs should be

	 * forced a full modeset.

	/*

	 * FIXME would probably be nice to know which crtc specifically

	 * caused the failure, in cases where we can pinpoint it.

	/*

	 * Update pipe size and adjust fitter if needed: the reason for this is

	 * that in compute_mode_changes we check the native mode (not the pfit

	 * mode) to see if we can flip rather than do a full mode set. In the

	 * fastboot case, we'll flip, but if we don't update the pipesrc and

	 * pfit state, we'll end up with a big fb scanned out into the wrong

	 * sized surface.

 on skylake this is done by detaching scalers */

	/*

	 * The register is supposedly single buffered so perhaps

	 * not 100% correct to do this here. But SKL+ calculate

	 * this based on the adjust pixel rate so pfit changes do

	 * affect it and so it must be updated for fastsets.

	 * HSW/BDW only really need this here for fastboot, after

	 * that the value should not change without a full modeset.

	/*

	 * During modesets pipe configuration was programmed as the

	 * CRTC was enabled.

	/*

	 * Disable the scaler(s) after the plane(s) so that we don't

	 * get a catastrophic underrun even if the two operations

	 * end up happening in two different frames.

 vblanks work again, re-enable pipe CRC. */

 Perform vblank evasion around commit operation */

	/*

	 * We usually enable FIFO underrun interrupts as part of the

	 * CRTC enable sequence during modesets.  But when we inherit a

	 * valid pipe configuration from the BIOS we need to take care

	 * of enabling them on the CRTC's first fastset.

	/*

	 * We still need special handling for disabling bigjoiner master

	 * and slaves since for slave we do not have encoder or plls

	 * so we dont need to disable those.

	/*

	 * We need to disable pipe CRC before disabling the pipe,

	 * or we race against vblank off.

 FIXME unify this for all platforms */

 Only disable port sync and MST slaves */

		/* In case of Transcoder port Sync master slave CRTCs can be

		 * assigned in any order and we need to make sure that

		 * slave CRTCs are disabled first and then master CRTC since

		 * Slave vblanks are masked till Master Vblanks.

 Disable everything else left on */

 ignore allocations for crtc's that have been turned off. */

	/*

	 * Whenever the number of active pipes changes, we need to make sure we

	 * update the pipes in the right order so that their ddb allocations

	 * never overlap with each other between CRTC updates. Otherwise we'll

	 * cause pipe underruns and other bad stuff.

	 *

	 * So first lets enable all pipes that do not need a fullmodeset as

	 * those don't have any external dependency.

			/*

			 * If this is an already active pipe, it's DDB changed,

			 * and this isn't the last pipe that needs updating

			 * then we need to wait for a vblank to pass for the

			 * new ddb allocation to take effect.

	/*

	 * Enable all pipes that needs a modeset and do not depends on other

	 * pipes

	/*

	 * Then we enable all remaining pipes that depend on other

	 * pipes: MST slaves and port sync masters, big joiner master

	/*

	 * Finally we do the plane updates/etc. for all pipes that got enabled.

		/*

		 * The layout of the fast clear color value expected by HW

		 * (the DRM ABI requiring this value to be located in fb at offset 0 of plane#2):

		 * - 4 x 4 bytes per-channel value

		 *   (in surface type specific float/int format provided by the fb user)

		 * - 8 bytes native color value used by the display

		 *   (converted/written by GPU during a fast clear operation using the

		 *    above per-channel values)

		 *

		 * The commit's FB prepare hook already ensured that FB obj is pinned and the

		 * caller made sure that the object is synced wrt. the related color clear value

		 * GPU write on it.

 The above could only fail if the FB obj has an unexpected backing store type. */

 FIXME: Eventually get rid of our crtc->config pointer */

 Complete the events for pipes that have now been disabled */

 Complete events for now disable pipes here. */

 Now enable the clocks, plane, pipe, and connectors that we set up. */

	/* FIXME: We should call drm_atomic_helper_commit_hw_done() here

	 * already, but still need the state for the delayed optimization. To

	 * fix this:

	 * - wrap the optimization/post_plane_update stuff into a per-crtc work.

	 * - schedule that vblank worker _before_ calling hw_done

	 * - at the start of commit_tail, cancel it _synchrously

	 * - switch over to the vblank wait helper in the core after that since

	 *   we don't need out special handling any more.

	/*

	 * Now that the vblank has passed, we can go ahead and program the

	 * optimal watermarks on platforms that need two-step watermark

	 * programming.

	 *

	 * TODO: Move this (and other cleanup) to an async worker eventually.

		/*

		 * Gen2 reports pipe underruns whenever all planes are disabled.

		 * So re-enable underrun reporting after some planes get enabled.

		 *

		 * We do this before .optimize_watermarks() so that we have a

		 * chance of catching underruns with the intermediate watermarks

		 * vs. the new plane configuration.

		/*

		 * DSB cleanup is done in cleanup_work aligning with framebuffer

		 * cleanup. So copy and reset the dsb structure to sync with

		 * commit_done and later do dsb cleanup in cleanup_work.

 Underruns don't always raise interrupts, so check manually */

		/* As one of the primary mmio accessors, KMS has a high

		 * likelihood of triggering bugs in unclaimed access. After we

		 * finish modesetting, see if an error has been flagged, and if

		 * so enable debugging for the next modeset - and hope we catch

		 * the culprit.

	/*

	 * Defer the cleanup of the old state to a separate worker to not

	 * impede the current task (userspace for blocking modesets) that

	 * are executed inline. For out-of-line asynchronous modesets/flips,

	 * deferring to a new worker seems overkill, but we would place a

	 * schedule point (cond_resched()) here anyway to keep latencies

	 * down.

 we do blocking waits in the worker, nothing to do here */

	/*

	 * The intel_legacy_cursor_update() fast path takes care

	 * of avoiding the vblank waits for simple cursor

	 * movement and flips. For cursor on/off and size changes,

	 * we want to perform the vblank waits so that watermark

	 * updates happen during the correct frames. Gen9+ have

	 * double buffered watermarks and so shouldn't need this.

	 *

	 * Unset state->legacy_cursor_update before the call to

	 * drm_atomic_helper_setup_commit() because otherwise

	 * drm_atomic_helper_wait_for_flip_done() is a noop and

	 * we get FIFO underruns because we didn't wait

	 * for vblank.

	 *

	 * FIXME doing watermarks and fb cleanup from a vblank worker

	 * (assuming we had any) would solve these problems.

/**

 * intel_plane_destroy - destroy a plane

 * @plane: plane to destroy

 *

 * Common destruction function for all types of planes (primary, cursor,

 * sprite).

 DDI E can't be used if DDI A requires 4 lanes */

 Haswell uses DDI functions to detect digital outputs. */

		/*

		 * intel_edp_init_connector() depends on this completing first,

		 * to prevent the registration of both eDP and LVDS and the

		 * incorrect sharing of the PPS.

 PCH SDVOB multiplex with HDMIB */

		/*

		 * The DP_DETECTED bit is the latched state of the DDC

		 * SDA pin at boot. However since eDP doesn't require DDC

		 * (no way to plug in a DP->HDMI dongle) the DDC pins for

		 * eDP ports may have been muxed to an alternate function.

		 * Thus we can't rely on the DP_DETECTED bit alone to detect

		 * eDP ports. Consult the VBT as well as DP_DETECTED to

		 * detect eDP ports.

		 *

		 * Sadly the straps seem to be missing sometimes even for HDMI

		 * ports (eg. on Voyo V3 - CHT x7-Z8700), so check both strap

		 * and VBT for the presence of the port. Additionally we can't

		 * trust the port type the VBT declares as we've seen at least

		 * HDMI ports that the VBT claim are DP or eDP.

			/*

			 * eDP not supported on port D,

			 * so no need to worry about it

 Before G4X SDVOC doesn't have its own detect register */

	/*

	 * Can't reject DBLSCAN here because Xorg ddxen can add piles

	 * of DBLSCAN modes to the output's mode list when they detect

	 * the scaling mode property on the connector. And they don't

	 * ask the kernel to validate those modes in any way until

	 * modeset time at which point the client gets a protocol error.

	 * So in order to not upset those clients we silently ignore the

	 * DBLSCAN flag on such connectors. For other connectors we will

	 * reject modes with the DBLSCAN flag in encoder->compute_config().

	 * And we always reject DBLSCAN modes in connector->mode_valid()

	 * as we never want such modes on the connector's mode list.

 Transcoder timing limits */

 FDI max 4096 handled elsewhere */

	/*

	 * Cantiga+ cannot handle modes with a hsync front porch of 0.

	 * WaPruneModeWithIncorrectHsyncOffset:ctg,elk,ilk,snb,ivb,vlv,hsw.

	/*

	 * intel_mode_valid() should be

	 * sufficient on older platforms.

	/*

	 * Most people will probably want a fullscreen

	 * plane so let's not advertize modes that are

	 * too big for that.

/**

 * intel_init_display_hooks - initialize the display modesetting hooks

 * @dev_priv: device private

			/*

			 * Preserve the inherited flag to avoid

			 * taking the full modeset path.

/*

 * Calculate what we think the watermarks should be for the state we've read

 * out of the hardware and then immediately program those watermarks so that

 * we ensure the hardware settings match our internal state.

 *

 * We can calculate what we think WM's should be by creating a duplicate of the

 * current state (which was constructed during hardware readout) and running it

 * through the atomic check code to calculate new watermark values in the

 * state object.

 Only supported on platforms that use atomic watermark design */

	/*

	 * Hardware readout is the only time we don't want to calculate

	 * intermediate watermarks (since we don't trust the current

	 * watermarks).

 Write calculated watermark values back */

	/*

	 * If we fail here, it means that the hardware appears to be

	 * programmed in a way that shouldn't be possible, given our

	 * understanding of watermark requirements.  This might mean a

	 * mistake in the hardware readout code or a mistake in the

	 * watermark calculations for a given platform.  Raise a WARN

	 * so that this is noticeable.

	 *

	 * If this actually happens, we'll have to just leave the

	 * BIOS-programmed watermarks untouched and hope for the best.

			/*

			 * We've not yet detected sink capabilities

			 * (audio,infoframes,etc.) and thus we don't want to

			 * force a full state recomputation yet. We want that to

			 * happen only for the first real commit from userspace.

			 * So preserve the inherited flag for the time being.

			/*

			 * FIXME hack to force a LUT update to avoid the

			 * plane update forcing the pipe gamma on without

			 * having a proper LUT loaded. Remove once we

			 * have readout for pipe gamma enable.

	/*

	 * Maximum framebuffer dimensions, chosen to match

	 * the maximum render engine surface size on gen4+.

 part #1: call before irq install */

 FIXME: completely on the wrong abstraction layer */

 1-4 */

 No DSB so no window2 delay */

 part #2: call after irq install, but before gem init */

	/*

	 * If the platform has HTI, we need to find out whether it has reserved

	 * any display resources before we create our display outputs.

 Just disable it once at startup */

	/*

	 * Make sure hardware watermarks really match the state we read out.

	 * Note that we need to do this after reconstructing the BIOS fb's

	 * since the watermark calculation done here will use pstate->fb.

 part #3: call after gem init */

	/*

	 * Force all active planes to recompute their states. So that on

	 * mode_setcrtc after probe, all the intel_plane_state variables

	 * are already calculated and there is no assert_plane warnings

	 * during bootup.

 Only enable hotplug handling once the fbdev is fully set up. */

 640x480@60Hz, ~25175 kHz */

	/*

	 * Apparently we need to have VGA mode enabled prior to changing

	 * the P1/P2 dividers. Otherwise the DPLL will keep using the old

	 * dividers, even though the register value does change.

 Wait for the clocks to stabilize. */

	/* The pixel multiplier can only be updated once the

	 * DPLL is enabled and the clocks are stable.

	 *

	 * So write it again.

 We do this three times for luck */

 wait for warmup */

 Clear any frame start delays used for debugging left by the BIOS */

 Disable everything but the primary plane */

 Disable any background color/etc. set by the BIOS */

	/* Adjust the state of the output pipe according to whether we

		/*

		 * We start out with underrun reporting disabled to avoid races.

		 * For correct bookkeeping mark this on active crtcs.

		 *

		 * Also on gmch platforms we dont have any hardware bits to

		 * disable the underrun reporting. Which means we need to start

		 * out with underrun reporting disabled also on inactive pipes,

		 * since otherwise we'll complain about the garbage we read when

		 * e.g. coming up after runtime pm.

		 *

		 * No protection against concurrent access is required - at

		 * worst a fifo underrun happens which also sets this to false.

		/*

		 * We track the PCH trancoder underrun reporting state

		 * within the crtc. With crtc for pipe A housing the underrun

		 * reporting state for PCH transcoder A, crtc for pipe B housing

		 * it for PCH transcoder B, etc. LPT-H has only PCH transcoder A,

		 * and marking underrun reporting as disabled for the non-existing

		 * PCH transcoders B and C would prevent enabling the south

		 * error interrupt (see cpt_can_enable_serr_int()).

	/*

	 * Some SNB BIOSen (eg. ASUS K53SV) are known to misprogram

	 * the hardware when a high res displays plugged in. DPLL P

	 * divider is zero, and the pipe timings are bonkers. We'll

	 * try to disable everything in that case.

	 *

	 * FIXME would be nice to be able to sanitize this state

	 * without several WARNs, but for now let's take the easy

	 * road.

	/* We need to check both for a crtc link (meaning that the

	 * encoder is active and trying to read from a pipe) and the

		/* Connector is active, but has no active pipe. This is

		 * fallout from our resume register restoring. Disable

 avoid oopsing in case the hooks consult best_encoder */

 FIXME NULL atomic state passed! */

		/* Inconsistent output/port/pipe state happens presumably due to

		 * a bug in one of the get_hw_state functions. Or someplace else

		 * in our code, like the register restore mess on resume. Clamp

 notify opregion of the sanitized encoder state */

 FIXME read out full plane state for all planes */

 read out to slave crtc as well for bigjoiner */

 encoder should read be linked to bigjoiner master */

				/*

				 * This has to be done during hardware readout

				 * because anything calling .crtc_disable may

				 * rely on the connector_mask being accurate.

			/*

			 * The initial mode needs to be set in order to keep

			 * the atomic core happy. It wants a valid mode if the

			 * crtc's enabled, so we do the above call.

			 *

			 * But we don't set all the derived state fully, hence

			 * set a flag to indicate that a full recalculation is

			 * needed on the next commit.

			/*

			 * FIXME don't have the fb yet, so can't

			 * use intel_plane_data_rate() :(

			/*

			 * FIXME don't have the fb yet, so can't

			 * use plane->min_cdclk() :(

 discard our incomplete slave state, copy it from master */

				/*

				 * FIXME don't have the fb yet, so can't

				 * use intel_plane_data_rate() :(

		/*

		 * MST-primary and inactive encoders don't have a crtc state

		 * and neither of these require any power domain references.

	/*

	 * Display WA #1185 WaDisableDARBFClkGating:glk,icl,ehl,tgl

	 * Also known as Wa_14010480278.

		/*

		 * WaRsPkgCStateDisplayPMReq:hsw

		 * System hang if this isn't done before disabling all planes!

 Display WA #1142:kbl,cfl,cml */

	/*

	 * The BIOS may select transcoder B on some of the PCH

	 * ports even it doesn't enable the port. This would trip

	 * assert_pch_dp_disabled() and assert_pch_hdmi_disabled().

	 * Sanitize the transcoder select bits to prevent that. We

	 * assume that the BIOS never actually enabled the port,

	 * because if it did we'd actually have to toggle the port

	 * on and back off to make the transcoder A select stick

	 * (see. intel_dp_link_down(), intel_disable_hdmi(),

	 * intel_disable_sdvo()).

 PCH SDVOB multiplex with HDMIB */

/* Scan out the current hw modeset state,

 * and sanitizes it to the current state

 HW state is read out, now we need to sanitize this mess. */

	/*

	 * intel_sanitize_plane_mapping() may need to do vblank

	 * waits, so we need vblank interrupts restored beforehand.

 Kill all the work that may have been queued by hpd. */

 part #1: call before irq uninstall */

 part #2: call after irq uninstall */

	/*

	 * Due to the hpd irq storm handling the hotplug work can re-arm the

	 * poll handlers. Hence disable polling after hpd handling is shut down.

	/*

	 * MST topology needs to be suspended so we don't have any calls to

	 * fbdev after it's finalized. MST will be destroyed later as part of

	 * drm_mode_config_cleanup()

 poll work can call into fbdev, hence clean that up afterwards */

 flush any delayed tasks or pending work */

 part #3: call after gem init */

 Must be done after probing outputs */

	/*

	 * Some ports require correctly set-up hpd registers for

	 * detection to work properly (leading to ghost connected

	 * connector status), e.g. VGA on gm45.  Hence we can only set

	 * up the initial fbdev config after hpd irqs are fully

	 * enabled. We do it last so that the async config cannot run

	 * before the connectors are registered.

	/*

	 * We need to coordinate the hotplugs with the asynchronous

	 * fbdev configuration, for which we use the

	 * fbdev->async_cookie.

	/*

	 * After flushing the fbdev (incl. a late async config which

	 * will have delayed queuing of a hotplug event), then flush

	 * the hotplug events.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 *

 * DisplayPort support for G4x,ILK,SNB,IVB,VLV,CHV (HSW+ handled by the DDI code).

/*

 * CHV supports eDP 1.4 that have  more link rates.

 * Below only provides the fixed rate but exclude variable rate.

	/*

	 * CHV requires to program fractional division for m2.

	 * m2 is stored in fixed point format using formula below

	 * (m2_int << 22) | m2_fraction

 m2_int = 32, m2_fraction = 1677722 */

 m2_int = 27, m2_fraction = 0 */

	/*

	 * There are four kinds of DP registers:

	 * IBX PCH

	 * SNB CPU

	 * IVB CPU

	 * CPT PCH

	 *

	 * IBX PCH and CPU are the same for almost everything,

	 * except that the CPU DP PLL is configured in this

	 * register

	 *

	 * CPT PCH is quite different, having many bits moved

	 * to the TRANS_DP_CTL register instead. That

	 * configuration happens (oddly) in ilk_pch_enable

	/* Preserve the BIOS-computed detected bit. This is

	 * supposed to be read-only.

 Handle DP bits in common between all three register formats */

 Split out the IBX/CPU vs CPT settings */

	/*

	 * [DevILK] Work around required when enabling DP PLL

	 * while a pipe is enabled going to FDI:

	 * 1. Wait for the start of vertical blank on the enabled pipe going to FDI

	 * 2. Program DP PLL enable

 must initialize pipe to something for the asserts */

 asserts want to know the pipe even if the port is disabled */

		/*

		 * This is a big fat ugly hack.

		 *

		 * Some machines in UEFI boot mode provide us a VBT that has 18

		 * bpp and 1.62 GHz link bandwidth for eDP, which for reasons

		 * unknown we fail to light up. Yet the same BIOS boots up with

		 * 24 bpp and 2.7 GHz link. Use the same bpp as the BIOS uses as

		 * max, not what it tells us to use.

		 *

		 * Note: This will still be broken if the eDP panel is not lit

		 * up by the BIOS, and thus we can't get the mode at module

		 * load.

	/*

	 * HW workaround for IBX, we need to move the port

	 * to transcoder A after disabling it to allow the

	 * matching HDMI port to be enabled on transcoder A.

		/*

		 * We get CPU/PCH FIFO underruns on the other pipe when

		 * doing the workaround. Sweep them under the rug.

 always enable with pattern 1 (as per spec) */

	/*

	 * Make sure the panel is off before trying to change the mode.

	 * But also ensure that we have vdd while we switch off the panel.

	/*

	 * Bspec does not list a specific disable sequence for g4x DP.

	 * Follow the ilk+ sequence (disable pipe before the port) for

	 * g4x DP as it does not suffer from underruns like the normal

	 * g4x modeset sequence (disable pipe after the port).

 Only ilk+ has port A */

 Assert data lane reset */

 enable with pattern 1 (as per spec) */

	/*

	 * Magic for VLV/CHV. We _must_ first set up the register

	 * without actually enabling the port, and then do another

	 * write to enable the port. Otherwise link training will

	 * fail when the power sequencer is freshly used for this port.

 Only ilk+ has port A */

 Second common lane will stay alive on its own now */

 SNB CPU eDP voltage swing and pre-emphasis control */

 IVB CPU eDP voltage swing and pre-emphasis control */

/*

 * If display is now connected check links status,

 * there has been known issues of link loss triggering

 * long pulse.

 *

 * Some sinks (eg. ASUS PB287Q) seem to perform some

 * weird HPD ping pong during modesets. So we can apparently

 * end up with HPD going low during a modeset, and then

 * going back up soon after. And once that happens we must

 * retrain the link to get a picture. That's in case no

 * userspace component reacted to intermittent HPD dip.

 just do the PHY test and nothing else */

	/*

	 * Keeping it consistent with intel_ddi_hotplug() and

	 * intel_hdmi_hotplug().

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Daniel Vetter <daniel.vetter@ffwll.ch>

 *

/**

 * DOC: fifo underrun handling

 *

 * The i915 driver checks for display fifo underruns using the interrupt signals

 * provided by the hardware. This is enabled by default and fairly useful to

 * debug display issues, especially watermark settings.

 *

 * If an underrun is detected this is logged into dmesg. To avoid flooding logs

 * and occupying the cpu underrun interrupts are disabled after the first

 * occurrence until the next modeset on a given pipe.

 *

 * Note that underrun detection on gmch platforms is a bit more ugly since there

 * is no interrupt (despite that the signalling bit is in the PIPESTAT pipe

 * interrupt register). Also on some other platforms underrun interrupts are

 * shared, which means that if we detect an underrun we need to disable underrun

 * reporting on all pipes.

 *

 * The code also supports underrun detection on the PCH transcoder.

/**

 * intel_set_cpu_fifo_underrun_reporting - set cpu fifo underrrun reporting state

 * @dev_priv: i915 device instance

 * @pipe: (CPU) pipe to set state for

 * @enable: whether underruns should be reported or not

 *

 * This function sets the fifo underrun state for @pipe. It is used in the

 * modeset code to avoid false positives since on many platforms underruns are

 * expected when disabling or enabling the pipe.

 *

 * Notice that on some platforms disabling underrun reports for one pipe

 * disables for all due to shared interrupts. Actual reporting is still per-pipe

 * though.

 *

 * Returns the previous state of underrun reporting.

/**

 * intel_set_pch_fifo_underrun_reporting - set PCH fifo underrun reporting state

 * @dev_priv: i915 device instance

 * @pch_transcoder: the PCH transcoder (same as pipe on IVB and older)

 * @enable: whether underruns should be reported or not

 *

 * This function makes us disable or enable PCH fifo underruns for a specific

 * PCH transcoder. Notice that on some PCHs (e.g. CPT/PPT), disabling FIFO

 * underrun reporting for one transcoder may also disable all the other PCH

 * error interruts for the other transcoders, due to the fact that there's just

 * one interrupt mask/enable bit for all the transcoders.

 *

 * Returns the previous state of underrun reporting.

	/*

	 * NOTE: Pre-LPT has a fixed cpu pipe -> pch transcoder mapping, but LPT

	 * has only one pch transcoder A that all pipes can use. To avoid racy

	 * pch transcoder -> pipe lookups from interrupt code simply store the

	 * underrun statistics in crtc A. Since we never expose this anywhere

	 * nor use it outside of the fifo underrun code here using the "wrong"

	 * crtc on LPT won't cause issues.

/**

 * intel_cpu_fifo_underrun_irq_handler - handle CPU fifo underrun interrupt

 * @dev_priv: i915 device instance

 * @pipe: (CPU) pipe to set state for

 *

 * This handles a CPU fifo underrun interrupt, generating an underrun warning

 * into dmesg if underrun reporting is enabled and then disables the underrun

 * interrupt to avoid an irq storm.

 We may be called too early in init, thanks BIOS! */

 GMCH can't disable fifo underruns, filter them. */

	/*

	 * Starting with display version 11, the PIPE_STAT register records

	 * whether an underrun has happened, and on XELPD+, it will also record

	 * whether the underrun was soft/hard and whether it was triggered by

	 * the downstream port logic.  We should clear these bits (which use

	 * write-1-to-clear logic) too.

	 *

	 * Note that although the IIR gives us the same underrun and soft/hard

	 * information, PIPE_STAT is the only place we can find out whether

	 * the underrun was caused by the downstream port.

/**

 * intel_pch_fifo_underrun_irq_handler - handle PCH fifo underrun interrupt

 * @dev_priv: i915 device instance

 * @pch_transcoder: the PCH transcoder (same as pipe on IVB and older)

 *

 * This handles a PCH fifo underrun interrupt, generating an underrun warning

 * into dmesg if underrun reporting is enabled and then disables the underrun

 * interrupt to avoid an irq storm.

/**

 * intel_check_cpu_fifo_underruns - check for CPU fifo underruns immediately

 * @dev_priv: i915 device instance

 *

 * Check for CPU fifo underruns immediately. Useful on IVB/HSW where the shared

 * error interrupt may have been disabled, and so CPU fifo underruns won't

 * necessarily raise an interrupt, and on GMCH platforms where underruns never

 * raise an interrupt.

/**

 * intel_check_pch_fifo_underruns - check for PCH fifo underruns immediately

 * @dev_priv: i915 device instance

 *

 * Check for PCH fifo underruns immediately. Useful on CPT/PPT where the shared

 * error interrupt may have been disabled, and so PCH fifo underruns won't

 * necessarily raise an interrupt.

/*

 * Copyright Â© 2011 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

 * Authors:

 *   Jesse Barnes <jbarnes@virtuousgeek.org>

 *

 * New plane/sprite handling.

 *

 * The older chips had a separate interface for programming plane related

 * registers; newer ones are much simpler and we can use the new DRM plane

 * support.

	/*

	 * FIXME hsub/vsub vs. block size is a mess. Pre-tgl CCS

	 * abuses hsub/vsub so we can't use them here. But as they

	 * are limited to 32bpp RGB formats we don't actually need

	 * to check anything.

	/*

	 * Hardware doesn't handle subpixel coordinates.

	 * Adjust to (macro)pixel boundary, but be careful not to

	 * increase the source viewport size, because that could

	 * push the downscaling factor out of bounds.

 The points are not evenly spaced. */

	/*

	 * |r|   | c0 c1 c2 |   |cr|

	 * |g| = | c3 c4 c5 | x |y |

	 * |b|   | c6 c7 c8 |   |cb|

	 *

	 * Coefficients are s3.12.

	 *

	 * Cb and Cr apparently come in as signed already, and

	 * we always get full range data in on account of CLRC0/1.

 BT.601 full range YCbCr -> full range RGB */

 BT.709 full range YCbCr -> full range RGB */

 Seems RGB data bypasses the CSC always */

		/*

		 * Expand limited range to full range:

		 * Contrast is applied first and is used to expand Y range.

		 * Brightness is applied second and is used to remove the

		 * offset from Y. Saturation/hue is used to expand CbCr range.

 Pass-through everything. */

 FIXME these register are single buffered :( */

	/*

	 * VLV bspec only considers cases where all three planes are

	 * enabled, and cases where the primary and one sprite is enabled.

	 * Let's assume the case with just two sprites enabled also

	 * maps to the latter case.

	/*

	 * Note that crtc_state->pixel_rate accounts for both

	 * horizontal and vertical panel fitter downscaling factors.

	 * Pre-HSW bspec tells us to only consider the horizontal

	 * downscaling factor here. We ignore that and just consider

	 * both for simplicity.

 Seems RGB data bypasses the gamma always */

 FIXME these register are single buffered :( */

 The two end points are implicit (0.0 and 1.0) */

 Sizes are 0 based */

	/*

	 * The control register self-arms if the plane was previously

	 * disabled. Try to make the plane enable atomic by writing

	 * the control register just before the surface register.

	/*

	 * Note that crtc_state->pixel_rate accounts for both

	 * horizontal and vertical panel fitter downscaling factors.

	 * Pre-HSW bspec tells us to only consider the horizontal

	 * downscaling factor here. We ignore that and just consider

	 * both for simplicity.

	/*

	 * Note that crtc_state->pixel_rate accounts for both

	 * horizontal and vertical panel fitter downscaling factors.

	 * Pre-HSW bspec tells us to only consider the horizontal

	 * downscaling factor here. We ignore that and just consider

	 * both for simplicity.

 Horizontal downscaling limits the maximum pixel rate */

	/*

	 * WaFP16GammaEnabling:ivb,hsw

	 * "Workaround : When using the 64-bit format, the sprite output

	 *  on each color channel has one quarter amplitude. It can be

	 *  brought up to full amplitude by using sprite internal gamma

	 *  correction, pipe gamma correction, or pipe color space

	 *  conversion to multiply the sprite output by four."

 FIXME these register are single buffered :( */

 Sizes are 0 based */

	/* HSW consolidates SPRTILEOFF and SPRLINOFF into a single SPROFFSET

	/*

	 * The control register self-arms if the plane was previously

	 * disabled. Try to make the plane enable atomic by writing

	 * the control register just before the surface register.

 Disable the scaler */

	/*

	 * Note that crtc_state->pixel_rate accounts for both

	 * horizontal and vertical panel fitter downscaling factors.

	 * Pre-HSW bspec tells us to only consider the horizontal

	 * downscaling factor here. We ignore that and just consider

	 * both for simplicity.

 Horizontal downscaling limits the maximum pixel rate */

 Decimation steps at 2x,4x,8x,16x */

 Starting limit is 90% of cdclk */

 -10% per decimation step */

 -10% for RGB */

	/*

	 * We should also do -10% if sprite scaling is enabled

	 * on the other pipe, but we can't really check for that,

	 * so we ignore it.

 Limit to 4k pixels to guarantee TILEOFF.x doesn't get too big. */

 Limit to 8k pixels to guarantee OFFSET.x doesn't get too big. */

 Seems RGB data bypasses the gamma always */

 FIXME these register are single buffered :( */

 The two end points are implicit (0.0 and 1.0) */

 Seems RGB data bypasses the gamma always */

 FIXME these register are single buffered :( */

 Sizes are 0 based */

	/*

	 * The control register self-arms if the plane was previously

	 * disabled. Try to make the plane enable atomic by writing

	 * the control register just before the surface register.

 Disable the scaler */

 CHV ignores the mirror bit when the rotate bit is set :( */

	/*

	 * We want src key enabled on the

	 * sprite and not on the primary.

	/*

	 * On SKL+ we want dst key enabled on

	 * the primary and not on the sprite.

 ignore the pointless "none" flag */

 Make sure we don't try to enable both src & dest simultaneously */

	/*

	 * SKL+ only plane 2 can do destination keying against plane 1.

	 * Also multiple planes can't do destination keying on the same

	 * pipe simultaneously.

		/*

		 * On some platforms we have to configure

		 * the dst colorkey on the primary plane.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2018 Intel Corporation

	/*

	 * Some platforms only expect PHY_MISC to be programmed for PHY-A and

	 * PHY-B and may not even have instances of the register for the

	 * other combo PHY's.

	 *

	 * ADL-S technically has three instances of PHY_MISC, but only requires

	 * that we program it for PHY A.

 The PHY C added by EHL has no PHY_MISC register */

	/*

	 * VBT's 'dvo port' field for child devices references the DDI, not

	 * the PHY.  So if combo PHY A is wired up to drive an external

	 * display, we should see a child device present on PORT_D and

	 * nothing on PORT_A and no DSI.

	/*

	 * If we encounter a VBT that claims to have an external display on

	 * DDI-D _and_ an internal display on DDI-A/DSI leave an error message

	 * in the log and let the internal display win.

	/*

	 * Certain PHYs are connected to compensation resistors and act

	 * as masters to other PHYs.

	 *

	 * ICL,TGL:

	 *   A(master) -> B(slave), C(slave)

	 * RKL,DG1:

	 *   A(master) -> B(slave)

	 *   C(master) -> D(slave)

	 * ADL-S:

	 *   A(master) -> B(slave), C(slave)

	 *   D(master) -> E(slave)

	 *

	 * We must set the IREFGEN bit for any PHY acting as a master

	 * to another PHY.

		/*

		 * EHL's combo PHY A can be hooked up to either an external

		 * display (via DDI-D) or an internal display (via DDI-A or

		 * the DSI DPHY).  This is a motherboard design decision that

		 * can't be changed on the fly, so initialize the PHY's mux

		 * based on whether our VBT indicates the presence of any

		 * "internal" child devices.

				/*

				 * A known problem with old ifwi:

				 * https://gitlab.freedesktop.org/drm/intel/-/issues/2411

				 * Suppress the warning for CI. Remove ASAP!

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Pierre-Louis Bossart <pierre-louis.bossart@linux.intel.com>

 *    Jerome Anand <jerome.anand@intel.com>

 *    based on VED patches

 *

/**

 * DOC: LPE Audio integration for HDMI or DP playback

 *

 * Motivation:

 * Atom platforms (e.g. valleyview and cherryTrail) integrates a DMA-based

 * interface as an alternative to the traditional HDaudio path. While this

 * mode is unrelated to the LPE aka SST audio engine, the documentation refers

 * to this mode as LPE so we keep this notation for the sake of consistency.

 *

 * The interface is handled by a separate standalone driver maintained in the

 * ALSA subsystem for simplicity. To minimize the interaction between the two

 * subsystems, a bridge is setup between the hdmi-lpe-audio and i915:

 * 1. Create a platform device to share MMIO/IRQ resources

 * 2. Make the platform device child of i915 device for runtime PM.

 * 3. Create IRQ chip to forward the LPE audio irqs.

 * the hdmi-lpe-audio driver probes the lpe audio device and creates a new

 * sound card

 *

 * Threats:

 * Due to the restriction in Linux platform device model, user need manually

 * uninstall the hdmi-lpe-audio driver before uninstalling i915 module,

 * otherwise we might run into use-after-free issues after i915 removes the

 * platform device: even though hdmi-lpe-audio driver is released, the modules

 * is still in "installed" status.

 *

 * Implementation:

 * The MMIO/REG platform resources are created according to the registers

 * specification.

 * When forwarding LPE audio irqs, the flow control handler selection depends

 * on the platform, for example on valleyview handle_simple_irq is enough.

 *

 B,C,D or B,C */

	/* XXX Note that platform_device_register_full() allocates a dma_mask

	 * and never frees it. We can't free it here as we cannot guarantee

	 * this is the last reference (i.e. that the dma_mask will not be

	 * used after our unregister). So ee choose to leak the sizeof(u64)

	 * allocation here - it should be fixed in the platform_device rather

	 * than us fiddle with its internals.

 Baytrail */

 Braswell */

	/* enable chicken bit; at least this is required for Dell Wyse 3040

	 * with DP outputs (but only sometimes by some reason!)

/**

 * intel_lpe_audio_irq_handler() - forwards the LPE audio irq

 * @dev_priv: the i915 drm device private data

 *

 * the LPE Audio irq is forwarded to the irq handler registered by LPE audio

 * driver.

/**

 * intel_lpe_audio_init() - detect and setup the bridge between HDMI LPE Audio

 * driver and i915

 * @dev_priv: the i915 drm device private data

 *

 * Return: 0 if successful. non-zero if detection or

 * llocation/initialization fails

/**

 * intel_lpe_audio_teardown() - destroy the bridge between HDMI LPE

 * audio driver and i915

 * @dev_priv: the i915 drm device private data

 *

 * release all the resources for LPE audio <-> i915 bridge.

/**

 * intel_lpe_audio_notify() - notify lpe audio event

 * audio driver and i915

 * @dev_priv: the i915 drm device private data

 * @pipe: pipe

 * @port: port

 * @eld : ELD data

 * @ls_clock: Link symbol clock in kHz

 * @dp_output: Driving a DP output?

 *

 * Notify lpe audio driver of eld change.

 Unmute the amp for both DP and HDMI */

 Mute the amp for both DP and HDMI */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel ACPI functions

 *

 * _DSM related code stolen from nouveau_acpi.c.

 For Calpella anyway... */

 No args */

 No args */

/*

 * ACPI Specification, Revision 5.0, Appendix B.3.2 _DOD (Enumerate All Devices

 * Attached to the Display Adapter).

 Populate the ACPI IDs for all connectors for a given drm_device */

 Use display type specific display index. */

 NOTE: The connector order must be final before this is called. */

 Always getting the next, even when the last was not used. */

			/*

			 * Integrated displays have a specific address 0x1f on

			 * most Intel platforms, but not on all of them.

	/*

	 * device_get_next_child_node() takes a reference on the fwnode, if

	 * we stopped iterating because we are out of connectors we need to

	 * put this, otherwise fwnode is NULL and the put is a no-op.

/*

 * Copyright Â© 2008 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 * Authors:

 *    Keith Packard <keithp@keithp.com>

 *

 DP DSC throughput values used for slice count calculations KPixels/s */

 DP DSC FEC Overhead factor = 1/(0.972261) */

 Compliance test status bits  */

 Constants for DP DSC configurations */

/* With Single pipe configuration, HW is capable of supporting maximum

 * of 4 slices per line.

/**

 * intel_dp_is_edp - is the given port attached to an eDP panel (either CPU or PCH)

 * @intel_dp: DP struct

 *

 * If a CPU or PCH DP output is attached to an eDP panel, this function

 * will return true, and false otherwise.

 *

 * This function is not safe to use prior to encoder type being set.

 Is link rate UHBR and thus 128b/132b? */

 update sink rates from dpcd */

 Needed, e.g., for Apple MBP 2017, 15 inch eDP Retina panel */

	/*

	 * Sink rates for 8b/10b.

	/*

	 * Sink rates for 128b/132b. If set, sink should support all 8b/10b

	 * rates and 10 Gbps.

 We have a repeater */

 Repeater supports 128b/132b, valid UHBR rates */

 Does not support 128b/132b */

 Get length of rates array potentially limited by max_rate. */

 Limit results by potentially reduced max rate */

 Get length of common rates array potentially limited by max_rate. */

 Theoretical max between source and sink */

 Theoretical max between source and sink */

/*

 * The required data bandwidth for a mode with given pixel clock and bpp. This

 * is the required net bandwidth independent of the data bandwidth efficiency.

 pixel_clock is in kHz, divide bpp by 8 for bit to Byte conversion */

/*

 * Given a link rate and lanes, get the data bandwidth.

 *

 * Data bandwidth is the actual payload rate, which depends on the data

 * bandwidth efficiency and the link rate.

 *

 * For 8b/10b channel encoding, SST and non-FEC, the data bandwidth efficiency

 * is 80%. For example, for a 1.62 Gbps link, 1.62*10^9 bps * 0.80 * (1/8) =

 * 162000 kBps. With 8-bit symbols, we have 162000 kHz symbol clock. Just by

 * coincidence, the port clock in kHz matches the data bandwidth in kBps, and

 * they equal the link bit rate in Gbps multiplied by 100000. (Note that this no

 * longer holds for data bandwidth as soon as FEC or MST is taken into account!)

 *

 * For 128b/132b channel encoding, the data bandwidth efficiency is 96.71%. For

 * example, for a 10 Gbps link, 10*10^9 bps * 0.9671 * (1/8) = 1208875

 * kBps. With 32-bit symbols, we have 312500 kHz symbol clock. The value 1000000

 * does not match the symbol clock, the port clock (not even if you think in

 * terms of a byte clock), nor the data bandwidth. It only matches the link bit

 * rate in units of 10000 bps.

		/*

		 * UHBR rates always use 128b/132b channel encoding, and have

		 * 97.71% data bandwidth efficiency. Consider max_link_rate the

		 * link bit rate in units of 10000 bps.

	/*

	 * Lower than UHBR rates always use 8b/10b channel encoding, and have

	 * 80% data bandwidth efficiency for SST non-FEC. However, this turns

	 * out to be a nop by coincidence, and can be skipped:

	 *

	 *	int max_link_rate_kbps = max_link_rate * 10;

	 *	max_link_rate_kbps = DIV_ROUND_CLOSEST_ULL(max_link_rate_kbps * 8, 10);

	 *	max_link_rate = max_link_rate_kbps / 8;

 The values must be in increasing order */

 This should only be done once */

 return index of rate in rates array, or -1 if not found */

 Paranoia, there should always be something in common. */

	/*

	 * FIXME: we need to synchronize the current link parameters with

	 * hardware readout. Currently fast link training doesn't work on

	 * boot-up.

	/*

	 * TODO: Enable fallback on MST links once MST link compute can handle

	 * the fallback params.

	/*

	 * Available Link Bandwidth(Kbits/sec) = (NumberOfLanes)*

	 * (LinkSymbolClock)* 8 * (TimeSlotsPerMTP)

	 * for SST -> TimeSlotsPerMTP is 1,

	 * for MST -> TimeSlotsPerMTP has to be calculated

 Small Joiner Check: output bpp <= joiner RAM (bits) / Horiz. width */

	/*

	 * Greatest allowed DSC BPP = MIN (output BPP from available Link BW

	 * check, output bpp from small joiner RAM check)

 Error out if the max bpp is less than smallest allowed valid bpp */

 From XE_LPD onwards we support from bpc upto uncompressed bpp-1 BPPs */

 Find the nearest match in the array of known BPPs from VESA */

	/*

	 * Compressed BPP in U6.4 format so multiply by 16, for Gen 11,

	 * fractional part is 0

 Also take into account max slice width */

 Find the closest match to the valid slice count values */

 big joiner needs small joiner to be enabled */

	/*

	 * bpp value was assumed to RGB format. And YCbCr 4:2:0 output

	 * format of the number of bytes per pixel will be half the number

	 * of bytes of RGB pixel.

	/*

	 * Older platforms don't like hdisplay==4096 with DP.

	 *

	 * On ILK/SNB/IVB the pipe seems to be somewhat running (scanline

	 * and frame counter increment), but we don't get vblank interrupts,

	 * and the pipe underruns immediately. The link also doesn't seem

	 * to get trained properly.

	 *

	 * On CHV the vblank interrupts don't seem to disappear but

	 * otherwise the symptoms are similar.

	 *

	 * TODO: confirm the behaviour on HSW+

 If PCON supports FRL MODE, check FRL bandwidth constraints */

 converting bw from Gbps to Kbps*/

 Assume 8bpc for the DP++/HDMI/DVI TMDS clock check */

	/*

	 * Output bpp is stored in 6.4 format so right shift by 4 to get the

	 * integer value since we support only integer values of bpp.

		/*

		 * TBD pass the connector BPC,

		 * for now U8_MAX so that max BPC on that platform would be picked

	/*

	 * Big joiner configuration needs DSC for TGL which is not true for

	 * XE_LPD where uncompressed joiner is supported.

 FIXME: too big for stack? */

 eDP 1.4 rate select method. */

 On TGL, FEC is supported on all Pipes */

 Get bpp from vbt only for panels that dont have bpp in edid */

 Adjust link config limits based on compliance test requests. */

 For DP Compliance we override the computed bpp for the pipe */

 Use values requested by Compliance Test Request */

		/* Validate the compliance test data since max values

		 * might have changed due to link train fallback.

 Optimize link config in order: max bpp, min clock, min lanes */

 Max DSC Input BPC for ICL is 10 and for TGL+ is 12 */

	/*

	 * RC_MODEL_SIZE is currently a constant across all configurations.

	 *

	 * FIXME: Look into using sink defined DPCD DP_DSC_RC_BUF_BLK_SIZE and

	 * DP_DSC_RC_BUF_SIZE for this.

	/*

	 * Slice Height of 8 works for all currently available panels. So start

	 * with that if pic_height is an integral multiple of 8. Eventually add

	 * logic to try multiple slice heights.

 Min Input BPC for ICL+ is 8 */

	/*

	 * For now enable DSC for max bpp, max link rate, max lane count.

	 * Optimize this later for the minimum possible link rate/lane count

	 * with DSC enabled for the requested mode.

 As of today we support DSC for only RGB */

	/*

	 * VDSC engine operates at 1 Pixel per clock, so if peak pixel rate

	 * is greater than the maximum Cdclock and if slice count is even

	 * then we need to use 2 VDSC instances.

 No common link rates between source and sink */

		/*

		 * Use the maximum clock and number of lanes the eDP panel

		 * advertizes being capable of in case the initial fast

		 * optimal params failed us. The panels are generally

		 * designed to support only a single clock and lane

		 * configuration, and typically on older panels these

		 * values correspond to the native resolution of the panel.

	/*

	 * Optimize for slow and wide for everything, because there are some

	 * eDP 1.3 and 1.4 panels don't work well with fast and narrow.

	/*

	 * Pipe joiner needs compression upto display12 due to BW limitation. DG2

	 * onwards pipe joiner can be enabled without compression.

	/*

	 * Our YCbCr output is always limited range.

	 * crtc_state->limited_color_range only applies to RGB,

	 * and it must never be set for YCbCr or we risk setting

	 * some conflicting bits in PIPECONF which will mess up

	 * the colors on the monitor.

		/*

		 * See:

		 * CEA-861-E - 5.1 Default Encoding Parameters

		 * VESA DisplayPort Ver.1.2a - 5.1.1.1 Video Colorimetry

	/*

	 * Prepare VSC Header for SU as per DP 1.4 spec, Table 2-118

	 * VSC SDP supporting 3D stereo, PSR2, and Pixel Encoding/

	 * Colorimetry Format indication.

 DP 1.4a spec, Table 2-120 */

		/*

		 * RGB->YCBCR color conversion uses the BT.709

		 * color space.

 only RGB pixelformat supports 6 bpc */

 all YCbCr are always limited range */

 When a crtc state has PSR, VSC SDP will be handled by PSR routine */

 [PSR2, +Colorimetry] */

			/*

			 * [PSR2, -Colorimetry]

			 * Prepare VSC Header for SU as per eDP 1.4 spec, Table 6-11

			 * 3D stereo + PSR/PSR2 + Y-coordinate.

		/*

		 * [PSR1]

		 * Prepare VSC Header for SU as per DP 1.4 spec, Table 2-118

		 * VSC SDP supporting 3D stereo + PSR (applies to eDP v1.3 or

		 * higher).

 FIXME: abstract this better */

 Enable backlight PWM and backlight PP control. */

 Disable backlight PP control and backlight PWM. */

	/*

	 * DPCD 1.2+ should support BRANCH_DEVICE_CTRL, and thus

	 * be capable of signalling downstream hpd with a long pulse.

	 * Whether or not that means D3 is safe to use is not clear,

	 * but let's assume so until proven otherwise.

	 *

	 * FIXME should really check all downstream ports...

	/*

	 * During driver init, we want to be careful and avoid changing the source OUI if it's

	 * already set to what we want, so as to avoid clearing any state by accident

 If the device supports it, try to set the power state appropriately */

 Should have a valid DPCD by this point */

 Write the source OUI as early as possible */

		/*

		 * When turning on, we need to retry for 1ms to give the sink

		 * time to wake up.

/**

 * intel_dp_sync_state - sync the encoder state during init/resume

 * @encoder: intel encoder to sync

 * @crtc_state: state for the CRTC connected to the encoder

 *

 * Sync any state stored in the encoder wrt. HW state during driver init

 * and system resume.

	/*

	 * Don't clobber DPCD if it's been already read out during output

	 * setup (eDP) or detect.

	/*

	 * If BIOS has set an unsupported or non-standard link rate for some

	 * reason force an encoder recompute and full modeset.

	/*

	 * FIXME hack to force full modeset when DSC is being used.

	 *

	 * As long as we do not have full state readout and config comparison

	 * of crtc_state->dsc, we have no way to ensure reliable fastset.

	 * Remove once we have readout for DSC.

 Clear the cached register set to avoid using stale values */

 Wait for PCON to be FRL Ready */

	/*

	 * Wait for FRL to be completed

	 * Check if the HDMI Link is up and active.

 Verify HDMI Link configuration shows FRL Mode */

	/*

	 * Always go for FRL training if:

	 * -PCON supports SRC_CTL_MODE (VESA DP2.0-HDMI2.1 PCON Spec Draft-1 Sec-7)

	 * -sink is HDMI2.1

		/*

		 * FIXME: Currently if userspace selects BT2020 or BT709, but PCON supports only

		 * RGB->YCbCr for BT601 colorspace, we go ahead with BT601, as default.

		 *

	/*

	 * Clear the cached register set to avoid using stale values

	 * for the sinks that do not support DSC.

 Clear fec_capable to avoid using stale values */

 Cache the DSC DPCD if eDP or DP rev >= 1.4 */

 FEC is supported only on DP 1.4 */

 Valid configurations are SST or MSO 2x1, 2x2, 4x1 */

 this function is meant to be called only once */

	/*

	 * Read the eDP display control registers.

	 *

	 * Do this independent of DP_DPCD_DISPLAY_CONTROL_CAPABLE bit in

	 * DP_EDP_CONFIGURATION_CAP, because some buggy displays do not have it

	 * set, but require eDP 1.4+ detection (e.g. for supported link rates

	 * method). The display control registers should read zero if they're

	 * not supported anyway.

	/*

	 * This has to be called after intel_dp->edp_dpcd is filled, PSR checks

	 * for SET_POWER_CAPABLE bit in intel_dp->edp_dpcd[1]

 Clear the default sink rates */

 Read the eDP 1.4+ supported link rates. */

			/* Value read multiplied by 200kHz gives the per-lane

			 * link rate in kHz. The source rates are, however,

			 * stored in terms of LS_Clk kHz. The full conversion

			 * back to symbols is

			 * (val * 200kHz)*(8/10 ch. encoding)*(1/8 bit to Byte)

	/*

	 * Use DP_LINK_RATE_SET if DP_SUPPORTED_LINK_RATES are available,

	 * default to DP_MAX_LINK_RATE and DP_LINK_BW_SET otherwise.

 Read the eDP DSC DPCD registers */

	/*

	 * If needed, program our source OUI so we can make various Intel-specific AUX services

	 * available (such as HDR backlight controls)

	/*

	 * Don't clobber cached eDP rates. Also skip re-reading

	 * the OUI/ID since we know it won't change.

		/*

		 * Sink count can change between short pulse hpd hence

		 * a member variable in intel_dp will track any changes

		 * between short pulse interrupts.

		/*

		 * SINK_COUNT == 0 and DOWNSTREAM_PORT_PRESENT == 1 implies that

		 * a dongle is present but no display. Unless we require to know

		 * if a dongle is present or not, we don't need to update

		 * downstream port information. So, an early return here saves

		 * time from performing other operations which are not required.

	/*

	 * As per DP 1.4a spec section 2.2.4.3 [MSA Field for Indication

	 * of Color Encoding Format and Content Color Gamut], in order to

	 * sending YCBCR 420 or HDR BT.2020 signals we should use DP VSC SDP.

	/*

	 * Prepare VSC Header for SU as per DP 1.4a spec, Table 2-119

	 * VSC SDP Header Bytes

 Secondary-Data Packet ID = 0 */

 Secondary-data Packet Type */

 Revision Number */

 Number of Valid Data Bytes */

	/*

	 * Only revision 0x5 supports Pixel Encoding/Colorimetry Format as

	 * per DP 1.4a spec.

 VSC SDP Payload for DB16 through DB18 */

 Pixel Encoding and Colorimetry Formats  */

 DB16[7:4] */

 DB16[3:0] */

 6bpc: 0x0 */

 DB17[3:0] */

 Dynamic Range and Component Bit Depth */

 DB17[7] */

 Content Type */

	/*

	 * Set up the infoframe sdp packet for HDR static metadata.

	 * Prepare VSC Header for SU as per DP 1.4a spec,

	 * Table 2-100 and Table 2-101

 Secondary-Data Packet ID, 00h for non-Audio INFOFRAME */

	/*

	 * Packet Type 80h + Non-audio INFOFRAME Type value

	 * HDMI_INFOFRAME_TYPE_DRM: 0x87

	 * - 80h + Non-audio INFOFRAME Type value

	 * - InfoFrame Type: 0x07

	 *    [CTA-861-G Table-42 Dynamic Range and Mastering InfoFrame]

	/*

	 * Least Significant Eight Bits of (Data Byte Count â 1)

	 * infoframe_size - 1

 INFOFRAME SDP Version Number */

 CTA Header Byte 2 (INFOFRAME Version Number) */

 CTA Header Byte 3 (Length of INFOFRAME): HDMI_DRM_INFOFRAME_SIZE */

	/*

	 * Copy HDMI_DRM_INFOFRAME_SIZE size from a buffer after

	 * HDMI_INFOFRAME_HEADER_SIZE

	/*

	 * Size of DP infoframe sdp packet for HDR static metadata consists of

	 * - DP SDP Header(struct dp_sdp_header): 4 bytes

	 * - Two Data Blocks: 2 bytes

	 *    CTA Header Byte2 (INFOFRAME Version Number)

	 *    CTA Header Byte3 (Length of INFOFRAME)

	 * - HDMI_DRM_INFOFRAME_SIZE: 26 bytes

	 *

	 * Prior to GEN11's GMP register size is identical to DP HDR static metadata

	 * infoframe size. But GEN11+ has larger than that size, write_infoframe

	 * will pad rest of the size.

 TODO: Add DSC case (DIP_ENABLE_PPS) */

 When PSR is enabled, this routine doesn't disable VSC DIP */

 When PSR is enabled, VSC SDP is handled by PSR routine */

		/*

		 * - HB2 = 0x2, HB3 = 0x8

		 *   VSC SDP supporting 3D stereo + PSR

		 * - HB2 = 0x4, HB3 = 0xe

		 *   VSC SDP supporting 3D stereo + PSR2 with Y-coordinate of

		 *   first scan line of the SU region (applies to eDP v1.4b

		 *   and higher).

		/*

		 * - HB2 = 0x5, HB3 = 0x13

		 *   VSC SDP supporting 3D stereo + PSR2 + Pixel Encoding/Colorimetry

		 *   Format.

	/*

	 * Least Significant Eight Bits of (Data Byte Count â 1)

	 * 1Dh (i.e., Data Byte Count = 30 bytes).

 Most Significant Two Bits of (Data Byte Count â 1), Clear to 00b. */

 INFOFRAME SDP Version Number */

 CTA Header Byte 2 (INFOFRAME Version Number) */

 CTA Header Byte 3 (Length of INFOFRAME): HDMI_DRM_INFOFRAME_SIZE */

 When PSR is enabled, VSC SDP is handled by PSR routine */

	/* (DP CTS 1.2)

	 * 4.3.1.11

 Read the TEST_LANE_COUNT and TEST_LINK_RTAE fields (DP CTS 3.1.4) */

 Validate the requested link rate and lane count */

 Read the TEST_PATTERN (DP CTS 3.1.5) */

 Set test active flag here so userspace doesn't interrupt things */

		/* Check EDID read for NACKs, DEFERs and corruption

		 * (DP CTS 1.2 Core r1.1)

		 *    4.2.2.4 : Failed EDID read, I2C_NAK

		 *    4.2.2.5 : Failed EDID read, I2C_DEFER

		 *    4.2.2.6 : EDID corruption detected

		 * Use failsafe mode for all cases

		/* We have to write the checksum

		 * of the last block read

 Set test active flag here so userspace doesn't interrupt things */

		/*

		 * FIXME: Ideally pattern should come from DPCD 0x250. As

		 * current firmware of DPR-100 could not set it, so hardcoding

		 * now for complaince test.

		/*

		 * FIXME: Ideally pattern should come from DPCD 0x24A. As

		 * current firmware of DPR-100 could not set it, so hardcoding

		 * now for complaince test.

 retrieve vswing & pre-emphasis setting */

 Set test active flag here so userspace doesn't interrupt things */

/**

 * intel_dp_check_mst_status - service any pending MST interrupts, check link status

 * @intel_dp: Intel DP struct

 *

 * Read any pending MST interrupts, call MST core to handle these and ack the

 * interrupts. Check if the main and AUX link state is ok.

 *

 * Returns:

 * - %true if pending interrupts were serviced (or no interrupts were

 *   pending) w/o detecting an error condition.

 * - %false if an error condition - like AUX failure or a loss of link - is

 *   detected, which needs servicing from the hotplug work.

		/*

		 * The +2 is because DP_DPRX_ESI_LEN is 14, but we then

		 * pass in "esi+10" to drm_dp_channel_eq_ok(), which

		 * takes a 6-byte array. So we actually need 16 bytes

		 * here.

		 *

		 * Somebody who knows what the limits actually are

		 * should check this, but for now this is at least

		 * harmless and avoids a valid compiler warning about

		 * using more of the array than we have allocated.

 check link status - esi[10] = 0x200c */

 Restart FRL training or fall back to TMDS mode */

	/*

	 * While PSR source HW is enabled, it will control main-link sending

	 * frames, enabling and disabling it so trying to do a retrain will fail

	 * as the link would or not be on or it could mix training patterns

	 * and frame data at the same time causing retrain to fail.

	 * Also when exiting PSR, HW will retrain the link anyways fixing

	 * any link status error.

	/*

	 * Validate the cached values of intel_dp->link_rate and

	 * intel_dp->lane_count before attempting to retrain.

	 *

	 * FIXME would be nice to user the crtc state here, but since

	 * we need to call this from the short HPD handler that seems

	 * a bit hard.

 Retrain if Channel EQ or CR not ok */

 SST */

 MST */

 Suppress underruns caused by re-training */

 retrain on the MST master transcoder */

 Keep underrun reporting disabled until things are stable */

 test on the MST master transcoder */

/*

 * According to DP spec

 * 5.1.2:

 *  1. Read DPCD

 *  2. Configure link according to Receiver Capabilities

 *  3. Use Link Training from 2.5.3.3 and 3.5.1.3

 *  4. Check link status on receipt of hot-plug interrupt

 *

 * intel_dp_short_pulse -  handles short pulse interrupts

 * when full detection is not required.

 * Returns %true if short pulse is handled and full detection

 * is NOT required and %false otherwise.

	/*

	 * Clearing compliance test variables to allow capturing

	 * of values for next automated test request.

	/*

	 * Now read the DPCD to see if it's actually running

	 * If the current value of sink count doesn't match with

	 * the value that was stored earlier or dpcd read failed

	 * we need to do full detection

 No need to proceed if we are going to do full detect */

 Handle CEC interrupts, if any */

 defer to the hotplug work for link retraining if needed */

 Send a Hotplug Uevent to userspace to start modeset */

		/*

		 * Schedule long hpd to do the test

		 *

		 * FIXME get rid of the ad-hoc phy test modeset code

		 * and properly incorporate it into the normal modeset.

 XXX this is probably wrong for multiple downstream ports */

 if there's no downstream port, we're done */

 If we're HPD-aware, SINK_COUNT changes dynamically */

 If no HPD, poke DDC gently */

 Well we tried, say unknown for unreliable port types */

 Anything else is out of spec, warn and ignore */

/*

 * intel_digital_port_connected - is the specified port connected?

 * @encoder: intel_encoder

 *

 * In cases where there's a connector physically connected but it can't be used

 * by our hardware we also return false, since the rest of the driver should

 * pretty much treat the port as disconnected. This is relevant for type-C

 * (starting on ICL) where there's ownership involved.

 *

 * Return %true if port is connected, %false otherwise.

 use cached edid if we have one */

 invalid edid */

 No YCbCr output support on gmch platforms */

	/*

	 * ILK doesn't seem capable of DP YCbCr output. The

	 * displayed image is severly corrupted. SNB+ is fine.

 on-board LSPCON always assumed to support 4:4:4->4:2:0 conversion */

 Let PCON convert from RGB->YCbCr if possible */

 Prefer 4:2:0 passthrough over 4:4:4->4:2:0 conversion */

 4:4:4->4:2:0 conversion is the only way */

 Can't disconnect eDP */

 Read DP Sink DSC Cap DPCD regs for DP v1.4 */

	/*

	 * TODO: Reset link params when switching to MST mode, until MST

	 * supports link training fallback params.

		/*

		 * If we are in MST mode then this connector

		 * won't appear connected or have anything

		 * with EDID on it

	/*

	 * Some external monitors do not signal loss of link synchronization

	 * with an IRQ_HPD, so force a link status check.

	/*

	 * Clearing NACK and defer counts to get their exact values

	 * while reading EDID which are required by Compliance tests

	 * 4.2.2.4 and 4.2.2.5

	/*

	 * Make sure the refs for power wells enabled during detect are

	 * dropped to avoid a new detect cycle triggered by HPD polling.

 Also add fixed mode, which may or may not be present in EDID */

	/*

	 * ToDo: Clean this up to handle lspcon init and resume more

	 * efficiently and streamlined.

	/*

	 * We don't enable port sync on BDW due to missing w/as and

	 * due to not having adjusted the modeset sequence appropriately.

		/*

		 * vdd off can generate a long/short pulse on eDP which

		 * would require vdd on to handle it, and thus we

		 * would end up in an endless cycle of

		 * "vdd off -> long/short hpd -> vdd on -> detect -> vdd off -> ..."

 check the VBT to see whether the eDP is on another port */

	/*

	 * eDP not supported on g4x. so bail out early just

	 * for a bit extra safety in case the VBT is bonkers.

 Register HDMI colorspace for case of lspcon */

	/*

	 * On IBX/CPT we may get here with LVDS already registered. Since the

	 * driver uses the only internal power sequencer available for both

	 * eDP and LVDS bail out early in this case to prevent interfering

	 * with an already powered-on LVDS power sequencer.

 Cache DPCD and EDID for edp. */

 if this fails, presume the device is a ghost */

 MSO requires information from the EDID */

 multiply the mode clock and horizontal timings for MSO */

 fallback to VBT if available for eDP */

		/*

		 * Figure out the current pipe for the initial backlight setup.

		 * If the current pipe isn't valid, try the PPS pipe, and if that

		 * fails just assume pipe A.

 Grab the locks before changing connector property*/

	/* Set connector link status to BAD and send a Uevent to notify

	 * userspace to do a modeset.

 Send Hotplug uevent so userspace can reprobe */

 Initialize the work for modeset in case of link train failure */

 Preserve the current hw state. */

		/*

		 * Currently we don't support eDP on TypeC ports, although in

		 * theory it could work on TypeC legacy ports.

 eDP only on port B and/or C on vlv/chv */

 init MST on ports that can support it */

	/* For G4X desktop chip, PEG_BAND_GAP_DATA 3:0 must first be written

	 * 0xd.  Failure to do so will result in spurious interrupts being

	 * generated on the port when a cable is not attached.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 no global SR status; inspect per-plane WM */;

	/*

	 * SKL+ Perf counter is reset to 0 everytime DC state is entered

		/*

		 * Reading all 3 registers before hand to minimize crossing a

		 * frame boundary between register reads

 Find the first EDP which supports PSR */

 TODO: split to each transcoder's PSR debug state

 TODO: split to each transcoder's PSR debug state

		/*

		 * NOTE: DMC_DEBUG3 is a general purpose reg.

		 * According to B.Specs:49196 DMC f/w reuses DC5/6 counter

		 * reg for DC3CO debugging and validation,

		 * but TGL DMC f/w is using DMC_DEBUG3 reg for DC3CO counter.

	/*

	 * Deliberately omitting default: to generate compiler warnings

	 * when a new drm_plane_type gets added.

	/*

	 * According to doc only one DRM_MODE_ROTATE_ is allowed but this

	 * will print them all to visualize if the values are misused

 Not all platformas have a scaler */

 May race with an update. Meh. */

 DRRS Supported */

 disable_drrs() will make drrs->dp NULL */

 DRRS not supported. Print the VBT parameter*/

			/* To prevent erroneous activation of the compliance

			 * testing code, only accept an actual value of 1 here

		/*

		 * - WM1+ latency values in 0.5us units

		 * - latencies are in us on gen9/vlv/chv

	/* Synchronize with everything first in case there's been an HPD

	 * storm, but we haven't finished handling it in the kernel yet

 Strip newline, if any */

 Reset the HPD storm stats so we don't accidentally trigger a storm */

 Re-enable hpd immediately if we were in an irq storm */

 Strip newline, if any */

 Reset to the "default" state for this system */

 Reset the HPD storm stats so we don't accidentally trigger a storm */

 Re-enable hpd immediately if we were in an irq storm */

		/*

		 * Actually TGL can drive LPSP on port till DDI_C

		 * but there is no physical connected DDI_C on TGL sku's,

		 * even driver is not initilizing DDI_C port for gen12.

/**

 * intel_connector_debugfs_add - add i915 specific connector debugfs files

 * @connector: pointer to a registered drm_connector

 *

 * Cleanup will be done by drm_connector_unregister() through a call to

 * drm_debugfs_connector_remove().

 The connector must have been registered beforehands. */

/**

 * intel_crtc_debugfs_add - add i915 specific crtc debugfs files

 * @crtc: pointer to a drm_crtc

 *

 * Failure to add debugfs entries should generally be ignored.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

	/*

	 * See intel_pps_reset_all() why we need a power domain reference here.

	/* Preserve the BIOS-computed detected bit. This is

	 * supposed to be read-only.

	/*

	 * The DPLL for the pipe must be enabled for this to work.

	 * So enable temporarily it if it's not already enabled.

	/*

	 * Similar magic as in intel_dp_enable_port().

	 * We _must_ do this port enable + disable trick

	 * to make this power sequencer lock onto the port.

	 * Otherwise even VDD force bit won't work.

	/*

	 * We don't have power sequencer currently.

	 * Pick one that's not used by other ports.

 We should never land here with regular DP ports */

	/*

	 * Didn't find one. This should not happen since there

	 * are two power sequencers and up to two eDP ports.

 init power sequencer on this pipe and port */

	/*

	 * Even vdd force doesn't work until we've made

	 * the power sequencer lock in on the port.

 We should never land here with regular DP ports */

	/*

	 * Only the HW needs to be reprogrammed, the SW state is fixed and

	 * has been setup during connector init.

 try to find a pipe with this port selected */

 first pick one where the panel is on */

 didn't find one? pick one where vdd is on */

 didn't find one? pick one with just the correct port */

 didn't find one? just let vlv_power_sequencer_pipe() pick one when needed */

	/*

	 * We can't grab pps_mutex here due to deadlock with power_domain

	 * mutex when power_domain functions are called while holding pps_mutex.

	 * That also means that in order to use pps_pipe the code needs to

	 * hold both a power domain reference and pps_mutex, and the power domain

	 * reference get/put must be done while _not_ holding pps_mutex.

	 * pps_{lock,unlock}() do these steps in the correct order, so one

	 * should use them always.

 Cycle delay moved from PP_DIVISOR to PP_CONTROL */

	/* take the difference of currrent time and panel power off time

	/* When we disable the VDD override bit last we have to do the manual

/* Read the current pp_control value, unlocking the register if it

 * is locked

/*

 * Must be paired with intel_pps_vdd_off_unlocked().

 * Must hold pps_mutex around the whole on/off sequence.

 * Can be nested with intel_pps_vdd_{on,off}() calls.

	/*

	 * If the panel wasn't on, delay before accessing aux channel

/*

 * Must be paired with intel_pps_off().

 * Nested calls to these functions are not allowed since

 * we drop the lock. Caller must use some higher level

 * locking to prevent nested calls from other threads.

 Make sure sequencer is idle before allowing subsequent activity */

	/*

	 * vdd might still be enabled due to the delayed vdd off.

	 * Make sure vdd is actually turned off here.

	/*

	 * Queue the timer to fire a long time from now (relative to the power

	 * down delay) to keep the panel power up across a sequence of

	 * operations.

/*

 * Must be paired with edp_panel_vdd_on().

 * Must hold pps_mutex around the whole on/off sequence.

 * Can be nested with intel_pps_vdd_{on,off}() calls.

 ILK workaround: disable reset around power sequence */

 restore panel reset bit */

	/* We need to switch off panel power _and_ force vdd, for otherwise some

 We got a reference when we enabled the VDD. */

 Enable backlight in the panel power control. */

	/*

	 * If we enable the backlight right away following a panel power

	 * on, we may see slight flicker as the panel syncs with the eDP

	 * link.  So delay a bit to make sure the image is solid before

	 * allowing it to appear.

 Disable backlight in the panel power control. */

/*

 * Hook for controlling the panel power control backlight through the bl_power

 * sysfs attribute. Take care to handle multiple calls.

	/*

	 * VLV seems to get confused when multiple power sequencers

	 * have the same port selected (even if only one has power/vdd

	 * enabled). The failure manifests as vlv_wait_port_ready() failing

	 * CHV on the other hand doesn't seem to mind having the same port

	 * selected in multiple power sequencers, but let's clear the

	 * port select always when logically disconnecting a power sequencer

	 * from a port.

 make sure vdd is off before we steal it */

		/*

		 * If another power sequencer was being used on this

		 * port previously make sure to turn off vdd there while

		 * we still have control of it.

	/*

	 * We may be stealing the power

	 * sequencer from another port.

 now it's all ours */

 init power sequencer on this pipe and port */

	/*

	 * The VDD bit needs a power domain reference, so if the bit is

	 * already enabled when we boot or resume, grab this reference and

	 * schedule a vdd off, so we don't hold on to the reference

	 * indefinitely.

 Ensure PPS is unlocked */

 Pull timing values out of registers */

 already initialized? */

	/* On Toshiba Satellite P50-C-18C system the VBT T12 delay

	 * of 500ms appears to be too short. Ocassionally the panel

	 * just fails to power back on. Increasing the delay to 800ms

	 * seems sufficient to avoid this problem.

	/* T11_T12 delay is special and actually in units of 100ms, but zero

	 * based in the hw (so we need to add 100 ms). But the sw vbt

	 * table multiplies it with 1000 to make it in units of 100usec,

	/* Upper limits from eDP 1.3 spec. Note that we use the clunky units of

 no limit for t8, use t7 instead */

 no limit for t9, make it symmetric with t8 */

	/* This one is special and actually in units of 100ms, but zero

	 * based in the hw (so we need to add 100 ms). But the sw vbt

	 * table multiplies it with 1000 to make it in units of 100usec,

	/* Use the max of the register settings and vbt. If both are

	/*

	 * We override the HW backlight delays to 1 because we do manual waits

	 * on them. For T8, even BSpec recommends doing it. For T9, if we

	 * don't do this, we'll end up waiting for the backlight off delay

	 * twice: once when we do the manual sleep, and once when we disable

	 * the panel and wait for the PP_STATUS bit to become zero.

	/*

	 * HW has only a 100msec granularity for t11_t12 so round it up

	 * accordingly.

	/*

	 * On some VLV machines the BIOS can leave the VDD

	 * enabled even on power sequencers which aren't

	 * hooked up to any port. This would mess up the

	 * power domain tracking the first time we pick

	 * one of these power sequencers for use since

	 * intel_pps_vdd_on_unlocked() would notice that the VDD was

	 * already on and therefore wouldn't grab the power

	 * domain reference. Disable VDD first to avoid this.

	 * This also avoids spuriously turning the VDD on as

	 * soon as the new power sequencer gets initialized.

	/* Haswell doesn't have any port selection bits for the panel

	/*

	 * Compute the divisor for the pp clock, simply match the Bspec formula.

		/*

		 * Reinit the power sequencer also on the resume path, in case

		 * BIOS did something nasty with it.

	/*

	 * This w/a is needed at least on CPT/PPT, but to be sure apply it

	 * everywhere where registers can be write protected.

 presumably write lock depends on pipe, not port select */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 We build large requests to read the registers from the ring */

	/*

	 * Registers in this range are affected by the MCR selector

	 * which only controls CPU initiated MMIO. Routing does not

	 * work for CS access so we cannot verify them on this path.

 Can we read the MCR range 0xb00 directly? See intel_workarounds! */

 Read the mocs tables back using SRM */

 Compare the results against the expected tables */

 Basic check the system is configured with the expected mocs table */

 Every new context should see the same mocs table */

 Ensure the reset happens and kills the engine */

 Check the mocs setup is retained over per-engine and global resets */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 Inside suspend/resume so single threaded, no races to worry about. */

	/*

	 * It seems that the DMC likes to transition between the DC states a lot

	 * when there are no connected displays (no active power domains) during

	 * command submission.

	 *

	 * This activity has negative impact on the performance of the chip with

	 * huge latencies observed in the interrupt handler and elsewhere.

	 *

	 * Work around it by grabbing a GT IRQ power domain whilst there is any

	 * GT activity, preventing any DC state transitions.

 Everything switched off, flush any residual interrupt just in case */

 Defer dropping the display power well for 100ms, it's slow! */

	/*

	 * Enabling power-management should be "self-healing". If we cannot

	 * enable a feature, simply leave it disabled with a notice to the

	 * user.

 Use a raw wakeref to avoid calling intel_display_power_get early */

	/*

	 * As we have just resumed the machine and woken the device up from

	 * deep PCI sleep (presumably D3_cold), assume the HW has been reset

	 * back to defaults, recovering from whatever wedged state we left it

	 * in and so worth trying to use the device once more.

	/*

	 * After resume, we may need to poke into the pinned kernel

	 * contexts to paper over any damage caused by the sudden suspend.

	 * Only the kernel contexts should remain pinned over suspend,

	 * allowing us to fixup the user contexts on their first pin.

 Only when the HW is re-initialised, can we replay the requests */

 kernel context lost */

		/*

		 * Forcibly cancel outstanding work and leave

		 * the gpu quiet.

 We expect to be idle already; but also want to be independent */

	/*

	 * On disabling the device, we want to turn off HW access to memory

	 * that we no longer own.

	 *

	 * However, not all suspend-states disable the device. S0 (s2idle)

	 * is effectively runtime-suspend, the device is left powered on

	 * but needs to be put into a low power state. We need to keep

	 * powermanagement enabled, but we also retain system state and so

	 * it remains safe to keep on using our allocated memory.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

	/*

	 * read/write caches:

	 *

	 * I915_GEM_DOMAIN_RENDER is always invalidated, but is

	 * only flushed if MI_NO_WRITE_FLUSH is unset.  On 965, it is

	 * also flushed at 2d versus 3d pipeline switches.

	 *

	 * read-only caches:

	 *

	 * I915_GEM_DOMAIN_SAMPLER is flushed on pre-965 if

	 * MI_READ_FLUSH is set, and is always flushed on 965.

	 *

	 * I915_GEM_DOMAIN_COMMAND may not exist?

	 *

	 * I915_GEM_DOMAIN_INSTRUCTION, which exists on 965, is

	 * invalidated when MI_EXE_FLUSH is set.

	 *

	 * I915_GEM_DOMAIN_VERTEX, which exists on 965, is

	 * invalidated with every MI_FLUSH.

	 *

	 * TLBs:

	 *

	 * On 965, TLBs associated with I915_GEM_DOMAIN_COMMAND

	 * and I915_GEM_DOMAIN_CPU in are invalidated at PTE write and

	 * I915_GEM_DOMAIN_RENDER and I915_GEM_DOMAIN_SAMPLER

	 * are flushed at any MI_FLUSH.

	/*

	 * A random delay to let the CS invalidate take effect? Without this

	 * delay, the GPU relocation path fails as the CS does not see

	 * the updated contents. Just as important, if we apply the flushes

	 * to the EMIT_FLUSH branch (i.e. immediately after the relocation

	 * write and before the invalidate on the next batch), the relocations

	 * still fail. This implies that is a delay following invalidation

	 * that is required to reset the caches as opposed to a delay to

	 * ensure the memory is written.

 Just userspace ABI convention to limit the wa batch bo to a resonable size */

 Evict the invalid PTE TLBs */

 load each page */

		/*

		 * Blit the batch (which has now all relocs applied) to the

		 * stable batch scratch bo area (so that the CS never

		 * stumbles over its tlb invalidation bug) ...

 ... and execute it. */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 Preallocate tracking nodes */

 Nodes preallocated in intel_context_active() */

	/*

	 * And mark it as a globally pinned object to let the shrinker know

	 * it cannot reclaim the object until we release it.

	/*

	 * We always pin the context/ring/timeline here, to ensure a pin

	 * refcount for __intel_context_active(), which prevent a lock

	 * inversion of ce->pin_mutex vs dma_resv_lock().

 flush pin before it is visible */

 no overflow! */

	/*

	 * Unlock the hwsp_ggtt object since it's shared.

	 * In principle we can unlock all the global state locked above

	 * since it's pinned and doesn't need fencing, and will

	 * thus remain resident until it is explicitly unpinned.

	/*

	 * Once released, we may asynchronously drop the active reference.

	 * As that may be the only reference keeping the context alive,

	 * take an extra now so that it is not freed before we finish

	 * dereferencing it.

 everything should already be activated by intel_context_pre_pin() */

 NB ce->signal_link/lock is used under RCU */

	/*

	 * Initialize fence to be complete as this is expected to be complete

	 * unless there is a pending schedule disable outstanding.

 Need to put the creation ref for the children */

 Only suitable for use in remotely modifying this context */

 timeline sharing! */

 Queue this switch after current activity by this context. */

	/*

	 * Guarantee context image and the timeline remains pinned until the

	 * modifying request is retired by setting the ce activity tracker.

	 *

	 * But we only need to take one pin on the account of it. Or in other

	 * words transfer the pinned ce object to tracked active request.

	/*

	 * timeline->mutex should be the inner lock, but is used as outer lock.

	 * Hack around this to shut up lockdep in selftests..

	/*

	 * We search the parent list to find an active request on the submitted

	 * context. The parent list contains the requests for all the contexts

	 * in the relationship so we have to do a compare of each request's

	 * context.

	/*

	 * Callers responsibility to validate that this function is used

	 * correctly but we use GEM_BUG_ON here ensure that they do.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 XXX VLV_GT_MEDIA_RC6? */

	/*

	 * Our claim is that we can "encourage" the GPU to enter rc6 at will.

	 * Let's try it!

 bsw/byt use a PCU and decouple RC6 from our manual control */

 Force RC6 off for starters */

 wakeup is not immediate, takes about 100us on icl */

 Manually enter RC6 */

 Restore what should have been the original state! */

 A read of CTX_INFO upsets rc6. Poke the bear! */

 Use a sacrifical context */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

/*

 * Emits a PIPE_CONTROL with a non-zero post-sync operation, for

 * implementing two workarounds on gen6.  From section 1.4.7.1

 * "PIPE_CONTROL" of the Sandy Bridge PRM volume 2 part 1:

 *

 * [DevSNB-C+{W/A}] Before any depth stall flush (including those

 * produced by non-pipelined state commands), software needs to first

 * send a PIPE_CONTROL with no bits set except Post-Sync Operation !=

 * 0.

 *

 * [Dev-SNB{W/A}]: Before a PIPE_CONTROL with Write Cache Flush Enable

 * =1, a PIPE_CONTROL with any non-zero post-sync-op is required.

 *

 * And the workaround for these two requires this workaround first:

 *

 * [Dev-SNB{W/A}]: Pipe-control with CS-stall bit set must be sent

 * BEFORE the pipe-control with a post-sync op and no write-cache

 * flushes.

 *

 * And this last workaround is tricky because of the requirements on

 * that bit.  From section 1.4.7.2.3 "Stall" of the Sandy Bridge PRM

 * volume 2 part 1:

 *

 *     "1 of the following must also be set:

 *      - Render Target Cache Flush Enable ([12] of DW1)

 *      - Depth Cache Flush Enable ([0] of DW1)

 *      - Stall at Pixel Scoreboard ([1] of DW1)

 *      - Depth Stall ([13] of DW1)

 *      - Post-Sync Operation ([13] of DW1)

 *      - Notify Enable ([8] of DW1)"

 *

 * The cache flushes require the workaround flush that triggered this

 * one, so we can't use it.  Depth stall would trigger the same.

 * Post-sync nonzero is what triggered this second workaround, so we

 * can't use that one either.  Notify enable is IRQs, which aren't

 * really our business.  That leaves only stall at scoreboard.

 low dword */

 high dword */

 Force SNB workarounds for PIPE_CONTROL flushes */

	/*

	 * Just flush everything.  Experiments have shown that reducing the

	 * number of bits based on the write domains has little performance

	 * impact. And when rearranging requests, the order of flushes is

	 * unknown.

		/*

		 * Ensure that any following seqno writes only happen

		 * when the render cache is indeed flushed.

		/*

		 * TLB invalidate requires a post-sync write.

 First we do the gen6_emit_post_sync_nonzero_flush w/a */

 Finally we can flush and with it emit the breadcrumb */

	/*

	 * We always require a command barrier so that subsequent

	 * commands, such as breadcrumb interrupts, are strictly ordered

	 * wrt the contents of the write cache being flushed to memory

	 * (and thus being coherent from the CPU).

	/*

	 * Bspec vol 1c.3 - blitter engine command streamer:

	 * "If ENABLED, all TLBs will be invalidated once the flush

	 * operation is complete. This bit is only valid when the

	 * Post-Sync Operation field is a value of 1h or 3h."

	/*

	 * Ensure that any following seqno writes only happen when the render

	 * cache is indeed flushed.

	 *

	 * Workaround: 4th PIPE_CONTROL command (except the ones with only

	 * read-cache invalidate bits set) must have the CS_STALL bit set. We

	 * don't try to be clever and just set it unconditionally.

	/*

	 * CS_STALL suggests at least a post-sync write.

	/*

	 * Just flush everything.  Experiments have shown that reducing the

	 * number of bits based on the write domains has little performance

	 * impact.

		/*

		 * Workaround: we must issue a pipe_control with CS-stall bit

		 * set before a pipe_control command that has the state cache

		 * invalidate bit set.

 Flush/delay to ensure the RING_IMR is active before the GT IMR */

 Flush/delay to ensure the RING_IMR is active before the GT IMR */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 skip disabled subslice */

		/*

		 * FIXME: Valid SS Mask respects the spec and read

		 * only valid bits for those registers, excluding reserved

		 * although this seems wrong because it would leave many

		 * subslices without ACK.

 skip disabled slice */

 skip disabled subslice */

 skip disabled slice */

 skip disabled subslice */

 subtract fused off EU(s) from enabled slice(s) */

/*

 * this is called from top-level debugfs as well, so we can't get the gt from

 * the seq_file.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2008-2018 Intel Corporation

 XXX How to handle concurrent GGTT updates using tiling registers? */

 Cool contexts are too cool to be banned! (Used for reset testing.) */

 Record the timestamp for the last N hangs */

 If we have hung N+1 times in rapid succession, we ban the context! */

 protect the GEM context */

 Assert reset for at least 20 usec, and wait for acknowledgement. */

 Clear the reset request. */

 WaVcpClkGateDisableForMediaReset:ctg,elk */

 Reset the hardware domains (GENX_GRDOM_*) specified by mask */

	/*

	 * GEN6_GDRST is not in the gt power well, no need to check

	 * for fifo space for the write or forcewake the chip for

	 * the read

 Wait for the device to ack the reset requests */

		/*

		 * Wa_14010733141

		 *

		 * If the VCS-MFX isn't using the SFC, we also need to check

		 * whether VCS-HCP is using it.  If so, we need to issue a *VE*

		 * forced lock on the VE engine that shares the same SFC.

	/*

	 * If the engine is using an SFC, tell the engine that a software reset

	 * is going to happen. The engine will then try to force lock the SFC.

	 * If SFC ends up being locked to the engine we want to reset, we have

	 * to reset it as well (we will unlock it once the reset sequence is

	 * completed).

	/*

	 * Was the SFC released while we were trying to lock it?

	 *

	 * We should reset both the engine and the SFC if:

	 *  - We were locking the SFC to this engine and the lock succeeded

	 *       OR

	 *  - We were locking the SFC to a different engine (Wa_14010733141)

	 *    but the SFC was released before the lock was obtained.

	 *

	 * Otherwise we need only reset the engine by itself and we can

	 * leave the SFC alone.

	/*

	 * We unlock the SFC based on the lock status and not the result of

	 * gen11_lock_sfc to make sure that we clean properly if something

	 * wrong happened during the lock (e.g. lock acquired after timeout

	 * expiration).

	 *

	 * Due to Wa_14010733141, we may have locked an SFC to an engine that

	 * wasn't being reset.  So instead of calling gen11_unlock_sfc()

	 * on engine_mask, we instead call it on the mask of engines that our

	 * gen11_lock_sfc() calls told us actually had locks attempted.

		/*

		 * For catastrophic errors, ready-for-reset sequence

		 * needs to be bypassed: HAS#396813

 Catastrophic errors need to be cleared by HW */

		/*

		 * If this is not the first failed attempt to prepare,

		 * we decide to proceed anyway.

		 *

		 * By doing so we risk context corruption and with

		 * some gens (kbl), possible system hang if reset

		 * happens during active bb execution.

		 *

		 * We rather take context corruption instead of

		 * failed reset with a wedged driver/gpu. And

		 * active bb execution case should be covered by

		 * stop_engines() we have before the reset.

	/*

	 * If the power well sleeps during the reset, the reset

	 * request may be dropped and never completes (causing -EIO).

/*

 * Ensure irq handler finishes, and not run again.

 * Also return the active request so that we only search for it once.

	/*

	 * During the reset sequence, we must prevent the engine from

	 * entering RC6. As the context state is undefined until we restart

	 * the engine, if it does enter RC6 during the reset, the state

	 * written to the powercontext is undefined and so we may lose

	 * GPU state upon resume, i.e. fail to restart after a reset.

	/*

	 * Everything depends on having the GTT running, so we need to start

	 * there.

	/*

	 * First, stop submission to hw, but do not yet complete requests by

	 * rolling the global seqno forward (since this would complete requests

	 * for which we haven't set the fence error to EIO yet).

 Even if the GPU reset fails, it should still stop the engines */

	/*

	 * Make sure no request can slip through without getting completed by

	 * either this call here to intel_engine_write_global_seqno, or the one

	 * in nop_submit_request.

 Mark all executing requests as skipped */

 Never fully initialised, recovery impossible */

	/*

	 * Before unwedging, make sure that all pending operations

	 * are flushed and errored out - we may have requests waiting upon

	 * third party fences. We marked all inflight requests as EIO, and

	 * every execbuf since returned EIO, for consistency we want all

	 * the currently pending requests to also be marked as EIO, which

	 * is done inside our nop_submit_request - and so we must wait.

	 *

	 * No more can be submitted until we reset the wedged bit.

		/*

		 * All internal dependencies (i915_requests) will have

		 * been flushed by the set-wedge, but we may be stuck waiting

		 * for external fences. These should all be capped to 10s

		 * (I915_FENCE_TIMEOUT) so this wait should not be unbounded

		 * in the worst case.

 Restart iteration after droping lock */

 We must reset pending GPU events before restoring our submission */

 XXX better agnosticism desired */

		/*

		 * Warn CI about the unrecoverable wedged condition.

		 * Time for a reboot.

	/*

	 * Undo nop_submit_request. We prevent all new i915 requests from

	 * being queued (by disallowing execbuf whilst wedged) so having

	 * waited for all active requests above, we know the system is idle

	 * and do not have to worry about a thread being inside

	 * engine->submit_request() as we swap over. So unlike installing

	 * the nop_submit_request on reset, we can do this from normal

	 * context and do not require stop_machine().

 complete takeover before enabling execbuf */

/**

 * intel_gt_reset - reset chip after a hang

 * @gt: #intel_gt to reset

 * @stalled_mask: mask of the stalled engines with the guilty requests

 * @reason: user error message for why we are resetting

 *

 * Reset the chip.  Useful if a hang is detected. Marks the device as wedged

 * on failure.

 *

 * Procedure is fairly simple:

 *   - reset the chip using the reset reg

 *   - re-init context state

 *   - re-init hardware status page

 *   - re-init ring buffer

 *   - re-init interrupt state

 *   - re-init display

	/*

	 * FIXME: Revoking cpu mmap ptes cannot be done from a dma_fence

	 * critical section like gpu reset.

 Clear any previous failed attempts at recovery. Time to try again. */

	/*

	 * Next we need to restore the context, but we don't use those

	 * yet either...

	 *

	 * Ring buffer needs to be re-initialized in the KMS case, or if X

	 * was running at the time of the reset (i.e. we weren't VT

	 * switched away).

	/*

	 * History tells us that if we cannot reset the GPU now, we

	 * never will. This then impacts everything that is run

	 * subsequently. On failing the reset, we mark the driver

	 * as wedged, preventing further execution on the GPU.

	 * We also want to go one step further and add a taint to the

	 * kernel so that any subsequent faults can be traced back to

	 * this failure. This is important for CI, where if the

	 * GPU/driver fails we would like to reboot and restart testing

	 * rather than continue on into oblivion. For everyone else,

	 * the system should still plod along, but they have been warned!

 If we fail here, we expect to fallback to a global reset */

	/*

	 * The request that caused the hang is stuck on elsp, we know the

	 * active request and can drop it, adjust head to skip the offending

	 * request to resume executing remaining requests in the queue.

	/*

	 * The engine and its registers (and workarounds in case of render)

	 * have been reset to their default values. Follow the init_ring

	 * process to program RING_MODE, HWSP and re-enable submission.

/**

 * intel_engine_reset - reset GPU engine to recover from a hang

 * @engine: engine to reset

 * @msg: reason for GPU reset; or NULL for no drm_notice()

 *

 * Reset a specific GPU engine. Useful if a hang is detected.

 * Returns zero on successful reset or otherwise an error code.

 *

 * Procedure is:

 *  - identifies the request that caused the hang and it is dropped

 *  - reset engine (which will force the engine to idle)

 *  - re-init/configure engine

 Use a watchdog to ensure that our reset completes */

 Flush everyone using a resource about to be clobbered */

/**

 * intel_gt_handle_error - handle a gpu error

 * @gt: the intel_gt

 * @engine_mask: mask representing engines that are hung

 * @flags: control flags

 * @fmt: Error message format string

 *

 * Do some basic checking of register state at error time and

 * dump it to the syslog.  Also call i915_capture_error_state() to make

 * sure we get a record and make it available in debugfs.  Fire a uevent

 * so userspace knows something bad happened (should trigger collection

 * of a ring dump etc.).

	/*

	 * In most cases it's guaranteed that we get here with an RPM

	 * reference held, for example because there is a pending GPU

	 * request that won't finish until the reset is done. This

	 * isn't the case at least when we get here by doing a

	 * simulated reset via debugfs, so get an RPM reference.

	/*

	 * Try engine reset when available. We fall back to full reset if

	 * single reset fails.

 Full reset needs the mutex, stop any other user trying to do so. */

 piggy-back on the other reset */

 Make sure i915_reset_trylock() sees the I915_RESET_BACKOFF */

 Prevent any other reset-engine attempt. */

 Reset still in progress? Maybe we will recover? */

 Wedged on init is non-recoverable */

 cleanup any wedged requests */

	/*

	 * While undesirable to wait inside the shrinker, complain anyway.

	 *

	 * If we have to wait during shrinking, we guarantee forward progress

	 * by forcing the reset. Therefore during the reset we must not

	 * re-enter the shrinker. By declaring that we take the reset mutex

	 * within the shrinker, we forbid ourselves from performing any

	 * fs-reclaim or taking related locks during reset.

 no GPU until we are ready! */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 Use the median of both cycle/dt; close enough */

 unknown */

 Any CS_TIMESTAMP? */

		/*

		 * XXX CS_TIMESTAMP low dword is dysfunctional?

		 *

		 * Ville's experiments indicate the high dword still works,

		 * but at a correspondingly reduced frequency.

		/*

		 * XXX CS_TIMESTAMP appears gibberish

		 *

		 * Ville's experiments indicate that it mostly appears 'stuck'

		 * in that we see the register report the same cycle count

		 * for a couple of reads.

 Do several suspend/resume cycles to check we don't explode! */

		/*

		 * These tests may leave the system in an undesirable state.

		 * They are intended to be run last in CI and the system

		 * rebooted afterwards.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

		/*

		 * PRMs say:

		 *

		 *     "The value in this register increments once every 16

		 *      hclks." (through the âClocking Configurationâ

		 *      (âCLKCFGâ) MCHBAR register)

		/*

		 * PRMs say:

		 *

		 *     "The PCU TSC counts 10ns increments; this timestamp

		 *      reflects bits 38:3 of the TSC (i.e. 80ns granularity,

		 *      rolling over every 1.5 hours).

			/*

			 * Now figure out how the command stream's timestamp

			 * register increments from this frequency (it might

			 * increment only every few clock cycle).

		/*

		 * First figure out the reference frequency. There are 2 ways

		 * we can compute the frequency, either through the

		 * TIMESTAMP_OVERRIDE register or through RPM_CONFIG. CTC_MODE

		 * tells us which one we should use.

			/*

			 * Now figure out how the command stream's timestamp

			 * register increments from this frequency (it might

			 * increment only every few clock cycle).

	/*

	 * Note that on gen11+, the clock frequency may be reconfigured.

	 * We do not, and we assume nobody else does.

	/*

	 * Make these a multiple of magic 25 to avoid SNB (eg. Dell XPS

	 * 8300) freezing up around GPU hangs. Looks as if even

	 * scheduling/timer interrupts start misbehaving if the RPS

	 * EI/thresholds are "bad", leading to a very sluggish or even

	 * frozen machine.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 Write pde (index) from the page directory @pd to the page table @pt */

 Caller needs to make sure the write completes if necessary */

 may be disabled for VT-d */

 PPGTT support for Sandybdrige/Gen6 and later */

		/*

		 * Note that the hw doesn't support removing PDE on the fly

		 * (they are cached inside the context with no means to

		 * invalidate the cache), so we can only reset the PTE

		 * entries back to scratch.

 Free all no longer used page tables */

 prevent fencing */

	/*

	 * Workaround the limited maximum vma->pin_count and the aliasing_ppgtt

	 * which will be pinned into every active context.

	 * (When vma->pin_count becomes atomic, I expect we will naturally

	 * need a larger, unpacked, type and kill this redundancy.)

	/*

	 * PPGTT PDEs reside in the GGTT and consists of 512 entries. The

	 * allocator works in address space sizes, so it's multiplied by page

	 * size. We allocate at the top of the GTT to avoid fragmentation.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 Each thread pre-pins the pd, and we may have a thread per pde. */

 Applicable to VLV, and gen8+ */

 Beware later misalignment */

 Each PD holds 512 entries */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 dummy residuals */

	/*

	 * Exercise the inter-context wa batch.

	 *

	 * Between each user context we run a wa batch, and since it may

	 * have implications for user visible state, we have to check that

	 * we do actually execute it.

	 *

	 * The trick we use is to replace the normal wa batch with a custom

	 * one that writes to a marker within it, and we can then look for

	 * that marker to confirm if the batch was run when we expect it,

	 * and equally important it was wasn't run when we don't!

 MI_STORE_DWORD is privileged! */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 Trim off the trailing space and replace with a newline */

	/*

	 * When waiting for a request, if is it currently being executed

	 * on the GPU, we busywait for a short while before sleeping. The

	 * premise is that most requests are short, and if it is already

	 * executing then there is a good chance that it will complete

	 * before we can setup the interrupt handler and go to sleep.

	 * We try to offset the cost of going to sleep, by first spinning

	 * on the request -- if it completed in less time than it would take

	 * to go sleep, process the interrupt and return back to the client,

	 * then we have saved the client some latency, albeit at the cost

	 * of spinning on an expensive CPU core.

	 *

	 * While we try to avoid waiting at all for a request that is unlikely

	 * to complete, deciding how long it is worth spinning is for is an

	 * arbitrary decision: trading off power vs latency.

	/*

	 * Execlists uses a scheduling quantum (a timeslice) to alternate

	 * execution between ready-to-run contexts of equal priority. This

	 * ensures that all users (though only if they of equal importance)

	 * have the opportunity to run and prevents livelocks where contexts

	 * may have implicit ordering due to userspace semaphores.

	/*

	 * When we allow ourselves to sleep before a GPU reset after disabling

	 * submission, even for a few milliseconds, gives an innocent context

	 * the opportunity to clear the GPU before the reset occurs. However,

	 * how long to sleep depends on the typical non-preemptible duration

	 * (a similar problem to determining the ideal preempt-reset timeout

	 * or even the heartbeat interval).

	/*

	 * After initialising a preemption request, we give the current

	 * resident a small amount of time to vacate the GPU. The preemption

	 * request is for a higher priority context and should be immediate to

	 * maintain high quality of service (and avoid priority inversion).

	 * However, the preemption granularity of the GPU can be quite coarse

	 * and so we need a compromise.

	/*

	 * We monitor the health of the system via periodic heartbeat pulses.

	 * The pulses also provide the opportunity to perform garbage

	 * collection.  However, we interpret an incomplete pulse (a missed

	 * heartbeat) as an indication that the system is no longer responsive,

	 * i.e. hung, and perform an engine or full GPU reset. Given that the

	 * preemption granularity can be very coarse on a system, the optimal

	 * value for any workload is unknowable!

 xfer ownership to sysfs tree */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 Flush any previous reset before applying for a new one */

/*

 * keep the interface clean where the first parameter

 * is a 'struct intel_gt *' instead of 'void *'

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2008-2021 Intel Corporation

/* Rough estimate of the typical request size, performing a flush,

 * set-context and then emitting the batch.

	/*

	 * Keep the render interrupt unmasked as this papers over

	 * lost interrupts following a reset.

	/*

	 * The ring status page addresses are no longer next to the rest of

	 * the ring registers as of gen7.

		/*

		 * No more rings exist on Gen7. Default case is only to shut up

		 * gcc switch check warning.

 ring should be idle before issuing a sync flush*/

 Empty the ring by skipping to the end */

 The ring must be empty before it is disabled */

 Then reset the disabled ring */

	/*

	 * Double check the ring is empty & disabled before we resume. Called

	 * from atomic context during PCI probe, so _hardirq().

 Enforce ordering by reading HEAD register back */

	/*

	 * Initialize the ring. This must happen _after_ we've cleared the ring

	 * registers with the above sequence (the readback of the HEAD registers

	 * also enforces ordering), otherwise the hw might lose the new ring

	 * register values.

 Check that the ring offsets point within the ring! */

 First wake the ring up to an empty/idle ring */

 If the head is still not zero, the ring is dead */

 Now awake, let it get started */

 Papering over lost _interrupts_ immediately following the restart */

	/*

	 * Poison residual state on resume, in case the suspend didn't!

	 *

	 * We have to assume that across suspend/resume (or other loss

	 * of control) that the contents of our pinned buffers has been

	 * lost, replaced by garbage. Since this doesn't always happen,

	 * let's poison such state so that we more quickly spot when

	 * we falsely assume it has been preserved.

	/*

	 * The kernel_context HWSP is stored in the status_page. As above,

	 * that may be lost on resume/initialisation, and so we need to

	 * reset the value in the HWSP.

 And scrub the dirty cachelines for the HWSP */

	/*

	 * We stop engines, otherwise we might get failed reset and a

	 * dead gpu (on elk). Also as modern gpu as kbl can suffer

	 * from system hang if batchbuffer is progressing when

	 * the reset is issued, regardless of READY_TO_RESET ack.

	 * Thus assume it is best to stop engines on all gens

	 * where we have a gpu reset.

	 *

	 * WaKBLVECSSemaphoreWaitPoll:kbl (on ALL_ENGINES)

	 *

	 * WaMediaResetMainRingCleanup:ctg,elk (presumably)

	 * WaClearRingBufHeadRegAtInit:ctg,elk

	 *

	 * FIXME: Wa for more modern gens needs to be validated

 G45 ring initialization often fails to reset head to zero */

	/*

	 * The guilty request will get skipped on a hung engine.

	 *

	 * Users of client default contexts do not rely on logical

	 * state preserved between batches so it is safe to execute

	 * queued requests following the hang. Non default contexts

	 * rely on preserved state, so skipping a batch loses the

	 * evolution of the state and it needs to be considered corrupted.

	 * Executing more queued batches on top of corrupted state is

	 * risky. But we take the risk by trying to advance through

	 * the queued requests in order to make the client behaviour

	 * more predictable around resets, by not throwing away random

	 * amount of batches it has prepared for execution. Sophisticated

	 * clients can use gem_reset_stats_ioctl and dma fence status

	 * (exported via sync_file info ioctl on explicit fences) to observe

	 * when it loses the context state and should rebuild accordingly.

	 *

	 * The context ban, and ultimately the client ban, mechanism are safety

	 * valves if client submission ends up resulting in nothing more than

	 * subsequent hangs.

		/*

		 * Try to restore the logical GPU state to match the

		 * continuation of the request queue. If we skip the

		 * context/PD restore, then the next request may try to execute

		 * assuming that its context is valid and loaded on the GPU and

		 * so may try to access invalid memory, prompting repeated GPU

		 * hangs.

		 *

		 * If the request was guilty, we still restore the logical

		 * state in case the next request requires it (e.g. the

		 * aliasing ppgtt), but skip over the hung batch.

		 *

		 * If the request was innocent, we try to replay the request

		 * with the restored context.

 Mark all submitted requests as skipped. */

 Remaining _unready_ requests will be nop'ed when submitted */

 paranoid flush writes out of the WCB before mmio */

	/*

	 * Try to make the context utilize L3 as well as LLC.

	 *

	 * On VLV we don't have L3 controls in the PTEs so we

	 * shouldn't touch the cache level, especially as that

	 * would make the object snooped which might have a

	 * negative performance impact.

	 *

	 * Snooping is required on non-llc platforms in execlist

	 * mode, but since all GGTT accesses use PAT entry 0 we

	 * get snooping anyway regardless of cache_level.

	 *

	 * This is only applicable for Ivy Bridge devices since

	 * later platforms don't have L3 control bits in the PTE.

 One ringbuffer to rule them all */

 Stall until the page table load is complete? */

 WaProgramMiArbOnOffAroundMiSetContext:ivb,vlv,hsw,bdw,chv */

		/*

		 * This w/a is only listed for pre-production ilk a/b steppings,

		 * but is also mentioned for programming the powerctx. To be

		 * safe, just apply the workaround; we do not use SyncFlush so

		 * this should never take effect and so be a no-op!

		/*

		 * The HW doesn't handle being told to restore the current

		 * context very well. Quite often it likes goes to go off and

		 * sulk, especially when it is meant to be reloading PP_DIR.

		 * A very simple fix to force the reload is to simply switch

		 * away from the current context and back again.

		 *

		 * Note that the kernel_context will contain random state

		 * following the INHIBIT_RESTORE. We accept this since we

		 * never use the kernel_context state; it is merely a

		 * placeholder we use to flush other contexts.

	/*

	 * w/a: MI_SET_CONTEXT must always be followed by MI_NOOP

	 * WaMiSetContext_Hang:snb,ivb,vlv

 keep gcc quiet */

 Insert a delay before the next switch! */

	/*

	 * Note: We do not worry about the concurrent register cacheline hang

	 * here because no other code should access these registers other than

	 * at initialization time.

	/*

	 * Not only do we need a full barrier (post-sync write) after

	 * invalidating the TLBs, but we need to wait a little bit

	 * longer. Whether this is merely delaying us, or the

	 * subsequent flush is a key part of serialising with the

	 * post-sync op, this extra pass appears vital before a

	 * mm switch!

 Always invalidate before the next switch_mm() */

 For resource streamer on HSW+ and power context elsewhere */

	/*

	 * Now past the point of no return, this request _will_ be emitted.

	 *

	 * Or at least this preamble will be emitted, the request may be

	 * interrupted prior to submitting the user payload. If so, we

	 * still submit the "empty" request in order to preserve global

	 * state tracking such as this, our tracking of the current

	 * dirty context.

	/*

	 * Flush enough space to reduce the likelihood of waiting after

	 * we start building the request - in which case we will just

	 * have to repeat work.

 Unconditionally invalidate GPU caches and TLBs. */

 Every tail move must follow the sequence below */

	/* Disable notification that the ring is IDLE. The GT

	 * will then assume that it is busy and bring it out of rc6.

 Clear the context id. Here be magic! */

 Wait for the ring not to be idle, i.e. for it to wake up. */

 Now that the ring is fully powered up, update the tail */

	/* Let the ring send IDLE messages to the GT again,

	 * and so let it sleep to conserve power when idle.

 Prevent further __await_execution() registering a cb, then flush */

 gen8+ are only supported with execlists */

	/*

	 * Using a global execution timeline; the previous final breadcrumb is

	 * equivalent to our next initial bread so we can elide

	 * engine->emit_init_breadcrumb().

 gen6 bsd needs a special wa for tail updates */

 probe size */);

 dummy residuals */

 Finally, take ownership and responsibility for cleanup! */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 *

 * Generated by: IGT Gpu Tools on Fri 21 Feb 2020 05:29:32 AM UTC

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright Â© 2018 Intel Corporation

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2018 Intel Corporation

 safety net! */

 Currently a no-op as the reset is handled by GuC */

 Ensure the spinner hasn't aborted */

 Ensure the reset happens and kills the engine */

 timestamps are expected to autoincrement */

 Clear non priv flags */

 SRM original */

 LRI garbage */

 SRM result */

 LRI garbage */

 SRM result */

 LRM original -- don't leave garbage in the context! */

 Be nice if we hang */

 detect write masking */

 Can the user write to the whitelisted registers? */

 minimum requirement for LRI, SRM, LRM */

 If we reset the gpu, we should not lose the RING_NONPRIV */

 Clear non priv flags */

 Clear non priv flags */

 Be nice if we hang */

 Perform the writes from an unprivileged "user" batch */

 Alas, we must pardon some whitelists. Mistakes already made */

 Some registers do not seem to behave and our writes unreadable */

	/*

	 * Check that a write into a whitelist register works, but

	 * invisible to a second context.

 Read default values */

 Try to overwrite registers (should only affect ctx0) */

 Read values from ctx1, we expect these to be defaults */

 Verify that both reads return the same default values */

 Read back the updated values in ctx0 */

 User should be granted privilege to overwhite regs */

 Ensure the spinner hasn't aborted */

 Ensure the reset happens and kills the engine */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 0xEA00 - OxEFFF is unused */

	/*

	 * An mslice is unavailable only if both the meml3 for the slice is

	 * disabled *and* all of the DSS in the slice (quadrant) are disabled.

 Double layer security blanket, see i915_gem_init() */

 Apply the GT workarounds... */

 ...and determine whether they are sticking. */

	/*

	 * At least 830 can leave some of the unused rings

	 * "active" (ie. head != tail) after resume which

	 * will prevent c3 entry. Makes sure all unused rings

	 * are totally idle.

 We can't enable contexts until all firmware is loaded */

		/*

		 * some errors might have become stuck,

		 * mask them.

 From GEN8 onwards we only have one 'All Engine Fault Register' */

	/*

	 * No actual flushing is required for the GTT write domain for reads

	 * from the GTT domain. Writes to it "immediately" go to main memory

	 * as far as we know, so there's no chipset flush. It also doesn't

	 * land in the GPU render cache.

	 *

	 * However, we do have to enforce the order so that all writes through

	 * the GTT land before any writes to the device, such as updates to

	 * the GATT itself.

	 *

	 * We also have to wait a bit for the writes to land from the GTT.

	 * An uncached read (i.e. mmio) seems to be ideal for the round-trip

	 * timing. This issue has only been observed when switching quickly

	 * between GTT writes and CPU reads from inside the kernel on recent hw,

	 * and it appears to only affect discrete GTT blocks (i.e. on LLC

	 * system agents we cannot reproduce this behaviour, until Cannonlake

	 * that was!).

	/*

	 * As we reset the gpu during very early sanitisation, the current

	 * register state on the GPU should reflect its defaults values.

	 * We load a context onto the hw (with restore-inhibit), then switch

	 * over to a second context to save that default register state. We

	 * can then prime every new context with that state so they all start

	 * from the same default HW values.

 We must be able to switch to something! */

 Flush the default context image to memory, and enable powersaving. */

 Keep a copy of the state's backing pages; free the obj */

	/*

	 * If we have to abandon now, we expect the engines to be idle

	 * and ready to be torn-down. The quickest way we can accomplish

	 * this is by declaring ourselves wedged.

 Flush and restore the kernel context for safety */

 If the device is asleep, we have no requests outstanding */

	/*

	 * This is just a security blanket to placate dragons.

	 * On some systems, we very sporadically observe that the first TLBs

	 * used by the CS may be stale, despite us poking the TLB reset. If

	 * we hold the forcewake during initialisation these problems

	 * just magically go away.

	/*

	 * Upon unregistering the device to prevent any new users, cancel

	 * all in-flight requests so that we can quickly unbind the active

	 * resources.

 Scrub all HW state upon release */

 FIXME being called twice on error paths :( */

 We need to wait for inflight RCU frees to release their grip */

/**

 * intel_gt_reg_needs_read_steering - determine whether a register read

 *     requires explicit steering

 * @gt: GT structure

 * @reg: the register to check steering requirements for

 * @type: type of multicast steering to check

 *

 * Determines whether @reg needs explicit steering of a specific type for

 * reads.

 *

 * Returns false if @reg does not belong to a register range of the given

 * steering type, or if the default (subslice-based) steering IDs are suitable

 * for @type steering too.

/**

 * intel_gt_get_valid_steering - determines valid IDs for a class of MCR steering

 * @gt: GT structure

 * @type: multicast register type

 * @sliceid: Slice ID returned

 * @subsliceid: Subslice ID returned

 *

 * Determines sliceid and subsliceid values that will steer reads

 * of a specific multicast register class to a valid value.

 should be impossible! */

 unused */

 should be impossible! */

 unused */

 should be impossible! */

		/*

		 * An LNCF is always present if its mslice is present, so we

		 * can safely just steer to LNCF 0 in all cases.

 unused */

/**

 * intel_gt_read_register_fw - reads a GT register with support for multicast

 * @gt: GT structure

 * @reg: register to read

 *

 * This function will read a GT register.  If the register is a multicast

 * register, the read will be steered to a valid instance (i.e., one that

 * isn't fused off or powered down by power gating).

 *

 * Returns the value from a valid instance of @reg.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

/*

 * While the engine is active, we send a periodic pulse along the engine

 * to check on its health and to flush any idle-barriers. If that request

 * is stuck, and we fail to preempt it, we declare the engine hung and

 * issue a reset -- in the hope that restores progress.

		/*

		 * GuC itself is toast or GuC's hang detection

		 * is disabled. Either way, need to find the

		 * hang culprit manually.

 Just in case everything has gone horribly wrong, give it a kick */

 Safeguard against too-fast worker invocations */

			/*

			 * Not yet submitted, system is stalled.

			 *

			 * This more often happens for ring submission,

			 * where all contexts are funnelled into a common

			 * ringbuffer. If one context is blocked on an

			 * external fence, not only is it not submitted,

			 * but all other contexts, including the kernel

			 * context are stuck waiting for the signal.

			/*

			 * Gradually raise the priority of the heartbeat to

			 * give high priority work [which presumably desires

			 * low latency and no jitter] the chance to naturally

			 * complete before being preempted.

 Unable to lock the kernel timeline, is the engine stuck? */

 recheck current execution */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright Â© 2019 Intel Corporation

 Opencode i915_request_add() so we can keep the timeline locked. */

 Wait for all barriers to complete (remote CPU) before we check */

	/*

	 * Note that execlists also applies a redzone which it checks on

	 * context unpin when debugging. We are using the same location

	 * and same poison value so that our checks overlap. Despite the

	 * redundancy, we want to keep this little selftest so that we

	 * get coverage of any and all submission backends, and we can

	 * always extend this test to ensure we trick the HW into a

	 * compromising position wrt to the various sections that need

	 * to be written into the context state.

	 *

	 * TLDR; this overlaps with the execlists redzone.

 Force the context switch */

	/*

	 * Check that our context sizes are correct by seeing if the

	 * HW tries to write past the end of one.

		/*

		 * Hide the old default state -- we lie about the context size

		 * and get confused when the default state is smaller than

		 * expected. For our do nothing request, inheriting the

		 * active state is sufficient, we are only checking that we

		 * don't use more than we planned.

 Overlaps with the execlists redzone */

	/*

	 * We keep active contexts alive until after a subsequent context

	 * switch as the final write from the context-save will be after

	 * we retire the final request. We track when we unpin the context,

	 * under the presumption that the final pin is from the last request,

	 * and instead of immediately unpinning the context, we add a task

	 * to unpin the context from the next idle-barrier.

	 *

	 * This test makes sure that the context is kept alive until a

	 * subsequent idle-barrier (emitted when the engine wakeref hits 0

	 * with no more outstanding requests).

	 *

	 * In GuC submission mode we don't use idle barriers and we instead

	 * get a message from the GuC to signal that it is safe to unpin the

	 * context from memory.

 Context will be kept active until after an idle-barrier. */

 Now make sure our idle-barriers are flushed */

 Wait for the barrier and in the process wait for engine to park */

	/*

	 * Check that our idle barriers do not interfere with normal

	 * activity tracking. In particular, check that operating

	 * on the context image remotely (intel_context_prepare_remote_request),

	 * which inserts foreign fences into intel_context.active, does not

	 * clobber the idle-barrier.

	 *

	 * In GuC submission mode we don't use idle barriers.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 ms */

/*

 * Lock protecting IPS related data structures

		/*

		 * Our goal is to evaluate each engine independently, so we run

		 * at the lowest clocks required to sustain the heaviest

		 * workload. However, a task may be split into sequential

		 * dependent operations across a set of engines, such that

		 * the independent contributions do not account for high load,

		 * but overall the task is GPU bound. For example, consider

		 * video decode on vcs followed by colour post-processing

		 * on vecs, followed by general post-processing on rcs.

		 * Since multi-engines being active does imply a single

		 * continuous workload across all engines, we hedge our

		 * bets by only contributing a factor of the distributed

		 * load into our busyness calculation.

 We use UP_EI_EXPIRED interrupts for both up/down in manual mode */

	/*

	 * Now that we will not be generating any more work, flush any

	 * outstanding tasks. As we are called on the RPS idle path,

	 * we will reset the GPU to minimum frequencies, so the current

	 * state of the worker can be discarded.

 Set up min, max, and cur for interrupt handling */

	/*

	 * Prevent division-by-zero if we are asking too fast.

	 * Also, we don't get interesting results if we are polling

	 * faster than once in 10ms, so just return the saved value

	 * in such cases.

 FIXME: handle per-counter overflow */

 Don't divide by 0 */

 More magic constants... */

 Invert the frequency bin into an ips delay */

 still busy with another command */

 Invert the frequency bin into an ips delay */

 Disable to program */

 Program energy weights for various events */

 Program P-state weights to account for frequency power adjustment */

 Render standby states get 0 weight */

 Adjust magic regs to magic values (more experimental results) */

 Enable PMON + select events */

 Enable temp reporting */

 100ms RC evaluation intervals */

 Set max/min thresholds to 90ms and 80ms respectively */

 Set up min, max, and cur for interrupt handling */

 Ack interrupts, disable EFC interrupt */

 Go back to the starting frequency */

	/*

	 * Only set the down limit when we've reached the lowest level to avoid

	 * getting more interrupts, otherwise leave this clear. This prevents a

	 * race in the hw when coming out of rc6: There's a tiny window where

	 * the hw runs at the minimal clock before selecting the desired

	 * frequency, if the down threshold expires in that window we will not

	 * receive a down interrupt.

 in % */

 Note the units here are not exactly 1us, but 1280ns. */

	/* When byt can survive without system hang with dynamic

	 * sw freq adjustments, this restriction can be lifted.

 Max/min bins are special */

	/*

	 * Use the user's desired frequency as a guide, but for better

	 * performance, jump directly to RPe as our starting frequency.

	/*

	 * The punit delays the write of the frequency and voltage until it

	 * determines the GPU is awake. During normal usage we don't want to

	 * waste power changing the frequency if the GPU is sleeping (rc6).

	 * However, the GPU and driver is now idle and we do not want to delay

	 * switching to minimum voltage (reducing power whilst idle) as we do

	 * not expect to be woken in the near future and so must flush the

	 * change by waking the device.

	 *

	 * We choose to take the media powerwell (either would do to trick the

	 * punit into committing the voltage change) as that takes a lot less

	 * power than the render powerwell.

	/*

	 * Since we will try and restart from the previously requested

	 * frequency on unparking, treat this idle point as a downclock

	 * interrupt and reduce the frequency for resume. If we park/unpark

	 * more frequently than the rps worker can run, we will not respond

	 * to any EI and never see a change in frequency.

	 *

	 * (Note we accommodate Cherryview's limitation of only using an

	 * even bin by applying it to all.)

 CHV needs even encode values */

 Serializes with i915_request_retire() */

 debug only */

		/*

		 * Make sure we continue to get interrupts

		 * until we hit the minimum or maximum frequencies.

 All of these values are in units of 50MHz */

 static values from HW: RP0 > RP1 > RPn (min_freq) */

 hw_max = RP0 until we check for overclocking */

		/* Store the frequency values in 16.66 MHZ units, which is

		 * the natural hardware unit for SKL

 force a reset */

 See the Gen9_GT_PM_Programming_Guide doc for the below */

 Program defaults and thresholds for RPS */

 Power down if completely idle for over 50ms */

 (2 * 4) config */

 (2 * 6) config */

 (2 * 8) config */

 Setting (2 * 8) Min RP0 for any other combination */

 1: Program defaults and thresholds for RPS*/

 2: Enable RPS */

 Setting Fixed Bias */

 RPS code assumes GPLL is used */

 Clamp to max */

	/*

	 * According to the BYT Punit GPU turbo HAS 1.1.6.3 the minimum value

	 * for the minimum frequency in GPLL mode is 0xc1. Contrary to this on

	 * a BYT-M B0 the above register contains 0xbf. Moreover when setting

	 * a frequency Punit will not allow values below 0xc0. Clamp it 0xc0

	 * to make sure it matches what Punit accepts.

 WaGsvRC0ResidencyMethod:vlv */

 Setting Fixed Bias */

 RPS code assumes GPLL is used */

 Revel in the empirically derived constants */

 Correction factor in 1/100000 units */

 < 50 */

 convert to mW */

 leave disabled, no room for dynamic reclocking */;

 Ironlake currently uses intel_ips.ko */ {}

	/*

	 * N = val - 0xb7

	 * Slow = Fast = GPLL ref * N

	/*

	 * N = val / 2

	 * CU (slow) = CU2x (fast) / 2 = GPLL ref * N / 2

 CHV needs even values */

		/* Workload can be split between render + media,

		 * e.g. SwapBuffers being blitted in X after being rendered in

		 * mesa. To account for this we need to combine both engines

		 * into our activity counter.

 to usecs and scale to threshold% */

 Make sure we didn't queue anything we're not going to process. */

 CHV needs even encode values */

 CHV needs even encode values */

 unknown event */

	/*

	 * sysfs frequency limits may have snuck in while

	 * servicing the interrupt

 Handle RCS change request from hw */

 Derive initial user preferences/limits from the hardware limits */

 After setting max-softlimit, find the overclock max freq */

 OC supported */

 Finally allow us to boost to max by default */

 Start in the middle, from here we will autotune based on workload */

	/*

	 * SNB,IVB,HSW can while VLV,CHV may hard hang on looping batchbuffer

	 * if GEN6_PM_UP_EI_EXPIRED is masked.

	 *

	 * TODO: verify if this can be reproduced on VLV,CHV.

 GuC needs ARAT expired interrupt unmasked */

	/*

	 * We still need *_set_rps to process the new max_delay and

	 * update the interrupt limits and PMINTRMSK even though

	 * frequency request may be unchanged.

	/*

	 * We still need *_set_rps to process the new min_delay and

	 * update the interrupt limits and PMINTRMSK even though

	 * frequency request may be unchanged.

 External interface for intel_ips.ko */

/**

 * Tells the intel_ips driver that the i915 driver is now loaded, if

 * IPS got loaded first.

 *

 * This awkward dance is so that neither module has to depend on the

 * other in order for IPS to do the appropriate communication of

 * GPU turbo limits to i915.

	/*

	 * We only register the i915 ips part with intel-ips once everything is

	 * set up, to avoid intel-ips sneaking in and reading bogus values.

/**

 * i915_read_mch_val - return value for IPS use

 *

 * Calculate and return a value for the IPS driver to use when deciding whether

 * we have thermal and power headroom to increase CPU or GPU power budget.

/**

 * i915_gpu_raise - raise GPU frequency limit

 *

 * Raise the limit; IPS indicates we have thermal headroom.

/**

 * i915_gpu_lower - lower GPU frequency limit

 *

 * IPS indicates we're close to a thermal limit, so throttle back the GPU

 * frequency maximum.

/**

 * i915_gpu_busy - indicate GPU business to IPS

 *

 * Tell the IPS driver whether or not the GPU is busy.

/**

 * i915_gpu_turbo_disable - disable graphics turbo

 *

 * Disable graphics turbo by resetting the max frequency and setting the

 * current frequency to the default.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 all engines must agree! */

 ignore incomplete engines */

 Replace the internal name with the final user facing name */

 Fix up the mapping to match default execbuf::user_map[] */

		/*

		 * Make sure that classes with multiple engine instances all

		 * share the same basic configuration.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Generated by: intel-gpu-tools-1.19-177-g68e2eab2

 reloc */

 reloc */

 reloc */

 reloc */

 cmds end */

 state start */

 state end */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 Not exactly sure what this is */

 RPSTAT1 is in the GT power well */

			/*

			 * The equivalent to the PM ISR & IIR cannot be read

			 * without affecting the current state of the system

 Convert GT frequency to 50 HZ units */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 We want to 1:1 map the mappable aperture to our reserved region */

 Your mappable aperture belongs to me now! */

 Stolen starts from GSMBASE on DG1 */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 never remove */

 g2v_notify atomically (via hv trap) consumes the message packet. */

 Index shifts into the pagetable are offset by GEN8_PTE_SHIFT [12] */

 page and page-directory sizes are the same */

 Must be pinned! */

 All other pdes may be simultaneously removed */

 Limited by sg length for 3lvl */

		/*

		 * Is it safe to mark the 2M block as 64K? -- Either we have

		 * filled whole page-table with 64K entries, or filled part of

		 * it and have reached the end of the sg table and we have

		 * enough padding.

			/*

			 * We write all 4K page entries, even when using 64K

			 * pages. In order to verify that the HW isn't cheating

			 * by using the 4K PTE instead of the 64K PTE, we want

			 * to remove all the surplus entries. If the HW skipped

			 * the 64K PTE, it will read/write into the scratch page

			 * instead - which we detect as missing results during

			 * selftests.

	/*

	 * If everybody agrees to not to write into the scratch page,

	 * we can reuse it for all vm, keeping contexts and processes separate.

 keep pinned */

 mark as pinned */

/*

 * GEN8 legacy ppgtt programming is accomplished through a max 4 PDP registers

 * with a net effect resembling a 2-level page table in normal x86 terms. Each

 * PDP represents 1GB of memory 4 * 512 * 512 * 4096 = 4GB legacy 32b address

 * space.

 *

	/*

	 * From bdw, there is hw support for read-only pages in the PPGTT.

	 *

	 * Gen11 has HSDES#:1807136187 unresolved. Disable ro support

	 * for now.

	 *

	 * Gen12 has inherited the same read-only fault issue from gen11.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 for_each_engine() */

 And check nothing new was submitted */

 Flush the background retirement and idle barriers */

 Is the idle barrier still outstanding? */

		/*

		 * Our goal here is to retire _idle_ timelines as soon as

		 * possible (as they are idle, we do not expect userspace

		 * to be cleaning up anytime soon).

		 *

		 * If the timeline is currently locked, either it is being

		 * retired elsewhere or about to be!

	/*

	 * We open-code a llist here to include the additional tag [BIT(0)]

	 * so that we know when the timeline is already on a

	 * retirement queue: either this engine or another.

 already queued */

 We don't deal well with the engine disappearing beneath us */

 kick the ksoftirqd tasklets */

 report busy to caller, try again? */

 pin the list element */

 Retirement is best effort */

 Resume list iteration after reacquiring spinlock */

 Defer the final release to after the spinlock */

 Wait, there's more! */

 Wait until the work is marked as finished before unloading! */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 g_ss_en/c_ss_en represent entire subslice mask across all slices */

		/*

		 * XeHP introduces the concept of compute vs geometry DSS. To

		 * reduce variation between GENs around subslice usage, store a

		 * mask for both the geometry and compute enabled masks since

		 * userspace will need to be able to query these masks

		 * independently.  Also compute a total enabled subslice count

		 * for the purposes of selecting subslices to use in a

		 * particular GEM context.

	/*

	 * Gen12 has Dual-Subslices, which behave similarly to 2 gen11 SS.

	 * Instead of splitting these, provide userspace with an array

	 * of DSS to more closely represent the hardware resource.

	 *

	 * In addition, the concept of slice has been removed in Xe_HP.

	 * To be compatible with prior generations, assume a single slice

	 * across the entire device. Then calculate out the DSS for each

	 * workload type within that software slice.

	/*

	 * As mentioned above, Xe_HP does not have the concept of a slice.

	 * Enable one for software backwards compatibility.

 one bit per pair of EUs */

 TGL only supports slice-level power gating */

 ICL has no power gating restrictions. */

	/*

	 * CHV expected to always have a uniform distribution of EU

	 * across subslices.

	/*

	 * CHV supports subslice power gating on devices with more than

	 * one subslice, and supports EU power gating on devices with

	 * more than one EU pair per subslice.

 BXT has a single slice and at most 3 subslices. */

	/*

	 * The subslice disable field is global, i.e. it applies

	 * to each of the enabled slices.

	/*

	 * Iterate through enabled slices and subslices to

	 * count the total enabled EU.

 skip disabled slice */

 skip disabled subslice */

			/*

			 * Record which subslice(s) has(have) 7 EUs. we

			 * can tune the hash used to spread work among

			 * subslices if they are unbalanced.

	/*

	 * SKL is expected to always have a uniform distribution

	 * of EU across subslices with the exception that any one

	 * EU in any one subslice may be fused off for die

	 * recovery. BXT is expected to be perfectly uniform in EU

	 * distribution.

	/*

	 * SKL+ supports slice power gating on devices with more than

	 * one slice, and supports EU power gating on devices with

	 * more than one EU pair per subslice. BXT+ supports subslice

	 * power gating on devices with more than one subslice, and

	 * supports EU power gating on devices with more than one EU

	 * pair per subslice.

 s_max */

	/*

	 * The subslice disable field is global, i.e. it applies

	 * to each of the enabled slices.

	/*

	 * Iterate through enabled slices and subslices to

	 * count the total enabled EU.

 skip disabled slice */

 skip disabled subslice */

			/*

			 * Record which subslices have 7 EUs.

	/*

	 * BDW is expected to always have a uniform distribution of EU across

	 * subslices with the exception that any one EU in any one subslice may

	 * be fused off for die recovery.

	/*

	 * BDW supports slice power gating on devices with more than

	 * one slice.

	/*

	 * There isn't a register to tell us how many slices/subslices. We

	 * work off the PCI-ids here.

 No powergating for you. */

	/*

	 * No explicit RPCS request is needed to ensure full

	 * slice/subslice/EU enablement prior to Gen9.

	/*

	 * If i915/perf is active, we want a stable powergating configuration

	 * on the system. Use the configuration pinned by i915/perf.

	/*

	 * Since the SScount bitfield in GEN8_R_PWR_CLK_STATE is only three bits

	 * wide and Icelake has up to eight subslices, specfial programming is

	 * needed in order to correctly enable all subslices.

	 *

	 * According to documentation software must consider the configuration

	 * as 2x4x8 and hardware will translate this to 1x8x8.

	 *

	 * Furthemore, even though SScount is three bits, maximum documented

	 * value for it is four. From this some rules/restrictions follow:

	 *

	 * 1.

	 * If enabled subslice count is greater than four, two whole slices must

	 * be enabled instead.

	 *

	 * 2.

	 * When more than one slice is enabled, hardware ignores the subslice

	 * count altogether.

	 *

	 * From these restrictions it follows that it is not possible to enable

	 * a count of subslices between the SScount maximum of four restriction,

	 * and the maximum available number on a particular SKU. Either all

	 * subslices are enabled, or a count between one and four on the first

	 * slice.

	/*

	 * Starting in Gen9, render power gating can leave

	 * slice/subslice/EU in a partially enabled state. We

	 * must make an explicit request through RPCS for full

	 * enablement.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Generated by: intel-gpu-tools-1.8-220-g01153e7

 reloc */

 reloc */

 reloc */

 reloc */

 cmds end */

 state start */

 state end */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 *

 * Generated by: IGT Gpu Tools on Fri 21 Feb 2020 05:30:13 AM UTC

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

/**

 * DOC: RC6

 *

 * RC6 is a special power stage which allows the GPU to enter an very

 * low-voltage mode when idle, using down to 0V while at this stage.  This

 * stage is entered automatically when the GPU is idle when RC6 support is

 * enabled, and as soon as new workload arises GPU wakes up automatically as

 * well.

 *

 * There are different RC6 modes available in Intel GPU, which differentiate

 * among each other with the latency required to enter and leave RC6 and

 * voltage consumed by the GPU in different states.

 *

 * The combination of the following flags define which states GPU is allowed

 * to enter, while RC6 is the normal RC6 state, RC6p is the deep RC6, and

 * RC6pp is deepest RC6. Their support by hardware varies according to the

 * GPU, BIOS, chipset and platform. RC6 is usually the safest one and the one

 * which brings the most power savings; deeper states save more power, but

 * require higher latency to switch to and wake up.

	/*

	 * With GuCRC, these parameters are set by GuC

 2b: Program RC6 thresholds.*/

 12500 * 1280ns */

 25 * 1280ns */

 50/125ms per EI */

	/*

	 * 2c: Program Coarse Power Gating Policies.

	 *

	 * Bspec's guidance is to use 25us (really 25 * 1280ns) here. What we

	 * use instead is a more conservative estimate for the maximum time

	 * it takes us to service a CS interrupt and submit a new ELSP - that

	 * is the time which the GPU is idle waiting for the CPU to select the

	 * next request to execute. If the idle hysteresis is less than that

	 * interrupt service latency, the hardware will automatically gate

	 * the power well and we will then incur the wake up cost on top of

	 * the service latency. A similar guide from plane_state is that we

	 * do not want the enable hysteresis to less than the wakeup latency.

	 *

	 * igt/gem_exec_nop/sequential provides a rough estimate for the

	 * service latency, and puts it under 10us for Icelake, similar to

	 * Broadwell+, To be conservative, we want to factor in a context

	 * switch on top (due to ksoftirqd).

	/* 3a: Enable RC6

	 *

	 * With GuCRC, we do not enable bit 31 of RC_CTL,

	 * thus allowing GuC to control RC6 entry/exit fully instead.

	 * We will not set the HW ENABLE and EI bits

 2b: Program RC6 thresholds.*/

		/*

		 * WaRsDoubleRc6WrlWithCoarsePowerGating:skl Doubling WRL only

		 * when CPG is enabled

 12500 * 1280ns */

 25 * 1280ns */

	/*

	 * 2c: Program Coarse Power Gating Policies.

	 *

	 * Bspec's guidance is to use 25us (really 25 * 1280ns) here. What we

	 * use instead is a more conservative estimate for the maximum time

	 * it takes us to service a CS interrupt and submit a new ELSP - that

	 * is the time which the GPU is idle waiting for the CPU to select the

	 * next request to execute. If the idle hysteresis is less than that

	 * interrupt service latency, the hardware will automatically gate

	 * the power well and we will then incur the wake up cost on top of

	 * the service latency. A similar guide from plane_state is that we

	 * do not want the enable hysteresis to less than the wakeup latency.

	 *

	 * igt/gem_exec_nop/sequential provides a rough estimate for the

	 * service latency, and puts it around 10us for Broadwell (and other

	 * big core) and around 40us for Broxton (and other low power cores).

	 * [Note that for legacy ringbuffer submission, this is less than 1us!]

	 * However, the wakeup latency on Broxton is closer to 100us. To be

	 * conservative, we have to factor in a context switch on top (due

	 * to ksoftirqd).

 3a: Enable RC6 */

 37.5/125ms per EI */

	/*

	 * WaRsDisableCoarsePowerGating:skl,cnl

	 *   - Render/Media PG need to be disabled with RC6.

 2b: Program RC6 thresholds.*/

 12500 * 1280ns */

 25 * 1280ns */

 800us/1.28 for TO */

 3: Enable RC6 */

 unused */

 We don't use those on Haswell */

 Check that the pcbr address is not empty. */

 BIOS set it up already, grab the pre-alloc'd space */

	/*

	 * From the Gunit register HAS:

	 * The Gfx driver is expected to program this register and ensure

	 * proper allocation within Gfx stolen memory.  For example, this

	 * register should be programmed such than the PCBR range does not

	 * overlap with other ranges, such as the frame buffer, protected

	 * memory, or any other relevant ranges.

 2a: Program RC6 thresholds.*/

 12500 * 1280ns */

 25 * 1280ns */

 TO threshold set to 500 us (0x186 * 1.28 us) */

 Allows RC6 residency counter to work */

 3: Enable RC6 */

 Allows RC6 residency counter to work */

	/*

	 * The exact context size is not known for BXT, so assume a page size

	 * for this check.

 Take control of RC6 back from GuC */

 Disable runtime-pm until we can save the GPU state with rc6 pctx */

 Sanitize rc6, ensure it is disabled before we are ready. */

 unbalanced suspend/resume */

 rc6 is ready, runtime-pm is go! */

 Restore HW timers for automatic RC6 entry while busy */

 Turn off the HW timers and go directly to rc6 */

 deepest rc6 */

 deep rc6 */

 normal rc6 */

	/*

	 * The register accessed do not need forcewake. We borrow

	 * uncore lock to prevent concurrent access to range reg.

	/*

	 * vlv and chv residency counters are 40 bits in width.

	 * With a control bit, we can choose between upper or lower

	 * 32bit window into this counter.

	 *

	 * Although we always use the counter in high-range mode elsewhere,

	 * userspace may attempt to read the value before rc6 is initialised,

	 * before we have set the default VLV_COUNTER_CONTROL value. So always

	 * set the high bit to be safe.

	/*

	 * Everywhere else we always use VLV_COUNTER_CONTROL with the

	 * VLV_COUNT_RANGE_HIGH bit set - so it is safe to leave it set

	 * now.

	/*

	 * Store previous hw counter values for counter wrap-around handling.

	 *

	 * There are only four interesting registers and they live next to each

	 * other so we can use the relative address, compared to the smallest

	 * one as the index into driver storage.

 On VLV and CHV, residency time is in CZ units rather than 1.28us */

 833.33ns units on Gen9LP, 1.28us elsewhere. */

	/*

	 * Counter wrap handling.

	 *

	 * But relying on a sufficient frequency of queries otherwise counters

	 * can still wrap.

 RC6 delta from last sample. */

 Add delta to RC6 extended raw driver copy. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright Â© 2018 Intel Corporation

 Boost gpufreq to max [waitboost] and keep it fixed */

 for per-engine CS_TIMESTAMP */

 for per-engine CS_TIMESTAMP */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2015 Intel Corporation

 structures required */

 Defines for the tables (XXX_MOCS_0 - XXX_MOCS_63) */

 Defines for the tables (LNCFMOCS0 - LNCFMOCS31) - two entries per word */

 Helper defines */

 63-64 are reserved, but configured. */

 (e)LLC caching options */

/*

 * Note: LE_0_PAGETABLE works only up to Gen11; for newer gens it means

 * the same as LE_UC

 Target cache */

 L3 caching options */

/*

 * MOCS tables

 *

 * These are the MOCS tables that are programmed across all the rings.

 * The control value is programmed to all the rings that support the

 * MOCS registers. While the l3cc_values are only programmed to the

 * LNCFCMOCS0 - LNCFCMOCS32 registers.

 *

 * These tables are intended to be kept reasonably consistent across

 * HW platforms, and for ICL+, be identical across OSes. To achieve

 * that, for Icelake and above, list of entries is published as part

 * of bspec.

 *

 * Entries not part of the following tables are undefined as far as

 * userspace is concerned and shouldn't be relied upon.  For Gen < 12

 * they will be initialized to PTE. Gen >= 12 don't have a setting for

 * PTE and those platforms except TGL/RKL will be initialized L3 WB to

 * catch accidental use of reserved and unused mocs indexes.

 *

 * The last few entries are reserved by the hardware. For ICL+ they

 * should be initialized according to bspec and never used, for older

 * platforms they should never be written to.

 *

 * NOTE1: These tables are part of bspec and defined as part of hardware

 *       interface for ICL+. For older platforms, they are part of kernel

 *       ABI. It is expected that, for specific hardware platform, existing

 *       entries will remain constant and the table will only be updated by

 *       adding new entries, filling unused positions.

 *

 * NOTE2: For GEN >= 12 except TGL and RKL, reserved and unspecified MOCS

 *       indices have been set to L3 WB. These reserved entries should never

 *       be used, they may be changed to low performant variants with better

 *       coherency in the future if more entries are needed.

 *       For TGL/RKL, all the unspecified MOCS indexes are mapped to L3 UC.

	/*

	 * mocs:63

	 * - used by the L3 for all of its evictions.

	 *   Thus it is expected to allow LLC cacheability to enable coherent

	 *   flows to be maintained.

	 * - used to force L3 uncachable cycles.

	 *   Thus it is expected to make the surface L3 uncacheable.

 NOTE: the LE_TGT_CACHE is not used on Broxton */

 Entries 0 and 1 are defined per-platform */ \

 Base - L3 + LLC */ \

 Base - Uncached */ \

 Base - L3 */ \

 Base - LLC */ \

 Age 0 - LLC */ \

 Age 0 - L3 + LLC */ \

 Age: Don't Chg. - LLC */ \

 Age: Don't Chg. - L3 + LLC */ \

 No AOM - LLC */ \

 No AOM - L3 + LLC */ \

 No AOM; Age 0 - LLC */ \

 No AOM; Age 0 - L3 + LLC */ \

 No AOM; Age:DC - LLC */ \

 No AOM; Age:DC - L3 + LLC */ \

 Self-Snoop - L3 + LLC */ \

 Skip Caching - L3 + LLC(12.5%) */ \

 Skip Caching - L3 + LLC(25%) */ \

 Skip Caching - L3 + LLC(50%) */ \

 Skip Caching - L3 + LLC(75%) */ \

 Skip Caching - L3 + LLC(87.5%) */ \

 HW Reserved - SW program but never use */ \

 HW Reserved - SW program but never use */ \

	/*

	 * NOTE:

	 * Reserved and unspecified MOCS indices have been set to (L3 + LCC).

	 * These reserved entries should never be used, they may be changed

	 * to low performant variants with better coherency in the future if

	 * more entries are needed. We are programming index I915_MOCS_PTE(1)

	 * only, __init_mocs_table() take care to program unused index with

	 * this entry.

 Implicitly enable L1 - HDC:L1 + L3 + LLC */

 Implicitly enable L1 - HDC:L1 + L3 */

 Implicitly enable L1 - HDC:L1 + LLC */

 Implicitly enable L1 - HDC:L1 */

 HW Special Case (CCS) */

 HW Special Case (Displayable) */

 Base - Uncached (Deprecated) */

 Base - L3 + LeCC:PAT (Deprecated) */

 UC */

 WB - L3 */

 WB - L3 50% */

 WB - L3 25% */

 WB - L3 12.5% */

 HDC:L1 + L3 */

 HDC:L1 */

 HW Reserved */

 Implicitly enable L1 - HDC:L1 + L3 + LLC */

 Implicitly enable L1 - HDC:L1 + L3 */

 Implicitly enable L1 - HDC:L1 + LLC */

 Implicitly enable L1 - HDC:L1 */

 HW Special Case (CCS) */

 HW Special Case (Displayable) */

 wa_1608975824 */

 UC - Coherent; GO:L3 */

 UC - Coherent; GO:Memory */

 UC - Non-Coherent; GO:Memory */

 UC - Non-Coherent; GO:L3 */

 WB */

 HW Reserved - SW program but never use. */

 UC - Coherent; GO:L3 */

 UC - Coherent; GO:Memory */

 UC - Non-Coherent; GO:Memory */

 WB - LC */

 Wa_14011441408: Set Go to Memory for MOCS#0 */

 UC - Coherent; GO:Memory */

 UC - Non-Coherent; GO:Memory */

 WB - LC */

 For TGL/RKL, Can't be changed now for ABI reasons */

 WaDisableSkipCaching:skl,bxt,kbl,glk */

/*

 * Get control_value from MOCS entry taking into account when it's not used

 * then if unused_entries_index is non-zero then its value will be returned

 * otherwise I915_MOCS_PTE's value is returned in this case.

/*

 * Get l3cc_value from MOCS entry taking into account when it's not used

 * then if unused_entries_index is not zero then its value will be returned

 * otherwise I915_MOCS_PTE's value is returned in this case.

 Called under a blanket forcewake */

 Platforms with global MOCS do not need per-engine initialization. */

	/*

	 * LLC and eDRAM control values are not applicable to dgfx

	/*

	 * Initialize the L3CC table as part of mocs initalization to make

	 * sure the LNCFCMOCSx registers are programmed for the subsequent

	 * memory transactions including guc transactions

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 ~1ms at 8GiB/s preemption delay */

	/*

	 * We need the ability to prevent aribtration (MI_ARB_ON_OFF),

	 * the ability to write PTE using inline data (MI_STORE_DATA)

	 * and of course the ability to do the block transfer (blits).

	/*

	 * We construct a very special VM for use by all migration contexts,

	 * it is kept pinned so that it can be used at any time. As we need

	 * to pre-allocate the page directories for the migration VM, this

	 * limits us to only using a small number of prepared vma.

	 *

	 * To be able to pipeline and reschedule migration operations while

	 * avoiding unnecessary contention on the vm itself, the PTE updates

	 * are inline with the blits. All the blits use the same fixed

	 * addresses, with the backing store redirection being updated on the

	 * fly. Only 2 implicit vma are used for all migration operations.

	 *

	 * We lay the ppGTT out as:

	 *

	 *	[0, CHUNK_SZ) -> first object

	 *	[CHUNK_SZ, 2 * CHUNK_SZ) -> second object

	 *	[2 * CHUNK_SZ, 2 * CHUNK_SZ + 2 * CHUNK_SZ >> 9] -> PTE

	 *

	 * By exposing the dma addresses of the page directories themselves

	 * within the ppGTT, we are then able to rewrite the PTE prior to use.

	 * But the PTE update and subsequent migration operation must be atomic,

	 * i.e. within the same non-preemptible window so that we do not switch

	 * to another migration context that overwrites the PTE.

	 *

	 * TODO: Add support for huge LMEM PTEs

	/*

	 * Each engine instance is assigned its own chunk in the VM, so

	 * that we can run multiple instances concurrently

		/*

		 * We copy in 8MiB chunks. Each PDE covers 2MiB, so we need

		 * 4x2 page directories for source/destination.

		/*

		 * We need another page directory setup so that we can write

		 * the 8x512 PTE in each chunk.

 Now allow the GPU to rewrite the PTE via its own ppGTT */

	/*

	 * We randomly distribute contexts across the engines upon constrction,

	 * as they all share the same pinned vm, and so in order to allow

	 * multiple blits to run in parallel, we must construct each blit

	 * to use a different range of the vm for its GTT. This has to be

	 * known at construction, so we can not use the late greedy load

	 * balancing of the virtual-engine.

 Explicitly disable preemption for this request. */

 Compute the page directory offset for the target address range */

 Pack as many PTE updates as possible into a single MI command */

 as qword elements */

 dst offset */

 src offset */

 dst offset */

 src offset */

 dst offset */

 src offset */

 The PTE updates + copy must not be interrupted. */

 Arbitration is re-enabled between requests. */

 offset */

 The PTE updates + clear must not be interrupted. */

 Arbitration is re-enabled between requests. */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 Discard stale context state from across idling */

 Flush all pending HW writes before we touch the context */

 First poison the image to verify we never fully trust it */

 Scrub the context image after our loss of control */

 !IS_ENABLED(CONFIG_LOCKDEP) */

	/*

	 * We have to serialise all potential retirement paths with our

	 * submission, as we don't want to underflow either the

	 * engine->wakeref.counter or our timeline->active_count.

	 *

	 * Equally, we cannot allow a new submission to start until

	 * after we finish queueing, nor could we allow that submitter

	 * to retire us before we are ready!

 Let intel_gt_retire_requests() retire us (acquired under lock) */

 Hand the request over to HW and so engine_retire() */

 Let new submissions commence (and maybe retire this timeline) */

	/*

	 * This is execlist specific behaviour intended to ensure the GPU is

	 * idle by switching to a known 'safe' context. With GuC submission, the

	 * same idle guarantee is achieved by other means (disabling

	 * scheduling). Further, switching to a 'safe' context has no effect

	 * with GuC submission as the scheduler can just switch back again.

	 *

	 * FIXME: Move this backend scheduler specific behaviour into the

	 * scheduler backend.

 GPU is pointing to the void, as good as in the kernel context. */

 Already inside the kernel context, safe to power down. */

	/*

	 * Note, we do this without taking the timeline->mutex. We cannot

	 * as we may be called while retiring the kernel context and so

	 * already underneath the timeline->mutex. Instead we rely on the

	 * exclusive property of the __engine_park that prevents anyone

	 * else from creating a request on this engine. This also requires

	 * that the ring is empty and we avoid any waits while constructing

	 * the context, as they assume protection by the timeline->mutex.

	 * This should hold true as we can only park the engine after

	 * retiring the last request, thus all rings should be empty and

	 * all timelines idle.

	 *

	 * For unlocking, there are 2 other parties and the GPU who have a

	 * stake here.

	 *

	 * A new gpu user will be waiting on the engine-pm to start their

	 * engine_unpark. New waiters are predicated on engine->wakeref.count

	 * and so intel_wakeref_defer_park() acts like a mutex_unlock of the

	 * engine->wakeref.

	 *

	 * The other party is intel_gt_retire_requests(), which is walking the

	 * list of active timelines looking for completions. Meanwhile as soon

	 * as we call __i915_request_queue(), the GPU may complete our request.

	 * Ergo, if we put ourselves on the timelines.active_list

	 * (se intel_timeline_enter()) before we increment the

	 * engine->wakeref.count, we may see the request completion and retire

	 * it causing an underflow of the engine->wakeref.

 Context switch failed, hope for the best! Maybe reset? */

 Check again on the next retirement. */

 Install ourselves as a preemption barrier */

 engine should be idle! */

		/*

		 * Use an interrupt for precise measurement of duration,

		 * otherwise we rely on someone else retiring all the requests

		 * which may delay the signaling (i.e. we will likely wait

		 * until the background request retirement running every

		 * second or two).

 Expose ourselves to the world */

	/*

	 * If one and only one request is completed between pm events,

	 * we know that we are inside the kernel context and it is

	 * safe to power down. (We are paranoid in case that runtime

	 * suspend causes corruption to the active context image, and

	 * want to avoid that impacting userspace.)

 cleanup after wedging */

 Must be reset upon idling, or we may miss the busy wakeup. */

 While gt calls i915_vma_parked(), we have to break the lock cycle */

/**

 * intel_engine_reset_pinned_contexts - Reset the pinned contexts of

 * an engine.

 * @engine: The engine whose pinned contexts we want to reset.

 *

 * Typically the pinned context LMEM images lose or get their content

 * corrupted on suspend. This function resets their images.

 kernel context gets reset at __engine_unpark() */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2018 Intel Corporation

 synchronize with the retirement callback */

 Check that we can flush the idle barriers */

 Check that heartbeat pulses flush the idle barriers */

 Manufacture a tick */

  pretend we are not idle! */

 work is on the local cpu! */

	/*

	 * Ideally, the upper bound on min work delay would be something like

	 * 2 * 2 (worst), +1 for scheduling, +1 for slack. In practice, we

	 * are, even with system_wq_highpri, at the mercy of the CPU scheduler

	 * and may be stuck behind some slow work for many millisecond. Such

	 * as our very own display workers.

 Check that the heartbeat ticks at the desired rate. */

 Check that we can turn off heartbeat and not interrupt VIP */

	/*

	 * Park the heartbeat but without holding the PM lock as that

	 * makes the engines appear not-idle. Note that if/when unpark

	 * is called due to the PM lock being acquired later the

	 * heartbeat still won't be enabled because of the above = 0.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 Just a quick and causal check of the shmem_utils API */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014-2018 Intel Corporation

/**

 * DOC: Hardware workarounds

 *

 * This file is intended as a central place to implement most [1]_ of the

 * required workarounds for hardware to work as originally intended. They fall

 * in five basic categories depending on how/when they are applied:

 *

 * - Workarounds that touch registers that are saved/restored to/from the HW

 *   context image. The list is emitted (via Load Register Immediate commands)

 *   everytime a new context is created.

 * - GT workarounds. The list of these WAs is applied whenever these registers

 *   revert to default values (on GPU reset, suspend/resume [2]_, etc..).

 * - Display workarounds. The list is applied during display clock-gating

 *   initialization.

 * - Workarounds that whitelist a privileged register, so that UMDs can manage

 *   them directly. This is just a special case of a MMMIO workaround (as we

 *   write the list of these to/be-whitelisted registers to some special HW

 *   registers).

 * - Workaround batchbuffers, that get executed automatically by the hardware

 *   on every HW context restore.

 *

 * .. [1] Please notice that there are other WAs that, due to their nature,

 *    cannot be applied from a central place. Those are peppered around the rest

 *    of the code, as needed.

 *

 * .. [2] Technically, some registers are powercontext saved & restored, so they

 *    survive a suspend/resume. In practice, writing them again is not too

 *    costly and simplifies things. We can revisit this in the future.

 *

 * Layout

 * ~~~~~~

 *

 * Keep things in this file ordered by WA type, as per the above (context, GT,

 * display, register whitelist, batchbuffer). Then, inside each type, keep the

 * following order:

 *

 * - Infrastructure functions and macros

 * - WAs per platform in standard gen/chrono order

 * - Public functions to init or apply the given workaround type.

 Trim unused entries. */

 Either uninitialized or full. */

/*

 * WA operations on "masked register". A masked register has the upper 16 bits

 * documented as "masked" in b-spec. Its purpose is to allow writing to just a

 * portion of the register without a rmw: you simply write in the upper 16 bits

 * the mask of bits you are going to modify.

 *

 * The wa_masked_* family of functions already does the necessary operations to

 * calculate the mask based on the parameters passed, so user only has to

 * provide the lower 16 bits of that register.

 WaDisableAsyncFlipPerfMode:bdw,chv */

 WaDisablePartialInstShootdown:bdw,chv */

	/* Use Force Non-Coherent whenever executing a 3D context. This is a

	 * workaround for a possible hang in the unlikely event a TLB

	 * invalidation occurs during a PSD flush.

 WaForceEnableNonCoherent:bdw,chv */

 WaHdcDisableFetchWhenMasked:bdw,chv */

	/* From the Haswell PRM, Command Reference: Registers, CACHE_MODE_0:

	 * "The Hierarchical Z RAW Stall Optimization allows non-overlapping

	 *  polygons in the same 8x4 pixel/sample area to be processed without

	 *  stalling waiting for the earlier ones to write to Hierarchical Z

	 *  buffer."

	 *

	 * This optimization is off by default for BDW and CHV; turn it on.

 Wa4x4STCOptimizationDisable:bdw,chv */

	/*

	 * BSpec recommends 8x4 when MSAA is used,

	 * however in practice 16x4 seems fastest.

	 *

	 * Note that PS/WM thread counts depend on the WIZ hashing

	 * disable bit, which we don't touch here, but it's good

	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).

 WaDisableThreadStallDopClockGating:bdw (pre-production) */

	/* WaDisableDopClockGating:bdw

	 *

	 * Also see the related UCGTCL1 write in bdw_init_clock_gating()

	 * to disable EUTC clock gating.

 WaForceContextSaveRestoreNonCoherent:bdw */

 WaDisableFenceDestinationToSLM:bdw (pre-prod) */

 WaDisableThreadStallDopClockGating:chv */

 Improve HiZ throughput on CHV. */

		/* WaCompressedResourceSamplerPbeMediaNewHashMode:skl,kbl

		 *

		 * Must match Display Engine. See

		 * WaCompressedResourceDisplayNewHashMode.

 WaClearFlowControlGpgpuContextSave:skl,bxt,kbl,glk,cfl */

 WaDisablePartialInstShootdown:skl,bxt,kbl,glk,cfl */

 WaEnableYV12BugFixInHalfSliceChicken7:skl,bxt,kbl,glk,cfl */

 WaEnableSamplerGPGPUPreemptionSupport:skl,bxt,kbl,cfl */

 Wa4x4STCOptimizationDisable:skl,bxt,kbl,glk,cfl */

 WaDisablePartialResolveInVc:skl,bxt,kbl,cfl */

 WaCcsTlbPrefetchDisable:skl,bxt,kbl,glk,cfl */

 WaForceContextSaveRestoreNonCoherent:skl,bxt,kbl,cfl */

	/* WaForceEnableNonCoherent and WaDisableHDCInvalidation are

	 * both tied to WaForceContextSaveRestoreNonCoherent

	 * in some hsds for skl. We keep the tie for all gen9. The

	 * documentation is a bit hazy and so we want to get common behaviour,

	 * even though there is no clear evidence we would need both on kbl/bxt.

	 * This area has been source of system hangs so we play it safe

	 * and mimic the skl regardless of what bspec says.

	 *

	 * Use Force Non-Coherent whenever executing a 3D context. This

	 * is a workaround for a possible hang in the unlikely event

	 * a TLB invalidation occurs during a PSD flush.

 WaForceEnableNonCoherent:skl,bxt,kbl,cfl */

 WaDisableSamplerPowerBypassForSOPingPong:skl,bxt,kbl,cfl */

 WaDisableSTUnitPowerOptimization:skl,bxt,kbl,glk,cfl */

	/*

	 * Supporting preemption with fine-granularity requires changes in the

	 * batch buffer programming. Since we can't break old userspace, we

	 * need to set our default preemption level to safe value. Userspace is

	 * still able to use more fine-grained preemption levels, since in

	 * WaEnablePreemptionGranularityControlByUMD we're whitelisting the

	 * per-ctx register. As such, WaDisable{3D,GPGPU}MidCmdPreemption are

	 * not real HW workarounds, but merely a way to start using preemption

	 * while maintaining old contract with userspace.

 WaDisable3DMidCmdPreemption:skl,bxt,glk,cfl,[cnl] */

 WaDisableGPGPUMidCmdPreemption:skl,bxt,blk,cfl,[cnl] */

 WaClearHIZ_WM_CHICKEN3:bxt,glk */

		/*

		 * Only consider slices where one, and only one, subslice has 7

		 * EUs

		/*

		 * subslice_7eu[i] != 0 (because of the check above) and

		 * ss_max == 4 (maximum number of subslices possible per slice)

		 *

		 * ->    0 <= ss <= 3;

 Tune IZ hashing. See intel_device_info_runtime_init() */

 WaDisableThreadStallDopClockGating:bxt */

 WaToEnableHwFixForPushConstHWBug:bxt */

 WaToEnableHwFixForPushConstHWBug:kbl */

 WaDisableSbeCacheDispatchPortSharing:kbl */

 WaToEnableHwFixForPushConstHWBug:glk */

 WaToEnableHwFixForPushConstHWBug:cfl */

 WaDisableSbeCacheDispatchPortSharing:cfl */

 Wa_1406697149 (WaDisableBankHangMode:icl) */

	/* WaForceEnableNonCoherent:icl

	 * This is not the same workaround as in early Gen9 platforms, where

	 * lacking this could cause system hangs, but coherency performance

	 * overhead is high and only a few compute workloads really need it

	 * (the register is whitelisted in hardware now, so UMDs can opt in

	 * for coherency if they have a good reason).

 WaEnableFloatBlendOptimization:icl */

 write-only, so skip validation */,

 WaDisableGPGPUMidThreadPreemption:icl */

 allow headerless messages for preemptible GPGPU context */

 Wa_1604278689:icl,ehl */

 write-only register; skip validation */

 Wa_1406306137:icl,ehl */

/*

 * These settings aren't actually workarounds, but general tuning settings that

 * need to be programmed on several platforms.

	/*

	 * Although some platforms refer to it as Wa_1604555607, we need to

	 * program it even on those that don't explicitly list that

	 * workaround.

	 *

	 * Note that the programming of this register is further modified

	 * according to the FF_MODE2 guidance given by Wa_1608008084:gen12.

	 * Wa_1608008084 tells us the FF_MODE2 register will return the wrong

	 * value when read. The default value for this register is zero for all

	 * fields and there are no bit masks. So instead of doing a RMW we

	 * should just write TDS timer value. For the same reason read

	 * verification is ignored.

	/*

	 * Wa_1409142259:tgl,dg1,adl-p

	 * Wa_1409347922:tgl,dg1,adl-p

	 * Wa_1409252684:tgl,dg1,adl-p

	 * Wa_1409217633:tgl,dg1,adl-p

	 * Wa_1409207793:tgl,dg1,adl-p

	 * Wa_1409178076:tgl,dg1,adl-p

	 * Wa_1408979724:tgl,dg1,adl-p

	 * Wa_14010443199:tgl,rkl,dg1,adl-p

	 * Wa_14010698770:tgl,rkl,dg1,adl-s,adl-p

	 * Wa_1409342910:tgl,rkl,dg1,adl-s,adl-p

 WaDisableGPGPUMidThreadPreemption:gen12 */

	/*

	 * Wa_16011163337

	 *

	 * Like in gen12_ctx_gt_tuning_init(), read verification is ignored due

	 * to Wa_1608008084.

	/*

	 * Wa_14012131227:dg1

	 * Wa_1508744258:tgl,rkl,dg1,adl-s,adl-p

 Wa_1409044764 */

 Wa_22010493298 */

	/*

	 * This is a "fake" workaround defined by software to ensure we

	 * maintain reliable, backward-compatible behavior for userspace with

	 * regards to how nested MI_BATCH_BUFFER_START commands are handled.

	 *

	 * The per-context setting of MI_MODE[12] determines whether the bits

	 * of a nested MI_BATCH_BUFFER_START instruction should be interpreted

	 * in the traditional manner or whether they should instead use a new

	 * tgl+ meaning that breaks backward compatibility, but allows nesting

	 * into 3rd-level batchbuffers.  When this new capability was first

	 * added in TGL, it remained off by default unless a context

	 * intentionally opted in to the new behavior.  However Xe_HPG now

	 * flips this on by default and requires that we explicitly opt out if

	 * we don't want the new behavior.

	 *

	 * From a SW perspective, we want to maintain the backward-compatible

	 * behavior for userspace, so we'll apply a fake workaround to set it

	 * back to the legacy behavior on platforms where the hardware default

	 * is to break compatibility.  At the moment there is no Linux

	 * userspace that utilizes third-level batchbuffers, so this will avoid

	 * userspace from needing to make any changes.  using the legacy

	 * meaning is the correct thing to do.  If/when we have userspace

	 * consumers that want to utilize third-level batch nesting, we can

	 * provide a context parameter to allow them to opt-in.

	/*

	 * Some blitter commands do not have a field for MOCS, those

	 * commands will use MOCS index pointed by BLIT_CCTL.

	 * BLIT_CCTL registers are needed to be programmed to un-cached.

/*

 * gen12_ctx_gt_fake_wa_init() aren't programmingan official workaround

 * defined by the hardware team, but it programming general context registers.

 * Adding those context register programming in context workaround

 * allow us to use the wa framework for proper application and validation.

 Applies to all engines */

	/*

	 * Fake workarounds are not the actual workaround but

	 * programming of context registers using workaround framework.

 WaDisable_RenderCache_OperationalFlush:gen4,ilk */

 WaDisableRenderCachePipelinedFlush:g4x,ilk */

 Apply the WaDisableRHWOOptimizationForRenderHang:ivb workaround. */

 WaApplyL3ControlAndL3ChickenMode:ivb */

 WaForceL3Serialization:ivb */

 WaForceL3Serialization:vlv */

	/*

	 * WaIncreaseL3CreditsForVLVB0:vlv

	 * This is the hardware default actually.

 L3 caching of data atomics doesn't work -- disable it. */

 XXX does this reg exist? */, true);

 WaVSRefCountFullforceMissDisable:hsw */

 WaDisableKillLogic:bxt,skl,kbl */

		/* WaCompressedResourceSamplerPbeMediaNewHashMode:skl,kbl

		 *

		 * Must match Display Engine. See

		 * WaCompressedResourceDisplayNewHashMode.

 WaDisableHDCInvalidation:skl,bxt,kbl,cfl */

 WaDisableGafsUnitClkGating:skl */

 WaInPlaceDecompressionHang:skl */

 WaDisableDynamicCreditSharing:kbl */

 WaDisableGafsUnitClkGating:kbl */

 WaInPlaceDecompressionHang:kbl */

 WaDisableGafsUnitClkGating:cfl */

 WaInPlaceDecompressionHang:cfl */

	/*

	 * Although a platform may have subslices, we need to always steer

	 * reads to the lowest instance that isn't fused off.  When Render

	 * Power Gating is enabled, grabbing forcewake will only power up a

	 * single subslice (the "minconfig") if there isn't a real workload

	 * that needs to be run; this means that if we steer register reads to

	 * one of the higher subslices, we run the risk of reading back 0's or

	 * random garbage.

	/*

	 * If the subslice we picked above also steers us to a valid L3 bank,

	 * then we can just rely on the default steering and won't need to

	 * worry about explicitly re-steering L3BANK reads later.

	/*

	 * On Xe_HP the steering increases in complexity. There are now several

	 * more units that require steering and we're not guaranteed to be able

	 * to find a common setting for all of them. These are:

	 * - GSLICE (fusable)

	 * - DSS (sub-unit within gslice; fusable)

	 * - L3 Bank (fusable)

	 * - MSLICE (fusable)

	 * - LNCF (sub-unit within mslice; always present if mslice is present)

	 *

	 * We'll do our default/implicit steering based on GSLICE (in the

	 * sliceid field) and DSS (in the subsliceid field).  If we can

	 * find overlap between the valid MSLICE and/or LNCF values with

	 * a suitable GSLICE, then we can just re-use the default value and

	 * skip and explicit steering at runtime.

	 *

	 * We only need to look for overlap between GSLICE/MSLICE/LNCF to find

	 * a valid sliceid value.  DSS steering is the only type of steering

	 * that utilizes the 'subsliceid' bits.

	 *

	 * Also note that, even though the steering domain is called "GSlice"

	 * and it is encoded in the register using the gslice format, the spec

	 * says that the combined (geometry | compute) fuse should be used to

	 * select the steering.

 Find the potential gslice candidates */

	/*

	 * Find the potential LNCF candidates.  Either LNCF within a valid

	 * mslice is fine.

	/*

	 * Are there any sliceid values that work for both GSLICE and LNCF

	 * steering?

 How about sliceid values that also work for MSLICE steering? */

	/*

	 * SQIDI ranges are special because they use different steering

	 * registers than everything else we work with.  On XeHP SDV and

	 * DG2-G10, any value in the steering registers will work fine since

	 * all instances are present, but DG2-G11 only has SQIDI instances at

	 * ID's 2 and 3, so we need to steer to one of those.  For simplicity

	 * we'll just steer to a hardcoded "2" since that value will work

	 * everywhere.

 WaModifyGamTlbPartitioning:icl */

	/* Wa_1405766107:icl

	 * Formerly known as WaCL2SFHalfMaxAlloc

	/* Wa_220166154:icl

	 * Formerly known as WaDisCtxReload

	/* Wa_1406463099:icl

	 * Formerly known as WaGamTlbPendError

 Wa_1607087056:icl,ehl,jsl */

	/*

	 * This is not a documented workaround, but rather an optimization

	 * to reduce sampler power.

/*

 * Though there are per-engine instances of these registers,

 * they retain their value through engine resets and should

 * only be provided on the GT workaround list rather than

 * the engine-specific workaround list.

 Wa_14011060649:tgl,rkl,dg1,adl-s,adl-p */

 Wa_14011059788:tgl,rkl,adl-s,dg1,adl-p */

 Wa_1409420604:tgl */

 Wa_1607087056:tgl also know as BUG:1409180338 */

 Wa_1408615072:tgl[a0] */

 Wa_1607087056:dg1 */

 Wa_1409420604:dg1 */

 Wa_1408615072:dg1 */

 Empirical testing shows this register is unaffected by engine reset. */

 open-coded rmw due to steering */

 Check only valid flag bits are set */

 NB: Only 3 out of 4 enum values are valid for access field */

 WaVFEStateAfterPipeControlwithMediaStateClear:skl,bxt,glk,cfl */

 WaEnablePreemptionGranularityControlByUMD:skl,bxt,kbl,cfl,[cnl] */

 WaAllowUMDToModifyHDCChicken1:skl,bxt,kbl,glk,cfl */

 WaSendPushConstantsFromMMIO:skl,bxt */

 WaDisableLSQCROPERFforOCL:skl */

 WaDisableLSQCROPERFforOCL:kbl */

 WA #0862: Userspace has to set "Barrier Mode" to avoid hangs. */

	/*

	 * WaAllowPMDepthAndInvocationCountAccessFromUMD:cfl,whl,cml,aml

	 *

	 * This covers 4 register which are next to one another :

	 *   - PS_INVOCATION_COUNT

	 *   - PS_INVOCATION_COUNT_UDW

	 *   - PS_DEPTH_COUNT

	 *   - PS_DEPTH_COUNT_UDW

 WaAllowUMDToModifyHalfSliceChicken7:icl */

 WaAllowUMDToModifySamplerMode:icl */

 WaEnableStateCacheRedirectToCS:icl */

		/*

		 * WaAllowPMDepthAndInvocationCountAccessFromUMD:icl

		 *

		 * This covers 4 register which are next to one another :

		 *   - PS_INVOCATION_COUNT

		 *   - PS_INVOCATION_COUNT_UDW

		 *   - PS_DEPTH_COUNT

		 *   - PS_DEPTH_COUNT_UDW

 hucStatusRegOffset */

 hucUKernelHdrInfoRegOffset */

 hucStatus2RegOffset */

		/*

		 * WaAllowPMDepthAndInvocationCountAccessFromUMD:tgl

		 * Wa_1408556865:tgl

		 *

		 * This covers 4 registers which are next to one another :

		 *   - PS_INVOCATION_COUNT

		 *   - PS_INVOCATION_COUNT_UDW

		 *   - PS_DEPTH_COUNT

		 *   - PS_DEPTH_COUNT_UDW

 Wa_1808121037:tgl */

 Wa_1806527549:tgl */

 GEN:BUG:1409280441:dg1 */

 And clear the rest just in case of garbage */

/*

 * engine_fake_wa_init(), a place holder to program the registers

 * which are not part of an official workaround defined by the

 * hardware team.

 * Adding programming of those register inside workaround will

 * allow utilizing wa framework to proper application and verification.

	/*

	 * RING_CMD_CCTL are need to be programed to un-cached

	 * for memory writes and reads outputted by Command

	 * Streamers on Gen12 onward platforms.

		/*

		 * Wa_1607138336:tgl[a0],dg1[a0]

		 * Wa_1607063988:tgl[a0],dg1[a0]

		/*

		 * Wa_1606679103:tgl

		 * (see also Wa_1606682166:icl)

 Wa_1606931601:tgl,rkl,dg1,adl-s,adl-p */

		/*

		 * Wa_1407928979:tgl A*

		 * Wa_18011464164:tgl[B0+],dg1[B0+]

		 * Wa_22010931296:tgl[B0+],dg1[B0+]

		 * Wa_14010919138:rkl,dg1,adl-s,adl-p

		/*

		 * Wa_1606700617:tgl,dg1,adl-p

		 * Wa_22010271021:tgl,rkl,dg1,adl-s,adl-p

		 * Wa_14010826681:tgl,dg1,rkl,adl-p

 Wa_1409804808:tgl,rkl,dg1[a0],adl-s,adl-p */

		/*

		 * Wa_1409085225:tgl

		 * Wa_14010229206:tgl,rkl,dg1[a0],adl-s,adl-p

		/*

		 * Wa_1607030317:tgl

		 * Wa_1607186500:tgl

		 * Wa_1607297627:tgl,rkl,dg1[a0]

		 *

		 * On TGL and RKL there are multiple entries for this WA in the

		 * BSpec; some indicate this is an A0-only WA, others indicate

		 * it applies to all steppings so we trust the "all steppings."

		 * For DG1 this only applies to A0.

 Wa_1406941453:tgl,rkl,dg1,adl-s,adl-p */

 This is not an Wa. Enable for better image quality */

		/*

		 * Wa_1405543622:icl

		 * Formerly known as WaGAPZPriorityScheme

		/*

		 * Wa_1604223664:icl

		 * Formerly known as WaL3BankAddressHashing

		/*

		 * Wa_1405733216:icl

		 * Formerly known as WaDisableCleanEvicts

 Wa_1606682166:icl */

 Wa_1409178092:icl */

 WaEnable32PlaneMode:icl */

		/*

		 * Wa_1408615072:icl,ehl  (vsunit)

		 * Wa_1407596294:icl,ehl  (hsunit)

 Wa_1407352427:icl,ehl */

 Wa_1406680159:icl,ehl */

		/*

		 * Wa_1408767742:icl[a2..forever],ehl[all]

		 * Wa_1605460711:icl[a0..c0]

 Wa_22010271021 */

 FtrPerCtxtPreemptionGranularityControl:skl,bxt,kbl,cfl,cnl,icl,tgl */

 WaEnableGapsTsvCreditFix:skl,kbl,cfl */

 WaDisablePooledEuLoadBalancingFix:bxt */

 WaContextSwitchWithConcurrentTLBInvalidate:skl,bxt,kbl,glk,cfl */

 WaEnableLbsSlaRetryTimerDecrement:skl,bxt,kbl,glk,cfl */

 WaProgramL3SqcReg1DefaultForPerf:bxt,glk */

 WaOCLCoherentLineFlush:skl,bxt,kbl,cfl */

 Disable atomics in L3 to prevent unrecoverable hangs */

 WaSampleCChickenBitEnable:hsw */

 enable HiZ Raw Stall Optimization */

 WaDisableEarlyCull:vlv */

		/*

		 * WaVSThreadDispatchOverride:ivb,vlv

		 *

		 * This actually overrides the dispatch

		 * mode for all thread types.

 WaPsdDispatchEnable:vlv */

 WaDisablePSDDualDispatchEnable:vlv */

 WaDisableEarlyCull:ivb */

 causes HiZ corruption on ivb:gt1 */

 enable HiZ Raw Stall Optimization */

		/*

		 * WaVSThreadDispatchOverride:ivb,vlv

		 *

		 * This actually overrides the dispatch

		 * mode for all thread types.

 WaDisablePSDDualDispatchEnable:ivb */

 WaBCSVCSTlbInvalidationMode:ivb,vlv,hsw */

 WaDisable_RenderCache_OperationalFlush:ivb,vlv,hsw */

		/*

		 * BSpec says this must be set, even though

		 * WaDisable4x2SubspanOptimization:ivb,hsw

		 * WaDisable4x2SubspanOptimization isn't listed for VLV.

		/*

		 * BSpec recommends 8x4 when MSAA is used,

		 * however in practice 16x4 seems fastest.

		 *

		 * Note that PS/WM thread counts depend on the WIZ hashing

		 * disable bit, which we don't touch here, but it's good

		 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).

		/*

		 * We need to disable the AsyncFlip performance optimisations in

		 * order to use MI_WAIT_FOR_EVENT within the CS. It should

		 * already be programmed to '1' on all products.

		 *

		 * WaDisableAsyncFlipPerfMode:snb,ivb,hsw,vlv

		/*

		 * Required for the hardware to program scanline values for

		 * waiting

		 * WaEnableFlushTlbInvalidationMode:snb

 WaDisableHiZPlanesWhenMSAAEnabled:snb */

 WaStripsFansDisableFastClipPerformanceFix:snb */

			     /*

			      * Bspec says:

			      * "This bit must be set if 3DSTATE_CLIP clip mode is set

			      * to normal and 3DSTATE_SF number of SF output attributes

			      * is more than 16."

		/*

		 * BSpec recommends 8x4 when MSAA is used,

		 * however in practice 16x4 seems fastest.

		 *

		 * Note that PS/WM thread counts depend on the WIZ hashing

		 * disable bit, which we don't touch here, but it's good

		 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).

 WaDisable_RenderCache_OperationalFlush:snb */

		/*

		 * From the Sandybridge PRM, volume 1 part 3, page 24:

		 * "If this bit is set, STCunit will have LRA as replacement

		 *  policy. [...] This bit must be reset. LRA replacement

		 *  policy is not supported."

 WaTimedSingleVertexDispatch:cl,bw,ctg,elk,ilk,snb */

 XXX bit doesn't stick on Broadwater */

		/*

		 * Disable CONSTANT_BUFFER before it is loaded from the context

		 * image. For as it is loaded, it is executed and the stored

		 * address may no longer be valid, leading to a GPU hang.

		 *

		 * This imposes the requirement that userspace reload their

		 * CONSTANT_BUFFER on every batch, fortunately a requirement

		 * they are already accustomed to from before contexts were

		 * enabled.

 XXX bit doesn't stick on Broadwater */,

 WaKBLVECSSemaphoreWaitPoll:kbl */

	/*

	 * Registers in these ranges are affected by the MCR selector

	 * which only controls CPU initiated MMIO. Routing does not

	 * work for CS access so we cannot verify them on this path.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2016 Intel Corporation

/* Haswell does have the CXT_SIZE register however it does not appear to be

 * valid. Now, docs explain in dwords what is in the context object. The full

 * size is 70720 bytes, however, the power context and execlist context will

 * never be saved (power context is stored elsewhere, and execlists don't work

 * on HSW) - so the final size, including the extra state required for the

 * Resource Streamer, is 66944 bytes, which rounds to 17 pages.

 mmio bases table *must* be sorted in reverse graphics_ver order */

/**

 * intel_engine_context_size() - return the size of the context for an engine

 * @gt: the gt

 * @class: engine class

 *

 * Each engine class may require a different amount of space for a context

 * image.

 *

 * Return: size (in bytes) of an engine class specific context image

 *

 * Note: this size includes the HWSP, which is part of the context image

 * in LRC mode, but does not include the "shared data page" used with

 * GuC submission. The caller should account for this if using the GuC.

			/*

			 * There is a discrepancy here between the size reported

			 * by the register and the size of the context layout

			 * in the docs. Both are described as authorative!

			 *

			 * The discrepancy is on the order of a few cachelines,

			 * but the total is under one page (4k), which is our

			 * minimum allocation anyway so it should all come

			 * out in the wash.

 For the special day when i810 gets merged. */

	/*

	 * Before we know what the uABI name for this engine will be,

	 * we still would like to keep track of this engine in the debug logs.

	 * We throw in a ' here as a reminder that this isn't its final name.

	/*

	 * Though they added more rings on g4x/ilk, they did not add

	 * per-engine HWSTAM until gen6.

 Mask off all writes into the unknown HWSP */

 Override to uninterruptible for OpenCL workloads. */

 never to change again */

 Scrub mmio state on takeover */

		/*

		 * HEVC support is present on first engine instance

		 * before Gen11 and on all instances afterwards.

		/*

		 * SFC block is present only on even logical engine

		 * instances.

/**

 * intel_engines_release() - free the resources allocated for Command Streamers

 * @gt: pointer to struct intel_gt

	/*

	 * Before we release the resources held by engine, we must be certain

	 * that the HW is no longer accessing them -- having the GPU scribble

	 * to or read from a page being used for something else causes no end

	 * of fun.

	 *

	 * The GPU should be reset by this point, but assume the worst just

	 * in case we aborted before completely initialising the engines.

 Decouple the backend; but keep the layout for late GPU resets */

 Free the requests! dma-resv keeps fences around for an eternity */

	/*

	 * In Gen11, only even numbered logical VDBOXes are hooked

	 * up to an SFC (Scaler & Format Converter) unit.

	 * In Gen12, Even numbered physical instance always are connected

	 * to an SFC. Odd numbered physical instances have SFC only if

	 * previous even instance is fused off.

	 *

	 * Starting with Xe_HP, there's also a dedicated SFC_ENABLE field

	 * in the fuse register that tells us whether a specific SFC is present.

/*

 * Determine which engines are fused off in our particular hardware.

 * Note that we have a catch-22 situation where we need to be able to access

 * the blitter forcewake domain to read the engine fuses, but at the same time

 * we need to know which engines are available on the system to know which

 * forcewake domains are present. We solve this by intializing the forcewake

 * domains based on the full engine mask in the platform capabilities before

 * calling this function and pruning the domains for fused-off engines

 * afterwards.

	/*

	 * On newer platforms the fusing register is called 'enable' and has

	 * enable semantics, while on older platforms it is called 'disable'

	 * and bits have disable semantices.

/**

 * intel_engines_init_mmio() - allocate and prepare the Engine Command Streamers

 * @gt: pointer to struct intel_gt

 *

 * Return: non-zero if the initialization failed.

	/*

	 * Catch failures to update intel_engines table when the new engines

	 * are added to the driver by a warning and disabling the forgotten

	 * engines.

 Prevent writes into HWSP after returning the page to the system */

		/*

		 * On g33, we cannot place HWS above 256MiB, so

		 * restrict its pinning to the low mappable arena.

		 * Though this restriction is not documented for

		 * gen4, gen5, or byt, they also behave similarly

		 * and hang if the HWS is placed at the top of the

		 * GTT. To generalise, it appears that all !llc

		 * platforms have issues with us placing the HWS

		 * above the mappable region (even though we never

		 * actually map it).

	/*

	 * Though the HWS register does support 36bit addresses, historically

	 * we have had hangs and corruption reported due to wild writes if

	 * the HWS is placed above 4G. We only allow objects to be allocated

	 * in GFP_DMA32 for i965, and no earlier physical address users had

	 * access to more than 4G.

 Use the whole device by default */

 RING_TAIL must be qword aligned */

 perma-pin so it is always available */

	/*

	 * Give our perma-pinned kernel timelines a separate lockdep class,

	 * so that we can use them from within the normal user timelines

	 * should we need to inject GPU operations during their request

	 * construction.

/**

 * intel_engines_init_common - initialize cengine state which might require hw access

 * @engine: Engine to initialize.

 *

 * Initializes @engine@ structure members shared between legacy and execlists

 * submission modes which do require hardware access.

 *

 * Typcally done at later stages of submission mode specific engine setup.

 *

 * Returns zero on success or an error code on failure.

	/*

	 * We may need to do things with the shrinker which

	 * require us to immediately switch back to the default

	 * context. This can cause a problem as pinning the

	 * default context also requires GTT space which may not

	 * be available. To avoid this we always pin the default

	 * context.

/**

 * intel_engines_cleanup_common - cleans up the engine state created by

 *                                the common initiailizers.

 * @engine: Engine to cleanup.

 *

 * This cleans up everything created by the common helpers.

/**

 * intel_engine_resume - re-initializes the HW state of the engine

 * @engine: Engine to resume.

 *

 * Returns zero on success or an error code on failure.

 inside atomic preempt-reset? */

	/*

	 * If we are doing a normal GPU reset, we can take our time and allow

	 * the engine to quiesce. We've stopped submission to the engine, and

	 * if we wait long enough an innocent context should complete and

	 * leave the engine idle. So they should not be caught unaware by

	 * the forthcoming GPU reset (which usually follows the stop_cs)!

 A final mmio read to let GPU writes be hopefully flushed to memory */

		/*

		 * Sometimes we observe that the idle flag is not

		 * set even though the ring is empty. So double

		 * check before giving up.

 NB: please notice the memset */

 HACK: Using the wrong struct member */

 First check that no commands are left in the ring */

 No bit for gen2, so assume the CS parser is idle */

 Must wait for any GPU reset in progress. */

 Synchronise and wait for the tasklet on another CPU */

/**

 * intel_engine_is_idle() - Report if the engine has finished process all work

 * @engine: the intel_engine_cs

 *

 * Return true if there are no requests pending, nothing left to be submitted

 * to hardware, and that the engine is idle.

 More white lies, if wedged, hw state is inconsistent */

 Waiting to drain ELSP? */

 ELSP is empty, but there are ready requests? E.g. after reset */

 Ring stopped? */

	/*

	 * If the driver is wedged, HW state may be very inconsistent and

	 * report that it is still busy, even though we have stopped using it.

 Already parked (and passed an idleness test); must still be idle */

 Caller disables interrupts */

 Caller disables interrupts */

 uses physical not virtual addresses */

 maybe only uses physical not virtual addresses */

 who knows! */

 b0rked */

	/*

	 * Even though we are holding the engine->sched_engine->lock here, there

	 * is no control over the submission queue per-se and we are

	 * inspecting the active state at a random point in time, with an

	 * unknown queue. Play safe and make sure the timeline remains valid.

	 * (Only being used for pretty printing, one extra kref shouldn't

	 * cause a camel stampede!)

 nothing to print yet */

	/*

	 * No need for an engine->irq_seqno_barrier() before the seqno reads.

	 * The GPU is still running so requests are still executing and any

	 * hardware reads will be out of date by the time they are reported.

	 * But the intention here is just to report an instantaneous snapshot

	 * so that's fine.

	/*

	 * If the engine is executing something at the moment

	 * add it to the total.

/**

 * intel_engine_get_busy_time() - Return current accumulated engine busyness

 * @engine: engine to report on

 * @now: monotonic timestamp of sampling

 *

 * Returns accumulated time @engine was busy since engine stats were enabled.

	/*

	 * This search does not work in GuC submission mode. However, the GuC

	 * will report the hanging context directly to the driver itself. So

	 * the driver should never get here when in GuC mode.

	/*

	 * We are called by the error capture, reset and to dump engine

	 * state at random points in time. In particular, note that neither is

	 * crucially ordered with an interrupt. After a hang, the GPU is dead

	 * and we assume that no more writes can happen (we waited long enough

	 * for all writes that were in transaction to be flushed) - adding an

	 * extra delay for a recent interrupt is pointless. Hence, we do

	 * not need an engine->irq_seqno_barrier() before the seqno reads.

	 * At all other times, we must assume the GPU is still running, but

	 * we only care about the snapshot of this moment.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2018 Intel Corporation

 each GPR is 2 dwords */

 Ignore our own attempts to suppress excess tasklets */

 that was quick! */

 Wait until the HW has acknowleged the submission (or err) */

 Give the request a jiffie to complete after flushing the worker */

	/*

	 * Check that we can correctly context switch between 2 instances

	 * on the same engine from the same parent context.

			/*

			 * Setup the pair of contexts such that if we

			 * lite-restore using the RING_TAIL from ce[1] it

			 * will execute garbage from ce[0]->ring.

 IPEHR: 0x5a5a5a5a [hung!] */

			/*

			 * Ensure we do the switch to ce[1] on completion.

			 *

			 * rq[0] is already submitted, so this should reduce

			 * to a no-op (a wait on a request on the same engine

			 * uses the submit fence, not the completion fence),

			 * but it will install a dependency on rq[1] for rq[0]

			 * that will prevent the pair being reordered by

			 * timeslicing.

 Alternatively preempt the spinner with ce[1] */

 And switch back to ce[0] for good measure */

	/*

	 * Setup a preemption event that will cause almost the entire ring

	 * to be unwound, potentially fooling our intel_ring_direction()

	 * into emitting a forward lite-restore instead of the rollback.

 trigger a hang if executed */

 Create max prio spinner, followed by N low prio nops */

 Fill the ring, until we will cause a wrap */

 Create a second ring to preempt the first ring after rq[0] */

	/*

	 * We have to be careful not to trust intel_ring too much, for example

	 * ring->head is updated upon retire which is out of sync with pinning

	 * the context. Thus we cannot use ring->head to set CTX_RING_HEAD,

	 * or else we risk writing an older, stale value.

	 *

	 * To simulate this, let's apply a bit of deliberate sabotague.

 Keep the context awake while we play games */

 Poison the ring, and offset the next request from HEAD */

 Submit a simple nop request */

 e.g. async retire */

 Expect not to hang! */

	/*

	 * In order to support offline error capture for fast preempt reset,

	 * we need to decouple the guilty request and ensure that it and its

	 * descendents are not executed while the capture is in progress.

 We have our request executing, now remove it and reset */

 Check that we do not resubmit the held request */

 But is resubmitted on release */

 sentinel */

	/*

	 * We hook up the CS_MASTER_ERROR_INTERRUPT to have forewarning

	 * of invalid commands in user batches that will cause a GPU hang.

	 * This is a faster mechanism than using hangcheck/heartbeats, but

	 * only detects problems the HW knows about -- it will not warn when

	 * we kill the HW!

	 *

	 * To verify our detection and reset, we throw some invalid commands

	 * at the HW and wait for the interrupt.

 Kick the tasklet to process the error */

 kick tasklet */

	/*

	 * If a request takes too long, we would like to give other users

	 * a fair go on the GPU. In particular, users may create batches

	 * that wait upon external input, where that input may even be

	 * supplied by another GPU job. To avoid blocking forever, we

	 * need to preempt the current task and replace it with another

	 * ready task.

	/*

	 * The usual presumption on timeslice expiration is that we replace

	 * the active context with another. However, given a chain of

	 * dependencies we may end up with replacing the context with itself,

	 * but only a few of those requests, forcing us to rewind the

	 * RING_TAIL of the original request.

		/*

		 * A:rq1 -- semaphore wait, timestamp X

		 * A:rq2 -- write timestamp Y

		 *

		 * B:rq1 [await A:rq1] -- write timestamp Z

		 *

		 * Force timeslice, release semaphore.

		 *

		 * Expect execution/evaluation order XZY

 ELSP[] = { { A:rq1, A:rq2 }, { B:rq1 } } */

 semaphore yield! */

 Wait for the timeslice to kick in */

 -> ELSP[] = { { A:rq1 }, { B:rq1 } } */

 Release the hounds! */

 "pairs" with GPU; paranoid kick of internal CPU$ */

 XZY: XZ < XY */

 Enough time for a timeslice to kick in, and kick out */

 Enough time for the nop request to complete */

	/*

	 * Make sure that even if ELSP[0] and ELSP[1] are filled with

	 * timeslicing between them disabled, we *do* enable timeslicing

	 * if the queue demands it. (Normally, we do not submit if

	 * ELSP[1] is already occupied, so must rely on timeslicing to

	 * eject ELSP[0] in favour of the queue.)

 ELSP[0]: semaphore wait */

 ELSP[1]: nop request */

 Queue: semaphore signal, matching priority as semaphore */

 Wait until we ack the release_queue and start timeslicing */

 Timeslice every jiffy, so within 2 we should signal */

	/*

	 * We should not timeslice into a request that is marked with

	 * I915_REQUEST_NOPREEMPT.

 Create an unpreemptible spinner */

 Followed by a maximum priority barrier (heartbeat) */

		/*

		 * Wait until the barrier is in ELSP, and we know timeslicing

		 * will have been activated.

		/*

		 * Since the ELSP[0] request is unpreemptible, it should not

		 * allow the maximum priority barrier through. Wait long

		 * enough to see if it is timesliced in by mistake.

	/*

	 * Verify that even without HAS_LOGICAL_RING_PREEMPTION, we can

	 * preempt the busywaits used to synchronise between rings.

		/*

		 * We create two requests. The low priority request

		 * busywaits on a semaphore (inside the ringbuffer where

		 * is should be preemptible) and the high priority requests

		 * uses a MI_STORE_DWORD_IMM to update the semaphore value

		 * allowing the first request to complete. If preemption

		 * fails, we hang instead.

 XXX Do we need a flush + invalidate here? */

 Low priority request should be busywaiting now */

 Make sure ctx_lo stays before ctx_hi until we trigger preemption. */

	/*

	 * Verify that we can disable preemption for an individual request

	 * that may be being observed and not want to be interrupted.

 Low priority client, but unpreemptable! */

 B is much more important than A! (But A is unpreemptable.) */

 Wait long enough for preemption and timeslicing */

 Preempt cancel of ELSP0 */

 Preempt cancel of ELSP1 */

 no preemption */

 Full ELSP and one in the wings */

 Preempt cancel non-preemptible spinner in ELSP0 */

 preemption disabled */

 force reset */

 preemption disabled */

 force preempt reset [failure] */

 after failure, require heartbeats to reset device */

	/*

	 * To cancel an inflight context, we need to first remove it from the

	 * GPU. That sounds like preemption! Plus a little bit of bookkeeping.

	/*

	 * Verify that if a preemption request does not cause a change in

	 * the current execution order, the preempt-to-idle injection is

	 * skipped and that we do not accidentally apply it after the CS

	 * completion event.

 presume black blox */

 GVT forces single port & request submission */

 Keep postponing the timer to avoid premature slicing */

	/*

	 * Build a chain AB...BA between two contexts (A, B) and request

	 * preemption of the last request. It should then complete before

	 * the previously submitted spinner in B.

 Semaphore target: spin until zero */

 Terminate the spinner in the next lower priority batch. */

 trigger a hang if executed */

 Fill the ring, until we will cause a wrap */

 Create a second request to preempt the first ring */

	/*

	 * Check that we rollback large chunks of a ring in order to do a

	 * preemption event. Similar to live_unlite_ring, but looking at

	 * ring size rather than the impact of intel_ring_direction().

	/*

	 * Build as long a chain of preempters as we can, with each

	 * request higher priority than the last. Once we are ready, we release

	 * the last batch which then precolates down the chain, each releasing

	 * the next oldest in turn. The intent is to simply push as hard as we

	 * can with the number of preemptions, trying to exceed narrow HW

	 * limits. At a minimum, we insist that we can sort all the user

	 * high priority levels into execution order.

 Submit each spinner at increasing priority */

		/*

		 * Such that the last spinner is the highest priority and

		 * should execute first. When that spinner completes,

		 * it will terminate the next lowest spinner until there

		 * are no more spinners and the gang is complete.

 wait for each rq from highest to lowest prio */

 All GPR are clear for new contexts. We use GPR(0) as a constant */

		/*

		 * Perform: GPR[i]++

		 *

		 * As we read and write into the context saved GPR[i], if

		 * we restart this batch buffer from an earlier point, we

		 * will repeat the increment and store a value > 1.

	/*

	 * In our other tests, we look at preemption in carefully

	 * controlled conditions in the ringbuffer. Since most of the

	 * time is spent in user batches, most of our preemptions naturally

	 * occur there. We want to verify that when we preempt inside a batch

	 * we continue on from the current instruction and do not roll back

	 * to the start, or another earlier arbitration point.

	 *

	 * To verify this, we create a batch which is a mixture of

	 * MI_MATH (gpr++) MI_SRM (gpr) and preemption points. Then with

	 * a few preempting contexts thrown into the mix, we look for any

	 * repeated instructions (which show up as incorrect values).

 we need per-context GPR */

 Continuously preempt the set of 3 running contexts */

 Flush the semaphores on error */

	/*

	 * Check that we force preemption to occur by cancelling the previous

	 * context if it refuses to yield the GPU.

 preemption disabled */

 Flush the previous CS ack before changing timeouts */

 in ms, -> 1 jiffie */

 start all threads before we kthread_stop() */

	/*

	 * Check that by setting the execution mask on a request, we can

	 * restrict it to our desired engine within the virtual engine.

 Reverse order as it's more likely to be unnatural */

	/*

	 * Virtual requests must take part in timeslicing on the target engines.

	/*

	 * Virtual requests must allow others a fair timeslice.

 XXX We do not handle oversubscription and fairness with normal rq */

 Restrict this request to run on a particular engine */

	/*

	 * Check that the context image retains non-privileged (user) registers

	 * from one engine to the next. For this we check that the CS_GPR

	 * are preserved.

 As we use CS_GPR we cannot run before they existed on all engines. */

	/*

	 * In order to support offline error capture for fast preempt reset,

	 * we need to decouple the guilty request and ensure that it and its

	 * descendents are not executed while the capture is in progress.

 Take ownership of the reset and tasklet */

 Fake a preemption event; failed of course */

 Reset the engine while keeping our active request on hold */

 Release our grasp on the engine, letting CS flow again */

 Check that we do not resubmit the held request */

 But is resubmitted on release */

	/*

	 * Check that we handle a reset event within a virtual engine.

	 * Only the physical engine is reset, but we have to check the flow

	 * of the virtual requests around the reset, and make sure it is not

	 * forgotten.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

		/*

		 * Default to measured freq if none found, PCU will ensure we

		 * don't go over

 convert DDR frequency from units of 266.6MHz to bandwidth */

 Convert GT frequency to 50 HZ units */

		/*

		 * ring_freq = 2 * GT. ring_freq is in 100MHz units

		 * No floor required for ring frequency on SKL.

 max(2 * GT, DDR). NB: GT is 50MHz units */

 leave ia_freq as the default, chosen by cpufreq */

		/*

		 * On older processors, there is no separate ring

		 * clock domain, so in order to boost the bandwidth

		 * of the ring, we need to upclock the CPU (ia_freq).

		 *

		 * For GPU frequencies less than 750MHz,

		 * just use the lowest ring freq.

	/*

	 * For each potential GPU frequency, load a ring frequency we'd like

	 * to use for memory access.  We do this by specifying the IA frequency

	 * the PCU should use as a reference to determine the ring frequency.

 Currently there is no HW configuration to be done to disable. */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 pm is in upper half */

 pm is in upper half */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 Try to isolate the impact of cstates from determing frequency response */

 -1 to disable pm_qos, 0 to disable cstates */

 Unroll the loop to avoid MI_BB_START stalls impacting measurements */

 The PCU does not change instantly, but drifts towards the goal? */

 Target acquired */

 Any change within the last N samples? */

 Set the evaluation interval to infinity! */

 Just skip the test; assume lack of HW support */

 Use the median of both cycle/dt; close enough */

 once is enough */

 skipped, don't report a fail */

	/*

	 * Check that the actual frequency matches our requested frequency,

	 * to verify our control mechanism. We have to be careful that the

	 * PCU may throttle the GPU in which case the actual frequency used

	 * will be lowered than requested.

 XXX fragile PCU */

 Convert GT frequency to 50 HZ units */

 A simple triangle filter for better result stability */

 A simple triangle filter for better result stability */

	/*

	 * The premise is that the GPU does change frequency at our behest.

	 * Let's check there is a correspondence between the requested

	 * frequency, the actual frequency, and the observed clock rate.

 for CS simplicity */

 may skip ahead [pcu granularity] */

 ignore error, continue on with test */

	/*

	 * The premise is that the GPU does change frequency at our behest.

	 * Let's check there is a correspondence between the requested

	 * frequency, the actual frequency, and the observed clock rate.

 for CS simplicity */

 may skip ahead [pcu granularity] */

 ignore error, continue on with test */

 Flush any previous EI */

 Reset the interrupt status */

 And then wait for the timeout, for real this time */

	/*

	 * First, let's check whether or not we are receiving interrupts.

 Keep the engine busy with a spinner; expect an UP! */

 Keep the engine awake but idle and check for DOWN */

 A simple triangle filter for better result stability */

	/*

	 * Our fundamental assumption is that running at lower frequency

	 * actually saves power. Let's see if our RAPL measurement support

	 * that theory.

	/*

	 * We've looked at the bascs, and have established that we

	 * can change the clock frequency and that the HW will generate

	 * interrupts based on load. Now we check how we integrate those

	 * moving parts into dynamic reclocking based on load.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014 Intel Corporation

		/*

		 * On GEN9: before VF_CACHE_INVALIDATE we need to emit a NULL

		 * pipe control.

 WaForGAMHang:kbl */

	/*

	 * We always require a command barrier so that subsequent

	 * commands, such as breadcrumb interrupts, are strictly ordered

	 * wrt the contents of the write cache being flushed to memory

	 * (and thus being coherent from the CPU).

 upper addr */

 value */

 Wa_1409600907:tgl,adl-p */

		/*

		 * Prevent the pre-parser from skipping past the TLB

		 * invalidate and loading a stale page for the batch

		 * buffer / request payload.

 hsdes: 1809175790 */

	/*

	 * We always require a command barrier so that subsequent

	 * commands, such as breadcrumb interrupts, are strictly ordered

	 * wrt the contents of the write cache being flushed to memory

	 * (and thus being coherent from the CPU).

 upper addr */

 value */

 hsdes: 1809175790 */

 Before the request is executed, the timeline is fixed */

 See the comment in i915_request_active_seqno(). */

	/*

	 * Check if we have been preempted before we even get started.

	 *

	 * After this point i915_request_started() reports true, even if

	 * we get preempted and so are no longer running.

	 *

	 * i915_request_started() is used during preemption processing

	 * to decide if the request is currently inside the user payload

	 * or spinning on a kernel semaphore (or earlier). For no-preemption

	 * requests, we do allow preemption on the semaphore before the user

	 * payload, but do not allow preemption once the request is started.

	 *

	 * i915_request_started() is similarly used during GPU hangs to

	 * determine if the user's payload was guilty, and if so, the

	 * request is banned. Before the request is started, it is assumed

	 * to be unharmed and an innocent victim of another's hang.

 Record the updated position of the request's payload */

	/*

	 * WaDisableCtxRestoreArbitration:bdw,chv

	 *

	 * We don't need to perform MI_ARB_ENABLE as often as we do (in

	 * particular all the gen that do not need the w/a at all!), if we

	 * took care to make sure that on every switch into this context

	 * (both ordinary and for preemption) that arbitrartion was enabled

	 * we would be fine.  However, for gen8 there is another w/a that

	 * requires us to not preempt inside GPGPU execution, so we keep

	 * arbitration disabled for gen8 batches. Arbitration will be

	 * re-enabled before we close the request

	 * (engine->emit_fini_breadcrumb).

 FIXME(BDW+): Address space and security selectors. */

 Can we unwind this request without appearing to go forwards? */

/*

 * Reserve space for 2 NOOPs at the end of each request to be

 * used as a workaround for not being allowed to do lite

 * restore with HEAD==TAIL (WaIdleLiteRestore).

 Ensure there's always at least one preemption point per-request. */

 Check that entire request is less than half the ring */

 trigger IDLE->ACTIVE first */

 XXX flush+write+CS_STALL all in one upsets gem_concurrent_blt:kbl */

/*

 * Note that the CS instruction pre-parser will not stall on the breadcrumb

 * flush and will continue pre-fetching the instructions after it before the

 * memory sync is completed. On pre-gen12 HW, the pre-parser will stop at

 * BB_START/END instructions, so, even though we might pre-fetch the pre-amble

 * of the next request before the memory has been flushed, we're guaranteed that

 * we won't access the batch itself too early.

 * However, on gen12+ the parser can pre-fetch across the BB_START/END commands,

 * so, if the current request is modifying an instruction in the next request on

 * the same intel_context, we might pre-fetch and then execute the pre-update

 * instruction. To avoid this, the users of self-modifying code should either

 * disable the parser around the code emitting the memory writes, via a new flag

 * added to MI_ARB_CHECK, or emit the writes from a different intel_context. For

 * the in-kernel use-cases we've opted to use a separate context, see

 * reloc_gpu() as an example.

 * All the above applies only to the instructions themselves. Non-inline data

 * used by the instructions is not pre-fetched.

 trigger IDLE->ACTIVE first */

 XXX Stalling flush before seqno write; post-sync not */

 Wa_1409600907:tgl */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Generated by: intel-gpu-tools-1.8-220-g01153e7

 reloc */

 reloc */

 reloc */

 reloc */

 cmds end */

 state start */

 state end */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

	/*

	 * If the context is not idle, we have to submit an ordered request to

	 * modify its context image via the kernel context (writing to our own

	 * image, or into the registers directory, does not stick). Pristine

	 * and idle contexts will be configured on pinning.

 Serialise with the remote context */

 Nothing to do if unmodified. */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2015-2021 Intel Corporation

	/*

	 * Since we are waiting on a request, the GPU should be busy

	 * and should have its own rpm reference.

	/*

	 * The breadcrumb irq will be disarmed on the interrupt after the

	 * waiters are signaled. This gives us a single interrupt window in

	 * which we can add a new waiter and avoid the cost of re-enabling

	 * the irq.

 Requests may have completed before we could enable the interrupt. */

	/*

	 * Keep the irq armed until the interrupt after all listeners are gone.

	 *

	 * Enabling/disabling the interrupt is rather costly, roughly a couple

	 * of hundred microseconds. If we are proactive and enable/disable

	 * the interrupt around every request that wants a breadcrumb, we

	 * quickly drown in the extra orders of magnitude of latency imposed

	 * on request submission.

	 *

	 * So we try to be lazy, and keep the interrupts enabled until no

	 * more listeners appear within a breadcrumb interrupt interval (that

	 * is until a request completes that no one cares about). The

	 * observation is that listeners come in batches, and will often

	 * listen to a bunch of requests in succession. Though note on icl+,

	 * interrupts are always enabled due to concerns with rc6 being

	 * dysfunctional with per-engine interrupt masking.

	 *

	 * We also try to avoid raising too many interrupts, as they may

	 * be generated by userspace batches and it is unfortunately rather

	 * too easy to drown the CPU under a flood of GPU interrupts. Thus

	 * whenever no one appears to be listening, we turn off the interrupts.

	 * Fewer interrupts should conserve power -- at the very least, fewer

	 * interrupt draw less ire from other users of the system and tools

	 * like powertop.

			/*

			 * Queue for execution after dropping the signaling

			 * spinlock as the callback chain may end up adding

			 * more signalers to the same context or engine.

 We own signal_node now, xfer to local list */

 Kick the work once more to drain the signalers, and disarm the irq */

	/*

	 * If the request is already completed, we can transfer it

	 * straight onto a signaled list, and queue the irq worker for

	 * its signal completion.

		/*

		 * We keep the seqno in retirement order, so we can break

		 * inside intel_engine_signal_breadcrumbs as soon as we've

		 * passed the last completed request (or seen a request that

		 * hasn't event started). We could walk the timeline->requests,

		 * but keeping a separate signalers_list has the advantage of

		 * hopefully being much smaller than the full list and so

		 * provides faster iteration and detection when there are no

		 * more interrupts required for this context.

		 *

		 * We typically expect to add new signalers in order, so we

		 * start looking for our insertion point from the tail of

		 * the list.

	/*

	 * Defer enabling the interrupt to after HW submission and recheck

	 * the request as it may have completed and raised the interrupt as

	 * we were attaching it into the lists.

 Serialises with i915_request_retire() using rq->lock */

	/*

	 * Peek at i915_request_submit()/i915_request_unsubmit() status.

	 *

	 * If the request is not yet active (and not signaled), we will

	 * attach the breadcrumb later.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014 Intel Corporation

/**

 * DOC: Logical Rings, Logical Ring Contexts and Execlists

 *

 * Motivation:

 * GEN8 brings an expansion of the HW contexts: "Logical Ring Contexts".

 * These expanded contexts enable a number of new abilities, especially

 * "Execlists" (also implemented in this file).

 *

 * One of the main differences with the legacy HW contexts is that logical

 * ring contexts incorporate many more things to the context's state, like

 * PDPs or ringbuffer control registers:

 *

 * The reason why PDPs are included in the context is straightforward: as

 * PPGTTs (per-process GTTs) are actually per-context, having the PDPs

 * contained there mean you don't need to do a ppgtt->switch_mm yourself,

 * instead, the GPU will do it for you on the context switch.

 *

 * But, what about the ringbuffer control registers (head, tail, etc..)?

 * shouldn't we just need a set of those per engine command streamer? This is

 * where the name "Logical Rings" starts to make sense: by virtualizing the

 * rings, the engine cs shifts to a new "ring buffer" with every context

 * switch. When you want to submit a workload to the GPU you: A) choose your

 * context, B) find its appropriate virtualized ring, C) write commands to it

 * and then, finally, D) tell the GPU to switch to that context.

 *

 * Instead of the legacy MI_SET_CONTEXT, the way you tell the GPU to switch

 * to a contexts is via a context execution list, ergo "Execlists".

 *

 * LRC implementation:

 * Regarding the creation of contexts, we have:

 *

 * - One global default context.

 * - One local default context for each opened fd.

 * - One local extra context for each context create ioctl call.

 *

 * Now that ringbuffers belong per-context (and not per-engine, like before)

 * and that contexts are uniquely tied to a given engine (and not reusable,

 * like before) we need:

 *

 * - One ringbuffer per-engine inside each context.

 * - One backing object per-engine inside each context.

 *

 * The global default context starts its life with these new objects fully

 * allocated and populated. The local default context for each opened fd is

 * more complex, because we don't know at creation time which engine is going

 * to use them. To handle this, we have implemented a deferred creation of LR

 * contexts:

 *

 * The local context starts its life as a hollow or blank holder, that only

 * gets populated for a given engine once we receive an execbuffer. If later

 * on we receive another execbuffer ioctl for the same context but a different

 * engine, we allocate/populate a new ringbuffer and context backing object and

 * so on.

 *

 * Finally, regarding local contexts created using the ioctl call: as they are

 * only allowed with the render ring, we can allocate & populate them right

 * away (no need to defer anything, at least for now).

 *

 * Execlists implementation:

 * Execlists are the new method by which, on gen8+ hardware, workloads are

 * submitted for execution (as opposed to the legacy, ringbuffer-based, method).

 * This method works as follows:

 *

 * When a request is committed, its commands (the BB start and any leading or

 * trailing commands, like the seqno breadcrumbs) are placed in the ringbuffer

 * for the appropriate context. The tail pointer in the hardware context is not

 * updated at this time, but instead, kept by the driver in the ringbuffer

 * structure. A structure representing this request is added to a request queue

 * for the appropriate engine: this structure contains a copy of the context's

 * tail after the request was written to the ring buffer and a pointer to the

 * context itself.

 *

 * If the engine's request queue was empty before the request was added, the

 * queue is processed immediately. Otherwise the queue will be processed during

 * a context switch interrupt. In any case, elements on the queue will get sent

 * (in pairs) to the GPU's ExecLists Submit Port (ELSP, for short) with a

 * globally unique 20-bits submission ID.

 *

 * When execution of a request completes, the GPU updates the context status

 * buffer with a context complete event and generates a context switch interrupt.

 * During the interrupt handling, the driver examines the events in the buffer:

 * for each context complete event, if the announced ID matches that on the head

 * of the request queue, then that request is retired and removed from the queue.

 *

 * After processing, if any requests were retired and the queue is not empty

 * then a new execution list can be submitted. The two requests at the front of

 * the queue are next to be submitted but since a context may not occur twice in

 * an execution list, if subsequent requests have the same ID as the first then

 * the two requests must be combined. This is done simply by discarding requests

 * at the head of the queue until either only one requests is left (in which case

 * we use a NULL second context) or the first two requests have unique IDs.

 *

 * By always executing the first two requests in the queue the driver ensures

 * that the GPU is kept as busy as possible. In the case where a single context

 * completes but a second context is still executing, the request for this second

 * context will be at the head of the queue when we remove the first one. This

 * request will then be resubmitted along with a new request for a different context,

 * which will cause the hardware to continue executing the second request and queue

 * the new request (the GPU detects the condition of a context getting preempted

 * with the same context and optimizes the context switch flow by not doing

 * preemption, but just sampling the new tail pointer).

 *

 lower csb dword */

 upper csb dword */

 upper csb dword */

 Typical size of the average request (2 pipecontrols and a MI_BB) */

 bytes */

	/*

	 * We allow only a single request through the virtual engine at a time

	 * (each request in the timeline waits for the completion fence of

	 * the previous before being submitted). By restricting ourselves to

	 * only submitting a single request, each request is placed on to a

	 * physical to maximise load spreading (by virtue of the late greedy

	 * scheduling -- each real engine takes the next available request

	 * upon idling).

	/*

	 * We keep a rbtree of available virtual engines inside each physical

	 * engine, sorted by priority. Here we preallocate the nodes we need

	 * for the virtual engine, indexed by physical_engine->id.

 And finally, which physical engines this virtual engine maps onto. */

	/*

	 * We inspect HWS_PREEMPT with a semaphore inside

	 * engine->emit_fini_breadcrumb. If the dword is true,

	 * the ring is paused as the semaphore will busywait

	 * until the dword is false.

	/*

	 * If this request is special and must not be interrupted at any

	 * cost, so be it. Note we are only checking the most recent request

	 * in the context and so may be masking an earlier vip request. It

	 * is hoped that under the conditions where nopreempt is used, this

	 * will not matter (i.e. all requests to that context will be

	 * nopreempt for as long as desired).

	/*

	 * Check if the current priority hint merits a preemption attempt.

	 *

	 * We record the highest value priority we saw during rescheduling

	 * prior to this dequeue, therefore we know that if it is strictly

	 * less than the current tail of ESLP[0], we do not need to force

	 * a preempt-to-idle cycle.

	 *

	 * However, the priority hint is a mere hint that we may need to

	 * preempt. If that hint is stale or we may be trying to preempt

	 * ourselves, ignore the request.

	 *

	 * More naturally we would write

	 *      prio >= max(0, last);

	 * except that we wish to prevent triggering preemption at the same

	 * priority level: the task that is running should remain running

	 * to preserve FIFO ordering of dependencies.

	/*

	 * Check against the first request in ELSP[1], it will, thanks to the

	 * power of PI, be the highest priority of that context.

	/*

	 * If the inflight context did not trigger the preemption, then maybe

	 * it was the set of queued requests? Pick the highest priority in

	 * the queue (the first active priolist) and see if it deserves to be

	 * running instead of ELSP[0].

	 *

	 * The highest priority request in the queue can not be either

	 * ELSP[0] or ELSP[1] as, thanks again to PI, if it was the same

	 * context, it's priority would not exceed ELSP[0] aka last_prio.

	/*

	 * Without preemption, the prev may refer to the still active element

	 * which we refuse to let go.

	 *

	 * Even with preemption, there are times when we think it is better not

	 * to preempt and leave an ostensibly lower priority request in flight.

 Check in case we rollback so far we wrap [size/2] */

	/*

	 * Only used when GVT-g is enabled now. When GVT-g is disabled,

	 * The compiler should eliminate this function as dead-code.

	/*

	 * The executing context has been cancelled. We want to prevent

	 * further execution along this context and propagate the error on

	 * to anything depending on its results.

	 *

	 * In __i915_request_submit(), we apply the -EIO and remove the

	 * requests' payloads for any banned requests. But first, we must

	 * rewind the context back to the start of the incomplete request so

	 * that we do not jump back into the middle of the batch.

	 *

	 * We preserve the breadcrumbs and semaphores of the incomplete

	 * requests so that inter-timeline dependencies (i.e other timelines)

	 * remain correctly ordered. And we defer to __i915_request_submit()

	 * so that all asynchronous waits are correctly handled.

 On resubmission of the active request, payload will be scrubbed */

 Scrub the context image to prevent replaying the previous batch */

 We've switched away, so this should be a no-op, but intent matters */

 Use a fixed tag for OA and friends */

 We don't need a strict matching tag, just different values */

 We don't need a strict matching tag, just different values */

	/*

	 * After this point, the rq may be transferred to a new sibling, so

	 * before we clear ce->inflight make sure that the context has been

	 * removed from the b->signalers and furthermore we need to make sure

	 * that the concurrent iterator in signal_irq_work is no longer

	 * following ce->signal_link.

	/*

	 * This engine is now too busy to run this virtual request, so

	 * see if we can find an alternative engine for it to execute on.

	 * Once a request has become bonded to this engine, we treat it the

	 * same as other native request.

	/*

	 * NB process_csb() is not under the engine->sched_engine->lock and hence

	 * schedule_out can race with schedule_in meaning that we should

	 * refrain from doing non-trivial work here.

	/*

	 * If we have just completed this context, the engine may now be

	 * idle and we want to re-enter powersaving.

	/*

	 * If this is part of a virtual engine, its next request may

	 * have been blocked waiting for access to the active context.

	 * We have to kick all the siblings again in case we need to

	 * switch (e.g. the next request is not runnable on this

	 * engine). Hopefully, we will already have submitted the next

	 * request before the tasklet runs and do not need to rebuild

	 * each virtual tree and kick everyone again.

	/*

	 * WaIdleLiteRestore:bdw,skl

	 *

	 * We should never submit the context with the same RING_TAIL twice

	 * just in case we submit an empty ring, which confuses the HW.

	 *

	 * We append a couple of NOOPs (gen8_emit_wa_tail) after the end of

	 * the normal request to be able to always advance the RING_TAIL on

	 * subsequent resubmissions (for lite restore). Should that fail us,

	 * and we try and submit the same tail again, force the context

	 * reload.

	 *

	 * If we need to return to a preempted context, we need to skip the

	 * lite-restore and force it to reload the RING_TAIL. Otherwise, the

	 * HW has a tendency to ignore us rewinding the TAIL to the end of

	 * an earlier request.

	/*

	 * Make sure the context image is complete before we submit it to HW.

	 *

	 * Ostensibly, writes (including the WCB) should be flushed prior to

	 * an uncached write such as our mmio register access, the empirical

	 * evidence (esp. on Braswell) suggests that the WC write into memory

	 * may not be visible to the HW prior to the completion of the UC

	 * register write and that we may begin execution from the context

	 * before its image is complete leading to invalid PD chasing.

 We may be messing around with the lists during reset, lalala */

		/*

		 * Sentinels are supposed to be the last request so they flush

		 * the current execution off the HW. Check that they are the only

		 * request in the pending submission.

		 *

		 * NB: Due to the async nature of preempt-to-busy and request

		 * cancellation we need to handle the case where request

		 * becomes a sentinel in parallel to CSB processing.

		/*

		 * We want virtual requests to only be in the first slot so

		 * that they are never stuck behind a hog and can be immediately

		 * transferred onto the next idle engine.

 Hold tightly onto the lock to prevent concurrent retires! */

	/*

	 * We can skip acquiring intel_runtime_pm_get() here as it was taken

	 * on our behalf by the request (see i915_gem_mark_busy()) and it will

	 * not be relinquished until the device is idle (see

	 * i915_gem_idle_work_handler()). As a precaution, we make sure

	 * that all ELSP are drained i.e. we have processed the CSB,

	 * before allowing ourselves to idle and calling intel_runtime_pm_put().

	/*

	 * ELSQ note: the submit queue is not cleared after being submitted

	 * to the HW so we need to make sure we always clean it up. This is

	 * currently ensured by the fact that we always write the same number

	 * of elsq entries, keep this in mind before changing the loop below.

 we need to manually load the submit queue */

	/*

	 * We do not submit known completed requests. Therefore if the next

	 * request is already completed, we can pretend to merge it in

	 * with the previous context (and we will skip updating the ELSP

	 * and tracking). Thus hopefully keeping the ELSP full with active

	 * contexts, despite the best efforts of preempt-to-busy to confuse

	 * us.

 We peeked too soon! */

	/*

	 * We track when the HW has completed saving the context image

	 * (i.e. when we have seen the final CS event switching out of

	 * the context) and must not overwrite the context image before

	 * then. This restricts us to only using the active engine

	 * while the previous virtualized request is inflight (so

	 * we reuse the register offsets). This is a very small

	 * hystersis on the greedy seelction algorithm.

 lazily cleanup after another engine handled rq */

	/*

	 * Move the bound engine to the top of the list for

	 * future execution. We then kick this tasklet first

	 * before checking others, so that we preferentially

	 * reuse this set of bound registers.

	/*

	 * We want to move the interrupted request to the back of

	 * the round-robin list (i.e. its priority level), but

	 * in doing so, we must then move all requests that were in

	 * flight and were waiting for the interrupted request to

	 * be run after it again.

 Leave semaphores spinning on the other engines */

 No waiter should start before its signaler */

	/*

	 * Once bitten, forever smitten!

	 *

	 * If the active context ever busy-waited on a semaphore,

	 * it will be treated as a hog until the end of its timeslice (i.e.

	 * until it is scheduled out and replaced by a new submission,

	 * possibly even its own lite-restore). The HW only sends an interrupt

	 * on the first miss, and we do know if that semaphore has been

	 * signaled, or even if it is now stuck on another semaphore. Play

	 * safe, yield if it might be stuck -- it will be given a fresh

	 * timeslice in the near future.

 If not currently active, or about to switch, wait for next event */

 We do not need to start the timeslice until after the ACK */

 If ELSP[1] is occupied, always check to see if worth slicing */

 Otherwise, ELSP[0] is by itself, but may be waiting in the queue */

 Disable the timer if there is nothing to switch to */

 Avoid continually prolonging an active timeslice */

			/*

			 * If we just submitted a new ELSP after an old

			 * context, that context may have already consumed

			 * its timeslice, so recheck.

 Force a fast reset for terminated contexts (ignoring sysfs!) */

	/*

	 * Hardware submission is through 2 ports. Conceptually each port

	 * has a (RING_START, RING_HEAD, RING_TAIL) tuple. RING_START is

	 * static for a context, and unique to each, so we only execute

	 * requests belonging to a single context from each ring. RING_HEAD

	 * is maintained by the CS in the context image, it marks the place

	 * where it got up to last time, and through RING_TAIL we tell the CS

	 * where we want to execute up to this time.

	 *

	 * In this list the requests are in order of execution. Consecutive

	 * requests from the same context are adjacent in the ringbuffer. We

	 * can combine these requests into a single RING_TAIL update:

	 *

	 *              RING_HEAD...req1...req2

	 *                                    ^- RING_TAIL

	 * since to execute req2 the CS must first execute req1.

	 *

	 * Our goal then is to point each port to the end of a consecutive

	 * sequence of requests as being the most optimal (fewest wake ups

	 * and context switches) submission.

	/*

	 * If the queue is higher priority than the last

	 * request in the currently active context, submit afresh.

	 * We will resubmit again afterwards in case we need to split

	 * the active context to interject the preemption request,

	 * i.e. we will retrigger preemption following the ack in case

	 * of trouble.

	 *

			/*

			 * Don't let the RING_HEAD advance past the breadcrumb

			 * as we unwind (and until we resubmit) so that we do

			 * not accidentally tell it to go backwards.

			/*

			 * Note that we have not stopped the GPU at this point,

			 * so we are unwinding the incomplete requests as they

			 * remain inflight and so by the time we do complete

			 * the preemption, some of the unwound requests may

			 * complete!

			/*

			 * Consume this timeslice; ensure we start a new one.

			 *

			 * The timeslice expired, and we will unwind the

			 * running contexts and recompute the next ELSP.

			 * If that submit will be the same pair of contexts

			 * (due to dependency ordering), we will skip the

			 * submission. If we don't cancel the timer now,

			 * we will see that the timer has expired and

			 * reschedule the tasklet; continually until the

			 * next context switch or other preeemption event.

			 *

			 * Since we have decided to reschedule based on

			 * consumption of this timeslice, if we submit the

			 * same context again, grant it a full timeslice.

			/*

			 * Unlike for preemption, if we rewind and continue

			 * executing the same context as previously active,

			 * the order of execution will remain the same and

			 * the tail will only advance. We do not need to

			 * force a full context restore, as a lite-restore

			 * is sufficient to resample the monotonic TAIL.

			 *

			 * If we switch to any other context, similarly we

			 * will not rewind TAIL of current context, and

			 * normal save/restore will preserve state and allow

			 * us to later continue executing the same request.

			/*

			 * Otherwise if we already have a request pending

			 * for execution after the current one, we can

			 * just wait until the next CS event before

			 * queuing more. In either case we will force a

			 * lite-restore preemption event, but if we wait

			 * we hopefully coalesce several updates into a single

			 * submission.

				/*

				 * Even if ELSP[1] is occupied and not worthy

				 * of timeslices, our queue might be.

 XXX virtual is always taking precedence */

 lost the race to a sibling */

 leave this for another sibling */

			/*

			 * Only after we confirm that we will submit

			 * this request (i.e. it has not already

			 * completed), do we want to update the context.

			 *

			 * This serves two purposes. It avoids

			 * unnecessary work if we are resubmitting an

			 * already completed request after timeslicing.

			 * But more importantly, it prevents us altering

			 * ve->siblings[] on an idle context, where

			 * we may be using ve->siblings[] in

			 * virtual_context_enter / virtual_context_exit.

		/*

		 * Hmm, we have a bunch of virtual engine requests,

		 * but the first one was already completed (thanks

		 * preempt-to-busy!). Keep looking at the veng queue

		 * until we have no more relevant requests (i.e.

		 * the normal submit queue has higher priority).

			/*

			 * Can we combine this request with the current port?

			 * It has to be the same context/ringbuffer and not

			 * have any exceptions (e.g. GVT saying never to

			 * combine contexts).

			 *

			 * If we can combine the requests, we can execute both

			 * by updating the RING_TAIL to point to the end of the

			 * second request, and so we never need to tell the

			 * hardware about the first.

				/*

				 * If we are on the second port and cannot

				 * combine this request with the last, then we

				 * are done.

				/*

				 * We must not populate both ELSP[] with the

				 * same LRCA, i.e. we must submit 2 different

				 * contexts if we submit 2 ELSP.

				/*

				 * We avoid submitting virtual requests into

				 * the secondary ports so that we can migrate

				 * the request immediately to another engine

				 * rather than wait for the primary request.

				/*

				 * If GVT overrides us we only ever submit

				 * port[0], leaving port[1] empty. Note that we

				 * also have to be careful that we don't queue

				 * the same context (even though a different

				 * request) to the second port.

	/*

	 * Here be a bit of magic! Or sleight-of-hand, whichever you prefer.

	 *

	 * We choose the priority hint such that if we add a request of greater

	 * priority than this, we kick the submission tasklet to decide on

	 * the right order of submitting the requests to hardware. We must

	 * also be prepared to reorder requests as they are in-flight on the

	 * HW. We derive the priority hint then as the first "hole" in

	 * the HW submission ports and if there are no available slots,

	 * the priority of the lowest executing request, i.e. last.

	 *

	 * When we do receive a higher priority request ready to run from the

	 * user, see queue_request(), the priority hint is bumped to that

	 * request triggering preemption on the next dequeue (or subsequent

	 * interrupt for secondary ports).

	/*

	 * We can skip poking the HW if we ended up with exactly the same set

	 * of requests as currently running, e.g. trying to timeslice a pair

	 * of ordered contexts.

 Suspend interrupts across request submission */

 flush irq_work (e.g. breadcrumb enabling) */

 A memcpy_p() would be very useful here! */

 avoid write tearing */

 Mark the end of active before we overwrite *active */

 complete the seqlock for execlists_active() */

 Having cancelled all outstanding process_csb(), stop their timers */

/*

 * Starting with Gen12, the status has a new format:

 *

 *     bit  0:     switched to new queue

 *     bit  1:     reserved

 *     bit  2:     semaphore wait mode (poll or signal), only valid when

 *                 switch detail is set to "wait on semaphore"

 *     bits 3-5:   engine class

 *     bits 6-11:  engine instance

 *     bits 12-14: reserved

 *     bits 15-25: sw context id of the lrc the GT switched to

 *     bits 26-31: sw counter of the lrc the GT switched to

 *     bits 32-35: context switch detail

 *                  - 0: ctx complete

 *                  - 1: wait on sync flip

 *                  - 2: wait on vblank

 *                  - 3: wait on scanline

 *                  - 4: wait on semaphore

 *                  - 5: context preempted (not on SEMAPHORE_WAIT or

 *                       WAIT_FOR_EVENT)

 *     bit  36:    reserved

 *     bits 37-43: wait detail (for switch detail 1 to 4)

 *     bits 44-46: reserved

 *     bits 47-57: sw context id of the lrc the GT switched away from

 *     bits 58-63: sw counter of the lrc the GT switched away from

 *

 * Xe_HP csb shuffles things around compared to TGL:

 *

 *     bits 0-3:   context switch detail (same possible values as TGL)

 *     bits 4-9:   engine instance

 *     bits 10-25: sw context id of the lrc the GT switched to

 *     bits 26-31: sw counter of the lrc the GT switched to

 *     bit  32:    semaphore wait mode (poll or signal), Only valid when

 *                 switch detail is set to "wait on semaphore"

 *     bit  33:    switched to new queue

 *     bits 34-41: wait detail (for switch detail 1 to 4)

 *     bits 42-57: sw context id of the lrc the GT switched away from

 *     bits 58-63: sw counter of the lrc the GT switched away from

	/*

	 * The context switch detail is not guaranteed to be 5 when a preemption

	 * occurs, so we can't just check for that. The check below works for

	 * all the cases we care about, including preemptions of WAIT

	 * instructions and lite-restore. Preempt-to-idle via the CTRL register

	 * would require some extra handling, but we don't support that.

	/*

	 * switch detail = 5 is covered by the case above and we do not expect a

	 * context switch on an unsuccessful wait instruction since we always

	 * use polling mode.

 cxt to */

 cxt away */

 cxt to */

 cxt away */

	/*

	 * Reading from the HWSP has one particular advantage: we can detect

	 * a stale entry. Since the write into HWSP is broken, we have no reason

	 * to trust the HW at all, the mmio entry may equally be unordered, so

	 * we prefer the path that is self-checking and as a last resort,

	 * return the mmio value.

	 *

	 * tgl,dg1:HSDES#22011327657

	/*

	 * Unfortunately, the GPU does not always serialise its write

	 * of the CSB entries before its write of the CSB pointer, at least

	 * from the perspective of the CPU, using what is known as a Global

	 * Observation Point. We may read a new CSB tail pointer, but then

	 * read the stale CSB entries, causing us to misinterpret the

	 * context-switch events, and eventually declare the GPU hung.

	 *

	 * icl:HSDES#1806554093

	 * tgl:HSDES#22011248461

 Consume this entry so that we can spot its future reuse. */

 ELSP is an implicit wmb() before the GPU wraps and overwrites csb */

 By cancelling, we will start afresh in start_timeslice() */

	/*

	 * As we modify our execlists state tracking we require exclusive

	 * access. Either we are inside the tasklet, or the tasklet is disabled

	 * and we assume that is only inside the reset paths and so serialised.

	/*

	 * Note that csb_write, csb_status may be either in HWSP or mmio.

	 * When reading from the csb_write mmio register, we have to be

	 * careful to only use the GEN8_CSB_WRITE_PTR portion, which is

	 * the low 4bits. As it happens we know the next 4bits are always

	 * zero and so we can simply masked off the low u8 of the register

	 * and treat it identically to reading from the HWSP (without having

	 * to use explicit shifting and masking, and probably bifurcating

	 * the code to handle the legacy mmio read).

	/*

	 * We will consume all events from HW, or at least pretend to.

	 *

	 * The sequence of events from the HW is deterministic, and derived

	 * from our writes to the ELSP, with a smidgen of variability for

	 * the arrival of the asynchronous requests wrt to the inflight

	 * execution. If the HW sends an event that does not correspond with

	 * the one we are expecting, we have to abandon all hope as we lose

	 * all tracking of what the engine is actually executing. We will

	 * only detect we are out of sequence with the HW when we get an

	 * 'impossible' event because we have already drained our own

	 * preemption/promotion queue. If this occurs, we know that we likely

	 * lost track of execution earlier and must unwind and restart, the

	 * simplest way is by stop processing the event queue and force the

	 * engine to reset.

	/*

	 * Hopefully paired with a wmb() in HW!

	 *

	 * We must complete the read of the write pointer before any reads

	 * from the CSB, so that we do not see stale values. Without an rmb

	 * (lfence) the HW may speculatively perform the CSB[] reads *before*

	 * we perform the READ_ONCE(*csb_write).

 Remember who was last running under the timer */

		/*

		 * We are flying near dragons again.

		 *

		 * We hold a reference to the request in execlist_port[]

		 * but no more than that. We are operating in softirq

		 * context and so cannot hold any mutex or sleep. That

		 * prevents us stopping the requests we are processing

		 * in port[] from being retired simultaneously (the

		 * breadcrumb will be complete before we see the

		 * context-switch). As we only hold the reference to the

		 * request, any pointer chasing underneath the request

		 * is subject to a potential use-after-free. Thus we

		 * store all of the bookkeeping within port[] as

		 * required, and avoid using unguarded pointers beneath

		 * request itself. The same applies to the atomic

		 * status notifier.

 Point active to the new ELSP; prevent overwriting */

 notify execlists_active() */

 cancel old inflight, prepare for switch */

 switch pending to inflight */

 complete the seqlock */

 XXX Magic delay for tgl */

 port0 completed, advanced to port1 */

			/*

			 * We rely on the hardware being strongly

			 * ordered, that the breadcrumb write is

			 * coherent (visible from the CPU) before the

			 * user interrupt is processed. One might assume

			 * that the breadcrumb write being before the

			 * user interrupt and the CS event for the context

			 * switch would therefore be before the CS event

			 * itself...

	/*

	 * Gen11 has proven to fail wrt global observation point between

	 * entry and tail update, failing on the ordering and thus

	 * we see an old entry in the context status buffer.

	 *

	 * Forcibly evict out entries for the next gpu csb update,

	 * to increase the odds that we get a fresh entries with non

	 * working hardware. The cost for doing so comes out mostly with

	 * the wash as hardware, working or not, will need to do the

	 * invalidation before.

	/*

	 * We assume that any event reflects a change in context flow

	 * and merits a fresh timeslice. We reinstall the timer after

	 * inspecting the queue to see if we need to resumbit.

 elide lite-restores */

 Leave semaphores spinning on the other engines */

 too late! */

	/*

	 * Transfer this request onto the hold queue to prevent it

	 * being resumbitted to HW (and potentially completed) before we have

	 * released it. Since we may have already submitted following

	 * requests, we need to remove those as well.

	/*

	 * If one of our ancestors is on hold, we must also be on hold,

	 * otherwise we will bypass it and execute before it.

 Also release any children on this engine that are ready */

 Check that no other parents are also on hold */

	/*

	 * Move this request back to the priority queue, and all of its

	 * children and grandchildren that were suspended along with it.

 Compress all the objects attached to the request, slow! */

 Publish the error state, and announce it to the world */

 Return this request and all that depend upon it for signaling */

	/*

	 * Use the most recent result from process_csb(), but just in case

	 * we trigger an error (via interrupt) before the first CS event has

	 * been written, peek at the next submission.

	/*

	 * We need to _quickly_ capture the engine state before we reset.

	 * We are inside an atomic section (softirq) here and we are delaying

	 * the forced preemption event.

	/*

	 * Remove the request from the execlists queue, and take ownership

	 * of the request. We pass it to our worker who will _slowly_ compress

	 * all the pages the _user_ requested for debugging their batch, after

	 * which we return it to the queue for signaling.

	 *

	 * By removing them from the execlists queue, we also remove the

	 * requests from being processed by __unwind_incomplete_requests()

	 * during the intel_engine_reset(), and so they will *not* be replayed

	 * afterwards.

	 *

	 * Note that because we have not yet reset the engine at this point,

	 * it is possible for the request that we have identified as being

	 * guilty, did in fact complete and we will then hit an arbitration

	 * point allowing the outstanding preemption to succeed. The likelihood

	 * of that is very low (as capturing of the engine registers should be

	 * fast enough to run inside an irq-off atomic section!), so we will

	 * simply hold that request accountable for being non-preemptible

	 * long enough to force the reset.

 Mark this tasklet as disabled to avoid waiting for it to complete */

 Freeze the current request in place */

/*

 * Check the unread Context Status Buffers and manage the submission of new

 * contexts to the ELSP accordingly.

 Generate the error message in priority wrt to the user! */

 thrown by a user payload */

 Upper 16b are the enabling mask, rsvd for internal errors */

 Disable the error interrupt until after the reset */

 Kick the tasklet for some interrupt coalescing and reset handling */

 Will be called from irq-context when using foreign fences. */

	/*

	 * Beware ye of the dragons, this sequence is magic!

	 *

	 * Small changes to this sequence can cause anything from

	 * GPU hangs to forcewake errors and machine lockups!

 Flush any residual operations from the context load */

 Magic required to prevent forcewake errors! */

 Ensure the LRI have landed before we invalidate & continue */

	/*

	 * Flush enough space to reduce the likelihood of waiting after

	 * we start building the request - in which case we will just

	 * have to repeat work.

	/*

	 * Note that after this point, we have committed to using

	 * this request as it is being used to both track the

	 * state of engine initialisation and liveness of the

	 * golden renderstate above. Think twice before you try

	 * to cancel/unwind this request now.

 Unconditionally invalidate GPU caches and TLBs. */

	/*

	 * Sometimes Icelake forgets to reset its pointers on a GPU reset.

	 * Bludgeon them with a mmio update to be sure.

	/*

	 * After a reset, the HW starts writing into CSB entry [0]. We

	 * therefore have to set our HEAD pointer back one entry so that

	 * the *first* entry we check is entry 0. To complicate this further,

	 * as we don't wait for the first interrupt after reset, we have to

	 * fake the HW write to point back to the last entry so that our

	 * inline comparison of our cached head position against the last HW

	 * write works even before the first interrupt.

 Make sure this is visible to HW (paranoia?) */

 Check that the GPU does indeed update the CSB entries! */

 Once more for luck and our trusty paranoia */

	/*

	 * Poison residual state on resume, in case the suspend didn't!

	 *

	 * We have to assume that across suspend/resume (or other loss

	 * of control) that the contents of our pinned buffers has been

	 * lost, replaced by garbage. Since this doesn't always happen,

	 * let's poison such state so that we more quickly spot when

	 * we falsely assume it has been preserved.

	/*

	 * The kernel_context HWSP is stored in the status_page. As above,

	 * that may be lost on resume/initialisation, and so we need to

	 * reset the value in the HWSP.

 And scrub the dirty cachelines for the HWSP */

 clear all existing errors */

	/*

	 * On current gen8+, we have 2 signals to play with

	 *

	 * - I915_ERROR_INSTUCTION (bit 0)

	 *

	 *    Generate an error if the command parser encounters an invalid

	 *    instruction

	 *

	 *    This is a fatal error.

	 *

	 * - CP_PRIV (bit 2)

	 *

	 *    Generate an error on privilege violation (where the CP replaces

	 *    the instruction with a no-op). This also fires for writes into

	 *    read-only scratch pages.

	 *

	 *    This is a non-fatal error, parsing continues.

	 *

	 * * there are a few others defined for odd HW that we do not use

	 *

	 * Since CP_PRIV fires for cases where we have chosen to ignore the

	 * error (as the HW is validating and suppressing the mistakes), we

	 * only unmask the instruction error bit.

 HWSTAM */

	/*

	 * Prevent request submission to the hardware until we have

	 * completed the reset in i915_gem_reset_finish(). If a request

	 * is completed by one engine, it may then queue a request

	 * to a second via its execlists->tasklet *just* as we are

	 * calling engine->resume() and also writing the ELSP.

	 * Turning off the execlists->tasklet until the reset is over

	 * prevents the race.

	/*

	 * We stop engines, otherwise we might get failed reset and a

	 * dead gpu (on elk). Also as modern gpu as kbl can suffer

	 * from system hang if batchbuffer is progressing when

	 * the reset is issued, regardless of READY_TO_RESET ack.

	 * Thus assume it is best to stop engines on all gens

	 * where we have a gpu reset.

	 *

	 * WaKBLVECSSemaphoreWaitPoll:kbl (on ALL_ENGINES)

	 *

	 * FIXME: Wa for more modern gens needs to be validated

 paranoia: read the CSB pointers from after the reset */

 drain preemption events */

 Following the reset, we need to reload the CSB read/write pointers */

	/*

	 * Save the currently executing context, even if we completed

	 * its request, it was still running at the time of the

	 * reset and will have been clobbered.

 Idle context; tidy up the ring so we can restart afresh */

 We still have requests in-flight; the engine should be active */

 Context has requests still in-flight; it should not be idle! */

	/*

	 * If this request hasn't started yet, e.g. it is waiting on a

	 * semaphore, we need to avoid skipping the request or else we

	 * break the signaling chain. However, if the context is corrupt

	 * the request will not restart and we will be stuck with a wedged

	 * device. It is quite often the case that if we issue a reset

	 * while the GPU is loading the context image, that the context

	 * image becomes corrupt.

	 *

	 * Otherwise, if we have not started yet, the request should replay

	 * perfectly and we do not need to flag the result as being erroneous.

	/*

	 * If the request was innocent, we leave the request in the ELSP

	 * and will try to replay it on restarting. The context image may

	 * have been corrupted by the reset, in which case we may have

	 * to service a new GPU hang, but more likely we can continue on

	 * without impact.

	 *

	 * If the request was guilty, we presume the context is corrupt

	 * and have to at least restore the RING register in the context

	 * image back to the expected values to skip over the guilty request.

	/*

	 * We want a simple context + ring to execute the breadcrumb update.

	 * We cannot rely on the context being intact across the GPU hang,

	 * so clear it and rebuild just what we need for the breadcrumb.

	 * All pending requests for this context will be zapped, and any

	 * future request will be after userspace has had the opportunity

	 * to recreate its own state.

 Process the csb, find the guilty context and throw away */

 Push back any incomplete requests for replay after the reset. */

 The driver is wedged; don't process any more events. */

	/*

	 * Before we call engine->cancel_requests(), we should have exclusive

	 * access to the submission state. This is arranged for us by the

	 * caller disabling the interrupt generation, the tasklet and other

	 * threads that may then access the same state, giving us a free hand

	 * to reset state. However, we still need to let lockdep be aware that

	 * we know this state may be accessed in hardirq context, so we

	 * disable the irq around this manipulation and we want to keep

	 * the spinlock focused on its duties and not accidentally conflate

	 * coverage to the submission's irq state. (Similarly, although we

	 * shouldn't need to disable irq around the manipulation of the

	 * submission's irq state, we also wish to remind ourselves that

	 * it is irq state.)

 Mark all executing requests as skipped. */

 Flush the queued requests to the timeline list (for retiring). */

 On-hold requests will be flushed to timeline upon their release */

 Cancel all attached virtual engines */

 Remaining _unready_ requests will be nop'ed when submitted */

	/*

	 * After a GPU reset, we may have requests to replay. Do so now while

	 * we still have the forcewake to be sure that the GPU is not allowed

	 * to sleep before we restart and reload a context.

	 *

	 * If the GPU reset fails, the engine may still be alive with requests

	 * inflight. We expect those to complete, or for the device to be

	 * reset as the next level of recovery, and as a final resort we

	 * will declare the device wedged.

 And kick in case we missed a new request submission. */

	/*

	 * Virtual engines complicate acquiring the engine timeline lock,

	 * as their rq->engine pointer is not stable until under that

	 * engine lock. The simple ploy we use is to take the lock then

	 * check that the rq still belongs to the newly locked engine.

 Prevent further __await_execution() registering a cb, then flush */

 GPGPU on bdw requires extra w/a; not implemented */

	/*

	 * We only need to kick the tasklet once for the high priority

	 * new context we add into the queue.

 Nothing currently active? We're overdue for a submission! */

	/*

	 * If we are already the currently executing context, don't

	 * bother evaluating if we should preempt ourselves.

	/*

	 * Allow preemption of low -> normal -> high, but we do

	 * not allow low priority tasks to preempt other low priority

	 * tasks under the impression that latency for low priority

	 * tasks does not matter (as much as background throughput),

	 * so kiss.

 Synchronise with residual timers and any softirq they raise */

 no longer in control, nothing to sanitize */

 Default vfuncs which can be overridden by each engine. */

		/*

		 * TODO: On Gen11 interrupt masks need to be clear

		 * to allow C6 entry. Keep interrupts enabled at

		 * and take the hit of generating extra interrupts

		 * until a more refined solution exists.

 Finally, take ownership and responsibility for cleanup! */

 Preempt-to-busy may leave a stale request behind. */

	/*

	 * Flush the tasklet in case it is still running on another core.

	 *

	 * This needs to be done before we remove ourselves from the siblings'

	 * rbtrees as in the case it is running in parallel, it may reinsert

	 * the rb_node into a sibling.

 Decouple ourselves from the siblings, no more access allowed. */

 Detachment is lazily performed in the sched_engine->tasklet */

	/*

	 * When destroying the virtual engine, we have to be aware that

	 * it may still be in use from an hardirq/softirq context causing

	 * the resubmission of a completed request (background completion

	 * due to preempt-to-busy). Before we can free the engine, we need

	 * to flush the submission code and tasklets that are still potentially

	 * accessing the engine. Flushing the tasklets requires process context,

	 * and since we can guard the resubmit onto the engine with an RCU read

	 * lock, we can delegate the free of the engine to an RCU worker.

	/*

	 * Pick a random sibling on starting to help spread the load around.

	 *

	 * New contexts are typically created with exactly the same order

	 * of siblings, and often started in batches. Due to the way we iterate

	 * the array of sibling when submitting requests, sibling[0] is

	 * prioritised for dequeuing. If we make sure that sibling[0] is fairly

	 * randomised across the system, we also help spread the load by the

	 * first engine we inspect being different each time.

	 *

	 * NB This does not force us to execute on this engine, it will just

	 * typically be the first we inspect for submission.

 Note: we must use a real engine class for setting up reg state */

 The rq is ready for submission; rq->execution_mask is now stable. */

 Invalid selection, submit to a random engine in error */

 already handled by a sibling's tasklet */

			/*

			 * Cheat and avoid rebalancing the tree if we can

			 * reuse this node in situ.

 By the time we resubmit a request, it may be completed */

 background completion from preempt-to-busy */

	/*

	 * The decision on whether to submit a request using semaphores

	 * depends on the saturated state of the engine. We only compute

	 * this during HW submission of the request, and we need for this

	 * state to be globally applied to all requests being submitted

	 * to this engine. Virtual engines encompass more than one physical

	 * engine and so we cannot accurately tell in advance if one of those

	 * engines is already saturated and so cannot afford to use a semaphore

	 * and be pessimized in priority for doing so -- if we are the only

	 * context using semaphores after all other clients have stopped, we

	 * will be starved on the saturated system. Such a global switch for

	 * semaphores is less than ideal, but alas is the current compromise.

		/*

		 * The virtual engine implementation is tightly coupled to

		 * the execlists backend -- we push out request directly

		 * into a tree inside each physical engine. We could support

		 * layering if we handle cloning of the requests and

		 * submitting a copy into each backend.

		/*

		 * All physical engines must be compatible for their emission

		 * functions (as we build the instructions during request

		 * construction and do not alter them before submission

		 * on the physical engine). We use the engine class as a guide

		 * here, although that could be refined.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2018 Intel Corporation

 Check that we can issue a global GPU reset */

 Check that we can recover a wedged device with a GPU reset */

 Check that the resets are usable from atomic context */

 Flush any requests before we get started and check basics */

 As we poke around the guts, do a full reset before continuing. */

 Check that the resets are usable from atomic context */

 Flush any requests before we get started and check basics */

 As we poke around the guts, do a full reset before continuing. */

 attempt to recover GPU first */

 we're long past hope of a successful reset */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright Â© 2020 Intel Corporation

 Precision of wrap detection is limited to ring->size / 2 */

 And check unwrapped handling for good measure */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright Â© 2018 Intel Corporation

 Signal & wait for start */

 Busy wait */

 Wait for the request to start executing, that then waits for us */

 Run the request for a 100us, sampling timestamps before/after */

 wait for the gpu to catch up */

 Fixed 80ns for GEN11 ctx timestamp? */

	/*

	 * Check that CS_TIMESTAMP / CTX_TIMESTAMP are in sync, i.e. share

	 * the same CS clock.

	/*

	 * Check that if an engine supports busy-stats, they tell the truth.

 100% busy */

	/*

	 * Check we can call intel_engine_pm_put from any context. No

	 * failures are reported directly, but if we mess up lockdep should

	 * tell us.

			/*

			 * Acquisition is always synchronous, except if we

			 * know that the engine is already awake, in which

			 * case we should use intel_engine_pm_get_if_awake()

			 * to atomically grab the wakeref.

			 *

			 * In practice,

			 *    intel_engine_pm_get();

			 *    intel_engine_pm_put();

			 * occurs in one thread, while simultaneously

			 *    intel_engine_pm_get_if_awake();

			 *    intel_engine_pm_put();

			 * occurs from atomic context in another.

 gt wakeref is async (deferred to workqueue) */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2016 Intel Corporation

 Timer fired, first request is complete */

	/*

	 * Also immediately signal any subsequent 0-delay requests, but

	 * requeue the timer for the next delayed request.

	/*

	 * Virtual engines complicate acquiring the engine timeline lock,

	 * as their rq->engine pointer is not stable until under that

	 * engine lock. The simple ploy we use is to take the lock then

	 * check that the rq still belongs to the newly locked engine.

 Mark all submitted requests as skipped. */

 Cancel and submit all pending requests. */

 minimal engine setup for requests */

 fake hw queue */

 We insist the kernel context is using the status_page */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2018 Intel Corporation

 each GPR is 2 dwords */

 Ignore our own attempts to suppress excess tasklets */

 that was quick! */

 Wait until the HW has acknowleged the submission (or err) */

 We know the request is written, make sure all state is too! */

	/*

	 * Check the registers offsets we use to create the initial reg state

	 * match the layout saved by HW.

 requires page alignment */

				/*

				 * Skip over the actual register value as we

				 * expect that to differ.

	/*

	 * Check the assumed register offsets match the actual locations in

	 * the context image.

	/*

	 * Check the live register state matches what we expect for this

	 * intel_context.

 GPR only on rcs0 for gen8 */

	/*

	 * Check that GPR registers are cleared in new contexts as we need

	 * to avoid leaking any information from previous contexts.

 And wait for switch to kernel (to save our context to memory) */

	/*

	 * We want to verify that the timestamp is saved and restore across

	 * context switches and is monotonic.

	 *

	 * So we do this with a little bit of LRC poisoning to check various

	 * boundary conditions, and see what happens if we preempt the context

	 * with a second request (carrying more poison into the timestamp).

 RING_HEAD */

 RING_TAIL */

	/*

	 * Our goal is try and verify that per-context state cannot be

	 * tampered with by another non-privileged client.

	 *

	 * We take the list of context registers from the LRI in the default

	 * context image and attempt to modify that list from a remote context.

 Just don't even ask */

 We use the already reserved extra page in context state */

	/*

	 * In order to test that our per context bb is truly per context,

	 * and executes at the intended spot on context restoring process,

	 * make the batch store the ring start value to memory.

	 * As ring start is restored apriori of starting the indirect ctx bb and

	 * as it will be different for each context, it fits to this purpose.

	/*

	 * Verify that we can recover if one context state is completely

	 * corrupted.

	/*

	 * Check that cumulative context runtime as stored in the pphwsp[16]

	 * is monotonic.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014 Intel Corporation

/*

 * Macro to add commands to auxiliary batch.

 * This macro only checks for page overflow before inserting the commands,

 * this is sufficient as the null state generator makes the final batch

 * with two passes to build command and state separately. At this point

 * the size of both are known and it compacts them by relocating the state

 * right after the commands taking care of alignment so we should sufficient

 * space below them for adding new commands.

		/*

		 * We always program 3x6 pool config but depending upon which

		 * subslice is disabled HW drops down to appropriate config

		 * shown below.

		 *

		 * In the below table 2x6 config always refers to

		 * fused-down version, native 2x6 is not available and can

		 * be ignored

		 *

		 * SNo  subslices config                eu pool configuration

		 * -----------------------------------------------------------

		 * 1    3 subslices enabled (3x6)  -    0x00777000  (9+9)

		 * 2    ss0 disabled (2x6)         -    0x00777000  (3+9)

		 * 3    ss1 disabled (2x6)         -    0x00770000  (6+6)

		 * 4    ss2 disabled (2x6)         -    0x00007000  (9+3)

	/*

	 * Since we are sending length, we need to strictly conform to

	 * all requirements. For Gen2 this must be a multiple of 8.

 return early if there's nothing to setup */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

 Delay to ensure h2g completes */

 Delay to ensure h2g completes */

 Go from min to max in 5 steps */

			/* Wait for GuC to detect business and raise

			 * requested frequency if necessary.

 GuC requests freq in multiples of 50/3 MHz */

 Actual frequency should rise above min */

 Restore min/max frequencies */

 Go from max to min in 5 steps */

 Verify that SWREQ indeed was set to specific value */

 GuC requests freq in multiples of 50/3 MHz */

 Actual frequency should rise above min */

 Restore min/max freq */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 intel_gpu_freq() */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2016-2018 Intel Corporation

	/*

	 * A small race exists between intel_gt_retire_requests_timeout and

	 * intel_timeline_exit which could result in the syncmap not getting

	 * free'd. Rather than work to hard to seal this race, simply cleanup

	 * the syncmap on fini.

 Borrow a nearby lock; we only create these timelines during init */

 Must be pinned to be writable, and no requests in flight. */

	/*

	 * Pretend we are serialised by the timeline->mutex.

	 *

	 * While generally true, there are a few exceptions to the rule

	 * for the engine->kernel_context being used to manage power

	 * transitions. As the engine_park may be called from under any

	 * timeline, it uses the power mutex as a global serialisation

	 * lock to prevent any other request entering its timeline.

	 *

	 * The rule is generally tl->mutex, otherwise engine->wakeref.mutex.

	 *

	 * However, intel_gt_retire_request() does not know which engine

	 * it is retiring along and so cannot partake in the engine-pm

	 * barrier, and there we use the tl->active_count as a means to

	 * pin the timeline in the active_list while the locks are dropped.

	 * Ergo, as that is outside of the engine-pm barrier, we need to

	 * use atomic to manipulate tl->active_count.

		/*

		 * The HWSP is volatile, and may have been lost while inactive,

		 * e.g. across suspend/resume. Be paranoid, and ensure that

		 * the HWSP value matches our seqno so we don't proclaim

		 * the next request as already complete.

 See intel_timeline_enter() */

	/*

	 * Since this timeline is idle, all bariers upon which we were waiting

	 * must also be complete and so we can discard the last used barriers

	 * without loss of information.

 w/a: bit 5 needs to be zero for MI_FLUSH_DW address. */

 Replace the HWSP on wraparound for HW semaphores */

 hwsp_offset may wraparound, so use from->hwsp_seqno */

 ensure we wait on the right request, if not, we completed */

 Can't do semaphore waits on kernel context */

 pin the list element */

 Resume list iteration after reacquiring spinlock */

 Defer the final release to after the spinlock */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2017-2018 Intel Corporation

 Only half of seqno's are usable, see __intel_timeline_get_seqno() */

	/*

	 * Create a bunch of timelines and check that their HWSP do not overlap.

	 * Free some, and try again.

	/* Lookups from cache are very fast and so the random number generation

	 * and the loop itself becomes a significant factor in the per-iteration

	 * timings. We try to compensate the results by measuring the overhead

	 * of the prng and subtract it from the reported results.

 Make sure the compiler doesn't optimise away the prng call */

 Benchmark (only) setting random context ids */

 Benchmark looking up the exact same context ids as we just set */

 Benchmark setting the first N (in order) contexts */

 Benchmark looking up the exact same context ids as we just set */

 Benchmark searching for a random context id and maybe changing it */

 Benchmark searching for a known context id and changing the seqno */

			/* Without assuming too many details of the underlying

			 * implementation, try to identify its phase-changes

			 * (if any)!

	/*

	 * Create a bunch of timelines and check we can write

	 * independently to each of their breadcrumb slots.

	/*

	 * Create a bunch of timelines and check we can write

	 * independently to each of their breadcrumb slots with adjacent

	 * engines.

	/*

	 * Across a seqno wrap, we need to keep the old cacheline alive for

	 * foreign GPU references.

 With wrap should come a new hwsp */

 recycle HWSP */

 some light mutex juggling required; think co-routines */

 Cause a wrap */

	/*

	 * If we take a reference to the HWSP for reading on the GPU, that

	 * read may be arbitrarily delayed (either by foreign fence or

	 * priority saturation) and a wrap can happen within 30 minutes.

	 * When the GPU read is finally submitted it should be correct,

	 * even across multiple wraps.

 CS convenience [SRM/LRM] */

 Create a request we can use for remote reading of the HWSP */

 Ensure timeline is mapped, done during first pin */

			/*

			 * Start at a new wrap, and set seqno right before another wrap,

			 * saving 30 minutes of nops

 before */

 after */

 Flush the timeline before manually wrapping again */

 Single requests are limited to half a ring at most */

	/*

	 * Run the host for long enough, and even the kernel context will

	 * see a seqno rollover.

 We expected a wrap! */

	/*

	 * Simulate a long running user context, and force the seqno wrap

	 * on the user's timeline.

 We expected a wrap! */

	/*

	 * Check seqno writes into one timeline at a time. We expect to

	 * recycle the breadcrumb slot between iterations and neither

	 * want to confuse ourselves or the GPU.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 for_each_engine! */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2008-2015 Intel Corporation

/**

 * DOC: fence register handling

 *

 * Important to avoid confusions: "fences" in the i915 driver are not execution

 * fences used to track command completion but hardware detiler objects which

 * wrap a given range of the global GTT. Each platform has only a fairly limited

 * set of these objects.

 *

 * Fences are used to detile GTT memory mappings. They're also connected to the

 * hardware frontbuffer render tracking and hence interact with frontbuffer

 * compression. Furthermore on older platforms fences are required for tiled

 * objects used by the display engine. They can also be used by the render

 * engine - they're required for blitter commands and are optional for render

 * commands. But on gen4+ both display (with the exception of fbc) and rendering

 * have their own tiling state bits and don't need fences.

 *

 * Also note that fences only support X and Y tiling and hence can't be used for

 * the fancier new tiling formats like W, Ys and Yf.

 *

 * Finally note that because fences are such a restricted resource they're

 * dynamically associated with objects. Furthermore fence state is committed to

 * the hardware lazily to avoid unnecessary stalls on gen2/3. Therefore code must

 * explicitly call i915_gem_object_get_fence() to synchronize fencing status

 * for cpu access. Also note that some code wants an unfenced view, for those

 * cases the fence can be removed forcefully with i915_gem_object_put_fence().

 *

 * Internally these functions will synchronize with userspace access by removing

 * CPU ptes into GTT mmaps (not the GTT ptes themselves) as needed.

		/*

		 * To w/a incoherency with non-atomic 64-bit register updates,

		 * we split the 64-bit update into two 32-bit writes. In order

		 * for a partial fence not to be evaluated between writes, we

		 * precede the update with write to turn off the fence register,

		 * and only enable the fence as the last step.

		 *

		 * For extra levels of paranoia, we make sure each step lands

		 * before applying the next step.

	/*

	 * Previous access through the fence register is marshalled by

	 * the mb() inside the fault handlers (i915_gem_release_mmaps)

	 * and explicitly managed for internal users.

	/*

	 * Access through the fenced region afterwards is

	 * ordered by the posting reads whilst writing the registers.

 implicit 'unfenced' GPU blits */

 XXX Ideally we would move the waiting to outside the mutex */

		/*

		 * Ensure that all userspace CPU access is completed before

		 * stealing the fence.

	/*

	 * We only need to update the register itself if the device is awake.

	 * If the device is currently powered down, we will defer the write

	 * to the runtime resume, see intel_ggtt_restore_fences().

	 *

	 * This only works for removing the fence register, on acquisition

	 * the caller must hold the rpm wakeref. The fence register must

	 * be cleared before we can use any other fences to ensure that

	 * the new fences do not overlap the elided clears, confusing HW.

/**

 * i915_vma_revoke_fence - force-remove fence for a VMA

 * @vma: vma to map linearly (not through a fence reg)

 *

 * This function force-removes any fence from the given object, which is useful

 * if the kernel wants to do untiled GTT access.

	/*

	 * Skip the write to HW if and only if the device is currently

	 * suspended.

	 *

	 * If the driver does not currently hold a wakeref (if_in_use == 0),

	 * the device may currently be runtime suspended, or it may be woken

	 * up before the suspend takes place. If the device is not suspended

	 * (powered down) and we skip clearing the fence register, the HW is

	 * left in an undefined state where we may end up with multiple

	 * registers overlapping.

 now seen this fence twice */

 Prefer idle fences so we do not have to wait on the GPU */

 Wait for completion of pending flips which consume fences */

 Just update our place in the LRU if our fence is getting reused. */

/**

 * i915_vma_pin_fence - set up fencing for a vma

 * @vma: vma to map through a fence reg

 *

 * When mapping objects through the GTT, userspace wants to be able to write

 * to them without having to worry about swizzling if the object is tiled.

 * This function walks the fence regs looking for a free one for @obj,

 * stealing one if it can't find any.

 *

 * It then sets up the reg based on the object's properties: address, pitch

 * and tiling format.

 *

 * For an untiled surface, this removes any existing fence.

 *

 * Returns:

 *

 * 0 on success, negative error code on failure.

	/*

	 * Note that we revoke fences on runtime suspend. Therefore the user

	 * must keep the device awake whilst using the fence.

/**

 * i915_reserve_fence - Reserve a fence for vGPU

 * @ggtt: Global GTT

 *

 * This function walks the fence regs looking for a free one and remove

 * it from the fence_list. It is used to reserve fence for vGPU to use.

 Keep at least one fence available for the display engine. */

 Force-remove fence from VMA */

/**

 * i915_unreserve_fence - Reclaim a reserved fence

 * @fence: the fence reg

 *

 * This function add a reserved fence register from vGPU to the fence_list.

/**

 * intel_ggtt_restore_fences - restore fence state

 * @ggtt: Global GTT

 *

 * Restore the hw fence state to match the software tracking again, to be called

 * after a gpu reset and on resume. Note that on runtime suspend we only cancel

 * the fences, to be reacquired by the user later.

/**

 * DOC: tiling swizzling details

 *

 * The idea behind tiling is to increase cache hit rates by rearranging

 * pixel data so that a group of pixel accesses are in the same cacheline.

 * Performance improvement from doing this on the back/depth buffer are on

 * the order of 30%.

 *

 * Intel architectures make this somewhat more complicated, though, by

 * adjustments made to addressing of data when the memory is in interleaved

 * mode (matched pairs of DIMMS) to improve memory bandwidth.

 * For interleaved memory, the CPU sends every sequential 64 bytes

 * to an alternate memory channel so it can get the bandwidth from both.

 *

 * The GPU also rearranges its accesses for increased bandwidth to interleaved

 * memory, and it matches what the CPU does for non-tiled.  However, when tiled

 * it does it a little differently, since one walks addresses not just in the

 * X direction but also Y.  So, along with alternating channels when bit

 * 6 of the address flips, it also alternates when other bits flip --  Bits 9

 * (every 512 bytes, an X tile scanline) and 10 (every two X tile scanlines)

 * are common to both the 915 and 965-class hardware.

 *

 * The CPU also sometimes XORs in higher bits as well, to improve

 * bandwidth doing strided access like we do so frequently in graphics.  This

 * is called "Channel XOR Randomization" in the MCH documentation.  The result

 * is that the CPU is XORing in either bit 11 or bit 17 to bit 6 of its address

 * decode.

 *

 * All of this bit 6 XORing has an effect on our memory management,

 * as we need to make sure that the 3d driver can correctly address object

 * contents.

 *

 * If we don't have interleaved memory, all tiling is safe and no swizzling is

 * required.

 *

 * When bit 17 is XORed in, we simply refuse to tile at all.  Bit

 * 17 is not just a page offset, so as we page an object out and back in,

 * individual pages in it will have different bit 17 addresses, resulting in

 * each 64 bytes being swapped with its neighbor!

 *

 * Otherwise, if interleaved, we have to tell the 3d driver what the address

 * swizzling it needs to do is, since it's writing with the CPU to the pages

 * (bit 6 and potentially bit 11 XORed in), and the GPU is reading from the

 * pages (bit 6, 9, and 10 XORed in), resulting in a cumulative bit swizzling

 * required by the CPU of XORing in bit 6, 9, 10, and potentially 11, in order

 * to match what the GPU expects.

/**

 * detect_bit_6_swizzle - detect bit 6 swizzling pattern

 * @ggtt: Global GGTT

 *

 * Detects bit 6 swizzling of address lookup between IGD access and CPU

 * access through main memory.

		/*

		 * On BDW+, swizzling is not used. We leave the CPU memory

		 * controller in charge of optimizing memory accesses without

		 * the extra address manipulation GPU side.

		 *

		 * VLV and CHV don't have GPU swizzling.

			/*

			 * Enable swizzling when the channels are populated

			 * with identically sized dimms. We don't need to check

			 * the 3rd channel because no cpu with gpu attached

			 * ships in that configuration. Also, swizzling only

			 * makes sense for 2 channels anyway.

		/*

		 * On Ironlake whatever DRAM config, GPU always do

		 * same swizzling setup.

		/*

		 * As far as we know, the 865 doesn't have these bit 6

		 * swizzling issues.

		/*

		 * The 965, G33, and newer, have a very flexible memory

		 * configuration.  It will enable dual-channel mode

		 * (interleaving) on as much memory as it can, and the GPU

		 * will additionally sometimes enable different bit 6

		 * swizzling for tiled objects from the CPU.

		 *

		 * Here's what I found on the G965:

		 *    slot fill         memory size  swizzling

		 * 0A   0B   1A   1B    1-ch   2-ch

		 * 512  0    0    0     512    0     O

		 * 512  0    512  0     16     1008  X

		 * 512  0    0    512   16     1008  X

		 * 0    512  0    512   16     1008  X

		 * 1024 1024 1024 0     2048   1024  O

		 *

		 * We could probably detect this based on either the DRB

		 * matching, which was the case for the swizzling required in

		 * the table above, or from the 1-ch value being less than

		 * the minimum size of a rank.

		 *

		 * Reports indicate that the swizzling actually

		 * varies depending upon page placement inside the

		 * channels, i.e. we see swizzled pages where the

		 * banks of memory are paired and unswizzled on the

		 * uneven portion, so leave that as unknown.

		/*

		 * On 9xx chipsets, channel interleave by the CPU is

		 * determined by DCC.  For single-channel, neither the CPU

		 * nor the GPU do swizzling.  For dual channel interleaved,

		 * the GPU's interleave is bit 9 and 10 for X tiled, and bit

		 * 9 for Y tiled.  The CPU's interleave is independent, and

		 * can be based on either bit 11 (haven't seen this yet) or

		 * bit 17 (common).

				/*

				 * This is the base swizzling by the GPU for

				 * tiled buffers.

 Bit 11 swizzling by the CPU in addition. */

 Bit 17 swizzling by the CPU in addition. */

 check for L-shaped memory aka modified enhanced addressing */

		/*

		 * Userspace likes to explode if it sees unknown swizzling,

		 * so lie. We will finish the lie when reporting through

		 * the get-tiling-ioctl by reporting the physical swizzle

		 * mode as unknown instead.

		 *

		 * As we don't strictly know what the swizzling is, it may be

		 * bit17 dependent, and so we need to also prevent the pages

		 * from being moved.

/*

 * Swap every 64 bytes of this page around, to account for it having a new

 * bit 17 of its physical address and therefore being interpreted differently

 * by the GPU.

/**

 * i915_gem_object_do_bit_17_swizzle - fixup bit 17 swizzling

 * @obj: i915 GEM buffer object

 * @pages: the scattergather list of physical pages

 *

 * This function fixes up the swizzling in case any page frame number for this

 * object has changed in bit 17 since that state has been saved with

 * i915_gem_object_save_bit_17_swizzle().

 *

 * This is called when pinning backing storage again, since the kernel is free

 * to move unpinned backing storage around (either by directly moving pages or

 * by swapping them out and back in again).

/**

 * i915_gem_object_save_bit_17_swizzle - save bit 17 swizzling

 * @obj: i915 GEM buffer object

 * @pages: the scattergather list of physical pages

 *

 * This function saves the bit 17 of each page frame number so that swizzling

 * can be fixed up later on with i915_gem_object_do_bit_17_swizzle(). This must

 * be called before the backing storage can be unpinned.

 Initialize fence registers to zero */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014-2018 Intel Corporation

	/*

	 * Compute a power-of-two bucket, but throw everything greater than

	 * 16KiB into the same bucket: i.e. the buckets hold objects of

	 * (1 page, 2 pages, 4 pages, 8+ pages).

 Free buffers that have not been used in the past second */

 Most recent at head; oldest at tail */

 Check we are the first to claim this node */

 Return this object to the shrinker pool */

 0 reserved for active nodes */

 Hide this pinned object from the shrinker until retired */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2016 Intel Corporation

 ms; time to wait after flushing between tests */

 not reached */

 Basic check that we can execute our hanging batch */

 100ms */)

 Check that we can reset during non-user portions of requests */

 Check that we can engine-reset during non-user portions */

			/* Engine level resets are triggered by GuC when a hang

			 * is detected. They can't be triggered by the KMD any

			 * more. Thus a nop batch cannot be used as a reset test

 Check that we can recover from engine-reset failues */

 Can't manually break the reset if i915 doesn't perform it */

 timeouts only generated on gen8+ */

 Check that we can issue an engine reset on an idle engine (no-op) */

 Ensure the reset happens and kills the engine */

 GuC based resets are not logged per engine */

 Keep the first error */

	/* Check that issuing a reset on one engine does not interfere

	 * with any other engine.

 start all threads before we begin */

 Ensure the reset happens and kills the engine */

 GuC based resets are not logged per engine */

 GuC based resets are not logged per engine */

 Check that we detect a stuck waiter and issue a reset */

 Mark the fence register as dirty to force the mmio update. */

 Check that we can recover an unbind stuck on a hanging request */

 The reset, even indirectly, should take less than 10ms. */

 100ms */)

 aliasing == global gtt locking, covered above */

 Check that we replay pending requests following a hang */

			/*

			 * XXX We don't handle resetting the kernel context

			 * very well. If we trigger a device reset twice in

			 * quick succession while the kernel context is

			 * executing, we may end up skipping the breadcrumb.

			 * This is really only a problem for the selftest as

			 * normally there is a large interlude between resets

			 * (hangcheck), or we focus on resetting just one

			 * engine and so avoid repeatedly resetting innocents.

 Check that we can issue a global GPU and engine reset */

 Temporarily disable error capture */

 50ms */)

 Check that the engines resets are usable from atomic context */

 Flush any requests before we get started and check basics */

 As we poke around the guts, do a full reset before continuing. */

 we're long past hope of a successful reset */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 fault-inject.h is not standalone! */

	/*

	 * To avoid severe over-allocation when dealing with min_page_size

	 * restrictions, we override that behaviour here by allowing an object

	 * size and page layout which can be smaller. In practice this should be

	 * totally fine, since GTT paging structures are not typically inserted

	 * into the GTT.

	 *

	 * Note that we also hit this path for the scratch page, and for this

	 * case it might need to be 64K, but that should work fine here since we

	 * used the passed in size for the page size, which should ensure it

	 * also has the same alignment.

	/*

	 * Ensure all paging structures for this vm share the same dma-resv

	 * object underneath, with the idea that one object_lock() will lock

	 * them all at once.

	/*

	 * Ensure all paging structures for this vm share the same dma-resv

	 * object underneath, with the idea that one object_lock() will lock

	 * them all at once.

 Keep the obj (and hence the vma) alive as _we_ destroy it */

 lock the vm into the current ww, if we lock one, we lock all */

 We borrowed the scratch page from ggtt, take the top level object */

/**

 * i915_vm_resv_release - Final struct i915_address_space destructor

 * @kref: Pointer to the &i915_address_space.resv_ref member.

 *

 * This function is called when the last lock sharer no longer shares the

 * &i915_address_space._resv lock.

	/*

	 * Special case for GGTT that has already done an early

	 * kref_init here.

	/*

	 * The vm->mutex must be reclaim safe (for use in the shrinker).

	 * Do a dummy acquire now under fs_reclaim so that any allocation

	 * attempt holding the lock is immediately reported by lockdep.

		/*

		 * CHV + BXT VTD workaround use stop_machine(),

		 * which is allowed to allocate memory. This means &vm->mutex

		 * is the outer lock, and in theory we can allocate memory inside

		 * it through stop_machine().

		 *

		 * Add the annotation for this, we use trylock in shrinker.

	/*

	 * In order to utilize 64K pages for an object with a size < 2M, we will

	 * need to support a 64K scratch page, given that every 16th entry for a

	 * page-table operating in 64K mode must point to a properly aligned 64K

	 * region, including any PTEs which happen to point to scratch.

	 *

	 * This is only relevant for the 48b PPGTT where we support

	 * huge-gtt-pages, see also i915_vma_insert(). However, as we share the

	 * scratch (read-only) between all vm, we create one 64k scratch page

	 * for all.

 We need a single contiguous page for our scratch */

 And it needs to be correspondingly aligned */

		/*

		 * Use a non-zero scratch page for debugging.

		 *

		 * We want a value that should be reasonably obvious

		 * to spot in the error state, while also causing a GPU hang

		 * if executed. We prefer using a clear page in production, so

		 * should it ever be accidentally used, the effect should be

		 * fairly benign.

	/*

	 * This function is for gtt related workarounds. This function is

	 * called on driver load and after a GPU reset, so you can place

	 * workarounds here even if they get overwritten by GPU reset.

 WaIncreaseDefaultTLBEntries:chv,bdw,skl,bxt,kbl,glk,cfl,cnl,icl */

	/*

	 * To support 64K PTEs we need to first enable the use of the

	 * Intermediate-Page-Size(IPS) bit of the PDE field via some magical

	 * mmio, otherwise the page-walker will simply ignore the IPS bit. This

	 * shouldn't be needed after GEN10.

	 *

	 * 64K pages were first introduced from BDW+, although technically they

	 * only *work* from gen9+. For pre-BDW we instead have the option for

	 * 32K pages, but we don't currently have any support for it in our

	 * driver.

		/*

		 * According to the BSpec if we use 2M/1G pages then we also

		 * need to disable the GTT cache. At least on BDW we can see

		 * visual corruption when using 2M pages, and not disabling the

		 * GTT cache.

 WaGttCachingOffByDefault */

 TGL doesn't support LLC or AGE settings */

/*

 * The GGTT and PPGTT need a private PPAT setup in order to handle cacheability

 * bits. When using advanced contexts each context stores its own PAT, but

 * writing this data shouldn't be harmful even in those cases.

 for normal objects, no eLLC */

 for something pointing to ptes? */

 Uncached objects, mostly for scanout */

 for scanout with eLLC */

	/*

	 * Map WB on BDW to snooped on CHV.

	 *

	 * Only the snoop bit has meaning for CHV, the rest is

	 * ignored.

	 *

	 * The hardware will never snoop for certain types of accesses:

	 * - CPU GTT (GMADR->GGTT->no snoop->memory)

	 * - PPGTT page tables

	 * - some other special cycles

	 *

	 * As with BDW, we also need to consider the following for GT accesses:

	 * "For GGTT, there is NO pat_sel[2:0] from the entry,

	 * so RTL will always use the value corresponding to

	 * pat_sel = 000".

	 * Which means we must set the snoop bit in PAT entry 0

	 * in order to keep the global status page working.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

	/*

	 * We need to saturate the GPU with work in order to dispatch

	 * a shader on every HW thread, and clear the thread-local registers.

	 * In short, we have to dispatch work faster than the shaders can

	 * run in order to fill the EU and occupy each HW thread.

 including vlv */

 1 - 63dummy idds */

 general */

 surface */

 dynamic */

 indirect */

 instruction */

 general/dynamic/indirect/instruction access Bound */

 scratch buffer */

 number of threads & urb entries for GPGPU vs Media Mode */

 urb entry size & curbe size in 256 bits unit */

 scoreboard */

	/*

	 * interface descriptor address - it is relative to the dynamics base

	 * address

 interface descriptor offset */

 without indirect data */

 scoreboard */

 inline */

 ivb: Stall before STATE_CACHE_INVALIDATE */

 Reset inherited context registers */

 Switch to the media pipeline and our base address */

 Set the clear-residual kernel state */

 Execute the kernel on all HW threads */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

	/*

	 * NB: Specs do not specify how long to spin wait,

	 * so we do ~100us as an educated guess.

 Clear must be after shared has been served for engine */

		/*

		 * According to the BSpec, DW_IIR bits cannot be cleared without

		 * first servicing the Selector & Shared IIR registers.

		/*

		 * We locked GT INT DW by reading it. If we want to (try

		 * to) recover from this successfully, we need to clear

		 * our bit, otherwise we are locking the register for

		 * everybody.

 Disable RCS, BCS, VCS and VECS class engines. */

 Restore masks irqs on RCS, BCS, VCS and VECS engines. */

 Enable RCS, BCS, VCS and VECS class interrupts. */

 Unmask irqs on RCS, BCS, VCS and VECS engines. */

	/*

	 * RPS interrupts will get enabled/disabled on demand when RPS itself

	 * is enabled/disabled.

 Same thing for GuC interrupts */

 These are interrupts we'll toggle with the ring mask register */

	/*

	 * RPS interrupts will get enabled/disabled on demand when RPS itself

	 * is enabled/disabled. Same wil be the case for GuC interrupts.

 L3 parity interrupt is always unmasked. */

		/*

		 * RPS interrupts will get enabled/disabled on demand when RPS

		 * itself is enabled/disabled.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014 Intel Corporation

 *

 * Generated by: intel-gpu-tools-1.8-220-g01153e7

 reloc */

 reloc */

 reloc */

 reloc */

 reloc */

 cmds end */

 state start */

 state end */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

	/*

	 * Also leave a space between the unallocated reserved node after the

	 * GTT and any objects within the GTT, i.e. we use the color adjustment

	 * to insert a guard page to prevent prefetches crossing over the

	 * GTT boundary.

 Only VLV supports read-only GGTT mappings */

/**

 * i915_ggtt_init_hw - Initialize GGTT hardware

 * @i915: i915 device

	/*

	 * Note that we use page colouring to enforce a guard page at the

	 * end of the address space. This is required as the CS may prefetch

	 * beyond the end of the batch buffer, across the page boundary,

	 * and beyond the end of the GTT if we do not provide a guard.

/*

 * Certain Gen5 chipsets require idling the GPU before

 * unmapping anything from the GTT when VT-d is enabled.

	/*

	 * Query intel_iommu to see if we need the workaround. Presumably that

	 * was loaded first.

 XXX DMAR fault reason 7 */

 Skip rewriting PTE on VMA unbind. */

	/*

	 * Note that as an uncached mmio write, this will flush the

	 * WCB of the writes into the GGTT before it triggers the invalidate.

	/*

	 * Note that we ignore PTE_READ_ONLY here. The caller must be careful

	 * not to allow the user to override access to a read only page.

 Fill the allocated but "unused" space beyond the end of the buffer */

	/*

	 * We want to flush the TLBs only after we're certain all the PTE

	 * updates have finished.

/*

 * Binds an object into the global gtt with the specified cache level.

 * The object will be accessible to the GPU via commands whose operands

 * reference offsets within the global GTT as well as accessible by the GPU

 * through the GMADR mapped BAR (i915->mm.gtt->gtt).

 Fill the allocated but "unused" space beyond the end of the buffer */

	/*

	 * We want to flush the TLBs only after we're certain all the PTE

	 * updates have finished.

	/*

	 * Make sure the internal GAM fifo has been cleared of all GTT

	 * writes before exiting stop_machine(). This guarantees that

	 * any aperture accesses waiting to start in another process

	 * cannot back up behind the GTT writes causing a hang.

	 * The register can be any arbitrary GAM register.

 Applicable to VLV (gen8+ do not support RO in the GGTT) */

	/*

	 * Let GEM Manage all of the aperture.

	 *

	 * However, leave one page at the end still bound to the scratch page.

	 * There are a number of places where the hardware apparently prefetches

	 * past the end of the object, and we've seen multiple hangs with the

	 * GPU head pointer stuck in a batchbuffer bound at the last page of the

	 * aperture.  One page should be enough to keep any prefetching inside

	 * of the aperture.

	/*

	 * GuC requires all resources that we're sharing with it to be placed in

	 * non-WOPCM memory. If GuC is not present or not in use we still need a

	 * small bias as ring wraparound at offset 0 sometimes hangs. No idea

	 * why.

		/*

		 * Reserve a mappable slot for our lockless error capture.

		 *

		 * We strongly prefer taking address 0x0 in order to protect

		 * other critical buffers against accidental overwrites,

		 * as writing to address 0 is a very common mistake.

		 *

		 * Since 0 may already be in use by the system (e.g. the BIOS

		 * framebuffer), we let the reservation fail quietly and hope

		 * 0 remains reserved always.

		 *

		 * If we fail to reserve 0, and then fail to find any space

		 * for an error-capture, remain silent. We can afford not

		 * to reserve an error_capture node as we have fallback

		 * paths, and we trust that 0 will remain reserved. However,

		 * the only likely reason for failure to insert is a driver

		 * bug, which we expect to cause other failures...

	/*

	 * The upper portion of the GuC address space has a sizeable hole

	 * (several MB) that is inaccessible by GuC. Reserve this range within

	 * GGTT as it can comfortably hold GuC/HuC firmware images.

 Clear any non-preallocated blocks */

 And finally clear the reserved guard page */

 Currently applicable only to VLV */

	/*

	 * Note we only pre-allocate as far as the end of the global

	 * GTT. On 48b / 4-level page-tables, the difference is very,

	 * very significant! We have to preallocate as GVT/vgpu does

	 * not like the page directory disappearing.

/**

 * i915_ggtt_driver_release - Clean up GGTT hardware initialization

 * @i915: i915 device

/**

 * i915_ggtt_driver_late_release - Cleanup of GGTT that needs to be done after

 * all free objects have been drained.

 * @i915: i915 device

 Limit 32b platforms to a 2GB GGTT: 4 << 20 / pte size * I915_GTT_PAGE_SIZE */

	/*

	 * GEN6: GTTMMADR size is 4MB and GTTADR starts at 2MB offset

	 * GEN8: GTTMMADR size is 16MB and GTTADR starts at 8MB offset

	/*

	 * On BXT+/ICL+ writes larger than 64 bit to the GTT pagetable range

	 * will be dropped. For WC mappings in general we have 64 byte burst

	 * writes when the WC buffer is flushed, so we can't use it, but have to

	 * resort to an uncached mapping. The WC issue is easily caught by the

	 * readback check when writing GTT PTE entries.

 iounmap will also get called at remove, but meh */

 TODO: We're not aware of mappable constraints on gen8 yet */

	/*

	 * Serialize GTT updates with aperture access on BXT if VT-d is on,

	 * and always on CHV.

	/*

	 * 64/512MB is the current min/max we actually know of, but this is

	 * just a coarse sanity check.

 GMADR is the PCI mmio aperture into the global GTT. */

/**

 * i915_ggtt_probe_hw - Probe GGTT hardware location

 * @i915: i915 device

 XXX Temporary pardon for error unload */

 We should only be called after i915_ggtt_enable_guc() */

 First fill our portion of the GTT with scratch pages */

 Skip rewriting PTE on VMA unbind. */

 clflush objects bound into the GGTT and rebind them. */

 only used during resume => exclusive access */

			/*

			 * We don't need the pages, but need to initialize

			 * the entries so the sg list can be happily traversed.

			 * The only thing we need are DMA addresses.

		/*

		 * The DE ignores the PTEs for the padding tiles, the sg entry

		 * here is just a conenience to indicate how many padding PTEs

		 * to insert at this spot.

 Allocate target SG list. */

		/*

		 * The DE ignores the PTEs for the padding tiles, the sg entry

		 * here is just a convenience to indicate how many padding PTEs

		 * to insert at this spot.

			/*

			 * We don't need the pages, but need to initialize

			 * the entries so the sg list can be happily traversed.

			 * The only thing we need are DMA addresses.

		/*

		 * The DE ignores the PTEs for the padding tiles, the sg entry

		 * here is just a conenience to indicate how many padding PTEs

		 * to insert at this spot.

 Allocate target SG list. */

 Drop any unused tail entries. */

	/*

	 * The vma->pages are only valid within the lifespan of the borrowed

	 * obj->mm.pages. When the obj->mm.pages sg_table is regenerated, so

	 * must be the vma->pages. A simple rule is that vma->pages must only

	 * be accessed when the obj->mm.pages are pinned.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014 Intel Corporation

 skip */

 Close the batch; used mainly by live_lrc_layout() */

	/*

	 * The gen12+ lists only have the registers we program in the basic

	 * default state. We rely on the context image using relative

	 * addressing to automatic fixup the register state between the

	 * physical engines for virtual engine.

		/*

		 * Note that the CSFE context has a dummy slot for CMD_BUF_CCTL

		 * simply to match the RCS context image layout.

		/* 64b PPGTT (48bit canonical)

		 * PDP0_DESCRIPTOR contains the base address to PML4 and

		 * other PDP Descriptors are ignored.

	/*

	 * A context is actually a big batch buffer with several

	 * MI_LOAD_REGISTER_IMM commands followed by (reg, value) pairs. The

	 * values we are setting here are only for the first context restore:

	 * on a subsequent save, the GPU will recreate this batchbuffer with new

	 * values (including all the missing MI_LOAD_REGISTER_IMM commands that

	 * we are not initializing here).

	 *

	 * Must keep consistent with virtual_update_register_offsets().

 Clear the ppHWSP (inc. per-context counters) */

	/*

	 * The second page of the context object contains some registers which

	 * must be set up prior to the first execution.

 for redzone */

		/*

		 * Use the static global HWSP for the kernel context, and

		 * a dynamically allocated cacheline for everyone else.

 Scrub away the garbage */

 back to start of context image */

/*

 * The context descriptor encodes various attributes of a context,

 * including its GTT address and some flags. Because it's fairly

 * expensive to calculate, we'll just do it once and cache the result,

 * which remains valid until the context is unpinned.

 *

 * This is what a descriptor looks like, from LSB to MSB::

 *

 *      bits  0-11:    flags, GEN8_CTX_* (cached in ctx->desc_template)

 *      bits 12-31:    LRCA, GTT address of (the HWSP of) this context

 *      bits 32-52:    ctx ID, a globally unique tag (highest bit used by GuC)

 *      bits 53-54:    mbz, reserved for use by hardware

 *      bits 55-63:    group ID, currently unused and set to 0

 *

 * Starting from Gen11, the upper dword of the descriptor has a new format:

 *

 *      bits 32-36:    reserved

 *      bits 37-47:    SW context ID

 *      bits 48:53:    engine instance

 *      bit 54:        mbz, reserved for use by hardware

 *      bits 55-60:    SW counter

 *      bits 61-63:    engine class

 *

 * On Xe_HP, the upper dword of the descriptor has a new format:

 *

 *      bits 32-37:    virtual function number

 *      bit 38:        mbz, reserved for use by hardware

 *      bits 39-54:    SW context ID

 *      bits 55-57:    reserved

 *      bits 58-63:    SW counter

 *

 * engine info, SW context ID and SW counter need to form a unique number

 * (Context ID) per lrc.

 RPCS */

 Mutually exclusive wrt to global indirect bb */

/*

 * In this WA we need to set GEN8_L3SQCREG4[21:21] and reset it after

 * PIPE_CONTROL instruction. This is required for the flush to happen correctly

 * but there is a slight complication as this is applied in WA batch where the

 * values are only initialized once so we cannot take register value at the

 * beginning and reuse it further; hence we save its value to memory, upload a

 * constant value with bit21 set and then we restore it back with the saved value.

 * To simplify the WA, a constant value is formed by using the default value

 * of this register. This shouldn't be a problem because we are only modifying

 * it for a short period and this batch in non-premptible. We can ofcourse

 * use additional instructions that read the actual value of the register

 * at that time and set our bit of interest but it makes the WA complicated.

 *

 * This WA is also required for Gen9 so extracting as a function avoids

 * code duplication.

 NB no one else is allowed to scribble over scratch + 256! */

/*

 * Typically we only have one indirect_ctx and per_ctx batch buffer which are

 * initialized at the beginning and shared across all contexts but this field

 * helps us to have multiple batches at different offsets and select them based

 * on a criteria. At the moment this batch always start at the beginning of the page

 * and at this point we don't have multiple wa_ctx batch buffers.

 *

 * The number of WA applied are not known at the beginning; we use this field

 * to return the no of DWORDS written.

 *

 * It is to be noted that this batch does not contain MI_BATCH_BUFFER_END

 * so it adds NOOPs as padding to make it cacheline aligned.

 * MI_BATCH_BUFFER_END will be added to perctx batch and both of them together

 * makes a complete batch buffer.

 WaDisableCtxRestoreArbitration:bdw,chv */

 WaFlushCoherentL3CacheLinesAtContextSwitch:bdw */

 WaClearSlmSpaceAtContextSwitch:bdw,chv */

 Actual scratch location is at 128 bytes offset */

 Pad to end of cacheline */

	/*

	 * MI_BATCH_BUFFER_END is not required in Indirect ctx BB because

	 * execution depends on the length specified in terms of cache lines

	 * in the register CTX_RCS_INDIRECT_CTX

 WaDisableGatherAtSetShaderCommonSlice:skl,bxt,kbl,glk */

 BSpec: 11391 */

 BSpec: 11299 */

 WaFlushCoherentL3CacheLinesAtContextSwitch:skl,bxt,glk */

 WaClearSlmSpaceAtContextSwitch:skl,bxt,kbl,glk,cfl */

 WaMediaPoolStateCmdInWABB:bxt,glk */

		/*

		 * EU pool configuration is setup along with golden context

		 * during context initialization. This value depends on

		 * device type (2x6 or 3x6) and needs to be updated based

		 * on which subslice is disabled especially for 2x6

		 * devices, however it is safe to load default

		 * configuration of 3x6 device instead of masking off

		 * corresponding bits because HW ignores bits of a disabled

		 * subslice and drops down to appropriate config. Please

		 * see render_state_setup() in i915_gem_render_state.c for

		 * possible configurations, to avoid duplication they are

		 * not shown here again.

 Pad to end of cacheline */

		/*

		 * We continue even if we fail to initialize WA batch

		 * because we only expect rare glitches but nothing

		 * critical to prevent us from using GPU

	/*

	 * Emit the two workaround batch buffers, recording the offset from the

	 * start of the workaround batch buffer object for each and their

	 * respective sizes.

 Verify that we can handle failure to setup the wa_ctx */

 Clear all flags to prevent further use */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 start all threads before we kthread_stop() */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 Ring wraparound at offset 0 sometimes hangs. No idea why. */

 Discard any unused bytes beyond that submitted to hw. */

	/*

	 * Mark ring buffers as read-only from GPU side (so no stray overwrites)

	 * if supported by the platform's GGTT.

	/*

	 * Workaround an erratum on the i830 which causes a hang if

	 * the TAIL pointer points to within the last 2 cachelines

	 * of the buffer.

 Would completion of this request free enough space? */

 Packets must be qword aligned. */

			/*

			 * Not enough space for the basic request. So need to

			 * flush out the remainder and then wait for

			 * base + reserved.

			/*

			 * The base request will fit but the reserved space

			 * falls off the end. So we don't need an immediate

			 * wrap and only need to effectively wait for the

			 * reserved size from the start of ringbuffer.

		/*

		 * Space is reserved in the ringbuffer for finalising the

		 * request, as that cannot be allowed to fail. During request

		 * finalisation, reserved_space is set to 0 to stop the

		 * overallocation and the assumption is that then we never need

		 * to wait (which has the risk of failing with EINTR).

		 *

		 * See also i915_request_alloc() and i915_request_add().

 Fill the tail with MI_NOOP */

 Align the ring tail to a cacheline boundary */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2016-2019 Intel Corporation

/*

 * List of required GuC and HuC binaries per-platform.

 * Must be ordered based on platform + revid, from newer to older.

 *

 * Note that RKL and ADL-S have the same GuC/HuC device ID's and use the same

 * firmware as TGL.

 All blobs need to be declared via MODULE_FIRMWARE() */

 The below structs and macros are used to iterate across the list of blobs */

 first platform rev using this FW */

 make sure the list is ordered as expected */

/**

 * intel_uc_fw_init_early - initialize the uC object and select the firmware

 * @uc_fw: uC firmware

 * @type: type of uC

 *

 * Initialize the state of our uC object and relevant tracking and select the

 * firmware to fetch and load.

	/*

	 * we use FIRMWARE_UNINITIALIZED to detect checks against uc_fw->status

	 * before we're looked at the HW caps to see if we have uc support

 non-existing blob */

 require next major version */

 require next minor version */

 require prev major version */

 require prev minor version - hey, this should work! */

 officially unsupported platform */

/**

 * intel_uc_fw_fetch - fetch uC firmware

 * @uc_fw: uC firmware

 *

 * Fetch uC firmware into GEM obj.

 *

 * Return: 0 on success, a negative errno code on failure.

 Check the size of the blob before examining buffer contents */

 Check integrity of size values inside CSS header */

 uCode size must calculated from other sizes */

 now RSA */

 At least, it should have header, uCode and RSA. Size of all three. */

 Sanity check whether this fw is not larger than whole WOPCM memory */

 Get version numbers from the CSS header */

 OK even if fw is NULL */

 uc_fw->obj cache domains were not controlled across suspend */

 Set the source address for the uCode */

 Set the DMA destination */

	/*

	 * Set the transfer size. The header plus uCode will be copied to WOPCM

	 * via DMA, excluding any other components

 Start the DMA */

 Wait for DMA to finish */

 Disable the bits once DMA is over */

/**

 * intel_uc_fw_upload - load uC firmware using custom loader

 * @uc_fw: uC firmware

 * @dst_offset: destination offset

 * @dma_flags: flags for flags for dma ctrl

 *

 * Loads uC firmware and updates internal flags.

 *

 * Return: 0 on success, non-zero on failure.

 make sure the status was cleared the last time we reset the uc */

 Call custom loader */

 this should happen before the load! */

/**

 * intel_uc_fw_cleanup_fetch - cleanup uC firmware

 * @uc_fw: uC firmware

 *

 * Cleans up uC firmware by releasing the firmware GEM obj.

/**

 * intel_uc_fw_copy_rsa - copy fw RSA to buffer

 *

 * @uc_fw: uC firmware

 * @dst: dst buffer

 * @max_len: max number of bytes to copy

 *

 * Return: number of copied bytes.

 Called during reset handling, must be atomic [no fs_reclaim] */

/**

 * intel_uc_fw_dump - dump information about uC firmware

 * @uc_fw: uC firmware

 * @p: the &drm_printer

 *

 * Pretty printer for uC firmware.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

	/*

	 * Enable and start the guc log relay on value of 1.

	 * Flush log relay for any other value.

 SPDX-License-Identifier: MIT

/*

 * Copyright ï¿½ï¿½ 2021 Intel Corporation

 Submit requests and inject errors forcing G2H to be dropped */

 Force all H2G / G2H to be submitted / processed */

 Scrub missing G2H */

 GT will not idle if G2H are lost */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2016-2019 Intel Corporation

/**

 * DOC: CTB Blob

 *

 * We allocate single blob to hold both CTB descriptors and buffers:

 *

 *      +--------+-----------------------------------------------+------+

 *      | offset | contents                                      | size |

 *      +========+===============================================+======+

 *      | 0x0000 | H2G `CTB Descriptor`_ (send)                  |      |

 *      +--------+-----------------------------------------------+  4K  |

 *      | 0x0800 | G2H `CTB Descriptor`_ (recv)                  |      |

 *      +--------+-----------------------------------------------+------+

 *      | 0x1000 | H2G `CT Buffer`_ (send)                       | n*4K |

 *      |        |                                               |      |

 *      +--------+-----------------------------------------------+------+

 *      | 0x1000 | G2H `CT Buffer`_ (recv)                       | m*4K |

 *      | + n*4K |                                               |      |

 *      +--------+-----------------------------------------------+------+

 *

 * Size of each `CT Buffer`_ must be multiple of 4K.

 * We don't expect too many messages in flight at any time, unless we are

 * using the GuC submission. In that case each request requires a minimum

 * 2 dwords which gives us a maximum 256 queue'd requests. Hopefully this

 * enough space to avoid backpressure on the driver. We increase the size

 * of the receive buffer (relative to the send) to ensure a G2H response

 * CTB has a landing spot.

/**

 * intel_guc_ct_init_early - Initialize CT state without requiring device access

 * @ct: pointer to CT struct

 CT registration must go over MMIO */

 CT deregistration must go over MMIO */

/**

 * intel_guc_ct_init - Init buffer-based communication

 * @ct: pointer to CT struct

 *

 * Allocate memory required for buffer-based communication.

 *

 * Return: 0 on success, a negative errno code on failure.

 store pointers to desc and cmds for send ctb */

 store pointers to desc and cmds for recv ctb */

/**

 * intel_guc_ct_fini - Fini buffer-based communication

 * @ct: pointer to CT struct

 *

 * Deallocate memory required for buffer-based communication.

/**

 * intel_guc_ct_enable - Enable buffer based command transport.

 * @ct: pointer to CT struct

 *

 * Return: 0 on success, a negative errno code on failure.

 vma should be already allocated and map'ed */

 blob should start with send descriptor */

 (re)initialize descriptors */

	/*

	 * Register both CT buffers starting with RECV buffer.

	 * Descriptors are in first half of the blob.

/**

 * intel_guc_ct_disable - Disable buffer based command transport.

 * @ct: pointer to CT struct

 For now it's trivial */

 in dwords */,

	/*

	 * dw0: CT header (including fence)

	 * dw1: HXG header (including action code)

	 * dw2+: action data

	/*

	 * make sure H2G buffer update and LRC tail update (if this triggering a

	 * submission) are visible before updating the descriptor tail

 update local copies */

 now update descriptor */

/**

 * wait_for_ct_request_update - Wait for CT request state update.

 * @req:	pointer to pending request

 * @status:	placeholder for status

 *

 * For each sent request, GuC shall send back CT response message.

 * Our message handler will update status of tracked request once

 * response message with given fence is received. Wait here and

 * check for valid response status value.

 *

 * Return:

 * *	0 response received (status is valid)

 * *	-ETIMEDOUT no response within hardcoded timeout

	/*

	 * Fast commands should complete in less than 10us, so sample quickly

	 * up to that length of time, then switch to a slower sleep-wait loop.

	 * No GuC command should ever take longer than 10ms but many GuC

	 * commands can be inflight at time, so use a 1s timeout on the slower

	 * sleep-wait loop.

	/*

	 * We leave a certain amount of space in the G2H CTB buffer for

	 * unexpected G2H CTBs (e.g. logging, engine hang, etc...)

	/*

	 * We use a lazy spin wait loop here as we believe that if the CT

	 * buffers are sized correctly the flow control condition should be

	 * rare. Reserving the maximum size in the G2H credits as we don't know

	 * how big the response is going to be.

 There shall be no data in the status */

 Return actual response len */

 There shall be no response payload */

 Return data decoded from the status dword */

/*

 * Command Transport (CT) buffer based GuC send function.

 undefined */

/*

 * Return: number available remaining dwords to read (0 if empty)

 *         or a negative error code on failure

 tail == head condition indicates empty */

 beware of buffer wrap case */

 message len with header */

 update local copies */

 now update descriptor */

	/*

	 * Adjusting the space must be done in IRQ or deadlock can occur as the

	 * CTB processing in the below workqueue can send CTBs which creates a

	 * circular dependency if the space was returned there.

/*

 * Return: number available remaining dwords to read (0 if empty)

 *         or a negative error code on failure

/*

 * When we're communicating with the GuC over CT, GuC uses events

 * to notify us about new messages being posted on the RECV buffer.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014-2019 Intel Corporation

 *

 * Authors:

 *    Vinit Azad <vinit.azad@intel.com>

 *    Ben Widawsky <ben@bwidawsk.net>

 *    Dave Gordon <david.s.gordon@intel.com>

 *    Alex Dai <yu.dai@intel.com>

 Must program this register before loading the ucode with DMA */

 DOP Clock Gating Enable for GuC clocks */

 allows for 5us (in 10ns units) before GT can go to RC6 */

 Copy RSA signature from the fw image to HW for verification */

/*

 * Read the GuC status register (GUC_STATUS) and store it in the

 * specified location; then return a boolean indicating whether

 * the value matches either of two values representing completion

 * of the GuC boot process.

 *

 * This is used for polling the GuC status in a wait_for()

 * loop below.

	/*

	 * Wait for the GuC to start up.

	 * NB: Docs recommend not using the interrupt for completion.

	 * Measurements indicate this should take no more than 20ms, so a

	 * timeout here indicates that the GuC has failed and is unusable.

	 * (Higher levels of the driver may decide to reset the GuC and

	 * attempt the ucode load again if this happens.)

/**

 * intel_guc_fw_upload() - load GuC uCode to device

 * @guc: intel_guc structure

 *

 * Called from intel_uc_init_hw() during driver load, resume from sleep and

 * after a GPU reset.

 *

 * The firmware image should have already been fetched into memory, so only

 * check that fetch succeeded, and then transfer the image to the h/w.

 *

 * Return:	non-zero code on error

	/*

	 * Note that GuC needs the CSS header plus uKernel code to be copied

	 * by the DMA engine in one operation, whereas the RSA signature is

	 * loaded via MMIO.

	/*

	 * Current uCode expects the code to be loaded at 8k; locations below

	 * this are used for the stack.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014-2019 Intel Corporation

/**

 * DOC: GuC firmware log

 *

 * Firmware log is enabled by setting i915.guc_log_level to the positive level.

 * Log data is printed out via reading debugfs i915_guc_log_dump. Reading from

 * i915_guc_load_status will print out firmware loading status and scratch

 * registers value.

/*

 * Sub buffer switch callback. Called whenever relay has to switch to a new

 * sub buffer, relay stays on the same sub buffer if 0 is returned.

	/*

	 * Use no-overwrite mode by default, where relay will stop accepting

	 * new data if there are no empty sub buffers left.

	 * There is no strict synchronization enforced by relay between Consumer

	 * and Producer. In overwrite mode, there is a possibility of getting

	 * inconsistent/garbled data, the producer could be writing on to the

	 * same sub buffer from which Consumer is reading. This can't be avoided

	 * unless Consumer is fast enough and can always run in tandem with

	 * Producer.

/*

 * file_create() callback. Creates relay file in debugfs.

	/*

	 * This to enable the use of a single buffer for the relay channel and

	 * correspondingly have a single file exposed to User, through which

	 * it can collect the logs in order without any post-processing.

	 * Need to set 'is_global' even if parent is NULL for early logging.

/*

 * file_remove() default callback. Removes relay file in debugfs.

 relay channel callbacks */

	/*

	 * Make sure the updates made in the sub buffer are visible when

	 * Consumer sees the following update to offset inside the sub buffer.

 All data has been written, so now move the offset of sub buffer. */

 Switch to the next sub buffer */

	/*

	 * Just get the base address of a new sub buffer and copy data into it

	 * ourselves. NULL will be returned in no-overwrite mode, if all sub

	 * buffers are full. Could have used the relay_write() to indirectly

	 * copy the data, but that would have been bit convoluted, as we need to

	 * write to only certain locations inside a sub buffer which cannot be

	 * done without using relay_reserve() along with relay_write(). So its

	 * better to use relay_reserve() alone.

 buffer_full_cnt is a 4 bit counter */

 Get the pointer to shared GuC log buffer */

 Get the pointer to local buffer to store the logs */

		/*

		 * Used rate limited to avoid deluge of messages, logs might be

		 * getting consumed by User at a slow rate.

 Actual logs are present from the 2nd page */

		/*

		 * Make a copy of the state structure, inside GuC log buffer

		 * (which is uncached mapped), on the stack to avoid reading

		 * from it multiple times.

 Bookkeeping stuff */

 Update the state of shared log buffer */

 First copy the state structure in snapshot buffer */

		/*

		 * The write pointer could have been updated by GuC firmware,

		 * after sending the flush interrupt to Host, for consistency

		 * set write pointer value to same value of sampled_write_ptr

		 * in the snapshot buffer.

 Now copy the actual logs. */

 copy the whole buffer in case of overflow */

 copy whole buffer as offsets are unreliable */

 Just copy the newly written data */

	/*

	 * Create a WC (Uncached for read) vmalloc mapping of log

	 * buffer pages, so that we can directly get the data

	 * (up-to-date) from memory.

 Keep the size of sub buffers same as shared log buffer */

	/*

	 * Store up to 8 snapshots, which is large enough to buffer sufficient

	 * boot time logs and provides enough leeway to User, in terms of

	 * latency, for consuming the logs from relay. Also doesn't take

	 * up too much memory.

	/*

	 * Generally device is expected to be active only at this

	 * time, so get/put should be really quick.

 A negative value means "use platform/config default" */

	/*

	 *  GuC Log buffer Layout

	 *

	 *  +===============================+ 00B

	 *  |    Crash dump state header    |

	 *  +-------------------------------+ 32B

	 *  |      Debug state header       |

	 *  +-------------------------------+ 64B

	 *  |                               |

	 *  +===============================+ PAGE_SIZE (4KB)

	 *  |        Crash Dump logs        |

	 *  +===============================+ + CRASH_SIZE

	 *  |          Debug logs           |

	 *  +===============================+ + DEBUG_SIZE

	/*

	 * GuC is recognizing log levels starting from 0 to max, we're using 0

	 * as indication that logging should be disabled.

	/*

	 * We require SSE 4.1 for fast reads from the GuC log buffer and

	 * it should be present on the chipsets supporting GuC based

	 * submisssions.

	/*

	 * When GuC is logging without us relaying to userspace, we're ignoring

	 * the flush notification. This means that we need to unconditionally

	 * flush on relay enabling, since GuC only notifies us once.

	/*

	 * Before initiating the forceful flush, wait for any pending/ongoing

	 * flush to complete otherwise forceful flush may not actually happen.

 GuC would have updated log buffer by now, so capture it */

/*

 * Stops the relay log. Called from intel_guc_log_relay_close(), so no

 * possibility of race with start/flush since relay_write cannot race

 * relay_close.

/**

 * intel_guc_log_info - dump information about GuC log relay

 * @log: the GuC log

 * @p: the &drm_printer

 *

 * Pretty printer for GuC log info

/**

 * intel_guc_log_dump - dump the contents of the GuC log

 * @log: the GuC log

 * @p: the &drm_printer

 * @dump_load_err: dump the log saved on GuC load error

 *

 * Pretty printer for the GuC log

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

 GuC RC is unavailable for pre-Gen12 */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014-2019 Intel Corporation

/*

 * The Additional Data Struct (ADS) has pointers for different buffers used by

 * the GuC. One single gem object contains the ADS struct itself (guc_ads) and

 * all the extra buffers indirectly linked via the ADS struct's entries.

 *

 * Layout of the ADS blob allocated for the GuC:

 *

 *      +---------------------------------------+ <== base

 *      | guc_ads                               |

 *      +---------------------------------------+

 *      | guc_policies                          |

 *      +---------------------------------------+

 *      | guc_gt_system_info                    |

 *      +---------------------------------------+ <== static

 *      | guc_mmio_reg[countA] (engine 0.0)     |

 *      | guc_mmio_reg[countB] (engine 0.1)     |

 *      | guc_mmio_reg[countC] (engine 1.0)     |

 *      |   ...                                 |

 *      +---------------------------------------+ <== dynamic

 *      | padding                               |

 *      +---------------------------------------+ <== 4K aligned

 *      | golden contexts                       |

 *      +---------------------------------------+

 *      | padding                               |

 *      +---------------------------------------+ <== 4K aligned

 *      | private data                          |

 *      +---------------------------------------+

 *      | padding                               |

 *      +---------------------------------------+ <== 4K aligned

 From here on, location is dynamic! Refer to above diagram. */

 Table must be set to invalid values for entries not used */

/*

 * The save/restore register list must be pre-calculated to a temporary

 * buffer of driver defined size before it can be generated in place

 * inside the ADS.

 Arbitrary size, increase as needed */

	/*

	 * The mmio list is built using separate lists within the driver.

	 * It's possible that at some point we may attempt to add the same

	 * register more than once. Do not consider this an error; silently

	 * move on if the register is already in the list.

 Be extra paranoid and include all whitelist registers. */

 add in local MOCS registers */

	/*

	 * Need to actually build the list in order to filter out

	 * duplicates and other such data dependent constructions.

 Class index is checked in class converter */

	/*

	 * Reserve the memory for the golden contexts and point GuC at it but

	 * leave it empty for now. The context data will be filled in later

	 * once there is something available to put there.

	 *

	 * Note that the HWSP and ring context are not included.

	 *

	 * Note also that the storage must be pinned in the GGTT, so that the

	 * address won't change after GuC has been told where to find it. The

	 * GuC will also validate that the LRC base + size fall within the

	 * allowed GGTT range.

		/*

		 * This interface is slightly confusing. We need to pass the

		 * base address of the full golden context and the size of just

		 * the engine state, which is the section of the context image

		 * that starts after the execlists context. This is required to

		 * allow the GuC to restore just the engine state when a

		 * watchdog reset occurs.

		 * We calculate the engine state size by removing the size of

		 * what comes before it in the context image (which is identical

		 * on all engines).

	/*

	 * Go back and fill in the golden context data now that it is

	 * available.

 GuC scheduling policies */

 System info */

 Golden contexts for re-initialising after a watchdog reset */

 ADS */

 MMIO save/restore list */

 Private Data */

/**

 * intel_guc_ads_create() - allocates and initializes GuC ADS.

 * @guc: intel_guc struct

 *

 * GuC needs memory block (Additional Data Struct), where it will store

 * some data. Allocate and initialize such memory block for GuC use.

 Need to calculate the reg state size dynamically: */

 Likewise the golden contexts: */

 Now the total size can be determined: */

	/*

	 * The golden context setup requires the saved engine state from

	 * __engines_record_defaults(). However, that requires engines to be

	 * operational which means the ADS must already have been configured.

	 * Fortunately, the golden context state is not needed until a hang

	 * occurs, so it can be filled in during this late init phase.

/**

 * intel_guc_ads_reset() - prepares GuC Additional Data Struct for reuse

 * @guc: intel_guc struct

 *

 * GuC stores some data in ADS, which might be stale after a reset.

 * Reinitialize whole ADS in case any part of it was corrupted during

 * previous GuC run.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014-2019 Intel Corporation

/**

 * DOC: GuC

 *

 * The GuC is a microcontroller inside the GT HW, introduced in gen9. The GuC is

 * designed to offload some of the functionality usually performed by the host

 * driver; currently the main operations it can take care of are:

 *

 * - Authentication of the HuC, which is required to fully enable HuC usage.

 * - Low latency graphics context scheduling (a.k.a. GuC submission).

 * - GT Power management.

 *

 * The enable_guc module parameter can be used to select which of those

 * operations to enable within GuC. Note that not all the operations are

 * supported on all gen9+ platforms.

 *

 * Enabling the GuC is not mandatory and therefore the firmware is only loaded

 * if at least one of the operations is selected. However, not loading the GuC

 * might result in the loss of some features that do require the GuC (currently

 * just the HuC, but more are expected to land in the future).

	/*

	 * On Gen11+, the value written to the register is passes as a payload

	 * to the FW. However, the FW currently treats all values the same way

	 * (H2G interrupt), so we can just write the value that the HW expects

	 * on older gens.

/*

 * Initialise the GuC parameter block before starting the firmware

 * transfer. These parameters are read by the firmware on startup

 * and cannot be changed thereafter.

/*

 * Initialise the GuC parameter block before starting the firmware

 * transfer. These parameters are read by the firmware on startup

 * and cannot be changed thereafter.

	/*

	 * All SOFT_SCRATCH registers are in FORCEWAKE_GT domain and

	 * they are power context saved so it's ok to release forcewake

	 * when we are done here and take it again at xfer time.

		/*

		 * This is stuff we need to have available at fw load time

		 * if we are planning to enable submission later

 now that everything is perma-pinned, initialize the parameters */

 We need to notify the guc whenever we change the GGTT */

/*

 * This function implements the MMIO based host to GuC interface.

	/*

	 * No GuC command should ever take longer than 10ms.

	 * Fast commands should still complete in 10us.

 Use number of copied dwords as our return value */

 Use data from the GuC response as our return value */

 Make sure to handle only enabled messages */

/**

 * intel_guc_auth_huc() - Send action to GuC to authenticate HuC ucode

 * @guc: intel_guc structure

 * @rsa_offset: rsa offset w.r.t ggtt base of huc vma

 *

 * Triggers a HuC firmware authentication request to the GuC via intel_guc_send

 * INTEL_GUC_ACTION_AUTHENTICATE_HUC interface. This function is invoked by

 * intel_huc_auth().

 *

 * Return:	non-zero code on error

/**

 * intel_guc_suspend() - notify GuC entering suspend state

 * @guc:	the guc

		/*

		 * This H2G MMIO command tears down the GuC in two steps. First it will

		 * generate a G2H CTB for every active context indicating a reset. In

		 * practice the i915 shouldn't ever get a G2H as suspend should only be

		 * called when the GPU is idle. Next, it tears down the CTBs and this

		 * H2G MMIO command completes.

		 *

		 * Don't abort on a failure code from the GuC. Keep going and do the

		 * clean up in santize() and re-initialisation on resume and hopefully

		 * the error here won't be problematic.

 Signal that the GuC isn't running. */

/**

 * intel_guc_resume() - notify GuC resuming from suspend state

 * @guc:	the guc

	/*

	 * NB: This function can still be called even if GuC submission is

	 * disabled, e.g. if GuC is enabled for HuC authentication only. Thus,

	 * if any code is later added here, it must be support doing nothing

	 * if submission is disabled (as per intel_guc_suspend).

/**

 * DOC: GuC Memory Management

 *

 * GuC can't allocate any memory for its own usage, so all the allocations must

 * be handled by the host driver. GuC accesses the memory via the GGTT, with the

 * exception of the top and bottom parts of the 4GB address space, which are

 * instead re-mapped by the GuC HW to memory location of the FW itself (WOPCM)

 * or other parts of the HW. The driver must take care not to place objects that

 * the GuC is going to access in these reserved ranges. The layout of the GuC

 * address space is shown below:

 *

 * ::

 *

 *     +===========> +====================+ <== FFFF_FFFF

 *     ^             |      Reserved      |

 *     |             +====================+ <== GUC_GGTT_TOP

 *     |             |                    |

 *     |             |        DRAM        |

 *    GuC            |                    |

 *  Address    +===> +====================+ <== GuC ggtt_pin_bias

 *   Space     ^     |                    |

 *     |       |     |                    |

 *     |      GuC    |        GuC         |

 *     |     WOPCM   |       WOPCM        |

 *     |      Size   |                    |

 *     |       |     |                    |

 *     v       v     |                    |

 *     +=======+===> +====================+ <== 0000_0000

 *

 * The lower part of GuC Address Space [0, ggtt_pin_bias) is mapped to GuC WOPCM

 * while upper part of GuC Address Space [ggtt_pin_bias, GUC_GGTT_TOP) is mapped

 * to DRAM. The value of the GuC ggtt_pin_bias is the GuC WOPCM size.

/**

 * intel_guc_allocate_vma() - Allocate a GGTT VMA for GuC usage

 * @guc:	the guc

 * @size:	size of area to allocate (both virtual space and memory)

 *

 * This is a wrapper to create an object for use with the GuC. In order to

 * use it inside the GuC, an object needs to be pinned lifetime, so we allocate

 * both some backing storage and a range inside the Global GTT. We must pin

 * it in the GGTT somewhere other than than [0, GUC ggtt_pin_bias) because that

 * range is reserved inside GuC.

 *

 * Return:	A i915_vma if successful, otherwise an ERR_PTR.

/**

 * intel_guc_allocate_and_map_vma() - Allocate and map VMA for GuC usage

 * @guc:	the guc

 * @size:	size of area to allocate (both virtual space and memory)

 * @out_vma:	return variable for the allocated vma pointer

 * @out_vaddr:	return variable for the obj mapping

 *

 * This wrapper calls intel_guc_allocate_vma() and then maps the allocated

 * object with I915_MAP_WB.

 *

 * Return:	0 if successful, a negative errno code otherwise.

/**

 * intel_guc_load_status - dump information about GuC load status

 * @guc: the GuC

 * @p: the &drm_printer

 *

 * Pretty printer for GuC load status.

		/*

		 * Ensure intel_uncore_write_fw can be used rather than

		 * intel_uncore_write.

		/*

		 * This register is used by the i915 and GuC for MMIO based

		 * communication. Once we are in this code CTBs are the only

		 * method the i915 uses to communicate with the GuC so it is

		 * safe to write to this register (a value of 0 is NOP for MMIO

		 * communication). If we ever start mixing CTBs and MMIOs a new

		 * register will have to be chosen. This function is also used

		 * to enforce ordering of a work queue item write and an update

		 * to the process descriptor. When a work queue is being used,

		 * CTBs are also the only mechanism of communication.

 wmb() sufficient for a barrier if in smem */

 SPDX-License-Identifier: MIT

/*

 * Copyright ï¿½ï¿½ 2019 Intel Corporation

	/*

	 * Only the parent gets the creation ref put in the uAPI, the parent

	 * itself is responsible for creation ref put on the children.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

 GuC SLPC is unavailable for pre-Gen12 */

	/*

	 * When the flag bit is set, corresponding value will be read

	 * and applied by SLPC.

	/*

	 * Enabling a param involves setting the enable_id

	 * to 1 and disable_id to 0.

	/*

	 * Disabling a param involves setting the enable_id

	 * to 0 and disable_id to 1.

 Enable only GTPERF task, disable others */

/**

 * intel_guc_slpc_set_max_freq() - Set max frequency limit for SLPC.

 * @slpc: pointer to intel_guc_slpc.

 * @val: frequency (MHz)

 *

 * This function will invoke GuC SLPC action to update the max frequency

 * limit for unslice.

 *

 * Return: 0 on success, non-zero error code on failure.

 Return standardized err code for sysfs calls */

/**

 * intel_guc_slpc_get_max_freq() - Get max frequency limit for SLPC.

 * @slpc: pointer to intel_guc_slpc.

 * @val: pointer to val which will hold max frequency (MHz)

 *

 * This function will invoke GuC SLPC action to read the max frequency

 * limit for unslice.

 *

 * Return: 0 on success, non-zero error code on failure.

 Force GuC to update task data */

/**

 * intel_guc_slpc_set_min_freq() - Set min frequency limit for SLPC.

 * @slpc: pointer to intel_guc_slpc.

 * @val: frequency (MHz)

 *

 * This function will invoke GuC SLPC action to update the min unslice

 * frequency.

 *

 * Return: 0 on success, non-zero error code on failure.

 Return standardized err code for sysfs calls */

/**

 * intel_guc_slpc_get_min_freq() - Get min frequency limit for SLPC.

 * @slpc: pointer to intel_guc_slpc.

 * @val: pointer to val which will hold min frequency (MHz)

 *

 * This function will invoke GuC SLPC action to read the min frequency

 * limit for unslice.

 *

 * Return: 0 on success, non-zero error code on failure.

 Force GuC to update task data */

	/*

	 * Allow GuC to receive ARAT timer expiry event.

	 * This interrupt register is setup by RPS code

	 * when host based Turbo is enabled.

	/*

	 * Softlimits are initially equivalent to platform limits

	 * unless they have deviated from defaults, in which case,

	 * we retain the values and set min/max accordingly.

 Force SLPC to used platform rp0 */

/*

 * intel_guc_slpc_enable() - Start SLPC

 * @slpc: pointer to intel_guc_slpc.

 *

 * SLPC is enabled by setting up the shared data structure and

 * sending reset event to GuC SLPC. Initial data is setup in

 * intel_guc_slpc_init. Here we send the reset event. We do

 * not currently need a slpc_disable since this is taken care

 * of automatically when a reset/suspend occurs and the GuC

 * CTB is destroyed.

 *

 * Return: 0 on success, non-zero error code on failure.

 Ignore efficient freq and set min to platform min */

 Set SLPC max limit to RP0 */

 Revert SLPC min/max to softlimits if necessary */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 GuC and HuC go always in pair, no need to check both */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2016-2019 Intel Corporation

/**

 * DOC: HuC

 *

 * The HuC is a dedicated microcontroller for usage in media HEVC (High

 * Efficiency Video Coding) operations. Userspace can directly use the firmware

 * capabilities by adding HuC specific commands to batch buffers.

 *

 * The kernel driver is only responsible for loading the HuC firmware and

 * triggering its security authentication, which is performed by the GuC. For

 * The GuC to correctly perform the authentication, the HuC binary must be

 * loaded before the GuC one. Loading the HuC is optional; however, not using

 * the HuC might negatively impact power usage and/or performance of media

 * workloads, depending on the use-cases.

 *

 * See https://github.com/intel/media-driver for the latest details on HuC

 * functionality.

/**

 * DOC: HuC Memory Management

 *

 * Similarly to the GuC, the HuC can't do any memory allocations on its own,

 * with the difference being that the allocations for HuC usage are handled by

 * the userspace driver instead of the kernel one. The HuC accesses the memory

 * via the PPGTT belonging to the context loaded on the VCS executing the

 * HuC-specific commands.

	/*

	 * HuC firmware will sit above GUC_GGTT_TOP and will not map

	 * through GTT. Unfortunately, this means GuC cannot perform

	 * the HuC auth. as the rsa offset now falls within the GuC

	 * inaccessible range. We resort to perma-pinning an additional

	 * vma within the accessible range that only contains the rsa

	 * signature. The GuC can use this extra pinning to perform

	 * the authentication since its GGTT offset will be GuC

	 * accessible.

	/*

	 * HuC firmware image is outside GuC accessible range.

	 * Copy the RSA signature out of the image into

	 * a perma-pinned region set aside for it

/**

 * intel_huc_auth() - Authenticate HuC uCode

 * @huc: intel_huc structure

 *

 * Called after HuC and GuC firmware loading during intel_uc_init_hw().

 *

 * This function invokes the GuC action to authenticate the HuC firmware,

 * passing the offset of the RSA signature to intel_guc_auth_huc(). It then

 * waits for up to 50ms for firmware verification ACK.

 Check authentication status, it should be done by now */

/**

 * intel_huc_check_status() - check HuC status

 * @huc: intel_huc structure

 *

 * This function reads status register to verify if HuC

 * firmware was successfully loaded.

 *

 * Returns:

 *  * -ENODEV if HuC is not present on this platform,

 *  * -EOPNOTSUPP if HuC firmware is disabled,

 *  * -ENOPKG if HuC firmware was not installed,

 *  * -ENOEXEC if HuC firmware is invalid or mismatched,

 *  * 0 if HuC firmware is not running,

 *  * 1 if HuC firmware is authenticated and running.

/**

 * intel_huc_load_status - dump information about HuC load status

 * @huc: the HuC

 * @p: the &drm_printer

 *

 * Pretty printer for HuC load status.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2016-2019 Intel Corporation

 Don't enable GuC/HuC on pre-Gen12 */

 Don't enable GuC/HuC on older Gen12 platforms */

 Intermediate platforms are HuC authentication only */

 Default: enable HuC authentication and GuC submission */

/* Reset GuC providing us with fresh state for both GuC and HuC.

/**

 * intel_uc_init_mmio - setup uC MMIO access

 * @uc: the intel_uc structure

 *

 * Setup minimal state necessary for MMIO accesses later in the

 * initialization sequence.

/*

 * Events triggered while CT buffers are disabled are logged in the SCRATCH_15

 * register using the same bits used in the CT message payload. Since our

 * communication channel with guc is turned off at this point, we can save the

 * message and handle it after we turn it back on.

	/*

	 * clear all events, including the ones we're not currently servicing,

	 * to make sure we don't try to process a stale message if we enable

	 * handling of more events later.

 we need communication to be enabled to reply to GuC */

 check for mmio messages received before/during the CT enable */

 check for CT messages received before we enabled interrupts */

	/*

	 * Events generated during or after CT disable are logged by guc in

	 * via mmio. Make sure the register is clear before disabling CT since

	 * all events we cared about have already been processed via CT.

	/*

	 * Check for messages received during/after the CT disable. We do not

	 * expect any messages to have arrived via CT between the interrupt

	 * disable and the CT disable because GuC should've been idle until we

	 * triggered the CT disable protocol.

 Make sure we transition out of transient "SELECTED" state */

 Initialize and verify the uC regs related to uC positioning in WOPCM */

	/*

	 * We can silently continue without GuC only if it was never enabled

	 * before on this system after reboot, otherwise we risk GPU hangs.

	 * To check if GuC was loaded before we look at WOPCM registers.

 WaEnableuKernelHeaderValidFix:skl */

 WaEnableGuCBootHashCheckNotSet:skl,bxt,kbl */

		/*

		 * Always reset the GuC just before (re)loading, so

		 * that the state and timing are fairly predictable

 Did we succeded or run out of retries? */

	/*

	 * We've failed to load the firmware :(

 We want to run without GuC submission */

 We want to keep KMS alive */

/**

 * intel_uc_reset_prepare - Prepare for reset

 * @uc: the intel_uc structure

 *

 * Preparing for full gpu reset.

 Nothing to do if GuC isn't supported */

 Firmware expected to be running when this function is called */

 Firmware can not be running when this function is called  */

 Firmware expected to be running when this function is called */

 Firmware can not be running when this function is called  */

	/*

	 * Wait for any outstanding CTB before tearing down communication /w the

	 * GuC.

 Make sure we enable communication if and only if it's disabled */

	/* If we are only resuming GuC communication but not reloading

	 * GuC, we need to ensure the ARAT timer interrupt is enabled

	 * again. In case of GuC reload, it is enabled during SLPC enable.

	/*

	 * When coming out of S3/S4 we sanitize and re-init the HW, so

	 * communication is already re-enabled at this point.

	/*

	 * During runtime resume we don't sanitize, so we need to re-init

	 * communication as well.

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014-2019 Intel Corporation

/**

 * intel_huc_fw_upload() - load HuC uCode to device

 * @huc: intel_huc structure

 *

 * Called from intel_uc_init_hw() during driver load, resume from sleep and

 * after a GPU reset. Note that HuC must be loaded before GuC.

 *

 * The firmware image should have already been fetched into memory, so only

 * check that fetch succeeded, and then transfer the image to the h/w.

 *

 * Return:	non-zero code on error

 HW doesn't look at destination address for HuC, so set it to 0 */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2014 Intel Corporation

/**

 * DOC: GuC-based command submission

 *

 * The Scratch registers:

 * There are 16 MMIO-based registers start from 0xC180. The kernel driver writes

 * a value to the action register (SOFT_SCRATCH_0) along with any data. It then

 * triggers an interrupt on the GuC via another register write (0xC4C8).

 * Firmware writes a success/fail code back to the action register after

 * processes the request. The kernel driver polls waiting for this update and

 * then proceeds.

 *

 * Command Transport buffers (CTBs):

 * Covered in detail in other sections but CTBs (Host to GuC - H2G, GuC to Host

 * - G2H) are a message interface between the i915 and GuC.

 *

 * Context registration:

 * Before a context can be submitted it must be registered with the GuC via a

 * H2G. A unique guc_id is associated with each context. The context is either

 * registered at request creation time (normal operation) or at submission time

 * (abnormal operation, e.g. after a reset).

 *

 * Context submission:

 * The i915 updates the LRC tail value in memory. The i915 must enable the

 * scheduling of the context within the GuC for the GuC to actually consider it.

 * Therefore, the first time a disabled context is submitted we use a schedule

 * enable H2G, while follow up submissions are done via the context submit H2G,

 * which informs the GuC that a previously enabled context has new work

 * available.

 *

 * Context unpin:

 * To unpin a context a H2G is used to disable scheduling. When the

 * corresponding G2H returns indicating the scheduling disable operation has

 * completed it is safe to unpin the context. While a disable is in flight it

 * isn't safe to resubmit the context so a fence is used to stall all future

 * requests of that context until the G2H is returned.

 *

 * Context deregistration:

 * Before a context can be destroyed or if we steal its guc_id we must

 * deregister the context with the GuC via H2G. If stealing the guc_id it isn't

 * safe to submit anything to this guc_id until the deregister completes so a

 * fence is used to stall all requests associated with this guc_id until the

 * corresponding G2H returns indicating the guc_id has been deregistered.

 *

 * submission_state.guc_ids:

 * Unique number associated with private GuC context data passed in during

 * context registration / submission / deregistration. 64k available. Simple ida

 * is used for allocation.

 *

 * Stealing guc_ids:

 * If no guc_ids are available they can be stolen from another context at

 * request creation time if that context is unpinned. If a guc_id can't be found

 * we punt this problem to the user as we believe this is near impossible to hit

 * during normal use cases.

 *

 * Locking:

 * In the GuC submission code we have 3 basic spin locks which protect

 * everything. Details about each below.

 *

 * sched_engine->lock

 * This is the submission lock for all contexts that share an i915 schedule

 * engine (sched_engine), thus only one of the contexts which share a

 * sched_engine can be submitting at a time. Currently only one sched_engine is

 * used for all of GuC submission but that could change in the future.

 *

 * guc->submission_state.lock

 * Global lock for GuC submission state. Protects guc_ids and destroyed contexts

 * list.

 *

 * ce->guc_state.lock

 * Protects everything under ce->guc_state. Ensures that a context is in the

 * correct state before issuing a H2G. e.g. We don't issue a schedule disable

 * on a disabled context (bad idea), we don't issue a schedule enable when a

 * schedule disable is in flight, etc... Also protects list of inflight requests

 * on the context and the priority management state. Lock is individual to each

 * context.

 *

 * Lock ordering rules:

 * sched_engine->lock -> ce->guc_state.lock

 * guc->submission_state.lock -> ce->guc_state.lock

 *

 * Reset races:

 * When a full GT reset is triggered it is assumed that some G2H responses to

 * H2Gs can be lost as the GuC is also reset. Losing these G2H can prove to be

 * fatal as we do certain operations upon receiving a G2H (e.g. destroy

 * contexts, release guc_ids, etc...). When this occurs we can scrub the

 * context state and cleanup appropriately, however this is quite racey.

 * To avoid races, the reset code must disable submission before scrubbing for

 * the missing G2H, while the submission code must check for submission being

 * disabled and skip sending H2Gs and updating context states when it is. Both

 * sides must also make sure to hold the relevant locks.

 GuC Virtual Engine */

 bytes */

/*

 * We reserve 1/16 of the guc_ids for multi-lrc as these need to be contiguous

 * per the GuC submission interface. A different allocation algorithm is used

 * (bitmap vs. ida) between multi-lrc and single-lrc hence the reason to

 * partition the guc_id space. We believe the number of multi-lrc contexts in

 * use should be low and 1/16 should be sufficient. Minimum of 32 guc_ids for

 * multi-lrc.

/*

 * Below is a set of functions which control the GuC scheduling state which

 * require a lock.

	/*

	 * XXX: Kernel contexts can have SCHED_STATE_NO_LOCK_REGISTERED after

	 * suspend.

 Overflow check */

 Underflow check */

/*

 * When using multi-lrc submission a scratch memory area is reserved in the

 * parent's context state for the process descriptor, work queue, and handshake

 * between the parent + children contexts to insert safe preemption points

 * between each of the BBs. Currently the scratch area is sized to a page.

 *

 * The layout of this scratch area is below:

 * 0						guc_process_desc

 * + sizeof(struct guc_process_desc)		child go

 * + CACHELINE_BYTES				child join[0]

 * ...

 * + CACHELINE_BYTES				child join[n - 1]

 * ...						unused

 * PARENT_SCRATCH_SIZE / 2			work queue start

 * ...						work queue

 * PARENT_SCRATCH_SIZE - 1			work queue end

	/*

	 * Need to subtract LRC_STATE_OFFSET here as the

	 * parallel.guc.parent_page is the offset into ce->state while

	 * ce->lrc_reg_reg is ce->state + LRC_STATE_OFFSET.

	/*

	 * Check for space in work queue. Caching a value of head pointer in

	 * intel_context structure in order reduce the number accesses to shared

	 * GPU memory which may be across a PCIe bus.

		/*

		 * xarray API doesn't have xa_erase_irqsave wrapper, so calling

		 * the lower level functions directly.

	/*

	 * xarray API doesn't have xa_save_irqsave wrapper, so calling the

	 * lower level functions directly.

	/*

	 * We always loop when a send requires a reply (i.e. g2h_len_dw > 0),

	 * so we don't handle the case where we don't get a reply because we

	 * aborted the send due to the channel being busy.

	/*

	 * Corner case where requests were sitting in the priority list or a

	 * request resubmitted after the context was banned.

	/*

	 * The request / context will be run on the hardware when scheduling

	 * gets enabled in the unblock. For multi-lrc we still submit the

	 * context to move the LRC tails.

		/*

		 * Without multi-lrc KMD does the submission step (moving the

		 * lrc tail) so enabling scheduling is sufficient to submit the

		 * context. This isn't the case in multi-lrc submission as the

		 * GuC needs to move the tails, hence the need for another H2G

		 * to submit a multi-lrc context after enabling scheduling.

	/*

	 * Ensure WQI are visible before updating tail

 Ensure context is in correct state updating work queue */

 Insert NOOP if this work queue item will wrap the tail pointer. */

 fence_id */

	/*

	 * We expect the front end (execbuf IOCTL) to set this flag on the last

	 * request generated from a multi-BB submission. This indicates to the

	 * backend (GuC interface) that we should submit this context thus

	 * submitting all the requests generated in parallel.

				/*

				 * We need to coalesce all multi-lrc requests in

				 * a relationship into a single H2G. We are

				 * guaranteed that all of these requests will be

				 * submitted sequentially.

 Unexpected */

 Unexpected */

 Unexpected */

		/*

		 * Corner case where the ref count on the object is zero but and

		 * deregister G2H was lost. In this case we don't touch the ref

		 * count and finish the destroy of the context.

		/*

		 * Once we are at this point submission_disabled() is guaranteed

		 * to be visible to all callers who set the below flags (see above

		 * flush and flushes in reset_prepare). If submission_disabled()

		 * is set, the caller shouldn't set these flags.

 Not mutualy exclusive with above if statement. */

 Make sure callback visible */

 And kick in case we missed a new request submission. */

 Reset called during driver load? GuC not yet initialised! */

 Flush IRQ handler */

	/*

	 * Handle any outstanding G2Hs before reset. Call IRQ handler directly

	 * each pass as interrupt have been disabled. We always scrub for

	 * outstanding G2H as it is possible for outstanding_submission_g2h to

	 * be incremented after the context state update.

	/*

	 * We want a simple context + ring to execute the breadcrumb update.

	 * We cannot rely on the context being intact across the GPU hang,

	 * so clear it and rebuild just what we need for the breadcrumb.

	 * All pending requests for this context will be zapped, and any

	 * future request will be after userspace has had the opportunity

	 * to recreate its own state.

 Rerun the request; its payload has been neutered (if guilty). */

 Push the request back into the queue for later resubmission. */

	/*

	 * GuC will implicitly mark the context as non-schedulable when it sends

	 * the reset notification. Make sure our state reflects this change. The

	 * context will be marked enabled on resubmission.

	 *

	 * XXX: If the context is reset as a result of the request cancellation

	 * this G2H is received after the schedule disable complete G2H which is

	 * wrong as this creates a race between the request cancellation code

	 * re-submitting the context and this G2H handler. This is a bug in the

	 * GuC but can be worked around in the meantime but converting this to a

	 * NOP if a pending enable is in flight as this indicates that a request

	 * cancellation has occurred.

	/*

	 * For each context in the relationship find the hanging request

	 * resetting each context / request as needed

 Reset called during driver load? GuC not yet initialised! */

 GuC is blown away, drop all references to contexts */

 Mark all executing requests as skipped. */

 Can be called during boot if GuC fails to load */

	/*

	 * Before we call engine->cancel_requests(), we should have exclusive

	 * access to the submission state. This is arranged for us by the

	 * caller disabling the interrupt generation, the tasklet and other

	 * threads that may then access the same state, giving us a free hand

	 * to reset state. However, we still need to let lockdep be aware that

	 * we know this state may be accessed in hardirq context, so we

	 * disable the irq around this manipulation and we want to keep

	 * the spinlock focused on its duties and not accidentally conflate

	 * coverage to the submission's irq state. (Similarly, although we

	 * shouldn't need to disable irq around the manipulation of the

	 * submission's irq state, we also wish to remind ourselves that

	 * it is irq state.)

 Flush the queued requests to the timeline list (for retiring). */

 Remaining _unready_ requests will be nop'ed when submitted */

 GuC is blown away, drop all references to contexts */

 Reset called during driver load or during wedge? */

	/*

	 * Technically possible for either of these values to be non-zero here,

	 * but very unlikely + harmless. Regardless let's add a warn so we can

	 * see in CI if this happens frequently / a precursor to taking down the

	 * machine.

/*

 * Set up the memory resources to be shared with the GuC (via the GGTT)

 * at firmware loading time.

	/*

	 * Keep static analysers happy, let them know that we allocated the

	 * vma after testing that it didn't exist earlier.

 Will be called from irq-context when using foreign fences. */

 Indidcates newly assigned guc_id */

	/*

	 * -EAGAIN indicates no guc_id are available, let's retire any

	 * outstanding requests to see if that frees up a guc_id. If the first

	 * retire didn't help, insert a sleep with the timeslice duration before

	 * attempting to retire more requests. Double the sleep period each

	 * subsequent pass before finally giving up. The sleep period has max of

	 * 100ms and minimum of 1ms.

 NB: For both of these, zero means disabled. */

	/*

	 * Ensure LRC + CT vmas are is same region as write barrier is done

	 * based on CT vma region.

	/*

	 * If context is a parent, we need to register a process descriptor

	 * describing a work queue and register all child contexts.

	/*

	 * The context_lookup xarray is used to determine if the hardware

	 * context is currently registered. There are two cases in which it

	 * could be registered either the guc_id has been stolen from another

	 * context or the lrc descriptor address of this context has changed. In

	 * either case the context needs to be deregistered with the GuC before

	 * registering this context.

 Seal race with Reset */

 Will get registered later */

		/*

		 * If stealing the guc_id, this ce has the same guc_id as the

		 * context whose guc_id was stolen.

 Will get registered later */

 Will get registered later */

	/*

	 * GuC context gets pinned in guc_request_alloc. See that function for

	 * explaination of why.

 ce->guc_id.id not stable */

	/*

	 * This fence is always complete unless a pending schedule disable is

	 * outstanding. We arm the fence here and complete it when we receive

	 * the pending schedule disable complete message.

	/*

	 * We add +2 here as the schedule disable complete CTB handler calls

	 * intel_context_sched_disable_unpin (-2 to pin_count).

		/*

		 * XXX: Racey if context is reset, see comment in

		 * __guc_reset_context().

		/*

		 * We add +2 here as the schedule disable complete CTB handler

		 * calls intel_context_sched_disable_unpin (-2 to pin_count).

		/*

		 * In addition to disabling scheduling, set the preemption

		 * timeout to the minimum value (1 us) so the banned context

		 * gets kicked off the HW ASAP.

	/*

	 * We have to check if the context has been disabled by another thread,

	 * check if submssion has been disabled to seal a race with reset and

	 * finally check if any more requests have been committed to the

	 * context ensursing that a request doesn't slip through the

	 * 'context_pending_disable' fence.

 Seal race with Reset */

	/*

	 * If the guc_id is invalid this context has been stolen and we can free

	 * it immediately. Also can be freed immediately if the context is not

	 * registered with the GuC or the GuC is in the middle of a reset.

	/*

	 * We use a worker to issue the H2G to deregister the context as we can

	 * take the GT PM for the first time which isn't allowed from an atomic

	 * context.

 Overflow protection */

 Underflow protection */

 Lower value is higher priority */

 Prevent further __await_execution() registering a cb, then flush */

	/*

	 * Use an IRQ to ensure locking order of sched_engine->lock ->

	 * ce->guc_state.lock is preserved.

	/*

	 * Flush enough space to reduce the likelihood of waiting after

	 * we start building the request - in which case we will just

	 * have to repeat work.

	/*

	 * Note that after this point, we have committed to using

	 * this request as it is being used to both track the

	 * state of engine initialisation and liveness of the

	 * golden renderstate above. Think twice before you try

	 * to cancel/unwind this request now.

 Unconditionally invalidate GPU caches and TLBs. */

	/*

	 * Call pin_guc_id here rather than in the pinning step as with

	 * dma_resv, contexts can be repeatedly pinned / unpinned trashing the

	 * guc_id and creating horrible race conditions. This is especially bad

	 * when guc_id are being stolen due to over subscription. By the time

	 * this function is reached, it is guaranteed that the guc_id will be

	 * persistent until the generated request is retired. Thus, sealing these

	 * race conditions. It is still safe to fail here if guc_id are

	 * exhausted and return -EAGAIN to the user indicating that they can try

	 * again in the future.

	 *

	 * There is no need for a lock here as the timeline mutex ensures at

	 * most one context can be executing this code path at once. The

	 * guc_id_ref is incremented once for every request in flight and

	 * decremented on each retire. When it is zero, a lock around the

	 * increment (in pin_guc_id) is needed to seal a race with unpin_guc_id.

 returns 1 if new guc_id assigned */

 unwind */

 GPU will be reset */

	/*

	 * We block all requests on this context if a G2H is pending for a

	 * schedule disable or context deregistration as the GuC will fail a

	 * schedule enable or context registration if either G2H is pending

	 * respectfully. Once a G2H returns, the fence is released that is

	 * blocking these requests (see guc_signal_context_fence).

/*

 * The below override of the breadcrumbs is enabled when the user configures a

 * context for parallel submission (multi-lrc, parent-child).

 *

 * The overridden breadcrumbs implements an algorithm which allows the GuC to

 * safely preempt all the hw contexts configured for parallel submission

 * between each BB. The contract between the i915 and GuC is if the parent

 * context can be preempted, all the children can be preempted, and the GuC will

 * always try to preempt the parent before the children. A handshake between the

 * parent / children breadcrumbs ensures the i915 holds up its end of the deal

 * creating a window to preempt between each set of BBs.

	/*

	 * In GuC submission mode we do not know which physical engine a request

	 * will be scheduled on, this creates a problem because the breadcrumb

	 * interrupt is per physical engine. To work around this we attach

	 * requests and direct all breadcrumb interrupts to the first instance

	 * of an engine per class. In addition all breadcrumb interrupts are

	 * enabled / disabled across an engine class in unison.

 Short circuit function */

	/*

	 * Poison residual state on resume, in case the suspend didn't!

	 *

	 * We have to assume that across suspend/resume (or other loss

	 * of control) that the contents of our pinned buffers has been

	 * lost, replaced by garbage. Since this doesn't always happen,

	 * let's poison such state so that we more quickly spot when

	 * we falsely assume it has been preserved.

	/*

	 * The kernel_context HWSP is stored in the status_page. As above,

	 * that may be lost on resume/initialisation, and so we need to

	 * reset the value in the HWSP.

 And scrub the dirty cachelines for the HWSP */

 HWSTAM */

 make sure all descriptors are clean... */

	/*

	 * Some contexts might have been pinned before we enabled GuC

	 * submission, so we need to add them to the GuC bookeeping.

	 * Also, after a reset the of the GuC we want to make sure that the

	 * information shared with GuC is properly reset. The kernel LRCs are

	 * not attached to the gem_context, so they need to be added separately.

	 *

	 * Note: we purposefully do not check the return of guc_lrc_desc_pin,

	 * because that function can only fail if a reset is just starting. This

	 * is at the end of reset so presumably another reset isn't happening

	 * and even it did this code would be run again.

 no longer in control, nothing to sanitize */

 Default vfuncs which can be overridden by each engine. */

	/*

	 * TODO: GuC supports timeslicing and semaphores as well, but they're

	 * handled by the firmware so some minor tweaks are required before

	 * enabling.

	 *

	 * engine->flags |= I915_ENGINE_HAS_SEMAPHORES;

 flush the callback */

	/*

	 * The setup relies on several assumptions (e.g. irqs always enabled)

	 * that are only valid on gen11+

 Finally, take ownership and responsibility for cleanup! */

 Note: By the time we're here, GuC may have already been reset */

 GuC submission is unavailable for pre-Gen11 */

		/*

		 * Previous owner of this guc_id has been deregistered, now safe

		 * register this context.

 Context has been destroyed */

		/*

		 * Unpin must be done before __guc_signal_context_fence,

		 * otherwise a race exists between the requests getting

		 * submitted + retired before this unpin completes resulting in

		 * the pin_count going to zero and the context still being

		 * enabled.

	/*

	 * XXX: Racey if request cancellation has occurred, see comment in

	 * __guc_reset_context().

 Class index is checked in class converter */

 Reset called during driver load? GuC not yet initialised! */

 Can only cope with one hang at a time... */

 Reset called during driver load? GuC not yet initialised! */

 Wait on children */

 Turn off preemption */

 Tell children go */

 Jump to batch */

 Signal parent */

 Wait on parent for go */

 Turn off preemption */

 Jump to batch */

 Wait on children */

 Turn on preemption */

 Tell children go */

/*

 * If this true, a submission of multi-lrc requests had an error and the

 * requests need to be skipped. The front end (execuf IOCTL) should've called

 * i915_request_skip which squashes the BB but we still need to emit the fini

 * breadrcrumbs seqno write. At this point we don't know how many of the

 * requests in the multi-lrc submission were generated so we can't do the

 * handshake between the parent and children (e.g. if 4 requests should be

 * generated but 2nd hit an error only 1 would be seen by the GuC backend).

 * Simply skip the handshake, but still emit the breadcrumbd seqno, if an error

 * has occurred on any of the requests in submission / relationship.

		/*

		 * NOP everything in __emit_fini_breadcrumb_parent_no_preempt_mid_batch,

		 * the -6 comes from the length of the emits below.

 Emit fini breadcrumb */

 User interrupt */

 Turn on preemption */

 Signal parent */

 Wait parent on for go */

		/*

		 * NOP everything in __emit_fini_breadcrumb_child_no_preempt_mid_batch,

		 * the -6 comes from the length of the emits below.

 Emit fini breadcrumb */

 User interrupt */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2017-2018 Intel Corporation

/*

 * Copyright Â© 2017 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 Leave the fence for the caller to free it after testing */

 Test i915_sw_fence signaling and completion testing */

 Test detection of cycles within the i915_sw_fence graphs */

 Test i915_sw_fence (A) waiting on an event source (B) */

 Test a chain of fences, A waits on B who waits on C */

 Test multiple fences (AB) waiting on a single event (C) */

 Test multiple event sources (A,B) for a single fence (C) */

 Test a long chain of fences */

 Test use of i915_sw_fence as an interprocess signaling mechanism */

 use a completion to avoid chicken-and-egg testing */

 We round the timeout for the fence up to the next second */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2018 Intel Corporation

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2019 Intel Corporation

 Quick check we can create a perf stream */

 Check that the GPU delays matches expectations */

 Check that the delay does not clobber user context state (GPR) */

 Poison the ce->vm so we detect writes not to the GGTT gt->scratch */

 Fill the 16 qword [32 dword] GPR with a known unlikely value */

 Execute the GPU delay */

 Read the GPR back, using the pinned global HWSP for convenience */

 Verify that the GPR contain our expected values */

 Verify that the user's scratch page was not used for GPR storage */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2020 Intel Corporation

 Discrete cards require hwmon integration */

 convert to uJ */

/*

 * Copyright Â© 2017 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 Small library of different fence types useful for writing tests */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 No polluting the memory region between tests */

 Reserve a bunch of ranges within the region */

 Allow for some really big holes */

 XXX: maybe sanity check the block range here? */

 Try to see if we can allocate from the remaining space */

 Min size */

 Max size */

 Internal fragmentation should not bleed into the object size */

	/*

	 * Try to fragment the address space, such that half of it is free, but

	 * the max contiguous block size is SZ_64K.

 Make sure we can still allocate all the fragmented space */

	/*

	 * Even though we have enough free space, we don't have a big enough

	 * contiguous block. Make sure that holds true.

	/*

	 * Sanity check we can still allocate everything even if the

	 * mm.max_order != mm.size. i.e our starting address space size is not a

	 * power-of-two.

	/*

	 * While we should be able allocate everything without any flag

	 * restrictions, if we consider I915_BO_ALLOC_CONTIGUOUS then we are

	 * actually limited to the largest power-of-two for the region size i.e

	 * max_order, due to the inner workings of the buddy allocator. So make

	 * sure that does indeed hold true.

	/*

	 * While we may create very large contiguous blocks, we may need

	 * to break those down for consumption elsewhere. In particular,

	 * dma-mapping with scatterlist elements have an implicit limit of

	 * UINT_MAX on each element.

		/*

		 * Alternate between cleared and uncleared allocations, while

		 * also dirtying the pages each time to check that the pages are

		 * always cleared if requested, since we should get some overlap

		 * of the underlying pages, if not all, since we are the only

		 * user.

 rng placeholder */

 cl */

 Put the pages into a known state -- from the gpu for added fun */

 A random multiple of u32, picked between [64, PAGE_SIZE - 64] */

		/*

		 * Sample random dw -- don't waste precious time reading every

		 * single dw.

 Stolen memory */

 ignore the impossible to protect our sanity */

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 Basic preliminary test to create a request and let it loose! */

 Submit a request, then wait upon it */

 Submit a request, treat it as a fence and wait upon it */

 Simulate preemption by manual reordering */

	/*

	 * A very simple test to catch the most egregious of list handling bugs.

	 *

	 * At its heart, we simply create oodles of requests running across

	 * multiple kthreads and enable signaling on them, for the sole purpose

	 * of stressing our breadcrumb handling. The only inspection we do is

	 * that the fences were marked as signaled.

	/*

	 * Smoketest our breadcrumb/signal handling for requests across multiple

	 * threads. A very simple test to only catch the most egregious of bugs.

	 * See __igt_breadcrumbs_smoketest();

 start all threads before we begin */

	/*

	 * Submit various sized batches of empty requests, to each engine

	 * (individually), and wait for the batch to complete. We can check

	 * the overhead of submitting requests to the hardware.

				/*

				 * This space is left intentionally blank.

				 *

				 * We do not actually want to perform any

				 * action with this request, we just want

				 * to measure the latency in allocation

				 * and submission of our breadcrumbs -

				 * ensuring that the bare request is sufficient

				 * for the system to work (i.e. proper HEAD

				 * tracking of the rings, interrupt handling,

				 * etc). It also gives us the lowest bounds

				 * for latency.

	/*

	 * Check cancellation of requests. We expect to be able to immediately

	 * cancel active requests, even if they are currently on the GPU.

 Force the wait wait now to avoid including it in the benchmark */

	/*

	 * Submit various sized batches of empty requests, to each engine

	 * (individually), and wait for the batch to complete. We can check

	 * the overhead of submitting requests to the hardware.

 Warmup / preload */

 terminate early in case of error */

	/*

	 * Check we can submit requests to all engines simultaneously. We

	 * send a recursive batch to each engine - checking that we don't

	 * block doing so, and that they don't complete too soon.

	/*

	 * Check we can submit requests to all engines sequentially, such

	 * that each successive request waits for the earlier ones. This

	 * tests that we don't execute requests out of order, even though

	 * they are running on independent engines.

	/*

	 * Create a spinner running for eternity on each engine. If a second

	 * spinner is incorrectly placed on the same engine, it will not be

	 * able to start in time.

 no preemption */

 Occupy this engine for the whole test */

	/*

	 * Check we can submit requests to all engines concurrently. This

	 * tests that we load up the system maximally.

 start all threads before we kthread_stop() */

	/*

	 * Before execlists, all contexts share the same ringbuffer. With

	 * execlists, each context/engine has a separate ringbuffer and

	 * for the purposes of this test, inexhaustible.

	 *

	 * For the global ringbuffer though, we have to be very careful

	 * that we do not wrap while preventing the execution of requests

	 * with a unsignaled fence.

 leave half spare, in case of emergency! */

	/*

	 * Smoketest our breadcrumb/signal handling for requests across multiple

	 * threads. A very simple test to only catch the most egregious of bugs.

	 * See __igt_breadcrumbs_smoketest();

	 *

	 * On real hardware this time.

 One ring interleaved between requests from all cpus */

 start all threads before we begin */

 flush the update to the cache, and beyond */

	/*

	 * Measure how many cycles it takes for the HW to detect the change

	 * in a semaphore value.

	 *

	 *    A: read CS_TIMESTAMP from CPU

	 *    poke semaphore

	 *    B: read CS_TIMESTAMP on GPU

	 *

	 * Semaphore latency: B - A

	/*

	 * Measure how long it takes for us to submit a request while the

	 * engine is idle, but is resting in our context.

	 *

	 *    A: read CS_TIMESTAMP from CPU

	 *    submit request

	 *    B: read CS_TIMESTAMP on GPU

	 *

	 * Submission latency: B - A

	/*

	 * Measure how long it takes for us to submit a request while the

	 * engine is busy, polling on a semaphore in our context. With

	 * direct submission, this will include the cost of a lite restore.

	 *

	 *    A: read CS_TIMESTAMP from CPU

	 *    submit request

	 *    B: read CS_TIMESTAMP on GPU

	 *

	 * Submission latency: B - A

	/*

	 * Measure how long it takes to advance from one request into the

	 * next. Between each request we flush the GPU caches to memory,

	 * update the breadcrumbs, and then invalidate those caches.

	 * We queue up all the requests to be submitted in one batch so

	 * it should be one set of contiguous measurements.

	 *

	 *    A: read CS_TIMESTAMP on GPU

	 *    advance request

	 *    B: read CS_TIMESTAMP on GPU

	 *

	 * Request latency: B - A

	/*

	 * Measure how long it takes to advance from one request in one

	 * context to a request in another context. This allows us to

	 * measure how long the context save/restore take, along with all

	 * the inter-context setup we require.

	 *

	 *    A: read CS_TIMESTAMP on GPU

	 *    switch context

	 *    B: read CS_TIMESTAMP on GPU

	 *

	 * Context switch latency: B - A

	/*

	 * We measure two latencies while triggering preemption. The first

	 * latency is how long it takes for us to submit a preempting request.

	 * The second latency is how it takes for us to return from the

	 * preemption back to the original context.

	 *

	 *    A: read CS_TIMESTAMP from CPU

	 *    submit preemption

	 *    B: read CS_TIMESTAMP on GPU (in preempting context)

	 *    context switch

	 *    C: read CS_TIMESTAMP on GPU (in original context)

	 *

	 * Preemption dispatch latency: B - A

	 * Preemption switch latency: C - B

 be safe, be strong */

	/*

	 * Measure how long it takes for the signal (interrupt) to be

	 * sent from the GPU to be processed by the CPU.

	 *

	 *    A: read CS_TIMESTAMP on GPU

	 *    signal

	 *    B: read CS_TIMESTAMP from CPU

	 *

	 * Completion latency: B - A

 Pin the frequency to max */

 per-engine CS timestamp, semaphores */

 disable cstates */

 disable cstates */

 start all threads before we kthread_stop() */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2018 Intel Corporation

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

/*

 * Copyright Â© 2017 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 Embed the line number into the parameter name so that we can order tests */

 Tests are listed in order in i915_*_selftests.h */

 The selftests expect an idle system */

 The selftests expect an idle system */

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 4K, 64K, 2M */

 count should be less than 20 to prevent overflowing sg->length */

	/* Construct a table where each scatterlist contains different number

	 * of entries. The idea is to check that we can iterate the individual

	 * pages from inside the coalesced lists.

 Nobody expects the Sparse Memmap! */

 approximating a 4GiB object */

 Test at least one continuation before accepting oom */

 not prime! */

 Test at least one continuation before accepting oom */

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 Check that the table is watertight */

 Check that the table never goes backwards */

 Check that the entry is valid */

 vlv/chv with their pcu behave differently wrt reads */

	/*

	 * Not quite as reliable across the gen as one would hope.

	 *

	 * Either our theory of operation is incorrect, or there remain

	 * external parties interfering with the powerwells.

	 *

	 * https://bugs.freedesktop.org/show_bug.cgi?id=110210

 We have to pick carefully to get the exact behaviour we need */

 Flush the forcewake release (delayed onto a timer) */

 We then expect the read to return 0 outside of the fw */

	/*

	 * This test may lockup the machine or cause GPU hangs afterwards.

 Confirm the table we load is still valid */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2018 Intel Corporation

 not reached */

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 NB the i915->requests slab cache is enlarged to fit mock_request */

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 HACK to disable iommu for the fake device; force identity mapping */

 Using the global GTT may ask questions about KMS users, so prepare */

 disable; no hw support */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

#include "gt/intel_engine_user.h"

		/*

		 * Enable force pre-emption on time slice expiration

		 * together with engine reset on pre-emption timeout.

		 * This is required to make the GuC notice and reset

		 * the single hanging context.

		 * Also, reduce the preemption timeout to something

		 * small to speed the test up.

 Restore the original policies */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2018 Intel Corporation

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2018 Intel Corporation

 XXX: fsck. needs some more thought... */

	/*

	 * As a final sting in the tail, invalidate stolen. Under a real S4,

	 * stolen is lost and needs to be refilled on resume. However, under

	 * CI we merely do S4-device testing (as full S4 is too unreliable

	 * for automated testing across a cluster), so to simulate the effect

	 * of stolen being trashed across S4, we trash it ourselves.

	/*

	 * Both suspend and hibernate follow the same wakeup path and assume

	 * that runtime-pm just works.

 Here be dragons! Note that with S3RST any S3 may become S4! */

 Here be dragons! */

 Lock the objects, twice for good measure (-EALREADY handling) */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2018 Intel Corporation

 Check that we get a callback when requests retire upon waiting */

 Check that we get a callback when requests are indirectly retired */

 waits for & retires all requests */

 Check that we get a callback when requests retire upon waiting */

 serialise with add_active_barriers */

 serialise with fence->cb_list */

 Wait for all active callbacks */

 And wait for the retire callback */

 ... which may have been on a thread instead */

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 Manual checks, will be reinforced by i915_vma_compare! */

	/* Exercise creating many vma amonst many objections, checking the

	 * vma creation and lookup routines.

 Final pass to lookup all created contexts */

		/* Misusing BIAS is a programming error (it is not controllable

		 * from userspace) so when debugging is enabled, it explodes.

		 * However, the tests are still quite interesting for checking

		 * variable start, end and size.

	/* Exercise all the weird and wonderful i915_vma_pin requests,

	 * focusing on error handling of boundary conditions.

	/* Create VMA for many different combinations of planes and check

	 * that the page layout within the rotated VMA match our expectations.

 prime! */

	/* Create lots of different VMA for the object and check that

	 * we are returned the same VMA when we later request the same range.

 exercise both create/lookup */

 Check that we did create the whole object mapping */

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2018 Intel Corporation

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 Preallocate the "backing storage" */

 Allocate a ppggt and try to fill the entire range */

	/*

	 * While we only allocate the page tables here and so we could

	 * address a much larger GTT than we could actually fit into

	 * RAM, a practical limit is the amount of physical pages in the system.

	 * This should ensure that we do not run into the oomkiller during

	 * the test and take down the machine wilfully.

 Check we can allocate the entire range */

 Check we can incrementally allocate the entire range */

 Keep creating larger objects until one cannot fit into the hole */

		/* Ignore allocation failures (i.e. don't report them as

		 * a test failure) as we are purposefully allocating very

		 * large objects without checking that we have sufficient

		 * memory. We expect to hit -ENOMEM.

 quit */

 Try binding many VMA working inwards from either edge */

			/* Align differing sized objects against the edges, and

			 * check we don't walk off into the void when binding

			 * them into the GTT.

 Try binding a single VMA in different positions within the hole */

 Insert a pair of pages across every pot boundary within the hole */

 Keep creating larger objects until one cannot fit into the hole */

		/* Ignore allocation failures (i.e. don't report them as

		 * a test failure) as we are purposefully allocating very

		 * large objects without checking that we have sufficient

		 * memory. We expect to hit -ENOMEM.

 Keep creating larger objects until one cannot fit into the hole */

		/*

		 * Since we are injecting allocation faults at random intervals,

		 * wait for this allocation to complete before we change the

		 * faultinjection.

	/*

	 * Catch the case which shrink_hole seems to miss. The setup here

	 * requires invoking the shrinker as we do the alloc_pt/alloc_pd, while

	 * ensuring that all vma assiocated with the respective pd/pdp are

	 * unpinned at the time.

 Should now be ripe for purging */

 As we have manipulated the drm_mm, the list may be corrupt */

	/* i915_gem_gtt_reserve() tries to reserve the precise range

	 * for the node, and evicts if it has to. So our test checks that

	 * it can give us the requsted space and prevent overlaps.

 Start by filling the GGTT */

 Now we start forcing evictions */

 And then try at random */

	/* i915_gem_gtt_insert() tries to allocate some free space in the GTT

	 * to the node, evicting if required.

 Check a couple of obviously invalid requests */

 Start by filling the GGTT */

 maxed out the GGTT space */

 If we then reinsert, we should find the same hole */

 And then force evictions */

 detect a hang */

	/*

	 * Our mission here is to fool the hardware to execute something

	 * from scratch as it has not seen the batch move (due to missing

	 * the TLB invalidate).

 Create two pages; dummy we prefill the TLB, and intended */

 Track the execution of each request by writing into different slot */

 Prime the TLB with the dummy pages */

 Replace the TLB with target batches */

 Wait until the context chain has started */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019 Intel Corporation

 Nothing fancy, just try to get an interesting bit pattern */

 Let size be a random number of pages up to 8 GB (2M pages) */

 Let the chunk size be a random power of 2 less than size */

 Round size down to the chunk size */

 Convert from pages to bytes */

	/*

	 * Create a pot-sized mm, then allocate one of each possible

	 * order within. This should leave the mm with exactly one

	 * page left.

 And now the last remaining block available */

 Should be completely full! */

 As we free in increasing size, we make available larger blocks */

 To confirm, now the whole mm should be available */

	/*

	 * Create a mm with one block of each order available, and

	 * try to allocate them all.

 Should be completely full! */

	/*

	 * Create a pot-sized mm, then allocate one of each possible

	 * order within. This should leave the mm with exactly one

	 * page left. Free the largest block, then whittle down again.

	 * Eventually we will have a fully 50% fragmented mm.

 Make room by freeing the largest allocated block */

 There should be one final page for this sub-allocation */

 Nothing larger than blocks of chunk_size now available */

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 quirk is only for live tiled objects, use it to declare ownership */

 Fill the GGTT with pinned objects and try to evict one. */

 Everything is pinned, nothing should happen */

 Everything is unpinned, we should be able to evict something */

	/* Fill the GGTT with pinned objects and then try to pin one more.

	 * We expect it to fail.

 Fill the GGTT with pinned objects and try to evict a range. */

 Everything is pinned, nothing should happen */

 Everything is unpinned, we should be able to evict the node */

	/*

	 * Currently the use of color_adjust for the GGTT is limited to cache

	 * coloring and guard pages, and so the presence of mm.color_adjust for

	 * the GGTT is assumed to be i915_ggtt_color_adjust, hence using a mock

	 * color adjust will work just fine for our purposes.

 Neighbouring; same colour - should fit */

 Remove just the second vma */

	/* Attempt to remove the first *pinned* vma, by removing the (empty)

	 * neighbour -- this should fail.

 Fill the GGTT with pinned objects and try to evict everything. */

 Everything is pinned, nothing should happen */

	/*

	 * The purpose of this test is to verify that we will trigger an

	 * eviction in the GGTT when constructing a request that requires

	 * additional space in the GGTT for pinning the context. This space

	 * is not directly tied to the request so reclaiming it requires

	 * extra work.

	 *

	 * As such this test is only meaningful for full-ppgtt environments

	 * where the GTT space of the request is separate from the GGTT

	 * allocation required to build the request.

 Reserve a block so that we know we have enough to fit a few rq */

 Make the GGTT appear small by filling it with unevictable nodes */

 Overfill the GGTT with context objects and so try to evict one. */

 We will need some GGTT space for the rq's context */

 When full, fail_if_busy will trigger EBUSY */

 Keep every request/ctx pinned until we are full */

/*

 * Copyright Â© 2017 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 We mark bits after the prefix as "X" */

	/*

	 * Cursory check that we can initialise a random pointer and transform

	 * it into the root pointer of a syncmap.

	/*

	 * Check that inserting a new id, creates a leaf and only that leaf.

	/*

	 * When we have a new id that doesn't fit inside the existing tree,

	 * we need to add a new layer above.

	 *

	 * 1: 0x00000001

	 * 2: 0x00000010

	 * 3: 0x00000100

	 * 4: 0x00001000

	 * ...

	 * Each pass the common prefix shrinks and we have to insert a join.

	 * Each join will only contain two branches, the latest of which

	 * is always a leaf.

	 *

	 * If we then reuse the same set of contexts, we expect to build an

	 * identical tree.

 very first insert will have no parents */

	/*

	 * Check that we can split a compacted branch by replacing it with

	 * a join.

	/*

	 * Each leaf holds KSYNCMAP seqno. Check that when we create KSYNCMAP

	 * neighbouring ids, they all fit into the same leaf.

 Skip repeats */

	/*

	 * The syncmap are "space efficient" compressed radix trees - any

	 * branch with only one child is skipped and replaced by the child.

	 *

	 * If we construct a tree with ids that are neighbouring at a non-zero

	 * height, we form a join but each child of that join is directly a

	 * leaf holding the single id.

 Create neighbours in the parent */

 Each of our children should be a leaf */

	/*

	 * Having tried to test the individual operations within i915_syncmap,

	 * run a smoketest exploring the entire u64 space with random

	 * insertions.

/*

 * Copyright Â© 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS

 * IN THE SOFTWARE.

 *

 Fisher-Yates shuffle courtesy of Knuth */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2019-2021 Intel Corporation

/*

 * SPDX-License-Identifier: MIT

 *

 * Copyright Â© 2019 Intel Corporation

 no need to refcount, we own this object */

 Pretend to open("/dev/dri/card0") */

 SPDX-License-Identifier: MIT

/*

 * Copyright Â© 2021 Intel Corporation

 nothing to read */

 simulate a termination interrupt */

 SPDX-License-Identifier: MIT

/*

 * Copyright(c) 2020 Intel Corporation.

	/*

	 * Contexts using protected objects keep a runtime PM reference, so we

	 * can only runtime suspend when all of them have been either closed

	 * or banned. Therefore, there is no need to invalidate in that

	 * scenario.

	/*

	 * The PXP component gets automatically unbound when we go into S3 and

	 * re-bound after we come out, so in that scenario we can defer the

	 * hw init to the bind call.

 SPDX-License-Identifier: MIT

/*

 * Copyright(c) 2020 Intel Corporation.

	/*

	 * The binding of the component is asynchronous from i915 probe, so we

	 * can't be sure it has happened.

/**

 * i915_pxp_tee_component_bind - bind function to pass the function pointers to pxp_tee

 * @i915_kdev: pointer to i915 kernel device

 * @tee_kdev: pointer to tee kernel device

 * @data: pointer to pxp_tee_master containing the function pointers

 *

 * This bind function is called during the system boot or resume from system sleep.

 *

 * Return: return 0 if successful.

 if we are suspended, the HW will be re-initialized on resume */

 the component is required to fully start the PXP HW */

 SPDX-License-Identifier: MIT

/*

 * Copyright(c) 2020, Intel Corporation. All rights reserved.

 shorter define */

 KCR hwdrm session in play 0-31 */

 PXP global terminate register for session termination */

 if we're suspended the session is considered off */

 if we're suspended the session is considered off */

 must mark termination in progress calling this function */

 terminate the hw sessions */

	/*

	 * if we fail to submit the termination there is no point in waiting for

	 * it to complete. PXP will be marked as non-active until the next

	 * termination is issued.

 Re-create the arb session after teardown handle complete */

	/*

	 * If we're processing an event while suspending then don't bother,

	 * we're going to re-init everything on resume anyway.

 SPDX-License-Identifier: MIT

/*

 * Copyright(c) 2020, Intel Corporation. All rights reserved.

 stall until prior PXP and MFX/HCP/HUC objects are cmopleted */

 pxp off */

 select session */

 pxp on */

 session inline termination */

 wait for cmds to go through */

/*

 * if we ever need to terminate more than one session, we can submit multiple

 * selections and terminations back-to-back with a single wait at the end

 SPDX-License-Identifier: MIT

/*

 * Copyright(c) 2020 Intel Corporation.

/**

 * intel_pxp_irq_handler - Handles PXP interrupts.

 * @pxp: pointer to pxp struct

 * @iir: interrupt vector

 immediately mark PXP as inactive on termination */

	/*

	 * We always need to submit a global termination when we re-enable the

	 * interrupts, so there is no need to make sure that the session state

	 * makes sense at the end of this function. Just make sure this is not

	 * called in a path were the driver consider the session as valid and

	 * doesn't call a termination on restart.

 SPDX-License-Identifier: MIT

/*

 * Copyright(c) 2020 Intel Corporation.

/**

 * DOC: PXP

 *

 * PXP (Protected Xe Path) is a feature available in Gen12 and newer platforms.

 * It allows execution and flip to display of protected (i.e. encrypted)

 * objects. The SW support is enabled via the CONFIG_DRM_I915_PXP kconfig.

 *

 * Objects can opt-in to PXP encryption at creation time via the

 * I915_GEM_CREATE_EXT_PROTECTED_CONTENT create_ext flag. For objects to be

 * correctly protected they must be used in conjunction with a context created

 * with the I915_CONTEXT_PARAM_PROTECTED_CONTENT flag. See the documentation

 * of those two uapi flags for details and restrictions.

 *

 * Protected objects are tied to a pxp session; currently we only support one

 * session, which i915 manages and whose index is available in the uapi

 * (I915_PROTECTED_CONTENT_DEFAULT_SESSION) for use in instructions targeting

 * protected objects.

 * The session is invalidated by the HW when certain events occur (e.g.

 * suspend/resume). When this happens, all the objects that were used with the

 * session are marked as invalid and all contexts marked as using protected

 * content are banned. Any further attempt at using them in an execbuf call is

 * rejected, while flips are converted to black frames.

 *

 * Some of the PXP setup operations are performed by the Management Engine,

 * which is handled by the mei driver; communication between i915 and mei is

 * performed via the mei_pxp component module.

 KCR register definitions */

 Setting KCR Init bit is required after system boot */

	/*

	 * Find the first VCS engine present. We're guaranteed there is one

	 * if we're in this function due to the check in has_pxp

	/*

	 * we'll use the completion to check if there is a termination pending,

	 * so we start it as completed and we reinit it when a termination

	 * is triggered.

	/*

	 * We want to get the same effect as if we received a termination

	 * interrupt, so just pretend that we did.

/*

 * the arb session is restarted from the irq work when we receive the

 * termination completion interrupt

 make sure the compiler doesn't optimize the double access */

	/*

	 * If this is the first time we're using this object, it's not

	 * encrypted yet; it will be encrypted with the current key, so mark it

	 * as such. If the object is already encrypted, check instead if the

	 * used key is still valid.

 ban all contexts marked as protected */

		/*

		 * By the time we get here we are either going to suspend with

		 * quiesced execution or the HW keys are already long gone and

		 * in this case it is worthless to attempt to close the context

		 * and wait for its execution. It will hang the GPU if it has

		 * not already. So, as a fast mitigation, we can ban the

		 * context as quick as we can. That might race with the

		 * execbuffer, but currently this is the best that can be done.

		/*

		 * The context has been banned, no need to keep the wakeref.

		 * This is safe from races because the only other place this

		 * is touched is context_release and we're holding a ctx ref

/* sis.c -- sis driver -*- linux-c -*-

 *

 * Copyright 1999 Precision Insight, Inc., Cedar Park, Texas.

 * Copyright 2000 VA Linux Systems, Inc., Sunnyvale, California.

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR

 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,

 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

 *

/**************************************************************************

 *

 * Copyright 2006 Tungsten Graphics, Inc., Bismarck, ND., USA.

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the

 * "Software"), to deal in the Software without restriction, including

 * without limitation the rights to use, copy, modify, merge, publish,

 * distribute, sub license, and/or sell copies of the Software, and to

 * permit persons to whom the Software is furnished to do so, subject to

 * the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL

 * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,

 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR

 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE

 * USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

 *

/*

 * Authors:

 *    Thomas HellstrÃ¶m <thomas-at-tungstengraphics-dot-com>

 fb management via fb device */

 CONFIG_FB_SIS[_MODULE] */

 CONFIG_FB_SIS[_MODULE] */

	/* Unconditionally init the drm_mm, even though we don't use it when the

	/*

	 * Implement a device switch here if needed

	/*

	 * Timeout after 3 seconds. We cannot use DRM_WAIT_ON here

	 * because its polling frequency is too low.

	/*

	 * The caller never sees an error code. It gets trapped

	 * in libdrm.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (c) 2018 Jernej Skrabec <jernej.skrabec@siol.net>

/*

 * Address can be actually any value. Here is set to same value as

 * it is set in BSP driver.

 pixelclk    bpp8    bpp10   bpp12 */

pixelclk   symbol   term   vlev*/

 power down */

	/*

	 * Values are taken from BSP HDMI driver. Although AW didn't

	 * release any documentation, explanation of this values can

	 * be found in i.MX 6Dual/6Quad Reference Manual.

 bandwidth / frequency independent settings */

 bandwidth / frequency dependent settings */

	/*

	 * NOTE: We have to be careful not to overwrite PHY parent

	 * clock selection bit and clock divider.

 get B value */

 enable read access to HDMI controller */

 unscramble register offsets */

	/*

	 * Set PHY I2C address. It must match to the address set by

	 * dw_hdmi_phy_set_slave_addr().

 wait for calibration to finish */

 enable DDC communication */

 reset PHY PLL clock parent */

 set HW control of CEC pins */

 read calibration data */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Free Electrons

 * Copyright (C) 2015 NextThing Co

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

/*

 * While this isn't really working in the DRM theory, in practice we

 * can only ever have one encoder per TCON since we have a mux in our

 * TCON.

 Create our layers */

 find primary and cursor planes for drm_crtc_init_with_planes */

 Set crtc.port to output port node of the tcon */

 Set possible_crtcs to this crtc for overlay planes */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (c) 2018 Jernej Skrabec <jernej.skrabec@siol.net>

	/*

	 * Controller support maximum of 594 MHz, which correlates to

	 * 4K@60Hz 4:4:4 or RGB.

	/*

	 * If we failed to find the CRTC(s) which this encoder is

	 * supposed to be connected to, it's because the CRTC has

	 * not been registered yet.  Defer probing, and hope that

	 * the required CRTC is added later.

	/*

	 * If dw_hdmi_bind() fails we'll never call dw_hdmi_unbind(),

	 * which would have called the encoder cleanup.  Do it manually.

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2018 Jernej Skrabec <jernej.skrabec@siol.net>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Free Electrons

 * Copyright (C) 2015 NextThing Co

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

/*

 * FIXME: If only the drm_display_mode private field was usable, this

 * could go away...

 *

 * So far, it doesn't seem to be preserved when the mode is passed by

 * to mode_set for some reason.

 First try to identify the mode by name */

 Then by number of lines */

 Enable and map the DAC to the output */

 Set PAL settings */

 Configure the DAC for a composite output */

 Configure the sample delay between DAC0 and the other DAC */

 Set the front and back porch */

 Set the lines setup */

 Set burst width for a composite output */

 Set composite chroma gain to 50 % */

 TODO */

 SPDX-License-Identifier: GPL-2.0+

 Copyright (c) 2018 Jernej Skrabec <jernej.skrabec@siol.net> */

	/*

	 * At least on H6, some registers have some bits set by default

	 * which may cause issues. Clear them here.

	/*

	 * TCON TOP has two muxes, which select parent clock for each TCON TV

	 * channel clock. Parent could be either TCON TV or TVE clock. For now

	 * we leave this fixed to TCON TV, since TVE driver for R40 is not yet

	 * implemented. Once it is, graph needs to be traversed to determine

	 * if TVE is active on each TCON TV. If it is, mux should be switched

	 * to TVE clock parent.

 Nothing special */

 sun4i_drv uses this list to check if a device node is a TCON TOP */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Free Electrons

 * Copyright (C) 2015 NextThing Co

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

 possible crtcs are set later */

 We need to have a sentinel at the need, hence the overallocation */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) Jernej Skrabec <jernej.skrabec@siol.net>

 make coordinates dividable by subsampling factor */

 Set height and width */

	/*

	 * Scaler must be enabled for subsampled formats, so it scales

	 * chroma to same size as luma.

 BSP algorithm assumes 80% efficiency of VI scaler unit */

 it seems that every RGB scaler has buffer for 2048 pixels */

 Set base coordinates */

 Adjust x and y to be dividable by subsampling factor */

 Get the physical address of the buffer in memory */

 Compute the start of the displayed memory */

 Fixup framebuffer address for src coordinates */

 Set the line width */

/*

 * While DE2 VI layer supports same RGB formats as UI layer, alpha

 * channel is ignored. This structure lists all unique variants

 * where alpha channel is replaced with "don't care" (X) channel.

 possible crtcs are set later */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Free Electrons

 * Copyright (C) 2015 NextThing Co

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

 delay at least 1200 ns */

 delay at least 120 ns */

/*

 * This function is a helper for TCON output muxing. The TCON output

 * muxing control register in earlier SoCs (without the TCON TOP block)

 * are located in TCON0. This helper returns a pointer to TCON0's

 * sun4i_tcon structure, or NULL if not found.

 Configure the dot clock */

 Set the resolution */

 XXX Would this ever happen? */

	/*

	 * FIXME: Undocumented bits

	 *

	 * The whole dithering process and these parameters are not

	 * explained in the vendor documents or BSP kernel code.

 Do dithering if panel only supports 6 bits per color */

 Check the connection format */

 R and B components are only 5 bits deep */

 Fall through: enable dithering */

 Write dithering settings */

 TODO support normal CPU interface modes */

 Set dithering if needed */

	/*

	 * This looks suspicious, but it works...

	 *

	 * The datasheet says that this should be set higher than 20 *

	 * pixel cycle, but it's not clear what a pixel cycle is.

	/*

	 * The Allwinner BSP has a comment that the period should be

	 * the display clock * 15, but uses an hardcoded 3000...

 Enable the output on the pins */

 Set dithering if needed */

 Adjust clock delay */

	/*

	 * This is called a backporch in the register documentation,

	 * but it really is the back porch + hsync

 Set horizontal display timings */

	/*

	 * This is called a backporch in the register documentation,

	 * but it really is the back porch + hsync

 Set vertical display timings */

 Setup the polarity of the various signals */

 Map output pins to channel 0 */

 Enable the output on the pins */

 Set dithering if needed */

 Adjust clock delay */

	/*

	 * This is called a backporch in the register documentation,

	 * but it really is the back porch + hsync

 Set horizontal display timings */

	/*

	 * This is called a backporch in the register documentation,

	 * but it really is the back porch + hsync

 Set vertical display timings */

 Set Hsync and Vsync length */

 Setup the polarity of the various signals */

 Map output pins to channel 0 */

 Enable the output on the pins */

 Configure the dot clock */

 Adjust clock delay */

 Set interlaced mode */

 Set the input resolution */

 Set the upscaling resolution */

 Set the output resolution */

 Set horizontal display timings */

	/*

	 * The vertical resolution needs to be doubled in all

	 * cases. We could use crtc_vtotal and always multiply by two,

	 * but that leads to a rounding error in interlace when vtotal

	 * is odd.

	 *

	 * This happens with TV's PAL for example, where vtotal will

	 * be 625, crtc_vtotal 312, and thus crtc_vtotal * 2 will be

	 * 624, which apparently confuses the hardware.

	 *

	 * To work around this, we will always use vtotal, and

	 * multiply by two only if we're not in interlace.

 Set vertical display timings */

 Set Hsync and Vsync length */

 Setup the polarity of multiple signals */

 according to vendor driver, this bit must be always set */

 Map output pins to channel 1 */

 DSI is tied to special case of CPU interface */

 Acknowledge the interrupt */

 Make sure the TCON is disabled and all IRQs are off */

 Disable IO lines and set them to tristate */

/*

 * On SoCs with the old display pipeline design (Display Engine 1.0),

 * the TCON is always tied to just one backend. Hence we can traverse

 * the of_graph upwards to find the backend our tcon is connected to,

 * and take its ID as our own.

 *

 * We can either identify backends from their compatible strings, which

 * means maintaining a large list of them. Or, since the backend is

 * registered and binded before the TCON, we can just go through the

 * list of registered backends and compare the device node.

 *

 * As the structures now store engines instead of backends, here this

 * function in fact searches the corresponding engine, and the ID is

 * requested via the get_id function of the engine.

	/*

	 * This only works if there is only one path from the TCON

	 * to any display engine. Otherwise the probe order of the

	 * TCONs and display engines is not guaranteed. They may

	 * either bind to the wrong one, or worse, bind to the same

	 * one if additional checks are not done.

	 *

	 * Bail out if there are multiple input connections.

 Get the first connection without specifying an ID */

 does this node match any registered engines? */

	/*

	 * According to device tree binding input ports have even id

	 * number and output ports have odd id. Since component with

	 * more than one input and one output (TCON TOP) exits, correct

	 * remote input id has to be calculated by subtracting 1 from

	 * remote output id. If this for some reason can't be done, 0

	 * is used as input port id.

 keep looking through upstream ports */

/*

 * The device tree binding says that the remote endpoint ID of any

 * connection between components, up to and including the TCON, of

 * the display pipeline should be equal to the actual ID of the local

 * component. Thus we can look at any one of the input connections of

 * the TCONs, and use that connection's remote endpoint ID as our own.

 *

 * Since the user of this function already finds the input port,

 * the port is passed in directly without further checks.

 try finding an upstream endpoint */

/*

 * Once we know the TCON's id, we can look through the list of

 * engines to find a matching one. We assume all engines have

 * been probed and added to the list.

	/*

	 * Because TCON is added to the list at the end of the probe

	 * (after this function is called), index of the current TCON

	 * will be same as current TCON list size.

/*

 * On SoCs with the old display pipeline design (Display Engine 1.0),

 * we assumed the TCON was always tied to just one backend. However

 * this proved not to be the case. On the A31, the TCON can select

 * either backend as its source. On the A20 (and likely on the A10),

 * the backend can choose which TCON to output to.

 *

 * The device tree binding says that the remote endpoint ID of any

 * connection between components, up to and including the TCON, of

 * the display pipeline should be equal to the actual ID of the local

 * component. Thus we should be able to look at any one of the input

 * connections of the TCONs, and use that connection's remote endpoint

 * ID as our own.

 *

 * However  the connections between the backend and TCON were assumed

 * to be always singular, and their endpoit IDs were all incorrectly

 * set to 0. This means for these old device trees, we cannot just look

 * up the remote endpoint ID of a TCON input endpoint. TCON1 would be

 * incorrectly identified as TCON0.

 *

 * This function first checks if the TCON node has 2 input endpoints.

 * If so, then the device tree is a corrected version, and it will use

 * sun4i_tcon_of_get_id() and sun4i_tcon_get_engine_by_id() from above

 * to fetch the ID and engine directly. If not, then it is likely an

 * old device trees, where the endpoint IDs were incorrect, but did not

 * have endpoint connections between the backend and TCON across

 * different display pipelines. It will fall back to the old method of

 * traversing the  of_graph to try and find a matching engine by device

 * node.

 *

 * In the case of single display pipeline device trees, either method

 * works.

	/*

	 * Is this a corrected device tree with cross pipeline

	 * connections between the backend and TCON?

		/*

		 * When pipeline has the same number of TCONs and engines which

		 * are represented by frontends/backends (DE1) or mixers (DE2),

		 * we match them by their respective IDs. However, if pipeline

		 * contains TCON TOP, chances are that there are either more

		 * TCONs than engines (R40) or TCONs with non-consecutive ids.

		 * (H6). In that case it's easier just use TCON index in list

		 * as an id. That means that on R40, any 2 TCONs can be enabled

		 * in DT out of 4 (there are 2 mixers). Due to the design of

		 * TCON TOP, remaining 2 TCONs can't be connected to anything

		 * anyway.

 Get our engine by matching our ID */

 Fallback to old method by traversing input endpoints */

 Make sure our TCON is reset */

		/*

		 * This can only be made optional since we've had DT

		 * nodes without the LVDS reset properties.

		 *

		 * If the property is missing, just disable LVDS, and

		 * print a warning.

		/*

		 * This can only be made optional since we've had DT

		 * nodes without the LVDS reset properties.

		 *

		 * If the property is missing, just disable LVDS, and

		 * print a warning.

		/*

		 * If we have an LVDS panel connected to the TCON, we should

		 * just probe the LVDS connector. Otherwise, just probe RGB as

		 * we used to.

		/*

		 * We assume there is no dynamic muxing of backends

		 * and TCONs, so we select the backend with same ID.

		 *

		 * While dynamic selection might be interesting, since

		 * the CRTC is tied to the TCON, while the layers are

		 * tied to the backends, this means, we will need to

		 * switch between groups of layers. There might not be

		 * a way to represent this constraint in DRM.

 panels and bridges are present only on TCONs with channel 0 */

 platform specific TCON muxing callbacks */

 HDMI */

	/*

	 * FIXME: Undocumented bits

 HDMI */

 TODO A31 has MIPI DSI but A31s does not */

 find TCON TOP platform device and TCON id */

 Same display pipeline structure as A10 */

 Same display pipeline structure as A10 */

 sun4i_drv uses this list to check if a device node is a TCON */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) Icenowy Zheng <icenowy@aosc.io>

 *

 * Based on sun4i_layer.h, which is:

 *   Copyright (C) 2015 Free Electrons

 *   Copyright (C) 2015 NextThing Co

 *

 *   Maxime Ripard <maxime.ripard@free-electrons.com>

 Set height and width */

 Set base coordinates */

 Get the physical address of the buffer in memory */

 Compute the start of the displayed memory */

 Fixup framebuffer address for src coordinates */

 Set the line width */

 possible crtcs are set later */

/*

 * Copyright (C) 2017 Jernej Skrabec <jernej.skrabec@siol.net>

 *

 * Coefficients are taken from BSP driver, which is:

 * Copyright (C) 2014-2015 Allwinner

 *

 * This file is licensed under the terms of the GNU General Public

 * License version 2.  This program is licensed "as is" without any

 * warranty of any kind, whether express or implied.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Free Electrons

 * Copyright (C) 2015 NextThing Co

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

 backend <-> TCON muxing selection done in backend */

 alpha at the lowest z position is not always supported */

 Set color correction */

 Disable color correction */

 Set height and width */

 Set base coordinates */

	/*

	 * We should do that only for a single plane, but the

	 * framebuffer's atomic_check has our back on this.

 TODO: Add support for the multi-planar YUV formats */

	/*

	 * Allwinner seems to list the pixel sequence from right to left, while

	 * DRM lists it from left to right.

 Clear the YUV mode */

 TODO: Add support for the multi-planar YUV formats */

 Set the line width */

 Get the start of the displayed memory */

 Write the 32 lower bits of the address (in bits) */

 And the upper bits */

	/*

	 * TODO: The backend alone allows 2x and 4x integer scaling, including

	 * support for an alpha component (which the frontend doesn't support).

	 * Use the backend directly instead of the frontend in this case, with

	 * another test to return false.

	/*

	 * Here the format is supported by both the frontend and the backend

	 * and no frontend scaling is required, so use the backend directly.

 Scaling is not supported without the frontend. */

 Sort our planes by Zpos */

 All our planes were disabled, bail out */

	/*

	 * The hardware is a bit unusual here.

	 *

	 * Even though it supports 4 layers, it does the composition

	 * in two separate steps.

	 *

	 * The first one is assigning a layer to one of its two

	 * pipes. If more that 1 layer is assigned to the same pipe,

	 * and if pixels overlaps, the pipe will take the pixel from

	 * the layer with the highest priority.

	 *

	 * The second step is the actual alpha blending, that takes

	 * the two pipes as input, and uses the potential alpha

	 * component to do the transparency between the two.

	 *

	 * This two-step scenario makes us unable to guarantee a

	 * robust alpha blending between the 4 layers in all

	 * situations, since this means that we need to have one layer

	 * with alpha at the lowest position of our two pipes.

	 *

	 * However, we cannot even do that on every platform, since

	 * the hardware has a bug where the lowest plane of the lowest

	 * pipe (pipe 0, priority 0), if it has any alpha, will

	 * discard the pixel data entirely and just display the pixels

	 * in the background color (black by default).

	 *

	 * This means that on the affected platforms, we effectively

	 * have only three valid configurations with alpha, all of

	 * them with the alpha being on pipe1 with the lowest

	 * position, which can be 1, 2 or 3 depending on the number of

	 * planes and their zpos.

 For platforms that are not affected by the issue described above. */

 We can't have an alpha plane at the lowest position */

		/*

		 * The only alpha position is the lowest plane of the

		 * second pipe.

 We can only have a single YUV plane at a time */

	/*

	 * In a teardown scenario with the frontend involved, we have

	 * to keep the frontend enabled until the next vblank, and

	 * only then disable it.

	 *

	 * This is due to the fact that the backend will not take into

	 * account the new configuration (with the plane that used to

	 * be fed by the frontend now disabled) until we write to the

	 * commit bit and the hardware fetches the new configuration

	 * during the next vblank.

	 *

	 * So we keep the frontend around in order to prevent any

	 * visual artifacts.

/*

 * The display backend can take video output from the display frontend, or

 * the display enhancement unit on the A80, as input for one it its layers.

 * This relationship within the display pipeline is encoded in the device

 * tree with of_graph, and we use it here to figure out which backend, if

 * there are 2 or more, we are currently probing. The number would be in

 * the "reg" property of the upstream output port endpoint.

 Input port is 0, and we want the first endpoint. */

 TODO: This needs to take multiple pipelines into account */

 does this node match any registered engines? */

		/*

		 * This assume we have the same DMA constraints for all our the

		 * devices in our pipeline (all the backends, but also the

		 * frontends). This sounds bad, but it has always been the case

		 * for us, and DRM doesn't do per-device allocation either, so

		 * we would need to fix DRM first...

	/*

	 * Many of the backend's layer configuration registers have

	 * undefined default values. This poses a risk as we use

	 * regmap_update_bits in some places, and don't overwrite

	 * the whole register.

	 *

	 * Clear the registers here to have something predictable.

 Disable registers autoloading */

 Enable the backend */

 Set output selection if needed */

		/*

		 * We assume there is no dynamic muxing of backends

		 * and TCONs, so we select the backend with same ID.

		 *

		 * While dynamic selection might be interesting, since

		 * the CRTC is tied to the TCON, while the layers are

		 * tied to the backends, this means, we will need to

		 * switch between groups of layers. There might not be

		 * a way to represent this constraint in DRM.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2017 Free Electrons

 * Maxime Ripard <maxime.ripard@free-electrons.com>

/*

 * These coefficients are taken from the A33 BSP from Allwinner.

 *

 * The first three values of each row are coded as 13-bit signed fixed-point

 * numbers, with 10 bits for the fractional part. The fourth value is a

 * constant coded as a 14-bit signed fixed-point number with 4 bits for the

 * fractional part.

 *

 * The values in table order give the following colorspace translation:

 * G = 1.164 * Y - 0.391 * U - 0.813 * V + 135

 * R = 1.164 * Y + 1.596 * V - 222

 * B = 1.164 * Y + 2.018 * U + 276

 *

 * This seems to be a conversion from Y[16:235] UV[16:240] to RGB[0:255],

 * following the BT601 spec.

		/*

		 * The X1 offset is the offset to the bottom-right point in the

		 * end tile, which is the final pixel (at offset width - 1)

		 * within the end tile (with a 32-byte mask).

 Set the line width */

 Some planar formats require chroma channel swapping by hand. */

 Set the physical address of the buffer in memory */

 Planar formats have an explicit input sequence. */

	/*

	 * I have no idea what this does exactly, but it seems to be

	 * related to the scaler FIR filter phase parameters.

	/*

	 * Checking the input format is sufficient since we currently only

	 * support RGB output formats to the backend. If YUV output formats

	 * ever get supported, an YUV input and output would require bypassing

	 * the CSC engine too.

 Setup the CSC engine for YUV to RGB conversion. */

	/*

	 * TODO: It look like the A31 and A80 at least will need the

	 * bit 7 (ALPHA_EN) enabled when using a format with alpha (so

	 * ARGB8888).

 Set height and width */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Free Electrons

 * Copyright (C) 2015 NextThing Co

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

/*

 * VESA DMT defines a tolerance of 0.5% on the pixel clock, while the

 * CVT spec reuses that tolerance in its examples, so it looks to be a

 * good default tolerance for the EDID-based modes. Define it to 5 per

 * mille to avoid floating point operations.

	/*

	 * TODO: We should use the struct display_timing if available

	 * and / or trying to stretch the timings within that

	 * tolerancy to take care of panels that we wouldn't be able

	 * to have a exact match for.

	/*

	 * That shouldn't ever happen unless something is really wrong, but it

	 * doesn't harm to check.

 The RGB encoder can only work with the TCON channel 0 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Free Electrons

 * Copyright (C) 2016 NextThing Co

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Free Electrons

 * Copyright (C) 2015 NextThing Co

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

 The hardware only allows even pitches for YUV buffers. */

 Generic Operations */

 GEM Operations */

 drm_vblank_init calls kcalloc, which can fail */

 Remove early framebuffers (ie. simplefb) */

 Enable connectors polling */

/*

 * The encoder drivers use drm_of_find_possible_crtcs to get upstream

 * crtcs from the device tree using of_graph. For the results to be

 * correct, encoders must be probed/bound after _all_ crtcs have been

 * created. The existing code uses a depth first recursive traversal

 * of the of_graph, which means the encoders downstream of the TCON

 * get add right after the first TCON. The second TCON or CRTC will

 * never be properly associated with encoders connected to it.

 *

 * Also, in a dual display pipeline setup, both frontends can feed

 * either backend, and both backends can feed either TCON, we want

 * all components of the same type to be added before the next type

 * in the pipeline. Fortunately, the pipelines are perfectly symmetric,

 * i.e. components of the same type are at the same depth when counted

 * from the frontend. The only exception is the third pipeline in

 * the A80 SoC, which we do not support anyway.

 *

 * Hence we can use a breadth first search traversal order to add

 * components. We do not need to check for duplicates. The component

 * matching system handles this for us.

			/*

			 * TCON TOP is always probed before TCON. However, TCON

			 * points back to TCON TOP when it is source for HDMI.

			 * We have to skip it here to prevent infinite looping

			 * between TCON TOP and TCON.

			/*

			 * If the node is our TCON with channel 0, the first

			 * port is used for panel or bridges, and will not be

			 * part of the component framework.

	/*

	 * The frontend has been disabled in some of our old device

	 * trees. If we find a node that is the frontend and is

	 * disabled, we should just follow through and parse its

	 * child, but without adding it to the component list.

	 * Otherwise, we obviously want to add it to the list.

	/*

	 * The connectors will be the last nodes in our pipeline, we

	 * can just bail out.

	/*

	 * If the device is either just a regular device, or an

	 * enabled frontend supported by the driver, we add it to our

	 * component list.

 Add current component */

 each node has at least one output */

 TCON TOP has second and third output */

 process this endpoint */

 sun4i_drv_add_endpoints can fail to allocate memory */

/*

 * Copyright (C) 2017 Jernej Skrabec <jernej.skrabec@siol.net>

 *

 * Coefficients are taken from BSP driver, which is:

 * Copyright (C) 2014-2015 Allwinner

 *

 * This file is licensed under the terms of the GNU General Public

 * License version 2.  This program is licensed "as is" without any

 * warranty of any kind, whether express or implied.

	/*

	 * This is chroma V/H phase calculation as it appears in

	 * BSP driver. There is no detailed explanation. YUV 420

	 * chroma is threated specialy for some reason.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Free Electrons

 * Copyright (C) 2016 NextThing Co

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

		/*

		 * ideal has overflowed the max value that can be stored in an

		 * unsigned long, and every clk operation we might do on a

		 * truncated u64 value will give us incorrect results.

		 * Let's just stop there since bigger dividers will result in

		 * the same overflow issue.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Free Electrons

 * Copyright (C) 2015 NextThing Co

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (c) 2016 Allwinnertech Co., Ltd.

 * Copyright (C) 2017-2018 Bootlin

 *

 * Maxime Ripard <maxime.ripard@bootlin.com>

 Maaaaaagic */

	/*

	 * TODO: The format defines are only valid in video mode and

	 * change in command mode.

 Do all timing calculations up front to allocate buffer space */

		/*

		 * A sync period is composed of a blanking packet (4

		 * bytes + payload + 2 bytes) and a sync event packet

		 * (4 bytes). Its minimal size is therefore 10 bytes

		/*

		 * The backporch is set using a blanking packet (4

		 * bytes + payload + 2 bytes). Its minimal size is

		 * therefore 6 bytes

		/*

		 * The frontporch is set using a sync event (4 bytes)

		 * and two blanking packets (each one is 4 bytes +

		 * payload + 2 bytes). Its minimal size is therefore

		 * 16 bytes

		/*

		 * The blanking is set using a sync event (4 bytes)

		 * and a blanking packet (4 bytes + payload + 2

		 * bytes). Its minimal size is therefore 10 bytes.

		/*

		 * And I'm not entirely sure what vblk is about. The driver in

		 * Allwinner BSP is using a rather convoluted calculation

		 * there only for 4 lanes. However, using 0 (the !4 lanes

		 * case) even with a 4 lanes screen seems to work...

 How many bytes do we need to send all payloads? */

 sync */

 backporch */

 frontporch */

 hblk */

 vblk */

	/*

	 * Enable the DSI block.

	/*

	 * FIXME: This should be moved after the switch to HS mode.

	 *

	 * Unfortunately, once in HS mode, it seems like we're not

	 * able to send DCS commands anymore, which would prevent any

	 * panel to send any DCS command as part as their enable

	 * method, which is quite common.

	 *

	 * I haven't seen any artifact due to that sub-optimal

	 * ordering on the panels I've tested it with, so I guess this

	 * will do for now, until that IP is better understood.

	/*

	 * TODO: There's some bits (reg 0x200, bits 8/9) that

	 * apparently can be used to check whether the data have been

	 * sent, but I couldn't get it to work reliably.

	/*

	 * TODO: There's some bits (reg 0x200, bits 24/25) that

	 * apparently can be used to check whether the data have been

	 * received, but I couldn't get it to work reliably.

	/*

	 * In order to operate properly, that clock seems to be always

	 * set to 297MHz.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Free Electrons

 * Copyright (C) 2016 NextThing Co

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

	/*

	 * We only consider PLL3, since the TCON is very likely to be

	 * clocked from it, and to have the same rate than our HDMI

	 * clock, so we should not need to do anything.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2017 Icenowy Zheng <icenowy@aosc.io>

 *

 * Based on sun4i_backend.c, which is:

 *   Copyright (C) 2015 Free Electrons

 *   Copyright (C) 2015 NextThing Co

 for DE2 VI layer which ignores alpha */

 for DE2 VI layer which ignores alpha */

 for DE2 VI layer which ignores alpha */

 for DE2 VI layer which ignores alpha */

 for DE2 VI layer which ignores alpha */

 for DE2 VI layer which ignores alpha */

 for DE2 VI layer which ignores alpha */

 for DE2 VI layer which ignores alpha */

 guessed */

 Output port is 1, and we want the first endpoint. */

	/*

	 * The mixer uses single 32-bit register to store memory

	 * addresses, so that it cannot deal with 64-bit memory

	 * addresses.

	 * Restrict the DMA mask so that the mixer won't be

	 * allocated some memory that is too high.

		/*

		 * This assume we have the same DMA constraints for

		 * all our the mixers in our pipeline. This sounds

		 * bad, but it has always been the case for us, and

		 * DRM doesn't do per-device allocation either, so we

		 * would need to fix DRM first...

	/*

	 * While this function can fail, we shouldn't do anything

	 * if this happens. Some early DE2 DT entries don't provide

	 * mixer id but work nevertheless because matching between

	 * TCON and mixer is done by comparing node pointers (old

	 * way) instead comparing ids. If this function fails and

	 * id is needed, it will fail during id matching anyway.

	/*

	 * It seems that we need to enforce that rate for whatever

	 * reason for the mixer to be functional. Make sure it's the

	 * case.

 Reset registers and disable unused sub-engines */

 Enable the mixer */

 Set background color to black */

	/*

	 * Set fill color of bottom plane to black. Generally not needed

	 * except when VI plane is at bottom (zpos = 0) and enabled.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Free Electrons

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) Jernej Skrabec <jernej.skrabec@siol.net>

/*

 * Factors are in two's complement format, 10 bits for fractinal part.

 * First tree values in each line are multiplication factor and last

 * value is constant, which is added at the end.

/*

 * DE3 has a bit different CSC units. Factors are in two's complement format.

 * First three factors in a row are multiplication factors which have 17 bits

 * for fractional part. Fourth value in a row is comprised of two factors.

 * Upper 16 bits represents difference, which is subtracted from the input

 * value before multiplication and lower 16 bits represents constant, which

 * is addes at the end.

 *

 * x' = c00 * (x + d0) + c01 * (y + d1) + c02 * (z + d2) + const0

 * y' = c10 * (x + d0) + c11 * (y + d1) + c12 * (z + d2) + const1

 * z' = c20 * (x + d0) + c21 * (y + d1) + c22 * (z + d2) + const2

 *

 * Please note that above formula is true only for Blender CSC. Other DE3 CSC

 * units takes only positive value for difference. From what can be deducted

 * from BSP driver code, those units probably automatically assume that

 * difference has to be subtracted.

 *

 * Layout of factors in table:

 * c00 c01 c02 [d0 const0]

 * c10 c11 c12 [d1 const1]

 * c20 c21 c22 [d2 const2]

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Maxime Ripard

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

 Set input sync enable */

	/*

	 * Setup output pad (?) controls

	 *

	 * This is done here instead of at probe/bind time because

	 * the controller seems to toggle some of the bits on its own.

	 *

	 * We can't just initialize the register there, we need to

	 * protect the clock bits that have already been read out and

	 * cached by the clock framework.

 Setup timing registers */

 +-0.5% allowed by HDMI spec */

 165 MHz is the typical max pixelclock frequency for HDMI <= 1.2 */

 Start driving the CEC pin low */

	/*

	 * Stop driving the CEC pin, the pull up will take over

	 * unless another CEC device is driving the pin low.

 Only difference from sun5i is AMP is 4 instead of 6 */

 There is no HPD interrupt, so we need to poll the controller */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Maxime Ripard <maxime.ripard@free-electrons.com>

 * Copyright (C) 2017 Jonathan Liu <net147@gmail.com>

 FIFO request bit is set when FIFO level is above RX_THRESHOLD during read */

	/*

	 * 1 byte takes 9 clock cycles (8 bits + 1 ACK) = 90 us for 100 kHz

	 * clock. As clock rate is fixed, just round it up to 100 us.

	/*

	 * If threshold is inclusive, then the FIFO may only have

	 * RX_THRESHOLD number of bytes, instead of RX_THRESHOLD + 1.

	/*

	 * Limit transfer length by FIFO threshold or FIFO size.

	 * For TX the threshold is for an empty FIFO.

 Wait until error, FIFO request bit set or transfer complete */

 Clear FIFO request bit by forcing a write to that bit */

 Set FIFO direction */

 Clear address register (not cleared by soft reset) */

 Set I2C address */

	/*

	 * Set FIFO RX/TX thresholds and clear FIFO

	 *

	 * If threshold is inclusive, we can set the TX threshold to

	 * 0 instead of 1.

 Set transfer length */

 Set command */

 Clear interrupt status bits by forcing a write */

 Start command */

 Transfer bytes */

 Wait for command to finish */

 Check for errors */

 DDC clock needs to be enabled for the module to work */

 Reset I2C controller */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2017 Free Electrons

 * Maxime Ripard <maxime.ripard@free-electrons.com>

 The LVDS encoder can only work with the TCON channel 0 */

 SPDX-License-Identifier: GPL-2.0

/*

 * Panel driver for the Samsung LMS397KF04 480x800 DPI RGB panel.

 * According to the data sheet the display controller is called DB7430.

 * Found in the Samsung Galaxy Beam GT-I8350 mobile phone.

 * Linus Walleij <linus.walleij@linaro.org>

/**

 * struct db7430 - state container for a panel controlled by the DB7430

 * controller

* @dev: the container device */

* @dbi: the DBI bus abstraction handle */

* @panel: the DRM panel instance for this device */

* @width: the width of this panel in mm */

* @height: the height of this panel in mm */

* @reset: reset GPIO line */

* @regulators: VCCIO and VIO supply regulators */

	/*

	 * 31 ns period min (htotal*vtotal*vrefresh)/1000

	 * gives a Vrefresh of ~71 Hz.

 Power up */

 Assert reset >=1 ms */

 De-assert reset */

 Wait >= 10 ms */

	/*

	 * This is set to 0x0a (RGB/BGR order + horizontal flip) in order

	 * to make the display behave normally. If this is not set the displays

	 * normal output behaviour is horizontally flipped and BGR ordered. Do

	 * it twice because the first message doesn't always "take".

	/*

	 * 0x00 in datasheet 0x01 in vendor code 0x00, it seems 0x01 means

	 * DE active high and 0x00 means DE active low.

 R positive gamma */ 0x00,

 R negative gamma */ 0x00,

 G positive gamma */ 0x00,

 G negative gamma */ 0x00,

 B positive gamma */ 0x00,

 B negative gamma */ 0x00,

 Go into RESET and disable regulators */

 Exit sleep mode */

 NVM (non-volatile memory) load sequence */

 CABC turn on sequence (BC = backlight control) */

 Turn on display */

/**

 * db7430_get_modes() - return the mode

 * @panel: the panel to get the mode for

 * @connector: reference to the central DRM connector control structure

	/*

	 * VCI   is the analog voltage supply

	 * VCCIO is the digital I/O voltage supply

 FIXME: if no external backlight, use internal backlight */

/*

 * The DB7430 display controller may be used in several display products,

 * so list the different variants here and add per-variant data if needed.

 SPDX-License-Identifier: GPL-2.0

/*

 * Xinpeng xpp055c272 5.5" MIPI-DSI panel driver

 * Copyright (C) 2019 Theobroma Systems Design und Consulting GmbH

 *

 * based on

 *

 * Rockteck jh057n00900 5.5" MIPI-DSI panel driver

 * Copyright (C) Purism SPC 2019

 Manufacturer specific Commands send via DSI */

	/*

	 * Init sequence was supplied by the panel vendor without much

	 * documentation.

 T6: 10us */

 T8: 20ms */

 T9: 120ms */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (c) 2017, Fuzhou Rockchip Electronics Co., Ltd

 p079zca: t2 (20ms), p097pfg: t4 (15ms) */

 p079zca: t4, p097pfg: t5 */

			/*

			 * Included by random guessing, because without this

			 * (or at least, some delay), the panel sometimes

			 * didn't appear to pick up the command sequence.

 T6: 120ms - 1000ms*/

 T7: 5ms */

 T8: 80ms - 1000ms */

/*

 * Display manufacturer failed to provide init sequencing according to

 * https://chromium-review.googlesource.com/c/chromiumos/third_party/coreboot/+/892065/

 * so the init sequence stems from a register dump of a working panel.

 page 0 */

 page 1 */

 page 2 */

 page 3 */

 page 4 */

 page 5 */

 page 6 */

 T15 */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Toppoly TD043MTEA1 Panel Driver

 *

 * Copyright (C) 2019 Texas Instruments Incorporated

 *

 * Based on the omapdrm-specific panel-tpo-td043mtea1 driver

 *

 * Author: GraÅ¾vydas Ignotas <notasas@gmail.com>

/* -----------------------------------------------------------------------------

 * Hardware Access

 gamma bits [9:8] */

 gamma bits [7:0] */

 Wait for the panel to stabilize. */

 wait for at least 2 vsyncs before cutting off power */

/* -----------------------------------------------------------------------------

 * sysfs

/* -----------------------------------------------------------------------------

 * Panel Operations

	/*

	 * If we are resuming from system suspend, SPI might not be enabled

	 * yet, so we'll program the LCD from SPI PM resume callback.

	/*

	 * FIXME: According to the datasheet sync signals are sampled on the

	 * rising edge of the clock, but the code running on the OMAP3 Pandora

	 * indicates sampling on the falling edge. This should be tested on a

	 * real device.

/* -----------------------------------------------------------------------------

 * Power Management, Probe and Remove

 sentinel */ },

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0+

/*

 * LCD-OLinuXino support for panel driver

 *

 * Copyright (C) 2018 Olimex Ltd.

 *   Author: Stefan Mavrodiev <stefan@olimex.com>

 Always make the first mode preferred */

 Copy data into buffer */

 Check configuration checksum */

 Check magic header */

	/*

	 * The eeprom can hold up to 4 modes.

	 * If the stored value is bigger, overwrite it.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) STMicroelectronics SA 2017

 *

 * Authors: Philippe Cornu <philippe.cornu@st.com>

 *          Yannick Fertre <yannick.fertre@st.com>

 Manufacturer Command Set */

 Address Shift Function */

 Panel Type Setting */

 Source Driver Timing Setting */

 Panel Driving Mode */

 Oscillator Adjustment for Idle/Normal mode */

 RGB Video Mode Setting */

 Source Driver Precharge Control */

 Command not documented */

 Power Control Setting 1 */

 Power Control Setting 2 for Normal Mode */

 Power Control Setting 4 for DC Voltage */

 Panel Control Setting 1 */

 Panel Control Setting 2 */

 Panel Control Setting 3 */

 Panel Control Setting 4 */

 Panel Control Setting 5 */

 Panel Control Setting 6 */

 Panel Control Setting 7 */

 Panel Control Setting 8 */

 Panel U2D Setting 1 */

 Panel U2D Setting 2 */

 Panel U2D Setting 3 */

 Panel D2U Setting 1 */

 Panel D2U Setting 2 */

 Panel D2U Setting 3 */

 GOA VST Setting */

 GOA CLKA1 Setting */

 GOA CLKA3 Setting */

 GOA ECLK Setting */

 Command not documented */

 GVDD/NGVDD */

 VCOM Voltage Setting */

 Gamma Correction 2.2+ Setting */

 Gamma Correction 2.2- Setting */

 Command not documented */

 Enable Access Command2 "CMD2" */

 Enable Access Orise Command2 */

 50 Hz, preferred */

 60 Hz */

 Enter CMD2 */

 Enter Orise Command2 */

 65Hz */

 Exit CMD2 */

 Wait for sleep out exit */

 Default portrait 480x800 rgb24 */

 See otm8009a driver documentation for pixel format descriptions */

 Disable CABC feature */

 Send Command GRAM memory write (no parameters) */

 Wait a short while to let the panel be ready before the 1st frame */

 This is not an issue so we return 0 here */

 Setting first mode as preferred */

/*

 * DSI-BASED BACKLIGHT

		/* Power on the backlight with the requested brightness

		 * Note We can not use mipi_dsi_dcs_set_display_brightness()

		 * as otm8009a driver support only 8-bit brightness (1 param).

 set Brightness Control & Backlight on */

 Power off the backlight: set Brightness Control & Bl off */

 Update Brightness Control & Backlight */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2017 NXP Semiconductors.

 * Author: Marco Franchi <marco.franchi@nxp.com>

 *

 * Based on Panel Simple driver by Thierry Reding <treding@nvidia.com>

	/**

	 * @width: width (in millimeters) of the panel's active display area

	 * @height: height (in millimeters) of the panel's active display area

 Add a 100ms delay as per the panel datasheet */

 Add a 100ms delay as per the panel datasheet */

 add hard-coded panel modes */

 sentinel */

 SPDX-License-Identifier: GPL-2.0

/*

 * Elida kd35t133 5.5" MIPI-DSI panel driver

 * Copyright (C) 2020 Theobroma Systems Design und Consulting GmbH

 *

 * based on

 *

 * Rockteck jh057n00900 5.5" MIPI-DSI panel driver

 * Copyright (C) Purism SPC 2019

 Manufacturer specific Commands send via DSI */

	/*

	 * Init sequence was supplied by the panel vendor with minimal

	 * documentation.

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * S6E63M0 AMOLED LCD drm_panel driver.

 *

 * Copyright (C) 2019 PaweÅ Chmiel <pawel.mikolaj.chmiel@gmail.com>

 * Derived from drivers/gpu/drm/panel-samsung-ld9040.c

 *

 * Andrzej Hajda <a.hajda@samsung.com>

 array of gamma tables for gamma value 2.2 */

 30 cd */

 40 cd */

 50 cd */

 60 cd */

 70 cd */

 80 cd */

 90 cd */

 100 cd */

 110 cd */

 120 cd */

 130 cd */

 140 cd */

 150 cd */

 160 cd */

 170 cd */

 180 cd */

 190 cd */

 200 cd */

 210 cd */

 220 cd */

 230 cd */

 240 cd */

 250 cd */

 260 cd */

 270 cd */

 280 cd */

 290 cd */

 300 cd */

 NULL ACL */

 40P ACL */

 43P ACL */

 45P ACL */

 47P ACL */

 48P ACL */

 50P ACL */

 This tells us which ACL level goes with which gamma */

 30 - 60 cd: ACL off/NULL */

 70 - 250 cd: 40P ACL */

 260 - 300 cd: 50P ACL */

 The ELVSS backlight regulator has 5 levels */

 not set */

 30 cd - 100 cd */

 110 cd - 160 cd */

 170 cd - 200 cd */

 210 cd - 300 cd */

 This tells us which ELVSS level goes with which gamma */

 30 - 100 cd */

 110 - 160 cd */

 170 - 200 cd */

 210 - 300 cd */

	/*

	 * This field is tested by functions directly accessing bus before

	 * transfer, transfer is skipped if it is set. In case of transfer

	 * failure or unexpected response the field is set to error value.

	 * Such construct allows to eliminate many checks in higher level

	 * functions.

	/*

	 * We attempt to detect what panel is mounted on the controller.

	 * The third ID byte represents the desired ELVSS pulse for

	 * some displays.

 Default ELVSS pulse level */

	/*

	 * We do not know why there is a difference in the DSI mode.

	 * (No datasheet.)

	 *

	 * In the vendor driver this sequence is called

	 * "SEQ_PANEL_CONDITION_SET" or "DCS_CMD_SEQ_PANEL_COND_SET".

 Be sure to send a reset pulse */

 Magic to unlock level 2 control of the display */

 Magic to unlock MTP reading */

 Adjust ELVSS to candela level */

 Update the ACL per gamma value */

 Update gamma table */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2019, Huaqin Telecom Technology Co., Ltd

 *

 * Author: Jerry Han <jerry.han.hq@gmail.com>

 *

 sleep_mode_delay: 1ms - 2ms */

 T1: 5ms - 6ms */

 reset sequence */

 T2: 14ms - 15ms */

 T3: 1ms - 2ms */

 T4: 1ms - 2ms */

 T5: 5ms - 6ms */

 send init code */

 T6: 120ms - 121ms */

 T7: 20ms - 21ms */

 8 inch */

 10 inch */

 sentinel */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2020 Linaro Ltd

 * Author: Sumit Semwal <sumit.semwal@linaro.org>

 *

 * This driver is for the DSI interface to panels using the NT36672A display driver IC

 * from Novatek.

 * Currently supported are the Tianma FHD+ panels found in some Xiaomi phones, including

 * some variants of the Poco F1 phone.

 *

 * Panels using the Novatek NT37762A IC should add appropriate configuration per-panel and

 * use this driver.

 send off cmds */

 120ms delay required here as per DCS spec */

 0x3C = 60ms delay */

	/*

	 * As per downstream kernel, Reset sequence of Tianma FHD panel requires the panel to

	 * be out of reset for 10ms, followed by being held in reset for 10ms. But for Android

	 * AOSP, we needed to bump it upto 200ms otherwise we get white screen sometimes.

	 * FIXME: Try to reduce this 200ms to a lesser value.

 send first part of init cmds */

 0x46 = 70 ms delay */

 Send rest of the init cmds */

 skin enhancement mode */

 dimming enable */

 resolution 1080*2246 */

 UI mode */

 STILL mode */

 MOVING mode */

 SPDX-License-Identifier: GPL-2.0

/*

 * Sharp LS037V7DW01 LCD Panel Driver

 *

 * Copyright (C) 2019 Texas Instruments Incorporated

 *

 * Based on the omapdrm-specific panel-sharp-ls037v7dw01 driver

 *

 * Copyright (C) 2013 Texas Instruments Incorporated

 * Author: Tomi Valkeinen <tomi.valkeinen@ti.com>

 low = reset active min 20 us */

 high = power on */

 low = 480x640, high = 240x320 */

 high = conventional horizontal scanning */

 high = conventional vertical scanning */

 Wait at least 5 vsyncs after disabling the LCD. */

 Wait couple of vsyncs before enabling the LCD. */

	/*

	 * FIXME: According to the datasheet pixel data is sampled on the

	 * rising edge of the clock, but the code running on the SDP3430

	 * indicates sampling on the negative edge. This should be tested on a

	 * real device.

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2019, Amarula Solutions.

 * Author: Jagan Teki <jagan@amarulasolutions.com>

 Command2 BKx selection command */

 Command2, BK0 commands */

 Positive Voltage Gamma Control */

 Negative Voltage Gamma Control */

 Display Line setting */

 Porch control */

 Inversion selection, Frame Rate Control */

 Command2, BK1 commands */

 Vop amplitude setting */

 VCOM amplitude setting */

 VGH Voltage setting */

 TEST Command Setting */

 VGL Voltage setting */

 Power Control 1 */

 Power Control 2 */

 Source pre_drive timing set1 */

 Source EQ2 Setting */

 MIPI Setting 1 */

/*

 * Command2 with BK function selection.

 *

 * BIT[4, 0]: [CN2, BKXSEL]

 * 10 = CMD2BK0, Command2 BK0

 * 11 = CMD2BK1, Command2 BK1

 * 00 = Command2 disable

 Command2, BK0 bytes */

 Command2, BK1 bytes */

 Gamma OP bias, max */

 Source OP input bias, min */

 Source OP output bias, min */

 AVDD 6.6v */

 AVCL -4.4v */

 We need to wait 5ms before sending new commands */

 Command2, BK0 */

 Command2, BK1 */

	/**

	 * ST7701_SPEC_V1.2 is unable to provide enough information above this

	 * specific command sequence, so grab the same from vendor BSP driver.

 disable Command2 */

	/**

	 * During the Resetting period, the display will be blanked

	 * (The display is entering blanking sequence, which maximum

	 * time is 120 ms, when Reset Starts in Sleep Out âmode. The

	 * display remains the blank state in Sleep In âmode.) and

	 * then return to Default condition for Hardware Reset.

	 *

	 * So we need wait sleep_delay time to make sure reset completed.

 panel need extra 80ms for sleep out cmd */

	/**

	 * Once sleep out has been issued, ST7701 IC required to wait 120ms

	 * before initiating new commands.

	 *

	 * On top of that some panels might need an extra delay to wait, so

	 * add panel specific delay for those cases. As now this panel specific

	 * delay information is referenced from those panel BSP driver, example

	 * ts8550b and there is no valid documentation for that.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) STMicroelectronics SA 2017

 *

 * Authors: Philippe Cornu <philippe.cornu@st.com>

 *          Yannick Fertre <yannick.fertre@st.com>

** Manufacturer Command Set ***/

 CMD Mode Switch */

 User Command Set (UCS = CMD1) */

 Manufacture Command Set Page0 (CMD2 P0) */

 Manufacture Command Set Page1 (CMD2 P1) */

 Manufacture Command Set Page2 (CMD2 P2) */

 Manufacture Command Set Page3 (CMD2 P3) */

 CMD2 P0 commands (Display Options and Power) */

 TE1 Output Setting Zig-Zag Connection */

 Source Bias Current */

 Source Output Delay Time */

 Inversion Type */

 External PWR IC Control */

 PFM Control for AVDD Output */

 PFM Control for AVEE Output */

 DDVDL Charge Pump Control */

 VGH Charge Pump Control */

 VGL Charge Pump Control */

 VCOM Output Level Control */

 VG M/S N Control */

 VG M/S P Control */

 Interface Control for PFM and MIPI */

 CMD2 P2 commands (GOA Timing Control) - no description in datasheet */

 CMD2 P3 commands (Gamma) */

 Gamma VP1~VP16 */

 Gamma VN1~VN16 */

/*

 * This panel is not able to auto-increment all cmd addresses so for some of

 * them, we need to send them one by one...

 Enter CMD2 with page 0 */

 2 data lanes, see doc */

 No documentation */

 Exit CMD2 */

 SPDX-License-Identifier: GPL-2.0+

/*

 * MIPI-DSI Sony ACX424AKP panel driver. This is a 480x864

 * AMOLED panel with a command-only DSI interface.

 *

 * Copyright (C) Linaro Ltd. 2019

 * Author: Linus Walleij

 * Based on code and know-how from Marcus Lorentzon

 * Copyright (C) ST-Ericsson SA 2010

/*

 * Sony seems to use vendor ID 0x81

/*

 * The third ID looks like a bug, vendor IDs begin at 0x80

 * and panel 00 ... seems like default values.

/*

 * The timings are not very helpful as the display is used in

 * command mode using the maximum HS frequency.

	/*

	 * Some desired refresh rate, experiments at the maximum "pixel"

	 * clock speed (HS clock 420 MHz) yields around 117Hz.

 20Mhz */

 Disable backlight */

 Calculate the PWM duty cycle in n/256's */

 Set up PWM dutycycle ONE byte (differs from the standard) */

	/*

	 * Sequence to write PWMDIV:

	 *	address		data

	 *	0xF3		0xAA   CMD2 Unlock

	 *	0x00		0x01   Enter CMD2 page 0

	 *	0X7D		0x01   No reload MTP of CMD2 P1

	 *	0x22		PWMDIV

	 *	0x7F		0xAA   CMD2 page 1 lock

 Enable backlight */

 Assert RESET */

 De-assert RESET */

 Assert RESET */

 Enabe tearing mode: send TE (tearing effect) at VBLANK */

	/*

	 * Set MDDI

	 *

	 * This presumably deactivates the Qualcomm MDDI interface and

	 * selects DSI, similar code is found in other drivers such as the

	 * Sharp LS043T1LE01 which makes us suspect that this panel may be

	 * using a Novatek NT35565 or similar display driver chip that shares

	 * this command. Due to the lack of documentation we cannot know for

	 * sure.

 Exit sleep mode */

 In video mode turn peripheral on */

 Enter sleep mode */

 Number of modes */

	/*

	 * FIXME: these come from the ST-Ericsson vendor driver for the

	 * HREF520 and seems to reflect limitations in the PLLs on that

	 * platform, if you have the datasheet, please cross-check the

	 * actual max rates.

 Burst mode using event for sync */

 This asserts RESET by default */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Mantix MLAF057WE51 5.7" MIPI-DSI panel driver

 *

 * Copyright (C) Purism SPC 2020

 Manufacturer specific Commands send via DSI */

	/*

	 * Init sequence was supplied by the panel vendor.

 T11 */

 T14 */

 Focaltech FT8006P, section 7.3.1 and 7.3.4 */

 T1 + T2 */

 T2d */

 T3 + T4 + time for voltage to become stable: */

 T6 */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2020 Caleb Connolly <caleb@connolly.tech>

 * Generated with linux-mdss-dsi-panel-driver-generator from vendor device tree:

 * Copyright (c) 2020, The Linux Foundation. All rights reserved.

 This panel needs the high and low bytes swapped for the brightness value

 OnePlus 6 / enchilada

 OnePlus 6T / fajita

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Panel driver for the TPO TPG110 400CH LTPS TFT LCD Single Chip

 * Digital Driver.

 *

 * This chip drives a TFT LCD, so it does not know what kind of

 * display is actually connected to it, so the width and height of that

 * display needs to be supplied from the machine configuration.

 *

 * Author:

 * Linus Walleij <linus.walleij@linaro.org>

 Dual scan: outputs 800x480 */

 Dual scan: outputs 800x480 */

/**

 * struct tpg110_panel_mode - lookup struct for the supported modes

	/**

	 * @name: the name of this panel

	/**

	 * @magic: the magic value from the detection register

	/**

	 * @mode: the DRM display mode for this panel

	/**

	 * @bus_flags: the DRM bus flags for this panel e.g. inverted clock

/**

 * struct tpg110 - state container for the TPG110 panel

	/**

	 * @dev: the container device

	/**

	 * @spi: the corresponding SPI device

	/**

	 * @panel: the DRM panel instance for this device

	/**

	 * @panel_mode: the panel mode as detected

	/**

	 * @width: the width of this panel in mm

	/**

	 * @height: the height of this panel in mm

	/**

	 * @grestb: reset GPIO line

/*

 * TPG110 modes, these are the simple modes, the dualscan modes that

 * take 400x240 or 480x272 in and display as 800x480 are not listed.

		/*

		 * Clear address bit 0, 1 when writing, just to be sure

		 * The actual bit indicating a write here is bit 1, bit

		 * 0 is just surplus to pad it up to 8 bits.

 Set address bit 0 to 1 to read */

		/*

		 * The last bit/clock is Hi-Z turnaround cycle, so we need

		 * to send only 7 bits here. The 8th bit is the high impedance

		 * turn-around cycle.

 Read */

 De-assert the reset signal */

 Test display communication */

 Show display resolution */

 From the producer side, this is the same resolution */

 Take control over resolution and standby */

 Put chip into standby */

 Take chip out of standby */

/**

 * tpg110_get_modes() - return the appropriate mode

 * @panel: the panel to get the mode for

 * @connector: reference to the central DRM connector control structure

 *

 * This currently does not present a forest of modes, instead it

 * presents the mode that is configured for the system under use,

 * and which is detected by reading the registers of the display.

 We get the physical display dimensions from the DT */

 This asserts the GRESTB signal, putting the display into reset */

/*

 * Copyright (C) 2013, NVIDIA Corporation.  All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sub license,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/**

 * struct panel_desc - Describes a simple panel.

	/**

	 * @modes: Pointer to array of fixed modes appropriate for this panel.

	 *

	 * If only one mode then this can just be the address of the mode.

	 * NOTE: cannot be used with "timings" and also if this is specified

	 * then you cannot override the mode in the device tree.

* @num_modes: Number of elements in modes array. */

	/**

	 * @timings: Pointer to array of display timings

	 *

	 * NOTE: cannot be used with "modes" and also these will be used to

	 * validate a device tree override if one is present.

* @num_timings: Number of elements in timings array. */

* @bpc: Bits per color. */

* @size: Structure containing the physical size of this panel. */

		/**

		 * @size.width: Width (in mm) of the active display area.

		/**

		 * @size.height: Height (in mm) of the active display area.

* @delay: Structure containing various delay values for this panel. */

		/**

		 * @delay.prepare: Time for the panel to become ready.

		 *

		 * The time (in milliseconds) that it takes for the panel to

		 * become ready and start receiving video data

		/**

		 * @delay.enable: Time for the panel to display a valid frame.

		 *

		 * The time (in milliseconds) that it takes for the panel to

		 * display the first valid frame after starting to receive

		 * video data.

		/**

		 * @delay.disable: Time for the panel to turn the display off.

		 *

		 * The time (in milliseconds) that it takes for the panel to

		 * turn the display off (no content is visible).

		/**

		 * @delay.unprepare: Time to power down completely.

		 *

		 * The time (in milliseconds) that it takes for the panel

		 * to power itself down completely.

		 *

		 * This time is used to prevent a future "prepare" from

		 * starting until at least this many milliseconds has passed.

		 * If at prepare time less time has passed since unprepare

		 * finished, the driver waits for the remaining time.

* @bus_format: See MEDIA_BUS_FMT_... defines. */

* @bus_flags: See DRM_BUS_FLAG_... defines. */

* @connector_type: LVDS, eDP, DSI, DPI, etc. */

 Only add timings if override was not there or failed to validate */

	/*

	 * Only add fixed modes if timings/override added no mode.

	 *

	 * We should only ever have either the display timings specified

	 * or a fixed mode. Anything else is rather bogus.

 Unpreparing when already unprepared is a no-op */

 Preparing when already prepared is a no-op */

 probe EDID if a DDC bus is available */

 add hard-coded panel modes */

 set up connector's "panel orientation" property */

 Extract bus_flags from display_timing */

 We do not know the connector for the DT node, so guess it */

 Handle the generic panel-dpi binding */

 Catch common mistakes for panels. */

	/*

	 * We use runtime PM for prepare / unprepare since those power the panel

	 * on and off and those can be very slow operations. This is important

	 * to optimize powering the panel on briefly to read the EDID before

	 * fully enabling the panel.

 S070PWS19HP-FC21 2017/04/22 */

 S070SWV29HG-DC44 2017/09/21 */

	/*

	 * IWG22M: Y resolution changed for "dc_linuxfb" module crashing while

	 * fb_align

 60 Hz */

 50 Hz */

	/*

	 * According to the data sheet, the minimum horizontal blanking interval

	 * is 54 clocks (1 + 52 + 1), but tests with a Nitrogen6X have shown the

	 * minimum working horizontal blanking interval to be 60 clocks.

/*

 * 800x480 CVT. The panel appears to be quite accepting, at least as far as

 * pixel clocks, but this is the timing that was being used in the Adafruit

 * installation instructions.

/*

 * Specification at:

 * https://www.adafruit.com/images/product-files/2406/c3163.pdf

 56.16mm */

 74.88mm */

 60 Hz */

 50 Hz */

 152.4mm */

 91.4mm */

 50 Hz */

 60 Hz */

 The grayscale panel has 8 bit for the color .. Y (black) */

 This is the grayscale bus format */

 Must be the last entry */

 sentinel */

 sentinel */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (c) 2017, Fuzhou Rockchip Electronics Co., Ltd

/*

 * According to the discussion on

 * https://review.coreboot.org/#/c/coreboot/+/22472/

 * the panel init array is not part of the panels datasheet but instead

 * just came in this form from the panel vendor.

 voltage setting */

 VCOM disable */

 VCOM setting */

 VSP setting */

 VSN setting */

 VGH setting */

 VGL setting */

 Gamma setting */

 GOA MUX setting */

 GOA timing setting */

 GOE setting */

 T15: 120ms */

 T2: 15ms */

 T4: 15ms */

 T6: 120ms */

 T7: 10ms */

 SPDX-License-Identifier: GPL-2.0

/*

 * Raydium RM67191 MIPI-DSI panel driver

 *

 * Copyright 2019 NXP

 Panel specific color-format bits */

 Write Manufacture Command Set Control */

 Manufacturer Command Set pages (CMD2) */

/*

 * There is no description in the Reference Manual about these commands.

 * We received them from vendor, so just use them as is.

 for backward compatibility */

	/*

	 * Right after asserting the reset, we need to release it, so that the

	 * touch driver can have an active connection with the touch controller

	 * even after the display is turned off.

 Select User Command Set table (CMD1) */

 Software reset */

 Set DSI mode */

 Set tear ON */

 Set tear scanline */

 Set pixel format */

 Exit sleep mode */

 burst mode */

 non-burst mode with sync event */

 non-burst mode with sync pulse */

 sentinel */ }

/*

 * Copyright Â© 2016-2017 Broadcom

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 *

 * Portions of this file (derived from panel-simple.c) are:

 *

 * Copyright (C) 2013, NVIDIA Corporation.  All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sub license,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/*

 * Raspberry Pi 7" touchscreen panel driver.

 *

 * The 7" touchscreen consists of a DPI LCD panel, a Toshiba

 * TC358762XBG DSI-DPI bridge, and an I2C-connected Atmel ATTINY88-MUR

 * controlling power management, the LCD PWM, and initial register

 * setup of the Tohsiba.

 *

 * This driver controls the TC358762 and ATTINY88, presenting a DSI

 * device with a drm_panel.

 I2C registers of the Atmel microcontroller. */

 BIT(2) for horizontal flip, BIT(3) for vertical flip */

 DSI D-PHY Layer Registers */

 DSI PPI Layer Registers */

 DSI Protocol Layer Registers */

 DSI General Registers */

 DSI Application Layer Registers */

 LCDC/DPI Host Registers */

 DBI-B Host Registers */

 SPI Master Registers */

 System Controller Registers */

 GPIO Registers */

 I2C Registers */

 Chip/Rev Registers */

 Debug Registers */

		/* Modeline comes from the Raspberry Pi firmware, with HFP=1

		 * plugged in and clock re-computed from that.

 Wait for nPWRDWN to go low to indicate poweron is done. */

 Turn on the backlight. */

	/* Default to the same orientation as the closed source

	 * firmware used for the panel.  Runtime rotation

	 * configuration will be supported using VC4's plane

	 * orientation bits.

 ver 1 */

 ver 2 */

 Turn off at boot, so we can cleanly sequence powering on. */

 Look up the DSI host.  It needs to probe before we do. */

	/* This appears last, as it's what will unblock the DSI host

	 * driver's component bind function.

 sentinel */

 SPDX-License-Identifier: GPL-2.0+

/*

 * MIPI-DSI Samsung s6d16d0 panel driver. This is a 864x480

 * AMOLED panel with a command-only DSI interface.

/*

 * The timings are not very helpful as the display is used in

 * command mode.

 HS clock, (htotal*vtotal*vrefresh)/1000 */

 Enter sleep mode */

 Assert RESET */

 Assert RESET */

 De-assert RESET */

 Enabe tearing mode: send TE (tearing effect) at VBLANK */

 Exit sleep mode and power on */

 Number of modes */

	/*

	 * This display uses command mode so no MIPI_DSI_MODE_VIDEO

	 * or MIPI_DSI_MODE_VIDEO_SYNC_PULSE

	 *

	 * As we only send commands we do not need to be continuously

	 * clocked.

 This asserts RESET by default */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0+

/*

 * NEC NL8048HL11 Panel Driver

 *

 * Copyright (C) 2019 Texas Instruments Incorporated

 *

 * Based on the omapdrm-specific panel-nec-nl8048hl11 driver

 *

 * Copyright (C) 2010 Texas Instruments Incorporated

 * Author: Erik Gilling <konkers@android.com>

  NEC PIX Clock Ratings MIN:21.8MHz TYP:23.8MHz MAX:25.7MHz */

 Reinitialize the panel. */

 sentinel */ },

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Innolux/Chimei EJ030NA TFT LCD panel driver

 *

 * Copyright (C) 2020, Paul Cercueil <paul@crapouillou.net>

 * Copyright (C) 2020, Christophe Branchereau <cbranchereau@gmail.com>

 Reset the chip */

 60 Hz */

 50 Hz */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2018-2019, Bridge Systems BV

 * Copyright (C) 2018-2019, Bootlin

 * Copyright (C) 2017, Free Electrons

 *

 * This file based on panel-ilitek-ili9881c.c

 Default timings */

	/*

	 * We don't change the state of that GPIO later on but we need

	 * to force it into a low state.

	/*

	 * We don't change the state of that GPIO later on but we need

	 * to force it into a low state.

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0

/*

 * Panel driver for the Samsung S6D27A1 480x800 DPI RGB panel.

 * Found in the Samsung Galaxy Ace 2 GT-I8160 mobile phone.

 Password Command for Level 2 Control */

 Resolution Select Control */

 ASG Signal Control */

 Read panel ID 1 */

 Read panel ID 2 */

 Read panel ID 3 */

 Display Control */

 Manual Control */

 Power Control */

 Source Control */

 Panel Control*/

 sentinel */

	/*

	 * The vendor driver states that the S6D27A1 panel

	 * has a pixel clock frequency of 49920000 Hz / 2 = 24960000 Hz.

 Power up */

 Assert reset >=1 ms */

 De-assert reset */

 Wait >= 10 ms */

	/*

	 * Exit sleep mode and initialize display - some hammering is

	 * necessary.

 Magic to unlock level 2 control of the display */

 Configure resolution to 480RGBx800 */

 lock the level 2 control */

 Go into RESET and disable regulators */

	/*

	 * VCI   is the analog voltage supply

	 * VCCIO is the digital I/O voltage supply

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2018 Amarula Solutions

 * Author: Jagan Teki <jagan@amarulasolutions.com>

 T1 (dvdd start + dvdd rise) 0 < T1 <= 10ms */

 T3 (dvdd rise + avdd start + avdd rise) T3 >= 20ms */

	/*

	 * T5 + T6 (avdd rise + video & logic signal rise)

	 * T5 >= 10ms, 0 < T6 <= 10ms

 T12 (video & logic signal rise + backlight rise) T12 >= 200ms */

 T12 (video & logic signal rise + backlight rise) T12 >= 200ms */

 T13 (backlight fall + video & logic signal fall) T13 >= 200ms */

 T11 (dvdd rise to fall) 0 < T11 <= 10ms  */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Generic DSI Command Mode panel driver

 *

 * Copyright (C) 2013 Texas Instruments Incorporated - https://www.ti.com/

 * Author: Tomi Valkeinen <tomi.valkeinen@ti.com>

	unsigned long	hw_guard_end;	/* next value of jiffies when we can

					 * issue the next sleep in/out command

 max guard time in jiffies */

 runtime variables */

 reset the panel */

 assert reset */

 wait after releasing reset */

 BL | BCTRL */

 possible panel bug */

 If no backlight device is found assume native backlight support */

 SPDX-License-Identifier: GPL-2.0

/*

 * Driver for panels based on Sitronix ST7703 controller, souch as:

 *

 * - Rocktech jh057n00900 5.5" MIPI-DSI panel

 *

 * Copyright (C) Purism SPC 2019

 Manufacturer specific Commands send via DSI */

	/*

	 * Init sequence was supplied by the panel vendor. Most of the commands

	 * resemble the ST7703 but the number of parameters often don't match

	 * so it's likely a clone.

	/*

	 * Init sequence was supplied by the panel vendor.

 Magic sequence to unlock user commands below. */

 VC_main = 0, Lane_Number = 3 (4 lanes) */

 DSI_LDO_SEL = 1.7V, RTERM = 90 Ohm */

 IHSRX = x6 (Low High Speed driving ability) */

 TX_CLK_SEL = fDSICLK/16 */

 HFP_OSC (min. HFP number in DSI mode) */

 HBP_OSC (min. HBP number in DSI mode) */

 The rest is undocumented in ST7703 datasheet */

 PCCS = 2, ECP_DC_DIV = 1/4 HSYNC */

 DT = 15ms XDK_ECP = x2 */

 PFM_DC_DIV = /1 */

 ECP_SYNC_EN = 1, VGX_SYNC_EN = 1 */);

 RGB I/F porch timing */

 VBP_RGB_GEN */

 VFP_RGB_GEN */

 DE_BP_RGB_GEN */

 DE_FP_RGB_GEN */

 The rest is undocumented in ST7703 datasheet */

 Source driving settings. */

 N_POPON */

 N_NOPON */

 I_POPON */

 I_NOPON */

 SCR[31,24] */

 SCR[23,16] */

 SCR[15,8] */

 SCR[7,0] */

 Undocumented */);

 NVDDD_SEL = -1.8V, VDDD_SEL = out of range (possibly 1.9V?) */

	/*

	 * SS_PANEL = 1 (reverse scan), GS_PANEL = 0 (normal scan)

	 * REV_PANEL = 1 (normally black panel), BGR_PANEL = 1 (BGR)

 Zig-Zag Type C column inversion. */

 Set display resolution. */

 NL = 240 */

			  0x12, /* RES_V_LSB = 0, BLK_CON = VSSD,

				 * RESO_SEL = 720RGB

			  0xF0  /* WHITE_GND_EN = 1 (GND),

				 * WHITE_FRAME_SEL = 7 frames,

				 * ISC = 0 frames

 PNOEQ */

 NNOEQ */

 PEQGND */

 NEQGND */

 PEQVCI */

 NEQVCI */

 PEQVCI1 */

 NEQVCI1 */

 reserved */

 reserved */

 reserved */

 reserved */

 ESD_DET_DATA_WHITE = 1, ESD_WHITE_EN = 1 */

			  0x10  /* SLPIN_OPTION = 1 (no need vsync after sleep-in)

				 * VEDIO_NO_CHECK_EN = 0

				 * ESD_WHITE_GND_EN = 0

				 * ESD_DET_TIME_SEL = 0 frames

 Undocumented command. */

 VBTHS, VBTLS: VGH = 17V, VBL = -11V */

 FBOFF_VGH = 0, FBOFF_VGL = 0 */

 VRP  */

 VRN */

 reserved */

			  0xF1, /* APS = 1 (small),

				 * VGL_DET_EN = 1, VGH_DET_EN = 1,

				 * VGL_TURBO = 1, VGH_TURBO = 1

 VGH1_L_DIV, VGL1_L_DIV (1.5MHz) */

 VGH1_R_DIV, VGL1_R_DIV (1.5MHz) */

 VGH2_L_DIV, VGL2_L_DIV (2.6MHz) */

 VGH2_R_DIV, VGL2_R_DIV (2.6MHz) */

 VGH3_L_DIV, VGL3_L_DIV (4.5MHz) */

 VGH3_R_DIV, VGL3_R_DIV (4.5MHz) */);

 Reference voltage. */

 VREF_SEL = 4.2V */

 NVREF_SEL = 4.2V */);

 VCOMDC_F = -0.67V */

 VCOMDC_B = -0.67V */);

 Undocumented command. */

 This command is to set forward GIP timing. */

 This command is to set backward GIP timing. */

 Adjust the gamma characteristics of the panel. */

 Panel is operational 120 msec after reset */

 Reset the panel to get video back */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Novatek NT39016 TFT LCD panel driver

 *

 * Copyright (C) 2017, Maarten ter Huurne <maarten@treewalker.org>

 * Copyright (C) 2019, Paul Cercueil <paul@crapouillou.net>

	/*

	 * Reset the NT39016.

	 * The documentation says the reset pulse should be at least 40 us to

	 * pass the glitch filter, but when testing I see some resets fail and

	 * some succeed when using a 70 us delay, so we use 100 us instead.

 Init all registers. */

 Wait for the picture to be ready before enabling backlight */

 60 Hz */

 50 Hz */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2018 MediaTek Inc.

 * Author: Jitao Shi <jitao.shi@mediatek.com>

	/**

	 * @width_mm: width of the panel's active display area

	 * @height_mm: height of the panel's active display area

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2016 InforceComputing

 * Author: Vinay Simha BN <simhavcs@gmail.com>

 *

 * Copyright (C) 2016 Linaro Ltd

 * Author: Sumit Semwal <sumit.semwal@linaro.org>

 *

 * From internet archives, the panel for Nexus 7 2nd Gen, 2013 model is a

 * JDI model LT070ME05000, and its data sheet is at:

 * http://panelone.net/en/7-0-inch/JDI_LT070ME05000_7.0_inch-datasheet

	/*

	 * BIT(5) BCTRL = 1 Backlight Control Block On, Brightness registers

	 *                  are active

	 * BIT(3) BL = 1    Backlight Control On

	 * BIT(2) DD = 0    Display Dimming is Off

 CABC off */

 Interface setting, video mode */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Ilitek ILI9341 TFT LCD drm_panel driver.

 *

 * This panel can be configured to support:

 * - 16-bit parallel RGB interface

 * - 18-bit parallel RGB interface

 * - 4-line serial spi interface

 *

 * Copyright (C) 2021 Dillon Min <dillon.minfei@gmail.com>

 *

 * For dbi+dpi part:

 * Derived from drivers/drm/gpu/panel/panel-ilitek-ili9322.c

 * the reuse of DBI abstraction part referred from Linus's patch

 * "drm/panel: s6e63m0: Switch to DBI abstraction for SPI"

 *

 * For only-dbi part, copy from David's code (drm/tiny/ili9341.c)

 * Copyright 2018 David Lechner <david@lechnology.com>

 RGB Interface Signal Control */

 Frame Rate Control register */

 Display Function Control register */

 Power Control 1 register */

 Power Control 2 register */

 VCOM Control 1 register */

 VCOM Control 2 register */

 Power control A register */

 Power control B register */

 Positive Gamma Correction register */

 Negative Gamma Correction register */

 Driver timing control A */

 Driver timing control B */

 Power on sequence register */

 3 Gamma enable register */

 Interface control register */

 Pump ratio control register */

 Entry mode set */

 struct ili9341_config - the system specific ILI9341 configuration */

 mode: the drm display mode */

 ca: TODO: need comments for this register */

 power_b: TODO: need comments for this register */

 power_seq: TODO: need comments for this register */

 dtca: TODO: need comments for this register */

 dtcb: TODO: need comments for this register */

 power_a: TODO: need comments for this register */

 frc: Frame Rate Control (In Normal Mode/Full Colors) (B1h) */

 prc: TODO: need comments for this register */

 dfc_1: B6h DISCTRL (Display Function Control) */

 power_1: Power Control 1 (C0h) */

 power_2: Power Control 2 (C1h) */

 vcom_1: VCOM Control 1(C5h) */

 vcom_2: VCOM Control 2(C7h) */

 address_mode: Memory Access Control (36h) */

 g3amma_en: TODO: need comments for this register */

 rgb_interface: RGB Interface Signal Control (B0h) */

 dfc_2: refer to dfc_1 */

 column_addr: Column Address Set (2Ah) */

 page_addr: Page Address Set (2Bh) */

 interface: Interface Control (F6h) */

	/*

	 * pixel_format: This command sets the pixel format for the RGB

	 * image data used by

	/*

	 * gamma_curve: This command is used to select the desired Gamma

	 * curve for the

 pgamma: Positive Gamma Correction (E0h) */

 ngamma: Negative Gamma Correction (E1h) */

/*

 * The Stm32f429-disco board has a panel ili9341 connected to ltdc controller

 hfp 10 */

 hsync 10 */

 hbp 20 */

 vfp 4 */

 vsync 2 */

 vbp 2 */

 0x00 fosc, 0x1b 70hz */

	/*

	 * 0x0a Interval scan, AGND AGND AGND AGND

	 * 0xa2 Normally white, G1 -> G320, S720 -> S1,

	 *	Scan Cycle 5 frames,85ms

 0x10 3.65v */

 0x10 AVDD=vci*2, VGH=vci*7, VGL=-vci*4 */

 0x45 VCOMH 4.425v, 0x15 VCOML -1.975*/

 0x90 offset voltage, VMH-48, VML-48 */

	/*

	 * 0xc8 Row Address Order, Column Address Order

	 * BGR 1

	/*

	 * 0xc2

	 * Display Data Path: Memory

	 * RGB: DE mode

	 * DOTCLK polarity set (data fetched at the falling time)

	/*

	 * 0x0a

	 * Gate outputs in non-display area: Interval scan

	 * Determine source/VCOM output in a non-display area in the partial

	 * display mode: AGND AGND AGND AGND

	 *

	 * 0xa7

	 * Scan Cycle: 15 frames

	 * fFLM = 60Hz: 255ms

	 * Liquid crystal type: Normally white

	 * Gate Output Scan Direction: G1 -> G320

	 * Source Output Scan Direction: S720 -> S1

	 *

	 * 0x27

	 * LCD Driver Line: 320 lines

	 *

	 * 0x04

	 * PCDIV: 4

 column address: 240 */

 page address: 320 */

	/*

	 * Memory write control: When the transfer number of data exceeds

	 * (EC-SC+1)*(EP-SP+1), the column and page number will be

	 * reset, and the exceeding data will be written into the following

	 * column and page.

	 * Display Operation Mode: RGB Interface Mode

	 * Interface for RAM Access: RGB interface

	 * 16- bit RGB interface (1 transfer/pixel)

 DPI: 16 bits / pixel */

 Curve Selected: Gamma curve 1 (G2.2) */

 Power Control */

 VCOM */

 Gamma */

 Colomn address set */

 Page address set */

 Format */

 Assert RESET */

 Enable power */

 De-assert RESET */

 Assert RESET */

 Disable power */

 Set up the polarity */

 Number of modes */

 Power Control */

 VCOM */

 Memory Access Control */

 Frame Rate */

 Gamma */

 DDRAM */

 Display */

	/*

	 * Every new incarnation of this display must have a unique

	 * data entry for the system in this driver.

		/* porting from tiny/ili9341.c

		 * for original mipi dbi compitable

 SPDX-License-Identifier: GPL-2.0-only

 Copyright (C) 2019, Michael Srba

 enable LEVEL2 commands

 set Pixel Clock Divider polarity

 set default brightness/gama

 V255 RR,GG,BB

 V203 R,G,B

 V151 R,G,B

 V87  R,G,B

 V51  R,G,B

 V35  R,G,B

 V23  R,G,B

 V11  R,G,B

 V3   R,G,B

 V1   R,G,B

 set default Amoled Off Ratio

 set default elvss voltage

 gamma/aor update

 disable LEVEL2 commands

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MIPI-DSI based s6e3ha2 AMOLED 5.7 inch panel driver.

 *

 * Copyright (c) 2016 Samsung Electronics Co., Ltd.

 * Donghwa Lee <dh09.lee@samsung.com>

 * Hyungwon Hwang <human.hwang@samsung.com>

 * Hoegeun Kwon <hoegeun.kwon@samsung.com>

 need for 100ns delay */

 common setting */

 pcd setting off for TB */

 brightness setting */

 elvss temp compensation */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2018, The Linux Foundation. All rights reserved.

 CMD2_P0 */

 CMD2_P4 */

 STV */

 Vend */

 UD */

 CLK */

 Reset XDONB */

 Resolution:1440x2560 */

 mux */

 ABOFF */

 Source EQ */

 FP BP */

 Inversion Type */

 IMGSWAP =1 @PortSwap=1 */

 FRM */

 CMD1 */

 VBP+VSA=,VFP = 10H */

 FTE on */

 EN_BK =1(auto black) */

 CMD mode(10) VDO mode(03) */

 Non Reload MTP */

	/*

	 * Reset sequence of truly panel requires the panel to be

	 * out of reset for 10ms, followed by being held in reset

	 * for 10ms and then out again

 120ms delay required here as per DCS spec */

 Per DSI spec wait 120ms after sending exit sleep DCS command */

 Per DSI spec wait 120ms after sending set_display_on DCS command */

 dual port */

	/*

	 * This device represents itself as one with two input ports which are

	 * fed by the output ports of the two DSI controllers . The DSI0 is

	 * the master controller and has most of the panel related info in its

	 * child node.

 register the second DSI device */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MIPI-DSI based S6E63J0X03 AMOLED lcd 1.63 inch panel driver.

 *

 * Copyright (c) 2014-2017 Samsung Electronics Co., Ltd

 *

 * Inki Dae <inki.dae@samsung.com>

 * Hoegeun Kwon <hoegeun.kwon@samsung.com>

 Gamma 10 */

 gamma 30 */

 gamma 60 */

 gamma 90 */

 gamma 120 */

 gamma 150 */

 gamma 200 */

 gamma 240 */

 gamma 300 */

 set porch adjustment */

 set frame freq */

 set caset, paset */

 set ltps timming 0, 1 */

 set param pos te_edge */

 set te rising edge */

 set param pos default */

 set elvss_cond */

 set pos */

 set default white brightness */

 set white ctrl */

 set acl off */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2017-2018, Bootlin

 GPWR1/2 non overlap time 2.62us */

 ILI4003D sel */

 Set VCORE voltage = 1.5V */

 di_pwr_reg=0 for power mode 2A, VGH clamp 18V */

 pumping ratio VGH=5x VGL=-3x */

 VGL clamp -10V */

 ESD */

 POWER SAVING */

 BGR, SS */

 Zigzag type3 inversion */

 ILI4003D sel */

 VP251 */

 VP247 */

 VP243 */

 VP239 */

 VP231 */

 VP219 */

 VP203 */

 VP175 */

 VP144 */

 VP111 */

 VP80 */

 VP52 */

 VP36 */

 VP24 */

 VP16 */

 VP12 */

 VP8 */

 VP4 */

 VP0 */

 VN255 GAMMA N */

 VN251 */

 VN247 */

 VN243 */

 VN239 */

 VN231 */

 VN219 */

 VN203 */

 VN175 */

 VN144 */

 VN111 */

 VN80 */

 VN52 */

 VN36 */

 VN24 */

 VN16 */

 VN12 */

 VN8 */

 VN4 */

 VN0 */

/*

 * The panel seems to accept some private DCS commands that map

 * directly to registers.

 *

 * It is organised by page, with each page having its own set of

 * registers, and the first page looks like it's holding the standard

 * DCS commands.

 *

 * So before any attempt at sending a command or data, we have to be

 * sure if we're in the right page or not.

 Power the panel */

 And reset it */

 SPDX-License-Identifier: GPL-2.0

/*

 * DSI interface to the Samsung S6E63M0 panel.

 * (C) 2019 Linus Walleij

 CMD + 15 bytes max */

 Pick out and skip past the DCS command */

 Send max S6E63M0_DSI_MAX_CHUNK bytes at a time */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Panel driver for the ARM Versatile family reference designs from

 * ARM Limited.

 *

 * Author:

 * Linus Walleij <linus.wallei@linaro.org>

 *

 * On the Versatile AB, these panels come mounted on daughterboards

 * named "IB1" or "IB2" (Interface Board 1 & 2 respectively.) They

 * are documented in ARM DUI 0225D Appendix C and D. These daughter

 * boards support TFT display panels.

 *

 * - The IB1 is a passive board where the display connector defines a

 *   few wires for encoding the display type for autodetection,

 *   suitable display settings can then be looked up from this setting.

 *   The magic bits can be read out from the system controller.

 *

 * - The IB2 is a more complex board intended for GSM phone development

 *   with some logic and a control register, which needs to be accessed

 *   and the board display needs to be turned on explicitly.

 *

 * On the Versatile PB, a special CLCD adaptor board is available

 * supporting the same displays as the Versatile AB, plus one more

 * Epson QCIF display.

 *

/*

 * This configuration register in the Versatile and RealView

 * family is uniformly present but appears more and more

 * unutilized starting with the RealView series.

 The Versatile can detect the connected panel type */

 IB2 control register for the Versatile daughterboard */

 1 = shut down LCD */

/**

 * struct versatile_panel_type - lookup struct for the supported panels

	/**

	 * @name: the name of this panel

	/**

	 * @magic: the magic value from the detection register

	/**

	 * @mode: the DRM display mode for this panel

	/**

	 * @bus_flags: the DRM bus flags for this panel e.g. inverted clock

	/**

	 * @width_mm: the panel width in mm

	/**

	 * @height_mm: the panel height in mm

	/**

	 * @ib2: the panel may be connected on an IB2 daughterboard

/**

 * struct versatile_panel - state container for the Versatile panels

	/**

	 * @dev: the container device

	/**

	 * @panel: the DRM panel instance for this device

	/**

	 * @panel_type: the Versatile panel type as detected

	/**

	 * @map: map to the parent syscon where the main register reside

	/**

	 * @ib2_map: map to the IB2 syscon, if applicable

	/*

	 * Sanyo TM38QV67A02A - 3.8 inch QVGA (320x240) Color TFT

	 * found on the Versatile AB IB1 connector or the Versatile

	 * PB adaptor board connector.

	/*

	 * Sharp LQ084V1DG21 640x480 VGA Color TFT module

	 * found on the Versatile AB IB1 connector or the Versatile

	 * PB adaptor board connector.

	/*

	 * Epson L2F50113T00 - 2.2 inch QCIF 176x220 Color TFT

	 * found on the Versatile PB adaptor board connector.

	/*

	 * Sanyo ALR252RGT 240x320 portrait display found on the

	 * Versatile AB IB2 daughterboard for GSM prototyping.

 If we're on an IB2 daughterboard, turn off display */

 If we're on an IB2 daughterboard, turn on display */

 No panel detected or VGA, let's leave this show */

 Check if the panel is mounted on an IB2 daughterboard */

/*

 * Copyright (C) 2013, NVIDIA Corporation.  All rights reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sub license,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING

 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER

 * DEALINGS IN THE SOFTWARE.

/**

 * struct panel_delay - Describes delays for a simple panel.

	/**

	 * @hpd_reliable: Time for HPD to be reliable

	 *

	 * The time (in milliseconds) that it takes after powering the panel

	 * before the HPD signal is reliable. Ideally this is 0 but some panels,

	 * board designs, or bad pulldown configs can cause a glitch here.

	 *

	 * NOTE: on some old panel data this number appers to be much too big.

	 * Presumably some old panels simply didn't have HPD hooked up and put

	 * the hpd_absent here because this field predates the

	 * hpd_absent. While that works, it's non-ideal.

	/**

	 * @hpd_absent: Time to wait if HPD isn't hooked up.

	 *

	 * Add this to the prepare delay if we know Hot Plug Detect isn't used.

	 *

	 * This is T3-max on eDP timing diagrams or the delay from power on

	 * until HPD is guaranteed to be asserted.

	/**

	 * @prepare_to_enable: Time between prepare and enable.

	 *

	 * The minimum time, in milliseconds, that needs to have passed

	 * between when prepare finished and enable may begin. If at

	 * enable time less time has passed since prepare finished,

	 * the driver waits for the remaining time.

	 *

	 * If a fixed enable delay is also specified, we'll start

	 * counting before delaying for the fixed delay.

	 *

	 * If a fixed prepare delay is also specified, we won't start

	 * counting until after the fixed delay. We can't overlap this

	 * fixed delay with the min time because the fixed delay

	 * doesn't happen at the end of the function if a HPD GPIO was

	 * specified.

	 *

	 * In other words:

	 *   prepare()

	 *     ...

	 *     // do fixed prepare delay

	 *     // wait for HPD GPIO if applicable

	 *     // start counting for prepare_to_enable

	 *

	 *   enable()

	 *     // do fixed enable delay

	 *     // enforce prepare_to_enable min time

	 *

	 * This is not specified in a standard way on eDP timing diagrams.

	 * It is effectively the time from HPD going high till you can

	 * turn on the backlight.

	/**

	 * @enable: Time for the panel to display a valid frame.

	 *

	 * The time (in milliseconds) that it takes for the panel to

	 * display the first valid frame after starting to receive

	 * video data.

	 *

	 * This is (T6-min + max(T7-max, T8-min)) on eDP timing diagrams or

	 * the delay after link training finishes until we can turn the

	 * backlight on and see valid data.

	/**

	 * @disable: Time for the panel to turn the display off.

	 *

	 * The time (in milliseconds) that it takes for the panel to

	 * turn the display off (no content is visible).

	 *

	 * This is T9-min (delay from backlight off to end of valid video

	 * data) on eDP timing diagrams. It is not common to set.

	/**

	 * @unprepare: Time to power down completely.

	 *

	 * The time (in milliseconds) that it takes for the panel

	 * to power itself down completely.

	 *

	 * This time is used to prevent a future "prepare" from

	 * starting until at least this many milliseconds has passed.

	 * If at prepare time less time has passed since unprepare

	 * finished, the driver waits for the remaining time.

	 *

	 * This is T12-min on eDP timing diagrams.

/**

 * struct panel_desc - Describes a simple panel.

	/**

	 * @modes: Pointer to array of fixed modes appropriate for this panel.

	 *

	 * If only one mode then this can just be the address of the mode.

	 * NOTE: cannot be used with "timings" and also if this is specified

	 * then you cannot override the mode in the device tree.

* @num_modes: Number of elements in modes array. */

	/**

	 * @timings: Pointer to array of display timings

	 *

	 * NOTE: cannot be used with "modes" and also these will be used to

	 * validate a device tree override if one is present.

* @num_timings: Number of elements in timings array. */

* @bpc: Bits per color. */

* @size: Structure containing the physical size of this panel. */

		/**

		 * @size.width: Width (in mm) of the active display area.

		/**

		 * @size.height: Height (in mm) of the active display area.

* @delay: Structure containing various delay values for this panel. */

/**

 * struct edp_panel_entry - Maps panel ID to delay / panel name.

* @panel_id: 32-bit ID for panel, encoded with drm_edid_encode_panel_id(). */

 @delay: The power sequencing delays needed for this panel. */

 @name: Name of this panel (for printing to logs). */

 Only add timings if override was not there or failed to validate */

	/*

	 * Only add fixed modes if timings/override added no mode.

	 *

	 * We should only ever have either the display timings specified

	 * or a fixed mode. Anything else is rather bogus.

 Unpreparing when already unprepared is a no-op */

/*

 * Some panels simply don't always come up and need to be power cycled to

 * work properly.  We'll allow for a handful of retries.

 Preparing when already prepared is a no-op */

	/*

	 * If there is a "prepare_to_enable" delay then that's supposed to be

	 * the delay from HPD going high until we can turn the backlight on.

	 * However, we can only count this if HPD is handled by the panel

	 * driver, not if it goes to a dedicated pin on the controller.

	 * If we aren't handling the HPD pin ourselves then the best we

	 * can do is assume that HPD went high immediately before we were

	 * called (and link training took zero time).

	 *

	 * NOTE: if we ever end up in this "if" statement then we're

	 * guaranteed that the panel_edp_wait() call below will do no delay.

	 * It already handles that case, though, so we don't need any special

	 * code for it.

 probe EDID if a DDC bus is available */

	/*

	 * Add hard-coded panel modes. Don't call this if there are no timings

	 * and no modes (the generic edp-panel case) because it will clobber

	 * the display_info that was already set by drm_add_edid_modes().

 set up connector's "panel orientation" property */

	/*

	 * Read the dts properties for the initial probe. These are used by

	 * the runtime resume code which will get called by the

	 * pm_runtime_get_sync() call below.

 Power the panel on so we can read the EDID */

	/*

	 * We're using non-optimized timings and want it really obvious that

	 * someone needs to add an entry to the table, so we'll do a WARN_ON

	 * splat.

		/*

		 * It's highly likely that the panel will work if we use very

		 * conservative timings, so let's do that. We already know that

		 * the HPD-related delays must have worked since we got this

		 * far, so we really just need the "unprepare" / "enable"

		 * delays. We don't need "prepare_to_enable" since that

		 * overlaps the "enable" delay anyway.

		 *

		 * Nearly all panels have a "unprepare" delay of 500 ms though

		 * there are a few with 1000. Let's stick 2000 in just to be

		 * super conservative.

		 *

		 * An "enable" delay of 80 ms seems the most common, but we'll

		 * throw in 200 ms to be safe.

 Update the delay; everything else comes from EDID */

	/*

	 * We use runtime PM for prepare / unprepare since those power the panel

	 * on and off and those can be very slow operations. This is important

	 * to optimize powering the panel on briefly to read the EDID before

	 * fully enabling the panel.

 generic_edp_panel_probe() replaces desc in the panel */

 TODO: should be hpd-absent and no-hpd should be set? */

 Also used for boe_nv133fhm_n62 */

 Also used for boe_nv133fhm_n62 */

		/*

		 * When power is first given to the panel there's a short

		 * spike on the HPD line.  It was explained that this spike

		 * was until the TCON data download was complete.  On

		 * one system this was measured at 8 ms.  We'll put 15 ms

		 * in the prepare delay just to be safe.  That means:

		 * - If HPD isn't hooked up you still have 200 ms delay.

		 * - If HPD is hooked up we won't try to look at it for the

		 *   first 15 ms.

 TODO: should be hpd-absent and no-hpd should be set? */

/*

 * Datasheet specifies that at 60 Hz refresh rate:

 * - total horizontal time: { 1506, 1592, 1716 }

 * - total vertical time: { 788, 800, 868 }

 *

 * ...but doesn't go into exactly how that should be split into a front

 * porch, back porch, or sync length.  For now we'll leave a single setting

 * here which allows a bit of tweaking of the pixel clock at the expense of

 * refresh rate.

 TODO: should be hpd-absent and no-hpd should be set? */

 Must be first */

 sentinel */

/*

 * This table is used to figure out power sequencing delays for panels that

 * are detected by EDID. Entries here may point to entries in the

 * platform_of_match table (if a panel is listed in both places).

 *

 * Sort first by vendor, then by product ID.

 sentinal */ }

 Skip one since "edp-panel" is only supported on DP AUX bus */

 Same as platform one! */

 SPDX-License-Identifier: GPL-2.0

/*

 * Sony ACX565AKM LCD Panel driver

 *

 * Copyright (C) 2019 Texas Instruments Incorporated

 *

 * Based on the omapdrm-specific panel-sony-acx565akm driver

 *

 * Copyright (C) 2010 Nokia Corporation

 * Author: Imre Deak <imre.deak@nokia.com>

/*

 * TODO (to be addressed with hardware access to test the changes):

 *

 * - Update backlight support to use backlight_update_status() etc.

 * - Use prepare/unprepare for the basic power on/off of the backligt

	/*

	 * Next value of jiffies when we can issue the next sleep in/out

	 * command.

 max guard time in jiffies */

		/*

		 * Between the command and the response data there is a

		 * dummy clock cycle. Add an extra bit after the command

		 * word to account for this.

/* -----------------------------------------------------------------------------

 * Auto Brightness Control Via sysfs

 always used when CABC is not supported */

/* -----------------------------------------------------------------------------

 * Backlight Device

/* -----------------------------------------------------------------------------

 * DRM Bridge Operations

	/*

	 * We have to keep 120msec between sleep in/out commands.

	 * (8.2.15, 8.2.16).

FIXME tweak me */

	/*

	 * We have to meet all the following delay requirements:

	 * 1. tRW: reset pulse width 10usec (7.12.1)

	 * 2. tRT: reset cancel time 5msec (7.12.1)

	 * 3. Providing PCLK,HS,VS signals for 2 frames = ~50msec worst

	 *    case (7.6.2)

	 * 4. 120msec before the sleep out command (7.12.1)

 5msec between sleep out and the next command. (8.2.16) */

	/*

	 * We have to provide PCLK,HS,VS signals for 2 frames (worst case

	 * ~50msec) after sending the sleep in command and asserting the

	 * reset signal. We probably could assert the reset w/o the delay

	 * but we still delay to avoid possible artifacts. (7.6.1)

 FIXME need to tweak this delay */

/* -----------------------------------------------------------------------------

 * Probe, Detect and Remove

	/*

	 * After being taken out of reset the panel needs 5ms before the first

	 * command can be sent.

 sentinel */ },

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Panel driver for the WideChips WS2401 480x800 DPI RGB panel, used in

 * the Samsung Mobile Display (SMD) LMS380KF01.

 * Found in the Samsung Galaxy Ace 2 GT-I8160 mobile phone.

 * Linus Walleij <linus.walleij@linaro.org>

 * Inspired by code and know-how in the vendor driver by Gareth Phillips.

 Resolution select control */

 SMPS positive control */

 SMPS negative control */

 Backlight control mode */

 Backlight control */

 Write manual brightness */

 Write BL control */

 Write MIE mode */

 Read panel ID 1 */

 Read panel ID 2 */

 Read panel ID 3 */

 Gamma red 1 */

 Gamma green 1 */

 Gamma blue 1 */

 Gamma red 2 */

 Gamma green 2 */

 Gamma blue 2 */

 Password command for level 2 */

 Display control */

 Power control */

 VCOM control */

 Source control */

 Panel control */

 sentinel */

/**

 * struct ws2401 - state container for a panel controlled by the WS2401

 * controller

* @dev: the container device */

* @dbi: the DBI bus abstraction handle */

* @panel: the DRM panel instance for this device */

* @width: the width of this panel in mm */

* @height: the height of this panel in mm */

* @reset: reset GPIO line */

* @regulators: VCCIO and VIO supply regulators */

* @internal_bl: If using internal backlight */

	/*

	 * The vendor driver states that the "SMD panel" has a clock

	 * frequency of 49920000 Hz / 2 = 24960000 Hz.

 Power up */

 Assert reset >=1 ms */

 De-assert reset */

 Wait >= 10 ms */

	/*

	 * Exit sleep mode and initialize display - some hammering is

	 * necessary.

 Magic to unlock level 2 control of the display */

 Configure resolution to 480RGBx800 */

 Set addressing mode Flip V(d0), Flip H(d1) RGB/BGR(d3) */

 Set pixel format: 24 bpp */

 DDVDH: 4.6v */

 DDVDH: -4.6v */

 VGH:16.1v, VGL:-13.8v */

 GREFP:4.2v (default) */

 GREFN:-4.2v (default) */

 VOUTL:-10v (default) */

 2 dot inversion */

 Set up gamma, probably these are P-gamma and N-gamma for each color */

		/*

		 * When not using internal backlight we do not need any further

		 * L2 accesses to the panel so we close the door on our way out.

		 * Otherwise we need to leave the L2 door open.

 Go into RESET and disable regulators */

 Make sure we disable backlight, if any */

/**

 * ws2401_get_modes() - return the mode

 * @panel: the panel to get the mode for

 * @connector: reference to the central DRM connector control structure

	/*

	 * We just support the LMS380KF01 so far, if we implement more panels

	 * this mode, the following connector display_info settings and

	 * probably the custom DCS sequences needs to selected based on what

	 * the target panel needs.

	/*

	 * VCI   is the analog voltage supply

	 * VCCIO is the digital I/O voltage supply

/*

 * Samsung LMS380KF01 is the one instance of this display controller that we

 * know about, but if more are found, the controller can be parameterized

 * here and used for other configurations.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MIPI-DSI based s6e8aa0 AMOLED LCD 5.3 inch panel driver.

 *

 * Copyright (c) 2013 Samsung Electronics Co., Ltd

 *

 * Inki Dae, <inki.dae@samsung.com>

 * Donghwa Lee, <dh09.lee@samsung.com>

 * Joongmock Shin <jmock.shin@samsung.com>

 * Eunchul Kim <chulspro.kim@samsung.com>

 * Tomasz Figa <t.figa@samsung.com>

 * Andrzej Hajda <a.hajda@samsung.com>

	/* This field is tested by functions directly accessing DSI bus before

	 * transfer, transfer is skipped if it is set. In case of transfer

	 * failure or unexpected response the field is set to error value.

	 * Such construct allows to eliminate many checks in higher level

	 * functions.

 GTCON */

 SS */

 CLK1,2_CON */

 INT1,2_CON */

 BICTL,B_CON */

 EM_CLK1,1B_CON */

 EM_CLK2,2B_CON */

 EM_INT1,2_CON */

 30cd ~ 100cd */

 120cd ~ 150cd */

 180cd ~ 210cd */

 240cd ~ 300cd */

 update gamma table. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 Theobroma Systems Design und Consulting GmbH

 BGR SS GS */

 column inversion */

 VCOM1 */

 VCOM2 */

 VREG1OUT=5V */

 VREG2OUT=-5V */

 EQT Time setting */

/*

 * The vendor init selected page 1 here _again_

 * Is this supposed to be page 2?

 VGH_MOD clamp level=15v */

 VGH clamp level 15V */

 VGL clamp level (-10V) */

 GAMMA OP */

 SOURCE OP */

 PS_EN OFF */

 LVD */

	/*

	 * Init sequence was supplied by the panel vendor without much

	 * documentation.

	/*

	 * Init sequence was supplied by the panel vendor without much

	 * documentation.

 vendor code called this without param, where there should be one */

 T9: 120ms */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2019-2020 Icenowy Zheng <icenowy@aosc.io>

 Switch to page 0 */

 Seems to be some password */

 Lane number, 0x02 - 3 lanes, 0x03 - 4 lanes */

 Sequence control */

 Switch to page 1 */

 Set VCOM */

 Set VCOM_Reverse */

 Set Gamma Power, VG[MS][PN] */

 VGMN = -4.5V */

 Set Gate Power */

 VGH_R = 15V */

 VGL_R = -11V */

 VGL_R2 = -11V */

 PA[6:4] = 0, PA[0] = 0 */

 Set Panel */

 SS = 1, BGR = 1 */

 Set RGBCYC */

 JDT = 100 column inversion */

 RGB_N_EQ1 */

 RGB_N_EQ2 */

 set EQ3 for TE_H */

 set CHGEN_ON */

 set CHGEN_OFF */

 set CHGEN_OFF2 */

 Set TCON parameter */

 RSO = 800 points */

 LN = 1280 lines */

 Set power voltage */

 DCDCM */

 Set gamma */

 Switch to page 2, for GIP */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Toppoly TD028TTEC1 Panel Driver

 *

 * Copyright (C) 2019 Texas Instruments Incorporated

 *

 * Based on the omapdrm-specific panel-tpo-td028ttec1 driver

 *

 * Copyright (C) 2008 Nokia Corporation

 * Author: Tomi Valkeinen <tomi.valkeinen@ti.com>

 *

 * Neo 1973 code (jbt6k74.c):

 * Copyright (C) 2006-2007 OpenMoko, Inc.

 * Author: Harald Welte <laforge@openmoko.org>

 *

 * Ported and adapted from Neo 1973 U-Boot by:

 * H. Nikolaus Schaller <hns@goldelico.com>

/*

 * noinline_for_stack so we don't get multiple copies of tx_buf

 * on the stack in case of gcc-plugin-structleak

 Three times command zero */

 deep standby out */

 RGB I/F on, RAM write off, QVGA through, SIGCON enable */

 Quad mode off */

 AVDD on, XVDD on */

 Output control */

 Sleep mode off */

 at this point we have like 50% grey */

 initialize register set */

	/*

	 * default of 0x02 in JBT_REG_ASW_SLEW responsible for 72Hz requirement

	 * to avoid red / blue flicker

	/*

	 * FIXME: According to the datasheet sync signals are sampled on the

	 * rising edge of the clock, but the code running on the OpenMoko Neo

	 * FreeRunner and Neo 1973 indicates sampling on the falling edge. This

	 * should be tested on a real device.

 DT backward compatibility. */

 sentinel */ },

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (c) 2019 Theobroma Systems Design und Consulting GmbH

 *

 * base on panel-kingdisplay-kd097d04.c

 * Copyright (c) 2017, Fuzhou Rockchip Electronics Co., Ltd

/*

 * There is no description in the Reference Manual about these commands.

 * We received them from the vendor, so just use them as is.

 120ms to enter sleep mode */

 tRW: 10us */

 tRT: >= 5ms */

 120ms to exit sleep mode */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

 sentinel */

 Register our custom MCS read commands */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Novatek NT35510 panel driver

 * Copyright (C) 2020 Linus Walleij <linus.walleij@linaro.org>

 * Based on code by Robert Teather (C) 2012 Samsung

 *

 * This display driver (and I refer to the physical component NT35510,

 * not this Linux kernel software driver) can handle:

 * 480x864, 480x854, 480x800, 480x720 and 480x640 pixel displays.

 * It has 480x840x24bit SRAM embedded for storing a frame.

 * When powered on the display is by default in 480x800 mode.

 *

 * The actual panels using this component have different names, but

 * the code needed to set up and configure the panel will be similar,

 * so they should all use the NT35510 driver with appropriate configuration

 * per-panel, e.g. for physical size.

 *

 * This driver is for the DSI interface to panels using the NT35510.

 *

 * The NT35510 can also use an RGB (DPI) interface combined with an

 * I2C or SPI interface for setting up the NT35510. If this is needed

 * this panel driver should be refactored to also support that use

 * case.

 Manufacturer command enable */

 Uncertain about name */

 Uncertain about name */

/*

 * These manufacturer commands are available after we enable manufacturer

 * command set (MCS) for page 0.

 Contents kept in sleep */

 Enable TE signal */

 Enable generic read/write */

 Enable video mode on DSI */

 Support EoTP */

 RGB or BGR pixel format */

 TE power selector */

 RGB or BGR byte order */

 Vertical scanning direction */

 Source driver data shift */

 0 = normal operation, 1 = VGLO */

 source driver output = AVDD */

 source driver output = off color */

 source driver output = AVSS */

 source driver output = High impedance */

/*

 * These manufacturer commands are available after we enable manufacturer

 * command set (MCS) for page 1.

 VGH boosting times/freq */

 VGH output ctrl */

 AVDD and AVEE setting 3 bytes */

 BT1CTR thru BT5CTR setting 3 bytes */

 52 gamma parameters times two per color: positive and negative */

/**

 * struct nt35510_config - the display-specific NT35510 configuration

 *

 * Some of the settings provide an array of bytes, A, B C which mean:

 * A = normal / idle off mode

 * B = idle on mode

 * C = partial / idle off mode

 *

 * Gamma correction arrays are 10bit numbers, two consecutive bytes

 * makes out one point on the gamma correction curve. The points are

 * not linearly placed along the X axis, we get points 0, 1, 3, 5

 * 7, 11, 15, 23, 31, 47, 63, 95, 127, 128, 160, 192, 208, 224, 232,

 * 240, 244, 248, 250, 252, 254, 255. The voltages tuples form

 * V0, V1, V3 ... V255, with 0x0000 being the lowest voltage and

 * 0x03FF being the highest voltage.

 *

 * Each value must be strictly higher than the previous value forming

 * a rising curve like this:

 *

 * ^

 * |                                        V255

 * |                                 V254

 * |                         ....

 * |                    V5

 * |           V3

 * |     V1

 * | V0

 * +------------------------------------------->

 *

 * The details about all settings can be found in the NT35510 Application

 * Note.

	/**

	 * @width_mm: physical panel width [mm]

	/**

	 * @height_mm: physical panel height [mm]

	/**

	 * @mode: the display mode. This is only relevant outside the panel

	 * in video mode: in command mode this is configuring the internal

	 * timing in the display controller.

	/**

	 * @avdd: setting for AVDD ranging from 0x00 = 6.5V to 0x14 = 4.5V

	 * in 0.1V steps the default is 0x05 which means 6.0V

	/**

	 * @bt1ctr: setting for boost power control for the AVDD step-up

	 * circuit (1)

	 * bits 0..2 in the lower nibble controls PCK, the booster clock

	 * frequency for the step-up circuit:

	 * 0 = Hsync/32

	 * 1 = Hsync/16

	 * 2 = Hsync/8

	 * 3 = Hsync/4

	 * 4 = Hsync/2

	 * 5 = Hsync

	 * 6 = Hsync x 2

	 * 7 = Hsync x 4

	 * bits 4..6 in the upper nibble controls BTP, the boosting

	 * amplification for the the step-up circuit:

	 * 0 = Disable

	 * 1 = 1.5 x VDDB

	 * 2 = 1.66 x VDDB

	 * 3 = 2 x VDDB

	 * 4 = 2.5 x VDDB

	 * 5 = 3 x VDDB

	 * The defaults are 4 and 4 yielding 0x44

	/**

	 * @avee: setting for AVEE ranging from 0x00 = -6.5V to 0x14 = -4.5V

	 * in 0.1V steps the default is 0x05 which means -6.0V

	/**

	 * @bt2ctr: setting for boost power control for the AVEE step-up

	 * circuit (2)

	 * bits 0..2 in the lower nibble controls NCK, the booster clock

	 * frequency, the values are the same as for PCK in @bt1ctr.

	 * bits 4..5 in the upper nibble controls BTN, the boosting

	 * amplification for the the step-up circuit.

	 * 0 = Disable

	 * 1 = -1.5 x VDDB

	 * 2 = -2 x VDDB

	 * 3 = -2.5 x VDDB

	 * 4 = -3 x VDDB

	 * The defaults are 4 and 3 yielding 0x34

	/**

	 * @vgh: setting for VGH ranging from 0x00 = 7.0V to 0x0B = 18.0V

	 * in 1V steps, the default is 0x08 which means 15V

	/**

	 * @bt4ctr: setting for boost power control for the VGH step-up

	 * circuit (4)

	 * bits 0..2 in the lower nibble controls HCK, the booster clock

	 * frequency, the values are the same as for PCK in @bt1ctr.

	 * bits 4..5 in the upper nibble controls BTH, the boosting

	 * amplification for the the step-up circuit.

	 * 0 = AVDD + VDDB

	 * 1 = AVDD - AVEE

	 * 2 = AVDD - AVEE + VDDB

	 * 3 = AVDD x 2 - AVEE

	 * The defaults are 4 and 3 yielding 0x34

	/**

	 * @vgl: setting for VGL ranging from 0x00 = -2V to 0x0f = -15V in

	 * 1V steps, the default is 0x08 which means -10V

	/**

	 * @bt5ctr: setting for boost power control for the VGL step-up

	 * circuit (5)

	 * bits 0..2 in the lower nibble controls LCK, the booster clock

	 * frequency, the values are the same as for PCK in @bt1ctr.

	 * bits 4..5 in the upper nibble controls BTL, the boosting

	 * amplification for the the step-up circuit.

	 * 0 = AVEE + VCL

	 * 1 = AVEE - AVDD

	 * 2 = AVEE + VCL - AVDD

	 * 3 = AVEE x 2 - AVDD

	 * The defaults are 3 and 2 yielding 0x32

	/**

	 * @vgp: setting for VGP, the positive gamma divider voltages

	 * VGMP the high voltage and VGSP the low voltage.

	 * The first byte contains bit 8 of VGMP and VGSP in bits 4 and 0

	 * The second byte contains bit 0..7 of VGMP

	 * The third byte contains bit 0..7 of VGSP

	 * VGMP 0x00 = 3.0V .. 0x108 = 6.3V in steps of 12.5mV

	 * VGSP 0x00 = 0V .. 0x111 = 3.7V in steps of 12.5mV

	/**

	 * @vgn: setting for VGN, the negative gamma divider voltages,

	 * same layout of bytes as @vgp.

	/**

	 * @sdeqctr: Source driver control settings, first byte is

	 * 0 for mode 1 and 1 for mode 2. Mode 1 uses two steps and

	 * mode 2 uses three steps meaning EQS3 is not used in mode

	 * 1. Mode 2 is default. The last three parameters are EQS1, EQS2

	 * and EQS3, setting the rise time for each equalizer step:

	 * 0x00 = 0.0 us to 0x0f = 7.5 us in steps of 0.5us. The default

	 * is 0x07 = 3.5 us.

	/**

	 * @sdvpctr: power/voltage behaviour during vertical porch time

	/**

	 * @t1: the number of pixel clocks on one scanline, range

	 * 0x100 (258 ticks) .. 0x3FF (1024 ticks) so the value + 1

	 * clock ticks.

	/**

	 * @vbp: vertical back porch toward the PANEL note: not toward

	 * the DSI host; these are separate interfaces, in from DSI host

	 * and out to the panel.

	/**

	 * @vfp: vertical front porch toward the PANEL.

	/**

	 * @psel: pixel clock divisor: 0 = 1, 1 = 2, 2 = 4, 3 = 8.

	/**

	 * @dpmctr12: Display timing control 12

	 * Byte 1 bit 4 selects LVGL voltage level: 0 = VGLX, 1 = VGL_REG

	 * Byte 1 bit 1 selects gate signal mode: 0 = non-overlap, 1 = overlap

	 * Byte 1 bit 0 selects output signal control R/L swap, 0 = normal

	 * 1 = swap all O->E, L->R

	 * Byte 2 is CLW delay clock for CK O/E and CKB O/E signals:

	 * 0x00 = 0us .. 0xFF = 12.75us in 0.05us steps

	 * Byte 3 is FTI_H0 delay time for STP O/E signals:

	 * 0x00 = 0us .. 0xFF = 12.75us in 0.05us steps

	/**

	 * @gamma_corr_pos_r: Red gamma correction parameters, positive

	/**

	 * @gamma_corr_pos_g: Green gamma correction parameters, positive

	/**

	 * @gamma_corr_pos_b: Blue gamma correction parameters, positive

	/**

	 * @gamma_corr_neg_r: Red gamma correction parameters, negative

	/**

	 * @gamma_corr_neg_g: Green gamma correction parameters, negative

	/**

	 * @gamma_corr_neg_b: Blue gamma correction parameters, negative

/**

 * struct nt35510 - state container for the NT35510 panel

	/**

	 * @dev: the container device

	/**

	 * @conf: the specific panel configuration, as the NT35510

	 * can be combined with many physical panels, they can have

	 * different physical dimensions and gamma correction etc,

	 * so this is stored in the config.

	/**

	 * @panel: the DRM panel object for the instance

	/**

	 * @supplies: regulators supplying the panel

	/**

	 * @reset_gpio: the reset line

 Manufacturer command has strictly this byte sequence */

	/*

	 * Multi-Time Programmable (?) memory contains manufacturer

	 * ID (e.g. Hydis 0x55), driver ID (e.g. NT35510 0xc0) and

	 * version.

/**

 * nt35510_setup_power() - set up power config in page 1

 * @nt: the display instance to set up

 Typically 10 ms */

/**

 * nt35510_setup_display() - set up display config in page 0

 * @nt: the display instance to set up

 FIXME: set up any rotation (assume none for now) */

 Enable TE, EoTP and RGB pixel format */

	/*

	 * Source data hold time, default 0x05 = 2.5us

	 * 0x00..0x3F = 0 .. 31.5us in steps of 0.5us

	 * 0x0A = 5us

 EQ control for gate signals, 0x00 = 0 us */

	/*

	 * Display timing control for active and idle off mode:

	 * the first byte contains

	 * the two high bits of T1A and second byte the low 8 bits, and

	 * the valid range is 0x100 (257) to 0x3ff (1023) representing

	 * 258..1024 (+1) pixel clock ticks for one scanline. At 20MHz pixel

	 * clock this covers the range of 12.90us .. 51.20us in steps of

	 * 0.05us, the default is 0x184 (388) representing 389 ticks.

	 * The third byte is VBPDA, vertical back porch display active

	 * and the fourth VFPDA, vertical front porch display active,

	 * both given in number of scanlines in the range 0x02..0xff

	 * for 2..255 scanlines. The fifth byte is 2 bits selecting

	 * PSEL for active and idle off mode, how much the 20MHz clock

	 * is divided by 0..3.  This needs to be adjusted to get the right

	 * frame rate.

 Vertical back porch */

 Vertical front porch */

 For idle and partial idle off mode we decrease front porch by one */

 Enable TE on vblank */

 Turn on the pads? */

/*

 * This power-on sequence

 Toggle RESET in accordance with datasheet page 370 */

 Active min 10 us according to datasheet, let's say 20 */

		/*

		 * 5 ms during sleep mode, 120 ms during sleep out mode

		 * according to datasheet, let's use 120-140 ms.

 Set up stuff in  manufacturer control, page 1 */

 Set up stuff in  manufacturer control, page 0 */

 Enter sleep mode */

 Wait 4 frames, how much is that 5ms in the vendor driver */

 Exit sleep mode */

 Up to 120 ms */

 Some 10 ms */

 Number of modes */

	/*

	 * Datasheet suggests max HS rate for NT35510 is 250 MHz

	 * (period time 4ns, see figure 7.6.4 page 365) and max LP rate is

	 * 20 MHz (period time 50ns, see figure 7.6.6. page 366).

	 * However these frequencies appear in source code for the Hydis

	 * HVA40WV1 panel and setting up the LP frequency makes the panel

	 * not work.

	 *

	 * TODO: if other panels prove to be closer to the datasheet,

	 * maybe make this a per-panel config in struct nt35510_config?

	/*

	 * Every new incarnation of this display must have a unique

	 * data entry for the system in this driver.

 2.3-4.8 V */

 1.65-3.3V */

	/*

	 * First, try to locate an external backlight (such as on GPIO)

	 * if this fails, assume we will want to use the internal backlight

	 * control.

 Power off */

/*

 * These gamma correction values are 10bit tuples, so only bits 0 and 1 is

 * ever used in the first byte. They form a positive and negative gamma

 * correction curve for each color, values must be strictly higher for each

 * step on the curve. As can be seen these default curves goes from 0x0001

 * to 0x03FE.

/*

 * The Hydis HVA40WV1 panel

	/**

	 * As the Hydis panel is used in command mode, the porches etc

	 * are settings programmed internally into the NT35510 controller

	 * and generated toward the physical display. As the panel is not

	 * used in video mode, these are not really exposed to the DSI

	 * host.

	 *

	 * Display frame rate control:

	 * Frame rate = (20 MHz / 1) / (389 * (7 + 50 + 800)) ~= 60 Hz

 The internal pixel clock of the NT35510 is 20 MHz */

 HFP = 2 */

 HSync = 0 */

 HFP = 5 */

 VFP = 2 */

 VSync = 0 */

 VBP = 5 */

 0x09: AVDD = 5.6V */

 0x34: PCK = Hsync/2, BTP = 2 x VDDB */

 0x09: AVEE = -5.6V */

 0x24: NCK = Hsync/2, BTN =  -2 x VDDB */

 0x05 = 12V */

 0x24: NCKA = Hsync/2, VGH = 2 x AVDD - AVEE */

 0x0B = -13V */

 0x24: LCKA = Hsync, VGL = AVDD + VCL - AVDD */

 VGMP: 0x0A3 = 5.0375V, VGSP = 0V */

 VGMP: 0x0A3 = 5.0375V, VGSP = 0V */

 SDEQCTR: source driver EQ mode 2, 2.5 us rise time on each step */

 SDVPCTR: Normal operation off color during v porch */

 T1: number of pixel clocks on one scanline: 0x184 = 389 clocks */

 VBP: vertical back porch toward the panel */

 VFP: vertical front porch toward the panel */

 PSEL: divide pixel clock 20MHz with 1 (no clock downscaling) */

 DPTMCTR12: 0x03: LVGL = VGLX, overlap mode, swap R->L O->E */

 Default gamma correction values */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 Free Electrons

 We need to wait 120ms after a sleep out command */

 SPDX-License-Identifier: GPL-2.0

/*

 * LG.Philips LB035Q02 LCD Panel Driver

 *

 * Copyright (C) 2019 Texas Instruments Incorporated

 *

 * Based on the omapdrm-specific panel-lgphilips-lb035q02 driver

 *

 * Copyright (C) 2013 Texas Instruments Incorporated

 * Author: Tomi Valkeinen <tomi.valkeinen@ti.com>

 *

 * Based on a driver by: Steve Sakoman <steve@sakoman.com>

 register index */

 register value */

 Init sequence from page 28 of the lb035q02 spec. */

	/*

	 * FIXME: According to the datasheet pixel data is sampled on the

	 * rising edge of the clock, but the code running on the Gumstix Overo

	 * Palo35 indicates sampling on the negative edge. This should be

	 * tested on a real device.

 sentinel */ },

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2014 NVIDIA Corporation

 the datasheet refers to them as DSI-LINK1 and DSI-LINK2 */

	/*

	 * According to the datasheet, the panel needs around 10 ms to fully

	 * power up. At least another 120 ms is required before exiting sleep

	 * mode to make sure the panel is ready. Throw in another 20 ms for

	 * good measure.

	/*

	 * The MIPI DCS specification mandates this delay only between the

	 * exit_sleep_mode and enter_sleep_mode commands, so it isn't strictly

	 * necessary here.

	/*

	msleep(120);

 set left-right mode */

 enable command mode */

	/*

	 * TODO: The device supports both left-right and even-odd split

	 * configurations, but this driver currently supports only the left-

	 * right split. To support a different mode a mechanism needs to be

	 * put in place to communicate the configuration back to the DSI host

	 * controller.

 wait for 6 frames before continuing */

 Find DSI-LINK1 */

 register a panel for only the DSI-LINK1 interface */

 only detach from host for the DSI-LINK2 interface */

 nothing to do for DSI-LINK2 */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright 2021 Google Inc.

 *

 * Panel driver for the Samsung ATNA33XC20 panel. This panel can't be handled

 * by the DRM_PANEL_SIMPLE driver because its power sequencing is non-standard.

	/*

	 * Note 3 (Example of power off sequence in detail) in spec

	 * specifies to wait 150 ms after deasserting EL3_ON before

	 * powering off.

 T12 (Power off time) is min 500 ms */

	/*

	 * Handle HPD. Note: if HPD is hooked up to a dedicated pin on the

	 * eDP controller then "no_hpd" will be false _and_ "hpd_gpio" will be

	 * NULL. It's up to the controller driver to wait for HPD after

	 * preparing the panel in that case.

 T3 VCC to HPD high is max 200 ms */

 Disabling when already disabled is a no-op */

	/*

	 * Keep track of the fact that EL_ON3 was on but we haven't power

	 * cycled yet. This lets us know that "el_on3_off_time" is recent (we

	 * don't need to worry about ktime wraparounds) and also makes it

	 * obvious if we try to enable again without a power cycle (see the

	 * warning in atana33xc20_enable()).

	/*

	 * Sleeping 20 ms here (after setting the GPIO) avoids a glitch when

	 * powering off.

 Enabling when already enabled is a no-op */

	/*

	 * Once EL_ON3 drops we absolutely need a power cycle before the next

	 * enable or the backlight will never come on again. The code ensures

	 * this because disable() is _always_ followed by unprepare() and

	 * unprepare() forces a suspend with pm_runtime_put_sync_suspend(),

	 * but let's track just to make sure since the requirement is so

	 * non-obvious.

	/*

	 * Note 2 (Example of power on sequence in detail) in spec specifies

	 * to wait 400 ms after powering on before asserting EL3_on.

 Unpreparing when already unprepared is a no-op */

	/*

	 * Purposely do a put_sync, don't use autosuspend. The panel's tcon

	 * seems to sometimes crash when you stop giving it data and this is

	 * the best way to ensure it will come back.

	 *

	 * NOTE: we still want autosuspend for cases where we only turn on

	 * to get the EDID or otherwise send DP AUX commands to the panel.

 Preparing when already prepared is a no-op */

 sentinal */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Asia Better Technology Ltd. Y030XX067A IPS LCD panel driver

 *

 * Copyright (C) 2020, Paul Cercueil <paul@crapouillou.net>

 * Copyright (C) 2020, Christophe Branchereau <cbranchereau@gmail.com>

 Reset the chip */

 60 Hz */

 50 Hz */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Ilitek ILI9322 TFT LCD drm_panel driver.

 *

 * This panel can be configured to support:

 * - 8-bit serial RGB interface

 * - 24-bit parallel RGB interface

 * - 8-bit ITU-R BT.601 interface

 * - 8-bit ITU-R BT.656 interface

 * - Up to 320RGBx240 dots resolution TFT LCD displays

 * - Scaling, brightness and contrast

 *

 * The scaling means that the display accepts a 640x480 or 720x480

 * input and rescales it to fit to the 320x240 display. So what we

 * present to the system is something else than what comes out on the

 * actual display.

 *

 * Copyright (C) 2017 Linus Walleij <linus.walleij@linaro.org>

 * Derived from drivers/drm/gpu/panel/panel-samsung-ld9040.c

/*

 * Voltage on the communication interface, from 0.7 (0x00)

 * to 1.32 (0x1f) times the VREG1OUT voltage in 2% increments.

 * 1.00 (0x0f) is the default.

/*

 * High voltage on the communication signals, from 0.37 (0x00) to

 * 1.0 (0x3f) times the VREGOUT1 voltage in 1% increments.

 * 0.83 (0x2e) is the default.

/*

 * VREG1 voltage regulator from 3.6V (0x00) to 6.0V (0x18) in 0.1V

 * increments. 5.4V (0x12) is the default. This is the reference

 * voltage for the VCOM levels and the greyscale level.

 Describes the incoming signal */

 0 = right-to-left, 1 = left-to-right (default), horizontal flip */

 0 = down-to-up, 1 = up-to-down (default), vertical flip  */

 NTSC, PAL or autodetect */

 Input format */

 Power control */

 0 = standby, 1 = normal */

 0 = off, 1 = on  */

 0 = off, 1 = on  */

 0 = off, 1 = on  */

 0 = off, 1 = on  */

 0 = off, 1 = on  */

 0 = interactive, 1 = auto */

 Vertical back porch bits 0..5 */

 Horizontal back porch, 8 bits */

/*

 * Polarity settings:

 * 1 = positive polarity

 * 0 = negative polarity

 1 default */

 0 default */

 0 default */

 1 default */

/*

 * 0 means YCBCR are ordered Cb0,Y0,Cr0,Y1,Cb2,Y2,Cr2,Y3 (default)

 *   in RGB mode this means RGB comes in RGBRGB

 * 1 means YCBCR are ordered Cr0,Y0,Cb0,Y1,Cr2,Y2,Cb2,Y3

 *   in RGB mode this means RGB comes in BGRBGR

 Formula A for YCbCR->RGB = 0, Formula B = 1 */

 Reverse polarity: 0 = 0..255, 1 = 255..0 */

 Not set means frame inv */

 bit 0 = 0 -> reset */

/*

 * 4+4 bits of negative and positive gamma correction

 * Upper nybble, bits 4-7 are negative gamma

 * Lower nybble, bits 0-3 are positive gamma

/*

 * enum ili9322_input - the format of the incoming signal to the panel

 *

 * The panel can be connected to various input streams and four of them can

 * be selected by electronic straps on the display. However it is possible

 * to select another mode or override the electronic default with this

 * setting.

/**

 * struct ili9322_config - the system specific ILI9322 configuration

 * @width_mm: physical panel width [mm]

 * @height_mm: physical panel height [mm]

 * @flip_horizontal: flip the image horizontally (right-to-left scan)

 * (only in RGB and YUV modes)

 * @flip_vertical: flip the image vertically (down-to-up scan)

 * (only in RGB and YUV modes)

 * @input: the input/entry type used in this system, if this is set to

 * ILI9322_INPUT_UNKNOWN the driver will try to figure it out by probing

 * the hardware

 * @vreg1out_mv: the output in microvolts for the VREGOUT1 regulator used

 * to drive the physical display. Valid ranges are 3600 thru 6000 in 100

 * microvolt increments. If not specified, hardware defaults will be

 * used (4.5V).

 * @vcom_high_percent: the percentage of VREGOUT1 used for the peak

 * voltage on the communications link. Valid ranges are 37 thru 100

 * percent. If not specified, hardware defaults will be used (91%).

 * @vcom_amplitude_percent: the percentage of VREGOUT1 used for the

 * peak-to-peak amplitude of the communcation signals to the physical

 * display. Valid ranges are 70 thru 132 percent in increments if two

 * percent. Odd percentages will be truncated. If not specified, hardware

 * defaults will be used (114%).

 * @dclk_active_high: data/pixel clock active high, data will be clocked

 * in on the rising edge of the DCLK (this is usually the case).

 * @syncmode: The synchronization mode, what sync signals are emitted.

 * See the enum for details.

 * @de_active_high: DE (data entry) is active high

 * @hsync_active_high: HSYNC is active high

 * @vsync_active_high: VSYNC is active high

 * @gamma_corr_pos: a set of 8 nybbles describing positive

 * gamma correction for voltages V1 thru V8. Valid range 0..15

 * @gamma_corr_neg: a set of 8 nybbles describing negative

 * gamma correction for voltages V1 thru V8. Valid range 0..15

 *

 * These adjust what grayscale voltage will be output for input data V1 = 0,

 * V2 = 16, V3 = 48, V4 = 96, V5 = 160, V6 = 208, V7 = 240 and V8 = 255.

 * The curve is shaped like this:

 *

 *  ^

 *  |                                                        V8

 *  |                                                   V7

 *  |                                          V6

 *  |                               V5

 *  |                    V4

 *  |            V3

 *  |     V2

 *  | V1

 *  +----------------------------------------------------------->

 *    0   16     48      96         160        208      240  255

 *

 * The negative and postive gamma values adjust the V1 thru V8 up/down

 * according to the datasheet specifications. This is a property of the

 * physical display connected to the display controller and may vary.

 * If defined, both arrays must be supplied in full. If the properties

 * are not supplied, hardware defaults will be used.

 Clear bit 7 to write */

 Set bit 7 to 1 to read */

 Just register 0 is read-only */

 Reset display */

 Set up the main voltage regulator */

 Set up gamma correction */

	/*

	 * Polarity and inverted color order for RGB input.

	 * None of this applies in the BT.656 mode.

	/*

	 * Set up interface control.

	 * This is not used in the BT.656 mode (no H/Vsync or DE signals).

 Set up the input mode */

 These are inverted, setting to 1 is the default, clearing flips */

/*

 * This power-on sequence if from the datasheet, page 57.

 Assert RESET */

 De-assert RESET */

 Serial RGB modes */

 This is the only mode listed for parallel RGB in the datasheet */

 YUV modes */

 BT.656 VGA mode, 640x480 */

 BT.656 D1 mode 720x480 */

	/*

	 * This is the preferred mode because most people are going

	 * to want to use the display with VGA type graphics.

 Set up the polarity */

 Number of modes */

	/*

	 * Every new incarnation of this display must have a unique

	 * data entry for the system in this driver.

 Default HW value, do not touch (should be 4.5V) */

 Default HW value, do not touch (should be 91%) */

 Default HW value, do not touch (should be 114%) */

 Increments of 2% */

 2.7-3.6 V */

 1.65-3.6V */

 2.7-3.6V */

 Probe the system to find the display setting */

 Input enum corresponds to HW setting */

/*

 * The D-Link DIR-685 panel is marked LM918A01-1A SY-B4-091116-E0199

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 Red Hat

 * Copyright (C) 2015 Sony Mobile Communications Inc.

 * Author: Werner Johansson <werner.johansson@sonymobile.com>

 *

 * Based on AUO panel driver by Rob Clark <robdclark@gmail.com>

 Novatek two-lane operation */

 Set both MCU and RGB I/F to 24bpp */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2019, The Linux Foundation. All rights reserved.

	/*

	 * Reset sequence of visionox panel requires the panel to be

	 * out of reset for 10ms, followed by being held in reset

	 * for 10ms and then out again

 120ms delay required here as per DCS spec */

 Per DSI spec wait 120ms after sending exit sleep DCS command */

 Per DSI spec wait 120ms after sending set_display_on DCS command */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 Only the CMD1 User Command set is documented */

 Select Unknown CMD Page (Undocumented) */

 Reload CMD1: Don't reload default value to register */

 Select CMD2 Page0 (Undocumented) */

 Reload CMD1: Don't reload default value to register */

 Select CMD2 Page4 (Undocumented) */

 Reload CMD1: Don't reload default value to register */

 Select CMD2 Page0 (Undocumented) */

 Reload CMD1: Don't reload default value to register */

 Select CMD2 Page2 (Undocumented) */

 Reload CMD1: Don't reload default value to register */

 Select CMD2 Page0 (Undocumented) */

 Reload CMD1: Don't reload default value to register */

 Select CMD2 Page1 (Undocumented) */

 Reload CMD1: Don't reload default value to register */

 Select CMD2 Page3 (Undocumented) */

 Reload CMD1: Don't reload default value to register */

 Select CMD1 */

 RGBMIPICTRL: VSYNC back porch = 5 */

 RGBMIPICTRL: VSYNC front porch = 4 */

 Select CMD2 page 4 (Undocumented) */

 Reload CMD1: Don't reload default value to register */

 Select CMD1 */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 2019 Texas Instruments Incorporated - https://www.ti.com

 *  Author: Peter Ujfalusi <peter.ujfalusi@ti.com>

 sentinel */

 SPDX-License-Identifier: GPL-2.0-only

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2021 Linaro Ltd.

 * Generated with linux-mdss-dsi-panel-driver-generator from vendor device tree:

 *   Copyright (c) 2013-2014, The Linux Foundation. All rights reserved.

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0+

/*

 * Generic LVDS panel driver

 *

 * Copyright (C) 2016 Laurent Pinchart

 * Copyright (C) 2016 Renesas Electronics Corporation

 *

 * Contact: Laurent Pinchart (laurent.pinchart@ideasonboard.com)

 Get GPIOs and backlight controller. */

	/*

	 * TODO: Handle all power supplies specified in the DT node in a generic

	 * way for panels that don't care about power supply ordering. LVDS

	 * panels that require a specific power sequence will need a dedicated

	 * driver.

 Register the panel. */

 Sentinel */ },

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 Heiko Schocher <hs@denx.de>

 *

 * from:

 * drivers/gpu/drm/panel/panel-ld9040.c

 * ld9040 AMOLED LCD drm_panel driver.

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd

 * Derived from drivers/video/backlight/ld9040.c

 *

 * Andrzej Hajda <a.hajda@samsung.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 Red Hat

 * Copyright (C) 2015 Sony Mobile Communications Inc.

 * Author: Werner Johansson <werner.johansson@sonymobile.com>

 *

 * Based on AUO panel driver by Rob Clark <robdclark@gmail.com>

/*

 * When power is turned off to this panel a minimum off time of 500ms has to be

 * observed before powering back on as there's no external reset pin. Keep

 * track of earliest wakeup time and delay subsequent prepare call accordingly

	/*

	 * If the user re-enabled the panel before the required off-time then

	 * we need to wait the remaining period before re-enabling regulator

 Sanity check, this should never happen */

	/*

	 * A minimum delay of 250ms is required after power-up until commands

	 * can be sent

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ld9040 AMOLED LCD drm_panel driver.

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd

 * Derived from drivers/video/backlight/ld9040.c

 *

 * Andrzej Hajda <a.hajda@samsung.com>

 Manufacturer Command Set */

 array of gamma tables for gamma value 2.2 */

	/* This field is tested by functions directly accessing bus before

	 * transfer, transfer is skipped if it is set. In case of transfer

	 * failure or unexpected response the field is set to error value.

	 * Such construct allows to eliminate many checks in higher level

	 * functions.

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0+

 Copyright 2018 IBM Corporation

 SPDX-License-Identifier: GPL-2.0+

 Copyright 2018 IBM Corporation

/**

 * DOC: ASPEED GFX Driver

 *

 * This driver is for the ASPEED BMC SoC's 'GFX' display hardware, also called

 * the 'SOC Display Controller' in the datasheet. This driver runs on the ARM

 * based BMC systems, unlike the ast driver which runs on a host CPU and is for

 * a PCIe graphics device.

 *

 * The AST2500 supports a total of 3 output paths:

 *

 *   1. VGA output, the output target can choose either or both to the DAC

 *   or DVO interface.

 *

 *   2. Graphics CRT output, the output target can choose either or both to

 *   the DAC or DVO interface.

 *

 *   3. Video input from DVO, the video input can be used for video engine

 *   capture or DAC display output.

 *

 * Output options are selected in SCU2C.

 *

 * The "VGA mode" device is the PCI attached controller. The "Graphics CRT"

 * is the ARM's internal display controller.

 *

 * The driver only supports a simple configuration consisting of a 40MHz

 * pixel clock, fixed by hardware limitations, and the VGA output path.

 *

 * The driver was written with the 'AST2500 Software Programming Guide' v17,

 * which is available under NDA from ASPEED.

 DAC register in SCU */

 VGA scratch register in SCU */

 Default Threshold Seting */

 Max memory size of one scan line */

 Sanitize control registers */

 SPDX-License-Identifier: GPL-2.0+

 Copyright 2018 IBM Corporation

 Set DAC source for display output to Graphics CRT (GFX) */

	/* TODO: we have only been able to test with the 40MHz USB clock. The

 Horizontal timing */

 Vertical timing */

	/*

	 * Display Offset: address difference between consecutive scan lines

	 * Terminal Count: memory size of one scan line

	/*

	 * Threshold: FIFO thresholds of refill and stop (16 byte chunks

	 * per line, rounded up)

 Clear pending VBLANK IRQ */

 Clear pending VBLANK IRQ */

/*

 * Copyright 2016 Intel Corporation

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software")

 * to deal in the software without restriction, including without limitation

 * on the rights to use, copy, modify, merge, publish, distribute, sub

 * license, and/or sell copies of the Software, and to permit persons to whom

 * them Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTIBILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER

 * IN AN ACTION OF CONTRACT, TORT, OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 We force the fence to expire within 10s to prevent driver hangs */

/*

 * vgem_fence_attach_ioctl (DRM_IOCTL_VGEM_FENCE_ATTACH):

 *

 * Create and attach a fence to the vGEM handle. This fence is then exposed

 * via the dma-buf reservation object and visible to consumers of the exported

 * dma-buf. If the flags contain VGEM_FENCE_WRITE, the fence indicates the

 * vGEM buffer is being written to by the client and is exposed as an exclusive

 * fence, otherwise the fence indicates the client is current reading from the

 * buffer and all future writes should wait for the client to signal its

 * completion. Note that if a conflicting fence is already on the dma-buf (i.e.

 * an exclusive fence when adding a read, or any fence when adding a write),

 * -EBUSY is reported. Serialisation between operations should be handled

 * by waiting upon the dma-buf.

 *

 * This returns the handle for the new fence that must be signaled within 10

 * seconds (or otherwise it will automatically expire). See

 * vgem_fence_signal_ioctl (DRM_IOCTL_VGEM_FENCE_SIGNAL).

 *

 * If the vGEM handle does not exist, vgem_fence_attach_ioctl returns -ENOENT.

 Check for a conflicting fence */

 Expose the fence via the dma-buf */

 Record the fence in our idr for later signaling */

/*

 * vgem_fence_signal_ioctl (DRM_IOCTL_VGEM_FENCE_SIGNAL):

 *

 * Signal and consume a fence ealier attached to a vGEM handle using

 * vgem_fence_attach_ioctl (DRM_IOCTL_VGEM_FENCE_ATTACH).

 *

 * All fences must be signaled within 10s of attachment or otherwise they

 * will automatically expire (and a vgem_fence_signal_ioctl returns -ETIMEDOUT).

 *

 * Signaling a fence indicates to all consumers of the dma-buf that the

 * client has completed the operation associated with the fence, and that the

 * buffer is then ready for consumption.

 *

 * If the fence does not exist (or has already been signaled by the client),

 * vgem_fence_signal_ioctl returns -ENOENT.

/*

 * Copyright 2011 Red Hat, Inc.

 * Copyright Â© 2014 The Chromium OS Authors

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software")

 * to deal in the software without restriction, including without limitation

 * on the rights to use, copy, modify, merge, publish, distribute, sub

 * license, and/or sell copies of the Software, and to permit persons to whom

 * them Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice (including the next

 * paragraph) shall be included in all copies or substantial portions of the

 * Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTIBILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.  IN NO EVENT SHALL

 * THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER

 * IN AN ACTION OF CONTRACT, TORT, OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

 * Authors:

 *	Adam Jackson <ajax@redhat.com>

 *	Ben Widawsky <ben@bwidawsk.net>

/*

 * This is vgem, a (non-hardware-backed) GEM service.  This is used by Mesa's

 * software renderer and the X server for efficient buffer sharing.

	/*

	 * vgem doesn't have any begin/end cpu access ioctls, therefore must use

	 * coherent memory or dma-buf sharing just wont work.

 Final step: expose the device/driver to userspace */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Texas Instruments Incorporated - https://www.ti.com/

 * Author: Rob Clark <rob@ti.com>

/*

 * encoder funcs

/* The encoder and connector both map to same dssdev.. the encoder

 * handles the 'active' parts, ie. anything the modifies the state

 * of the hw, and the connector handles the 'read-only' parts, like

 * detecting connection and reading edid.

	/*

	 * HACK: This fixes the vm flags.

	 * struct drm_display_mode does not contain the VSYNC/HSYNC/DE flags and

	 * they get lost when converting back and forth between struct

	 * drm_display_mode and struct videomode. The hack below goes and

	 * fetches the missing flags.

	 *

	 * A better solution is to use DRM's bus-flags through the whole driver.

 Set timings for all devices in the display pipeline. */

 initialize encoder */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Texas Instruments Incorporated - https://www.ti.com/

 * Author: Rob Clark <rob.clark@linaro.org>

/*

 * GEM buffer object implementation.

 note: we use upper 8 bits of flags for driver-internal flags: */

 memory allocated with the dma_alloc_* API */

 memory allocated through shmem backing */

 memory imported from a dmabuf */

* width/height for tiled formats (rounded up to slot boundaries) */

* roll applied when mapping to DMM */

* protects dma_addr_cnt, block, pages, dma_addrs and vaddr */

	/**

	 * dma_addr contains the buffer DMA address. It is valid for

	 *

	 * - buffers allocated through the DMA mapping API (with the

	 *   OMAP_BO_MEM_DMA_API flag set)

	 *

	 * - buffers imported from dmabuf (with the OMAP_BO_MEM_DMABUF flag set)

	 *   if they are physically contiguous (when sgt->orig_nents == 1)

	 *

	 * - buffers mapped through the TILER when dma_addr_cnt is not zero, in

	 *   which case the DMA address points to the TILER aperture

	 *

	 * Physically contiguous buffers have their DMA address equal to the

	 * physical address as we don't remap those buffers through the TILER.

	 *

	 * Buffers mapped to the TILER have their DMA address pointing to the

	 * TILER aperture. As TILER mappings are refcounted (through

	 * dma_addr_cnt) the DMA address must be accessed through omap_gem_pin()

	 * to ensure that the mapping won't disappear unexpectedly. References

	 * must be released with omap_gem_unpin().

	/**

	 * # of users of dma_addr

	/**

	 * If the buffer has been imported from a dmabuf the OMAP_DB_DMABUF flag

	 * is set and the sgt field is valid.

	/**

	 * tiler block used when buffer is remapped in DMM/TILER.

	/**

	 * Array of backing pages, if allocated.  Note that pages are never

	 * allocated for buffers originally allocated from contiguous memory

* addresses corresponding to pages in above array */

	/**

	 * Virtual address, if mapped.

/* To deal with userspace mmap'ings of 2d tiled buffers, which (a) are

 * not necessarily pinned in TILER all the time, and (b) when they are

 * they are not necessarily page aligned, we reserve one or more small

 * regions in each of the 2d containers to use as a user-GART where we

 * can create a second page-aligned mapping of parts of the buffer

 * being accessed from userspace.

 *

 * Note that we could optimize slightly when we know that multiple

 * tiler containers are backed by the same PAT.. but I'll leave that

 * for later..

 the reserved tiler block */

 the current pinned obj */

	pgoff_t obj_pgoff;		/* page offset of obj currently

 height in rows */

 ilog2(height in rows) */

 ilog2(width per slot) */

 stride in pages */

 index of last used entry */

/* -----------------------------------------------------------------------------

 * Helpers

* get mmap offset */

 Make it mmapable */

/* -----------------------------------------------------------------------------

 * Eviction

 if stride > than PAGE_SIZE then sparse mapping: */

 Evict a buffer from usergart, if it is mapped there */

/* -----------------------------------------------------------------------------

 * Page Management

/*

 * Ensure backing pages are allocated. Must be called with the omap_obj.lock

 * held.

	/*

	 * If not using shmem (in which case backing pages don't need to be

	 * allocated) or if pages are already allocated we're done.

	/* for non-cached buffers, ensure the new pages are clean because

	 * DSS, GPU, etc. are not cache coherent:

 Release backing pages. Must be called with the omap_obj.lock held. */

 get buffer flags */

* get mmap size */

		/* for tiled buffers, the virtual size has stride rounded up

		 * to 4kb.. (to hide the fact that row n+1 might start 16kb or

		 * 32kb later!).  But we don't back the entire buffer with

		 * pages, only the valid picture part.. so need to adjust for

		 * this in the size used to mmap and generate mmap offset

/* -----------------------------------------------------------------------------

 * Fault Handling

 Normal handling for the case of faulting in non-tiled buffers */

 We don't use vmf->pgoff since that has the fake offset: */

 Special handling for the case of faulting in 2d tiled buffers */

 XXX is this too much to have on stack? */

	/*

	 * Note the height of the slot is also equal to the number of pages

	 * that need to be mapped in to fill 4kb wide CPU page.  If the slot

	 * height is 64, then 64 pages fill a 4kb wide by 64 row region.

	/*

	 * If buffer width in bytes > PAGE_SIZE then the virtual stride is

	 * rounded up to next multiple of PAGE_SIZE.. this need to be taken

	 * into account in some of the math, so figure out virtual stride

	 * in pages

 We don't use vmf->pgoff since that has the fake offset: */

	/*

	 * Actual address we start mapping at is rounded down to previous slot

	 * boundary in the y direction:

 figure out buffer width in slots */

 evict previous buffer using this usergart entry, if any: */

 now convert base_pgoff to phys offset from virt offset: */

 for wider-than 4k.. figure out which part of the slot-row we want: */

	/*

	 * Map in pages. Beyond the valid pixel part of the buffer, we set

	 * pages[i] to NULL to get a dummy page mapped in.. if someone

	 * reads/writes it they will get random/undefined content, but at

	 * least it won't be corrupting whatever other random page used to

	 * be mapped in, or other undefined behavior.

 simple round-robin: */

/**

 * omap_gem_fault		-	pagefault handler for GEM objects

 * @vmf: fault detail

 *

 * Invoked when a fault occurs on an mmap of a GEM managed area. GEM

 * does most of the work for us including the actual map/unmap calls

 * but we need to do the actual page work.

 *

 * The VMA was set up by GEM. In doing so it also ensured that the

 * vma->vm_private_data points to the GEM object that is backing this

 * mapping.

	/* Make sure we don't parallel update on a fault, nor move or remove

	 * something from beneath our feet

 if a shmem backed object, make sure we have pages attached now */

	/* where should we do corresponding put_pages().. we are mapping

	 * the original page, rather than thru a GART, so we can't rely

	 * on eviction to trigger this.  But munmap() or all mappings should

	 * probably trigger put_pages()?

* We override mainly to fix up some of the vm mapping flags.. */

		/*

		 * We do have some private objects, at least for scanout buffers

		 * on hardware without DMM/TILER.  But these are allocated write-

		 * combine

		/*

		 * Shunt off cached objs to shmem file so they have their own

		 * address_space (so unmap_mapping_range does what we want,

		 * in particular in the case of mmap'd dmabufs)

/* -----------------------------------------------------------------------------

 * Dumb Buffers

/**

 * omap_gem_dumb_create	-	create a dumb buffer

 * @file: our client file

 * @dev: our device

 * @args: the requested arguments copied from userspace

 *

 * Allocate a buffer suitable for use for a frame buffer of the

 * form described by user space. Give userspace a handle by which

 * to reference it.

/**

 * omap_gem_dumb_map	-	buffer mapping for dumb interface

 * @file: our drm client file

 * @dev: drm device

 * @handle: GEM handle to the object (from dumb_create)

 * @offset: memory map offset placeholder

 *

 * Do the necessary setup to allow the mapping of the frame buffer

 * into user memory. We don't have to do much here at the moment.

 GEM does all our handle to object mapping */

/* Set scrolling position.  This allows us to implement fast scrolling

 * for console.

 *

 * Call only from non-atomic contexts.

 if we aren't mapped yet, we don't need to do anything */

/* -----------------------------------------------------------------------------

 * Memory Management & DMA Sync

/*

 * shmem buffers that are mapped cached are not coherent.

 *

 * We keep track of dirty pages using page faulting to perform cache management.

 * When a page is mapped to the CPU in read/write mode the device can't access

 * it and omap_obj->dma_addrs[i] is NULL. When a page is mapped to the device

 * the omap_obj->dma_addrs[i] is set to the DMA address, and the page is

 * unmapped from the CPU.

/* Sync the buffer for CPU access.. note pages should already be

 * attached, ie. omap_gem_get_pages()

 sync the buffer for DMA access */

/**

 * omap_gem_pin() - Pin a GEM object in memory

 * @obj: the GEM object

 * @dma_addr: the DMA address

 *

 * Pin the given GEM object in memory and fill the dma_addr pointer with the

 * object's DMA address. If the buffer is not physically contiguous it will be

 * remapped through the TILER to provide a contiguous view.

 *

 * Pins are reference-counted, calling this function multiple times is allowed

 * as long the corresponding omap_gem_unpin() calls are balanced.

 *

 * Return 0 on success or a negative error code otherwise.

 TODO: enable async refill.. */

/**

 * omap_gem_unpin_locked() - Unpin a GEM object from memory

 * @obj: the GEM object

 *

 * omap_gem_unpin() without locking.

/**

 * omap_gem_unpin() - Unpin a GEM object from memory

 * @obj: the GEM object

 *

 * Unpin the given GEM object previously pinned with omap_gem_pin(). Pins are

 * reference-counted, the actual unpin will only be performed when the number

 * of calls to this function matches the number of calls to omap_gem_pin().

/* Get rotated scanout address (only valid if already pinned), at the

 * specified orientation and x,y offset from top-left corner of buffer

 * (only valid for tiled 2d buffers)

 Get tiler stride for the buffer (only valid for 2d tiled buffers) */

/* if !remap, and we don't have pages backing, then fail, rather than

 * increasing the pin count (which we don't really do yet anyways,

 * because we don't support swapping pages back out).  And 'remap'

 * might not be quite the right name, but I wanted to keep it working

 * similarly to omap_gem_pin().  Note though that mutex is not

 * aquired if !remap (because this can be called in atomic ctxt),

 * but probably omap_gem_unpin() should be changed to work in the

 * same way.  If !remap, a matching omap_gem_put_pages() call is not

 * required (and should not be made).

 release pages when DMA no longer being performed */

	/* do something here if we dynamically attach/detach pages.. at

	 * least they would no longer need to be pinned if everyone has

	 * released the pages..

/*

 * Get kernel virtual address for CPU access.. this more or less only

 * exists for omap_fbdev.

/* -----------------------------------------------------------------------------

 * Power Management

 re-pin objects in DMM in resume path: */

 this can't happen */

/* -----------------------------------------------------------------------------

 * DebugFS

/* -----------------------------------------------------------------------------

 * Constructor & Destructor

	/*

	 * We own the sole reference to the object at this point, but to keep

	 * lockdep happy, we must still take the omap_obj_lock to call

	 * omap_gem_detach_pages(). This should hardly make any difference as

	 * there can't be any lock contention.

 The object should not be pinned. */

 GEM buffer object constructor */

 Validate the flags and compute the memory and cache flags. */

		/*

		 * Tiled buffers are always shmem paged backed. When they are

		 * scanned out, they are remapped into DMM/TILER.

		/*

		 * Currently don't allow cached buffers. There is some caching

		 * stuff that needs to be handled better.

		/*

		 * If we don't have DMM, we must allocate scanout buffers

		 * from contiguous DMA memory.

		/*

		 * All other buffers not backed by dma_buf are shmem-backed.

 Allocate the initialize the OMAP GEM object. */

		/*

		 * For tiled buffers align dimensions to slot boundaries and

		 * calculate size based on aligned dimensions.

 Initialize the GEM object. */

 Allocate memory if needed. */

 Without a DMM only physically contiguous buffers can be supported. */

 Create pages list from sgt */

 convenience method to construct a GEM buffer object, and userspace handle */

 drop reference from allocate - handle holds it now */

/* -----------------------------------------------------------------------------

 * Init & Cleanup

 If DMM is used, we need to set some stuff up.. */

 DMM only supported on OMAP4 and later, so this isn't fatal */

 reserve 4k aligned/wide regions for userspace mappings: */

		/* note: since each region is 1 4kb page wide, and minimum

		 * number of rows, the height ends up being the same as the

		 * # of pages in the region

	/* I believe we can rely on there being no more outstanding GEM

	 * objects which could depend on usergart/dmm at this point.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Texas Instruments Incorporated - https://www.ti.com/

 * Author: Rob Clark <rob@ti.com>

 Must be first. */

 Shadow values for legacy userspace support. */

/* -----------------------------------------------------------------------------

 * Helper Functions

	/*

	 * Timeout is set to a "sufficiently" high value, which should cover

	 * a single frame refresh even on slower displays.

/* -----------------------------------------------------------------------------

 * DSS Manager Functions

/*

 * Manager-ops, callbacks from output when they need to configure

 * the upstream part of the video pipe.

 Called only from the encoder enable/disable and suspend/resume handlers. */

		/*

		 * Digit output produces some sync lost interrupts during the

		 * first frame when enabling, so we need to ignore those.

		/*

		 * When we disable the digit output, we need to wait for

		 * FRAMEDONE to know that DISPC has finished with the output.

		 *

		 * OMAP2/3 does not have FRAMEDONE irq for digit output, and in

		 * that case we need to use vsync interrupt, and wait for both

		 * even and odd frames.

 make sure the irq handler sees the value above */

/* -----------------------------------------------------------------------------

 * Setup, Flush and Page Flip

	/*

	 * If the dispc is busy we're racing the flush operation. Try again on

	 * the next vblank interrupt.

 Send the vblank event if one has been requested. */

 Wake up omap_atomic_complete. */

 Send the vblank event if one has been requested. */

 Wake up omap_atomic_complete. */

/* -----------------------------------------------------------------------------

 * CRTC Functions

 manual updated display will not trigger vsync irq */

	/*

	 * DSI might not call this, since the supplied mode is not a

	 * valid DISPC mode. DSI will calculate and configure the

	 * proper DISPC mode later.

 Check for bandwidth limit */

		/*

		 * Estimation for the bandwidth need of a given mode with one

		 * full screen plane:

		 * bandwidth = resolution * 32bpp * (pclk / (vtotal * htotal))

		 *					^^ Refresh rate ^^

		 *

		 * The interlaced mode is taken into account by using the

		 * pixelclock in the calculation.

		 *

		 * The equation is rearranged for 64bit arithmetic.

		/*

		 * Reject modes which would need more bandwidth if used with one

		 * full resolution plane (most common use case).

 Mirror new values for zpos and rotation in omap_crtc_state */

 Check if this CRTC is for a manually updated display */

 Only flush the CRTC if it is currently enabled. */

 send new image for page flips and modeset changes */

	/*

	 * Delegate property set to the primary plane. Get the plane state and

	 * set the property directly, the shadow copy will be assigned in the

	 * omap_crtc_atomic_check callback. This way updates to plane state will

	 * always be mirrored in the crtc state correctly.

/* -----------------------------------------------------------------------------

 * Init and Cleanup

 initialize crtc */

	/*

	 * We want to refresh manually updated displays from dirty callback,

	 * which is called quite often (e.g. for each drawn line). This will

	 * be used to do the display update asynchronously to avoid blocking

	 * the rendering process and merges multiple dirty calls into one

	 * update if they arrive very fast. We also call this function for

	 * atomic display updates (e.g. for page flips), which means we do

	 * not need extra locking. Atomic updates should be synchronous, but

	 * need to wait for the framedone interrupt anyways.

	/* The dispc API adapts to what ever size, but the HW supports

	 * 256 element gamma table for LCDs and 1024 element table for

	 * OMAP_DSS_CHANNEL_DIGIT. X server assumes 256 element gamma

	 * tables so lets use that. Size of HW gamma table can be

	 * extracted with dispc_mgr_gamma_size(). If it returns 0

	 * gamma table is not supported.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Texas Instruments Incorporated - https://www.ti.com/

 * Author: Rob Clark <rob.clark@linaro.org>

 call with wait_lock and dispc runtime held */

/**

 * enable_vblank - enable vblank interrupt events

 * @crtc: DRM CRTC

 *

 * Enable vblank interrupts for @crtc.  If the device doesn't have

 * a hardware vblank counter, this routine should be a no-op, since

 * interrupts will have to stay on to keep the count accurate.

 *

 * RETURNS

 * Zero on success, appropriate errno if the given @crtc's vblank

 * interrupt cannot be enabled.

/**

 * disable_vblank - disable vblank interrupt events

 * @crtc: DRM CRTC

 *

 * Disable vblank interrupts for @crtc.  If the device doesn't have

 * a hardware vblank counter, this routine should be a no-op, since

 * interrupts will have to stay on to keep the count accurate.

 flush posted write */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Texas Instruments Incorporated - https://www.ti.com/

 * Author: Rob Clark <rob@ti.com>

/*

 * mode config funcs

/* Notes about mapping DSS and DRM entities:

 *    CRTC:        overlay

 *    encoder:     manager.. with some extension to allow one primary CRTC

 *                 and zero or more video CRTC's to be mapped to one encoder?

 *    connector:   dssdev.. manager can be attached/detached from different

 *                 devices

 Apply the atomic update. */

		/* With the current dss dispc implementation we have to enable

		 * the new modeset before we can commit planes. The dispc ovl

		 * configuration relies on the video mode configuration been

		 * written into the HW when the ovl configuration is

		 * calculated.

		 *

		 * This approach is not ideal because after a mode change the

		 * plane update is executed only after the first vblank

		 * interrupt. The dispc implementation should be fixed so that

		 * it is able use uncommitted drm state information.

		/*

		 * OMAP3 DSS seems to have issues with the work-around above,

		 * resulting in endless sync losts if a crtc is enabled without

		 * a plane. For now, skip the WA for OMAP3.

	/*

	 * Wait for completion of the page flips to ensure that old buffers

	 * can't be touched by the hardware anymore before cleaning up planes.

 To balance the 'for_each_dss_output' loop */

	/*

	 * This function creates exactly one connector, encoder, crtc,

	 * and primary plane per each connected dss-device. Each

	 * connector->encoder->crtc chain is expected to be separate

	 * and each crtc is connect to a single dss-channel. If the

	 * configuration does not match the expectations or exceeds

	 * the available resources, the configuration is rejected.

 Create all planes first. They can all be put to any CRTC. */

	/*

	 * Create the encoders, attach the bridges and get the pipeline alias

	 * IDs.

 Sort the pipelines by DT aliases. */

	/*

	 * Populate the pipeline lookup table by DISPC channel. Only one display

	 * is allowed per channel.

 Create the connectors and CRTCs. */

	/*

	 * Note: these values are used for multiple independent things:

	 * connector mode filtering, buffer sizes, crtc sizes...

	 * Use big enough values here to cover all use cases, and do more

	 * specific checking in the respective code paths.

 We want the zpos to be normalized */

/*

 * Enable the HPD in external components if supported

/*

 * Disable the HPD in external components if supported

/*

 * drm ioctl funcs

 flags settable by userspace */

 Deprecated, to be removed. */

 Deprecated, to be removed. */

/*

 * drm driver funcs

 sentinel */ }

 Allocate and initialize the DRM device. */

 Get memory bandwidth limits */

 Initialize vblank handling, start with all CRTCs disabled. */

	/*

	 * Register the DRM device with the core and the connectors with

	 * sysfs.

 Allocate and initialize the driver private structure. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Texas Instruments Incorporated - https://www.ti.com/

 * Author: Rob Clark <rob@ti.com>

/*

 * framebuffer funcs

 16bpp [A]RGB: */

 RGB16-565 */

 RGB12x-4444 */

 xRGB12-4444 */

 RGBA12-4444 */

 ARGB16-4444 */

 xRGB15-1555 */

 ARGB16-1555 */

 24bpp RGB: */

 RGB24-888 */

 32bpp [A]RGB: */

 RGBx24-8888 */

 xRGB24-8888 */

 RGBA32-8888 */

 ARGB32-8888 */

 YUV: */

 per-plane info for the fb: */

 lock for pinning (pin_count and planes.dma_addr) */

 Note: DRM rotates counter-clockwise, TILER & DSS rotates clockwise */

/* update ovl info for scanout, handles cases of multi-planar fb's, etc.

 DSS driver wants the w & h in rotated orientation */

		/*

		 * omap_gem_rotated_paddr() wants the x & y in tiler units.

		 * Usually tiler unit size is the same as the pixel size, except

		 * for YUV422 formats, for which the tiler unit size is 32 bits

		 * and pixel size is 16 bits.

 adjust x,y offset for invert: */

 Note: x and y are in TILER units, not pixels */

 Note: stride in TILER units, not pixels */

 OK */

 convert to pixels: */

 pin, prepare for scanout: */

 unpin, no longer being scanned out: */

	/*

	 * The code below assumes that no format use more than two planes, and

	 * that the two planes of multiplane formats need the same number of

	 * bytes per pixel.

/*

 * SImple Tiler Allocator (SiTA): 2D and 1D allocation(reservation) algorithm

 *

 * Authors: Ravi Ramachandra <r.ramachandra@ti.com>,

 *          Lajos Molnar <molnar@ti.com>

 *          Andy Gross <andy.gross@ti.com>

 *

 * Copyright (C) 2012 Texas Instruments Incorporated - https://www.ti.com/

 *

 * This package is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 *

 * THIS PACKAGE IS PROVIDED ``AS IS'' AND WITHOUT ANY EXPRESS OR

 * IMPLIED WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED

 * WARRANTIES OF MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE.

 *

/*

 * pos		position in bitmap

 * w		width in slots

 * h		height in slots

 * map		ptr to bitmap

 * stride		slots in a row

/*

 * w		width in slots

 * pos		ptr to position

 * map		ptr to bitmap

 * num_bits	number of bits in bitmap

 found a long enough free area */

/*

 * w = width in slots

 * h = height in slots

 * a = align in slots	(mask, 2^n-1, 0 is unaligned)

 * offset = offset in bytes from 4KiB

 * pos = position in bitmap for buffer

 * map = bitmap ptr

 * num_bits = size of bitmap

 * stride = bits in one row of container

 reset alignment to 1 if we are matching a specific offset */

 adjust alignment - 1 to get to the format expected in bitmaps */

 FIXME Return error if slots_per_band > stride */

 skip forward if we are not at right offset */

 skip forward to next row if we overlap end of row */

 TODO: Handle overlapping 4K boundaries */

 break out of look if we will go past end of container */

 generate mask that represents out matching pattern */

 assume the area is free until we find an overlap */

 check subsequent rows to see if complete area is free */

 go forward past this match */

 set area as in-use. iterate over rows */

 Updating the pointers to SiTA implementation APIs */

/*

 * DMM IOMMU driver support functions for TI OMAP processors.

 *

 * Copyright (C) 2011 Texas Instruments Incorporated - https://www.ti.com/

 * Author: Rob Clark <rob@ti.com>

 *         Andy Gross <andy.gross@ti.com>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License as

 * published by the Free Software Foundation version 2.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether express or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

 platform_device() */

 mappings for associating views to luts */

 global spinlock for protecting lists */

 Geometry table */

 unused X-bits (as part of bpp) */

 unused Y-bits (as part of bpp) */

 bytes/chars per pixel */

 width of each slot (in pixels) */

 height of each slot (in pixels) */

 lookup table for registers w/ per-engine instances */

	/*

	 * As per i878 workaround, the DMA is used to access the DMM registers.

	 * Make sure that the readl is not moved by the compiler or the CPU

	 * earlier than the DMA finished writing the value to memory.

	/*

	 * As per i878 workaround, the DMA is used to access the DMM registers.

	 * Make sure that the writel is not moved by the compiler or the CPU, so

	 * the data will be in place before we start the DMA to do the actual

	 * register write.

 simple allocator to grab next 16 byte aligned memory from txn */

 dmm programming requires 16 byte aligned addresses */

 check status and spin until wait_mask comes true */

 ack IRQ */

/*

 * Get a handle for a DMM transaction

 wait until an engine is available */

 grab an idle engine */

/*

 * Add region to DMM transaction.  If pages or pages[i] is NULL, then the

 * corresponding slot is cleared (ie. dummy_pa is programmed)

 adjust Y coordinates based off of container parameters */

 FIXME: what if data_pa is more than 32-bit ? */

/*

 * Commit the DMM transaction.

 ensure that the written descriptors are visible to DMM */

	/*

	 * NOTE: the wmb() above should be enough, but there seems to be a bug

	 * in OMAP's memory barrier implementation, which in some rare cases may

	 * cause the writes not to be observable after wmb().

 read back to ensure the data is in RAM */

 write to PAT_DESCR to clear out any pending transaction */

 wait for engine ready: */

 mark whether it is async to denote list management in IRQ handler */

 verify that the irq handler sees the 'async' and completion value */

 kick reload */

 Check the engine status before continue */

 only place engine back on list if we are done with it */

/*

 * DMM programming

	/*

	 * FIXME

	 *

	 * Asynchronous fill does not work reliably, as the driver does not

	 * handle errors in the async code paths. The fill operation may

	 * silently fail, leading to leaking DMM engines, which may eventually

	 * lead to deadlock if we run out of DMM engines.

	 *

	 * For now, always set 'wait' so that we only use sync fills. Async

	 * fills should be fixed, or alternatively we could decide to only

	 * support sync fills and so the whole async code path could be removed.

/*

 * Pin/unpin

/* note: slots for which pages[i] == NULL are filled w/ dummy page

/*

 * Reserve/release

 convert width/height to slots */

 convert alignment to slots */

 add to allocation list */

 note: if you have pin'd pages, you should have already unpin'd first! */

/*

 * Utils

/* calculate the tiler space address of a pixel in a view orientation...

 * below description copied from the display subsystem section of TRM:

 *

 * When the TILER is addressed, the bits:

 *   [28:27] = 0x0 for 8-bit tiled

 *             0x1 for 16-bit tiled

 *             0x2 for 32-bit tiled

 *             0x3 for page mode

 *   [31:29] = 0x0 for 0-degree view

 *             0x1 for 180-degree view + mirroring

 *             0x2 for 0-degree view + mirroring

 *             0x3 for 180-degree view

 *             0x4 for 270-degree view + mirroring

 *             0x5 for 270-degree view

 *             0x6 for 90-degree view

 *             0x7 for 90-degree view + mirroring

 * Otherwise the bits indicated the corresponding bit address to access

 * the SDRAM.

 validate coordinate */

 account for mirroring */

 get coordinate address */

 Disable all enabled interrupts */

 free all area regions */

 initialize lists */

 lookup hwmod data - base address and irq */

		/*

		 * DRA7 Errata i878 says that MPU should not be used to access

		 * RAM and DMM at the same time. As it's not possible to prevent

		 * MPU accessing RAM, we need to access DMM via a proxy.

 read out actual LUT width and height */

 increment LUT by one if on OMAP5 */

 LUT has twice the height, and is split into a separate container */

 initialize DMM registers */

 set dma mask for device */

 alloc refill memory */

 alloc engines */

 init containers */

	/* Each LUT is associated with a TCM (container manager).  We use the

	   lut_id to denote the lut_id used to identify the correct LUT for

 assign access mode containers to applicable tcm container */

 OMAP 4 has 1 container for all 4 views */

 OMAP 5 has 2 containers, 1 for 2D and 1 for 1D */

		/* second LUT is used for PAGE mode.  Programming must use

		   y offset that is added to all y coordinates.  LUT id is still

	/* Enable all interrupts for each refill engine except

	 * ERR_LUT_MISS<n> (which is just advisory, and we don't care

	 * about because we want to be able to refill live scanout

	 * buffers for accelerated pan/scroll) and FILL_DSC<n> which

	 * we just generally don't care about.

 initialize all LUTs to dummy page entries */

/*

 * debugfs support

 early return if dmm/tiler device is not initialized */

 initialize all LUTs to dummy page entries */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Texas Instruments Incorporated - https://www.ti.com/

 * Author: Rob Clark <rob.clark@linaro.org>

/* -----------------------------------------------------------------------------

 * DMABUF Export

	/* camera, etc, need physically contiguous.. but we need a

	 * better way to know this..

 this must be after omap_gem_pin() to ensure we have pages attached */

		/* TODO we would need to pin at least part of the buffer to

		 * get de-tiled view.  For now just reject it.

 make sure we have the pages: */

/* -----------------------------------------------------------------------------

 * DMABUF Import

			/*

			 * Importing dmabuf exported from out own gem increases

			 * refcount on gem itself instead of f_count of dmabuf.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Texas Instruments Incorporated - https://www.ti.com/

 * Author: Rob Clark <rob@ti.com>

/*

 * fbdev funcs, to implement legacy fbdev interface on top of drm driver

 for deferred dmm roll when getting called in atomic ctx */

 DMM roll shifts in 4K pages: */

 need to align pitch to page size if using DMM scrolling */

 allocate backing bo */

		/* note: if fb creation failed, we can't rely on fb destroy

		 * to unref the bo:

	/* note: this keeps the bo pinned.. which is perhaps not ideal,

	 * but is needed as long as we use fb_mmap() to mmap to userspace

	 * (since this happens using fix.smem_start).  Possibly we could

	 * implement our own mmap using GEM mmap support to avoid this

	 * (non-tiled buffer doesn't need to be pinned for fbcon to write

	 * to it).  Then we just need to be sure that we are able to re-

	 * pin it in case of an opps.

	/* if we have DMM, then we can use it for scrolling by just

	 * shuffling pages around in DMM rather than doing sw blit.

 these are not the fb's you're looking for */

 initialize fbdev helper */

 unpin the GEM object pinned in omap_fbdev_create() */

 this will free the backing object */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Texas Instruments Incorporated - https://www.ti.com/

 * Author: Rob Clark <rob.clark@linaro.org>

 list of debufs files that are applicable to all devices */

 list of debugfs files that are specific to devices with dmm/tiler */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Texas Instruments Incorporated - https://www.ti.com/

 * Author: Rob Clark <rob.clark@linaro.org>

/*

 * plane funcs

 update scanout: */

 and finally, update omapdss: */

 crtc should only be NULL when disabling (i.e., !new_plane_state->fb) */

 we should have a crtc state if the plane is attached to a crtc */

 helper to install properties which are common to planes and crtcs */

 Attach the rotation property also to the crtc object */

	/*

	 * Set the zpos default depending on whether we are a primary or overlay

	 * plane.

 initialize plane */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * OMAP5 HDMI CORE IP driver library

 *

 * Copyright (C) 2014 Texas Instruments Incorporated - https://www.ti.com/

 * Authors:

 *	Yong Zhi

 *	Mythri pk

 *	Archit Taneja <archit@ti.com>

 *	Tomi Valkeinen <tomi.valkeinen@ti.com>

 DSS L3 ICLK */

 ns */

 ns */

 ns */

 ns */

 ns */

 SFR_DIV */

 SFR clock in kHz */

 Reset */

 Standard (0) or Fast (1) Mode */

 Standard Mode SCL High counter */

 Standard Mode SCL Low counter */

 Fast Mode SCL High Counter */

 Fast Mode SCL Low Counter */

 SDA Hold Time */

 NACK_POL to high */

 NACK_MASK to unmasked */

 ARBITRATION_POL to high */

 ARBITRATION_MASK to unmasked */

 DONE_POL to high */

 DONE_MASK to unmasked */

 Mask I2C interrupts */

	/*

	 * TODO: We use polling here, although we probably should use proper

	 * interrupts.

 clear ERROR and DONE */

 I2CM_ERROR */

 I2CM_DONE */

 video core */

 It is always 1*/

 set vblank_osc if vblank is fractional */

 DSS_HDMI_CORE_VIDEO_CONFIG */

 Set hsync, vsync and data-enable polarity  */

 set x resolution */

 set y resolution */

 set horizontal blanking pixels */

 set vertial blanking pixels */

 set horizontal sync offset */

 set vertical sync offset */

 set horizontal sync pulse width */

  set vertical sync pulse width */

 select DVI mode */

 24 bit color depth */

 COLOR_DEPTH */

 BYPASS_EN */

 PP_EN */

 YCC422_EN */

 PP_STUFFING */

 YCC422_STUFFING */

 OUTPUT_SELECTOR */

 for 24 bit color depth */

 VIDEO_MAPPING */

 enable CSC */

 CSC_COLORDEPTH  = 24 bits*/

 Master IRQ mask */

 Mask all the interrupts in HDMI core */

 Clear all the current interrupt bits */

 Unmute interrupts */

 All CEA modes other than VIC 1 use limited quantization range. */

 video config */

	/*

	 * configure core video part, set software reset in the core

 Mute audio before configuring */

 Set the N parameter */

	/*

	 * CTS manual mode. Automatic mode is not supported when using audio

	 * parallel interface.

 Layout of Audio Sample Packets: 2-channel or multichannels */

 Configure IEC-609580 Validity bits */

 Channel 0 is valid */

 Channels 1, 2 setting */

 Channel 3 setting */

 Configure IEC-60958 User bits */

 TODO: should be set by user. */

 Configure IEC-60958 Channel Status word */

 CGMSA */

 Copyright */

 Category */

 PCM audio mode */

 Source number */

 Channel number right 0  */

 Channel number right 1*/

 Channel number right 2  */

 Channel number right 3*/

 Channel number left 0  */

 Channel number left 1*/

 Channel number left 2  */

 Channel number left 3*/

 Clock accuracy and sample rate */

 Original sample rate and word length */

 Enable FIFO empty and full interrupts */

 Configure GPA */

 select HBR/SPDIF interfaces */

 select HBR/SPDIF interfaces */

 enable two channels in GPA */

 select HBR/SPDIF interfaces */

 enable six channels in GPA */

 select HBR/SPDIF interfaces */

 enable eight channels in GPA */

 disable HBR */

 enable PCUV */

 enable GPA FIFO full and empty mask */

 set polarity of GPA FIFO empty interrupts */

 unmute audio */

 channel count and coding type fields in AUDICONF0 are swapped */

 only 16-bit word length supported atm */

 Audio channels settings */

 DMA settings */

 in number of samples */

 audio FIFO format settings for 16-bit samples*/

 only LPCM atm */

 only allowed option */

 disable start/stop signals of IEC 60958 blocks */

 configure DMA and audio FIFO format*/

 configure the core */

 configure CEA 861 audio infoframe */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2009 Nokia Corporation

 * Author: Tomi Valkeinen <tomi.valkeinen@ti.com>

 *

 * Some code and ideas taken from drivers/video/omap/ driver

 * by Imre Deak.

 DISPC */

 DISPC has feature id */

 Independent core clk divider */

 An unknown HW bug causing the normal FIFO thresholds not to work */

 swap GFX & WB fifos */

 no DISPC_IRQ_FRAMEDONETV on this SoC */

 revert to the OMAP4 mechanism of DISPC Smart Standby operation */

 PIXEL_INC is not added to the last pixel of a line */

 POL_FREQ has ALIGN bit */

	/*

	 * Field order for VENC is different than HDMI. We should handle this in

	 * some intelligent manner, but as the SoCs have either HDMI or VENC,

	 * never both, we can just use this flag for now.

 maps which plane is using a fifo. fifo-id -> plane-id */

	/* used for all color formats for OMAP3 and earlier

	 * and for RGB and Y color component on OMAP4

	/* used for UV component for

	 * DRM_FORMAT_YUYV, DRM_FORMAT_UYVY, DRM_FORMAT_NV12

	 * color formats on OMAP4

 used to maintain a count of the above fields */

 DISPC register field id */

RR(dispc, IRQENABLE);*/

RR(dispc, CONTROL);*/

 enable last, because LCD & DIGIT enable are here */

 clear spurious SYNC_LOST_DIGIT interrupts */

	/*

	 * enable last so IRQs won't trigger before

	 * the context is fully restored

 flush posted write */

 YUV -> RGB, ITU-R BT.601, full range */

 ry, rcb, rcr |1.000  0.000  1.402|*/

 gy, gcb, gcr |1.000 -0.344 -0.714|*/

 by, bcb, bcr |1.000  1.772  0.000|*/

 full range */

 YUV -> RGB, ITU-R BT.601, limited range */

 ry, rcb, rcr |1.164  0.000  1.596|*/

 gy, gcb, gcr |1.164 -0.392 -0.813|*/

 by, bcb, bcr |1.164  2.017  0.000|*/

 limited range */

 YUV -> RGB, ITU-R BT.709, full range */

 ry, rcb, rcr |1.000  0.000  1.570|*/

 gy, gcb, gcr |1.000 -0.187 -0.467|*/

 by, bcb, bcr |1.000  1.856  0.000|*/

 full range */

 YUV -> RGB, ITU-R BT.709, limited range */

 ry, rcb, rcr |1.164  0.000  1.793|*/

 gy, gcb, gcr |1.164 -0.213 -0.533|*/

 by, bcb, bcr |1.164  2.112  0.000|*/

 limited range */

 Configure burst size always to maximum size */

 burst multiplier is always x8 (see dispc_configure_burst_sizes()) */

		/*

		 * By default fifos are mapped directly to overlays, fifo 0 to

		 * ovl 0, fifo 1 to ovl 1, etc.

	/*

	 * The GFX fifo on OMAP4 is smaller than the other fifos. The small fifo

	 * causes problems with certain use cases, like using the tiler in 2D

	 * mode. The below hack swaps the fifos of GFX and WB planes, thus

	 * giving GFX plane a larger fifo. WB but should work fine with a

	 * smaller fifo.

 GFX BUF top to WB */

 GFX BUF bottom to WB */

 WB BUF top to GFX */

 WB BUF bottom to GFX */

	/*

	 * Setup default fifo thresholds.

	/*

	 * configure the preload to the pipeline's high threhold, if HT it's too

	 * large for the preload field, set the threshold to the maximum value

	 * that can be held by the preload register

	/*

	 * All sizes are in bytes. Both the buffer and burst are made of

	 * buffer_units, and the fifo thresholds must be buffer_unit aligned.

	/*

	 * We use the same low threshold for both fifomerge and non-fifomerge

	 * cases, but for fifomerge we calculate the high threshold using the

	 * combined fifo size

		/*

		 * Most optimal configuration for writeback is to push out data

		 * to the interconnect the moment writeback pushes enough pixels

		 * in the FIFO to form a burst

	/*

	 * HACK: NV12 color format and MFLAG seem to have problems working

	 * together: using two displays, and having an NV12 overlay on one of

	 * the displays will cause underflows/synclosts when MFLAG_CTRL=2.

	 * Changing MFLAG thresholds and PRELOAD to certain values seem to

	 * remove the errors, but there doesn't seem to be a clear logic on

	 * which values work and which not.

	 *

	 * As a work-around, set force MFLAG to always on.

 MFLAG_CTRL = force always on */

 MFLAG_START = disable */

		/*

		 * Simulation team suggests below thesholds:

		 * HT = fifosize * 5 / 8;

		 * LT = fifosize * 4 / 8;

		/*

		 * Simulation team suggests below thesholds:

		 * HT = fifosize * 5 / 8;

		 * LT = fifosize * 4 / 8;

 Note: DSS HW rotates clockwise, DRM_MODE_ROTATE_* counter-clockwise */

 RESIZEENABLE and VERTICALTAPS */

 VRESIZECONF and HRESIZECONF */

 LINEBUFFERSPLIT */

	/*

	 * field 0 = even field = bottom field

	 * field 1 = odd field = top field

 reset chroma resampling for RGB formats  */

 UV is subsampled by 2 horizontally and vertically */

 UV is downsampled by 2 horizontally and vertically */

 For YUV422 with 90/270 rotation, we don't upsample chroma */

 UV is subsampled by 2 horizontally */

 UV is downsampled by 2 horizontally */

 must use FIR for YUV422 if rotated */

 set H scaling */

 set V scaling */

 Note: DSS HW rotates clockwise, DRM_MODE_ROTATE_* counter-clockwise */

	/*

	 * OMAP4/5 Errata i631:

	 * NV12 in 1D mode must use ROTATION=1. Otherwise DSS will fetch extra

	 * rows beyond the framebuffer, which may cause OCP error.

 DOUBLESTRIDE */

		/*

		 * HACK: ROW_INC needs to be calculated with TILER units.

		 * We get such 'screen_width' that multiplying it with the

		 * YUV422 pixel size gives the correct TILER container width.

		 * However, 'width' is in pixels and multiplying it with YUV422

		 * pixel size gives incorrect result. We thus multiply it here

		 * with 2 to match the 32 bit TILER unit size.

	/*

	 * field 0 = even field = bottom field

	 * field 1 = odd field = top field

/*

 * This function is used to avoid synclosts in OMAP3, because of some

 * undocumented horizontal position and timing related limitations.

 FIXME add checks for 3-tap filter once the limitations are known */

	/*

	 * Pixel data should be prepared before visible display point starts.

	 * So, atleast DS-2 lines must have already been fetched by DISPC

	 * during nonactive - pos_x period.

	/*

	 * All lines need to be refilled during the nonactive period of which

	 * only one line can be loaded during the active period. So, atleast

	 * DS - 1 lines should be loaded during nonactive period.

	/*

	 * FIXME how to determine the 'A' factor

	 * for the no downscaling case ?

	/*

	 * If the overlay/writeback is in mem to mem mode, there are no

	 * downscaling limitations with respect to pixel clock, return 1 as

	 * required core clock to represent that we have sufficient enough

	 * core clock to do maximum downscaling

 verify that we're inside the limits of scaler */

		/*

		 * Let's disable all scaling that requires horizontal

		 * decimation with higher factor than 4, until we have

		 * better estimates of what we can and can not

		 * do. However, NV12 color format appears to work Ok

		 * with all decimation factors.

		 *

		 * When decimating horizontally by more that 4 the dss

		 * is not able to fetch the data in burst mode. When

		 * this happens it is hard to tell if there enough

		 * bandwidth. Despite what theory says this appears to

		 * be true also for 16-bit color formats.

 when setting up WB, dispc_plane_pclk_rate() returns 0 */

		/*

		 * when downscaling the bottom field may have to start several

		 * source lines below the top field. Unfortunately ACCUI

		 * registers will only hold the fractional part of the offset

		 * so the integer part must be added to the base address of the

		 * bottom field.

 Fields are independent but interleaved in memory. */

 setup extra DISPC_WB_ATTRIBUTES */

 TRUNCATIONENABLE */

 CHANNELIN */

 WRITEBACKMODE */

 CAPTUREMODE */

 CAPTUREMODE */

 WBDELAYCOUNT */

 WBDELAYCOUNT */

 TODO: OMAP4+ supports interlace for LCD outputs */

 always use the 'rf' setting */

 always set ALIGN bit when available */

 change name to mode? */

 for TV, LCLK rate is the FCLK rate */

 DISPC common registers */

 DISPC channel specific registers */

 Video pipeline coefficient registers */

 start from OMAP_DSS_VIDEO1 */

 calculate clock rates using dividers in cinfo */

			/*

			 * For OMAP2/3 the DISPC fclk is the same as LCD's logic

			 * clock, which means we're configuring DISPC fclk here

			 * also. Thus we need to use the calculated lck. For

			 * OMAP4+ the DISPC fclk is a separate clock.

 clear the irqstatus for newly enabled irqs */

 flush posted write */

 SIDLEMODE: smart idle */

 SIDLEMODE: no idle */

 Exclusively enable DISPC_CORE_CLK and set divider to 1 */

 Use DISPC_DIVISOR.LCD, instead of DISPC_DIVISOR1.LCD */

 Use gamma table mode, instead of palette mode */

	/* For older DSS versions (FEAT_FUNCGATED) this enables

	 * func-clock auto-gating. For newer versions

	 * (dispc->feat->has_gamma_table) this enables tv-out gamma tables.

 OMAP_DSS_GFX */

 OMAP_DSS_VIDEO1 */

 OMAP_DSS_VIDEO2 */

 OMAP_DSS_GFX */

 OMAP_DSS_VIDEO1 */

 OMAP_DSS_VIDEO2 */

 OMAP_DSS_GFX */

 OMAP_DSS_VIDEO1 */

 OMAP_DSS_VIDEO2 */

 OMAP_DSS_GFX */

 OMAP_DSS_VIDEO1 */

 OMAP_DSS_VIDEO2 */

 OMAP_DSS_VIDEO3 */

 OMAP_DSS_GFX */

 OMAP_DSS_VIDEO1 */

 OMAP_DSS_VIDEO2 */

 OMAP_DSS_GFX */

 OMAP_DSS_VIDEO1 */

 OMAP_DSS_VIDEO2 */

 OMAP_DSS_GFX */

 OMAP_DSS_VIDEO1 */

 OMAP_DSS_VIDEO2 */

 OMAP_DSS_VIDEO3 */

 OMAP_DSS_WB */

	/*

	 * Assume the line width buffer to be 768 pixels as OMAP2 DISPC scaler

	 * cannot scale an image width larger than 768.

 ensure the dispc_irq_handler sees the values above */

 Optional maximum memory bandwidth */

/*

 * Workaround for errata i734 in DSS dispc

 *  - LCD1 Gamma Correction Is Not Working When GFX Pipe Is Disabled

 *

 * For gamma tables to work on LCD1 the GFX plane has to be used at

 * least once after DSS HW has come out of reset. The workaround

 * sets up a minimal LCD setup with GFX plane and waits for one

 * vertical sync irq before disabling the setup and continuing with

 * the context restore. The physical outputs are gated during the

 * operation. This workaround requires that gamma table's LOADMODE

 * is set to 0x2 in DISPC_CONTROL1 register.

 *

 * For details see:

 * OMAP543x Multimedia Device Silicon Revision 2.0 Silicon Errata

 * Literature Number: SWPZ037E

 * Or some other relevant errata document for the DSS IP version.

 Gate all LCD1 outputs */

 Setup and enable GFX plane */

 Set up and enable display manager for LCD1 */

 Enable and shut the channel to produce just one frame */

	/* Busy wait for framedone. We can't fiddle with irq handlers

	 * in PM resume. Typically the loop runs less than 5 times and

	 * waits less than a micro second.

 Clear all irq bits before continuing */

 Restore the original state to LCD1 output gates */

 DISPC HW IP initialisation */

 sentinel */ }

	/*

	 * The OMAP3-based models can't be told apart using the compatible

	 * string, use SoC device matching.

 ensure the dispc_irq_handler sees the is_enabled value */

 wait for current handler to finish before turning the DISPC off */

	/*

	 * The reset value for load mode is 0 (OMAP_DSS_LOAD_CLUT_AND_FRAME)

	 * but we always initialize it to 2 (OMAP_DSS_LOAD_FRAME_ONLY) in

	 * _omap_dispc_initial_config(). We can thus use it to detect if

	 * we have lost register context.

 ensure the dispc_irq_handler sees the is_enabled value */

