 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * suballoc.c

 *

 * metadata alloc and free

 * Inspired by ext3 block groups.

 *

 * Copyright (C) 2002, 2004 Oracle.  All rights reserved.

	u64		sr_bg_blkno;	/* The bg we allocated from.  Set

					   to 0 when a block group is

	u64		sr_bg_stable_blkno; /*

					     * Doesn't change, always

					     * set to target block

					     * group descriptor

					     * block.

 The first allocated block */

 The bit in the bg */

 How many bits we claimed */

 In resize, we may meet the case bg_chain == cl_next_free_rec. */

/*

 * This version only prints errors.  It does not fail the filesystem, and

 * exists only for resize.

	/*

	 * If the ecc fails, we return the error but otherwise

	 * leave the filesystem running.  We know any error is

	 * local to this block.

	/*

	 * If the ecc fails, we return the error but otherwise

	 * leave the filesystem running.  We know any error is

	 * local to this block.

	/*

	 * Errors after here are fatal.

 If ocfs2_read_block() got us a new bh, pass it up. */

 set the 1st bit in the bitmap to account for the descriptor block */

	/* There is no need to zero out or otherwise initialize the

	 * other blocks in a group - All valid FS metadata in a block

	 * group stores the superblock fs_generation value at

 setup the group */

		/*

		 * We have used up all the extent rec but can't fill up

		 * the cpg. So bail out.

 Try all the clusters to free */

	/*

	 * We're going to be grabbing from multiple cluster groups.

	 * We don't have enough credits to relink them all, and the

	 * cluster groups will be staying in cache for the duration of

	 * this operation.

 Claim the first region */

 setup the group */

/*

 * We expect the block group allocator to already be locked.

 save the new last alloc group so that the caller can cache it. */

	/* The bh was validated by the inode read inside

 cluster bitmap never grows */

 You should never ask for this much metadata */

 Start to steal resource from the first slot after ours. */

	/*

	 * stat(2) can't handle i_ino > 32bits, so we tell the

	 * lower levels not to allocate us a block group past that

	 * limit.  The 'inode64' mount option avoids this behavior.

	/*

	 * slot is set when we successfully steal inode from other nodes.

	 * It is reset in 3 places:

	 * 1. when we flush the truncate log

	 * 2. when we complete local alloc recovery.

	 * 3. when we successfully allocate from our own slot.

	 * After it is set, we will go on stealing inodes until we find the

	 * need to check our slots to see whether there is some space for us.

		/*

		 * Some inodes must be freed by us, so try to allocate

		 * from our own next time.

/* local alloc code has to do the same thing, so rather than do this

/* Callers don't need to care which bitmap (local alloc or main) to

 * use so we figure it out for them, but unfortunately this clutters

 Retry if there is sufficient space cached in truncate log */

/*

 * More or less lifted from ext3. I'll leave their description below:

 *

 * "For ext3 allocations, we must not reuse any blocks which are

 * allocated in the bitmap buffer's "last committed data" copy.  This

 * prevents deletes from freeing up the page for reuse until we have

 * committed the delete transaction.

 *

 * If we didn't do this, then deleting something and reallocating it as

 * data would allow the old block to be overwritten before the

 * transaction committed (because we force data to disk before commit).

 * This would lead to corruption if we crashed between overwriting the

 * data and committing the delete.

 *

 * @@@ We may want to make this allocation behaviour conditional on

 * data-writes at some point, and disable it for metadata allocations or

 * sync-data inodes."

 *

 * Note: OCFS2 already does this differently for metadata vs data

 * allocations, as those bitmaps are separate and undo access is never

 * called on a metadata group descriptor.

	/* Callers got this descriptor from

			/* We found a zero, but we can't use it as it

 we found a zero */

 move start to the next bit to test */

 got a zero after some ones */

 we got everything we needed */

 mlog(0, "Found it all!\n"); */

		/* No error log here -- see the comment above

	/* All callers get the descriptor via

 find the one with the most empty bits */

	/* there is a really tiny chance the journal calls could fail,

	/* The caller got these descriptors from

/* return 0 on success, -ENOSPC to keep searching and any other < 0

		/* Tail groups in cluster bitmaps which aren't cpg

		 * aligned are prone to partial extension by a failed

		 * fs resize. If the file system resize never got to

		 * update the dinode cluster count, then we don't want

		 * to trust any clusters past it, regardless of what

		/* ocfs2_block_group_find_clear_bits() might

		 * return success, but we still want to return

		 * -ENOSPC unless it found the minimum number

 success */

			/*

			 * Don't show bits which we'll be returning

			 * for allocation to the local alloc bitmap.

 Save off */

 Clear it for contig block groups */

 Restore */

	/*

	 * sr_bg_blkno might have been changed by

	 * ocfs2_bg_discontig_fix_result

	/* for now, the chain search is a bit simplistic. We just use

	/*

	 * sr_bg_blkno might have been changed by

	 * ocfs2_bg_discontig_fix_result

	/*

	 * Keep track of previous block descriptor read. When

	 * we find a target, if we have read more than X

	 * number of descriptors, and the target is reasonably

	 * empty, relink him to top of his chain.

	 *

	 * We've read 0 extra blocks and only send one more to

	 * the transaction, yet the next guy to search has a

	 * much easier time.

	 *

	 * Do this *after* figuring out how many bits we're taking out

	 * of our target group.

 will give out up to bits_wanted contiguous bits. */

	/* The bh was validated by the inode read during

		/* Attempt to short-circuit the usual search mechanism

		 * by jumping straight to the most recently used

		 * allocation group. This helps us maintain some

	/* If we didn't pick a good victim, then just default to

	 * searching each chain in order. Don't allow chain relinking

	 * because we only calculate enough journal credits for one

		/* If the next search of this group is not likely to

		 * yield a suitable extent, then we reset the last

	/*

	 * Try to allocate inodes from some specific group.

	 *

	 * If the parent dir has recorded the last group used in allocation,

	 * cool, use it. Otherwise if we try to allocate new inode from the

	 * same slot the parent dir belongs to, use the same chunk.

	 *

	 * We are very careful here to avoid the mistake of setting

	 * ac_last_group to a group descriptor from a different (unlocked) slot.

	/*

	 * The handle started here is for chain relink. Alternatively,

	 * we could just disable relink for these calls.

	/*

	 * This will instruct ocfs2_claim_suballoc_bits and

	 * ocfs2_search_one_group to search but save actual allocation

	 * for later.

	/*

	 * Since di_blkno is being passed back in, we check for any

	 * inconsistencies which may have happened between

	 * calls. These are code bugs as di_blkno is not expected to

	 * change once returned from ocfs2_find_new_inode_loc()

/* translate a group desc. blkno and it's bitmap offset into

/* given a cluster offset, calculate which block group it belongs to

/* given the block number of a cluster start, calculate which cluster

/*

 * min_bits - minimum contiguous chunk from this total allocation we

 * can handle. set to what we asked for originally for a full

 * contig. allocation, set to '1' to indicate we can deal with extents

 * of any size.

			/* The only paths asking for contiguousness

 clamp the current request down to a realistic size. */

 cluster alloc can't set */

	/* The caller got this descriptor from

/*

 * expects the suballoc inode to already be locked.

	/* The alloc_bh comes from ocfs2_free_dinode() or

	 * ocfs2_free_clusters().  The callers have all locked the

	 * allocator and gotten alloc_bh from the lock call.  This

	 * validates the dinode buffer.  Any corruption that has happened

	/* You can't ever have a contiguous set of clusters

	 * bigger than a block group bitmap so we never have to worry

	 * about looping on them.

	 * This is expensive. We can safely remove once this stuff has

/*

 * Give never-used clusters back to the global bitmap.  We don't need

 * to protect these bits in the undo buffer.

/*

 * For a given allocation, determine which allocators will need to be

 * accessed, and lock them, reserving the appropriate number of bits.

 *

 * Sparse file systems call this from ocfs2_write_begin_nolock()

 * and ocfs2_allocate_unwritten_extents().

 *

 * File systems which don't support holes call this from

 * ocfs2_extend_allocation().

	/*

	 * Sparse allocation file systems need to be more conservative

	 * with reserving room for expansion - the actual allocation

	 * happens while we've got a journal handle open so re-taking

	 * a cluster lock (because we ran out of room for another

	 * extent) will violate ordering rules.

	 *

	 * Most of the time we'll only be seeing this 1 cluster at a time

	 * anyway.

	 *

	 * Always lock for any unwritten extents - we might want to

	 * add blocks during a split.

		/*

		 * We cannot have an error and a non null *data_ac.

/*

 * Read the inode specified by blkno to get suballoc_slot and

 * suballoc_bit.

 dirty read disk */

/*

 * test whether bit is SET in allocator bitmap or not.  on success, 0

 * is returned and *res is 1 for SET; 0 otherwise.  when fails, errno

 * is returned and *res is meaningless.  Call this after you have

 * cluster locked against suballoc, or you may get a result based on

 * non-up2date contents

/*

 * Test if the bit representing this inode (blkno) is set in the

 * suballocator.

 *

 * On success, 0 is returned and *res is 1 for SET; 0 otherwise.

 *

 * In the event of failure, a negative value is returned and *res is

 * meaningless.

 *

 * Callers must make sure to hold nfs_sync_lock to prevent

 * ocfs2_delete_inode() on another node from accessing the same

 * suballocator concurrently.

		/* the error code could be inaccurate, but we are not able to

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * resize.c

 *

 * volume resize.

 * Inspired by ext3/resize.c.

 *

 * Copyright (C) 2007 Oracle.  All rights reserved.

/*

 * Check whether there are new backup superblocks exist

 * in the last group. If there are some, mark them or clear

 * them in the bitmap.

 *

 * Return how many backups we find in the last group.

 check if already done backup super */

 update the group first. */

	/*

	 * check whether there are some new backup superblocks exist in

	 * this group and update the group bitmap accordingly.

 update the inode accordingly. */

 calculate the real backups we need to update. */

	/*

	 * update the superblock last.

	 * It doesn't matter if the write failed.

/*

 * Extend the filesystem to the new number of clusters specified.  This entry

 * point is only used to extend the current filesystem to the end of the last

 * existing group.

	/* main_bm_bh is validated by inode read inside ocfs2_inode_lock(),

 update the last group descriptor and inode. */

 Add a new group descriptor to global_bitmap. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * localalloc.c

 *

 * Node local data allocation

 *

 * Copyright (C) 2002, 2004 Oracle.  All rights reserved.

/*

 * ocfs2_la_default_mb() - determine a default size, in megabytes of

 * the local alloc.

 *

 * Generally, we'd like to pick as large a local alloc as

 * possible. Performance on large workloads tends to scale

 * proportionally to la size. In addition to that, the reservations

 * code functions more efficiently as it can reserve more windows for

 * write.

 *

 * Some things work against us when trying to choose a large local alloc:

 *

 * - We need to ensure our sizing is picked to leave enough space in

 *   group descriptors for other allocations (such as block groups,

 *   etc). Picking default sizes which are a multiple of 4 could help

 *   - block groups are allocated in 2mb and 4mb chunks.

 *

 * - Likewise, we don't want to starve other nodes of bits on small

 *   file systems. This can easily be taken care of by limiting our

 *   default to a reasonable size (256M) on larger cluster sizes.

 *

 * - Some file systems can't support very large sizes - 4k and 8k in

 *   particular are limited to less than 128 and 256 megabytes respectively.

 *

 * The following reference table shows group descriptor and local

 * alloc maximums at various cluster sizes (4k blocksize)

 *

 * csize: 4K	group: 126M	la: 121M

 * csize: 8K	group: 252M	la: 243M

 * csize: 16K	group: 504M	la: 486M

 * csize: 32K	group: 1008M	la: 972M

 * csize: 64K	group: 2016M	la: 1944M

 * csize: 128K	group: 4032M	la: 3888M

 * csize: 256K	group: 8064M	la: 7776M

 * csize: 512K	group: 16128M	la: 15552M

 * csize: 1024K	group: 32256M	la: 31104M

	/*

	 * This takes care of files systems with very small group

	 * descriptors - 512 byte blocksize at cluster sizes lower

	 * than 16K and also 1k blocksize with 4k cluster size.

	/*

	 * Leave enough room for some block groups and make the final

	 * value we work from a multiple of 4.

	/*

	 * Keep window sizes down to a reasonable default

		/*

		 * Some clustersize / blocksize combinations will have

		 * given us a larger than OCFS2_LA_MAX_DEFAULT_MB

		 * default size, but get poor distribution when

		 * limited to exactly 256 megabytes.

		 *

		 * As an example, 16K clustersize at 4K blocksize

		 * gives us a cluster group size of 504M. Paring the

		 * local alloc size down to 256 however, would give us

		 * only one window and around 200MB left in the

		 * cluster group. Instead, find the first size below

		 * 256 which would give us an even distribution.

		 *

		 * Larger cluster group sizes actually work out pretty

		 * well when pared to 256, so we don't have to do this

		 * for any group that fits more than two

		 * OCFS2_LA_MAX_DEFAULT_MB windows.

 Too many nodes, too few disk clusters. */

 We can't store more bits than we can in a block. */

 No user request - use defaults */

 Request is too big, we give the maximum available */

/*

 * Tell us whether a given allocation should use the local alloc

 * file. Otherwise, it has to go to the main bitmap.

 *

 * This function does semi-dirty reads of local alloc size and state!

 * This is ok however, as the values are re-checked once under mutex.

	/* la_bits should be at least twice the size (in clusters) of

	 * a new block group. We want to be sure block group

	 * allocations go through the local alloc, so allow an

 read the alloc off disk */

 do a little verification. */

	/* hopefully the local alloc has always been recovered before

/*

 * return any unused bits to the bitmap and write out a clean

 * local_alloc.

 *

 * local_alloc_bh is optional. If not passed, we will simply use the

 * one off osb. If you do pass it however, be warned that it *will* be

 WINDOW_MOVE_CREDITS is a bit heavy... */

/*

 * We want to free the bitmap bits outside of any recovery context as

 * we'll need a cluster lock to do so, but we must clear the local

 * alloc before giving up the recovered nodes journal. To solve this,

 * we kmalloc a copy of the local alloc before it's change for the

 * caller to process with ocfs2_complete_local_alloc_recovery

/*

 * Step 2: By now, we've completed the journal recovery, we've stamped

 * a clean local alloc on disk and dropped the node out of the

 * recovery map. Dlm locks will no longer stall, so lets clear out the

 * main bitmap.

 we want the bitmap change to be recorded on disk asap */

/*

 * make sure we've got at least bits_wanted contiguous bits in the

 * local alloc. You lose them when you drop i_mutex.

 *

 * We will add ourselves to the transaction passed in, but may start

 * our own in order to shift windows.

	/*

	 * We must double check state and allocator bits because

	 * another process may have changed them while holding i_mutex.

 uhoh, window change time. */

		/*

		 * Under certain conditions, the window slide code

		 * might have reduced the number of bits available or

		 * disabled the local alloc entirely. Re-check

		 * here and return -ENOSPC if necessary.

 We should never use localalloc from another slot */

 TODO: Shouldn't we just BUG here? */

	/*

	 * Code error. While reservations are enabled, local

	 * allocation should _always_ go through them.

	/*

	 * Reservations are disabled. Handle this the old way.

 mlog(0, "bitoff (%d) == left", bitoff); */

		/* mlog(0, "Found a zero: bitoff = %d, startoff = %d, "

		/* Ok, we found a zero bit... is it contig. or do we

 we found a zero */

 got a zero after some ones */

 we got everything we needed */

 mlog(0, "Found it all!\n"); */

 turn this on and uncomment below to aid debugging window shifts. */

/*

 * sync the local alloc to main bitmap.

 *

 * assumes you've already locked the main bitmap -- the bitmap inode

 * passed is used for caching.

 Normal window slide. */

	OCFS2_LA_EVENT_FRAGMENTED,	/* The global bitmap has

					 * enough bits theoretically

					 * free, but a contiguous

					 * allocation could not be

	OCFS2_LA_EVENT_ENOSPC,		/* Global bitmap doesn't have

					 * enough bits free to satisfy

/*

 * Given an event, calculate the size of our next local alloc window.

 *

 * This should always be called under i_mutex of the local alloc inode

 * so that local alloc disabling doesn't race with processes trying to

 * use the allocator.

 *

 * Returns the state which the local alloc was left in. This value can

 * be ignored by some paths.

	/*

	 * ENOSPC and fragmentation are treated similarly for now.

		/*

		 * We ran out of contiguous space in the primary

		 * bitmap. Drastically reduce the number of bits used

		 * by local alloc until we have to disable it.

			/*

			 * By setting state to THROTTLED, we'll keep

			 * the number of local alloc bits used down

			 * until an event occurs which would give us

			 * reason to assume the bitmap situation might

			 * have changed.

	/*

	 * Don't increase the size of the local alloc window until we

	 * know we might be able to fulfill the request. Otherwise, we

	 * risk bouncing around the global bitmap during periods of

	 * low space.

/*

 * pass it the bitmap lock in lock_bh if you have it.

	/* Instruct the allocation code to try the most recently used

	 * cluster group. We'll re-record the group used this pass

	/* we used the generic suballoc reserve function, but we set

	 * everything up nicely, so there's no reason why we can't use

		/*

		 * Note: We could also try syncing the journal here to

		 * allow use of any free bits which the current

		 * transaction can't give us access to. --Mark

		/*

		 * We only shrunk the *minimum* number of in our

		 * request - it's entirely possible that the allocator

		 * might give us more than we asked for.

	/* just in case... In the future when we find space ourselves,

	 * we don't have to get all contiguous -- but we'll have to

	 * set all previously used bits in bitmap and update

/* Note that we do *NOT* lock the local alloc inode here as

 This will lock the main bitmap for us. */

	/* We want to clear the local alloc before doing anything

	 * else, so that if we error later during this operation,

	 * local alloc shutdown won't try to double free main bitmap

	 * bits. Make a copy so the sync function knows which bits to

 SPDX-License-Identifier: GPL-2.0-only

/*

 * move_extents.c

 *

 * Copyright (C) 2011 Oracle.  All rights reserved.

	/*

	 * after moving/defraging to new location, the extent is not going

	 * to be refcounted anymore.

	/*

	 * need I to append truncate log for old clusters?

/*

 * lock allocator, and reserve appropriate number of bits for

 * meta blocks.

/*

 * Using one journal handle to guarantee the data consistency in case

 * crash happens anywhere.

 *

 *  XXX: defrag can end up with finishing partial extent as requested,

 * due to not enough contiguous clusters can be found in allocator.

	/*

	 * should be using allocation reservation strategy there?

	 *

	 * if (context->data_ac)

	 *	context->data_ac->ac_resv = &OCFS2_I(inode)->ip_la_data_resv;

	/*

	 * Make sure ocfs2_reserve_cluster is called after

	 * __ocfs2_flush_truncate_log, otherwise, dead lock may happen.

	 *

	 * If ocfs2_reserve_cluster is called

	 * before __ocfs2_flush_truncate_log, dead lock on global bitmap

	 * may happen.

	 *

	/*

	 * allowing partial extent moving is kind of 'pros and cons', it makes

	 * whole defragmentation less likely to fail, on the contrary, the bad

	 * thing is it may make the fs even more fragmented after moving, let

	 * userspace make a good decision here.

	/*

	 * Here we should write the new page out first if we are

	 * in write-back mode.

/*

 * find the victim alloc group, where #blkno fits.

	/*

	 * 'vict_blkno' was out of the valid range.

	/*

	 * caller has to release the gd_bh properly.

/*

 * XXX: helper to validate and adjust moving goal.

	/*

	 * make goal become cluster aligned.

	/*

	 * validate goal sits within global_bitmap, and return the victim

	 * group desc

	/*

	 * moving goal is not allowd to start with a group desc blok(#0 blk)

	 * let's compromise to the latter cluster.

	/*

	 * movement is not gonna cross two groups.

	/*

	 * more exact validations/adjustments will be performed later during

	 * moving operation for each extent range.

			/*

			 * we even tried searching the free chunk by jumping

			 * a 'max_hop' distance, but still failed.

	/*

	 * need to count 2 extra credits for global_bitmap inode and

	 * group descriptor.

	/*

	 * ocfs2_move_extent() didn't reserve any clusters in lock_allocators()

	 * logic, while we still need to lock the global_bitmap.

	/*

	 * probe the victim cluster group to find a proper

	 * region to fit wanted movement, it even will perfrom

	 * a best-effort attempt by compromising to a threshold

	 * around the goal.

	/*

	 * Here we should write the new page out first if we are

	 * in write-back mode.

/*

 * Helper to calculate the defraging length in one run according to threshold.

		/*

		 * proceed defragmentation until we meet the thresh

		/*

		 * XXX: skip a large extent.

		/*

		 * split this extent to coalesce with former pieces as

		 * to reach the threshold.

		 *

		 * we're done here with one cycle of defragmentation

		 * in a size of 'thresh', resetting 'len_defraged'

		 * forces a new defragmentation.

	/*

	 * TO-DO XXX:

	 *

	 * - xattr extents.

	/*

	 * extents moving happens in unit of clusters, for the sake

	 * of simplicity, we may ignore two clusters where 'byte_start'

	 * and 'byte_start + len' were within.

		/*

		 * XXX: how to deal with a hole:

		 *

		 * - skip the hole of course

		 * - force a new defragmentation

			/*

			 * skip large extents

	/*

	 * This prevents concurrent writes from other nodes

	/*

	 * rememer ip_xattr_sem also needs to be held if necessary

	/*

	 * We update ctime for these changes

		/*

		 * ok, the default theshold for the defragmentation

		 * is 1M, since our maximum clustersize was 1M also.

		 * any thought?

		/*

		 * first best-effort attempt to validate and adjust the goal

		 * (physical address in block), while it can't guarantee later

		 * operation can succeed all the time since global_bitmap may

		 * change a bit over time.

	/*

	 * movement/defragmentation may end up being partially completed,

	 * that's the reason why we need to return userspace the finished

	 * length and new_offset even if failure happens somewhere.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * file.c

 *

 * File open, close, extend, truncate

 *

 * Copyright (C) 2002, 2004 Oracle.  All rights reserved.

	/* Check that the inode hasn't been wiped from disk by another

	 * node. If it hasn't then we're safe as long as we hold the

		/*

		 * We want to set open count back if we're failing the

		 * open.

	/*

	 * We can be called with no vfsmnt structure - NFSD will

	 * sometimes do this.

	 *

	 * Note that our action here is different than touch_atime() -

	 * if we can't tell whether this is a noatime mount, then we

	 * don't know whether to trust the value of s_atime_quantum.

	/*

	 * Don't use ocfs2_mark_inode_dirty() here as we don't always

	 * have i_mutex to guard against concurrent changes to other

	 * inode fields.

	/*

	 * If the new offset is aligned to the range of the cluster, there is

	 * no space for ocfs2_zero_range_for_truncate to fill, so no need to

	 * CoW either.

	/*

	 * We need to CoW the cluster contains the offset if it is reflinked

	 * since we will call ocfs2_zero_range_for_truncate later which will

	 * write "0" from offset to the end of the cluster.

	/* TODO: This needs to actually orphan the inode in this

	/*

	 * Do this before setting i_size.

	/* We trust di_bh because it comes from ocfs2_inode_lock(), which

	/*

	 * The inode lock forced other nodes to sync and drop their

	 * pages, which (correctly) happens even if we have a truncate

	 * without allocation change - ocfs2 cluster sizes can be much

	 * greater than page size, so we have to truncate them

	 * anyway.

	/* alright, we're going to need to do a full blown alloc size

	 * change. Orphan the inode so that recovery can complete the

	 * truncate if necessary. This does the task of marking

 TODO: orphan dir cleanup here. */

/*

 * extend file allocation only here.

 * we'll update all the disk stuff, and oip->alloc_size

 *

 * expect stuff to be locked, a transaction started and enough data /

 * metadata reservations in the contexts.

 *

 * Will return -EAGAIN, and a reason if a restart is needed.

 * If passed in, *reason will always be set, even in error.

	/*

	 * Unwritten extent only exists for file systems which

	 * support holes.

	/* reserve a write to the file entry early on - that we if we

	 * run out of credits in the allocation path, we can still

 Release unused quota reservation */

				/* handle still has to be committed at

/*

 * While a write will already be ordering the data, a truncate will not.

 * Thus, we need to explicitly order the zeroed pages.

/* Some parts of this taken from generic_cont_expand, which turned out

 * to be too fragile to do exactly what we need without us having to

 Get the offsets within the page that we want to zero */

 We know that zero_from is block aligned */

		/*

		 * block_start is block-aligned.  Bump it by one to force

		 * __block_write_begin and block_commit_write to zero the

		 * whole block.

 must not update i_size! */

	/*

	 * fs-writeback will release the dirty pages without page lock

	 * whose offset are over inode size, the release happens at

	 * block_write_full_page().

/*

 * Find the next range to zero.  We do this in terms of bytes because

 * that's what ocfs2_zero_extend() wants, and it is dealing with the

 * pagecache.  We may return multiple extents.

 *

 * zero_start and zero_end are ocfs2_zero_extend()s current idea of what

 * needs to be zeroed.  range_start and range_end return the next zeroing

 * range.  A subsequent call should pass the previous range_end as its

 * zero_start.  If range_end is 0, there's nothing to do.

 *

 * Unwritten extents are skipped over.  Refcounted extents are CoWd.

/*

 * Zero one range returned from ocfs2_zero_extend_get_range().  The caller

 * has made sure that the entire range needs zeroing.

		/*

		 * Very large extends have the potential to lock up

		 * the cpu for extended periods of time.

 Trim the ends */

	/*

	 * Only quota files call this without a bh, and they can't be

	 * refcounted.

	/*

	 * Call this even if we don't add any clusters to the tree. We

	 * still need to zero the area between the old i_size and the

	 * new i_size.

 setattr sometimes calls us like this. */

	/*

	 * The alloc sem blocks people in read/write from reading our

	 * allocation until we're done changing it. We depend on

	 * i_mutex to block other extend/truncate calls while we're

	 * here.  We even have to hold it for sparse files because there

	 * might be some tail zeroing.

		/*

		 * We can optimize small extends by keeping the inodes

		 * inline data.

 ensuring we don't even attempt to truncate a symlink */

		/*

		 * Here we should wait dio to finish before inode lock

		 * to avoid a deadlock between ocfs2_setattr() and

		 * ocfs2_dio_end_io_write()

		/*

		 * As far as we know, ocfs2_setattr() could only be the first

		 * VFS entry point in the call chain of recursive cluster

		 * locking issue.

		 *

		 * For instance:

		 * chmod_common()

		 *  notify_change()

		 *   ocfs2_setattr()

		 *    posix_acl_chmod()

		 *     ocfs2_iop_get_acl()

		 *

		 * But, we're not 100% sure if it's always true, because the

		 * ordering of the VFS entry points in the call chain is out

		 * of our control. So, we'd better dump the stack here to

		 * catch the other cases of recursive locking.

		/*

		 * Gather pointers to quota structures so that allocation /

		 * freeing of quota structures happens here and not inside

		 * dquot_transfer() where we have problems with lock ordering

 Release quota pointers in case we acquired them */

	/*

	 * If there is inline data in the inode, the inode will normally not

	 * have data blocks allocated (it may have an external xattr block).

	 * Report at least one sector for such files, so tools like tar, rsync,

	 * others don't incorrectly think the file is completely sparse.

 We set the blksize from the cluster size for performance */

		/* See comments in ocfs2_setattr() for details.

		 * The call chain of this case could be:

		 * do_sys_open()

		 *  may_open()

		 *   inode_permission()

		 *    ocfs2_permission()

		 *     ocfs2_iop_get_acl()

/*

 * Allocate enough extents to cover the region starting at byte offset

 * start for len bytes. Existing extents are skipped, any extents

 * added are marked as "unwritten".

		/*

		 * Nothing to do if the requested reservation range

		 * fits within the inode.

	/*

	 * We consider both start and len to be inclusive.

		/*

		 * Hole or existing extent len can be arbitrary, so

		 * cap it to our own allocation request.

			/*

			 * We already have an allocation at this

			 * region so we can safely skip it.

/*

 * Truncate a byte range, avoiding pages within partial clusters. This

 * preserves those pages for the zeroing code to write to.

/*

 * zero out partial blocks of one cluster.

 *

 * start: file offset where zero starts, will be made upper block aligned.

 * len: it will be trimmed to the end of current cluster if "start + len"

 *      is bigger than it.

	/*

	 * The "start" and "end" values are NOT necessarily part of

	 * the range whose allocation is being deleted. Rather, this

	 * is what the user passed in with the request. We must zero

	 * partial clusters here. There's no need to worry about

	 * physical allocation - the zeroing code knows to skip holes.

	/*

	 * If both edges are on a cluster boundary then there's no

	 * zeroing required as the region is part of the allocation to

	 * be truncated.

 No page cache for EOF blocks, issue zero out to disk. */

		/*

		 * zeroout eof blocks in last cluster starting from

		 * "isize" even "start" > "isize" because it is

		 * complicated to zeroout just at "start" as "start"

		 * may be not aligned with block size, buffer write

		 * would be required to do that, but out of eof buffer

		 * write is not supported.

	/*

	 * If start is on a cluster boundary and end is somewhere in another

	 * cluster, we have not COWed the cluster starting at start, unless

	 * end is also within the same cluster. So, in this case, we skip this

	 * first call to ocfs2_zero_range_for_truncate() truncate and move on

	 * to the next one.

		/*

		 * We want to get the byte offset of the end of the 1st

		 * cluster.

		/*

		 * This may make start and end equal, but the zeroing

		 * code will skip any work in that case so there's no

		 * need to catch it up here.

/*

 * Helper to calculate the punching pos and length in one run, we handle the

 * following three cases in order:

 *

 * - remove the entire record

 * - remove a partial record

 * - no record needs to be removed (hole-punching completed)

		/*

		 * remove an entire extent record.

		/*

		 * Skip holes if any.

		/*

		 * remove a partial extent record, which means we're

		 * removing the last extent record.

		/*

		 * skip hole if any.

		/*

		 * It may have two following possibilities:

		 *

		 * - last record has been removed

		 * - trunc_start was within a hole

		 *

		 * both two cases mean the completion of hole punching.

		/*

		 * There's no need to get fancy with the page cache

		 * truncate of an inline-data inode. We're talking

		 * about less than a page here, which will be cached

		 * in the dinode buffer anyway.

	/*

	 * For reflinks, we may need to CoW 2 clusters which might be

	 * partially zero'd later, if hole's start and end offset were

	 * within one cluster(means is not exactly aligned to clustersize).

		/*

		 * Need to go to previous extent block.

			/*

			 * We've reached the leftmost extent block,

			 * it's safe to leave.

			/*

			 * The 'pos' searched for previous extent block is

			 * always one cluster less than actual trunc_end.

/*

 * Parts of this function taken from xfs_change_file_space()

	/*

	 * This prevents concurrent writes on other nodes

SEEK_SET*/

SEEK_CUR*/

SEEK_END*/

		/*

		 * This takes unsigned offsets, but the signed ones we

		 * pass have been checked against overflow above.

 zeroout eof blocks in the cluster. */

	/*

	 * We update c/mtime for these changes

	/*

	 * We start with a read level meta lock and only jump to an ex

	 * if we need to make modifications here.

		/*

		 * Check if IO will overwrite allocated blocks in case

		 * IOCB_NOWAIT flag is set.

		/* Clear suid / sgid if necessary. We do this here

		 * instead of later in the write path because

		 * remove_suid() calls ->setattr without any hint that

		 * we may have already done our cluster locking. Since

		 * ocfs2_setattr() *must* take cluster locks to

		 * proceed, this will lead us to recursively lock the

		 * inode. There's also the dinode i_size state which

		 * can be lost via setattr during extending writes (we

 GRRRRR */

	/*

	 * Concurrent O_DIRECT writes are allowed with

	 * mount_option "coherency=buffered".

	 * For append write, we must take rw EX.

	/*

	 * O_DIRECT writes with "coherency=full" need to take EX cluster

	 * inode_lock to guarantee coherency.

		/*

		 * We need to take and drop the inode lock to force

		 * other nodes to drop their caches.  Buffered I/O

		 * already does this in write_begin().

		/*

		 * Make it a sync io if it's an unaligned aio.

 communicate with ocfs2_dio_end_io */

 buffered aio wouldn't have proper lock coverage today */

	/*

	 * deep in g_f_a_w_n()->ocfs2_direct_IO we pass in a ocfs2_dio_end_io

	 * function pointer which is called when o_direct io completes so that

	 * it can unlock our rw lock.

	 * Unfortunately there are error cases which call end_io and others

	 * that don't.  so we don't have to unlock the rw_lock if either an

	 * async dio is going to do it in the future or an end_io after an

	 * error has already done it.

 GRRRRR */

	/*

	 * buffered reads protect themselves in ->readpage().  O_DIRECT reads

	 * need locks to protect pending reads from racing with truncate.

 communicate with ocfs2_dio_end_io */

	/*

	 * We're fine letting folks race truncates and extending

	 * writes with read across the cluster, just like they can

	 * locally. Hence no rw_lock during read.

	 *

	 * Take and drop the meta data lock to update inode fields

	 * like i_size. This allows the checks down below

	 * generic_file_read_iter() a chance of actually working.

 buffered aio wouldn't have proper lock coverage today */

 see ocfs2_file_write_iter */

 Refer generic_file_llseek_unlocked() */

		/* SEEK_END requires the OCFS2 inode lock for the file

		 * because it references the file's size.

 Lock both files against IO */

 Check file eligibility and prepare for block sharing. */

 Lock out changes to the allocation maps and remap. */

 Zap any page cache for the destination file's range. */

	/*

	 * Empty the extent map so that we may get the right extent

	 * record from the disk.

/*

 * Other than ->lock, keep ocfs2_fops and ocfs2_dops in sync with

 * ocfs2_fops_no_plocks and ocfs2_dops_no_plocks!

/*

 * POSIX-lockless variants of our file_operations.

 *

 * These will be used if the underlying cluster stack does not support

 * posix file locking, if the user passes the "localflocks" mount

 * option, or if we have a local-only fs.

 *

 * ocfs2_flock is in here because all stacks handle UNIX file locks,

 * so we still want it in the case of no stack support for

 * plocks. Internally, it will do the right thing when asked to ignore

 * the cluster.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * blockcheck.c

 *

 * Checksum and ECC codes for the OCFS2 userspace library.

 *

 * Copyright (C) 2006, 2008 Oracle.  All rights reserved.

/*

 * We use the following conventions:

 *

 * d = # data bits

 * p = # parity bits

 * c = # total code bits (d + p)

/*

 * Calculate the bit offset in the hamming code buffer based on the bit's

 * offset in the data buffer.  Since the hamming code reserves all

 * power-of-two bits for parity, the data bit number and the code bit

 * number are offset by all the parity bits beforehand.

 *

 * Recall that bit numbers in hamming code are 1-based.  This function

 * takes the 0-based data bit from the caller.

 *

 * An example.  Take bit 1 of the data buffer.  1 is a power of two (2^0),

 * so it's a parity bit.  2 is a power of two (2^1), so it's a parity bit.

 * 3 is not a power of two.  So bit 1 of the data buffer ends up as bit 3

 * in the code buffer.

 *

 * The caller can pass in *p if it wants to keep track of the most recent

 * number of parity bits added.  This allows the function to start the

 * calculation at the last place.

	/*

	 * Data bits are 0-based, but we're talking code bits, which

	 * are 1-based.

 Use the cache if it is there */

	/*

	 * For every power of two below our bit number, bump our bit.

	 *

	 * We compare with (b + 1) because we have to compare with what b

	 * would be _if_ it were bumped up by the parity bit.  Capice?

	 *

	 * p is set above.

/*

 * This is the low level encoder function.  It can be called across

 * multiple hunks just like the crc32 code.  'd' is the number of bits

 * _in_this_hunk_.  nr is the bit offset of this hunk.  So, if you had

 * two 512B buffers, you would do it like so:

 *

 * parity = ocfs2_hamming_encode(0, buf1, 512 * 8, 0);

 * parity = ocfs2_hamming_encode(parity, buf2, 512 * 8, 512 * 8);

 *

 * If you just have one buffer, use ocfs2_hamming_encode_block().

	/*

	 * b is the hamming code bit number.  Hamming code specifies a

	 * 1-based array, but C uses 0-based.  So 'i' is for C, and 'b' is

	 * for the algorithm.

	 *

	 * The i++ in the for loop is so that the start offset passed

	 * to ocfs2_find_next_bit_set() is one greater than the previously

	 * found bit.

		/*

		 * i is the offset in this hunk, nr + i is the total bit

		 * offset.

		/*

		 * Data bits in the resultant code are checked by

		 * parity bits that are part of the bit number

		 * representation.  Huh?

		 *

		 * <wikipedia href="https://en.wikipedia.org/wiki/Hamming_code">

		 * In other words, the parity bit at position 2^k

		 * checks bits in positions having bit k set in

		 * their binary representation.  Conversely, for

		 * instance, bit 13, i.e. 1101(2), is checked by

		 * bits 1000(2) = 8, 0100(2)=4 and 0001(2) = 1.

		 * </wikipedia>

		 *

		 * Note that 'k' is the _code_ bit number.  'b' in

		 * our loop.

	/* While the data buffer was treated as little endian, the

/*

 * Like ocfs2_hamming_encode(), this can handle hunks.  nr is the bit

 * offset of the current hunk.  If bit to be fixed is not part of the

 * current hunk, this does nothing.

 *

 * If you only have one hunk, use ocfs2_hamming_fix_block().

	/*

	 * If the bit to fix has an hweight of 1, it's a parity bit.  One

	 * busted parity bit is its own error.  Nothing to do here.

	/*

	 * nr + d is the bit right past the data hunk we're looking at.

	 * If fix after that, nothing to do

	/*

	 * nr is the offset in the data hunk we're starting at.  Let's

	 * start b at the offset in the code buffer.  See hamming_encode()

	 * for a more detailed description of 'b'.

 If the fix is before this hunk, nothing to do */

 Skip past parity bits */

		/*

		 * i is the offset in this data hunk.

		 * nr + i is the offset in the total data buffer.

		 * b is the offset in the total code buffer.

		 *

		 * Thus, when b == fix, bit i in the current hunk needs

		 * fixing.

/*

 * Debugfs handling.

 CONFIG_DEBUG_FS */

 Always-called wrappers for starting and stopping the debugfs files */

/*

 * These are the low-level APIs for using the ocfs2_block_check structure.

/*

 * This function generates check information for a block.

 * data is the block to be checked.  bc is a pointer to the

 * ocfs2_block_check structure describing the crc32 and the ecc.

 *

 * bc should be a pointer inside data, as the function will

 * take care of zeroing it before calculating the check information.  If

 * bc does not point inside data, the caller must make sure any inline

 * ocfs2_block_check structures are zeroed.

 *

 * The data buffer must be in on-disk endian (little endian for ocfs2).

 * bc will be filled with little-endian values and will be ready to go to

 * disk.

	/*

	 * No ecc'd ocfs2 structure is larger than 4K, so ecc will be no

	 * larger than 16 bits.

/*

 * This function validates existing check information.  Like _compute,

 * the function will take care of zeroing bc before calculating check codes.

 * If bc is not a pointer inside data, the caller must have zeroed any

 * inline ocfs2_block_check structures.

 *

 * Again, the data passed in should be the on-disk endian.

 Fast path - if the crc32 validates, we're good to go */

 Ok, try ECC fixups */

 And check the crc32 again */

/*

 * This function generates check information for a list of buffer_heads.

 * bhs is the blocks to be checked.  bc is a pointer to the

 * ocfs2_block_check structure describing the crc32 and the ecc.

 *

 * bc should be a pointer inside data, as the function will

 * take care of zeroing it before calculating the check information.  If

 * bc does not point inside data, the caller must make sure any inline

 * ocfs2_block_check structures are zeroed.

 *

 * The data buffer must be in on-disk endian (little endian for ocfs2).

 * bc will be filled with little-endian values and will be ready to go to

 * disk.

		/*

		 * The number of bits in a buffer is obviously b_size*8.

		 * The offset of this buffer is b_size*i, so the bit offset

		 * of this buffer is b_size*8*i.

	/*

	 * No ecc'd ocfs2 structure is larger than 4K, so ecc will be no

	 * larger than 16 bits.

/*

 * This function validates existing check information on a list of

 * buffer_heads.  Like _compute_bhs, the function will take care of

 * zeroing bc before calculating check codes.  If bc is not a pointer

 * inside data, the caller must have zeroed any inline

 * ocfs2_block_check structures.

 *

 * Again, the data passed in should be the on-disk endian.

 Fast path - if the crc32 validates, we're good to go */

 Ok, try ECC fixups */

		/*

		 * The number of bits in a buffer is obviously b_size*8.

		 * The offset of this buffer is b_size*i, so the bit offset

		 * of this buffer is b_size*8*i.

		/*

		 * Try the fix against each buffer.  It will only affect

		 * one of them.

 And check the crc32 again */

/*

 * These are the main API.  They check the superblock flag before

 * calling the underlying operations.

 *

 * They expect the buffer(s) to be in disk format.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * stackglue.c

 *

 * Code which implements an OCFS2 specific interface to underlying

 * cluster stacks.

 *

 * Copyright (C) 2007, 2009 Oracle.  All rights reserved.

/*

 * The stack currently in use.  If not null, active_stack->sp_count > 0,

 * the module is pinned, and the locking protocol cannot be changed.

	/*

	 * If the stack passed by the filesystem isn't the selected one,

	 * we can't continue.

		/*

		 * If the active stack isn't the one we want, it cannot

		 * be selected right now.

 If we found it, pin it */

/*

 * This function looks up the appropriate stack and makes it active.  If

 * there is no stack, it tries to load it.  It will fail if the stack still

 * cannot be found.  It will also fail if a different stack is in use.

	/*

	 * Classic stack does not pass in a stack name.  This is

	 * compatible with older tools as well.

 Anything that isn't the classic stack is a user stack */

/*

 * The ocfs2_dlm_lock() and ocfs2_dlm_unlock() functions take no argument

 * for the ast and bast functions.  They will pass the lksb to the ast

 * and bast.  The caller can wrap the lksb with their own structure to

 * get more information.

/*

 * ocfs2_plock() can only be safely called if

 * ocfs2_stack_supports_plocks() returned true

 Start the new connection at our maximum compatibility level */

 This will pin the stack driver if successful */

 The caller will ensure all nodes have the same cluster stack */

 If hangup_pending is 0, the stack driver will be dropped */

 XXX Should we free it anyway? */

/*

 * Leave the group for this filesystem.  This is executed by a userspace

 * program (stored in ocfs2_hb_ctl_path).

 minimal command environment taken from cpu_run_sbin_hotplug */

/*

 * Hangup is a required post-umount.  ocfs2-tools software expects the

 * filesystem to call "ocfs2_hb_ctl" during unmount.  This happens

 * regardless of whether the DLM got started, so we can't do it

 * in ocfs2_cluster_disconnect().  The ocfs2_leave_group() function does

 * the actual work.

 cluster_disconnect() was called with hangup_pending==1 */

/*

 * Sysfs bits

 snprintf() didn't fit */

/*

 * Sysctl bits

 *

 * The sysctl lives at /proc/sys/fs/ocfs2/nm/hb_ctl_path.  The 'nm' doesn't

 * make as much sense in a multiple cluster stack world, but it's safer

 * and easier to preserve the name.

/*

 * Initialization

 or something. */

/*

 *  linux/cluster/ssi/cfs/symlink.c

 *

 *	This program is free software; you can redistribute it and/or

 *	modify it under the terms of the GNU General Public License as

 *	published by the Free Software Foundation; either version 2 of

 *	the License, or (at your option) any later version.

 *

 *	This program is distributed in the hope that it will be useful,

 *	but WITHOUT ANY WARRANTY; without even the implied warranty of

 *	MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE

 *	or NON INFRINGEMENT.  See the GNU General Public License for more

 *	details.

 *

 * 	You should have received a copy of the GNU General Public License

 * 	along with this program; if not, write to the Free Software

 * 	Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.

 *

 *	Questions/Comments/Bugfixes to ssic-linux-devel@lists.sourceforge.net

 *

 *  Copyright (C) 1992  Rick Sladkey

 *

 *  Optimization changes Copyright (C) 1994 Florian La Roche

 *

 *  Jun 7 1999, cache symlink lookups in the page cache.  -DaveM

 *

 *  Portions Copyright (C) 2001 Compaq Computer Corporation

 *

 *  ocfs2 symlink handling code.

 *

 *  Copyright (C) 2004, 2005 Oracle.

 *

 will be less than a page size */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * sysfile.c

 *

 * Initialize, read, write, etc. system files.

 *

 * Copyright (C) 2002, 2004 Oracle.  All rights reserved.

			/*

			 * return NULL here so that ocfs2_get_sytem_file_inodes

			 * will try to create an inode and use it. We will try

			 * to initialize local_system_inodes next time.

 Someone has initialized it for us. */

 avoid the lookup if cached in local system file array */

 get a ref in addition to the array ref */

 this gets one ref thru iget */

 add one more if putting into array for first time */

		/* Ignore inode lock on these inodes as the lock does not

		 * really belong to any process and lockdep cannot handle

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * io.c

 *

 * Buffer cache handling

 *

 * Copyright (C) 2002, 2004 Oracle.  All rights reserved.

/*

 * Bits on bh->b_state used by ocfs2.

 *

 * These MUST be after the JBD2 bits.  Hence, we use BH_JBDPrivateStart.

 Expand the magic b_state functions */

	/* No need to check for a soft readonly file system here. non

	 * journalled writes are only ever done on system files which

 remove from dirty list before I/O. */

 for end_buffer_write_sync() */

		/* We don't need to remove the clustered uptodate

		 * information for this bh as it's not marked locally

/* Caller must provide a bhs[] with all NULL or non-NULL entries, so it

 * will be easier to handle read failure.

	/* Don't put buffer head and re-assign it to NULL if it is allocated

	 * outside since the caller can't be aware of this alternation!

			/* This should probably be a BUG, or

 for end_buffer_read_sync() */

				/* If middle bh fails, let previous bh

				 * finish its read and then put it to

				 * aovoid bh leak

 No need to wait on the buffer if it's managed by JBD. */

			/* Status won't be cleared from here on out,

			 * so we can safely record this and loop back

/* Caller must provide a bhs[] with all NULL or non-NULL entries, so it

 * will be easier to handle read failure.

	/* Don't put buffer head and re-assign it to NULL if it is allocated

	 * outside since the caller can't be aware of this alternation!

 Don't forget to put previous bh! */

		/* There are three read-ahead cases here which we need to

		 * be concerned with. All three assume a buffer has

		 * previously been submitted with OCFS2_BH_READAHEAD

		 * and it hasn't yet completed I/O.

		 *

		 * 1) The current request is sync to disk. This rarely

		 *    happens these days, and never when performance

		 *    matters - the code can just wait on the buffer

		 *    lock and re-submit.

		 *

		 * 2) The current request is cached, but not

		 *    readahead. ocfs2_buffer_uptodate() will return

		 *    false anyway, so we'll wind up waiting on the

		 *    buffer lock to do I/O. We re-check the request

		 *    with after getting the lock to avoid a re-submit.

		 *

		 * 3) The current request is readahead (and so must

		 *    also be a caching one). We short circuit if the

		 *    buffer is locked (under I/O) and if it's in the

		 *    uptodate cache. The re-check from #2 catches the

		 *    case that the previous read-ahead completes just

		 *    before our is-it-in-flight check.

			/* We're using ignore_cache here to say

				/* This should probably be a BUG, or

			/* A read-ahead request was made - if the

			 * buffer is already under read-ahead from a

			 * previously submitted request than we are

			/* Re-check ocfs2_buffer_uptodate() as a

			 * previously read-ahead buffer may have

			 * completed I/O while we were waiting for the

 for end_buffer_read_sync() */

				/* Clear the buffers on error including those

				 * ever succeeded in reading

					/* If middle bh fails, let previous bh

					 * finish its read and then put it to

					 * aovoid bh leak

			/* We know this can't have changed as we hold the

			 * owner sem. Avoid doing any work on the bh if the

				/* Status won't be cleared from here on out,

				 * so we can safely record this and loop back

				 * to cleanup the other buffers. Don't need to

				 * remove the clustered uptodate information

				 * for this bh as it's not marked locally

				/* We never set NeedsValidate if the

				 * buffer was held by the journal, so

		/* Always set the buffer in the cache, even if it was

		 * a forced read, or read-ahead which hasn't yet

 Check whether the blkno is the super block or one of the backups. */

/*

 * Write super block and backups doesn't need to collaborate with journal,

 * so we don't need to lock ip_io_mutex and ci doesn't need to bea passed

 * into this function.

 remove from dirty list before I/O. */

 for end_buffer_write_sync() */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * locks.c

 *

 * Userspace file locking support

 *

 * Copyright (C) 2007 Oracle.  All rights reserved.

		/*

		 * Converting an existing lock is not guaranteed to be

		 * atomic, so we can get away with simply unlocking

		 * here and allowing the lock code to try at the new

		 * level.

/*

 * Overall flow of ocfs2_flock() was influenced by gfs2_flock().

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * export.c

 *

 * Functions to facilitate NFS exporting

 *

 * Copyright (C) 2002, 2005 Oracle.  All rights reserved.

	/*

	 * If the inode exists in memory, we only need to check it's

	 * generation number

	/*

	 * This will synchronize us against ocfs2_delete_inode() on

	 * all nodes

			/*

			 * The blkno NFS gave us doesn't even show up

			 * as an inode, we return -ESTALE to be

			 * nice

 If the inode allocator bit is clear, this inode must be stale */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * stack_user.c

 *

 * Code which interfaces ocfs2 with fs/dlm and a userspace stack.

 *

 * Copyright (C) 2007 Oracle.  All rights reserved.

/*

 * The control protocol starts with a handshake.  Until the handshake

 * is complete, the control device will fail all write(2)s.

 *

 * The handshake is simple.  First, the client reads until EOF.  Each line

 * of output is a supported protocol tag.  All protocol tags are a single

 * character followed by a two hex digit version number.  Currently the

 * only things supported is T01, for "Text-base version 0x01".  Next, the

 * client writes the version they would like to use, including the newline.

 * Thus, the protocol tag is 'T01\n'.  If the version tag written is

 * unknown, -EINVAL is returned.  Once the negotiation is complete, the

 * client can start sending messages.

 *

 * The T01 protocol has three messages.  First is the "SETN" message.

 * It has the following syntax:

 *

 *  SETN<space><8-char-hex-nodenum><newline>

 *

 * This is 14 characters.

 *

 * The "SETN" message must be the first message following the protocol.

 * It tells ocfs2_control the local node number.

 *

 * Next comes the "SETV" message.  It has the following syntax:

 *

 *  SETV<space><2-char-hex-major><space><2-char-hex-minor><newline>

 *

 * This is 11 characters.

 *

 * The "SETV" message sets the filesystem locking protocol version as

 * negotiated by the client.  The client negotiates based on the maximum

 * version advertised in /sys/fs/ocfs2/max_locking_protocol.  The major

 * number from the "SETV" message must match

 * ocfs2_user_plugin.sp_max_proto.pv_major, and the minor number

 * must be less than or equal to ...sp_max_version.pv_minor.

 *

 * Once this information has been set, mounts will be allowed.  From this

 * point on, the "DOWN" message can be sent for node down notification.

 * It has the following syntax:

 *

 *  DOWN<space><32-char-cap-hex-uuid><space><8-char-hex-nodenum><newline>

 *

 * eg:

 *

 *  DOWN 632A924FDD844190BDA93C0DF6B94899 00000001\n

 *

 * This is 47 characters.

/*

 * Whether or not the client has done the handshake.

 * For now, we have just one protocol version.

 Handshake states */

 Messages */

/*

 * ocfs2_live_connection is refcounted because the filesystem and

 * miscdevice sides can detach in different order.  Let's just be safe.

 SETN<space><8-char-hex-nodenum><newline> */

 SETV<space><2-char-hex-major><space><2-char-hex-minor><newline> */

 DOWN<space><32-char-cap-hex-uuid><space><8-char-hex-nodenum><newline> */

/*

 * ocfs2_live_connection structures are created underneath the ocfs2

 * mount path.  Since the VFS prevents multiple calls to

 * fill_super(), we can't get dupes here.

/*

 * This function disconnects the cluster connection from ocfs2_control.

 * Afterwards, userspace can't affect the cluster connection.

 The T01 expects write(2) calls to have exactly one command */

/*

 * Called whenever configuration elements are sent to /dev/ocfs2_control.

 * If all configuration elements are present, try to set the global

 * values.  If there is a problem, return an error.  Skip any missing

 * elements, and only bump ocfs2_control_opened when we have all elements

 * and are successful.

 We set the global values successfully */

	/*

	 * The major must be between 1 and 255, inclusive.  The minor

	 * must be between 0 and 255, inclusive.  The version passed in

	 * must be within the maximum version supported by the filesystem.

 Try to catch padding issues */

/*

 * This is a naive version.  If we ever have a new protocol, we'll expand

 * it.  Probably using seq_file.

 Have we read the whole protocol list? */

 XXX: Do bad things! */

		/*

		 * Last valid close clears the node number and resets

		 * the locking protocol version

	/*

	 * For now we're punting on the issue of other non-standard errors

	 * where we can't tell if the unlock_ast or lock_ast should be called.

	 * The main "other error" that's possible is EINVAL which means the

	 * function was called with invalid args, which shouldn't be possible

	 * since the caller here is under our control.  Other non-standard

	 * errors probably fall into the same category, or otherwise are fatal

	 * which means we can't carry on anyway.

	/*

	 * This more or less just demuxes the plock request into any

	 * one of three dlm calls.

	 *

	 * Internally, fs/dlm will pass these to a misc device, which

	 * a userspace daemon will read and write to.

	 *

	 * For now, cancel requests (which happen internally only),

	 * are turned into unlocks. Most of this function taken from

	 * gfs2_lock.

/*

 * Compare a requested locking protocol version against the current one.

 *

 * If the major numbers are different, they are incompatible.

 * If the current minor is greater than the request, they are incompatible.

 * If the current minor is less than or equal to the request, they are

 * compatible, and the requester should run at the current minor version.

	/*

	 * ocfs2_protocol_version has two u8 variables, so we don't

	 * need any endian conversion.

	/*

	 * ocfs2_protocol_version has two u8 variables, so we don't

	 * need any endian conversion.

/* get_protocol_version()

 *

 * To exchange ocfs2 versioning, we use the LVB of the version dlm lock.

 * The algorithm is:

 * 1. Attempt to take the lock in EX mode (non-blocking).

 * 2. If successful (which means it is the first mount), write the

 *    version number and downconvert to PR lock.

 * 3. If unsuccessful (returns -EAGAIN), read the version from the LVB after

 *    taking the PR lock.

	/*

	 * running_proto must have been set before we allowed any mounts

	 * to proceed.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * heartbeat.c

 *

 * Register ourselves with the heartbaet service, keep our node maps

 * up to date, and fire off recovery when needed.

 *

 * Copyright (C) 2002, 2004 Oracle.  All rights reserved.

/* special case -1 for now

		/*

		 * No cluster connection means we're not even ready to

		 * participate yet.  We check the slots after the cluster

		 * comes up, so we will notice the node death then.  We

		 * can safely ignore it here.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * filecheck.c

 *

 * Code which implements online file check.

 *

 * Copyright (C) 2016 SuSE.  All rights reserved.

/* File check error strings,

 * must correspond with error number in header file.

 To free a undone file check entry */

 too short/long args length */

 snprintf() didn't fit */

			/* Delete the oldest entry which was done,

			 * make sure the entry size in list does

			 * not exceed maximum value

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2002, 2004 Oracle.  All rights reserved.

	/* We don't use the page cache to create symlink data, so if

		/* we haven't locked out transactions, so a commit

		 * could've happened. Since we've got a reference on

		 * the bh, even if it commits while we're doing the

 this always does I/O for some reason. */

	/*

	 * ocfs2 never allocates in this function - the only time we

	 * need to use BH_New is when we're extending i_size on a file

	 * system which doesn't support holes, in which case BH_New

	 * allows __block_write_begin() to zero.

	 *

	 * If we see this on a sparse file system, then a truncate has

	 * raced us and removed the cluster. In this case, we clear

	 * the buffers dirty and uptodate bits and let the buffer code

	 * ignore it as a hole.

 Treat the unwritten extent as a hole for zeroing purposes. */

 Clear the remaining part of the page */

		/*

		 * Unlock the page and cycle ip_alloc_sem so that we don't

		 * busyloop waiting for ip_alloc_sem to unlock

	/*

	 * i_size might have just been updated as we grabed the meta lock.  We

	 * might now be discovering a truncate that hit on another node.

	 * block_read_full_page->get_block freaks out if it is asked to read

	 * beyond the end of a file, so we check here.  Callers

	 * (generic_file_read, vm_ops->fault) are clever enough to check i_size

	 * and notice that the page they just read isn't needed.

	 *

	 * XXX sys_readahead() seems to get that wrong?

/*

 * This is used only for read-ahead. Failures or difficult to handle

 * situations are safe to ignore.

 *

 * Right now, we don't bother with BH_Boundary - in-inode extent lists

 * are quite large (243 extents on 4k blocks), so most inodes don't

 * grow out to a tree. If need be, detecting boundary extents could

 * trivially be added in a future version of ocfs2_get_block().

	/*

	 * Use the nonblocking flag for the dlm code to avoid page

	 * lock inversion, but don't bother with retrying.

	/*

	 * Don't bother with inline-data. There isn't anything

	 * to read-ahead in that case anyway...

	/*

	 * Check whether a remote node truncated this file - we just

	 * drop out in that case as it's not worth handling here.

/* Note: Because we don't support holes, our allocation has

 * already happened (allocation writes zeros to the file data)

 * so we don't have to worry about ordered writes in

 * ocfs2_writepage.

 *

 * ->writepage is called during the process of invalidating the page cache

 * during blocked lock processing.  It can't block on any cluster locks

 * to during block mapping.  It's relying on the fact that the block

 * mapping can't have disappeared under the dirty pages that it is

 * being asked to write back.

/* Taken from ext3. We don't necessarily need the full blown

 * functionality yet, but IMHO it's better to cut and paste the whole

 * thing so we can avoid introducing our own bugs (and easily pick up

	/*

	 * The swap code (ab-)uses ->bmap to get a block mapping and then

	 * bypasseѕ the file system for actual I/O.  We really can't allow

	 * that on refcounted inodes, so we have to skip out here.  And yes,

	 * 0 is the magic code for a bmap error..

	/* We don't need to lock journal system files, since they aren't

	 * accessed concurrently from multiple nodes.

/*

 * 'from' and 'to' are the region in the page to avoid zeroing.

 *

 * If pagesize > clustersize, this function will avoid zeroing outside

 * of the cluster boundary.

 *

 * from == to == 0 is code for "zero the entire cluster region"

/*

 * Nonsparse file systems fully allocate before we get to the write

 * code. This prevents ocfs2_write() from tagging the write as an

 * allocating one, which means ocfs2_map_page_blocks() might try to

 * read-in the blocks at the tail of our file. Avoid reading them by

 * testing i_size against each block offset.

/*

 * Some of this taken from __block_write_begin(). We already have our

 * mapping by now though, and the entire write will be allocating or

 * it won't, so not much need to use BH_New.

 *

 * This will also skip zeroing, which is handled externally.

		/*

		 * Ignore blocks outside of our i/o range -

		 * they may belong to unallocated clusters.

		/*

		 * For an allocating write with cluster size >= page

		 * size, we always write the entire page.

	/*

	 * If we issued read requests - let them complete.

	/*

	 * If we get -EIO above, zero out any newly allocated blocks

	 * to avoid exposing stale data.

/*

 * Describe the state of a single cluster to be written to.

	/*

	 * Give this a unique field because c_phys eventually gets

	 * filled.

 Logical cluster position / len of write */

 First cluster allocated in a nonsparse extend */

 Type of caller. Must be one of buffer, mmap, direct.  */

	/*

	 * This is true if page_size > cluster_size.

	 *

	 * It triggers a set of special cases during write which might

	 * have to deal with allocating writes to partial pages.

	/*

	 * Pages involved in this write.

	 *

	 * w_target_page is the page being written to by the user.

	 *

	 * w_pages is an array of pages which always contains

	 * w_target_page, and in the case of an allocating write with

	 * page_size < cluster size, it will contain zero'd and mapped

	 * pages adjacent to w_target_page which need to be written

	 * out in so that future reads from that region will get

	 * zero's.

	/*

	 * w_target_locked is used for page_mkwrite path indicating no unlocking

	 * against w_target_page in ocfs2_write_end_nolock.

	/*

	 * ocfs2_write_end() uses this to know what the real range to

	 * write in the target should be.

	/*

	 * We could use journal_current_handle() but this is cleaner,

	 * IMHO -Mark

	/*

	 * w_target_locked is only set to true in the page_mkwrite() case.

	 * The intent is to allow us to lock the target page from write_begin()

	 * to write_end(). The caller must hold a ref on w_target_page.

/*

 * If a page has any new buffers, zero them out here, and mark them uptodate

 * and dirty so they'll be written out (in order to prevent uninitialised

 * block data from leaking). And clear the new bit.

/*

 * Only called when we have a failure during allocating write to write

 * zero's to the newly allocated region.

	/* treat the write as new if the a hole/lseek spanned across

	 * the page boundary.

		/*

		 * If we haven't allocated the new page yet, we

		 * shouldn't be writing it out without copying user

		 * data. This is likely a math error from the caller.

	/*

	 * Parts of newly allocated pages need to be zero'd.

	 *

	 * Above, we have also rewritten 'to' and 'from' - as far as

	 * the rest of the function is concerned, the entire cluster

	 * range inside of a page needs to be written.

	 *

	 * We can skip this if the page is up to date - it's already

	 * been zero'd from being read in as a hole.

/*

 * This function will only grab one clusters worth of pages.

	/*

	 * Figure out how many pages we'll be manipulating here. For

	 * non allocating write, we just change the one

	 * page. Otherwise, we'll need a whole clusters worth.  If we're

	 * writing past i_size, we only need enough pages to cover the

	 * last page of the write.

		/*

		 * We need the index *past* the last page we could possibly

		 * touch.  This is the page past the end of the write or

		 * i_size, whichever is greater.

			/*

			 * ocfs2_pagemkwrite() is a little different

			 * and wants us to directly use the page

			 * passed in.

 Exit and let the caller retry */

 Direct write has no mapping page. */

/*

 * Prepare a single cluster for write one cluster into the file.

		/*

		 * This is safe to call with the page locks - it won't take

		 * any additional semaphores or cluster locks.

		/*

		 * This shouldn't happen because we must have already

		 * calculated the correct meta data allocation required. The

		 * internal tree allocation code should know how to increase

		 * transaction credits itself.

		 *

		 * If need be, we could handle -EAGAIN for a

		 * RESTART_TRANS here.

	/*

	 * The only reason this should fail is due to an inability to

	 * find the extent added.

 This is the direct io target page. */

	/*

	 * We only have cleanup to do in case of allocating write.

		/*

		 * We have to make sure that the total write passed in

		 * doesn't extend past a single cluster.

/*

 * ocfs2_write_end() wants to know which parts of the target page it

 * should complete the write on. It's easiest to compute them ahead of

 * time when a more complete view of the write is available.

	/*

	 * Allocating write - we may have different boundaries based

	 * on page size and cluster size.

	 *

	 * NOTE: We can no longer compute one value from the other as

	 * the actual write length and user provided length may be

	 * different.

		/*

		 * We only care about the 1st and last cluster within

		 * our range and whether they should be zero'd or not. Either

		 * value may be extended out to the start/end of a

		 * newly allocated cluster.

/*

 * Check if this extent is marked UNWRITTEN by direct io. If so, we need not to

 * do the zero work. And should not to clear UNWRITTEN since it will be cleared

 * by the direct io procedure.

 * If this is a new extent that allocated by direct io, we should mark it in

 * the ip_unwritten_list.

	/* Needs not to zero no metter buffer or direct. The one who is zero

	 * the cluster is doing zero. And he will clear unwritten after all

 This direct write will doing zero. */

/*

 * Populate each single-cluster write descriptor in the write context

 * with information about the i/o to be done.

 *

 * Returns the number of clusters that will have to be allocated, as

 * well as a worst case estimate of the number of extent records that

 * would have to be created during a write to an unwritten region.

			/*

			 * Need to look up the next extent record.

 We should already CoW the refcountd extent. */

			/*

			 * Assume worst case - that we're writing in

			 * the middle of the extent.

			 *

			 * We can assume that the write proceeds from

			 * left to right, in which case the extent

			 * insert code is smart enough to coalesce the

			 * next splits into the previous records created.

			/*

			 * Only increment phys if it doesn't describe

			 * a hole.

		/*

		 * If w_first_new_cpos is < UINT_MAX, we have a non-sparse

		 * file that got extended.  w_first_new_cpos tells us

		 * where the newly allocated clusters are so we can

		 * zero them.

	/*

	 * If we don't set w_num_pages then this page won't get unlocked

	 * and freed on cleanup of the write context.

	/*

	 * Handle inodes which already have inline data 1st.

		/*

		 * The write won't fit - we have to give this inode an

		 * inline extent list now.

	/*

	 * Check whether the inode can accept inline data.

	/*

	 * Check whether the write can fit.

	/*

	 * This signals to the caller that the data can be written

	 * inline.

/*

 * This function only does anything for file systems which can't

 * handle sparse files.

 *

 * What we want to do here is fill in any hole between the current end

 * of allocation and the end of our write. That way the rest of the

 * write path can treat it as an non-allocating write, which has no

 * special case code for sparse/nonsparse files.

 There is no wc if this is call from direct. */

 Direct io change i_size late, should not zero tail here. */

	/*

	 * We set w_target_from, w_target_to here so that

	 * ocfs2_write_end() knows which range in the target page to

	 * write out. An allocation requires that we write the entire

	 * cluster range.

		/*

		 * XXX: We are stretching the limits of

		 * ocfs2_lock_allocators(). It greatly over-estimates

		 * the work to be done.

 direct write needs not to start trans if no extents alloc. */

	/*

	 * We have to zero sparse allocated clusters, unwritten extent clusters,

	 * and non-sparse clusters we just extended.  For non-sparse writes,

	 * we know zeros will only be needed in the first and/or last cluster.

	/*

	 * Fill our page array first. That way we've grabbed enough so

	 * that we can zero and flush if we error after adding the

	 * extent.

	/*

	 * ocfs2_grab_pages_for_write() returns -EAGAIN if it could not lock

	 * the target page. In this case, we exit with no error and no target

	 * page. This will trigger the caller, page_mkwrite(), to re-try

	 * the operation.

	/*

	 * The mmapped page won't be unlocked in ocfs2_free_write_ctxt(),

	 * even in case of error here like ENOSPC and ENOMEM. So, we need

	 * to unlock the target page manually to prevent deadlocks when

	 * retrying again on ENOSPC, or when returning non-VM_FAULT_LOCKED

	 * to VM code.

		/*

		 * Try to free some truncate log so that we can have enough

		 * clusters to allocate.

	/*

	 * Take alloc sem here to prevent concurrent lookups. That way

	 * the mapping, zeroing and tree manipulation within

	 * ocfs2_write() will be safe against ->readpage(). This

	 * should also serve to lock out allocation from a shared

	 * writeable region.

 This is the direct io target page. */

			/*

			 * Pages adjacent to the target (if any) imply

			 * a hole-filling write in which case we want

			 * to flush their entire range.

 Direct io do not update i_size here. */

	/* unlock pages before dealloc since it needs acquiring j_trans_barrier

	 * lock, or it will cause a deadlock since journal commit threads holds

	 * this lock and will ask for the page lock when flushing the data.

	 * put it here to preserve the unlock order.

/*

 * TODO: Make this into a generic get_blocks function.

 *

 * From do_direct_io in direct-io.c:

 *  "So what we do is to permit the ->get_blocks function to populate

 *   bh.b_size with the size of IO which is permitted at this offset and

 *   this i_blkbits."

 *

 * This function is called directly from get_more_blocks in direct-io.c.

 *

 * called like this: dio->get_blocks(dio->inode, fs_startblk,

 * 					fs_count, map_bh, dio->rw == WRITE);

	/*

	 * bh_result->b_size is count in get_more_blocks according to write

	 * "pos" and "end", we need map twice to return different buffer state:

	 * 1. area in file size, not set NEW;

	 * 2. area out file size, set  NEW.

	 *

	 *		   iblock    endblk

	 * |--------|---------|---------|---------

	 * |<-------area in file------->|

	/*

	 * Because we need to change file size in ocfs2_dio_end_io_write(), or

	 * we may need to add it to orphan dir. So can not fall to fast path

	 * while file size will be changed.

 This is the fast path for re-write. */

 Clear state set by ocfs2_get_block. */

		/*

		 * when we are going to alloc extents beyond file size, add the

		 * inode to orphan dir, so we can recall those spaces when

		 * system crashed during write.

	/* May sleep in end_io. It should not happen in a irq context. So defer

 The physical address may be 0, fill it. */

	/* We do clear unwritten, delete orphan, change i_size here. If neither

 Delete orphan before acquire i_mutex. */

	/* Attach dealloc with extent tree in case that we may reuse extents

	 * which are already unlinked from current extent tree due to extent

	 * rotation and merging.

/*

 * ocfs2_dio_end_io is called by the dio core when a dio is finished.  We're

 * particularly interested in the aio/dio case.  We use the rw_lock DLM lock

 * to protect io on one node from truncation on another.

 this io's submitter should not have unlocked this before we could */

	/*

	 * Fallback to buffered I/O if we see an inode without

	 * extents.

 Fallback to buffered I/O if we do not support append dio. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * reservations.c

 *

 * Allocation reservations implementation

 *

 * Some code borrowed from fs/ext3/balloc.c and is:

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 * The rest is copyright (C) 2010 Novell.  All rights reserved.

 8, 16, 32, 64, 128, 256, 512, 1024 */

 m_bitmap_len is initialized to zero by the above memset. */

	/*

	 * last_len and last_start no longer make sense if

	 * we're changing the range of our allocations.

 does nothing if 'resv' is null */

 Does nothing for now. Keep this around for API symmetry */

			/*

			 * This is a good place to check for

			 * overlapping reservations.

 This should never happen! */

/**

 * ocfs2_find_resv_lhs() - find the window which contains goal

 * @resmap: reservation map to search

 * @goal: which bit to search for

 *

 * If a window containing that goal is not found, we return the window

 * which comes before goal. Returns NULL on empty rbtree or no window

 * before goal.

 Check if we overshot the reservation just before goal? */

/*

 * We are given a range within the bitmap, which corresponds to a gap

 * inside the reservations tree (search_start, search_len). The range

 * can be anything from the whole bitmap, to a gap between

 * reservations.

 *

 * The start value of *rstart is insignificant.

 *

 * This function searches the bitmap range starting at search_start

 * with length search_len for a set of contiguous free bits. We try

 * to find up to 'wanted' bits, but can sometimes return less.

 *

 * Returns the length of allocation, 0 if no free bits are found.

 *

 * *cstart and *clen will also be populated with the result.

 Search reached end of the region */

 we found a zero */

 move start to the next bit to test */

 got a zero after some ones */

	/*

	 * Nasty cases to consider:

	 *

	 * - rbtree is empty

	 * - our window should be first in all reservations

	 * - our window should be last in all reservations

	 * - need to make sure we don't go past end of bitmap

		/*

		 * Easiest case - empty tree. We can just take

		 * whatever window of free bits we want.

		/*

		 * This should never happen - the local alloc window

		 * will always have free bits when we're called.

		/*

		 * A NULL here means that the search code couldn't

		 * find a window that starts before goal.

		 *

		 * However, we can take the first window after goal,

		 * which is also by definition, the leftmost window in

		 * the entire tree. If we can find free bits in the

		 * gap between goal and the LHS window, then the

		 * reservation can safely be placed there.

		 *

		 * Otherwise we fall back to a linear search, checking

		 * the gaps in between windows for a place to

		 * allocate.

		/*

		 * The search should never return such a window. (see

		 * comment above

 Now we do a linear search for a window, starting at 'prev_rsv' */

			/*

			 * We're at the rightmost edge of the

			 * tree. See if a reservation between this

			 * window and the end of the bitmap will work.

		/*

		 * No need to check this gap if we have already found

		 * a larger region of free bits.

		min_bits = wanted; /* We at know the temp window will use all

	/*

	 * Take the first reservation off the LRU as our 'target'. We

	 * don't try to be smart about it. There might be a case for

	 * searching based on size but I don't have enough data to be

	 * sure. --Mark (3/16/2010)

	/*

	 * Cannibalize (some or all) of the target reservation and

	 * feed it to the current window.

		/*

		 * Discard completely if size is less than or equal to a

		 * reasonable threshold - 50% of window bits for non temporary

		 * windows.

	/*

	 * Begin by trying to get a window as close to the previous

	 * one as possible. Using the most recent allocation as a

	 * start goal makes sense.

 Search from last alloc didn't work, try once more from beginning. */

		/*

		 * Still empty? Pull oldest one off the LRU, remove it from

		 * tree, put this one in it's place.

		/*

		 * We don't want to over-allocate for temporary

		 * windows. Otherwise, we run the risk of fragmenting the

		 * allocation space.

		/*

		 * Try to get a window here. If it works, we must fall

		 * through and test the bitmap . This avoids some

		 * ping-ponging of windows due to non-reserved space

		 * being allocation before we initialize a window for

		 * that inode.

	/*

	 * Completely used? We can remove it then.

	/*

	 * This should have been trapped above.

	/*

	 * May have been discarded above from

	 * ocfs2_adjust_resv_from_alloc().

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * alloc.c

 *

 * Extent allocs and frees

 *

 * Copyright (C) 2002, 2004 Oracle.  All rights reserved.

/*

 * Operations for a specific extent tree type.

 *

 * To implement an on-disk btree (extent tree) type in ocfs2, add

 * an ocfs2_extent_tree_operations structure and the matching

 * ocfs2_init_<thingy>_extent_tree() function.  That's pretty much it

 * for the allocation portion of the extent tree.

	/*

	 * last_eb_blk is the block number of the right most leaf extent

	 * block.  Most on-disk structures containing an extent tree store

	 * this value for fast access.  The ->eo_set_last_eb_blk() and

	 * ->eo_get_last_eb_blk() operations access this value.  They are

	 *  both required.

	/*

	 * The on-disk structure usually keeps track of how many total

	 * clusters are stored in this extent tree.  This function updates

	 * that value.  new_clusters is the delta, and must be

	 * added to the total.  Required.

	/*

	 * If this extent tree is supported by an extent map, insert

	 * a record into the map.

	/*

	 * If this extent tree is supported by an extent map, truncate the

	 * map to clusters,

	/*

	 * If ->eo_insert_check() exists, it is called before rec is

	 * inserted into the extent tree.  It is optional.

	/*

	 * --------------------------------------------------------------

	 * The remaining are internal to ocfs2_extent_tree and don't have

	 * accessor functions

	/*

	 * ->eo_fill_root_el() takes et->et_object and sets et->et_root_el.

	 * It is required.

	/*

	 * ->eo_fill_max_leaf_clusters sets et->et_max_leaf_clusters if

	 * it exists.  If it does not, et->et_max_leaf_clusters is set

	 * to 0 (unlimited).  Optional.

	/*

	 * ->eo_extent_contig test whether the 2 ocfs2_extent_rec

	 * are contiguous or not. Optional. Don't need to set it if use

	 * ocfs2_extent_rec as the tree leaf.

/*

 * Pre-declare ocfs2_dinode_et_ops so we can use it as a sanity check

 * in the methods.

/*

 * Reset the actual path elements so that we can re-use the structure

 * to build another path. Generally, this involves freeing the buffer

 * heads.

	/*

	 * Tree depth may change during truncate, or insert. If we're

	 * keeping the root extent list, then make sure that our path

	 * structure reflects the proper depth.

/*

 * All the elements of src into dest. After this call, src could be freed

 * without affecting dest.

 *

 * Both paths should have the same root. Any non-root elements of dest

 * will be freed.

/*

 * Make the *dest path the same as src and re-initialize src path to

 * have a root only.

/*

 * Insert an extent block at given index.

 *

 * This will not take an additional reference on eb_bh.

	/*

	 * Right now, no root bh is an extent block, so this helps

	 * catch code errors with dinode trees. The assertion can be

	 * safely removed if we ever need to insert extent block

	 * structures at the root.

/*

 * Journal the buffer at depth idx.  All idx>0 are extent_blocks,

 * otherwise it's the root_access function.

 *

 * I don't like the way this function's name looks next to

 * ocfs2_journal_access_path(), but I don't have a better one.

/*

 * Convenience function to journal all components in a path.

/*

 * Return the index of the extent record which contains cluster #v_cluster.

 * -1 is returned if it was not found.

 *

 * Should work fine on interior and exterior nodes.

/*

 * NOTE: ocfs2_block_extent_contig(), ocfs2_extents_adjacent() and

 * ocfs2_extent_rec_contig only work properly against leaf nodes!

	/*

	 * Refuse to coalesce extent records with different flag

	 * fields - we don't want to mix unwritten extents with user

	 * data.

/*

 * NOTE: We can have pretty much any combination of contiguousness and

 * appending.

 *

 * The usefulness of APPEND_TAIL is more in that it lets us know that

 * we'll have to update the path to that leaf.

	/*

	 * If the ecc fails, we return the error but otherwise

	 * leave the filesystem running.  We know any error is

	 * local to this block.

	/*

	 * Errors after here are fatal.

 If ocfs2_read_block() got us a new bh, pass it up. */

/*

 * How many free extents have we got before we need more meta data?

/* expects array to already be allocated

 *

 * sets h_signature, h_blkno, h_suballoc_bit, h_suballoc_slot, and

 * l_count for you

 Ok, setup the minimal stuff here. */

			/* We'll also be dirtied by the caller, so

/*

 * Helper function for ocfs2_add_branch() and ocfs2_shift_tree_depth().

 *

 * Returns the sum of the rightmost extent rec logical offset and

 * cluster count.

 *

 * ocfs2_add_branch() uses this to determine what logical cluster

 * value should be populated into the leftmost new branch records.

 *

 * ocfs2_shift_tree_depth() uses this to determine the # clusters

 * value for the new topmost tree record.

/*

 * Change range of the branches in the right most path according to the leaf

 * extent block's rightmost record.

/*

 * Add an entire tree branch to our inode. eb_bh is the extent block

 * to start at, if we don't want to start the branch at the root

 * structure.

 *

 * last_eb_bh is required as we have to update it's next_leaf pointer

 * for the new last extent block.

 *

 * the new branch will be 'empty' in the sense that every block will

 * contain a single record with cluster count == 0.

 we never add a branch to a leaf. */

	/*

	 * If there is a gap before the root end and the real end

	 * of the righmost leaf block, we need to remove the gap

	 * between new_cpos and root_end first so that the tree

	 * is consistent after we add a new branch(it will start

	 * from new_cpos).

 allocate the number of new eb blocks we need */

	/* Firstyly, try to reuse dealloc since we have already estimated how

	 * many extent blocks we may use.

	/* Note: new_eb_bhs[new_blocks - 1] is the guy which will be

	 * linked with the rest of the tree.

	 * conversly, new_eb_bhs[0] is the new bottommost leaf.

	 *

	 * when we leave the loop, new_last_eb_blk will point to the

	 * newest leaf, and next_blkno will point to the topmost extent

 ocfs2_create_new_meta_bhs() should create it right! */

		/*

		 * This actually counts as an empty extent as

		 * c_clusters == 0

		/*

		 * eb_el isn't always an interior node, but even leaf

		 * nodes want a zero'd flags and reserved field so

		 * this gets the whole 32 bits regardless of use.

	/* This is a bit hairy. We want to update up to three blocks

	 * here without leaving any of them in an inconsistent state

	 * in case of error. We don't have to worry about

	 * journal_dirty erroring as it won't unless we've aborted the

	 * handle (in which case we would never be here) so reserving

	/* Link the new branch into the rest of the tree (el will

	/* fe needs a new last extent block pointer, as does the

	/*

	 * Some callers want to track the rightmost leaf so pass it

	 * back here.

/*

 * adds another level to the allocation tree.

 * returns back the new extent block so you can add a branch to it

 * after this call.

 ocfs2_create_new_meta_bhs() should create it right! */

 copy the root extent list data into the new extent block */

 update root_bh now */

	/* If this is our 1st tree depth shift, then last_eb_blk

/*

 * Should only be called when there is no space left in any of the

 * leaf nodes. What we want to do is find the lowest tree depth

 * non-leaf extent block with room for new records. There are three

 * valid results of this search:

 *

 * 1) a lowest extent block is found, then we pass it back in

 *    *lowest_eb_bh and return '0'

 *

 * 2) the search fails to find anything, but the root_el has room. We

 *    pass NULL back in *lowest_eb_bh, but still return '0'

 *

 * 3) the search fails to find anything AND the root_el is full, in

 *    which case we return > 0

 *

 * return status < 0 indicates an error.

	/* If we didn't find one and the fe doesn't have any room,

/*

 * Grow a b-tree so that it has more records.

 *

 * We might shift the tree depth in which case existing paths should

 * be considered invalid.

 *

 * Tree depth after the grow is returned via *final_depth.

 *

 * *last_eb_bh will be updated by ocfs2_add_branch().

	/* We traveled all the way to the bottom of the allocation tree

	 * and didn't find room for any more extents - we need to add

		/* ocfs2_shift_tree_depth will return us a buffer with

		 * the new extent block (so we can pass that to

			/*

			 * Special case: we have room now if we shifted from

			 * tree_depth 0, so no more work needs to be done.

			 *

			 * We won't be calling add_branch, so pass

			 * back *last_eb_bh as the new leaf. At depth

			 * zero, it should always be null so there's

			 * no reason to brelse.

	/* call ocfs2_add_branch to add the final part of the tree with

/*

 * This function will discard the rightmost extent record.

 This will cause us to go off the end of our extent list. */

 The tree code before us didn't allow enough room in the leaf. */

	/*

	 * The easiest way to approach this is to just remove the

	 * empty extent and temporarily decrement next_free.

		/*

		 * If next_free was 1 (only an empty extent), this

		 * loop won't execute, which is fine. We still want

		 * the decrement above to happen.

	/*

	 * Figure out what the new record index should be.

	/*

	 * No need to memmove if we're just adding to the tail.

	/*

	 * Either we had an empty extent, and need to re-increment or

	 * there was no empty extent on a non full rightmost leaf node,

	 * in which case we still need to increment.

	/*

	 * Make sure none of the math above just messed up our tree.

/*

 * Create an empty extent record .

 *

 * l_next_free_rec may be updated.

 *

 * If an empty extent already exists do nothing.

/*

 * For a rotation which involves two leaf nodes, the "root node" is

 * the lowest level tree node which contains a path to both leafs. This

 * resulting set of information can be used to form a complete "subtree"

 *

 * This function is passed two full paths from the dinode down to a

 * pair of adjacent leaves. It's task is to figure out which path

 * index contains the subtree root - this can be the root index itself

 * in a worst-case rotation.

 *

 * The array index of the subtree root is passed back.

	/*

	 * Check that the caller passed in two paths from the same tree.

		/*

		 * The caller didn't pass two adjacent paths.

/*

 * Traverse a btree path in search of cpos, starting at root_el.

 *

 * This code can be called with a cpos larger than the tree, in which

 * case it will return the rightmost path.

			/*

			 * In the case that cpos is off the allocation

			 * tree, this should just wind up returning the

			 * rightmost record.

	/*

	 * Catch any trailing bh that the loop didn't handle.

/*

 * Given an initialized path (that is, it has a valid root extent

 * list), this function will traverse the btree in search of the path

 * which would contain cpos.

 *

 * The path traveled is recorded in the path structure.

 *

 * Note that this will not do any comparisons on leaf node extent

 * records, so it will work fine in the case that we just added a tree

 * branch.

 We want to retain only the leaf block. */

/*

 * Find the leaf block in the tree which would contain cpos. No

 * checking of the actual leaf is done.

 *

 * Some paths want to call this instead of allocating a path structure

 * and calling ocfs2_find_path().

 *

 * This function doesn't handle non btree extent lists.

/*

 * Adjust the adjacent records (left_rec, right_rec) involved in a rotation.

 *

 * Basically, we've moved stuff around at the bottom of the tree and

 * we need to fix up the extent records above the changes to reflect

 * the new changes.

 *

 * left_rec: the record on the left.

 * right_rec: the record to the right of left_rec

 * right_child_el: is the child list pointed to by right_rec

 *

 * By definition, this only works on interior nodes.

	/*

	 * Interior nodes never have holes. Their cpos is the cpos of

	 * the leftmost record in their child list. Their cluster

	 * count covers the full theoretical range of their child list

	 * - the range between their cpos and the cpos of the record

	 * immediately to their right.

	/*

	 * Calculate the rightmost cluster count boundary before

	 * moving cpos - we will need to adjust clusters after

	 * updating e_cpos to keep the same highest cluster count.

/*

 * Adjust the adjacent root node records involved in a

 * rotation. left_el_blkno is passed in as a key so that we can easily

 * find it's index in the root list.

	/*

	 * The path walking code should have never returned a root and

	 * two paths which are not adjacent.

/*

 * We've changed a leaf block (in right_path) and need to reflect that

 * change back up the subtree.

 *

 * This happens in multiple places:

 *   - When we've moved an extent record from the left path leaf to the right

 *     path leaf to make room for an empty extent in the left path leaf.

 *   - When our insert into the right path leaf is at the leftmost edge

 *     and requires an update of the path immediately to it's left. This

 *     can occur at the end of some types of rotation and appending inserts.

 *   - When we've adjusted the last extent record in the left path leaf and the

 *     1st extent record in the right path leaf during cross extent block merge.

	/*

	 * Update the counts and position values within all the

	 * interior nodes to reflect the leaf rotation we just did.

	 *

	 * The root node is handled below the loop.

	 *

	 * We begin the loop with right_el and left_el pointing to the

	 * leaf lists and work our way up.

	 *

	 * NOTE: within this loop, left_el and right_el always refer

	 * to the *child* lists.

		/*

		 * One nice property of knowing that all of these

		 * nodes are below the root is that we only deal with

		 * the leftmost right node record and the rightmost

		 * left node record.

		/*

		 * Setup our list pointers now so that the current

		 * parents become children in the next iteration.

	/*

	 * At the root node, adjust the two adjacent records which

	 * begin our path to the leaves.

	/*

	 * This extent block may already have an empty record, so we

	 * return early if so.

 This is a code error, not a disk corruption. */

 Do the copy now. */

	/*

	 * Clear out the record we just copied and shift everything

	 * over, leaving an empty extent in the left leaf.

	 *

	 * We temporarily subtract from next_free_rec so that the

	 * shift will lose the tail record (which is now defunct).

/*

 * Given a full path, determine what cpos value would return us a path

 * containing the leaf immediately to the left of the current one.

 *

 * Will return zero if the path passed in is already the leftmost path.

 Start at the tree node just above the leaf and work our way up. */

		/*

		 * Find the extent record just before the one in our

		 * path.

						/*

						 * We've determined that the

						 * path specified is already

						 * the leftmost one - return a

						 * cpos of zero.

					/*

					 * The leftmost record points to our

					 * leaf - we need to travel up the

					 * tree one level.

		/*

		 * If we got here, we never found a valid node where

		 * the tree indicated one should be.

/*

 * Extend the transaction by enough credits to complete the rotation,

 * and still leave at least the original number of credits allocated

 * to this transaction.

/*

 * Trap the case where we're inserting into the theoretical range past

 * the _actual_ left leaf range. Otherwise, we'll rotate a record

 * whose cpos is less than ours into the right leaf.

 *

 * It's only necessary to look at the rightmost record of the left

 * leaf because the logic that calls us should ensure that the

 * theoretical ranges in the path components above the leaves are

 * correct.

 Empty list. */

/*

 * Rotate all the records in a btree right one record, starting at insert_cpos.

 *

 * The path to the rightmost leaf should be passed in.

 *

 * The array is assumed to be large enough to hold an entire path (tree depth).

 *

 * Upon successful return from this function:

 *

 * - The 'right_path' array will contain a path to the leaf block

 *   whose range contains e_cpos.

 * - That leaf block will have a single empty extent in list index 0.

 * - In the case that the rotation requires a post-insert update,

 *   *ret_left_path will contain a valid path which can be passed to

 *   ocfs2_insert_path().

	/*

	 * What we want to do here is:

	 *

	 * 1) Start with the rightmost path.

	 *

	 * 2) Determine a path to the leaf block directly to the left

	 *    of that leaf.

	 *

	 * 3) Determine the 'subtree root' - the lowest level tree node

	 *    which contains a path to both leaves.

	 *

	 * 4) Rotate the subtree.

	 *

	 * 5) Find the next subtree by considering the left path to be

	 *    the new right path.

	 *

	 * The check at the top of this while loop also accepts

	 * insert_cpos == cpos because cpos is only a _theoretical_

	 * value to get us the left path - insert_cpos might very well

	 * be filling that hole.

	 *

	 * Stop at a cpos of '0' because we either started at the

	 * leftmost branch (i.e., a tree with one branch and a

	 * rotation inside of it), or we've gone as far as we can in

	 * rotating subtrees.

			/*

			 * We've rotated the tree as much as we

			 * should. The rest is up to

			 * ocfs2_insert_path() to complete, after the

			 * record insertion. We indicate this

			 * situation by returning the left path.

			 *

			 * The reason we don't adjust the records here

			 * before the record insert is that an error

			 * later might break the rule where a parent

			 * record e_cpos will reflect the actual

			 * e_cpos of the 1st nonempty record of the

			 * child list.

			/*

			 * A rotate moves the rightmost left leaf

			 * record over to the leftmost right leaf

			 * slot. If we're doing an extent split

			 * instead of a real insert, then we have to

			 * check that the extent to be split wasn't

			 * just moved over. If it was, then we can

			 * exit here, passing left_path back -

			 * ocfs2_split_extent() is smart enough to

			 * search both leaves.

		/*

		 * There is no need to re-read the next right path

		 * as we know that it'll be our current left

		 * path. Optimize by copying values instead.

 Path should always be rightmost. */

		/*

		 * Not all nodes might have had their final count

		 * decremented by the caller - handle this here.

		/*

		 * It's legal for us to proceed if the right leaf is

		 * the rightmost one and it has an empty extent. There

		 * are two cases to handle - whether the leaf will be

		 * empty after removal or not. If the leaf isn't empty

		 * then just remove the empty extent up front. The

		 * next block will handle empty leaves by flagging

		 * them for unlink.

		 *

		 * Non rightmost leaves will throw -EAGAIN and the

		 * caller can manually move the subtree and retry.

		/*

		 * We have to update i_last_eb_blk during the meta

		 * data delete.

	/*

	 * Getting here with an empty extent in the right path implies

	 * that it's the rightmost path and will be deleted.

		/*

		 * Only do this if we're moving a real

		 * record. Otherwise, the action is delayed until

		 * after removal of the right path in which case we

		 * can do a simple shift to remove the empty extent.

		/*

		 * Move recs over to get rid of empty extent, decrease

		 * next_free. This is allowed to remove the last

		 * extent in our leaf (setting l_next_free_rec to

		 * zero) - the delete code below won't care.

		/*

		 * Removal of the extent in the left leaf was skipped

		 * above so we could delete the right path

		 * 1st.

/*

 * Given a full path, determine what cpos value would return us a path

 * containing the leaf immediately to the right of the current one.

 *

 * Will return zero if the path passed in is already the rightmost path.

 *

 * This looks similar, but is subtly different to

 * ocfs2_find_cpos_for_left_leaf().

 Start at the tree node just above the leaf and work our way up. */

		/*

		 * Find the extent record just after the one in our

		 * path.

						/*

						 * We've determined that the

						 * path specified is already

						 * the rightmost one - return a

						 * cpos of zero.

					/*

					 * The rightmost record points to our

					 * leaf - we need to travel up the

					 * tree one level.

		/*

		 * If we got here, we never found a valid node where

		 * the tree indicated one should be.

		/*

		 * Caller might still want to make changes to the

		 * tree root, so re-add it to the journal here.

			/*

			 * The rotation has to temporarily stop due to

			 * the right subtree having an empty

			 * extent. Pass it back to the caller for a

			 * fixup.

		/*

		 * The subtree rotate might have removed records on

		 * the rightmost edge. If so, then rotation is

		 * complete.

		/*

		 * We have a path to the left of this one - it needs

		 * an update too.

		/*

		 * 'path' is also the leftmost path which

		 * means it must be the only one. This gets

		 * handled differently because we want to

		 * revert the root back to having extents

		 * in-line.

/*

 * Left rotation of btree records.

 *

 * In many ways, this is (unsurprisingly) the opposite of right

 * rotation. We start at some non-rightmost path containing an empty

 * extent in the leaf block. The code works its way to the rightmost

 * path by rotating records to the left in every subtree.

 *

 * This is used by any code which reduces the number of extent records

 * in a leaf. After removal, an empty record should be placed in the

 * leftmost list position.

 *

 * This won't handle a length update of the rightmost path records if

 * the rightmost tree leaf record is removed so the caller is

 * responsible for detecting and correcting that.

		/*

		 * Inline extents. This is trivially handled, so do

		 * it up front.

	/*

	 * Handle rightmost branch now. There's several cases:

	 *  1) simple rotation leaving records in there. That's trivial.

	 *  2) rotation requiring a branch delete - there's no more

	 *     records left. Two cases of this:

	 *     a) There are branches to the left.

	 *     b) This is also the leftmost (the only) branch.

	 *

	 *  1) is handled via ocfs2_rotate_rightmost_leaf_left()

	 *  2a) we need the left branch so that we can update it with the unlink

	 *  2b) we need to bring the root back to inline extents.

		/*

		 * This gets a bit tricky if we're going to delete the

		 * rightmost path. Get the other cases out of the way

		 * 1st.

		/*

		 * XXX: The caller can not trust "path" any more after

		 * this as it will have been deleted. What do we do?

		 *

		 * In theory the rotate-for-merge code will never get

		 * here because it'll always ask for a rotate in a

		 * nonempty list.

	/*

	 * Now we can loop, remembering the path we get from -EAGAIN

	 * and restarting from there.

		/*

		 * We consumed all of the merged-from record. An empty

		 * extent cannot exist anywhere but the 1st array

		 * position, so move things over if the merged-from

		 * record doesn't occupy that position.

		 *

		 * This creates a new empty extent so the caller

		 * should be smart enough to have removed any existing

		 * ones.

		/*

		 * Always memset - the caller doesn't check whether it

		 * created an empty extent, so there could be junk in

		 * the other fields.

 This function shouldn't be called for non-trees. */

 This function shouldn't be called for the rightmost leaf. */

/*

 * Remove split_rec clusters from the record at index and merge them

 * onto the beginning of the record "next" to it.

 * For index < l_count - 1, the next means the extent rec at index + 1.

 * For index == l_count - 1, the "next" means the 1st extent rec of the

 * next extent block.

 we meet with a cross extent block merge. */

 This function shouldn't be called for non-trees. */

 This function shouldn't be called for the leftmost leaf. */

/*

 * Remove split_rec clusters from the record at index and merge them

 * onto the tail of the record "before" it.

 * For index > 0, the "before" means the extent rec at index - 1.

 *

 * For index == 0, the "before" means the last record of the previous

 * extent block. And there is also a situation that we may need to

 * remove the rightmost leaf extent block in the right_path and change

 * the right path to indicate the new rightmost path.

 we meet with a cross extent block merge. */

		/*

		 * The easy case - we can just plop the record right in.

		/*

		 * In the situation that the right_rec is empty and the extent

		 * block is empty also,  ocfs2_complete_edge_insert can't handle

		 * it and we need to delete the right extent block.

 extend credit for ocfs2_remove_rightmost_path */

			/* Now the rightmost extent block has been deleted.

			 * So we use the new rightmost path.

 extend credit for ocfs2_remove_rightmost_path */

		/*

		 * The merge code will need to create an empty

		 * extent to take the place of the newly

		 * emptied slot. Remove any pre-existing empty

		 * extents - having more than one in a leaf is

		 * illegal.

		/*

		 * Left-right contig implies this.

		/*

		 * Since the leftright insert always covers the entire

		 * extent, this call will delete the insert record

		 * entirely, resulting in an empty extent record added to

		 * the extent block.

		 *

		 * Since the adding of an empty extent shifts

		 * everything back to the right, there's no need to

		 * update split_index here.

		 *

		 * When the split_index is zero, we need to merge it to the

		 * prevoius extent block. It is more efficient and easier

		 * if we do merge_right first and merge_left later.

		/*

		 * We can only get this from logic error above.

 extend credit for ocfs2_remove_rightmost_path */

 The merge left us with an empty extent, remove it. */

		/*

		 * Note that we don't pass split_rec here on purpose -

		 * we've merged it into the rec already.

 extend credit for ocfs2_remove_rightmost_path */

		/*

		 * Error from this last rotate is not critical, so

		 * print but don't bubble it up.

		/*

		 * Merge a record to the left or right.

		 *

		 * 'contig_type' is relative to the existing record,

		 * so for example, if we're "right contig", it's to

		 * the record on the left (hence the left merge).

 extend credit for ocfs2_remove_rightmost_path */

			/*

			 * The merge may have left an empty extent in

			 * our leaf. Try to rotate it away.

		/*

		 * Region is on the left edge of the existing

		 * record.

		/*

		 * Region is on the right edge of the existing

		 * record.

/*

 * Do the final bits of extent record insertion at the target leaf

 * list. If this leaf is part of an allocation tree, it is assumed

 * that the tree above has been prepared.

	/*

	 * Contiguous insert - either left or right.

	/*

	 * Handle insert into an empty leaf.

	/*

	 * Appending insert.

	/*

	 * Ok, we have to rotate.

	 *

	 * At this point, it is safe to assume that inserting into an

	 * empty leaf and appending to a leaf have both been handled

	 * above.

	 *

	 * This leaf needs to have space, either by the empty 1st

	 * extent record, or by virtue of an l_next_free_rec < l_count.

	/*

	 * Update everything except the leaf block.

	/*

	 * This shouldn't happen for non-trees. The extent rec cluster

	 * count manipulation below only works for interior nodes.

	/*

	 * If our appending insert is at the leftmost edge of a leaf,

	 * then we might need to update the rightmost records of the

	 * neighboring path.

		/*

		 * No need to worry if the append is already in the

		 * leftmost leaf.

			/*

			 * ocfs2_insert_path() will pass the left_path to the

			 * journal for us.

			/*

			 * This typically means that the record

			 * started in the left path but moved to the

			 * right as a result of rotation. We either

			 * move the existing record to the left, or we

			 * do the later insert there.

			 *

			 * In this case, the left path should always

			 * exist as the rotate code will have passed

			 * it back for a post-insert update.

				/*

				 * It's a left split. Since we know

				 * that the rotate code gave us an

				 * empty extent in the left path, we

				 * can just do the insert there.

				/*

				 * Right split - we have to move the

				 * existing record over to the left

				 * leaf. The insert will be into the

				 * newly created empty extent in the

				 * right leaf.

		/*

		 * Left path is easy - we can just allow the insert to

		 * happen.

/*

 * This function only does inserts on an allocation b-tree. For tree

 * depth = 0, ocfs2_insert_at_leaf() is called directly.

 *

 * right_path is the path we want to do the actual insert

 * in. left_path should only be passed in if we need to update that

 * portion of the tree after an edge insert.

		/*

		 * There's a chance that left_path got passed back to

		 * us without being accounted for in the

		 * journal. Extend our transaction here to be sure we

		 * can change those blocks.

	/*

	 * Pass both paths to the journal. The majority of inserts

	 * will be touching all components anyway.

		/*

		 * We could call ocfs2_insert_at_leaf() for some types

		 * of splits, but it's easier to just let one separate

		 * function sort it all out.

		/*

		 * Split might have modified either leaf and we don't

		 * have a guarantee that the later edge insert will

		 * dirty this for us.

		/*

		 * The rotate code has indicated that we need to fix

		 * up portions of the tree after the insert.

		 *

		 * XXX: Should we extend the transaction here?

	/*

	 * Determine the path to start with. Rotations need the

	 * rightmost path, everything else can go directly to the

	 * target leaf.

	/*

	 * Rotations and appends need special treatment - they modify

	 * parts of the tree's above them.

	 *

	 * Both might pass back a path immediate to the left of the

	 * one being inserted to. This will be cause

	 * ocfs2_insert_path() to modify the rightmost records of

	 * left_path to account for an edge insert.

	 *

	 * XXX: When modifying this code, keep in mind that an insert

	 * can wind up skipping both of these two special cases...

		/*

		 * ocfs2_rotate_tree_right() might have extended the

		 * transaction without re-journaling our tree root.

	/*

	 * We're careful to check for an empty extent record here -

	 * the merge code will know what to do if it sees one.

		/*

		 * Caller might want us to limit the size of extents, don't

		 * calculate contiguousness if we might exceed that limit.

/*

 * This should only be called against the righmost leaf extent list.

 *

 * ocfs2_figure_appending_type() will figure out whether we'll have to

 * insert at the tail of the rightmost leaf.

 *

 * This should also work against the root extent list for tree's with 0

 * depth. If we consider the root extent list to be the rightmost leaf node

 * then the logic here makes sense.

 Were all records empty? */

/*

 * Helper function called at the beginning of an insert.

 *

 * This computes a few things that are commonly used in the process of

 * inserting into the btree:

 *   - Whether the new extent is contiguous with an existing one.

 *   - The current tree depth.

 *   - Whether the insert is an appending one.

 *   - The total # of free records in the tree.

 *

 * All of the information is stored on the ocfs2_insert_type

 * structure.

		/*

		 * If we have tree depth, we read in the

		 * rightmost extent block ahead of time as

		 * ocfs2_figure_insert_type() and ocfs2_add_branch()

		 * may want it later.

	/*

	 * Unless we have a contiguous insert, we'll need to know if

	 * there is room left in our allocation tree for another

	 * extent record.

	 *

	 * XXX: This test is simplistic, we can search for empty

	 * extent records too.

	/*

	 * In the case that we're inserting past what the tree

	 * currently accounts for, ocfs2_find_path() will return for

	 * us the rightmost tree path. This is accounted for below in

	 * the appending code.

	/*

	 * Now that we have the path, there's two things we want to determine:

	 * 1) Contiguousness (also set contig_index if this is so)

	 *

	 * 2) Are we doing an append? We can trivially break this up

         *     into two types of appends: simple record append, or a

         *     rotate inside the tail leaf.

	/*

	 * The insert code isn't quite ready to deal with all cases of

	 * left contiguousness. Specifically, if it's an insert into

	 * the 1st record in a leaf, it will require the adjustment of

	 * cluster count on the last record of the path directly to it's

	 * left. For now, just catch that case and fool the layers

	 * above us. This works just fine for tree_depth == 0, which

	 * is why we allow that above.

	/*

	 * Ok, so we can simply compare against last_eb to figure out

	 * whether the path doesn't exist. This will only happen in

	 * the case that we're doing a tail append, so maybe we can

	 * take advantage of that information somehow.

		/*

		 * Ok, ocfs2_find_path() returned us the rightmost

		 * tree path. This might be an appending insert. There are

		 * two cases:

		 *    1) We're doing a true append at the tail:

		 *	-This might even be off the end of the leaf

		 *    2) We're "appending" by rotating in the tail

/*

 * Insert an extent into a btree.

 *

 * The caller needs to update the owning btree's cluster count.

 Finally, we can add clusters. This might rotate the tree for us. */

/*

 * Allcate and add clusters into the extent b-tree.

 * The new clusters(clusters_to_add) will be inserted at logical_offset.

 * The extent b-tree's root is specified by et, and

 * it is not limited to the file storage. Any extent tree can use this

 * function if it implements the proper ocfs2_extent_tree.

	/* there are two cases which could cause us to EAGAIN in the

	 * we-need-more-metadata case:

	 * 1) we haven't reserved *any*

	 * 2) we are so fragmented, we've needed to add metadata too

 reserve our write early -- insert_extent may update the tree root */

	/*

	 * Store a copy of the record on the stack - it might move

	 * around as the tree is manipulated below.

		/*

		 * Left/right split. We fake this as a right split

		 * first and then make a second pass as a left split.

/*

 * Split part or all of the extent record at split_index in the leaf

 * pointed to by path. Merge with the contiguous extent record if needed.

 *

 * Care is taken to handle contiguousness so as to not grow the tree.

 *

 * meta_ac is not strictly necessary - we only truly need it if growth

 * of the tree is required. All other cases will degrade into a less

 * optimal tree layout.

 *

 * last_eb_bh should be the rightmost leaf block for any extent

 * btree. Since a split may grow the tree or a merge might shrink it,

 * the caller cannot trust the contents of that buffer after this call.

 *

 * This code is optimized for readability - several passes might be

 * made over certain portions of the tree. All of those blocks will

 * have been brought into cache (and pinned via the journal), so the

 * extra overhead is not expressed in terms of disk reads.

	/*

	 * The core merge / split code wants to know how much room is

	 * left in this allocation tree, so we pass the

	 * rightmost extent list.

/*

 * Change the flags of the already-existing extent at cpos for len clusters.

 *

 * new_flags: the flags we want to set.

 * clear_flags: the flags we want to clear.

 * phys: the new physical offset we want this new extent starts from.

 *

 * If the existing extent is larger than the request, initiate a

 * split. An attempt will be made at merging with adjacent extents.

 *

 * The caller is responsible for passing down meta_ac if we'll need it.

/*

 * Mark the already-existing extent at cpos as written for len clusters.

 * This removes the unwritten extent flag.

 *

 * If the existing extent is larger than the request, initiate a

 * split. An attempt will be made at merging with adjacent extents.

 *

 * The caller is responsible for passing down meta_ac if we'll need it.

	/*

	 * XXX: This should be fixed up so that we just re-insert the

	 * next extent records.

	/*

	 * Setup the record to split before we grow the tree.

 extend credit for ocfs2_remove_rightmost_path */

		/*

		 * Check whether this is the rightmost tree record. If

		 * we remove all of this record or part of its right

		 * edge then an update of the record lengths above it

		 * will be required.

		/*

		 * Changing the leftmost offset (via partial or whole

		 * record truncate) of an interior (or rightmost) path

		 * means we have to update the subtree that is formed

		 * by this leaf and the one to it's left.

		 *

		 * There are two cases we can skip:

		 *   1) Path is the leftmost one in our btree.

		 *   2) The leaf is rightmost and will be empty after

		 *      we remove the extent record - the rotate code

		 *      knows how to update the newly formed edge.

			/*

			 * We skip the edge update if this path will

			 * be deleted by the rotate code.

 Remove leftmost portion of the record. */

 Remove rightmost portion of the record */

 Caller should have trapped this. */

	/*

	 * XXX: Why are we truncating to 0 instead of wherever this

	 * affects us?

	/*

	 * We have 3 cases of extent removal:

	 *   1) Range covers the entire extent rec

	 *   2) Range begins or ends on one edge of the extent rec

	 *   3) Range is in the middle of the extent rec (no shared edges)

	 *

	 * For case 1 we remove the extent rec and left rotate to

	 * fill the hole.

	 *

	 * For case 2 we just shrink the existing extent rec, with a

	 * tree update if the shrinking edge is also the edge of an

	 * extent block.

	 *

	 * For case 3 we do a right split to turn the extent rec into

	 * something case 2 can handle.

		/*

		 * The split could have manipulated the tree enough to

		 * move the record location, so we have to look for it again.

		/*

		 * Double check our values here. If anything is fishy,

		 * it's easier to catch it at the top level.

/*

 * ocfs2_reserve_blocks_for_rec_trunc() would look basically the

 * same as ocfs2_lock_alloctors(), except for it accepts a blocks

 * number to reserve some extra blocks, and it only handles meta

 * data allocations.

 *

 * Currently, only ocfs2_remove_btree_range() uses it for truncating

 * and punching holes.

 No records, nothing to coalesce */

	/* tl_bh is loaded from ocfs2_truncate_log_init().  It's validated

	 * by the underlying call to ocfs2_read_inode_block(), so any

 Caller should have known to flush before calling us. */

		/*

		 * Move index back to the record we are coalescing with.

		 * ocfs2_truncate_log_can_coalesce() guarantees nonzero

		/* Caller has given us at least enough credits to

		/* if start_blk is not set, we ignore the record as

 Expects you to already be holding tl_inode->i_mutex */

	/* tl_bh is loaded from ocfs2_truncate_log_init().  It's validated

	 * by the underlying call to ocfs2_read_inode_block(), so any

	/* Appending truncate log(TA) and flushing truncate log(TF) are

	 * two separated transactions. They can be both committed but not

	 * checkpointed. If crash occurs then, both two transaction will be

	 * replayed with several already released to global bitmap clusters.

	 * Then truncate log will be replayed resulting in cluster double free.

		/* We want to push off log flushes while truncates are

/*

 * Try to flush truncate logs if we can free enough clusters from it.

 * As for return value, "< 0" means error, "0" no space and "1" means

 * we have freed enough spaces and let the caller try to allocate again.

	/*

	 * Check whether we can succeed in allocating if we free

	 * the truncate log.

/* called during the 1st stage of node recovery. we stamp a clean

 * truncate log and pass back a copy for processing later. if the

 * truncate log does not require processing, a *tl_copy is set to

	/* tl_bh is loaded from ocfs2_get_truncate_log_info().  It's

	 * validated by the underlying call to ocfs2_read_inode_block(),

		/*

		 * Assuming the write-out below goes well, this copy will be

		 * passed back to recovery for processing.

		/* All we need to do to clear the truncate log is set

	/* ocfs2_truncate_log_shutdown keys on the existence of

	 * osb->osb_tl_inode so we don't set any of the osb variables

/*

 * Delayed de-allocation of suballocator blocks.

 *

 * Some sets of block de-allocations might involve multiple suballocator inodes.

 *

 * The locking for this can get extremely complicated, especially when

 * the suballocator inodes to delete from aren't known until deep

 * within an unrelated codepath.

 *

 * ocfs2_extent_block structures are a good example of this - an inode

 * btree could have been grown by any number of nodes each allocating

 * out of their own suballoc inode.

 *

 * These structures allow the delay of block de-allocation until a

 * later time, when locking of multiple cluster inodes won't cause

 * deadlock.

/*

 * Describe a single bit freed from a suballocator.  For the block

 * suballocators, it represents one block.  For the global cluster

 * allocator, it represents some clusters and free_bit indicates

 * clusters number.

 Premature exit may have left some dangling items. */

 Premature exit may have left some dangling items. */

	/* If we can't find any free list matching preferred slot, just use

	 * the first one.

 Return Value 1 indicates empty */

/* If extent was deleted from tree due to extent rotation and merging, and

 * no metadata is reserved ahead of time. Try to reuse some extents

 * just deleted. This is only used to reuse extent blocks.

 * It is supposed to find enough extent blocks in dealloc if our estimation

 * on metadata is accurate.

	/* If extent tree doesn't have a dealloc, this is not faulty. Just

	 * tell upper caller dealloc can't provide any block and it should

	 * ask for alloc to claim more space.

 Prefer to use local slot */

		/* If no more block can be reused, we should claim more

		 * from alloc. Just return here normally.

		/* We can't guarantee that buffer head is still cached, so

		 * polutlate the extent block again.

		/* We'll also be dirtied by the caller, so

		 * this isn't absolutely necessary.

	/*

	 * Need to set the buffers we zero'd into uptodate

	 * here if they aren't - ocfs2_map_page_blocks()

	 * might've skipped some

/*

 * Zero partial cluster for a hole punch or truncate. This avoids exposing

 * nonzero data on subsequent file extends.

 *

 * We need to call this before i_size is updated on the inode because

 * otherwise block_write_full_page() will skip writeout of pages past

 * i_size.

	/*

	 * File systems which don't support sparse files zero on every

	 * extend.

	/*

	 * Avoid zeroing pages fully beyond current i_size. It is pointless as

	 * underlying blocks of those pages should be already zeroed out and

	 * page writeback will skip them anyway.

	/*

	 * Tail is a hole, or is marked unwritten. In either case, we

	 * can count on read and write to return/push zero's.

	/*

	 * Initiate writeout of the pages we zero'd here. We don't

	 * wait on them - the truncate_inode_pages() call later will

	 * do that for us.

	/*

	 * We clear the entire i_data structure here so that all

	 * fields can be properly initialized.

		/*

		 * Save two copies, one for insert, and one that can

		 * be changed by ocfs2_map_and_dirty_page() below.

		/*

		 * This should populate the 1st page for us and mark

		 * it up to date.

		/*

		 * An error at this point should be extremely rare. If

		 * this proves to be false, we could always re-build

		 * the in-inode data from our pages.

/*

 * It is expected, that by the time you call this function,

 * inode->i_size and fe->i_size have been adjusted.

 *

 * WARNING: This will kfree the truncate context

	/*

	 * Check that we still have allocation to delete.

	/*

	 * Truncate always works against the rightmost tree branch.

	/*

	 * By now, el will point to the extent list on the bottom most

	 * portion of this tree. Only the tail record is considered in

	 * each pass.

	 *

	 * We handle the following cases, in order:

	 * - empty extent: delete the remaining branch

	 * - remove the entire record

	 * - remove a partial record

	 * - no record needs to be removed (truncate has completed)

		/*

		 * Lower levels depend on this never happening, but it's best

		 * to check it up here before changing the tree.

		/*

		 * Truncate entire record.

		/*

		 * Partial truncate. it also should be

		 * the last truncate we're doing.

		/*

		 * Truncate completed, leave happily.

	/*

	 * The check above will catch the case where we've truncated

	 * away all allocation.

/*

 * 'start' is inclusive, 'end' is not.

 No need to punch hole beyond i_size. */

	/*

	 * No need to worry about the data page here - it's been

	 * truncated already and inline data doesn't need it for

	 * pushing zero's to disk, so we'll let readpage pick it up

	 * later.

	/*

	 * For the first cluster group, the gd->bg_blkno is not at the start

	 * of the group, but at an offset from the start. If we add it while

	 * calculating discard for first group, we will wrongly start fstrim a

	 * few blocks after the desried start block and the range can cross

	 * over into the next cluster group. So, add it only if this is not

	 * the first cluster group.

	/*

	 * Do some check before trim the first group.

		/*

		 * Determine first and last group to examine based on

		 * start and len

	/*

	 * If all the groups trim are not done or failed, but we should release

	 * main_bm related locks for avoiding the current IO starve, then go to

	 * trim the next group

 Avoid sending duplicated trim to a shared device */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * userdlm.c

 *

 * Code which implements the kernel side of a minimal userspace

 * interface to our DLM.

 *

 * Many of the functions here are pared down versions of dlmglue.c

 * functions.

 *

 * Copyright (C) 2003, 2004 Oracle.  All rights reserved.

 I heart container_of... */

/* WARNING: This function lives in a world where the only three lock

 * levels are EX, PR, and NL. It *will* have to be adjusted when more

 we're downconverting. */

	/* The teardown flag gets set early during the unlock process,

	 * so test the cancel flag to make sure that this ast isn't

		/* We tried to cancel a convert request, but it was

		 * already granted. Don't clear the busy flag - the

 Cancel succeeded, we want to re-queue */

		lockres->l_requested = DLM_LOCK_IV; /* cancel an

						    * upconvert

		/* we want the unblock thread to look at it again

/*

 * This is the userdlmfs locking protocol version.

 *

 * See fs/ocfs2/dlmglue.c for more details on locking versions.

	/* notice that we don't clear USER_LOCK_BLOCKED here. If it's

	/* It's valid to get here and no longer be blocked - if we get

	 * several basts in a row, we might be queued by the first

	 * one, the unblock thread might run and clear the queued

	 * flag, and finally we might get another bast which re-queues

	/* If there are still incompat holders, we can exit safely

	 * without worrying about re-queueing this lock as that will

 yay, we can downconvert now. */

 need lock downconvert request now... */

/* predict what lock level we'll be dropping down to on behalf

 * of another node, and return true if the currently wanted

	/* We only compare against the currently granted level

	 * here. If the lock is blocked waiting on a downconvert,

		/* is someone sitting in dlm_lock? If so, wait on

		/* is the lock is currently blocked on behalf of

 call dlm_lock to upgrade lock now */

 should have been checked before getting here. */

 We ignore recovery events */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * dlmfs.c

 *

 * Code which implements the kernel side of a minimal userspace

 * interface to our DLM. This file handles the virtual file system

 * used for communication with userspace. Credit should go to ramfs,

 * which was a template for the fs side of this module.

 *

 * Copyright (C) 2003, 2004 Oracle.  All rights reserved.

 Simple VFS hooks based on: */

/*

 * Resizable simple ram filesystem for Linux.

 *

 * Copyright (C) 2000 Linus Torvalds.

 *               2000 Transmeta Corp.

/*

 * These are the ABI capabilities of dlmfs.

 *

 * Over time, dlmfs has added some features that were not part of the

 * initial ABI.  Unfortunately, some of these features are not detectable

 * via standard usage.  For example, Linux's default poll always returns

 * EPOLLIN, so there is no way for a caller of poll(2) to know when dlmfs

 * added poll support.  Instead, we provide this list of new capabilities.

 *

 * Capabilities is a read-only attribute.  We do it as a module parameter

 * so we can discover it whether dlmfs is built in, loaded, or even not

 * loaded.

 *

 * The ABI features are local to this machine's dlmfs mount.  This is

 * distinct from the locking protocol, which is concerned with inter-node

 * interaction.

 *

 * Capabilities:

 * - bast	: EPOLLIN against the file descriptor of a held lock

 *		  signifies a bast fired on the lock.

/*

 * decodes a set of open flags into a valid lock level and a set of flags.

 * returns < 0 if we have invalid flags

 * flags which mean something to us:

 * O_RDONLY -> PRMODE level

 * O_WRONLY -> EXMODE level

 *

 * O_NONBLOCK -> NOQUEUE

	/* We don't want to honor O_APPEND at read/write time as it

		/* this is a strange error to return here but I want

		 * to be able userspace to be able to distinguish a

		 * valid lock request from one that simply couldn't be

/*

 * We do ->setattr() just to override size changes.  Our size is the size

 * of the LVB and nothing else.

 don't write past the lvb */

	/* we must be a directory. If required, lets unregister the

		/* for now we don't support anything other than

		/* released at clear_inode time, this insures that we

		 * get to drop the dlm reference on each lock *before*

		 * we call the unregister code for releasing parent

		/* directory inodes start off with i_nlink ==

/*

 * File creation. Allocate an inode, and we're done..

 SMP-safe */

 verify that we have a proper domain */

 Extra count - pin the dentry in core */

	/* verify name is valid and doesn't contain any dlm reserved

 Extra count - pin the dentry in core */

	/* if there are no current holders, or none that are waiting

 this way we can restrict mkdir to only the toplevel of the fs. */

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) 2005 Oracle.  All rights reserved.

/* This quorum hack is only here until we transition to some more rational

 * approach that is driven from userspace.  Honest.  No foolin'.

 *

 * Imagine two nodes lose network connectivity to each other but they're still

 * up and operating in every other way.  Presumably a network timeout indicates

 * that a node is broken and should be recovered.  They can't both recover each

 * other and both carry on without serialising their access to the file system.

 * They need to decide who is authoritative.  Now extend that problem to

 * arbitrary groups of nodes losing connectivity between each other.

 *

 * So we declare that a node which has given up on connecting to a majority

 * of nodes who are still heartbeating will fence itself.

 *

 * There are huge opportunities for races here.  After we give up on a node's

 * connection we need to wait long enough to give heartbeat an opportunity

 * to declare the node as truly dead.  We also need to be careful with the

 * race between when we see a node start heartbeating and when we connect

 * to it.

 *

 * So nodes that are in this transtion put a hold on the quorum decision

 * with a counter.  As they fall out of this transition they drop the count

 * and if they're the last, they fire off the decision.

/* this is horribly heavy-handed.  It should instead flip the file

	/* panic spins with interrupts enabled.  with preempt

/* Indicate that a timeout occurred on a heartbeat region write. The

 * other nodes in the cluster may consider us dead at that time so we

 * want to "fence" ourselves so that we don't scribble on the disk

 * after they think they've recovered us. This can't solve all

 * problems related to writeout after recovery but this hack can at

 * least close some of those gaps. When we have real fencing, this can

 * go away as our node would be fenced externally before other nodes

		/* the odd numbered cluster case is straight forward --

		/* the even numbered cluster adds the possibility of each half

		 * of the cluster being able to talk amongst themselves.. in

		 * that case we're hosed if we can't talk to the group that has

/* as a node comes up we delay the quorum decision until we know the fate of

 * the connection.  the hold will be droped in conn_up or hb_down.  it might be

 * perpetuated by con_err until hb_down.  if we already have a conn, we might

/* hb going down releases any holds we might have had due to this node from

/* this tells us that we've decided that the node is still heartbeating

 * even though we've lost it's conn.  it must only be called after conn_err

 * and indicates that we must now make a quorum decision in the future,

 * though we might be doing so after waiting for holds to drain.  Here

/* This is analogous to hb_up.  as a node's connection comes up we delay the

 * quorum decision until we see it heartbeating.  the hold will be droped in

 * hb_up or hb_down.  it might be perpetuated by con_err until hb_down.  if

 * it's already heartbeating we might be dropping a hold that conn_up got.

/* we've decided that we won't ever be connecting to the node again.  if it's

 * still heartbeating we grab a hold that will delay decisions until either the

 * node stops heartbeating from hb_down or the caller decides that the node is

 SPDX-License-Identifier: GPL-2.0-only

/*

 * sys.c

 *

 * OCFS2 cluster sysfs interface

 *

 * Copyright (C) 2005 Oracle.  All rights reserved.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2004, 2005 Oracle.  All rights reserved.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) 2004 Oracle.  All rights reserved.

 *

 * ----

 *

 * Callers for this were originally written against a very simple synchronus

 * API.  This implementation reflects those simple callers.  Some day I'm sure

 * we'll need to move to a more robust posting/callback mechanism.

 *

 * Transmit calls pass in kernel virtual addresses and block copying this into

 * the socket's tx buffers via a usual blocking sendmsg.  They'll block waiting

 * for a failed socket to timeout.  TX callers can also pass in a poniter to an

 * 'int' which gets filled with an errno off the wire in response to the

 * message they send.

 *

 * Handlers for unsolicited messages are registered.  Each socket has a page

 * that incoming data is copied into.  First the header, then the data.

 * Handlers are called from only one thread with a reference to this per-socket

 * page.  This page is destroyed after the handler call, so it can't be

 * referenced beyond the call.  Handlers may block but are discouraged from

 * doing so.

 *

 * Any framing errors (bad magic, large payload lengths) close a connection.

 *

 * Our sock_container holds the state we associate with a socket.  It's current

 * framing state is held there as well as the refcounting we do around when it

 * is safe to tear down the socket.  The socket is only finally torn down from

 * the container when the container loses all of its references -- so as long

 * as you hold a ref on the container you can trust that the socket is valid

 * for use with kernel socket APIs.

 *

 * Connections are initiated between a pair of nodes when the node with the

 * higher node number gets a heartbeat callback which indicates that the lower

 * numbered node has started heartbeating.  The lower numbered node is passive

 * and only accepts the connection if the higher numbered node is heartbeating.

/*

 * In the following two log macros, the whitespace after the ',' just

 * before ##args is intentional. Otherwise, gcc 2.95 will eat the

 * previous token if args expands to nothing.

 XXX someday we'll need better accounting */

/*

 * listen work is only queued by the listening socket callbacks on the

 * o2net_wq.  teardown detaches the callbacks before destroying the workqueue.

 * quorum work is queued as sock containers are shutdown.. stop_listening

 * tears down all the node's sock containers, preventing future shutdowns

 * and queued quroum work, before canceling delayed quorum work and

 * destroying the work queue.

 can't quite avoid *all* internal declarations :/ */

 CONFIG_DEBUG_FS */

 CONFIG_DEBUG_FS */

 CONFIG_OCFS2_FS_STATS */

 Just in case we mess up the translation table above */

 ------------------------------------------------------------ */

 ------------------------------------------------------------ */

 pin the node item of the remote node */

 ------------------------------------------------------------ */

	/* the node num comparison and single connect/accept path should stop

 mirrors o2net_tx_can_proceed() */

	/* trigger the connecting worker func as long as we're not valid,

	 * it will back off if it shouldn't connect.  This can be called

	 * from node config teardown and so needs to be careful about

		/* delay if we're within a RECONNECT_DELAY of the

		/*

		 * Delay the expired work after idle timeout.

		 *

		 * We might have lots of failed connection attempts that run

		 * through here but we only cancel the connect_expired work when

		 * a connection attempt succeeds.  So only the first enqueue of

		 * the connect_expired work will do anything.  The rest will see

		 * that it's already queued and do nothing.

 keep track of the nn's sc ref for the caller */

 see o2net_register_callbacks() */

 see o2net_register_callbacks() */

 ignore connecting sockets as they make progress */

/*

 * we register callbacks so we can queue work on events before calling

 * the original callbacks.  our callbacks our careful to test user_data

 * to discover when they've reaced with o2net_unregister_callbacks().

 accepted sockets inherit the old listen socket data ready */

/*

 * this is a little helper that is called by callers who have seen a problem

 * with an sc and want to detach it from the nn if someone already hasn't beat

 * them to it.  if an error is given then the shutdown will be persistent

 * and pending transmits will be canceled.

/*

 * This work queue function performs the blocking parts of socket shutdown.  A

 * few paths lead here.  set_nn_state will trigger this callback if it sees an

 * sc detached from the nn.  state_change will also trigger this callback

 * directly when it sees errors.  In that case we need to call set_nn_state

 * ourselves as state_change couldn't get the nn_lock and call set_nn_state

 * itself.

 drop the callbacks ref and call shutdown only once */

		/* we shouldn't flush as we're in the thread, the

	/* not fatal so failed connects before the other guy has our

 ------------------------------------------------------------ */

/* max_len is protection for the handler func.  incoming messages won't

	/* the tree and list get this ref.. they're both removed in

 we've had some trouble with handlers seemingly vanishing. */

 ------------------------------------------------------------ */

 should be smarter, I bet */

 Get a map of all nodes to which this node is currently connected to */

	/* finally, convert the message header to network byte-order

 wait on other node's handler */

	/* Note that we avoid overwriting the callers status return

	 * variable if a system error was reported on the other

 must be before dropping sc and node */

	/* leave other fields intact from the incoming message, msg_num

 twiddle the magic

 hdr has been in host byteorder this whole time */

/* this returns -errno if the header was unknown or too large, etc.

 special type for returning message status */

 find a handler for it */

 this destroys the hdr, so don't use it after this */

 don't bother reconnecting if its the wrong version. */

	/*

	 * Ensure timeouts are consistent with other nodes, otherwise

	 * we can end up with one node thinking that the other must be down,

	 * but isn't. This can ultimately cause corruption.

	/* set valid and queue the idle timers only if it hasn't been

 shift everything up as though it wasn't there */

/* this demuxes the queued rx bytes into header or payload bits and calls

 * handlers as each full message is read off the socket.  it returns -error,

 do we need more header? */

			/* only swab incoming here.. we can

			 * only get here once as we cross from

 oof, still don't have a header */

 this was swabbed above when we first read it */

 do we need more payload? */

 need more payload */

		/* we can only get here once, the first time we read

		 * the payload.. so set ret to progress if the handler

/* this work func is triggerd by data ready.  it reads until it can read no

 * more.  it interprets 0, eof, as fatal.  if data_ready hits while we're doing

 not permanent so read failed handshake can retry */

 ------------------------------------------------------------ */

/* called when a connect completes and after a sock is accepted.  the

 this is called as a work_struct func. */

/* socket shutdown does a del_timer_sync against this as it tears down.

 * we can't start this timer until we've got to the point in sc buildup

	/* idle timerout happen, don't shutdown the connection, but

	 * make fence decision. Maybe the connection can recover before

	 * the decision is made.

 clear fence decision since the connection recover from timeout*/

 Only push out an existing timer */

/* this work func is kicked whenever a path sets the nn state which doesn't

 * have valid set.  This includes seeing hb come up, losing a connection,

 * having a connect attempt fail, etc. This centralizes the logic which decides

 * if a connect attempt should be made or if we should give up and all future

	/*

	 * sock_create allocates the sock with GFP_KERNEL. We must

	 * prevent the filesystem from being reentered by memory reclaim.

 if we're greater we initiate tx, otherwise we accept */

 watch for racing with tearing a node down */

	/*

	 * see if we already have one pending or have given up.

	 * For nn_timeout, it is set when we close the connection

	 * because of the idle time out. So it means that we have

	 * at least connected to that node successfully once,

	 * now try to connect to it again.

 freed by sc_kref_release */

 any port */

 handshake completion will set nn->nn_sc_valid */

		/* 0 err so that another will be queued and attempted

 ------------------------------------------------------------ */

 don't reconnect until it's heartbeating again */

 ensure an immediate connect attempt */

		/* believe it or not, accept and node heartbeating testing

		 * can succeed for this node before we got here.. so

		 * only use set_nn_state to clear the persistent error

 ------------------------------------------------------------ */

	/*

	 * sock_create_lite allocates the sock with GFP_KERNEL. We must

	 * prevent the filesystem from being reentered by memory reclaim.

	/* this happens all the time when the other node sees our heartbeat

/*

 * This function is invoked in response to one or more

 * pending accepts at softIRQ level. We must drain the

 * entire que before returning.

	/*

	 * It is critical to note that due to interrupt moderation

	 * at the network driver level, we can't assume to get a

	 * softIRQ for every single conn since tcp SYN packets

	 * can arrive back-to-back, and therefore many pending

	 * accepts may result in just 1 softIRQ. If we terminate

	 * the o2net_accept_one() loop upon seeing an err, what happens

	 * to the rest of the conns in the queue? If no new SYN

	 * arrives for hours, no softIRQ  will be delivered,

	 * and the connections will just sit in the queue.

 check for teardown race */

	/* This callback may called twice when a new connection

	 * is  being established as a child socket inherits everything

	 * from a parent LISTEN socket, including the data_ready cb of

	 * the parent. This leads to a hazard. In o2net_accept_one()

	 * we are still initializing the child socket but have not

	 * changed the inherited data_ready callback yet when

	 * data starts arriving.

	 * We avoid this hazard by checking the state.

	 * For the listening socket,  the state will be TCP_LISTEN; for the new

	 * socket, will be  TCP_ESTABLISHED. Also, in this case,

	 * sk->sk_user_data is not a valid function pointer.

/*

 * called from node manager when we should bring up our network listening

 * socket.  node manager handles all the serialization to only call this

 * once and to match it with o2net_stop_listening().  note,

 * o2nm_this_node() doesn't work yet as we're being called while it

 * is being set up.

 ? */

/* again, o2nm_this_node() doesn't work here as we're involved in

 stop the listening socket from generating work */

 finish all work and tear down the work queue */

 ------------------------------------------------------------ */

 until we see hb from a node we'll return einval */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2004, 2005 Oracle.  All rights reserved.

/* for now we operate under the assertion that there can be only one

 * cluster active at a time.  Changing this will require trickling

 O2NM_FENCE_RESET */

 O2NM_FENCE_PANIC */

 node configfs bits */

	/* through the first node_set .parent

	/* once we're in the cl_nodes tree networking can look us up by

	 * node number and try to use our address and port attributes

	 * to connect to this node.. make sure that they've been set

 XXX */

 boolean of whether this node wants to be local */

	/* setting local turns on networking rx for now so we require having

 XXX */

	/* the only failure case is trying to set a new local node

 bring up the rx thread if we're setting the new local node. */

 node set */

 some stuff? */

 use item.ci_namebuf instead? */

 XXX call into net to stop this node from trading messages */

 XXX sloppy */

 nd_num might be 0 if the node number hasn't been set.. */

 cluster */

 cluster set */

 some stuff? */

	/* this runs under the parent dir's i_mutex; there can be only

 XXX sync with hb callbacks and shut down hb? */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2004, 2005 Oracle.  All rights reserved.

/*

 * The first heartbeat pass had one global thread that would serialize all hb

 * callback calls.  This global serializing sem should only be removed once

 * we've made sure that all callees can deal with being called concurrently

 * from multiple hb region threads.

/*

 * multiple hb threads are watching multiple regions.  A node is live

 * whenever any of the threads sees activity from the node in its region.

/*

 * In global heartbeat, we maintain a series of region bitmaps.

 * 	- o2hb_region_bitmap allows us to limit the region number to max region.

 * 	- o2hb_live_region_bitmap tracks live regions (seen steady iterations).

 * 	- o2hb_quorum_region_bitmap tracks live regions that have seen all nodes

 * 		heartbeat on it.

 * 	- o2hb_failed_region_bitmap tracks the regions that have seen io timeouts.

 O2HB_HEARTBEAT_LOCAL */

 O2HB_HEARTBEAT_GLOBAL */

/*

 * o2hb_dependent_users tracks the number of registered callbacks that depend

 * on heartbeat. o2net and o2dlm are two entities that register this callback.

 * However only o2dlm depends on the heartbeat. It does not want the heartbeat

 * to stop while a dlm domain is still active.

/*

 * In global heartbeat mode, all regions are pinned if there are one or more

 * dependent users and the quorum region count is <= O2HB_PIN_CUT_OFF. All

 * regions are unpinned if the region count exceeds the cut off or the number

 * of dependent users falls to zero.

/*

 * In local heartbeat mode, we assume the dlm domain name to be the same as

 * region uuid. This is true for domains created for the file system but not

 * necessarily true for userdlm domains. This is a known limitation.

 *

 * In global heartbeat mode, we pin/unpin all o2hb regions. This solution

 * works for both file system and userdlm domains.

/* Only sets a new threshold if there are no active regions.

 *

 * No locking or otherwise interesting code is required for reading

 * o2hb_dead_threshold as it can't change once regions are active and

/* each thread owns a region.. when we're asked to tear down the region

 protected by the hr_callback_sem */

 live node map of this region */

	/* let the person setting up hb wait for it to return until it

	 * has reached a 'steady' state.  This will be fixed when we have

	/* terminate o2hb thread if it does not reach steady state

	/* randomized as the region goes up and down so that a node

 negotiate timer, used to negotiate extending hb timeout. */

	/* Used during o2hb_check_slot to hold a copy of the block

	 * being checked because we temporarily have to zero out the

 Message key for negotiate timeout message. */

 last hb status, 0 for success, other value for error. */

		/*

		 * Fence if the number of failed regions >= half the number

		 * of  quorum regions

 Arm writeout only after thread reaches steady state */

 negotiate timeout must be less than write timeout. */

	/* don't negotiate timeout if last hb failed since it is very

	 * possible io failed. Should let write timeout fence self.

 lowest node as master node to make negotiate decision. */

			/* check negotiate bitmap every second to do timeout

			 * approve decision.

 approve negotiate timeout request. */

 negotiate timeout with master node. */

 Used in error paths too */

	/* sadly atomic_sub_and_test() isn't available on all platforms.  The

/* Setup a Bio to cover I/O against num_slots slots starting at

	/* Testing has shown this allocation to take long enough under

	 * GFP_KERNEL that the local node can get fenced. It would be

	 * nicest if we could pre-allocate these bios and avoid this

 Must put everything in 512 byte sectors for the bio... */

	/* We want to compute the block crc with a 0 value in the

	 * hb_cksum field. Save it off here and replace after the

/*

 * Compare the slot data with what we wrote in the last iteration.

 * If the match fails, print an appropriate error message. This is to

 * detect errors like... another node hearting on the same slot,

 * flaky device that is losing writes, etc.

 * Returns 1 if check succeeds, 0 otherwise.

 Don't check on our 1st timestamp */

 TODO: time stuff */

 This step must always happen last! */

 Will run the list in order until we process the passed event */

	/* Holding callback sem assures we don't alter the callback

	 * lists when doing this, and serializes ourselves with other

		/* We should *never* have gotten on to the list with a

		 * bad type... This isn't something that we should try

 Prevent race with o2hb_heartbeat_group_drop_item() */

 Tag region as quorum only after thread reaches steady state */

	/*

	 * A region can be added to the quorum only when it sees all

	 * live nodes heartbeat on it. In other words, the region has been

	 * added to all nodes.

	/*

	 * If global heartbeat active, unpin all regions if the

	 * region count > CUT_OFF

	/*

	 * If a node is no longer configured but is still in the livemap, we

	 * may need to clear that bit from the livemap.

		/* all paths from here will drop o2hb_live_lock for

		/* Don't print an error on the console in this case -

		 * a freshly formatted heartbeat area will not have a

		/* The node is live but pushed out a bad crc. We

		 * consider it a transient miss but don't populate any

	/* we don't care if these wrap.. the state transitions below

	/* The node changed heartbeat generations. We assume this to

	 * mean it dropped off but came back before we timed out. We

	 * want to consider it down for the time being but don't want

	 * to lose any changed_samples state we might build up to

	/* dead nodes only come to life after some number of

 first on the list generates a callback */

		/* We want to be sure that all nodes agree on the

		 * number of milliseconds before a node will be

		 * considered dead. The self-fencing timeout is

		 * computed from this value, and a discrepancy might

		 * result in heartbeat calling a node dead when it

 TODO: Perhaps we can fail the region here. */

 if the list is dead, we're done.. */

	/* live nodes only go dead after enough consequtive missed

	 * samples..  reset the missed counter whenever we see

 last off the live_slot generates a callback */

 node can be null */

		/* We don't clear this because the node is still

	/*

	 * If a node is not configured but is in the livemap, we still need

	 * to read the slot so as to be able to remove it from the livemap.

	/* No sense in reading the slots of nodes that don't exist

	 * yet. Of course, if the node definitions have holes in them

	 * then we're reading an empty slot anyway... Consider this

	/* With an up to date view of the slots, we can check that no

	 * other node has been improperly configured to heartbeat in

 fill in the proper info for our next heartbeat */

	/*

	 * We have to be sure we've advertised ourselves on disk

	 * before we can go to steady state.  This ensures that

	 * people we find in our steady state have seen us.

		/* Do not re-arm the write timeout on I/O error - we

		 * can't be sure that the new block ever made it to

 Skip disarming the timeout if own slot has stale/bad data */

 let the person who launched us know when things are steady */

/*

 * we ride the region ref that the region dir holds.  before the region

 * dir is removed and drops it ref it will wait to tear down this

 * thread.

 Pin node */

		/* We track the time spent inside

		 * o2hb_do_disk_heartbeat so that we avoid more than

		 * hr_timeout_ms between disk writes. On busy systems

		 * this should result in a heartbeat which is less

			/* the kthread api has blocked signals for us so no

 unclean stop is only used in very bad situation */

	/* Explicit down notification - avoid forcing the other nodes

	 * to timeout on this region when we could just as easily

	 * write a clear generation - thus indicating to them that

	 * this node has left this region.

 Unpin node */

 max_nodes should be the largest bitmap we pass here */

 If 0, it has never been set before */

 CONFIG_DEBUG_FS */

 if we're already in a callback then we're already serialized by the sem */

/*

 * get a map of all nodes that are heartbeating in any regions

	/* callers want to serialize this map and callbacks so that they

/*

 * heartbeat configfs bits.  The heartbeat set is a default set under

 * the cluster set in nodemanager.c.

/* drop_item only drops its ref after killing the thread, nothing should

 * be using the region anymore.  this has to clean up any state that

 Heartbeat and fs min / max block sizes are the same. */

/* Read in all the slots available and populate the tracking

 * structures so that we can start with a baseline idea of what's

	/* We only want to get an idea of the values initially in each

	 * slot, so we do no verification - o2hb_check_slot will

	 * actually determine if each configured slot is valid and

		/* Only fill the values that o2hb_check_slot uses to

 this is acting as commit; we set up all of hr_bdev and hr_task or nothing */

	/* We can't heartbeat without having had our node number

 Generation of zero is invalid */

	/*

	 * A node is considered live after it has beat LIVE_THRESHOLD

	 * times.  We're not steady until we've given them a chance

	 * _after_ our first read.

	 * The default threshold is bare minimum so as to limit the delay

	 * during mounts. For global heartbeat, the threshold doubled for the

	 * first region.

 unsteady_iterations is triple the steady_iterations */

 Ok, we were woken.  Make sure it wasn't by drop_item() */

 heartbeat set */

 some stuff? */

	/* this is the same way to generate msg key as dlm, for local heartbeat,

	 * name is also the same, so make initial crc value different to avoid

	 * message key conflict.

 stop the thread when the user removes the region dir */

	/*

	 * If we're racing a dev_write(), we need to wake them.  They will

	 * check reg->hr_task

	/*

	 * If global heartbeat active and there are dependent users,

	 * pin all regions if quorum region count <= CUT_OFF

 this will validate ranges for us. */

/* this is just here to avoid touching group in heartbeat.h which the

 hb callback registration and issuing */

/*

 * In local heartbeat mode, region_uuid passed matches the dlm domain name.

 * In global heartbeat mode, region_uuid passed is NULL.

 *

 * In local, we only pin the matching region. In global we pin all the active

 * regions.

 local heartbeat */

 Ignore ENOENT only for local hb (userdlm domain) */

/*

 * In local heartbeat mode, region_uuid passed matches the dlm domain name.

 * In global heartbeat mode, region_uuid passed is NULL.

 *

 * In local, we only unpin the matching region. In global we unpin all the

 * active regions.

 local heartbeat */

	/*

	 * if global heartbeat active and this is the first dependent user,

	 * pin all regions if quorum region count <= CUT_OFF

 local heartbeat */

	/*

	 * if global heartbeat active and there are no dependent users,

	 * unpin all quorum regions

 XXX Can this happen _with_ a region reference? */

/*

 * this is just a hack until we get the plumbing which flips file systems

 * read only and drops the hb ref instead of killing the node dead.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * netdebug.c

 *

 * debug functionality for o2net

 *

 * Copyright (C) 2005, 2008 Oracle.  All rights reserved.

 discover the head of the list */

 use st_task to detect real nsts in the list */

 unused, just needs to be null when done */

 get_task_comm isn't exported.  oh well. */

 discover the head of the list miscast as a sc */

 use sc_page to detect real scs in the list */

 unused, just needs to be null when done */

 So that debugfs.ocfs2 can determine which format is being used */

 the stack's structs aren't sparse endian clean */

	/* XXX sigh, inet-> doesn't have sparse annotation so any

 CONFIG_DEBUG_FS */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * dlmrecovery.c

 *

 * recovery stuff

 *

 * Copyright (C) 2004 Oracle.  All rights reserved.

 Worker function used during recovery. */

		/* already have ref on dlm to avoid having

		/* this is allowed to sleep and

/*

 * RECOVERY THREAD

	/* wake the recovery thread

	 * this will wake the reco thread in one of three places

	 * 1) sleeping with no recovery happening

	 * 2) sleeping with recovery mastered elsewhere

 Launch the recovery thread */

/*

 * this is lame, but here's how recovery works...

 * 1) all recovery threads cluster wide will work on recovering

 *    ONE node at a time

 * 2) negotiate who will take over all the locks for the dead node.

 *    thats right... ALL the locks.

 * 3) once a new master is chosen, everyone scans all locks

 *    and moves aside those mastered by the dead guy

 * 4) each of these locks should be locked until recovery is done

 * 5) the new master collects up all of secondary lock queue info

 *    one lock at a time, forcing each node to communicate back

 *    before continuing

 * 6) each secondary lock queue responds with the full known lock info

 * 7) once the new master has run all its locks, it sends a ALLDONE!

 *    message to everyone

 * 8) upon receiving this message, the secondary queue node unlocks

 *    and responds to the ALLDONE

 * 9) once the new master gets responses from everyone, he unlocks

 *    everything and recovery for this dead node is done

 *10) go back to 2) while there are still dead nodes

 *

 do not sleep, recheck immediately. */

 returns true when the recovery master has contacted us */

/* returns true if node is no longer in the domain

/* returns true if node is no longer in the domain

/* callers of the top-level api calls (dlmlock/dlmunlock) should

 * block on the dlm->reco.event when recovery is in progress.

 * the dlm recovery thread will set this state when it begins

 * recovering a dead node (as the new master or not) and clear

 * the state and wake as soon as all affected lock resources have

 check to see if the new master has died */

 unset the new_master, leave dead_node */

 select a target to recover */

 BUG? */

 mlog(0, "nothing to recover!  sleeping now!\n");

 return to main thread loop and sleep. */

 take write barrier */

 (stops the list reshuffling thread, proxy ast handling) */

		/* choose a new master, returns 0 if this node

		 * is the master, -EEXIST if it's another node.

		 * this does not return until a new master is chosen

 already notified everyone.  go. */

	/* it is safe to start everything back up here

	 * because all of the dead node's lock resources

 sleep out in main dlm_recovery_thread loop. */

 we should never hit this anymore */

		/* yield a bit to allow any final network messages

 success!  see if any other nodes need recovery */

 continue and look for another dead node */

		/* we have become recovery master.  there is no escaping

	/* safe to access the node data list without a lock, since this

 node died, ignore it for recovery */

					/* wait for the domain map to catch up

 -ENOMEM on the other node */

				/* fine.  don't need this node's info.

	/* nodes should be sending reco data now

		/* check all the nodes now to see if we are

			/* Set this flag on recovery master to avoid

			 * a new recovery for another dead node start

			 * before the recovery is not done. That may

			/* all nodes are now in DLM_RECO_NODE_DATA_DONE state

	 		 * just send a finalize message to everyone and

 rescan everything marked dirty along the way */

		/* wait to be signalled, with periodic timeout

	/* nodes can only be removed (by dying) after dropping

 send message

 negative status is handled by caller */

 return from here, then

 sleep until all received or error

 this is a hack */

 this will get freed by dlm_request_all_locks_worker */

 queue up work for dlm_request_all_locks_worker */

 get an extra ref for the work item */

		/* worker could have been created before the recovery master

	/* lock resources should have already been moved to the

 	 * dlm->reco.resources list.  now move items from that list

 	 * to a temp list if the dead owner matches.  note that the

	 * whole cluster recovers only one node at a time, so we

	 * can safely move UNKNOWN lock resources for each recovery

 now we can begin blasting lockreses without the dlm lock */

	/* any errors returned will be due to the new_master dying,

 move the resources back to the list */

 should have moved beyond INIT but not to FINALIZE yet */

			/* these states are possible at this point, anywhere along

 wake the recovery thread, some node is done */

		/* always prune any $RECOVERY entries for dead nodes,

					/* Can't schedule DLM_UNLOCK_FREE_LOCK

 add an all-done flag if we reached the last lock */

 send it */

		/* XXX: negative status is not handled.

 might get an -ENOMEM back here */

 zero and reinit the message buffer */

 mres here is one full page */

 Ignore lvb in all locks in the blocked list */

 Only consider lvbs in locks with granted EX or PR lock levels */

 Ensure the lvb copied for migration matches in other valid locks */

/* returns 1 if this lock fills the network structure,

 we reached the max, send this network message */

 rare, but possible */

 add another lock. */

			/* this filled the lock message,

 send a dummy lock to indicate a mastery reference only */

 flush any remaining locks */

/*

 * this message will contain no more than one page worth of

 * recovery data, and it will work on only one lockres.

 * there may be many locks in this page, and we may need to wait

 * for additional packets to complete all the locks (rare, but

 * possible).

/*

 * NOTE: the allocation error cases here are scary

 * we really cannot afford to fail an alloc in recovery

 * do we spin?  returning an error only delays the problem really

 cannot migrate a lockres with no master */

	/* lookup the lock to see if we have a secondary queue for this

	 * already...  just add the locks in and this will have its owner

 this will get a ref on res */

 mark it as recovering/migrating and hash it */

				/* this is at least the second

 caller should BUG */

		/* need to allocate, just like if it was

		/* to match the ref that we would have gotten if

 mark it as recovering/migrating and hash it */

		/* Add an extra ref for this lock-less lockres lest the

		 * dlm_thread purges it before we get the chance to add

		/* There are three refs that need to be put.

		 * 1. Taken above.

		 * 2. kref_init in dlm_new_lockres()->dlm_init_lockres().

		 * 3. dlm_lookup_lockres()

		 * The first one is handled at the end of this function. The

		 * other two are handled in the worker thread after locks have

		 * been attached. Yes, we don't wait for purge time to match

		 * kref_init. The lockres will still have atleast one ref

		/* now that the new lockres is inserted,

	/* at this point we have allocated everything we need,

	 * and we have a hashed lockres with an extra ref and

	/* drop this either when master requery finds a different master

 migration cannot have an unknown master */

		/* take a reference now to pin the lockres, drop it

 queue up work for dlm_mig_lockres_worker */

 get an extra ref for the work item */

 copy the whole message */

 already have a ref */

 One extra ref taken needs to be put here */

		/* this case is super-rare. only occurs if

 cannot touch this lockres */

 See comment in dlm_mig_lockres_handler() */

	/* we only reach here if one of the two nodes in a

	 * migration died while the migration was in progress.

	 * at this point we need to requery the master.  we

	 * know that the new_master got as far as creating

	 * an mle on at least one node, but we do not know

	 * if any nodes had actually cleared the mle and set

	 * the master to the new_master.  the old master

	 * is supposed to set the owner to UNKNOWN in the

	 * event of a new_master death, so the only possible

	 * responses that we can get from nodes here are

	 * that the master is new_master, or that the master

	 * is UNKNOWN.

	 * if all nodes come back with UNKNOWN then we know

	 * the lock needs remastering here.

	 * if any node comes back with a valid master, check

	 * to see if that master is the one that we are

	 * recovering.  if so, then the new_master died and

	 * we need to remaster this lock.  if not, then the

	 * new_master survived and that node will respond to

	 * other nodes about the owner.

	 * if there is an owner, this node needs to dump this

	 * lockres and alert the sender that this lockres

 do not send to self */

			/* host is down, so answer for that node would be

/* this function cannot error, so unless the sending

 * or receiving of the message failed, the owner can

		/* since the domain has gone away on this

 sender will take care of this and retry */

 put.. incase we are not the master */

/* TODO: do ast flush business

 * TODO: do MIGRATING and RECOVERING spinning

/*

* NOTE about in-flight requests during migration:

*

* Before attempting the migrate, the master has marked the lockres as

* MIGRATING and then flushed all of its pending ASTS.  So any in-flight

* requests either got queued before the MIGRATING flag got set, in which

* case the lock data will reflect the change and a return message is on

* the way, or the request failed to get in before MIGRATING got set.  In

* this case, the caller will be told to spin and wait for the MIGRATING

* flag to be dropped, then recheck the master.

* This holds true for the convert, cancel and unlock cases, and since lvb

* updates are tied to these same messages, it applies to lvb updates as

* well.  For the lock case, there is no way a lock can be on the master

* queue and not be on the secondary queue since the lock is always added

* locally first.  This means that the new target node will never be sent

* a lock that he doesn't already have on the list.

* In total, this means that the local lock is correct and should not be

* updated to match the one sent by the master.  Any messages sent back

* from the master before the MIGRATING flag will bring the lock properly

* up-to-date, and the change will be ordered properly for the waiter.

* We will *not* attempt to modify the lock underneath the waiter.

 placeholder, just need to set the refmap bit */

		/* if the lock is for the local node it needs to

		 * be moved to the proper location within the queue.

 MIGRATION ONLY! */

			/* lock is always created locally first, and

			/* see NOTE above about why we do not update

 move the lock to its proper place */

 do not alter lock refcount.  switching lists. */

 lock is for another node. */

		/*

		 * If the lock is in the blocked list it can't have a valid lvb,

		 * so skip it

				/* other node was trying to update

				 * lvb when node died.  recreate the

				/* the lock resource lvb update must happen

				 * NOW, before the spinlock is dropped.

				 * we no longer wait for the AST to update

				/* otherwise, the node is sending its

		/* NOTE:

		 * wrt lock queue ordering and recovery:

		 *    1. order of locks on granted queue is

		 *       meaningless.

		 *    2. order of locks on converting queue is

		 *       LOST with the node death.  sorry charlie.

		 *    3. order of locks on the blocked queue is

		 *       also LOST.

		 * order of locks does not affect integrity, it

		 * just means that a lock request may get pushed

		 * back in line as a result of the node death.

		 * also note that for a given node the lock order

		 * for its secondary queue locks is preserved

		 * relative to each other, but clearly *not*

		 * preserved relative to locks from other nodes.

				/* newlock is doing downconvert, add it to the

 balance the ref taken when the work was queued */

 We need to hold a reference while on the recovery list */

 find any pending locks and put them back on proper list */

 move converting lock back to granted */

 remove pending lock requests completely */

				/* lock will be floating until ref in

				 * dlmlock_remote is freed after the network

				 * call returns.  ok for it to not be on any

				 * list since no ast can be called

				/* if an unlock was in progress, treat as

				 * if this had completed successfully

				 * before sending this lock state to the

				 * new master.  note that the dlm_unlock

				 * call is still responsible for calling

				 * the unlockast.  that will happen after

				 * the network call times out.  for now,

				 * just move lists to prepare the new

				/* if a cancel was in progress, treat as

				 * if this had completed successfully

				 * before sending this lock state to the

/* removes all recovered locks from the recovery list.

 * sets the res->owner to the new master.

			/* new_master has our reference from

	/* this will become unnecessary eventually, but

	 * for now we need to run the whole hash, clear

	 * the RECOVERING state and set the owner

			/* new_master has our reference from

		/* if this node owned the lockres, and if the dead node

		/* if this is a secondary lockres, and we had no EX or PR

 check local state for valid lvb */

 zero the lksb lvb and lockres lvb */

	/* this node is the lockres master:

	 * 1) remove any stale locks for the dead node

	 * 2) if the dead node had an EX when he died, blank out the lvb

	/* We do two dlm_lock_put(). One for removing from list and the other is

 TODO: check pending_asts, pending_basts here */

 Can't schedule DLM_UNLOCK_FREE_LOCK - do manually */

 Can't schedule DLM_UNLOCK_FREE_LOCK - do manually */

 Can't schedule DLM_UNLOCK_FREE_LOCK - do manually */

 do not kick thread yet */

 purge any stale mles */

	/*

	 * now clean up all lock resources.  there are two rules:

	 *

	 * 1) if the dead node was the master, move the lockres

	 *    to the recovering list.  set the RECOVERING flag.

	 *    this lockres needs to be cleaned up before it can

	 *    be used further.

	 *

	 * 2) if this node was the master, remove all locks from

	 *    each of the lockres queues that were owned by the

	 *    dead node.  once recovery finishes, the dlm thread

	 *    can be kicked again to see if any ASTs or BASTs

	 *    need to be fired as a result.

 			/* always prune any $RECOVERY entries for dead nodes,

						/* Can't schedule

						 * DLM_UNLOCK_FREE_LOCK

 zero the lvb if necessary */

			/* finalize1 was reached, so it is safe to clear

			 * the new_master and dead_node.  that recovery

 Clean up join state on node death. */

 check to see if the node is already considered dead */

 check to see if we do not care about this node */

		/* This also catches the case that we get a node down

 make sure local cleanup occurs before the heartbeat events */

 notify anything attached to the heartbeat events */

	/* wake up migration waiters if a node goes down.

	/*

	 * This will notify any dlm users that a node in our domain

	 * went away without notifying us first.

	/* do NOT notify mle attached to the heartbeat events.

/*

 * dlm_pick_recovery_master will continually attempt to use

 * dlmlock() on the special "$RECOVERY" lockres with the

 * LKM_NOQUEUE flag to get an EX.  every thread that enters

 * this function on each node racing to become the recovery

 * master will not stop attempting this until either:

 * a) this node gets the EX (and becomes the recovery master),

 * or b) dlm->reco.new_master gets set to some nodenum

 * != O2NM_INVALID_NODE_NUM (another node will do the reco).

 * so each time a recovery master is needed, the entire cluster

 * will sync at this point.  if the new master dies, that will

		/* got the EX lock.  check to see if another node

 see if recovery was already finished elsewhere */

		/* if this node has actually become the recovery master,

 this always succeeds */

 set the new_master to this node */

		/* recovery lock is a special case.  ast will not get fired,

			/* this would really suck. this could only happen

			 * if there was a network error during the unlock

			 * because of node death.  this means the unlock

			 * is actually "done" and the lock structure is

			 * even freed.  we can continue, but only

		/* another node is master. wait on

		 * reco.new_master != O2NM_INVALID_NODE_NUM

 another node has informed this one that it is reco master */

 dlmlock returned something other than NOTQUEUED or NORMAL */

 negative status is handled ok by caller here */

			/* node is down.  not involved in recovery

		/*

		 * Prior to commit aad1b15310b9bcd59fa81ab8f2b1513b59553ea8,

		 * dlm_begin_reco_handler() returned EAGAIN and not -EAGAIN.

		 * We are handling both for compatibility reasons.

			/* this is now a serious problem, possibly ENOMEM

			/* sleep for a bit in hopes that we can avoid

 ok to return 0, domain has gone away */

 may not have seen the new master as dead yet */

		/* force the recovery cleanup in __dlm_hb_node_down

				/* this has no effect on this recovery

				 * session, so set the status to zero to

 reset the node_iter back to the top and send finalize2 */

 ok to return 0, domain has gone away */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * dlmdebug.c

 *

 * debug functionality for the dlm

 *

 * Copyright (C) 2004, 2008 Oracle.  All rights reserved.

/* NOTE: This function converts a lockname into a string. It uses knowledge

 * of the format of the lockname that should be outside the purview of the dlm.

 * We are adding only to make dlm debugging slightly easier.

 *

 * For more on lockname formats, please refer to dlmglue.c and ocfs2_lockid.h.

 begin - utils funcs */

 end - util funcs */

 begin - purge list funcs */

 end - purge list funcs */

 begin - debug mle funcs */

 end - debug mle funcs */

 begin - debug lockres funcs */

 refmap */

 lvb */

 granted */

 converting */

 blocked */

 passed to seq_show */

 end - debug lockres funcs */

 begin - debug state funcs */

 Domain: xxxxxxxxxx  Key: 0xdfbac769 */

 Thread Pid: xxx  Node: xxx  State: xxxxx */

 Number of Joins: xxx  Joining Node: xxx */

 Domain Map: xx xx xx */

 Exit Domain Map: xx xx xx */

 Live Map: xx xx xx */

 Lock Resources: xxx (xxx) */

 MLEs: xxx (xxx) */

  Blocking: xxx (xxx) */

  Mastery: xxx (xxx) */

  Migration: xxx (xxx) */

 Lists: Dirty=Empty  Purge=InUse  PendingASTs=Empty  ... */

 Purge Count: xxx  Refs: xxx */

 Dead Node: xxx */

 What about DLM_RECO_STATE_FINALIZE? */

 Recovery Pid: xxxx  Master: xxx  State: xxxx */

 Recovery Map: xx xx */

 Recovery Node State: */

 end  - debug state funcs */

 files in subroot */

 for dumping dlm_ctxt */

 for dumping lockres */

 for dumping mles */

 for dumping lockres on the purge list */

 subroot - domain dir */

 debugfs root */

 CONFIG_DEBUG_FS */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * dlmlock.c

 *

 * underlying calls for lock creation

 *

 * Copyright (C) 2004 Oracle.  All rights reserved.

/* Tell us whether we can grant a new lock request.

 * locking:

 *   caller needs:  res->spinlock

 *   taken:         none

 *   held on exit:  none

 * returns: 1 if the lock can be granted, 0 otherwise.

/* performs lock creation at the lockres master site

 * locking:

 *   caller needs:  none

 *   taken:         takes and drops res->spinlock

 *   held on exit:  none

 * returns: DLM_NORMAL, DLM_NOTQUEUED

	/* if called from dlm_create_lock_handler, need to

 erf.  state changed after lock was dropped. */

 got it right away */

		/* for the recovery lock, we can't allow the ast

		 * to be queued since the dlmthread is already

		 * frozen.  but the recovery lock is always locked

		 * with LKM_NOQUEUE so we do not need the ast in

		/* for NOQUEUE request, unless we get the

 either queue the ast or release it */

 remove from local queue if it failed */

/*

 * locking:

 *   caller needs:  none

 *   taken:         takes and drops res->spinlock

 *   held on exit:  none

 * returns: DLM_DENIED, DLM_RECOVERING, or net status

	/*

	 * Wait if resource is getting recovered, remastered, etc.

	 * If the resource was remastered and new owner is self, then exit.

 add lock to local (secondary) queue */

	/* spec seems to say that you will get DLM_NORMAL when the lock

			/* recovery lock was mastered by dead node.

			 * we need to have calc_usage shoot down this

			/*

			 * DO NOT call calc_usage, as this would unhash

			 * the remote lockres before we ever get to use

			 * it.  treat as if we never made any change to

			 * the lockres.

		/* special case for the $RECOVERY lock.

		 * there will never be an AST delivered to put

		 * this lock on the proper secondary queue

/* for remote lock creation.

 * locking:

 *   caller needs:  none, but need res->state & DLM_LOCK_RES_IN_PROGRESS

 *   taken:         none

 *   held on exit:  none

 * returns: DLM_NOLOCKMGR, or net status

 associate a lock with it's lockres, getting a ref on the lockres */

 drop ref on lockres, if there is still one associated with lock */

 zero memory only if kernel-allocated */

/* handler for lock creation net message

 * locking:

 *   caller needs:  none

 *   taken:         takes and drops res->spinlock

 *   held on exit:  none

 * returns: DLM_NORMAL, DLM_SYSERR, DLM_IVLOCKID, DLM_NOTQUEUED

 fetch next node-local (u8 nodenum + u56 cookie) into u64 */

 shift single byte of node num into top 8 bits */

	/* yes this function is a mess.

	 * TODO: clean this up.  lots of common code in the

 CONVERT request */

 if converting, must pass in a valid dlm_lock */

		/* XXX: for ocfs2 purposes, the ast/bast/astdata/lksb are

	 	 * static after the original lock call.  convert requests will

		 * ensure that everything is the same, or return DLM_BADARGS.

	 	 * this means that DLM_DENIED_NOASTS will never be returned.

			/* for now, see how this works without sleeping

			 * and just retry right away.  I suspect the reco

			 * or migration will complete fast enough that

 LOCK request */

 find or create the lock resource */

			/* LVB requests for non PR, PW or EX locks are

				/* wait to see the node go down, then

				 * drop down and allow the lockres to

 Inflight taken in dlm_get_lock_resource() is dropped here */

 this is kind of unnecessary

	/* put lockres ref from the convert path

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * dlmunlock.c

 *

 * underlying calls for unlocking locks

 *

 * Copyright (C) 2004 Oracle.  All rights reserved.

/*

 * according to the spec:

 * http://opendlm.sourceforge.net/cvsmirror/opendlm/docs/dlmbook_final.pdf

 *

 *  flags & LKM_CANCEL != 0: must be converting or blocked

 *  flags & LKM_CANCEL == 0: must be granted

 *

 * So to unlock a converting lock, you must first cancel the

 * convert (passing LKM_CANCEL in flags), then call the unlock

 * again (with no LKM_CANCEL in flags).

/*

 * locking:

 *   caller needs:  none

 *   taken:         res->spinlock and lock->spinlock taken and dropped

 *   held on exit:  none

 * returns: DLM_NORMAL, DLM_NOLOCKMGR, status from network

 * all callers should have taken an extra ref on lock coming in

	/* We want to be sure that we're not freeing a lock

 ok for this to sleep if not in a network handler */

	/* see above for what the spec says about

 By now this has been masked out of cancel requests. */

 make the final update to the lvb */

			flags |= LKM_PUT_LVB; /* let the send function

 drop locks and send message */

		/* if the master told us the lock was already granted,

			/* must clear the actions because this unlock

			 * is about to be retried.  cannot free or do

	/* get an extra ref on lock.  if we are just switching

 remove the extra ref on lock */

		/* Unlock request will directly succeed after owner dies,

		 * and the lock is already removed from grant list. We have to

		 * wait for RECOVERING done or we miss the chance to purge it

		 * since the removement is much faster than RECOVERING proc.

 let the caller's final dlm_lock_put handle the actual kfree */

 this should always be coupled with list removal */

 if cancel or unlock succeeded, lvb work is done */

	/* leave DLM_LKSB_PUT_LVB on the lksb so any final

/*

 * locking:

 *   caller needs:  none

 *   taken:         none

 *   held on exit:  none

 * returns: DLM_NORMAL, DLM_NOLOCKMGR, status from network

		/* ended up trying to contact ourself.  this means

		 * that the lockres had been remote but became local

 extra data to send if we are updating lvb */

 successfully sent and received

			/* NOTE: this seems strange, but it is what we want.

			 * when the master goes down during a cancel or

			 * unlock, the recovery code completes the operation

			 * as if the master had not died, then passes the

			 * updated state to the recovery master.  this thread

			 * just needs to finish out the operation and call

 something bad.  this will BUG in ocfs2 */

/*

 * locking:

 *   caller needs:  none

 *   taken:         takes and drops res->spinlock

 *   held on exit:  none

 * returns: DLM_NORMAL, DLM_BADARGS, DLM_IVLOCKID,

 *          return value from dlmunlock_master

		/* We assume here that a no lock resource simply means

		 * it was migrated away and destroyed before the other

 scan granted -> converting -> blocked queues */

 lock was found on queue */

 unlockast only called on originating node */

	/* if this is in-progress, propagate the DLM_FORWARD

 cancel this outright */

 cancel the request, put back on granted */

 too late, already granted. */

 unlock request */

 unlock granted lock */

/* there seems to be no point in doing this async

 * since (even for the remote case) there is really

 * no work to queue up... so just do it and fire the

 need to retry up here because owner may have changed */

		/* We want to go away for a tiny bit to allow recovery

		 * / migration to complete on this resource. I don't

		 * know of any wait queue we could sleep on as this

		 * may be happening on another node. Perhaps the

		 * proper solution is to queue up requests on the

 do we want to yield(); ?? */

			/* it is possible that there is one last bast

			 * pending.  make sure it is flushed, then

			 * call the unlockast.

			 * not an issue if this is a mastered remotely,

			 * since this lock has been removed from the

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * dlmmod.c

 *

 * standalone DLM module

 *

 * Copyright (C) 2004 Oracle.  All rights reserved.

		case -EINVAL:   /* if returned from our tcp code,

/*

 * MASTER LIST FUNCTIONS

/*

 * regarding master list entries and heartbeat callbacks:

 *

 * in order to avoid sleeping and allocation that occurs in

 * heartbeat, master list entries are simply attached to the

 * dlm's established heartbeat callbacks.  the mle is attached

 * when it is created, and since the dlm->spinlock is held at

 * that time, any heartbeat event will be properly discovered

 * by the mle.  the mle needs to be detached from the

 * dlm->mle_hb_events list as soon as heartbeat events are no

 * longer useful to the mle, and before the mle is freed.

 *

 * as a general rule, heartbeat events are no longer needed by

 * the mle once an "answer" regarding the lock master has been

 * received.

 remove from list and free */

		/* this may or may not crash, but who cares.

 must not have any spinlocks coming in */

 copy off the node_map and register hb callbacks on our copy */

 attach the mle to the domain node up/down events */

 returns 1 if found, 0 if not */

 remove from list if not already */

 detach the mle from the domain node up/down events */

	/* NOTE: kfree under spinlock here.

/*

 * LOCK RESOURCE FUNCTIONS

	/* This should not happen -- all lockres' have a name

	/* By the time we're ready to blow this guy away, we shouldn't

	/* If we memset here, we lose our reference to the kmalloc'd

	 * res->lockname.name, so be sure to init every field

 just for consistency */

/*

 * lookup a lock resource by name.

 * may already exist in the hashtable.

 * lockid is null terminated

 *

 * if not, allocate enough for the lockres and for

 * the temporary structure used in doing the mastering.

 *

 * also, do a lookup in the dlm->master_list to see

 * if another node has begun mastering the same lock.

 * if so, there should be a block entry in there

 * for this name, and we should *not* attempt to master

 * the lock here.   need to wait around for that node

 * to assert_master (or die).

 *

		/*

		 * Right after dlm spinlock was released, dlm_thread could have

		 * purged the lockres. Check if lockres got unhashed. If so

		 * start over.

 Wait on the thread that is mastering the resource */

 Wait on the resource purge to complete before continuing */

 Grab inflight ref to pin the resource */

 nothing found and we need to allocate one. */

		/* caller knows it's safe to assume it's not mastered elsewhere

 lockres still marked IN_PROGRESS */

 check master list to see if another node has started mastering it */

 if we found a block, wait for lock to be mastered by another node */

		/* if there is a migration in progress, let the migration

		 * finish before continuing.  we can wait for the absence

		 * of the MIGRATION mle: either the migrate finished or

		 * one of the nodes died and the mle was cleaned up.

		 * if there is a BLOCK here, but it already has a master

		 * set, we are too late.  the master does not have a ref

		 * for us in the refmap.  detach the mle and drop it.

			/* we arrived too late.  the master does not

 master is known, detach */

			/* this is lame, but we can't wait on either

 go ahead and try to master lock on this node */

 make sure this does not get freed below */

		/* still holding the dlm spinlock, check the recovery map

		 * to see if there are any nodes that still need to be

		 * considered.  these will not appear in the mle nodemap

	/* at this point there is either a DLM_MLE_BLOCK or a

	 * DLM_MLE_MASTER on the master list, so it's safe to add the

	 * lockres to the hashtable.  anyone who finds the lock will

 finally add the lockres to its hash bucket */

 since this lockres is new it doesn't not require the spinlock */

	/* get an extra ref on the mle in case this is a BLOCK

	 * if so, the creator of the BLOCK may try to put the last

	 * ref at this time in the assert master handler, so we

		/* any cluster changes that occurred after dropping the

		 * dlm spinlock would be detectable be a change on the mle,

 must wait for lock to be mastered elsewhere */

 found a master ! */

			/* if our master request has not reached the master

			 * yet, keep going until it does.  this is how the

			 * master will know that asserts are needed back to

 keep going until the response map includes all nodes */

 make sure we never continue without this */

 master is known, detach if not already detached */

 put the extra ref */

 need to free the unused mle */

 check if another node has already become the owner */

		/* this will cause the master to re-assert across

 give recovery a chance to run */

 restart if we hit any errors */

		/* another node has done an assert!

 have all nodes responded? */

				/* my node number is lowest.

			 	 * now tell other nodes that I am

				/* ref was grabbed in get_lock_resource

			/* if voting is done, but we have not received

 sleep if we haven't finished voting yet */

 done */

			/* This is a failure in the network path,

			 * not in the response to the assert_master

			 * (any nonzero response is a BUG on this node).

			 * Most likely a socket just got disconnected

		/* no longer need to restart lock mastery.

 set the lockres owner */

	/* mastery reference obtained either during

 if it was there in the original then this node died */

			/* a node came up.  clear any old vote from

			 * the response map and set it in the vote map

 redo the master request, but only for the new node */

 act like it was never there */

						/* mle is an MLE_BLOCK, but

						 * there is now nothing left to

						 * block on.  we need to return

						 * all the way back out and try

						 * again with an MLE_MASTER.

						 * dlm_do_local_recovery_cleanup

						 * has already run, so the mle

			/* now blank out everything, as if we had never

 reset the vote_map to the current node_map */

 put myself into the maybe map */

/*

 * DLM_MASTER_REQUEST_MSG

 *

 * returns: 0 on success,

 *          -errno on a network error

 *

 * on error, the caller should assume the target node is "dead"

 *

 should never happen */

 this is totally crude */

 not a network error. bad. */

		/* all other errors should be network errors,

 this is also totally crude */

/*

 * locks that can be taken here:

 * dlm->spinlock

 * res->spinlock

 * mle->spinlock

 * dlm->master_list

 *

 * if possible, TRIM THIS DOWN!!!

 take care of the easy cases up front */

		/*

		 * Right after dlm spinlock was released, dlm_thread could have

		 * purged the lockres. Check if lockres got unhashed. If so

		 * start over.

			/* this node is the owner.

			 * there is some extra work that needs to

			 * happen now.  the requesting node has

			 * caused all nodes up to this one to

			 * create mles.  this node now needs to

 mlog(0, "node %u is the master\n", res->owner);

		/* ok, there is no owner.  either this node is

		 * being blocked, or it is actively trying to

 mlog(0, "lockres is in progress...\n");

 mlog(0, "this node is waiting for "

 "lockres to be mastered\n");

 the real master can respond on its own */

				/* this node will be the owner.

				 * go back and clean the mles on any

 mlog(0, "this node is attempting to "

 "master lockres\n");

 keep the mle attached to heartbeat events */

	/*

	 * lockres doesn't exist on this node

	 * if there is an MLE_BLOCK, return NO

	 * if there is an MLE_MASTER, return MAYBE

	 * otherwise, add an MLE_BLOCK, return NO

 this lockid has never been seen on this node yet */

 mlog(0, "no mle found\n");

 mlog(0, "this is second time thru, already allocated, "

 "add the block.\n");

 real master can respond on its own */

 keep the mle attached to heartbeat events */

	/*

	 * __dlm_lookup_lockres() grabbed a reference to this lockres.

	 * The reference is released by dlm_assert_master_worker() under

	 * the call to dlm_dispatch_assert_master().  If

	 * dlm_assert_master_worker() isn't called, we drop it here.

/*

 * DLM_ASSERT_MASTER_MSG

/*

 * NOTE: this can be used for debugging

 * can periodically run all locks owned by this node

 * and re-assert across the cluster...

 note that if this nodemap is empty, it returns 0 */

 a node died.  finish out the rest of the nodes. */

 any nonzero status return will do */

 ok, something horribly messed.  kill thyself. */

/*

 * locks that can be taken here:

 * dlm->spinlock

 * res->spinlock

 * mle->spinlock

 * dlm->master_list

 *

 * if possible, TRIM THIS DOWN!!!

 find the MLE */

 not an error, could be master just re-asserting */

			/* not necessarily an error, though less likely.

				/* with the fix for bug 569, a higher node

				 * number winning the mastery will respond

				 * YES to mastery requests, but this node

	/* ok everything checks out with the MLE

 owner is just re-asserting */

 mle->type == DLM_MLE_MIGRATION */ {

 should only be getting an assert from new master */

 mlog(0, "woo!  got an assert_master from node %u!\n",

 	     assert->node_idx);

			/* MASTER mle: if any bits set in the response map

			 * then the calling node needs to re-assert to clear

		/* master is known, detach if not already detached.

		 * ensures that only one assert_master call will happen

			/* the assert master message now balances the extra

		 	 * ref given by the master / migration request message.

		 	 * if this is the last put, it will be removed

 positive. negative would shoot down the node. */

 let the master know we have a reference to the lockres */

 kill the caller! */

 queue up work for dlm_assert_master_worker */

 already have a ref */

 can optionally ignore node numbers higher than this node */

		/* if is this just to clear up mles for nodes below

		 * this node, do not send the message to the original

	/*

	 * If we're migrating this lock to someone else, we are no

	 * longer allowed to assert out own mastery.  OTOH, we need to

	 * prevent migration from starting while we're still asserting

	 * our dominance.  The reserved ast delays migration.

	/* this call now finishes out the nodemap

 no need to restart, we are done */

 Ok, we've asserted ourselves.  Let's let migration start. */

/* SPECIAL CASE for the $RECOVERY lock used by the recovery thread.

 * We cannot wait for node recovery to complete to begin mastering this

 * lockres because this lockres is used to kick off recovery! ;-)

 * So, do a pre-check on all living nodes to see if any of those nodes

 * think that $RECOVERY is currently mastered by a dead node.  If so,

 * we wait a short time to allow that node to get notified by its own

 * heartbeat stack, then check again.  All $RECOVERY lock resources

 * mastered by dead nodes are purged when the heartbeat callback is

 * fired, so we can know for sure that it is safe to continue once

 do not send to self */

			/* host is down, so answer for that node would be

 check to see if this master is in the recovery map */

/*

 * DLM_DEREF_LOCKRES_MSG

 BAD.  other node says I did not have a ref. */

 ignore the error */

/*

 * A migratable resource is one that is :

 * 1. locally mastered, and,

 * 2. zero local locks, and,

 * 3. one or more non-local locks, or, one or more references

 * Returns 1 if yes, 0 if not.

 delay migration when the lockres is in MIGRATING state */

 delay migration when the lockres is in RECOCERING state */

/*

 * DLM_MIGRATE_LOCKRES

 preallocate up front. if this fails, abort */

	/*

	 * clear any existing master requests and

	 * add the migration mle to the list

	/* get an extra reference on the mle.

	 * otherwise the assert_master from the new

	 * master will destroy this.

	/*

	 * set the MIGRATING flag and flush asts

	 * if we fail after this we need to re-dirty the lockres

 master is known, detach if not already detached */

	/*

	 * at this point, we have a migration target, an mle

	 * in the master list, and the MIGRATING flag set on

	 * the lockres

	/* now that remote nodes are spinning on the MIGRATING flag,

 notify new node and send all lock state */

	/* call send_one_lockres with migration flag.

	 * this serves as notice to the target node that a

 migration failed, detach and clean up mle */

	/* at this point, the target sends a message to all nodes,

	 * (using dlm_do_migrate_request).  this node is skipped since

	 * we had to put an mle in the list to begin the process.  this

	 * node now waits for target to do an assert master.  this node

	 * will be the last one notified, ensuring that the migration

	 * is complete everywhere.  if the target dies while this is

	 * going on, some nodes could potentially see the target as the

	 * master, so it is important that my recovery finds the migration

 wait for new node to assert master */

			/* avoid hang during shutdown when migrating lockres

 migration failed, detach and clean up mle */

 all done, set the owner, clear the flag */

 master is known, detach if not already detached */

 re-dirty the lockres if we failed */

	/* wake up waiters if the MIGRATING flag got set

/*

 * Should be called only after beginning the domain leave process.

 * There should not be any remaining locks on nonlocal lock resources,

 * and there should be no local locks left on locally mastered resources.

 *

 * Called with the dlm spinlock held, may drop it to do migration, but

 * will re-acquire before exit.

 *

 * Returns: 1 if dlm->spinlock was dropped/retaken, 0 if never dropped

 Wheee! Migrate lockres here! Will sleep so drop spinlock. */

	/* target has died, so make the caller break out of the

	/* need to set MIGRATING flag on lockres.  this is done by

	/* strategy is to reserve an extra ast then release

 now flush all the pending asts */

	/* before waiting on DIRTY, block processes which may

 now wait on any pending asts and the DIRTY state */

	/* if the extra ref we just put was the final one, this

	 * will pass thru immediately.  otherwise, we need to wait

 did the target go down or die? */

	/*

	 * if target is down, we need to clear DLM_LOCK_RES_BLOCK_DIRTY for

	 * another try; otherwise, we are sure the MIGRATING state is there,

	 * drop the unneeded state which blocked threads trying to DIRTY

	/*

	 * at this point:

	 *

	 *   o the DLM_LOCK_RES_MIGRATING flag is set if target not down

	 *   o there are no pending asts on this lockres

	 *   o all processes trying to reserve an ast on this

	 *     lockres must wait for the MIGRATING flag to clear

/* last step in the migration process.

 * original master calls this to free all of the dlm_lock

 be extra careful */

				/* In a normal unlock, we would have added a

		/* do not clear the local node reference, if there is a

/*

 * Pick a node to migrate the lock resource to. This function selects a

 * potential target based first on the locks and then on refmap. It skips

 * nodes that are in the process of exiting the domain.

 Go through all the locks */

 Go thru the refmap */

/* this is called by the new master once all lockres

 send message to all nodes, except the master and myself */

 We could race exit domain. If exited, skip. */

			/* during the migration request we short-circuited

			 * the mastery of the lockres.  make sure we have

/* if there is an existing mle for this lockres, we now know who the master is.

 * (the one who sent us *this* message) we can clear it up right away.

 * since the process that put the mle on the list still has a reference to it,

 * we can unhash it now, set the master and wake the process.  as a result,

 * we will have no mle in the list to start with.  now we can add an mle for

 * the migration and this should be the only one found for those scanning the

 preallocate.. if this fails, abort */

 check for pre-existing lock */

			/* if all is working ok, this can only mean that we got

		 	* a migrate request from a node that we now see as

 need a better solution */

 ignore status.  only nonzero status would BUG. */

 master is known, detach if not already detached */

/* must be holding dlm->spinlock and dlm->master_lock

 * when adding a migration mle, we can clear any other mles

 * in the master list because we know with certainty that

 * the master is "master".  so we remove any old mle from

 * the list after setting it's master field, and then add

 * the new migration mle.  this way we can hold with the rule

 caller is responsible for any ref taken here on oldmle */

 ah another process raced me to it */

 bad.  2 NODES are trying to migrate! */

 request: "

 "

 this is essentially what assert_master does */

 remove it so that only one mle will be found */

 now add a migration mle to the tail of the list */

	/* the new master will be sending an assert master for this.

 do this for consistency with other mle types */

/*

 * Sets the owner of the lockres, associated to the mle, to UNKNOWN

 Find the lockres associated to the mle and set its owner to UNK */

 move lockres onto recovery list */

 about to get rid of mle, detach from heartbeat */

 dump the mle */

		/* Must drop the refcount by one since the assert_master will

		 * never arrive. This may result in the mle being unlinked and

		 * freed, but there may still be a process waiting in the

 Do not need events any longer, so detach from heartbeat */

 clean the master list */

			/* MASTER mles are initiated locally. The waiting

			 * process will notice the node map change shortly.

			/* BLOCK mles are initiated by other nodes. Need to

			 * clean up if the dead node would have been the

 Everything else is a MIGRATION mle */

			/* The rule for MIGRATION mles is that the master

			 * becomes UNKNOWN if *either* the original or the new

			 * master dies. All UNKNOWN lockres' are sent to

			 * whichever node becomes the recovery master. The new

			 * master is responsible for determining if there is

			 * still a master for this lockres, or if he needs to

			 * take over mastery. Either way, this node should

			/* If we have reached this point, this mle needs to be

			/* If we find a lockres associated with the mle, we've

			 * hit this rare case that messes up our lock ordering.

			 * If so, we need to drop the master lock so that we can

			 * take the lockres lock, meaning that we will have to

 restart */

 This may be the last reference */

	/* ownership of the lockres is changing.  account for the

	 * mastery reference here since old_master will briefly have

	/* this call now finishes out the nodemap

 no longer need to retry.  all living nodes contacted. */

		/* the only nonzero status here would be because of

 all done, set the owner, clear the flag */

 re-dirty it on the new master */

/*

 * LOCKRES AST REFCOUNT

 * this is integral to migration

/* for future intent to call an ast, reserve one ahead of time.

 * this should be called only after waiting on the lockres

 * with dlm_wait_on_lockres, and while still holding the

/*

 * used to drop the reserved ast, either because it went unused,

 * or because the ast/bast was actually called.

 *

 * also, if there is a pending migration on this lockres,

 * and this was the last pending ast on the lockres,

 * atomically set the MIGRATING flag before we drop the lock.

 * this is how we ensure that migration can proceed with no

 * asts in progress.  note that it is ok if the state of the

 * queues is such that a lock should be granted in the future

 * or that a bast should be fired, because the new master will

 * shuffle the lists on this lockres as soon as it is migrated.

	/*

	 * We notified all other nodes that we are exiting the domain and

	 * marked the dlm state to DLM_CTXT_LEAVING. If any mles are still

	 * around we force free them and wake any processes that are waiting

	 * on the mles

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * dlmast.c

 *

 * AST and BAST functionality for local and remote nodes

 *

 * Copyright (C) 2004 Oracle.  All rights reserved.

/* Should be called as an ast gets queued to see if the new

 * lock level will obsolete a pending bast.

 * For example, if dlm_thread queued a bast for an EX lock that

 * was blocking another EX, but before sending the bast the

 * lock owner downconverted to NL, the bast is now obsolete.

 * Only the ast should be sent.

 * This is needed because the lock and convert paths can queue

 * asts out-of-band (not waiting for dlm_thread) in order to

 old bast already sent, ok */

 EX blocks anything left, any bast still valid */

 NL blocks nothing, no reason to send any bast, cancel it */

 PR only blocks EX */

 putting lock on list, add a ref */

 check to see if this ast obsoletes the bast */

		/* removing lock from list, remove a ref.  guaranteed

		 * this won't be the last ref because of the get above,

		/* free up the reserved bast that we are cancelling.

		 * guaranteed that this will not be the last reserved

		 * ast because *both* an ast and a bast were reserved

		 * to get to this point.  the res->spinlock will not be

 putting lock on list, add a ref */

 only updates if this node masters the lockres */

 check the lksb flags for the direction */

		/* Do nothing for lvb put requests - they should be done in

 		 * place when the lock is downconverted - otherwise we risk

 		 * racing gets and puts which could result in old lvb data

 		 * being propagated. We leave the put flag set and clear it

 		 * here. In the future we might want to clear it at the time

 		 * the put is actually done.

 reset any lvb flags on the lksb */

	/* lock request came from another node

 cannot get a proxy ast message if this node owns it */

 try convert queue for both ast/bast */

 if not on convert, try blocked for ast, granted for bast */

 if lock is found but unlock is pending ignore the bast */

 do not alter lock refcount.  switching lists. */

 should already be there....

 if we requested the lvb, fetch it into our lksb now */

 ignore it */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * dlmdomain.c

 *

 * defines domain join / leave apis

 *

 * Copyright (C) 2004 Oracle.  All rights reserved.

/*

 * ocfs2 node maps are array of long int, which limits to send them freely

 * across the wire due to endianness issues. To workaround this, we convert

 * long ints to byte arrays. Following 3 routines are helper functions to

 * set/test/copy bits within those array of bytes

/*

 *

 * spinlock lock ordering: if multiple locks are needed, obey this ordering:

 *    dlm_domain_lock

 *    struct dlm_ctxt->spinlock

 *    struct dlm_lock_resource->spinlock

 *    struct dlm_ctxt->master_lock

 *    struct dlm_ctxt->ast_lock

 *    dlm_master_list_entry->spinlock

 *    dlm_lock->spinlock

 *

/*

 * The supported protocol version for DLM communication.  Running domains

 * will have a negotiated version with the same major number and a minor

 * number equal or smaller.  The dlm_ctxt->dlm_locking_proto field should

 * be used to determine what a running domain is actually using.

 *

 * New in version 1.1:

 *	- Message DLM_QUERY_REGION added to support global heartbeat

 *	- Message DLM_QUERY_NODEINFO added to allow online node removes

 * New in version 1.2:

 * 	- Message DLM_BEGIN_EXIT_DOMAIN_MSG added to mark start of exit domain

 * New in version 1.3:

 *	- Message DLM_DEREF_LOCKRES_DONE added to inform non-master that the

 *	  refmap is cleared

 get a reference for our hashtable */

/* intended to be called by functions which do not care about lock

 * resources which are being purged (most net _handler functions).

 * this will return NULL for any lock resource which is found but

 * currently in the process of dropping its mastery reference.

 * use __dlm_lookup_lockres_full when you need the lock resource

	/* tmp->name here is always NULL terminated,

 For null terminated domain strings ONLY */

/* returns true on one of two conditions:

 * 1) the domain does not exist

/* A little strange - this function will be called while holding

 * dlm_domain_lock and is expected to be holding it on the way out. We

 we may still be in the list if we hit an error during join. */

/* given a questionable reference to a dlm object, gets a reference if

 * it can find it in the list, otherwise returns NULL in which case

	/* We've left the domain. Now we can take ourselves out of the

	 * list and allow the kref stuff to help us free the

 Wake up anyone waiting for us to remove this domain */

			/* migrate, if necessary.  this will drop the dlm

	/* let the dlm thread take care of purging, keep scanning until

	/* Yikes, a double spinlock! I need domain_lock for the dlm

 notify anything attached to the heartbeat events */

 Support for begin exit domain was added in 1.2 */

	/*

	 * Unlike DLM_EXIT_DOMAIN_MSG, DLM_BEGIN_EXIT_DOMAIN_MSG is purely

	 * informational. Meaning if a node does not receive the message,

	 * so be it.

	/* At this point we've migrated away all our locks and won't

	 * accept mastership of new ones. The dlm is responsible for

	 * almost nothing now. We make sure not to confuse any joining

 Clear ourselves from the domain map */

		/* Drop the dlm spinlock. This is safe wrt the domain_map.

		 * -nodes cannot be added now as the

		 *   query_join_handlers knows to respond with OK_NO_MAP

		 * -we catch the right network errors if a node is

		 *   removed from the map while we're sending him the

			/* Not sure what to do here but lets sleep for

			 * a bit in case this was a transient

		/* If we're not clearing the node bit then we intend

		/* We mark it "in shutdown" now so new register

		 * requests wait until we've completely left the

		 * domain. Don't use DLM_CTXT_LEAVING yet as we still

		 * want new domain joins to communicate with us at

		 * least until we've completed migration of our

 We changed dlm state, notify the thread */

 Give dlm_thread time to purge the lockres' */

 This list should be empty. If not, print remaining lockres */

/*

 * struct dlm_query_join_packet is made up of four one-byte fields.  They

 * are effectively in big-endian order already.  However, little-endian

 * machines swap them before putting the packet on the wire (because

 * query_join's response is a status, and that status is treated as a u32

 * on the wire).  Thus, a big-endian and little-endian machines will treat

 * this structure differently.

 *

 * The solution is to have little-endian machines swap the structure when

 * converting from the structure to the u32 representation.  This will

 * result in the structure having the correct format on the wire no matter

 * the host endian format.

	/*

	 * If heartbeat doesn't consider the node live, tell it

	 * to back off and try again.  This gives heartbeat a chance

	 * to catch up.

	/*

	 * There is a small window where the joining node may not see the

	 * node(s) that just left but still part of the cluster. DISALLOW

	 * join request if joining node has different node map.

	/* Once the dlm ctxt is marked as leaving then we don't want

	 * to be put in someone's domain map.

	 * Also, explicitly disallow joining at certain troublesome

			/*If this is a brand new context and we

			 * haven't started our join process yet, then

 Disallow parallel joins. */

			/* Alright we're fully a part of this domain

			 * so we keep some state as to who's joining

			 * and indicate to him that needs to be fixed

 Make sure we speak compatible locking protocols.  */

 XXX should we consider no dlm ctxt an error? */

		/* Alright, this node has officially joined our

		 * domain. Set him in the map and clean up our

 notify anything attached to the heartbeat events */

 compare local regions with remote */

 compare remote with local regions */

 if local hb, the numregions will be zero */

 buffer used in dlm_mast_regions() */

 Support for global heartbeat was added in 1.1 */

 Support for node query was added in 1.1 */

		/* Yikes, this guy wants to cancel his join. No

 map_size should be in bytes. */

 copy live node map to join message */

	/* -ENOPROTOOPT from the net code means the other side isn't

	    listening for our message type -- that's fine, it means

	    his dlm isn't up, so we can consider him a 'yes' but not

 Use the same locking protocol as the remote node */

 Reset response to JOIN_DISALLOW */

			/* It is very important that this message be

			 * received so we spin until either the node

 give us some time between errors... */

	/* For now, we restart the process if the node maps have

	/* group sem locking should work for us here -- we're already

	 * registered for heartbeat events so filling this should be

		/* Ok, either we got a response or the node doesn't have a

	/* Yay, everyone agree's we can join the domain. My domain is

	 * comprised of all nodes who were put in the

	 * yes_resp_map. Copy that into our domain map and send a join

 Support for global heartbeat and node info was added in 1.1 */

	/* Joined state *must* be set before the joining node

	 * information, otherwise the query_join handler may read no

	 * current joiner but a state of NEW and tell joining nodes

 Do we need to send a cancel message to any nodes? */

		/* If we're racing another node to the join, then we

		 * need to back off temporarily and let them

			/*

			 * <chip> After you!

			 * <dale> No, after you!

			 * <chip> I insist!

			 * <dale> But you first!

			 * ...

/*

 * Compare a requested locking protocol version against the current one.

 *

 * If the major numbers are different, they are incompatible.

 * If the current minor is greater than the request, they are incompatible.

 * If the current minor is less than or equal to the request, they are

 * compatible, and the requester should run at the current minor version.

/*

 * dlm_register_domain: one-time setup per "domain".

 *

 * The filesystem passes in the requested locking version via proto.

 * If registration was successful, proto will contain the negotiated

 * locking protocol.

 doesn't exist */

 a little variable switch-a-roo here... */

 add the new domain */

	/*

	 * Pass the locking protocol version into the join.  If the join

	 * succeeds, it will have the negotiated protocol set.

 Tell the caller what locking protocol we negotiated */

/* Domain eviction callback handling.

 *

 * The file system requires notification of node death *before* the

 * dlm completes it's recovery work, otherwise it may be able to

 * acquire locks on resources requiring recovery. Since the dlm can

 * evict a node from it's domain *before* heartbeat fires, a similar

/* Eviction is not expected to happen often, so a per-domain lock is

 * not necessary. Eviction callbacks are allowed to sleep for short

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * dlmthread.c

 *

 * standalone DLM module

 *

 * Copyright (C) 2004 Oracle.  All rights reserved.

 will exit holding res->spinlock, but may drop in function */

 waits until flags are cleared on res->state */

/* "unused": the lockres has no locks, is not on the dirty list,

 * has no inflight locks (in the gap between mastery and acquiring

 * the first lock), and has no bits in its refmap.

 Locks are in the process of being created */

 Another node has this resource with this node as the master */

/* Call whenever you may have added or deleted something from one of

 * the lockres queue's. This will figure out whether it belongs on the

/*

 * Do the real purge work:

 *     unhash the lockres, and

 *     clear flag DLM_LOCK_RES_DROPPING_REF.

 * It requires dlm and lockres spinlock to be taken.

	/*

	 * lockres is not in the hash now. drop the flag and wake up

	 * any processes waiting in dlm_get_lock_resource.

 drop spinlock...  retake below */

 This ensures that clear refmap is sent after the set */

 clear our bit from the master's refmap, ignore errors */

	/* lockres is not in the hash now.  drop the flag and wake up

		/* Make sure that we want to be processing this guy at

			/* Since resources are added to the purge list

			 * in tail order, we can stop at the first

			 * unpurgable resource -- anyone added after

		/* Status of the lockres *might* change so double

		 * check. If the lockres is unused, holding the dlm

		 * spinlock will prevent people from getting and more

 Avoid adding any scheduling latencies */

	/*

	 * Because this function is called with the lockres

	 * spinlock, and because we know that it is not migrating/

	 * recovering/in-progress, it is fine to reserve asts and

	 * basts right before queueing them all throughout

 queue the BAST if not already */

 update the highest_blocked if needed */

 we can convert the lock */

 go back and check for more */

	/* we can grant the blocked lock (only

 target->ml.type is already correct */

 go back and check for more */

 must have NO locks when calling this with res !=NULL * */

 don't shuffle secondary queues */

 ref for dirty_list */

 Launch the NM thread for the mounted volume */

 get an extra ref on lock */

 remove from list (including ref) */

		/* possible that another ast was queued while

		/* drop the extra ref.

 get an extra ref on lock */

 get the highest blocked lock, and reset */

 remove from list (including ref) */

		/* possible that another bast was queued while

		/* drop the extra ref.

		/* dlm_shutting_down is very point-in-time, but that

		 * doesn't matter as we'll just loop back around if we

		 * get false on the leading edge of a state

		/* We really don't want to hold dlm->spinlock while

		 * calling dlm_shuffle_lists on each lockres that

		 * needs to have its queues adjusted and AST/BASTs

		 * run.  So let's pull each entry off the dirty_list

		 * and drop dlm->spinlock ASAP.  Once off the list,

		 * res->spinlock needs to be taken again to protect

			/* peel a lockres off, remove it from the list,

 We clear the DLM_LOCK_RES_DIRTY state once we shuffle lists below */

 Drop dirty_list ref */

		 	/* lockres can be re-dirtied/re-added to the

			/* it is now ok to move lockreses in these states

			 * to the dirty list, assuming that they will only be

 move it to the tail and keep going */

			/* at this point the lockres is not migrating/

			 * recovering/in-progress.  we have the lockres

			 * spinlock and do NOT have the dlm lock.

 called while holding lockres lock */

			/* if the lock was in-progress, stick

			/* unlikely, but we may need to give time to

 yield and continue right away if there is more work to do */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * dlmconvert.c

 *

 * underlying calls for lock conversion

 *

 * Copyright (C) 2004 Oracle.  All rights reserved.

/* NOTE: __dlmconvert_master is the only function in here that

 * needs a spinlock held on entry (res->spinlock) and it is the

 * only one that holds a lock on exit (res->spinlock).

 * All other functions in here need no locks and drop all of

/*

 * this is only called directly by dlmlock(), and only when the

 * local node is the owner of the lockres

 * locking:

 *   caller needs:  none

 *   taken:         takes and drops res->spinlock

 *   held on exit:  none

 * returns: see __dlmconvert_master

 we are not in a network handler, this is fine */

 either queue the ast or release it */

/* performs lock conversion at the lockres master site

 * locking:

 *   caller needs:  res->spinlock

 *   taken:         takes and drops lock->spinlock

 *   held on exit:  res->spinlock

 * returns: DLM_NORMAL, DLM_NOTQUEUED, DLM_DENIED

 *   call_ast: whether ast should be called for this lock

 *   kick_thread: whether dlm_kick_thread should be called

 already converting? */

 must be on grant queue to convert */

 EX + LKM_VALBLK + convert == set lvb */

 refetch if new level is not NL */

 in-place downconvert? */

 upconvert from here on */

 existing conversion requests take precedence */

 fall thru to grant */

 immediately grant the new lock type */

	/*

	 * Move the lock to the tail because it may be the only lock which has

	 * an invalid lvb.

 do not alter lock refcount.  switching lists. */

 do not alter lock refcount.  switching lists. */

/* messages the master site to do lock conversion

 * locking:

 *   caller needs:  none

 *   taken:         takes and drops res->spinlock, uses DLM_LOCK_RES_IN_PROGRESS

 *   held on exit:  none

 * returns: DLM_NORMAL, DLM_RECOVERING, status from remote node

 __dlm_print_one_lock_resource(res); */

 will exit this call with spinlock held */

 move lock to local convert queue */

 do not alter lock refcount.  switching lists. */

	/* no locks held here.

	/* if it failed, move it back to granted queue.

	 * if master returns DLM_NORMAL and then down before sending ast,

	 * it may have already been moved to granted queue, reset to

 TODO: should this be a wake_one? */

 wake up any IN_PROGRESS waiters */

/* sends DLM_CONVERT_LOCK_MSG to master site

 * locking:

 *   caller needs:  none

 *   taken:         none

 *   held on exit:  none

 * returns: DLM_NOLOCKMGR, status from remote node

 extra data to send if we are updating lvb */

 successfully sent and received

 this is already a dlm_status

			/* instead of logging the same network error over

			 * and over, sleep here and wait for the heartbeat

/* handler for DLM_CONVERT_LOCK_MSG on master site

 * locking:

 *   caller needs:  none

 *   taken:         takes and drop res->spinlock

 *   held on exit:  none

 * returns: DLM_NORMAL, DLM_IVLOCKID, DLM_BADARGS,

 *          status from __dlmconvert_master

 found the lock */

 see if caller needed to get/put lvb */

 either queue the ast or release it, if reserved */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hpfs/namei.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  adding & removing files & directories

dee.archive = 0;*/

de->hidden = de->system = 0;*/

 no space for deleting */

 order doesn't matter, due to VFS exclusion */

 Erm? Moving over the empty non-busy directory is perfectly legal */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hpfs/dentry.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  dcache operations

/*

 * Note: the dentry argument is the parent dentry.

if (hpfs_chk_name(qstr->name,&l))*/

return -ENAMETOOLONG;*/

return -ENOENT;*/

hpfs_adjust_length(b->name, &bl);*/

	/*

	 * 'str' is the nane of an already existing dentry, so the name

	 * must be valid. 'name' must be validated first.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/fs/hpfs/super.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  mounting, unmounting, error handling

 Mark the filesystem dirty, so that chkdsk checks it when os/2 booted */

/* Mark the filesystem clean (mark it dirty for chkdsk if chkdsk==2 or if there

 Filesystem error... */

/* 

 * A little trick to detect cycles in many hpfs structures and don't let the

 * kernel crash on corrupted filesystem. When first called, set c2 to 0.

 *

 * BTW. chkdsk doesn't detect cycles correctly. When I had 2 lost directories

 * nested each in other, chkdsk locked up happilly.

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

/*

 * A tiny parser for option strings, stolen from dosfs.

 * Stolen again from read-only hpfs.

 * And updated for table-driven option parsing.

pr_info("Parsing opts: '%s'\n",opts);*/

 Super operations */

sbi->sb_mounting = 1;*/

 Check magics */

	if (/*le16_to_cpu(bootblock->magic) != BB_MAGIC

 Check version */

artax.karlin.mff.cuni.cz/~mikulas/vyplody/hpfs/index-e.cgi and if it still can't understand this format, contact author - mikulas@artax.karlin.mff.cuni.cz\n");

 Fill superblock stuff */

 Load bitmap directory */

 Check for general fs errors*/

 Load code page table */

	/*

	 * find the root directory's . pointer & finish filling in the inode

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hpfs/anode.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  handling HPFS anode tree that contains file allocation info

 Find a sector in allocation tree */

 Add a sector to tree */

a*/0, &ra, &bh2))) {

fs*/-1);

anode->up = cpu_to_le32(up != -1 ? up : ra);*/

/*

 * Remove allocation tree. Recursion would look much nicer but

 * I want to avoid it because it can cause stack overflow.

 Just a wrapper around hpfs_bplus_lookup .. used for reading eas */

 Truncate allocation tree. Doesn't join anodes - I hope it doesn't matter */

 I hope gcc optimizes this :-) */

/* Remove file or directory and it's eas - note that directory must

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hpfs/inode.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  inode VFS functions

		/*i->i_mode |= S_IFREG;

		i->i_mode &= ~0111;

		i->i_op = &hpfs_file_iops;

		i->i_fop = &hpfs_file_ops;

	/*if (le32_to_cpu(fnode->acl_size_l) || le16_to_cpu(fnode->acl_size_s)) {

		   Some unknown structures like ACL may be in fnode,

		   we'd better not overwrite them

		hpfs_error(i->i_sb, "fnode %08x has some unknown HPFS386 structures", i->i_ino);

 sick, but legal */

hpfs_inode->i_ea_size*/0);

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hpfs/dnode.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  handling directory dnode tree - adding, deleteing & searching for dirents

	/*pr_warn("position pointer %p->%08x not found\n",

/*void hpfs_hpfs_pos_substd(loff_t *p, loff_t f, loff_t t)

{

	if ((*p & ~0x3f) == (f & ~0x3f)) *p = (t & ~0x3f) | (*p & 0x3f);

 Add an entry to dnode and don't care if it grows over 2048 bytes */

 Delete dirent and don't care about its subtree */

 Add an entry to dnode and do dnode splitting if required */

		/* 0x924 is a max size of dnode after adding a dirent with

		   max name length. We alloc this only once. There must

		   not be any error while splitting dnodes, otherwise the

		   whole directory, not only file we're adding, would

/*

 * Add an entry to directory btree.

 * I hate such crazy directory structure.

 * It's easy to read but terrible to write.

 * I wrote this directory code 4 times.

 * I hope, now it's finally bug-free.

/* 

 * Find dirent with higher name in 'from' subtree and move it to 'to' dnode.

 * Return the dnode we moved from (to be checked later if it's empty)

/* 

 * Check if a dnode is empty and delete it from the tree

 * (chkdsk doesn't like empty dnodes)

		/*pr_info("UP-TO-DNODE: %08x (ndown = %08x, down = %08x, dno = %08x)\n",

 Delete dirent from directory */

 Going to the next dirent */

 We're going down the tree */

 Going up */

 Find a dirent in tree */

/*

 * Remove empty directory. In normal cases it is only one dnode with two

 * entries, but we must handle also such obscure cases when it's a tree

 * of empty dnodes.

/* 

 * Find dirent for specified fnode. Use truncated 15-char name in fnode as

 * a help for searching.

name2[15] = 0xff;*/

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hpfs/buffer.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  general buffer i/o

 Map a sector into a buffer and return pointers to it and to the buffer. */

 Like hpfs_map_sector but don't read anything */

return hpfs_map_sector(s, secno, bhp, 0);*/

 Map 4 sectors into a 4buffer and return pointers to it and to the buffer. */

 Don't read sectors */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hpfs/dir.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  directory VFS functions

hpfs_write_if_changed(inode);*/

 This is slow, but it's not used often */

 Somebody else will have to figure out what to do here */

pr_info("dir lseek\n");*/

pr_warn("illegal lseek: %016llx\n", new_off);*/

 diff -r requires this (note, that diff -r */

 also fails on msdos filesystem in 2.0) */

		/* This won't work when cycle is longer than number of dirents

		   accepted by filldir, but what can I do?

/*

 * lookup.  Search the specified directory for the specified name, set

 * *result to the corresponding inode.

 *

 * lookup uses the inode number to tell read_inode whether it is reading

 * the inode of a directory or a file -- file ino's are odd, directory

 * ino's are even.  read_inode avoids i/o for file inodes; everything

 * needed is up here in the directory.  (And file fnodes are out in

 * the boondocks.)

 *

 *    - M.P.: this is over, sometimes we've got to read file's fnode for eas

 *	      inode numbers are just fnode sector numbers; iget lock is used

 *	      to tell read_inode to read fnode or not.

	/*

	 * '.' and '..' will never be passed here.

	/*

	 * This is not really a bailout, just means file not found.

	/*

	 * Get inode number, what we're after.

	/*

	 * Go find or make an inode.

	/*

	 * Fill in the info from the directory if this is a newly created

	 * inode.

			/*

			 * i_blocks should count the fnode and any anodes.

			 * We count 1 for the fnode and don't bother about

			 * anodes -- the disk heads are on the directory band

			 * and we want them to stay there.

	/*

	 * Made it.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hpfs/file.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  file VFS functions

/*

 * generic_file_read often calls bmap with non-existing sector,

 * so we must ignore such errors.

-EPERM*/;

 make sure we write it on close, if not earlier */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hpfs/ea.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  handling extended attributes

/* Remove external extended attributes. ano specifies whether a is a 

 Read an extended attribute named 'key' into the provided buffer */

 Read an extended attribute named 'key' */

/* 

 * Update or create extended attribute 'key' with value 'data'. Note that

 * when this ea exists, it MUST have the same size as size of data.

 * This driver can't change sizes of eas ('cause I just don't need it).

		/*if (le16_to_cpu(fnode->ea_size_s)) {

			hpfs_error(s, "fnode %08x: ea_size_s == %03x, ea_offs == 0",

				inode->i_ino, le16_to_cpu(fnode->ea_size_s));

			return;

	/* Most the code here is 99.9993422% unused. I hope there are no bugs.

 Aargh... don't know how to create ea anodes :-( */

				/*struct buffer_head *bh;

				struct anode *anode;

				anode_secno a_s;

				if (!(anode = hpfs_alloc_anode(s, fno, &a_s, &bh)))

					goto bail;

				anode->up = cpu_to_le32(fno);

				anode->btree.fnode_parent = 1;

				anode->btree.n_free_nodes--;

				anode->btree.n_used_nodes++;

				anode->btree.first_free = cpu_to_le16(le16_to_cpu(anode->btree.first_free) + 12);

				anode->u.external[0].disk_secno = cpu_to_le32(le32_to_cpu(fnode->ea_secno));

				anode->u.external[0].file_secno = cpu_to_le32(0);

				anode->u.external[0].length = cpu_to_le32(len);

				mark_buffer_dirty(bh);

				brelse(bh);

				fnode->flags |= FNODE_anode;

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hpfs/name.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  operations with filenames

 Characters that are allowed in HPFS but not in DOS */

 OS/2 clears dots and spaces at the end of file name, so we have to */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hpfs/map.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  mapping structures to memory with some minimal checks

/*

 * Load first code page into kernel memory, return pointer to 256-byte array,

 * first 128 bytes are uppercasing table for chars 128-255, next 128 bytes are

 * lowercasing table

 Try to build lowercasing table from uppercasing one */

/*

 * Load fnode to memory

/*

 * Load dnode to memory and do some checks

			/* Check dirents - bad dirents would cause infinite

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hpfs/alloc.c

 *

 *  Mikulas Patocka (mikulas@artax.karlin.mff.cuni.cz), 1998-1999

 *

 *  HPFS bitmap operations

/*

 * Check if a sector is allocated in bitmap

 * This is really slow. Turned on only if chk==2

/*

 * Check if sector(s) have proper number and additionally check if they're

 * allocated in bitmap.

unsigned mnr;*/

for (i = nr + 1; i != nr; i++, i &= 0x1ff) */

/*

 * Allocation strategy:	1) search place near the sector specified

 *			2) search bitmap where free sectors last found

 *			3) search all bitmaps

 *			4) search all bitmaps ignoring number of pre-allocated

 *				sectors

	/*

	if (b != -1) {

		if ((sec = alloc_in_bmp(s, b<<14, n, f_p ? forward : forward/2))) {

			b &= 0x0fffffff;

			goto ret;

		}

		if (b > 0x10000000) if ((sec = alloc_in_bmp(s, (b&0xfffffff)<<14, n, f_p ? forward : 0))) goto ret;

 Alloc sector if it's free */

 Free sectors in bitmaps */

pr_info("2 - ");*/

/*

 * Check if there are at least n free dnodes on the filesystem.

 * Called before adding to dnode. If we run out of space while

 * splitting dnodes, it would corrupt dnode tree.

 SPDX-License-Identifier: LGPL-2.1

/*

 * Copyright IBM Corporation, 2010

 * Author Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>

 get the default/access acl values and cache them */

	/*

	 * 9p Always cache the acl value when

	 * instantiating the inode (v9fs_inode_from_fid)

		/*

		 * On access = client  and acl = on mode get the acl

		 * values from the server

 Set a setxattr request to server */

	/*

	 * We allow set/get/list of acl when access=client is not specified

	/*

	 * set the attribute on the remote. Without even looking at the

	 * xattr value. We leave it to the server to validate

 update the cached acl value */

				/*

				 * ACL can be represented

				 * by the mode bits. So don't

				 * update ACL.

			/* FIXME should we update ctime ?

			 * What is the following setxattr update the

			 * mode ?

 SPDX-License-Identifier: GPL-2.0-only

/*

 * V9FS FID Management

 *

 *  Copyright (C) 2007 by Latchesar Ionkov <lucho@ionkov.net>

 *  Copyright (C) 2005, 2006 by Eric Van Hensbergen <ericvh@gmail.com>

/**

 * v9fs_fid_add - add a fid to a dentry

 * @dentry: dentry that the fid is being added to

 * @fid: fid to add

 *

/**

 * v9fs_fid_find_inode - search for an open fid off of the inode list

 * @inode: return a fid pointing to a specific inode

 * @uid: return a fid belonging to the specified user

 *

/**

 * v9fs_open_fid_add - add an open fid to an inode

 * @inode: inode that the fid is being added to

 * @fid: fid to add

 *

/**

 * v9fs_fid_find - retrieve a fid that belongs to the specified uid

 * @dentry: dentry to look for fid in

 * @uid: return fid that belongs to the specified user

 * @any: if non-zero, return any fid associated with the dentry

 *

 we'll recheck under lock if there's anything to look in */

/*

 * We need to hold v9ses->rename_sem as long as we hold references

 * to returned path array. Array element contain pointers to

 * dentry names.

	/*

	 * we don't have a matching fid. To do a TWALK we need

	 * parent fid. We need to prevent rename when we want to

	 * look at the parent.

 Found the parent fid do a lookup with that */

 start from the root and try to do a lookup */

 the user is not attached to the fs yet */

 If we are root ourself just return that */

	/*

	 * Do a multipath walk with attached root.

	 * When walking parent we need to make sure we

	 * don't have a parallel rename happening

		/*

		 * We need to hold rename lock when doing a multipath

		 * walk to ensure none of the patch component change

				/*

				 * If we fail, clunk fid which are mapping

				 * to path component and not the last component

				 * of the path.

/**

 * v9fs_fid_lookup - lookup for a fid, try to walk if not found

 * @dentry: dentry to look for fid in

 *

 * Look for a fid in the specified dentry for the current user.

 * If no fid is found, try to create one walking from a fid from the parent

 * dentry (if it has one), or the root dentry. If the user haven't accessed

 * the fs yet, attach now and walk from the root.

	/*

	 * writeback fid will only be used to write back the

	 * dirty pages. We always request for the open fid in read-write

	 * mode so that a partial page write which result in page

	 * read can work.

 SPDX-License-Identifier: LGPL-2.1

/*

 * Copyright IBM Corporation, 2010

 * Author Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>

 request to get the attr_size */

/*

 * v9fs_xattr_get()

 *

 * Copy an extended attribute into the buffer

 * provided, or compute the buffer size required.

 * Buffer is NULL to compute the size of the buffer required.

 *

 * Returns a negative error number on failure, or the number of bytes

 * used / required on success.

/*

 * v9fs_xattr_set()

 *

 * Create, replace or remove an extended attribute for this inode. Buffer

 * is NULL to remove an existing extended attribute, and non-NULL to

 * either replace an existing extended attribute, or create a new extended

 * attribute. The flags XATTR_REPLACE and XATTR_CREATE

 * specify that an extended attribute must exist and must not exist

 * previous to the call, respectively.

 *

 * Returns 0, or a negative error number on failure.

 Clone it */

	/*

	 * On success fid points to xattr

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This file contains vfs inode ops for the 9P2000 protocol.

 *

 *  Copyright (C) 2004 by Eric Van Hensbergen <ericvh@gmail.com>

 *  Copyright (C) 2002 by Ron Minnich <rminnich@lanl.gov>

/**

 * unixmode2p9mode - convert unix mode bits to plan 9

 * @v9ses: v9fs session information

 * @mode: mode to convert

 *

/**

 * p9mode2perm- convert plan9 mode bits to unix permission bits

 * @v9ses: v9fs session information

 * @stat: p9_wstat from which mode need to be derived

 *

/**

 * p9mode2unixmode- convert plan9 mode bits to unix mode bits

 * @v9ses: v9fs session information

 * @stat: p9_wstat from which mode need to be derived

 * @rdev: major number, minor number in case of device files.

 *

/**

 * v9fs_uflags2omode- convert posix open flags to plan 9 mode bits

 * @uflags: flags to convert

 * @extended: if .u extensions are active

/**

 * v9fs_blank_wstat - helper function to setup a 9P stat structure

 * @wstat: structure to initialize

 *

/**

 * v9fs_alloc_inode - helper function to allocate an inode

 * @sb: The superblock to allocate the inode from

/**

 * v9fs_free_inode - destroy an inode

 * @inode: The inode to be freed

/**

 * v9fs_get_inode - helper function to setup an inode

 * @sb: superblock

 * @mode: mode to setup inode with

 * @rdev: The device numbers to set

/**

 * v9fs_evict_inode - Remove an inode from the inode cache

 * @inode: inode to release

 *

 clunk the fid stashed in writeback_fid */

 don't match inode of different type */

 compare qid details */

	/*

	 * initialize the inode with the stat info

	 * FIXME!! we may need support for stale inodes

	 * later.

/**

 * v9fs_at_to_dotl_flags- convert Linux specific AT flags to

 * plan 9 AT flag.

 * @flags: flags to convert

/**

 * v9fs_dec_count - helper functon to drop i_nlink.

 *

 * If a directory had nlink <= 2 (including . and ..), then we should not drop

 * the link count, which indicates the underlying exported fs doesn't maintain

 * nlink accurately. e.g.

 * - overlayfs sets nlink to 1 for merged dir

 * - ext4 (with dir_nlink feature enabled) sets nlink to 1 if a dir has more

 *   than EXT4_LINK_MAX (65000) links.

 *

 * @inode: inode whose nlink is being dropped

/**

 * v9fs_remove - helper function to remove files and directories

 * @dir: directory inode that is being deleted

 * @dentry:  dentry that is being deleted

 * @flags: removing a directory

 *

 Try the one based on path */

		/*

		 * directories on unlink should have zero

		 * link count

 invalidate all fids associated with dentry */

 NOTE: This will not include open fids */

/**

 * v9fs_create - Create a file

 * @v9ses: session information

 * @dir: directory that dentry is being created in

 * @dentry:  dentry that is being created

 * @extension: 9p2000.u extension string to support devices, etc.

 * @perm: create permissions

 * @mode: open mode

 *

 clone a fid to use for creation */

 now walk from the parent so we can get unopened fid */

		/*

		 * instantiate inode and assign the unopened fid to the dentry

/**

 * v9fs_vfs_create - VFS hook to create a regular file

 * @mnt_userns: The user namespace of the mount

 * @dir: The parent directory

 * @dentry: The name of file to be created

 * @mode: The UNIX file mode to set

 * @excl: True if the file must not yet exist

 *

 * open(.., O_CREAT) is handled in v9fs_vfs_atomic_open().  This is only called

 * for mknod(2).

 *

 P9_OEXCL? */

/**

 * v9fs_vfs_mkdir - VFS mkdir hook to create a directory

 * @mnt_userns: The user namespace of the mount

 * @dir:  inode that is being unlinked

 * @dentry: dentry that is being unlinked

 * @mode: mode for new directory

 *

/**

 * v9fs_vfs_lookup - VFS lookup hook to "walk" to a new inode

 * @dir:  inode that is being walked from

 * @dentry: dentry that is being walked to?

 * @flags: lookup flags (unused)

 *

 We can walk d_parent because we hold the dir->i_mutex */

	/*

	 * Make sure we don't use a wrong inode due to parallel

	 * unlink. For cached mode create calls request for new

	 * inode. But with cache disabled, lookup should do this.

	/*

	 * If we had a rename on the server and a parallel lookup

	 * for the new name, then make sure we instantiate with

	 * the new name. ie look up for a/b, while on server somebody

	 * moved b under k and client parallely did a lookup for

	 * k/b.

 Only creates */

		/*

		 * clone a fid and add it to writeback_fid

		 * we do it during open time instead of

		 * page dirty time via write_begin/page_mkwrite

		 * because we want write after unlink usecase

		 * to work.

/**

 * v9fs_vfs_unlink - VFS unlink hook to delete an inode

 * @i:  inode that is being unlinked

 * @d: dentry that is being unlinked

 *

/**

 * v9fs_vfs_rmdir - VFS unlink hook to delete a directory

 * @i:  inode that is being unlinked

 * @d: dentry that is being unlinked

 *

/**

 * v9fs_vfs_rename - VFS hook to rename an inode

 * @mnt_userns: The user namespace of the mount

 * @old_dir:  old dir inode

 * @old_dentry: old dentry

 * @new_dir: new dir inode

 * @new_dentry: new dentry

 * @flags: RENAME_* flags

 *

		/*

		 * 9P .u can only handle file rename in the same directory

 successful rename */

/**

 * v9fs_vfs_getattr - retrieve file metadata

 * @mnt_userns: The user namespace of the mount

 * @path: Object to query

 * @stat: metadata structure to populate

 * @request_mask: Mask of STATX_xxx flags indicating the caller's interests

 * @flags: AT_STATX_xxx setting

 *

/**

 * v9fs_vfs_setattr - set file metadata

 * @mnt_userns: The user namespace of the mount

 * @dentry: file whose metadata to set

 * @iattr: metadata assignment structure

 *

 Write all dirty data */

/**

 * v9fs_stat2inode - populate an inode structure with mistat info

 * @stat: Plan 9 metadata (mistat) structure

 * @inode: inode to populate

 * @sb: superblock of filesystem

 * @flags: control flags (e.g. V9FS_STAT2INODE_KEEP_ISIZE)

 *

			/*

			 * Hadlink support got added later to the .u extension.

			 * So there can be a server out there that doesn't

			 * support this even with .u extension. That would

			 * just leave us with stat->extension being an empty

			 * string, though.

 HARDLINKCOUNT %u */

 not real number of blocks, but 512 byte ones ... */

/**

 * v9fs_qid2ino - convert qid into inode number

 * @qid: qid to hash

 *

 * BUG: potential for inode number collisions?

/**

 * v9fs_vfs_get_link - follow a symlink path

 * @dentry: dentry for symlink

 * @inode: inode for symlink

 * @done: delayed call for when we are done with the return value

/**

 * v9fs_vfs_mkspecial - create a special file

 * @dir: inode to create special file in

 * @dentry: dentry to create

 * @perm: mode to create special file

 * @extension: 9p2000.u format extension string representing special file

 *

/**

 * v9fs_vfs_symlink - helper function to create symlinks

 * @mnt_userns: The user namespace of the mount

 * @dir: directory inode containing symlink

 * @dentry: dentry for symlink

 * @symname: symlink data

 *

 * See Also: 9P2000.u RFC for more information

 *

/**

 * v9fs_vfs_link - create a hardlink

 * @old_dentry: dentry for file to link to

 * @dir: inode destination for new link

 * @dentry: dentry for link

 *

 sign + number + \n + \0 */

/**

 * v9fs_vfs_mknod - create a special file

 * @mnt_userns: The user namespace of the mount

 * @dir: inode destination for new link

 * @dentry: dentry for file

 * @mode: mode for creation

 * @rdev: device associated with special file

 *

 build extension */

	/*

	 * Don't update inode if the file type is different

	/*

	 * We don't want to refresh inode->i_size,

	 * because we may have cached data

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This file contians vfs dentry ops for the 9P2000 protocol.

 *

 *  Copyright (C) 2004 by Eric Van Hensbergen <ericvh@gmail.com>

 *  Copyright (C) 2002 by Ron Minnich <rminnich@lanl.gov>

/**

 * v9fs_cached_dentry_delete - called when dentry refcount equals 0

 * @dentry:  dentry in question

 *

 Don't cache negative dentries */

/**

 * v9fs_dentry_release - called when dentry is going to be freed

 * @dentry:  dentry that is being release

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This file contains vfs directory ops for the 9P2000 protocol.

 *

 *  Copyright (C) 2004 by Eric Van Hensbergen <ericvh@gmail.com>

 *  Copyright (C) 2002 by Ron Minnich <rminnich@lanl.gov>

/**

 * struct p9_rdir - readdir accounting

 * @head: start offset of current dirread buffer

 * @tail: end offset of current dirread buffer

 * @buf: dirread buffer

 *

 * private structure for keeping track of readdir

 * allocated on demand

/**

 * dt_type - return file type

 * @mistat: mistat structure

 *

/**

 * v9fs_alloc_rdir_buf - Allocate buffer used for read and readdir

 * @filp: opened file structure

 * @buflen: Length in bytes of buffer to allocate

 *

/**

 * v9fs_dir_readdir - iterate through a directory

 * @file: opened file structure

 * @ctx: actor we feed the entries to

 *

/**

 * v9fs_dir_readdir_dotl - iterate through a directory

 * @file: opened file structure

 * @ctx: actor we feed the entries to

 *

/**

 * v9fs_dir_release - close a directory

 * @inode: inode of the directory

 * @filp: file pointer to a directory

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This file contians vfs address (mmap) ops for 9P2000.

 *

 *  Copyright (C) 2005 by Eric Van Hensbergen <ericvh@gmail.com>

 *  Copyright (C) 2002 by Ron Minnich <rminnich@lanl.gov>

/**

 * v9fs_req_issue_op - Issue a read from 9P

 * @subreq: The read to make

/**

 * v9fs_init_rreq - Initialise a read request

 * @rreq: The read request

 * @file: The file being read from

/**

 * v9fs_req_cleanup - Cleanup request initialized by v9fs_init_rreq

 * @mapping: unused mapping of request to cleanup

 * @priv: private data to cleanup, a fid, guaranted non-null.

/**

 * v9fs_is_cache_enabled - Determine if caching is enabled for an inode

 * @inode: The inode to check

/**

 * v9fs_begin_cache_operation - Begin a cache operation for a read

 * @rreq: The read request

/**

 * v9fs_vfs_readpage - read an entire page in from 9P

 * @file: file being read

 * @page: structure to page

 *

/**

 * v9fs_vfs_readahead - read a set of pages from 9P

 * @ractl: The readahead parameters

/**

 * v9fs_release_page - release the private state associated with a page

 * @page: The page to be released

 * @gfp: The caller's allocation restrictions

 *

 * Returns 1 if the page can be released, false otherwise.

/**

 * v9fs_invalidate_page - Invalidate a page completely or partially

 * @page: The page to be invalidated

 * @offset: offset of the invalidated region

 * @length: length of the invalidated region

 Simultaneous truncation occurred */

 We should have writeback_fid always set */

/**

 * v9fs_launder_page - Writeback a dirty page

 * @page: The page to be cleaned up

 *

 * Returns 0 on success.

/**

 * v9fs_direct_IO - 9P address space operation for direct I/O

 * @iocb: target I/O control block

 * @iter: The data/buffer to use

 *

 * The presence of v9fs_direct_IO() in the address space ops vector

 * allowes open() O_DIRECT flags which would have failed otherwise.

 *

 * In the non-cached mode, we shunt off direct read and write requests before

 * the VFS gets them, so this method should never be called.

 *

 * Direct IO is not 'yet' supported in the cached mode. Hence when

 * this routine is called through generic_file_aio_read(), the read/write fails

 * with an error.

 *

	/* Prefetch area to be written into the cache if we're caching this

	 * file.  We need to do this before we get a lock on the page in case

	 * there's more than one writer competing for the same cache block.

	/*

	 * No need to use i_size_read() here, the i_size

	 * cannot change under us because we hold the i_mutex.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * V9FS cache definitions.

 *

 *  Copyright (C) 2009 by Abhishek Kulkarni <adkulkar@umail.iu.edu>

/*

 * v9fs_random_cachetag - Generate a random tag to be associated

 *			  with a new cache session.

 *

 * The value of jiffies is used for a fairly randomly cache tag.

 If no cache session tag was specified, we generate a random one. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This file contians vfs file ops for 9P2000.

 *

 *  Copyright (C) 2004 by Eric Van Hensbergen <ericvh@gmail.com>

 *  Copyright (C) 2002 by Ron Minnich <rminnich@lanl.gov>

/**

 * v9fs_file_open - open a file (or directory)

 * @inode: inode to be opened

 * @file: file being opened

 *

		/*

		 * clone a fid and add it to writeback_fid

		 * we do it during open time instead of

		 * page dirty time via write_begin/page_mkwrite

		 * because we want write after unlink usecase

		 * to work.

/**

 * v9fs_file_lock - lock a file (or directory)

 * @filp: file to be locked

 * @cmd: lock command

 * @fl: file lock structure

 *

 * Bugs: this looks like a local only lock, we should extend into 9P

 *       by using open exclusive

 convert posix lock to p9 tlock args */

 map the lock type */

	/*

	 * if its a blocked request and we get P9_LOCK_BLOCKED as the status

	 * for lock request, keep on trying

		/*

		 * p9_client_lock_dotl overwrites flock.client_id with the

		 * server message, free and reuse the client name

 map 9p status to VFS status */

	/*

	 * incase server returned error for lock request, revert

	 * it locally

 Even if this fails we want to return the remote error */

	/*

	 * if we have a conflicting lock locally, no need to validate

	 * with server

 convert posix lock to p9 tgetlock args */

 map 9p lock type to os lock type */

/**

 * v9fs_file_lock_dotl - lock a file (or directory)

 * @filp: file to be locked

 * @cmd: lock command

 * @fl: file lock structure

 *

/**

 * v9fs_file_flock_dotl - lock a file

 * @filp: file to be locked

 * @cmd: lock command

 * @fl: file lock structure

 *

 Convert flock to posix lock */

/**

 * v9fs_file_read_iter - read from a file

 * @iocb: The operation parameters

 * @to: The buffer to read into

 *

/**

 * v9fs_file_write_iter - write to a file

 * @iocb: The operation parameters

 * @from: The data to write

 *

			/*

			 * Need to serialize against i_size_write() in

			 * v9fs_stat2inode()

		/*

		 * clone a fid and add it to writeback_fid

		 * we do it during mmap instead of

		 * page dirty time via write_begin/page_mkwrite

		 * because we want write after unlink usecase

		 * to work.

	/* Wait for the page to be written to the cache before we allow it to

	 * be modified.  We then assume the entire page will need writing back.

 Update file times before taking page lock */

/**

 * v9fs_mmap_file_read_iter - read from a file

 * @iocb: The operation parameters

 * @to: The buffer to read into

 *

 TODO: Check if there are dirty pages */

/**

 * v9fs_mmap_file_write_iter - write to a file

 * @iocb: The operation parameters

 * @from: The data to write

 *

	/*

	 * TODO: invalidate mmaps on filp's inode between

	 * offset and offset+count

 absolute end, byte at end included */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2004 by Eric Van Hensbergen <ericvh@gmail.com>

 *  Copyright (C) 2002 by Ron Minnich <rminnich@lanl.gov>

/**

 * v9fs_set_super - set the superblock

 * @s: super block

 * @data: file system specific data

 *

/**

 * v9fs_fill_super - populate superblock with info

 * @sb: superblock

 * @v9ses: session information

 * @flags: flags propagated from v9fs_mount()

 *

/**

 * v9fs_mount - mount a superblock

 * @fs_type: file system type

 * @flags: mount flags

 * @dev_name: device name that was mounted

 * @data: mount options

 *

	/*

	 * we will do the session_close and root dentry release

	 * in the below call. But we need to clunk fid, because we haven't

	 * attached the fid to dentry so it won't get clunked

	 * automatically.

/**

 * v9fs_kill_super - Kill Superblock

 * @s: superblock

 *

	/*

	 * in case of non cached mode always drop the

	 * inode because we want the inode attribute

	 * to always match that on the server.

	/*

	 * send an fsync request to server irrespective of

	 * wbc->sync_mode.

	/*

	 * send an fsync request to server irrespective of

	 * wbc->sync_mode.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  This file contains functions assisting in mapping VFS to 9P2000

 *

 *  Copyright (C) 2004-2008 by Eric Van Hensbergen <ericvh@gmail.com>

 *  Copyright (C) 2002 by Ron Minnich <rminnich@lanl.gov>

/*

 * Option Parsing (code inspired by NFS code)

 *  NOTE: each transport will parse its own options

 Options that take integer arguments */

 String options */

 Options that take no arguments */

 Cache options */

 Access options */

 Lock timeout option */

 Error token */

 Interpret mount options for cache mode */

/*

 * Display the mount options in /proc/mounts.

/**

 * v9fs_parse_options - parse mount options into session structure

 * @v9ses: existing v9fs session information

 * @opts: The mount option string

 *

 * Return 0 upon success, -ERRNO upon failure.

 setup defaults */

/**

 * v9fs_session_init - initialize session

 * @v9ses: session information structure

 * @dev_name: device being mounted

 * @data: options

 *

		/*

		 * We support ACCESS_CLIENT only for dotl.

		 * Fall back to ACCESS_USER

FIXME !! */

 for legacy mode, fall back to V9FS_ACCESS_ANY */

		/*

		 * We support ACL checks on clinet only if the protocol is

		 * 9P2000.L and access is V9FS_ACCESS_CLIENT.

 register the session for caching */

/**

 * v9fs_session_close - shutdown a session

 * @v9ses: session information structure

 *

/**

 * v9fs_session_cancel - terminate a session

 * @v9ses: session to terminate

 *

 * mark transport as disconnected and cancel all pending requests.

/**

 * v9fs_session_begin_cancel - Begin terminate of a session

 * @v9ses: session to terminate

 *

 * After this call we don't allow any request other than clunk.

/*

 * List caches associated with a session

 CONFIG_9P_FSCACHE */

/**

 * v9fs_sysfs_init - Initialize the v9fs sysfs interface

 *

/**

 * v9fs_sysfs_cleanup - Unregister the v9fs sysfs interface

 *

/**

 * v9fs_init_inode_cache - initialize a cache for 9P

 * Returns 0 on success.

/**

 * v9fs_destroy_inode_cache - destroy the cache of 9P inode

 *

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

/**

 * init_v9fs - Initialize module

 *

 TODO: Setup list of registered trasnport modules */

/**

 * exit_v9fs - shutdown module

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This file contains vfs inode ops for the 9P2000.L protocol.

 *

 *  Copyright (C) 2004 by Eric Van Hensbergen <ericvh@gmail.com>

 *  Copyright (C) 2002 by Ron Minnich <rminnich@lanl.gov>

/**

 * v9fs_get_fsgid_for_create - Helper function to get the gid for a new object

 * @dir_inode: The directory inode

 *

 * Helper function to get the gid for creating a

 * new file system object. This checks the S_ISGID to determine the owning

 * group of the new file system object.

 set_gid bit is set.*/

 don't match inode of different type */

 compare qid details */

 Always get a new inode */

	/*

	 * initialize the inode with the stat info

	 * FIXME!! we may need support for stale inodes

	 * later.

/**

 * v9fs_open_to_dotl_flags- convert Linux specific open flags to

 * plan 9 open flag.

 * @flags: flags to convert

	/*

	 * We have same bits for P9_DOTL_READONLY, P9_DOTL_WRONLY

	 * and P9_DOTL_NOACCESS

/**

 * v9fs_vfs_create_dotl - VFS hook to create files for 9P2000.L protocol.

 * @mnt_userns: The user namespace of the mount

 * @dir: directory inode that is being created

 * @dentry:  dentry that is being deleted

 * @omode: create permissions

 * @excl: True if the file must not yet exist

 *

 Only creates */

 clone a fid to use for creation */

 Update mode based on ACL value */

 instantiate inode and assign the unopened fid to the dentry */

 Now set the ACL based on the default value */

		/*

		 * clone a fid and add it to writeback_fid

		 * we do it during open time instead of

		 * page dirty time via write_begin/page_mkwrite

		 * because we want write after unlink usecase

		 * to work.

 Since we are opening a file, assign the open fid to the file */

/**

 * v9fs_vfs_mkdir_dotl - VFS mkdir hook to create a directory

 * @mnt_userns: The user namespace of the mount

 * @dir:  inode that is being unlinked

 * @dentry: dentry that is being unlinked

 * @omode: mode for new directory

 *

 Update mode based on ACL value */

 instantiate inode and assign the unopened fid to the dentry */

		/*

		 * Not in cached mode. No need to populate

		 * inode with stat. We need to get an inode

		 * so that we can set the acl with dentry

	/* Ask for all the fields in stat structure. Server will return

	 * whatever it supports

 Change block size to what the server returned */

/*

 * Attribute flags.

/**

 * v9fs_vfs_setattr_dotl - set file metadata

 * @mnt_userns: The user namespace of the mount

 * @dentry: file whose metadata to set

 * @iattr: metadata assignment structure

 *

 Write all dirty data */

 We also want to update ACL when we update mode bits */

/**

 * v9fs_stat2inode_dotl - populate an inode structure with stat info

 * @stat: stat structure

 * @inode: inode to populate

 * @flags: ctrl flags (e.g. V9FS_STAT2INODE_KEEP_ISIZE)

 *

	/* Currently we don't support P9_STATS_BTIME and P9_STATS_DATA_VERSION

	 * because the inode structure does not have fields for them.

 Server doesn't alter fid on TSYMLINK. Hence no need to clone it. */

 Now walk from the parent so we can get an unopened fid. */

 instantiate inode and assign the unopened fid to dentry */

 Not in cached mode. No need to populate inode with stat */

/**

 * v9fs_vfs_link_dotl - create a hardlink for dotl

 * @old_dentry: dentry for file to link to

 * @dir: inode destination for new link

 * @dentry: dentry for link

 *

 Get the latest stat info from server. */

/**

 * v9fs_vfs_mknod_dotl - create a special file

 * @mnt_userns: The user namespace of the mount

 * @dir: inode destination for new link

 * @dentry: dentry for file

 * @omode: mode for creation

 * @rdev: device associated with special file

 *

 Update mode based on ACL value */

 instantiate inode and assign the unopened fid to the dentry */

		/*

		 * Not in cached mode. No need to populate inode with stat.

		 * socket syscall returns a fd, so we need instantiate

/**

 * v9fs_vfs_get_link_dotl - follow a symlink path

 * @dentry: dentry for symlink

 * @inode: inode for symlink

 * @done: destructor for return value

	/*

	 * Don't update inode if the file type is different

	/*

	 * We don't want to refresh inode->i_size,

	 * because we may have cached data

 SPDX-License-Identifier: GPL-2.0-only

/*

 *	fs/bfs/inode.c

 *	BFS superblock and inode operations.

 *	Copyright (C) 1999-2018 Tigran Aivazian <aivazian.tigran@gmail.com>

 *	From fs/minix, Copyright (C) 1991, 1992 Linus Torvalds.

 *	Made endianness-clean by Andrew Stribblehill <ads@wompom.org>, 2005.

 clear on-disk inode */

	/*

	 * If this was the last file, make the previous block

	 * "last block of the last file" even if there is no

	 * real file there, saves us 1 gap.

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

 can we read the last block? */

 test if filesystem is not corrupted */

 SPDX-License-Identifier: GPL-2.0

/*

 *	fs/bfs/dir.c

 *	BFS directory operations.

 *	Copyright (C) 1999-2018  Tigran Aivazian <aivazian.tigran@gmail.com>

 *  Made endianness-clean by Andrew Stribblehill <ads@wompom.org> 2005

 SPDX-License-Identifier: GPL-2.0

/*

 *	fs/bfs/file.c

 *	BFS file operations.

 *	Copyright (C) 1999-2018 Tigran Aivazian <aivazian.tigran@gmail.com>

 *

 *	Make the file block allocation algorithm understand the size

 *	of the underlying block device.

 *	Copyright (C) 2007 Dmitri Vorobiev <dmitri.vorobiev@gmail.com>

 *

	/*

	 * If the file is not empty and the requested block is within the

	 * range of blocks allocated for this file, we can grant it.

 The file will be extended, so let's see if there is enough space. */

 The rest has to be protected against itself. */

	/*

	 * If the last data block for this file is the last allocated

	 * block, we can extend the file trivially, without moving it

	 * anywhere.

 Ok, we have to move this entire file to the next free block. */

	/*

	 * This assumes nothing can write the inode back while we are here

	 * and thus update inode->i_blocks! (XXX)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

	/*

	 * Is it quota file? Do not allow user to mess with it

/*

 * reiserfs_ioctl - handler for ioctl for inode

 * supported commands:

 *  1) REISERFS_IOC_UNPACK - try to unpack tail from direct item into indirect

 *                           and prevent packing file (argument arg has t

 *			      be non-zero)

 *  2) REISERFS_IOC_[GS]ETFLAGS, REISERFS_IOC_[GS]ETVERSION

 *  3) That's all for a while ...

		/*

		 * following two cases are taken from fs/ext2/ioctl.c by Remy

		 * Card (card@masi.ibp.fr)

	/*

	 * These are just misnamed, they actually

	 * get/put from/to user an int

/*

 * reiserfs_unpack

 * Function try to convert tail from direct item into indirect.

 * It set up nopack attribute in the REISERFS_I(inode)->nopack

 ioctl already done */

 we need to make sure nobody is changing the file size beneath us */

 if we are on a block boundary, we are already unpacked.  */

	/*

	 * we unpack by finding the page with the tail, and calling

	 * __reiserfs_write_begin on that page.  This will force a

	 * reiserfs_get_block to unpack the tail for us.

 conversion can change page contents, must flush */

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

 find where objectid map starts */

 FIXME: add something else here */

/*

 * When we allocate objectids we allocate the first unused objectid.

 * Each sequence of objectids in use (the odd sequences) is followed

 * by a sequence of objectids not in use (the even sequences).  We

 * only need to record the last objectid in each of these sequences

 * (both the odd and even sequences) in order to fully define the

 * boundaries of the sequences.  A consequence of allocating the first

 * objectid not in use is that under most conditions this scheme is

 * extremely compact.  The exception is immediately after a sequence

 * of operations which deletes a large number of objects of

 * non-sequential objectids, and even then it will become compact

 * again as soon as more objects are created.  Note that many

 * interesting optimizations of layout could result from complicating

 * objectid assignment, but we have deferred making them for now.

 get unique object identifier */

 comment needed -Hans */

	/*

	 * This incrementation allocates the first unused objectid. That

	 * is to say, the first entry on the objectid map is the first

	 * unused objectid, and by incrementing it we use it.  See below

	 * where we check to see if we eliminated a sequence of unused

	 * objectids....

	/*

	 * Now we check to see if we eliminated the last remaining member of

	 * the first even sequence (and can eliminate the sequence by

	 * eliminating its last objectid from oids), and can collapse the

	 * first two odd sequences into one sequence.  If so, then the net

	 * result is to eliminate a pair of objectids from oids.  We do this

	 * by shifting the entire map to the left.

 makes object identifier unused */

return; */

	/*

	 * start at the beginning of the objectid map (i = 0) and go to

	 * the end of it (i = disk_sb->s_oid_cursize).  Linear search is

	 * what we use, though it is possible that binary search would be

	 * more efficient after performing lots of deletions (which is

	 * when oids is large.)  We only check even i's.

 This incrementation unallocates the objectid. */

			/*

			 * Did we unallocate the last member of an

			 * odd sequence, and can shrink oids?

 shrink objectid map */

 size of objectid map is not changed */

			/*

			 * JDM comparing two little-endian values for

			 * equality -- safe

			/*

			 * objectid map must be expanded, but

			 * there is no space

 expand the objectid map */

		/*

		 * mark everyone used that was listed as free at

		 * the end of the objectid map

 move the smaller objectid map past the end of the new super */

 set the max size so we don't overflow later */

 Zero out label and generate random UUID */

 finally, zero out the unused chunk of the new super */

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

 *

 * Trivial changes by Alan Cox to remove EHASHCOLLISION for compatibility

 *

 * Trivial Changes:

 * Rights granted to Hans Reiser to redistribute under other terms providing

 * he accepts all liability including but not limited to patent, fitness

 * for purpose, and direct or indirect claims arising from failure to perform.

 *

 * NO WARRANTY

/*

 * directory item contains array of entry headers. This performs

 * binary search through that array

 this is not name found, but matched third key component */

/*

 * comment?  maybe something like set de to point to what the path points to?

/*

 * de_bh, de_ih, de_deh (points to first element of array), de_item_num is set

 what entry points to */

 store key of the found entry */

/*

 * We assign a key to each directory item, and place multiple entries in a

 * single directory item.  A directory item has a key equal to the key of

 * the first directory entry in it.



 * This function first calls search_by_key, then, if item whose first entry

 * matches is not found it looks for the entry inside directory item found

 * by search_by_key. Fills the path to the entry, and to the entry position

 * in the item

 The function is NOT SCHEDULE-SAFE! */

 CONFIG_REISERFS_CHECK */

	/*

	 * binary search in directory item by third component of the

	 * key. sets de->de_entry_num of de

		/*

		 * ugly, but rename needs de_bh, de_deh, de_name,

		 * de_namelen, de_objectid set

 Keyed 32-bit hash function using TEA in a Davis-Meyer function */

/*

 * The third component is hashed, and you can choose from more than

 * one hash function.  Per directory hashes are not yet implemented

 * but are thought about. This function should be moved to hashes.c

 * Jedi, please do so.  -Hans

 take bits from 7-th to 30-th including both bounds */

		/*

		 * needed to have no names before "." and ".." those have hash

		 * value == 0 and generation conters 1 and 2 accordingly

 de's de_bh, de_ih, de_deh, de_item_num, de_entry_num are set already */

 used when hash collisions exist */

 hash value does not match, no need to check whole name */

 mark that this generation number is used */

 calculate pointer to name and namelen */

		/*

		 * de's de_name, de_namelen, de_recordlen are set.

		 * Fill the rest.

 key of pointed object */

 retval can be NAME_FOUND or NAME_FOUND_INVISIBLE */

		/*

		 * we have reached left most entry in the node. In common we

		 * have to go to the left neighbor, but if generation counter

		 * is 0 already, we know for sure, that there is no name with

		 * the same hash value

		/*

		 * FIXME: this work correctly only because hash value can not

		 *  be 0. Btw, in case of Yura's hash it is probably possible,

		 * so, this is a bug

/*

 * may return NAME_FOUND, NAME_FOUND_INVISIBLE, NAME_NOT_FOUND

 * FIXME: should add something like IOERROR

 we will search for this key in the tree */

 compare names for all entries having given hash value */

		/*

		 * there is no need to scan directory anymore.

		 * Given entry found or does not exist

		/*

		 * there is left neighboring item of this directory

		 * and given entry can be there

 while (1) */

		/*

		 * Propagate the private flag so we know we're

		 * in the priv tree.  Also clear IOP_XATTR

		 * since we don't have xattrs on xattr files.

/*

 * looks up the dentry of the parent directory for child.

 * taken from ext2_get_parent

/* add entry to the directory (entry can be hidden).



insert definition of when hidden directories are used here -Hans



	/*

	 * 48 bytes now and we avoid kmalloc if we

	 * create file with short name

 cannot allow items to be added into a busy deleted directory */

 each entry has unique key. compose it */

 get memory for composing the entry */

	/*

	 * fill buffer : directory entry head, name[, dir objectid | ,

	 * stat data | ,stat data, dir objectid ]

 JDM Endian safe if 0 */

 JDM Endian safe if 0 */

 put key (ino analog) to de */

 safe: k_dir_id is le */

 safe: k_objectid is le */

 copy name */

 padd by 0s to the 4 byte boundary */

	/*

	 * entry is ready to be pasted into tree, set 'visibility'

	 * and 'stat data in entry' attributes

 find the proper place for the new entry */

 there is no free generation number */

 adjust offset of directory enrty */

 update max-hash-collisions counter in reiserfs_sb_info */

 we need to re-search for the insertion point */

 perform the insertion of the entry that we have prepared */

 reiserfs_mkdir or reiserfs_rename will do that by itself */

/*

 * quota utility function, call if you've had to abort after calling

 * new_inode_init, and have not called reiserfs_new_inode yet.

 * This should only be called on inodes that do not have stat data

 * inserted into the tree yet.

/*

 * utility function that does setup for reiserfs_new_inode.

 * dquot_initialize needs lots of credits so it's better to have it

 * outside of a transaction, so we had to pull some bits of

 * reiserfs_new_inode out into this func.

	/*

	 * Make inode invalid - just in case we are going to drop it before

	 * the initialization happens

	/*

	 * the quota init calls have to know who to charge the quota to, so

	 * we have to set uid and gid here

	/*

	 * We need blocks for transaction + (user+group)*(quotas

	 * for new inode + update of quota for directory owner)

i_size */ , dentry,

visible */ );

	/*

	 * We need blocks for transaction + (user+group)*(quotas

	 * for new inode + update of quota for directory owner)

i_size */ , dentry,

 FIXME: needed for block and char devices only */

visible */ );

	/*

	 * We need blocks for transaction + (user+group)*(quotas

	 * for new inode + update of quota for directory owner)

	/*

	 * set flag that new packing locality created and new blocks

	 * for the content of that directory are not displaced yet

	/*

	 * inc the link count now, so another writer doesn't overflow

	 * it while we sleep later on.

symlink */,

 note, _this_ add_entry will not update dir's stat data */

visible */ );

 the above add_entry did not update dir's stat data */

	/*

	 * we can cheat because an old format dir cannot have

	 * EMPTY_DIR_SIZE, and a new format dir cannot have

	 * EMPTY_DIR_SIZE_V1.  So, if the inode is either size,

	 * regardless of disk format version, the directory is empty.

	/*

	 * we will be doing 2 balancings and update 2 stat data, we

	 * change quotas of the owner of the directory and of the owner

	 * of the parent directory.  The quota structure is possibly

	 * deleted only on last iput => outside of this transaction

		/*

		 * FIXME: compare key of an object and a key found in the entry

 cut entry from dir directory */

 page */

new file size - not used here */ );

 prevent empty directory from getting lost */

 not truncate */ );

	/*

	 * we must release path, because we did not call

	 * reiserfs_cut_from_item, or reiserfs_cut_from_item does not

	 * release path if operation was not complete

	/*

	 * in this transaction we can be doing at max two balancings and

	 * update two stat datas, we change quotas of the owner of the

	 * directory and of the owner of the parent directory. The quota

	 * structure is possibly deleted only on iput => outside of

	 * this transaction

		/*

		 * FIXME: compare key of an object and a key found in the entry

	/*

	 * we schedule before doing the add_save_link call, save the link

	 * count so we don't race

 prevent file from getting lost */

 not truncate */ );

	/*

	 * We need blocks for transaction + (user+group)*(quotas for

	 * new inode + update of quota for directory owner)

 reiserfs_new_inode iputs for us */

visible */ );

	/*

	 * We need blocks for transaction + update of quotas for

	 * the owners of the directory

 FIXME: sd_nlink is 32 bit for new files */

 inc before scheduling so reiserfs_unlink knows we are here */

 create new entry */

visible */ );

 de contains information pointing to an entry which */

 recalculate pointer to name and name length */

 FIXME: could check more */

 this must be added hidden entry */

 sets key of objectid the entry has to point to */

 JDM These operations are endian safe - both are le */

/*

 * process, that is going to call fix_nodes/do_balance must hold only

 * one path. If it holds 2 or more, it can get into endless waiting in

 * get_empty_nodes or its clones

	/*

	 * three balancings: (1) old name removal, (2) new name insertion

	 * and (3) maybe "save" link insertion

	 * stat data updates: (1) old directory,

	 * (2) new directory and (3) maybe old object stat data (when it is

	 * directory) and (4) maybe stat data of object to which new entry

	 * pointed initially and (5) maybe block containing ".." of

	 * renamed directory

	 * quota updates: two parent directories

	/*

	 * make sure that oldname still exists and points to an object we

	 * are going to rename

		/*

		 * make sure that directory being renamed has correct ".."

		 * and that its new parent directory has not too many links

		 * already

		/*

		 * directory is renamed, its parent directory will be changed,

		 * so find ".." entry

 inode number of .. must equal old_dir->i_ino */

 add new entry (or find the existing one) */

	/*

	 * this makes it so an fsync on an open fd for the old name will

	 * commit the rename operation

		/*

		 * look for old name using corresponding entry key

		 * (found by reiserfs_find_entry)

 look for new name by reiserfs_find_entry */

		/*

		 * reiserfs_add_entry should not return IO_ERROR,

		 * because it is called with essentially same parameters from

		 * reiserfs_add_entry above, and we'll catch any i/o errors

		 * before we get here.

 node containing ".." gets into transaction */

		/*

		 * we should check seals here, not do

		 * this stuff, yes? Then, having

		 * gathered everything into RAM we

		 * should lock the buffers, yes?  -Hans

		/*

		 * probably.  our rename needs to hold more

		 * than one path at once.  The seals would

		 * have to be written to deal with multi-path

		 * issues -chris

		/*

		 * sanity checking before doing the rename - avoid races many

		 * of the above checks could have scheduled.  We have to be

		 * sure our items haven't been shifted by another process.

	/*

	 * ok, all the changes can be done in one fell swoop when we

	 * have claimed all the buffers needed.

	/*

	 * thanks to Alex Adriaanse <alex_a@caltech.edu> for patch

	 * which adds ctime update of renamed object

 adjust link number of the victim */

 adjust ".." of renamed directory */

		/*

		 * there (in new_dir) was no directory, so it got new link

		 * (".."  of renamed directory)

 old directory lost one link - ".. " of renamed directory */

	/*

	 * looks like in 2.3.99pre3 brelse is atomic.

	 * so we can use pathrelse

	/*

	 * FIXME: this reiserfs_cut_from_item's return value may screw up

	 * anybody, but it will panic if will not be able to find the

	 * entry. This needs one more clean up

 not truncate */ );

 directories can handle most operations...  */

/*

 * symlink operations.. same as page_symlink_inode_operations, with xattr

 * stuff added

/*

 * special file operations.. just xattr/acl stuff

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

 *

 * Trivial changes by Alan Cox to add the LFS fixes

 *

 * Trivial Changes:

 * Rights granted to Hans Reiser to redistribute under other terms providing

 * he accepts all liability including but not limited to patent, fitness

 * for purpose, and direct or indirect claims arising from failure to perform.

 *

 * NO WARRANTY

	/*

	 * Writeback quota in non-journalled quota case - journalled quota has

	 * no dirty dquots

	/*

	 * We need s_umount for protecting quota writeback. We have to use

	 * trylock as reiserfs_cancel_old_flush() may be waiting for this work

	 * to complete with s_umount held.

 Requeue work if we are not cancelling it */

 Avoid clobbering the cancel state... */

	/*

	 * Avoid scheduling flush when sb is being shut down. It can race

	 * with journal shutdown and free still queued delayed work.

 Make sure no new flushes will be queued */

 Allow old_work to run again */

/*

 * this is used to delete "save link" when there are no items of a

 * file it points to. It can either happen if unlink is completed but

 * "save unlink" removal, or if file has both unlink and truncate

 * pending and as unlink completes first (because key of "save link"

 * protecting unlink is bigger that a key lf "save link" which

 * protects truncate), so there left no items to make truncate

 * completion on

 we are going to do one balancing */

 removals are protected by direct items */

/*

 * Look for uncompleted unlinks and truncates and complete them

 *

 * Called with superblock write locked.  If quotas are enabled, we have to

 * release/retake lest we call dquot_quota_on_mount(), proceed to

 * schedule_on_each_cpu() in invalidate_bdev() and deadlock waiting for the per

 * cpu worklets to complete flush_async_commits() that in turn wait for the

 * superblock write lock.

 compose key to look for "save" links */

 Needed for iput() to work correctly and not trash data */

 Turn on quotas so that they are updated correctly */

 there are no "save" links anymore */

 reiserfs_iget needs k_dirid and k_objectid only */

			/*

			 * the unlink almost completed, it just did not

			 * manage to remove "save" link and release objectid

 file is not unlinked */

			/*

			 * We got a truncate request for a dir which

			 * is impossible.  The only imaginable way is to

			 * execute unfinished truncate request then boot

			 * into old kernel, remove the file and create dir

			 * with the same key.

			/*

			 * not completed truncate found. New size was

			 * committed together with "save" link

 don't update modification time */

 not completed unlink (rmdir) found */

 removal gets completed in iput */

 Turn quotas off */

 Restore the flag back */

/*

 * to protect file being unlinked from getting lost we "safe" link files

 * being unlinked. This link will be deleted in the same transaction with last

 * item of file. mounting the filesystem we scan all these links and remove

 * files which almost got lost

 file can only get one "save link" of each kind */

 setup key of "save" link */

 unlink, rmdir, rename */

 item head of "safe" link */

length */ , 0xffff 
 truncate */

 item head of "safe" link */

length */ , 0 
 look for its place in the tree */

 body of "save" link */

 put "save" link into tree, don't charge quota to anyone */

 this opens transaction unlike add_save_link */

 we are going to do one balancing only */

 setup key of "save" link */

 unlink, rmdir, rename */

 truncate */

 don't take quota bytes from anywhere */

		/*

		 * Force any pending inode evictions to occur now. Any

		 * inodes to be removed that have extended attributes

		 * associated with them need to clean them up before

		 * we can release the extended attribute root dentries.

		 * shrink_dcache_for_umount will BUG if we don't release

		 * those before it's called so ->put_super is too late.

	/*

	 * change file system state to current state if it was mounted

	 * with read-write permissions

	/*

	 * note, journal_release checks for readonly mount, and can

	 * decide not to do a journal_end

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

 we don't mark inodes dirty, we just log them */

	/*

	 * this is really only used for atime updates, so they don't have

	 * to be included in O_SYNC or fsync

 tails=small is default so we don't show it */

 barrier=flush is default so we don't show it */

 errors=ro is default so we don't show it */

 data=ordered is default so we don't show it */

 Block allocator options */

/*

 * this struct is used in reiserfs_getopt () for containing the value for

 * those mount options that have values rather than being toggles.

	/*

	 * bitmask which is to set on mount_options bitmask

	 * when this value is found, 0 is no bits are to be changed.

	/*

	 * bitmask which is to clear on mount_options bitmask

	 * when this value is found, 0 is no bits are to be changed.

	 * This is applied BEFORE setmask

 Set this bit in arg_required to allow empty arguments */

/*

 * this struct is used in reiserfs_getopt() for describing the

 * set of reiserfs mount options

 0 if argument is not required, not 0 otherwise */

 list of values accepted by an option */

	/*

	 * bitmask which is to set on mount_options bitmask

	 * when this value is found, 0 is no bits are to be changed.

	/*

	 * bitmask which is to clear on mount_options bitmask

	 * when this value is found, 0 is no bits are to be changed.

	 * This is applied BEFORE setmask

 possible values for -o data= */

 possible values for -o barrier= */

/*

 * possible values for "-o block-allocator=" and bits which are to be set in

 * s_mount_opt of reiserfs specific part of in-core super block

/*

 * proceed only one option from a list *cur - string containing of mount

 * options

 * opts - array of options which are accepted

 * opt_arg - if option is found and requires an argument and if it is specifed

 * in the input - pointer to the argument is stored here

 * bit_flags - if option requires to set a certain bit - it is set here

 * return -1 if unknown option is found, opt->arg_required otherwise

	/*

	 * foo=bar,

	 * ^   ^  ^

	 * |   |  +-- option_end

	 * |   +-- arg_start

	 * +-- option_start

 assume argument cannot contain commas */

		/*

		 * Ugly special case, probably we should redo options

		 * parser so that it can understand several arguments for

		 * some options, also so that it can fill several bitfields

		 * with option values.

 for every option in the list */

	/*

	 * move to the argument, or to next option if argument is not

	 * required

 this catches "option=," if not allowed */

 *=NULLopt_arg contains pointer to argument */

 values possible for this option are listed in opt->values */

 returns 0 if something is wrong in option string, 1 - otherwise */

 string given via mount's -o */

				  /*

				   * after the parsing phase, contains the

				   * collection of bitflags defining what

				   * mount options were selected.

 strtol-ed from NNN of resize=NNN */

		/*

		 * Compatibility stuff, so that -o notail for old

		 * setups still work

		/*

		 * use default configuration: create tails, journaling on, no

		 * conversion to newest format

 wrong option is given */

 "resize=NNN" or "resize=auto" */

 From JFS code, to auto-get the size. */

 NNN does not look like a number */

 commit=NNN (time in seconds) */

 Hm, already assigned? */

 Some filename specified? */

 Add options that are safe here */

	/*

	 * Update the bitmask, taking care to keep

	 * the bits we're not allowed to change here

 0 means restore defaults. */

 remount read-only */

 it is read-only already */

 try to remount file system with read-only permissions */

 Mounting a rw partition read-only. */

 remount read-write */

 We are read-write already */

 now it is safe to call journal_begin */

 Mount a partition which is read-only, read-write */

 mark_buffer_dirty (SB_BUFFER_WITH_SB (s), 1); */

 this will force a full flush of all journal lists */

	/*

	 * ok, reiserfs signature (old or new) found in at the given offset

	/*

	 * magic is of non-standard journal filesystem, look at s_version to

	 * find which format is in use

		/*

		 * s_version of standard format may contain incorrect

		 * information, so we just look at the magic string

	/*

	 * new format is limited by the 32 bit wide i_blocks field, want to

	 * be one full block below that.

 after journal replay, reread all bitmap and super blocks */

 hash detection stuff */

/*

 * if root directory is empty - we set default - Yura's - hash and

 * warn about it

 * FIXME: we look for only one name in a directory. If tea and yura

 * both have the same value - we ask user to send report to the

 * mailing list

 allow override in this case */

 finds out which hash names are sorted with */

	/*

	 * reiserfs_hash_detect() == true if any of the hash mount options

	 * were used.  We must check them to make sure the user isn't

	 * using a bad hash value

		/*

		 * detection has found the hash, and we must check against the

		 * mount options

		/*

		 * find_hash_out was not called or

		 * could not determine the hash

	/*

	 * if we are mounted RW, and we have a new valid hash code, update

	 * the super

 return pointer to appropriate function */

 this is used to set up correct value for old partitions */

 should never happen */

 Set default values for options: non-aggressive tails, RO on errors */

 no preallocation minimum, be smart in reiserfs_file_write instead */

 Preallocate by 16 blocks (17-1) at once */

 setup default block allocator options */

	/*

	 * try old format (undistributed bitmap, super block in 8-th 1k

	 * block of a device)

	/*

	 * try new format (64-th 1k block), which can contain reiserfs

	 * super block

	/*

	 * Let's do basic sanity check to verify that underlying device is not

	 * smaller than the filesystem. If the check fails then abort and

	 * scream, because bad stuff will happen otherwise.

 make data=ordered the default */

		/*

		 * once this is set, journal_release must be called

		 * if we error out of the mount

	/*

	 * This path assumed to be called with the BKL in the old times.

	 * Now we have inherited the big reiserfs lock from it and many

	 * reiserfs helpers called in the mount path and elsewhere require

	 * this lock to be held even if it's not always necessary. Let's be

	 * conservative and hold it early. The window can be reduced after

	 * careful review of the code.

 define and initialize hash function */

		/*

		 * Clear out s_bmap_nr if it would wrap. We can handle this

		 * case, but older revisions can't. This will cause the

		 * file system to fail mount on those older implementations,

		 * avoiding corruption. -jeffm

			/*

			 * filesystem of format 3.5 either with standard

			 * or non-standard journal

 and -o conv is given */

					/*

					 * put magic string of 3.6 format.

					 * 2.2 will not be able to

					 * mount this filesystem anymore

		/*

		 * look for files which were to be removed in previous session

	/*

	 * mark hash in super block: it could be unset. overwrite should be ok

 kill the commit thread, free journal ram */

 changed to accommodate gcc folks. */

 Release dquot anyway to avoid endless cycle in dqput() */

 Are we journaling quotas? */

 Data block + inode block */

/*

 * Turn on quotas during mount time - we need to find the quota file and such...

/*

 * Standard function to be called on quota_on

 Quotafile not on the same filesystem? */

	/*

	 * We must not pack tails for quota files on reiserfs for quota

	 * IO to work

 Journaling quota? */

 Quotafile not of fs root? */

	/*

	 * When we journal data on quota file, we have to flush journal to see

	 * all updates to the file when we bypass pagecache...

 Just start temporary transaction and finish it */

/*

 * Read data from quotafile - avoid pagecache and such because we cannot afford

 * acquiring the locks... As quota files are never truncated and quota code

 * itself serializes the operations (and no one else should touch the files)

 * we don't have to be afraid of races

		/*

		 * Quota files are without tails so we can safely

		 * use this function

 A hole? */

/*

 * Write to quotafile (we know the transaction is already started and has

 * enough credits)

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/reiserfs/xattr.c

 *

 * Copyright (c) 2002 by Jeff Mahoney, <jeffm@suse.com>

 *

/*

 * In order to implement EA/ACLs in a clean, backwards compatible manner,

 * they are implemented as files in a "private" directory.

 * Each EA is in it's own file, with the directory layout like so (/ is assumed

 * to be relative to fs root). Inside the /.reiserfs_priv/xattrs directory,

 * directories named using the capital-hex form of the objectid and

 * generation number are used. Inside each directory are individual files

 * named with the name of the extended attribute.

 *

 * So, for objectid 12648430, we could have:

 * /.reiserfs_priv/xattrs/C0FFEE.0/system.posix_acl_access

 * /.reiserfs_priv/xattrs/C0FFEE.0/system.posix_acl_default

 * /.reiserfs_priv/xattrs/C0FFEE.0/user.Content-Type

 * .. or similar.

 *

 * The file contents are the text of the EA. The size is known based on the

 * stat data describing the file.

 *

 * In the case of system.posix_acl_access and system.posix_acl_default, since

 * these are special cases for filesystem ACLs, they are interpreted by the

 * kernel, in addition, they are negatively and positively cached and attached

 * to the inode so that unnecessary lookups are avoided.

 *

 * Locking works like so:

 * Directory components (xattr root, xattr dir) are protectd by their i_mutex.

 * The xattrs themselves are protected by the xattr_sem.

/*

 * Helpers for inode ops. We do this so that we don't have all the VFS

 * overhead and also for proper i_mutex annotation.

 * dir->i_mutex must be held for all of them.

/*

 * We use I_MUTEX_CHILD here to silence lockdep. It's safe because xattr

 * mutation ops aren't called during rename or splace, which are the

 * only other users of I_MUTEX_CHILD. It violates the ordering, but that's

 * better than allocating another subclass just for this code.

/*

 * The following are side effects of other operations that aren't explicitly

 * modifying extended attributes. This includes operations such as permissions

 * or ownership changes, object deletions, etc.

 A directory entry exists, but no file? */

 Skip out, an xattr has no xattrs associated with it */

		/*

		 * We start a transaction here to avoid a ABBA situation

		 * between the xattr root's i_mutex and the journal lock.

		 * This doesn't incur much additional overhead since the

		 * new transaction will just nest inside the

		 * outer transaction.

	/*

	 * -ENODATA: this object doesn't have any xattrs

	 * -EOPNOTSUPP: this file system doesn't have xattrs enabled on disk.

	 * Neither are errors

 This is the xattr dir, handle specially. */

	/*

	 * We only want the ownership bits. Otherwise, we'll do

	 * things like change a directory to a regular file if

	 * ATTR_MODE is set.

 No i_mutex, but the inode is unconnected. */

 inode->i_mutex: down */

/*

 * Returns a dentry corresponding to a specific extended attribute file

 * for the inode. If flags allow, the file is created. Otherwise, a

 * valid or negative dentry, or an error is returned.

 Internal operations on file data */

	/*

	 * We can deadlock if we try to free dentries,

	 * and an unlink/rmdir has just occurred - GFP_NOFS avoids this

	/*

	 * csum_partial() gives different results for little-endian and

	 * big endian hosts. Images created on little-endian hosts and

	 * mounted on big-endian hosts(and vice versa) will see csum mismatches

	 * when trying to fetch xattrs. Treating the hash as __wsum_t would

	 * lower the frequency of mismatch.  This is an endianness bug in

	 * reiserfs.  The return statement would result in a sparse warning. Do

	 * not fix the sparse warning so as to not hide a reminder of the bug.

 Generic extended attribute operations that can be used by xa plugins */

/*

 * inode->i_mutex: down

 We need to start a transaction to maintain lock ordering */

 Check before we start a transaction and then do nothing. */

/*

 * inode->i_mutex: down

	/*

	 * We can't have xattrs attached to v1 items since they don't have

	 * generation numbers

	/*

	 * priv_root needn't be initialized during mount so allow initial

	 * lookups to succeed.

 Just return the size needed */

 Magic doesn't match up.. */

/*

 * In order to implement different sets of xattr operations for each xattr

 * prefix with the generic xattr API, a filesystem should create a

 * null-terminated array of struct xattr_handler (one for each prefix) and

 * hang a pointer to it off of the s_xattr field of the superblock.

 *

 * The generic_fooxattr() functions will use this list to dispatch xattr

 * operations to the correct xattr_handler.

 This is the implementation for the xattr plugin infrastructure */

 Unsupported xattr name */ ||

/*

 * Inode operation listxattr()

 *

 * We totally ignore the generic listxattr here because it would be stupid

 * not to. Since the xattrs are organized in a directory, we can just

 * readdir to find them.

 Not an error if there aren't any xattrs */

 Actual operations that are exported to VFS-land */

	/*

	 * We need generation numbers to ensure that the oid mapping is correct

	 * v3.5 filesystems don't have them.

			/*

			 * Old format filesystem, but optional xattrs have

			 * been enabled. Error out.

	/*

	 * We don't do permission checks on the internal objects.

	 * Permissions are determined by the "owning" object.

 If we don't have the privroot located yet - go find it */

/*

 * We need to take a copy of the mount flags since things like

 * SB_RDONLY don't get set until *after* we're called.

 * mount_flags != mount_options

 The super_block SB_POSIXACL must mirror the (no)acl mount option. */

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

 this is one and only function that is used outside (do_balance.c) */

/*

 * modes of internal_shift_left, internal_shift_right and

 * internal_insert_childs

 define dest, src, dest parent, dest position */

 used in internal_shift_left */

 dest position is analog of dest->b_item_order */

 used in internal_shift_left */

/*

 * Insert count node pointers into buffer cur before position to + 1.

 * Insert count items into buffer cur before position to.

 * Items and node pointers are specified by inserted and bh respectively.

 prepare space for count disk_child */

 copy to_be_insert disk children */

 prepare space for count items  */

 copy item headers (keys) */

 sizes, item number */

&&&&&&&&&&&&&&&&&&&&&&&& */

&&&&&&&&&&&&&&&&&&&&&&&& */

&&&&&&&&&&&&&&&&&&&&&&&& */

&&&&&&&&&&&&&&&&&&&&&&&& */

/*

 * Delete del_num items and node pointers from buffer cur starting from

 * the first_i'th item and first_p'th pointers respectively.

 deleting */

 sizes, item number */

&&&&&&&&&&&&&&&&&&&&&&& */

&&&&&&&&&&&&&&&&&&&&&&& */

&&&&&&&&&&&&&&&&&&&&&&&& */

&&&&&&&&&&&&&&&&&&&&&&&& */

 delete n node pointers and items starting from given position */

	/*

	 * delete n pointers starting from `from' position in CUR;

	 * delete n keys starting from 'i_from' position in CUR;

/*

 * copy cpy_num node pointers and cpy_num - 1 items from buffer src to buffer

 * dest

 * last_first == FIRST_TO_LAST means that we copy first items

 *                             from src to tail of dest

 * last_first == LAST_TO_FIRST means that we copy last items

 *                             from src to head of dest

	/*

	 * ATTENTION! Number of node pointers in DEST is equal to number

	 * of items in DEST  as delimiting key have already inserted to

	 * buffer dest.

 coping */

dest_order = (last_first == LAST_TO_FIRST) ? 0 : nr_dest; */

src_order = (last_first == LAST_TO_FIRST) ? (nr_src - cpy_num + 1) : 0; */

 prepare space for cpy_num pointers */

 insert pointers */

 prepare space for cpy_num - 1 item headers */

 insert headers */

 sizes, item number */

&&&&&&&&&&&&&&&&&&&&&&&& */

&&&&&&&&&&&&&&&&&&&&&&&& */

&&&&&&&&&&&&&&&&&&&&&&&& */

&&&&&&&&&&&&&&&&&&&&&&&& */

/*

 * Copy cpy_num node pointers and cpy_num - 1 items from buffer src to

 * buffer dest.

 * Delete cpy_num - del_par items and node pointers from buffer src.

 * last_first == FIRST_TO_LAST means, that we copy/delete first items from src.

 * last_first == LAST_TO_FIRST means, that we copy/delete last items from src.

 shift_left occurs */

		/*

		 * delete cpy_num - del_par pointers and keys starting for

		 * pointers with first_pointer, for key - with first_item

 shift_right occurs */

 Insert n_src'th key of buffer src before n_dest'th key of buffer dest. */

 insert key before key with n_dest number */

 prepare space for inserting key */

 insert key */

 Change dirt, free space, item number fields. */

/*

 * Insert d_key'th (delimiting) key from buffer cfl to tail of dest.

 * Copy pointer_amount node pointers and pointer_amount - 1 items from

 * buffer src to buffer dest.

 * Replace  d_key'th key in buffer cfl.

 * Delete pointer_amount items and node pointers from buffer src.

 this can be invoked both to shift from S to L and from R to S */

				/*

				 * INTERNAL_FROM_S_TO_L | INTERNAL_FROM_R_TO_S

printk("pointer_amount = %d\n",pointer_amount); */

		/*

		 * insert delimiting key from common father of dest and

		 * src to node dest into position B_NR_ITEM(dest)

src->b_item_order */  == 0)

src->b_parent */ , 0);

 last parameter is del_parameter */

/*

 * Insert delimiting key to L[h].

 * Copy n node pointers and n - 1 items from buffer S[h] to L[h].

 * Delete n - 1 items and node pointers from buffer S[h].

 it always shifts from S[h] to L[h] */

 insert lkey[h]-th key  from CFL[h] to left neighbor L[h] */

 last parameter is del_parameter */

/*

 * Insert d_key'th (delimiting) key from buffer cfr to head of dest.

 * Copy n node pointers and n - 1 items from buffer src to buffer dest.

 * Replace  d_key'th key in buffer cfr.

 * Delete n items and node pointers from buffer src.

				 /*

				  * INTERNAL_FROM_S_TO_R | INTERNAL_FROM_L_TO_S

		/*

		 * insert delimiting key from common father of dest

		 * and src to dest node into position 0

tb->S[h] */ ||

 when S[h] disappers replace left delemiting key as well */

 last parameter is del_parameter */

/*

 * Insert delimiting key to R[h].

 * Copy n node pointers and n - 1 items from buffer S[h] to R[h].

 * Delete n - 1 items and node pointers from buffer S[h].

 it always shift from S[h] to R[h] */

 insert rkey from CFR[h] to right neighbor R[h] */

 last parameter is del_parameter */

/*

 * Delete insert_num node pointers together with their left items

 * and balance current node.

 delete child-node-pointer(s) together with their left item(s) */

 node S[h] (root of the tree) is empty now */

 choose a new root */

			/*

			 * switch super block's tree root block

REISERFS_SB(tb->tb_sb)->s_rs->s_tree_height --; */

&&&&&&&&&&&&&&&&&&&&&& */

 use check_internal if new root is an internal node */

&&&&&&&&&&&&&&&&&&&&&& */

 do what is needed for buffer thrown from tree */

 join S[h] with L[h] */

 join S[h] with R[h] */

 borrow from left neighbor L[h] */

 borrow from right neighbor R[h] */

tb->S[h], tb->CFR[h], tb->rkey[h], tb->R[h], -tb->rnum[h]); */

 split S[h] into two parts and put them into neighbors */

tb->L[h], tb->CFL[h], tb->lkey[h], tb->S[h], tb->lnum[h]); */

 Replace delimiting key of buffers L[h] and S[h] by the given key.*/

 Replace delimiting key of buffers S[h] and R[h] by the given key.*/

/*

 * if inserting/pasting {

 *   child_pos is the position of the node-pointer in S[h] that

 *   pointed to S[h-1] before balancing of the h-1 level;

 *   this means that new pointers and items must be inserted AFTER

 *   child_pos

 * } else {

 *   it is the position of the leftmost pointer that must be deleted

 *   (together with its corresponding key to the left of the pointer)

 *   as a result of the previous level's balancing.

 * }

 level of the tree */

 key for insertion on higher level    */

 node for insertion on higher level */

	/*

	 * we return this: it is 0 if there is no S[h],

	 * else it is tb->S[h]->b_item_order

tb->S[h]->b_item_order */ : 0;

	/*

	 * Using insert_size[h] calculate the number insert_num of items

	 * that must be inserted to or deleted from S[h].

 Check whether insert_num is proper * */

 Make balance in case insert_num < 0 */

		/*

		 * shift lnum[h] items from S[h] to the left neighbor L[h].

		 * check how many of new items fall into L[h] or CFL[h] after

		 * shifting

 number of items in L[h] */

 new items don't fall into L[h] or CFL[h] */

 all new items fall into L[h] */

 insert insert_num keys and node-pointers into L[h] */

tb->L[h], tb->S[h-1]->b_next */

			/*

			 * some items fall into L[h] or CFL[h],

			 * but some don't fall

 calculate number of new items that fall into L[h] */

tb->L[h], tb->S[h-1]->b_next, */

			/*

			 * replace the first node-ptr in S[h] by

			 * node-ptr to insert_ptr[k]

 tb->lnum[h] > 0 */

shift rnum[h] items from S[h] to the right neighbor R[h] */

		/*

		 * check how many of new items fall into R or CFR

		 * after shifting

 number of items in S[h] */

 new items fall into S[h] */

 all new items fall into R[h] */

 insert insert_num keys and node-pointers into R[h] */

tb->R[h],tb->S[h-1]->b_next */

 one of the items falls into CFR[h] */

 calculate number of new items that fall into R[h] */

tb->R[h], tb->R[h]->b_child, */

			/*

			 * replace the first node-ptr in R[h] by

			 * node-ptr insert_ptr[insert_num-k-1]

* Fill new node that appears instead of S[h] **/

 node S[h] is empty now */

 do what is needed for buffer thrown from tree */

 create new root */

 S[h] = empty buffer from the list FEB. */

 Put the unique node-pointer to S[h] that points to S[h-1]. */

&&&&&&&&&&&&&&&&&&&&&&&& */

&&&&&&&&&&&&&&&&&&&&&&&& */

 put new root into path structure */

 Change root in structure super block. */

 S_new = free buffer from list FEB */

 number of items in S[h] */

 new items don't fall into S_new */

  store the delimiting key for the next level */

 new_insert_key = (n - snum)'th key in S[h] */

 last parameter is del_par */

 all new items fall into S_new */

  store the delimiting key for the next level */

			/*

			 * new_insert_key = (n + insert_item - snum)'th

			 * key in S[h]

 last parameter is del_par */

			/*

			 * insert insert_num keys and node-pointers

			 * into S_new

S_new,tb->S[h-1]->b_next, */

 some items fall into S_new, but some don't fall */

 last parameter is del_par */

 calculate number of new items that fall into S_new */

S_new, */ 0, k,

 new_insert_key = insert_key[insert_num - k - 1] */

			/*

			 * replace first node-ptr in S_new by node-ptr

			 * to insert_ptr[insert_num-k-1]

 new_insert_ptr = node_pointer to S_new */

 S_new is released in unfix_nodes */

number of items in S[h] */

tbSh, */

          ( tb->S[h-1]->b_parent == tb->S[h] ) ? tb->S[h-1]->b_next :  tb->S[h]->b_child->b_next, */

/*

 *  Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

/*

 *  Written by Anatoly P. Pinchuk pap@namesys.botik.ru

 *  Programm System Institute

 *  Pereslavl-Zalessky Russia

 Does the buffer contain a disk block which is in the tree. */

 to get item head in le form */

/*

 * k1 is pointer to on-disk structure which is stored in little-endian

 * form. k2 is pointer to cpu variable. For key of items of the same

 * object this returns 0.

 * Returns: -1 if key1 < key2

 * 0 if key1 == key2

 * 1 if key1 > key2

/*

 * k1 is pointer to on-disk structure which is stored in little-endian

 * form. k2 is pointer to cpu variable.

 * Compare keys using all 4 key fields.

 * Returns: -1 if key1 < key2 0

 * if key1 = key2 1 if key1 > key2

 this part is needed only when tail conversion is in progress */

 find out version of the key */

/*

 * this does not say which one is bigger, it only returns 1 if keys

 * are not equal, 0 otherwise

/**************************************************************************

 *  Binary search toolkit function                                        *

 *  Search for an item in the array by the item key                       *

 *  Returns:    1 if found,  0 if not found;                              *

 *        *pos = number of the searched element if found, else the        *

 *        number of the first element that is larger than key.            *

/*

 * For those not familiar with binary search: lbound is the leftmost item

 * that it could be, rbound the rightmost item that it could be.  We examine

 * the item halfway between lbound and rbound, and that tells us either

 * that we can increase lbound, or decrease rbound, or that we have found it,

 * or if lbound <= rbound that there are no possible items, and we have not

 * found it. With each examination we cut the number of possible items it

 * could be by one more than half rounded down, or we find it.

 Key to search for. */

 First item in the array. */

 Number of items in the array. */

			     /*

			      * Item size in the array.  searched. Lest the

			      * reader be confused, note that this is crafted

			      * as a general function, and when it is applied

			      * specifically to the array of item headers in a

			      * node, width is actually the item header size

			      * not the item size.

 Number of the searched for element. */

 Key found in the array.  */

	/*

	 * bin_search did not find given key, it returns position of key,

	 * that is minimal and greater than the given one.

 Minimal possible key. It is never in the tree. */

 Maximal possible key. It is never in the tree. */

/*

 * Get delimiting key of the buffer by looking for it in the buffers in the

 * path, starting from the bottom of the path, and going upwards.  We must

 * check the path's validity at each step.  If the key is not in the path,

 * there is no delimiting key in the tree (buffer is first or last buffer

 * in tree), and in this case we return a special key, either MIN_KEY or

 * MAX_KEY.

 While not higher in path than first element. */

 Parent at the path is not in the tree now. */

 Check whether position in the parent is correct. */

 Check whether parent at the path really points to the child. */

		/*

		 * Return delimiting key if position in the parent

		 * is not equal to zero.

 Return MIN_KEY if we are in the root of the buffer tree. */

 Get delimiting key of the buffer at the path and its right neighbor. */

 Parent at the path is not in the tree now. */

 Check whether position in the parent is correct. */

		/*

		 * Check whether parent at the path really points

		 * to the child.

		/*

		 * Return delimiting key if position in the parent

		 * is not the last one.

 Return MAX_KEY if we are in the root of the buffer tree. */

/*

 * Check whether a key is contained in the tree rooted from a buffer at a path.

 * This works by looking at the left and right delimiting keys for the buffer

 * in the last path_element in the path.  These delimiting keys are stored

 * at least one level above that buffer in the tree. If the buffer is the

 * first or last node in the tree order then one of the delimiting keys may

 * be absent, and in this case get_lkey and get_rkey return a special key

 * which is MIN_KEY or MAX_KEY.

 Path which should be checked. */

 Key which should be checked. */

 left delimiting key is bigger, that the key we look for */

  if ( comp_keys(key, get_rkey(chk_path, sb)) != -1 ) */

 key must be less than right delimitiing key */

/*

 * Drop the reference to each buffer in a path and restore

 * dirty bits clean when preparing the buffer for the log.

 * This version should only be called from fix_nodes()

 Drop the reference to each buffer in a path */

 item number is too big or too small */

 free space does not match to calculated amount of use space */

	/*

	 * FIXME: it is_leaf will hit performance too much - we may have

	 * return 1 here

 check tables of item heads */

 one may imagine many more checks */

 returns 1 if buf looks like an internal node, 0 otherwise */

 this level is not possible for internal nodes */

 for internal which is not root we might check min number of keys */

 one may imagine many more checks */

/*

 * make sure that bh contains formatted node of reiserfs tree of

 * 'level'-th level

/*

 * The function is NOT SCHEDULE-SAFE!

 * It might unlock the write lock if we needed to wait for a block

 * to be read. Note that in this case it won't recover the lock to avoid

 * high contention resulting from too much lock requests, especially

 * the caller (search_by_key) will perform other schedule-unsafe

 * operations just after calling this function.

 *

 * @return depth of lock to be restored after read completes

	/*

	 * We are going to read some blocks on which we

	 * have a reference. It's safe, though we might be

	 * reading blocks concurrently changed if we release

	 * the lock. But it's still fine because we check later

	 * if the tree changed

		/*

		 * note, this needs attention if we are getting rid of the BKL

		 * you have to make sure the prepared bit isn't set on this

		 * buffer

/*

 * This function fills up the path from the root to the leaf as it

 * descends the tree looking for the key.  It uses reiserfs_bread to

 * try to find buffers in the cache given their block number.  If it

 * does not find them in the cache it reads them from disk.  For each

 * node search_by_key finds using reiserfs_bread it then uses

 * bin_search to look through that node.  bin_search will find the

 * position of the block_number of the next node if it is looking

 * through an internal node.  If it is looking through a leaf node

 * bin_search will find the position of the item which has key either

 * equal to given key, or which is the maximal key less than the given

 * key.  search_by_key returns a path that must be checked for the

 * correctness of the top of the path but need not be checked for the

 * correctness of the bottom of the path

/*

 * search_by_key - search for key (and item) in stree

 * @sb: superblock

 * @key: pointer to key to search for

 * @search_path: Allocated and initialized struct treepath; Returned filled

 *		 on success.

 * @stop_level: How far down the tree to search, Use DISK_LEAF_NODE_LEVEL to

 *		stop at leaf level.

 *

 * The function is NOT SCHEDULE-SAFE!

	/*

	 * As we add each node to a path we increase its count.  This means

	 * that we must be careful to release all nodes in a path before we

	 * either discard the path struct or re-use the path struct, as we

	 * do here.

	/*

	 * With each iteration of this loop we search through the items in the

	 * current node, and calculate the next current node(next path element)

	 * for the next iteration of this loop..

 prep path to have another element added to it. */

		/*

		 * Read the next tree node, and set the last element

		 * in the path to have a pointer to it.

			/*

			 * We'll need to drop the lock if we encounter any

			 * buffers that need to be read. If all of them are

			 * already up to date, we don't need to drop the lock.

		/*

		 * It is possible that schedule occurred. We must check

		 * whether the key to search is still in the tree rooted

		 * from the current buffer. If not then repeat search

		 * from the root.

			/*

			 * Get the root block number so that we can

			 * repeat the search starting from the root.

 repeat search from the root */

		/*

		 * only check that the key is in the buffer if key is not

		 * equal to the MAX_KEY. Latter case is only possible in

		 * "finish_unfinished()" processing during mount.

		/*

		 * make sure, that the node contents look like a node of

		 * certain level

 ok, we have acquired next formatted node in the tree */

 we are not in the stop level */

		/*

		 * item has been found, so we choose the pointer which

		 * is to the right of the found one

		/*

		 * if item was not found we choose the position which is to

		 * the left of the found item. This requires no code,

		 * bin_search did it already.

		/*

		 * So we have chosen a position in the current node which is

		 * an internal node.  Now we calculate child block number by

		 * position in the node.

		/*

		 * if we are going to read leaf nodes, try for read

		 * ahead as well

				/*

				 * check to make sure we're in the same object

/*

 * Form the path to an item and position in this item which contains

 * file byte defined by key. If there is no such item

 * corresponding to the key, we point the path to the item with

 * maximal key less than key, and *pos_in_item is set to one

 * past the last entry/byte in the item.  If searching for entry in a

 * directory item, and it is not found, *pos_in_item is set to one

 * entry more than the entry with maximal key which is less than the

 * sought key.

 *

 * Note that if there is no entry in this same node which is one more,

 * then we point to an imaginary entry.  for direct items, the

 * position is in units of bytes, for indirect items the position is

 * in units of blocknr entries, for directory items the position is in

 * units of directory entries.

 The function is NOT SCHEDULE-SAFE! */

 Key to search (cpu variable) */

 Filled up by this function. */

 pointer to on-disk structure */

 If searching for directory entry. */

 If not searching for directory entry. */

 If item is found. */

 Item is not found. Set path to the previous item. */

 FIXME: quite ugly this far */

 Needed byte is contained in the item pointed to by the path. */

	/*

	 * Needed byte is not contained in the item pointed to by the

	 * path. Set pos_in_item out of the item.

 Compare given item and item pointed to by the path. */

 Last buffer at the path is not in the tree. */

 Last path position is invalid. */

 we need only to know, whether it is the same item */

 prepare for delete or cut of direct item */

 item has to be deleted */

 new file gets truncated */

 this was new_file_length < le_ih ... */

 Delete this item. */

 Calculate first position and size for cutting from item. */

 Cut from this item. */

 old file: items may have any length */

 Delete this item. */

 Calculate first position and size for cutting from item. */

 Cut from this item. */

 Delete the directory item containing "." and ".." entry. */

		/*

		 * Delete the directory item such as there is one record only

		 * in this item

 Cut one record from the directory item. */

/*

 * If the path points to a directory or direct item, calculate mode

 * and the size cut, for balance.

 * If the path points to an indirect item, remove some number of its

 * unformatted nodes.

 * In case of file truncate calculate whether this item must be

 * deleted/truncated or last unformatted node of this item will be

 * converted to a direct item.

 * This function returns a determination of what balance mode the

 * calling function should employ.

				      /*

				       * Number of unformatted nodes

				       * which were removed from end

				       * of the file.

 MAX_KEY_OFFSET in case of delete. */

 Stat_data item. */

 Directory item. */

 Direct item. */

 Case of an indirect item. */

		/*

		 * prepare_for_delete_or_cut() is called by

		 * reiserfs_delete_item()

		    /*

		     * Each unformatted block deletion may involve

		     * one additional bitmap block into the transaction,

		     * thereby the initial journal space reservation

		     * might not be enough.

		/*

		 * a trick.  If the buffer has been logged, this will

		 * do nothing.  If we've broken the loop without logging

		 * it, it will restore the buffer

		/*

		 * Nothing was cut. maybe convert last unformatted node to the

		 * direct item?

 Calculate number of bytes which will be deleted or cut during balance */

		/*

		 * return EMPTY_DIR_SIZE; We delete emty directories only.

		 * we can't use EMPTY_DIR_SIZE, as old format dirs have a

		 * different empty size.  ick. FIXME, is this right?

/*

 * Delete object item.

 * th       - active transaction handle

 * path     - path to the deleted item

 * item_key - key to search for the deleted item

 * indode   - used for updating i_blocks and quotas

 * un_bh    - NULL or unformatted node pointer

size is unknown */ );

 file system changed, repeat search */

 while (1) */

 reiserfs_delete_item returns item length when success */

	/*

	 * hack so the quota code doesn't have to guess if the file has a

	 * tail.  On tail insert, we allocate quota for 1 unformatted node.

	 * We test the offset because the tail might have been

	 * split into multiple items, and we only want to decrement for

	 * the unfm node once

		/*

		 * We are in direct2indirect conversion, so move tail contents

		 * to the unformatted node

		/*

		 * note, we do the copy before preparing the buffer because we

		 * don't care about the contents of the unformatted node yet.

		 * the only thing we really care about is the direct item's

		 * data is in the unformatted node.

		 *

		 * Otherwise, we would have to call

		 * reiserfs_prepare_for_journal on the unformatted node,

		 * which might schedule, meaning we'd have to loop all the

		 * way back up to the start of the while loop.

		 *

		 * The unformatted node must be dirtied later on.  We can't be

		 * sure here if the entire tail has been deleted yet.

		 *

		 * un_bh is from the page cache (all unformatted nodes are

		 * from the page cache) and might be a highmem page.  So, we

		 * can't use un_bh->b_data.

		 * -clm

 Perform balancing after all resources have been collected at once. */

 Return deleted body length */

/*

 * Summary Of Mechanisms For Handling Collisions Between Processes:

 *

 *  deletion of the body of the object is performed by iput(), with the

 *  result that if multiple processes are operating on a file, the

 *  deletion of the body of the file is deferred until the last process

 *  that has an open inode performs its iput().

 *

 *  writes and truncates are protected from collisions by use of

 *  semaphores.

 *

 *  creates, linking, and mknod are protected from collisions with other

 *  processes by making the reiserfs_add_entry() the last step in the

 *  creation, and then rolling back all changes if there was a collision.

 *  - Hans

 this deletes item which never gets split */

			/*

			 * No need for a warning, if there is just no free

			 * space to insert '..' item into the

			 * newly-created subdir

			/*

			 * Should we count quota for item? (we don't

			 * count quotas for save-links)

 IO_ERROR, NO_DISK_SPACE, etc */

 for directory this deletes item containing "." and ".." */

no timestamp updates */ );

 USE_INODE_GENERATION_COUNTER */

				/*

				 * we want to unmap the buffers that contain

				 * the tail, and all the buffers after it

				 * (since the tail must be at the end of the

				 * file).  We don't want to unmap file data

				 * before the tail, since it might be dirty

				 * and waiting to reach disk

	/*

	 * the page being sent in could be NULL if there was an i/o error

	 * reading in the last block.  The user will hit problems trying to

	 * read the file, but for now we just skip the indirect2direct

 leave tail in an unformatted node */

 Perform the conversion to a direct_item. */

/*

 * we did indirect_to_direct conversion. And we have inserted direct

 * item successesfully, but there were no disk space to cut unfm

 * pointer being converted. Therefore we have to delete inserted

 * direct item(s)

 look for the last byte of the tail */

unbh not needed */ );

 (Truncate or cut entry) or delete object item. Returns < 0 on failure */

	/*

	 * Every function which is going to call do_balance must first

	 * create a tree_balance structure.  Then it must fill up this

	 * structure by using the init_tb_struct and fix_nodes functions.

	 * After that we can make tree balancing.

 Amount to be cut. */

 Number of the removed unformatted nodes. */

 Mode of the balance. */

	/*

	 * Repeat this loop until we either cut the item without needing

	 * to balance, or we fix_nodes without schedule occurring

		/*

		 * Determine the balance mode, position of the first byte to

		 * be cut, and size to be cut.  In case of the indirect item

		 * free unformatted nodes which are pointed to by the cut

		 * pointers.

			/*

			 * convert last unformatted node to direct item or

			 * leave tail in the unformatted node

 tail has been left in the unformatted node */

			/*

			 * removing of last unformatted node will

			 * change value we have to return to truncate.

			 * Save it

			/*

			 * So, we have performed the first part of the

			 * conversion:

			 * inserting the new direct item.  Now we are

			 * removing the last unformatted node pointer.

			 * Set key to search for it.

 while */

 check fix_nodes results (IO_ERROR or NO_DISK_SPACE) */

			/*

			 * FIXME: this seems to be not needed: we are always

			 * able to cut item

 go ahead and perform balancing */

 Calculate number of bytes that need to be cut from the item. */

	/*

	 * For direct items, we only change the quota when deleting the last

	 * item.

 FIXME: this is to keep 3.5 happy */

		/*

		 * we are going to complete indirect2direct conversion. Make

		 * sure, that we exactly remove last unformatted node pointer

		 * of the item

		/*

		 * it would be useful to make sure, that right neighboring

		 * item is direct item of this file

		/*

		 * we've done an indirect->direct conversion.  when the

		 * data block was freed, it was removed from the list of

		 * blocks that must be flushed before the transaction

		 * commits, make sure to unmap and invalidate it

/*

 * Truncate file to the new size. Note, this must be called with a

 * transaction already started

 ->i_size contains new size */

 up to date for last block */

			 /*

			  * when it is called by file_release to convert

			  * the tail - no timestamps should be updated

 Path to the current object item. */

 Pointer to an item header. */

 Key to search for a previous file item. */

 Old file size. */

 New file size. */

 Number of deleted or truncated bytes. */

 deletion of directory - no need to update timestamps */

 Get new file size. */

 FIXME: note, that key type is unimportant here */

 Get real file size (total length of all file items) */

		/*

		 * this may mismatch with real file size: if last direct item

		 * had no padding zeros and last unformatted node had no free

		 * space, this file would have this file size

	/*

	 * are we doing a full truncate or delete, if so

	 * kick in the reada code

 Update key to search for the last file item. */

 Cut or delete file item. */

 Change key to search the last file item. */

		/*

		 * While there are bytes to truncate and previous

		 * file item is presented in the tree.

		/*

		 * This loop could take a really long time, and could log

		 * many more blocks than a transaction can hold.  So, we do

		 * a polite journal end here, and if the transaction needs

		 * ending, we make sure the file is consistent before ending

		 * the current trans and starting a new one

 this is truncate, not file closing */

 this makes sure, that we __append__, not overwrite or add holes */

 config reiserfs check */

/*

 * Paste bytes to the existing item.

 * Returns bytes number pasted into the item.

 Path to the pasted item. */

 Key to search for the needed item. */

 Inode item belongs to */

 Pointer to the bytes to paste. */

 Size of pasted bytes. */

 DQUOT_* can schedule, must check before the fix_nodes */

 file system changed while we were in the fix_nodes */

	/*

	 * Perform balancing after all resources are collected by fix_nodes,

	 * and accessing them will not risk triggering schedule.

ih */ , body, M_PASTE);

 this also releases the path */

/*

 * Insert new item into the buffer at the path.

 * th   - active transaction handle

 * path - path to the inserted item

 * ih   - pointer to the item header to insert

 * body - pointer to the bytes to insert

 Do we count quotas for item? */

		/*

		 * hack so the quota code doesn't have to guess

		 * if the file has a tail, links are always tails,

		 * so there's no guessing needed

		/*

		 * We can't dirty inode here. It would be immediately

		 * written but appropriate stat item isn't inserted yet...

	/*

	 * DQUOT_* can schedule, must check to be sure calling

	 * fix_nodes is safe

 file system changed while we were in the fix_nodes */

 make balancing after all resources will be collected at a time */

 also releases the path */

 SPDX-License-Identifier: GPL-2.0

/*

 * Write ahead logging implementation copyright Chris Mason 2000

 *

 * The background commits make this code very interrelated, and

 * overly complex.  I need to rethink things a bit....The major players:

 *

 * journal_begin -- call with the number of blocks you expect to log.

 *                  If the current transaction is too

 *		    old, it will block until the current transaction is

 *		    finished, and then start a new one.

 *		    Usually, your transaction will get joined in with

 *                  previous ones for speed.

 *

 * journal_join  -- same as journal_begin, but won't block on the current

 *                  transaction regardless of age.  Don't ever call

 *                  this.  Ever.  There are only two places it should be

 *                  called from, and they are both inside this file.

 *

 * journal_mark_dirty -- adds blocks into this transaction.  clears any flags

 *                       that might make them get sent to disk

 *                       and then marks them BH_JDirty.  Puts the buffer head

 *                       into the current transaction hash.

 *

 * journal_end -- if the current transaction is batchable, it does nothing

 *                   otherwise, it could do an async/synchronous commit, or

 *                   a full flush of all log and real blocks in the

 *                   transaction.

 *

 * flush_old_commits -- if the current transaction is too old, it is ended and

 *                      commit blocks are sent to disk.  Forces commit blocks

 *                      to disk for all backgrounded commits that have been

 *                      around too long.

 *		     -- Note, if you call this as an immediate flush from

 *		        within kupdate, it will ignore the immediate flag

 gets a struct reiserfs_journal_list * from a list head */

 must be correct to keep the desc and commit structs at 4k */

read ahead */

 cnode stat bits.  Move these into reiserfs_fs.h */

 this block was freed, and can't be written.  */

 this block was freed during this transaction, and can't be written */

 used in flush_journal_list */

 journal list state bits */

 someone will commit this list */

 flags for do_journal_end */

 flush commit and real blocks */

 end and commit this transaction */

 wait for the log blocks to hit the disk */

 values for join in do_journal_begin_r */

 regular journal begin */

 join the running transaction if at all possible */

 called from cleanup code, ignores aborted flag */

/*

 * clears BH_Dirty and sticks the buffer on the clean list.  Called because

 * I can't allow refile_buffer to make schedule happen after I've freed a

 * block.  Look at remove_from_transaction and journal_mark_freed for

 * more details.

 this is ok, we'll try again when more are needed */

/*

 * only call this on FS unmount.

/*

 * get memory for JOURNAL_NUM_BITMAPS worth of bitmaps.

 * jb_array is the array to be filled in.

/*

 * find an available list bitmap.  If you can't find one, flush a commit list

 * and try again

 double check to make sure if flushed correctly */

/*

 * allocates a new chunk of X nodes, and links them all together as a list.

 * Uses the cnode->next and cnode->prev pointers

 * returns NULL on failure

 if last one, overwrite it after the if */

 pulls a cnode off the free list, or returns NULL on failure */

/*

 * returns a cnode to the free list

 memset(cn, 0, sizeof(struct reiserfs_journal_cnode)) ; */

 not needed with the memset, but I might kill the memset, and forget to do this */

/*

 * return a cnode with same dev, block number and size in table,

 * or null if not found

/*

 * this actually means 'can this block be reallocated yet?'.  If you set

 * search_all, a block can only be allocated if it is not in the current

 * transaction, was not freed by the current transaction, and has no chance

 * of ever being overwritten by a replay after crashing.

 *

 * If you don't set search_all, a block can only be allocated if it is not

 * in the current transaction.  Since deleting a block removes it from the

 * current transaction, this case should never happen.  If you don't set

 * search_all, make sure you never write the block without logging it.

 *

 * next_zero_bit is a suggestion about the next block to try for find_forward.

 * when bl is rejected because it is set in a journal list bitmap, we search

 * for the next zero bit in the bitmap that rejected bl.  Then, we return

 * that through next_zero_bit for find_forward to try.

 *

 * Just because we return something in next_zero_bit does not mean we won't

 * reject it on the next call to reiserfs_in_journal

 always start this at zero. */

	/*

	 * If we aren't doing a search_all, this is a metablock, and it

	 * will be logged before use.  if we crash before the transaction

	 * that freed it commits,  this transaction won't have committed

	 * either, and the block will never be written

 is it in any old transactions? */

 is it in the current transaction.  This should never happen */

 safe for reuse */

 insert cn into table */

 lock the current transaction */

 unlock the current transaction */

/*

 * this used to be much more involved, and I'm keeping it just in case

 * things get ugly again.  it gets called by flush_commit_list, and

 * cleans up any data stored about blocks freed during a transaction.

/*

 * If page->mapping was null, we failed to truncate this page for

 * some reason.  Most likely because it was truncated after being

 * logged via data=journal.

 *

 * This does a check to see if the buffer belongs to one of these

 * lost pages before doing the final put_bh.  If page->mapping was

 * null, it tries to free buffers on the page, which should make the

 * final put_page drop the page from the lru.

/*

 * we want to free the jh when the buffer has been written

 * and waited on

		/*

		 * buffer must be locked for __add_jh, should be able to have

		 * two adds at the same time

		/*

		 * in theory, dirty non-uptodate buffers should never get here,

		 * but the upper layer io error paths still have a few quirks.

		 * Handle them here as gracefully as we can

		/*

		 * ugly interaction with invalidatepage here.

		 * reiserfs_invalidate_page will pin any buffer that has a

		 * valid journal head from an older transaction.  If someone

		 * else sets our buffer dirty after we write it in the first

		 * loop, and then someone truncates the page away, nobody

		 * will ever write the buffer. We're safe if we write the

		 * page one last time after freeing the journal header.

	/*

	 * first we walk backwards to find the oldest uncommitted transation

 if we didn't find any older uncommitted transactions, return now */

 list we were called with is gone, return */

				/*

				 * the one we just flushed is gone, this means

				 * all older lists are also gone, so first_jl

				 * is no longer valid either.  Go back to the

				 * beginning.

/*

 * if this journal list still has commit blocks unflushed, send them to disk.

 *

 * log areas must be flushed in order (transaction 2 can't commit before

 * transaction 1) Before the commit block can by written, every other log

 * block must be safely on disk

	/*

	 * before we can put our commit blocks on disk, we have to make

	 * sure everyone older than us is on disk too

			/*

			 * list disappeared during flush_older_commits.

			 * return

 make sure nobody is trying to flush this one at the same time */

 this commit is done, exit */

		/*

		 * We might sleep in numerous places inside

		 * write_ordered_buffers. Relax the write lock.

	/*

	 * for the description block and all the log blocks, submit any buffers

	 * that haven't already reached the disk.  Try to write at least 256

	 * log blocks. later on, we will only wait on blocks that correspond

	 * to this transaction, but while we're unplugging we might as well

	 * get a chunk of data on there.

		/*

		 * since we're using ll_rw_blk above, it might have skipped

		 * over a locked buffer.  Double check here

 redundant, sync_dirty_buffer() checks */

 once for journal_find_get_block */

 once due to original getblk in do_journal_end */

	/*

	 * If there was a write error in the journal - we can't commit

	 * this transaction - it will be invalid and, if successful,

	 * will just end up propagating the write error out to

	 * the file system.

	/*

	 * If there was a write error in the journal - we can't commit this

	 * transaction - it will be invalid and, if successful, will just end

	 * up propagating the write error out to the filesystem.

	/*

	 * now, every commit block is on the disk.  It is safe to allow

	 * blocks freed during this transaction to be reallocated

 mark the metadata dirty */

/*

 * flush_journal_list frequently needs to find a newer transaction for a

 * given block.  This does that, or returns NULL if it can't find anything

/*

 * once all the real blocks have been flushed, it is safe to remove them

 * from the journal list for this transaction.  Aside from freeing the

 * cnode, this also allows the block to be reallocated for data blocks

 * if it had been deleted.

	/*

	 * which is better, to lock once around the whole loop, or

	 * to lock for each call to remove_journal_hash?

/*

 * if this timestamp is greater than the timestamp we wrote last to the

 * header block, write it to the header block.  once this is done, I can

 * safely say the log area for this transaction won't ever be replayed,

 * and I can start releasing blocks in this transaction for reuse as data

 * blocks.  called by flush_journal_list, before it calls

 * remove_all_from_journal_list

/*

** flush any and all journal lists older than you are

** can only be called from flush_journal_list

	/*

	 * we know we are the only ones flushing things, no extra race

	 * protection is required.

 Did we wrap? */

 do not flush all */

 other_jl is now deleted from the list */

/*

 * flush a journal list, both commit and real blocks

 *

 * always set flushall to 1, unless you are calling from inside

 * flush_journal_list

 *

 * IMPORTANT.  This can only be called while there are no journal writers,

 * and the journal is locked.  That means it can only be called from

 * do_journal_end, or by journal_release

 if flushall == 0, the lock is already held */

 if all the work is already done, get out of here */

	/*

	 * start by putting the commit list on disk.  This will also flush

	 * the commit lists of any olders transactions

 are we done now? */

	/*

	 * loop through each cnode, see if we need to write it,

	 * or wait on a more recent transaction, or just ignore it

 blocknr of 0 is no longer in the hash, ignore it */

		/*

		 * This transaction failed commit.

		 * Don't write out to the disk

		/*

		 * the order is important here.  We check pjl to make sure we

		 * don't clear BH_JDirty_wait if we aren't the one writing this

		 * block to disk

			/*

			 * we do this to make sure nobody releases the

			 * buffer while we are working with it

				/*

				 * everything with !pjl && jwait

				 * should be writable

		/*

		 * if someone has this block in a newer transaction, just make

		 * sure they are committed, and don't try writing it to disk

		/*

		 * bh == NULL when the block got to disk on its own, OR,

		 * the block got freed in a future transaction

		/*

		 * this should never happen.  kupdate_one_transaction has

		 * this list locked while it works, so we should never see a

		 * buffer here that is not marked JDirty_wait

			/*

			 * we inc again because saved_bh gets decremented

			 * at free_cnode

			/*

			 * we incremented this to keep others from

			 * taking the buffer head away

				/*

				 * note, we must clear the JDirty_wait bit

				 * after the up to date check, otherwise we

				 * race against our flushpage routine

 drop one ref for us */

 drop one ref for journal_mark_dirty */

	/*

	 * before we can update the journal header block, we _must_ flush all

	 * real blocks from all older transactions to disk.  This is because

	 * once the header block is updated, this transaction will not be

	 * replayed after a crash

	/*

	 * before we can remove everything from the hash tables for this

	 * transaction, we must make sure it can never be replayed

	 *

	 * since we are only called from do_journal_end, we know for sure there

	 * are no allocations going on while we are flushing journal lists.  So,

	 * we only need to update the journal header block for the last list

	 * being flushed

	/*

	 * not strictly required since we are freeing the list, but it should

	 * help find code using dead lists later on

		/*

		 * if the blocknr == 0, this has been cleared from the hash,

		 * skip it

			/*

			 * we can race against journal_mark_freed when we try

			 * to lock_buffer(cn->bh), so we have to inc the buffer

			 * count, and recheck things after locking

 note, cn->bh might be null now */

 used by flush_commit_list */

		/*

		 * look for a more recent transaction that logged this

		 * buffer.  Only the most recent transaction with a buffer in

		 * it is allowed to send that buffer to disk

			/*

			 * if the buffer is prepared, it will either be logged

			 * or restored.  If restored, we need to make sure

			 * it actually gets marked dirty

	/*

	 * we've got j_flush_mutex held, nobody is going to delete any

	 * of these lists out from underneath us

 did we wrap? */

 don't bother with older transactions */

/*

 * for o_sync and fsync heavy applications, they tend to use

 * all the journa list slots with tiny transactions.  These

 * trigger lots and lots of calls to update the header block, which

 * adds seeks and slows things down.

 *

 * This function tries to clear out a large chunk of the journal lists

 * at once, which makes everything faster since only the newest journal

 * list updates the header block

 in data logging mode, try harder to flush a lot of blocks */

 flush for 256 transactions or limit blocks, whichever comes first */

	/*

	 * try to find a group of blocks we can flush across all the

	 * transactions, but only bother if we've actually spanned

	 * across multiple lists

/*

 * removes any nodes in table with name block and dev as bh.

 * only touchs the hnext and hprev pointers.

			/*

			 * anybody who clears the cur->bh will also

			 * dec the nonzerolen

 must be after free_list_bitmaps */

	/*

	 * j_header_bh is on the journal dev, make sure

	 * not to release the journal dev until we brelse j_header_bh

/*

 * call on unmount.  Only set error to 1 if you haven't made your way out

 * of read_super() yet.  Any other caller must keep error at 0.

	/*

	 * we only want to flush out transactions if we were

	 * called with error == 0

 end the current trans */

		/*

		 * make sure something gets logged to force

		 * our way into the flush code

 this also catches errors during the do_journal_end above */

	/*

	 * We must release the write lock here because

	 * the workqueue job (flush_async_commit) needs this lock

	/*

	 * Cancel flushing of old commits. Note that neither of these works

	 * will be requeued because superblock is being shutdown and doesn't

	 * have SB_ACTIVE set.

 wait for all commits to finish */

 * call on unmount.  flush all journal trans, release all alloc'd ram */

 only call from an error condition inside reiserfs_read_super!  */

/*

 * compares description block with commit block.

 * returns 1 if they differ, 0 if they are the same

/*

 * returns 0 if it did not find a description block

 * returns -1 if it found a corrupt commit block

 * returns 1 if both desc and commit were valid

 * NOTE: only called during fs mount

		/*

		 * ok, we have a journal description block,

		 * let's see if the transaction was valid

/*

 * given the start, and values for the oldest acceptable transactions,

 * this either reads in a replays a transaction, or returns because the

 * transaction is invalid, or too old.

 * NOTE: only called during fs mount

	/*

	 * now we know we've got a good transaction, and it was

	 * inside the valid time ranges

 get all the buffer heads */

 make sure we don't try to replay onto log or reserved area */

 read in the log blocks, memcpy to the corresponding real block */

 flush out the real blocks */

	/*

	 * init starting values for the first transaction, in case

	 * this is the last transaction to be replayed.

 check for trans_id overflow */

/*

 * This function reads blocks starting from block and to max_block of bufsize

 * size (but no more than BUFNR blocks at a time). This proved to improve

 * mounting speed on self-rebuilding raid5 arrays at least.

 * Right now it is only used from journal code. But later we might use it

 * from other places.

 * Note: Do not use journal_getblk/sb_getblk functions here!

/*

 * read and replay the log

 * on a clean unmount, the journal header's next unflushed pointer will be

 * to an invalid transaction.  This tests that before finding all the

 * transactions in the log, which makes normal mount times fast.

 *

 * After a crash, this starts with the next unflushed transaction, and

 * replays until it finds one too old, or invalid.

 *

 * On exit, it sets things up so the first transaction will work correctly.

 * NOTE: only called during fs mount

	/*

	 * step 1, read in the journal header block.  Check the transaction

	 * it says is the first unflushed, and if that transaction is not

	 * valid, replay is done

		/*

		 * now, we try to read the first unflushed offset.  If it

		 * is not valid, there is nothing more we can do, and it

		 * makes no sense to read through the whole log.

	/*

	 * ok, there are transactions that need to be replayed.  start

	 * with the first log block, find all the valid transactions, and

	 * pick out the oldest.

		/*

		 * Note that it is required for blocksize of primary fs

		 * device and journal device to be the same

 init all oldest_ values */

 one we just read was older */

	/*

	 * j_start does not get set correctly if we don't replay any

	 * transactions.  if we had a valid journal_header, set j_start

	 * to the first unflushed transaction value, copy the trans_id

	 * from the header

 check for trans_id overflow */

 needed to satisfy the locking in _update_journal_header_block */

		/*

		 * replay failed, caller must call free_journal_ram and abort

		 * the mount

 there is no "jdev" option and journal is on separate device */

/*

 * When creating/tuning a file system user can assign some

 * journal params within boundaries which depend on the ratio

 * blocksize/standard_blocksize.

 *

 * For blocks >= standard_blocksize transaction size should

 * be not less then JOURNAL_TRANS_MIN_DEFAULT, and not more

 * then JOURNAL_TRANS_MAX_DEFAULT.

 *

 * For blocks < standard_blocksize these boundaries should be

 * decreased proportionally.

 Non-default journal params.  Do sanity check for them. */

		/*

		 * Default journal params.

		 * The file system was created by old version

		 * of mkreiserfs, so some fields contain zeros,

		 * and we need to advise proper values for them

 must be called once on fs mount.  calls journal_read for you */

 reserved for journal area support */

	/*

	 * Sanity check to see is the standard journal fitting

	 * within first bitmap (actual for small blocksizes)

	/*

	 * Sanity check to see if journal first block is correct.

	 * If journal first block is invalid it can cause

	 * zeroing important superblock members.

 read journal header */

 make sure that journal matches to the super block */

	/*

	 * get_list_bitmap() may call flush_commit_list() which

	 * requires the lock. Calling flush_commit_list() shouldn't happen

	 * this early but I like to be paranoid.

/*

 * test for a polite end of the current transaction.  Used by file_write,

 * and should be used by delete to make sure they don't write more than

 * can fit inside a single transaction

 cannot restart while nested */

 this must be called inside a transaction */

 this must be called without a transaction started */

 this must be called without a transaction started */

	/*

	 * we don't want to use wait_event here because

	 * we only want to wait once.

/*

 * join == true if you must join an existing transaction.

 * join == false if you can deal with waiting for others to finish

 *

 * this will block until the transaction is joinable.  send the number of

 * blocks you expect to use in nblocks.

 set here for journal_join */

	/*

	 * if there is no room in the journal OR

	 * if this transaction is too old, and we weren't called joinable,

	 * wait for it to finish before beginning we don't sleep if there

	 * aren't other writers

 allow others to finish this transaction */

		/*

		 * don't mess with joining the transaction if all we

		 * have to do is wait for someone else to do a commit

 someone might have ended the transaction while we joined */

 we are the first writer, set trans_id */

	/*

	 * Re-set th->t_super, so we can properly keep track of how many

	 * persistent transactions there are. We need to do this so if this

	 * call is part of a failed restart_transaction, we can free it later

	/*

	 * if we're nesting into an existing transaction.  It will be

	 * persistent on its own

	/*

	 * this keeps do_journal_end from NULLing out the

	 * current->journal_info pointer

	/*

	 * this keeps do_journal_end from NULLing out the

	 * current->journal_info pointer

 we are nesting into the current transaction */

			/*

			 * we've ended up with a handle from a different

			 * filesystem.  save it and restore on journal_end.

			 * This should never really happen...

	/*

	 * I guess this boils down to being the reciprocal of clm-2100 above.

	 * If do_journal_begin_r fails, we need to put it back, since

/*

 * puts bh into the current transaction.  If it was already there, reorders

 * removes the old pointers from the hash, and puts new ones in (to make

 * sure replay happen in the right order).

 *

 * if it was dirty, cleans and files onto the clean list.  I can't let it

 * be dirty again until the transaction is committed.

 *

 * if j_len, is bigger than j_len_alloc, it pushes j_len_alloc to 10 + j_len.

 already in this transaction, we are done */

	/*

	 * this must be turned into a panic instead of a warning.  We can't

	 * allow a dirty or journal_dirty or locked buffer to be logged, as

	 * some changes could get to disk too early.  NOT GOOD.

	/*

	 * this error means I've screwed up, and we've overflowed

	 * the transaction.  Nothing can be done here, except make the

	 * FS readonly or panic.

 now put this guy on the end */

		/*

		 * we aren't allowed to close a nested transaction on a

		 * different filesystem from the one in the task struct

/*

 * removes from the current transaction, relsing and descrementing any counters.

 * also files the removed buffer directly onto the clean list

 *

 * called by journal_mark_freed when a block has been deleted

 *

 * returns 1 if it cleaned and relsed the buffer. 0 otherwise

 don't log this one */

/*

 * for any cnode in a journal list, it can only be dirtied of all the

 * transactions that include it are committed to disk.

 * this checks through each transaction, and returns 1 if you are allowed

 * to dirty, and 0 if you aren't

 *

 * it is called by dirty_journal_list, which is called after

 * flush_commit_list has gotten all the log blocks for a given

 * transaction on disk

 *

	/*

	 * first test hprev.  These are all newer than cn, so any node here

	 * with the same block number and dev means this node can't be sent

	 * to disk right now.

	/*

	 * then test hnext.  These are all older than cn.  As long as they

	 * are committed to the log, it is safe to write cn to disk

/*

 * syncs the commit blocks, but does not force the real buffers to disk

 * will wait until the current transaction is done/committed before returning

 you can sync while nested, very, very bad */

 writeback the pending async commits to disk */

 last entry is the youngest, commit it and you get everything */

/*

 * flushes any old transactions to disk

 * ends the current transaction if it is too old

	/*

	 * safety check so we don't flush while we are replaying the log during

	 * mount

	/*

	 * check the current transaction.  If there are no writers, and it is

	 * too old, finish it, and force the commit blocks to disk

			/*

			 * we're only being called from kreiserfsd, it makes

			 * no sense to do an async commit so that kreiserfsd

			 * can do it later

/*

 * returns 0 if do_journal_end should return right away, returns 1 if

 * do_journal_end should finish the commit

 *

 * if the current transaction is too old, but still has writers, this will

 * wait on j_join_wait until all the writers are done.  By the time it

 * wakes up, the transaction it was called has already ended, so it just

 * flushes the commit list and returns 0.

 *

 * Won't batch when flush or commit_now is set.  Also won't batch when

 * others are waiting on j_join_wait.

 *

 * Note, we can't allow the journal_end to proceed while there are still

 * writers in the log.

 <= 0 is allowed.  unmounting might not call begin */

	/*

	 * BUG, deal with case where j_len is 0, but people previously

	 * freed blocks need to be released will be dealt with by next

	 * transaction that actually writes something, but should be taken

	 * care of in this trans

	/*

	 * if wcount > 0, and we are called to with flush or commit_now,

	 * we wait on j_join_wait.  We will wake up when the last writer has

	 * finished the transaction, and started it on its way to the disk.

	 * Then, we flush the commit or journal list, and just return 0

	 * because the rest of journal end was already done for this

	 * transaction.

			/*

			 * sleep while the current transaction is

			 * still j_jlocked

 deal with old transactions where we are the last writers */

 don't batch when someone is waiting on j_join_wait */

 don't batch when syncing the commit or flushing the whole trans */

/*

 * Does all the work that makes deleting blocks safe.

 * when deleting a block mark BH_JNew, just remove it from the current

 * transaction, clean it's buffer_head and move on.

 *

 * otherwise:

 * set a bit for the block in the journal bitmap.  That will prevent it from

 * being allocated for unformatted nodes before this transaction has finished.

 *

 * mark any cnodes for this block as BLOCK_FREED, and clear their bh pointers.

 * That will prevent any old transactions with this block from trying to flush

 * to the real location.  Since we aren't removing the cnode from the

 * journal_list_hash, *the block can't be reallocated yet.

 *

 * Then remove it from the current transaction, decrementing any counters and

 * filing it on the clean list.

 if it is journal new, we just remove it from this transaction */

		/*

		 * set the bit for this block in the journal bitmap

		 * for this transaction

 Note, the entire while loop is not allowed to schedule.  */

		/*

		 * find all older transactions with this block,

		 * make sure they don't try to write it out

					/*

					 * remove_from_transaction will brelse

					 * the buffer if it was in the current

					 * trans

					/*

					 * since we are clearing the bh,

					 * we MUST dec nonzerolen

 get_hash grabs the buffer */

/*

 * returns -1 on error, 0 if no commits/barriers were done and 1

 * if a transaction was actually committed and the barrier was done

	/*

	 * is it from the current transaction,

	 * or from an unknown transaction?

		/*

		 * try to let other writers come in and

		 * grow this transaction

 someone might have ended this transaction while we joined */

		/*

		 * this gets tricky, we have to make sure the journal list in

		 * the inode still exists.  We know the list is still around

		 * if we've got a larger transaction id than the oldest list

			/*

			 * we only set ret to 1 when we know for sure

			 * the barrier hasn't been started yet on the commit

			 * block.

 otherwise the list is gone, and long since committed */

	/*

	 * for the whole inode, assume unset id means it was

	 * changed in the current transaction.  More conservative

 jl will be updated in __commit_trans_jl */

/*

 * before we can change a metadata block, we have to make sure it won't

 * be written to disk while we are altering it.  So, we must:

 * clean it

 * wait on it.

/*

 * long and ugly.  If flush, will not return until all commit

 * blocks and all real buffers in the trans are on disk.

 * If no_async, won't return until all commit blocks are on disk.

 *

 * keep reading, there are comments as you go along

 *

 * If the journal is aborted, we just clean up. Things like flushing

 * journal lists, etc just won't happen.

 commit bh */

 desc bh */

 start index of current log write */

	/*

	 * protect flush_older_commits from doing mistakes if the

	 * transaction ID counter gets overflowed.

	/*

	 * check_journal_end locks the journal, and unlocks if it does

	 * not return 1 it tells us if we should continue with the

	 * journal_end, or just return

 check_journal_end might set these, check again */

	/*

	 * j must wait means we have to flush the log blocks, and the

	 * real blocks for this transaction

	/*

	 * quota ops might need to nest, setup the journal_info pointer

	 * for them and raise the refcount so that it is > 0.

 it should not involve new blocks into the transaction */

 setup description block */

	/*

	 * setup commit block.  Don't write (keep it clean too) this one

	 * until after everyone else is written

 init this journal list */

	/*

	 * we lock the commit before doing anything because

	 * we want to make sure nobody tries to run flush_commit_list until

	 * the new transaction is fully setup, and we've already flushed the

	 * ordered bh list

 save the transaction id in case we need to commit it later */

	/*

	 * The ENTIRE FOR LOOP MUST not cause schedule to occur.

	 * for each real block, add it to the journal list hash,

	 * copy into real block index array in the commit or desc block

			/*

			 * make sure the block we are trying to log

			 * is not a block of journal or reserved area

	/*

	 * special check in case all buffers in the journal

	 * were marked for not logging

	/*

	 * we're about to dirty all the log blocks, mark the description block

	 * dirty now too.  Don't mark the commit block dirty until all the

	 * others are on disk

	/*

	 * first data block is j_start + 1, so add one to

	 * cur_write_start wherever you use it

 start at one so we don't get the desc again */

 copy all the real blocks into log area.  dirty log blocks */

			/*

			 * JDirty cleared sometime during transaction.

			 * don't log this one

	/*

	 * we are done with both the c_bh and d_bh, but

	 * c_bh must be written after all other commit blocks,

	 * so we dirty/relse c_bh in flush_commit_list, with commit_left <= 1.

 now it is safe to insert this transaction on the main list */

 reset journal values for the next transaction */

 check for trans_id overflow */

	/*

	 * make sure reiserfs_add_jh sees the new current_jl before we

	 * write out the tails

	/*

	 * tail conversion targets have to hit the disk before we end the

	 * transaction.  Otherwise a later transaction might repack the tail

	 * before this transaction commits, leaving the data block unflushed

	 * and clean, if we crash before the later transaction commits, the

	 * data block is lost.

	/*

	 * honor the flush wishes from the caller, simple commits can

	 * be done outside the journal lock, they are done below

	 *

	 * if we don't flush the commit list right now, we put it into

	 * the work queue so the people waiting on the async progress work

	 * queue don't wait for this proc to flush journal lists and such.

		/*

		 * Avoid queueing work when sb is being shut down. Transaction

		 * will be flushed on journal shutdown.

	/*

	 * if the next transaction has any chance of wrapping, flush

	 * transactions that might get overwritten.  If any journal lists

	 * are very old flush them as well.

				/*

				 * if we don't cross into the next

				 * transaction and we don't wrap, there is

				 * no way we can overlap any later transactions

				 * break now

				/*

				* we don't overlap anything from out start

				* to the end of the log, and our wrapped

				* portion doesn't overlap anything at

				* the start of the log.  We can break

 wake up any body waiting to join. */

	/*

	 * Re-set th->t_super, so we can properly keep track of how many

	 * persistent transactions there are. We need to do this so if this

	 * call is part of a failed restart_transaction, we can free it later

 Send the file system read only and refuse new transactions */

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

	/*

	 * We need blocks for transaction + (user+group) quota

	 * update (possibly delete)

	/*

	 * The = 0 happens when we abort creating a new inode

	 * for some reason like lack of space..

	 * also handles bad_inode case

		/*

		 * Do quota update inside a transaction for journaled quotas.

		 * We must do that after delete_object so that quota updates

		 * go into the same transaction as stat data deletion

		/*

		 * check return value from reiserfs_delete_object after

		 * ending the transaction

		/*

		 * all items of file are deleted, so we can remove

		 * "save" link

		 * we can't do anything about an error here

 not truncate */);

 no object items are in the tree */

 note this must go after the journal_end to prevent deadlock */

/*

 * take base of inode_key (it comes from inode always) (dirid, objectid)

 * and version from an inode, set offset and type of key

 when key is 0, do not set version and short key */

or ih_free_space */ )

    set_ih_free_space (ih, 0); */

	/*

	 * for directory items it is entry count, for directs and stat

	 * datas - 0xffff, for indirects - 0

/*

 * FIXME: we might cache recently accessed indirect item

 * Ugh.  Not too eager for that....

 * I cut the code until such time as I see a convincing argument (benchmark).

 * I don't want a bloated inode struct..., and I don't like code complexity....

/*

 * cutting the code is fine, since it really isn't in use yet and is easy

 * to add back in.  But, Vladimir has a really good idea here.  Think

 * about what happens for reading a file.  For each page,

 * The VFS layer calls reiserfs_readpage, who searches the tree to find

 * an indirect item.  This indirect item has X number of pointers, where

 * X is a big number if we've done the block allocation right.  But,

 * we only use one or two of these pointers during each call to readpage,

 * needlessly researching again later on.

 *

 * The size of the cache could be dynamic based on the size of the file.

 *

 * I'd also like to see us cache the location the stat data item, since

 * we are needlessly researching for that frequently.

 *

 * --chris

/*

 * If this page has a file tail in it, and

 * it was read in by get_block_create_0, the page data is valid,

 * but tail is still sitting in a direct item, and we can't write to

 * it.  So, look through this page, and check all the mapped buffers

 * to make sure they have valid block numbers.  Any that don't need

 * to be unmapped, so that __block_write_begin will correctly call

 * reiserfs_get_block to convert the tail into an unformatted node

/*

 * reiserfs_get_block does not need to allocate a block only if it has been

 * done already or non-hole position has been found in the indirect item

/*

 * files which were created in the earlier version can not be longer,

 * than 2 gb

 it is new file. */

 old file, but 'block' is inside of 2gb */

 we cannot restart while nested */

/*

 * it is called by get_block when create == 0. Returns block number

 * for 'block'-th logical block of file. When it hits direct item it

 * returns 0 (being called from bmap) or read direct item into piece

 * of page (bh_result)

 * Please improve the english/clarity in the comment above, as it is

 * hard to understand.

 prepare the key to look for the 'block'-th block of file */

		/*

		 * We do not return -ENOENT if there is a hole but page is

		 * uptodate, because it means that there is some MMAPED data

		 * associated with it that is yet to be written to disk.

		/*

		 * FIXME: here we could cache indirect item or part of it in

		 * the inode to avoid search_by_key in case of subsequent

		 * access to file

			/*

			 * We do not return -ENOENT if there is a hole but

			 * page is uptodate, because it means that there is

			 * some MMAPED data associated with it that is

			 * yet to be written to disk.

 requested data are in direct item(s) */

		/*

		 * we are called by bmap. FIXME: we can not map block of file

		 * when it is stored in direct item(s)

	/*

	 * if we've got a direct item, and the buffer or page was uptodate,

	 * we don't want to pull data off disk again.  skip to the

	 * end, where we map the buffer and return

		/*

		 * grab_tail_page can trigger calls to reiserfs_get_block on

		 * up to date pages without any buffers.  If the page is up

		 * to date, we don't want read old data off disk.  Set the up

		 * to date bit on the buffer instead and jump to the end

 read file tail into part of page */

	/*

	 * we only want to kmap if we are reading the tail into the page.

	 * this is not the common case, so we don't kmap until we are

	 * sure we need to.  But, this means the item might move if

	 * kmap schedules

		/*

		 * make sure we don't read more bytes than actually exist in

		 * the file.  This can happen in odd cases where i_size isn't

		 * correct, and when direct item padding results in a few

		 * extra bytes at the end of the direct item

		/*

		 * we done, if read direct item is not the last item of

		 * node FIXME: we could try to check right delimiting key

		 * to see whether direct item continues in the right

		 * neighbor or rely on i_size

 update key to look for the next piece */

 i/o error most likely */

	/*

	 * this buffer has valid data, but isn't valid for io.  mapping it to

	 * block #0 tells the rest of reiserfs it just has a tail in it

/*

 * this is called to create file map. So, _get_block_create_0 will not

 * read direct item

 do not read the direct item */

/*

 * special version of get_block that is only used by grab_tail_page right

 * now.  It is sent to __block_write_begin, and when you try to get a

 * block past the end of the file (or a block from a hole) it returns

 * -ENOENT instead of a valid buffer.  __block_write_begin expects to

 * be able to do i/o on the buffers returned, unless an error value

 * is also returned.

 *

 * So, this allows __block_write_begin to be used for reading a single block

 * in a page.  Where it does not produce a valid page for holes, or past the

 * end of the file.  This turns out to be exactly what we need for reading

 * tails for conversion.

 *

 * The point of the wrapper is forcing a certain value for create, even

 * though the VFS layer is calling this function with create==1.  If you

 * don't want to send create == GET_BLOCK_NO_HOLE to reiserfs_get_block,

 * don't use this function.

/*

 * This is special helper for reiserfs_get_block in case we are executing

 * direct_IO request.

	/*

	 * We set the b_size before reiserfs_get_block call since it is

	 * referenced in convert_tail_for_hole() that may be called from

	 * reiserfs_get_block()

 don't allow direct io onto tail pages */

		/*

		 * make sure future calls to the direct io funcs for this

		 * offset in the file fail by unmapping the buffer

	/*

	 * Possible unpacked tail. Flush the data before pages have

	 * disappeared

/*

 * helper function for when reiserfs_get_block is called for a hole

 * but the file tail is still in a direct item

 * bh_result is the buffer head for the hole

 * tail_offset is the offset of the start of the tail in the file

 *

 * This calls prepare_write, which will start a new transaction

 * you should not be in a transaction, or have any paths held when you

 * call this.

 always try to read until the end of the block */

	/*

	 * hole_page can be zero in case of direct_io, we are sure

	 * that we cannot get here if we write with O_DIRECT into tail page

	/*

	 * we don't have to make sure the conversion did not happen while

	 * we were locking the page because anyone that could convert

	 * must first take i_mutex.

	 *

	 * We must fix the tail page for writing because it might have buffers

	 * that are mapped, but have a block number of 0.  This indicates tail

	 * data that has been read directly into the page, and

	 * __block_write_begin won't trigger a get_block in this case.

 tail conversion might change the data in the page */

 b_blocknr_t is (unsigned) 32 bit int*/

	/*

	 * space reserved in transaction batch:

	 * . 3 balancings in direct->indirect conversion

	 * . 1 block involved into reiserfs_update_sd()

	 * XXX in practically impossible worst case direct2indirect()

	 * can incur (much) more than 3 balancings.

	 * quota update for user, group

	/*

	 * if !create, we aren't changing the FS, so we don't need to

	 * log anything, so we don't need to start a transaction

 find number of block-th logical block of the file */

	/*

	 * if we're already in a transaction, make sure to close

	 * any new transactions we start in this func

	/*

	 * If file is of such a size, that it might have a tail and

	 * tails are enabled  we should mark it as possibly needing

	 * tail packing on close

 set the key of the first byte in the 'block'-th block of file */

key length */ );

 we have to allocate block for the unformatted node */

		/*

		 * restart the transaction to give the journal a chance to free

		 * some blocks.  releases the path, so we have to go back to

		 * research if we succeed on the second try

		/*

		 * 'block'-th block is in the file already (there is

		 * corresponding cell in some indirect item). But it may be

		 * zero unformatted node pointer (hole)

 use allocated block to plug the hole */

		/*

		 * the item was found, so new blocks were not added to the file

		 * there is no need to make sure the inode is updated with this

		 * transaction

	/*

	 * desired position is not found or is in the direct item. We have

	 * to append file with holes up to 'block'-th block converting

	 * direct items to indirect one if necessary

 indirect item has to be inserted */

 free_space */ );

			/*

			 * we are going to add 'block'-th block to the file.

			 * Use allocated block for that

 ;) */

				/*

				 * retval == -ENOSPC, -EDQUOT or -EIO

				 * or -EEXIST

 direct item has to be converted */

			/*

			 * direct item we just found fits into block we have

			 * to map. Convert it into unformatted node: use

			 * bh_result for the conversion

				/*

				 * we have to pad file tail stored in direct

				 * item(s) up to block size and convert it

				 * to unformatted node. FIXME: this should

				 * also get into page cache

				/*

				 * ugly, but we can only end the transaction if

				 * we aren't nested

						/*

						 * the bitmap, the super,

						 * and the stat data == 3

			/*

			 * it is important the set_buffer_uptodate is done

			 * after the direct2indirect.  The buffer might

			 * contain valid data newer than the data on disk

			 * (read by readpage, changed, and then sent here by

			 * writepage).  direct2indirect needs to know if unbh

			 * was already up to date, so it can decide if the

			 * data in unbh needs to be replaced with data from

			 * the disk

			/*

			 * unbh->b_page == NULL in case of DIRECT_IO request,

			 * this means buffer will disappear shortly, so it

			 * should not be added to

				/*

				 * we've converted the tail, so we must

				 * flush unbh before the transaction commits

				/*

				 * mark it dirty now to prevent commit_write

				 * from adding this buffer to the inode's

				 * dirty buffer list

				/*

				 * AKPM: changed __mark_buffer_dirty to

				 * mark_buffer_dirty().  It's still atomic,

				 * but it sets the page dirty too, which makes

				 * it eligible for writeback at any time by the

				 * VM (which was also the case with

				 * __mark_buffer_dirty())

			/*

			 * append indirect item with holes if needed, when

			 * appending pointer to 'block'-th block use block,

			 * which is already allocated

			/*

			 * We use this in case we need to allocate

			 * only one block which is a fastpath

			/*

			 * indirect item has to be appended,

			 * set up key of that position

			 * (key type is unimportant)

				/*

				 * we are going to add target block to

				 * the file. Use allocated block for that

 paste hole to the indirect item */

				/*

				 * If kcalloc failed, max_to_insert becomes

				 * zero and it means we only have space for

				 * one block

				/*

				 * We need to mark new file size in case

				 * this function will be interrupted/aborted

				 * later on. And we may do this only for

				 * holes.

		/*

		 * this loop could log more blocks than we had originally

		 * asked for.  So, we have to allow the transaction to end

		 * if it is too big or too full.  Update the inode so things

		 * are consistent if we crash before the function returns

		 * release the path so that anybody waiting on the path before

		 * ending their transaction will be able to continue.

		/*

		 * inserting indirect pointers for a hole can take a

		 * long time.  reschedule if needed and also release the write

		 * lock for others.

/*

 * Compute real number of used bytes by file

 * Following three functions can go away when we'll have enough space in

 * stat item

	/*

	 * End of file is also in full block with indirect reference, so round

	 * up to the next block.

	 *

	 * there is just no way to know if the tail is actually packed

	 * on the file, so we have to assume it isn't.  When we pack the

	 * tail, we add 4 bytes to pretend there really is an unformatted

	 * node pointer

 Compute number of blocks used by file in ReiserFS counting */

 keeps fsck and non-quota versions of reiserfs happy */

	/*

	 * files from before the quota patch might i_blocks such that

	 * bytes < real_space.  Deal with that here to prevent it from

	 * going negative.

/*

 * BAD: new directories have stat data of new type and all other items

 * of old type. Version stored in the inode says about body items, so

 * in update_stat_data we can not rely on inode, but have to check

 * item version directly

 called by read_locked_inode */

		/*

		 * there was a bug in <=3.5.23 when i_blocks could take

		 * negative values. Starting from 3.5.17 this value could

		 * even be stored in stat data. For such files we set

		 * i_blocks based on file size. Just 2 notes: this can be

		 * wrong for sparse files. On-disk value will be only

		 * updated if file's inode will ever change

		/*

		 * an early bug in the quota code can give us an odd

		 * number for the block count.  This is incorrect, fix it here.

		/*

		 * nopack is initially zero for v1 objects. For v2 objects,

		 * nopack is initialised from sd_attrs

		/*

		 * new stat data found, but object may have old items

		 * (directories and symlinks)

		/*

		 * read persistent inode attributes from sd and initialise

		 * generic inode flags from them

 update new stat data with inode fields */

 used to copy inode's fields to old stat data */

 Sigh. i_first_direct_byte is back */

/*

 * NOTE, you must prepare the buffer head before sending it here,

 * and then log it after the call

 path points to old stat data */

 key type is unimportant */

 look for the object's stat data */

reiserfs_warning (inode->i_sb, "vs-13050: reiserfs_update_sd: i_nlink == 0, stat data not found"); */

		/*

		 * sigh, prepare_for_journal might schedule.  When it

		 * schedules the FS might change.  We have to detect that,

		 * and loop back to the search if the stat data item has moved

 Stat_data item has been moved after scheduling. */

/*

 * reiserfs_read_locked_inode is called to read the inode off disk, and it

 * does a make_bad_inode when things go wrong.  But, we need to make sure

 * and clear the key in the private portion of the inode, otherwise a

 * corresponding iput might try to delete whatever object the inode last

 * represented.

/*

 * initially this function was derived from minix or ext2's analog and

 * evolved as the prototype did

/*

 * looks for stat data in the tree, and fills up the fields of in-core

 * inode stat data fields

	/*

	 * set version 1, version 2 could be used too, because stat data

	 * key is the same in both versions

 look for the object's stat data */

 a stale NFS handle can trigger this without it being an error */

	/*

	 * It is possible that knfsd is trying to access inode of a file

	 * that is being removed from the disk by some other thread. As we

	 * update sd on unlink all that is required is to check for nlink

	 * here. This bug was first found by Sizif when debugging

	 * SquidNG/Butterfly, forgotten, and found again after Philippe

	 * Gramoulle <philippe.gramoulle@mmania.com> reproduced it.



	 * More logical fix would require changes in fs/inode.c:iput() to

	 * remove inode from hash-table _after_ fs cleaned disk stuff up and

	 * in iget() to return NULL if I_FREEING inode is found in

	 * hash-table.

	/*

	 * Currently there is one place where it's ok to meet inode with

	 * nlink==0: processing of open-unlinked and half-truncated files

	 * during mount (fs/reiserfs/super.c:finish_unfinished()).

 init inode should be relsing */

	/*

	 * Stat data v1 doesn't support ACLs.

/*

 * reiserfs_find_actor() - "find actor" reiserfs supplies to iget5_locked().

 *

 * @inode:    inode from hash table to check

 * @opaque:   "cookie" passed to iget5_locked(). This is &reiserfs_iget_args.

 *

 * This function is called by iget5_locked() to distinguish reiserfs inodes

 * having the same inode numbers. Such inodes can only exist due to some

 * error condition. One of them should be bad. Inodes with identical

 * inode numbers (objectids) are distinguished by parent directory ids.

 *

 args is already in CPU order */

 either due to i/o error or a stale NFS handle */

	/*

	 * fhtype happens to reflect the number of u32s encoded.

	 * due to a bug in earlier code, fhtype might indicate there

	 * are more u32s then actually fitted.

	 * so if fhtype seems to be more than len, reduce fhtype.

	 * Valid types are:

	 *   2 - objectid + dir_id - legacy support

	 *   3 - objectid + dir_id + generation

	 *   4 - objectid + dir_id + objectid and dirid of parent - legacy

	 *   5 - objectid + dir_id + generation + objectid and dirid of parent

	 *   6 - as above plus generation of directory

	 * 6 does not fit in NFSv2 handles

/*

 * looks for stat data, then copies fields to it, marks the buffer

 * containing stat data as dirty

/*

 * reiserfs inodes are never really dirty, since the dirty inode call

 * always logs them.  This call allows the VFS inode marking routines

 * to properly mark inodes for datasync and such, but only actually

 * does something when called for a synchronous update.

	/*

	 * memory pressure can sometimes initiate write_inode calls with

	 * sync == 1,

	 * these cases are just when the system needs ram, not when the

	 * inode needs to reach disk for safety, and they can safely be

	 * ignored because the altered inode has already been logged.

/*

 * stat data of new object is inserted already, this inserts the item

 * containing "." and ".." entries

key length */ );

	/*

	 * compose item head for new item. Directories consist of items of

	 * old type (ITEM_VERSION_1). Do not set key (second arg is 0), it

	 * is done by reiserfs_new_inode

 look for place in the tree for new item */

 insert item, that is empty directory item */

/*

 * stat data of object has been inserted, this inserts the item

 * containing the body of symlink

key length */ );

free_space */ );

 look for place in the tree for new item */

 insert item, that is body of symlink */

/*

 * inserts the stat data into the tree, and then calls

 * reiserfs_new_directory (to insert ".", ".." item if new object is

 * directory) or reiserfs_new_symlink (to insert symlink body if new

 * object is symlink) or nothing (if new object is regular file)



 * NOTE! uid and gid must already be set in the inode.  If we return

 * non-zero due to an error, we have to drop the quota previously allocated

 * for the fresh inode.  This can only be done outside a transaction, so

 * if we return non-zero, we also end the transaction.

 *

 * @th: active transaction handle

 * @dir: parent directory for new inode

 * @mode: mode of new inode

 * @symname: symlink contents if inode is symlink

 * @isize: 0 for regular file, EMPTY_DIR_SIZE for dirs, strlen(symname) for

 *         symlinks

 * @inode: inode to be filled

 * @security: optional security context to associate with this inode

		       /* 0 for regular, EMTRY_DIR_SIZE for dirs,

 item head of new item */

		/*

		 * not a perfect generation count, as object ids can be reused,

		 * but this is as good as reiserfs can do right now.

		 * note that the private part of inode isn't filled in yet,

		 * we have to use the directory.

 fill stat data */

 uid and gid must already be set by the caller for quota init */

NO_BYTES_IN_DIRECT_ITEM */ ;

 key to search for correct place for new stat data */

key length */ );

 find proper place for inserting of stat data */

 i_uid or i_gid is too big to be stored in stat data v3.5 */

	/*

	 * store in in-core inode the key of stat data and version all

	 * object items will have (directory items will have old offset

	 * format, other new objects will consist of new items)

 insert the stat data into the tree */

 insert item with "." and ".." */

 insert body of symlink */

	/*

	 * Mark it private if we're creating the privroot

	 * or something under it.

 Invalidate the object, nothing was inserted yet */

 Quota change must be inside a transaction for journaling */

	/*

	 * Drop can be outside and it needs more credits so it's better

	 * to have it outside

 so the caller can't use this handle later */

/*

 * finds the tail page in the page cache,

 * reads the last block in.

 *

 * On success, page_result is set to a locked, pinned page, and bh_result

 * is set to an up to date buffer for the last block in the file.  returns 0.

 *

 * tail conversion is not done, so bh_result might not be valid for writing

 * check buffer_mapped(bh_result) and bh_result->b_blocknr != 0 before

 * trying to write the block.

 *

 * on failure, nonzero is returned, page_result and bh_result are untouched.

	/*

	 * we want the page with the last byte in the file,

	 * not the page that will hold the next byte for appending

	/*

	 * we know that we are only called with inode->i_size > 0.

	 * we also know that a file tail can never be as big as a block

	 * If i_size % blocksize == 0, our file is currently block aligned

	 * and it won't need converting or zeroing after a truncate.

 start within the page of the last block in the file */

		/*

		 * note, this should never happen, prepare_write should be

		 * taking care of this for us.  If the buffer isn't up to

		 * date, I've screwed up the code to find the buffer, or the

		 * code to call prepare_write

/*

 * vfs version of truncate file.  Must NOT be called with

 * a transaction already started.

 *

 * some code taken from block_truncate_page

 we want the offset for the first byte after the end of the file */

			/*

			 * -ENOENT means we truncated past the end of the

			 * file, and get_block_create_0 could not find a

			 * block to read in, which is ok.

	/*

	 * so, if page != NULL, we have a buffer head for the offset at

	 * the end of the file. if the bh is mapped, and bh->b_blocknr != 0,

	 * then we have an unformatted node.  Otherwise, we have a direct item,

	 * and no zeroing is required on disk.  We zero after the truncate,

	 * because the truncate might pack the item anyway

	 * (it will unmap bh if it packs).

	 *

	 * it is enough to reserve space in transaction for 2 balancings:

	 * one for "save" link adding and another for the first

	 * cut_from_item. 1 is for update_sd

		/*

		 * we are doing real truncate: if the system crashes

		 * before the last transaction of truncating gets committed

		 * - on reboot the file either appears truncated properly

		 * or not truncated at all

 check reiserfs_do_truncate after ending the transaction */

 truncate */);

 if we are not on a block boundary */

	/*

	 * catch places below that try to log something without

	 * starting a trans

 we've found an unformatted node */

 crap, we are writing to a hole */

 vs-3050 is gone, no need to drop the path */

 are there still bytes left? */

 this is where we fill in holes in the file. */

 get_block failed to find a mapped unformatted node. */

		/*

		 * we've copied data from the page into the direct item, so the

		 * buffer in the page is now clean, mark it to reflect that.

/*

 * mason@suse.com: updated in 2.5.54 to follow the same general io

 * start/recovery path as __block_write_full_page, along with special

 * code to handle reiserfs tails.

 no logging allowed when nonblocking or from PF_MEMALLOC */

	/*

	 * The page dirty bit is cleared before writepage is called, which

	 * means we have to tell create_empty_buffers to make dirty buffers

	 * The page really should be up to date at this point, so tossing

	 * in the BH_Uptodate is just a sanity check.

	/*

	 * last page in the file, zero out any contents past the

	 * last byte in the file

 no file contents in this page */

 first map all the buffers, logging any direct items we find */

			/*

			 * This can happen when the block size is less than

			 * the page size.  The corresponding bytes in the page

			 * were zero filled above

			/*

			 * not mapped yet, or it points to a direct item, search

			 * the btree for the mapping info, and log any direct

			 * items found

	/*

	 * we start the transaction after map_block_for_writepage,

	 * because it can create holes in the file (an unbounded operation).

	 * starting it here, we can make a reliable estimate for how many

	 * blocks we're going to log

 now go through and lock any dirty buffers on the page */

		/*

		 * from this point on, we know the buffer is mapped to a

		 * real block and not a direct item

	/*

	 * since any buffer might be the only dirty buffer on the page,

	 * the first submit_bh can bring the page out of writeback.

	 * be careful with the buffers.

		/*

		 * if this page only had a direct item, it is very possible for

		 * no io to be required without there being an error.  Or,

		 * someone else could have locked them and sent them down the

		 * pipe without locking the page

	/*

	 * catches various errors, we need to make sure any valid dirty blocks

	 * get to the media.  The page is currently locked and not marked for

	 * writeback

			/*

			 * clear any dirty bits that might have come from

			 * getting attached to a dirty page

		/*

		 * this gets a little ugly.  If reiserfs_get_block returned an

		 * error and left a transacstion running, we've got to close

		 * it, and we've got to free handle if it was a persistent

		 * transaction.

		 *

		 * But, if we had nested into an existing transaction, we need

		 * to just drop the ref count on the handle.

		 *

		 * If old_ref == 0, the transaction is from reiserfs_get_block,

		 * and it was a persistent trans.  Otherwise, it was nested

		 * above.

 Truncate allocated blocks */

		/*

		 * this gets a little ugly.  If reiserfs_get_block returned an

		 * error and left a transacstion running, we've got to close

		 * it, and we've got to free handle if it was a persistent

		 * transaction.

		 *

		 * But, if we had nested into an existing transaction, we need

		 * to just drop the ref count on the handle.

		 *

		 * If old_ref == 0, the transaction is from reiserfs_get_block,

		 * and it was a persistent trans.  Otherwise, it was nested

		 * above.

	/*

	 * generic_commit_write does this for us, but does not update the

	 * transaction tracking stuff when the size changes.  So, we have

	 * to do the i_size updates here.

		/*

		 * If the file have grown beyond the border where it

		 * can have a tail, unmark it as needing a tail

		 * packing

		/*

		 * this will just nest into our transaction.  It's important

		 * to use mark_inode_dirty so the inode gets pushed around on

		 * the dirty lists, and so that O_SYNC works as expected

	/*

	 * generic_commit_write does this for us, but does not update the

	 * transaction tracking stuff when the size changes.  So, we have

	 * to do the i_size updates here.

		/*

		 * If the file have grown beyond the border where it

		 * can have a tail, unmark it as needing a tail

		 * packing

		/*

		 * this will just nest into our transaction.  It's important

		 * to use mark_inode_dirty so the inode gets pushed around

		 * on the dirty lists, and so that O_SYNC works as expected

/*

 * decide if this buffer needs to stay around for data logging or ordered

 * write purposes

	/*

	 * the page is locked, and the only places that log a data buffer

	 * also lock the page.

		/*

		 * very conservative, leave the buffer pinned if

		 * anyone might need it.

		/*

		 * why is this safe?

		 * reiserfs_setattr updates i_size in the on disk

		 * stat data before allowing vmtruncate to be called.

		 *

		 * If buffer was put onto the ordered list for this

		 * transaction, we know for sure either this transaction

		 * or an older one already has updated i_size on disk,

		 * and this ordered data won't be referenced in the file

		 * if we crash.

		 *

		 * if the buffer was put onto the ordered list for an older

		 * transaction, we need to leave it around

 clm -- taken from fs/buffer.c:block_invalidate_page */

		/*

		 * is this block fully invalidated?

	/*

	 * We release buffers only if the entire page is being invalidated.

	 * The get_block cached value has been unconditionally invalidated,

	 * so real IO is not possible anymore.

 maybe should BUG_ON(!ret); - neilb */

/*

 * Returns 1 if the page's buffers were dropped.  The page is locked.

 *

 * Takes j_dirty_buffers_lock to protect the b_assoc_buffers list_heads

 * in the buffers at page_buffers(page).

 *

 * even in -o notail mode, we can't be sure an old mount without -o notail

 * didn't create files with tails.

/*

 * We thank Mingming Cao for helping us understand in great detail what

 * to do in this section of the code.

	/*

	 * In case of error extending write may have instantiated a few

	 * blocks outside i_size. Trim these off again.

 must be turned off for recursive notify_change calls */

		/*

		 * version 2 items will be caught by the s_maxbytes check

		 * done for us in vmtruncate

 fill in hole pointers in the expanding truncate case. */

 we're changing at most 2 bitmaps, inode + super */

			/*

			 * file size is changed, ctime and mtime are

			 * to be updated

 stat data of format v3.5 has 16 bit uid and gid */

		/*

		 * (user+group)*(old+new) structure - we count quota

		 * info and , inode write (sb, inode)

		/*

		 * Update corresponding info in inode so that everything

		 * is in one transaction

			/*

			 * Could race against reiserfs_file_release

			 * if called from NFS, so take tailpack mutex.

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Pessimism: We can't assume that anything from the xattr root up

	 * has been created.

/*

 * Convert from filesystem to in-memory representation.

/*

 * Convert from in-memory to filesystem representation.

/*

 * Inode operation get_posix_acl().

 *

 * inode->i_mutex: down

 * BKL held [before 2.5.x]

		/*

		 * This shouldn't actually happen as it should have

		 * been caught above.. but just in case

/*

 * Inode operation set_posix_acl().

 *

 * inode->i_mutex: down

 * BKL held [before 2.5.x]

	/*

	 * Ensure that the inode gets dirtied if we're only using

	 * the mode bits and an old ACL didn't exist. We don't need

	 * to check if the inode is hashed here since we won't get

	 * called by reiserfs_inherit_default_acl().

/*

 * dir->i_mutex: locked,

 * inode is new and not released into the wild yet

 ACLs only get applied to files and directories */

	/*

	 * ACLs can only be used on "new" objects, so if it's an old object

	 * there is nothing to inherit from

	/*

	 * Don't apply ACLs to objects in the .reiserfs_priv tree.. This

	 * would be useless since permissions are ignored, and a pain because

	 * it introduces locking cycles

 no ACL, apply umask */

/* This is used to cache the default acl before a new object is created.

 * The biggest reason for this is to get an idea of how many blocks will

 * actually be required for the create operation if we must inherit an ACL.

 * An ACL write can add up to 3 object creations and an additional file write

 * so we'd prefer not to reserve that many blocks in the journal if we can.

 * It also has the advantage of not loading the ACL with a transaction open,

 * this may seem silly, but if the owner of the directory is doing the

 * creation, the ACL may not be loaded since the permissions wouldn't require

 * it.

 * We return the number of blocks required for the transaction.

		/* Other xattrs can be created during inode creation. We don't

		 * want to claim too many blocks, so we check to see if we

		 * need to create the tree to the xattrs, and then we

 We need to account for writes + bitmaps for two files */

/*

 * Called under i_mutex

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

 key of current position in the directory (key of directory entry) */

 avoid kmalloc if we can */

	/*

	 * form key for search the next directory entry using

	 * f_pos field of file structure

		/*

		 * search the directory item, containing entry with

		 * specified key

			/*

			 * FIXME: we could just skip part of directory

			 * which could not be read

 we must have found item, that is item of this directory, */

		/*

		 * and entry must be not more than number of entries

		 * in the item

		/*

		 * go through all entries in the directory item beginning

		 * from the entry, that has been found

 it is hidden entry */

					/*

					 * There is corrupted data in entry,

					 * We'd better stop here

 too big to send back to VFS */

 Ignore the .reiserfs_priv entry */

				/*

				 * Note, that we copy name to user space via

				 * temporary buffer (local_buf) because

				 * filldir will block if user space buffer is

				 * swapped out. At that time entry can move to

				 * somewhere else

				/*

				 * Since filldir might sleep, we can release

				 * the write lock here for other waiters

 deh_offset(deh) may be invalid now. */

 for */

 end of directory has been reached */

		/*

		 * item we went through is last item of node. Using right

		 * delimiting key check is it directory end

			/*

			 * set pos_key to key, that is the smallest and greater

			 * that key of the last entry in the item

 end of directory has been reached */

 directory continues in the right neighboring block */

 while */

/*

 * compose directory item containing "." and ".." entries (entries are

 * not aligned to 4 byte boundary)

 direntry header of "." */

 these two are from make_le_item_head, and are LE */

 Endian safe if 0 */

 direntry header of ".." */

 key of ".." for the root directory */

 these two are from the inode, and are LE */

 Endian safe if 0 */

 copy ".." and "." */

 compose directory item containing "." and ".." entries */

 direntry header of "." */

 these two are from make_le_item_head, and are LE */

 Endian safe if 0 */

 direntry header of ".." */

 key of ".." for the root directory */

 these two are from the inode, and are LE */

 Endian safe if 0 */

 copy ".." and "." */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright 1999 Hans Reiser, see reiserfs/README for licensing and copyright

 * details

/*

 * access to tail : when one is going to read tail it must make sure, that is

 * not running.  direct2indirect and indirect2direct can not run concurrently

/*

 * Converts direct items to an unformatted node. Panics if file has no

 * tail. -ENOSPC if no disk space for conversion

/*

 * path points to first direct item of the file regardless of how many of

 * them are there

 Key to search for the last byte of the converted item. */

	/*

	 * new indirect item to be inserted or key

	 * of unfm pointer to be pasted

 returned value for reiserfs_insert_item and clones */

 Handle on an unformatted node that will be inserted in the tree. */

	/*

	 * and key to search for append or insert pointer to the new

	 * unformatted node.

 Set the key to search for the place for new unfm pointer */

 FIXME: we could avoid this */

 Insert new indirect item. */

 delete at nearest future */

 Paste into last indirect item of an object. */

	/*

	 * note: from here there are two keys which have matching first

	 *  three key components. They only differ by the fourth one.

 Set the key to search for the direct items of the file */

	/*

	 * Move bytes from the direct items to the new unformatted node

	 * and delete them.

		/*

		 * end_key.k_offset is set so, that we will always have found

		 * last item of the file

		/*

		 * we only send the unbh pointer if the buffer is not

		 * up to date.  this avoids overwriting good data from

		 * writepage() with old data from the disk or buffer cache

		 * Special case: unbh->b_page will be NULL if we are coming

		 * through DIRECT_IO handler here.

 done: file does not have direct items anymore */

	/*

	 * if we've copied bytes from disk into the page, we need to zero

	 * out the unused part of the block (it was not up to date before)

 stolen from fs/buffer.c */

	/*

	 * Remove the buffer from whatever list it belongs to. We are mostly

	 * interested in removing it from per-sb j_dirty_buffers list, to avoid

	 * BUG() on attempt to write not mapped buffer

/*

 * this first locks inode (neither reads nor sync are permitted),

 * reads tail through page cache, insert direct item. When direct item

 * inserted successfully inode is left locked. Return value is always

 * what we expect from it (number of cut bytes). But when tail remains

 * in the unformatted node, we set mode to SKIP_BALANCING and unlock

 * inode

 path to the indirect item. */

		    const struct cpu_key *item_key,	/* Key to look for

							 * unformatted node

 New file size. */

 position of first byte of the tail */

 store item head path points to. */

	/*

	 * we are protected by i_mutex. The tail can not disapper, not

	 * append can be done either

	 * we are in truncate or packing tail in file_release

 this can schedule */

 re-search indirect item */

 Set direct item header to insert. */

ih_free_space */ );

	/*

	 * we want a pointer to the first byte of the tail in the page.

	 * the page was locked and this part of the page was up to date when

	 * indirect2direct was called, so we know the bytes are still valid

 Insert tail as new direct item in the tree */

		/*

		 * No disk memory. So we can not convert last unformatted node

		 * to the direct item.  In this case we used to adjust

		 * indirect items's ih_free_space. Now ih_free_space is not

		 * used, it would be ideal to write zeros to corresponding

		 * unformatted node. For now i_size is considered as guard for

		 * going out of file size

 make sure to get the i_blocks changes from reiserfs_insert_item */

	/*

	 * note: we have now the same as in above direct2indirect

	 * conversion: there are two keys which have matching first three

	 * key components. They only differ by the fourth one.

	/*

	 * We have inserted new direct item and must remove last

	 * unformatted node.

 we store position of first direct item in the in-core inode */

 mark_file_with_tail (inode, pos1 + 1); */

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

/*

 * Now we have all buffers that must be used in balancing of the tree

 * Further calculations can not cause schedule(), and thus the buffer

 * tree will be stable until the balancing will be finished

 * balance the tree according to the analysis made before,

 * and using buffers obtained after all above.

/*

 * summary:

 *  if deleting something ( tb->insert_size[0] < 0 )

 *    return(balance_leaf_when_delete()); (flag d handled here)

 *  else

 *    if lnum is larger than 0 we put items into the left node

 *    if rnum is larger than 0 we put items into the right node

 *    if snum1 is larger than 0 we put items into the new node s1

 *    if snum2 is larger than 0 we put items into the new node s2

 * Note that all *num* count new items being created.

 cut item in S[0] */

		/*

		 * UFS unlink semantics are such that you can only

		 * delete one directory entry at a time.

		 *

		 * when we cut a directory tb->insert_size[0] means

		 * number of entries to be cut (always 1)

 L[0] must be joined with S[0] */

 R[0] must be also joined with S[0] */

				/*

				 * all contents of all the

				 * 3 buffers will be in L[0]

 all contents of all the 3 buffers will be in R[0] */

 right_delimiting_key is correct in R[0] */

 all contents of L[0] and S[0] will be in L[0] */

	/*

	 * a part of contents of S[0] will be in L[0] and

	 * the rest part of S[0] will be in R[0]

/*

 * Balance leaf node in case of delete or cut: insert_size[0] < 0

 *

 * lnum, rnum can have values >= -1

 *	-1 means that the neighbor must be joined with S

 *	 0 means that nothing should be done with the neighbor

 *	>0 means to shift entirely or partly the specified number of items

 *         to the neighbor

 Delete or truncate the item */

 M_CUT */

	/*

	 * the rule is that no shifting occurs unless by shifting

	 * a node can be freed

 L[0] takes part in balancing */

 all contents of R[0] and S[0] will be in R[0] */

 part of new item falls into L[0] */

 Calculate item length to insert to S[0] */

 Calculate and check item length to insert to L[0] */

 Insert new item into L[0] */

		/*

		 * Calculate key component, item length and body to

		 * insert into S[0]

 new item in whole falls into L[0] */

 Shift lnum[0]-1 items to L[0] */

 Insert new item into L[0] */

 directory item */

 new directory entry falls into L[0] */

		/*

		 * Shift lnum[0] - 1 items in whole.

		 * Shift lbytes - 1 entries from given directory item

 Append given directory entry to directory item */

		/*

		 * previous string prepared space for pasting new entry,

		 * following string pastes this entry

		/*

		 * when we have merge directory item, pos_in_item

		 * has been changed too

 paste new directory entry. 1 is entry number */

 new directory item doesn't fall into L[0] */

		/*

		 * Shift lnum[0]-1 items in whole. Shift lbytes

		 * directory entries from directory item number lnum[0]

 Calculate new position to append in item body */

 appended item will be in L[0] in whole */

		/*

		 * this bytes number must be appended

		 * to the last item of L[h]

 Calculate new insert_size[0] */

 Append to body of item in L[0] */

		/*

		 * 0-th item in S0 can be only of DIRECT type

		 * when l_n != 0

 update key of first item in S0 */

 update left delimiting key */

		/*

		 * Calculate new body, position in item and

		 * insert_size[0]

 only part of the appended item will be in L[0] */

 Calculate position in item for append in S[0] */

		/*

		 * Shift lnum[0] - 1 items in whole.

		 * Shift lbytes - 1 byte from item number lnum[0]

 appended item will be in L[0] in whole */

 if we paste into first item of S[0] and it is left mergable */

		/*

		 * then increment pos_in_item by the size of the

		 * last item in L[0]

	/*

	 * Shift lnum[0] - 1 items in whole.

	 * Shift lbytes - 1 byte from item number lnum[0]

 Append to body of item in L[0] */

 if appended item is directory, paste entry */

	/*

	 * if appended item is indirect item, put unformatted node

	 * into un list

 we must shift the part of the appended item */

 Shift lnum[0] items from S[0] to the left neighbor L[0] */

 new item or it part falls to L[0], shift it too */

 M_PASTE */

 new item doesn't fall into L[0] */

 new item or part of it doesn't fall into R[0] */

 new item or its part falls to R[0] */

 part of new item falls into R[0] */

 Remember key component and item length */

		/*

		 * Calculate key component and item length to insert

		 * into R[0]

 Insert part of the item into R[0] */

 Replace right delimiting key by first key in R[0] */

		/*

		 * Calculate key component and item length to

		 * insert into S[0]

 whole new item falls into R[0] */

 Shift rnum[0]-1 items to R[0] */

 Insert new item into R[0] */

 new directory entry falls into R[0] */

		/*

		 * Shift rnum[0]-1 items in whole.

		 * Shift rbytes-1 directory entries from directory

		 * item number rnum[0]

 Paste given directory entry to directory item */

 paste entry */

 change delimiting keys */

 new directory entry doesn't fall into R[0] */

 we append to directory item */

 regular object */

	/*

	 * Calculate number of bytes which must be shifted

	 * from appended item

	/*

	 * Calculate number of bytes which must remain in body

	 * after appending to R[0]

 Append part of body into R[0] */

 append item in R[0] */

 paste new entry, if item is directory item */

 update delimiting keys */

 new item doesn't fall into R[0] */

 pasted item or part of it falls to R[0] */

 we must shift the part of the appended item */

 pasted item in whole falls into R[0] */

 shift rnum[0] items from S[0] to the right neighbor R[0] */

 M_PASTE */

 new item or it part don't falls into S_new[i] */

 new item or it's part falls to first new node S_new[i] */

 part of new item falls into S_new[i] */

 Move snum[i]-1 items from S[0] to S_new[i] */

 Remember key component and item length */

		/*

		 * Calculate key component and item length to insert

		 * into S_new[i]

 Insert part of the item into S_new[i] before 0-th item */

		/*

		 * Calculate key component and item length to

		 * insert into S[i]

 whole new item falls into S_new[i] */

		/*

		 * Shift snum[0] - 1 items to S_new[i]

		 * (sbytes[i] of split item)

 Insert new item into S_new[i] */

 we append to directory item */

 new directory entry falls into S_new[i] */

		/*

		 * Shift snum[i]-1 items in whole.

		 * Shift sbytes[i] directory entries

		 * from directory item number snum[i]

		/*

		 * Paste given directory entry to

		 * directory item

 paste new directory entry */

 new directory entry doesn't fall into S_new[i] */

 regular object */

	/*

	 * Calculate number of bytes which must be shifted from appended item

	/*

	 * Calculate number of bytes which must remain in body after

	 * append to S_new[i]

 Append part of body into S_new[0] */

 paste into item */

 if we paste to indirect item update ih_free_space */

 pasted item doesn't fall into S_new[i] */

 pasted item or part if it falls to S_new[i] */

 we must shift part of the appended item */

 item falls wholly into S_new[i] */

 Fill new nodes that appear in place of S[0] */

 here we shift from S to S_new nodes */

 initialized block type and tree level */

 M_PASTE */

 If we insert the first key change the delimiting key */

 can be 0 in reiserfsck */

 prepare space */

 paste entry */

 when directory, may be new entry already pasted */

 regular object */

/*

 * if the affected item was not wholly shifted then we

 * perform all necessary operations on that part or whole

 * of the affected item which remains in S

 if we must insert or append into buffer S[0] */

 M_PASTE */

/**

 * balance_leaf - reiserfs tree balancing algorithm

 * @tb: tree balance state

 * @ih: item header of inserted item (little endian)

 * @body: body of inserted item or bytes to paste

 * @flag: i - insert, d - delete, c - cut, p - paste (see do_balance)

 * passed back:

 * @insert_key: key to insert new nodes

 * @insert_ptr: array of nodes to insert at the next level

 *

 * In our processing of one level we sometimes determine what must be

 * inserted into the next higher level.  This insertion consists of a

 * key or two keys and their corresponding pointers.

 Make balance in case insert_size[0] < 0 */

	/*

	 * for indirect item pos_in_item is measured in unformatted node

	 * pointers. Recalculate to bytes

 tb->lnum[0] > 0 */

 Calculate new item position */

 tb->rnum[0] > 0 */

	/*

	 * if while adding to a node we discover that it is possible to split

	 * it in two, and merge the left part into the left neighbor and the

	 * right part into the right neighbor, eliminating the node

 node S[0] is empty now */

		/*

		 * if insertion was done before 0-th position in R[0], right

		 * delimiting key of the tb->L[0]'s and left delimiting key are

		 * not set correctly

 Leaf level of the tree is balanced (end of balance_leaf) */

 Make empty node */

 Endian safe if 0 */

 Get first empty buffer */

 This is now used because reiserfs_free_block has to be able to schedule. */

 free_thrown puts this */

 incremented in store_thrown */

 Replace n_dest'th key in buffer dest by n_src'th key of buffer src.*/

 source buffer contains leaf node */

	/*

	 * double check that buffers that we will modify are unlocked.

	 * (fix_nodes should already have prepped all of these for us).

 check all internal nodes */

/*

 * Now we have all of the buffers that must be used in balancing of

 * the tree.  We rely on the assumption that schedule() will not occur

 * while do_balance works. ( Only interrupt handlers are acceptable.)

 * We balance the tree according to the analysis made before this,

 * using buffers already obtained.  For SMP support it will someday be

 * necessary to add ordered locking of tb.

/*

 * Some interesting rules of balancing:

 * we delete a maximum of two nodes per level per balancing: we never

 * delete R, when we delete two of three nodes L, S, R then we move

 * them into R.

 *

 * we only delete L if we are deleting two nodes, if we delete only

 * one node we delete S

 *

 * if we shift leaves then we shift as much as we can: this is a

 * deliberate policy of extremism in node packing which results in

 * higher average utilization after repeated random balance operations

 * at the cost of more memory copies and more balancing as a result of

 * small insertions to full nodes.

 *

 * if we shift internal nodes we try to evenly balance the node

 * utilization, with consequent less balancing at the cost of lower

 * utilization.

 *

 * one could argue that the policy for directories in leaves should be

 * that of internal nodes, but we will wait until another day to

 * evaluate this....  It would be nice to someday measure and prove

 * these assumptions as to what is optimal....

 use print_cur_tb() to see initial state of struct tree_balance */

 store_print_tb (tb); */

 do not delete, just comment it out */

	/*

	print_tb(flag, PATH_LAST_POSITION(tb->tb_path),

		 tb->tb_path->pos_in_item, tb, "check");

	/*

	 * reiserfs_free_block is no longer schedule safe.  So, we need to

	 * put the buffers we want freed on the thrown list during do_balance,

	 * and then free them now

 release all nodes hold to perform the balancing */

/*

 * do_balance - balance the tree

 *

 * @tb: tree_balance structure

 * @ih: item header of inserted item

 * @body: body of inserted item or bytes to paste

 * @flag: 'i' - insert, 'd' - delete, 'c' - cut, 'p' paste

 *

 * Cut means delete part of an item (includes removing an entry from a

 * directory).

 *

 * Delete means delete whole item.

 *

 * Insert means add a new item into the tree.

 *

 * Paste means to append to the end of an existing file or to

 * insert a directory entry.

 position of a child node in its parent */

 level of the tree being processed */

	/*

	 * in our processing of one level we sometimes determine what

	 * must be inserted into the next higher level.  This insertion

	 * consists of a key or two keys and their corresponding

	 * pointers

 inserted node-ptrs for the next level */

 if we have no real work to do  */

	/*

	 * balance_leaf returns 0 except if combining L R and S into

	 * one node.  see balance_internal() for explanation of this

	 * line of code.

 Balance internal level of the tree. */

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

 %k */

 %K */

/*

 * debugging reiserfs we used to print out a lot of different

 * variables, like keys, item headers, buffer heads etc. Values of

 * most fields matter. So it took a long time just to write

 * appropriative printk. With this reiserfs_warning you can use format

 * specification for complex structures like you used to do with

 * printfs for integers, doubles and pointers. For instance, to print

 * out key structure you have to write just:

 * reiserfs_warning ("bad key %k", key);

 * instead of

 * printk ("bad key %lu %lu %lu %lu", key->k_dir_id, key->k_objectid,

 *         key->k_offset, key->k_uniqueness);

/*

 * in addition to usual conversion specifiers this accepts reiserfs

 * specific conversion specifiers:

 * %k to print little endian key,

 * %K to print cpu key,

 * %h to print item_head,

 * %t to print directory entry

 * %z to print block head (arg must be struct buffer_head *

 * %b to print buffer_head

 No newline.. reiserfs_info calls can be followed by printk's */

 No newline.. reiserfs_printk calls can be followed by printk's */

/*

 * The format:

 *

 *          maintainer-errorid: [function-name:] message

 *

 *   where errorid is unique to the maintainer and function-name is

 *   optional, is recommended, so that anyone can easily find the bug

 *   with a simple grep for the short to type string

 *   maintainer-errorid.  Don't bother with reusing errorids, there are

 *   lots of numbers out there.

 *

 *   Example:

 *

 *   reiserfs_panic(

 *     p_sb, "reiser-29: reiserfs_new_blocknrs: "

 *     "one of search_start or rn(%d) is equal to MAX_B_NUM,"

 *     "which means that we are optimizing location based on the "

 *     "bogus location of a temp buffer (%p).",

 *     rn, bh

 *   );

 *

 *   Regular panic()s sometimes clear the screen before the message can

 *   be read, thus the need for the while loop.

 *

 *   Numbering scheme for panic used by Vladimir and Anatoly( Hans completely

 *   ignores this scheme, and considers it pointless complexity):

 *

 *   panics in reiserfs_fs.h have numbers from 1000 to 1999

 *   super.c			2000 to 2999

 *   preserve.c (unused)	3000 to 3999

 *   bitmap.c			4000 to 4999

 *   stree.c			5000 to 5999

 *   prints.c			6000 to 6999

 *   namei.c			7000 to 7999

 *   fix_nodes.c		8000 to 8999

 *   dir.c			9000 to 9999

 *   lbalance.c			10000 to 10999

 *   ibalance.c			11000 to 11999 not ready

 *   do_balan.c			12000 to 12999

 *   inode.c			13000 to 13999

 *   file.c			14000 to 14999

 *   objectid.c			15000 - 15999

 *   buffer.c			16000 - 16999

 *   symlink.c			17000 - 17999

 *

/*

 * this prints internal nodes (4 keys/items in line) (dc_number,

 * dc_size)[k_dirid, k_objectid, k_offset, k_uniqueness](dc_number,

 * dc_size)...

 return 1 if this is not super block */

	/*

	 * FIXME: this would be confusing if

	 * someone stores reiserfs super block in some data block ;)

//    skipped = (bh->b_blocknr * bh->b_size) / sb_blocksize(rs);

 ..., int print_mode, int first, int last) */

 this stores initial state of tree balance in the print_tb_buf */

 this prints balance parameters for non-leaf levels */

 print FEB list (list of buffers in form (bh (b_blocknr, b_count), that will be used for new nodes) */

	/*

	   printk ("reiserfs_put_super: session statistics: balances %d, fix_nodes %d, \

	   bmap with search %d, without %d, dir2ind %d, ind2dir %d\n",

	   REISERFS_SB(s)->s_do_balance, REISERFS_SB(s)->s_fix_nodes,

	   REISERFS_SB(s)->s_bmaps, REISERFS_SB(s)->s_bmaps_without_search,

	   REISERFS_SB(s)->s_direct2indirect, REISERFS_SB(s)->s_indirect2direct);

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

/*

 * Written by Alexander Zarochentcev.

 *

 * The kernel part of the (on-line) reiserfs resizer.

 check the device size */

	/*

	 * old disk layout detection; those partitions can be mounted, but

	 * cannot be resized

 count used bits in last bitmap block */

 count bitmap blocks in new fs */

 save old values */

 resizing of reiserfs bitmaps (journal and real), if needed */

 reallocate journal bitmaps */

		/*

		 * the new journal bitmaps are zero filled, now we copy i

		 * the bitmap node pointers from the old journal bitmap

		 * structs, and then transfer the new data structures

		 * into the journal struct.

		 *

		 * using the copy_size var below allows this code to work for

		 * both shrinking and expanding the FS.

			/*

			 * just in case vfree schedules on us, copy the new

			 * pointer into the journal struct before freeing the

			 * old one

		/*

		 * allocate additional bitmap blocks, reallocate

		 * array of bitmap block pointers

			/*

			 * Journal bitmaps are still supersized, but the

			 * memory isn't leaked, so I guess it's ok

		/*

		 * This doesn't go through the journal, but it doesn't have to.

		 * The changes are still atomic: We're synced up when the

		 * journal transaction begins, and the new bitmaps don't

		 * matter if the transaction fails.

			/*

			 * don't use read_bitmap_block since it will cache

			 * the uninitialized bitmap

 update bitmap_info stuff */

 free old bitmap blocks array */

	/*

	 * begin transaction, if there was an error, it's fine. Yes, we have

	 * incorrect bitmaps now, but none of it is ever going to touch the

	 * disk anyway.

 Extend old last bitmap block - new blocks have been made available */

 Correct new last bitmap block - It may not be full */

 update super */

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

/*

 * We pack the tails of files on file close, not at the time they are written.

 * This implies an unnecessary copy of the tail and an unnecessary indirect item

 * insertion/balancing, for files that are written in one write.

 * It avoids unnecessary tail packings (balances) for files that are written in

 * multiple writes and are small enough to have tails.

 *

 * file_release is called by the VFS layer when the file is closed.  If

 * this is the last open file descriptor, and the file

 * small enough to have a tail, and the tail is currently in an

 * unformatted node, the tail is converted back into a direct item.

 *

 * We use reiserfs_truncate_file to pack the tail, since it already has

 * all the conditions coded.

 fast out for when nothing needs to be done */

	/*

	 * freeing preallocation only involves relogging blocks that

	 * are already in the current transaction.  preallocation gets

	 * freed at the end of each transaction, so it is impossible for

	 * us to log any additional blocks (including quota blocks)

		/*

		 * uh oh, we can't allow the inode to go away while there

		 * is still preallocation blocks pending.  Try to join the

		 * aborted transaction

			/*

			 * hmpf, our choices here aren't good.  We can pin

			 * the inode which will disallow unmount from ever

			 * happening, we can do nothing, which will corrupt

			 * random memory on unmount, or we can forcibly

			 * remove the file from the preallocation list, which

			 * will leak blocks on disk.  Lets pin the inode

			 * and let the admin know what is going on.

 copy back the error code from journal_begin */

		/*

		 * if regular file is released by last holder and it has been

		 * appended (we append by unformatted node only) or its direct

		 * item(s) had to be converted, then it may have to be

		 * indirect2direct converted

 somebody might be tailpacking on final close; wait for it */

 Sync a reiserfs file. */

/*

 * FIXME: sync_mapping_buffers() never has anything to sync.  Can

 * be removed...

 taken fs/buffer.c:__block_commit_write */

				/*

				 * do data=ordered on any page past the end

				 * of file and any buffer marked BH_New.

	/*

	 * If this is a partial write which happened to make all buffers

	 * uptodate then we can optimize away a bogus readpage() for

	 * the next read(). Here we 'discover' whether the page went

	 * uptodate as a result of this (potentially partial) write.

/*

 * Keyed 32-bit hash function using TEA in a Davis-Meyer function

 *   H0 = Key

 *   Hi = E Mi(Hi-1) + Hi-1

 *

 * (see Applied Cryptography, 2nd edition, p448).

 *

 * Jeremy Fitzhardinge <jeremy@zip.com.au> 1998

 *

 * Jeremy has agreed to the contents of reiserfs/README. -Hans

 * Yura's function is added (04/07/2000)

 32 is overkill, 16 is strong crypto */

 6 gets complete mixing */

 a, b, c, d - data; h0, h1 - accumulated hash */

      assert(len >= 0 && len < 256); */

	return 0;*/

/*

 * What follows in this file is copyright 2000 by Hans Reiser, and the

 * licensing of what follows is governed by reiserfs/README

 SPDX-License-Identifier: GPL-2.0

/*

 * The previous reiserfs locking scheme was heavily based on

 * the tricky properties of the Bkl:

 *

 * - it was acquired recursively by a same task

 * - the performances relied on the release-while-schedule() property

 *

 * Now that we replace it by a mutex, we still want to keep the same

 * recursive property to avoid big changes in the code structure.

 * We use our own lock_owner here because the owner field on a mutex

 * is only available in SMP or mutex debugging, also we only need this field

 * for this mutex, no need for a system wide mutex facility.

 *

 * Also this lock is often released before a call that could block because

 * reiserfs performances were partially based on the release while schedule()

 * property of the Bkl.

 No need to protect it, only the current task touches it */

	/*

	 * Are we unlocking without even holding the lock?

	 * Such a situation must raise a BUG() if we don't want

	 * to corrupt the data.

 this can happen when the lock isn't always held */

 this can happen when the lock isn't always held */

/*

 * Utility function to force a BUG if it is called without the superblock

 * write lock held.  caller is the string printed just before calling BUG()

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

/*

 * this contains item handlers for old item types: sd, direct,

 * indirect, directory

/*

 * and where are the comments? how about saying where we can find an

 * explanation of each item handler method? -Hans

 stat data functions */

 unused */

 direct item functions */

 FIXME: this should probably switch to indirect as well */

    return; */

 unused */

 indirect item functions */

 decrease offset, if it becomes 0, change type to stat data */

 if it is not first item of the body, then it is mergeable */

 printing of indirect item */

 unused */

/*

 * return size in bytes of 'units' units. If first == 0 - calculate

 * from the head (left), otherwise - from tail (right)

 unit of indirect item is byte (yet) */

 unit of indirect item is byte (yet) */

 direntry functions */

 unused */

/*

 * function returns old entry number in directory item in real node

 * using new entry number in virtual item in virtual node

 cut or paste is applied to another item */

/*

 * Create an array of sizes of directory entries for virtual

 * item. Return space used by an item. FIXME: no control over

 * consuming of space used by this item handler

 virtual directory item have this amount of entry after */

 set size of pasted entry */

 compare total size of entries with item length */

/*

 * return number of entries which may fit into specified amount of

 * free space, or -1 if free space is not enough even for 1 entry

 i-th entry doesn't fit into the remaining free space */

 "." and ".." can not be separated from each other */

 i-th entry doesn't fit into the remaining free space */

 "." and ".." can not be separated from each other */

 sum of entry sizes between from-th and to-th entries including both edges */

 Error catching functions to catch errors caused by incorrect item types. */

	/*

	 * We might return -1 here as well, but it won't help as

	 * create_virtual_node() from where this operation is called

	 * from is of return type void.

 This is to catch errors with invalid type (15th entry for TYPE_ANY) */

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

 Reiserfs block (de)allocator, bitmap-based. */

 different reiserfs block allocator options */

	/*

	 * It is in the bitmap block number equal to the block

	 * number divided by the number of bits in a block.

 Within that bitmap block it is located at bit offset *offset. */

	/*

	 * Old format filesystem? Unlikely, but the bitmaps are all

	 * up front so we need to account for it.

/*

 * Searches in journal structures for a given block number (bmap, off).

 * If block is found in reiserfs journal it suggests next free block

 * candidate to test.

 hint supplied */

 inc offset to avoid looping. */

/*

 * Searches for a window of zero bits with given minimum and maximum

 * lengths in one bitmap block

 No free blocks in this bitmap */

 search for a first zero bit -- beginning of a window */

		/*

		 * search for a zero bit fails or the rest of bitmap block

		 * cannot contain a zero window of minimum size

 first zero bit found; we check next bits */

			/*

			 * finding the other end of zero bit window requires

			 * looking into journal structures (in case of

			 * searching for free blocks for unformatted nodes)

		/*

		 * now (*beg) points to beginning of zero bits window,

		 * (end) points to one bit after the window end

 found window of proper size */

			/*

			 * try to set all blocks used checking are

			 * they still free

 Don't check in journal again. */

					/*

					 * bit was set by another process while

					 * we slept in prepare_for_journal()

					/*

					 * we can continue with smaller set

					 * of allocated blocks, if length of

					 * this set is more or equal to `min'

					/*

					 * otherwise we clear all bit

					 * were set ...

					/*

					 * Search again in current block

					 * from beginning

 free block count calculation */

 this can only be true when SB_BMAP_NR = 1 */

/*

 * hashes the id and then returns > 0 if the block group for the

 * corresponding hash is full

	/*

	 * If we don't have cached information on this bitmap block, we're

	 * going to have to load it later anyway. Loading it here allows us

	 * to make a better decision. This favors long-term performance gain

	 * with a better on-disk layout vs. a short term gain of skipping the

	 * read and potentially having a bad placement.

/*

 * the packing is returned in disk byte order

		/*

		 * some versions of reiserfsck expect packing locality 1 to be

		 * special

/*

 * Tries to find contiguous zero bit window (given size) in given region of

 * bitmap and place new blocks there. Returns number of allocated blocks.

 No point in looking for more free blocks */

	/*

	 * When the bitmap is more than 10% free, anyone can allocate.

	 * When it's less than 10% free, only files that already use the

	 * bitmap are allowed. Once we pass 80% full, this restriction

	 * is lifted.

	 *

	 * We do this so that files that grow later still have space close to

	 * their original allocation. This improves locality, and presumably

	 * performance as a result.

	 *

	 * This is only an allocation policy and does not make up for getting a

	 * bad hint. Decent hinting must be implemented for this to work well.

 we know from above that start is a reasonable number */

 clear bit for the given block in bit map */

 update super block */

 mark it before we clear it, just in case */

 preallocated blocks don't need to be run through journal_mark_freed */

		/*

		 * reiserfs_free_prealloc_block can drop the write lock,

		 * which could allow another caller to free the same block.

		 * We can protect against it by modifying the prealloc

		 * state before calling it.

 FIXME: It should be inline function */

 block allocator related options are parsed here */

 clear default settings */

hint->search_start = hint->beg;*/

/*

 * Relocation based on dirid, hashing them into a given bitmap block

 * files. Formatted nodes are unaffected, a separate policy covers them

 give a portion of the block group to metadata */

/*

 * Relocation based on oid, hashing them into a given bitmap block

 * files. Formatted nodes are unaffected, a separate policy covers them

		/*

		 * keep the root dir and it's first set of subdirs close to

		 * the start of the disk

/*

 * returns 1 if it finds an indirect item and gets valid hint info

 * from it, otherwise 0

	/*

	 * reiserfs code can call this function w/o pointer to path

	 * structure supplied; then we rely on supplied search_start

	/*

	 * for indirect item: go to left and look for the first non-hole entry

	 * in the indirect item

 does result value fit into specified region? */

/*

 * should be, if formatted node, then try to put on first part of the device

 * specified as number of percent with mount option device, else try to put

 * on last of device.  This is not to say it is good code to do so,

 * but the effect should be measured.

 This is former border algorithm. Now with tunable border offset */

	/*

	 * whenever we create a new directory, we displace it.  At first

	 * we will hash for location, later we might look for a moderately

	 * empty place for it

		/*

		 * we do not continue determine_search_start,

		 * if new packing locality is being displaced

	/*

	 * all persons should feel encouraged to add more special cases

	 * here and test them

	/*

	 * if none of our special cases is relevant, use the left

	 * neighbor in the tree order of the new node we are allocating for

	/*

	 * Mimic old block allocator behaviour, that is if VFS allowed for

	 * preallocation, new blocks are displaced based on directory ID.

	 * Also, if suggested search_start is less than last preallocated

	 * block, we start searching from it, assuming that HDD dataflow

	 * is faster in forward direction

 This is an approach proposed by Hans */

 old_hashed_relocation only works on unformatted */

 new_hashed_relocation works with both formatted/unformatted nodes */

 dirid grouping works only on unformatted nodes */

 oid grouping works only on unformatted nodes */

 make minimum size a mount option and benchmark both ways */

 we preallocate blocks only for regular files, specific size */

 benchmark preallocating always and see what happens */

 no new blocks allocated, return */

 fill free_blocknrs array first */

 do we have something to fill prealloc. array also ? */

			/*

			 * it means prealloc_size was greater that 0 and

			 * we do preallocation

 Quota exceeded? */

 for unformatted nodes, force large allocations */

 Search from hint->search_start to end of disk */

 Search from hint->beg to hint->search_start */

 Last chance: Search from 0 to hint->beg */

 We've tried searching everywhere, not enough space */

 Free the blocks */

 Free not allocated blocks */

 Some of preallocation blocks were not allocated */

 grab new blocknrs from preallocated list */

 return amount still needed after using them */

 return amount still needed after using preallocated blocks */

 Amount of blocks we have already reserved */

 Check if there is enough space, taking into account reserved space */

 should this be if !hint->inode &&  hint->preallocate? */

 do you mean hint->formatted_node can be removed ? - Zam */

	/*

	 * hint->formatted_node cannot be removed because we try to access

	 * inode information here, and there is often no inode associated with

	 * metadata allocations - green

		/*

		 * We have all the block numbers we need from the

		 * prealloc list

 find search start and save it in hint structure */

 allocation itself; fill new_blocknrs and preallocation arrays */

	/*

	 * We used prealloc. list to fill (partially) new_blocknrs array.

	 * If final allocation fails we need to return blocks back to

	 * prealloc. list or just free them. -- Zam (I chose second

	 * variant)

 The first bit must ALWAYS be 1 */

 0 and ~0 are special, we can optimize for them */

 A mix, investigate */

	/*

	 * Way old format filesystems had the bitmaps packed up front.

	 * I doubt there are any of these left, but just in case...

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/* Initializes the security context for a new inode and returns the number

 * of blocks needed for the transaction. If successful, reiserfs_security

 Don't add selinux attributes on xattrs - they'll never get used */

		/* We don't want to count the directories twice if we have

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

/*

 * To make any changes in the tree we find a node that contains item

 * to be changed/deleted or position in the node we insert a new item

 * to. We call this node S. To do balancing we need to decide what we

 * will shift to left/right neighbor, or to a new node, where new item

 * will be etc. To make this analysis simpler we build virtual

 * node. Virtual node is an array of items, that will replace items of

 * node S. (For instance if we are going to delete an item, virtual

 * node does not contain it). Virtual node keeps information about

 * item sizes and types, mergeability of first and last items, sizes

 * of all entries in directory item. We use this array of items when

 * calculating what we can shift to neighbors and how many nodes we

 * have to have if we do not any shiftings, if we shift to left/right

 * neighbor or to both.

/*

 * Takes item number in virtual node, returns number of item

 * that it has in source buffer

 delete mode */

 this comes from tb->S[h] */

 size of changed node */

 for internal nodes array if virtual items is not created */

 number of items in virtual node  */

 first virtual item */

 first item in the node */

 define the mergeability for 0-th item (if it is not being deleted) */

	/*

	 * go through all items that remain in the virtual

	 * node (except for the new (inserted) one)

 get item number in source node */

		/*

		 * FIXME: there is no check that item operation did not

		 * consume too much memory

 this is not being changed */

 pointer to data which is going to be pasted */

 virtual inserted item is not defined yet */

not pasted or cut */ ,

	/*

	 * set right merge flag we take right delimiting key and

	 * check whether it is a mergeable item

			/*

			 * we delete last item and it could be merged

			 * with right neighbor's first item

				/*

				 * node contains more than 1 item, or item

				 * is not directory item, or this item

				 * contains more than 1 entry

/*

 * Using virtual node check, how many items can be

 * shifted to left neighbor

 internal level */

 leaf level */

 no free space or nothing to move */

 all contents of S[0] fits into L[0] */

 first item may be merge with last item in left neighbor */

 the item can be shifted entirely */

 the item cannot be shifted entirely, try to split it */

		/*

		 * check whether L[0] can hold ih and at least one byte

		 * of the item body

 cannot shift even a part of the current item */

 count partially shifted item */

/*

 * Using virtual node check, how many items can be

 * shifted to right neighbor

 internal level */

 leaf level */

 no free space  */

 all contents of S[0] fits into R[0] */

 last item may be merge with first item in right neighbor */

 the item can be shifted entirely */

		/*

		 * check whether R[0] can hold ih and at least one

		 * byte of the item body

 cannot shift even a part of the current item */

		/*

		 * R[0] can hold the header of the item and at least

		 * one byte of its body

 cur_free is still > 0 */

 count partially shifted item */

/*

 * from - number of items, which are shifted to left neighbor entirely

 * to - number of item, which are shifted to right neighbor entirely

 * from_bytes - number of bytes of boundary item (or directory entries)

 *              which are shifted to left neighbor

 * to_bytes - number of bytes of boundary item (or directory entries)

 *            which are shifted to right neighbor

 position of item we start filling node from */

 position of item we finish filling node by */

	/*

	 * number of first bytes (entries for directory) of start_item-th item

	 * we do not include into node that is being filled

	/*

	 * number of last bytes (entries for directory) of end_item-th item

	 * we do node include into node that is being filled

	/*

	 * these are positions in virtual item of items, that are split

	 * between S[0] and S1new and S1new and S2new

	/*

	 * We only create additional nodes if we are in insert or paste mode

	 * or we are in replace mode at the internal level. If h is 0 and

	 * the mode is M_REPLACE then in fix_nodes we change the mode to

	 * paste or insert before we get here in the code.

	/*

	 * snum012 [0-2] - number of items, that lay

	 * to S[0], first new node and second new node

 s1bytes */

 s2bytes */

 internal level */

 leaf level */

 start from 'from'-th item */

 skip its first 'start_bytes' units */

 last included item is the 'end_item'-th one */

 do not count last 'end_bytes' units of 'end_item'-th item */

	/*

	 * go through all item beginning from the start_item-th item

	 * and ending by the end_item-th item. Do not count first

	 * 'start_bytes' units of 'start_item'-th item and last

	 * 'end_bytes' of 'end_item'-th item

 get size of current item */

		/*

		 * do not take in calculation head part (from_bytes)

		 * of from-th item

from start */ , start_bytes);

 do not take in calculation tail part of last item */

from end */ , skip_from_end);

 if item fits into current node entierly */

		/*

		 * virtual item length is longer, than max size of item in

		 * a node. It is impossible for direct item

 we will try to split it */

 as we do not split items, take new node and continue */

		/*

		 * calculate number of item units which fit into node being

		 * filled

			/*

			 * nothing fits into current node, take new

			 * node and continue

 something fits into the current node */

 continue from the same item with start_bytes != -1 */

	/*

	 * sum012[4] (if it is not -1) contains number of units of which

	 * are to be in S1new, snum012[3] - to be in S0. They are supposed

	 * to be S1bytes and S2bytes correspondingly, so recalculate

 s2bytes */

 now we know S2bytes, calculate S1bytes */

 s1bytes */

/*

 * Set parameters for balancing.

 * Performs write of results of analysis of balancing into structure tb,

 * where it will later be used by the functions that actually do the balancing.

 * Parameters:

 *	tb	tree_balance structure;

 *	h	current level of the node;

 *	lnum	number of items from S[h] that must be shifted to L[h];

 *	rnum	number of items from S[h] that must be shifted to R[h];

 *	blk_num	number of blocks that S[h] will be splitted into;

 *	s012	number of items that fall into splitted nodes.

 *	lbytes	number of bytes which flow to the left neighbor from the

 *              item that is not shifted entirely

 *	rbytes	number of bytes which flow to the right neighbor from the

 *              item that is not shifted entirely

 *	s1bytes	number of bytes which flow to the first  new node when

 *              S[0] splits (this number is contained in s012 array)

 only for leaf level */

/*

 * check if node disappears if we shift tb->lnum[0] items to left

 * neighbor and tb->rnum[0] to the right one.

	/*

	 * number of items that will be shifted to left (right) neighbor

	 * entirely

 how many items remain in S[0] after shiftings to neighbors */

 all content of node can be shifted to neighbors */

 S[0] is not removable */

 check whether we can divide 1 remaining item between neighbors */

 get size of remaining item (in item units) */

 check whether L, S, R can be joined in one node */

 there was only one item and it will be deleted */

			/*

			 * Directory must be in correct state here: that is

			 * somewhere at the left side should exist first

			 * directory item. But the item being deleted can

			 * not be that first one because its right neighbor

			 * is item of the same directory. (But first item

			 * always gets deleted in last turn). So, neighbors

			 * of deleted item can be merged, so we can save

			 * ih_size

				/*

				 * we might check that left neighbor exists

				 * and is of the same directory

 when we do not split item, lnum and rnum are numbers of entire items */

/*

 * Get new buffers for storing new nodes that are created while balancing.

 * Returns:	SCHEDULE_OCCURRED - schedule occurred while the function worked;

 *	        CARRY_ON - schedule didn't occur while the function worked;

 *	        NO_DISK_SPACE - no disk space.

 The function is NOT SCHEDULE-SAFE! */

 number of needed empty blocks */

	/*

	 * number_of_freeblk is the number of empty blocks which have been

	 * acquired for use by the balancing algorithm minus the number of

	 * empty blocks used in the previous levels of the analysis,

	 * number_of_freeblk = tb->cur_blknum can be non-zero if a schedule

	 * occurs after empty blocks are acquired, and the balancing analysis

	 * is then restarted, amount_needed is the number needed by this

	 * level (h) of the balancing analysis.

	 *

	 * Note that for systems with many processes writing, it would be

	 * more layout optimal to calculate the total number needed by all

	 * levels and then to run reiserfs_new_blocks to get all of them at

	 * once.

	/*

	 * Initiate number_of_freeblk to the amount acquired prior to the

	 * restart of the analysis or 0 if not restarted, then subtract the

	 * amount needed by all of the levels of the tree below h.

 blknum includes S[h], so we subtract 1 in this calculation */

 Allocate missing empty blocks. */

 if Sh == 0  then we are getting a new root */

	/*

	 * Amount_needed = the amount that we need more than the

	 * amount that we have.

 If we have enough already then there is nothing to do. */

	/*

	 * No need to check quota - is not allocated for blocks used

	 * for formatted nodes

 for each blocknumber we just got, get a buffer and stick it on FEB */

 Put empty buffers into the array. */

/*

 * Get free space of the left neighbor, which is stored in the parent

 * node of the left neighbor.

/*

 * Get free space of the right neighbor,

 * which is stored in the parent node of the right neighbor.

 Check whether left neighbor is in memory. */

 Father of the left neighbor does not exist. */

 Calculate father of the node to be balanced. */

	/*

	 * Get position of the pointer to the left neighbor

	 * into the left father.

 Get left neighbor block number. */

 Look for the left neighbor in the cache. */

 call item specific function for this key */

/*

 * Calculate far left/right parent of the left/right neighbor of the

 * current node, that is calculate the left/right (FL[h]/FR[h]) neighbor

 * of the parent F[h].

 * Calculate left/right common parent of the current node and L[h]/R[h].

 * Calculate left/right delimiting key position.

 * Returns:	PATH_INCORRECT    - path in the tree is not correct

 *		SCHEDULE_OCCURRED - schedule occurred while the function worked

 *	        CARRY_ON          - schedule didn't occur while the function

 *				    worked

	/*

	 * Starting from F[h] go upwards in the tree, and look for the common

	 * ancestor of F[h], and its neighbor l/r, that should be obtained.

		/*

		 * Check whether parent of the current buffer in the path

		 * is really parent in the tree.

 Check whether position in the parent is correct. */

		/*

		 * Check whether parent at the path really points

		 * to the child.

		/*

		 * Return delimiting key if position in the parent is not

		 * equal to first/last one.

(*pcom_father = parent)->b_count++; */

 if we are in the root of the tree, then there is no common father */

		/*

		 * Check whether first buffer in the path is the

		 * root of the tree.

 Check whether the common parent is locked. */

 Release the write lock while the buffer is busy */

	/*

	 * So, we got common parent of the current node and its

	 * left/right neighbor.  Now we are getting the parent of the

	 * left/right neighbor.

 Form key to get parent of the left/right neighbor. */

 path is released */

/*

 * Get parents of neighbors of node in the path(S[path_offset]) and

 * common parents of S[path_offset] and L[path_offset]/R[path_offset]:

 * F[path_offset], FL[path_offset], FR[path_offset], CFL[path_offset],

 * CFR[path_offset].

 * Calculate numbers of left and right delimiting keys position:

 * lkey[path_offset], rkey[path_offset].

 * Returns:	SCHEDULE_OCCURRED - schedule occurred while the function worked

 *	        CARRY_ON - schedule didn't occur while the function worked

 Current node is the root of the tree or will be root of the tree */

		/*

		 * The root can not have parents.

		 * Release nodes which previously were obtained as

		 * parents of the current node neighbors.

 Get parent FL[path_offset] of L[path_offset]. */

 Current node is not the first child of its parent. */

		/*

		 * Calculate current parent of L[path_offset], which is the

		 * left neighbor of the current node.  Calculate current

		 * common parent of L[path_offset] and the current node.

		 * Note that CFL[path_offset] not equal FL[path_offset] and

		 * CFL[path_offset] not equal F[path_offset].

		 * Calculate lkey[path_offset].

 New initialization of FL[h]. */

 New initialization of CFL[h]. */

 Get parent FR[h] of R[h]. */

 Current node is the last child of F[h]. FR[h] != F[h]. */

		/*

		 * Calculate current parent of R[h], which is the right

		 * neighbor of F[h].  Calculate current common parent of

		 * R[h] and current node. Note that CFR[h] not equal

		 * FR[path_offset] and CFR[h] not equal F[h].

 Current node is not the last child of its parent F[h]. */

 New initialization of FR[path_offset]. */

 New initialization of CFR[path_offset]. */

/*

 * it is possible to remove node as result of shiftings to

 * neighbors even when we insert or paste item.

 shifting may merge items which might save space */

 node can not be removed */

 new item fits into node S[h] without any shifting */

/*

 * Check whether current node S[h] is balanced when increasing its size by

 * Inserting or Pasting.

 * Calculate parameters for balancing for current level h.

 * Parameters:

 *	tb	tree_balance structure;

 *	h	current level of the node;

 *	inum	item number in S[h];

 *	mode	i - insert, p - paste;

 * Returns:	1 - schedule occurred;

 *	        0 - balancing for higher levels needed;

 *	       -1 - no balancing for higher levels needed;

 *	       -2 - no disk space.

 ip means Inserting or Pasting */

	/*

	 * Number of bytes that must be inserted into (value is negative

	 * if bytes are deleted) buffer which contains node being balanced.

	 * The mnemonic is that the attempted change in node space used

	 * level is levbytes bytes.

 free space in L, S and R */ ;

	/*

	 * nver is short for number of vertixes, and lnver is the number if

	 * we shift to the left, rnver is the number if we shift to the

	 * right, and lrnver is the number if we shift in both directions.

	 * The goal is to minimize first the number of vertixes, and second,

	 * the number of vertixes whose contents are changed by shifting,

	 * and third the number of uncached vertixes whose contents are

	 * changed by shifting and must be read from disk.

	/*

	 * used at leaf level only, S0 = S[0] is the node being balanced,

	 * sInum [ I = 0,1,2 ] is the number of items that will

	 * remain in node SI after balancing.  S1 and S2 are new

	 * nodes that might be created.

	/*

	 * we perform 8 calls to get_num_ver().  For each call we

	 * calculate five parameters.  where 4th parameter is s1bytes

	 * and 5th - s2bytes

	 *

	 * s0num, s1num, s2num for 8 cases

	 * 0,1 - do not shift and do not shift but bottle

	 * 2   - shift only whole item to left

	 * 3   - shift to left and bottle as much as possible

	 * 4,5 - shift to right (whole items and as much as possible

	 * 6,7 - shift to both directions (whole items and as much as possible)

 Sh is the node whose balance is currently being checked */

 Calculate balance parameters for creating new root. */

 no balancing for higher levels needed */

 get parents of S[h] neighbors. */

 get free space of neighbors */

 and new item fits into node S[h] without any shifting */

	/*

	 * determine maximal number of items we can shift to the left

	 * neighbor (in tb structure) and the maximal number of bytes

	 * that can flow to the left neighbor from the left most liquid

	 * item that cannot be shifted from S[0] entirely (returned value)

	/*

	 * determine maximal number of items we can shift to the right

	 * neighbor (in tb structure) and the maximal number of bytes

	 * that can flow to the right neighbor from the right most liquid

	 * item that cannot be shifted from S[0] entirely (returned value)

	/*

	 * all contents of internal node S[h] can be moved into its

	 * neighbors, S[h] will be removed after balancing

		/*

		 * Since we are working on internal nodes, and our internal

		 * nodes have fixed size entries, then we can balance by the

		 * number of items rather than the space they consume.  In this

		 * routine we set the left node equal to the right node,

		 * allowing a difference of less than or equal to 1 child

		 * pointer.

	/*

	 * this checks balance condition, that any two neighboring nodes

	 * can not fit in one node

	/*

	 * all contents of S[0] can be moved into its neighbors

	 * S[0] will be removed after balancing.

	/*

	 * why do we perform this check here rather than earlier??

	 * Answer: we can win 1 node in some cases above. Moreover we

	 * checked it above, when we checked, that S[0] is not removable

	 * in principle

 new item fits into node S[h] without any shifting */

 regular overflowing of the node */

		/*

		 * get_num_ver works in 2 modes (FLOW & NO_FLOW)

		 * lpar, rpar - number of items we can shift to left/right

		 *              neighbor (including splitting item)

		 * nset, lset, rset, lrset - shows, whether flowing items

		 *                           give better packing

 do not any splitting */

 we choose one of the following */

		/*

		 * calculate number of blocks S[h] must be split into when

		 * nothing is shifted to the neighbors, as well as number of

		 * items in each part of the split node (s012 numbers),

		 * and number of bytes (s1bytes) of the shared drop which

		 * flow to S1 if any

			/*

			 * note, that in this case we try to bottle

			 * between S[0] and S1 (S1 - the first new node)

		/*

		 * calculate number of blocks S[h] must be split into when

		 * l_shift_num first items and l_shift_bytes of the right

		 * most liquid item to be shifted are shifted to the left

		 * neighbor, as well as number of items in each part of the

		 * splitted node (s012 numbers), and number of bytes

		 * (s1bytes) of the shared drop which flow to S1 if any

		/*

		 * calculate number of blocks S[h] must be split into when

		 * r_shift_num first items and r_shift_bytes of the left most

		 * liquid item to be shifted are shifted to the right neighbor,

		 * as well as number of items in each part of the splitted

		 * node (s012 numbers), and number of bytes (s1bytes) of the

		 * shared drop which flow to S1 if any

		/*

		 * calculate number of blocks S[h] must be split into when

		 * items are shifted in both directions, as well as number

		 * of items in each part of the splitted node (s012 numbers),

		 * and number of bytes (s1bytes) of the shared drop which

		 * flow to S1 if any

		/*

		 * Our general shifting strategy is:

		 * 1) to minimized number of new nodes;

		 * 2) to minimized number of neighbors involved in shifting;

		 * 3) to minimized number of disk reads;

 we can win TWO or ONE nodes by shifting in both directions */

		/*

		 * if shifting doesn't lead to better packing

		 * then don't shift

		/*

		 * now we know that for better packing shifting in only one

		 * direction either to the left or to the right is required

		/*

		 * if shifting to the left is better than

		 * shifting to the right

		/*

		 * if shifting to the right is better than

		 * shifting to the left

		/*

		 * now shifting in either direction gives the same number

		 * of nodes and we can make use of the cached neighbors

		/*

		 * shift to the right independently on whether the

		 * right neighbor in cache or not

/*

 * Check whether current node S[h] is balanced when Decreasing its size by

 * Deleting or Cutting for INTERNAL node of S+tree.

 * Calculate parameters for balancing for current level h.

 * Parameters:

 *	tb	tree_balance structure;

 *	h	current level of the node;

 *	inum	item number in S[h];

 *	mode	i - insert, p - paste;

 * Returns:	1 - schedule occurred;

 *	        0 - balancing for higher levels needed;

 *	       -1 - no balancing for higher levels needed;

 *	       -2 - no disk space.

 *

 * Note: Items of internal nodes have fixed size, so the balance condition for

 * the internal part of S+tree is as for the B-trees.

	/*

	 * Sh is the node whose balance is currently being checked,

	 * and Fh is its father.

 free space in L and R */ ;

	/*

	 * using tb->insert_size[h], which is negative in this case,

	 * create_virtual_node calculates:

	 * new_nr_item = number of items node would have if operation is

	 * performed without balancing (new_nr_item);

 S[h] is the root. */

 no balancing for higher levels needed */

		/*

		 * new_nr_item == 0.

		 * Current root will be deleted resulting in

		 * decrementing the tree height.

 get free space of neighbors */

 determine maximal number of items we can fit into neighbors */

	/*

	 * Balance condition for the internal node is valid.

	 * In this case we balance only if it leads to better packing.

		/*

		 * Here we join S[h] with one of its neighbors,

		 * which is impossible with greater values of new_nr_item.

 All contents of S[h] can be moved to L[h]. */

 All contents of S[h] can be moved to R[h]. */

		/*

		 * All contents of S[h] can be moved to the neighbors

		 * (L[h] & R[h]).

 Balancing does not lead to better packing. */

	/*

	 * Current node contain insufficient number of items.

	 * Balancing is required.

 Check whether we can merge S[h] with left neighbor. */

 Check whether we can merge S[h] with right neighbor. */

 All contents of S[h] can be moved to the neighbors (L[h] & R[h]). */

 For internal nodes try to borrow item from a neighbor */

 Borrow one or two items from caching neighbor */

/*

 * Check whether current node S[h] is balanced when Decreasing its size by

 * Deleting or Truncating for LEAF node of S+tree.

 * Calculate parameters for balancing for current level h.

 * Parameters:

 *	tb	tree_balance structure;

 *	h	current level of the node;

 *	inum	item number in S[h];

 *	mode	i - insert, p - paste;

 * Returns:	1 - schedule occurred;

 *	        0 - balancing for higher levels needed;

 *	       -1 - no balancing for higher levels needed;

 *	       -2 - no disk space.

	/*

	 * Number of bytes that must be deleted from

	 * (value is negative if bytes are deleted) buffer which

	 * contains node being balanced.  The mnemonic is that the

	 * attempted change in node space used level is levbytes bytes.

 the maximal item size */

	/*

	 * S0 is the node whose balance is currently being checked,

	 * and F0 is its father.

 free space in L and R */ ;

 maximal possible size of an item */

 S[0] is the root now. */

 get free space of neighbors */

 if 3 leaves can be merge to one, set parameters and return */

	/*

	 * determine maximal number of items we can shift to the left/right

	 * neighbor and the maximal number of bytes that can flow to the

	 * left/right neighbor from the left/right most liquid item that

	 * cannot be shifted from S[0] entirely

 check whether we can merge S with left neighbor. */

 S can not be merged with R */

 set parameter to merge S[0] with its left neighbor */

 check whether we can merge S[0] with right neighbor. */

	/*

	 * All contents of S[0] can be moved to the neighbors (L[0] & R[0]).

	 * Set parameters and return

 Balancing is not required. */

/*

 * Check whether current node S[h] is balanced when Decreasing its size by

 * Deleting or Cutting.

 * Calculate parameters for balancing for current level h.

 * Parameters:

 *	tb	tree_balance structure;

 *	h	current level of the node;

 *	inum	item number in S[h];

 *	mode	d - delete, c - cut.

 * Returns:	1 - schedule occurred;

 *	        0 - balancing for higher levels needed;

 *	       -1 - no balancing for higher levels needed;

 *	       -2 - no disk space.

/*

 * Check whether current node S[h] is balanced.

 * Calculate parameters for balancing for current level h.

 * Parameters:

 *

 *	tb	tree_balance structure:

 *

 *              tb is a large structure that must be read about in the header

 *		file at the same time as this procedure if the reader is

 *		to successfully understand this procedure

 *

 *	h	current level of the node;

 *	inum	item number in S[h];

 *	mode	i - insert, p - paste, d - delete, c - cut.

 * Returns:	1 - schedule occurred;

 *	        0 - balancing for higher levels needed;

 *	       -1 - no balancing for higher levels needed;

 *	       -2 - no disk space.

 Calculate balance parameters when size of node is increasing. */

 Calculate balance parameters when  size of node is decreasing. */

 Check whether parent at the path is the really parent of the current node.*/

 We are in the root or in the new root. */

 Root is not changed. */

 Root is changed and we must recalculate the path. */

 Parent in the path is not in the tree. */

 Parent in the path is not parent of the current node in the tree. */

	/*

	 * Parent in the path is unlocked and really parent

	 * of the current node.

/*

 * Using lnum[h] and rnum[h] we should determine what neighbors

 * of S[h] we

 * need in order to balance S[h], and get them if necessary.

 * Returns:	SCHEDULE_OCCURRED - schedule occurred while the function worked;

 *	        CARRY_ON - schedule didn't occur while the function worked;

 We need left neighbor to balance S[h]. */

 We need right neighbor to balance S[path_offset]. */

/*

 * maybe we should fail balancing we are going to perform when kmalloc

 * fails several times. But now it will loop until kmalloc gets

 * required memory

 we have to allocate more memory for virtual node */

 free memory allocated before */

 this is not needed if kfree is atomic */

 virtual node requires now more memory */

 get memory for virtual item */

			/*

			 * getting memory with GFP_KERNEL priority may involve

			 * balancing now (due to indirect_to_direct conversion

			 * on dcache shrinking). So, release path and collected

			 * resources here

				/*

				 * if I understand correctly, we can only

				 * be sure the last buffer in the path is

				 * in the tree --clm

		/*

		 * as far as I can tell, this is not required.  The FEB list

		 * seems to be full of newly allocated nodes, which will

		 * never be locked, dirty, or anything else.

		 * To be safe, I'm putting in the checks and waits in.

		 * For the moment, they are needed to keep the code in

		 * journal.c from complaining about the buffer.

		 * That code is inside CONFIG_REISERFS_CHECK as well.  --clm

 Don't loop forever.  Try to recover from possible error. */

/*

 * Prepare for balancing, that is

 *	get all necessary parents, and neighbors;

 *	analyze what and where should be moved;

 *	get sufficient number of new nodes;

 * Balancing will start only after all resources will be collected at a time.

 *

 * When ported to SMP kernels, only at the last moment after all needed nodes

 * are collected in cache, will the resources be locked using the usual

 * textbook ordered lock acquisition algorithms.  Note that ensuring that

 * this code neither write locks what it does not need to write lock nor locks

 * out of order will be a pain in the butt that could have been avoided.

 * Grumble grumble. -Hans

 *

 * fix is meant in the sense of render unchanging

 *

 * Latency might be improved by first gathering a list of what buffers

 * are needed and then getting as many of them in parallel as possible? -Hans

 *

 * Parameters:

 *	op_mode	i - insert, d - delete, c - cut (truncate), p - paste (append)

 *	tb	tree_balance structure;

 *	inum	item number in S[h];

 *      pos_in_item - comment this if you can

 *      ins_ih	item head of item being inserted

 *	data	inserted item or data to be pasted

 * Returns:	1 - schedule occurred while the function worked;

 *	        0 - schedule didn't occur while the function worked;

 *             -1 - if no_disk_space

	/*

	 * we set wait_tb_buffers_run when we have to restore any dirty

	 * bits cleared during wait_tb_buffers_run

	/*

	 * we prepare and log the super here so it will already be in the

	 * transaction when do_balance needs to change it.

	 * This way do_balance won't have to schedule when trying to prepare

	 * the super for logging

 if it possible in indirect_to_direct conversion */

 Check parameters. */

 FIXME: maybe -ENOMEM when tb->vn_buf == 0? Now just repeat */

 Starting from the leaf level; for all levels h of the tree. */

 No balancing for higher levels needed. */

				/*

				 * ok, analysis and resource gathering

				 * are complete

		/*

		 * No disk space, or schedule occurred and analysis may be

		 * invalid and needs to be redone.

		/*

		 * We have a positive insert size but no nodes exist on this

		 * level, this means that we are creating a new root.

			/*

			 * The tree needs to be grown, so this node S[h]

			 * which is the root node is split into two nodes,

			 * and a new node (S[h+1]) will be created to

			 * become the root node.

	/*

	 * fix_nodes was unable to perform its calculation due to

	 * filesystem got changed under us, lack of free disk space or i/o

	 * failure. If the first is the case - the search will be

	 * repeated. For now - free all resources acquired so far except

	 * for the new allocated nodes

 Release path buffers. */

 brelse all resources collected for balancing */

 Release path buffers. */

 brelse all resources collected for balancing */

 deal with list of allocated (used and unused) nodes */

			/*

			 * de-allocated block which was not used by

			 * balancing and bforget about buffer for it

 release used as new nodes including a new root */

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

/*

 * copy copy_count entries from source directory item to dest buffer

 * (creating new item if needed)

	/*

	 * either the number of target item, or if we must create a

	 * new item, the number of the item we will create it next to

 length of all records in item to be copied */

	/*

	 * length of all record to be copied and first byte of

	 * the last of them

 when copy last to first, dest buffer can contain 0 items */

	/*

	 * if there are no items in dest or the first/last item in

	 * dest is not item of the same directory

COMP_SHORT_KEYS */ (&ih->ih_key,

 create new item in dest */

 form item header */

 calculate item len */

 form key by the following way */

				/*

				 * no entries will be copied to this

				 * item in this function

				/*

				 * this item is not yet valid, but we

				 * want I_IS_DIRECTORY_ITEM to return 1

				 * for it, so we -1

 insert item into dest buffer */

 prepare space for entries */

/*

 * Copy the first (if last_first == FIRST_TO_LAST) or last

 * (last_first == LAST_TO_FIRST) item or part of it or nothing

 * (see the return 0 below) from SOURCE to the end (if last_first)

 * or beginning (!last_first) of the DEST

 returns 1 if anything was copied, else 0 */

 number of items in the source and destination buffers */

	/*

	 * if ( DEST is empty or first item of SOURCE and last item of

	 * DEST are the items of different objects or of different types )

	 * then there is no need to treat this item differently from the

	 * other items that we copy, so we return

 there is nothing to merge */

 copy all entries to dest */

		/*

		 * copy part of the body of the first item of SOURCE

		 * to the end of the body of the last item of the DEST

		 * part defined by 'bytes_or_entries'; if bytes_or_entries

		 * == -1 copy whole body; don't create new item header

		/*

		 * merge first item (or its part) of src buffer with the last

		 * item of dest buffer. Both are of the same file

 copy boundary item to right (last_first == LAST_TO_FIRST) */

	/*

	 * (DEST is empty or last item of SOURCE and first item of DEST

	 * are the items of different object or of different types)

		/*

		 * bytes_or_entries = entries number in last

		 * item body of SOURCE

	/*

	 * copy part of the body of the last item of SOURCE to the

	 * begin of the body of the first item of the DEST; part defined

	 * by 'bytes_or_entries'; if byte_or_entriess == -1 copy whole body;

	 * change first item key of the DEST; don't create new item header

 bytes_or_entries = length of last item body of SOURCE */

 change first item key of the DEST */

 item becomes non-mergeable */

 or mergeable if left item was */

 merge to right only part of item */

 change first item key of the DEST */

/*

 * copy cpy_mun items from buffer src to buffer dest

 * last_first == FIRST_TO_LAST means, that we copy cpy_num items beginning

 *                             from first-th item in src to tail of dest

 * last_first == LAST_TO_FIRST means, that we copy cpy_num items beginning

 *                             from first-th item in src to head of dest

	/*

	 * we will insert items before 0-th or nr-th item in dest buffer.

	 * It depends of last_first parameter

 location of head of first new item */

 prepare space for headers */

 copy item headers */

 location of unmovable item */

 prepare space for items */

 check free space */

 copy items */

 sizes, item number */

/*

 * This function splits the (liquid) item into two items (useful when

 * shifting part of an item into another node.)

		/*

		 * if ( if item in position item_num in buffer SOURCE

		 * is directory item )

			/*

			 * copy part of the body of the item number 'item_num'

			 * of SOURCE to the end of the DEST part defined by

			 * 'cpy_bytes'; create new item header; change old

			 * item_header (????); n_ih = new item_header;

 JDM Endian safe, both le */

		/*

		 * if ( if item in position item_num in buffer

		 * SOURCE is directory item )

			/*

			 * copy part of the body of the item number 'item_num'

			 * of SOURCE to the begin of the DEST part defined by

			 * 'cpy_bytes'; create new item header;

			 * n_ih = new item_header;

 Endian safe, both le */

 indirect item */

 set item length */

 Endian safe, both le */

/*

 * If cpy_bytes equals minus one than copy cpy_num whole items from SOURCE

 * to DEST.  If cpy_bytes not equal to minus one than copy cpy_num-1 whole

 * items from SOURCE to DEST.  From last item copy cpy_num bytes for regular

 * item and cpy_num directory entries for directory item.

 copy items to left */

		/*

		 * copy the first item or it part or nothing to the end of

		 * the DEST (i = leaf_copy_boundary_item(DEST,SOURCE,0,bytes))

			/*

			 * copy first cpy_num items starting from position

			 * 'pos' of SOURCE to end of DEST

			/*

			 * copy first cpy_num-1 items starting from position

			 * 'pos-1' of the SOURCE to the end of the DEST

			/*

			 * copy part of the item which number is

			 * cpy_num+pos-1 to the end of the DEST

 copy items to right */

		/*

		 * copy the last item or it part or nothing to the

		 * begin of the DEST

		 * (i = leaf_copy_boundary_item(DEST,SOURCE,1,bytes));

			/*

			 * starting from position 'pos' copy last cpy_num

			 * items of SOURCE to begin of DEST

			/*

			 * copy last cpy_num-1 items starting from position

			 * 'pos+1' of the SOURCE to the begin of the DEST;

			/*

			 * copy part of the item which number is pos to

			 * the begin of the DEST

/*

 * there are types of coping: from S[0] to L[0], from S[0] to R[0],

 * from R[0] to L[0]. for each of these we have to define parent and

 * positions of destination and source buffers

 define dest, src, dest parent, dest position */

 it is used in leaf_shift_left */

 src->b_item_order */

 it is used in leaf_shift_right */

 it is used in balance_leaf_when_delete */

 it is used in balance_leaf_when_delete */

/*

 * copy mov_num items and mov_bytes of the (mov_num-1)th item to

 * neighbor. Delete them from source

/*

 * Shift shift_num items (and shift_bytes of last shifted item if

 * shift_bytes != -1) from S[0] to L[0] and replace the delimiting key

	/*

	 * move shift_num (and shift_bytes bytes) items from S[0]

	 * to left neighbor L[0]

 number of items in S[0] == 0 */

 replace lkey in CFL[0] by 0-th key from S[0]; */

 CLEANING STOPPED HERE */

/*

 * Shift shift_num (shift_bytes) items from S[0] to the right neighbor,

 * and replace the delimiting key

	/*

	 * move shift_num (and shift_bytes) items from S[0] to

	 * right neighbor R[0]

 replace rkey in CFR[0] by the 0-th key from R[0] */

/*

 * If del_bytes == -1, starting from position 'first' delete del_num

 * items in whole in buffer CUR.

 *   If not.

 *   If last_first == 0. Starting from position 'first' delete del_num-1

 *   items in whole. Delete part of body of the first item. Part defined by

 *   del_bytes. Don't delete first item header

 *   If last_first == 1. Starting from position 'first+1' delete del_num-1

 *   items in whole. Delete part of body of the last item . Part defined by

 *   del_bytes. Don't delete last item header.

 delete del_num items beginning from item in position first */

			/*

			 * delete del_num-1 items beginning from

			 * item in position first

			/*

			 * delete the part of the first item of the bh

			 * do not delete item header

			/*

			 * delete del_num-1 items beginning from

			 * item in position first+1

 the last item is directory  */

				/*

				 * len = numbers of directory entries

				 * in this item

 len = body len of item */

			/*

			 * delete the part of the last item of the bh

			 * do not delete item header

 insert item into the leaf node in position before */

 check free space */

 get item new item must be inserted before */

 prepare space for the body of new item */

 copy body to prepared space */

 insert item header */

 change locations */

 sizes, free space, item number */

/*

 * paste paste_size bytes to affected_item_num-th item.

 * When item is a directory, this only prepare space for new entries

 check free space */

 CONFIG_REISERFS_CHECK */

 item to be appended */

 prepare space */

 change locations */

 shift data to right */

 paste data in the head of item */

 change free space */

/*

 * cuts DEL_COUNT entries beginning from FROM-th entry. Directory item

 * does not have free space, so it moves DEHs and remaining records as

 * necessary. Return value is size of removed part of directory item

 * in bytes.

 offset of record, that is (from-1)th */

 */

 length of all removed records */

	/*

	 * make sure that item is directory and there are enough entries to

	 * remove

 first byte of item */

 entry head array */

	/*

	 * first byte of remaining entries, those are BEFORE cut entries

	 * (prev_record) and length of all removed records (cut_records_len)

from_record */  -

 adjust locations of remaining entries */

 shift entry head array and entries those are AFTER removed entries */

 shift records, those are BEFORE removed entries */

/*

 * when cut item is part of regular file

 *      pos_in_item - first byte that must be cut

 *      cut_size - number of bytes to be cut beginning from pos_in_item

 *

 * when cut item is part of directory

 *      pos_in_item - number of first deleted entry

 *      cut_size - count of deleted entries

 item head of truncated item */

 first cut entry () */

 change key */

 change item key by key of first entry in the item */

 item is direct or indirect */

 shift item body to left if cut is from the head of item */

 change key of item */

 location of the last item */

 location of the item, which is remaining at the same place */

 shift */

 change item length */

 change locations */

 size, free space */

 delete del_num items from buffer starting from the first'th item */

 this does not work */

 location of unmovable item */

 delete items */

 delete item headers */

 change item location */

 sizes, item number */

/*

 * paste new_entry_count entries (new_dehs, records) into position

 * before to item_num-th item

	/*

	 * make sure, that item is directory, and there are enough

	 * records in it

 first byte of dest item */

 entry head array */

 new records will be pasted at this point */

 adjust locations of records that will be AFTER new records */

 adjust locations of records that will be BEFORE new records */

 prepare space for pasted records */

 copy new records */

 prepare space for new entry heads */

 copy new entry heads */

 set locations of new records */

 change item key if necessary (when we paste before 0-th entry */

 check record locations */

 -*- linux-c -*- */

 fs/reiserfs/procfs.c */

/*

 * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README

 proc info support a la one created by Sizif@Botik.RU for PGC */

/*

 * LOCKING:

 *

 * These guys are evicted from procfs as the very first step in ->kill_sb().

 *

 on-disk fields */

 incore fields */

 reiserfs_proc_info_data_t.journal fields */

 Some block devices use /'s */

 Some block devices use /'s */

/*

 * Revision 1.1.8.2  2001/07/15 17:08:42  god

 *  . use get_super() in procfs.c

 *  . remove remove_save_link() from reiserfs_do_truncate()

 *

 * I accept terms and conditions stated in the Legal Agreement

 * (available at http://www.namesys.com/legalese.html)

 *

 * Revision 1.1.8.1  2001/07/11 16:48:50  god

 * proc info support

 *

 * I accept terms and conditions stated in the Legal Agreement

 * (available at http://www.namesys.com/legalese.html)

 *

 SPDX-License-Identifier: GPL-2.0-or-later

/* CacheFiles path walking and related routines

 *

 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

/*

 * dump debugging info about an object

/*

 * dump debugging info about a pair of objects

/*

 * mark the owner of a dentry, if there is one, to indicate that that dentry

 * has been preemptively deleted

 * - the caller must hold the i_mutex on the dentry's parent as required to

 *   call vfs_unlink(), vfs_rmdir() or vfs_rename()

 found the dentry for  */

/*

 * record the fact that an object is now active

	/* an old object from a previous incarnation is hogging the slot - we

		/* if the object we're waiting for is queued for processing,

		/* otherwise we sleep until either the object we're waiting for

/*

 * Mark an object as being inactive.

	/* This object can now be culled, so we need to let the daemon know

	 * that there is something it can remove if it needs to.

/*

 * delete an object representation from the cache

 * - file backed objects are unlinked

 * - directory backed objects are stuffed into the graveyard for userspace to

 *   delete

 * - unlocks the directory mutex

 non-directories can just be unlinked */

 directories have to be moved to the graveyard */

 first step is to make up a grave dentry in the graveyard */

 do the multiway lock magic */

 do some checks before getting the grave dentry */

		/* the entry was probably culled when we dropped the parent dir

 target should not be an ancestor of source */

 attempt the rename */

/*

 * delete an object representation from the cache

		/* object allocation for the same key preemptively deleted this

		/* we need to check that our parent is _still_ our parent - it

			/* it got moved, presumably by cachefilesd culling it,

			 * so it's no longer in the key path and we can ignore

/*

 * walk from the parent object to the child object through the backing

 * filesystem, creating directories as we go

 TODO: convert file to dir

 attempt to transit the first directory component */

 key ends in a double NUL */

 search the current directory for the element name */

	/* if this element of the path doesn't exist, then the lookup phase

	 * failed, and we can release any readers in the certain knowledge that

 we need to create the object if it's negative */

 index objects and intervening tree levels must be subdirs */

 non-index objects start out life as files */

 process the next component */

 we've found the object we were looking for */

	/* if we've found that the terminal object exists, then we need to

			/* delete the object (the deleter drops the directory

 note that we're now using this object */

 attach data to a newly constructed terminal object */

		/* always update the atime on an object we've just looked up

		 * (this is used to keep track of culling, and atimes are only

		 * updated by read, write and readdir but not lookup or

 open a file interface onto a data file */

 TODO: open file in data-class subdir

/*

 * get a subdirectory

 search the current directory for the element name */

 we need to create the subdir if it doesn't exist yet */

 we need to make sure the subdir is a directory */

/*

 * find out if an object is in use or not

 * - if finds object and it's not in use:

 *   - returns a pointer to the object and a reference on it

 *   - returns with the directory locked

_enter(",%pd/,%s",

       dir, filename);

 look up the victim */

_debug("victim -> %pd %s",

       victim, d_backing_inode(victim) ? "positive" : "negative");

	/* if the object is no longer there then we probably retired the object

	 * at the netfs's request whilst the cull was in progress

 check to see if we're using this object */

_leave(" = %pd", victim);

_leave(" = -EBUSY [in use]");

 file or dir now absent - probably retired by netfs */

/*

 * cull an object if it's not in use

 * - called only by cache manager daemon

	/* okay... the victim is not being used so we can cull it

	 * - start by marking it as stale

  actually remove the victim (drops the dir mutex) */

 file or dir now absent - probably retired by netfs */

/*

 * find out if an object is in use or not

 * - called only by cache manager daemon

 * - returns -EBUSY or 0 to indicate whether an object is in use or not

_enter(",%pd/,%s",

       dir, filename);

_leave(" = 0");

 SPDX-License-Identifier: GPL-2.0-or-later

/* kiocb-using read/write

 *

 * Copyright (C) 2021 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

/*

 * Handle completion of a read from the cache.

/*

 * Initiate a read from the cache.

	/* If the caller asked us to seek for data before doing the read, then

	 * we should do that now.  If we find a gap, we fill it with zeros.

			/* The region is beyond the EOF or there's no more data

			 * in the region, so clear the rest of the buffer and

			 * return success.

		/* There's no easy way to restart the syscall since other AIO's

		 * may be already running. Just fail this IO with EINTR.

/*

 * Handle completion of a write to the cache.

 Tell lockdep we inherited freeze protection from submission thread */

/*

 * Initiate a write to the cache.

	/* Open-code file_start_write here to grab freeze protection, which

	 * will be released by another thread in aio_complete_rw().  Fool

	 * lockdep by telling it the lock got released so that it doesn't

	 * complain about the held lock when we return to userspace.

		/* There's no easy way to restart the syscall since other AIO's

		 * may be already running. Just fail this IO with EINTR.

/*

 * Prepare a read operation, shortening it to a cached/uncached

 * boundary as appropriate.

/*

 * Prepare for a write to occur.

 Round to DIO size */

/*

 * Clean up an operation.

/*

 * Open the cache file when beginning a cache operation.

 SPDX-License-Identifier: GPL-2.0-or-later

/* CacheFiles extended attribute management

 *

 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

/*

 * check the type label on an object

 * - done using xattrs

 attempt to install a type label directly */

 we succeeded */

 read the current type label */

 check the type is what we're expecting */

/*

 * set the state xattr on a cache file

 attempt to install the cache metadata directly */

/*

 * update the state xattr on a cache file

 attempt to install the cache metadata directly */

/*

 * check the consistency between the backing cache and the FS-Cache cookie

/*

 * check the state xattr on a cache file

 * - return -ESTALE if the object should be deleted

 read the current type label */

			goto stale; /* no attribute - power went off

 check the on-disk object */

 consult the netfs */

 entry okay as is */

 entry requires update */

 entry requires deletion */

 update the current label */

/*

 * remove the object's xattr to mark it stale

 SPDX-License-Identifier: GPL-2.0-or-later

/* CacheFiles security management

 *

 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

/*

 * determine the security context within which we access the cache from within

 * the kernel

/*

 * see if mkdir and create can be performed in the root directory

/*

 * check the security details of the on-disk cache

 * - must be called with security override in force

 * - must return with a security override in force - even in the case of an

 *   error

	/* duplicate the cache creds for COW (the override is currently in

	/* use the cache root dir's security context as the basis with

 SPDX-License-Identifier: GPL-2.0-or-later

/* Key to pathname encoder

 *

 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

 0 - 9 */

 10 - 35 */

 36 - 61 */

 62 - 63 */

 we skip space and tab and control chars */

 '!' -> '.' */

 we skip '/' as it's significant to pathwalk */

 '0' -> '~' */

/*

 * turn the raw key into something cooked

 * - the raw key should include the length in the two bytes at the front

 * - the key may be up to 514 bytes in length (including the length word)

 *   - "base64" encode the strange keys, mapping 3 bytes of raw to four of

 *     cooked

 *   - need to cut the cooked key into 252 char lengths (189 raw bytes)

 if the path is usable ASCII, then we render it directly */

 two base64'd length chars on the front */

 @checksum/M */

		max += 3 * 2;	/* maximum number of segment dividers (".../M")

				 * is ((514 + 251) / 252) = 3

 NUL on end */

 calculate the maximum length of the cooked key */

 @checksum/M */

		max += 3 * 2;	/* maximum number of segment dividers (".../M")

				 * is ((514 + 188) / 189) = 3

 NUL on end */

 2nd NUL on end */

 build the cooked key */

 SPDX-License-Identifier: GPL-2.0-or-later

/* Bind and unbind a cache from the filesystem backing it

 *

 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

/*

 * bind a directory as a cache

 start by checking things over */

 don't permit already bound caches to be re-bound */

 make sure we have copies of the tag and dirname strings */

		/* the tag string is released by the fops->release()

 add the cache */

/*

 * add a cache

 we want to work under the module's security ID */

 allocate the root index object */

 look up the directory at the root of the cache */

 check parameters */

	/* determine the security of the on-disk cache as this governs

 get the cache size and blocksize */

 set up caching limits */

 get the cache directory and check its type */

 get the graveyard directory */

 publish the cache */

 done */

 check how much space the cache has */

/*

 * unbind a cache on fd release

 SPDX-License-Identifier: GPL-2.0-or-later

/* Storage object read/write

 *

 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

/*

 * detect wake up events generated by the unlocking of pages in which we're

 * interested

 * - we use this to detect read completion of backing pages

 * - the caller holds the waitqueue lock

 unlocked, not uptodate and not erronous? */

 remove from the waitqueue */

 move onto the action list and queue for FS-Cache thread pool */

	/* We need to temporarily bump the usage count as we don't own a ref

	 * here otherwise cachefiles_read_copier() may free the op between the

	 * monitor being enqueued on the op->to_do list and the op getting

	 * enqueued on the work queue.

/*

 * handle a probably truncated page

 * - check to see if the page is still relevant and reissue the read if

 *   possible

 * - return -EIO on error, -ENODATA if the page is gone, -EINPROGRESS if we

 *   must wait again and 0 if successful

 skip if the page was truncated away completely */

	/* the page is still there and we already have a ref on it, so we don't

	/* but the page may have been read before the monitor was installed, so

	 * the monitor may miss the event - so we have to ensure that we do get

 it'll reappear on the todo list */

/*

 * copy data from backing pages to netfs pages to complete a read operation

 * - driven by FS-Cache's thread pool

 the page has probably been truncated */

 let the thread pool have some air occasionally */

/*

 * read the corresponding page to the given set from the backing file

 * - an uncertain page is simply discarded, to be tried again another time

 attempt to get hold of the backing page */

	/* we've installed a new backing page, so now we need to start

 set the monitor to transfer the data across */

 install the monitor */

	/* but the page may have been read before the monitor was installed, so

	 * the monitor may miss the event - so we have to ensure that we do get

	/* if the backing page is already present, it can be in one of

	/* the backing page is already up to date, attach the netfs

/*

 * read a page from the cache or allocate a block in which to store it

 * - cache withdrawal is prevented by the caller

 * - returns -EINTR if interrupted

 * - returns -ENOMEM if ran out of memory

 * - returns -ENOBUFS if no buffers can be made available

 * - returns -ENOBUFS if page is beyond EOF

 * - if the page is backed by a block in the cache:

 *   - a read will be started which will call the callback on completion

 *   - 0 will be returned

 * - else if the page is unbacked:

 *   - the metadata will be retained

 *   - -ENODATA will be returned

 calculate the shift required to use bmap */

	/* we assume the absence or presence of the first block is a good

	 * enough indication for the page as a whole

	 * - TODO: don't use bmap() for this as it is _not_ actually good

	 *   enough for this as it doesn't indicate errors, but it's all we've

	 *   got for the moment

		/* submit the apparently valid page to the backing fs to be

 there's space in the cache we can use */

/*

 * read the corresponding pages to the given set from the backing file

 * - any uncertain pages are simply discarded, to be tried again another time

		/* we've installed a new backing page, so now we need

		/* add the netfs page to the pagecache and LRU, and set the

 install a monitor */

		/* but the page may have been read before the monitor was

		 * installed, so the monitor may miss the event - so we have to

		/* if the backing page is already present, it can be in one of

		/* we've locked a page that's neither up to date nor erroneous,

		/* the backing page is already up to date, attach the netfs

 the netpage is unlocked and marked up to date here */

 tidy up */

/*

 * read a list of pages from the cache or allocate blocks in which to store

 * them

 calculate the shift required to use bmap */

		/* we assume the absence or presence of the first block is a

		 * good enough indication for the page as a whole

		 * - TODO: don't use bmap() for this as it is _not_ actually

		 *   good enough for this as it doesn't indicate errors, but

		 *   it's all we've got for the moment

			/* we have data - add it to the list to give to the

	/* submit the apparently valid pages to the backing fs to be read from

/*

 * allocate a block in the cache in which to store a page

 * - cache withdrawal is prevented by the caller

 * - returns -EINTR if interrupted

 * - returns -ENOMEM if ran out of memory

 * - returns -ENOBUFS if no buffers can be made available

 * - returns -ENOBUFS if page is beyond EOF

 * - otherwise:

 *   - the metadata will be retained

 *   - 0 will be returned

/*

 * allocate blocks in the cache in which to store a set of pages

 * - cache withdrawal is prevented by the caller

 * - returns -EINTR if interrupted

 * - returns -ENOMEM if ran out of memory

 * - returns -ENOBUFS if some buffers couldn't be made available

 * - returns -ENOBUFS if some pages are beyond EOF

 * - otherwise:

 *   - -ENODATA will be returned

 * - metadata will be retained for any page marked

/*

 * request a page be stored in the cache

 * - cache withdrawal is prevented by the caller

 * - this request may be ignored if there's no cache block available, in which

 *   case -ENOBUFS will be returned

 * - if the op is in progress, 0 will be returned

	/* We mustn't write more data than we have, so we have to beware of a

	 * partial page at EOF.

	/* write the page to the backing filesystem and let it store it in its

/*

 * detach a backing block from a page

 * - cache withdrawal is prevented by the caller

 SPDX-License-Identifier: GPL-2.0-or-later

/* Daemon interface

 *

 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

/*

 * do various checks

 only the superuser may do this */

 the cachefiles device may only be open once at a time */

 allocate a cache record */

	/* set default caching limits

	 * - limit at 1% free space and/or free files

	 * - cull below 5% free space and/or free files

	 * - cease culling above 7% free space and/or free files

/*

 * release a cache

 clean up the control file interface */

/*

 * read the cache state

_enter(",,%zu,", buflen);

 check how much space the cache has */

 summarise */

/*

 * command the cache

_enter(",,%zu,", datalen);

 drag the command string into the kernel so we can parse it */

 strip any newline */

 parse the command */

 run the appropriate command handler */

_leave(" = %zd", ret);

/*

 * poll for culling state

 * - use EPOLLOUT to indicate culling state

/*

 * give a range error for cache space constraints

 * - can be tail-called

/*

 * set the percentage of files at which to stop culling

 * - command: "frun <N>%"

/*

 * set the percentage of files at which to start culling

 * - command: "fcull <N>%"

/*

 * set the percentage of files at which to stop allocating

 * - command: "fstop <N>%"

/*

 * set the percentage of blocks at which to stop culling

 * - command: "brun <N>%"

/*

 * set the percentage of blocks at which to start culling

 * - command: "bcull <N>%"

/*

 * set the percentage of blocks at which to stop allocating

 * - command: "bstop <N>%"

/*

 * set the cache directory

 * - command: "dir <name>"

/*

 * set the cache security context

 * - command: "secctx <ctx>"

/*

 * set the cache tag

 * - command: "tag <name>"

/*

 * request a node in the cache be culled from the current working directory

 * - command: "cull <name>"

 extract the directory dentry from the cwd */

/*

 * set debugging mode

 * - command: "debug <mask>"

/*

 * find out whether an object in the current working directory is in use or not

 * - command: "inuse <name>"

_enter(",%s", args);

 extract the directory dentry from the cwd */

_leave(" = %d", ret);

/*

 * see if we have space for a number of pages and/or a number of files in the

 * cache

_enter("{%llu,%llu,%llu,%llu,%llu,%llu},%u,%u",

       (unsigned long long) cache->frun,

       (unsigned long long) cache->fcull,

       (unsigned long long) cache->fstop,

       (unsigned long long) cache->brun,

       (unsigned long long) cache->bcull,

       (unsigned long long) cache->bstop,

       fnr, bnr);

 find out how many pages of blockdev are available */

_debug("avail %llu,%llu",

       (unsigned long long) stats.f_ffree,

       (unsigned long long) stats.f_bavail);

 see if there is sufficient space */

_leave(" = 0");

 SPDX-License-Identifier: GPL-2.0-or-later

/* FS-Cache interface to CacheFiles

 *

 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

 auxiliary data */

 key path */

/*

 * allocate an object record for a cookie lookup and prepare the lookup data

 create a new object record and a temporary leaf image */

	/* get hold of the raw key

	 * - stick the length on the front and leave space on the back for the

	 *   encoder

 turn the raw key into something that can work with as a filename */

 get hold of the auxiliary data and prepend the object type */

/*

 * attempt to look up the nominated node in this cache

 * - return -ETIMEDOUT to be scheduled again

 look up the key, creating any missing bits */

 polish off by setting the attributes of non-index files */

/*

 * indication of lookup completion

/*

 * increment the usage count on an inode object (may fail if unmounting)

/*

 * update the auxiliary data for an object object on disk

/*

 * discard the resources pinned by an object and effect retirement if

 * requested

	/* We need to tidy the object up if we did in fact manage to open it.

	 * It's possible for us to get here before the object is fully

	 * initialised if the parent goes away or the object gets retired

	 * before we set it up.

 delete retired objects */

 close the filesystem stuff attached to the object */

 note that the object is now inactive */

/*

 * dispose of a reference to an object

/*

 * sync a cache

	/* make sure all pages pinned by operations on behalf of the netfs are

/*

 * check if the backing cache is updated to FS-Cache

 * - called by FS-Cache when evaluates if need to invalidate the cache

/*

 * notification the attributes on an object have changed

 * - called with reads/writes excluded by FS-Cache

	/* if there's an extension to a partial page at the end of the backing

	 * file, we need to discard the partial page so that we pick up new

/*

 * Invalidate an object

/*

 * dissociate a cache from all the pages it was backing

 SPDX-License-Identifier: GPL-2.0-or-later

/* Network filesystem caching backend to use cache files on a premounted

 * filesystem

 *

 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

/*

 * initialise the fs caching module

 create an object jar */

/*

 * clean up on module removal

 SPDX-License-Identifier: GPL-2.0

/*

 * Implementation of the diskquota system for the LINUX operating system. QUOTA

 * is implemented using the BSD system call interface as the means of

 * communication with the user level. This file contains the generic routines

 * called by the different filesystems on allocation of an inode or block.

 * These routines take care of the administration needed to have a consistent

 * diskquota tracking system. The ideas of both user and group quotas are based

 * on the Melbourne quota system as used on BSD derived systems. The internal

 * implementation is based on one of the several variants of the LINUX

 * inode-subsystem with added complexity of the diskquota system.

 *

 * Author:	Marco van Wieringen <mvw@planets.elm.net>

 *

 * Fixes:   Dmitry Gorodchanin <pgmdsg@ibi.com>, 11 Feb 96

 *

 *		Revised list management to avoid races

 *		-- Bill Hawes, <whawes@star.net>, 9/98

 *

 *		Fixed races in dquot_transfer(), dqget() and dquot_alloc_...().

 *		As the consequence the locking was moved from dquot_decr_...(),

 *		dquot_incr_...() to calling functions.

 *		invalidate_dquots() now writes modified dquots.

 *		Serialized quota_off() and quota_on() for mount point.

 *		Fixed a few bugs in grow_dquots().

 *		Fixed deadlock in write_dquot() - we no longer account quotas on

 *		quota files

 *		remove_dquot_ref() moved to inode.c - it now traverses through inodes

 *		add_dquot_ref() restarts after blocking

 *		Added check for bogus uid and fixed check for group in quotactl.

 *		Jan Kara, <jack@suse.cz>, sponsored by SuSE CR, 10-11/99

 *

 *		Used struct list_head instead of own list struct

 *		Invalidation of referenced dquots is no longer possible

 *		Improved free_dquots list management

 *		Quota and i_blocks are now updated in one place to avoid races

 *		Warnings are now delayed so we won't block in critical section

 *		Write updated not to require dquot lock

 *		Jan Kara, <jack@suse.cz>, 9/2000

 *

 *		Added dynamic quota structure allocation

 *		Jan Kara <jack@suse.cz> 12/2000

 *

 *		Rewritten quota interface. Implemented new quota format and

 *		formats registering.

 *		Jan Kara, <jack@suse.cz>, 2001,2002

 *

 *		New SMP locking.

 *		Jan Kara, <jack@suse.cz>, 10/2002

 *

 *		Added journalled quota support, fix lock inversion problems

 *		Jan Kara, <jack@suse.cz>, 2003,2004

 *

 * (C) Copyright 1994 - 1997 Marco van Wieringen

 ugh */

/*

 * There are five quota SMP locks:

 * * dq_list_lock protects all lists with quotas and quota formats.

 * * dquot->dq_dqb_lock protects data from dq_dqb

 * * inode->i_lock protects inode->i_blocks, i_bytes and also guards

 *   consistency of dquot->dq_dqb with inode->i_blocks, i_bytes so that

 *   dquot_transfer() can stabilize amount it transfers

 * * dq_data_lock protects mem_dqinfo structures and modifications of dquot

 *   pointers in the inode

 * * dq_state_lock protects modifications of quota state (on quotaon and

 *   quotaoff) and readers who care about latest values take it as well.

 *

 * The spinlock ordering is hence:

 *   dq_data_lock > dq_list_lock > i_lock > dquot->dq_dqb_lock,

 *   dq_list_lock > dq_state_lock

 *

 * Note that some things (eg. sb pointer, type, id) doesn't change during

 * the life of the dquot structure and so needn't to be protected by a lock

 *

 * Operation accessing dquots via inode pointers are protected by dquot_srcu.

 * Operation of reading pointer needs srcu_read_lock(&dquot_srcu), and

 * synchronize_srcu(&dquot_srcu) is called after clearing pointers from

 * inode and before dropping dquot references to avoid use of dquots after

 * they are freed. dq_data_lock is used to serialize the pointer setting and

 * clearing operations.

 * Special care needs to be taken about S_NOQUOTA inode flag (marking that

 * inode is a quota file). Functions adding pointers from inode to dquots have

 * to check this flag under dq_data_lock and then (if S_NOQUOTA is not set) they

 * have to do all pointer modifications before dropping dq_data_lock. This makes

 * sure they cannot race with quotaon which first sets S_NOQUOTA flag and

 * then drops all pointers to dquots from an inode.

 *

 * Each dquot has its dq_lock mutex.  Dquot is locked when it is being read to

 * memory (or space for it is being allocated) on the first dqget(), when it is

 * being written out, and when it is being released on the last dqput(). The

 * allocation and release operations are serialized by the dq_lock and by

 * checking the use count in dquot_release().

 *

 * Lock ordering (including related VFS locks) is the following:

 *   s_umount > i_mutex > journal_lock > dquot->dq_lock > dqio_sem

 List of registered formats */

 SLAB cache for dquot structures */

/*

 * Dquot List Management:

 * The quota code uses four lists for dquot management: the inuse_list,

 * free_dquots, dqi_dirty_list, and dquot_hash[] array. A single dquot

 * structure may be on some of those lists, depending on its current state.

 *

 * All dquots are placed to the end of inuse_list when first created, and this

 * list is used for invalidate operation, which must look at every dquot.

 *

 * Unused dquots (dq_count == 0) are added to the free_dquots list when freed,

 * and this list is searched whenever we need an available dquot.  Dquots are

 * removed from the list as soon as they are used again, and

 * dqstats.free_dquots gives the number of dquots on the list. When

 * dquot is invalidated it's completely released from memory.

 *

 * Dirty dquots are added to the dqi_dirty_list of quota_info when mark

 * dirtied, and this list is searched when writing dirty dquots back to

 * quota file. Note that some filesystems do dirty dquot tracking on their

 * own (e.g. in a journal) and thus don't use dqi_dirty_list.

 *

 * Dquots with a specific identity (device, type and id) are placed on

 * one of the dquot_hash[] hash chains. The provides an efficient search

 * mechanism to locate a specific dquot.

/*

 * Following list functions expect dq_list_lock to be held

 Add a dquot to the tail of the free list */

	/* We add to the back of inuse list so we don't have to restart

/*

 * End of list functions needing dq_list_lock

 Mark dquot dirty in atomic manner, and return it's old dirty flag state */

 If quota is dirty already, we don't have to acquire dq_list_lock */

 Dirtify all the dquots - this can block when journalling */

 Even in case of error we have to continue */

/*

 *	Read dquot from disk and alloc space for it

 Make sure flags update is visible after dquot has been filled */

 Instantiate dquot if needed */

 Write the info if needed */

	/*

	 * Make sure flags update is visible after on-disk struct has been

	 * allocated. Paired with smp_rmb() in dqget().

/*

 *	Write dquot to disk

	/* Inactive dquot can be only if there was error during read/init

/*

 *	Release dquot

 Check whether we are not racing with some other dqget() */

 Write the info */

/* Invalidate all dquots on the list. Note that this function is called after

 * quota is disabled and pointers from inodes removed so there cannot be new

 * quota users. There can still be some users of quotas due to inodes being

 * just deleted or pruned by prune_icache() (those are not attached to any

 * list) or parallel quotactl call. We have to wait for such users.

 Wait for dquot users */

			/*

			 * Once dqput() wakes us up, we know it's time to free

			 * the dquot.

			 * IMPORTANT: we rely on the fact that there is always

			 * at most one process waiting for dquot to free.

			 * Otherwise dq_count would be > 1 and we would never

			 * wake up.

			/* At this moment dquot() need not exist (it could be

			 * reclaimed by prune_dqcache(). Hence we must

		/*

		 * Quota now has no users and it has been written on last

		 * dqput()

 Call callback for every active dquot on given filesystem */

 Now we have active dquot so we can just increase use count */

		/*

		 * ->release_dquot() can be racing with us. Our reference

		 * protects us from new calls to it so just wait for any

		 * outstanding call and recheck the DQ_ACTIVE_B after that.

		/* We are safe to continue now because our dquot could not

 Write all dquot structures to quota files */

 Move list away to avoid livelock. */

			/* Now we have active dquot from which someone is

 			 * holding reference so we can safely just increase

				/*

				 * Clear dirty bit anyway to avoid infinite

				 * loop here.

 Write all dquot structures to disk and make them visible from userspace */

	/* This is not very clever (and fast) but currently I don't know about

	 * any other simple way of getting quota data to disk and we must get

	/*

	 * Now when everything is written we can discard the pagecache so

	 * that userspace sees the changes.

/*

 * Put reference to dquot

 We have more than one user... nothing to do */

 Releasing dquot during quotaoff phase? */

 Need to release dquot? */

 Commit dquot before releasing */

			/*

			 * We clear dirty bit anyway, so that we avoid

			 * infinite loop here

 sanity check */

/*

 * Get reference to dquot

 *

 * Locking is slightly tricky here. We are guarded from parallel quotaoff()

 * destroying our dquot by:

 *   a) checking for quota flags under dq_list_lock and

 *   b) getting a reference to dquot before we release dq_list_lock

 Try to wait for a moment... */

 all dquots go on the inuse_list */

 hash it first so it can be found */

	/* Wait for dq_lock - after this we know that either dquot_release() is

 Read the dquot / allocate space in quota file */

	/*

	 * Make sure following reads see filled structure - paired with

	 * smp_mb__before_atomic() in dquot_acquire().

 Has somebody invalidated entry under us? */

 This routine is guarded by s_umount semaphore */

		/*

		 * We hold a reference to 'inode' so it couldn't have been

		 * removed from s_inodes list while we dropped the

		 * s_inode_list_lock. We cannot iput the inode now as we can be

		 * holding the last reference and we cannot iput it under

		 * s_inode_list_lock. So we keep the reference and iput it

		 * later.

/*

 * Remove references to dquots from inode and add dquot to list for freeing

 * if we have the last reference to dquot

		/*

		 * The inode still has reference to dquot so it can't be in the

		 * free list

		/*

		 * Dquot is already in a list to put so we won't drop the last

		 * reference here.

/*

 * Free list of dquots

 * Dquots are removed from inodes and no new references can be got so we are

 * the only ones holding reference

 Remove dquot from the list so we won't have problems... */

		/*

		 *  We have to scan also I_NEW inodes because they can already

		 *  have quota pointer initialized. Luckily, we need to touch

		 *  only quota pointers and these have separate locking

		 *  (dq_data_lock).

 Gather all references from inodes and drop them */

 Print warning to user which exceeded quota */

/*

 * Write warnings to the console and send warning messages over netlink.

 *

 * Note that this function can call into tty and networking code.

			/*

			 * We don't allow preallocation to exceed softlimit so exceeding will

			 * be always printed

	/*

	 * We have to be careful and go through warning generation & grace time

	 * setting even if DQUOT_SPACE_NOFAIL is set. That's why we check it

	 * only here...

/*

 * Initialize quota pointers in inode

 *

 * It is better to call this function outside of any transaction as it

 * might need a lot of space in journal for dquot structure allocation.

 First get references to structures we might need. */

		/*

		 * The i_dquot should have been initialized in most cases,

		 * we check it without locking here to avoid unnecessary

		 * dqget()/dqput() calls.

 We raced with somebody turning quotas off... */

 All required i_dquot has been initialized */

 Avoid races with quotaoff() */

 We could race with quotaon or dqget() could have failed */

			/*

			 * Make quota reservation system happy if someone

			 * did a write before quota was turned on

 Get reservation again under proper lock */

 Drop unused references */

/*

 * Release all quotas referenced by inode.

 *

 * This function only be called on inode free or converting

 * a file to quota file, no other users for the i_dquot in

 * both cases, so we needn't call synchronize_srcu() after

 * clearing i_dquot.

	/*

	 * Test before calling to rule out calls from proc and such

	 * where we are not allowed to block. Note that this is

	 * actually reliable test even without the lock - the caller

	 * must assure that nobody can come after the DQUOT_DROP and

	 * add quota pointers back anyway.

/*

 * inode_reserved_space is managed internally by quota, and protected by

 * i_lock similar to i_blocks+i_bytes.

	/* Filesystem must explicitly define it's own method in order to use

/*

 * This functions updates i_blocks+i_bytes fields and quota information

 * (together with appropriate checks).

 *

 * NOTE: We absolutely rely on the fact that caller dirties the inode

 * (usually helpers in quotaops.h care about this) and holds a handle for

 * the current transaction so that dquot write and inode write go into the

 * same transaction.

/*

 * This operation can block, but only after everything is updated

 Back out changes we already did */

/*

 * This operation can block, but only after everything is updated

 Back out changes we already did */

/*

 * Convert in-memory reserved quotas to real consumed quotas

 Claim reserved quotas to allocated quotas */

 Update inode bytes */

/*

 * Convert allocated space back to in-memory reserved quotas

 Claim reserved quotas to allocated quotas */

 Update inode bytes */

/*

 * This operation can block, but only after everything is updated

/*

 * This operation can block, but only after everything is updated

/*

 * Transfer the number of inode and blocks from one diskquota to an other.

 * On success, dquot references in transfer_to are consumed and references

 * to original dquots that need to be released are placed there. On failure,

 * references are kept untouched.

 *

 * This operation can block, but only after everything is updated

 * A transaction must be started when entering this function.

 *

 * We are holding reference on transfer_from & transfer_to, no need to

 * protect them by srcu_read_lock().

 Initialize the arrays */

 File without quota accounting? */

	/*

	 * Build the transfer_from list, check limits, and update usage in

	 * the target structures.

		/*

		 * Skip changes for same uid or gid or for turned off quota-type.

 Avoid races with quotaoff() */

 Decrease usage for source structures and update quota pointers */

 Due to IO error we might not have transfer_from[] structure */

 Pass back references to put */

 Back out changes we already did */

/* Wrapper for transferring ownership of an inode for uid/gid only

 * Called from FSXXX_setattr()

/*

 * Write info of quota file to disk

/*

 * Definitions of diskquota operations.

/*

 * Generic helper for ->open on filesystems supporting disk quotas.

/*

 * Turn quota off on a device. type == -1 ==> quotaoff for all types (umount)

 s_umount should be held in exclusive mode */

	/* Cannot turn off usage accounting without turning off limits, or

	/*

	 * Skip everything if there's nothing to do. We have to do this because

	 * sometimes we are called when fill_super() failed and calling

	 * sync_fs() in such cases does no good.

 Turning off suspended quotas? */

 We still have to keep quota loaded? */

 Note: these are blocking operations */

		/*

		 * Now all dquots should be invalidated, all writes done so we

		 * should be only users of the info. No locks needed.

 Skip syncing and setting flags if quota files are hidden */

	/* Sync the superblock so that buffers with quota data are written to

	/* Now the quota files are just ordinary files and we can set the

	 * inode flags back. Moreover we discard the pagecache so that

	 * userspace sees the writes we did bypassing the pagecache. We

	 * must also discard the blockdev buffers so that we see the

 We are done when suspending quotas */

/*

 *	Turn quotas on on a device

		/* We don't want quota and atime on quota files (deadlocks

		 * possible) Also nobody should write to the file - we use

		/*

		 * When S_NOQUOTA is set, remove dquot references as no more

		 * references can be added

 Just unsuspend quotas? */

 s_umount should be held in exclusive mode */

 Filesystems outside of init_user_ns not yet supported */

 Usage always has to be set... */

		/* As we bypass the pagecache we must now flush all the

		 * dirty data and invalidate caches so that kernel sees

		 * changes from userspace. It is not enough to just flush

		 * the quota file since if blocksize < pagesize, invalidation

		 * of the cache could fail because of other unrelated dirty

/*

 * More powerful function for turning on quotas on given quota inode allowing

 * setting of individual quota flags

 Reenable quotas on remount RW */

 s_umount should be held in exclusive mode */

 Quota file not on the same filesystem? */

/*

 * This function is used when filesystem needs to initialize quotas

 * during mount time.

 Accounting cannot be turned on while fs is mounted */

 Can't enforce without accounting */

 Backout enforcement enablement we already did */

 Error code translation for better compatibility with XFS */

	/*

	 * We don't support turning off accounting via quotactl. In principle

	 * quota infrastructure can do this but filesystems don't expect

	 * userspace to be able to do it.

 Filter out limits not enabled */

 Nothing left? */

 Backout enforcement disabling we already did */

 Generic routine for getting common part of quota structure */

 Generic routine for setting common part of quota structure */

 Set grace only if user hasn't provided his own... */

 Set grace only if user hasn't provided his own... */

 Generic routine for getting common part of quota file information */

 We don't know... */

 Generic routine for setting common part of quota file information */

 Force write to disk */

 Filter negative values for non-monotonic counters */

 Update global table */

 Find power-of-two hlist_heads which can fit into allocation */

 SPDX-License-Identifier: GPL-2.0-only

 Set structure to 0s in case read fails/is after end of file */

 Magics of new quota format */

 USRQUOTA */\

 GRPQUOTA */\

 Header of new quota format */

 Magic number identifying file */

 File version */

	/* Doublecheck whether we didn't get file with new format - with old

 Probably not new format */

 Definitely not new format */

 Seems like a new format file -> refuse it */

 limits are stored as unsigned 32-bit data */

 SPDX-License-Identifier: GPL-2.0

/*

 * Quota code necessary even when VFS quota support is not compiled

 * into the kernel.  The interesting stuff is over in dquot.c, here

 * we have symbols for initial quotactl(2) handling, the sysctl(2)

 * variables, etc - things needed even when quota support disabled.

 these commands do not require any special privilegues */

 allow to query information for dquots we "own" */

/*

 * Return quota for next active quota >= this id, if any exists,

 * otherwise return -ENOENT via ->get_nextdqblk

 struct if_nextdqblk is a superset of struct if_dqblk */

 No quota enabled? */

 Inodes may be allocated even if inactive; copy out if present */

		/*

		 * Q_XGETQSTAT doesn't have room for both group and project

		 * quotas.  So, allow the project quota values to be copied out

		 * only if there is no group quota information available.

 No quota enabled? */

 Inodes may be allocated even if inactive; copy out if present */

 Just read qs_version */

 If this kernel doesn't support user specified version, fail */

/*

 * XFS defines BBTOB and BTOBB macros inside fs/xfs/ and we cannot move them

 * out of there as xfsprogs rely on definitions being in that header file. So

 * just define same functions here for quota purposes.

 Are we actually setting timer / warning limits for all users? */

 These are already done */

/*

 * Return quota for next active quota >= this id, if any exists,

 * otherwise return -ENOENT via ->get_nextdqblk.

 Copy parameters and call proper function */

	/*

	 * Quota not supported on this fs? Check this before s_quota_types

	 * since they needn't be set if quota is not supported at all.

 XFS quotas are fully coherent now, making this call a noop */

 Return 1 if 'cmd' will block on frozen filesystem */

	/*

	 * We cannot allow Q_GETQUOTA and Q_GETNEXTQUOTA without write access

	 * as dquot_acquire() may allocate space for new structure and OCFS2

	 * needs to increment on-disk use count.

 Return true if quotactl command is manipulating quota on/off state */

/*

 * look up a superblock on which quota ops will be performed

 * - use the name of a block device to find the superblock thereon

/*

 * This is the system call interface. This communicates with

 * the user-level programs. Currently this only supports diskquota

 * calls. Maybe we need to add the process quotas etc. in the future,

 * but we probably should use rlimits for that.

	/*

	 * As a special case Q_SYNC can be called without a specific device.

	 * It will iterate all superblocks that have quota enabled and call

	 * the sync action on each of them.

	/*

	 * Path for quotaon has to be resolved before grabbing superblock

	 * because that gets s_umount sem which is also possibly needed by path

	 * resolution (think about autofs) and thus deadlocks could arise.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *	vfsv0 quota IO operations on file

 Number of entries in one blocks */

 Remove empty block from list and return it */

 Assure block allocation... */

 Insert empty block to the list */

 Remove given block from the list of blocks with free entries */

 No matter whether write succeeds block is out of list */

 Insert given block to the beginning of list with free entries */

 Is the entry in the block free? */

 Find space for dquot */

		/* This is enough as the block is already zeroed and the entry

 Block will be full? */

 Find free structure in block */

 Insert reference to structure into the trie */

 Wrapper for inserting quota structure into tree */

/*

 * We don't have to be afraid of deadlocks as we never have quotas on quota

 * files...

 dq_off is guarded by dqio_sem */

 Free dquot entry in data block */

 Block got free? */

 Insert will write block itself */

 Quota is now unattached */

 Remove reference to dquot from tree */

 Block got empty? */

 Don't put the root block into the free block list */

 Delete dquot from tree */

 Even not allocated? */

 Find entry in block */

 Find entry for given id in the tree */

 No reference? */

 Find entry for given id in the tree - wrapper function */

 Invalidated quota? */

 Do we know offset of the dquot entry in the quota file? */

 Entry not present? */

/* Check whether dquot should not be deleted. We know we are

 SPDX-License-Identifier: GPL-2.0

 Netlink family structure for quota */

/**

 * quota_send_warning - Send warning to userspace about exceeded quota

 * @qid: The kernel internal quota identifier.

 * @dev: The device on which the fs is mounted (sb->s_dev)

 * @warntype: The type of the warning: QUOTA_NL_...

 *

 * This can be used by filesystems (including those which don't use

 * dquot) to send a message to userspace relating to quota limits.

 *

	/* We have to allocate using GFP_NOFS as we are called from a

	 * filesystem performing write and thus further recursion into

 SPDX-License-Identifier: GPL-2.0-only

/*

 *	vfsv0 quota IO operations on file

 Check whether given file is really vfsv0 quotafile */

 Read information header from quota file */

 limits are stored as unsigned 32-bit data */

		/*

		 * Used space is stored as unsigned 64-bit value in bytes but

		 * quota core supports only signed 64-bit values so use that

		 * as a limit

 2^63-1 */

 No flags currently supported */

 Some sanity checks of the read headers... */

 Write information header to quota file */

 No flags currently supported */

 We need to escape back all-zero structure */

 We need to escape back all-zero structure */

	/*

	 * If space for dquot is already allocated, we don't need any

	 * protection as we'll only overwrite the place of dquot. We are

	 * still protected by concurrent writes of the same dquot by

	 * dquot->dq_lock.

 SPDX-License-Identifier: GPL-2.0

/**

 *	qid_eq - Test to see if to kquid values are the same

 *	@left: A qid value

 *	@right: Another quid value

 *

 *	Return true if the two qid values are equal and false otherwise.

/**

 *	qid_lt - Test to see if one qid value is less than another

 *	@left: The possibly lesser qid value

 *	@right: The possibly greater qid value

 *

 *	Return true if left is less than right and false otherwise.

/**

 *	from_kqid - Create a qid from a kqid user-namespace pair.

 *	@targ: The user namespace we want a qid in.

 *	@kqid: The kernel internal quota identifier to start with.

 *

 *	Map @kqid into the user-namespace specified by @targ and

 *	return the resulting qid.

 *

 *	There is always a mapping into the initial user_namespace.

 *

 *	If @kqid has no mapping in @targ (qid_t)-1 is returned.

/**

 *	from_kqid_munged - Create a qid from a kqid user-namespace pair.

 *	@targ: The user namespace we want a qid in.

 *	@kqid: The kernel internal quota identifier to start with.

 *

 *	Map @kqid into the user-namespace specified by @targ and

 *	return the resulting qid.

 *

 *	There is always a mapping into the initial user_namespace.

 *

 *	Unlike from_kqid from_kqid_munged never fails and always

 *	returns a valid projid.  This makes from_kqid_munged

 *	appropriate for use in places where failing to provide

 *	a qid_t is not a good option.

 *

 *	If @kqid has no mapping in @targ the kqid.type specific

 *	overflow identifier is returned.

/**

 *	qid_valid - Report if a valid value is stored in a kqid.

 *	@qid: The kernel internal quota identifier to test.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/ufs/namei.c

 *

 * Migration to usage of "page cache" on May 2006 by

 * Evgeniy Dushistov <dushistov@mail.ru> based on ext2 code base.

 *

 * Copyright (C) 1998

 * Daniel Pirkl <daniel.pirkl@email.cz>

 * Charles University, Faculty of Mathematics and Physics

 *

 *  from

 *

 *  linux/fs/ext2/namei.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/namei.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

/*

 * By the time this is called, we already have created

 * the directory cache entry for the new file, but it

 * is so far negative - it has no inode.

 *

 * If the create succeeds, we fill in the inode information

 * with d_instantiate(). 

 slow symlink */

 fast symlink */

	/*

	 * Like most other Unix systems, set the ctime for inodes on a

 	 * rename.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/fs/ufs/super.c

 *

 * Copyright (C) 1998

 * Daniel Pirkl <daniel.pirkl@email.cz>

 * Charles University, Faculty of Mathematics and Physics

/* Derived from

 *

 *  linux/fs/ext2/super.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/inode.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

/*

 * Inspired by

 *

 *  linux/fs/ufs/super.c

 *

 * Copyright (C) 1996

 * Adrian Rodriguez (adrian@franklins-tower.rutgers.edu)

 * Laboratory for Computer Science Research Computing Facility

 * Rutgers, The State University of New Jersey

 *

 * Copyright (C) 1996  Eddie C. Dost  (ecd@skynet.be)

 *

 * Kernel module support added on 96/04/26 by

 * Stefan Reinauer <stepan@home.culture.mipt.ru>

 *

 * Module usage counts added on 96/04/29 by

 * Gertjan van Wingerde <gwingerde@gmail.com>

 *

 * Clean swab support on 19970406 by

 * Francois-Rene Rideau <fare@tunes.org>

 *

 * 4.4BSD (FreeBSD) support added on February 1st 1998 by

 * Niels Kristian Bech Jensen <nkbj@image.dk> partially based

 * on code by Martin von Loewis <martin@mira.isdn.cs.tu-berlin.de>.

 *

 * NeXTstep support added on February 5th 1998 by

 * Niels Kristian Bech Jensen <nkbj@image.dk>.

 *

 * write support Daniel Pirkl <daniel.pirkl@email.cz> 1998

 * 

 * HP/UX hfs filesystem support added by

 * Martin K. Petersen <mkp@mkp.net>, August 1999

 *

 * UFS2 (of FreeBSD 5.x) support added by

 * Niraj Kumar <niraj17@iitbombay.org>, Jan 2004

 *

 * UFS2 write support added by

 * Evgeniy Dushistov <dushistov@mail.ru>, 2007

/*

 * Print contents of ufs_super_block, useful for debugging

/*

 * Print contents of ufs_cylinder_group, useful for debugging

*/

*/

 CONFIG_UFS_DEBUG */

end of possible ufs types */

/*

 * Different types of UFS hold fs_cstotal in different

 * places, and use different data structure for it.

 * To make things simpler we just copy fs_cstotal to ufs_sb_private_info

we have statistic in different place, then usual*/

/*

 * Read on-disk structures associated with cylinder groups

	/*

	 * Read cs structures from (usually) first data block

	 * on the device. 

	/*

	 * Read cylinder group (we read only first fragment from block

	 * at this time) and prepare internal data structures for cg caching.

/*

 * Sync our internal copy of fs_cstotal with disk

we have statistic in different place, then usual*/

 store stats in both old and new places */

/**

 * ufs_put_super_internal() - put on-disk intrenal structures

 * @sb: pointer to super_block structure

 * Put on-disk structures associated with cylinder groups

 * and write them back to disk, also update cs_total on disk

	/*

	 * Set default mount options

	 * Parse mount options

 Not supported on disk */

 Not supported on disk */

 Not supported on disk */

	/*

	 * read ufs super block from device

 Sort out mod used on SunOS 4.1.3 for fs_state */

	/*

	 * Check ufs magic number

	/*

	 * Check block and fragment sizes

after that line some functions use s_flags*/

	/*

	 * Check, if file system was correctly unmounted.

	 * If not, make it read only.

	/*

	 * Read ufs_super_block into internal data structures

 s_bsize already set */

 s_fsize already set */

 s_sbsize already set */

	/*

	 * Compute another frequently used values

	/*

	 * Read cylinder group structures

	/*

	 * Allow the "check" option to be passed as a remount option.

	 * It is not possible to change ufstype option during remount

	/*

	 * fs was mouted as rw, remounting ro

	/*

	 * fs was mounted as ro, remounting rw

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ufs/ialloc.c

 *

 * Copyright (c) 1998

 * Daniel Pirkl <daniel.pirkl@email.cz>

 * Charles University, Faculty of Mathematics and Physics

 *

 *  from

 *

 *  linux/fs/ext2/ialloc.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  BSD ufs-inspired inode and directory allocation by 

 *  Stephen Tweedie (sct@dcs.ed.ac.uk), 1993

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

 *

 * UFS2 write support added by

 * Evgeniy Dushistov <dushistov@mail.ru>, 2007

/*

 * NOTE! When we get the inode, we're the only people

 * that have access to it, and as such there are no

 * race conditions we have to worry about. The inode

 * is not on the hash-lists, and it cannot be reached

 * through the filesystem because the directory entry

 * has been deleted earlier.

 *

 * HOWEVER: we must make sure that we get no aliases,

 * which means that we have to call "clear_inode()"

 * _before_ we mark the inode not in use in the inode

 * bitmaps. Otherwise a newly created file might use

 * the same inode number (not actually the same pointer

 * though), and then we'd have two inodes sharing the

 * same inode number and space on the harddisk.

/*

 * Nullify new chunk of inodes,

 * BSD people also set ui_gen field of inode

 * during nullification, but we not care about

 * that because of linux ufs do not support NFS

/*

 * There are two policies for allocating an inode.  If the new inode is

 * a directory, then a forward search is made for a block group with both

 * free space and a low directory-to-inode ratio; if that fails, then of

 * the groups with above-average free space, that group with the fewest

 * directories already is chosen.

 *

 * For other inodes, search forward from the parent directory's block

 * group to find a free inode.

 Cannot create files in a deleted directory */

	/*

	 * Try to place the inode in its parent directory

	/*

	 * Use a quadratic hash to find a group with a free inode

	/*

	 * That failed: try linear search for a free inode

		/*

		 * setup birth date, we do it here because of there is no sense

		 * to hold it in struct ufs_inode_info, and lose 64 bit

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ufs/inode.c

 *

 * Copyright (C) 1998

 * Daniel Pirkl <daniel.pirkl@email.cz>

 * Charles University, Faculty of Mathematics and Physics

 *

 *  from

 *

 *  linux/fs/ext2/inode.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/inode.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  Goal-directed block allocation by Stephen Tweedie (sct@dcs.ed.ac.uk), 1993

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

/*

 * Returns the location of the fragment from

 * the beginning of the filesystem.

/*

 * Unpacking tails: we have a file with partial final block and

 * we had been asked to extend it.  If the fragment being written

 * is within the same block, we need to extend the tail just to cover

 * that fragment.  Otherwise the tail is extended to full block.

 *

 * Note that we might need to create a _new_ tail, but that will

 * be handled elsewhere; this is strictly for resizing old

 * ones.

 it's a short file, so unsigned is enough */

/**

 * ufs_inode_getfrag() - allocate new fragment(s)

 * @inode: pointer to inode

 * @index: number of block pointer within the inode's array.

 * @new_fragment: number of new allocated fragment(s)

 * @err: we set it if something wrong

 * @new: we set it if we allocate new block

 * @locked_page: for ufs_new_fragments()

        /* TODO : to be done for write support

        if ( (flags & UFS_TYPE_MASK) == UFS_TYPE_UFS2)

             goto ufs2;

 will that be a new tail? */

     /* This part : To be implemented ....

        Required only for writing, not required for READ-ONLY.

ufs2:



	u2_block = ufs_fragstoblks(fragment);

	u2_blockoff = ufs_fragnum(fragment);

	p = ufsi->i_u1.u2_i_data + block;

	goal = 0;



repeat2:

	tmp = fs32_to_cpu(sb, *p);

	lastfrag = ufsi->i_lastfrag;



/**

 * ufs_inode_getblock() - allocate new block

 * @inode: pointer to inode

 * @ind_block: block number of the indirect block

 * @index: number of pointer within the indirect block

 * @new_fragment: number of new allocated fragment

 *  (block will hold this fragment and also uspi->s_fpb-1)

 * @err: see ufs_inode_getfrag()

 * @new: see ufs_inode_getfrag()

 * @locked_page: see ufs_inode_getfrag()

/**

 * ufs_getfrag_block() - `get_block_t' function, interface between UFS and

 * readpage, writepage and so on

 This code entered only while writing ....? */

	/*

	 * Copy data to the in-core inode.

	/*

	 * Linux now has 32-bit uid and gid, so we can support EFT.

	/*

	 * Copy data to the in-core inode.

        /*

         * Linux now has 32-bit uid and gid, so we can support EFT.

	/*

	ufsi->i_shadow = fs32_to_cpu(sb, ufs_inode->ui_u3.ui_sun.ui_shadow);

	ufsi->i_oeftflag = fs32_to_cpu(sb, ufs_inode->ui_u3.ui_sun.ui_oeftflag);

 ufs_inode->ui_u2.ui_addr.ui_db[0] = cpu_to_fs32(sb, inode->i_rdev); */

 ufs_inode->ui_u2.ui_addr.ui_db[0] = cpu_to_fs32(sb, inode->i_rdev); */

	/*

	 * Free first free fragments

	/*

	 * Free whole blocks

	/*

	 * Free last free fragments

	       /*

		* we do not zeroize fragment, because of

		* if it maped to hole, it already contains zeroes

 get the blocks that should be partially emptied */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ufs/ufs_dir.c

 *

 * Copyright (C) 1996

 * Adrian Rodriguez (adrian@franklins-tower.rutgers.edu)

 * Laboratory for Computer Science Research Computing Facility

 * Rutgers, The State University of New Jersey

 *

 * swab support by Francois-Rene Rideau <fare@tunes.org> 19970406

 *

 * 4.4BSD (FreeBSD) support added on February 1st 1998 by

 * Niels Kristian Bech Jensen <nkbj@image.dk> partially based

 * on code by Martin von Loewis <martin@mira.isdn.cs.tu-berlin.de>.

 *

 * Migration to usage of "page cache" on May 2006 by

 * Evgeniy Dushistov <dushistov@mail.ru> based on ext2 code base.

/*

 * NOTE! unlike strncmp, ufs_match returns 1 for success, 0 for failure.

 *

 * len <= UFS_MAXNAMLEN and de != NULL are guaranteed by caller.

 Releases the page */

 Too bad, we had an error */

/*

 * Return the offset into page `page_nr' of the last valid

 * byte in that page, plus one.

/*

 *	ufs_find_entry()

 *

 * finds an entry in the specified directory with the wanted name. It

 * returns the page in which the entry was found, and the entry itself

 * (as a parameter - res_dir). Page is returned mapped and unlocked.

 * Entry is guaranteed to be valid.

 OFFSET_CACHE */

/*

 *	Parent is locked.

	/*

	 * We take care of directory expansion in the same loop.

	 * This code plays outside i_size, so it locks the page

	 * to protect that region.

 We hit i_size */

 OFFSET_CACHE */

/*

 * This is blatantly stolen from ext2fs

/*

 * ufs_delete_entry deletes a directory entry by merging it with the

 * previous entry.

/*

 * routine to check that the specified directory is empty (for rmdir)

 check for . and .. */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ufs/balloc.c

 *

 * Copyright (C) 1998

 * Daniel Pirkl <daniel.pirkl@email.cz>

 * Charles University, Faculty of Mathematics and Physics

 *

 * UFS2 write support Evgeniy Dushistov <dushistov@mail.ru>, 2007

/*

 * Free 'count' fragments from fragment number 'fragment'

	/*

	 * Trying to reassemble free fragments into block

/*

 * Free 'count' fragments from fragment number 'fragment' (free whole blocks)

/*

 * Modify inode page cache in such way:

 * have - blocks with b_blocknr equal to oldb...oldb+count-1

 * get - blocks with b_blocknr equal to newb...newb+count-1

 * also we suppose that oldb...oldb+count-1 blocks

 * situated at the end of file.

 *

 * We can come here from ufs_writepage or ufs_prepare_write,

 * locked_page is argument of these functions, so we already lock it.

 it was truncated */

 or EIO */

	/*

	 * Somebody else has just allocated our fragments

	/*

	 * There is not enough space for user on the device

	/*

	 * allocate new fragment

	/*

	 * resize block

	/*

	 * allocate new block and move data

	/*

	 * Block can be extended

	/*

	 * 1. searching on preferred cylinder group

	/*

	 * 2. quadratic rehash

	/*

	 * 3. brute force search

	 * We start at i = 2 ( 0 is checked at 1.step, 1 at 2.step )

	/*

	 * If the requested block is available, use it.

/*

 * Find a block of the specified size in the specified cylinder group.

 * @sp: pointer to super block

 * @ucpi: pointer to cylinder group info

 * @goal: near which block we want find new one

 * @count: specified size

	/*

	 * Bit patterns for identifying fragments in the block map

	 * used as ((map & mask_arr) == want_arr)

	/*

	 * found the byte in the map

	/*

	 * Find the size of the cluster going forward.

	/*

	 * Find the size of the cluster going backward.

	/*

	 * Account for old cluster and the possibly new forward and

	 * back clusters.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ufs/cylinder.c

 *

 * Copyright (C) 1998

 * Daniel Pirkl <daniel.pirkl@email.cz>

 * Charles University, Faculty of Mathematics and Physics

 *

 *  ext2 - inode (block) bitmap caching inspired

/*

 * Read cylinder group into cache. The memory space for ufs_cg_private_info

 * structure is already allocated during ufs_read_super.

	/*

	 * We have already the first fragment of cylinder group block in buffer

/*

 * Remove cylinder group from cache, doesn't release memory

 * allocated for cylinder group (this is done at ufs_put_super only).

	/*

	 * rotor is not so important data, so we put it to disk 

	 * at the end of working with cylinder

/*

 * Find cylinder group in cache and return it as pointer.

 * If cylinder group is not in cache, we will load it from disk.

 *

 * The cache is managed by LRU algorithm. 

	/*

	 * Cylinder group number cg it in cache and it was last used

	/*

	 * Number of cylinder groups is not higher than UFS_MAX_GROUP_LOADED

	/*

	 * Cylinder group number cg is in cache but it was not last used, 

	 * we will move to the first position

	/*

	 * Cylinder group number cg is not in cache, we will read it from disk

	 * and put it to the first position

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ufs/file.c

 *

 * Copyright (C) 1998

 * Daniel Pirkl <daniel.pirkl@email.cz>

 * Charles University, Faculty of Mathematics and Physics

 *

 *  from

 *

 *  linux/fs/ext2/file.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/file.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  ext2 fs regular file handling primitives

/*

 * We have mostly NULL's here: the current defaults are ok for

 * the ufs filesystem.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ufs/util.c

 *

 * Copyright (C) 1998

 * Daniel Pirkl <daniel.pirkl@email.cz>

 * Charles University, Faculty of Mathematics and Physics

/**

 * ufs_get_locked_page() - locate, pin and lock a pagecache page, if not exist

 * read it from disk.

 * @mapping: the address_space to search

 * @index: the page index

 *

 * Locates the desired pagecache page, if not exist we'll read it,

 * locks it, increments its reference

 * count and returns its address.

 *

 Truncate got there first */

/*

 * Compressed rom filesystem for Linux.

 *

 * Copyright (C) 1999 Linus Torvalds.

 *

 * This file is released under the GPL.

/*

 * These are the VFS interfaces to the compressed rom filesystem.

 * The actual compression is based on zlib, see the other files.

/*

 * cramfs super-block data in memory

 These macros may change in future, to provide better st_ino semantics. */

	/*

	 * The file mode test fixes buggy mkcramfs implementations where

	 * cramfs_inode->offset is set to a non zero value for entries

	 * which did not contain data, like devices node and fifos.

 if the lower 2 bits are zero, the inode contains data */

 Struct copy intentional */

	/* inode->i_nlink is left 1 - arguably wrong for directories,

	   but it's the best we can do without reading the directory

	   contents.  1 yields the right result in GNU find, even

/*

 * We have our own block cache: don't fill up the buffer cache

 * with the rom-image, because the way the filesystem is set

 * up the accesses should be fairly regular and cached in the

 * page cache and dentry tree anyway..

 *

 * This also acts as a way to guarantee contiguous areas of up to

 * BLKS_PER_BUF*PAGE_SIZE, so that the caller doesn't need to

 * worry about end-of-buffer issues even when decompressing a full

 * page cache.

 *

 * Note: This is all optimized away at compile time when

 *       CONFIG_CRAMFS_BLOCKDEV=n.

 NEXT_BUFFER(): Loop over [0..(READ_BUFFERS-1)]. */

/*

 * BLKS_PER_BUF_SHIFT should be at least 2 to allow for "compressed"

 * data that takes up more space than the original and with unlucky

 * alignment.

/*

 * Populate our block cache and return a pointer to it.

 Check if an existing buffer already has the data.. */

 Ok, read in BLKS_PER_BUF pages completely first. */

 synchronous error? */

 asynchronous error */

/*

 * Return a pointer to the linearly addressed cramfs image in memory.

/*

 * Returns a pointer to a buffer containing at least LEN bytes of

 * filesystem starting at byte offset OFFSET into the filesystem.

/*

 * For a mapping to be possible, we need a range of uncompressed and

 * contiguous blocks. Return the offset for the first block and number of

 * valid blocks for which that is true, or zero otherwise.

	/*

	 * We can dereference memory directly here as this code may be

	 * reached only when there is a direct filesystem image mapping

	 * available in memory.

/*

 * Return true if the last page of a file in the filesystem image contains

 * some other data that doesn't belong to that file. It is assumed that the

 * last block is CRAMFS_BLK_FLAG_DIRECT_PTR | CRAMFS_BLK_FLAG_UNCOMPRESSED

 * (verified by cramfs_get_block_range() and directly accessible in memory.

	/*

	 * Now try to pre-populate ptes for this vma with a direct

	 * mapping avoiding memory allocation when possible.

 Could COW work here? */

 Don't map the last page if it contains some other data */

		/*

		 * The entire vma is mappable. remap_pfn_range() will

		 * make it distinguishable from a non-direct mapping

		 * in /proc/<pid>/maps by substituting the file offset

		 * with the actual physical address.

		/*

		 * Let's create a mixed map if we can't map it all.

		 * The normal paging machinery will take care of the

		 * unpopulated ptes via cramfs_readpage().

 Didn't manage any direct map, but normal paging is still possible */

 CONFIG_MMU */

 CONFIG_MMU */

 We don't know the real size yet */

 Read the first block and get the superblock from it */

 Do sanity checks on the superblock */

 check for wrong endianness */

 check at 512 byte offset */

 get feature flags first */

 Check that the root inode is in a sane state */

 correct strange, hard-coded permissions of mkcramfs */

 Set it all up.. */

 Invalidate the read buffers on mount: think disk change.. */

 Map only one page for now.  Will remap it when fs size is known. */

 Remap the whole filesystem now */

/*

 * Read a cramfs directory entry.

 Offset within the thing. */

 Directory entries are always 4-byte aligned */

		/*

		 * Namelengths on disk are shifted by two

		 * and the name padded out to 4-byte boundaries

		 * with zeroes.

/*

 * Lookup and fill in the inode data..

 Try to take advantage of sorted directories */

 Quick check that the name is roughly the right length */

 else (retval < 0) */

			/*

			 * The block pointer is an absolute start pointer,

			 * shifted by 2 bits. The size is included in the

			 * first 2 bytes of the data block when compressed,

			 * or PAGE_SIZE otherwise.

 if last block: cap to file length */

			/*

			 * The block pointer indicates one past the end of

			 * the current block (start of next block). If this

			 * is the first block then it starts where the block

			 * pointer table ends, otherwise its start comes

			 * from the previous block's pointer.

 Beware... previous ptr might be a direct ptr */

 See comments on earlier code. */

 hole */

/*

 * Our operations:

/*

 * A directory can only readdir

/*

 * Set up the filesystem mount context.

 SPDX-License-Identifier: GPL-2.0

/*

 * uncompress.c

 *

 * (C) Copyright 1999 Linus Torvalds

 *

 * cramfs interfaces to the uncompression library. There's really just

 * three entrypoints:

 *

 *  - cramfs_uncompress_init() - called to initialize the thing.

 *  - cramfs_uncompress_exit() - tell me when you're done

 *  - cramfs_uncompress_block() - uncompress a block.

 *

 * NOTE NOTE NOTE! The uncompression is entirely single-threaded. We

 * only have one stream, and we'll initialize it only once even if it

 * then is used by multiple filesystems.

 Returns length of decompressed data. */

/* file-mmu.c: ramfs MMU-based file operations

 *

 * Resizable simple ram filesystem for Linux.

 *

 * Copyright (C) 2000 Linus Torvalds.

 *               2000 Transmeta Corp.

 *

 * Usage limits added by David Gibson, Linuxcare Australia.

 * This file is released under the GPL.

/*

 * NOTE! This filesystem is probably most useful

 * not as a real filesystem, but as an example of

 * how virtual filesystems can be written.

 *

 * It doesn't get much simpler than this. Consider

 * that this file implements the full semantics of

 * a POSIX-compliant read-write filesystem.

 *

 * Note in particular how the filesystem does not

 * need to implement any data structures of its own

 * to keep track of the virtual data: using the VFS

 * caches is sufficient.

/*

 * Resizable simple ram filesystem for Linux.

 *

 * Copyright (C) 2000 Linus Torvalds.

 *               2000 Transmeta Corp.

 *

 * Usage limits added by David Gibson, Linuxcare Australia.

 * This file is released under the GPL.

/*

 * NOTE! This filesystem is probably most useful

 * not as a real filesystem, but as an example of

 * how virtual filesystems can be written.

 *

 * It doesn't get much simpler than this. Consider

 * that this file implements the full semantics of

 * a POSIX-compliant read-write filesystem.

 *

 * Note in particular how the filesystem does not

 * need to implement any data structures of its own

 * to keep track of the virtual data: using the VFS

 * caches is sufficient.

 directory inodes start off with i_nlink == 2 (for "." entry) */

/*

 * File creation. Allocate an inode, and we're done..

 SMP-safe */

 Extra count - pin the dentry in core */

/*

 * Display the mount options in /proc/mounts.

		/*

		 * We might like to report bad mount options here;

		 * but traditionally ramfs has ignored all mount options,

		 * and as it is used as a !CONFIG_SHMEM simple substitute

		 * for tmpfs, better continue to ignore other mount options.

 SPDX-License-Identifier: GPL-2.0-or-later

/* file-nommu.c: no-MMU version of ramfs

 *

 * Copyright (C) 2005 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

****************************************************************************/

/*

 * add a contiguous set of pages into a ramfs inode when it's truncated from

 * size 0 on the assumption that it's going to be used for an mmap of shared

 * memory

 make various checks */

	/* allocate enough contiguous pages to be able to satisfy the

 split the high-order page into an array of single pages */

 trim off any pages we don't actually require */

 clear the memory we allocated */

 attach all the pages to the inode's address space */

 prevent the page from being discarded on memory pressure */

****************************************************************************/

/*

 *

	/* assume a truncate from zero size is going to be for the purposes of

 check that a decrease in size doesn't cut off any shared mappings */

****************************************************************************/

/*

 * handle a change of attributes

 * - we're specifically interested in a change of size

 POSIX UID/GID verification for setting inode attributes */

 pick out size-changing events */

			/* we skipped the truncate but must still update

			 * timestamps

****************************************************************************/

/*

 * try to determine where a shared mapping can be made

 * - we require that:

 *   - the pages to be mapped must exist

 *   - the pages be physically contiguous in sequence

 the mapping mustn't extend beyond the EOF */

 gang-find the pages */

 leave if some pages were missing */

 check the pages for physical adjacency */

 okay - all conditions fulfilled */

****************************************************************************/

/*

 * set up a mapping for shared memory segments

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2016-2021 Christoph Hellwig.

 skip holes */

 success */

 extent array full */

 error */

 inode with no (attribute) mapping will give ENOENT */

 legacy ->bmap interface.  0 is the error return (!) */

 leave iter.processed unset to abort loop */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2010 Red Hat, Inc.

 * Copyright (C) 2016-2019 Christoph Hellwig.

/*

 * Structure allocated for each page or THP when block size < page size

 * to track sub-page uptodate status and I/O completions.

	/*

	 * per-block data is stored in the head page.  Callers should

	 * not be dealing with tail pages, and if they are, they can

	 * call thp_head() first.

/*

 * Calculate the range inside the page that we actually need to read.

	/*

	 * If the block size is smaller than the page size, we need to check the

	 * per-block uptodate status and adjust the offset and length if needed

	 * to avoid reading in already uptodate ranges.

 move forward for each leading block marked uptodate */

 truncate len if we find any trailing uptodate block(s) */

	/*

	 * If the extent spans the block that contains the i_size, we need to

	 * handle both halves separately so that we properly zero data in the

	 * page cache for blocks that are entirely outside of i_size.

 zero post-eof blocks as the page may be mapped */

 same as readahead_gfp_mask */

		/*

		 * If the bio_alloc fails, try it again for a single page to

		 * avoid having to deal with partial page reads.  This emulates

		 * what do_mpage_readpage does.

	/*

	 * Move the caller beyond our range so that it keeps making progress.

	 * For that, we have to include any leading non-uptodate ranges, but

	 * we can skip trailing ones as they will be handled in the next

	 * iteration.

	/*

	 * Just like mpage_readahead and block_read_full_page, we always

	 * return 0 and just mark the page as PageError on errors.  This

	 * should be cleaned up throughout the stack eventually.

/**

 * iomap_readahead - Attempt to read pages from a file.

 * @rac: Describes the pages to be read.

 * @ops: The operations vector for the filesystem.

 *

 * This function is for filesystems to call to implement their readahead

 * address_space operation.

 *

 * Context: The @ops callbacks may submit I/O (eg to read the addresses of

 * blocks from disc), and may wait for it.  The caller may be trying to

 * access a different page, and so sleeping excessively should be avoided.

 * It may allocate memory, but should avoid costly allocations.  This

 * function is called with memalloc_nofs set, so allocations will not cause

 * the filesystem to be reentered.

/*

 * iomap_is_partially_uptodate checks whether blocks within a page are

 * uptodate or not.

 *

 * Returns true if all blocks which correspond to a file portion

 * we want to read within the page are uptodate.

 Limit range to one page */

 First and last blocks in range within page */

	/*

	 * mm accommodates an old ext3 case where clean pages might not have had

	 * the dirty bit cleared. Thus, it can send actual dirty pages to

	 * ->releasepage() via shrink_active_list(); skip those here.

	/*

	 * If we're invalidating the entire page, clear the dirty state from it

	 * and release it to avoid unnecessary buildup of the LRU.

 CONFIG_MIGRATION */

	/*

	 * Only truncate newly allocated pages beyoned EOF, even if the

	 * write started inside the existing inode size.

 needs more work for the tailpacking case; disable for now */

	/*

	 * The blocks that were entirely written will now be uptodate, so we

	 * don't have to worry about a readpage reading them and overwriting a

	 * partial write.  However, if we've encountered a short write and only

	 * partially written into a block, it will not be marked uptodate, so a

	 * readpage might come in and destroy our partial write.

	 *

	 * Do the simplest thing and just treat any short write to a

	 * non-uptodate page as a zero-length write, and force the caller to

	 * redo the whole thing.

 Returns the number of bytes copied.  May be 0.  Cannot be an errno. */

	/*

	 * Update the in-memory inode size after copying the data into the page

	 * cache.  It's up to the file system to write the updated size to disk,

	 * preferably after I/O completion so that no stale data is exposed.

 Offset into pagecache page */

 Bytes to write to page */

 Bytes copied from user */

		/*

		 * Bring in the user page that we'll copy from _first_.

		 * Otherwise there's a nasty deadlock on copying from the

		 * same page as we're writing to, without it being marked

		 * up-to-date.

			/*

			 * A short copy made iomap_write_end() reject the

			 * thing entirely.  Might be memory poisoning

			 * halfway through, might be a race with munmap,

			 * might be severe memory pressure.

 don't bother with blocks that are not shared to start with */

 don't bother with holes or unwritten extents */

 already zeroed?  we're done. */

 Block boundary? Nothing to do */

/*

 * We're now finished for good with this ioend structure.  Update the page

 * state, release holds on bios, and finally free up memory.  Do not use the

 * ioend after this.

		/*

		 * For the last bio, bi_private points to the ioend, so we

		 * need to explicitly end the iteration here.

 walk each page on bio, ending page IO on them */

 The ioend has been freed by bio_put() */

/*

 * We can merge two adjacent ioends if they have the same set of work to do.

/*

 * Submit the final bio for an ioend.

 *

 * If @error is non-zero, it means that we have a situation where some part of

 * the submission process has failed after we've marked pages for writeback

 * and unlocked them.  In this situation, we need to fail the bio instead of

 * submitting it.  This typically only happens on a filesystem shutdown.

		/*

		 * If we're failing the IO now, just mark the ioend with an

		 * error and finish it.  This will run IO completion immediately

		 * as there is only one reference to the ioend at this point in

		 * time.

/*

 * Allocate a new bio, and chain the old bio to the new one.

 *

 * Note that we have to perform the chaining in this unintuitive order

 * so that the bi_private linkage is set up in the right direction for the

 * traversal in iomap_finish_ioend().

 also copies over blkcg information */

 for iomap_finish_ioend */

/*

 * Test to see if we have an existing ioend structure that we could append to

 * first; otherwise finish off the current ioend and start another.

/*

 * We implement an immediate ioend submission policy here to avoid needing to

 * chain multiple ioends and hence nest mempool allocations which can violate

 * the forward progress guarantees we need to provide. The current ioend we're

 * adding blocks to is cached in the writepage context, and if the new block

 * doesn't append to the cached ioend, it will create a new ioend and cache that

 * instead.

 *

 * If a new ioend is created and cached, the old ioend is returned and queued

 * locally for submission once the entire page is processed or an error has been

 * detected.  While ioends are submitted immediately after they are completed,

 * batching optimisations are provided by higher level block plugging.

 *

 * At the end of a writeback pass, there will be a cached ioend remaining on the

 * writepage context that the caller will need to submit.

 file offset of page */

	/*

	 * Walk through the page to find areas to write back. If we run off the

	 * end of the current map or find the current map invalid, grab a new

	 * one.

	/*

	 * We cannot cancel the ioend directly here on error.  We may have

	 * already set other pages under writeback and hence we have to run I/O

	 * completion to mark the error state of the pages under writeback

	 * appropriately.

		/*

		 * Let the filesystem know what portion of the current page

		 * failed to map. If the page hasn't been added to ioend, it

		 * won't be affected by I/O completion and we must unlock it

		 * now.

	/*

	 * Preserve the original error if there was one; catch

	 * submission errors here and propagate into subsequent ioend

	 * submissions.

	/*

	 * We can end up here with no error and nothing to write only if we race

	 * with a partial page truncate on a sub-page block sized filesystem.

/*

 * Write out a dirty page.

 *

 * For delalloc space on the page, we need to allocate space and flush it.

 * For unwritten space on the page, we need to start the conversion to

 * regular allocated space.

	/*

	 * Refuse to write the page out if we're called from reclaim context.

	 *

	 * This avoids stack overflows when called from deeply used stacks in

	 * random callers for direct reclaim or memcg reclaim.  We explicitly

	 * allow reclaim from kswapd as the stack usage there is relatively low.

	 *

	 * This should never happen except in the case of a VM regression so

	 * warn about it.

	/*

	 * Is this page beyond the end of the file?

	 *

	 * The page index is less than the end_index, adjust the end_offset

	 * to the highest offset that this page should represent.

	 * -----------------------------------------------------

	 * |			file mapping	       | <EOF> |

	 * -----------------------------------------------------

	 * | Page ... | Page N-2 | Page N-1 |  Page N  |       |

	 * ^--------------------------------^----------|--------

	 * |     desired writeback range    |      see else    |

	 * ---------------------------------^------------------|

		/*

		 * Check whether the page to write out is beyond or straddles

		 * i_size or not.

		 * -------------------------------------------------------

		 * |		file mapping		        | <EOF>  |

		 * -------------------------------------------------------

		 * | Page ... | Page N-2 | Page N-1 |  Page N   | Beyond |

		 * ^--------------------------------^-----------|---------

		 * |				    |      Straddles     |

		 * ---------------------------------^-----------|--------|

		/*

		 * Skip the page if it's fully outside i_size, e.g. due to a

		 * truncate operation that's in progress. We must redirty the

		 * page so that reclaim stops reclaiming it. Otherwise

		 * iomap_vm_releasepage() is called on it and gets confused.

		 *

		 * Note that the end_index is unsigned long.  If the given

		 * offset is greater than 16TB on a 32-bit system then if we

		 * checked if the page is fully outside i_size with

		 * "if (page->index >= end_index + 1)", "end_index + 1" would

		 * overflow and evaluate to 0.  Hence this page would be

		 * redirtied and written out repeatedly, which would result in

		 * an infinite loop; the user program performing this operation

		 * would hang.  Instead, we can detect this situation by

		 * checking if the page is totally beyond i_size or if its

		 * offset is just equal to the EOF.

		/*

		 * The page straddles i_size.  It must be zeroed out on each

		 * and every writepage invocation because it may be mmapped.

		 * "A file is mapped in multiples of the page size.  For a file

		 * that is not a multiple of the page size, the remaining

		 * memory is zeroed when mapped, and writes to that region are

		 * not written out to the file."

 Adjust the end_offset to the end of file */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2018 Oracle.  All Rights Reserved.

 * Author: Darrick J. Wong <darrick.wong@oracle.com>

 Swapfile activation */

 accumulated iomap */

 lowest physical addr seen (pages) */

 highest physical addr seen (pages) */

 number of pages collected */

 extent count */

/*

 * Collect physical extents for this swap file.  Physical extents reported to

 * the swap code must be trimmed to align to a page boundary.  The logical

 * offset within the file is irrelevant since the swapfile code maps logical

 * page numbers of the swap device to the physical page-aligned extents.

	/*

	 * Round the start up and the end down so that the physical

	 * extent aligns to a page boundary.

 Skip too-short physical extents. */

	/*

	 * Calculate how much swap space we're adding; the first page contains

	 * the swap header and doesn't count.  The mm still wants that first

	 * page fed to add_swap_extent, however.

 Add extent, set up for the next call. */

/*

 * Accumulate iomaps for this swap file.  We have to accumulate iomaps because

 * swap only cares about contiguous page-aligned physical extents and makes no

 * distinction between written and unwritten extents.

 Only real or unwritten extents. */

 No inline data. */

 No uncommitted metadata or shared blocks. */

 Only one bdev per swap file. */

 No accumulated extent, so just store it. */

 Append this to the accumulated extent. */

 Otherwise, add the retained iomap and store this one. */

/*

 * Iterate a swap file's iomaps to construct physical extents that can be

 * passed to the swapfile subsystem.

	/*

	 * Persist all file mapping metadata so that we won't have any

	 * IOMAP_F_DIRTY iomaps.

	/*

	 * If this swapfile doesn't contain even a single page-aligned

	 * contiguous range of blocks, reject this useless swapfile to

	 * prevent confusion later on.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2010 Red Hat, Inc.

 * Copyright (c) 2016-2021 Christoph Hellwig.

 handle the previous iteration (if any) */

 clear the state for the next iteration */

/**

 * iomap_iter - iterate over a ranges in a file

 * @iter: iteration structue

 * @ops: iomap ops provided by the file system

 *

 * Iterate over filesystem-provided space mappings for the provided file range.

 *

 * This function handles cleanup of resources acquired for iteration when the

 * filesystem indicates there are no more space mappings, which means that this

 * function must be called in a loop that continues as long it returns a

 * positive value.  If 0 or a negative value is returned, the caller must not

 * return to the loop body.  Within a loop body, there are two ways to break out

 * of the loop body:  leave @iter.processed unchanged, or set it to a negative

 * errno.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2010 Red Hat, Inc.

 * Copyright (c) 2016-2021 Christoph Hellwig.

/*

 * Private flags for iomap_dio, must not overlap with the public ones in

 * iomap.h:

 used during submission and for synchronous completion: */

 used for aio completion: */

 check for short read */

	/*

	 * Try again to invalidate clean pages which might have been cached by

	 * non-direct readahead, or faulted in by get_user_pages() if the source

	 * of the write was an mmap'ed region of the file we're writing.  Either

	 * one is a pretty crazy thing to do, so we don't support it 100%.  If

	 * this invalidation fails, tough, the write still worked...

	 *

	 * And this page cache invalidation has to be after ->end_io(), as some

	 * filesystems convert unwritten extents to real allocations in

	 * ->end_io() when necessary, otherwise a racing buffer read would cache

	 * zeros from unwritten extents.

	/*

	 * If this is a DSYNC write, make sure we push it to stable storage now

	 * that we've written data.

/*

 * Set an error in the dio if none is set yet.  We have to use cmpxchg

 * as the submission context and the completion context(s) can race to

 * update the error.

/*

 * Figure out the bio's operation flags from the dio request, the

 * mapping, and whether or not we want FUA.  Note that we can end up

 * clearing the WRITE_FUA flag in the dio request.

		/*

		 * Use a FUA write if we need datasync semantics, this is a pure

		 * data IO that doesn't require any metadata updates (including

		 * after IO completion such as unwritten extent conversion) and

		 * the underlying device supports FUA. This allows us to avoid

		 * cache flushes on IO completion.

	/*

	 * Save the original count and trim the iter to just the extent we

	 * are operating on right now.  The iter will be re-expanded once

	 * we are done.

	/*

	 * We can only poll for single bio I/Os.

 zero out from the start of the block to the write offset */

	/*

	 * Set the operation flags early so that bio_iov_iter_get_pages

	 * can set up the page vector appropriately for a ZONE_APPEND

	 * operation.

			/*

			 * We have to stop part way through an IO. We must fall

			 * through to the sub-block tail zeroing here, otherwise

			 * this short IO may expose stale data in the tail of

			 * the block we haven't written data to.

		/*

		 * We can only poll for single bio I/Os.

	/*

	 * We need to zeroout the tail of a sub-block write if the extent type

	 * requires zeroing or the write extends beyond EOF. If we don't zero

	 * the block tail in the latter case, we can expose stale data via mmap

	 * reads of the EOF block.

 zero out from the end of the write to the end of the block */

 Undo iter limitation to current extent */

		/*

		 * DIO is not serialised against mmap() access at all, and so

		 * if the page_mkwrite occurs between the writeback and the

		 * iomap_iter() call in the DIO path, then it will see the

		 * DELALLOC block that the page-mkwrite allocated.

/*

 * iomap_dio_rw() always completes O_[D]SYNC writes regardless of whether the IO

 * is being issued as AIO or not.  This allows us to optimise pure data writes

 * to use REQ_FUA rather than requiring generic_write_sync() to issue a

 * REQ_FLUSH post write. This is slightly tricky because a single request here

 * can be mapped into multiple disjoint IOs and only a subset of the IOs issued

 * may be pure data writes. In that case, we still need to do a full data sync

 * completion.

 *

 * When page faults are disabled and @dio_flags includes IOMAP_DIO_PARTIAL,

 * __iomap_dio_rw can return a partial result if it encounters a non-resident

 * page in @iter after preparing a transfer.  In that case, the non-resident

 * pages can be faulted in and the request resumed with @done_before set to the

 * number of bytes previously transferred.  The request will then complete with

 * the correct total number of bytes transferred; this is essential for

 * completing partial requests asynchronously.

 *

 * Returns -ENOTBLK In case of a page invalidation invalidation failure for

 * writes.  The callers needs to fall back to buffered I/O in this case.

 for data sync or sync, we need sync completion processing */

		/*

		 * For datasync only writes, we optimistically try using FUA for

		 * this IO.  Any non-FUA write that occurs will clear this flag,

		 * hence we know before completion whether a cache flush is

		 * necessary.

		/*

		 * Try to invalidate cache pages for the range we are writing.

		 * If this invalidation fails, let the caller fall back to

		 * buffered I/O.

		/*

		 * We can only poll for single bio I/Os.

	/*

	 * We only report that we've read data up to i_size.

	 * Revert iter to a state corresponding to that as some callers (such

	 * as the splice code) rely on it.

 magic error code to fall back to buffered I/O */

	/*

	 * If all the writes we issued were FUA, we don't need to flush the

	 * cache on IO completion. Clear the sync flag for this case.

	/*

	 * We are about to drop our additional submission reference, which

	 * might be the last reference to the dio.  There are three different

	 * ways we can progress here:

	 *

	 *  (a) If this is the last reference we will always complete and free

	 *	the dio ourselves.

	 *  (b) If this is not the last reference, and we serve an asynchronous

	 *	iocb, we must never touch the dio after the decrement, the

	 *	I/O completion handler will complete and free it.

	 *  (c) If this is not the last reference, but we serve a synchronous

	 *	iocb, the I/O completion handler will wake us up on the drop

	 *	of the final reference, and we will complete and free it here

	 *	after we got woken by the I/O completion handler.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2017 Red Hat, Inc.

 * Copyright (c) 2018-2021 Christoph Hellwig.

 Nothing to be found before or beyond the end of the file. */

 found hole before EOF */

 Nothing to be found before or beyond the end of the file. */

 found data before EOF */

 We've reached the end of the file without finding data */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2019 Christoph Hellwig

/*

 * We include this last to have the helpers above available for the trace

 * event implementations.

/*

 * hugetlbpage-backed filesystem.  Based on ramfs.

 *

 * Nadia Yvette Chambers, 2002

 *

 * Copyright (C) 2002 Linus Torvalds.

 * License: GPL

 remove ASAP */

/*

 * Mask used when checking the page offset value passed in via system

 * calls.  This value will be converted to a loff_t which is signed.

 * Therefore, we want to check the upper PAGE_SHIFT + 1 bits of the

 * value.  The extra bit (- 1 in the shift value) is to take the sign

 * bit into account.

	/*

	 * vma address alignment (but not the pgoff alignment) has

	 * already been checked by prepare_hugepage_range.  If you add

	 * any error returns here, do so after setting VM_HUGETLB, so

	 * is_vm_hugetlb_page tests below unmap_region go the right

	 * way when do_mmap unwinds (may be important on powerpc

	 * and ia64).

	/*

	 * page based offset in vm_pgoff could be sufficiently large to

	 * overflow a loff_t when converted to byte offset.  This can

	 * only happen on architectures where sizeof(loff_t) ==

	 * sizeof(unsigned long).  So, only check in those instances.

 must be huge page aligned */

 check for overflow */

/*

 * Called under mmap_write_lock(mm).

	/*

	 * A failed mmap() very likely causes application failure,

	 * so fall back to the bottom-up function here. This scenario

	 * can happen with large stack limits and large mmap()

	 * allocations.

	/*

	 * Use mm->get_unmapped_area value as a hint to use topdown routine.

	 * If architectures have special needs, they should define their own

	 * version of hugetlb_get_unmapped_area.

 Find which 4k chunk and offset with in that chunk */

/*

 * Support for read() - Find the page attached to f_mapping and copy out the

 * data. Its *very* similar to generic_file_buffered_read(), we can't use that

 * since it has PAGE_SIZE assumptions.

 nr is the maximum number of bytes to copy from this page */

 Find the page */

			/*

			 * We have a HOLE, zero out the user-buffer for the

			 * length of the hole or request.

			/*

			 * We have the page, copy it to user space buffer.

	/*

	 * end == 0 indicates that the entire range after

	 * start should be unmapped.

		/*

		 * Can the expression below overflow on 32-bit arches?

		 * No, because the interval tree returns us only those vmas

		 * which overlap the truncated area starting at pgoff,

		 * and no vma on a 32-bit arch can span beyond the 4GB.

/*

 * remove_inode_hugepages handles two distinct cases: truncation and hole

 * punch.  There are subtle differences in operation for each case.

 *

 * truncation is indicated by end of range being LLONG_MAX

 *	In this case, we first scan the range and release found pages.

 *	After releasing pages, hugetlb_unreserve_pages cleans up region/reserve

 *	maps and global counts.  Page faults can not race with truncation

 *	in this routine.  hugetlb_no_page() holds i_mmap_rwsem and prevents

 *	page faults in the truncated range by checking i_size.  i_size is

 *	modified while holding i_mmap_rwsem.

 * hole punch is indicated if end is not LLONG_MAX

 *	In the hole punch case we scan the range and release found pages.

 *	Only when releasing a page is the associated region/reserve map

 *	deleted.  The region/reserve map for ranges without associated

 *	pages are not modified.  Page faults can race with hole punch.

 *	This is indicated if we find a mapped page.

 * Note: If the passed end of range value is beyond the end of file, but

 * not LLONG_MAX this routine still performs a hole punch operation.

		/*

		 * When no more pages are found, we are done.

				/*

				 * Only need to hold the fault mutex in the

				 * hole punch case.  This prevents races with

				 * page faults.  Races are not possible in the

				 * case of truncation.

			/*

			 * If page is mapped, it was faulted in after being

			 * unmapped in caller.  Unmap (again) now after taking

			 * the fault mutex.  The mutex will prevent faults

			 * until we finish removing the page.

			 *

			 * This race can only happen in the hole punch case.

			 * Getting here in a truncate operation is a bug.

			/*

			 * We must free the huge page and remove from page

			 * cache (remove_huge_page) BEFORE removing the

			 * region/reserve map (hugetlb_unreserve_pages).  In

			 * rare out of memory conditions, removal of the

			 * region/reserve map could fail. Correspondingly,

			 * the subpool and global reserve usage count can need

			 * to be adjusted.

	/*

	 * Get the resv_map from the address space embedded in the inode.

	 * This is the address space which points to any resv_map allocated

	 * at inode creation time.  If this is a device special inode,

	 * i_mapping may not point to the original address space.

 Only regular and link inodes have associated reserve maps */

	/*

	 * For hole punch round up the beginning offset of the hole and

	 * round down the end.

 protected by i_rwsem */

	/*

	 * Default preallocate case.

	 * For this range, start is rounded down and end is rounded up

	 * as well as being converted to page offsets.

 We need to check rlimit even when FALLOC_FL_KEEP_SIZE */

	/*

	 * Initialize a pseudo vma as this is required by the huge page

	 * allocation routines.  If NUMA is configured, use page index

	 * as input to create an allocation policy.

		/*

		 * This is supposed to be the vaddr where the page is being

		 * faulted in, but we have no vaddr here.

		/*

		 * fallocate(2) manpage permits EINTR; we may have been

		 * interrupted because we are using up too much memory.

 Set numa allocation policy based on index */

 addr is the offset within the file (zero based) */

		/*

		 * fault mutex taken here, protects against fault path

		 * and hole punch.  inode_lock previously taken protects

		 * against truncation.

 See if already present in mapping to avoid alloc/free */

		/*

		 * Allocate page without setting the avoid_reserve argument.

		 * There certainly are no reserves associated with the

		 * pseudo_vma.  However, there could be shared mappings with

		 * reserves for the file at the inode level.  If we fallocate

		 * pages in these areas, we need to consume the reserves

		 * to keep reservation accounting consistent.

		/*

		 * unlock_page because locked by add_to_page_cache()

		 * put_page() due to reference from alloc_huge_page()

 protected by i_rwsem */

 directory inodes start off with i_nlink == 2 (for "." entry) */

/*

 * Hugetlbfs is not reclaimable; therefore its i_mmap_rwsem will never

 * be taken from reclaim -- unlike regular filesystems. This needs an

 * annotation because huge_pmd_share() does an allocation under hugetlb's

 * i_mmap_rwsem.

	/*

	 * Reserve maps are only needed for inodes that can have associated

	 * page allocations.

 directory inodes start off with i_nlink == 2 (for "." entry) */

/*

 * File creation. Allocate an inode, and we're done..

 Extra count - pin the dentry in core */

/*

 * Display the mount options in /proc/mounts.

		/* If no limits set, just report 0 for max/free/used

	/*

	 * Any time after allocation, hugetlbfs_destroy_inode can be called

	 * for the inode.  mpol_free_shared_policy is unconditionally called

	 * as part of hugetlbfs_destroy_inode.  So, initialize policy here

	 * in case of a quick call to destroy.

	 *

	 * Note that the policy is initialized even if we are creating a

	 * private inode.  This simplifies hugetlbfs_destroy_inode.

/*

 * Convert size option passed from command line to number of huge pages

 * in the pool specified by hstate.  Size option could be in bytes

 * (val_type == SIZE_STD) or percentage of the pool (val_type == SIZE_PERCENT).

/*

 * Parse one mount parameter.

 memparse() will accept a K/M/G without a digit */

 memparse() will accept a K/M/G without a digit */

 memparse() will accept a K/M/G without a digit */

/*

 * Validate the parsed options.

	/*

	 * Use huge page pool size (in hstate) to convert the size

	 * options to number of huge pages.  If NO_SIZE, -1 is returned.

	/*

	 * If max_size was specified, then min_size must be smaller

	/*

	 * Allocate and initialize subpool if maximum or minimum size is

	 * specified.  Any needed reservations (for minimum size) are taken

	 * taken when the subpool is created.

	/*

	 * Due to the special and limited functionality of hugetlbfs, it does

	 * not work well as a stacking filesystem.

 No limit on size by default */

 No limit on number of inodes by default */

 No default minimum size */

/*

 * Note that size should be aligned to proper hugepage size in caller side,

 * otherwise hugetlb_reserve_pages reserves one less hugepages than intended.

 default hstate mount is required */

 other hstates are optional */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Red Hat.  All rights reserved.

		/*

		 * We're holding a transaction handle, so use a NOFS memory

		 * allocation context to avoid deadlock if reclaim happens.

 this happens with subvols */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/* If we have a 32-bit userspace and 64-bit kernel, then the UAPI

 * structures are incorrect, as the timespec structure from userspace

 * is 4 bytes too small. We define these alternatives here to teach

 * the kernel about the 32-bit struct packing.

 in */

 in */

 out */

 in */

 out */

 in */

 in */

 in */

 in */

 in */

 in */

 in */

 in */

 in */

 Mask out flags that are inappropriate for the given type of inode. */

/*

 * Export internal inode flags to the format expected by the FS_IOC_GETFLAGS

 * ioctl.

/*

 * Update inode->i_flags based on the btrfs internal flags.

/*

 * Check if @flags are a supported and valid set of FS_*_FL flags and that

 * the old and new flags are not conflicting

 COMPR and NOCOMP on new/old are valid */

 NOCOW and compression options are mutually exclusive */

/*

 * Set flags/xflags from the internal inode flags. The remaining items of

 * fsxattr are zeroed.

 If coming from FS_IOC_FSSETXATTR then skip unconverted flags */

 1 item for the inode */

			/*

			 * It's safe to turn csums off here, no extents exist.

			 * Otherwise we want the flag to reflect the real COW

			 * status of the file and will not set it.

		/*

		 * Revert back under same assumptions as above

	/*

	 * The COMPRESS flag can only be changed by users, while the NOCOMPRESS

	 * flag may be changed automatically if compression code won't make

	 * things smaller.

	/*

	 * 1 for inode item

	 * 2 for properties

/*

 * Start exclusive operation @type, return true on success

/*

 * Conditionally allow to enter the exclusive operation in case it's compatible

 * with the running one.  This must be paired with btrfs_exclop_start_unlock and

 * btrfs_exclop_finish.

 *

 * Compatibility:

 * - the same type is already running

 * - not BTRFS_EXCLOP_NONE - this is intentionally incompatible and the caller

 *   must check the condition first that would allow none -> @type

	/*

	 * btrfs_trim_block_group() depends on space cache, which is not

	 * available in zoned filesystem. So, disallow fitrim on a zoned

	 * filesystem for now.

	/*

	 * If the fs is mounted with nologreplay, which requires it to be

	 * mounted in RO mode as well, we can not allow discard on free space

	 * inside block groups, because log trees refer to extents that are not

	 * pinned in a block group's free space cache (pinning the extents is

	 * precisely the first phase of replaying a log tree).

	/*

	 * NOTE: Don't truncate the range using super->total_bytes.  Bytenr of

	 * block group is in the logical address space, which can be any

	 * sectorsize aligned bytenr in  the range [0, U64_MAX].

	/*

	 * Don't create subvolume whose level is not zero. Or qgroup will be

	 * screwed up since it assumes subvolume qgroup's level to be 0.

	/*

	 * The same as the snapshot creation, please see the comment

	 * of create_snapshot().

		/*

		 * Since we don't abort the transaction in this case, free the

		 * tree block so that we don't leak space and leave the

		 * filesystem in an inconsistent state (an extent item in the

		 * extent tree without backreferences). Also no need to have

		 * the tree block locked since it is not in any tree at this

		 * point, so no other task can find it and use it.

 Freeing will be done in btrfs_put_root() of new_root */

 We potentially lose an unused inode item here */

	/*

	 * insert the directory item

	/*

	 * 1 - parent dir inode

	 * 2 - dir entries

	 * 1 - root item

	 * 2 - root ref/backref

	 * 1 - root of snapshot

	 * 1 - UUID item

 Prevent double freeing of anon_dev */

/*  copy of may_delete in fs/namei.c()

 *	Check whether we can remove a link victim from directory dir, check

 *  whether the type of victim is right.

 *  1. We can't do it if dir is read-only (done in permission())

 *  2. We should have write and exec permissions on dir

 *  3. We can't remove anything from append-only dir

 *  4. We can't do anything with immutable dir (done in permission())

 *  5. If the sticky bit on dir is set we should either

 *	a. be owner of dir, or

 *	b. be owner of victim, or

 *	c. have CAP_FOWNER capability

 *  6. If the victim is append-only or immutable we can't do anything with

 *     links pointing to it.

 *  7. If we were asked to remove a directory and victim isn't one - ENOTDIR.

 *  8. If we were asked to remove a non-directory and victim isn't one - EISDIR.

 *  9. We can't remove a root or mountpoint.

 * 10. We don't allow removal of NFS sillyrenamed files; it's handled by

 *     nfs_async_unlink().

 copy of may_create in fs/namei.c() */

/*

 * Create a new subvolume below @parent.  This is largely modeled after

 * sys_mkdirat and vfs_mkdir, but we only do a single component lookup

 * inside this filesystem so it's quite a bit simpler.

	/*

	 * even if this name doesn't exist, we may get hash collisions.

	 * check for them now when we can safely fail

	/*

	 * Force new buffered writes to reserve space even when NOCOW is

	 * possible. This is to avoid later writeback (running dealloc) to

	 * fallback to COW mode and unexpectedly fail with ENOSPC.

	/*

	 * All previous writes have started writeback in NOCOW mode, so now

	 * we force future writes to fallback to COW mode during snapshot

	 * creation.

	/*

	 * hopefully we have this extent in the tree already, try without

	 * the full extent lock

 get the big lock and read metadata off disk */

 this is the last extent */

/*

 * Prepare one page to be defragged.

 *

 * This will ensure:

 *

 * - Returned page is locked and has been set up properly.

 * - No ordered extent exists in the page.

 * - The page is uptodate.

 *

 * NOTE: Caller should also wait for page writeback after the cluster is

 * prepared, here we don't do writeback wait for each page.

	/*

	 * Since we can defragment files opened read-only, we can encounter

	 * transparent huge pages here (see CONFIG_READ_ONLY_THP_FOR_FS). We

	 * can't do I/O using huge pages yet, so return an error for now.

	 * Filesystem transparent huge pages are typically only used for

	 * executables that explicitly enable them, so this isn't very

	 * restrictive.

 Wait for any existing ordered extent in the range */

		/*

		 * We unlocked the page above, so we need check if it was

		 * released or not.

	/*

	 * Now the page range has no ordered extent any more.  Read the page to

	 * make it uptodate.

/*

 * Collect all valid target extents.

 *

 * @start:	   file offset to lookup

 * @len:	   length to lookup

 * @extent_thresh: file extent size threshold, any extent size >= this value

 *		   will be ignored

 * @newer_than:    only defrag extents newer than this value

 * @do_compress:   whether the defrag is doing compression

 *		   if true, @extent_thresh will be ignored and all regular

 *		   file extents meeting @newer_than will be targets.

 * @locked:	   if the range has already held extent lock

 * @target_list:   list of targets file extents

 Skip hole/inline/preallocated extents */

 Skip older extent */

		/*

		 * For do_compress case, we want to compress all valid file

		 * extents, thus no @extent_thresh or mergeable check.

 Skip too large extent */

 Empty target list, no way to merge with last entry */

 Not mergeable with last entry */

 Mergeable, fall through to add it to @target_list. */

		/*

		 * This one is a good target, check if it can be merged into

		 * last range of the target list.

 Mergeable, enlarge the last entry */

 Fall through to allocate a new entry */

 Allocate new defrag_target_range */

/*

 * Defrag one contiguous target range.

 *

 * @inode:	target inode

 * @target:	target range to defrag

 * @pages:	locked pages covering the defrag range

 * @nr_pages:	number of locked pages

 *

 * Caller should ensure:

 *

 * - Pages are prepared

 *   Pages should be locked, no ordered extent in the pages range,

 *   no writeback.

 *

 * - Extent bits are locked

 Update the page status */

 Prepare all pages */

 Lock the pages range */

	/*

	 * Now we have a consistent view about the extent map, re-check

	 * which range really needs to be defragged.

	 *

	 * And this time we have extent locked already, pass @locked = true

	 * so that we won't relock the extent range and cause deadlock.

 Reached the limit */

		/*

		 * Here we may not defrag any range if holes are punched before

		 * we locked the pages.

		 * But that's fine, it only affects the @sectors_defragged

		 * accounting.

/*

 * Entry point to file defragmentation.

 *

 * @inode:	   inode to be defragged

 * @ra:		   readahead state (can be NUL)

 * @range:	   defrag options including range and flags

 * @newer_than:	   minimum transid to defrag

 * @max_to_defrag: max number of sectors to be defragged, if 0, the whole inode

 *		   will be defragged.

 Got a specific range */

 Defrag until file end */

	/*

	 * If we were not given a ra, allocate a readahead context. As

	 * readahead is just an optimization, defrag will work without it so

	 * we don't error out.

 Align the range */

 The cluster size 256K should always be page aligned */

 We want the cluster end at page boundary when possible */

		/*

		 * We have defragged some sectors, for compression case they

		 * need to be written back immediately.

/*

 * Try to start exclusive operation @type or cancel it if it's running.

 *

 * Return:

 *   0        - normal mode, newly claimed op started

 *  >0        - normal mode, something else is running,

 *              return BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS to user space

 * ECANCELED  - cancel mode, successful cancel

 * ENOTCONN   - cancel mode, operation not running anymore

 Start normal op */

 Exclusive operation is now claimed */

 Cancel running op */

		/*

		 * This blocks any exclop finish from setting it to NONE, so we

		 * request cancellation. Either it runs and we will wait for it,

		 * or it has finished and no waiting will happen.

 Something else is running or none */

	/*

	 * Read the arguments before checking exclusivity to be able to

	 * distinguish regular resize and cancel

 Exclusive operation is now claimed */

 equal, nothing need to do */

			/*

			 * Subvolume creation is not restricted, but snapshots

			 * are limited to own subvolumes only

 nothing to do */

		/*

		 * Block RO -> RW transition if this subvolume is involved in

		 * send

			/*

			 * return one empty item back for v1, which does not

			 * handle -EOVERFLOW

		/*

		 * Copy search result header. If we fault then loop again so we

		 * can fault in the pages and -EFAULT there if there's a

		 * problem. Otherwise we'll fault and then copy the buffer in

		 * properly this next time through

			/*

			 * Copy the item, same behavior as above, but reset the

			 * * sk_offset so we copy the full thing again.

 -EOVERFLOW from above */

	/*

	 *  0: all items from this leaf copied, continue with next

	 *  1: * more items can be copied, but unused buffer is too small

	 *     * all items were found

	 *     Either way, it will stops the loop which iterates to the next

	 *     leaf

	 *  -EOVERFLOW: item was to large for buffer

	 *  -EFAULT: could not copy extent buffer back to userspace

 search the root of the inode that was passed */

	/*

	 * In the origin implementation an overflow is handled by returning a

	 * search header with a len of zero, so reset ret.

 copy search header and buffer size */

 limit result size to 16MB */

/*

 * Search INODE_REFs to identify path name of 'dirid' directory

 * in a 'tree_id' tree. and sets path name to 'name'.

	/*

	 * If the bottom subvolume does not exist directly under upper_limit,

	 * construct the path in from the bottom up.

 Check the read+exec permission of this directory */

 Get the bottom subvolume's name from ROOT_REF */

 Check if dirid in ROOT_REF corresponds to passed dirid */

 Copy subvolume's name */

	/*

	 * Unprivileged query to obtain the containing subvolume root id. The

	 * path is reset so it's consistent with btrfs_search_path_in_tree.

/*

 * Version of ino_lookup ioctl (unprivileged)

 *

 * The main differences from ino_lookup ioctl are:

 *

 *   1. Read + Exec permission will be checked using inode_permission() during

 *      path construction. -EACCES will be returned in case of failure.

 *   2. Path construction will be stopped at the inode number which corresponds

 *      to the fd with which this ioctl is called. If constructed path does not

 *      exist under fd's inode, -EACCES will be returned.

 *   3. The name of bottom subvolume is also searched and filled.

		/*

		 * The subvolume does not exist under fd with which this is

		 * called

 Get the subvolume information in BTRFS_ROOT_ITEM and BTRFS_ROOT_BACKREF */

 Get root_item of inode's subvolume */

 Search root tree for ROOT_BACKREF of this subvolume */

/*

 * Return ROOT_REF information of the subvolume containing this inode

 * except the subvolume name.

 update min_treeid for next search */

		/*

		 * If SPEC_BY_ID is not set, we are looking for the subvolume by

		 * name, same as v1 currently does.

			/*

			 * Change the default parent since the subvolume being

			 * deleted can be outside of the current mount point.

			/*

			 * At this point dentry->d_name can point to '/' if the

			 * subvolume we want to destroy is outsite of the

			 * current mount point, so we need to release the

			 * current dentry and execute the lookup to return a new

			 * one with ->d_name pointing to the

			 * <mount point>/subvol_name.

			/*

			 * If v2 was used with SPEC_BY_ID, a new parent was

			 * allocated since the subvolume can be outside of the

			 * current mount point. Later on we need to release this

			 * new parent dentry.

			/*

			 * On idmapped mounts, deletion via subvolid is

			 * restricted to subvolumes that are immediate

			 * ancestors of the inode referenced by the file

			 * descriptor in the ioctl. Otherwise the idmapping

			 * could potentially be abused to delete subvolumes

			 * anywhere in the filesystem the user wouldn't be able

			 * to delete without an idmapped mount.

 subvol_name_ptr is already nul terminated */

		/*

		 * Regular user.  Only allow this with a special mount

		 * option, when the user has write+exec access to the

		 * subvol root, and when rmdir(2) would have been

		 * allowed.

		 *

		 * Note that this is _not_ check that the subvol is

		 * empty or doesn't contain data that we wouldn't

		 * otherwise be able to delete.

		 *

		 * Users who want to delete empty subvols should try

		 * rmdir(2).

		/*

		 * Do not allow deletion if the parent dir is the same

		 * as the dir to be deleted.  That means the ioctl

		 * must be called on the dentry referencing the root

		 * of the subvol, not a random directory contained

		 * within it.

 check if subvolume may be deleted by a user */

		/*

		 * Note that this does not check the file descriptor for write

		 * access. This prevents defragmenting executables that are

		 * running and allows defrag on files open in read-only mode.

 compression requires us to start the IO */

 the rest are all set to zero by kzalloc */

 Exclusive operation is now claimed */

	/*

	 * Global block reserve, exported as a space_info

 space_slots == 0 means they are asking for a count */

	/* we generally have at most 6 or so space infos, one for each raid

	 * level.  So, a whole page should be more than enough for everyone

 now we have a buffer to copy into */

	/*

	 * Add global block reserve

 No running transaction, don't bother */

 current trans */

	/*

	 * Copy scrub args to user space even if btrfs_scrub_dev() returned an

	 * error. This is important as it allows user space to know how much

	 * progress scrub has done. For example, if scrub is canceled we get

	 * -ECANCELED from btrfs_scrub_dev() and return that error back to user

	 * space. Later user space can inspect the progress from the structure

	 * btrfs_ioctl_scrub_args and resume scrub from where it left off

	 * previously (btrfs-progs does this).

	 * If we fail to copy the btrfs_ioctl_scrub_args structure to user space

	 * then return -EFAULT to signal the structure was not copied or it may

	 * be corrupt and unreliable due to a partial copy.

 All reserved bits must be 0 for now */

 Only accept flags we have defined so far */

 for mut. excl. ops lock */

	/*

	 * mut. excl. ops lock is locked.  Three possibilities:

	 *   (1) some other op is running

	 *   (2) balance is running

	 *   (3) balance is paused -- special case (think resume)

 this is either (2) or (3) */

			/*

			 * Lock released to allow other waiters to continue,

			 * we'll reexamine the status again.

 this is (3) */

 this is (2) */

 this is (1) */

 balance everything - no filters */

	/*

	 * Ownership of bctl and exclusive operation goes to btrfs_balance.

	 * bctl is freed in reset_balance_state, or, if restriper was paused

	 * all the way until unmount, in free_fs_info.  The flag should be

	 * cleared after reset_balance_state.

 update qgroup status and info */

 take the current subvol as qgroup */

	/*

	 * 1 - root item

	 * 2 - uuid items (received uuid + subvol uuid)

 Nothing to do */

		/*

		 * The transaction thread may want to do more work,

		 * namely it pokes the cleaner kthread that will start

		 * processing uncleaned subvols.

	/*

	 * These all access 32-bit values anyway so no further

	 * handling is necessary.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2008 Red Hat.  All rights reserved.

 JDM: Really? */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2008 Oracle.  All rights reserved.

		/*

		 * This can happen when compression races with remount setting

		 * it to 'no compress', while caller doesn't call

		 * inode_need_compress() to check if we really need to

		 * compress.

		 *

		 * Not a big deal, just need to inform caller that we

		 * haven't allocated any pages yet.

		/*

		 * This can't happen, the type is validated several times

		 * before we get here.

		/*

		 * This can't happen, the type is validated several times

		 * before we get here.

 Determine the remaining bytes inside the page first */

 Hash through the page sector by sector */

/*

 * Reduce bio and io accounting for a compressed_bio with its corresponding bio.

 *

 * Return true if there is no pending bio nor io.

 * Return false otherwise.

	/*

	 * At endio time, bi_iter.bi_size doesn't represent the real bio size.

	 * Thus here we have to iterate through all segments to grab correct

	 * bio size.

	/*

	 * Here we must wake up the possible error handler after all other

	 * operations on @cb finished, or we can race with

	 * finish_compressed_bio_*() which may free @cb.

 Release the compressed pages */

 Do io completion on the original bio */

		/*

		 * We have verified the checksum already, set page checked so

		 * the end_io handlers know about it

 Finally free the cb struct */

/* when we finish reading compressed pages from the disk, we

 * decompress them and then run the bio end_io routines on the

 * decompressed pages (in the inode address space).

 *

 * This allows the checksumming and other IO error handling routines

 * to work normally

 *

 * The compressed pages are freed here, and it must be run

 * in process context

	/*

	 * Record the correct mirror_num in cb->orig_bio so that

	 * read-repair can work properly.

	/*

	 * Some IO in this cb have failed, just skip checksum as there

	 * is no way it could be correct.

	/* ok, we're the last bio for this extent, lets start

	 * the decompression.

/*

 * Clear the writeback bits on all of the file

 * pages for a compressed write

 the inode may be gone now */

	/*

	 * Ok, we're the last bio for this extent, step one is to call back

	 * into the FS and do all the end_io operations.

 Note, our inode could be gone now */

	/*

	 * Release the compressed pages, these came from alloc_page and

	 * are not attached to the inode at all

 Finally free the cb struct */

/*

 * Do the cleanup once all the compressed pages hit the disk.  This will clear

 * writeback on the file pages and free the compressed pages.

 *

 * This also calls the writeback end hooks for the file pages so that metadata

 * and checksums can be updated in the file.

/*

 * Allocate a compressed_bio, which will be used to read/write on-disk

 * (aka, compressed) * data.

 *

 * @cb:                 The compressed_bio structure, which records all the needed

 *                      information to bind the compressed data to the uncompressed

 *                      page cache.

 * @disk_byten:         The logical bytenr where the compressed data will be read

 *                      from or written to.

 * @endio_func:         The endio function to call after the IO for compressed data

 *                      is finished.

 * @next_stripe_start:  Return value of logical bytenr of where next stripe starts.

 *                      Let the caller know to only fill the bio up to the stripe

 *                      boundary.

/*

 * worker function to build and submit bios for previously compressed pages.

 * The corresponding pages in the inode should be marked for writeback

 * and the compressed pages should have a reference on them for dropping

 * when the IO is complete.

 *

 * This also checksums the file bytes and gets things ready for

 * the end io hooks.

 Allocate new bio if submitted or not yet allocated */

		/*

		 * We should never reach next_stripe_start start as we will

		 * submit comp_bio when reach the boundary immediately.

		/*

		 * We have various limits on the real read size:

		 * - stripe boundary

		 * - page boundary

		 * - compressed length boundary

 Reached zoned boundary */

 Reached stripe boundary */

 Finished the range */

 Last byte of @cb is submitted, endio will free @cb */

	/*

	 * Even with previous bio ended, we should still have io not yet

	 * submitted, thus need to finish manually.

 Now we are the only one referring @cb, can finish it safely. */

/*

 * Add extra pages in the same compressed file extent so that we don't need to

 * re-read the same extent again and again.

 *

 * NOTE: this won't work well for subpage, as for subpage read, we lock the

 * full page then submit bio for each compressed/regular extents.

 *

 * This means, if we have several sectors in the same page points to the same

 * on-disk compressed data, we will re-read the same extent many times and

 * this function can only help for the next page.

	/*

	 * For current subpage support, we only support 64K page size,

	 * which means maximum compressed extent size (128K) is just 2x page

	 * size.

	 * This makes readahead less effective, so here disable readahead for

	 * subpage for now, until full compressed write is supported.

 Beyond threshold, no need to continue */

			/*

			 * Jump to next page start as we already have page for

			 * current offset.

 There is already a page, skip to page end */

		/*

		 * At this point, we have a locked page in the page cache for

		 * these bytes in the file.  But, we have to make sure they map

		 * to this compressed extent on disk.

		/*

		 * If it's subpage, we also need to increase its

		 * subpage::readers number, as at endio we will decrease

		 * subpage::readers and to unlock the page.

/*

 * for a compressed read, the bio we get passed has all the inode pages

 * in it.  We don't actually do IO on those pages but allocate new ones

 * to hold the compressed pages on disk.

 *

 * bio->bi_iter.bi_sector points to the compressed extent on disk

 * bio->bi_io_vec points to all of the inode pages

 *

 * After the compressed pages are read, we copy the bytes into the

 * bio we were passed and then call the bio end_io calls

 we need the actual starting offset of this extent in the file */

 include any pages we added in add_ra-bio_pages */

 Allocate new bio if submitted or not yet allocated */

		/*

		 * We should never reach next_stripe_start start as we will

		 * submit comp_bio when reach the boundary immediately.

		/*

		 * We have various limit on the real read size:

		 * - stripe boundary

		 * - page boundary

		 * - compressed length boundary

		/*

		 * Maximum compressed extent is smaller than bio size limit,

		 * thus bio_add_page() should always success.

 Reached stripe boundary, need to submit */

 Has finished the range, need to submit */

 All bytes of @cb is submitted, endio will free @cb */

	/*

	 * Even with previous bio ended, we should still have io not yet

	 * submitted, thus need to finish @cb manually.

 Now we are the only one referring @cb, can finish it safely. */

/*

 * Heuristic uses systematic sampling to collect data from the input data

 * range, the logic can be tuned by the following constants:

 *

 * @SAMPLING_READ_SIZE - how many bytes will be copied from for each sample

 * @SAMPLING_INTERVAL  - range from which the sampled data can be collected

/*

 * For statistical analysis of the input data we consider bytes that form a

 * Galois Field of 256 objects. Each object has an attribute count, ie. how

 * many times the object appeared in the sample.

/*

 * The size of the sample is based on a statistical sampling rule of thumb.

 * The common way is to perform sampling tests as long as the number of

 * elements in each cell is at least 5.

 *

 * Instead of 5, we choose 32 to obtain more accurate results.

 * If the data contain the maximum number of symbols, which is 256, we obtain a

 * sample size bound by 8192.

 *

 * For a sample of at most 8KB of data per data range: 16 consecutive bytes

 * from up to 512 locations.

 Partial copy of input data */

 Buckets store counters for each byte value */

 Sorting buffer */

 The heuristic is represented as compression type 0 */

		/*

		 * This can't happen, the type is validated several times

		 * before we get here.

		/*

		 * This can't happen, the type is validated several times

		 * before we get here.

	/*

	 * Preallocate one workspace for each compression type so we can

	 * guarantee forward progress in the worst case

/*

 * This finds an available workspace or allocates a new one.

 * If it's not possible to allocate a new one, waits until there's one.

 * Preallocation makes a forward progress guarantees and we do not return

 * errors.

	/*

	 * Allocation helpers call vmalloc that can't use GFP_NOFS, so we have

	 * to turn it off here because we might get called from the restricted

	 * context of btrfs_compress_bio/btrfs_compress_pages

		/*

		 * Do not return the error but go back to waiting. There's a

		 * workspace preallocated for each type and the compression

		 * time is bounded so we get to a workspace eventually. This

		 * makes our caller's life easier.

		 *

		 * To prevent silent and low-probability deadlocks (when the

		 * initial preallocation fails), check if there are any

		 * workspaces at all.

 once per minute */ 60 * HZ,

 no burst */ 1);

		/*

		 * This can't happen, the type is validated several times

		 * before we get here.

/*

 * put a workspace struct back on the list or free it if we have enough

 * idle ones sitting around

		/*

		 * This can't happen, the type is validated several times

		 * before we get here.

/*

 * Adjust @level according to the limits of the compression algorithm or

 * fallback to default

/*

 * Given an address space and start and length, compress the bytes into @pages

 * that are allocated on demand.

 *

 * @type_level is encoded algorithm and level, where level 0 means whatever

 * default the algorithm chooses and is opaque here;

 * - compression algo are 0-3

 * - the level are bits 4-7

 *

 * @out_pages is an in/out parameter, holds maximum number of pages to allocate

 * and returns number of actually allocated pages

 *

 * @total_in is used to return the number of bytes actually read.  It

 * may be smaller than the input length if we had to exit early because we

 * ran out of room in the pages array or because we cross the

 * max_out threshold.

 *

 * @total_out is an in/out parameter, must be set to the input length and will

 * be also used to return the total number of compressed bytes

/*

 * a less complex decompression routine.  Our compressed data fits in a

 * single page, and we want to read a single page out of it.

 * start_byte tells us the offset into the compressed data we're interested in

/*

 * Copy decompressed data from working buffer to pages.

 *

 * @buf:		The decompressed data buffer

 * @buf_len:		The decompressed data length

 * @decompressed:	Number of bytes that are already decompressed inside the

 * 			compressed extent

 * @cb:			The compressed extent descriptor

 * @orig_bio:		The original bio that the caller wants to read for

 *

 * An easier to understand graph is like below:

 *

 * 		|<- orig_bio ->|     |<- orig_bio->|

 * 	|<-------      full decompressed extent      ----->|

 * 	|<-----------    @cb range   ---->|

 * 	|			|<-- @buf_len -->|

 * 	|<--- @decompressed --->|

 *

 * Note that, @cb can be a subpage of the full decompressed extent, but

 * @cb->start always has the same as the orig_file_offset value of the full

 * decompressed extent.

 *

 * When reading compressed extent, we have to read the full compressed extent,

 * while @orig_bio may only want part of the range.

 * Thus this function will ensure only data covered by @orig_bio will be copied

 * to.

 *

 * Return 0 if we have copied all needed contents for @orig_bio.

 * Return >0 if we need continue decompress.

 Offset inside the full decompressed extent */

 The main loop to do the copy */

 Offset inside the full decompressed extent */

		/*

		 * cb->start may underflow, but subtracting that value can still

		 * give us correct offset inside the full decompressed extent.

 Haven't reached the bvec range, exit */

		/*

		 * Extra range check to ensure we didn't go beyond

		 * @buf + @buf_len.

 Finished the bio */

/*

 * Shannon Entropy calculation

 *

 * Pure byte distribution analysis fails to determine compressibility of data.

 * Try calculating entropy to estimate the average minimum number of bits

 * needed to encode the sampled data.

 *

 * For convenience, return the percentage of needed bits, instead of amount of

 * bits directly.

 *

 * @ENTROPY_LVL_ACEPTABLE - below that threshold, sample has low byte entropy

 *			    and can be compressible with high probability

 *

 * @ENTROPY_LVL_HIGH - data are not compressible with high probability

 *

 * Use of ilog2() decreases precision, we lower the LVL to 5 to compensate.

/*

 * For increasead precision in shannon_entropy calculation,

 * let's do pow(n, M) to save more digits after comma:

 *

 * - maximum int bit length is 64

 * - ilog2(MAX_SAMPLE_SIZE)	-> 13

 * - 13 * 4 = 52 < 64		-> M = 4

 *

 * So use pow(n, 4).

 Reverse order */

/*

 * Use 4 bits as radix base

 * Use 16 u32 counters for calculating new position in buf array

 *

 * @array     - array that will be sorted

 * @array_buf - buffer array to store sorting results

 *              must be equal in size to @array

 * @num       - array size

	/*

	 * Try avoid useless loop iterations for small numbers stored in big

	 * counters.  Example: 48 33 4 ... in 64bit array

		/*

		 * Normal radix expects to move data from a temporary array, to

		 * the main one.  But that requires some CPU time. Avoid that

		 * by doing another sort iteration to original array instead of

		 * memcpy()

/*

 * Size of the core byte set - how many bytes cover 90% of the sample

 *

 * There are several types of structured binary data that use nearly all byte

 * values. The distribution can be uniform and counts in all buckets will be

 * nearly the same (eg. encrypted data). Unlikely to be compressible.

 *

 * Other possibility is normal (Gaussian) distribution, where the data could

 * be potentially compressible, but we have to take a few more steps to decide

 * how much.

 *

 * @BYTE_CORE_SET_LOW  - main part of byte values repeated frequently,

 *                       compression algo can easy fix that

 * @BYTE_CORE_SET_HIGH - data have uniform distribution and with high

 *                       probability is not compressible

 Sort in reverse order */

/*

 * Count byte values in buckets.

 * This heuristic can detect textual data (configs, xml, json, html, etc).

 * Because in most text-like data byte set is restricted to limited number of

 * possible characters, and that restriction in most cases makes data easy to

 * compress.

 *

 * @BYTE_SET_THRESHOLD - consider all data within this byte set size:

 *	less - compressible

 *	more - need additional analysis

	/*

	 * Continue collecting count of byte values in buckets.  If the byte

	 * set size is bigger then the threshold, it's pointless to continue,

	 * the detection technique would fail for this type of data.

	/*

	 * Compression handles the input data by chunks of 128KiB

	 * (defined by BTRFS_MAX_UNCOMPRESSED)

	 *

	 * We do the same for the heuristic and loop over the whole range.

	 *

	 * MAX_SAMPLE_SIZE - calculated under assumption that heuristic will

	 * process no more than BTRFS_MAX_UNCOMPRESSED at a time.

 Don't miss unaligned end */

 Handle case where the start is not aligned to PAGE_SIZE */

 Don't sample any garbage from the last page */

/*

 * Compression heuristic.

 *

 * For now is's a naive and optimistic 'return true', we'll extend the logic to

 * quickly (compared to direct compression) detect data characteristics

 * (compressible/uncompressible) to avoid wasting CPU time on uncompressible

 * data.

 *

 * The following types of analysis can be performed:

 * - detect mostly zero data

 * - detect data with low "byte set" size (text, etc)

 * - detect data with low/high "core byte" set

 *

 * Return non-zero if the compression should be done, 0 otherwise.

	/*

	 * For the levels below ENTROPY_LVL_HIGH, additional analysis would be

	 * needed to give green light to compression.

	 *

	 * For now just assume that compression at that level is not worth the

	 * resources because:

	 *

	 * 1. it is possible to defrag the data later

	 *

	 * 2. the data would turn out to be hardly compressible, eg. 150 byte

	 * values, every bucket has counter at level ~54. The heuristic would

	 * be confused. This can happen when data have some internal repeated

	 * patterns like "abbacbbc...". This can be detected by analyzing

	 * pairs of bytes, which is too costly.

/*

 * Convert the compression suffix (eg. after "zlib" starting with ":") to

 * level, unrecognized string will set the default level

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/*

 * Defrag all the leaves in a given btree.

 * Read all the leaves and try to get key order to

 * better reflect disk order

		/*

		 * there's recursion here right now in the tree locking,

		 * we can't defrag the extent root without deadlock

 from above we know this is not a leaf */

	/*

	 * We don't need a lock on a leaf. btrfs_realloc_node() will lock all

	 * leafs from path->nodes[1], so set lowest_level to 1 to avoid later

	 * a deadlock (attempting to write lock an already write locked leaf).

	/*

	 * The node at level 1 must always be locked when our path has

	 * keep_locks set and lowest_level is 1, regardless of the value of

	 * path->slots[1].

	/*

	 * Now that we reallocated the node we can find the next key. Note that

	 * btrfs_find_next_key() can release our path and do another search

	 * without COWing, this is because even with path->keep_locks = 1,

	 * btrfs_search_slot() / ctree.c:unlock_up() does not keeps a lock on a

	 * node when path->slots[node_level - 1] does not point to the last

	 * item or a slot beyond the last item (ctree.c:unlock_up()). Therefore

	 * we search for the next key after reallocating our node.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2008 Oracle.  All rights reserved.

/* magic values for the inode_only field in btrfs_log_inode:

 *

 * LOG_INODE_ALL means to log everything

 * LOG_INODE_EXISTS means to log just enough to recreate the inode

 * during log replay

/*

 * directory trouble cases

 *

 * 1) on rename or unlink, if the inode being unlinked isn't in the fsync

 * log, we must force a full commit before doing an fsync of the directory

 * where the unlink was done.

 * ---> record transid of last unlink/rename per directory

 *

 * mkdir foo/some_dir

 * normal commit

 * rename foo/some_dir foo2/some_dir

 * mkdir foo/some_dir

 * fsync foo/some_dir/some_file

 *

 * The fsync above will unlink the original some_dir without recording

 * it in its new location (foo2).  After a crash, some_dir will be gone

 * unless the fsync of some_file forces a full commit

 *

 * 2) we must log any new names for any file or dir that is in the fsync

 * log. ---> check inode while renaming/linking.

 *

 * 2a) we must log any new names for any file or dir during rename

 * when the directory they are being removed from was logged.

 * ---> check inode and old parent dir during rename

 *

 *  2a is actually the more important variant.  With the extra logging

 *  a crash might unlink the old name without recreating the new one

 *

 * 3) after a crash, we must go through any directories with a link count

 * of zero and redo the rm -rf

 *

 * mkdir f1/foo

 * normal commit

 * rm -rf f1/foo

 * fsync(f1)

 *

 * The directory f1 was fully removed from the FS, but fsync was never

 * called on f1, only its parent dir.  After a crash the rm -rf must

 * be replayed.  This must be able to recurse down the entire

 * directory tree.  The inode link count fixup code takes care of the

 * ugly details.

/*

 * stages for the tree walking.  The first

 * stage (0) is to only pin down the blocks we find

 * the second stage (1) is to make sure that all the inodes

 * we find in the log are created in the subvolume.

 *

 * The last stage is to deal with directories and links and extents

 * and all the other fun semantics

/*

 * tree logging is a special write ahead log used to make sure that

 * fsyncs and O_SYNCs can happen without doing full tree commits.

 *

 * Full tree commits are expensive because they require commonly

 * modified blocks to be recowed, creating many dirty pages in the

 * extent tree an 4x-6x higher write load than ext3.

 *

 * Instead of doing a tree commit on every fsync, we use the

 * key ranges and transaction ids to find items for a given file or directory

 * that have changed in this transaction.  Those items are copied into

 * a special tree (one per subvolume root), that tree is written to disk

 * and then the fsync is considered complete.

 *

 * After a crash, items are copied out of the log-tree back into the

 * subvolume tree.  Any file data extents found are recorded in the extent

 * allocation tree, and the log-tree freed.

 *

 * The log tree is read three times, once to pin down all the extents it is

 * using in ram and once, once to create all the inodes logged in the tree

 * and once to do all the other items.

/*

 * start a sub transaction and setup the log tree

 * this increments the log tree writer count to make the people

 * syncing the tree wait for us to finish

	/*

	 * First check if the log root tree was already created. If not, create

	 * it before locking the root's log_mutex, just to keep lockdep happy.

		/*

		 * This means fs_info->log_root_tree was already created

		 * for some other FS trees. Do the full commit not to mix

		 * nodes from multiple log transactions to do sequential

		 * writing.

/*

 * returns 0 if there was a log transaction running and we were able

 * to join, or returns -ENOENT if there were not transactions

 * in progress

/*

 * This either makes the current running log transaction wait

 * until you call btrfs_end_log_trans() or it makes any future

 * log transactions wait until you call btrfs_end_log_trans()

/*

 * indicate we're done making changes to the log tree

 * and wake up anyone waiting to do a sync

 atomic_dec_and_test implies a barrier */

/*

 * the walk control struct is used to pass state down the chain when

 * processing the log tree.  The stage field tells us which part

 * of the log tree processing we are currently doing.  The others

 * are state fields used for that specific part

	/* should we free the extent on disk when done?  This is used

	 * at transaction commit time while freeing a log tree

	/* should we write out the extent buffer?  This is used

	 * while flushing the log tree to disk during a sync

	/* should we wait for the extent buffer io to finish?  Also used

	 * while flushing the log tree to disk for a sync

	/* pin only walk, we record which extents on disk belong to the

	 * log trees

 what stage of the replay code we're currently in */

	/*

	 * Ignore any items from the inode currently being processed. Needs

	 * to be set every time we find a BTRFS_INODE_ITEM_KEY and we are in

	 * the LOG_WALK_REPLAY_INODES stage.

 the root we are currently replaying */

 the trans handle for the current replay */

	/* the function that gets used to process blocks we find in the

	 * tree.  Note the extent_buffer might not be up to date when it is

	 * passed in, and it must be checked or read if you need the data

	 * inside it

/*

 * process_func used to pin down extents, write them or wait on them

	/*

	 * If this fs is mixed then we need to be able to process the leaves to

	 * pin down any logged extents, so we have to read the block.

 Our caller must have done a search for the key for us. */

	/*

	 * And the slot must point to the exact key or the slot where the key

	 * should be at (the first item with a key greater than 'key')

		/*

		 * they have the same contents, just return, this saves

		 * us from cowing blocks in the destination tree and doing

		 * extra writes that may not have been done by a previous

		 * sync

		/*

		 * We need to load the old nbytes into the inode so when we

		 * replay the extents we've logged we get the right nbytes.

			/*

			 * If this is a directory we need to reset the i_size to

			 * 0 so that we can set it up properly when replaying

			 * the rest of the items in this log.

		/*

		 * New inode, set nbytes to 0 so that the nbytes comes out

		 * properly when we replay the extents.

		/*

		 * If this is a directory we need to reset the i_size to 0 so

		 * that we can set it up properly when replaying the rest of

		 * the items in this log.

 try to insert the key into the destination tree */

 make sure any existing item is the correct size */

	/* don't overwrite an existing inode if the generation number

	 * was logged as zero.  This is done when the tree logging code

	 * is just logging an inode to make sure it exists after recovery.

	 *

	 * Also, don't overwrite i_size on directories during replay.

	 * log replay inserts and removes directory items based on the

	 * state of the tree found in the subvolume, and i_size is modified

	 * as it goes

			/*

			 * For regular files an ino_size == 0 is used only when

			 * logging that an inode exists, as part of a directory

			 * fsync, and the inode wasn't fsynced before. In this

			 * case don't set the size of the inode in the fs/subvol

			 * tree, otherwise we would be throwing valid data away.

 make sure the generation is filled in */

/*

 * Item overwrite used by replay and tree logging.  eb, slot and key all refer

 * to the src data we are copying out.

 *

 * root is the tree we are copying into, and path is a scratch

 * path for use in this function (it should be released on entry and

 * will be released on exit).

 *

 * If the key is already in the destination tree the existing item is

 * overwritten.  If the existing item isn't big enough, it is extended.

 * If it is too large, it is truncated.

 *

 * If the key isn't in the destination yet, a new item is inserted.

 Look for the key in the destination tree. */

/*

 * simple helper to read an inode off the disk from a given root

 * This can only be called for subvolume roots and not for the log

/* replays a single extent in 'eb' at 'slot' with 'key' into the

 * subvolume 'root'.  path is released on entry and should be released

 * on exit.

 *

 * extents in the log tree have not been allocated out of the extent

 * tree yet.  So, this completes the allocation, taking a reference

 * as required if the extent already exists or creating a new extent

 * if it isn't in the extent allocation tree yet.

 *

 * The extent is inserted into the file, dropping any existing extents

 * from the file that overlap the new one.

		/*

		 * We don't add to the inodes nbytes if we are prealloc or a

		 * hole.

	/*

	 * first check to see if we already have this extent in the

	 * file.  This must be done before the btrfs_drop_extents run

	 * so we don't try to drop this extent.

		/*

		 * we already have a pointer to this exact extent,

		 * we don't have to do anything

 drop any overlapping extents */

		/*

		 * Manually record dirty extent, as here we did a shallow

		 * file extent item copy and skip normal backref update,

		 * but modifying extent tree all by ourselves.

		 * So need to manually record dirty extent for qgroup,

		 * as the owner of the file extent changed from log tree

		 * (doesn't affect qgroup) to fs/file tree(affects qgroup)

			/*

			 * is this extent already allocated in the extent

			 * allocation tree?  If so, just add a reference

				/*

				 * insert the extent pointer in the extent

				 * allocation tree

			/*

			 * Now delete all existing cums in the csum root that

			 * cover our range. We do this because we can have an

			 * extent that is completely referenced by one file

			 * extent item and partially referenced by another

			 * file extent item (like after using the clone or

			 * extent_same ioctls). In this case if we end up doing

			 * the replay of the one that partially references the

			 * extent first, and we do not do the csum deletion

			 * below, we can get 2 csum items in the csum tree that

			 * overlap each other. For example, imagine our log has

			 * the two following file extent items:

			 *

			 * key (257 EXTENT_DATA 409600)

			 *     extent data disk byte 12845056 nr 102400

			 *     extent data offset 20480 nr 20480 ram 102400

			 *

			 * key (257 EXTENT_DATA 819200)

			 *     extent data disk byte 12845056 nr 102400

			 *     extent data offset 0 nr 102400 ram 102400

			 *

			 * Where the second one fully references the 100K extent

			 * that starts at disk byte 12845056, and the log tree

			 * has a single csum item that covers the entire range

			 * of the extent:

			 *

			 * key (EXTENT_CSUM EXTENT_CSUM 12845056) itemsize 100

			 *

			 * After the first file extent item is replayed, the

			 * csum tree gets the following csum item:

			 *

			 * key (EXTENT_CSUM EXTENT_CSUM 12865536) itemsize 20

			 *

			 * Which covers the 20K sub-range starting at offset 20K

			 * of our extent. Now when we replay the second file

			 * extent item, if we do not delete existing csum items

			 * that cover any of its blocks, we end up getting two

			 * csum items in our csum tree that overlap each other:

			 *

			 * key (EXTENT_CSUM EXTENT_CSUM 12845056) itemsize 100

			 * key (EXTENT_CSUM EXTENT_CSUM 12865536) itemsize 20

			 *

			 * Which is a problem, because after this anyone trying

			 * to lookup up for the checksum of any block of our

			 * extent starting at an offset of 40K or higher, will

			 * end up looking at the second csum item only, which

			 * does not contain the checksum for any block starting

			 * at offset 40K or higher of our extent.

 inline extents are easy, we just overwrite them */

/*

 * when cleaning up conflicts between the directory names in the

 * subvolume, directory names in the log and directory names in the

 * inode back references, we may have to unlink inodes from directories.

 *

 * This is a helper function to do the unlink of a specific directory

 * item

/*

 * See if a given name and sequence number found in an inode back reference are

 * already in a directory and correctly point to this inode.

 *

 * Returns: < 0 on error, 0 if the directory entry does not exists and 1 if it

 * exists.

/*

 * helper function to check a log tree for a named back reference in

 * an inode.  This is used to decide if a back reference that is

 * found in the subvolume conflicts with what we find in the log.

 *

 * inode backreferences may have multiple refs in a single item,

 * during replay we process one reference at a time, and we don't

 * want to delete valid links to a file from the subvolume if that

 * link is also in the log.

 Search old style refs */

		/* are we trying to overwrite a back ref for the root directory

		 * if so, just jump out, we're done

		/* check all the names in this back reference to see

		 * if they are in the log.  if so, we allow them to stay

		 * otherwise they must be unlinked as a conflict

		/*

		 * NOTE: we have searched root tree and checked the

		 * corresponding ref, it does not need to check again.

 Same search but for extended refs */

 look for a conflicting sequence number */

 look for a conflicting name */

/*

 * Take an inode reference item from the log tree and iterate all names from the

 * inode reference item in the subvolume tree with the same key (if it exists).

 * For any name that is not in the inode reference item from the log tree, do a

 * proper unlink of that name (that is, remove its entry from the inode

 * reference item and both dir index keys).

	/*

	 * Our inode's dentry collides with the dentry of another inode which is

	 * in the log but not yet processed since it has a higher inode number.

	 * So delete that other dentry.

	/*

	 * If we dropped the link count to 0, bump it so that later the iput()

	 * on the inode will not free it. We will fixup the link count later.

/*

 * replay one inode back reference item found in the log tree.

 * eb, slot and key refer to the buffer and key found in the log tree.

 * root is the destination we are replaying into, and path is for temp

 * use by this function.  (it should be released on return).

	/*

	 * it is possible that we didn't log all the parent directories

	 * for a given inode.  If we don't find the dir, just don't

	 * copy the back ref in.  The link count fixup code will take

	 * care of the rest

			/*

			 * parent object can change from one array

			 * item to another.

			/*

			 * look for a conflicting back reference in the

			 * metadata. if we find one we have to unlink that name

			 * of the file before we add our new link.  Later on, we

			 * overwrite any existing back reference, and we don't

			 * want to create dangling pointers in the directory.

			/*

			 * If a reference item already exists for this inode

			 * with the same parent and name, but different index,

			 * drop it and the corresponding directory index entries

			 * from the parent before adding the new reference item

			 * and dir index entries, otherwise we would fail with

			 * -EEXIST returned from btrfs_add_link() below.

				/*

				 * If we dropped the link count to 0, bump it so

				 * that later the iput() on the inode will not

				 * free it. We will fixup the link count later.

 insert our name */

 Else, ret == 1, we already have a perfect match, we're done. */

	/*

	 * Before we overwrite the inode reference item in the subvolume tree

	 * with the item from the log tree, we must unlink all names from the

	 * parent directory that are in the subvolume's tree inode reference

	 * item, otherwise we end up with an inconsistent subvolume tree where

	 * dir index entries exist for a name but there is no inode reference

	 * item with the same name.

 finally write the back reference in the inode */

/*

 * There are a few corners where the link count of the file can't

 * be properly maintained during replay.  So, instead of adding

 * lots of complexity to the log code, we just scan the backrefs

 * for any file that has been through replay.

 *

 * The scan will update the link count on the inode to reflect the

 * number of back refs found.  If it goes down to zero, the iput

 * will free the inode.

		/*

		 * fixup on a directory may create new entries,

		 * make sure we always look for the highset possible

		 * offset

/*

 * record a given inode in the fixup dir so we can check its link

 * count when replay is done.  The link count is incremented here

 * so the inode won't go away until we check it

/*

 * when replaying the log for a directory, we only insert names

 * for inodes that actually exist.  This means an fsync on a directory

 * does not implicitly fsync all the new files in it

 FIXME, put inode into FIXUP list */

/*

 * take a single entry in a log directory item and replay it into

 * the subvolume.

 *

 * if a conflicting item exists in the subdirectory already,

 * the inode it points to is unlinked and put into the link count

 * fix up tree.

 *

 * If a name from the log points to a file or directory that does

 * not exist in the FS, it is skipped.  fsyncs on directories

 * do not force down inodes inside that directory, just changes to the

 * names or unlinks in a directory.

 *

 * Returns < 0 on error, 0 if the name wasn't replayed (dentry points to a

 * non-existing inode) and 1 if the name was replayed.

 Corruption */

		/* we need a sequence number to insert, so we only

		 * do inserts for the BTRFS_DIR_INDEX_KEY types

 the existing item matches the logged item */

	/*

	 * don't drop the conflicting directory entry if the inode

	 * for the new entry doesn't exist

	/*

	 * Check if the inode reference exists in the log for the given name,

	 * inode and parent inode

 The dentry will be added later. */

 The dentry will be added later. */

/*

 * find all the names in a directory item and reconcile them into

 * the subvolume.  Only BTRFS_DIR_ITEM_KEY types will have more than

 * one name in a directory item, but the same code gets used for

 * both directory index types

		/*

		 * If this entry refers to a non-directory (directories can not

		 * have a link count > 1) and it was added in the transaction

		 * that was not committed, make sure we fixup the link count of

		 * the inode it the entry points to. Otherwise something like

		 * the following would result in a directory pointing to an

		 * inode with a wrong link that does not account for this dir

		 * entry:

		 *

		 * mkdir testdir

		 * touch testdir/foo

		 * touch testdir/bar

		 * sync

		 *

		 * ln testdir/bar testdir/bar_link

		 * ln testdir/foo testdir/foo_link

		 * xfs_io -c "fsync" testdir/bar

		 *

		 * <power failure>

		 *

		 * mount fs, log replay happens

		 *

		 * File foo would remain with a link count of 1 when it has two

		 * entries pointing to it in the directory testdir. This would

		 * make it impossible to ever delete the parent directory has

		 * it would result in stale dentries that can never be deleted.

/*

 * directory replay has two parts.  There are the standard directory

 * items in the log copied from the subvolume, and range items

 * created in the log while the subvolume was logged.

 *

 * The range items tell us which parts of the key space the log

 * is authoritative for.  During replay, if a key in the subvolume

 * directory is in a logged range item, but not actually in the log

 * that means it was deleted from the directory before the fsync

 * and should be removed.

 check the next slot in the tree to see if it is a valid item */

/*

 * this looks for a given directory item in the log.  If the directory

 * item is not in the log, the item is removed and the inode it points

 * to is unlinked

			/* there might still be more names under this key

			 * check and repeat if required

 Doesn't exist in log tree, so delete it. */

/*

 * deletion replay happens before we copy any new directory items

 * out of the log or out of backreferences from inodes.  It

 * scans the log to find ranges of keys that log is authoritative for,

 * and then scans the directory to find items in those ranges that are

 * not present in the log.

 *

 * Anything we don't find in the log is unlinked and removed from the

 * directory.

	/* it isn't an error if the inode isn't there, that can happen

	 * because we replay the deletes before we copy in the inode item

	 * from the log

/*

 * the process_func used to replay items from the log tree.  This

 * gets called in two different stages.  The first stage just looks

 * for inodes and makes sure they are all copied into the subvolume.

 *

 * The second stage copies all the other item types from the log into

 * the subvolume.  The two stage approach is slower, but gets rid of

 * lots of complexity around inodes referencing other inodes that exist

 * only in the log (references come from either directory items or inode

 * back refs).

 inode keys are done during the first stage */

			/*

			 * If we have a tmpfile (O_TMPFILE) that got fsync'ed

			 * and never got linked before the fsync, skip it, as

			 * replaying it is pointless since it would be deleted

			 * later. We skip logging tmpfiles, but it's always

			 * possible we are replaying a log created with a kernel

			 * that used to log tmpfiles.

			/*

			 * Before replaying extents, truncate the inode to its

			 * size. We need to do it now and not after log replay

			 * because before an fsync we can have prealloc extents

			 * added beyond the inode's i_size. If we did it after,

			 * through orphan cleanup for example, we would drop

			 * those prealloc extents just after replaying them.

 Update the inode's nbytes. */

 these keys are simply copied */

/*

 * Correctly adjust the reserved bytes occupied by a log tree extent buffer

/*

 * drop the reference count on the tree rooted at 'snap'.  This traverses

 * the tree freeing any blocks that have a ref count of zero after being

 * decremented.

 was the root node processed? if not, catch it here */

/*

 * helper function to update the item for a given subvolumes log root

 * in the tree of log roots

 insert root item on the first sync */

	/*

	 * we only allow two pending log transactions at a time,

	 * so we know that if ours is more than 2 older than the

	 * current transaction, we're done

/* 

 * Invoked in log mutex context, or be sure there is no other task which

 * can access the list.

/*

 * btrfs_sync_log does sends a given tree log down to the disk and

 * updates the super blocks to record it.  When this call is done,

 * you know that any inodes previously logged are safely on disk only

 * if it returns 0.

 *

 * Any other return value means you need to call btrfs_commit_transaction.

 * Some of the edge cases for fsyncing directories that have had unlinks

 * or renames done in the past mean that sometimes the only safe

 * fsync is to commit the whole FS.  When btrfs_sync_log returns -EAGAIN,

 * that has happened.

 wait for previous tree log sync to complete */

 when we're on an ssd, just kick the log commit out */

 bail out if we need to do a full commit */

	/* we start IO on  all the marked extents here, but we don't actually

	 * wait for them until later.

	/*

	 * -EAGAIN happens when someone, e.g., a concurrent transaction

	 *  commit, writes a dirty extent in this tree-log commit. This

	 *  concurrent write will create a hole writing out the extents,

	 *  and we cannot proceed on a zoned filesystem, requiring

	 *  sequential writing. While we can bail out to a full commit

	 *  here, but we can continue hoping the concurrent writing fills

	 *  the hole.

	/*

	 * We _must_ update under the root->log_mutex in order to make sure we

	 * have a consistent view of the log root we are trying to commit at

	 * this moment.

	 *

	 * We _must_ copy this into a local copy, because we are not holding the

	 * log_root_tree->log_mutex yet.  This is important because when we

	 * commit the log_root_tree we must have a consistent view of the

	 * log_root_tree when we update the super block to point at the

	 * log_root_tree bytenr.  If we update the log_root_tree here we'll race

	 * with the commit and possibly point at the new block which we may not

	 * have written out.

	/*

	 * IO has been started, blocks of the log tree have WRITTEN flag set

	 * in their headers. new modifications of the log will be written to

	 * new positions. so it's safe to allow log writers to go in.

	/*

	 * Now we are safe to update the log_root_tree because we're under the

	 * log_mutex, and we're a current writer so we're holding the commit

	 * open until we drop the log_mutex.

	/*

	 * now that we've moved on to the tree of log tree roots,

	 * check the full commit flag again

	/*

	 * As described above, -EAGAIN indicates a hole in the extents. We

	 * cannot wait for these write outs since the waiting cause a

	 * deadlock. Bail out to the full commit instead.

	/*

	 * Here we are guaranteed that nobody is going to write the superblock

	 * for the current transaction before us and that neither we do write

	 * our superblock before the previous transaction finishes its commit

	 * and writes its superblock, because:

	 *

	 * 1) We are holding a handle on the current transaction, so no body

	 *    can commit it until we release the handle;

	 *

	 * 2) Before writing our superblock we acquire the tree_log_mutex, so

	 *    if the previous transaction is still committing, and hasn't yet

	 *    written its superblock, we wait for it to do it, because a

	 *    transaction commit acquires the tree_log_mutex when the commit

	 *    begins and releases it only after writing its superblock.

	/*

	 * The previous transaction writeout phase could have failed, and thus

	 * marked the fs in an error state.  We must not commit here, as we

	 * could have updated our generation in the super_for_commit and

	 * writing the super here would result in transid mismatches.  If there

	 * is an error here just bail.

	/*

	 * We know there can only be one task here, since we have not yet set

	 * root->log_commit[index1] to 0 and any task attempting to sync the

	 * log must wait for the previous log transaction to commit if it's

	 * still in progress or wait for the current log transaction commit if

	 * someone else already started it. We use <= and not < because the

	 * first log transaction has an ID of 0.

	/*

	 * The barrier before waitqueue_active (in cond_wake_up) is needed so

	 * all the updates above are seen by the woken threads. It might not be

	 * necessary, but proving that seems to be hard.

	/*

	 * The barrier before waitqueue_active (in cond_wake_up) is needed so

	 * all the updates above are seen by the woken threads. It might not be

	 * necessary, but proving that seems to be hard.

/*

 * free all the extents used by the tree log.  This should be called

 * at commit time of the full transaction

/*

 * Check if an inode was logged in the current transaction. This may often

 * return some false positives, because logged_trans is an in memory only field,

 * not persisted anywhere. This is meant to be used in contexts where a false

 * positive has no functional consequences.

	/*

	 * The inode's logged_trans is always 0 when we load it (because it is

	 * not persisted in the inode item or elsewhere). So if it is 0, the

	 * inode was last modified in the current transaction then the inode may

	 * have been logged before in the current transaction, then evicted and

	 * loaded again in the current transaction - or may have never been logged

	 * in the current transaction, but since we can not be sure, we have to

	 * assume it was, otherwise our callers can leave an inconsistent log.

/*

 * If both a file and directory are logged, and unlinks or renames are

 * mixed in, we have a few interesting corners:

 *

 * create file X in dir Y

 * link file X to X.link in dir Y

 * fsync file X

 * unlink file X but leave X.link

 * fsync dir Y

 *

 * After a crash we would expect only X.link to exist.  But file X

 * didn't get fsync'd again so the log has back refs for X and X.link.

 *

 * We solve this by removing directory entries and inode backrefs from the

 * log when a file that was logged in the current transaction is

 * unlinked.  Any later fsync will include the updated log entries, and

 * we'll be able to reconstruct the proper directory items from backrefs.

 *

 * This optimizations allows us to avoid relogging the entire inode

 * or the entire directory.

	/*

	 * We do not need to update the size field of the directory's inode item

	 * because on log replay we update the field to reflect all existing

	 * entries in the directory (see overwrite_item()).

 see comments for btrfs_del_dir_entries_in_log */

/*

 * creates a range item in the log for 'dirid'.  first_offset and

 * last_offset tell us which parts of the key space the log should

 * be considered authoritative for.

	/*

	 * Copy all the items in bulk, in a single copy operation. Item data is

	 * organized such that it's placed at the end of a leaf and from right

	 * to left. For example, the data for the second item ends at an offset

	 * that matches the offset where the data for the first item starts, the

	 * data for the third item ends at an offset that matches the offset

	 * where the data of the second items starts, and so on.

	 * Therefore our source and destination start offsets for copy match the

	 * offsets of the last items (highest slots).

		/*

		 * We must make sure that when we log a directory entry, the

		 * corresponding inode, after log replay, has a matching link

		 * count. For example:

		 *

		 * touch foo

		 * mkdir mydir

		 * sync

		 * ln foo mydir/bar

		 * xfs_io -c "fsync" mydir

		 * <crash>

		 * <mount fs and log replay>

		 *

		 * Would result in a fsync log that when replayed, our file inode

		 * would have a link count of 1, but we get two directory entries

		 * pointing to the same inode. After removing one of the names,

		 * it would not be possible to remove the other name, which

		 * resulted always in stale file handle errors, and would not be

		 * possible to rmdir the parent directory, since its i_size could

		 * never be decremented to the value BTRFS_EMPTY_DIR_SIZE,

		 * resulting in -ENOTEMPTY errors.

		/*

		 * If we were logged before and have logged dir items, we can skip

		 * checking if any item with a key offset larger than the last one

		 * we logged is in the log tree, saving time and avoiding adding

		 * contention on the log tree.

		/*

		 * Check if the key was already logged before. If not we can add

		 * it to a batch for bulk insertion.

		/*

		 * Item exists in the log. Overwrite the item in the log if it

		 * has different content or do nothing if it has exactly the same

		 * content. And then flush the current batch if any - do it after

		 * overwriting the current item, or we would deadlock otherwise,

		 * since we are holding a path for the existing item.

/*

 * log all the items included in the current transaction for a given

 * directory.  This also creates the range items in the log tree required

 * to replay anything deleted before the fsync

	/*

	 * we didn't find anything from this transaction, see if there

	 * is anything at all

		/* if ret == 0 there are items for this type,

		 * create a range to tell us the last key of this type.

		 * otherwise, there are no items in this directory after

		 * *min_offset, and we create a range to indicate that.

 go backward to find any previous key */

	/*

	 * Find the first key from this transaction again.  See the note for

	 * log_new_dir_dentries, if we're logging a directory recursively we

	 * won't be holding its i_mutex, which means we can modify the directory

	 * while we're logging it.  If we remove an entry between our first

	 * search and this search we'll not find the key again and can just

	 * bail.

	/*

	 * we have a block from this transaction, log every item in it

	 * from our directory

		/*

		 * look ahead to the next item and see if it is also

		 * from this directory and from this transaction

		/*

		 * insert the log range keys to indicate where the log

		 * is valid

/*

 * logging directories is very similar to logging inodes, We find all the items

 * from the current transaction and write them to the log.

 *

 * The recovery code scans the directory in the subvolume, and if it finds a

 * key in the range logged that is not present in the log tree, then it means

 * that dir entry was unlinked during the transaction.

 *

 * In order for that scan to work, we must include one key smaller than

 * the smallest logged by this transaction and one key larger than the largest

 * key logged by this transaction.

	/*

	 * If this is the first time we are being logged in the current

	 * transaction, or we were logged before but the inode was evicted and

	 * reloaded later, in which case its logged_trans is 0, reset the values

	 * of the last logged key offsets. Note that we don't use the helper

	 * function inode_logged() here - that is because the function returns

	 * true after an inode eviction, assuming the worst case as it can not

	 * know for sure if the inode was logged before. So we can not skip key

	 * searches in the case the inode was evicted, because it may not have

	 * been logged in this transaction and may have been logged in a past

	 * transaction, so we need to reset the last dir item and index offsets

	 * to (u64)-1.

/*

 * a helper function to drop items from the log before we relog an

 * inode.  max_key_type indicates the highest item type to remove.

 * This cannot be run for file data extents because it does not

 * free the extents they point to.

 Logic error */

		/*

		 * If start slot isn't 0 then we don't need to re-search, we've

		 * found the last guy with the objectid in this tree.

		/* set the generation to zero so the recover code

		 * can tell the difference between an logging

		 * just to say 'this inode exists' and a logging

		 * to say 'update this inode with these values'

	/*

	 * We do not need to set the nbytes field, in fact during a fast fsync

	 * its value may not even be correct, since a fast fsync does not wait

	 * for ordered extent completion, which is where we update nbytes, it

	 * only waits for writeback to complete. During log replay as we find

	 * file extent items and replay them, we adjust the nbytes field of the

	 * inode item in subvolume tree as needed (see overwrite_item()).

	/*

	 * If we are doing a fast fsync and the inode was logged before in the

	 * current transaction, then we know the inode was previously logged and

	 * it exists in the log tree. For performance reasons, in this case use

	 * btrfs_search_slot() directly with ins_len set to 0 so that we never

	 * attempt a write lock on the leaf's parent, which adds unnecessary lock

	 * contention in case there are concurrent fsyncs for other inodes of the

	 * same subvolume. Using btrfs_insert_empty_item() when the inode item

	 * already exists can also result in unnecessarily splitting a leaf.

		/*

		 * This means it is the first fsync in the current transaction,

		 * so the inode item is not in the log and we need to insert it.

		 * We can never get -EEXIST because we are only called for a fast

		 * fsync and in case an inode eviction happens after the inode was

		 * logged before in the current transaction, when we load again

		 * the inode, we set BTRFS_INODE_NEEDS_FULL_SYNC on its runtime

		 * flags and set ->logged_trans to 0.

	/*

	 * If this inode was not used for reflink operations in the current

	 * transaction with new extents, then do the fast path, no need to

	 * worry about logging checksum items with overlapping ranges.

	/*

	 * Serialize logging for checksums. This is to avoid racing with the

	 * same checksum being logged by another task that is logging another

	 * file which happens to refer to the same extent as well. Such races

	 * can leave checksum items in the log with overlapping ranges.

	/*

	 * Due to extent cloning, we might have logged a csum item that covers a

	 * subrange of a cloned extent, and later we can end up logging a csum

	 * item for a larger subrange of the same extent or the entire range.

	 * This would leave csum items in the log tree that cover the same range

	 * and break the searches for checksums in the log tree, resulting in

	 * some checksums missing in the fs/subvolume tree. So just delete (or

	 * trim and adjust) any existing csum items in the log for this range.

		/* take a reference on file data extents so that truncates

		 * or deletes of this inode don't have to relog the inode

		 * again

 ds == 0 is a hole */

	/*

	 * we have to do this after the loop above to avoid changing the

	 * log tree while trying to change the log tree.

		/*

		 * We are going to copy all the csums on this ordered extent, so

		 * go ahead and adjust mod_start and mod_len in case this ordered

		 * extent has already been logged.

			/*

			 * If we have this case

			 *

			 * |--------- logged extent ---------|

			 *       |----- ordered extent ----|

			 *

			 * Just don't mess with mod_start and mod_len, we'll

			 * just end up logging more csums than we need and it

			 * will be ok.

		/*

		 * To keep us from looping for the above case of an ordered

		 * extent that falls inside of the logged extent.

 We're done, found all csums in the ordered extents. */

 If we're compressed we have to save the entire range of csums. */

 block start is already adjusted for the file extent offset. */

	/*

	 * If this is the first time we are logging the inode in the current

	 * transaction, we can avoid btrfs_drop_extents(), which is expensive

	 * because it does a deletion search, which always acquires write locks

	 * for extent buffers at levels 2, 1 and 0. This not only wastes time

	 * but also adds significant contention in a log tree, since log trees

	 * are small, with a root at level 2 or 3 at most, due to their short

	 * life span.

/*

 * Log all prealloc extents beyond the inode's i_size to make sure we do not

 * lose them after doing a fast fsync and replaying the log. We scan the

 * subvolume's root instead of iterating the inode's extent map tree because

 * otherwise we can log incorrect extent items based on extent map conversion.

 * That can happen due to the fact that extent maps are merged when they

 * are not in the extent map tree's list of modified extents.

	/*

	 * We must check if there is a prealloc extent that starts before the

	 * i_size and crosses the i_size boundary. This is to ensure later we

	 * truncate down to the end of that extent and not to the i_size, as

	 * otherwise we end up losing part of the prealloc extent after a log

	 * replay and with an implicit hole if there is another prealloc extent

	 * that starts at an offset beyond i_size.

			/*

			 * Avoid logging extent items logged in past fsync calls

			 * and leading to duplicate keys in the log tree.

		/*

		 * Just an arbitrary number, this can be really CPU intensive

		 * once we start getting a lot of extents, and really once we

		 * have a bunch of extents we just want to commit since it will

		 * be faster.

 We log prealloc extents beyond eof later. */

 Need a ref to keep it from getting evicted from cache */

		/*

		 * If we had an error we just need to delete everybody from our

		 * private list.

	/*

	 * We have logged all extents successfully, now make sure the commit of

	 * the current transaction waits for the ordered extents to complete

	 * before it commits and wipes out the log trees, otherwise we would

	 * lose data if an ordered extents completes after the transaction

	 * commits and a power failure happens after the transaction commit.

		/*

		 * If the in-memory inode's i_size is smaller then the inode

		 * size stored in the btree, return the inode's i_size, so

		 * that we get a correct inode size after replaying the log

		 * when before a power failure we had a shrinking truncate

		 * followed by addition of a new name (rename / new hard link).

		 * Otherwise return the inode size from the btree, to avoid

		 * data loss when replaying a log due to previously doing a

		 * write that expands the inode's size and logging a new name

		 * immediately after.

/*

 * At the moment we always log all xattrs. This is to figure out at log replay

 * time which xattrs must have their deletion replayed. If a xattr is missing

 * in the log tree and exists in the fs/subvol tree, we delete it. This is

 * because if a xattr is deleted, the inode is fsynced and a power failure

 * happens, causing the log to be replayed the next time the fs is mounted,

 * we want the xattr to not exist anymore (same behaviour as other filesystems

 * with a journal, ext3/4, xfs, f2fs, etc).

/*

 * When using the NO_HOLES feature if we punched a hole that causes the

 * deletion of entire leafs or all the extent items of the first leaf (the one

 * that contains the inode item and references) we may end up not processing

 * any extents, because there are no leafs with a generation matching the

 * current transaction that have extent items for our inode. So we need to find

 * if any holes exist and then log them. We also need to log holes after any

 * truncate operation that changes the inode's size.

 We have a hole, log it. */

			/*

			 * Release the path to avoid deadlocks with other code

			 * paths that search the root while holding locks on

			 * leafs from the log root.

			/*

			 * Search for the same key again in the root. Since it's

			 * an extent item and we are holding the inode lock, the

			 * key must still exist. If it doesn't just emit warning

			 * and return an error to fall back to a transaction

			 * commit.

/*

 * When we are logging a new inode X, check if it doesn't have a reference that

 * matches the reference from some other inode Y created in a past transaction

 * and that was renamed in the current transaction. If we don't do this, then at

 * log replay time we can lose inode Y (and all its files if it's a directory):

 *

 * mkdir /mnt/x

 * echo "hello world" > /mnt/x/foobar

 * sync

 * mv /mnt/x /mnt/y

 * mkdir /mnt/x                 # or touch /mnt/x

 * xfs_io -c fsync /mnt/x

 * <power fail>

 * mount fs, trigger log replay

 *

 * After the log replay procedure, we would lose the first directory and all its

 * files (file foobar).

 * For the case where inode Y is not a directory we simply end up losing it:

 *

 * echo "123" > /mnt/foo

 * sync

 * mv /mnt/foo /mnt/bar

 * echo "abc" > /mnt/foo

 * xfs_io -c fsync /mnt/foo

 * <power fail>

 *

 * We also need this for cases where a snapshot entry is replaced by some other

 * entry (file or directory) otherwise we end up with an unreplayable log due to

 * attempts to delete the snapshot entry (entry of type BTRFS_ROOT_ITEM_KEY) as

 * if it were a regular entry:

 *

 * mkdir /mnt/x

 * btrfs subvolume snapshot /mnt /mnt/x/snap

 * btrfs subvolume delete /mnt/x/snap

 * rmdir /mnt/x

 * mkdir /mnt/x

 * fsync /mnt/x or fsync some new file inside it

 * <power fail>

 *

 * The snapshot delete, rmdir of x, mkdir of a new x and the fsync all happen in

 * the same transaction.

		/*

		 * If the other inode that had a conflicting dir entry was

		 * deleted in the current transaction, we need to log its parent

		 * directory.

		/*

		 * If the inode was already logged skip it - otherwise we can

		 * hit an infinite loop. Example:

		 *

		 * From the commit root (previous transaction) we have the

		 * following inodes:

		 *

		 * inode 257 a directory

		 * inode 258 with references "zz" and "zz_link" on inode 257

		 * inode 259 with reference "a" on inode 257

		 *

		 * And in the current (uncommitted) transaction we have:

		 *

		 * inode 257 a directory, unchanged

		 * inode 258 with references "a" and "a2" on inode 257

		 * inode 259 with reference "zz_link" on inode 257

		 * inode 261 with reference "zz" on inode 257

		 *

		 * When logging inode 261 the following infinite loop could

		 * happen if we don't skip already logged inodes:

		 *

		 * - we detect inode 258 as a conflicting inode, with inode 261

		 *   on reference "zz", and log it;

		 *

		 * - we detect inode 259 as a conflicting inode, with inode 258

		 *   on reference "a", and log it;

		 *

		 * - we detect inode 258 as a conflicting inode, with inode 259

		 *   on reference "zz_link", and log it - again! After this we

		 *   repeat the above steps forever.

		/*

		 * Check the inode's logged_trans only instead of

		 * btrfs_inode_in_log(). This is because the last_log_commit of

		 * the inode is not updated when we only log that it exists (see

		 * btrfs_log_inode()).

		/*

		 * We are safe logging the other inode without acquiring its

		 * lock as long as we log with the LOG_INODE_EXISTS mode. We

		 * are safe against concurrent renames of the other inode as

		 * well because during a rename we pin the log and update the

		 * log with the new name before we unpin it.

 Note, ins_nr might be > 0 here, cleanup outside the loop */

 Skip xattrs, we log them later with btrfs_log_all_xattrs() */

/* log a single inode in the tree log.

 * At least one parent directory for this inode must exist in the tree

 * or be logged already.

 *

 * Any items from this inode changed by the current transaction are copied

 * to the log tree.  An extra reference is taken on any extents in this

 * file, allowing us to avoid a whole pile of corner cases around logging

 * blocks that have been removed from the tree.

 *

 * See LOG_INODE_ALL and related defines for a description of what inode_only

 * does.

 *

 * This handles both files and directories.

 today the code can only do partial logging of directories */

	/*

	 * Only run delayed items if we are a directory. We want to make sure

	 * all directory indexes hit the fs/subvolume tree so we can find them

	 * and figure out which index ranges have to be logged.

	/*

	 * This is for cases where logging a directory could result in losing a

	 * a file after replaying the log. For example, if we move a file from a

	 * directory A to a directory B, then fsync directory A, we have no way

	 * to known the file was moved from A to B, so logging just A would

	 * result in losing the file after a log replay.

	/*

	 * a brute force approach to making sure we get the most uptodate

	 * copies of everything.

			/*

			 * Make sure the new inode item we write to the log has

			 * the same isize as the current one (if it exists).

			 * This is necessary to prevent data loss after log

			 * replay, and also to prevent doing a wrong expanding

			 * truncate - for e.g. create file, write 4K into offset

			 * 0, fsync, write 4K into offset 4096, add hard link,

			 * fsync some other file (to sync log), power fail - if

			 * we use the inode's current i_size, after log replay

			 * we get a 8Kb file, with the last 4Kb extent as a hole

			 * (zeroes), as if an expanding truncate happened,

			 * instead of getting a file of 4Kb only.

		/*

		 * If we are doing a fast fsync and the inode was logged before

		 * in this transaction, we don't need to log the xattrs because

		 * they were logged before. If xattrs were added, changed or

		 * deleted since the last time we logged the inode, then we have

		 * already logged them because the inode had the runtime flag

		 * BTRFS_INODE_COPY_EVERYTHING set.

	/*

	 * Don't update last_log_commit if we logged that an inode exists.

	 * We do this for three reasons:

	 *

	 * 1) We might have had buffered writes to this inode that were

	 *    flushed and had their ordered extents completed in this

	 *    transaction, but we did not previously log the inode with

	 *    LOG_INODE_ALL. Later the inode was evicted and after that

	 *    it was loaded again and this LOG_INODE_EXISTS log operation

	 *    happened. We must make sure that if an explicit fsync against

	 *    the inode is performed later, it logs the new extents, an

	 *    updated inode item, etc, and syncs the log. The same logic

	 *    applies to direct IO writes instead of buffered writes.

	 *

	 * 2) When we log the inode with LOG_INODE_EXISTS, its inode item

	 *    is logged with an i_size of 0 or whatever value was logged

	 *    before. If later the i_size of the inode is increased by a

	 *    truncate operation, the log is synced through an fsync of

	 *    some other inode and then finally an explicit fsync against

	 *    this inode is made, we must make sure this fsync logs the

	 *    inode with the new i_size, the hole between old i_size and

	 *    the new i_size, and syncs the log.

	 *

	 * 3) If we are logging that an ancestor inode exists as part of

	 *    logging a new name from a link or rename operation, don't update

	 *    its last_log_commit - otherwise if an explicit fsync is made

	 *    against an ancestor, the fsync considers the inode in the log

	 *    and doesn't sync the log, resulting in the ancestor missing after

	 *    a power failure unless the log was synced as part of an fsync

	 *    against any other unrelated inode.

/*

 * Check if we need to log an inode. This is used in contexts where while

 * logging an inode we need to log another inode (either that it exists or in

 * full mode). This is used instead of btrfs_inode_in_log() because the later

 * requires the inode to be in the log and have the log transaction committed,

 * while here we do not care if the log transaction was already committed - our

 * caller will commit the log later - and we want to avoid logging an inode

 * multiple times when multiple tasks have joined the same log transaction.

	/*

	 * If a directory was not modified, no dentries added or removed, we can

	 * and should avoid logging it.

	/*

	 * If this inode does not have new/updated/deleted xattrs since the last

	 * time it was logged and is flagged as logged in the current transaction,

	 * we can skip logging it. As for new/deleted names, those are updated in

	 * the log by link/unlink/rename operations.

	 * In case the inode was logged and then evicted and reloaded, its

	 * logged_trans will be 0, in which case we have to fully log it since

	 * logged_trans is a transient field, not persisted.

/*

 * Log the inodes of the new dentries of a directory. See log_dir_items() for

 * details about the why it is needed.

 * This is a recursive operation - if an existing dentry corresponds to a

 * directory, that directory's new entries are logged too (same behaviour as

 * ext3/4, xfs, f2fs, reiserfs, nilfs2). Note that when logging the inodes

 * the dentries point to we do not lock their i_mutex, otherwise lockdep

 * complains about the following circular lock dependency / possible deadlock:

 *

 *        CPU0                                        CPU1

 *        ----                                        ----

 * lock(&type->i_mutex_dir_key#3/2);

 *                                            lock(sb_internal#2);

 *                                            lock(&type->i_mutex_dir_key#3/2);

 * lock(&sb->s_type->i_mutex_key#14);

 *

 * Where sb_internal is the lock (a counter that works as a lock) acquired by

 * sb_start_intwrite() in btrfs_start_transaction().

 * Not locking i_mutex of the inodes is still safe because:

 *

 * 1) For regular files we log with a mode of LOG_INODE_EXISTS. It's possible

 *    that while logging the inode new references (names) are added or removed

 *    from the inode, leaving the logged inode item with a link count that does

 *    not match the number of logged inode reference items. This is fine because

 *    at log replay time we compute the real number of links and correct the

 *    link count in the inode item (see replay_one_buffer() and

 *    link_to_fixup_dir());

 *

 * 2) For directories we log with a mode of LOG_INODE_ALL. It's possible that

 *    while logging the inode's items new items with keys BTRFS_DIR_ITEM_KEY and

 *    BTRFS_DIR_INDEX_KEY are added to fs/subvol tree and the logged inode item

 *    has a size that doesn't match the sum of the lengths of all the logged

 *    names. This does not result in a problem because if a dir_item key is

 *    logged but its matching dir_index key is not logged, at log replay time we

 *    don't use it to replay the respective name (see replay_one_name()). On the

 *    other hand if only the dir_index key ends up being logged, the respective

 *    name is added to the fs/subvol tree with both the dir_item and dir_index

 *    keys created (see replay_one_name()).

 *    The directory's inode item with a wrong i_size is not a problem as well,

 *    since we don't use it at log replay time to set the i_size in the inode

 *    item of the fs/subvol tree (see overwrite_item()).

	/*

	 * If we are logging a new name, as part of a link or rename operation,

	 * don't bother logging new dentries, as we just want to log the names

	 * of an inode and that any new parents exist.

 BTRFS_INODE_EXTREF_KEY is BTRFS_INODE_REF_KEY + 1 */

			/*

			 * If the parent inode was deleted, return an error to

			 * fallback to a transaction commit. This is to prevent

			 * getting an inode that was moved from one parent A to

			 * a parent B, got its former parent A deleted and then

			 * it got fsync'ed, from existing at both parents after

			 * a log replay (and the old parent still existing).

			 * Example:

			 *

			 * mkdir /mnt/A

			 * mkdir /mnt/B

			 * touch /mnt/B/bar

			 * sync

			 * mv /mnt/B/bar /mnt/A/bar

			 * mv -T /mnt/A /mnt/B

			 * fsync /mnt/B/bar

			 * <power fail>

			 *

			 * If we ignore the old parent B which got deleted,

			 * after a log replay we would have file bar linked

			 * at both parents and the old parent B would still

			 * exist.

	/*

	 * For a single hard link case, go through a fast path that does not

	 * need to iterate the fs/subvolume tree.

		/*

		 * Don't deal with extended references because they are rare

		 * cases and too complex to deal with (we would need to keep

		 * track of which subitem we are processing for each item in

		 * this loop, etc). So just return some error to fallback to

		 * a transaction commit.

		/*

		 * Logging ancestors needs to do more searches on the fs/subvol

		 * tree, so it releases the path as needed to avoid deadlocks.

		 * Keep track of the last inode ref key and resume from that key

		 * after logging all new ancestors for the current hard link.

/*

 * helper function around btrfs_log_inode to make sure newly created

 * parent directories also end up in the log.  A minimal inode and backref

 * only logging is done of any parent directories that are older than

 * the last committed transaction

	/*

	 * Skip already logged inodes or inodes corresponding to tmpfiles

	 * (since logging them is pointless, a link count of 0 means they

	 * will never be accessible).

	/*

	 * for regular files, if its inode is already on disk, we don't

	 * have to worry about the parents at all.  This is because

	 * we can use the last_unlink_trans field to record renames

	 * and other fun in this file.

	/*

	 * On unlink we must make sure all our current and old parent directory

	 * inodes are fully logged. This is to prevent leaving dangling

	 * directory index entries in directories that were our parents but are

	 * not anymore. Not doing this results in old parent directory being

	 * impossible to delete after log replay (rmdir will always fail with

	 * error -ENOTEMPTY).

	 *

	 * Example 1:

	 *

	 * mkdir testdir

	 * touch testdir/foo

	 * ln testdir/foo testdir/bar

	 * sync

	 * unlink testdir/bar

	 * xfs_io -c fsync testdir/foo

	 * <power failure>

	 * mount fs, triggers log replay

	 *

	 * If we don't log the parent directory (testdir), after log replay the

	 * directory still has an entry pointing to the file inode using the bar

	 * name, but a matching BTRFS_INODE_[REF|EXTREF]_KEY does not exist and

	 * the file inode has a link count of 1.

	 *

	 * Example 2:

	 *

	 * mkdir testdir

	 * touch foo

	 * ln foo testdir/foo2

	 * ln foo testdir/foo3

	 * sync

	 * unlink testdir/foo3

	 * xfs_io -c fsync foo

	 * <power failure>

	 * mount fs, triggers log replay

	 *

	 * Similar as the first example, after log replay the parent directory

	 * testdir still has an entry pointing to the inode file with name foo3

	 * but the file inode does not have a matching BTRFS_INODE_REF_KEY item

	 * and has a link count of 2.

/*

 * it is not safe to log dentry if the chunk root has added new

 * chunks.  This returns 0 if the dentry was logged, and 1 otherwise.

 * If this returns 1, you must commit the transaction to safely get your

 * data on disk.

/*

 * should be called during mount to recover any replay any log trees

 * from the FS

			/*

			 * We didn't find the subvol, likely because it was

			 * deleted.  This is ok, simply skip this log and go to

			 * the next one.

			 *

			 * We need to exclude the root because we can't have

			 * other log replays overwriting this log as we'll read

			 * it back in a few more times.  This will keep our

			 * block from being modified, and we'll just bail for

			 * each subsequent pass.

 The loop needs to continue due to the root refs */

			/*

			 * We have just replayed everything, and the highest

			 * objectid of fs roots probably has changed in case

			 * some inode_item's got replayed.

			 *

			 * root->objectid_mutex is not acquired as log replay

			 * could only happen during mount.

 step one is to pin it all, step two is to replay just inodes */

 step three is to replay everything */

 step 4: commit the transaction, which also unpins the blocks */

/*

 * there are some corner cases where we want to force a full

 * commit instead of allowing a directory to be logged.

 *

 * They revolve around files there were unlinked from the directory, and

 * this function updates the parent directory so that a full commit is

 * properly done if it is fsync'd later after the unlinks are done.

 *

 * Must be called before the unlink operations (updates to the subvolume tree,

 * inodes, etc) are done.

	/*

	 * when we're logging a file, if it hasn't been renamed

	 * or unlinked, and its inode is fully committed on disk,

	 * we don't have to worry about walking up the directory chain

	 * to log its parents.

	 *

	 * So, we use the last_unlink_trans field to put this transid

	 * into the file.  When the file is logged we check it and

	 * don't log the parents if the file is fully on disk.

	/*

	 * if this directory was already logged any new

	 * names for this file/dir will get recorded

	/*

	 * if the inode we're about to unlink was logged,

	 * the log will be properly updated for any new names

	/*

	 * when renaming files across directories, if the directory

	 * there we're unlinking from gets fsync'd later on, there's

	 * no way to find the destination directory later and fsync it

	 * properly.  So, we have to be conservative and force commits

	 * so the new name gets discovered.

 we can safely do the unlink without any special recording */

/*

 * Make sure that if someone attempts to fsync the parent directory of a deleted

 * snapshot, it ends up triggering a transaction commit. This is to guarantee

 * that after replaying the log tree of the parent directory's root we will not

 * see the snapshot anymore and at log replay time we will not see any log tree

 * corresponding to the deleted snapshot's root, which could lead to replaying

 * it after replaying the log tree of the parent directory (which would replay

 * the snapshot delete operation).

 *

 * Must be called before the actual snapshot destroy operation (updates to the

 * parent root and tree of tree roots trees, etc) are done.

/*

 * Call this after adding a new name for a file and it will properly

 * update the log to reflect the new name.

	/*

	 * this will force the logging code to walk the dentry chain

	 * up for the file

	/*

	 * if this inode hasn't been logged and directory we're renaming it

	 * from hasn't been logged, we don't need to log it

	/*

	 * If we are doing a rename (old_dir is not NULL) from a directory that

	 * was previously logged, make sure the next log attempt on the directory

	 * is not skipped and logs the inode again. This is because the log may

	 * not currently be authoritative for a range including the old

	 * BTRFS_DIR_ITEM_KEY and BTRFS_DIR_INDEX_KEY keys, so we want to make

	 * sure after a log replay we do not end up with both the new and old

	 * dentries around (in case the inode is a directory we would have a

	 * directory with two hard links and 2 inode references for different

	 * parents). The next log attempt of old_dir will happen at

	 * btrfs_log_all_parents(), called through btrfs_log_inode_parent()

	 * below, because we have previously set inode->last_unlink_trans to the

	 * current transaction ID, either here or at btrfs_record_unlink_dir() in

	 * case inode is a directory.

	/*

	 * We don't care about the return value. If we fail to log the new name

	 * then we know the next attempt to sync the log will fallback to a full

	 * transaction commit (due to a call to btrfs_set_log_full_commit()), so

	 * we don't need to worry about getting a log committed that has an

	 * inconsistent state after a rename operation.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

 * Copyright (C) 2014 Fujitsu.  All rights reserved.

 File system this workqueue services */

 List head pointing to ordered work list */

 Spinlock for ordered_list */

 Thresholding related variants */

 Up limit of concurrency workers */

 Current number of concurrency workers */

 Threshold to change current_active */

	/*

	 * We could compare wq->normal->pending with num_online_cpus()

	 * to support "thresh == NO_THRESHOLD" case, but it requires

	 * moving up atomic_inc/dec in thresh_queue/exec_hook. Let's

	 * postpone it until someone needs the support of that case.

 For low threshold, disabling threshold is a better choice */

		/*

		 * For threshold-able wq, let its concurrency grow on demand.

		 * Use minimal max_active at alloc time to reduce resource

		 * usage.

/*

 * Hook for threshold which will be called in btrfs_queue_work.

 * This hook WILL be called in IRQ handler context,

 * so workqueue_set_max_active MUST NOT be called in this hook

/*

 * Hook for threshold which will be called before executing the work,

 * This hook is called in kthread content.

 * So workqueue_set_max_active is called here.

	/*

	 * Use wq->count to limit the calling frequency of

	 * workqueue_set_max_active.

	/*

	 * pending may be changed later, but it's OK since we really

	 * don't need it so accurate to calculate new_max_active.

		/*

		 * Orders all subsequent loads after reading WORK_DONE_BIT,

		 * paired with the smp_mb__before_atomic in btrfs_work_helper

		 * this guarantees that the ordered function will see all

		 * updates from ordinary work function.

		/*

		 * we are going to call the ordered done function, but

		 * we leave the work item on the list as a barrier so

		 * that later work items that are done don't have their

		 * functions called before this one returns

 now take the lock again and drop our item from the list */

			/*

			 * This is the work item that the worker is currently

			 * executing.

			 *

			 * The kernel workqueue code guarantees non-reentrancy

			 * of work items. I.e., if a work item with the same

			 * address and work function is queued twice, the second

			 * execution is blocked until the first one finishes. A

			 * work item may be freed and recycled with the same

			 * work function; the workqueue code assumes that the

			 * original work item cannot depend on the recycled work

			 * item in that case (see find_worker_executing_work()).

			 *

			 * Note that different types of Btrfs work can depend on

			 * each other, and one type of work on one Btrfs

			 * filesystem may even depend on the same type of work

			 * on another Btrfs filesystem via, e.g., a loop device.

			 * Therefore, we must not allow the current work item to

			 * be recycled until we are really done, otherwise we

			 * break the above assumption and can deadlock.

			/*

			 * We don't want to call the ordered free functions with

			 * the lock held.

 NB: work must not be dereferenced past this point. */

 NB: self must not be dereferenced past this point. */

	/*

	 * We should not touch things inside work in the following cases:

	 * 1) after work->func() if it has no ordered_free

	 *    Since the struct is freed in work->func().

	 * 2) after setting WORK_DONE_BIT

	 *    The work may be freed in other threads almost instantly.

	 * So we save the needed things here.

		/*

		 * Ensures all memory accesses done in the work function are

		 * ordered before setting the WORK_DONE_BIT. Ensuring the thread

		 * which is going to executed the ordered work sees them.

		 * Pairs with the smp_rmb in run_ordered_work.

 NB: work must not be dereferenced past this point. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2011 STRATO.  All rights reserved.

/* TODO XXX FIXME

 *  - subvol delete -> delete when ref goes to 0? delete limits also?

 *  - reorganize keys

 *  - compressed

 *  - sync

 *  - copy also limits on subvol creation

 *  - limit

 *  - caches for ulists

 *  - performance benchmarks

 *  - check all ioctl parameters

/*

 * Helpers to access qgroup reservation

 *

 * Callers should ensure the lock context and type are valid

/*

 * glue structure to represent the relations between qgroups.

 must be called with qgroup_ioctl_lock held */

 must be called with qgroup_lock held */

 must be called with qgroup_lock held */

 must be called with qgroup_lock held */

 must be called with qgroup_lock held */

/*

 * The full config is read in one go, only called from open_ctree()

 * It doesn't use any locking, as at this point we're still single-threaded

 default this to quota off, in case no status key is found */

	/*

	 * pass 1: read status, all qgroup infos and limits

 generation currently unused */

	/*

	 * pass 2: read all qgroup relations

 parent <- member, not needed to build config */

 FIXME should we omit the key completely? */

 ignore the error */

/*

 * Called in close_ctree() when quota is still enabled.  This verifies we don't

 * leak some reserved space.

 *

 * Return false if no reserved space is left.

 * Return true if some reserved space is leaked.

	/*

	 * Since we're unmounting, there is no race and no need to grab qgroup

	 * lock.  And here we don't go post-order to provide a more user

	 * friendly sorted result.

/*

 * This is called from close_ctree() or open_ctree() or btrfs_quota_disable(),

 * first two are in single-threaded paths.And for the third one, we have set

 * quota_root to be null with qgroup_lock held before, so it is safe to clean

 * up the in-memory structures without qgroup_lock held.

	/*

	 * We call btrfs_free_qgroup_config() when unmounting

	 * filesystem and disabling quota, so we set qgroup_ulist

	 * to be null here to avoid double free.

	/*

	 * Avoid a transaction abort by catching -EEXIST here. In that

	 * case, we proceed by re-initializing the existing structure

	 * on disk.

/*

 * called with qgroup_lock held

		/*

		 * delete the leaf one by one

		 * since the whole tree is going

		 * to be deleted.

	/*

	 * Unlock qgroup_ioctl_lock before starting the transaction. This is to

	 * avoid lock acquisition inversion problems (reported by lockdep) between

	 * qgroup_ioctl_lock and the vfs freeze semaphores, acquired when we

	 * start a transaction.

	 * After we started the transaction lock qgroup_ioctl_lock again and

	 * check if someone else created the quota root in the meanwhile. If so,

	 * just return success and release the transaction handle.

	 *

	 * Also we don't need to worry about someone else calling

	 * btrfs_sysfs_add_qgroups() after we unlock and getting an error because

	 * that function returns 0 (success) when the sysfs entries already exist.

	/*

	 * 1 for quota root item

	 * 1 for BTRFS_QGROUP_STATUS item

	 *

	 * Yet we also need 2*n items for a QGROUP_INFO/QGROUP_LIMIT items

	 * per subvolume. However those are not currently reserved since it

	 * would be a lot of overkill.

	/*

	 * initially create the quota tree

 Release locks on tree_root before we access quota_root */

				/*

				 * Shouldn't happen, but in case it does we

				 * don't need to do the btrfs_next_item, just

				 * continue.

	/*

	 * Set quota enabled flag after committing the transaction, to avoid

	 * deadlocks on fs_info->qgroup_ioctl_lock with concurrent snapshot

	 * creation.

	/*

	 * 1 For the root item

	 *

	 * We should also reserve enough items for the quota tree deletion in

	 * btrfs_clean_quota_tree but this is not done.

	 *

	 * Also, we must always start a transaction without holding the mutex

	 * qgroup_ioctl_lock, see btrfs_quota_enable().

/*

 * The easy accounting, we're updating qgroup relationship whose child qgroup

 * only has exclusive extents.

 *

 * In this case, all exclusive extents will also be exclusive for parent, so

 * excl/rfer just get added/removed.

 *

 * So is qgroup reservation space, which should also be added/removed to

 * parent.

 * Or when child tries to release reservation space, parent will underflow its

 * reservation (for relationship adding case).

 *

 * Caller should hold fs_info->qgroup_lock.

 Get all of the parent groups that contain this qgroup */

 Iterate all of the parents and adjust their reference counts */

 Add any parents of the parents */

/*

 * Quick path for updating qgroup with only excl refs.

 *

 * In that case, just update all parent will be enough.

 * Or we needs to do a full rescan.

 * Caller should also hold fs_info->qgroup_lock.

 *

 * Return 0 for quick update, return >0 for need to full rescan

 * and mark INCONSISTENT flag.

 * Return < 0 for other error.

 Check the level of src and dst first */

 We hold a transaction handle open, must do a NOFS allocation. */

 check if such qgroup relation exist firstly */

 We hold a transaction handle open, must do a NOFS allocation. */

	/*

	 * The parent/member pair doesn't exist, then try to delete the dead

	 * relation items only.

 check if such qgroup relation exist firstly */

 At least one deletion succeeded, return 0 */

 Check if there are no children of this qgroup */

	/*

	 * Remove the qgroup from sysfs now without holding the qgroup_lock

	 * spinlock, since the sysfs_remove_group() function needs to take

	 * the mutex kernfs_mutex through kernfs_remove_by_name_ns().

	/* Sometimes we would want to clear the limit on this qgroup.

	 * To meet this requirement, we treat the -1 as a special value

	 * which tell kernel to clear the limit on this qgroup.

	/*

	 * We are always called in a context where we are already holding a

	 * transaction handle. Often we are called when adding a data delayed

	 * reference from btrfs_truncate_inode_items() (truncating or unlinking),

	 * in which case we will be holding a write lock on extent buffer from a

	 * subvolume tree. In this case we can't allow btrfs_find_all_roots() to

	 * acquire fs_info->commit_root_sem, because that is a higher level lock

	 * that must be acquired before locking any extent buffers.

	 *

	 * So we want btrfs_find_all_roots() to not acquire the commit_root_sem

	 * but we can't pass it a non-NULL transaction handle, because otherwise

	 * it would not use commit roots and would lock extent buffers, causing

	 * a deadlock if it ends up trying to read lock the same extent buffer

	 * that was previously write locked at btrfs_truncate_inode_items().

	 *

	 * So pass a NULL transaction handle to btrfs_find_all_roots() and

	 * explicitly tell it to not acquire the commit_root_sem - if we are

	 * holding a transaction handle we don't need its protection.

	/*

	 * Here we don't need to get the lock of

	 * trans->transaction->delayed_refs, since inserted qrecord won't

	 * be deleted, only qrecord->node may be modified (new qrecord insert)

	 *

	 * So modifying qrecord->old_roots is safe here

 We can be called directly from walk_up_proc() */

 filter out non qgroup-accountable extents  */

/*

 * Walk up the tree from the bottom, freeing leaves and any interior

 * nodes which have had all slots visited. If a node (leaf or

 * interior) is freed, the node above it will have it's slot

 * incremented. The root node will never be freed.

 *

 * At the end of this function, we should have a path which has all

 * slots incremented to the next position for a search. If we need to

 * read a new node it will be NULL and the node above it will have the

 * correct slot selected for a later read.

 *

 * If we increment the root nodes slot counter past the number of

 * elements, 1 is returned to signal completion of the search.

			/*

			 * Don't free the root -  we will detect this

			 * condition after our loop and return a

			 * positive value for caller to stop walking the tree.

			/*

			 * We have a valid slot to walk back down

			 * from. Stop here so caller can process these

			 * new nodes.

/*

 * Helper function to trace a subtree tree block swap.

 *

 * The swap will happen in highest tree block, but there may be a lot of

 * tree blocks involved.

 *

 * For example:

 *  OO = Old tree blocks

 *  NN = New tree blocks allocated during balance

 *

 *           File tree (257)                  Reloc tree for 257

 * L2              OO                                NN

 *               /    \                            /    \

 * L1          OO      OO (a)                    OO      NN (a)

 *            / \     / \                       / \     / \

 * L0       OO   OO OO   OO                   OO   OO NN   NN

 *                  (b)  (c)                          (b)  (c)

 *

 * When calling qgroup_trace_extent_swap(), we will pass:

 * @src_eb = OO(a)

 * @dst_path = [ nodes[1] = NN(a), nodes[0] = NN(c) ]

 * @dst_level = 0

 * @root_level = 1

 *

 * In that case, qgroup_trace_extent_swap() will search from OO(a) to

 * reach OO(c), then mark both OO(c) and NN(c) as qgroup dirty.

 *

 * The main work of qgroup_trace_extent_swap() can be split into 3 parts:

 *

 * 1) Tree search from @src_eb

 *    It should acts as a simplified btrfs_search_slot().

 *    The key for search can be extracted from @dst_path->nodes[dst_level]

 *    (first key).

 *

 * 2) Mark the final tree blocks in @src_path and @dst_path qgroup dirty

 *    NOTE: In above case, OO(a) and NN(a) won't be marked qgroup dirty.

 *    They should be marked during previous (@dst_level = 1) iteration.

 *

 * 3) Mark file extents in leaves dirty

 *    We don't have good way to pick out new file extents only.

 *    So we still follow the old method by scanning all file extents in

 *    the leave.

 *

 * This function can free us from keeping two paths, thus later we only need

 * to care about how to iterate all new tree blocks in reloc tree.

 Level mismatch */

 For src_path */

 A simplified version of btrfs_search_slot() */

 Content mismatch, something went wrong */

	/*

	 * Now both @dst_path and @src_path have been populated, record the tree

	 * blocks for qgroup accounting.

 Record leaf file extents */

/*

 * Helper function to do recursive generation-aware depth-first search, to

 * locate all new tree blocks in a subtree of reloc tree.

 *

 * E.g. (OO = Old tree blocks, NN = New tree blocks, whose gen == last_snapshot)

 *         reloc tree

 * L2         NN (a)

 *          /    \

 * L1    OO        NN (b)

 *      /  \      /  \

 * L0  OO  OO    OO  NN

 *               (c) (d)

 * If we pass:

 * @dst_path = [ nodes[1] = NN(b), nodes[0] = NULL ],

 * @cur_level = 1

 * @root_level = 1

 *

 * We will iterate through tree blocks NN(b), NN(d) and info qgroup to trace

 * above tree blocks along with their counter parts in file tree.

 * While during search, old tree blocks OO(c) will be skipped as tree block swap

 * won't affect OO(c).

 Level sanity check */

 Read the tree block if needed */

		/*

		 * dst_path->nodes[root_level] must be initialized before

		 * calling this function.

		/*

		 * We need to get child blockptr/gen from parent before we can

		 * read it.

 This node is old, no need to trace */

 Now record this tree block and its counter part for qgroups */

 Iterate all child tree blocks */

 Skip old tree blocks as they won't be swapped */

 Recursive call (at most 7 times) */

 Clean up */

 Wrong parameter order */

 For dst_path */

 Do the generation aware breadth-first search */

	/*

	 * Walk down the tree.  Missing extent blocks are filled in as

	 * we go. Metadata is accounted every time we read a new

	 * extent block.

	 *

	 * When we reach a leaf, we account for file extent items in it,

	 * walk back up the tree (adjusting slot pointers as we go)

	 * and restart the search process.

 For path */

 so release_path doesn't try to unlock */

			/*

			 * We need to get child blockptr from parent before we

			 * can read it.

 Nonzero return here means we completed our search */

 Restart search with new slots */

/*

 * Walk all of the roots that points to the bytenr and adjust their refcnts.

/*

 * Update qgroup rfer/excl counters.

 * Rfer update is easy, codes can explain themselves.

 *

 * Excl update is tricky, the update is split into 2 parts.

 * Part 1: Possible exclusive <-> sharing detect:

 *	|	A	|	!A	|

 *  -------------------------------------

 *  B	|	*	|	-	|

 *  -------------------------------------

 *  !B	|	+	|	**	|

 *  -------------------------------------

 *

 * Conditions:

 * A:	cur_old_roots < nr_old_roots	(not exclusive before)

 * !A:	cur_old_roots == nr_old_roots	(possible exclusive before)

 * B:	cur_new_roots < nr_new_roots	(not exclusive now)

 * !B:	cur_new_roots == nr_new_roots	(possible exclusive now)

 *

 * Results:

 * +: Possible sharing -> exclusive	-: Possible exclusive -> sharing

 * *: Definitely not changed.		**: Possible unchanged.

 *

 * For !A and !B condition, the exception is cur_old/new_roots == 0 case.

 *

 * To make the logic clear, we first use condition A and B to split

 * combination into 4 results.

 *

 * Then, for result "+" and "-", check old/new_roots == 0 case, as in them

 * only on variant maybe 0.

 *

 * Lastly, check result **, since there are 2 variants maybe 0, split them

 * again(2x2).

 * But this time we don't need to consider other things, the codes and logic

 * is easy to understand now.

 Rfer update part */

 Excl update part */

 Exclusive/none -> shared case */

 Exclusive -> shared */

 Shared -> exclusive/none case */

 Shared->exclusive */

 Exclusive/none -> exclusive/none case */

 None -> exclusive/none */

 None -> exclusive */

 None -> none, nothing changed */

 Exclusive -> exclusive/none */

 Exclusive -> none */

 Exclusive -> exclusive, nothing changed */

/*

 * Check if the @roots potentially is a list of fs tree roots

 *

 * Return 0 for definitely not a fs/subvol tree roots ulist

 * Return 1 for possible fs/subvol tree roots in the list (considering an empty

 *          one as well)

 Empty one, still possible for fs roots */

	/*

	 * If it contains fs tree roots, then it must belong to fs/subvol

	 * trees.

	 * If it contains a non-fs tree, it won't be shared with fs/subvol trees.

	/*

	 * If quotas get disabled meanwhile, the resources need to be freed and

	 * we can't just exit here.

 Quick exit, either not fs tree roots, or won't affect any qgroup */

 Update old refcnts using old_roots */

 Update new refcnts using new_roots */

	/*

	 * Bump qgroup_seq to avoid seq overlap

			/*

			 * Old roots should be searched when inserting qgroup

			 * extent record

 Search commit root to find old_roots */

 Free the reserved data space */

			/*

			 * Use BTRFS_SEQ_LAST as time_seq to do special search,

			 * which doesn't lock tree or delayed_refs and search

			 * current root. It's safe inside commit_transaction().

/*

 * called from commit_transaction. Writes all changed qgroups to disk.

/*

 * Copy the accounting information between qgroups. This is necessary

 * when a snapshot or a subvolume is created. Throwing an error will

 * cause a transaction abort so we take extra care here to only error

 * when a readonly fs is a reasonable outcome.

	/*

	 * There are only two callers of this function.

	 *

	 * One in create_subvol() in the ioctl context, which needs to hold

	 * the qgroup_ioctl_lock.

	 *

	 * The other one in create_pending_snapshot() where no other qgroup

	 * code can modify the fs as they all need to either start a new trans

	 * or hold a trans handler, thus we don't need to hold

	 * qgroup_ioctl_lock.

	 * This would avoid long and complex lock chain and make lockdep happy.

			/*

			 * Zero out invalid groups so we can ignore

			 * them later.

	/*

	 * create a tracking group for the subvol itself

	/*

	 * add qgroup to all inherited groups

		/*

		 * We call inherit after we clone the root in order to make sure

		 * our counts don't go crazy, so at this point the only

		 * difference between the two roots should be the root node.

 inherit the limit info */

		/*

		 * If we're doing a snapshot, and adding the snapshot to a new

		 * qgroup, the numbers are guaranteed to be incorrect.

 Manually tweaking numbers certainly needs a rescan */

	/*

	 * in a first step, we check all affected qgroups if any limits would

	 * be exceeded

	/*

	 * no limits exceeded, now record the reservation into all qgroups

/*

 * Free @num_bytes of reserved space with @type for qgroup.  (Normally level 0

 * qgroup).

 *

 * Will handle all higher level qgroup too.

 *

 * NOTE: If @num_bytes is (u64)-1, this means to free all bytes of this qgroup.

 * This special case is only used for META_PERTRANS type.

		/*

		 * We're freeing all pertrans rsv, get reserved value from

		 * level 0 qgroup as real num_bytes to free.

/*

 * Check if the leaf is the last leaf. Which means all node pointers

 * are at their last position.

/*

 * returns < 0 on error, 0 when more leafs are to be scanned.

 * returns 1 when done.

		/*

		 * The rescan is about to end, we will not be scanning any

		 * further blocks. We cannot unset the RESCAN flag here, because

		 * we want to commit the transaction if everything went well.

		 * To make the live accounting work in this phase, we set our

		 * scan progress pointer such that every real extent objectid

		 * will be smaller.

 For rescan, just pass old_roots as NULL */

	/*

	 * Rescan should only search for commit root, and any later difference

	 * should be recorded by qgroup

	/*

	 * only update status, since the previous part has already updated the

	 * qgroup info.

/*

 * Checks that (a) no rescan is running and (b) quota is enabled. Allocates all

 * memory required for the rescan context.

 we're resuming qgroup rescan at mount time */

 clear all current qgroup tracking information */

	/*

	 * We have set the rescan_progress to 0, which means no more

	 * delayed refs will be accounted by btrfs_qgroup_account_ref.

	 * However, btrfs_qgroup_account_ref may be right after its call

	 * to btrfs_find_all_roots, in which case it would still do the

	 * accounting.

	 * To solve this, we're committing the transaction, which will

	 * ensure we run all delayed refs and only after that, we are

	 * going to clear all tracking information for a clean start.

/*

 * this is only called from open_ctree where we're still single threaded, thus

 * locking is omitted here.

		/*

		 * Now the entry is in [start, start + len), revert the

		 * EXTENT_QGROUP_RESERVED bit.

/*

 * Try to free some space for qgroup.

 *

 * For qgroup, there are only 3 ways to free qgroup space:

 * - Flush nodatacow write

 *   Any nodatacow write will free its reserved data space at run_delalloc_range().

 *   In theory, we should only flush nodatacow inodes, but it's not yet

 *   possible, so we need to flush the whole root.

 *

 * - Wait for ordered extents

 *   When ordered extents are finished, their reserved metadata is finally

 *   converted to per_trans status, which can be freed by later commit

 *   transaction.

 *

 * - Commit transaction

 *   This would free the meta_per_trans space.

 *   In theory this shouldn't provide much space, but any more qgroup space

 *   is needed.

 Can't hold an open transaction or we run the risk of deadlocking. */

	/*

	 * We don't want to run flush again and again, so if there is a running

	 * one, we won't try to start a new flush, but exit directly.

 @reserved parameter is mandatory for qgroup */

 Record already reserved space */

 Newly reserved space */

/*

 * Reserve qgroup space for range [start, start + len).

 *

 * This function will either reserve space from related qgroups or do nothing

 * if the range is already reserved.

 *

 * Return 0 for successful reservation

 * Return <0 for error (including -EQUOT)

 *

 * NOTE: This function may sleep for memory allocation, dirty page flushing and

 *	 commit transaction. So caller should not hold any dirty page locked.

 Free ranges specified by @reserved, normally in error path */

 unode->aux is the inclusive end */

 Only free range in range [start, start + len) */

		/*

		 * TODO: To also modify reserved->ranges_reserved to reflect

		 * the modification.

		 *

		 * However as long as we free qgroup reserved according to

		 * EXTENT_QGROUP_RESERVED, we won't double free.

		 * So not need to rush.

 In release case, we shouldn't have @reserved */

/*

 * Free a reserved space range from io_tree and related qgroups

 *

 * Should be called when a range of pages get invalidated before reaching disk.

 * Or for error cleanup case.

 * if @reserved is given, only reserved range in [@start, @start + @len) will

 * be freed.

 *

 * For data written to disk, use btrfs_qgroup_release_data().

 *

 * NOTE: This function may sleep for memory allocation.

/*

 * Release a reserved space range from io_tree only.

 *

 * Should be called when a range of pages get written to disk and corresponding

 * FILE_EXTENT is inserted into corresponding root.

 *

 * Since new qgroup accounting framework will only update qgroup numbers at

 * commit_transaction() time, its reserved space shouldn't be freed from

 * related qgroups.

 *

 * But we should release the range from io_tree, to allow further write to be

 * COWed.

 *

 * NOTE: This function may sleep for memory allocation.

	/*

	 * Record what we have reserved into root.

	 *

	 * To avoid quota disabled->enabled underflow.

	 * In that case, we may try to free space we haven't reserved

	 * (since quota was disabled), so record what we reserved into root.

	 * And ensure later release won't underflow this number.

 TODO: Update trace point to handle such free */

 Special value -1 means to free all reserved space */

	/*

	 * reservation for META_PREALLOC can happen before quota is enabled,

	 * which can lead to underflow.

	 * Here ensure we will only free what we really have reserved.

 Same as btrfs_qgroup_free_meta_prealloc() */

/*

 * Check qgroup reserved space leaking, normally at destroy inode

 * time

/*

 * Delete all swapped blocks record of @root.

 * Every record here means we skipped a full subtree scan for qgroup.

 *

 * Gets called when committing one transaction.

/*

 * Add subtree roots record into @subvol_root.

 *

 * @subvol_root:	tree root of the subvolume tree get swapped

 * @bg:			block group under balance

 * @subvol_parent/slot:	pointer to the subtree root in subvolume tree

 * @reloc_parent/slot:	pointer to the subtree root in reloc tree

 *			BOTH POINTERS ARE BEFORE TREE SWAP

 * @last_snapshot:	last snapshot generation of the subvolume tree

	/*

	 * @reloc_parent/slot is still before swap, while @block is going to

	 * record the bytenr after swap, so we do the swap here.

	/*

	 * If we have bg == NULL, we're called from btrfs_recover_relocation(),

	 * no one else can modify tree blocks thus we qgroup will not change

	 * no matter the value of trace_leaf.

 Insert @block into @blocks */

				/*

				 * Duplicated but mismatch entry found.

				 * Shouldn't happen.

				 *

				 * Marking qgroup inconsistent should be enough

				 * for end users.

/*

 * Check if the tree block is a subtree root, and if so do the needed

 * delayed subtree trace for qgroup.

 *

 * This is called during btrfs_cow_block().

 Found one, remove it from @blocks first and update blocks->swapped */

 Read out reloc subtree root */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/*

 * Types for mounting the default subvolume and a subvolume explicitly

 * requested by subvol=/path. That way the callchain is straightforward and we

 * don't have to play tricks with the mount options and recursive calls to

 * btrfs_mount.

 *

 * The new btrfs_root_fs_type also servers as a tag for the bdev_holder.

/*

 * Generally the error codes correspond to their respective errors, but there

 * are a few special cases.

 *

 * EUCLEAN: Any sort of corruption that we encounter.  The tree-checker for

 *          instance will return EUCLEAN if any of the blocks are corrupted in

 *          a way that is problematic.  We want to reserve EUCLEAN for these

 *          sort of corruptions.

 *

 * EROFS: If we check BTRFS_FS_STATE_ERROR and fail out with a return error, we

 *        need to use EROFS for this case.  We will have no idea of the

 *        original failure, that will have been reported at the time we tripped

 *        over the error.  Each subsequent error that doesn't have any context

 *        of the original error should use EROFS when handling BTRFS_FS_STATE_ERROR.

 -2 */

 -5 */

 -12*/

 -17 */

 -28 */

 -30 */

 -95 */

 -117 */

 -122 */

/*

 * __btrfs_handle_fs_error decodes expected errors from the caller and

 * invokes the appropriate error response.

	/*

	 * Special case: if the error is EROFS, and we're already

	 * under SB_RDONLY, then it is safe here.

	/*

	 * Today we only save the error info to memory.  Long term we'll

	 * also send it down to the disk

 Don't go through full error handling during mount */

 btrfs handle error by forcing the filesystem readonly */

	/*

	 * Note that a running device replace operation is not canceled here

	 * although there is no way to update the progress. It would add the

	 * risk of a deadlock, therefore the canceling is omitted. The only

	 * penalty is that some I/O remains active until the procedure

	 * completes. The next time when the filesystem is mounted writable

	 * again, the device replace operation continues.

/*

 * Use one ratelimit state per log level so that a flood of less important

 * messages doesn't cause more important ones to be dropped.

/*

 * We only mark the transaction aborted and then set the file system read-only.

 * This will prevent new transactions from starting or trying to join this

 * one.

 *

 * This means that error recovery at the call site is limited to freeing

 * any local memory allocations and passing the error code up without

 * further cleanup. The transaction should complete as it normally would

 * in the call path but will return -EIO.

 *

 * We'll complete the cleanup in btrfs_end_transaction and

 * btrfs_commit_transaction.

 Wake up anybody who may be waiting on this transaction */

/*

 * __btrfs_panic decodes unexpected, fatal errors from the caller,

 * issues an alert, and either panics or BUGs, depending on mount options.

 Caller calls BUG() */

 Rescue options */

 Deprecated options */

 Debugging options */

 Rescue options */

 Deprecated, with alias rescue=nologreplay */

 Deprecated, with alias rescue=usebackuproot */

 Deprecated options */

 Debugging options */

/*

 * Regular mount options parser.  Everything that is needed only when

 * reading in a new superblock is parsed here.

 * XXX JDM: This needs to be cleaned up for remount.

	/*

	 * Even the options are empty, we still need to do extra check

	 * against new flags

			/*

			 * These are parsed by btrfs_parse_subvol_options or

			 * btrfs_parse_device_options and can be ignored here.

				/*

				 * args[0] contains uninitialized data since

				 * for these tokens we don't expect any

				 * parameter.

				/*

				 * If we remount from compress-force=xxx to

				 * compress=xxx, we need clear FORCE_COMPRESS

				 * flag, otherwise, there is no way for users

				 * to disable forcible compression separately.

 We're read-only, don't have to check. */

/*

 * Parse mount options that are required early in the mount process.

 *

 * All other options will be parsed on much later in the mount process and

 * only when we need to allocate a new super block.

	/*

	 * strsep changes the string, duplicate it because btrfs_parse_options

	 * gets called later

/*

 * Parse mount options that are related to subvolume id

 *

 * The value is later passed to mount_subvol()

	/*

	 * strsep changes the string, duplicate it because

	 * btrfs_parse_device_options gets called later

 we want the original fs_tree */

	/*

	 * Walk up the subvolume trees in the tree of tree roots by root

	 * backrefs until we hit the top-level subvolume.

		/*

		 * Walk up the filesystem tree by inode refs until we hit the

		 * root directory.

	/*

	 * Find the "default" dir item which points to the root item that we

	 * will mount by default if we haven't been given a specific subvolume

	 * to mount.

		/*

		 * Ok the default dir item isn't there.  This is weird since

		 * it's always been there, but don't freak out, just try and

		 * mount the top-level subvolume.

 no transaction, don't bother */

			/*

			 * Exit unless we have some pending changes

			 * that need to go through commit

			/*

			 * A non-blocking test if the fs is frozen. We must not

			 * start a new transaction here otherwise a deadlock

			 * happens. The pending operations are delayed to the

			 * next commit after thawing.

/*

 * subvolumes are identified by ino 256

 mount_subtree() drops our reference on the vfsmount. */

			/*

			 * This will also catch a race condition where a

			 * subvolume which was passed by ID is renamed and

			 * another subvolume is renamed over the old location.

/*

 * Find a superblock for the given device / mount point.

 *

 * Note: This is based on mount_bdev from fs/super.c with a few additions

 *       for multiple device setup.  Make sure to keep it in sync.

	/*

	 * Setup a dummy root and fs_info for test/set super.  This is because

	 * we don't actually fill this stuff out until open_ctree, but we need

	 * then open_ctree will properly initialize the file system specific

	 * settings later.  btrfs_init_fs_info initializes the static elements

	 * of the fs_info (locks and such) to make cleanup easier if we find a

	 * superblock with our given fs_devices later on at sget() time.

/*

 * Mount function which is called by VFS layer.

 *

 * In order to allow mounting a subvolume directly, btrfs uses mount_subtree()

 * which needs vfsmount* of device's root (/).  This means device's root has to

 * be mounted internally in any case.

 *

 * Operation flow:

 *   1. Parse subvol id related options for later use in mount_subvol().

 *

 *   2. Mount device's root (/) by calling vfs_kern_mount().

 *

 *      NOTE: vfs_kern_mount() is used by VFS to call btrfs_mount() in the

 *      first place. In order to avoid calling btrfs_mount() again, we use

 *      different file_system_type which is not registered to VFS by

 *      register_filesystem() (btrfs_root_fs_type). As a result,

 *      btrfs_mount_root() is called. The return value will be used by

 *      mount_subtree() in mount_subvol().

 *

 *   3. Call mount_subvol() to get the dentry of subvolume. Since there is

 *      "btrfs subvolume set-default", mount_subvol() is called always.

 mount device's root (/) */

 mount_subvol() will free subvol_name and mnt_root */

 wait for any defraggers to finish */

	/*

	 * We need to cleanup all defragable inodes if the autodefragment is

	 * close or the filesystem is read only.

 If we toggled discard async */

 If we toggled space cache */

 Make sure free space cache options match the state on disk */

		/*

		 * this also happens on 'umount -rf' or on shutdown, when

		 * the filesystem is busy.

 wait for the uuid_scan task to finish */

 avoid complains from lockdep et al. */

		/*

		 * Setting SB_RDONLY will put the cleaner thread to

		 * sleep at the next loop if it's already active.

		 * If it's already asleep, we'll leave unused block

		 * groups on disk until we're mounted read-write again

		 * unless we clean them up here.

		/*

		 * The cleaner task could be already running before we set the

		 * flag BTRFS_FS_STATE_RO (and SB_RDONLY in the superblock).

		 * We must make sure that after we finish the remount, i.e. after

		 * we call btrfs_commit_super(), the cleaner can no longer start

		 * a transaction - either because it was dropping a dead root,

		 * running delayed iputs or deleting an unused block group (the

		 * cleaner picked a block group from the list of unused block

		 * groups before we were able to in the previous call to

		 * btrfs_delete_unused_bgs()).

		/*

		 * We've set the superblock to RO mode, so we might have made

		 * the cleaner task sleep without running all pending delayed

		 * iputs. Go through all the delayed iputs here, so that if an

		 * unmount happens without remounting RW we don't end up at

		 * finishing close_ctree() with a non-empty list of delayed

		 * iputs.

		/*

		 * Pause the qgroup rescan worker if it is running. We don't want

		 * it to be still running after we are in RO mode, as after that,

		 * by the time we unmount, it might have left a transaction open,

		 * so we would leak the transaction and/or crash.

		/*

		 * NOTE: when remounting with a change that does writes, don't

		 * put it anywhere above this point, as we are not sure to be

		 * safe to write until we pass the above checks.

	/*

	 * We need to set SB_I_VERSION here otherwise it'll get cleared by VFS,

	 * since the absence of the flag means it can be toggled off by remount.

 We've hit an error - don't reset SB_RDONLY */

 Used to sort the devices by max_avail(descending sort) */

/*

 * sort the devices by max_avail, in which max free extent size of each device

 * is stored.(Descending Sort)

/*

 * The helper to calc the free space on the devices that can be used to store

 * file data.

	/*

	 * We aren't under the device list lock, so this is racy-ish, but good

	 * enough for our purposes.

 calc min stripe number for data space allocation */

 Adjust for more than 1 stripe per device */

 align with stripe_len */

		/*

		 * In order to avoid overwriting the superblock on the drive,

		 * btrfs starts at an offset of at least 1MB when doing chunk

		 * allocation.

		 *

		 * This ensures we have at least min_stripe_size free space

		 * after excluding 1MB.

/*

 * Calculate numbers for 'df', pessimistic in case of mixed raid profiles.

 *

 * If there's a redundant raid level at DATA block groups, use the respective

 * multiplier to scale the sizes.

 *

 * Unused device space usage is based on simulating the chunk allocator

 * algorithm that respects the device sizes and order of allocations.  This is

 * a close approximation of the actual use but there are other factors that may

 * change the result (like a new metadata chunk).

 *

 * If metadata is exhausted, f_bavail will be 0.

		/*

		 * Metadata in mixed block goup profiles are accounted in data

 Account global block reserve as used, it's in logical size already */

 Mixed block groups accounting is not byte-accurate, avoid overflow */

	/*

	 * We calculate the remaining metadata space minus global reserve. If

	 * this is (supposedly) smaller than zero, there's no space. But this

	 * does not hold in practice, the exhausted state happens where's still

	 * some positive delta. So we apply some guesswork and compare the

	 * delta to a 4M threshold.  (Practically observed delta was ~2M.)

	 *

	 * We probably cannot calculate the exact threshold value because this

	 * depends on the internal reservations requested by various

	 * operations, so some operations that consume a few metadata will

	 * succeed even if the Avail is zero. But this is better than the other

	 * way around.

	/*

	 * We only want to claim there's no available space if we can no longer

	 * allocate chunks for our metadata profile and our global reserve will

	 * not fit in the free metadata space.  If we aren't ->full then we

	 * still can allocate chunks and thus are fine using the currently

	 * calculated f_bavail.

	/* We treat it as constant endianness (it doesn't matter _which_)

	   because we want the fsid to come out the same whether mounted

 Mask in the root object ID too, to disambiguate subvols */

	/*

	 * The control file's private_data is used to hold the

	 * transaction when it is started and is used to keep

	 * track of whether a transaction is already in progress.

/*

 * Used by /dev/btrfs-control for devices ioctls.

	/*

	 * We don't need a barrier here, we'll wait for any transaction that

	 * could be in progress on other threads (and do delayed iputs that

	 * we want to avoid on a frozen filesystem), or do the commit

	 * ourselves.

 no transaction, don't bother */

	/*

	 * There should be always a valid pointer in latest_dev, it may be stale

	 * for a short moment in case it's being deleted but still valid until

	 * the end of RCU grace period.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2015 Facebook.  All rights reserved.

	/*

	 * We convert to bitmaps when the disk space required for using extents

	 * exceeds that required for using bitmaps.

	/*

	 * We allow for a small buffer between the high threshold and low

	 * threshold to avoid thrashing back and forth between the two formats.

/*

 * btrfs_search_slot() but we're looking for the greatest key less than the

 * passed key.

	/*

	 * GFP_NOFS doesn't work with kvmalloc(), but we really can't recurse

	 * into the filesystem as the free space bitmap can be modified in the

	 * critical section of a transaction commit.

	 *

	 * TODO: push the memalloc_nofs_{save,restore}() to the caller where we

	 * know that recursion is unsafe.

/*

 * We can't use btrfs_next_item() in modify_free_space_bitmap() because

 * btrfs_next_leaf() doesn't get the path for writing. We can forgo the fancy

 * tree walking in btrfs_next_leaf() anyways because we know exactly what we're

 * looking for.

/*

 * If remove is 1, then we are removing free space, thus clearing bits in the

 * bitmap. If remove is 0, then we are adding free space, thus setting bits in

 * the bitmap.

	/*

	 * Read the bit for the block immediately before the extent of space if

	 * that block is within the block group.

 The previous block may have been in the previous bitmap. */

	/*

	 * Iterate over all of the bitmaps overlapped by the extent of space,

	 * clearing/setting bits as required.

	/*

	 * Read the bit for the block immediately after the extent of space if

	 * that block is within the block group.

 The next block may be in the next bitmap. */

 Leftover on the left. */

 Leftover on the right. */

 Merging with neighbor on the left. */

 Merging with neighbor on the right. */

	/*

	 * Okay, now that we've found the free space extent which contains the

	 * free space that we are removing, there are four cases:

	 *

	 * 1. We're using the whole extent: delete the key we found and

	 * decrement the free space extent count.

	 * 2. We are using part of the extent starting at the beginning: delete

	 * the key we found and insert a new key representing the leftover at

	 * the end. There is no net change in the number of extents.

	 * 3. We are using part of the extent ending at the end: delete the key

	 * we found and insert a new key representing the leftover at the

	 * beginning. There is no net change in the number of extents.

	 * 4. We are using part of the extent in the middle: delete the key we

	 * found and insert two new keys representing the leftovers on each

	 * side. Where we used to have one extent, we now have two, so increment

	 * the extent count. We may need to convert the block group to bitmaps

	 * as a result.

 Delete the existing key (cases 1-4). */

 Add a key for leftovers at the beginning (cases 3 and 4). */

 Add a key for leftovers at the end (cases 2 and 4). */

	/*

	 * We are adding a new extent of free space, but we need to merge

	 * extents. There are four cases here:

	 *

	 * 1. The new extent does not have any immediate neighbors to merge

	 * with: add the new key and increment the free space extent count. We

	 * may need to convert the block group to bitmaps as a result.

	 * 2. The new extent has an immediate neighbor before it: remove the

	 * previous key and insert a new key combining both of them. There is no

	 * net change in the number of extents.

	 * 3. The new extent has an immediate neighbor after it: remove the next

	 * key and insert a new key combining both of them. There is no net

	 * change in the number of extents.

	 * 4. The new extent has immediate neighbors on both sides: remove both

	 * of the keys and insert a new key combining all of them. Where we used

	 * to have two extents, we now have one, so decrement the extent count.

 Search for a neighbor on the left. */

	/*

	 * Delete the neighbor on the left and absorb it into the new key (cases

	 * 2 and 4).

 Search for a neighbor on the right. */

	/*

	 * Delete the neighbor on the right and absorb it into the new key

	 * (cases 3 and 4).

 Insert the new key (cases 1-4). */

/*

 * Populate the free space tree by walking the extent tree. Operations on the

 * extent tree that happen as a result of writes to the free space tree will go

 * through the normal add/remove hooks.

	/*

	 * Iterate through all of the extent and metadata items in this block

	 * group, adding the free space between them and the free space at the

	 * end. Note that EXTENT_ITEM and METADATA_ITEM are less than

	 * BLOCK_GROUP_ITEM, so an extent may precede the block group that it's

	 * contained in.

	/*

	 * Now that we've committed the transaction any reading of our commit

	 * root will be safe, so we can cache from the free space tree now.

 We never added this block group to the free space tree. */

 Initialize to silence GCC. */

	/*

	 * Just like caching_thread() doesn't want to deadlock on the extent

	 * tree, we don't want to deadlock on the free space tree.

	/*

	 * We left path pointing to the free space info item, so now

	 * load_free_space_foo can just iterate through the free space tree from

	 * there.

 SPDX-License-Identifier: GPL-2.0

/*

 * HOW DOES THIS WORK

 *

 * There are two stages to data reservations, one for data and one for metadata

 * to handle the new extents and checksums generated by writing data.

 *

 *

 * DATA RESERVATION

 *   The general flow of the data reservation is as follows

 *

 *   -> Reserve

 *     We call into btrfs_reserve_data_bytes() for the user request bytes that

 *     they wish to write.  We make this reservation and add it to

 *     space_info->bytes_may_use.  We set EXTENT_DELALLOC on the inode io_tree

 *     for the range and carry on if this is buffered, or follow up trying to

 *     make a real allocation if we are pre-allocating or doing O_DIRECT.

 *

 *   -> Use

 *     At writepages()/prealloc/O_DIRECT time we will call into

 *     btrfs_reserve_extent() for some part or all of this range of bytes.  We

 *     will make the allocation and subtract space_info->bytes_may_use by the

 *     original requested length and increase the space_info->bytes_reserved by

 *     the allocated length.  This distinction is important because compression

 *     may allocate a smaller on disk extent than we previously reserved.

 *

 *   -> Allocation

 *     finish_ordered_io() will insert the new file extent item for this range,

 *     and then add a delayed ref update for the extent tree.  Once that delayed

 *     ref is written the extent size is subtracted from

 *     space_info->bytes_reserved and added to space_info->bytes_used.

 *

 *   Error handling

 *

 *   -> By the reservation maker

 *     This is the simplest case, we haven't completed our operation and we know

 *     how much we reserved, we can simply call

 *     btrfs_free_reserved_data_space*() and it will be removed from

 *     space_info->bytes_may_use.

 *

 *   -> After the reservation has been made, but before cow_file_range()

 *     This is specifically for the delalloc case.  You must clear

 *     EXTENT_DELALLOC with the EXTENT_CLEAR_DATA_RESV bit, and the range will

 *     be subtracted from space_info->bytes_may_use.

 *

 * METADATA RESERVATION

 *   The general metadata reservation lifetimes are discussed elsewhere, this

 *   will just focus on how it is used for delalloc space.

 *

 *   We keep track of two things on a per inode bases

 *

 *   ->outstanding_extents

 *     This is the number of file extent items we'll need to handle all of the

 *     outstanding DELALLOC space we have in this inode.  We limit the maximum

 *     size of an extent, so a large contiguous dirty area may require more than

 *     one outstanding_extent, which is why count_max_extents() is used to

 *     determine how many outstanding_extents get added.

 *

 *   ->csum_bytes

 *     This is essentially how many dirty bytes we have for this inode, so we

 *     can calculate the number of checksum items we would have to add in order

 *     to checksum our outstanding data.

 *

 *   We keep a per-inode block_rsv in order to make it easier to keep track of

 *   our reservation.  We use btrfs_calculate_inode_block_rsv_size() to

 *   calculate the current theoretical maximum reservation we would need for the

 *   metadata for this inode.  We call this and then adjust our reservation as

 *   necessary, either by attempting to reserve more space, or freeing up excess

 *   space.

 *

 * OUTSTANDING_EXTENTS HANDLING

 *

 *  ->outstanding_extents is used for keeping track of how many extents we will

 *  need to use for this inode, and it will fluctuate depending on where you are

 *  in the life cycle of the dirty data.  Consider the following normal case for

 *  a completely clean inode, with a num_bytes < our maximum allowed extent size

 *

 *  -> reserve

 *    ->outstanding_extents += 1 (current value is 1)

 *

 *  -> set_delalloc

 *    ->outstanding_extents += 1 (current value is 2)

 *

 *  -> btrfs_delalloc_release_extents()

 *    ->outstanding_extents -= 1 (current value is 1)

 *

 *    We must call this once we are done, as we hold our reservation for the

 *    duration of our operation, and then assume set_delalloc will update the

 *    counter appropriately.

 *

 *  -> add ordered extent

 *    ->outstanding_extents += 1 (current value is 2)

 *

 *  -> btrfs_clear_delalloc_extent

 *    ->outstanding_extents -= 1 (current value is 1)

 *

 *  -> finish_ordered_io/btrfs_remove_ordered_extent

 *    ->outstanding_extents -= 1 (current value is 0)

 *

 *  Each stage is responsible for their own accounting of the extent, thus

 *  making error handling and cleanup easier.

 Make sure bytes are sectorsize aligned */

 align the range */

 Use new btrfs_qgroup_reserve_data to reserve precious data space. */

/*

 * Called if we need to clear a data reservation for this inode

 * Normally in a error case.

 *

 * This one will *NOT* use accurate qgroup reserved space API, just for case

 * which we can't sleep and is sure it won't affect qgroup reserved space.

 * Like clear_bit_hook().

/*

 * Called if we need to clear a data reservation for this inode

 * Normally in a error case.

 *

 * This one will handle the per-inode data rsv map for accurate reserved

 * space framework.

 Make sure the range is aligned to sectorsize */

/**

 * Release any excessive reservation

 *

 * @inode:       the inode we need to release from

 * @qgroup_free: free or convert qgroup meta. Unlike normal operation, qgroup

 *               meta reservation needs to know if we are freeing qgroup

 *               reservation or just converting it into per-trans.  Normally

 *               @qgroup_free is true for error handling, and false for normal

 *               release.

 *

 * This is the same as btrfs_block_rsv_release, except that it handles the

 * tracepoint for the reservation.

	/*

	 * Since we statically set the block_rsv->size we just want to say we

	 * are releasing 0 bytes, and then we'll just get the reservation over

	 * the size free'd.

	/*

	 * Insert size for the number of outstanding extents, 1 normal size for

	 * updating the inode.

	/*

	 * For qgroup rsv, the calculation is very simple:

	 * account one nodesize for each outstanding extent

	 *

	 * This is overestimating in most cases.

	/*

	 * finish_ordered_io has to update the inode, so add the space required

	 * for an inode update.

	/*

	 * If we are a free space inode we need to not flush since we will be in

	 * the middle of a transaction commit.  We also don't need the delalloc

	 * mutex since we won't race with anybody.  We need this mostly to make

	 * lockdep shut its filthy mouth.

	 *

	 * If we have a transaction open (can happen if we call truncate_block

	 * from truncate), then we need FLUSH_LIMIT so we don't deadlock.

	/*

	 * We always want to do it this way, every other way is wrong and ends

	 * in tears.  Pre-reserving the amount we are going to add will always

	 * be the right way, because otherwise if we have enough parallelism we

	 * could end up with thousands of inodes all holding little bits of

	 * reservations they were able to make previously and the only way to

	 * reclaim that space is to ENOSPC out the operations and clear

	 * everything out and try again, which is bad.  This way we just

	 * over-reserve slightly, and clean up the mess when we are done.

	/*

	 * Now we need to update our outstanding extents and csum bytes _first_

	 * and then add the reservation to the block_rsv.  This keeps us from

	 * racing with an ordered completion or some such that would think it

	 * needs to free the reservation we just made.

 Now we can safely add our space to our block rsv */

/**

 * Release a metadata reservation for an inode

 *

 * @inode: the inode to release the reservation for.

 * @num_bytes: the number of bytes we are releasing.

 * @qgroup_free: free qgroup reservation or convert it to per-trans reservation

 *

 * This will release the metadata reservation for an inode.  This can be called

 * once we complete IO for a given set of bytes to release their metadata

 * reservations, or on error for the same reason.

/**

 * btrfs_delalloc_release_extents - release our outstanding_extents

 * @inode: the inode to balance the reservation for.

 * @num_bytes: the number of bytes we originally reserved with

 *

 * When we reserve space we increase outstanding_extents for the extents we may

 * add.  Once we've set the range as delalloc or created our ordered extents we

 * have outstanding_extents to track the real usage, so we use this to free our

 * temporarily tracked outstanding_extents.  This _must_ be used in conjunction

 * with btrfs_delalloc_reserve_metadata.

/**

 * btrfs_delalloc_reserve_space - reserve data and metadata space for

 * delalloc

 * @inode: inode we're writing to

 * @start: start range we are writing to

 * @len: how long the range we are writing to

 * @reserved: mandatory parameter, record actually reserved qgroup ranges of

 * 	      current reservation.

 *

 * This will do the following things

 *

 * - reserve space in data space info for num bytes

 *   and reserve precious corresponding qgroup space

 *   (Done in check_data_free_space)

 *

 * - reserve space for metadata space, based on the number of outstanding

 *   extents and how much csums will be needed

 *   also reserve metadata space in a per root over-reserve method.

 * - add to the inodes->delalloc_bytes

 * - add it to the fs_info's delalloc inodes list.

 *   (Above 3 all done in delalloc_reserve_metadata)

 *

 * Return 0 for success

 * Return <0 for error(-ENOSPC or -EQUOT)

/**

 * Release data and metadata space for delalloc

 *

 * @inode:       inode we're releasing space for

 * @reserved:    list of changed/reserved ranges

 * @start:       start position of the space already reserved

 * @len:         length of the space already reserved

 * @qgroup_free: should qgroup reserved-space also be freed

 *

 * This function will release the metadata space that was not used and will

 * decrement ->delalloc_bytes and remove it from the fs_info delalloc_inodes

 * list if there are no delalloc bytes left.

 * Also it will handle the qgroup reserved space.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2016-present, Facebook, Inc.

 * All rights reserved.

 *

 307s to avoid pathologically clashing with transaction commit */

 jiffies */

/*

 * Zstd Workspace Management

 *

 * Zstd workspaces have different memory requirements depending on the level.

 * The zstd workspaces are managed by having individual lists for each level

 * and a global lru.  Forward progress is maintained by protecting a max level

 * workspace.

 *

 * Getting a workspace is done by using the bitmap to identify the levels that

 * have available workspaces and scans up.  This lets us recycle higher level

 * workspaces because of the monotonic memory guarantee.  A workspace's

 * last_used is only updated if it is being used by the corresponding memory

 * level.  Putting a workspace involves adding it back to the appropriate places

 * and adding it back to the lru if necessary.

 *

 * A timer is used to reclaim workspaces if they have not been used for

 * ZSTD_BTRFS_RECLAIM_JIFFIES.  This helps keep only active workspaces around.

 * The upper bound is provided by the workqueue limit which is 2 (percpu limit).

/*

 * zstd_reclaim_timer_fn - reclaim timer

 * @t: timer

 *

 * This scans the lru_list and attempts to reclaim any workspace that hasn't

 * been used for ZSTD_BTRFS_RECLAIM_JIFFIES.

 workspace is in use */

/*

 * zstd_calc_ws_mem_sizes - calculate monotonic memory bounds

 *

 * It is possible based on the level configurations that a higher level

 * workspace uses less memory than a lower level workspace.  In order to reuse

 * workspaces, this must be made a monotonic relationship.  This precomputes

 * the required memory for each level and enforces the monotonicity between

 * level and memory required.

/*

 * zstd_find_workspace - find workspace

 * @level: compression level

 *

 * This iterates over the set bits in the active_map beginning at the requested

 * compression level.  This lets us utilize already allocated workspaces before

 * allocating a new one.  If the workspace is of a larger size, it is used, but

 * the place in the lru_list and last_used times are not updated.  This is to

 * offer the opportunity to reclaim the workspace in favor of allocating an

 * appropriately sized one in the future.

 keep its place if it's a lower level using this */

/*

 * zstd_get_workspace - zstd's get_workspace

 * @level: compression level

 *

 * If @level is 0, then any compression level can be used.  Therefore, we begin

 * scanning from 1.  We first scan through possible workspaces and then after

 * attempt to allocate a new workspace.  If we fail to allocate one due to

 * memory pressure, go to sleep waiting for the max level workspace to free up.

 level == 0 means we can use any workspace */

/*

 * zstd_put_workspace - zstd put_workspace

 * @ws: list_head for the workspace

 *

 * When putting back a workspace, we only need to update the LRU if we are of

 * the requested compression level.  Here is where we continue to protect the

 * max level workspace or update last_used accordingly.  If the reclaim timer

 * isn't set, it is also set here.  Only the max level workspace tries and wakes

 * up waiting workspaces.

 A node is only taken off the lru if we are the corresponding level */

 Hide a max level workspace from reclaim */

 The current page to read */

 The current page to write to */

 Initialize the stream */

 map in the first page of input data */

 Allocate and map in the output buffer */

 Check to see if we are making it bigger */

 We've reached the end of our output range */

 Check if we need more output space */

 We've reached the end of the input */

 Check if we need more input */

 Cleanup */

 Check if we've hit the end of a frame */

 Check if the frame is over and we still need more input */

 ZSTD uses own workspace manager */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Red Hat.  All rights reserved.

 lookup the xattr by name */

 if size is 0, that means we want the size of the attr */

 now get the data out of our dir_item */

	/*

	 * The way things are packed into the leaf is like this

	 * |struct btrfs_dir_item|name|data|

	 * where name is the xattr name, so security.foo, and data is the

	 * content of the xattr.  data_ptr points to the location in memory

	 * where the data starts in the in memory leaf

	/*

	 * For a replace we can't just do the insert blindly.

	 * Do a lookup first (read-only btrfs_search_slot), and return if xattr

	 * doesn't exist. If it exists, fall down below to the insert/replace

	 * path - we can't race with a concurrent xattr delete, because the VFS

	 * locks the inode's i_mutex before calling setxattr or removexattr.

		/*

		 * We have an existing item in a leaf, split_leaf couldn't

		 * expand it. That item might have or not a dir_item that

		 * matches our target xattr, so lets check.

 logic error */

		/*

		 * We're doing a replace, and it must be atomic, that is, at

		 * any point in time we have either the old or the new xattr

		 * value in the tree. We don't want readers (getxattr and

		 * listxattrs) to miss a value, this is specially important

		 * for ACLs.

 No other xattrs packed in the same leaf item. */

 There are other xattrs packed in the same item. */

		/*

		 * Insert, and we had space for the xattr, so path->slots[0] is

		 * where our xattr dir_item is and btrfs_insert_xattr_item()

		 * filled it.

/*

 * @value: "" makes the attribute to empty, NULL removes it

		/*

		 * 1 unit for inserting/updating/deleting the xattr

		 * 1 unit for the inode item update

		/*

		 * This can happen when smack is enabled and a directory is being

		 * created. It happens through d_instantiate_new(), which calls

		 * smack_d_instantiate(), which in turn calls __vfs_setxattr() to

		 * set the transmute xattr (XATTR_NAME_SMACKTRANSMUTE) on the

		 * inode. We have already reserved space for the xattr and inode

		 * update at btrfs_mkdir(), so just use the transaction handle.

		 * We don't join or start a transaction, as that will reset the

		 * block_rsv of the handle and trigger a warning for the start

		 * case.

	/*

	 * ok we want all objects associated with this id.

	 * NOTE: we set key.offset = 0; because we want to start with the

	 * first xattr that we find and walk forward

 search for our xattrs */

 this is where we start walking through the path */

			/*

			 * if we've reached the last slot in this leaf we need

			 * to go to the next leaf and reset everything

 check to make sure this item is what we want */

			/*

			 * We are just looking for how big our buffer needs to

			 * be.

	/*

	 * We're holding a transaction handle, so use a NOFS memory allocation

	 * context to avoid deadlock if reclaim happens.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/*

 * btrfs_inode_lock - lock inode i_rwsem based on arguments passed

 *

 * ilock_flags can have the following bit set:

 *

 * BTRFS_ILOCK_SHARED - acquire a shared lock on the inode

 * BTRFS_ILOCK_TRY - try to acquire the lock, if fails on first attempt

 *		     return -EAGAIN

 * BTRFS_ILOCK_MMAP - acquire a write lock on the i_mmap_lock

/*

 * btrfs_inode_unlock - unock inode i_rwsem

 *

 * ilock_flags should contain the same bits set as passed to btrfs_inode_lock()

 * to decide whether the lock acquired is shared or exclusive.

/*

 * Cleanup all submitted ordered extents in specified range to handle errors

 * from the btrfs_run_delalloc_range() callback.

 *

 * NOTE: caller must ensure that when an error happens, it can not call

 * extent_clear_unlock_delalloc() to clear both the bits EXTENT_DO_ACCOUNTING

 * and EXTENT_DELALLOC simultaneously, because that causes the reserved metadata

 * to be released, which we want to happen only when finishing the ordered

 * extent (btrfs_finish_ordered_io()).

		/*

		 * For locked page, we will call end_extent_writepage() on it

		 * in run_delalloc_range() for the error handling.  That

		 * end_extent_writepage() function will call

		 * btrfs_mark_ordered_io_finished() to clear page Ordered and

		 * run the ordered extent accounting.

		 *

		 * Here we can't just clear the Ordered bit, or

		 * btrfs_mark_ordered_io_finished() would skip the accounting

		 * for the page range, and the ordered extent will never finish.

		/*

		 * Here we just clear all Ordered bits for every page in the

		 * range, then __endio_write_update_ordered() will handle

		 * the ordered extent accounting for the range.

 The locked page covers the full range, nothing needs to be done */

	/*

	 * In case this page belongs to the delalloc range being instantiated

	 * then skip it, since the first page of a range is going to be

	 * properly cleaned up by the caller of run_delalloc_range

/*

 * this does all the hard work for inserting an inline extent into

 * the btree.  The caller should have done a btrfs_drop_extents so that

 * no overlapping inline items exist in the btree

	/*

	 * We align size to sectorsize for inline extents just for simplicity

	 * sake.

	/*

	 * we're an inline extent, so nobody can

	 * extend the file past i_size without locking

	 * a page we already have locked.

	 *

	 * We must do any isize and inode updates

	 * before we unlock the pages.  Otherwise we

	 * could end up racing with unlink.

/*

 * conditionally insert an inline extent into the file.  This

 * does the checks required to make sure the data is small enough

 * to fit as an inline extent.

	/*

	 * Don't forget to free the reserved space, as for inlined extent

	 * it won't count as data extent, free them directly here.

	 * And at reserve time, it's always aligned to page size, so

	 * just free one page here.

 -ENOMEM */

/*

 * Check if the inode has flags compatible with compression

/*

 * Check if the inode needs to be submitted to compression, based on mount

 * options, defragmentation, properties or heuristics.

	/*

	 * Special check for subpage.

	 *

	 * We lock the full page then run each delalloc range in the page, thus

	 * for the following case, we will hit some subpage specific corner case:

	 *

	 * 0		32K		64K

	 * |	|///////|	|///////|

	 *		\- A		\- B

	 *

	 * In above case, both range A and range B will try to unlock the full

	 * page [0, 64K), causing the one finished later will have page

	 * unlocked already, triggering various page lock requirement BUG_ON()s.

	 *

	 * So here we add an artificial limit that subpage compression can only

	 * if the range is fully page aligned.

	 *

	 * In theory we only need to ensure the first page is fully covered, but

	 * the tailing partial page will be locked until the full compression

	 * finishes, delaying the write of other range.

	 *

	 * TODO: Make btrfs_run_delalloc_range() to lock all delalloc range

	 * first to prevent any submitted async extent to unlock the full page.

	 * By this, we can ensure for subpage case that only the last async_cow

	 * will unlock the full page.

 force compress */

 defrag ioctl */

 bad compression ratios */

 If this is a small write inside eof, kick off a defrag */

/*

 * we create compressed extents in two phases.  The first

 * phase compresses a range of pages that have already been

 * locked (both pages and state bits are locked).

 *

 * This is done inside an ordered work queue, and the compression

 * is spread across many cpus.  The actual IO submission is step

 * two, and the ordered work queue takes care of making sure that

 * happens in the same order things were put onto the queue by

 * writepages and friends.

 *

 * If this code finds it can't get good compression, it puts an

 * entry onto the work queue to write the uncompressed bytes.  This

 * makes sure that both compressed inodes and uncompressed inodes

 * are written in the same order that the flusher thread sent them

 * down.

	/*

	 * We need to save i_size before now because it could change in between

	 * us evaluating the size and assigning it.  This is because we lock and

	 * unlock the page in truncate and fallocate, and then modify the i_size

	 * later on.

	 *

	 * The barriers are to emulate READ_ONCE, remove that once i_size_read

	 * does that for us.

	/*

	 * we don't want to send crud past the end of i_size through

	 * compression, that's just a waste of CPU time.  So, if the

	 * end of the file is before the start of our current

	 * requested range of bytes, we bail out to the uncompressed

	 * cleanup code that can deal with all of this.

	 *

	 * It isn't really the fastest way to fix things, but this is a

	 * very uncommon corner.

	/*

	 * Skip compression for a small file range(<=blocksize) that

	 * isn't an inline extent, since it doesn't save disk space at all.

	/*

	 * For subpage case, we require full page alignment for the sector

	 * aligned range.

	 * Thus we must also check against @actual_end, not just @end.

	/*

	 * we do compression for mount -o compress and when the

	 * inode has not been flagged as nocompress.  This flag can

	 * change at any time if we discover bad compression ratios.

 just bail out to the uncompressed code */

		/*

		 * we need to call clear_page_dirty_for_io on each

		 * page in the range.  Otherwise applications with the file

		 * mmap'd can wander in and change the page contents while

		 * we are compressing them.

		 *

		 * If the compression fails for any reason, we set the pages

		 * dirty again later on.

		 *

		 * Note that the remaining part is redirtied, the start pointer

		 * has moved, the end is the original one.

 Compression level is applied here and only here */

			/* zero the tail end of the last page, we might be

			 * sending it down to disk

	/*

	 * Check cow_file_range() for why we don't even try to create inline

	 * extent for subpage case.

 lets try to make an inline extent */

			/* we didn't compress the entire range, try

			 * to make an uncompressed inline extent.

 try making a compressed inline extent */

			/*

			 * inline extent creation worked or returned error,

			 * we don't need to create any more async work items.

			 * Unlock and free up our temp pages.

			 *

			 * We use DO_ACCOUNTING here because we need the

			 * delalloc_release_metadata to be done _after_ we drop

			 * our outstanding extent for clearing delalloc for this

			 * range.

			/*

			 * Ensure we only free the compressed pages if we have

			 * them allocated, as we can still reach here with

			 * inode_need_compress() == false.

		/*

		 * we aren't doing an inline extent round the compressed size

		 * up to a block size boundary so the allocator does sane

		 * things

		/*

		 * one last check to make sure the compression is really a

		 * win, compare the page count read with the blocks on disk,

		 * compression must free at least one sector size

			/*

			 * The async work queues will take care of doing actual

			 * allocation on disk for these compressed pages, and

			 * will submit them to the elevator.

		/*

		 * the compression code ran but failed to make things smaller,

		 * free any pages it allocated and our page pointer array

 flag the file so we don't compress in the future */

	/*

	 * No compression, but we still need to write the pages in the file

	 * we've been given so far.  redirty the locked page if it corresponds

	 * to our extent and set things up for the async work queue to run

	 * cow_file_range to do the normal delalloc dance.

 unlocked later on in the async handlers */

	/*

	 * Call cow_file_range() to run the delalloc range directly, since we

	 * won't go to NOCOW or async path again.

	 *

	 * Also we call cow_file_range() with @unlock_page == 0, so that we

	 * can directly submit them without interruption.

 Inline extent inserted, page gets unlocked and everything is done */

 All pages will be unlocked, including @locked_page */

	/*

	 * If async_chunk->locked_page is in the async_extent range, we need to

	 * handle it.

 We have fall back to uncompressed write */

		/*

		 * Here we used to try again by going back to non-compressed

		 * path for ENOSPC.  But we can't reserve space even for

		 * compressed size, how could it work for uncompressed size

		 * which requires larger size?  So here we directly go error

		 * path.

 Here we're doing allocation and writeback of the compressed pages */

 len */

 orig_start */

 block_start */

 block_len */

 orig_block_len */

 ram_bytes */

 file_offset */

 disk_bytenr */

 num_bytes */

 disk_num_bytes */

 Clear dirty, set writeback and unlock the pages. */

 file_offset */

 num_bytes */

 disk_bytenr */

 compressed_len */

 compressed_pages */

/*

 * Phase two of compressed writeback.  This is the ordered portion of the code,

 * which only gets called in the order the work was queued.  We walk all the

 * async extents created by compress_file_range and send them down to the disk.

		/*

		 * if block start isn't an actual block number then find the

		 * first block in this inode and use that as a hint.  If that

		 * block is also bogus then just don't worry about it.

/*

 * when extent_io.c finds a delayed allocation range in the file,

 * the call backs end up in this code.  The basic idea is to

 * allocate extents on disk for the range, and create ordered data structs

 * in ram to track those extents.

 *

 * locked_page is the page that writepage had locked already.  We use

 * it to make sure we don't do extra locks or unlocks.

 *

 * *page_started is set to one if we unlock locked_page and do everything

 * required to start IO on it.  It may be clean and already done with

 * IO when we return.

	/*

	 * Due to the page size limit, for subpage we can only trigger the

	 * writeback for the dirty sectors of page, that means data writeback

	 * is doing more writeback than what we want.

	 *

	 * This is especially unexpected for some call sites like fallocate,

	 * where we only increase i_size after everything is done.

	 * This means we can trigger inline extent even if we didn't want to.

	 * So here we skip inline extent creation completely.

 lets try to make an inline extent */

			/*

			 * We use DO_ACCOUNTING here because we need the

			 * delalloc_release_metadata to be run _after_ we drop

			 * our outstanding extent for clearing delalloc for this

			 * range.

			/*

			 * locked_page is locked by the caller of

			 * writepage_delalloc(), not locked by

			 * __process_pages_contig().

			 *

			 * We can't let __process_pages_contig() to unlock it,

			 * as it doesn't have any subpage::writers recorded.

			 *

			 * Here we manually unlock the page, since the caller

			 * can't use page_started to determine if it's an

			 * inline extent or a compressed extent.

	/*

	 * Relocation relies on the relocated extents to have exactly the same

	 * size as the original extents. Normally writeback for relocation data

	 * extents follows a NOCOW path because relocation preallocates the

	 * extents. However, due to an operation such as scrub turning a block

	 * group to RO mode, it may fallback to COW mode, so we must make sure

	 * an extent allocated during COW has exactly the requested size and can

	 * not be split into smaller extents, otherwise relocation breaks and

	 * fails during the stage where it updates the bytenr of file extent

	 * items.

 len */

 orig_start */

 block_start */

 block_len */

 orig_block_len */

 ram_bytes */

 compress_type */

 type */);

			/*

			 * Only drop cache here, and process as normal.

			 *

			 * We must not allow extent_clear_unlock_delalloc()

			 * at out_unlock label to free meta of this ordered

			 * extent, as its meta should be freed by

			 * btrfs_finish_ordered_io().

			 *

			 * So we must continue until @start is increased to

			 * skip current ordered extent.

		/*

		 * We're not doing compressed IO, don't unlock the first page

		 * (which the caller expects to stay locked), don't clear any

		 * dirty bits and don't set any writeback bits

		 *

		 * Do set the Ordered (Private2) bit so we know this page was

		 * properly setup for writepage.

		/*

		 * btrfs_reloc_clone_csums() error, since start is increased

		 * extent_clear_unlock_delalloc() at out_unlock label won't

		 * free metadata of current ordered extent, we're OK to exit.

	/*

	 * If we reserved an extent for our delalloc range (or a subrange) and

	 * failed to create the respective ordered extent, then it means that

	 * when we reserved the extent we decremented the extent's size from

	 * the data space_info's bytes_may_use counter and incremented the

	 * space_info's bytes_reserved counter by the same amount. We must make

	 * sure extent_clear_unlock_delalloc() does not try to decrement again

	 * the data space_info's bytes_may_use counter, therefore we do not pass

	 * it the flag EXTENT_CLEAR_DATA_RESV.

/*

 * work queue call back to started compression on a file and pages

/*

 * work queue call back to submit previously compressed pages

	/*

	 * ->inode could be NULL if async_chunk_start has failed to compress,

	 * in which case we don't have anything to submit, yet we need to

	 * always adjust ->async_delalloc_pages as its paired with the init

	 * happening in cow_file_range_async

 atomic_sub_return implies a barrier */

		/*

		 * igrab is called higher up in the call chain, take only the

		 * lightweight reference for the callback lifetime

		/*

		 * The locked_page comes all the way from writepage and its

		 * the original page we were actually given.  As we spread

		 * this large delalloc region across multiple async_chunk

		 * structs, only the first struct needs a pointer to locked_page

		 *

		 * This way we don't need racey decisions about who is supposed

		 * to unlock it.

			/*

			 * Depending on the compressibility, the pages might or

			 * might not go through async.  We want all of them to

			 * be accounted against wbc once.  Let's do it here

			 * before the paths diverge.  wbc accounting is used

			 * only for foreign writeback detection and doesn't

			 * need full accuracy.  Just account the whole thing

			 * against the first page.

	/*

	 * If EXTENT_NORESERVE is set it means that when the buffered write was

	 * made we had not enough available data space and therefore we did not

	 * reserve data space for it, since we though we could do NOCOW for the

	 * respective file range (either there is prealloc extent or the inode

	 * has the NOCOW bit set).

	 *

	 * However when we need to fallback to COW mode (because for example the

	 * block group for the corresponding extent was turned to RO mode by a

	 * scrub or relocation) we need to do the following:

	 *

	 * 1) We increment the bytes_may_use counter of the data space info.

	 *    If COW succeeds, it allocates a new data extent and after doing

	 *    that it decrements the space info's bytes_may_use counter and

	 *    increments its bytes_reserved counter by the same amount (we do

	 *    this at btrfs_add_reserved_bytes()). So we need to increment the

	 *    bytes_may_use counter to compensate (when space is reserved at

	 *    buffered write time, the bytes_may_use counter is incremented);

	 *

	 * 2) We clear the EXTENT_NORESERVE bit from the range. We do this so

	 *    that if the COW path fails for any reason, it decrements (through

	 *    extent_clear_unlock_delalloc()) the bytes_may_use counter of the

	 *    data space info, which we incremented in the step above.

	 *

	 * If we need to fallback to cow and the inode corresponds to a free

	 * space cache inode or an inode of the data relocation tree, we must

	 * also increment bytes_may_use of the data space_info for the same

	 * reason. Space caches and relocated data extents always get a prealloc

	 * extent for them, however scrub or balance may have set the block

	 * group that contains that extent to RO mode and therefore force COW

	 * when starting writeback.

/*

 * when nowcow writeback call back.  This checks for snapshots or COW copies

 * of the extents that exist in the file, and COWs the file as required.

 *

 * If no cow copies or snapshots exist, we write directly to the existing

 * blocks on disk

		/*

		 * If there is no extent for our range when doing the initial

		 * search, then go back to the previous slot as it will be the

		 * one containing the search offset

 Go to next leaf if we have exhausted the current one */

 Didn't find anything for our INO */

		/*

		 * Keep searching until we find an EXTENT_ITEM or there are no

		 * more extents for this inode

 Found key is not EXTENT_DATA_KEY or starts after req range */

		/*

		 * If the found extent starts after requested offset, then

		 * adjust extent_end to be right before this extent begins

		/*

		 * Found extent which begins before our range and potentially

		 * intersect it

			/*

			 * If the extent we got ends before our current offset,

			 * skip to the next extent.

 Skip holes */

 Skip compressed/encrypted/encoded extents */

			/*

			 * If extent is created before the last volume's snapshot

			 * this implies the extent is shared, hence we can't do

			 * nocow. This is the same check as in

			 * btrfs_cross_ref_exist but without calling

			 * btrfs_search_slot.

			/*

			 * The following checks can be expensive, as they need to

			 * take other locks and do btree or rbtree searches, so

			 * release the path to avoid blocking other tasks for too

			 * long.

				/*

				 * ret could be -EIO if the above fails to read

				 * metadata.

			/*

			 * If there are pending snapshots for this root, we

			 * fall into common COW way

			/*

			 * force cow if csum exists in the range.

			 * this ensure that csum for a given extent are

			 * either valid or do not exist.

				/*

				 * ret could be -EIO if the above fails to read

				 * metadata.

 If the extent's block group is RO, we must COW */

 Skip extents outside of our requested range */

 If this triggers then we have a memory corruption */

		/*

		 * If nocow is false then record the beginning of the range

		 * that needs to be COWed

		/*

		 * COW range from cow_start to found_key.offset - 1. As the key

		 * will contain the beginning of the first extent that can be

		 * NOCOW, following one which needs to be COW'ed

 block_start */

 block_len */

 orig_block_len */

			/*

			 * Error handled later, as we must prevent

			 * extent_clear_unlock_delalloc() in error handler

			 * from freeing metadata of created ordered extent.

		/*

		 * btrfs_reloc_clone_csums() error, now we're OK to call error

		 * handler, as metadata for created ordered extent will only

		 * be freed by btrfs_finish_ordered_io().

/*

 * Function to process delayed allocation (create CoW) for ranges which are

 * being touched for the first time.

	/*

	 * The range must cover part of the @locked_page, or the returned

	 * @page_started can confuse the caller.

		/*

		 * Normally on a zoned device we're only doing COW writes, but

		 * in case of relocation on a zoned filesystem we have taken

		 * precaution, that we're only writing sequentially. It's safe

		 * to use run_delalloc_nocow() here, like for  regular

		 * preallocated inodes.

 not delalloc, ignore it */

		/*

		 * See the explanation in btrfs_merge_delalloc_extent, the same

		 * applies here, just in reverse.

/*

 * Handle merged delayed allocation extents so we can keep track of new extents

 * that are just merged onto old extents, such as when we are doing sequential

 * writes, so we can properly account for the metadata space we'll need.

 not delalloc, ignore it */

 we're not bigger than the max, unreserve the space and go */

	/*

	 * We have to add up either side to figure out how many extents were

	 * accounted for before we merged into one big extent.  If the number of

	 * extents we accounted for is <= the amount we need for the new range

	 * then we can return, otherwise drop.  Think of it like this

	 *

	 * [ 4k][MAX_SIZE]

	 *

	 * So we've grown the extent by a MAX_SIZE extent, this would mean we

	 * need 2 outstanding extents, on one side we have 1 and the other side

	 * we have 1 so they are == and we can return.  But in this case

	 *

	 * [MAX_SIZE+4k][MAX_SIZE+4k]

	 *

	 * Each range on their own accounts for 2 extents, but merged together

	 * they are only 3 extents worth of accounting, so we need to drop in

	 * this case.

/*

 * Properly track delayed allocation bytes in the inode and to maintain the

 * list of inodes that have pending delalloc work to be done.

	/*

	 * set_bit and clear bit hooks normally require _irqsave/restore

	 * but in this case, we are only testing for the DELALLOC

	 * bit, which is only set or cleared with irqs on

 For sanity tests */

/*

 * Once a range is no longer delalloc this function ensures that proper

 * accounting happens.

	/*

	 * set_bit and clear bit hooks normally require _irqsave/restore

	 * but in this case, we are only testing for the DELALLOC

	 * bit, which is only set or cleared with irqs on

		/*

		 * We don't reserve metadata space for space cache inodes so we

		 * don't need to call delalloc_release_metadata if there is an

		 * error.

 For sanity tests. */

/*

 * in order to insert checksums into the metadata in large chunks,

 * we wait until bio submission time.   All the pages in the bio are

 * checksummed and sums are attached onto the ordered extent record.

 *

 * At IO completion time the cums attached on the ordered extent record

 * are inserted into the btree

/*

 * Split an extent_map at [start, start + len]

 *

 * This function is intended to be used only for extract_ordered_extent().

 Sanity check */

 First, replace the em with a new extent_map starting from * em->start */

	/*

	 * Now we only have an extent_map at:

	 *     [em->start, em->start + pre] if pre != 0

	 *     [em->start, em->start + em->len - post] if pre == 0

 Insert the middle extent_map */

 Once for us */

 Once for the tree */

 No need to split */

 We cannot split once end_bio'd ordered extent */

 We cannot split a compressed ordered extent */

 bio must be in one ordered extent */

 Checksum list should be empty */

/*

 * extent_io.c submission hook. This does the right thing for csum calculation

 * on write, or reading the csums from the tree before a read.

 *

 * Rules about async/sync submit,

 * a) read:				sync submit

 *

 * b) write without checksum:		sync submit

 *

 * c) write with checksum:

 *    c-1) if bio is issued by fsync:	sync submit

 *         (sync_writers != 0)

 *

 *    c-2) if root is reloc root:	sync submit

 *         (only in case of buffered IO)

 *

 *    c-3) otherwise:			async submit

			/*

			 * Lookup bio sums does extra checks around whether we

			 * need to csum or not, which is why we ignore skip_sum

			 * here.

 csum items have already been cloned */

 we're doing a write, do the async checksumming */

/*

 * given a list of ordered sums record them in the inode.  This happens

 * at IO completion time based on sums calculated at bio submission time.

		/*

		 * There can't be any extents following eof in this case so just

		 * set the delalloc new bit for the range directly.

 see btrfs_writepage_start_hook for details on why this is required */

	/*

	 * This is similar to page_mkwrite, we need to reserve the space before

	 * we take the page lock.

	/*

	 * Before we queued this fixup, we took a reference on the page.

	 * page->mapping may go NULL, but it shouldn't be moved to a different

	 * address space.

		/*

		 * Unfortunately this is a little tricky, either

		 *

		 * 1) We got here and our page had already been dealt with and

		 *    we reserved our space, thus ret == 0, so we need to just

		 *    drop our space reservation and bail.  This can happen the

		 *    first time we come into the fixup worker, or could happen

		 *    while waiting for the ordered extent.

		 * 2) Our page was already dealt with, but we happened to get an

		 *    ENOSPC above from the btrfs_delalloc_reserve_space.  In

		 *    this case we obviously don't have anything to release, but

		 *    because the page was already dealt with we don't want to

		 *    mark the page with an error, so make sure we're resetting

		 *    ret to 0.  This is why we have this check _before_ the ret

		 *    check, because we do not want to have a surprise ENOSPC

		 *    when the page was already properly dealt with.

	/*

	 * We can't mess with the page state unless it is locked, so now that

	 * it is locked bail if we failed to make our space reservation.

 already ordered? We're done */

	/*

	 * Everything went as planned, we're now the owner of a dirty page with

	 * delayed allocation bits set and space reserved for our COW

	 * destination.

	 *

	 * The page was dirty when we started, nothing should have cleaned it.

		/*

		 * We hit ENOSPC or other errors.  Update the mapping and page

		 * to reflect the errors and clean the page.

	/*

	 * As a precaution, do a delayed iput in case it would be the last iput

	 * that could need flushing space. Recursing back to fixup worker would

	 * deadlock.

/*

 * There are a few paths in the higher layers of the kernel that directly

 * set the page dirty bit without asking the filesystem if it is a

 * good idea.  This causes problems because we want to make sure COW

 * properly happens and the data=ordered rules are followed.

 *

 * In our case any range that doesn't have the ORDERED bit set

 * hasn't been properly setup for IO.  We kick off an async process

 * to fix it up.  The async helper will wait for ordered extents, set

 * the delalloc bit and make it safe to write the page.

 This page has ordered extent covering it already */

	/*

	 * PageChecked is set below when we create a fixup worker for this page,

	 * don't try to create another one if we're already PageChecked()

	 *

	 * The extent_io writepage code will redirty the page if we send back

	 * EAGAIN.

	/*

	 * We are already holding a reference to this inode from

	 * write_cache_pages.  We need to hold it because the space reservation

	 * takes place outside of the page lock, and we can't trust

	 * page->mapping outside of the page lock.

	/*

	 * we may be replacing one extent in the tree with another.

	 * The new extent is pinned in the extent map, and we don't want

	 * to drop it from the cache until it is completely in the btree.

	 *

	 * So, tell btrfs_drop_extents to leave this extent in the cache.

	 * the caller is expected to unpin it and allow it to be merged

	 * with the others.

	/*

	 * If we dropped an inline extent here, we know the range where it is

	 * was not marked with the EXTENT_DELALLOC_NEW bit, so we update the

	 * number of bytes only for that range containing the inline extent.

	 * The remaining of the range will be processed when clearning the

	 * EXTENT_DELALLOC_BIT bit through the ordered extent completion.

 Encryption and other encoding is reserved and all 0 */

	/*

	 * For delalloc, when completing an ordered extent we update the inode's

	 * bytes when clearing the range in the inode's io tree, so pass false

	 * as the argument 'update_inode_bytes' to insert_reserved_file_extent(),

	 * except if the ordered extent was truncated.

/*

 * As ordered data IO finishes, this gets called so we can finish

 * an ordered extent if the range of bytes in the file it covers are

 * fully written.

 A valid bdev implies a write on a sequential zone */

 Truncated the entire extent, don't bother adding */

 Logic error */

 -ENOMEM or corruption */

	/*

	 * If this is a new delalloc range, clear its new delalloc flag to

	 * update the inode's number of bytes. This needs to be done first

	 * before updating the inode item.

 -ENOMEM or corruption */

		/*

		 * If we failed to finish this ordered extent for any reason we

		 * need to make sure BTRFS_ORDERED_IOERR is set on the ordered

		 * extent, and mark the inode with the error if it wasn't

		 * already set.  Any error during writeback would have already

		 * set the mapping error, so we need to set it if we're the ones

		 * marking this ordered extent as failed.

 Drop the cache for the part of the extent we didn't write. */

		/*

		 * If the ordered extent had an IOERR or something else went

		 * wrong we need to return the space for this ordered extent

		 * back to the allocator.  We only free the extent in the

		 * truncated case if we didn't write out the extent at all.

		 *

		 * If we made it past insert_reserved_file_extent before we

		 * errored out then we don't need to do this as the accounting

		 * has already been done.

			/*

			 * Discard the range before returning it back to the

			 * free space pool

	/*

	 * This needs to be done to make sure anybody waiting knows we are done

	 * updating everything for this ordered extent.

 once for us */

 once for the tree */

/*

 * check_data_csum - verify checksum of one sector of uncompressed data

 * @inode:	inode

 * @io_bio:	btrfs_io_bio which contains the csum

 * @bio_offset:	offset to the beginning of the bio (in bytes)

 * @page:	page where is the data to be verified

 * @pgoff:	offset inside the page

 * @start:	logical offset in the file

 *

 * The length of such check is always one sector size.

/*

 * When reads are done, we need to check csums to verify the data is correct.

 * if there's a match, we allow the bio to finish.  If not, the code in

 * extent_io.c will try to find good copies for us.

 *

 * @bio_offset:	offset to the beginning of the bio (in bytes)

 * @start:	file offset of the range start

 * @end:	file offset of the range end (inclusive)

 *

 * Return a bitmap where bit set means a csum mismatch, and bit not set means

 * csum match.

	/*

	 * This only happens for NODATASUM or compressed read.

	 * Normally this should be covered by above check for compressed read

	 * or the next check for NODATASUM.  Just do a quicker exit here.

 Skip the range without csum for data reloc inode */

/*

 * btrfs_add_delayed_iput - perform a delayed iput on @inode

 *

 * @inode: The inode we want to perform iput on

 *

 * This function uses the generic vfs_inode::i_count to track whether we should

 * just decrement it (in case it's > 1) or if this is the last iput then link

 * the inode to the delayed iput machinery. Delayed iputs are processed at

 * transaction commit time/superblock commit/cleaner kthread.

/**

 * Wait for flushing all delayed iputs

 *

 * @fs_info:  the filesystem

 *

 * This will wait on any delayed iputs that are currently running with KILLABLE

 * set.  Once they are all done running we will return, unless we are killed in

 * which case we return EINTR. This helps in user operations like fallocate etc

 * that might get blocked on the iputs.

 *

 * Return EINTR if we were killed, 0 if nothing's pending

/*

 * This creates an orphan entry for the given inode in case something goes wrong

 * in the middle of an unlink.

/*

 * We have done the delete so we can go ahead and remove the orphan item for

 * this particular inode.

/*

 * this cleans up any orphans that may be left on the list from the last use

 * of this root.

		/*

		 * if ret == 0 means we found what we were searching for, which

		 * is weird, but possible, so only screw with path if we didn't

		 * find the key and see if we have stuff that matches

 pull out the item */

 make sure the item matches what we want */

 release the path since we're done with it */

		/*

		 * this is where we are basically btrfs_lookup, without the

		 * crossing root thing.  we store the inode number in the

		 * offset of the orphan item.

			/*

			 * This is an orphan in the tree root. Currently these

			 * could come from 2 sources:

			 *  a) a root (snapshot/subvolume) deletion in progress

			 *  b) a free space cache inode

			 * We need to distinguish those two, as the orphan item

			 * for a root must not get deleted before the deletion

			 * of the snapshot/subvolume's tree completes.

			 *

			 * btrfs_find_orphan_roots() ran before us, which has

			 * found all deleted roots and loaded them into

			 * fs_info->fs_roots_radix. So here we can find if an

			 * orphan item corresponds to a deleted root by looking

			 * up the root from that radix tree.

 prevent this orphan from being found again */

		/*

		 * If we have an inode with links, there are a couple of

		 * possibilities:

		 *

		 * 1. We were halfway through creating fsverity metadata for the

		 * file. In that case, the orphan item represents incomplete

		 * fsverity metadata which must be cleaned up with

		 * btrfs_drop_verity_items and deleting the orphan item.



		 * 2. Old kernels (before v3.12) used to create an

		 * orphan item for truncate indicating that there were possibly

		 * extent items past i_size that needed to be deleted. In v3.12,

		 * truncate was changed to update i_size in sync with the extent

		 * items, but the (useless) orphan item was still created. Since

		 * v4.18, we don't create the orphan item for truncate at all.

		 *

		 * So, this item could mean that we need to do a truncate, but

		 * only if this filesystem was last used on a pre-v3.12 kernel

		 * and was not cleanly unmounted. The odds of that are quite

		 * slim, and it's a pain to do the truncate now, so just delete

		 * the orphan item.

		 *

		 * It's also possible that this orphan item was supposed to be

		 * deleted but wasn't. The inode number may have been reused,

		 * but either way, we can delete the orphan item.

 this will do delete_inode and everything for us */

 release the path since we're done with it */

/*

 * very simple check to peek ahead in the leaf looking for xattrs.  If we

 * don't find any xattrs, we know there can't be any acls.

 *

 * slot is the slot the inode is in, objectid is the objectid of the inode

 we found a different objectid, there must not be acls */

 we found an xattr, assume we've got an acl */

		/*

		 * we found a key greater than an xattr key, there can't

		 * be any acls later on

		/*

		 * it goes inode, inode backrefs, xattrs, extents,

		 * so if there are a ton of hard links to an inode there can

		 * be a lot of backrefs.  Don't waste time searching too hard,

		 * this is just an optimization

	/* we hit the end of the leaf before we found an xattr or

	 * something larger than an xattr.  We have to assume the inode

	 * has acls

/*

 * read an inode from the btree into the in-memory inode

	/*

	 * If we were modified in the current generation and evicted from memory

	 * and then re-read we need to do a full sync since we don't have any

	 * idea about which extents were modified before we were evicted from

	 * cache.

	 *

	 * This is required for both inode re-read from disk and delayed inode

	 * in delayed_nodes_tree.

	/*

	 * We don't persist the id of the transaction where an unlink operation

	 * against the inode was last made. So here we assume the inode might

	 * have been evicted, and therefore the exact value of last_unlink_trans

	 * lost, and set it to last_trans to avoid metadata inconsistencies

	 * between the inode and its parent if the inode is fsync'ed and the log

	 * replayed. For example, in the scenario:

	 *

	 * touch mydir/foo

	 * ln mydir/foo mydir/bar

	 * sync

	 * unlink mydir/bar

	 * echo 2 > /proc/sys/vm/drop_caches   # evicts inode

	 * xfs_io -c fsync mydir/foo

	 * <power failure>

	 * mount fs, triggers fsync log replay

	 *

	 * We must make sure that when we fsync our inode foo we also log its

	 * parent inode, otherwise after log replay the parent still has the

	 * dentry with the "bar" name but our inode foo has a link count of 1

	 * and doesn't have an inode ref with the name "bar" anymore.

	 *

	 * Setting last_unlink_trans to last_trans is a pessimistic approach,

	 * but it guarantees correctness at the expense of occasional full

	 * transaction commits on fsync if our inode is a directory, or if our

	 * inode is not a directory, logging its parent unnecessarily.

	/*

	 * Same logic as for last_unlink_trans. We don't persist the generation

	 * of the last transaction where this inode was used for a reflink

	 * operation, so after eviction and reloading the inode we must be

	 * pessimistic and assume the last transaction that modified the inode.

	/*

	 * try to precache a NULL acl entry for files that don't have

	 * any xattrs or acls

/*

 * given a leaf and an inode, copy the inode fields into the leaf

/*

 * copy everything in the in-memory inode into the btree.

/*

 * copy everything in the in-memory inode into the btree.

	/*

	 * If the inode is a free space inode, we can deadlock during commit

	 * if we put it into the delayed code.

	 *

	 * The data relocation inode should also be directly updated

	 * without delay

/*

 * unlink helper that gets used here in inode.c and in the tree logging

 * recovery code.  It remove a link in a directory with a given name, and

 * also drops the back refs in the inode to the directory

	/*

	 * If we don't have dir index, we have to get it by looking up

	 * the inode ref, since we get the inode ref, remove it directly,

	 * it is unnecessary to do delayed deletion.

	 *

	 * But if we have dir index, needn't search inode ref to get it.

	 * Since the inode ref is close to the inode item, it is better

	 * that we delay to delete it, and just do this deletion when

	 * we update the inode item.

	/*

	 * If we have a pending delayed iput we could end up with the final iput

	 * being run in btrfs-cleaner context.  If we have enough of these built

	 * up we can end up burning a lot of time in btrfs-cleaner without any

	 * way to throttle the unlinks.  Since we're currently holding a ref on

	 * the inode we can run the delayed iput here without any issues as the

	 * final iput won't be done until after we drop the ref we're currently

	 * holding.

/*

 * helper to start transaction for unlink and rmdir.

 *

 * unlink and rmdir are special in btrfs, they do not always free space, so

 * if we cannot make our reservations the normal way try and see if there is

 * plenty of slack room in the global reserve to migrate, otherwise we cannot

 * allow the unlink to occur.

	/*

	 * 1 for the possible orphan item

	 * 1 for the dir item

	 * 1 for the dir index

	 * 1 for the inode ref

	 * 1 for the inode

	/*

	 * This is a placeholder inode for a subvolume we didn't have a

	 * reference to at the time of the snapshot creation.  In the meantime

	 * we could have renamed the real subvol link into our snapshot, so

	 * depending on btrfs_del_root_ref to return -ENOENT here is incorrect.

	 * Instead simply lookup the dir_index_item for this entry so we can

	 * remove it.  Otherwise we know we have a ref to the root and we can

	 * call btrfs_del_root_ref, and it _shouldn't_ fail.

/*

 * Helper to check if the subvolume references other subvolumes or if it's

 * default.

 Make sure this root isn't set as the default subvol */

 Delete all dentries for inodes belonging to the root */

			/*

			 * btrfs_drop_inode will have it removed from the inode

			 * cache when its usage count hits zero.

	/*

	 * Don't allow to delete a subvolume with send in progress. This is

	 * inside the inode lock so the error handling that has to drop the bit

	 * again is not run concurrently.

	/*

	 * One for dir inode,

	 * two for dir entries,

	 * two for root ref/backref.

 now the directory is empty */

		/*

		 * Propagate the last_unlink_trans value of the deleted dir to

		 * its parent directory. This is to prevent an unrecoverable

		 * log tree in the case we do something like this:

		 * 1) create dir foo

		 * 2) create snapshot under dir foo

		 * 3) delete the snapshot

		 * 4) rmdir foo

		 * 5) mkdir foo

		 * 6) fsync foo or some file inside foo

/*

 * Return this if we need to call truncate_block for the last bit of the

 * truncate.

/*

 * Remove inode items from a given root.

 *

 * @trans:		A transaction handle.

 * @root:		The root from which to remove items.

 * @inode:		The inode whose items we want to remove.

 * @new_size:		The new i_size for the inode. This is only applicable when

 *			@min_type is BTRFS_EXTENT_DATA_KEY, must be 0 otherwise.

 * @min_type:		The minimum key type to remove. All keys with a type

 *			greater than this value are removed and all keys with

 *			this type are removed only if their offset is >= @new_size.

 * @extents_found:	Output parameter that will contain the number of file

 *			extent items that were removed or adjusted to the new

 *			inode i_size. The caller is responsible for initializing

 *			the counter. Also, it can be NULL if the caller does not

 *			need this counter.

 *

 * Remove all keys associated with the inode from the given root that have a key

 * with a type greater than or equals to @min_type. When @min_type has a value of

 * BTRFS_EXTENT_DATA_KEY, only remove file extent items that have an offset value

 * greater than or equals to @new_size. If a file extent item that starts before

 * @new_size and ends after it is found, its length is adjusted.

 *

 * Returns: 0 on success, < 0 on error and NEED_TRUNCATE_BLOCK when @min_type is

 * BTRFS_EXTENT_DATA_KEY and the caller must truncate the last block.

	/*

	 * For non-free space inodes and non-shareable roots, we want to back

	 * off from time to time.  This means all inodes in subvolume roots,

	 * reloc roots, and data reloc roots.

		/*

		 * We want to drop from the next block forward in case this

		 * new size is not block aligned since we will be keeping the

		 * last block of the extent just the way it is.

	/*

	 * This function is also used to drop the items in the log tree before

	 * we relog the inode, so if root != BTRFS_I(inode)->root, it means

	 * it is used to drop the logged items. So we shouldn't kill the delayed

	 * items.

	/*

	 * with a 16K leaf size and 128MB extents, you can actually queue

	 * up a huge file in a single leaf.  Most of the time that

	 * bytes_deleted is > 0, it will be huge by the time we get here

		/* there are no items in the tree for us to truncate, we're

		 * done

 FIXME, shrink the extent if the ref count is only 1 */

 FIXME blocksize != 4096 */

			/*

			 * we can't truncate inline items that have had

			 * special encodings

				/*

				 * We have to bail so the last_size is set to

				 * just before this extent.

				/*

				 * Inline extents are special, we just treat

				 * them as a full sector worth in the file

				 * extent tree just for simplicity sake.

		/*

		 * We use btrfs_truncate_inode_items() to clean up log trees for

		 * multiple fsyncs, and in this case we don't want to clear the

		 * file extent range because it's just the log.

 no pending yet, add ourselves */

 hop on the pending chunk */

			/*

			 * We can generate a lot of delayed refs, so we need to

			 * throttle every once and a while and make sure we're

			 * adding enough space to keep up with the work we are

			 * generating.  Since we hold a transaction here we

			 * can't flush, and we don't want to FLUSH_LIMIT because

			 * we could have generated too many delayed refs to

			 * actually allocate, so just bail if we're short and

			 * let the normal reservation dance happen higher up.

/*

 * btrfs_truncate_block - read, zero a chunk and write a block

 * @inode - inode that we're zeroing

 * @from - the offset to start zeroing

 * @len - the length to zero, 0 to zero the entire range respective to the

 *	offset

 * @front - zero up to the offset instead of from the offset on

 *

 * This will find the block for the "from" offset and cow the block and zero the

 * part we want to zero.  This is used with truncate and hole punching.

 For nocow case, no need to reserve data space */

	/*

	 * If NO_HOLES is enabled, we don't need to do anything.

	 * Later, up in the call chain, either btrfs_set_inode_last_sub_trans()

	 * or btrfs_update_inode() will be called, which guarantee that the next

	 * fsync will know this inode was changed and needs to be logged.

	/*

	 * 1 - for the one we're dropping

	 * 1 - for the one we're adding

	 * 1 - for updating the inode.

/*

 * This function puts in dummy file extents for the area we're creating a hole

 * for.  So if we are truncating this file to a larger size we need to insert

 * these file extents so that btrfs_get_extent will return a EXTENT_MAP_HOLE for

 * the range between oldsize and size

	/*

	 * If our size started in the middle of a block we need to zero out the

	 * rest of the block before we expand the i_size, otherwise we could

	 * expose stale data.

	/*

	 * The regular truncate() case without ATTR_CTIME and ATTR_MTIME is a

	 * special case where we need to update the times despite not having

	 * these flags set.  For all other operations the VFS set these flags

	 * explicitly if it wants a timestamp update.

		/*

		 * Don't do an expanding truncate while snapshotting is ongoing.

		 * This is to ensure the snapshot captures a fully consistent

		 * state of this file - if the snapshot captures this expanding

		 * truncation, it must capture all writes that happened before

		 * this truncation.

		/*

		 * We're truncating a file that used to have good data down to

		 * zero. Make sure any new writes to the file get on disk

		 * on close.

			/*

			 * Truncate failed, so fix up the in-memory size. We

			 * adjusted disk_i_size down as we removed extents, so

			 * wait for disk_i_size to be stable and then update the

			 * in-memory size to match.

/*

 * While truncating the inode pages during eviction, we get the VFS calling

 * btrfs_invalidatepage() against each page of the inode. This is slow because

 * the calls to btrfs_invalidatepage() result in a huge amount of calls to

 * lock_extent_bits() and clear_extent_bit(), which keep merging and splitting

 * extent_state structures over and over, wasting lots of time.

 *

 * Therefore if the inode is being evicted, let btrfs_invalidatepage() skip all

 * those expensive operations on a per page basis and do only the ordered io

 * finishing, while we release here the extent_map and extent_state structures,

 * without the excessive merging and splitting.

	/*

	 * Keep looping until we have no more ranges in the io tree.

	 * We can have ongoing bios started by readahead that have

	 * their endio callback (extent_io.c:end_bio_extent_readpage)

	 * still in progress (unlocked the pages in the bio but did not yet

	 * unlocked the ranges in the io tree). Therefore this means some

	 * ranges can still be locked and eviction started because before

	 * submitting those bios, which are executed by a separate task (work

	 * queue kthread), inode references (inode->i_count) were not taken

	 * (which would be dropped in the end io callback of each bio).

	 * Therefore here we effectively end up waiting for those bios and

	 * anyone else holding locked ranges without having bumped the inode's

	 * reference count - if we don't do it, when they access the inode's

	 * io_tree to unlock a range it may be too late, leading to an

	 * use-after-free issue.

		/*

		 * If still has DELALLOC flag, the extent didn't reach disk,

		 * and its reserved space won't be freed by delayed_ref.

		 * So we need to free its reserved space here.

		 * (Refer to comment in btrfs_invalidatepage, case 2)

		 *

		 * Note, end is the bytenr of last byte, so we need + 1 here.

	/*

	 * Eviction should be taking place at some place safe because of our

	 * delayed iputs.  However the normal flushing code will run delayed

	 * iputs, so we cannot use FLUSH_ALL otherwise we'll deadlock.

	 *

	 * We reserve the delayed_refs_extra here again because we can't use

	 * btrfs_start_transaction(root, 0) for the same deadlocky reason as

	 * above.  We reserve our extra bit here because we generate a ton of

	 * delayed refs activity by truncating.

	 *

	 * If we cannot make our reservation we'll attempt to steal from the

	 * global reserve, because we really want to be able to free up space.

		/*

		 * Try to steal from the global reserve if there is space for

		 * it.

	/*

	 * Errors here aren't a big deal, it just means we leave orphan items in

	 * the tree. They will be cleaned up on the next mount. If the inode

	 * number gets reused, cleanup deletes the orphan item without doing

	 * anything, and unlink reuses the existing orphan item.

	 *

	 * If it turns out that we are dropping too many of these, we might want

	 * to add a mechanism for retrying these after a commit.

	/*

	 * If we didn't successfully delete, the orphan item will still be in

	 * the tree and we'll retry on the next mount. Again, we might also want

	 * to retry these periodically in the future.

/*

 * Return the key found in the dir entry in the location pointer, fill @type

 * with BTRFS_FT_*, and return 0.

 *

 * If no dir entries were found, returns -ENOENT.

 * If found a corrupted location in dir entry, returns -EUCLEAN.

/*

 * when we hit a tree root in a directory, the btrfs part of the inode

 * needs to be changed to reflect the root directory of the tree root.  This

 * is kind of like crossing a mount point.

/*

 * Get an inode object given its inode number and corresponding root.

 * Path can be preallocated to prevent recursing back to iget through

 * allocator. NULL is also valid but may require an additional allocation

 * later.

			/*

			 * ret > 0 can come from btrfs_search_slot called by

			 * btrfs_read_locked_inode, this means the inode item

			 * was not found.

	/*

	 * We only need lookup, the rest is read-only and there's no inode

	 * associated with the dentry

	/*

	 * Compile-time asserts that generic FT_* types still match

	 * BTRFS_FT_* types

 Do extra check against inode mode with di_type */

/*

 * All this infrastructure exists because dir_emit can fault, and we are holding

 * the tree lock when doing readdir.  For now just allocate a buffer and copy

 * our information into that, and then dir_emit from the buffer.  This is

 * similar to what NFS does, only we don't keep the buffer around in pagecache

 * because I'm afraid I'll mess that up.  Long term we need to make filldir do

 * copy_to_user_inatomic so we don't have to worry about page faulting under the

 * tree lock.

	/*

	 * Stop new entries from being returned after we return the last

	 * entry.

	 *

	 * New directory entries are assigned a strictly increasing

	 * offset.  This means that new entries created during readdir

	 * are *guaranteed* to be seen in the future by that readdir.

	 * This has broken buggy programs which operate on names as

	 * they're returned by readdir.  Until we re-use freed offsets

	 * we have this hack to stop new entries from being returned

	 * under the assumption that they'll never reach this huge

	 * offset.

	 *

	 * This is being careful not to overflow 32bit loff_t unless the

	 * last entry requires it because doing so has broken 32bit apps

	 * in the past.

/*

 * This is somewhat expensive, updating the tree every time the

 * inode changes.  But, it is most likely to find the inode in cache.

 * FIXME, needs more benchmarking...there are no reasons other than performance

 * to keep or drop this code.

 whoops, lets try again with the full transaction */

/*

 * This is a copy of file_update_time.  We need this so we can return error on

 * ENOSPC for updating the inode in the case of file write and mmap writes.

/*

 * find the highest existing sequence number in a directory

 * and then set the in-memory index_cnt variable to reflect

 * free sequence numbers

 FIXME: we should be able to handle this */

	/*

	 * MAGIC NUMBER EXPLANATION:

	 * since we search a directory based on f_pos we have to start at 2

	 * since '.' and '..' have f_pos of 0 and 1 respectively, so everybody

	 * else has to start at 2

/*

 * helper to find a free sequence number in a given directory.  This current

 * code is very simple, later versions will do smarter things in the btree

/*

 * Inherit flags from the parent inode.

 *

 * Currently only the compression flags and the cow flags are inherited.

	/*

	 * O_TMPFILE, set link count to 0, so that after this point,

	 * we fill in an inode item with the correct link count.

	/*

	 * we have to initialize this early, so we can reclaim the inode

	 * number if we fail afterwards in this function.

	/*

	 * index_cnt is ignored for everything but a dir,

	 * btrfs_set_inode_index_count has an explanation for the magic

	 * number

	/*

	 * We could have gotten an inode number from somebody who was fsynced

	 * and then removed in this same transaction, so let's just set full

	 * sync since it will be a full sync anyway and this will blow away the

	 * old info in the log.

		/*

		 * Start new inodes with an inode_ref. This is slightly more

		 * efficient for small numbers of hard links since they will

		 * be packed into one item. Extended refs will kick in if we

		 * add more hard links than can fit in the ref item.

/*

 * utility function to add 'inode' into 'parent_inode' with

 * a give name and a given sequence number.

 * if 'add_backref' is true, also insert a backref from the

 * inode to the parent directory.

 Nothing to clean up yet */

	/*

	 * If we are replaying a log tree, we do not want to update the mtime

	 * and ctime of the parent directory with the current time, since the

	 * log replay procedure is responsible for setting them to their correct

	 * values (the ones it had when the fsync was done).

 Return the original error code */

	/*

	 * 2 for inode item and ref

	 * 2 for dir items

	 * 1 for xattr if selinux is on

	/*

	* If the active LSM wants to access the inode during

	* d_instantiate it needs these. Smack checks to see

	* if the filesystem supports xattrs by looking at the

	* ops vector.

	/*

	 * 2 for inode item and ref

	 * 2 for dir items

	 * 1 for xattr if selinux is on

	/*

	* If the active LSM wants to access the inode during

	* d_instantiate it needs these. Smack checks to see

	* if the filesystem supports xattrs by looking at the

	* ops vector.

 do not allow sys_link's with other subvols of the same device */

	/*

	 * 2 items for inode and inode ref

	 * 2 items for dir items

	 * 1 item for parent inode

	 * 1 item for orphan item deletion if O_TMPFILE

 There are several dir indexes for this inode, clear the cache. */

			/*

			 * If new hard link count is 1, it's a file created

			 * with open(2) O_TMPFILE flag.

	/*

	 * 2 items for inode and ref

	 * 2 items for dir items

	 * 1 for xattr if selinux is on

 these must be set before we unlock the inode */

	/*

	 * decompression code contains a memset to fill in any space between the end

	 * of the uncompressed data and the end of max_size in case the decompressed

	 * data ends up shorter than ram_bytes.  That doesn't cover the hole between

	 * the end of an inline extent and the beginning of the next block, so we

	 * cover that region here.

/**

 * btrfs_get_extent - Lookup the first extent overlapping a range in a file.

 * @inode:	file to search in

 * @page:	page to read extent data into if the extent is inline

 * @pg_offset:	offset into @page to copy to

 * @start:	file offset

 * @len:	length of range starting at @start

 *

 * This returns the first &struct extent_map which overlaps with the given

 * range, reading it from the B-tree and caching it if necessary. Note that

 * there may be more extents which overlap the given range after the returned

 * extent_map.

 *

 * If @page is not NULL and the extent is inline, this also reads the extent

 * data directly into the page and marks the extent up to date in the io_tree.

 *

 * Return: ERR_PTR on error, non-NULL extent_map on success.

 Chances are we'll be called again, so go ahead and do readahead */

	/*

	 * The same explanation in load_free_space_cache applies here as well,

	 * we only read when we're loading the free space cache, and at that

	 * point the commit_root has everything we need.

		/*

		 * If we backup past the first extent we want to move forward

		 * and see if there is an extent in front of us, otherwise we'll

		 * say there is a hole for our whole search range which can

		 * cause problems.

 Only regular file could have regular/prealloc extent */

 New extent overlaps with existing one */

	/*

	 * If our em maps to:

	 * - a hole or

	 * - a pre-alloc extent,

	 * there might actually be delalloc bytes behind it.

 check to see if we've wrapped (len == -1 or similar) */

 ok, we didn't find anything, lets look for delalloc */

	/*

	 * We didn't find anything useful, return the original results from

	 * get_extent()

	/*

	 * Adjust the delalloc_start to make sure it doesn't go backwards from

	 * the start they passed in

		/*

		 * When btrfs_get_extent can't find anything it returns one

		 * huge hole

		 *

		 * Make sure what it found really fits our range, and adjust to

		 * make sure it is based on the start from the caller

			/*

			 * Our hole starts before our delalloc, so we have to

			 * return just the parts of the hole that go until the

			 * delalloc starts

			/*

			 * Don't adjust block start at all, it is fixed at

			 * EXTENT_MAP_HOLE

			/*

			 * Hole is out of passed range or it starts after

			 * delalloc range

 compress_type */

/*

 * Check if we can do nocow write into the range [@offset, @offset + @len)

 *

 * @offset:	File offset

 * @len:	The length to write, will be updated to the nocow writeable

 *		range

 * @orig_start:	(optional) Return the original file offset of the file extent

 * @orig_len:	(optional) Return the original on-disk length of the file extent

 * @ram_bytes:	(optional) Return the ram_bytes of the file extent

 * @strict:	if true, omit optimizations that might force us into unnecessary

 *		cow. e.g., don't trust generation number.

 *

 * Return:

 * >0	and update @len if we can do nocow write

 *  0	if we can't do nocow write

 * <0	if error happened

 *

 * NOTE: This only checks the file extents, caller is responsible to wait for

 *	 any ordered extents.

 can't find the item, must cow */

 not our file or wrong item type, must cow */

 Wrong offset, must cow */

 not a regular extent, must cow */

	/*

	 * Do the same check as in btrfs_cross_ref_exist but without the

	 * unnecessary search.

	/*

	 * look for other files referencing this extent, if we

	 * find any we must cow

	/*

	 * adjust disk_bytenr and num_bytes to cover just the bytes

	 * in this extent we are about to write.  If there

	 * are any csums in that range we have to cow in order

	 * to keep the csums correct

	/*

	 * all of the above have passed, it is safe to overwrite this extent

	 * without cow

		/*

		 * We're concerned with the entire range that we're going to be

		 * doing DIO to, so we need to make sure there's no ordered

		 * extents in this range.

		/*

		 * We need to make sure there are no buffered pages in this

		 * range either, we could have raced between the invalidate in

		 * generic_file_direct_write and locking the extent.  The

		 * invalidate needs to happen so that reads after a write do not

		 * get stale data.

			/*

			 * If we are doing a DIO read and the ordered extent we

			 * found is for a buffered write, we can not wait for it

			 * to complete and retry, because if we do so we can

			 * deadlock with concurrent buffered writes on page

			 * locks. This happens only if our DIO read covers more

			 * than one extent map, if at this point has already

			 * created an ordered extent for a previous extent map

			 * and locked its range in the inode's io tree, and a

			 * concurrent write against that previous extent map's

			 * range and this range started (we unlock the ranges

			 * in the io tree only when the bios complete and

			 * buffered writes always lock pages before attempting

			 * to lock range in the io tree).

			/*

			 * We could trigger writeback for this range (and wait

			 * for it to complete) and then invalidate the pages for

			 * this range (through invalidate_inode_pages2_range()),

			 * but that can lead us to a deadlock with a concurrent

			 * call to readahead (a buffered read or a defrag call

			 * triggered a readahead) on a page lock due to an

			 * ordered dio extent we created before but did not have

			 * yet a corresponding bio submitted (whence it can not

			 * complete), which makes readahead wait for that

			 * ordered extent to complete while holding a lock on

			 * that page.

 The callers of this must take lock_extent() */

		/*

		 * The caller has taken lock_extent(), who could race with us

		 * to add em?

 em got 2 refs now, callers needs to do free_extent_map once. */

	/*

	 * We don't allocate a new extent in the following cases

	 *

	 * 1) The inode is marked as NODATACOW. In this case we'll just use the

	 * existing extent.

	 * 2) The extent is marked as PREALLOC. We're good to go here and can

	 * just use the extent.

	 *

			/*

			 * For inode marked NODATACOW or extent marked PREALLOC,

			 * use the existing or preallocated extent, so does not

			 * need to adjust btrfs_space_info's bytes_may_use.

 this will cow the extent */

	/*

	 * Need to update the i_size under the extent lock so buffered

	 * readers will get the updated i_size when we unlock.

	/*

	 * The generic stuff only does filemap_write_and_wait_range, which

	 * isn't enough if we've written compressed pages to this area, so we

	 * need to flush the dirty pages again to make absolutely sure that any

	 * outstanding dirty pages are on disk.

	/*

	 * If this errors out it's because we couldn't invalidate pagecache for

	 * this range and we need to fallback to buffered.

	/*

	 * Ok for INLINE and COMPRESSED extents we need to fallback on buffered

	 * io.  INLINE is special, and we could probably kludge it in here, but

	 * it's still buffered so for safety lets just fall back to the generic

	 * buffered path.

	 *

	 * For COMPRESSED we _have_ to read the entire extent in so we can

	 * decompress it, so there will be buffering required no matter what we

	 * do, so go ahead and fallback to buffered.

	 *

	 * We return -ENOTBLK because that's what makes DIO go ahead and go back

	 * to buffered IO.  Don't blame me, this is the price we pay for using

	 * the generic code.

 Recalc len in case the new em is smaller than requested */

		/*

		 * We need to unlock only the end area that we aren't using.

		 * The rest is going to be unlocked by the endio routine.

	/*

	 * Translate extent map information to iomap.

	 * We trim the extents (and move the addr) even though iomap code does

	 * that, since we have locked only the parts we are performing I/O in.

 If reading from a hole, unlock and return */

	/*

	 * This implies a barrier so that stores to dio_bio->bi_status before

	 * this and loads of dio_bio->bi_status after this are fully ordered.

 Check btrfs_submit_bio_hook() for rules about async submit. */

		/*

		 * If we aren't doing async submit, calculate the csum of the

		 * bio now.

/*

 * If this succeeds, the btrfs_dio_private is responsible for cleaning up locked

 * or ordered extents whether or not we submit any bios.

		/*

		 * Load the csums up front to reduce csum tree searches and

		 * contention when submitting bios.

		 *

		 * If we have csums disabled this will do nothing.

		/*

		 * This will never fail as it's passing GPF_NOFS and

		 * the allocation is backed by btrfs_bioset.

		/*

		 * Increase the count before we submit the bio so we know

		 * the end IO handler won't happen before we increase the

		 * count. Otherwise, the dip might get freed before we're

		 * done setting it up.

		 *

		 * We transfer the initial reference to the last bio, so we

		 * don't need to increment the reference count for the last one.

			/*

			 * If we are submitting more than one bio, submit them

			 * all asynchronously. The exception is RAID 5 or 6, as

			 * asynchronous checksums make it difficult to collect

			 * full stripe writes.

	/*

	 * If we are under memory pressure we will call this directly from the

	 * VM, we need to make sure we have the inode referenced for the ordered

	 * extent.  If not just return like we didn't do anything.

/*

 * For releasepage() and invalidatepage() we have a race window where

 * end_page_writeback() is called but the subpage spinlock is not yet released.

 * If we continue to release/invalidate the page, we could cause use-after-free

 * for subpage spinlock.  So this function is to spin and wait for subpage

 * spinlock.

	/*

	 * This may look insane as we just acquire the spinlock and release it,

	 * without doing anything.  But we just want to make sure no one is

	 * still holding the subpage spinlock.

	 * And since the page is not dirty nor writeback, and we have page

	 * locked, the only possible way to hold a spinlock is from the endio

	 * function to clear page writeback.

	 *

	 * Here we just acquire the spinlock so that all existing callers

	 * should exit and we're safe to release/invalidate the page.

	/*

	 * We have page locked so no new ordered extent can be created on this

	 * page, nor bio can be submitted for this page.

	 *

	 * But already submitted bio can still be finished on this page.

	 * Furthermore, endio function won't skip page which has Ordered

	 * (Private2) already cleared, so it's possible for endio and

	 * invalidatepage to do the same ordered extent accounting twice

	 * on one page.

	 *

	 * So here we wait for any submitted bios to finish, so that we won't

	 * do double ordered extent accounting on the same page.

	/*

	 * For subpage case, we have call sites like

	 * btrfs_punch_hole_lock_range() which passes range not aligned to

	 * sectorsize.

	 * If the range doesn't cover the full page, we don't need to and

	 * shouldn't clear page extent mapped, as page->private can still

	 * record subpage dirty bits for other part of the range.

	 *

	 * For cases that can invalidate the full even the range doesn't

	 * cover the full page, like invalidating the last page, we're

	 * still safe to wait for ordered extent to finish.

			/*

			 * No ordered extent covering this range, we are safe

			 * to delete all extent states in the range.

			/*

			 * There is a range between [cur, oe->file_offset) not

			 * covered by any ordered extent.

			 * We are safe to delete all extent states, and handle

			 * the ordered extent in the next iteration.

			/*

			 * If Ordered (Private2) is cleared, it means endio has

			 * already been executed for the range.

			 * We can't delete the extent states as

			 * btrfs_finish_ordered_io() may still use some of them.

		/*

		 * IO on this page will never be started, so we need to account

		 * for any ordered extents now. Don't clear EXTENT_DELALLOC_NEW

		 * here, must leave that up for the ordered extent completion.

		 *

		 * This will also unlock the range for incoming

		 * btrfs_finish_ordered_io().

			/*

			 * The ordered extent has finished, now we're again

			 * safe to delete all extent states of the range.

			/*

			 * btrfs_finish_ordered_io() will get executed by endio

			 * of other pages, thus we can't delete extent states

			 * anymore

		/*

		 * Qgroup reserved space handler

		 * Sector(s) here will be either:

		 *

		 * 1) Already written to disk or bio already finished

		 *    Then its QGROUP_RESERVED bit in io_tree is already cleared.

		 *    Qgroup will be handled by its qgroup_record then.

		 *    btrfs_qgroup_free_data() call will do nothing here.

		 *

		 * 2) Not written to disk yet

		 *    Then btrfs_qgroup_free_data() call will clear the

		 *    QGROUP_RESERVED bit of its io_tree, and free the qgroup

		 *    reserved data space.

		 *    Since the IO will never happen for this page.

	/*

	 * We have iterated through all ordered extents of the page, the page

	 * should not have Ordered (Private2) anymore, or the above iteration

	 * did something wrong.

/*

 * btrfs_page_mkwrite() is not allowed to change the file size as it gets

 * called from a page fault handler when a page is first dirtied. Hence we must

 * be careful to check for EOF conditions here. We set the page up correctly

 * for a written page which means we get ENOSPC checking when writing into

 * holes and correct delalloc and unwritten extent mapping on filesystems that

 * support these features.

 *

 * We are not allowed to take the i_mutex here so we have to play games to

 * protect against truncate races as the page could now be beyond EOF.  Because

 * truncate_setsize() writes the inode size before removing pages, once we have

 * the page lock we can determine safely if the page is beyond EOF. If it is not

 * beyond EOF, then the page is guaranteed safe against truncation until we

 * unlock the page.

	/*

	 * Reserving delalloc space after obtaining the page lock can lead to

	 * deadlock. For example, if a dirty page is locked by this function

	 * and the call to btrfs_delalloc_reserve_space() ends up triggering

	 * dirty page write out, then the btrfs_writepage() function could

	 * end up waiting indefinitely to get a lock on the page currently

	 * being processed by btrfs_page_mkwrite() function.

 make the VM retry the fault */

 page got truncated out from underneath us */

	/*

	 * we can't set the delalloc bits if there are pending ordered

	 * extents.  Drop our locks and wait for them to finish

	/*

	 * page_mkwrite gets called when the page is firstly dirtied after it's

	 * faulted in, but write(2) could also dirty a page and set delalloc

	 * bits, thus in this case for space account reason, we still need to

	 * clear any delalloc bits within this page range since we have to

	 * reserve data&meta space before lock_page() (see above comments).

 page is wholly or partially inside EOF */

	/*

	 * Yes ladies and gentlemen, this is indeed ugly.  We have a couple of

	 * things going on here:

	 *

	 * 1) We need to reserve space to update our inode.

	 *

	 * 2) We need to have something to cache all the space that is going to

	 * be free'd up by the truncate operation, but also have some slack

	 * space reserved in case it uses space during the truncate (thank you

	 * very much snapshotting).

	 *

	 * And we need these to be separate.  The fact is we can use a lot of

	 * space doing the truncate, and we have no earthly idea how much space

	 * we will use, so we need the truncate reservation to be separate so it

	 * doesn't end up using space reserved for updating the inode.  We also

	 * need to be able to stop the transaction and start a new one, which

	 * means we need to be able to update the inode several times, and we

	 * have no idea of knowing how many times that will be, so we can't just

	 * reserve 1 item for the entirety of the operation, so that has to be

	 * done separately as well.

	 *

	 * So that leaves us with

	 *

	 * 1) rsv - for the truncate reservation, which we will steal from the

	 * transaction reservation.

	 * 2) fs_info->trans_block_rsv - this will have 1 items worth left for

	 * updating the inode.

	/*

	 * 1 for the truncate slack space

	 * 1 for updating the inode.

 Migrate the slack space for the truncate to our reserve */

 shouldn't happen */

	/*

	 * We can't call btrfs_truncate_block inside a trans handle as we could

	 * deadlock with freeze, if we got NEED_TRUNCATE_BLOCK then we know

	 * we've truncated everything except the last little bit, and can do

	 * btrfs_truncate_block and then update the disk_i_size.

	/*

	 * So if we truncate and then write and fsync we normally would just

	 * write the extents that changed, which is a problem if we need to

	 * first truncate that entire inode.  So set this flag so we write out

	 * all of the extents in the inode to the sync log so we're completely

	 * safe.

	 *

	 * If no extents were dropped or trimmed we don't need to force the next

	 * fsync to truncate all the inode's items from the log and re-log them

	 * all. This means the truncate operation did not change the file size,

	 * or changed it to a smaller size but there was only an implicit hole

	 * between the old i_size and the new i_size, and there were no prealloc

	 * extents beyond i_size to drop.

/*

 * create a new subvolume directory/inode (helper for the ioctl).

	/*

	 * This can happen where we create an inode, but somebody else also

	 * created the same inode and we need to destroy the one we already

	 * created.

 the snap/subvol tree is on deleting */

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

	/*

	 * For non-subvolumes allow exchange only within one subvolume, in the

	 * same inode namespace. Two subvolumes (represented as directory) can

	 * be exchanged as they're a logical link and have a fixed inode number.

 close the race window with snapshot create/destroy ioctl */

	/*

	 * We want to reserve the absolute worst case amount of items.  So if

	 * both inodes are subvols and we need to unlink them then that would

	 * require 4 item modifications, but if they are both normal inodes it

	 * would require 5 item modifications, so we'll assume their normal

	 * inodes.  So 5 * 2 is 10, plus 2 for the new links, so 12 total items

	 * should cover the worst case number of items we'll modify.

	/*

	 * We need to find a free sequence number both in the source and

	 * in the destination directory for the exchange.

 Reference for the source. */

 force full log commit if subvolume involved. */

 And now for the dest. */

 force full log commit if subvolume involved. */

 Update inode version and ctime/mtime. */

	/*

	 * Now pin the logs of the roots. We do it to ensure that no other task

	 * can sync the logs while we are in progress with the rename, because

	 * that could result in an inconsistency in case any of the inodes that

	 * are part of this rename operation were logged before.

	 *

	 * We pin the logs even if at this precise moment none of the inodes was

	 * logged before. This is because right after we checked for that, some

	 * other task fsyncing some other inode not involved with this rename

	 * operation could log that one of our inodes exists.

	 *

	 * We don't need to pin the logs before the above calls to

	 * btrfs_insert_inode_ref(), since those don't ever need to change a log.

 src is a subvolume */

 src is an inode */

 dest is a subvolume */

 dest is an inode */

	/*

	 * If we have pinned a log and an error happened, we unpin tasks

	 * trying to sync the log and force them to fallback to a transaction

	 * commit if the log currently contains any of the inodes involved in

	 * this rename operation (to ensure we do not persist a log with an

	 * inconsistent state for any of these inodes or leading to any

	 * inconsistencies when replayed). If the transaction was aborted, the

	 * abortion reason is propagated to userspace when attempting to commit

	 * the transaction. If the log does not contain any of these inodes, we

	 * allow the tasks to sync it.

 we only allow rename subvolume link between subvolumes */

 check for collisions, even if the  name isn't there */

			/* we shouldn't get

 maybe -EOVERFLOW */

	/*

	 * we're using rename to replace one file with another.  Start IO on it

	 * now so  we don't add too much work to the end of the transaction

 close the racy window with snapshot create/destroy ioctl */

	/*

	 * We want to reserve the absolute worst case amount of items.  So if

	 * both inodes are subvols and we need to unlink them then that would

	 * require 4 item modifications, but if they are both normal inodes it

	 * would require 5 item modifications, so we'll assume they are normal

	 * inodes.  So 5 * 2 is 10, plus 1 for the new link, so 11 total items

	 * should cover the worst case number of items we'll modify.

	 * If our rename has the whiteout flag, we need more 5 units for the

	 * new inode (1 inode item, 1 inode ref, 2 dir items and 1 xattr item

	 * when selinux is enabled).

 force full log commit if subvolume involved. */

		/*

		 * Now pin the log. We do it to ensure that no other task can

		 * sync the log while we are in progress with the rename, as

		 * that could result in an inconsistency in case any of the

		 * inodes that are part of this rename operation were logged

		 * before.

		 *

		 * We pin the log even if at this precise moment none of the

		 * inodes was logged before. This is because right after we

		 * checked for that, some other task fsyncing some other inode

		 * not involved with this rename operation could log that one of

		 * our inodes exists.

		 *

		 * We don't need to pin the logs before the above call to

		 * btrfs_insert_inode_ref(), since that does not need to change

		 * a log.

	/*

	 * If we have pinned the log and an error happened, we unpin tasks

	 * trying to sync the log and force them to fallback to a transaction

	 * commit if the log currently contains any of the inodes involved in

	 * this rename operation (to ensure we do not persist a log with an

	 * inconsistent state for any of these inodes or leading to any

	 * inconsistencies when replayed). If the transaction was aborted, the

	 * abortion reason is propagated to userspace when attempting to commit

	 * the transaction. If the log does not contain any of these inodes, we

	 * allow the tasks to sync it.

/*

 * some fairly slow code that needs optimization. This walks the list

 * of all the inodes with pending delalloc and forces them to disk.

		/*

		 * Reset nr_to_write here so we know that we're doing a full

		 * flush.

	/*

	 * 2 items for inode item and ref

	 * 2 items for dir items

	 * 1 item for updating parent inode item

	 * 1 item for the inline extent item

	 * 1 item for xattr if selinux is on

	/*

	* If the active LSM wants to access the inode during

	* d_instantiate it needs these. Smack checks to see

	* if the filesystem supports xattrs by looking at the

	* ops vector.

	/*

	 * Last step, add directory indexes for our symlink inode. This is the

	 * last step to avoid extra cleanup of these indexes if an error happens

	 * elsewhere above.

 Encryption and other encoding is reserved and all 0 */

	/*

	 * We have released qgroup data range at the beginning of the function,

	 * and normally qgroup_released bytes will be freed when committing

	 * transaction.

	 * But if we error out early, we have to free what we have released

	 * or we leak qgroup data reservation.

		/*

		 * If we are severely fragmented we could end up with really

		 * small allocations, so if the allocator is returning small

		 * chunks lets make its job easier by only searching for those

		 * sized chunks.

		/*

		 * We've reserved this space, and thus converted it from

		 * ->bytes_may_use to ->bytes_reserved.  Any error that happens

		 * from here on out we will only need to clear our reservation

		 * for the remaining unreserved area, so advance our

		 * clear_offset by our extent size.

		/*

		 * Now that we inserted the prealloc extent we can finally

		 * decrement the number of reservations in the block group.

		 * If we did it before, we could race with relocation and have

		 * relocation miss the reserved extent, making it fail later.

	/*

	 * 5 units required for adding orphan entry

	/*

	 * We set number of links to 0 in btrfs_new_inode(), and here we set

	 * it to 1 because d_tmpfile() will issue a warning if the count is 0,

	 * through:

	 *

	 *    d_tmpfile() -> inode_dec_link_count() -> drop_nlink()

 Pages should be in the extent_io_tree */

/*

 * Add an entry indicating a block group or device which is pinned by a

 * swapfile. Returns 0 on success, 1 if there is already an entry for it, or a

 * negative errno on failure.

 Free all of the entries pinned by this swapfile. */

	/*

	 * If the swap file was just created, make sure delalloc is done. If the

	 * file changes again after this, the user is doing something stupid and

	 * we don't really care.

	/*

	 * The inode is locked, so these flags won't change after we check them.

	/*

	 * Balance or device remove/replace/resize can move stuff around from

	 * under us. The exclop protection makes sure they aren't running/won't

	 * run concurrently while we are mapping the swap extents, and

	 * fs_info->swapfile_pins prevents them from running while the swap

	 * file is active and moving the extents. Note that this also prevents

	 * a concurrent device add which isn't actually necessary, but it's not

	 * really worth the trouble to allow it.

	/*

	 * Prevent snapshot creation while we are activating the swap file.

	 * We do not want to race with snapshot creation. If snapshot creation

	 * already started before we bumped nr_swapfiles from 0 to 1 and

	 * completes before the first write into the swap file after it is

	 * activated, than that write would fallback to COW.

	/*

	 * Snapshots can create extents which require COW even if NODATACOW is

	 * set. We use this counter to prevent snapshots. We must increment it

	 * before walking the extents because we don't want a concurrent

	 * snapshot to run after we've already checked the extents.

			/*

			 * It's unlikely we'll ever actually find ourselves

			 * here, as a file small enough to fit inline won't be

			 * big enough to store more than the swap header, but in

			 * case something changes in the future, let's catch it

			 * here rather than later.

/*

 * Update the number of bytes used in the VFS' inode. When we replace extents in

 * a range (clone, dedupe, fallocate's zero range), we must update the number of

 * bytes used by the inode in an atomic manner, so that concurrent stat(2) calls

 * always get a correct value.

/*

 * btrfs doesn't support the bmap operation because swapfiles

 * use bmap to make a mapping of extents in the file.  They assume

 * these extents won't change over the life of the file and they

 * use the bmap result to do IO directly to the drive.

 *

 * the btrfs bmap call would return logical addresses that aren't

 * suitable for IO and they also will change frequently as COW

 * operations happen.  So, swapfile + btrfs == corruption.

 *

 * For now we're avoiding this by dropping bmap.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2011 STRATO.  All rights reserved.

/*

 * This is the implementation for the generic read ahead framework.

 *

 * To trigger a readahead, btrfs_reada_add must be called. It will start

 * a read ahead for the given range [start, end) on tree root. The returned

 * handle can either be used to wait on the readahead to finish

 * (btrfs_reada_wait), or to send it to the background (btrfs_reada_detach).

 *

 * The read ahead works as follows:

 * On btrfs_reada_add, the root of the tree is inserted into a radix_tree.

 * reada_start_machine will then search for extents to prefetch and trigger

 * some reads. When a read finishes for a node, all contained node/leaf

 * pointers that lie in the given range will also be enqueued. The reads will

 * be triggered in sequential order, thus giving a big win over a naive

 * enumeration. It will also make use of multi-device layouts. Each disk

 * will have its on read pointer and all disks will by utilized in parallel.

 * Also will no two disks read both sides of a mirror simultaneously, as this

 * would waste seeking capacity. Instead both disks will read different parts

 * of the filesystem.

 * Any number of readaheads can be started in parallel. The read order will be

 * determined globally, i.e. 2 parallel readaheads will normally finish faster

 * than the 2 started one after another.

	struct btrfs_device	*devs[BTRFS_MAX_MIRRORS]; /* full list, incl

 recurses */

 in case of err, eb might be NULL */

	/*

	 * just take the full list from the extent. afterwards we

	 * don't need the lock anymore

	/*

	 * this is the error case, the extent buffer has not been

	 * read correctly. We won't access anything from it and

	 * just cleanup our data structures. Effectively this will

	 * cut the branch below this node from read ahead.

	/*

	 * FIXME: currently we just set nritems to 0 if this is a leaf,

	 * effectively ignoring the content. In a next step we could

	 * trigger more readahead depending from the content, e.g.

	 * fetch the checksums for the extents in the leaf.

			/*

			 * if the generation doesn't match, just ignore this

			 * extctl. This will probably cut off a branch from

			 * prefetch. Alternatively one could start a new (sub-)

			 * prefetch for this branch, starting again from root.

			 * FIXME: move the generation check out of this loop

	/*

	 * free extctl records

 one ref for each entry */

 find extent */

 our ref */

 our device always sits at index 0 */

 bounds have already been checked */

	/*

	 * map block

 cannot read ahead on missing device. */

 not a single zone found, error and out */

 Insert extent in reada tree + all per-device trees, all or nothing */

			/*

			 * in case of DUP, just add the first zone. As both

			 * are on the same device, there's nothing to gain

			 * from adding both.

			 * Also, it wouldn't work, as the tree is per device

			 * and adding would fail with EEXIST

			/*

			 * as this device is selected for reading only as

			 * a last resort, skip it for read ahead.

 ignore whether the entry was inserted */

			/*

			 * no fs_info->reada_lock needed, as this can't be

			 * the last ref

			/* no fs_info->reada_lock needed, as this can't be

 takes one ref */

 leave the ref on the extent */

/*

 * called with fs_info->reada_lock held

/*

 * called with fs_info->reada_lock held

 pick the zone with the most elements */

	/*

	 * FIXME currently we issue the reads one extent at a time. If we have

	 * a contiguous block of extents, we could also coagulate them or use

	 * plugging to speed things up

	/*

	 * find mirror num

 Try to start up to 10k READA requests for a group of devices */

	/*

	 * If everything is already in the cache, this is effectively single

	 * threaded. To a) not hold the caller for too long and b) to utilize

	 * more cores, we broke the loop above after 10000 iterations and now

	 * enqueue to workers to finish it. This will distribute the load to

	 * the cores.

 FIXME we cannot handle this properly right now */

/*

 * interface

 one ref for having elements */

/*

 * Before removing a device (device replace or device remove ioctls), call this

 * function to wait for all existing readahead requests on the device and to

 * make sure no one queues more readahead requests for the device.

 *

 * Must be called without holding neither the device list mutex nor the device

 * replace semaphore, otherwise it will deadlock.

 Serialize with readahead extent creation at reada_find_extent(). */

	/*

	 * There might be readahead requests added to the radix trees which

	 * were not yet added to the readahead work queue. We need to start

	 * them and wait for their completion, otherwise we can end up with

	 * use-after-free problems when dropping the last reference on the

	 * readahead extents and their zones, as they need to access the

	 * device structure.

/*

 * If when removing a device (device replace or device remove ioctls) an error

 * happens after calling btrfs_reada_remove_dev(), call this to undo what that

 * function did. This is safe to call even if btrfs_reada_remove_dev() was not

 * called before.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

 0 == as many as possible */

/*

 * Convert block group flags (BTRFS_BLOCK_GROUP_*) to btrfs_raid_types, which

 * can be used as index to access btrfs_raid_array[].

 BTRFS_BLOCK_GROUP_SINGLE */

/*

 * Fill @buf with textual description of @bg_flags, no more than @size_buf

 * bytes including terminating null byte.

 remove last | */

	/*

	 * The text is trimmed, it's up to the caller to provide sufficiently

	 * large buffer

/*

 * Device locking

 * ==============

 *

 * There are several mutexes that protect manipulation of devices and low-level

 * structures like chunks but not block groups, extents or files

 *

 * uuid_mutex (global lock)

 * ------------------------

 * protects the fs_uuids list that tracks all per-fs fs_devices, resulting from

 * the SCAN_DEV ioctl registration or from mount either implicitly (the first

 * device) or requested by the device= mount option

 *

 * the mutex can be very coarse and can cover long-running operations

 *

 * protects: updates to fs_devices counters like missing devices, rw devices,

 * seeding, structure cloning, opening/closing devices at mount/umount time

 *

 * global::fs_devs - add, remove, updates to the global list

 *

 * does not protect: manipulation of the fs_devices::devices list in general

 * but in mount context it could be used to exclude list modifications by eg.

 * scan ioctl

 *

 * btrfs_device::name - renames (write side), read is RCU

 *

 * fs_devices::device_list_mutex (per-fs, with RCU)

 * ------------------------------------------------

 * protects updates to fs_devices::devices, ie. adding and deleting

 *

 * simple list traversal with read-only actions can be done with RCU protection

 *

 * may be used to exclude some operations from running concurrently without any

 * modifications to the list (see write_all_supers)

 *

 * Is not required at mount and close times, because our device list is

 * protected by the uuid_mutex at that point.

 *

 * balance_mutex

 * -------------

 * protects balance structures (status, state) and context accessed from

 * several places (internally, ioctl)

 *

 * chunk_mutex

 * -----------

 * protects chunks, adding or removing during allocation, trim or when a new

 * device is added/removed. Additionally it also protects post_commit_list of

 * individual devices, since they can be added to the transaction's

 * post_commit_list only with chunk_mutex held.

 *

 * cleaner_mutex

 * -------------

 * a big lock that is held by the cleaner thread and prevents running subvolume

 * cleaning together with relocation or delayed iputs

 *

 *

 * Lock nesting

 * ============

 *

 * uuid_mutex

 *   device_list_mutex

 *     chunk_mutex

 *   balance_mutex

 *

 *

 * Exclusive operations

 * ====================

 *

 * Maintains the exclusivity of the following operations that apply to the

 * whole filesystem and cannot run in parallel.

 *

 * - Balance (*)

 * - Device add

 * - Device remove

 * - Device replace (*)

 * - Resize

 *

 * The device operations (as above) can be in one of the following states:

 *

 * - Running state

 * - Paused state

 * - Completed state

 *

 * Only device operations marked with (*) can go into the Paused state for the

 * following reasons:

 *

 * - ioctl (only Balance can be Paused through ioctl)

 * - filesystem remounted as read-only

 * - filesystem unmounted and mounted as read-only

 * - system power-cycle and filesystem mounted as read-only

 * - filesystem or device errors leading to forced read-only

 *

 * The status of exclusive operation is set and cleared atomically.

 * During the course of Paused state, fs_info::exclusive_operation remains set.

 * A device operation in Paused or Running state can be canceled or resumed

 * either by ioctl (Balance only) or when remounted as read-write.

 * The exclusive status is cleared when the device operation is canceled or

 * completed.

/*

 * alloc_fs_devices - allocate struct btrfs_fs_devices

 * @fsid:		if not NULL, copy the UUID to fs_devices::fsid

 * @metadata_fsid:	if not NULL, copy the UUID to fs_devices::metadata_fsid

 *

 * Return a pointer to a new struct btrfs_fs_devices on success, or ERR_PTR().

 * The returned struct is not linked onto any lists and can be destroyed with

 * kfree() right away.

 Handle non-split brain cases */

	/*

	 * Handle scanned device having completed its fsid change but

	 * belonging to a fs_devices that was created by first scanning

	 * a device which didn't have its fsid/metadata_uuid changed

	 * at all and the CHANGING_FSID_V2 flag set.

	/*

	 * Handle scanned device having completed its fsid change but

	 * belonging to a fs_devices that was created by a device that

	 * has an outdated pair of fsid/metadata_uuid and

	 * CHANGING_FSID_V2 flag set.

/*

 *  Search and remove all stale (devices which are not mounted) devices.

 *  When both inputs are NULL, it will search and release all stale devices.

 *  path:	Optional. When provided will it release all unmounted devices

 *		matching this path only.

 *  skip_dev:	Optional. Will skip this device when searching for the stale

 *		devices.

 *  Return:	0 for success or if @path is NULL.

 * 		-EBUSY if @path is a mounted device.

 * 		-ENOENT if @path does not match any device in the list.

 for an already deleted device return 0 */

 delete the stale device */

/*

 * This is only used on mount, and we are protected from competing things

 * messing with our fs_devices by the uuid_mutex, thus we do not need the

 * fs_devices->device_list_mutex here.

/*

 * Handle scanned device having its CHANGING_FSID_V2 flag set and the fs_devices

 * being created with a disk that has already completed its fsid change. Such

 * disk can belong to an fs which has its FSID changed or to one which doesn't.

 * Handle both cases here.

	/*

	 * Handles the case where scanned device is part of an fs that had

	 * multiple successful changes of FSID but currently device didn't

	 * observe it. Meaning our fsid will be different than theirs. We need

	 * to handle two subcases :

	 *  1 - The fs still continues to have different METADATA/FSID uuids.

	 *  2 - The fs is switched back to its original FSID (METADATA/FSID

	 *  are equal).

 Changed UUIDs */

 Unchanged UUIDs */

	/*

	 * Handle the case where the scanned device is part of an fs whose last

	 * metadata UUID change reverted it to the original FSID. At the same

	 * time * fs_devices was first created by another constitutent device

	 * which didn't fully observe the operation. This results in an

	 * btrfs_fs_devices created with metadata/fsid different AND

	 * btrfs_fs_devices::fsid_change set AND the metadata_uuid of the

	 * fs_devices equal to the FSID of the disk.

/*

 * Add new device to list of registered devices

 *

 * Returns:

 * device pointer which was just added or updated when successful

 * error pointer when failed

		/*

		 * If this disk has been pulled into an fs devices created by

		 * a device which had the CHANGING_FSID_V2 flag then replace the

		 * metadata_uuid/fsid values of the fs_devices.

 we can safely leave the fs_devices entry around */

		/*

		 * When FS is already mounted.

		 * 1. If you are here and if the device->name is NULL that

		 *    means this device was missing at time of FS mount.

		 * 2. If you are here and if the device->name is different

		 *    from 'path' that means either

		 *      a. The same device disappeared and reappeared with

		 *         different name. or

		 *      b. The missing-disk-which-was-replaced, has

		 *         reappeared now.

		 *

		 * We must allow 1 and 2a above. But 2b would be a spurious

		 * and unintentional.

		 *

		 * Further in case of 1 and 2a above, the disk at 'path'

		 * would have missed some transaction when it was away and

		 * in case of 2a the stale bdev has to be updated as well.

		 * 2b must not be allowed at all time.

		/*

		 * For now, we do allow update to btrfs_fs_device through the

		 * btrfs dev scan cli after FS has been mounted.  We're still

		 * tracking a problem where systems fail mount by subvolume id

		 * when we reject replacement on a mounted FS.

			/*

			 * That is if the FS is _not_ mounted and if you

			 * are here, that means there is more than one

			 * disk with same uuid and devid.We keep the one

			 * with larger generation number or the last-in if

			 * generation are equal.

		/*

		 * We are going to replace the device path for a given devid,

		 * make sure it's the same device if the device is mounted

				/*

				 * device->fs_info may not be reliable here, so

				 * pass in a NULL instead. This avoids a

				 * possible use-after-free when the fs_info and

				 * fs_info->sb are already torn down.

	/*

	 * Unmount does not free the btrfs_device struct but would zero

	 * generation along with most of the other members. So just update

	 * it back. We need it to pick the disk with largest generation

	 * (as above).

		/*

		 * This is ok to do without rcu read locked because we hold the

		 * uuid mutex so nothing we touch in here is going to disappear.

 This is the initialized path, it is safe to release the devices. */

		/*

		 * We have already validated the presence of BTRFS_DEV_REPLACE_DEVID,

		 * in btrfs_init_dev_replace() so just continue.

/*

 * After we have read the system tree and know devids belonging to this

 * filesystem, remove the device which does not belong there.

	/*

	 * Reset the flush error record. We might have a transient flush error

	 * in this mount, and if so we aborted the current transaction and set

	 * the fs to an error state, guaranteeing no super blocks can be further

	 * committed. However that error might be transient and if we unmount the

	 * filesystem and mount it again, we should allow the mount to succeed

	 * (btrfs_check_rw_degradable() should not fail) - if after mounting the

	 * filesystem again we still get flush errors, then we will again abort

	 * any transaction and set the error state, guaranteeing no commits of

	 * unsafe super blocks.

 Verify the device is back in a pristine state  */

	/*

	 * The device_list_mutex cannot be taken here in case opening the

	 * underlying device takes further locks like open_mutex.

	 *

	 * We also don't need the lock here as this is called during mount and

	 * exclusion is provided by uuid_mutex

 make sure our super fits in the device */

 make sure our super fits in the page */

 make sure our super doesn't straddle pages on disk */

 pull in the page with our super */

 align our pointer to the offset of the super block */

/*

 * Look for a btrfs signature on a device. This may be called out of the mount path

 * and we are not allowed to call set_blocksize during the scan. The superblock

 * is read via pagecache

	/*

	 * we would like to check all the supers, but that would make

	 * a btrfs mount succeed after a mkfs from a different FS.

	 * So, we need to add a special mount option to scan for

	 * later supers, using BTRFS_SUPER_MIRROR_MAX instead

/*

 * Try to find a chunk that intersects [start, start + len] range and when one

 * such is found, record the end of it in *start

		/*

		 * We don't want to overwrite the superblock on the drive nor

		 * any area used by the boot loader (grub for example), so we

		 * make sure to start at an offset of at least 1MB.

		/*

		 * We don't care about the starting region like regular

		 * allocator, because we anyway use/reserve the first two zones

		 * for superblock logging.

 Range is ensured to be empty */

 Given hole range was invalid (outside of device) */

/**

 * dev_extent_hole_check - check if specified hole is suitable for allocation

 * @device:	the device which we have the hole

 * @hole_start: starting position of the hole

 * @hole_size:	the size of the hole

 * @num_bytes:	the size of the free space that we need

 *

 * This function may modify @hole_start and @hole_size to reflect the suitable

 * position for allocation. Returns 1 if hole position is updated, 0 otherwise.

		/*

		 * Check before we set max_hole_start, otherwise we could end up

		 * sending back this offset anyway.

 No extra check */

				/*

				 * The changed hole can contain pending extent.

				 * Loop again to check that.

/*

 * find_free_dev_extent_start - find free space in the specified device

 * @device:	  the device which we search the free space in

 * @num_bytes:	  the size of the free space that we need

 * @search_start: the position from which to begin the search

 * @start:	  store the start of the free space.

 * @len:	  the size of the free space. that we find, or the size

 *		  of the max free space if we don't find suitable free space

 *

 * this uses a pretty simple search, the expectation is that it is

 * called very infrequently and that a given device has a small number

 * of extents

 *

 * @start is used to store the start of the free space if we find. But if we

 * don't find suitable free space, it will be used to store the start position

 * of the max free space.

 *

 * @len is used to store the size of the free space that we find.

 * But if we don't find suitable free space, it is used to store the size of

 * the max free space.

 *

 * NOTE: This function will search *commit* root of device tree, and does extra

 * check to ensure dev extents are not double allocated.

 * This makes the function safe to allocate dev extents but may not report

 * correct usable device space, as device extent freed in current transaction

 * is not reported as available.

			/*

			 * If this free space is greater than which we need,

			 * it must be the max free space that we have found

			 * until now, so max_hole_start must point to the start

			 * of this free space and the length of this free space

			 * is stored in max_hole_size. Thus, we return

			 * max_hole_start and max_hole_size and go back to the

			 * caller.

	/*

	 * At this point, search_start should be the end of

	 * allocated dev extents, and when shrinking the device,

	 * search_end may be smaller than search_start.

 See above. */

 FIXME use last free of some kind */

 Corruption */

/*

 * the device information is stored in the chunk root

 * the btrfs_device struct should be fully filled in

/*

 * Function to update ctime/mtime for a given device path.

 * Mainly used for ctime/mtime based probe like libblkid.

 *

 * We don't care about errors here, this is just to be kind to userspace.

/*

 * Verify that @num_devices satisfies the RAID profile constraints in the whole

 * filesystem. It's up to the caller to adjust that number regarding eg. device

 * replace.

/*

 * Helper function to check if the given device is part of s_bdev / latest_dev

 * and replace it with the provided or the next active device, in the context

 * where this function called, there should be always be another device (or

 * this_dev) which is active.

/*

 * Return btrfs_fs_devices::num_devices excluding the device that's being

 * currently replaced.

 write_on_page() unlocks the page */

 Notify udev that device has changed */

 Update ctime/mtime for device path for libblkid */

	/*

	 * The device list in fs_devices is accessed without locks (neither

	 * uuid_mutex nor device_list_mutex) as it won't change on a mounted

	 * filesystem and another device rm cannot run.

	/*

	 * TODO: the superblock still includes this device in its num_devices

	 * counter although write_all_supers() is not locked out. This

	 * could give a filesystem state which requires a degraded mount.

	/*

	 * the device list mutex makes sure that we don't change

	 * the device list while someone else is writing out all

	 * the device supers. Whoever is writing all supers, should

	 * lock the device list mutex before getting the number of

	 * devices in the super block (super_copy). Conversely,

	 * whoever updates the number of devices in the super block

	 * (super_copy) should hold the device list mutex.

	/*

	 * In normal cases the cur_devices == fs_devices. But in case

	 * of deleting a seed device, the cur_devices should point to

	 * its own fs_devices listed under the fs_devices->seed_list.

 Update total_devices of the parent fs_devices if it's seed */

 remove sysfs entry */

	/*

	 * At this point, the device is zero sized and detached from the

	 * devices list.  All that's left is to zero out the old supers and

	 * free the device.

	 *

	 * We cannot call btrfs_close_bdev() here because we're holding the sb

	 * write lock, and blkdev_put() will pull in the ->open_mutex on the

	 * block device and it's dependencies.  Instead just flush the device

	 * and let the caller do the final blkdev_put.

	/*

	 * This can happen if cur_devices is the private seed devices list.  We

	 * cannot call close_fs_devices() here because it expects the uuid_mutex

	 * to be held, but in fact we don't need that for the private

	 * seed_devices, we can simply decrement cur_devices->opened and then

	 * remove it from our list and free the fs_devices.

	/*

	 * in case of fs with no seed, srcdev->fs_devices will point

	 * to fs_devices of fs_info. However when the dev being replaced is

	 * a seed dev it will point to the seed's local fs_devices. In short

	 * srcdev will have its correct fs_devices in both the cases.

 if this is no devs we rather delete the fs_devices */

		/*

		 * On a mounted FS, num_devices can't be zero unless it's a

		 * seed. In case of a seed device being replaced, the replace

		 * target added to the sprout FS, so there will be no more

		 * device left under the seed FS.

/**

 * Populate args from device at path

 *

 * @fs_info:	the filesystem

 * @args:	the args to populate

 * @path:	the path to the device

 *

 * This will read the super block of the device at @path and populate @args with

 * the devid, fsid, and uuid.  This is meant to be used for ioctls that need to

 * lookup a device to operate on, but need to do it before we take any locks.

 * This properly handles the special case of "missing" that a user may pass in,

 * and does some basic sanity checks.  The caller must make sure that @path is

 * properly NUL terminated before calling in, and must call

 * btrfs_put_dev_args_from_path() in order to free up the temporary fsid and

 * uuid buffers.

 *

 * Return: 0 for success, -errno for failure

/*

 * Only use this jointly with btrfs_get_dev_args_from_path() because we will

 * allocate our ->uuid and ->fsid pointers, everybody else uses local variables

 * that don't need to be freed.

/*

 * does all the dirty work required for changing file system's UUID.

	/*

	 * Private copy of the seed devices, anchored at

	 * fs_info->fs_devices->seed_list

	/*

	 * It's necessary to retain a copy of the original seed fs_devices in

	 * fs_uuids so that filesystems which have been seeded can successfully

	 * reference the seed device from open_seed_devices. This also supports

	 * multiple fs seed.

/*

 * Store the expected generation for seed devices in device items.

 Logic error */

 we can safely leave the fs_devices entry around */

	/*

	 * we've got more storage, clear any full flags on the space

	 * infos

 Add sysfs device entry */

		/*

		 * fs_devices now represents the newly sprouted filesystem and

		 * its fsid has been changed by btrfs_prepare_sprout

 transaction commit */

	/*

	 * Now that we have written a new super block to this device, check all

	 * other fs_devices list if device_path alienates any other scanned

	 * device.

	 * We can ignore the return value as it typically returns -EINVAL and

	 * only succeeds if the device was an alien.

 Update ctime/mtime for blkid or udev */

 Logic error or corruption */

/*

 * btrfs_get_chunk_map() - Find the mapping containing the given logical extent.

 * @logical: Logical block offset in bytes.

 * @length: Length of extent in bytes.

 *

 * Return: Chunk mapping or ERR_PTR.

 callers are responsible for dropping em's ref. */

	/*

	 * Removing chunk items and updating the device items in the chunks btree

	 * requires holding the chunk_mutex.

	 * See the comment at btrfs_chunk_alloc() for the details.

		/*

		 * This is a logic error, but we don't want to just rely on the

		 * user having built with ASSERT enabled, so if ASSERT doesn't

		 * do anything we still error out.

	/*

	 * First delete the device extent items from the devices btree.

	 * We take the device_list_mutex to avoid racing with the finishing phase

	 * of a device replace operation. See the comment below before acquiring

	 * fs_info->chunk_mutex. Note that here we do not acquire the chunk_mutex

	 * because that can result in a deadlock when deleting the device extent

	 * items from the devices btree - COWing an extent buffer from the btree

	 * may result in allocating a new metadata chunk, which would attempt to

	 * lock again fs_info->chunk_mutex.

	/*

	 * We acquire fs_info->chunk_mutex for 2 reasons:

	 *

	 * 1) Just like with the first phase of the chunk allocation, we must

	 *    reserve system space, do all chunk btree updates and deletions, and

	 *    update the system chunk array in the superblock while holding this

	 *    mutex. This is for similar reasons as explained on the comment at

	 *    the top of btrfs_chunk_alloc();

	 *

	 * 2) Prevent races with the final phase of a device replace operation

	 *    that replaces the device object associated with the map's stripes,

	 *    because the device object's id can change at any time during that

	 *    final phase of the device replace operation

	 *    (dev-replace.c:btrfs_dev_replace_finishing()), so we could grab the

	 *    replaced device and then see it with an ID of

	 *    BTRFS_DEV_REPLACE_DEVID, which would cause a failure when updating

	 *    the device item, which does not exists on the chunk btree.

	 *    The finishing phase of device replace acquires both the

	 *    device_list_mutex and the chunk_mutex, in that order, so we are

	 *    safe by just acquiring the chunk_mutex.

	/*

	 * Normally we should not get -ENOSPC since we reserved space before

	 * through the call to check_system_chunk().

	 *

	 * Despite our system space_info having enough free space, we may not

	 * be able to allocate extents from its block groups, because all have

	 * an incompatible profile, which will force us to allocate a new system

	 * block group with the right profile, or right after we called

	 * check_system_space() above, a scrub turned the only system block group

	 * with enough free space into RO mode.

	 * This is explained with more detail at do_chunk_alloc().

	 *

	 * So if we get -ENOSPC, allocate a new system chunk and retry once.

	/*

	 * We are done with chunk btree updates and deletions, so release the

	 * system space we previously reserved (with check_system_chunk()).

 once for us */

	/*

	 * Prevent races with automatic removal of unused block groups.

	 * After we relocate and before we remove the chunk with offset

	 * chunk_offset, automatic removal of the block group can kick in,

	 * resulting in a failure when calling btrfs_remove_chunk() below.

	 *

	 * Make sure to acquire this mutex before doing a tree search (dev

	 * or chunk trees) to find chunks. Otherwise the cleaner kthread might

	 * call btrfs_remove_chunk() (through btrfs_delete_unused_bgs()) after

	 * we release the path used to search the chunk/dev tree and before

	 * the current task acquires this mutex and calls us.

 step one, relocate all the extents inside this chunk */

	/*

	 * On a zoned file system, discard the whole block group, this will

	 * trigger a REQ_OP_ZONE_RESET operation on the device zone. If

	 * resetting the zone fails, don't treat it as a fatal problem from the

	 * filesystem's point of view.

	/*

	 * step two, delete the device extents and the

	 * chunk tree entries

 Corruption */

/*

 * return 1 : allocate a data chunk successfully,

 * return <0: errors during allocating a data chunk,

 * return 0 : no need to allocate a data chunk.

/*

 * This is a heuristic used to reduce the number of chunks balanced on

 * resume after balance was interrupted.

	/*

	 * Turn on soft mode for chunk types that were being converted.

	/*

	 * Turn on usage filter if is not already used.  The idea is

	 * that chunks that we have already balanced should be

	 * reasonably full.  Don't do it for chunks that are being

	 * converted - that will keep us from relocating unconverted

	 * (albeit full) chunks.

/*

 * Clear the balance status in fs_info and delete the balance item from disk.

/*

 * Balance filters.  Return 1 if chunk should be filtered out

 * (should not be balanced).

 [pstart, pend) */

 [vstart, vend) */

 at least part of the chunk is inside this vrange */

 type filter */

 profiles filter */

 usage filter */

 devid filter */

 drange filter, makes sense only with devid filter */

 vrange filter */

 stripes filter */

 soft profile changing mode */

	/*

	 * limited by count, must be the last filter

		/*

		 * Same logic as the 'limit' filter; the minimum cannot be

		 * determined here because we do not have the global information

		 * about the count of all chunks that satisfy the filters.

 The single value limit and min/max limits use the same bytes in the */

 zero out stat counters */

		/*

		 * The single value limit and min/max limits use the same bytes

		 * in the

		/*

		 * this shouldn't happen, it means the last relocate

		 * failed

 FIXME break ? */

		/*

		 * Apply limit_min filter, no need to check if the LIMITS

		 * filter is used, limit_min is 0 by default

			/*

			 * We may be relocating the only data chunk we have,

			 * which could potentially end up with losing data's

			 * raid profile, so lets allocate an empty one in

			 * advance.

/**

 * alloc_profile_is_valid - see if a given profile is valid and reduced

 * @flags: profile to validate

 * @extended: if true @flags is treated as an extended profile

 1) check that all other bits are zeroed */

 2) see if profile is reduced */

 "0" is valid for usual profiles */

 cancel requested || normal exit path */

/*

 * Validate target profile against allowed profiles and return true if it's OK.

 * Otherwise print the error message and return false.

 Profile is valid and does not have bits outside of the allowed set */

/*

 * Fill @buf with textual description of balance filter flags @bargs, up to

 * @size_buf including the terminating null. The output may be trimmed if it

 * does not fit into the provided buffer.

 remove last , */

 remove last " " */

/*

 * Should be called with balance mutexe held

	/*

	 * In case of mixed groups both data and meta should be picked,

	 * and identical options should be given for both of them.

	/*

	 * rw_devices will not change at the moment, device add/delete/replace

	 * are exclusive

	/*

	 * SINGLE profile on-disk has no profile bit, but in-memory we have a

	 * special bit for it, to make it easier to distinguish.  Thus we need

	 * to set it manually, or balance would refuse the profile.

	/*

	 * Allow to reduce metadata or system integrity only if force set for

	 * profiles with redundancy (copies, parity)

 if we're not converting, the target field is uninitialized */

	/*

	 * Balance can be canceled by:

	 *

	 * - Regular cancel request

	 *   Then ret == -ECANCELED and balance_cancel_req > 0

	 *

	 * - Fatal signal to "btrfs" process

	 *   Either the signal caught by wait_reserve_ticket() and callers

	 *   got -EINTR, or caught by btrfs_should_cancel_balance() and

	 *   got -ECANCELED.

	 *   Either way, in this case balance_cancel_req = 0, and

	 *   ret == -EINTR or ret == -ECANCELED.

	 *

	 * So here we only check the return value to catch canceled balance.

	/*

	 * A ro->rw remount sequence should continue with the paused balance

	 * regardless of who pauses it, system or the user as of now, so set

	 * the resume flag.

 ret = -ENOENT; */

	/*

	 * This should never happen, as the paused balance state is recovered

	 * during mount without any chance of other exclusive ops to collide.

	 *

	 * This gives the exclusive op status to balance and keeps in paused

	 * state until user intervention (cancel or umount). If the ownership

	 * cannot be assigned, show a message but do not fail. The balance

	 * is in a paused state and must have fs_info::balance_ctl properly

	 * set up.

 we are good with balance_ctl ripped off from under us */

	/*

	 * A paused balance with the item stored on disk can be resumed at

	 * mount time if the mount is read-write. Otherwise it's still paused

	 * and we must not allow cancelling as it deletes the item.

	/*

	 * if we are running just wait and return, balance item is

	 * deleted in btrfs_balance in this case

		/*

		 * Lock released to allow other waiters to continue, we'll

		 * reexamine the status again.

			/*

			 * 1 - subvol uuid item

			 * 1 - received_subvol uuid item

	/*

	 * 1 - root node

	 * 1 - root item

 fs_info->update_uuid_tree_gen remains 0 in all error case */

/*

 * shrinking a device means finding all of the device extents past

 * the new size, and then following the back refs to the chunks.

 * The chunk relocation code actually frees the device extent

	/*

	 * Once the device's size has been set to the new size, ensure all

	 * in-memory chunks are synced to disk so that the loop below sees them

	 * and relocates them accordingly.

		/*

		 * We may be relocating the only data chunk we have,

		 * which could potentially end up with losing data's

		 * raid profile, so lets allocate an empty one in

		 * advance.

 Shrinking succeeded, else we would be at "done". */

 Clear all state bits beyond the shrunk device size */

 Now btrfs_update_device() will change the on-disk size. */

/*

 * sort the devices in descending order by max_avail, total_avail

/*

 * Structure used internally for btrfs_create_chunk() function.

 * Wraps needed parameters.

 Total number of stripes to allocate */

 sub_stripes info for map */

 Stripes per device */

 Maximum number of devices to use */

 Minimum number of devices to use */

 ndevs has to be a multiple of this */

 Number of copies */

 Number of stripes worth of bytes to store parity information */

 For larger filesystems, use larger metadata chunks */

 We don't want a chunk larger than 10% of writable space */

 We don't want a chunk larger than 10% of writable space */

	/*

	 * in the first pass through the devices list, we gather information

	 * about the available holes on each device.

 If there is no space on this device, skip it. */

	/*

	 * now sort the devices by hole size / available space

 Number of stripes that count for block group size */

	/*

	 * The primary goal is to maximize the number of stripes, so use as

	 * many devices as possible, even if the stripes are not maximum sized.

	 *

	 * The DUP profile stores more than one stripe per device, the

	 * max_avail is the total size so we have to adjust.

 This will have to be fixed for RAID1 and RAID10 over more drives */

	/*

	 * Use the number of data stripes to figure out how big this chunk is

	 * really going to be in terms of logical address space, and compare

	 * that answer with the max chunk size. If it's higher, we try to

	 * reduce stripe_size.

		/*

		 * Reduce stripe_size, round it up to a 16MB boundary again and

		 * then use it, unless it ends up being even bigger than the

		 * previous value we had already.

 Align to BTRFS_STRIPE_LEN */

 Number of stripes that count for block group size */

	/*

	 * It should hold because:

	 *    dev_extent_min == dev_extent_want == zone_size * dev_stripes

 stripe_size is fixed in zoned filesysmte. Reduce ndevs instead. */

	/*

	 * Round down to number of usable stripes, devs_increment can be any

	 * number so we can't use round_down() that requires power of 2, while

	 * rounddown is safe.

 One for our allocation */

 One for the tree reference */

/*

 * This function, btrfs_chunk_alloc_add_chunk_item(), typically belongs to the

 * phase 1 of chunk allocation. It belongs to phase 2 only when allocating system

 * chunks.

 *

 * See the comment at btrfs_chunk_alloc() for details about the chunk allocation

 * phases.

	/*

	 * We take the chunk_mutex for 2 reasons:

	 *

	 * 1) Updates and insertions in the chunk btree must be done while holding

	 *    the chunk_mutex, as well as updating the system chunk array in the

	 *    superblock. See the comment on top of btrfs_chunk_alloc() for the

	 *    details;

	 *

	 * 2) To prevent races with the final phase of a device replace operation

	 *    that replaces the device object associated with the map's stripes,

	 *    because the device object's id can change at any time during that

	 *    final phase of the device replace operation

	 *    (dev-replace.c:btrfs_dev_replace_finishing()), so we could grab the

	 *    replaced device and then see it with an ID of BTRFS_DEV_REPLACE_DEVID,

	 *    which would cause a failure when updating the device item, which does

	 *    not exists, or persisting a stripe of the chunk item with such ID.

	 *    Here we can't use the device_list_mutex because our caller already

	 *    has locked the chunk_mutex, and the final phase of device replace

	 *    acquires both mutexes - first the device_list_mutex and then the

	 *    chunk_mutex. Using any of those two mutexes protects us from a

	 *    concurrent device replace.

	/*

	 * When adding a new device for sprouting, the seed device is read-only

	 * so we must first allocate a metadata and a system chunk. But before

	 * adding the block group items to the extent, device and chunk btrees,

	 * we must first:

	 *

	 * 1) Create both chunks without doing any changes to the btrees, as

	 *    otherwise we would get -ENOSPC since the block groups from the

	 *    seed device are read-only;

	 *

	 * 2) Add the device item for the new sprout device - finishing the setup

	 *    of a new block group requires updating the device item in the chunk

	 *    btree, so it must exist when we attempt to do it. The previous step

	 *    ensures this does not fail with -ENOSPC.

	 *

	 * After that we can add the block group items to their btrees:

	 * update existing device item in the chunk btree, add a new block group

	 * item to the extent btree, add a new chunk item to the chunk btree and

	 * finally add the new device extent items to the devices btree.

	/*

	 * If the number of missing devices is larger than max errors, we can

	 * not write the data into that chunk successfully.

 once for us */

 once for the tree */

		/*

		 * We could return errors for these cases, but that could get

		 * ugly and we'd probably do the same thing which is just not do

		 * anything else and exit, so return 1 so the callers don't try

		 * to use other copies.

		/*

		 * There could be two corrupted data stripes, we need

		 * to loop retry in order to rebuild the correct data.

		 *

		 * Fail a stripe at a time on every retry except the

		 * stripe under reconstruction.

 Shouldn't happen, just warn and use pid instead of failing */

	/*

	 * try to avoid the drive that is the source drive for a

	 * dev-replace procedure, only choose it if no other non-missing

	 * mirror is available

	/* we couldn't find one that doesn't fail.  Just return something

	 * and the io error handling code will clean up eventually

 Bubble-sort the stripe set to put the parity/syndrome stripes last */

 Swap if parity is on a smaller index */

 The size of btrfs_io_context */

 Plus the variable array for the stripes */

 Plus the variable array for the tgt dev */

		/*

		 * Plus the raid_map, which includes both the tgt dev

		 * and the stripes.

 can REQ_OP_DISCARD be sent with other REQ like REQ_OP_WRITE? */

/*

 * Please note that, discard won't be sent to target device of device

 * replace.

 Discard always returns a bioc. */

 we don't discard raid56 yet */

	/*

	 * stripe_nr counts the total number of stripes we have to stride

	 * to get to this block

 stripe_offset is the offset of this block in its stripe */

	/*

	 * after this, stripe_nr is the number of stripes on this

	 * device we have to walk to find the data, and stripe_index is

	 * the number of our device in the stripe array

			/*

			 * Special for the first stripe and

			 * the last stripe:

			 *

			 * |-------|...|-------|

			 *     |----------|

			 *    off     end_off

/*

 * In dev-replace case, for repair case (that's the only case where the mirror

 * is selected explicitly when calling btrfs_map_block), blocks left of the

 * left cursor can also be read from the target drive.

 *

 * For REQ_GET_READ_MIRRORS, the target drive is added as the last one to the

 * array of stripes.

 * For READ, it also needs to be supported using the same mirror number.

 *

 * If the requested block is not left of the left cursor, EIO is returned. This

 * can happen because btrfs_num_copies() returns one more in the dev-replace

 * case.

		/*

		 * BTRFS_MAP_GET_READ_MIRRORS does not contain this mirror,

		 * that means that the requested area is not left of the left

		 * cursor

	/*

	 * process the rest of the function using the mirror_num of the source

	 * drive. Therefore look it up first.  At the end, patch the device

	 * pointer to the one of the target drive.

		/*

		 * In case of DUP, in order to keep it simple, only add the

		 * mirror with the lowest physical address

 Non zoned filesystem does not use "to_copy" flag */

		/*

		 * A block group which have "to_copy" set will eventually

		 * copied by dev-replace process. We can avoid cloning IO here.

		/*

		 * duplicate the write operations while the dev replace

		 * procedure is running. Since the copying of the old disk to

		 * the new disk takes place at run time while the filesystem is

		 * mounted writable, the regular write operations to the old

		 * disk have to be duplicated to go to the new disk as well.

		 *

		 * Note that device->missing is handled by the caller, and that

		 * the write to the old disk is already set up in the stripes

		 * array.

 write to new disk, too */

		/*

		 * During the dev-replace procedure, the target drive can also

		 * be used to read data in case it is needed to repair a corrupt

		 * block elsewhere. This is possible if the requested area is

		 * left of the left cursor. In this area, the target drive is a

		 * full copy of the source drive.

				/*

				 * In case of DUP, in order to keep it simple,

				 * only add the mirror with the lowest physical

				 * address

/*

 * Calculate the geometry of a particular (address, len) tuple. This

 * information is used to calculate how big a particular bio can get before it

 * straddles a stripe.

 *

 * @fs_info: the filesystem

 * @em:      mapping containing the logical extent

 * @op:      type of operation - write or read

 * @logical: address that we want to figure out the geometry of

 * @io_geom: pointer used to return values

 *

 * Returns < 0 in case a chunk for the given logical address cannot be found,

 * usually shouldn't happen unless @logical is corrupted, 0 otherwise.

 Offset of this logical address in the chunk */

 Len of a stripe in a chunk */

 Stripe where this block falls in */

 Offset of stripe in the chunk */

 stripe_offset is the offset of this block in its stripe */

		/*

		 * In case of raid56, we need to know the stripe aligned start

			/*

			 * Allow a write of a full stripe, but make sure we

			 * don't allow straddling of stripes

			/*

			 * For writes to RAID[56], allow a full stripeset across

			 * all disks. For other RAID types and for RAID[56]

			 * reads, just allow a single stripe (on a single disk).

	/*

	 * Hold the semaphore for read during the whole operation, write is

	 * requested at commit time but must wait.

 push stripe_nr back to the start of the full stripe */

 RAID[56] write or recovery. Return all stripes */

			/*

			 * Mirror #0 or #1 means the original data block.

			 * Mirror #2 is RAID5 parity block.

			 * Mirror #3 is RAID6 Q block.

 We distribute the parity blocks across stripes */

		/*

		 * after this, stripe_nr is the number of stripes on this

		 * device we have to walk to find the data, and stripe_index is

		 * the number of our device in the stripe array

 Build raid_map */

 Work out the disk rotation on this stripe-set */

 Fill in the logical address of each stripe */

	/*

	 * this is the case that REQ_READ && dev_replace_is_ongoing &&

	 * mirror_num == num_stripes + 1 && dev_replace target drive is

	 * available as a mirror

 Unlock and let waiting writers proceed */

 For Scrub/replace */

		/* only send an error to the higher layers if it is

		 * beyond the tolerance of the btrfs bio

			/*

			 * this bio is actually up to date, we didn't

			 * go over the max number of errors

	/*

	 * For zone append writing, bi_sector must point the beginning of the

	 * zone

 Should be the original bio. */

		/* In this case, map_length has been set to the length of

/*

 * Find a device specified by @devid or @uuid in the list of @fs_devices, or

 * return NULL.

 *

 * If devid and uuid are both specified, the match must be exact, otherwise

 * only devid is used.

	/*

	 * We call this under the chunk_mutex, so we want to use NOFS for this

	 * allocation, however we don't want to change btrfs_alloc_device() to

	 * always do NOFS because we use it in a lot of other GFP_KERNEL safe

	 * places.

/**

 * btrfs_alloc_device - allocate struct btrfs_device

 * @fs_info:	used only for generating a new devid, can be NULL if

 *		devid is provided (i.e. @devid != NULL).

 * @devid:	a pointer to devid for this device.  If NULL a new devid

 *		is generated.

 * @uuid:	a pointer to UUID for this device.  If NULL a new UUID

 *		is generated.

 *

 * Return: a pointer to a new &struct btrfs_device on success; ERR_PTR()

 * on error.  Returned struct is not linked onto any lists and must be

 * destroyed with btrfs_free_device.

	/*

	 * Preallocate a bio that's always going to be used for flushing device

	 * barriers and matches the device lifespan

/*

 * Due to page cache limit, metadata beyond BTRFS_32BIT_MAX_FILE_SIZE

 * can't be accessed on 32bit systems.

 *

 * This function do mount time check to reject the fs if it already has

 * metadata chunk beyond that limit.

/*

 * This is to give early warning for any metadata chunk reaching

 * BTRFS_32BIT_EARLY_WARN_THRESHOLD.

 * Although we can still access the metadata, it's not going to be possible

 * once the limit is reached.

	/*

	 * Only need to verify chunk item if we're reading from sys chunk array,

	 * as chunk item in tree block is already verified by tree-checker.

 already mapped? */

 This will match only for multi-device seed fs */

	/*

	 * Upon first call for a seed fs fsid, just create a private copy of the

	 * respective fs_devices and anchor it at fs_info->fs_devices->seed_list

			/*

			 * this happens when a device that was properly setup

			 * in the device info lists suddenly goes bad.

			 * device->bdev is NULL, and so we have to set

			 * device->missing to one here

 Move the device to its own fs_devices */

	/*

	 * This will create extent buffer of nodesize, superblock size is

	 * fixed to BTRFS_SUPER_INFO_SIZE. If nodesize > sb size, this will

	 * overallocate but we can keep it as-is, only the first page is used.

	/*

	 * The sb extent buffer is artificial and just used to read the system array.

	 * set_extent_buffer_uptodate() call does not properly mark all it's

	 * pages up-to-date when the page is larger: extent does not cover the

	 * whole page and consequently check_page_uptodate does not find all

	 * the page's extents up-to-date (the hole beyond sb),

	 * write_extent_buffer then triggers a WARN_ON.

	 *

	 * Regular short extents go through mark_extent_buffer_dirty/writeback cycle,

	 * but sb spans only this function. Add an explicit SetPageUptodate call

	 * to silence the warning eg. on PowerPC 64.

		/*

		 * At least one btrfs_chunk with one stripe must be present,

		 * exact stripe count check comes afterwards

/*

 * Check if all chunks in the fs are OK for read-write degraded mount

 *

 * If the @failing_dev is specified, it's accounted as missing.

 *

 * Return true if all chunks meet the minimal RW mount requirements.

 * Return false if any chunk doesn't meet the minimal RW mount requirements.

 No chunk at all? Return false anyway */

	/*

	 * uuid_mutex is needed only if we are mounting a sprout FS

	 * otherwise we don't need it.

	/*

	 * It is possible for mount and umount to race in such a way that

	 * we execute this code path, but open_fs_devices failed to clear

	 * total_rw_bytes. We certainly want it cleared before reading the

	 * device items, so clear it here.

	/*

	 * Lockdep complains about possible circular locking dependency between

	 * a disk's open_mutex (struct gendisk.open_mutex), the rw semaphores

	 * used for freeze procection of a fs (struct super_block.s_writers),

	 * which we take when starting a transaction, and extent buffers of the

	 * chunk tree if we call read_one_dev() while holding a lock on an

	 * extent buffer of the chunk tree. Since we are mounting the filesystem

	 * and at this point there can't be any concurrent task modifying the

	 * chunk tree, to keep it simple, just skip locking on the chunk tree.

	/*

	 * Read all device items, and then all the chunk items. All

	 * device items are found before any chunk item (their object id

	 * is smaller than the lowest possible object id for a chunk

	 * item - BTRFS_FIRST_CHUNK_TREE_OBJECTID).

			/*

			 * We are only called at mount time, so no need to take

			 * fs_info->chunk_mutex. Plus, to avoid lockdep warnings,

			 * we always lock first fs_info->chunk_mutex before

			 * acquiring any locks on the chunk tree. This is a

			 * requirement for chunk allocation, see the comment on

			 * top of btrfs_chunk_alloc() for details.

	/*

	 * After loading chunk tree, we've got all device information,

	 * do another round of validation checks.

 need to delete old one and insert a new one */

 need to insert a new item */

/*

 * called from commit_transaction. Writes all changed device stats to disk.

		/*

		 * There is a LOAD-LOAD control dependency between the value of

		 * dev_stats_ccnt and updating the on-disk values which requires

		 * reading the in-memory counters. Such control dependencies

		 * require explicit read memory barriers.

		 *

		 * This memory barriers pairs with smp_mb__before_atomic in

		 * btrfs_dev_stat_inc/btrfs_dev_stat_set and with the full

		 * barrier implied by atomic_xchg in

		 * btrfs_dev_stats_read_and_reset

 all values == 0, suppress message */

/*

 * Update the size and bytes used for each device where it changed.  This is

 * delayed since we would otherwise get errors while writing out the

 * superblocks.

 *

 * Must be invoked during transaction commit.

	/*

	 * We don't need the device_list_mutex here.  This list is owned by the

	 * transaction and the transaction must complete before the device is

	 * released.

/*

 * Multiplicity factor for simple profiles: DUP, RAID1-like and RAID10.

 Make sure no dev extent is beyond device boundary */

/*

 * Ensure that all dev extents are mapped to correct chunk, otherwise

 * later chunk allocation/free would cause unexpected behavior.

 *

 * NOTE: This will iterate through the whole device tree, which should be of

 * the same size level as the chunk tree.  This slightly increases mount time.

	/*

	 * We don't have a dev_root because we mounted with ignorebadroots and

	 * failed to load the root, so we want to skip the verification in this

	 * case for sure.

	 *

	 * However if the dev root is fine, but the tree itself is corrupted

	 * we'd still fail to mount.  This verification is only to make sure

	 * writes can happen safely, so instead just bypass this check

	 * completely in the case of IGNOREBADROOTS.

 No dev extents at all? Not good */

 Check if this dev extent overlaps with the previous one */

 Ensure all chunks have corresponding dev extents */

/*

 * Check whether the given block group or device is pinned by any inode being

 * used as a swapfile.

 Ensure block group still exists */

 Do not attempt to repair in degraded state */

 SPDX-License-Identifier: GPL-2.0

/*

 * Return target flags in extended format or 0 if restripe for this chunk_type

 * is not in progress

 *

 * Should be called with balance_lock held

/*

 * @flags: available profiles in extended format (see ctree.h)

 *

 * Return reduced profile in chunk format.  If profile changing is in progress

 * (either running or paused) picks the target profile (if it's already

 * available), otherwise falls back to plain reducing.

	/*

	 * See if restripe for this chunk_type is in progress, if so try to

	 * reduce to the target profile

 First, mask out the RAID levels which aren't possible */

		/*

		 * A block_group shouldn't be on the discard_list anymore.

		 * Remove the block_group from the discard_list to prevent us

		 * from causing a panic due to NULL pointer dereference.

		/*

		 * If not empty, someone is still holding mutex of

		 * full_stripe_lock, which can only be released by caller.

		 * And it will definitely cause use-after-free when caller

		 * tries to release full stripe lock.

		 *

		 * No better way to resolve, but only to warn.

/*

 * This adds the block group to the fs_info rb tree for the block group cache

/*

 * This will return the block group at or after bytenr if contains is 0, else

 * it will return the block group that contains the bytenr

/*

 * Return the block group that starts at or after bytenr

/*

 * Return the block group that contains the given bytenr

 If our block group was removed, we need a full search. */

 No put on block group, done by btrfs_dec_nocow_writers */

	/*

	 * Once for our lookup and once for the lookup done by a previous call

	 * to btrfs_inc_nocow_writers()

	/*

	 * Our block group is read only but before we set it to read only,

	 * some task might have had allocated an extent from it already, but it

	 * has not yet created a respective ordered extent (and added it to a

	 * root's list of ordered extents).

	 * Therefore wait for any task currently allocating extents, since the

	 * block group's reservations counter is incremented while a read lock

	 * on the groups' semaphore is held and decremented after releasing

	 * the read access on that semaphore and creating the ordered extent.

/*

 * When we wait for progress in the block group caching, its because our

 * allocation attempt failed at least once.  So, we must sleep and let some

 * progress happen before we try again.

 *

 * This function will sleep at least once waiting for new free space to show

 * up, and then it will check the block group free space numbers for our min

 * num_bytes.  Another option is to have it go ahead and look in the rbtree for

 * a free extent of a given size, but this is a good start.

 *

 * Callers of this must check if cache->cached == BTRFS_CACHE_ERROR before using

 * any of the information in this block group.

/*

 * This is only called by btrfs_cache_block_group, since we could have freed

 * extents we need to check the pinned_extents for any extents that can't be

 * used yet since their free space will be released as soon as the transaction

 * commits.

 -ENOMEM or logic error */

 -ENOMEM or logic error */

	/*

	 * If we're fragmenting we don't want to make anybody think we can

	 * allocate from this block group until we've had a chance to fragment

	 * the free space.

	/*

	 * We don't want to deadlock with somebody trying to allocate a new

	 * extent for the extent root while also trying to search the extent

	 * root to add free space.  So we skip locking and search the commit

	 * root, since its read-only

		/*

		 * We failed to load the space cache, set ourselves to

		 * CACHE_STARTED and carry on.

	/*

	 * If we are in the transaction that populated the free space tree we

	 * can't actually cache from the free space tree as our commit root and

	 * real root are the same, so we could change the contents of the blocks

	 * while caching.  Instead do the slow caching in this case, and after

	 * the transaction has committed we will be safe.

 Allocator for zoned filesystems does not use the cache at all */

/*

 * Clear incompat bits for the following feature(s):

 *

 * - RAID56 - in case there's neither RAID5 nor RAID6 profile block group

 *            in the whole filesystem

 *

 * - RAID1C34 - same as above for RAID1C3 and RAID1C4 block groups

	/*

	 * Free the reserved super bytes from this block group before

	 * remove it.

 make sure this block group isn't part of an allocation cluster */

	/*

	 * make sure this block group isn't part of a metadata

	 * allocation cluster

	/*

	 * get the inode first so any iput calls done for the io_list

	 * aren't the final iput (no unlinks allowed now)

	/*

	 * Make sure our free space cache IO is done before removing the

	 * free space inode

 Once for the block groups rbtree */

	/*

	 * we must use list_del_init so people can check to see if they

	 * are still on the list after taking the semaphore

 Once for the caching bgs list and once for us. */

	/*

	 * Remove the free space for the block group from the free space tree

	 * and the block group's item from the extent tree before marking the

	 * block group as removed. This is to prevent races with tasks that

	 * freeze and unfreeze a block group, this task and another task

	 * allocating a new block group - the unfreeze task ends up removing

	 * the block group's extent map before the task calling this function

	 * deletes the block group item from the extent tree, allowing for

	 * another task to attempt to create another block group with the same

	 * item key (and failing with -EEXIST and a transaction abort).

	/*

	 * At this point trimming or scrub can't start on this block group,

	 * because we removed the block group from the rbtree

	 * fs_info->block_group_cache_tree so no one can't find it anymore and

	 * even if someone already got this block group before we removed it

	 * from the rbtree, they have already incremented block_group->frozen -

	 * if they didn't, for the trimming case they won't find any free space

	 * entries because we already removed them all when we called

	 * btrfs_remove_free_space_cache().

	 *

	 * And we must not remove the extent map from the fs_info->mapping_tree

	 * to prevent the same logical address range and physical device space

	 * ranges from being reused for a new block group. This is needed to

	 * avoid races with trimming and scrub.

	 *

	 * An fs trim operation (btrfs_trim_fs() / btrfs_ioctl_fitrim()) is

	 * completely transactionless, so while it is trimming a range the

	 * currently running transaction might finish and a new one start,

	 * allowing for new block groups to be created that can reuse the same

	 * physical device locations unless we take this special care.

	 *

	 * There may also be an implicit trim operation if the file system

	 * is mounted with -odiscard. The same protections must remain

	 * in place until the extents have been discarded completely when

	 * the transaction commit has completed.

 once for the tree */

 Once for the lookup reference */

	/*

	 * We need to reserve 3 + N units from the metadata space info in order

	 * to remove a block group (done at btrfs_remove_chunk() and at

	 * btrfs_remove_block_group()), which are used for:

	 *

	 * 1 unit for adding the free space inode's orphan (located in the tree

	 * of tree roots).

	 * 1 unit for deleting the block group item (located in the extent

	 * tree).

	 * 1 unit for deleting the free space item (located in tree of tree

	 * roots).

	 * N units for deleting N device extent items corresponding to each

	 * stripe (located in the device tree).

	 *

	 * In order to remove a block group we also need to reserve units in the

	 * system space info in order to update the chunk tree (update one or

	 * more device items and remove one chunk item), but this is done at

	 * btrfs_remove_chunk() through a call to check_system_chunk().

/*

 * Mark block group @cache read-only, so later write won't happen to block

 * group @cache.

 *

 * If @force is not set, this function will only mark the block group readonly

 * if we have enough free space (1M) in other metadata/system block groups.

 * If @force is not set, this function will mark the block group readonly

 * without checking free space.

 *

 * NOTE: This function doesn't care if other block groups can contain all the

 * data in this block group. That check should be done by relocation routine,

 * not this function.

	/*

	 * Data never overcommits, even in mixed mode, so do just the straight

	 * check of left over space in how much we have allocated.

		/*

		 * Here we make sure if we mark this bg RO, we still have enough

		 * free space as buffer.

		/*

		 * We overcommit metadata, so we need to do the

		 * btrfs_can_overcommit check here, and we need to pass in

		 * BTRFS_RESERVE_NO_FLUSH to give ourselves the most amount of

		 * leeway to allow us to mark this block group as read only.

 Migrate zone_unusable bytes to readonly */

	/*

	 * Hold the unused_bg_unpin_mutex lock to avoid racing with

	 * btrfs_finish_extent_commit(). If we are at transaction N, another

	 * task might be running finish_extent_commit() for the previous

	 * transaction N - 1, and have seen a range belonging to the block

	 * group in pinned_extents before we were able to clear the whole block

	 * group range from pinned_extents. This means that task can lookup for

	 * the block group after we unpinned it from pinned_extents and removed

	 * it, leading to a BUG_ON() at unpin_extent_range().

/*

 * Process the unused_bgs list and remove any that don't have any allocated

 * space inside of them.

	/*

	 * Long running balances can keep us blocked here for eternity, so

	 * simply skip deletion if we're unable to get the mutex.

 Don't want to race with allocators so take the groups_sem */

		/*

		 * Async discard moves the final block group discard to be prior

		 * to the unused_bgs code path.  Therefore, if it's not fully

		 * trimmed, punt it back to the async discard lists.

 Requeue if we failed because of async discard */

			/*

			 * We want to bail if we made new allocations or have

			 * outstanding allocations in this block group.  We do

			 * the ro check in case balance is currently acting on

			 * this block group.

 We don't want to force the issue, only flip if it's ok. */

		/*

		 * Want to do this before we do anything else so we can recover

		 * properly if we fail to join the transaction.

		/*

		 * We could have pending pinned extents for this block group,

		 * just delete them, we don't care about them anymore.

		/*

		 * At this point, the block_group is read only and should fail

		 * new allocations.  However, btrfs_finish_extent_commit() can

		 * cause this block_group to be placed back on the discard

		 * lists because now the block_group isn't fully discarded.

		 * Bail here and try again later after discarding everything.

 Reset pinned so btrfs_put_block_group doesn't complain */

		/*

		 * The normal path here is an unused block group is passed here,

		 * then trimming is handled in the transaction commit path.

		 * Async discard interposes before this to do the trimming

		 * before coming down the unused block group path as trimming

		 * will no longer be done later in the transaction commit path.

		/*

		 * DISCARD can flip during remount. On zoned filesystems, we

		 * need to reset sequential-required zones.

 Implicit trim during transaction commit. */

		/*

		 * Btrfs_remove_chunk will abort the transaction if things go

		 * horribly wrong.

		/*

		 * If we're not mounted with -odiscard, we can just forget

		 * about this block group. Otherwise we'll need to wait

		 * until transaction commit to do the actual discard.

			/*

			 * A concurrent scrub might have added us to the list

			 * fs_info->unused_bgs, so use a list_move operation

			 * to add the block group to the deleted_bgs list.

/*

 * We want block groups with a low number of used bytes to be in the beginning

 * of the list, so they will get reclaimed first.

	/*

	 * Long running balances can keep us blocked here for eternity, so

	 * simply skip reclaim if we're unable to get the mutex.

	/*

	 * Sort happens under lock because we can't simply splice it and sort.

	 * The block groups might still be in use and reachable via bg_list,

	 * and their presence in the reclaim_bgs list must be preserved.

 Don't race with allocators so take the groups_sem */

			/*

			 * We want to bail if we made new allocations or have

			 * outstanding allocations in this block group.  We do

			 * the ro check in case balance is currently acting on

			 * this block group.

 Get out fast, in case we're unmounting the filesystem */

		/*

		 * Cache the zone_unusable value before turning the block group

		 * to read only. As soon as the blog group is read only it's

		 * zone_unusable value gets moved to the block group's read-only

		 * bytes and isn't available for calculations anymore.

/**

 * Map a physical disk address to a list of logical addresses

 *

 * @fs_info:       the filesystem

 * @chunk_start:   logical address of block group

 * @bdev:	   physical device to resolve, can be NULL to indicate any device

 * @physical:	   physical address to map to logical addresses

 * @logical:	   return array of logical addresses which map to @physical

 * @naddrs:	   length of @logical

 * @stripe_len:    size of IO stripe for the given block group

 *

 * Maps a particular @physical disk address to a list of @logical addresses.

 * Used primarily to exclude those portions of a block group that contain super

 * block copies.

 For RAID5/6 adjust to a full IO stripe length */

		/*

		 * The remaining case would be for RAID56, multiply by

		 * nr_data_stripes().  Alternatively, just use rmap_len below

		 * instead of map->stripe_len

 Ensure we don't add duplicate addresses */

 Shouldn't have super stripes in sequential zones */

/*

 * Iterate all chunks and verify that each of them has the corresponding block

 * group

		/*

		 * lookup_extent_mapping will return the first extent map

		 * intersecting the range, so setting @len to 1 is enough to

		 * get the first chunk.

		/*

		 * When we mount with old space cache, we need to

		 * set BTRFS_DC_CLEAR and set dirty flag.

		 *

		 * a) Setting 'BTRFS_DC_CLEAR' makes sure that we

		 *    truncate the old free space cache inode and

		 *    setup a new one.

		 * b) Setting 'dirty flag' makes sure that we flush

		 *    the new space cache info onto disk.

	/*

	 * We need to exclude the super stripes now so that the space info has

	 * super bytes accounted for, otherwise we'll think we have more space

	 * than we actually do.

 We may have excluded something, so call this just in case. */

	/*

	 * For zoned filesystem, space after the allocation offset is the only

	 * free space for a block group. So, we don't need any caching work.

	 * btrfs_calc_zone_unusable() will set the amount of free space and

	 * zone_unusable space.

	 *

	 * For regular filesystem, check for two cases, either we are full, and

	 * therefore don't need to bother with the caching work since we won't

	 * find any space, or we are empty, and we can just add all the space

	 * in and be done with it.  This saves us _a_lot_ of time, particularly

	 * in the full case.

 Should not have any excluded extents. Just in case, though. */

 Fill dummy cache as FULL */

		/*

		 * We may have some valid block group cache added already, in

		 * that case we skip to the next one.

		/*

		 * Avoid allocating from un-mirrored block group if there are

		 * mirrored block groups.

	/*

	 * We've hit some error while reading the extent tree, and have

	 * rescue=ibadroots mount option.

	 * Try to fill the tree using dummy block groups so that the user can

	 * continue to mount and grab their data.

/*

 * This function, insert_block_group_item(), belongs to the phase 2 of chunk

 * allocation.

 *

 * See the comment at btrfs_chunk_alloc() for details about the chunk allocation

 * phases.

/*

 * This function belongs to phase 2.

 *

 * See the comment at btrfs_chunk_alloc() for details about the chunk allocation

 * phases.

	/*

	 * Take the device list mutex to prevent races with the final phase of

	 * a device replace operation that replaces the device object associated

	 * with the map's stripes, because the device object's id can change

	 * at any time during that final phase of the device replace operation

	 * (dev-replace.c:btrfs_dev_replace_finishing()), so we could grab the

	 * replaced device and then see it with an ID of BTRFS_DEV_REPLACE_DEVID,

	 * resulting in persisting a device extent item with such ID.

/*

 * This function, btrfs_create_pending_block_groups(), belongs to the phase 2 of

 * chunk allocation.

 *

 * See the comment at btrfs_chunk_alloc() for details about the chunk allocation

 * phases.

		/*

		 * If we restriped during balance, we may have added a new raid

		 * type, so now add the sysfs entries when it is safe to do so.

		 * We don't have to worry about locking here as it's handled in

		 * btrfs_sysfs_add_block_group_type.

 Already aborted the transaction if it failed. */

	/*

	 * New block group is likely to be used soon. Try to activate it now.

	 * Failure is OK for now.

 We may have excluded something, so call this just in case */

	/*

	 * Ensure the corresponding space_info object is created and

	 * assigned to our block group. We want our bg to be added to the rbtree

	 * with its ->space_info set.

	/*

	 * Now that our block group has its ->space_info set and is inserted in

	 * the rbtree, update the space info's counters.

/*

 * Mark one block group RO, can be called several times for the same block

 * group.

 *

 * @cache:		the destination block group

 * @do_chunk_alloc:	whether need to do chunk pre-allocation, this is to

 * 			ensure we still have some free space after marking this

 * 			block group RO.

		/*

		 * We're not allowed to set block groups readonly after the dirty

		 * block group cache has started writing.  If it already started,

		 * back off and let this transaction commit.

		/*

		 * If we are changing raid levels, try to allocate a

		 * corresponding block group with the new raid level.

			/*

			 * ENOSPC is allowed here, we may have enough space

			 * already allocated at the new raid level to carry on

 Migrate zone_unusable bytes back */

	/*

	 * If this block group is smaller than 100 megs don't bother caching the

	 * block group.

	/*

	 * We want to set the generation to 0, that way if anything goes wrong

	 * from here on out we know not to trust this cache when we load up next

	 * time.

		/*

		 * So theoretically we could recover from this, simply set the

		 * super cache generation to 0 so we know to invalidate the

		 * cache, but then we'd have to keep track of the block groups

		 * that fail this way so we know we _have_ to reset this cache

		 * before the next commit or risk reading stale cache.  So to

		 * limit our exposure to horrible edge cases lets just abort the

		 * transaction, this only happens in really bad situations

		 * anyway.

 We've already setup this transaction, go ahead and exit */

		/*

		 * don't bother trying to write stuff out _if_

		 * a) we're not cached,

		 * b) we're with nospace_cache mount option,

		 * c) we're with v2 space_cache (FREE_SPACE_TREE).

	/*

	 * We hit an ENOSPC when setting up the cache in this transaction, just

	 * skip doing the setup, we've already cleared the cache so we're safe.

	/*

	 * Try to preallocate enough space based on how big the block group is.

	 * Keep in mind this has to include any pinned space which could end up

	 * taking up quite a bit since it's not folded into the other space

	 * cache.

	/*

	 * Our cache requires contiguous chunks so that we don't modify a bunch

	 * of metadata or split extents when writing the cache out, which means

	 * we can enospc if we are heavily fragmented in addition to just normal

	 * out of space conditions.  So if we hit this just skip setting up any

	 * other block groups for this transaction, maybe we'll unpin enough

	 * space the next time around.

 Could add new block groups, use _safe just in case */

/*

 * Transaction commit does final block group cache writeback during a critical

 * section where nothing is allowed to change the FS.  This is required in

 * order for the cache to actually match the block group, but can introduce a

 * lot of latency into the commit.

 *

 * So, btrfs_start_dirty_block_groups is here to kick off block group cache IO.

 * There's a chance we'll have to redo some of it if the block group changes

 * again during the commit, but it greatly reduces the commit latency by

 * getting rid of the easy block groups while we're still allowing others to

 * join the commit.

 Make sure all the block groups on our dirty list actually exist */

	/*

	 * cache_write_mutex is here only to save us from balance or automatic

	 * removal of empty block groups deleting this block group while we are

	 * writing out the cache

		/*

		 * This can happen if something re-dirties a block group that

		 * is already under IO.  Just wait for it to finish and then do

		 * it all again

		/*

		 * btrfs_wait_cache_io uses the cache->dirty_list to decide if

		 * it should update the cache_state.  Don't delete until after

		 * we wait.

		 *

		 * Since we're not running in the commit critical section

		 * we need the dirty_bgs_lock to protect from update_block_group

				/*

				 * The cache_write_mutex is protecting the

				 * io_list, also refer to the definition of

				 * btrfs_transaction::io_bgs for more details

				/*

				 * If we failed to write the cache, the

				 * generation will be bad and life goes on

			/*

			 * Our block group might still be attached to the list

			 * of new block groups in the transaction handle of some

			 * other task (struct btrfs_trans_handle->new_bgs). This

			 * means its block group item isn't yet in the extent

			 * tree. If this happens ignore the error, as we will

			 * try again later in the critical section of the

			 * transaction commit.

 If it's not on the io list, we need to put the block group */

		/*

		 * Avoid blocking other tasks for too long. It might even save

		 * us from writing caches for block groups that are going to be

		 * removed.

	/*

	 * Go through delayed refs for all the stuff we've just kicked off

	 * and then loop back (just once)

		/*

		 * dirty_bgs_lock protects us from concurrent block group

		 * deletes too (not just cache_write_mutex).

	/*

	 * Even though we are in the critical section of the transaction commit,

	 * we can still have concurrent tasks adding elements to this

	 * transaction's list of dirty block groups. These tasks correspond to

	 * endio free space workers started when writeback finishes for a

	 * space cache, which run inode.c:btrfs_finish_ordered_io(), and can

	 * allocate new block groups as a result of COWing nodes of the root

	 * tree when updating the free space inode. The writeback for the space

	 * caches is triggered by an earlier call to

	 * btrfs_start_dirty_block_groups() and iterations of the following

	 * loop.

	 * Also we want to do the cache_save_setup first and then run the

	 * delayed refs to make sure we have the best chance at doing this all

	 * in one shot.

		/*

		 * This can happen if cache_save_setup re-dirties a block group

		 * that is already under IO.  Just wait for it to finish and

		 * then do it all again

		/*

		 * Don't remove from the dirty list until after we've waited on

		 * any pending IO

				/*

				 * If we failed to write the cache, the

				 * generation will be bad and life goes on

			/*

			 * One of the free space endio workers might have

			 * created a new block group while updating a free space

			 * cache's inode (at inode.c:btrfs_finish_ordered_io())

			 * and hasn't released its transaction handle yet, in

			 * which case the new block group is still attached to

			 * its transaction handle and its creation has not

			 * finished yet (no block group item in the extent tree

			 * yet, etc). If this is the case, wait for all free

			 * space endio workers to finish and retry. This is a

			 * very rare case so no need for a more efficient and

			 * complex approach.

 If its not on the io list, we need to put the block group */

	/*

	 * Refer to the definition of io_bgs member for details why it's safe

	 * to use it without any locking

 Block accounting for super block */

		/*

		 * If this block group has free space cache written out, we

		 * need to make sure to load it if we are removing space.  This

		 * is because we need the unpinning stage to actually add the

		 * space back to the block group, otherwise we will leak space.

		/*

		 * No longer have used bytes in this block group, queue it for

		 * deletion. We do this after adding the block group to the

		 * dirty list to avoid races between cleaner kthread and space

		 * cache writeout.

 Modified block groups are accounted for in the delayed_refs_rsv. */

/**

 * btrfs_add_reserved_bytes - update the block_group and space info counters

 * @cache:	The cache we are manipulating

 * @ram_bytes:  The number of bytes of file content, and will be same to

 *              @num_bytes except for the compress path.

 * @num_bytes:	The number of bytes in question

 * @delalloc:   The blocks are allocated for the delalloc write

 *

 * This is called by the allocator when it reserves space. If this is a

 * reservation and the block group has become read only we cannot make the

 * reservation and return -EAGAIN, otherwise this function always succeeds.

		/*

		 * Compression can use less space than we reserved, so wake

		 * tickets if that happens

/**

 * btrfs_free_reserved_bytes - update the block_group and space info counters

 * @cache:      The cache we are manipulating

 * @num_bytes:  The number of bytes in question

 * @delalloc:   The blocks are allocated for the delalloc write

 *

 * This is called by somebody who is freeing space that was never actually used

 * on disk.  For example if you reserve some space for a new leaf in transaction

 * A and before transaction A commits you free that leaf, you call this with

 * reserve set to 0 in order to clear the reservation.

	/*

	 * in limited mode, we want to have some free space up to

	 * about 1% of the FS size.

	/*

	 * Check if we have enough space in the system space info because we

	 * will need to update device items in the chunk btree and insert a new

	 * chunk item in the chunk btree as well. This will allocate a new

	 * system block group if needed.

	/*

	 * Normally we are not expected to fail with -ENOSPC here, since we have

	 * previously reserved space in the system space_info and allocated one

	 * new system chunk if necessary. However there are three exceptions:

	 *

	 * 1) We may have enough free space in the system space_info but all the

	 *    existing system block groups have a profile which can not be used

	 *    for extent allocation.

	 *

	 *    This happens when mounting in degraded mode. For example we have a

	 *    RAID1 filesystem with 2 devices, lose one device and mount the fs

	 *    using the other device in degraded mode. If we then allocate a chunk,

	 *    we may have enough free space in the existing system space_info, but

	 *    none of the block groups can be used for extent allocation since they

	 *    have a RAID1 profile, and because we are in degraded mode with a

	 *    single device, we are forced to allocate a new system chunk with a

	 *    SINGLE profile. Making check_system_chunk() iterate over all system

	 *    block groups and check if they have a usable profile and enough space

	 *    can be slow on very large filesystems, so we tolerate the -ENOSPC and

	 *    try again after forcing allocation of a new system chunk. Like this

	 *    we avoid paying the cost of that search in normal circumstances, when

	 *    we were not mounted in degraded mode;

	 *

	 * 2) We had enough free space info the system space_info, and one suitable

	 *    block group to allocate from when we called check_system_chunk()

	 *    above. However right after we called it, the only system block group

	 *    with enough free space got turned into RO mode by a running scrub,

	 *    and in this case we have to allocate a new one and retry. We only

	 *    need do this allocate and retry once, since we have a transaction

	 *    handle and scrub uses the commit root to search for block groups;

	 *

	 * 3) We had one system block group with enough free space when we called

	 *    check_system_chunk(), but after that, right before we tried to

	 *    allocate the last extent buffer we needed, a discard operation came

	 *    in and it temporarily removed the last free space entry from the

	 *    block group (discard removes a free space entry, discards it, and

	 *    then adds back the entry to the block group cache).

/*

 * Chunk allocation is done in 2 phases:

 *

 * 1) Phase 1 - through btrfs_chunk_alloc() we allocate device extents for

 *    the chunk, the chunk mapping, create its block group and add the items

 *    that belong in the chunk btree to it - more specifically, we need to

 *    update device items in the chunk btree and add a new chunk item to it.

 *

 * 2) Phase 2 - through btrfs_create_pending_block_groups(), we add the block

 *    group item to the extent btree and the device extent items to the devices

 *    btree.

 *

 * This is done to prevent deadlocks. For example when COWing a node from the

 * extent btree we are holding a write lock on the node's parent and if we

 * trigger chunk allocation and attempted to insert the new block group item

 * in the extent btree right way, we could deadlock because the path for the

 * insertion can include that parent node. At first glance it seems impossible

 * to trigger chunk allocation after starting a transaction since tasks should

 * reserve enough transaction units (metadata space), however while that is true

 * most of the time, chunk allocation may still be triggered for several reasons:

 *

 * 1) When reserving metadata, we check if there is enough free space in the

 *    metadata space_info and therefore don't trigger allocation of a new chunk.

 *    However later when the task actually tries to COW an extent buffer from

 *    the extent btree or from the device btree for example, it is forced to

 *    allocate a new block group (chunk) because the only one that had enough

 *    free space was just turned to RO mode by a running scrub for example (or

 *    device replace, block group reclaim thread, etc), so we can not use it

 *    for allocating an extent and end up being forced to allocate a new one;

 *

 * 2) Because we only check that the metadata space_info has enough free bytes,

 *    we end up not allocating a new metadata chunk in that case. However if

 *    the filesystem was mounted in degraded mode, none of the existing block

 *    groups might be suitable for extent allocation due to their incompatible

 *    profile (for e.g. mounting a 2 devices filesystem, where all block groups

 *    use a RAID1 profile, in degraded mode using a single device). In this case

 *    when the task attempts to COW some extent buffer of the extent btree for

 *    example, it will trigger allocation of a new metadata block group with a

 *    suitable profile (SINGLE profile in the example of the degraded mount of

 *    the RAID1 filesystem);

 *

 * 3) The task has reserved enough transaction units / metadata space, but when

 *    it attempts to COW an extent buffer from the extent or device btree for

 *    example, it does not find any free extent in any metadata block group,

 *    therefore forced to try to allocate a new metadata block group.

 *    This is because some other task allocated all available extents in the

 *    meanwhile - this typically happens with tasks that don't reserve space

 *    properly, either intentionally or as a bug. One example where this is

 *    done intentionally is fsync, as it does not reserve any transaction units

 *    and ends up allocating a variable number of metadata extents for log

 *    tree extent buffers;

 *

 * 4) The task has reserved enough transaction units / metadata space, but right

 *    before it tries to allocate the last extent buffer it needs, a discard

 *    operation comes in and, temporarily, removes the last free space entry from

 *    the only metadata block group that had free space (discard starts by

 *    removing a free space entry from a block group, then does the discard

 *    operation and, once it's done, it adds back the free space entry to the

 *    block group).

 *

 * We also need this 2 phases setup when adding a device to a filesystem with

 * a seed device - we must create new metadata and system chunks without adding

 * any of the block group items to the chunk, extent and device btrees. If we

 * did not do it this way, we would get ENOSPC when attempting to update those

 * btrees, since all the chunks from the seed device are read-only.

 *

 * Phase 1 does the updates and insertions to the chunk btree because if we had

 * it done in phase 2 and have a thundering herd of tasks allocating chunks in

 * parallel, we risk having too many system chunks allocated by many tasks if

 * many tasks reach phase 1 without the previous ones completing phase 2. In the

 * extreme case this leads to exhaustion of the system chunk array in the

 * superblock. This is easier to trigger if using a btree node/leaf size of 64K

 * and with RAID filesystems (so we have more device items in the chunk btree).

 * This has happened before and commit eafa4fd0ad0607 ("btrfs: fix exhaustion of

 * the system chunk array due to concurrent allocations") provides more details.

 *

 * Allocation of system chunks does not happen through this function. A task that

 * needs to update the chunk btree (the only btree that uses system chunks), must

 * preallocate chunk space by calling either check_system_chunk() or

 * btrfs_reserve_chunk_metadata() - the former is used when allocating a data or

 * metadata chunk or when removing a chunk, while the later is used before doing

 * a modification to the chunk btree - use cases for the later are adding,

 * removing and resizing a device as well as relocation of a system chunk.

 * See the comment below for more details.

 *

 * The reservation of system space, done through check_system_chunk(), as well

 * as all the updates and insertions into the chunk btree must be done while

 * holding fs_info->chunk_mutex. This is important to guarantee that while COWing

 * an extent buffer from the chunks btree we never trigger allocation of a new

 * system chunk, which would result in a deadlock (trying to lock twice an

 * extent buffer of the chunk btree, first time before triggering the chunk

 * allocation and the second time during chunk allocation while attempting to

 * update the chunks btree). The system chunk array is also updated while holding

 * that mutex. The same logic applies to removing chunks - we must reserve system

 * space, update the chunk btree and the system chunk array in the superblock

 * while holding fs_info->chunk_mutex.

 *

 * This function, btrfs_chunk_alloc(), belongs to phase 1.

 *

 * If @force is CHUNK_ALLOC_FORCE:

 *    - return 1 if it successfully allocates a chunk,

 *    - return errors including -ENOSPC otherwise.

 * If @force is NOT CHUNK_ALLOC_FORCE:

 *    - return 0 if it doesn't need to allocate a new chunk,

 *    - return 1 if it successfully allocates a chunk,

 *    - return errors including -ENOSPC otherwise.

 Don't re-enter if we're already allocating a chunk */

	/*

	 * Allocation of system chunks can not happen through this path, as we

	 * could end up in a deadlock if we are allocating a data or metadata

	 * chunk and there is another task modifying the chunk btree.

	 *

	 * This is because while we are holding the chunk mutex, we will attempt

	 * to add the new chunk item to the chunk btree or update an existing

	 * device item in the chunk btree, while the other task that is modifying

	 * the chunk btree is attempting to COW an extent buffer while holding a

	 * lock on it and on its parent - if the COW operation triggers a system

	 * chunk allocation, then we can deadlock because we are holding the

	 * chunk mutex and we may need to access that extent buffer or its parent

	 * in order to add the chunk item or update a device item.

	 *

	 * Tasks that want to modify the chunk tree should reserve system space

	 * before updating the chunk btree, by calling either

	 * btrfs_reserve_chunk_metadata() or check_system_chunk().

	 * It's possible that after a task reserves the space, it still ends up

	 * here - this happens in the cases described above at do_chunk_alloc().

	 * The task will have to either retry or fail.

 No more free physical space */

			/*

			 * Someone is already allocating, so we need to block

			 * until this someone is finished and then loop to

			 * recheck if we should continue with our allocation

			 * attempt.

 Proceed with allocation */

	/*

	 * If we have mixed data/metadata chunks we want to make sure we keep

	 * allocating mixed chunks instead of individual chunks.

	/*

	 * if we're doing a data chunk, go ahead and make sure that

	 * we keep a reasonable number of metadata chunks allocated in the

	 * FS as well.

	/*

	 * Needed because we can end up allocating a system chunk and for an

	 * atomic and race free space reservation in the chunk block reserve.

		/*

		 * Ignore failure to create system chunk. We might end up not

		 * needing it, as we might not need to COW all nodes/leafs from

		 * the paths we visit in the chunk tree (they were already COWed

		 * or created in the current transaction for example).

			/*

			 * If we fail to add the chunk item here, we end up

			 * trying again at phase 2 of chunk allocation, at

			 * btrfs_create_pending_block_groups(). So ignore

			 * any error here. An ENOSPC here could happen, due to

			 * the cases described at do_chunk_alloc() - the system

			 * block group we just created was just turned into RO

			 * mode by a scrub for example, or a running discard

			 * temporarily removed its free space entries, etc.

/*

 * Reserve space in the system space for allocating or removing a chunk.

 * The caller must be holding fs_info->chunk_mutex.

 num_devs device items to update and 1 chunk item to add or remove. */

/*

 * Reserve space in the system space, if needed, for doing a modification to the

 * chunk btree.

 *

 * @trans:		A transaction handle.

 * @is_item_insertion:	Indicate if the modification is for inserting a new item

 *			in the chunk btree or if it's for the deletion or update

 *			of an existing item.

 *

 * This is used in a context where we need to update the chunk btree outside

 * block group allocation and removal, to avoid a deadlock with a concurrent

 * task that is allocating a metadata or data block group and therefore needs to

 * update the chunk btree while holding the chunk mutex. After the update to the

 * chunk btree is done, btrfs_trans_release_chunk_metadata() should be called.

 *

/*

 * Must be called only after stopping all workers, since we could have block

 * group caching kthreads running, and therefore they could race with us if we

 * freed the block groups before stopping them.

		/*

		 * We haven't cached this block group, which means we could

		 * possibly have excluded extents on this block group.

		/*

		 * Do not hide this behind enospc_debug, this is actually

		 * important and indicates a real bug if this happens.

 logic error, can't happen */

 once for us and once for the tree */

		/*

		 * We may have left one free space entry and other possible

		 * tasks trimming this block group have left 1 entry each one.

		 * Free them if any.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2012 Alexander Block.  All rights reserved.

/*

 * Maximum number of references an extent can have in order for us to attempt to

 * issue clone operations instead of write operations. This currently exists to

 * avoid hitting limitations of the backreference walking code (taking a lot of

 * time and using too much memory for extents with large number of references).

/*

 * A fs_path is a helper to dynamically build path names with unknown size.

 * It reallocates the internal buffer on demand.

 * It allows fast adding of path elements on the right side (normal path) and

 * fast adding to the left side (reversed path). A reversed path can also be

 * unreversed if needed.

		/*

		 * Average path length does not exceed 200 bytes, we'll have

		 * better packing in the slab and higher chance to satisfy

		 * a allocation later during send.

 reused for each extent */

 'flags' member of btrfs_ioctl_send_args is u64 */

 Protocol version compatibility requested */

 current state of the compare_tree call */

	/*

	 * infos of the currently processed inode. In case of deleted inodes,

	 * these are the values from the deleted inode.

	/*

	 * We process inodes by their increasing order, so if before an

	 * incremental send we reverse the parent/child relationship of

	 * directories such that a directory with a lower inode number was

	 * the parent of a directory with a higher inode number, and the one

	 * becoming the new parent got renamed too, we can't rename/move the

	 * directory with lower inode number when we finish processing it - we

	 * must process the directory with higher inode number first, then

	 * rename/move it and then rename/move the directory with lower inode

	 * number. Example follows.

	 *

	 * Tree state when the first send was performed:

	 *

	 * .

	 * |-- a                   (ino 257)

	 *     |-- b               (ino 258)

	 *         |

	 *         |

	 *         |-- c           (ino 259)

	 *         |   |-- d       (ino 260)

	 *         |

	 *         |-- c2          (ino 261)

	 *

	 * Tree state when the second (incremental) send is performed:

	 *

	 * .

	 * |-- a                   (ino 257)

	 *     |-- b               (ino 258)

	 *         |-- c2          (ino 261)

	 *             |-- d2      (ino 260)

	 *                 |-- cc  (ino 259)

	 *

	 * The sequence of steps that lead to the second state was:

	 *

	 * mv /a/b/c/d /a/b/c2/d2

	 * mv /a/b/c /a/b/c2/d2/cc

	 *

	 * "c" has lower inode number, but we can't move it (2nd mv operation)

	 * before we move "d", which has higher inode number.

	 *

	 * So we just memorize which move/rename operations must be performed

	 * later when their respective parent is processed and moved/renamed.

 Indexed by parent directory inode number. */

	/*

	 * Reverse index, indexed by the inode number of a directory that

	 * is waiting for the move/rename of its immediate parent before its

	 * own move/rename can be performed.

	/*

	 * A directory that is going to be rm'ed might have a child directory

	 * which is in the pending directory moves index above. In this case,

	 * the directory can only be removed after the move/rename of its child

	 * is performed. Example:

	 *

	 * Parent snapshot:

	 *

	 * .                        (ino 256)

	 * |-- a/                   (ino 257)

	 *     |-- b/               (ino 258)

	 *         |-- c/           (ino 259)

	 *         |   |-- x/       (ino 260)

	 *         |

	 *         |-- y/           (ino 261)

	 *

	 * Send snapshot:

	 *

	 * .                        (ino 256)

	 * |-- a/                   (ino 257)

	 *     |-- b/               (ino 258)

	 *         |-- YY/          (ino 261)

	 *              |-- x/      (ino 260)

	 *

	 * Sequence of steps that lead to the send snapshot:

	 * rm -f /a/b/c/foo.txt

	 * mv /a/b/y /a/b/YY

	 * mv /a/b/c/x /a/b/YY

	 * rmdir /a/b/c

	 *

	 * When the child is processed, its move/rename is delayed until its

	 * parent is processed (as explained above), but all other operations

	 * like update utimes, chown, chgrp, etc, are performed and the paths

	 * that it uses for those operations must use the orphanized name of

	 * its parent (the directory we're going to rm later), so we need to

	 * memorize that name.

	 *

	 * Indexed by the inode number of the directory to be deleted.

	/*

	 * There might be some directory that could not be removed because it

	 * was waiting for this directory inode to be moved first. Therefore

	 * after this directory is moved, we can try to rmdir the ino rmdir_ino.

	/*

	 * radix_tree has only 32bit entries but we need to handle 64bit inums.

	 * We use the lower 32bit of the 64bit inum to store it in the tree. If

	 * more then one inum would fall into the same entry, we use radix_list

	 * to store the additional entries. radix_list is also used to store

	 * entries where two entries have the same inum but different

	 * generations.

	/*

	 * First time the inline_buf does not suffice

	/*

	 * The real size of the buffer is bigger, this will let the fast path

	 * happen most of the time

 TODO handle that correctly */

		/*if (ret == -ERESTARTSYS) {

			continue;

/*

 * For each command/item we want to send to userspace, we call this function.

/*

 * Sends a move instruction to user space

/*

 * Sends a link instruction to user space

/*

 * Sends an unlink instruction to user space

/*

 * Sends a rmdir instruction to user space

/*

 * Helper function to retrieve some fields from an inode item.

/*

 * Helper function to iterate the entries in ONE btrfs_inode_ref or

 * btrfs_inode_extref.

 * The iterate callback may return a non zero value to stop iteration. This can

 * be a negative value for error codes or 1 to simply stop it.

 *

 * path must point to the INODE_REF or INODE_EXTREF when called.

 overflow , try again with larger buffer */

/*

 * Helper function to iterate the entries in ONE btrfs_dir_item.

 * The iterate callback may return a non zero value to stop iteration. This can

 * be a negative value for error codes or 1 to simply stop it.

 *

 * path must point to the dir item when called.

	/*

	 * Start with a small buffer (1 page). If later we end up needing more

	 * space, which can happen for xattrs on a fs with a leaf size greater

	 * then the page size, attempt to increase the buffer. Typically xattr

	 * values are small.

			/*

			 * Path too long

 we want the first only */

/*

 * Retrieve the first path of an inode. If an inode has more then one

 * ref/hardlink, this is ignored.

 number of total found references */

	/*

	 * used for clones found in send_root. clones found behind cur_objectid

	 * and cur_offset are not considered as allowed clones.

 may be truncated in case it's the last extent in a file */

 Just to check for bugs in backref resolving */

/*

 * Called for every backref that is found for the current extent.

 * Results are collected in sctx->clone_roots->ino/offset/found_refs

 First check if the root is in the list of accepted clone sources */

	/*

	 * Make sure we don't consider clones from send_root that are

	 * behind the current inode/offset.

		/*

		 * If the source inode was not yet processed we can't issue a

		 * clone operation, as the source extent does not exist yet at

		 * the destination of the stream.

		/*

		 * We clone from the inode currently being sent as long as the

		 * source extent is already processed, otherwise we could try

		 * to clone from an extent that does not exist yet at the

		 * destination of the stream.

		/*

		 * same extent found more then once in the same file.

/*

 * Given an inode, offset and extent item, it finds a good clone for a clone

 * instruction. Returns -ENOENT when none could be found. The function makes

 * sure that the returned clone is usable at the point where sending is at the

 * moment. This means, that no clones are accepted which lie behind the current

 * inode+offset.

 *

 * path must point to the extent item when called.

 We only use this path under the commit sem */

		/*

		 * There may be extents that lie behind the file's size.

		 * I at least had this in combination with snapshotting while

		 * writing large files.

	/*

	 * Backreference walking (iterate_extent_inodes() below) is currently

	 * too expensive when an extent has a large number of references, both

	 * in time spent and used memory. So for now just fallback to write

	 * operations instead of clone operations when an extent has more than

	 * a certain amount of references.

	/*

	 * Setup the clone roots.

	/*

	 * The last extent of a file may be too large due to page alignment.

	 * We need to adjust extent_len in this case so that the checks in

	 * __iterate_backrefs work.

	/*

	 * Now collect all backrefs.

 found a bug in backref code? */

 prefer clones from send_root over others */

		/*

		 * An empty symlink inode. Can happen in rare error paths when

		 * creating a symlink (transaction committed before the inode

		 * eviction handler removed the symlink inode items and a crash

		 * happened in between or the subvol was snapshoted in between).

		 * Print an informative message to dmesg/syslog so that the user

		 * can delete the symlink.

/*

 * Helper function to generate a file name that is unique in the root of

 * send_root and parent_root. This is used to generate names for orphan inodes.

 not unique, try again */

 unique */

 not unique, try again */

 unique */

/*

 * Helper function to lookup a dir item in a dir.

/*

 * Looks up the first btrfs_inode_ref of a given ino. It returns the parent dir,

 * generation of the parent dir and the name of the dir entry.

/*

 * Used by process_recorded_refs to determine if a new ref would overwrite an

 * already existing ref. In case it detects an overwrite, it returns the

 * inode/gen in who_ino/who_gen.

 * When an overwrite is detected, process_recorded_refs does proper orphanizing

 * to make sure later references to the overwritten inode are possible.

 * Orphanizing is however only required for the first ref of an inode.

 * process_recorded_refs does an additional is_first_ref check to see if

 * orphanizing is really required.

	/*

	 * If we have a parent root we need to verify that the parent dir was

	 * not deleted and then re-created, if it was then we have no overwrite

	 * and we can just unlink this entry.

	/*

	 * Check if the overwritten ref was already processed. If yes, the ref

	 * was already unlinked/moved, so we can safely assume that we will not

	 * overwrite anything at this point in time.

/*

 * Checks if the ref was overwritten by an already processed inode. This is

 * used by __get_cur_name_and_parent to find out if the ref was orphanized and

 * thus the orphan name needs be used.

 * process_recorded_refs also uses it to avoid unlinking of refs that were

 * overwritten.

 check if the ref was overwritten by another ref */

 was never and will never be overwritten */

	/*

	 * We know that it is or will be overwritten. Check this now.

	 * The current inode being processed might have been the one that caused

	 * inode 'ino' to be orphanized, therefore check if ow_inode matches

	 * the current inode being processed.

/*

 * Same as did_overwrite_ref, but also checks if it is the first ref of an inode

 * that got overwritten. This is used by process_recorded_refs to determine

 * if it has to use the path as returned by get_cur_path or the orphan name.

/*

 * Insert a name cache entry. On 32bit kernels the radix tree index is 32bit,

 * so we need to do some special handling in case we have clashes. This function

 * takes care of this with the help of name_cache_entry::radix_list.

 * In case of error, nce is kfreed.

	/*

	 * We may not get to the final release of nce_head if the lookup fails

/*

 * Remove some entries from the beginning of name_cache_list.

/*

 * Used by get_cur_path for each ref up to the root.

 * Returns 0 if it succeeded.

 * Returns 1 if the inode is not existent or got overwritten. In that case, the

 * name is an orphan name. This instructs get_cur_path to stop iterating. If 1

 * is returned, parent_ino/parent_gen are not guaranteed to be valid.

 * Returns <0 in case of error.

	/*

	 * First check if we already did a call to this function with the same

	 * ino/gen. If yes, check if the cache entry is still up-to-date. If yes

	 * return the cached result.

			/*

			 * Removes the entry from the list and adds it back to

			 * the end.  This marks the entry as recently used so

			 * that name_cache_clean_unused does not remove it.

	/*

	 * If the inode is not existent yet, add the orphan name and return 1.

	 * This should only happen for the parent dir that we determine in

	 * __record_new_ref

	/*

	 * Depending on whether the inode was already processed or not, use

	 * send_root or parent_root for ref lookup.

	/*

	 * Check if the ref was overwritten by an inode's ref that was processed

	 * earlier. If yes, treat as orphan and return 1.

	/*

	 * Store the result of the lookup in the name cache.

/*

 * Magic happens here. This function returns the first ref to an inode as it

 * would look like while receiving the stream at this point in time.

 * We walk the path up to the root. For every inode in between, we check if it

 * was already processed/sent. If yes, we continue with the parent as found

 * in send_root. If not, we continue with the parent as found in parent_root.

 * If we encounter an inode that was deleted at this point in time, we use the

 * inodes "orphan" name instead of the real name and stop. Same with new inodes

 * that were not created yet and overwritten inodes/refs.

 *

 * When do we have orphan inodes:

 * 1. When an inode is freshly created and thus no valid refs are available yet

 * 2. When a directory lost all it's refs (deleted) but still has dir items

 *    inside which were not processed yet (pending for move/delete). If anyone

 *    tried to get the path to the dir items, it would get a path inside that

 *    orphan directory.

 * 3. When an inode is moved around or gets new links, it may overwrite the ref

 *    of an unprocessed inode. If in that case the first ref would be

 *    overwritten, the overwritten inode gets "orphanized". Later when we

 *    process this overwritten inode, it is restored at a new place by moving

 *    the orphan inode.

 *

 * sctx->send_progress tells this function at which point in time receiving

 * would be.

/*

 * Sends a BTRFS_SEND_C_SUBVOL command/item to userspace

 TODO Add otime support when the otime patches get into upstream */

/*

 * Sends a BTRFS_SEND_C_MKXXX or SYMLINK command to user space. We don't have

 * a valid path yet because we did not process the refs yet. So, the inode

 * is created as orphan.

/*

 * We need some special handling for inodes that get processed before the parent

 * directory got created. See process_recorded_refs for details.

 * This function does the check if we already created the dir out of order.

/*

 * Only creates the inode if it is:

 * 1. Not a directory

 * 2. Or a directory which was not created already due to out of order

 *    directories. See did_create_dir and process_recorded_refs for details.

/*

 * We need to process new refs before deleted refs, but compare_tree gives us

 * everything mixed. So we first record all refs and later process them.

 * This function is a helper to record one ref.

/*

 * Renames/moves a file/dir to its orphan name. Used when the first

 * ref of an unprocessed inode gets overwritten and for all non empty

 * directories.

/*

 * Returns 1 if a directory can be removed at this point in time.

 * We check this by iterating all dir items and checking if the inode behind

 * the dir item was already processed.

	/*

	 * Don't try to rmdir the top/root subvolume dir.

 already deleted */

	/*

	 * After rename/move, need to update the utimes of both new parent(s)

	 * and old parent(s).

		/*

		 * The parent inode might have been deleted in the send snapshot

/*

 * We might need to delay a directory rename even when no ancestor directory

 * (in the send root) with a higher inode number than ours (sctx->cur_ino) was

 * renamed. This happens when we rename a directory to the old name (the name

 * in the parent root) of some other unrelated directory that got its rename

 * delayed due to some ancestor with higher number that got renamed.

 *

 * Example:

 *

 * Parent snapshot:

 * .                                       (ino 256)

 * |---- a/                                (ino 257)

 * |     |---- file                        (ino 260)

 * |

 * |---- b/                                (ino 258)

 * |---- c/                                (ino 259)

 *

 * Send snapshot:

 * .                                       (ino 256)

 * |---- a/                                (ino 258)

 * |---- x/                                (ino 259)

 *       |---- y/                          (ino 257)

 *             |----- file                 (ino 260)

 *

 * Here we can not rename 258 from 'b' to 'a' without the rename of inode 257

 * from 'a' to 'x/y' happening first, which in turn depends on the rename of

 * inode 259 from 'c' to 'x'. So the order of rename commands the send stream

 * must issue is:

 *

 * 1 - rename 259 from 'c' to 'x'

 * 2 - rename 257 from 'a' to 'x/y'

 * 3 - rename 258 from 'b' to 'a'

 *

 * Returns 1 if the rename of sctx->cur_ino needs to be delayed, 0 if it can

 * be done right away and < 0 on error.

	/*

	 * di_key.objectid has the number of the inode that has a dentry in the

	 * parent directory with the same name that sctx->cur_ino is being

	 * renamed to. We need to check if that inode is in the send root as

	 * well and if it is currently marked as an inode with a pending rename,

	 * if it is, we need to delay the rename of sctx->cur_ino as well, so

	 * that it happens after that other inode is renamed.

 Different inode, no need to delay the rename of sctx->cur_ino */

/*

 * Check if inode ino2, or any of its ancestors, is inode ino1.

 * Return 1 if true, 0 if false and < 0 on error.

/*

 * Check if ino ino1 is an ancestor of inode ino2 in the given root for any

 * possible path (in case ino2 is not a directory and has multiple hard links).

 * Return 1 if true, 0 if false and < 0 on error.

	/*

	 * Our current directory inode may not yet be renamed/moved because some

	 * ancestor (immediate or not) has to be renamed/moved first. So find if

	 * such ancestor exists and make sure our own rename/move happens after

	 * that ancestor is processed to avoid path build infinite loops (done

	 * at get_cur_path()).

			/*

			 * If the current inode is an ancestor of ino in the

			 * parent root, we need to delay the rename of the

			 * current inode, otherwise don't delayed the rename

			 * because we can end up with a circular dependency

			 * of renames, resulting in some directories never

			 * getting the respective rename operations issued in

			 * the send stream or getting into infinite path build

			 * loops.

	/*

	 * Our reference's name member points to its full_path member string, so

	 * we use here a new path.

/*

 * When processing the new references for an inode we may orphanize an existing

 * directory inode because its old name conflicts with one of the new references

 * of the current inode. Later, when processing another new reference of our

 * inode, we might need to orphanize another inode, but the path we have in the

 * reference reflects the pre-orphanization name of the directory we previously

 * orphanized. For example:

 *

 * parent snapshot looks like:

 *

 * .                                     (ino 256)

 * |----- f1                             (ino 257)

 * |----- f2                             (ino 258)

 * |----- d1/                            (ino 259)

 *        |----- d2/                     (ino 260)

 *

 * send snapshot looks like:

 *

 * .                                     (ino 256)

 * |----- d1                             (ino 258)

 * |----- f2/                            (ino 259)

 *        |----- f2_link/                (ino 260)

 *        |       |----- f1              (ino 257)

 *        |

 *        |----- d2                      (ino 258)

 *

 * When processing inode 257 we compute the name for inode 259 as "d1", and we

 * cache it in the name cache. Later when we start processing inode 258, when

 * collecting all its new references we set a full path of "d1/d2" for its new

 * reference with name "d2". When we start processing the new references we

 * start by processing the new reference with name "d1", and this results in

 * orphanizing inode 259, since its old reference causes a conflict. Then we

 * move on the next new reference, with name "d2", and we find out we must

 * orphanize inode 260, as its old reference conflicts with ours - but for the

 * orphanization we use a source path corresponding to the path we stored in the

 * new reference, which is "d1/d2" and not "o259-6-0/d2" - this makes the

 * receiver fail since the path component "d1/" no longer exists, it was renamed

 * to "o259-6-0/" when processing the previous new reference. So in this case we

 * must recompute the path in the new reference and use it for the new

 * orphanization operation.

 Update the reference's base name pointer. */

/*

 * This does all the move/link/unlink/rmdir magic.

	/*

	 * This should never happen as the root dir always has the same ref

	 * which is always '..'

	/*

	 * First, check if the first ref of the current inode was overwritten

	 * before. If yes, we know that the current inode was already orphanized

	 * and thus use the orphan name. If not, we can use get_cur_path to

	 * get the path of the first ref as it would like while receiving at

	 * this point in time.

	 * New inodes are always orphan at the beginning, so force to use the

	 * orphan name in this case.

	 * The first ref is stored in valid_path and will be updated if it

	 * gets moved around.

	/*

	 * Before doing any rename and link operations, do a first pass on the

	 * new references to orphanize any unprocessed inodes that may have a

	 * reference that conflicts with one of the new references of the current

	 * inode. This needs to happen first because a new reference may conflict

	 * with the old reference of a parent directory, so we must make sure

	 * that the path used for link and rename commands don't use an

	 * orphanized name when an ancestor was not yet orphanized.

	 *

	 * Example:

	 *

	 * Parent snapshot:

	 *

	 * .                                                      (ino 256)

	 * |----- testdir/                                        (ino 259)

	 * |          |----- a                                    (ino 257)

	 * |

	 * |----- b                                               (ino 258)

	 *

	 * Send snapshot:

	 *

	 * .                                                      (ino 256)

	 * |----- testdir_2/                                      (ino 259)

	 * |          |----- a                                    (ino 260)

	 * |

	 * |----- testdir                                         (ino 257)

	 * |----- b                                               (ino 257)

	 * |----- b2                                              (ino 258)

	 *

	 * Processing the new reference for inode 257 with name "b" may happen

	 * before processing the new reference with name "testdir". If so, we

	 * must make sure that by the time we send a link command to create the

	 * hard link "b", inode 259 was already orphanized, since the generated

	 * path in "valid_path" already contains the orphanized name for 259.

	 * We are processing inode 257, so only later when processing 259 we do

	 * the rename operation to change its temporary (orphanized) name to

	 * "testdir_2".

		/*

		 * Check if this new ref would overwrite the first ref of another

		 * unprocessed inode. If yes, orphanize the overwritten inode.

		 * If we find an overwritten ref that is not the first ref,

		 * simply unlink it.

				/*

				 * If ow_inode has its rename operation delayed

				 * make sure that its orphanized name is used in

				 * the source path when performing its rename

				 * operation.

				/*

				 * Make sure we clear our orphanized inode's

				 * name from the name cache. This is because the

				 * inode ow_inode might be an ancestor of some

				 * other inode that will be orphanized as well

				 * later and has an inode number greater than

				 * sctx->send_progress. We need to prevent

				 * future name lookups from using the old name

				 * and get instead the orphan name.

				/*

				 * ow_inode might currently be an ancestor of

				 * cur_ino, therefore compute valid_path (the

				 * current path of cur_ino) again because it

				 * might contain the pre-orphanization name of

				 * ow_inode, which is no longer valid.

				/*

				 * If we previously orphanized a directory that

				 * collided with a new reference that we already

				 * processed, recompute the current path because

				 * that directory may be part of the path.

		/*

		 * We may have refs where the parent directory does not exist

		 * yet. This happens if the parent directories inum is higher

		 * than the current inum. To handle this case, we create the

		 * parent directory out of order. But we need to check if this

		 * did already happen before due to other refs in the same dir.

			/*

			 * First check if any of the current inodes refs did

			 * already create the dir.

			/*

			 * If that did not happen, check if a previous inode

			 * did already create the dir.

		/*

		 * link/move the ref to the new place. If we have an orphan

		 * inode, move it and update valid_path. If not, link or move

		 * it depending on the inode mode.

				/*

				 * Dirs can't be linked, so move it. For moved

				 * dirs, we always have one new and one deleted

				 * ref. The deleted ref is ignored later.

				/*

				 * We might have previously orphanized an inode

				 * which is an ancestor of our current inode,

				 * so our reference's full path, which was

				 * computed before any such orphanizations, must

				 * be updated.

		/*

		 * Check if we can already rmdir the directory. If not,

		 * orphanize it. For every dir item inside that gets deleted

		 * later, we do this check again and rmdir it then if possible.

		 * See the use of check_dirs for more details.

		/*

		 * We have a moved dir. Add the old parent to check_dirs

		/*

		 * We have a non dir inode. Go through all deleted refs and

		 * unlink them if they were not already overwritten by other

		 * inodes.

				/*

				 * If we orphanized any ancestor before, we need

				 * to recompute the full path for deleted names,

				 * since any such path was computed before we

				 * processed any references and orphanized any

				 * ancestor inode.

		/*

		 * If the inode is still orphan, unlink the orphan. This may

		 * happen when a previous inode did overwrite the first ref

		 * of this inode and no new refs were added for the current

		 * inode. Unlinking does not mean that the inode is deleted in

		 * all cases. There may still be links to this inode in other

		 * places.

	/*

	 * We did collect all parent dirs where cur_inode was once located. We

	 * now go through all these dirs and check if they are pending for

	 * deletion and if it's finally possible to perform the rmdir now.

	 * We also update the inode stats of the parent dirs here.

		/*

		 * In case we had refs into dirs that were not processed yet,

		 * we don't need to do the utime and rmdir logic for these dirs.

		 * The dir will be processed later.

 TODO delayed utimes */

		/*

		 * To avoid doing extra lookups we'll only do this if everything

		 * else matches.

/*

 * Record and process all refs at once. Needed when an inode changes the

 * generation number, which means that it was deleted and recreated.

	/*

	 * We don't actually care about pending_move as we are simply

	 * re-creating this inode and will be rename'ing it into place once we

	 * rename the parent directory.

 Capabilities are emitted by finish_inode_if_needed */

	/*

	 * This hack is needed because empty acls are stored as zero byte

	 * data in xattrs. Problem with that is, that receiving these zero byte

	 * acls will fail later. To fix this, we send a dummy acl list that

	 * only contains the version number and no entries.

 initial readahead */

/*

 * Read some bytes from the current inode/file and send a write command to

 * user space.

/*

 * Send a clone command to user space.

	/*

	 * If the parent we're using has a received_uuid set then use that as

	 * our clone source as that is what we will look for when doing a

	 * receive.

	 *

	 * This covers the case that we create a snapshot off of a received

	 * subvolume and then use that as the parent and try to receive on a

	 * different host.

/*

 * Send an update extent command to user space.

	/*

	 * A hole that starts at EOF or beyond it. Since we do not yet support

	 * fallocate (for extent preallocation and hole punching), sending a

	 * write of zeroes starting at EOF or beyond would later require issuing

	 * a truncate operation which would undo the write and achieve nothing.

	/*

	 * Don't go beyond the inode's i_size due to prealloc extents that start

	 * after the i_size.

/*

 * Search for a capability xattr related to sctx->cur_ino. If the capability is

 * found, call send_set_xattr function to emit it.

 *

 * Return 0 if there isn't a capability, or when the capability was emitted

 * successfully, or < 0 if an error occurred.

 There is no xattr for this inode */

	/*

	 * Prevent cloning from a zero offset with a length matching the sector

	 * size because in some scenarios this will make the receiver fail.

	 *

	 * For example, if in the source filesystem the extent at offset 0

	 * has a length of sectorsize and it was written using direct IO, then

	 * it can never be an inline extent (even if compression is enabled).

	 * Then this extent can be cloned in the original filesystem to a non

	 * zero file offset, but it may not be possible to clone in the

	 * destination filesystem because it can be inlined due to compression

	 * on the destination filesystem (as the receiver's write operations are

	 * always done using buffered IO). The same happens when the original

	 * filesystem does not have compression enabled but the destination

	 * filesystem has.

	/*

	 * There are inodes that have extents that lie behind its i_size. Don't

	 * accept clones from these extents.

	/*

	 * We can't send a clone operation for the entire range if we find

	 * extent items in the respective range in the source file that

	 * refer to different extents or if we find holes.

	 * So check for that and do a mix of clone and regular write/copy

	 * operations if needed.

	 *

	 * Example:

	 *

	 * mkfs.btrfs -f /dev/sda

	 * mount /dev/sda /mnt

	 * xfs_io -f -c "pwrite -S 0xaa 0K 100K" /mnt/foo

	 * cp --reflink=always /mnt/foo /mnt/bar

	 * xfs_io -c "pwrite -S 0xbb 50K 50K" /mnt/foo

	 * btrfs subvolume snapshot -r /mnt /mnt/snap

	 *

	 * If when we send the snapshot and we are processing file bar (which

	 * has a higher inode number than foo) we blindly send a clone operation

	 * for the [0, 100K[ range from foo to bar, the receiver ends up getting

	 * a file bar that matches the content of file foo - iow, doesn't match

	 * the content from bar in the original filesystem.

		/*

		 * We might have an implicit trailing hole (NO_HOLES feature

		 * enabled). We deal with it after leaving this loop.

 Implicit hole, NO_HOLES feature enabled. */

			/*

			 * We can't clone the last block, when its size is not

			 * sector size aligned, into the middle of a file. If we

			 * do so, the receiver will get a failure (-EINVAL) when

			 * trying to clone or will silently corrupt the data in

			 * the destination file if it's on a kernel without the

			 * fix introduced by commit ac765f83f1397646

			 * ("Btrfs: fix data corruption due to cloning of eof

			 * block).

			 *

			 * So issue a clone of the aligned down range plus a

			 * regular write for the eof block, if we hit that case.

			 *

			 * Also, we use the maximum possible sector size, 64K,

			 * because we don't know what's the sector size of the

			 * filesystem that receives the stream, so we have to

			 * assume the largest possible sector size.

		/*

		 * If we are cloning from the file we are currently processing,

		 * and using the send root as the clone root, we must stop once

		 * the current clone offset reaches the current eof of the file

		 * at the receiver, otherwise we would issue an invalid clone

		 * operation (source range going beyond eof) and cause the

		 * receiver to fail. So if we reach the current eof, bail out

		 * and fallback to a regular write.

	/*

	 * Following comments will refer to these graphics. L is the left

	 * extents which we are checking at the moment. 1-8 are the right

	 * extents that we iterate.

	 *

	 *       |-----L-----|

	 * |-1-|-2a-|-3-|-4-|-5-|-6-|

	 *

	 *       |-----L-----|

	 * |--1--|-2b-|...(same as above)

	 *

	 * Alternative situation. Happens on files where extents got split.

	 *       |-----L-----|

	 * |-----------7-----------|-6-|

	 *

	 * Alternative situation. Happens on files which got larger.

	 *       |-----L-----|

	 * |-8-|

	 * Nothing follows after 8.

	/*

	 * Handle special case where the right side has no extents at all.

 If we're a hole then just pretend nothing changed */

	/*

	 * We're now on 2a, 2b or 7.

		/*

		 * Are we at extent 8? If yes, we know the extent is changed.

		 * This may only happen on the first iteration.

 If we're a hole just pretend nothing changed */

		/*

		 * We just wanted to see if when we have an inline extent, what

		 * follows it is a regular extent (wanted to check the above

		 * condition for inline extents too). This should normally not

		 * happen but it's possible for example when we have an inline

		 * compressed extent representing data with a size matching

		 * the page size (currently the same as sector size).

 Fix the right offset for 2a and 7. */

 Fix the left offset for all behind 2a and 2b */

		/*

		 * Check if we have the same extent.

		/*

		 * Go to the next extent.

	/*

	 * We're now behind the left extent (treat as unchanged) or at the end

	 * of the right side (treat as changed).

		/*

		 * We might have skipped entire leafs that contained only

		 * file extent items for our current inode. These leafs have

		 * a generation number smaller (older) than the one in the

		 * current leaf and the leaf our last extent came from, and

		 * are located between these 2 leafs.

			/*

			 * The send spec does not have a prealloc command yet,

			 * so just leave a hole for prealloc'ed extents until

			 * we have enough commands queued up to justify rev'ing

			 * the send spec.

 Have a hole, just skip it. */

	/*

	 * We have processed the refs and thus need to advance send_progress.

	 * Now, calls to get_cur_xxx will take the updated refs of the current

	 * inode into account.

	 *

	 * On the other hand, if our current inode is a directory and couldn't

	 * be moved/renamed because its parent was renamed/moved too and it has

	 * a higher inode number, we can only move/rename our current inode

	 * after we moved/renamed its parent. Therefore in this case operate on

	 * the old path (pre move/rename) of our current inode, and the

	 * move/rename will be performed later.

	/*

	 * If other directory inodes depended on our current directory

	 * inode's move/rename, now do their move/rename operations.

		/*

		 * Need to send that every time, no matter if it actually

		 * changed between the two trees as we have done changes to

		 * the inode before. If our inode is a directory and it's

		 * waiting to be moved/renamed, we will send its utimes when

		 * it's moved/renamed, therefore we don't need to do it here.

/*

 * Issue unlink operations for all paths of the current inode found in the

 * parent snapshot.

	/*

	 * Set send_progress to current inode. This will tell all get_cur_xxx

	 * functions that the current inode's refs are not updated yet. Later,

	 * when process_recorded_refs is finished, it is set to cur_ino + 1.

		/*

		 * The cur_ino = root dir case is special here. We can't treat

		 * the inode as deleted+reused because it would generate a

		 * stream that tries to delete/mkdir the root dir.

	/*

	 * Normally we do not find inodes with a link count of zero (orphans)

	 * because the most common case is to create a snapshot and use it

	 * for a send operation. However other less common use cases involve

	 * using a subvolume and send it after turning it to RO mode just

	 * after deleting all hard links of a file while holding an open

	 * file descriptor against it or turning a RO snapshot into RW mode,

	 * keep an open file descriptor against a file, delete it and then

	 * turn the snapshot back to RO mode before using it for a send

	 * operation. So if we find such cases, ignore the inode and all its

	 * items completely if it's a new inode, or if it's a changed inode

	 * make sure all its previous paths (from the parent snapshot) are all

	 * unlinked and all other the inode items are ignored.

		/*

		 * We need to do some special handling in case the inode was

		 * reported as changed with a changed generation number. This

		 * means that the original inode was deleted and new inode

		 * reused the same inum. So we have to treat the old inode as

		 * deleted and the new one as new.

			/*

			 * First, process the inode as if it was deleted.

			/*

			 * Now process the inode as if it was new.

			/*

			 * Advance send_progress now as we did not get into

			 * process_recorded_refs_if_needed in the new_gen case.

			/*

			 * Now process all extents and xattrs of the inode as if

			 * they were all new.

/*

 * We have to process new refs before deleted refs, but compare_trees gives us

 * the new and deleted refs mixed. To fix this, we record the new/deleted refs

 * first and later process them in process_recorded_refs.

 * For the cur_inode_new_gen case, we skip recording completely because

 * changed_inode did already initiate processing of refs. The reason for this is

 * that in this case, compare_tree actually compares the refs of 2 different

 * inodes. To fix this, process_all_refs is used in changed_inode to handle all

 * refs of the right tree as deleted and all refs of the left tree as new.

/*

 * Process new/deleted/changed xattrs. We skip processing in the

 * cur_inode_new_gen case because changed_inode did already initiate processing

 * of xattrs. The reason is the same as in changed_ref

/*

 * Process new/deleted/changed extents. We skip processing in the

 * cur_inode_new_gen case because changed_inode did already initiate processing

 * of extents. The reason is the same as in changed_ref

	/*

	 * We have found an extent item that changed without the inode item

	 * having changed. This can happen either after relocation (where the

	 * disk_bytenr of an extent item is replaced at

	 * relocation.c:replace_file_extents()) or after deduplication into a

	 * file in both the parent and send snapshots (where an extent item can

	 * get modified or replaced with a new one). Note that deduplication

	 * updates the inode item, but it only changes the iversion (sequence

	 * field in the inode item) of the inode, so if a file is deduplicated

	 * the same amount of times in both the parent and send snapshots, its

	 * iversion becomes the same in both snapshots, whence the inode item is

	 * the same on both snapshots.

 Easy case, just check this one dirid */

/*

 * Updates compare related fields in sctx and simply forwards to the actual

 * changed_xxx functions.

 Ignore non-FS objects */

	/*

	 * Trigger readahead for the next leaves we will process, so that it is

	 * very likely that when we need them they are already in memory and we

	 * will not block on disk IO. For nodes we only do readahead for one,

	 * since the time window between processing nodes is typically larger.

 move upnext */

/*

 * Returns 1 if it had to move up and next. 0 is returned if it moved only next

 * or down.

/*

 * This function compares two trees and calls the provided callback for

 * every changed/new/deleted item it finds.

 * If shared tree blocks are encountered, whole subtrees are skipped, making

 * the compare pretty fast on snapshotted subvolumes.

 *

 * This currently works on commit roots only. As commit roots are read only,

 * we don't do any locking. The commit roots are protected with transactions.

 * Transactions are ended and rejoined when a commit is tried in between.

 *

 * This function checks for modifications done to the trees while comparing.

 * If it detects a change, it aborts immediately.

	/*

	 * Strategy: Go to the first items of both trees. Then do

	 *

	 * If both trees are at level 0

	 *   Compare keys of current items

	 *     If left < right treat left item as new, advance left tree

	 *       and repeat

	 *     If left > right treat right item as deleted, advance right tree

	 *       and repeat

	 *     If left == right do deep compare of items, treat as changed if

	 *       needed, advance both trees and repeat

	 * If both trees are at the same level but not at level 0

	 *   Compare keys of current nodes/leafs

	 *     If left < right advance left tree and repeat

	 *     If left > right advance right tree and repeat

	 *     If left == right compare blockptrs of the next nodes/leafs

	 *       If they match advance both trees but stay at the same level

	 *         and repeat

	 *       If they don't match advance both trees while allowing to go

	 *         deeper and repeat

	 * If tree levels are different

	 *   Advance the tree that needs it and repeat

	 *

	 * Advancing a tree means:

	 *   If we are at level 0, try to go to the next slot. If that's not

	 *   possible, go one level up and repeat. Stop when we found a level

	 *   where we could go to the next slot. We may at this point be on a

	 *   node or a leaf.

	 *

	 *   If we are not at level 0 and not on shared tree blocks, go one

	 *   level deeper.

	 *

	 *   If we are not at level 0 and on shared tree blocks, go one slot to

	 *   the right if possible or go up and right.

	/*

	 * Our right root is the parent root, while the left root is the "send"

	 * root. We know that all new nodes/leaves in the left root must have

	 * a generation greater than the right root's generation, so we trigger

	 * readahead for those nodes and leaves of the left root, as we know we

	 * will need to read them at some point.

					/*

					 * As we're on a shared block, don't

					 * allow to go deeper.

/*

 * If orphan cleanup did remove any orphans from a root, it means the tree

 * was modified and therefore the commit root is not the same as the current

 * root anymore. This is a problem, because send uses the commit root and

 * therefore can see inode items that don't exist in the current root anymore,

 * and for example make calls to btrfs_iget, which will do tree lookups based

 * on the current root and not on the commit root. Those lookups will fail,

 * returning a -ESTALE error, and making send fail with that error. So make

 * sure a send does not see any orphans we have just removed, and that it will

 * see the same inodes regardless of whether a transaction commit happened

 * before it started (meaning that the commit root will be the same as the

 * current root) or not.

 Use any root, all fs roots will get their commit roots updated. */

/*

 * Make sure any existing dellaloc is flushed for any root used by a send

 * operation so that we do not miss any data and we do not race with writeback

 * finishing and changing a tree while send is using the tree. This could

 * happen if a subvolume is in RW mode, has delalloc, is turned to RO mode and

 * a send operation then uses the subvolume.

 * After flushing delalloc ensure_commit_roots_uptodate() must be called.

	/*

	 * Not much left to do, we don't know why it's unbalanced and

	 * can't blindly reset it to 0.

	/*

	 * The subvolume must remain read-only during send, protect against

	 * making it RW. This also protects against deletion.

	/*

	 * Userspace tools do the checks and warn the user if it's

	 * not RO.

	/*

	 * Check that we don't overflow at later allocations, we request

	 * clone_sources_count + 1 items, and compare to unsigned long inside

	 * access_ok.

 Zero means "use the highest version" */

	/*

	 * Unlikely but possible, if the subvolume is marked for deletion but

	 * is slow to remove the directory entry, send can still be started

	/*

	 * Clones from send_root are allowed, but only if the clone source

	 * is behind the current send position. This is checked while searching

	 * for possible clone sources.

 We do a bsearch later */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2014 Facebook.  All rights reserved.

/*

 * Used to keep track the roots and number of refs each root has for a given

 * bytenr.  This just tracks the number of direct references, no shared

 * references.

/*

 * These are meant to represent what should exist in the extent tree, these can

 * be used to verify the extent tree is consistent as these should all match

 * what the extent tree says.

/*

 * Whenever we add/remove a reference we record the action.  The action maps

 * back to the delayed ref action.  We hold the ref we are changing in the

 * action so we can account for the history properly, and we record the root we

 * were called with since it could be different from ref_root.  We also store

 * stack traces because that's how I roll.

/*

 * One of these for every block we reference, it holds the roots and references

 * to it as well as all of the ref actions that have occurred to it.  We never

 * free it until we unmount the file system in order to make sure re-allocations

 * are happening properly.

 Walk down to the leaf from the given level */

 Walk up to the next node that needs to be processed */

/*

 * Dumps all the information from the block entry to printk, it's going to be

 * awesome.

/*

 * btrfs_ref_tree_mod: called when we modify a ref for a bytenr

 *

 * This will add an action item to the given bytenr and do sanity checks to make

 * sure we haven't messed something up.  If we are making a new allocation and

 * this block entry has history we will delete all previous actions as long as

 * our sanity checks pass as they are no longer needed.

	/*

	 * Save the extra info from the delayed ref in the ref action to make it

	 * easier to figure out what is happening.  The real ref's we add to the

	 * ref tree need to reflect what we save on disk so it matches any

	 * on-disk refs we pre-loaded.

	/*

	 * This is an allocation, preallocate the block_entry in case we haven't

	 * used it before.

		/*

		 * For subvol_create we'll just pass in whatever the parent root

		 * is and the new root objectid, so let's not treat the passed

		 * in root as if it really has a ref for this bytenr.

			/*

			 * This is the root that is modifying us, so it's the

			 * one we want to lookup below when we modify the

			 * re->num_refs.

			/*

			 * This shouldn't happen because we will add our re

			 * above when we lookup the be with !parent, but just in

			 * case catch this case so we don't panic because I

			 * didn't think of some other corner case.

 Free up the ref cache */

 We want to get as close to start as possible */

	/*

	 * Could have an empty block group, maybe have something to check for

	 * this case to verify we were actually empty?

 Walk down all roots and build the ref tree, meant to be called at mount */

		/*

		 * We have to keep track of the bytenr/num_bytes we last hit

		 * because we could have run out of space for an inline ref, and

		 * would have had to added a ref key item which may appear on a

		 * different leaf from the original extent item.

 SPDX-License-Identifier: GPL-2.0

/*

 * Subpage (sectorsize < PAGE_SIZE) support overview:

 *

 * Limitations:

 *

 * - Only support 64K page size for now

 *   This is to make metadata handling easier, as 64K page would ensure

 *   all nodesize would fit inside one page, thus we don't need to handle

 *   cases where a tree block crosses several pages.

 *

 * - Only metadata read-write for now

 *   The data read-write part is in development.

 *

 * - Metadata can't cross 64K page boundary

 *   btrfs-progs and kernel have done that for a while, thus only ancient

 *   filesystems could have such problem.  For such case, do a graceful

 *   rejection.

 *

 * Special behavior:

 *

 * - Metadata

 *   Metadata read is fully supported.

 *   Meaning when reading one tree block will only trigger the read for the

 *   needed range, other unrelated range in the same page will not be touched.

 *

 *   Metadata write support is partial.

 *   The writeback is still for the full page, but we will only submit

 *   the dirty extent buffers in the page.

 *

 *   This means, if we have a metadata page like this:

 *

 *   Page offset

 *   0         16K         32K         48K        64K

 *   |/////////|           |///////////|

 *        \- Tree block A        \- Tree block B

 *

 *   Even if we just want to writeback tree block A, we will also writeback

 *   tree block B if it's also dirty.

 *

 *   This may cause extra metadata writeback which results more COW.

 *

 * Implementation:

 *

 * - Common

 *   Both metadata and data will use a new structure, btrfs_subpage, to

 *   record the status of each sector inside a page.  This provides the extra

 *   granularity needed.

 *

 * - Metadata

 *   Since we have multiple tree blocks inside one page, we can't rely on page

 *   locking anymore, or we will have greatly reduced concurrency or even

 *   deadlocks (hold one tree lock while trying to lock another tree lock in

 *   the same page).

 *

 *   Thus for metadata locking, subpage support relies on io_tree locking only.

 *   This means a slightly higher tree locking latency.

	/*

	 * We have cases like a dummy extent buffer page, which is not mappped

	 * and doesn't need to be locked.

 Either not subpage, or the page already has private attached */

 Either not subpage, or already detached */

/*

 * Increase the eb_refs of current subpage.

 *

 * This is important for eb allocation, to prevent race with last eb freeing

 * of the same page.

 * With the eb_refs increased before the eb inserted into radix tree,

 * detach_extent_buffer_page() won't detach the page private while we're still

 * allocating the extent buffer.

 Basic checks */

	/*

	 * The range check only works for mapped page, we can still have

	 * unmapped page like dummy extent buffer pages.

	/*

	 * For data we need to unlock the page if the last read has finished.

	 *

	 * And please don't replace @last with atomic_sub_and_test() call

	 * inside if () condition.

	 * As we want the atomic_sub_and_test() to be always executed.

	/*

	 * For certain call sites like btrfs_drop_pages(), we may have pages

	 * beyond the target range. In that case, just set @len to 0, subpage

	 * helpers can handle @len == 0 without any problem.

	/*

	 * We have call sites passing @lock_page into

	 * extent_clear_unlock_delalloc() for compression path.

	 *

	 * This @locked_page is locked by plain lock_page(), thus its

	 * subpage::writers is 0.  Handle them in a special way.

/*

 * Lock a page for delalloc page writeback.

 *

 * Return -EAGAIN if the page is not properly initialized.

 * Return 0 with the page locked, and writer counter updated.

 *

 * Even with 0 returned, the page still need extra check to make sure

 * it's really the correct page, as the caller is using

 * find_get_pages_contig(), which can race with page invalidating.

/*

 * Extra clear_and_test function for subpage dirty bitmap.

 *

 * Return true if we're the last bits in the dirty_bitmap and clear the

 * dirty_bitmap.

 * Return false otherwise.

 *

 * NOTE: Callers should manually clear page dirty for true case, as we have

 * extra handling for tree blocks.

/*

 * Unlike set/clear which is dependent on each page status, for test all bits

 * are tested in the same way.

/*

 * Note that, in selftests (extent-io-tests), we can have empty fs_info passed

 * in.  We only test sectorsize == PAGE_SIZE cases so far, thus we can fall

 * back to regular sectorsize branch.

/*

 * Make sure not only the page dirty bit is cleared, but also subpage dirty bit

 * is cleared.

/*

 * Handle different locked pages with different page sizes:

 *

 * - Page locked by plain lock_page()

 *   It should not have any subpage::writers count.

 *   Can be unlocked by unlock_page().

 *   This is the most common locked page for __extent_writepage() called

 *   inside extent_write_cache_pages() or extent_write_full_page().

 *   Rarer cases include the @locked_page from extent_write_locked_range().

 *

 * - Page locked by lock_delalloc_pages()

 *   There is only one caller, all pages except @locked_page for

 *   extent_write_locked_range().

 *   In this case, we have to call subpage helper to handle the case.

 For regular page size case, we just unlock the page */

	/*

	 * For subpage case, there are two types of locked page.  With or

	 * without writers number.

	 *

	 * Since we own the page lock, no one else could touch subpage::writers

	 * and we are safe to do several atomic operations without spinlock.

 No writers, locked by plain lock_page() */

 Have writers, use proper subpage helper to end it */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2011 STRATO.  All rights reserved.

 Just an arbitrary number so we can be sure this happened */

	/*

	 * from the shared data ref, we only have the leaf but we need

	 * the key. thus, we must look into all items and see that we

	 * find one (some) with a reference to our extent item.

 don't skip BTRFS_FILE_EXTENT_PREALLOC, we can handle that */

 BTRFS_SHARED_[DATA|BLOCK]_REF_KEY */

 BTRFS_[TREE_BLOCK|EXTENT_DATA]_REF_KEY */

/*

 * Checks for a shared extent during backref search.

 *

 * The share_count tracks prelim_refs (direct and indirect) having a

 * ref->count >0:

 *  - incremented when a ref->count transitions to >0

 *  - decremented when a ref->count transitions to <1

/*

 * Return 0 when both refs are for the same block (and can be merged).

 * A -1 return indicates ref1 is a 'lower' block than ref2, while 1

 * indicates a 'higher' block.

/*

 * Add @newref to the @root rbtree, merging identical refs.

 *

 * Callers should assume that newref has been freed after calling.

 Identical refs, merge them and free @newref */

			/*

			 * A delayed ref can have newref->count < 0.

			 * The ref->count is updated to follow any

			 * BTRFS_[ADD|DROP]_DELAYED_REF actions.

/*

 * Release the entire tree.  We don't care about internal consistency so

 * just free everything and then reset the tree root.

/*

 * the rules for all callers of this function are:

 * - obtaining the parent is the goal

 * - if you add a key, you must know that it is a correct key

 * - if you cannot add the parent or a correct key, then we will look into the

 *   block later to set a correct key

 *

 * delayed refs

 * ============

 *        backref type | shared | indirect | shared | indirect

 * information         |   tree |     tree |   data |     data

 * --------------------+--------+----------+--------+----------

 *      parent logical |    y   |     -    |    -   |     -

 *      key to resolve |    -   |     y    |    y   |     y

 *  tree block logical |    -   |     -    |    -   |     -

 *  root for resolving |    y   |     y    |    y   |     y

 *

 * - column 1:       we've the parent -> done

 * - column 2, 3, 4: we use the key to find the parent

 *

 * on disk refs (inline or keyed)

 * ==============================

 *        backref type | shared | indirect | shared | indirect

 * information         |   tree |     tree |   data |     data

 * --------------------+--------+----------+--------+----------

 *      parent logical |    y   |     -    |    y   |     -

 *      key to resolve |    -   |     -    |    -   |     y

 *  tree block logical |    y   |     y    |    y   |     y

 *  root for resolving |    -   |     y    |    y   |     y

 *

 * - column 1, 3: we've the parent -> done

 * - column 2:    we take the first key from the block to find the parent

 *                (see add_missing_keys)

 * - column 4:    we use the key to find the parent

 *

 * additional information that's available but not required to find the parent

 * block might help in merging entries to gain some speed.

 direct refs use root == 0, key == NULL */

 indirect refs use parent == 0 */

	/*

	 * 1. We normally enter this function with the path already pointing to

	 *    the first item to check. But sometimes, we may enter it with

	 *    slot == nritems.

	 * 2. We are searching for normal backref but bytenr of this leaf

	 *    matches shared data backref

	 * 3. The leaf owner is not equal to the root we are searching

	 *

	 * For these cases, go to the next leaf before we continue.

		/*

		 * We are searching for normal backref but bytenr of this leaf

		 * matches shared data backref, OR

		 * the leaf owner is not equal to the root we are searching for

/*

 * resolve an indirect backref in the form (root_id, key, level)

 * to a logical address

	/*

	 * If we're search_commit_root we could possibly be holding locks on

	 * other tree nodes.  This happens when qgroups does backref walks when

	 * adding new delayed refs.  To deal with this we need to look in cache

	 * for the root, and if we don't find it then we need to search the

	 * tree_root's commit root, thus the btrfs_get_fs_root_commit_root usage

	 * here.

	/*

	 * We can often find data backrefs with an offset that is too large

	 * (>= LLONG_MAX, maximum allowed file offset) due to underflows when

	 * subtracting a file's offset with the data offset of its

	 * corresponding extent data item. This can happen for example in the

	 * clone ioctl.

	 *

	 * So if we detect such case we set the search key's offset to zero to

	 * make sure we will find the matching file extent item at

	 * add_all_parents(), otherwise we will miss it because the offset

	 * taken form the backref is much larger then the offset of the file

	 * extent item. This can make us scan a very large number of file

	 * extent items, but at least it will not make us miss any.

	 *

	 * This is an ugly workaround for a behaviour that should have never

	 * existed, but it does and a fix for the clone ioctl would touch a lot

	 * of places, cause backwards incompatibility and would not fix the

	 * problem for extents cloned with older kernels.

/*

 * We maintain three separate rbtrees: one for direct refs, one for

 * indirect refs which have a key, and one for indirect refs which do not

 * have a key. Each tree does merge on insertion.

 *

 * Once all of the references are located, we iterate over the tree of

 * indirect refs with missing keys. An appropriate key is located and

 * the ref is moved onto the tree for indirect refs. After all missing

 * keys are thus located, we iterate over the indirect ref tree, resolve

 * each reference, and then insert the resolved reference onto the

 * direct tree (merging there too).

 *

 * New backrefs (i.e., for parent nodes) are added to the appropriate

 * rbtree as they are encountered. The new backrefs are subsequently

 * resolved as above.

	/*

	 * We could trade memory usage for performance here by iterating

	 * the tree, allocating new refs for each insertion, and then

	 * freeing the entire indirect tree when we're done.  In some test

	 * cases, the tree can grow quite large (~200k objects).

		/*

		 * we can only tolerate ENOENT,otherwise,we should catch error

		 * and return directly.

 we put the first parent into the ref at hand */

 Add a prelim_ref(s) for any other parent(s). */

		/*

		 * Now it's a direct ref, put it in the direct tree. We must

		 * do this last because the ref could be merged/freed here.

/*

 * read tree blocks and add keys where required.

 should not be a direct ref */

/*

 * add all currently queued delayed refs from this head whose seq nr is

 * smaller or equal that seq to the list

 NORMAL INDIRECT METADATA backref */

 SHARED DIRECT METADATA backref */

 NORMAL INDIRECT DATA backref */

			/*

			 * Found a inum that doesn't match our known inum, we

			 * know it's shared.

 SHARED DIRECT FULL backref */

		/*

		 * We must ignore BACKREF_FOUND_SHARED until all delayed

		 * refs have been checked.

/*

 * add all inline backrefs for bytenr to the list

 *

 * Returns 0 on success, <0 on error, or BACKREF_FOUND_SHARED.

	/*

	 * enumerate all inline refs

/*

 * add all non-inline backrefs for bytenr to the list

 *

 * Returns 0 on success, <0 on error, or BACKREF_FOUND_SHARED.

 SHARED DIRECT METADATA backref */

 SHARED DIRECT FULL backref */

 NORMAL INDIRECT METADATA backref */

 NORMAL INDIRECT DATA backref */

/*

 * this adds all existing backrefs (inline backrefs, backrefs and delayed

 * refs) for the given bytenr to the refs list, merges duplicates and resolves

 * indirect refs to their parent bytenr.

 * When roots are found, they're added to the roots list

 *

 * If time_seq is set to BTRFS_SEQ_LAST, it will not search delayed_refs, and

 * behave much like trans == NULL case, the difference only lies in it will not

 * commit root.

 * The special case is for qgroup to search roots in commit_transaction().

 *

 * @sc - if !NULL, then immediately return BACKREF_FOUND_SHARED when a

 * shared extent is detected.

 *

 * Otherwise this returns 0 for success and <0 for an error.

 *

 * If ignore_offset is set to false, only extent refs whose offsets match

 * extent_item_pos are returned.  If true, every extent ref is returned

 * and extent_item_pos is ignored.

 *

 * FIXME some caching might speed things up

	/*

	 * grab both a lock on the path and a lock on the delayed ref head.

	 * We need both to get a consistent picture of how the refs look

	 * at a specified point in time

		/*

		 * look if there are updates for this ref queued and lock the

		 * head

				/*

				 * Mutex was contended, block until it's

				 * released and try again

	/*

	 * This walks the tree of merged and resolved refs. Tree blocks are

	 * read in as needed. Unique entries are added to the ulist, and

	 * the list of found roots is updated.

	 *

	 * We release the entire tree in one go before returning.

		/*

		 * ref->count < 0 can happen here if there are delayed

		 * refs with a node->action of BTRFS_DROP_DELAYED_REF.

		 * prelim_ref_insert() relies on this when merging

		 * identical refs to keep the overall count correct.

		 * prelim_ref_insert() will merge only those refs

		 * which compare identically.  Any refs having

		 * e.g. different offsets would not be merged,

		 * and would retain their original ref->count < 0.

 no parent == root of tree */

				/*

				 * we've recorded that parent, so we must extend

				 * its inode list here

/*

 * Finds all leafs with a reference to the specified combination of bytenr and

 * offset. key_list_head will point to a list of corresponding keys (caller must

 * free each list element). The leafs will be stored in the leafs ulist, which

 * must be freed with ulist_free.

 *

 * returns 0 on success, <0 on error

/*

 * walk all backrefs for a given extent to find all roots that reference this

 * extent. Walking a backref means finding all extents that reference this

 * extent and in turn walk the backrefs of those, too. Naturally this is a

 * recursive process, but here it is implemented in an iterative fashion: We

 * find all referencing extents for the extent in question and put them on a

 * list. In turn, we find all referencing extents for those, further appending

 * to the list. The way we iterate the list allows adding more elements after

 * the current while iterating. The process stops when we reach the end of the

 * list. Found roots are added to the roots list.

 *

 * returns 0 on success, < 0 on error.

/**

 * Check if an extent is shared or not

 *

 * @root:   root inode belongs to

 * @inum:   inode number of the inode whose extent we are checking

 * @bytenr: logical bytenr of the extent we are checking

 * @roots:  list of roots this extent is shared among

 * @tmp:    temporary list used for iteration

 *

 * btrfs_check_shared uses the backref walking code but will short

 * circuit as soon as it finds a root or inode that doesn't match the

 * one passed in. This provides a significant performance benefit for

 * callers (such as fiemap) which want to know whether the extent is

 * shared but do not need a ref count.

 *

 * This attempts to attach to the running transaction in order to account for

 * delayed refs, but continues on even when no running transaction exists.

 *

 * Return: 0 if extent is not shared, 1 if it is shared, < 0 on error.

 this is the only condition under which we return 1 */

			/*

			 * If the item at offset is not found,

			 * btrfs_search_slot will point us to the slot

			 * where it should be inserted. In our case

			 * that will be the slot directly before the

			 * next INODE_REF_KEY_V2 item. In the case

			 * that we're pointing to the last slot in a

			 * leaf, we must move one leaf over.

		/*

		 * Check that we're still looking at an extended ref key for

		 * this particular objectid. If we have different

		 * objectid or type then there are no more to be found

		 * in the tree and we can exit.

/*

 * this iterates to turn a name (from iref/extref) into a full filesystem path.

 * Elements of the path are separated by '/' and the path is guaranteed to be

 * 0-terminated. the path is only given within the current file system.

 * Therefore, it never starts with a '/'. the caller is responsible to provide

 * "size" bytes in "dest". the dest buffer will be filled backwards. finally,

 * the start point of the resulting string is returned. this pointer is within

 * dest, normally.

 * in case the path buffer would overflow, the pointer is decremented further

 * as if output was written to the buffer, though no more output is actually

 * generated. that way, the caller can determine how much space would be

 * required for the path to fit into the buffer. in that case, the returned

 * value will be smaller than dest. callers must check this!

 regular exit ahead */

 make sure we can use eb after releasing the path */

/*

 * this makes the path point to (logical EXTENT_ITEM *)

 * returns BTRFS_EXTENT_FLAG_DATA for data, BTRFS_EXTENT_FLAG_TREE_BLOCK for

 * tree blocks and <0 on error.

/*

 * helper function to iterate extent inline refs. ptr must point to a 0 value

 * for the first call and may be modified. it is used to track state.

 * if more refs exist, 0 is returned and the next call to

 * get_extent_inline_ref must pass the modified ptr parameter to get the

 * next ref. after the last ref was processed, 1 is returned.

 * returns <0 on error

 first call */

 a skinny metadata extent */

 last */

/*

 * reads the tree block backref for an extent. tree level and root are returned

 * through out_level and out_root. ptr must point to a 0 value for the first

 * call and may be modified (see get_extent_inline_ref comment).

 * returns 0 if data was provided, 1 if there was no more data to provide or

 * <0 on error.

 we can treat both ref types equally here */

/*

 * calls iterate() for every inode that references the extent identified by

 * the given parameters.

 * when the iterator function returns a non-zero value, iteration stops.

 path must be released before calling iterate()! */

/*

 * returns 0 if the path could be dumped (probably truncated)

 * returns <0 in case of an error

/*

 * this dumps all file system paths to the inode into the ipath struct, provided

 * is has been created large enough. each path is zero-terminated and accessed

 * from ipath->fspath->val[i].

 * when it returns, there are ipath->fspath->elem_cnt number of paths available

 * in ipath->fspath->val[]. when the allocated space wasn't sufficient, the

 * number of missed paths is recorded in ipath->fspath->elem_missed, otherwise,

 * it's zero. ipath->fspath->bytes_missing holds the number of bytes that would

 * have been needed to return all paths.

/*

 * allocates space to return multiple file system paths for an inode.

 * total_bytes to allocate are passed, note that space usable for actual path

 * information will be total_bytes - sizeof(struct inode_fs_paths).

 * the returned pointer must be freed with free_ipath() in the end.

 Current backref iterator only supports iteration in commit root */

	/*

	 * Only support iteration on tree backref yet.

	 *

	 * This is an extra precaution for non skinny-metadata, where

	 * EXTENT_ITEM is also used for tree blocks, that we can only use

	 * extent flags to determine if it's a tree block.

 If there is no inline backref, go search for keyed backref */

 No inline nor keyed ref */

/*

 * Go to the next backref item of current bytenr, can be either inlined or

 * keyed.

 *

 * Caller needs to check whether it's inline ref or not by iter->cur_key.

 *

 * Return 0 if we get next backref without problem.

 * Return >0 if there is no extra backref for this bytenr.

 * Return <0 if there is something wrong happened.

 We're still inside the inline refs */

 First tree block info */

 Use inline ref type to determine the size */

 All inline items iterated, fall through */

 We're at keyed items, there is no inline item, go to the next one */

/*

 * Drop the backref node from cache, also cleaning up all its

 * upper edges and any uncached nodes in the path.

 *

 * This cleanup happens bottom up, thus the node should either

 * be the lowest node in the cache or a detached node.

		/*

		 * Add the node to leaf node list if no other child block

		 * cached.

/*

 * Release all nodes/edges from current cache

/*

 * Handle direct tree backref

 *

 * Direct tree backref means, the backref item shows its parent bytenr

 * directly. This is for SHARED_BLOCK_REF backref (keyed or inlined).

 *

 * @ref_key:	The converted backref key.

 *		For keyed backref, it's the item key.

 *		For inlined backref, objectid is the bytenr,

 *		type is btrfs_inline_ref_type, offset is

 *		btrfs_inline_ref_offset.

 Only reloc root uses backref pointing to itself */

 Only reloc backref cache cares about a specific root */

			/*

			 * For generic purpose backref cache, reloc root node

			 * is useless.

 Parent node not yet cached */

		/*

		 *  Backrefs for the upper level block isn't cached, add the

		 *  block to pending list

 Parent node already cached */

/*

 * Handle indirect tree backref

 *

 * Indirect tree backref means, we only know which tree the node belongs to.

 * We still need to do a tree search to find out the parents. This is for

 * TREE_BLOCK_REF backref (keyed or inlined).

 *

 * @ref_key:	The same as @ref_key in  handle_direct_tree_backref()

 * @tree_key:	The first key of this tree block.

 * @path:	A clean (released) path, to avoid allocating path every time

 *		the function get called.

 Tree root */

		/*

		 * For reloc backref cache, we may ignore reloc root.  But for

		 * general purpose backref cache, we can't rely on

		 * btrfs_should_ignore_reloc_root() as it may conflict with

		 * current running relocation and lead to missing root.

		 *

		 * For general purpose backref cache, reloc root detection is

		 * completely relying on direct backref (key->offset is parent

		 * bytenr), thus only do such check for reloc cache.

 Search the tree to find parent blocks referring to the block */

 Add all nodes and edges in the path */

 Same as previous should_ignore_reloc_root() call */

			/*

			 * If we know the block isn't shared we can avoid

			 * checking its backrefs.

			/*

			 * Add the block to pending list if we need to check its

			 * backrefs, we only do this once while walking up a

			 * tree as we will catch anything else later on.

/*

 * Add backref node @cur into @cache.

 *

 * NOTE: Even if the function returned 0, @cur is not yet cached as its upper

 *	 links aren't yet bi-directional. Needs to finish such links.

 *	 Use btrfs_backref_finish_upper_links() to finish such linkage.

 *

 * @path:	Released path for indirect tree backref lookup

 * @iter:	Released backref iter for extent tree search

 * @node_key:	The first key of the tree block

	/*

	 * We skip the first btrfs_tree_block_info, as we don't use the key

	 * stored in it, but fetch it from the tree block

 No extra backref? This means the tree block is corrupted */

		/*

		 * The backref was added previously when processing backref of

		 * type BTRFS_TREE_BLOCK_REF_KEY

		/*

		 * Add the upper level block to pending list if we need check

		 * its backrefs

 Update key for inline backref */

		/*

		 * Parent node found and matches current inline ref, no need to

		 * rebuild this node for this inline ref

 SHARED_BLOCK_REF means key.offset is the parent bytenr */

		/*

		 * key.type == BTRFS_TREE_BLOCK_REF_KEY, inline ref offset

		 * means the root objectid. We need to search the tree to get

		 * its parent bytenr.

/*

 * Finish the upwards linkage created by btrfs_backref_add_tree_node()

 Insert this node to cache if it's not COW-only */

	/*

	 * Use breadth first search to iterate all related edges.

	 *

	 * The starting points are all the edges of this node

 Parent is detached, no need to keep any edges */

 Lower node is orphan, queue for cleanup */

		/*

		 * All new nodes added in current build_backref_tree() haven't

		 * been linked to the cache rb tree.

		 * So if we have upper->rb_node populated, this means a cache

		 * hit. We only need to link the edge, as @upper and all its

		 * parents have already been linked.

 Sanity check, we shouldn't have any unchecked nodes */

 Sanity check, COW-only node has non-COW-only parent */

 Only cache non-COW-only (subvolume trees) tree blocks */

		/*

		 * Also queue all the parent edges of this uncached node

		 * to finish the upper linkage

		/*

		 * Lower is no longer linked to any upper backref nodes and

		 * isn't in the cache, we can free it ourselves.

 Add this guy's upper edges to the list to process */

 SPDX-License-Identifier: GPL-2.0

/*

 * HOW DO BLOCK RESERVES WORK

 *

 *   Think of block_rsv's as buckets for logically grouped metadata

 *   reservations.  Each block_rsv has a ->size and a ->reserved.  ->size is

 *   how large we want our block rsv to be, ->reserved is how much space is

 *   currently reserved for this block reserve.

 *

 *   ->failfast exists for the truncate case, and is described below.

 *

 * NORMAL OPERATION

 *

 *   -> Reserve

 *     Entrance: btrfs_block_rsv_add, btrfs_block_rsv_refill

 *

 *     We call into btrfs_reserve_metadata_bytes() with our bytes, which is

 *     accounted for in space_info->bytes_may_use, and then add the bytes to

 *     ->reserved, and ->size in the case of btrfs_block_rsv_add.

 *

 *     ->size is an over-estimation of how much we may use for a particular

 *     operation.

 *

 *   -> Use

 *     Entrance: btrfs_use_block_rsv

 *

 *     When we do a btrfs_alloc_tree_block() we call into btrfs_use_block_rsv()

 *     to determine the appropriate block_rsv to use, and then verify that

 *     ->reserved has enough space for our tree block allocation.  Once

 *     successful we subtract fs_info->nodesize from ->reserved.

 *

 *   -> Finish

 *     Entrance: btrfs_block_rsv_release

 *

 *     We are finished with our operation, subtract our individual reservation

 *     from ->size, and then subtract ->size from ->reserved and free up the

 *     excess if there is any.

 *

 *     There is some logic here to refill the delayed refs rsv or the global rsv

 *     as needed, otherwise the excess is subtracted from

 *     space_info->bytes_may_use.

 *

 * TYPES OF BLOCK RESERVES

 *

 * BLOCK_RSV_TRANS, BLOCK_RSV_DELOPS, BLOCK_RSV_CHUNK

 *   These behave normally, as described above, just within the confines of the

 *   lifetime of their particular operation (transaction for the whole trans

 *   handle lifetime, for example).

 *

 * BLOCK_RSV_GLOBAL

 *   It is impossible to properly account for all the space that may be required

 *   to make our extent tree updates.  This block reserve acts as an overflow

 *   buffer in case our delayed refs reserve does not reserve enough space to

 *   update the extent tree.

 *

 *   We can steal from this in some cases as well, notably on evict() or

 *   truncate() in order to help users recover from ENOSPC conditions.

 *

 * BLOCK_RSV_DELALLOC

 *   The individual item sizes are determined by the per-inode size

 *   calculations, which are described with the delalloc code.  This is pretty

 *   straightforward, it's just the calculation of ->size encodes a lot of

 *   different items, and thus it gets used when updating inodes, inserting file

 *   extents, and inserting checksums.

 *

 * BLOCK_RSV_DELREFS

 *   We keep a running tally of how many delayed refs we have on the system.

 *   We assume each one of these delayed refs are going to use a full

 *   reservation.  We use the transaction items and pre-reserve space for every

 *   operation, and use this reservation to refill any gap between ->size and

 *   ->reserved that may exist.

 *

 *   From there it's straightforward, removing a delayed ref means we remove its

 *   count from ->size and free up reservations as necessary.  Since this is

 *   the most dynamic block reserve in the system, we will try to refill this

 *   block reserve first with any excess returned by any other block reserve.

 *

 * BLOCK_RSV_EMPTY

 *   This is the fallback block reserve to make us try to reserve space if we

 *   don't have a specific bucket for this allocation.  It is mostly used for

 *   updating the device tree and such, since that is a separate pool we're

 *   content to just reserve space from the space_info on demand.

 *

 * BLOCK_RSV_TEMP

 *   This is used by things like truncate and iput.  We will temporarily

 *   allocate a block reserve, set it to some size, and then truncate bytes

 *   until we have no space left.  With ->failfast set we'll simply return

 *   ENOSPC from btrfs_use_block_rsv() to signal that we need to unwind and try

 *   to make a new reservation.  This is because these operations are

 *   unbounded, so we want to do as much work as we can, and then back off and

 *   re-reserve.

	/*

	 * If we are the delayed_rsv then push to the global rsv, otherwise dump

	 * into the delayed rsv if it is not full.

	/*

	 * The global block rsv is based on the size of the extent tree, the

	 * checksum tree and the root tree.  If the fs is empty we want to set

	 * it to a minimal amount for safety.

	/*

	 * We at a minimum are going to modify the csum root, the tree root, and

	 * the extent root.

	/*

	 * But we also want to reserve enough space so we can do the fallback

	 * global reserve for an unlink, which is an additional 5 items (see the

	 * comment in __unlink_start_trans for what we're modifying.)

	 *

	 * But we also need space for the delayed ref updates from the unlink,

	 * so its 10, 5 for the actual operation, and 5 for the delayed ref

	 * updates.

	/*

	 * Our various recovery options can leave us with NULL roots, so check

	 * here and just bail before we go dereferencing NULLs everywhere.

	/*

	 * The global reserve still exists to save us from ourselves, so don't

	 * warn_on if we are short on our delayed refs reserve.

DEFAULT_RATELIMIT_BURST*/ 1);

	/*

	 * If we couldn't reserve metadata bytes try and use some from

	 * the global reserve if its space type is the same as the global

	 * reservation.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/*

 * insert a name into a directory, doing overflow properly if there is a hash

 * collision.  data_size indicates how big the item inserted should be.  On

 * success a struct btrfs_dir_item pointer is returned, otherwise it is

 * an ERR_PTR.

 *

 * The name is not copied into the dir item, you have to do that yourself.

/*

 * xattrs work a lot like directories, this inserts an xattr item

 * into the tree

/*

 * insert a directory item in the tree, doing all the magic for

 * both indexes. 'dir' indicates which objectid to insert it into,

 * 'location' is the key to stuff into the directory item, 'type' is the

 * type of the inode we're pointing to, and 'index' is the sequence number

 * to use for the second index (if one is created).

 * Will return 0 or -ENOMEM

 FIXME, use some real flag for selecting the extra index */

/*

 * Lookup for a directory item by name.

 *

 * @trans:	The transaction handle to use. Can be NULL if @mod is 0.

 * @root:	The root of the target tree.

 * @path:	Path to use for the search.

 * @dir:	The inode number (objectid) of the directory.

 * @name:	The name associated to the directory entry we are looking for.

 * @name_len:	The length of the name.

 * @mod:	Used to indicate if the tree search is meant for a read only

 *		lookup, for a modification lookup or for a deletion lookup, so

 *		its value should be 0, 1 or -1, respectively.

 *

 * Returns: NULL if the dir item does not exists, an error pointer if an error

 * happened, or a pointer to a dir item if a dir item exists for the given name.

 Nothing found, we're safe */

 we found an item, look for our name in the item */

 our exact name was found */

	/*

	 * see if there is room in the item to insert this

	 * name

 plenty of insertion room */

/*

 * Lookup for a directory index item by name and index number.

 *

 * @trans:	The transaction handle to use. Can be NULL if @mod is 0.

 * @root:	The root of the target tree.

 * @path:	Path to use for the search.

 * @dir:	The inode number (objectid) of the directory.

 * @index:	The index number.

 * @name:	The name associated to the directory entry we are looking for.

 * @name_len:	The length of the name.

 * @mod:	Used to indicate if the tree search is meant for a read only

 *		lookup, for a modification lookup or for a deletion lookup, so

 *		its value should be 0, 1 or -1, respectively.

 *

 * Returns: NULL if the dir index item does not exists, an error pointer if an

 * error happened, or a pointer to a dir item if the dir index item exists and

 * matches the criteria (name and index number).

/*

 * helper function to look at the directory item pointed to by 'path'

 * this walks through all the entries in a dir item and finds one

 * for a specific name.

/*

 * given a pointer into a directory item, delete it.  This

 * handles items that have more than one entry in them.

 MARKER */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2011 STRATO AG

 * written by Arne Jansen <sensille@gmx.net>

/*

 * ulist is a generic data structure to hold a collection of unique u64

 * values. The only operations it supports is adding to the list and

 * enumerating it.

 * It is possible to store an auxiliary value along with the key.

 *

 * A sample usage for ulists is the enumeration of directed graphs without

 * visiting a node twice. The pseudo-code could look like this:

 *

 * ulist = ulist_alloc();

 * ulist_add(ulist, root);

 * ULIST_ITER_INIT(&uiter);

 *

 * while ((elem = ulist_next(ulist, &uiter)) {

 * 	for (all child nodes n in elem)

 *		ulist_add(ulist, n);

 *	do something useful with the node;

 * }

 * ulist_free(ulist);

 *

 * This assumes the graph nodes are addressable by u64. This stems from the

 * usage for tree enumeration in btrfs, where the logical addresses are

 * 64 bit.

 *

 * It is also useful for tree enumeration which could be done elegantly

 * recursively, but is not possible due to kernel stack limitations. The

 * loop would be similar to the above.

/**

 * ulist_init - freshly initialize a ulist

 * @ulist:	the ulist to initialize

 *

 * Note: don't use this function to init an already used ulist, use

 * ulist_reinit instead.

/**

 * ulist_release - free up additionally allocated memory for the ulist

 * @ulist:	the ulist from which to free the additional memory

 *

 * This is useful in cases where the base 'struct ulist' has been statically

 * allocated.

/**

 * ulist_reinit - prepare a ulist for reuse

 * @ulist:	ulist to be reused

 *

 * Free up all additional memory allocated for the list elements and reinit

 * the ulist.

/**

 * ulist_alloc - dynamically allocate a ulist

 * @gfp_mask:	allocation flags to for base allocation

 *

 * The allocated ulist will be returned in an initialized state.

/**

 * ulist_free - free dynamically allocated ulist

 * @ulist:	ulist to free

 *

 * It is not necessary to call ulist_release before.

/**

 * ulist_add - add an element to the ulist

 * @ulist:	ulist to add the element to

 * @val:	value to add to ulist

 * @aux:	auxiliary value to store along with val

 * @gfp_mask:	flags to use for allocation

 *

 * Note: locking must be provided by the caller. In case of rwlocks write

 *       locking is needed

 *

 * Add an element to a ulist. The @val will only be added if it doesn't

 * already exist. If it is added, the auxiliary value @aux is stored along with

 * it. In case @val already exists in the ulist, @aux is ignored, even if

 * it differs from the already stored value.

 *

 * ulist_add returns 0 if @val already exists in ulist and 1 if @val has been

 * inserted.

 * In case of allocation failure -ENOMEM is returned and the ulist stays

 * unaltered.

/*

 * ulist_del - delete one node from ulist

 * @ulist:	ulist to remove node from

 * @val:	value to delete

 * @aux:	aux to delete

 *

 * The deletion will only be done when *BOTH* val and aux matches.

 * Return 0 for successful delete.

 * Return > 0 for not found.

 Not found */

 Found and delete */

/**

 * ulist_next - iterate ulist

 * @ulist:	ulist to iterate

 * @uiter:	iterator variable, initialized with ULIST_ITER_INIT(&iterator)

 *

 * Note: locking must be provided by the caller. In case of rwlocks only read

 *       locking is needed

 *

 * This function is used to iterate an ulist.

 * It returns the next element from the ulist or %NULL when the

 * end is reached. No guarantee is made with respect to the order in which

 * the elements are returned. They might neither be returned in order of

 * addition nor in ascending order.

 * It is allowed to call ulist_add during an enumeration. Newly added items

 * are guaranteed to show up in the running enumeration.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2008 Oracle.  All rights reserved.

 *

 * Based on jffs2 zlib code:

 * Copyright © 2001-2007 Red Hat, Inc.

 * Created by David Woodhouse <dwmw2@infradead.org>

 workspace buffer size for s390 zlib hardware support */

	/*

	 * In case of s390 zlib hardware support, allocate lager workspace

	 * buffer. If allocator fails, fall back to a single page buffer.

		/*

		 * Get next input pages and copy the contents to

		 * the workspace buffer if required.

 we're making it bigger, give up */

		/* we need another page for writing out.  Test this

		 * before the total_in so we will pull in a new page for

		 * the stream end if required

 we're all done */

	/*

	 * Call deflate with Z_FINISH flush parameter providing more output

	 * space but no more input data, until it returns with Z_STREAM_END.

 get another page for the stream end */

	/* If it's deflate, and it's got no preset dictionary, then

 we didn't make progress in this inflate call, we're done */

	/* If it's deflate, and it's got no preset dictionary, then

	/*

	 * this should only happen if zlib returned fewer bytes than we

	 * expected.  btrfs_get_block is responsible for zeroing from the

	 * end of the inline extent (destlen) to the end of the page

 SPDX-License-Identifier: GPL-2.0

/**

 * extent_map_tree_init - initialize extent map tree

 * @tree:		tree to initialize

 *

 * Initialize the extent tree @tree.  Should be called for each new inode

 * or other user of the extent_map interface.

/**

 * alloc_extent_map - allocate new extent map structure

 *

 * Allocate a new extent_map structure.  The new structure is

 * returned with a reference count of one and needs to be

 * freed using free_extent_map()

/**

 * free_extent_map - drop reference count of an extent_map

 * @em:		extent map being released

 *

 * Drops the reference out on @em by one and free the structure

 * if the reference count hits zero.

 simple helper to do math around the end of an extent, handling wrap */

/*

 * search through the tree for an extent_map with a given offset.  If

 * it can't be found, try to find some neighboring extents

 check to see if two extent_map structs are adjacent and safe to merge */

	/*

	 * don't merge compressed extents, we need to know their

	 * actual size

	/*

	 * We don't want to merge stuff that hasn't been written to the log yet

	 * since it may not reflect exactly what is on disk, and that would be

	 * bad.

	/*

	 * We can't modify an extent map that is in the tree and that is being

	 * used by another task, as it can cause that other task to see it in

	 * inconsistent state during the merging. We always have 1 reference for

	 * the tree and 1 for this task (which is unpinning the extent map or

	 * clearing the logging flag), so anything > 2 means it's being used by

	 * other tasks too.

/**

 * unpin_extent_cache - unpin an extent from the cache

 * @tree:	tree to unpin the extent in

 * @start:	logical offset in the file

 * @len:	length of the extent

 * @gen:	generation that this extent has been modified in

 *

 * Called after an extent has been written to disk properly.  Set the generation

 * to the generation that actually added the file item to the inode so we know

 * we need to sync this extent when we call fsync().

/**

 * Add new extent map to the extent tree

 *

 * @tree:	tree to insert new map in

 * @em:		map to insert

 * @modified:	indicate whether the given @em should be added to the

 *	        modified list, which indicates the extent needs to be logged

 *

 * Insert @em into @tree or perform a simple forward/backward merge with

 * existing mappings.  The extent_map struct passed in will be inserted

 * into the tree directly, with an additional reference taken, or a

 * reference dropped if the merge attempt was successful.

/**

 * lookup_extent_mapping - lookup extent_map

 * @tree:	tree to lookup in

 * @start:	byte offset to start the search

 * @len:	length of the lookup range

 *

 * Find and return the first extent_map struct in @tree that intersects the

 * [start, len] range.  There may be additional objects in the tree that

 * intersect, so check the object returned carefully to make sure that no

 * additional lookups are needed.

/**

 * search_extent_mapping - find a nearby extent map

 * @tree:	tree to lookup in

 * @start:	byte offset to start the search

 * @len:	length of the lookup range

 *

 * Find and return the first extent_map struct in @tree that intersects the

 * [start, len] range.

 *

 * If one can't be found, any nearby extent may be returned

/**

 * remove_extent_mapping - removes an extent_map from the extent tree

 * @tree:	extent tree to remove from

 * @em:		extent map being removed

 *

 * Removes @em from @tree.  No reference counts are dropped, and no checks

 * are done to see if the range is in use

/*

 * Helper for btrfs_get_extent.  Given an existing extent in the tree,

 * the existing extent is the nearest extent to map_start,

 * and an extent that you want to insert, deal with overlap and insert

 * the best fitted new extent into the tree.

/**

 * Add extent mapping into em_tree

 *

 * @fs_info:  the filesystem

 * @em_tree:  extent tree into which we want to insert the extent mapping

 * @em_in:    extent we are inserting

 * @start:    start of the logical range btrfs_get_extent() is requesting

 * @len:      length of the logical range btrfs_get_extent() is requesting

 *

 * Note that @em_in's range may be different from [start, start+len),

 * but they must be overlapped.

 *

 * Insert @em_in into @em_tree. In case there is an overlapping range, handle

 * the -EEXIST by either:

 * a) Returning the existing extent in @em_in if @start is within the

 *    existing em.

 * b) Merge the existing extent with @em_in passed in.

 *

 * Return 0 on success, otherwise -EEXIST.

 *

	/* it is possible that someone inserted the extent into the tree

	 * while we had the lock dropped.  It is also possible that

	 * an overlapping map exists in the tree

		/*

		 * existing will always be non-NULL, since there must be

		 * extent causing the -EEXIST.

			/*

			 * The existing extent map is the one nearest to

			 * the [start, start + len) range which overlaps

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2008 Oracle.  All rights reserved.

/*

 * Extent buffer locking

 * =====================

 *

 * We use a rw_semaphore for tree locking, and the semantics are exactly the

 * same:

 *

 * - reader/writer exclusion

 * - writer/writer exclusion

 * - reader/reader sharing

 * - try-lock semantics for readers and writers

 *

 * The rwsem implementation does opportunistic spinning which reduces number of

 * times the locking task needs to sleep.

/*

 * __btrfs_tree_read_lock - lock extent buffer for read

 * @eb:		the eb to be locked

 * @nest:	the nesting level to be used for lockdep

 *

 * This takes the read lock on the extent buffer, using the specified nesting

 * level for lockdep purposes.

/*

 * Try-lock for read.

 *

 * Return 1 if the rwlock has been taken, 0 otherwise

/*

 * Try-lock for write.

 *

 * Return 1 if the rwlock has been taken, 0 otherwise

/*

 * Release read lock.

/*

 * __btrfs_tree_lock - lock eb for write

 * @eb:		the eb to lock

 * @nest:	the nesting to use for the lock

 *

 * Returns with the eb->lock write locked.

/*

 * Release the write lock.

/*

 * This releases any locks held in the path starting at level and going all the

 * way up to the root.

 *

 * btrfs_search_slot will keep the lock held on higher nodes in a few corner

 * cases, such as COW of the block at slot zero in the node.  This ignores

 * those rules, and it should only be called when there are no more updates to

 * be done higher up in the tree.

/*

 * Loop around taking references on and locking the root node of the tree until

 * we end up with a lock on the root node.

 *

 * Return: root extent buffer with write lock held

/*

 * Loop around taking references on and locking the root node of the tree until

 * we end up with a lock on the root node.

 *

 * Return: root extent buffer with read lock held

/*

 * DREW locks

 * ==========

 *

 * DREW stands for double-reader-writer-exclusion lock. It's used in situation

 * where you want to provide A-B exclusion but not AA or BB.

 *

 * Currently implementation gives more priority to reader. If a reader and a

 * writer both race to acquire their respective sides of the lock the writer

 * would yield its lock as soon as it detects a concurrent reader. Additionally

 * if there are pending readers no new writers would be allowed to come in and

 * acquire the lock.

 Return true if acquisition is successful, false otherwise */

 Ensure writers count is updated before we check for pending readers */

	/*

	 * Ensure the pending reader count is perceieved BEFORE this reader

	 * goes to sleep in case of active writers. This guarantees new writers

	 * won't be allowed and that the current reader will be woken up when

	 * the last active writer finishes its jobs.

	/*

	 * atomic_dec_and_test implies a full barrier, so woken up writers

	 * are guaranteed to see the decrement

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/*

 * Transaction states and transitions

 *

 * No running transaction (fs tree blocks are not modified)

 * |

 * | To next stage:

 * |  Call start_transaction() variants. Except btrfs_join_transaction_nostart().

 * V

 * Transaction N [[TRANS_STATE_RUNNING]]

 * |

 * | New trans handles can be attached to transaction N by calling all

 * | start_transaction() variants.

 * |

 * | To next stage:

 * |  Call btrfs_commit_transaction() on any trans handle attached to

 * |  transaction N

 * V

 * Transaction N [[TRANS_STATE_COMMIT_START]]

 * |

 * | Will wait for previous running transaction to completely finish if there

 * | is one

 * |

 * | Then one of the following happes:

 * | - Wait for all other trans handle holders to release.

 * |   The btrfs_commit_transaction() caller will do the commit work.

 * | - Wait for current transaction to be committed by others.

 * |   Other btrfs_commit_transaction() caller will do the commit work.

 * |

 * | At this stage, only btrfs_join_transaction*() variants can attach

 * | to this running transaction.

 * | All other variants will wait for current one to finish and attach to

 * | transaction N+1.

 * |

 * | To next stage:

 * |  Caller is chosen to commit transaction N, and all other trans handle

 * |  haven been released.

 * V

 * Transaction N [[TRANS_STATE_COMMIT_DOING]]

 * |

 * | The heavy lifting transaction work is started.

 * | From running delayed refs (modifying extent tree) to creating pending

 * | snapshots, running qgroups.

 * | In short, modify supporting trees to reflect modifications of subvolume

 * | trees.

 * |

 * | At this stage, all start_transaction() calls will wait for this

 * | transaction to finish and attach to transaction N+1.

 * |

 * | To next stage:

 * |  Until all supporting trees are updated.

 * V

 * Transaction N [[TRANS_STATE_UNBLOCKED]]

 * |						    Transaction N+1

 * | All needed trees are modified, thus we only    [[TRANS_STATE_RUNNING]]

 * | need to write them back to disk and update	    |

 * | super blocks.				    |

 * |						    |

 * | At this stage, new transaction is allowed to   |

 * | start.					    |

 * | All new start_transaction() calls will be	    |

 * | attached to transid N+1.			    |

 * |						    |

 * | To next stage:				    |

 * |  Until all tree blocks are super blocks are    |

 * |  written to block devices			    |

 * V						    |

 * Transaction N [[TRANS_STATE_COMPLETED]]	    V

 *   All tree blocks and super blocks are written.  Transaction N+1

 *   This transaction is finished and all its	    [[TRANS_STATE_COMMIT_START]]

 *   data structures will be cleaned up.	    | Life goes on

		/*

		 * If any block groups are found in ->deleted_bgs then it's

		 * because the transaction was aborted and a commit did not

		 * happen (things failed before writing the new superblock

		 * and calling btrfs_finish_extent_commit()), so we can not

		 * discard the physical locations of the block groups.

 We can free old roots now. */

	/*

	 * We have to update the last_byte_to_unpin under the commit_root_sem,

	 * at the same time we swap out the commit roots.

	 *

	 * This is because we must have a real view of the last spot the caching

	 * kthreads were while caching.  Consider the following views of the

	 * extent tree for a block group

	 *

	 * commit root

	 * +----+----+----+----+----+----+----+

	 * |\\\\|    |\\\\|\\\\|    |\\\\|\\\\|

	 * +----+----+----+----+----+----+----+

	 * 0    1    2    3    4    5    6    7

	 *

	 * new commit root

	 * +----+----+----+----+----+----+----+

	 * |    |    |    |\\\\|    |    |\\\\|

	 * +----+----+----+----+----+----+----+

	 * 0    1    2    3    4    5    6    7

	 *

	 * If the cache_ctl->progress was at 3, then we are only allowed to

	 * unpin [0,1) and [2,3], because the caching thread has already

	 * processed those extents.  We are not allowed to unpin [5,6), because

	 * the caching thread will re-start it's search from 3, and thus find

	 * the hole from [4,6) to add to the free space cache.

/*

 * To be called after doing the chunk btree updates right after allocating a new

 * chunk (after btrfs_chunk_alloc_add_chunk_item() is called), when removing a

 * chunk after all chunk btree updates and after finishing the second phase of

 * chunk allocation (btrfs_create_pending_block_groups()) in case some block

 * group had its chunk item insertion delayed to the second phase.

/*

 * either allocate a new transaction or hop into the existing one

 The file system has been taken offline. No new transactions. */

	/*

	 * If we are ATTACH, we just want to catch the current transaction,

	 * and commit it. If there is no transaction, just return ENOENT.

	/*

	 * JOIN_NOLOCK only happens during the transaction commit, so

	 * it is impossible that ->running_transaction is NULL

		/*

		 * someone started a transaction after we unlocked.  Make sure

		 * to redo the checks above

	/*

	 * One for this trans handle, one so it will live on until we

	 * commit the transaction.

	/*

	 * although the tree mod log is per file system and not per transaction,

	 * the log must never go across transaction boundaries.

/*

 * This does all the record keeping required to make sure that a shareable root

 * is properly recorded in a given transaction.  This is required to make sure

 * the old root from before we joined the transaction is deleted when the

 * transaction commits.

		/*

		 * see below for IN_TRANS_SETUP usage rules

		 * we have the reloc mutex held now, so there

		 * is only one writer in this function

		/* make sure readers find IN_TRANS_SETUP before

		 * they find our root->last_trans update

		/* this is pretty tricky.  We don't want to

		 * take the relocation lock in btrfs_record_root_in_trans

		 * unless we're really doing the first setup for this root in

		 * this transaction.

		 *

		 * Normally we'd use root->last_trans as a flag to decide

		 * if we want to take the expensive mutex.

		 *

		 * But, we have to set root->last_trans before we

		 * init the relocation root, otherwise, we trip over warnings

		 * in ctree.c.  The solution used here is to flag ourselves

		 * with root IN_TRANS_SETUP.  When this is 1, we're still

		 * fixing up the reloc trees and everyone must wait.

		 *

		 * When this is zero, they can trust root->last_trans and fly

		 * through btrfs_record_root_in_trans without having to take the

		 * lock.  smp_wmb() makes sure that all the writes above are

		 * done before we pop in the zero below

 Add ourselves to the transaction dropped list */

 Make sure we don't try to update the root at commit time */

	/*

	 * see record_root_in_trans for comments about IN_TRANS_SETUP usage

	 * and barriers

/* wait for commit against the current transaction to become unblocked

 * when this is done, it is safe to start a new transaction, but the current

 * transaction might not be fully on disk.

	/*

	 * Do the reservation before we join the transaction so we can do all

	 * the appropriate flushing if need be.

		/*

		 * We want to reserve all the bytes we may need all at once, so

		 * we only do 1 enospc flushing cycle per transaction start.  We

		 * accomplish this by simply assuming we'll do 2 x num_items

		 * worth of delayed refs updates in this trans handle, and

		 * refill that amount for whatever is missing in the reserve.

		/*

		 * Do the reservation for the relocation root creation

		/*

		 * Some people call with btrfs_start_transaction(root, 0)

		 * because they can be throttled, but have some other mechanism

		 * for reserving space.  We still want these guys to refill the

		 * delayed block_rsv so just add 1 items worth of reservation

		 * here.

	/*

	 * If we are JOIN_NOLOCK we're already committing a transaction and

	 * waiting on this guy, so we don't need to do the sb_start_intwrite

	 * because we're already holding a ref.  We need this because we could

	 * have raced in and did an fsync() on a file which can kick a commit

	 * and then we deadlock with somebody doing a freeze.

	 *

	 * If we are ATTACH, it means we just want to catch the current

	 * transaction and commit it, so we needn't do sb_start_intwrite(). 

	/*

	 * If the space_info is marked ALLOC_FORCE then we'll get upgraded to

	 * ALLOC_FORCE the first run through, and then we won't allocate for

	 * anybody else who races in later.  We don't care about the return

	 * value here.

	/*

	 * btrfs_record_root_in_trans() needs to alloc new extents, and may

	 * call btrfs_join_transaction() while we're also starting a

	 * transaction.

	 *

	 * Thus it need to be called after current->journal_info initialized,

	 * or we can deadlock.

		/*

		 * The transaction handle is fully initialized and linked with

		 * other structures so it needs to be ended in case of errors,

		 * not just freed.

/*

 * Similar to regular join but it never starts a transaction when none is

 * running or after waiting for the current one to finish.

/*

 * btrfs_attach_transaction() - catch the running transaction

 *

 * It is used when we want to commit the current the transaction, but

 * don't want to start a new one.

 *

 * Note: If this function return -ENOENT, it just means there is no

 * running transaction. But it is possible that the inactive transaction

 * is still in the memory, not fully on disk. If you hope there is no

 * inactive transaction in the fs when -ENOENT is returned, you should

 * invoke

 *     btrfs_attach_transaction_barrier()

/*

 * btrfs_attach_transaction_barrier() - catch the running transaction

 *

 * It is similar to the above function, the difference is this one

 * will wait for all the inactive transactions until they fully

 * complete.

 Wait for a transaction commit to reach at least the given state. */

 find specified transaction */

		/*

		 * The specified transaction doesn't exist, or we

		 * raced with btrfs_commit_transaction

 find newest transaction that is committing | committed */

 nothing committing|committed */

/*

 * when btree blocks are allocated, they have some corresponding bits set for

 * them in one of two extent_io trees.  This is used to make sure all of

 * those extents are sent to disk but does not wait on them

		/*

		 * convert_extent_bit can return -ENOMEM, which is most of the

		 * time a temporary error. So when it happens, ignore the error

		 * and wait for writeback of this range to finish - because we

		 * failed to set the bit EXTENT_NEED_WAIT for the range, a call

		 * to __btrfs_wait_marked_extents() would not know that

		 * writeback for this range started and therefore wouldn't

		 * wait for it to finish - we don't want to commit a

		 * superblock that points to btree nodes/leafs for which

		 * writeback hasn't finished yet (and without errors).

		 * We cleanup any entries left in the io tree when committing

		 * the transaction (through extent_io_tree_release()).

/*

 * when btree blocks are allocated, they have some corresponding bits set for

 * them in one of two extent_io trees.  This is used to make sure all of

 * those extents are on disk for transaction or log commit.  We wait

 * on all the pages and clear them from the dirty pages state tree

		/*

		 * Ignore -ENOMEM errors returned by clear_extent_bit().

		 * When committing the transaction, we'll remove any entries

		 * left in the io tree. For a log commit, we don't remove them

		 * after committing the log because the tree can be accessed

		 * concurrently - we do it only at transaction commit time when

		 * it's safe to do it (through extent_io_tree_release()).

/*

 * When btree blocks are allocated the corresponding extents are marked dirty.

 * This function ensures such extents are persisted on disk for transaction or

 * log commit.

 *

 * @trans: transaction whose dirty pages we'd like to write

/*

 * this is used to update the root pointer in the tree of tree roots.

 *

 * But, in the case of the extent allocation tree, updating the root

 * pointer may allocate blocks which may change the root of the extent

 * allocation tree.

 *

 * So, this loops and repeats and makes sure the cowonly root didn't

 * change while the root pointer was being updated in the metadata.

/*

 * update all the cowonly tree roots on disk

 *

 * The error handling in this function may not be obvious. Any of the

 * failures will cause the file system to go offline. We still need

 * to clean up the delayed refs.

 Now flush any delayed refs generated by updating all of the roots */

		/*

		 * We're writing the dirty block groups, which could generate

		 * delayed refs, which could generate more dirty block groups,

		 * so we want to keep this flushing in this loop to make sure

		 * everything gets run.

 Update dev-replace pointer once everything is committed */

/*

 * dead roots are old snapshots that need to be deleted.  This allocates

 * a dirty root struct and adds it into the list of dead roots that need to

 * be deleted

/*

 * update all the cowonly tree roots on disk

 see comments in should_cow_block() */

/*

 * defrag a given btree.

 * Every leaf in the btree is read and defragged.

/*

 * Do all special snapshot related qgroup dirty hack.

 *

 * Will do all needed qgroup inherit and dirty hack like switch commit

 * roots inside one transaction and write all btree into disk, to make

 * qgroup works.

	/*

	 * Save some performance in the case that qgroups are not

	 * enabled. If this check races with the ioctl, rescan will

	 * kick in anyway.

	/*

	 * Ensure dirty @src will be committed.  Or, after coming

	 * commit_fs_roots() and switch_commit_roots(), any dirty but not

	 * recorded root will never be updated again, causing an outdated root

	 * item.

	/*

	 * btrfs_qgroup_inherit relies on a consistent view of the usage for the

	 * src root, so we must run the delayed refs here.

	 *

	 * However this isn't particularly fool proof, because there's no

	 * synchronization keeping us from changing the tree after this point

	 * before we do the qgroup_inherit, or even from making changes while

	 * we're doing the qgroup_inherit.  But that's a problem for the future,

	 * for now flush the delayed refs to narrow the race window where the

	 * qgroup counters could end up wrong.

	/*

	 * We are going to commit transaction, see btrfs_commit_transaction()

	 * comment for reason locking tree_log_mutex

 Now qgroup are all updated, we can inherit it to new qgroups */

	/*

	 * Now we do a simplified commit transaction, which will:

	 * 1) commit all subvolume and extent tree

	 *    To ensure all subvolume and extent tree have a valid

	 *    commit_root to accounting later insert_dir_item()

	 * 2) write all btree blocks onto disk

	 *    This is to make sure later btree modification will be cowed

	 *    Or commit_root can be populated and cause wrong qgroup numbers

	 * In this simplified commit, we don't really care about other trees

	 * like chunk and root tree, as they won't affect qgroup.

	 * And we don't write super to avoid half committed status.

	/*

	 * Force parent root to be updated, as we recorded it before so its

	 * last_trans == cur_transid.

	 * Or it won't be committed again onto disk after later

	 * insert_dir_item()

/*

 * new snapshots need to be created at a very specific time in the

 * transaction commit.  This does the actual creation.

 *

 * Note:

 * If the error which may affect the commitment of the current transaction

 * happens, we should return the error number. If the error which just affect

 * the creation of the pending snapshots, just return 0.

	/*

	 * Make qgroup to skip current new snapshot's qgroupid, as it is

	 * accounted by later btrfs_qgroup_inherit().

	/*

	 * insert the directory item

 -ENOMEM */

 check if there is a file/dir which has the same name. */

	/*

	 * pull in the delayed directory update

	 * and the delayed inode item

	 * otherwise we corrupt the FS during

	 * snapshot

 Transaction aborted */

 clean up in any case */

 see comments in should_cow_block() */

 record when the snapshot was created in key.offset */

	/*

	 * insert root back/forward references

	/*

	 * Do special qgroup accounting for snapshot, as we do some qgroup

	 * snapshot hack to do fast snapshot.

	 * To co-operate with that hack, we do hack again.

	 * Or snapshot will be greatly slowed down by a subtree qgroup rescan

 We have check then name at the beginning, so it is impossible. */

/*

 * create all the snapshots we've scheduled for creation

/*

 * commit transactions asynchronously. once btrfs_commit_transaction_async

 * returns, any subsequent transaction will not be allowed to join.

	/*

	 * We've got freeze protection passed with the transaction.

	 * Tell lockdep about it.

 take transaction reference */

	/*

	 * Tell lockdep we've released the freeze rwsem, since the

	 * async commit thread will be the one to unlock it.

	/*

	 * Wait for the current transaction commit to start and block

	 * subsequent transaction joins

	/*

	 * If the transaction is removed from the list, it means this

	 * transaction has been committed successfully, so it is impossible

	 * to call the cleanup function.

	/*

	 * Now that we know no one else is still using the transaction we can

	 * remove the transaction from the list of transactions. This avoids

	 * the transaction kthread from cleaning up the transaction while some

	 * other task is still using it, which could result in a use-after-free

	 * on things like log trees, as it forces the transaction kthread to

	 * wait for this transaction to be cleaned up by us.

/*

 * Release reserved delayed ref space of all pending block groups of the

 * transaction and remove them from the list

	/*

	 * We use writeback_inodes_sb here because if we used

	 * btrfs_start_delalloc_roots we would deadlock with fs freeze.

	 * Currently are holding the fs freeze lock, if we do an async flush

	 * we'll do btrfs_join_transaction() and deadlock because we need to

	 * wait for the fs freeze lock.  Using the direct flushing we benefit

	 * from already being in a transaction and our join_transaction doesn't

	 * have to re-take the fs freeze lock.

 Stop the commit early if ->aborted is set */

	/*

	 * We only want one transaction commit doing the flushing so we do not

	 * waste a bunch of time on lock contention on the extent root node.

		/*

		 * Make a pass through all the delayed refs we have so far.

		 * Any running threads may add more while we are here.

		/* this mutex is also taken before trying to set

		 * block groups readonly.  We need to make sure

		 * that nobody has set a block group readonly

		 * after a extents from that block group have been

		 * allocated for cache files.  btrfs_set_block_group_ro

		 * will wait for the transaction to commit if it

		 * finds BTRFS_TRANS_DIRTY_BG_RUN set.

		 *

		 * The BTRFS_TRANS_DIRTY_BG_RUN flag is also used to make sure

		 * only one process starts all the block group IO.  It wouldn't

		 * hurt to have more than one go through, but there's no

		 * real advantage to it either.

		/*

		 * The previous transaction was aborted and was already removed

		 * from the list of transactions at fs_info->trans_list. So we

		 * abort to prevent writing a new superblock that reflects a

		 * corrupt state (pointing to trees with unwritten nodes/leafs).

 some pending stuffs might be added after the previous flush. */

	/*

	 * Wait for all ordered extents started by a fast fsync that joined this

	 * transaction. Otherwise if this transaction commits before the ordered

	 * extents complete we lose logged data after a power failure.

	/*

	 * Ok now we need to make sure to block out any other joins while we

	 * commit the transaction.  We could have started a join before setting

	 * COMMIT_DOING so make sure to wait for num_writers to == 1 again.

	/*

	 * the reloc mutex makes sure that we stop

	 * the balancing code from coming in and moving

	 * extents around in the middle of the commit

	/*

	 * We needn't worry about the delayed items because we will

	 * deal with them in create_pending_snapshot(), which is the

	 * core function of the snapshot creation.

	/*

	 * We insert the dir indexes of the snapshots and update the inode

	 * of the snapshots' parents after the snapshot creation, so there

	 * are some delayed items which are not dealt with. Now deal with

	 * them.

	 *

	 * We needn't worry that this operation will corrupt the snapshots,

	 * because all the tree which are snapshoted will be forced to COW

	 * the nodes and leaves.

	/*

	 * make sure none of the code above managed to slip in a

	 * delayed item

	/* btrfs_commit_tree_roots is responsible for getting the

	 * various roots consistent with each other.  Every pointer

	 * in the tree of tree roots has to point to the most up to date

	 * root for every subvolume and other tree.  So, we have to keep

	 * the tree logging code from jumping in and changing any

	 * of the trees.

	 *

	 * At this point in the commit, there can't be any tree-log

	 * writers, but a little lower down we drop the trans mutex

	 * and let new people in.  By holding the tree_log_mutex

	 * from now until after the super is written, we avoid races

	 * with the tree-log code.

	/*

	 * Since the transaction is done, we can apply the pending changes

	 * before the next transaction.

	/* commit_fs_roots gets rid of all the tree log roots, it is now

	 * safe to free the root of tree log roots

	/*

	 * Since fs roots are all committed, we can get a quite accurate

	 * new_roots. So let's do quota accounting.

	/*

	 * The tasks which save the space cache and inode cache may also

	 * update ->aborted, check it.

		/*

		 * reloc_mutex has been unlocked, tree_log_mutex is still held

		 * but we can't jump to unlock_tree_log causing double unlock

	/*

	 * At this point, we should have written all the tree blocks allocated

	 * in this transaction. So it's now safe to free the redirtyied extent

	 * buffers.

	/*

	 * the super is written, we can safely allow the tree-loggers

	 * to go about their business

	/*

	 * We needn't acquire the lock here because there is no other task

	 * which can change it.

	/*

	 * We needn't acquire the lock here because there is no other task

	 * which can change it.

/*

 * return < 0 if error

 * 0 if there are no more dead_roots at the time of call

 * 1 there are more to be processed, call me again

 *

 * The return value indicates there are certainly more snapshots to delete, but

 * if there comes a new one during processing, it may return 0. We don't mind,

 * because btrfs_commit_super will poke cleaner thread and it will process it a

 * few seconds later.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) STRATO AG 2012.  All rights reserved.

/*

 * Device replace overview

 *

 * [Objective]

 * To copy all extents (both new and on-disk) from source device to target

 * device, while still keeping the filesystem read-write.

 *

 * [Method]

 * There are two main methods involved:

 *

 * - Write duplication

 *

 *   All new writes will be written to both target and source devices, so even

 *   if replace gets canceled, sources device still contains up-to-date data.

 *

 *   Location:		handle_ops_on_dev_replace() from __btrfs_map_block()

 *   Start:		btrfs_dev_replace_start()

 *   End:		btrfs_dev_replace_finishing()

 *   Content:		Latest data/metadata

 *

 * - Copy existing extents

 *

 *   This happens by re-using scrub facility, as scrub also iterates through

 *   existing extents from commit root.

 *

 *   Location:		scrub_write_block_to_dev_replace() from

 *   			scrub_block_complete()

 *   Content:		Data/meta from commit root.

 *

 * Due to the content difference, we need to avoid nocow write when dev-replace

 * is happening.  This is done by marking the block group read-only and waiting

 * for NOCOW writes.

 *

 * After replace is done, the finishing part is done by swapping the target and

 * source devices.

 *

 *   Location:		btrfs_dev_replace_update_device_in_mapping_tree() from

 *   			btrfs_dev_replace_finishing()

		/*

		 * We don't have a replace item or it's corrupted.  If there is

		 * a replace target, fail the mount.

		/*

		 * We don't have an active replace item but if there is a

		 * replace target, fail the mount.

		/*

		 * allow 'btrfs dev replace_cancel' if src/tgt device is

		 * missing

/*

 * Initialize a new device for device replace target from a given source dev

 * and path.

 *

 * Return 0 and new device in @device_out, otherwise return < 0

/*

 * called from commit_transaction. Writes changed device replace state to

 * disk.

		/*

		 * need to delete old one and insert a new one.

		 * Since no attempt is made to recover any old state, if the

		 * dev_replace state is 'running', the data on the target

		 * drive is lost.

		 * It would be possible to recover the state: just make sure

		 * that the beginning of the item is never changed and always

		 * contains all the essential information. Then read this

		 * minimal set of information and use it as a base for the

		 * new state.

 need to insert a new item */

 Do not use "to_copy" on non zoned filesystem for now */

 Ensure we don't have pending new block group */

 Do not use "to_copy" on non zoned filesystem for now */

 We have more device extent to copy */

		/*

		 * Has more stripes on this device. Keep this block group

		 * readonly until we finish all the stripes.

 Last stripe on this device */

	/*

	 * Here we commit the transaction to make sure commit_total_bytes

	 * of all the devices are updated.

	/*

	 * from now on, the writes to the srcdev are all duplicated to

	 * go to the tgtdev as well (refer to btrfs_map_block()).

 Commit dev_replace state and reserve 1 item for it. */

 the disk copy procedure reuses the scrub code */

 don't warn if EINPROGRESS, someone else might be running scrub */

/*

 * blocked until all in-flight bios operations are finished.

/*

 * we have removed target device, it is safe to allow new bios request.

/*

 * When finishing the device replace, before swapping the source device with the

 * target device we must update the chunk allocation state in the target device,

 * as it is empty because replace works by directly copying the chunks and not

 * through the normal chunk allocation path.

 don't allow cancel or unmount to disturb the finishing procedure */

 was the operation canceled, or is it finished? */

	/*

	 * flush all outstanding I/O and inode extent mappings before the

	 * copy operation is declared as being finished

	/*

	 * We have to use this loop approach because at this point src_device

	 * has to be available for transaction commit to complete, yet new

	 * chunks shouldn't be allocated on the device.

 Prevent write_all_supers() during the finishing procedure */

 Prevent new chunks being allocated on the source device */

	/*

	 * Update allocation state in the new device and replace the old device

	 * with the new one in the mapping tree.

	/*

	 * Increment dev_stats_ccnt so that btrfs_run_dev_stats() will

	 * update on-disk dev stats value during commit transaction

	/*

	 * this is again a consistent state where no dev_replace procedure

	 * is running, the target device is part of the filesystem, the

	 * source device is not part of the filesystem anymore and its 1st

	 * superblock is scratched out so that it is no longer marked to

	 * belong to this filesystem.

 replace the sysfs entry */

 write back the superblocks */

/*

 * Read progress of device replace status according to the state and last

 * stored position. The value format is the same as for

 * btrfs_dev_replace::progress_1000

	/* even if !dev_replace_is_valid, the values are good enough for

			/*

			 * btrfs_dev_replace_finishing() will handle the

			 * cleanup part

		/*

		 * Scrub doing the replace isn't running so we need to do the

		 * cleanup step of btrfs_dev_replace_finishing() here

 Scrub for replace must not be running in suspended state */

 resume dev_replace procedure that was interrupted by unmount */

	/*

	 * This could collide with a paused balance, but the exclusive op logic

	 * should never allow both to start and pause. We don't want to allow

	 * dev-replace to start anyway.

		/*

		 * return true even if tgtdev is missing (this is

		 * something that can happen if the dev_replace

		 * procedure is suspended by an umount and then

		 * the tgtdev is missing (or "btrfs dev scan") was

		 * not called and the filesystem is remounted

		 * in degraded state. This does not stop the

		 * dev_replace procedure. It needs to be canceled

		 * manually if the cancellation is wanted.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) STRATO AG 2013.  All rights reserved.

 return -ENOENT for !found, < 0 for errors, or 0 if an item was found */

 Add an item for the type for the first time */

		/*

		 * An item with that type already exists.

		 * Extend the item and store the new subid at the end.

 1 - for the uuid item */

/*

 * Check if there's an matching subvolume for given UUID

 *

 * Return:

 * 0	check succeeded, the entry is not outdated

 * > 0	if the check failed, the caller should remove the entry

 * < 0	if an error occurred

					/*

					 * this might look inefficient, but the

					 * justification is that it is an

					 * exception that check_func returns 1,

					 * and that in the regular case only one

					 * entry per UUID exists.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/*

 * when auto defrag is enabled we

 * queue up these defrag structs to remember which

 * inodes need defragging passes

 objectid */

	/*

	 * transid where the defrag was added, we search for

	 * extents newer than this

 root objectid */

 last offset we were able to defrag */

 if we've wrapped around back to zero once already */

/* pop a record for an inode into the defrag tree.  The lock

 * must be held already

 *

 * If you're inserting a record for an older transid than an

 * existing record, the transid already in the tree is lowered

 *

 * If an existing record is found the defrag item you

 * pass in is freed

			/* if we're reinserting an entry for

			 * an old defrag run, make sure to

			 * lower the transid of our existing record

/*

 * insert a defrag record for this inode if auto defrag is

 * enabled

		/*

		 * If we set IN_DEFRAG flag and evict the inode from memory,

		 * and then re-read this inode, this new inode doesn't have

		 * IN_DEFRAG flag. At the case, we may find the existed defrag.

/*

 * Requeue the defrag object. If there is a defrag object that points to

 * the same inode in the tree, we will merge them together (by

 * __btrfs_add_inode_defrag()) and free the one that we want to requeue.

	/*

	 * Here we don't check the IN_DEFRAG flag, because we need merge

	 * them together.

/*

 * pick the defragable inode that we want, if it doesn't exist, we will get

 * the next one.

 get the inode */

 do a chunk of defrag */

	/*

	 * if we filled the whole defrag batch, there

	 * must be more work to do.  Queue this defrag

	 * again

		/*

		 * we didn't fill our defrag batch, but

		 * we didn't start at zero.  Make sure we loop

		 * around to the start of the file.

/*

 * run through the list of inodes in the FS that need

 * defragging

 Pause the auto defragger. */

 find an inode to defrag */

	/*

	 * during unmount, we use the transaction_wait queue to

	 * wait for the defragger to stop

/* simple helper to fault in pages and copy.  This should go away

 * and be replaced with calls into generic code.

		/*

		 * Copy data from userspace to the current page

 Flush processor's dcache for this page */

		/*

		 * if we get a partial write, we can end up with

		 * partially up to date pages.  These add

		 * a lot of complexity, so make sure they don't

		 * happen by forcing this copy to be retried.

		 *

		 * The rest of the btrfs_file_write code will fall

		 * back to page at a time copies after we return 0.

/*

 * unlocks pages after btrfs_file_write is done with them

		/* page checked is some magic around finding pages that

		 * have been modified without going through btrfs_set_page_dirty

		 * clear it here. There should be no need to mark the pages

		 * accessed as prepare_pages should have marked them accessed

		 * in prepare_pages via find_or_create_page()

/*

 * After btrfs_copy_from_user(), update the following things for delalloc:

 * - Mark newly dirtied pages as DELALLOC in the io tree.

 *   Used to advise which range is to be written back.

 * - Mark modified pages as Uptodate/Dirty and not needing COW fixup

 * - Update inode size for past EOF write

	/*

	 * The pages may have already been dirty, clear out old accounting so

	 * we can set things up properly

	/*

	 * we've only changed i_size in ram, and we haven't updated

	 * the disk i_size.  There is no need to log the inode

	 * at this time.

/*

 * this drops all the extents in the cache that intersect the range

 * [start, end].  Existing extents are split as required.

 Logic error */

 once for us */

 once for the tree*/

/*

 * this is very complex, but the basic idea is to drop all extents

 * in the range start - end.  hint_block is filled in with a block number

 * that would be a good hint to the block allocator for this file.

 *

 * If an extent intersects the range but is not entirely inside the range

 * it is either truncated or split.  Anything entirely inside the range

 * is deleted from the tree.

 *

 * Note: the VFS' inode number of bytes is not updated, it's up to the caller

 * to deal with that. We set the field 'bytes_found' of the arguments structure

 * with the number of allocated bytes found in the target range, so that the

 * caller can update the inode's number of bytes in an atomic way when

 * replacing extents in a range to avoid races with stat(2).

 Must always have a path if ->replace_extent is true */

 can't happen */

		/*

		 * Don't skip extent items representing 0 byte lengths. They

		 * used to be created (bug) if while punching holes we hit

		 * -ENOSPC condition. So if we find one here, just ensure we

		 * delete it, otherwise we would insert a new file extent item

		 * with the same key (offset) as that 0 bytes length file

		 * extent item in the call to setup_items_for_insert() later

		 * in this function.

		/*

		 *     | - range to drop - |

		 *  | -------- extent -------- |

 -ENOMEM */

		/*

		 * From here on out we will have actually dropped something, so

		 * last_end can be updated.

		/*

		 *  | ---- range to drop ----- |

		 *      | -------- extent -------- |

		/*

		 *       | ---- range to drop ----- |

		 *  | -------- extent -------- |

		/*

		 *  | ---- range to drop ----- |

		 *    | ------ extent ------ |

 -ENOMEM */

		/*

		 * Set path->slots[0] to first slot, so that after the delete

		 * if items are move off from our leaf to its immediate left or

		 * right neighbor leafs, we end up with a correct and adjusted

		 * path->slots[0] for our insertion (if args->replace_extent).

	/*

	 * If btrfs_del_items() was called, it might have deleted a leaf, in

	 * which case it unlocked our path, so check path->locks[0] matches a

	 * write lock.

/*

 * Mark extent in the range start - end as written.

 *

 * This changes extent type from 'pre-allocated' to 'regular'. If only

 * part of extent is marked as written, the extent will be split into

 * two or three.

/*

 * on error we return an unlocked page and the error value

 * on success we return a locked page and 0

		/*

		 * Since btrfs_readpage() will unlock the page before it

		 * returns, there is a window where btrfs_releasepage() can be

		 * called to release the page.  Here we check both inode

		 * mapping and PagePrivate() to make sure the page was not

		 * released.

		 *

		 * The private flag check is essential for subpage as we need

		 * to store extra bitmap using page->private.

/*

 * this just gets pages into the page cache and locks them down.

/*

 * This function locks the extent and properly waits for data=ordered extents

 * to finish before allowing the pages to be modified if need.

 *

 * The return value:

 * 1 - the extent is locked

 * 0 - the extent is not locked, and everything is OK

 * -EAGAIN - need re-prepare the pages

 * the other < 0 number - Something wrong happens

	/*

	 * We should be called after prepare_pages() which should have locked

	 * all pages in the range.

/*

 * Check if we can do nocow write into the range [@pos, @pos + @write_bytes)

 *

 * @pos:	 File offset

 * @write_bytes: The length to write, will be updated to the nocow writeable

 *		 range

 *

 * This function will flush ordered extents in the range to ensure proper

 * nocow checks.

 *

 * Return:

 * >0		and update @write_bytes if we can do nocow write

 *  0		if we can't do nocow write

 * -EAGAIN	if we can't get the needed lock or there are ordered extents

 * 		for * (nowait == true) case

 * <0		if other error happened

 *

 * NOTE: Callers need to release the lock by btrfs_check_nocow_unlock().

 We will allocate space in case nodatacow is not set, so bail */

		/*

		 * There are holes in the range or parts of the range that must

		 * be COWed (shared extents, RO block groups, etc), so just bail

		 * out.

	/*

	 * We reserve space for updating the inode when we reserve space for the

	 * extent we are going to write, so we will enospc out there.  We don't

	 * need to start yet another transaction to update the inode as we will

	 * update the inode when we finish writing whatever data we write.

 Expand hole size to cover write data, preventing empty gap */

		/*

		 * Fault pages before locking them in prepare_pages

		 * to avoid recursive lock

			/*

			 * If we don't have to COW at the offset, reserve

			 * metadata only. write_bytes may get smaller than

			 * requested here.

		/*

		 * This is going to setup the pages array with the number of

		 * pages we want, so we don't really need to worry about the

		 * contents of pages from loop to loop

		/*

		 * if we have trouble faulting in the pages, fall

		 * back to one page at a time

 release everything except the sectors we dirtied */

		/*

		 * If we have not locked the extent range, because the range's

		 * start offset is >= i_size, we might still have a non-NULL

		 * cached extent state, acquired while marking the extent range

		 * as delalloc through btrfs_dirty_pages(). Therefore free any

		 * possible cached extent state to avoid a memory leak.

 If the write DIO is within EOF, use a shared lock */

	/*

	 * Re-check since file size may have changed just before taking the

	 * lock or pos may have changed because of O_APPEND in generic_write_check()

	/*

	 * We remove IOCB_DSYNC so that we don't deadlock when iomap_dio_rw()

	 * calls generic_write_sync() (through iomap_dio_complete()), because

	 * that results in calling fsync (btrfs_sync_file()) which will try to

	 * lock the inode in exclusive/write mode.

	/*

	 * The iov_iter can be mapped to the same file range we are writing to.

	 * If that's the case, then we will deadlock in the iomap code, because

	 * it first calls our callback btrfs_dio_iomap_begin(), which will create

	 * an ordered extent, and after that it will fault in the pages that the

	 * iov_iter refers to. During the fault in we end up in the readahead

	 * pages code (starting at btrfs_readahead()), which will lock the range,

	 * find that ordered extent and then wait for it to complete (at

	 * btrfs_lock_and_flush_ordered_range()), resulting in a deadlock since

	 * obviously the ordered extent can never complete as we didn't submit

	 * yet the respective bio(s). This always happens when the buffer is

	 * memory mapped to the same file range, since the iomap DIO code always

	 * invalidates pages in the target file range (after starting and waiting

	 * for any writeback).

	 *

	 * So here we disable page faults in the iov_iter and then retry if we

	 * got -EFAULT, faulting in the pages before the retry.

 No increment (+=) because iomap returns a cumulative value. */

		/*

		 * We have more data left to write. Try to fault in as many as

		 * possible of the remainder pages and retry. We do this without

		 * releasing and locking again the inode, to prevent races with

		 * truncate.

		 *

		 * Also, in case the iov refers to pages in the file range of the

		 * file we want to write to (due to a mmap), we could enter an

		 * infinite loop if we retry after faulting the pages in, since

		 * iomap will invalidate any pages in the range early on, before

		 * it tries to fault in the pages of the iov. So we keep track of

		 * how much was left of iov in the previous EFAULT and fallback

		 * to buffered IO in case we haven't made any progress.

	/*

	 * Add back IOCB_DSYNC. Our caller, btrfs_file_write_iter(), will do

	 * the fsync (call generic_write_sync()).

 If 'err' is -ENOTBLK then it means we must fallback to buffered IO. */

	/*

	 * Ensure all data is persisted. We want the next direct IO read to be

	 * able to read what was just written.

	/*

	 * If the fs flips readonly due to some impossible error, although we

	 * have opened a file as writable, we have to stop this write operation

	 * to ensure consistency.

	/*

	 * Set by setattr when we are about to truncate a file from a non-zero

	 * size to a zero size.  This tries to flush down new bytes that may

	 * have been written if the application were using truncate to replace

	 * a file in place.

	/*

	 * This is only called in fsync, which would do synchronous writes, so

	 * a plug can merge adjacent IOs as much as possible.  Esp. in case of

	 * multiple disks using raid profile, a large IO can be split to

	 * several segments of stripe length (currently 64K).

	/*

	 * If we are doing a fast fsync we can not bail out if the inode's

	 * last_trans is <= then the last committed transaction, because we only

	 * update the last_trans of the inode during ordered extent completion,

	 * and for a fast fsync we don't wait for that, we only wait for the

	 * writeback to complete.

/*

 * fsync call for both files and directories.  This logs the inode into

 * the tree log instead of forcing full commits whenever possible.

 *

 * It needs to call filemap_fdatawait so that all ordered extent updates are

 * in the metadata btree are up to date for copying to the log.

 *

 * It drops the inode mutex before doing the tree log commit.  This is an

 * important optimization for directories because holding the mutex prevents

 * new operations on the dir while we write to disk.

	/*

	 * Always set the range to a full range, otherwise we can get into

	 * several problems, from missing file extent items to represent holes

	 * when not using the NO_HOLES feature, to log tree corruption due to

	 * races between hole detection during logging and completion of ordered

	 * extents outside the range, to missing checksums due to ordered extents

	 * for which we flushed only a subset of their pages.

	/*

	 * We write the dirty pages in the range and wait until they complete

	 * out of the ->i_mutex. If so, we can flush the dirty pages by

	 * multi-task, and make the performance up.  See

	 * btrfs_wait_ordered_range for an explanation of the ASYNC check.

	/*

	 * Always check for the full sync flag while holding the inode's lock,

	 * to avoid races with other tasks. The flag must be either set all the

	 * time during logging or always off all the time while logging.

	/*

	 * Before we acquired the inode's lock and the mmap lock, someone may

	 * have dirtied more pages in the target range. We need to make sure

	 * that writeback for any such pages does not start while we are logging

	 * the inode, because if it does, any of the following might happen when

	 * we are not doing a full inode sync:

	 *

	 * 1) We log an extent after its writeback finishes but before its

	 *    checksums are added to the csum tree, leading to -EIO errors

	 *    when attempting to read the extent after a log replay.

	 *

	 * 2) We can end up logging an extent before its writeback finishes.

	 *    Therefore after the log replay we will have a file extent item

	 *    pointing to an unwritten extent (and no data checksums as well).

	 *

	 * So trigger writeback for any eventual new dirty pages and then we

	 * wait for all ordered extents to complete below.

	/*

	 * We have to do this here to avoid the priority inversion of waiting on

	 * IO of a lower priority task while holding a transaction open.

	 *

	 * For a full fsync we wait for the ordered extents to complete while

	 * for a fast fsync we wait just for writeback to complete, and then

	 * attach the ordered extents to the transaction so that a transaction

	 * commit waits for their completion, to avoid data loss if we fsync,

	 * the current transaction commits before the ordered extents complete

	 * and a power failure happens right after that.

	 *

	 * For zoned filesystem, if a write IO uses a ZONE_APPEND command, the

	 * logical address recorded in the ordered extent may change. We need

	 * to wait for the IO to stabilize the logical address.

		/*

		 * Get our ordered extents as soon as possible to avoid doing

		 * checksum lookups in the csum tree, and use instead the

		 * checksums attached to the ordered extents.

		/*

		 * We've had everything committed since the last time we were

		 * modified so clear this flag in case it was set for whatever

		 * reason, it's no longer relevant.

		/*

		 * An ordered extent might have started before and completed

		 * already with io errors, in which case the inode was not

		 * updated and we end up here. So check the inode's mapping

		 * for any errors that might have happened since we last

		 * checked called fsync.

	/*

	 * We use start here because we will need to wait on the IO to complete

	 * in btrfs_sync_log, which could require joining a transaction (for

	 * example checking cross references in the nocow path).  If we use join

	 * here we could get into a situation where we're waiting on IO to

	 * happen that is blocked on a transaction trying to commit.  With start

	 * we inc the extwriter counter, so we wait for all extwriters to exit

	 * before we start blocking joiners.  This comment is to keep somebody

	 * from thinking they are super smart and changing this to

	 * btrfs_join_transaction *cough*Josef*cough*.

 Fallthrough and commit/free transaction. */

	/* we've logged all the items and now have a consistent

	 * version of the file in the log.  It is possible that

	 * someone will come in and modify the file, but that's

	 * fine because the log is consistent on disk, and we

	 * have references to all of the file's extents

	 *

	 * It is possible that someone will come in and log the

	 * file again, but that will end up using the synchronization

	 * inside btrfs_sync_log to keep things safe.

		/*

		 * We should have dropped this offset, so if we find it then

		 * something has gone horribly wrong.

/*

 * Find a hole extent on given inode and change start/len to the end of hole

 * extent.(hole/vacuum extent whose em->start <= start &&

 *	   em->start + em->len > start)

 * When a hole extent is found, return 1 and modify start/len.

 Hole or vacuum extent(only exists in no-hole mode) */

	/*

	 * For subpage case, if the range is not at page boundary, we could

	 * have pages at the leading/tailing part of the range.

	 * This could lead to dead loop since filemap_range_has_page()

	 * will always return true.

	 * So here we need to do extra page alignment for

	 * filemap_range_has_page().

		/*

		 * We need to make sure we have no ordered extents in this range

		 * and nobody raced in and read a page in this range, if we did

		 * we need to try again.

 If it's a hole, nothing more needs to be done. */

/*

 * The respective range must have been previously locked, as well as the inode.

 * The end offset is inclusive (last byte of the range).

 * @extent_info is NULL for fallocate's hole punching and non-NULL when replacing

 * the file range with an extent.

 * When not punching a hole, we don't want to end up in a state where we dropped

 * extents without inserting a new one, so we must abort the transaction to avoid

 * a corruption.

	/*

	 * 1 - update the inode

	 * 1 - removing the extents in the range

	 * 1 - adding the hole extent if no_holes isn't set or if we are

	 *     replacing the range with a new extent

 If we are punching a hole decrement the inode's byte count */

			/*

			 * The only time we don't want to abort is if we are

			 * attempting to clone a partial inline extent, in which

			 * case we'll get EOPNOTSUPP.  However if we aren't

			 * clone we need to abort no matter what, because if we

			 * got EOPNOTSUPP via prealloc then we messed up and

			 * need to abort.

				/*

				 * If we failed then we didn't insert our hole

				 * entries for the area we dropped, so now the

				 * fs is corrupted, so we must abort the

				 * transaction.

			/*

			 * We are past the i_size here, but since we didn't

			 * insert holes we need to clear the mapped area so we

			 * know to not set disk_i_size in this area until a new

			 * file extent is inserted here.

				/*

				 * We couldn't clear our area, so we could

				 * presumably adjust up and corrupt the fs, so

				 * we need to abort.

 shouldn't happen */

	/*

	 * If we were cloning, force the next fsync to be a full one since we

	 * we replaced (or just dropped in the case of cloning holes when

	 * NO_HOLES is enabled) file extent items and did not setup new extent

	 * maps for the replacement extents (or holes).

	/*

	 * If we are using the NO_HOLES feature we might have had already an

	 * hole that overlaps a part of the region [lockstart, lockend] and

	 * ends at (or beyond) lockend. Since we have no file extent items to

	 * represent holes, drop_end can be less than lockend and so we must

	 * make sure we have an extent map representing the existing hole (the

	 * call to __btrfs_drop_extents() might have dropped the existing extent

	 * map representing the existing hole), otherwise the fast fsync path

	 * will not record the existence of the hole region

	 * [existing_hole_start, lockend].

	/*

	 * Don't insert file hole extent item if it's for a range beyond eof

	 * (because it's useless) or if it represents a 0 bytes range (when

	 * cur_offset == drop_end).

 Same comment as above. */

 See the comment in the loop above for the reasoning here. */

 Already in a large hole */

	/*

	 * We needn't truncate any block which is beyond the end of the file

	 * because we are sure there is no data there.

	/*

	 * Only do this if we are in the same block and we aren't doing the

	 * entire block.

 zero back part of the first block */

	/* Check the aligned pages after the first unaligned page,

	 * if offset != orig_start, which means the first unaligned page

	 * including several following pages are already in holes,

 after truncate page, check hole again */

 Check the tail unaligned part is in a hole */

 zero the front end of the last page */

		/*

		 * If we only end up zeroing part of a page, we still need to

		 * update the inode item, so that all the time fields are

		 * updated as well as the necessary btrfs inode in memory fields

		 * for detecting, at fsync time, if the inode isn't yet in the

		 * log tree or it's there but not up to date.

 Helper structure to record which range is already reserved */

/*

 * Helper function to add falloc range

 *

 * Caller should have locked the larger range of extent containing

 * [start, len)

		/*

		 * As fallocate iterates by bytenr order, we only need to check

		 * the last range.

	/*

	 * Avoid hole punching and extent allocation for some cases. More cases

	 * could be considered, but these are unlikely common and we keep things

	 * as simple as possible for now. Also, intentionally, if the target

	 * range contains one or more prealloc extents together with regular

	 * extents and holes, we drop all the existing extents and allocate a

	 * new prealloc extent, so that we get a larger contiguous disk extent.

			/*

			 * The whole range is already a prealloc extent,

			 * do nothing except updating the inode's i_size if

			 * needed.

		/*

		 * Part of the range is already a prealloc extent, so operate

		 * only on the remaining part of the range.

	/*

	 * For unaligned ranges, check the pages at the boundaries, they might

	 * map to an extent, in which case we need to partially zero them, or

	 * they might map to a hole, in which case we need our allocation range

	 * to cover them.

 btrfs_prealloc_file_range releases reserved space on error */

 Do not allow fallocate in ZONED mode */

 Make sure we aren't being give some crap mode */

	/*

	 * Only trigger disk allocation, don't trigger qgroup reserve

	 *

	 * For qgroup space, it will be checked later.

	/*

	 * TODO: Move these two operations after we have checked

	 * accurate reserved space, or fallocate can still fail but

	 * with page truncated or size expanded.

	 *

	 * But that's a minor problem and won't do much harm BTW.

		/*

		 * If we are fallocating from the end of the file onward we

		 * need to zero out the end of the block if i_size lands in the

		 * middle of a block.

	/*

	 * wait for ordered IO before we have any locks.  We'll loop again

	 * below with the locks held.

		/* the extent lock is ordered inside the running

		 * transaction

			/*

			 * we can't wait on the range with the transaction

			 * running or with the extent lock held

 First, check if we exceed the qgroup limit */

			/*

			 * Do not need to reserve unwritten extent for this

			 * range, free reserved data space first, otherwise

			 * it'll result in false ENOSPC error.

	/*

	 * If ret is still 0, means we're OK to fallocate.

	 * Or just cleanup the list and exit.

	/*

	 * We didn't need to allocate any more space, but we still extended the

	 * size of the file so we need to update i_size and the inode item.

 Let go of our reservation. */

	/*

	 * offset can be negative, in this case we start finding DATA/HOLE from

	 * the very start of the file.

	/*

	 * This is similar to what we do for direct IO writes, see the comment

	 * at btrfs_direct_write(), but we also disable page faults in addition

	 * to disabling them only at the iov_iter level. This is because when

	 * reading from a hole or prealloc extent, iomap calls iov_iter_zero(),

	 * which can still trigger page fault ins despite having set ->nofault

	 * to true of our 'to' iov_iter.

	 *

	 * The difference to direct IO writes is that we deadlock when trying

	 * to lock the extent range in the inode's tree during he page reads

	 * triggered by the fault in (while for writes it is due to waiting for

	 * our own ordered extent). This is because for direct IO reads,

	 * btrfs_dio_iomap_begin() returns with the extent range locked, which

	 * is only unlocked in the endio callback (end_bio_extent_readpage()).

 No increment (+=) because iomap returns a cumulative value. */

			/*

			 * We didn't make any progress since the last attempt,

			 * fallback to a buffered read for the remainder of the

			 * range. This is just to avoid any possibility of looping

			 * for too long.

			/*

			 * We made some progress since the last retry or this is

			 * the first time we are retrying. Fault in as many pages

			 * as possible and retry.

	/*

	 * So with compression we will find and lock a dirty page and clear the

	 * first one as dirty, setup an async extent, and immediately return

	 * with the entire range locked but with nobody actually marked with

	 * writeback.  So we can't just filemap_write_and_wait_range() and

	 * expect it to work since it will just kick off a thread to do the

	 * actual work.  So we need to call filemap_fdatawrite_range _again_

	 * since it will wait on the page lock, which won't be unlocked until

	 * after the pages have been marked as writeback and so we're good to go

	 * from there.  We have to do this otherwise we'll miss the ordered

	 * extents and that results in badness.  Please Josef, do not think you

	 * know better and pull this out at some point in the future, it is

	 * right and you are wrong.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2012 Fusion-io  All rights reserved.

 * Copyright (C) 2012 Intel Corp. All rights reserved.

 set when additional merges to this rbio are not allowed */

/*

 * set when this rbio is sitting in the hash, but it is just a cache

 * of past RMW

/*

 * set when it is safe to trust the stripe_pages for caching

 Used by the raid56 code to lock stripes for read/modify/write */

 Used by the raid56 code to lock stripes for read/modify/write */

	/* while we're doing rmw on a stripe

	 * we put it into a hash table so we can

	 * lock the stripe and merge more rbios

	 * into it.

	/*

	 * LRU list for the stripe cache

	/*

	 * for scheduling work in the helper threads

	/*

	 * bio list and bio_list_lock are used

	 * to add more bios into the stripe

	 * in hopes of avoiding the full rmw

	/* also protected by the bio_list_lock, the

	 * plug list is used by the plugging code

	 * to collect partial bios while plugged.  The

	 * stripe locking code also uses it to hand off

	 * the stripe lock to the next pending IO

	/*

	 * flags that tell us if it is safe to

	 * merge with this bio

 size of each individual stripe on disk */

 number of data stripes (no p/q) */

	/*

	 * set if we're doing a parity rebuild

	 * for a read from higher up, which is handled

	 * differently from a parity rebuild as part of

	 * rmw

 first bad stripe */

 second bad stripe (for raid6 use) */

	/*

	 * number of pages needed to represent the full

	 * stripe

	/*

	 * size of all the bios in the bio_list.  This

	 * helps us decide if the rbio maps to a full

	 * stripe or not

	/*

	 * these are two arrays of pointers.  We allocate the

	 * rbio big enough to hold them both and setup their

	 * locations when the rbio is allocated

	/* pointers to pages that we allocated for

	 * reading/writing stripes directly from the disk (including P/Q)

	/*

	 * pointers to the pages in the bio_list.  Stored

	 * here for faster lookup

	/*

	 * bitmap to record which horizontal stripe has data

 allocated with real_stripes-many pointers for finish_*() calls */

 allocated with stripe_npages-many bits for finish_*() calls */

/*

 * the stripe hash table is used for locking, and to collect

 * bios in hopes of making a full stripe

	/*

	 * The table is large, starting with order 4 and can go as high as

	 * order 7 in case lock debugging is turned on.

	 *

	 * Try harder to allocate and fallback to vmalloc to lower the chance

	 * of a failing mount.

/*

 * caching an rbio means to copy anything from the

 * bio_pages array into the stripe_pages array.  We

 * use the page uptodate bit in the stripe cache array

 * to indicate if it has valid data

 *

 * once the caching is done, we set the cache ready

 * bit.

/*

 * we hash on the first logical address of the stripe

	/*

	 * we shift down quite a bit.  We're using byte

	 * addressing, and most of the lower bits are zeros.

	 * This tends to upset hash_64, and it consistently

	 * returns just one or two different values.

	 *

	 * shifting off the lower bits fixes things.

/*

 * stealing an rbio means taking all the uptodate pages from the stripe

 * array in the source rbio and putting them into the destination rbio

/*

 * merging means we take the bio_list from the victim and

 * splice it into the destination.  The victim should

 * be discarded afterwards.

 *

 * must be called with dest->rbio_list_lock held

/*

 * used to prune items that are in the cache.  The caller

 * must hold the hash table lock.

	/*

	 * check the bit again under the hash table lock.

	/* hold the lock for the bucket because we may be

	 * removing it from the hash table

	/*

	 * hold the lock for the bio list because we need

	 * to make sure the bio list is empty

		/* if the bio list isn't empty, this rbio is

		 * still involved in an IO.  We take it out

		 * of the cache list, and drop the ref that

		 * was held for the list.

		 *

		 * If the bio_list was empty, we also remove

		 * the rbio from the hash_table, and drop

		 * the corresponding ref

/*

 * prune a given rbio from the cache

/*

 * remove everything in the cache

/*

 * remove all cached entries and free the hash table

 * used by unmount

/*

 * insert an rbio into the stripe cache.  It

 * must have already been prepared by calling

 * cache_rbio_pages

 *

 * If this rbio was already cached, it gets

 * moved to the front of the lru.

 *

 * If the size of the rbio cache is too big, we

 * prune an item.

 bump our ref if we were not in the list before */

/*

 * helper function to run the xor_blocks api.  It is only

 * able to do MAX_XOR_BLOCKS at a time, so we need to

 * loop through.

/*

 * Returns true if the bio list inside this rbio covers an entire stripe (no

 * rmw required).

/*

 * returns 1 if it is safe to merge two rbios together.

 * The merging is safe if the two rbios correspond to

 * the same stripe and if they are both going in the same

 * direction (read vs write), and if neither one is

 * locked for final IO

 *

 * The caller is responsible for locking such that

 * rmw_locked is safe to test

	/*

	 * we can't merge with cached rbios, since the

	 * idea is that when we merge the destination

	 * rbio is going to run our IO for us.  We can

	 * steal from cached rbios though, other functions

	 * handle that.

 we can't merge with different operations */

	/*

	 * We've need read the full stripe from the drive.

	 * check and repair the parity and write the new results.

	 *

	 * We're not allowed to add any new bios to the

	 * bio list here, anyone else that wants to

	 * change this stripe needs to do their own rmw.

/*

 * these are just the pages from the rbio array, not from anything

 * the FS sent down to us

/*

 * helper to index into the pstripe

/*

 * helper to index into the qstripe, returns null

 * if there is no qstripe

/*

 * The first stripe in the table for a logical address

 * has the lock.  rbios are added in one of three ways:

 *

 * 1) Nobody has the stripe locked yet.  The rbio is given

 * the lock and 0 is returned.  The caller must start the IO

 * themselves.

 *

 * 2) Someone has the stripe locked, but we're able to merge

 * with the lock owner.  The rbio is freed and the IO will

 * start automatically along with the existing rbio.  1 is returned.

 *

 * 3) Someone has the stripe locked, but we're not able to merge.

 * The rbio is added to the lock owner's plug list, or merged into

 * an rbio already on the plug list.  When the lock owner unlocks,

 * the next rbio on the list is run and the IO is started automatically.

 * 1 is returned

 *

 * If we return 0, the caller still owns the rbio and must continue with

 * IO submission.  If we return 1, the caller must assume the rbio has

 * already been freed.

 Can we steal this cached rbio's pages? */

 Can we merge into the lock owner? */

		/*

		 * We couldn't merge with the running rbio, see if we can merge

		 * with the pending ones.  We don't have to check for rmw_locked

		 * because there is no way they are inside finish_rmw right now

		/*

		 * No merging, put us on the tail of the plug list, our rbio

		 * will be started with the currently running rbio unlocks

/*

 * called as rmw or parity rebuild is completed.  If the plug list has more

 * rbios waiting for this stripe, the next one on the list will be started

		/*

		 * if we're still cached and there is no other IO

		 * to perform, just leave this rbio here for others

		 * to steal from later

		/*

		 * we use the plug list to hold all the rbios

		 * waiting for the chance to lock this stripe.

		 * hand the lock over to one of them.

/*

 * this frees the rbio and runs through all the bios in the

 * bio_list and calls end_io on them

	/*

	 * At this moment, rbio->bio_list is empty, however since rbio does not

	 * always have RBIO_RMW_LOCKED_BIT set and rbio is still linked on the

	 * hash list, rbio may be merged with others so that rbio->bio_list

	 * becomes non-empty.

	 * Once unlock_stripe() is done, rbio->bio_list will not be updated any

	 * more and we can call bio_endio() on all queued bios.

/*

 * end io function used by finish_rmw.  When we finally

 * get here, we've written a full stripe

 OK, we have read all the stripes we need to. */

/*

 * the read/modify/write code wants to use the original bio for

 * any pages it included, and then use the rbio for everything

 * else.  This function decides if a given index (stripe number)

 * and page number in that stripe fall inside the original bio

 * or the rbio.

 *

 * if you set bio_list_only, you'll get a NULL back for any ranges

 * that are outside the bio_list

 *

 * This doesn't take any refs on anything, you get a bare page pointer

 * and the caller must bump refs as required.

 *

 * You must call index_rbio_pages once before you can trust

 * the answers from this function.

/*

 * number of pages we need for the entire stripe across all the

 * drives

/*

 * allocation and initial setup for the btrfs_raid_bio.  Not

 * this does not allocate any pages for rbio->pages.

	/*

	 * the stripe_pages, bio_pages, etc arrays point to the extra

	 * memory we allocated past the end of the rbio

 allocate pages for all the stripes in the bio, including parity */

 only allocate pages for p/q stripes */

/*

 * add a single page from a specific stripe into our list of bios for IO

 * this will try to merge into existing bios if possible, and returns

 * zero if all went well.

 if the device is missing, just fail this stripe */

 see if we can add this page onto our existing bio */

		/*

		 * we can't merge these if they are from different

		 * devices or if they are not contiguous

 put a new bio on the list */

/*

 * while we're doing the read/modify/write cycle, we could

 * have errors in reading pages off the disk.  This checks

 * for errors and if we're not able to read the page it'll

 * trigger parity reconstruction.  The rmw will be finished

 * after we've reconstructed the failed stripes

/*

 * helper function to walk our bio list and populate the bio_pages array with

 * the result.  This seems expensive, but it is faster than constantly

 * searching through the bio list as we setup the IO in finish_rmw or stripe

 * reconstruction.

 *

 * This must be called before you trust the answers from page_in_rbio

/*

 * this is called from one of two situations.  We either

 * have a full stripe from the higher layers, or we've read all

 * the missing bits off disk.

 *

 * This will calculate the parity and then send down any

 * changed blocks.

	/* at this point we either have a full stripe,

	 * or we've read the full stripe from the drive.

	 * recalculate the parity and write the new results.

	 *

	 * We're not allowed to add any new bios to the

	 * bio list here, anyone else that wants to

	 * change this stripe needs to do their own rmw.

	/*

	 * now that we've set rmw_locked, run through the

	 * bio list one last time and map the page pointers

	 *

	 * We don't cache full rbios because we're assuming

	 * the higher layers are unlikely to use this area of

	 * the disk again soon.  If they do use it again,

	 * hopefully they will send another full bio.

 first collect one page from each data stripe */

 then add the parity stripe */

			/*

			 * raid6, add the qstripe and call the

			 * library function to fill in our p/q

 raid5 */

	/*

	 * time to start writing.  Make bios for everything from the

	 * higher layers (the bio_list in our rbio) and our p/q.  Ignore

	 * everything else.

/*

 * helper to find the stripe number for a given bio.  Used to figure out which

 * stripe has failed.  This expects the bio to correspond to a physical disk,

 * so it looks up based on physical sector numbers.

/*

 * helper to find the stripe number for a given

 * bio (before mapping).  Used to figure out which stripe has

 * failed.  This looks up based on logical block numbers.

/*

 * returns -EIO if we had too many failures

 we already know this stripe is bad, move on */

 first failure on this rbio */

 second failure on this rbio */

/*

 * helper to fail a stripe based on a physical disk

 * bio.

/*

 * this sets each page in the bio uptodate.  It should only be used on private

 * rbio pages, nothing that comes in from the higher layers

/*

 * end io for the read phase of the rmw cycle.  All the bios here are physical

 * stripe bios we've read from the disk so we can recalculate the parity of the

 * stripe.

 *

 * This will usually kick off finish_rmw once all the bios are read in, but it

 * may trigger parity reconstruction if we had any errors along the way

	/*

	 * this will normally call finish_rmw to start our write

	 * but if there are any failed stripes we'll reconstruct

	 * from parity first

/*

 * the stripe must be locked by the caller.  It will

 * unlock after all the writes are done

	/*

	 * build a list of bios to read all the missing parts of this

	 * stripe

			/*

			 * we want to find all the pages missing from

			 * the rbio and read them from the disk.  If

			 * page_in_rbio finds a page in the bio list

			 * we don't need to read it off the stripe.

			/*

			 * the bio cache may have handed us an uptodate

			 * page.  If so, be happy and use it

		/*

		 * this can happen if others have merged with

		 * us, it means there is nothing left to read.

		 * But if there are missing devices it may not be

		 * safe to do the full stripe write yet.

	/*

	 * The bioc may be freed once we submit the last bio. Make sure not to

	 * touch it after that.

 the actual write will happen once the reads are done */

/*

 * if the upper layers pass in a full stripe, we thank them by only allocating

 * enough pages to hold the parity, and sending it all down quickly.

/*

 * partial stripe writes get handed over to async helpers.

 * We're really hoping to merge a few more writes into this

 * rbio before calculating new parity

/*

 * sometimes while we were reading from the drive to

 * recalculate parity, enough new bios come into create

 * a full stripe.  So we do a check here to see if we can

 * go directly to finish_rmw

 head off into rmw land if we don't have a full stripe */

/*

 * We use plugging call backs to collect full stripes.

 * Any time we get a partial stripe write while plugged

 * we collect it into a list.  When the unplug comes down,

 * we sort the list by logical block number and merge

 * everything we can into the same rbios

/*

 * rbios on the plug list are sorted for easier merging.

	/*

	 * sort our plug list then try to merge

	 * everything we can in hopes of creating full

	 * stripes.

 we have a full stripe, send it down */

/*

 * if the unplug comes from schedule, we have to push the

 * work off to a helper thread

/*

 * our main entry point for writes from the rest of the FS.

	/*

	 * don't plug on full rbios, just get them out the door

	 * as quickly as we can

/*

 * all parity reconstruction happens here.  We've read in everything

 * we can find from the drives and this does the heavy lifting of

 * sorting the good from the bad.

	/*

	 * Store copy of pointers that does not get reordered during

	 * reconstruction so that kunmap_local works.

		/*

		 * Now we just use bitmap to mark the horizontal stripes in

		 * which we have data when doing parity scrub.

		/*

		 * Setup our array of pointers with pages from each stripe

		 *

		 * NOTE: store a duplicate array of pointers to preserve the

		 * pointer order

			/*

			 * if we're rebuilding a read, we have to use

			 * pages from the bio list

 all raid6 handling here */

			/*

			 * single failure, rebuild from parity raid5

			 * style

					/*

					 * Just the P stripe has failed, without

					 * a bad data or Q stripe.

					 * TODO, we should redo the xor here.

				/*

				 * a single failure in raid6 is rebuilt

				 * in the pstripe code below

 make sure our ps and qs are in order */

			/* if the q stripe is failed, do a pstripe reconstruction

			 * from the xors.

			 * If both the q stripe and the P stripe are failed, we're

			 * here due to a crc mismatch and we can't give them the

			 * data they want

				/*

				 * otherwise we have one bad data stripe and

				 * a good P stripe.  raid5!

 rebuild from P stripe here (raid5 or raid6) */

 Copy parity block into failed block to start with */

 rearrange the pointer array */

 xor in the rest */

		/* if we're doing this rebuild as part of an rmw, go through

		 * and set all of our private rbio pages in the

		 * failed stripes as uptodate.  This way finish_rmw will

		 * know they can be trusted.  If this was a read reconstruction,

		 * other endio functions will fiddle the uptodate bits

	/*

	 * Similar to READ_REBUILD, REBUILD_MISSING at this point also has a

	 * valid rbio which is consistent with ondisk content, thus such a

	 * valid rbio can be cached to avoid further disk reads.

		/*

		 * - In case of two failures, where rbio->failb != -1:

		 *

		 *   Do not cache this rbio since the above read reconstruction

		 *   (raid6_datap_recov() or raid6_2data_recov()) may have

		 *   changed some content of stripes which are not identical to

		 *   on-disk content any more, otherwise, a later write/recover

		 *   may steal stripe_pages from this rbio and end up with

		 *   corruptions or rebuild failures.

		 *

		 * - In case of single failure, where rbio->failb == -1:

		 *

		 *   Cache this rbio iff the above read reconstruction is

		 *   executed without problems.

/*

 * This is called only for stripes we've read from disk to

 * reconstruct the parity.

	/*

	 * we only read stripe pages off the disk, set them

	 * up to date if there were no errors

/*

 * reads everything we need off the disk to reconstruct

 * the parity. endio handlers trigger final reconstruction

 * when the IO is done.

 *

 * This is used both for reads from the higher layers and for

 * parity construction required to finish a rmw cycle.

	/*

	 * read everything that hasn't failed.  Thanks to the

	 * stripe cache, it is possible that some or all of these

	 * pages are going to be uptodate.

			/*

			 * the rmw code may have already read this

			 * page in

		/*

		 * we might have no bios to read just because the pages

		 * were up to date, or we might have no bios to read because

		 * the devices were gone.

	/*

	 * The bioc may be freed once we submit the last bio. Make sure not to

	 * touch it after that.

/*

 * the main entry point for reads from the higher layers.  This

 * is really only called when the normal read path had a failure,

 * so we assume the bio they send down corresponds to a failed part

 * of the drive.

	/*

	 * Loop retry:

	 * for 'mirror == 2', reconstruct from all other stripes.

	 * for 'mirror_num > 2', select a stripe to fail on every retry.

		/*

		 * 'mirror == 3' is to fail the p stripe and

		 * reconstruct from the q stripe.  'mirror > 3' is to

		 * fail a data stripe and reconstruct from p+q stripe.

	/*

	 * __raid56_parity_recover will end the bio with

	 * any errors it hits.  We don't want to return

	 * its error value up the stack because our caller

	 * will end up calling bio_endio with any nonzero

	 * return

	/*

	 * our rbio has been added to the list of

	 * rbios that will be handled after the

	 * currently lock owner is done

/*

 * The following code is used to scrub/replace the parity stripe

 *

 * Caller must have already increased bio_counter for getting @bioc.

 *

 * Note: We need make sure all the pages that add into the scrub/replace

 * raid bio are correct and not be changed during the scrub/replace. That

 * is those pages just hold metadata or file data with checksum.

	/*

	 * This is a special bio which is used to hold the completion handler

	 * and make the scrub rbio is similar to the other types

	/*

	 * After mapping bioc with BTRFS_MAP_WRITE, parities have been sorted

	 * to the end position, so this search can start from the first parity

	 * stripe.

 Now we just support the sectorsize equals to page size */

	/*

	 * We have already increased bio_counter when getting bioc, record it

	 * so we can free it at rbio_orig_end_io().

 Used for both parity scrub and missing. */

/*

 * We just scrub the parity that we have correct data on the same horizontal,

 * so we needn't allocate all pages for all the stripes.

	/*

	 * Because the higher layers(scrubber) are unlikely to

	 * use this area of the disk again soon, so don't cache

	 * it.

 RAID6, allocate and map temp space for the Q stripe */

 Map the parity stripe just once */

 first collect one page from each data stripe */

 RAID6, call the library function to fill in our P/Q */

 raid5 */

 Check scrubbing parity and repair it */

 Parity is right, needn't writeback */

	/*

	 * time to start writing.  Make bios for everything from the

	 * higher layers (the bio_list in our rbio) and our p/q.  Ignore

	 * everything else.

 Every parity is right */

/*

 * While we're doing the parity check and repair, we could have errors

 * in reading pages off the disk.  This checks for errors and if we're

 * not able to read the page it'll trigger parity reconstruction.  The

 * parity scrub will be finished after we've reconstructed the failed

 * stripes

		/*

		 * Because we can not use a scrubbing parity to repair

		 * the data, so the capability of the repair is declined.

		 * (In the case of RAID5, we can not repair anything)

		/*

		 * If all data is good, only parity is correctly, just

		 * repair the parity.

		/*

		 * Here means we got one corrupted data stripe and one

		 * corrupted parity on RAID6, if the corrupted parity

		 * is scrubbing parity, luckily, use the other one to repair

		 * the data, or we can not repair the data stripe.

/*

 * end io for the read phase of the rmw cycle.  All the bios here are physical

 * stripe bios we've read from the disk so we can recalculate the parity of the

 * stripe.

 *

 * This will usually kick off finish_rmw once all the bios are read in, but it

 * may trigger parity reconstruction if we had any errors along the way

	/*

	 * this will normally call finish_rmw to start our write

	 * but if there are any failed stripes we'll reconstruct

	 * from parity first

	/*

	 * build a list of bios to read all the missing parts of this

	 * stripe

			/*

			 * we want to find all the pages missing from

			 * the rbio and read them from the disk.  If

			 * page_in_rbio finds a page in the bio list

			 * we don't need to read it off the stripe.

			/*

			 * the bio cache may have handed us an uptodate

			 * page.  If so, be happy and use it

		/*

		 * this can happen if others have merged with

		 * us, it means there is nothing left to read.

		 * But if there are missing devices it may not be

		 * safe to do the full stripe write yet.

	/*

	 * The bioc may be freed once we submit the last bio. Make sure not to

	 * touch it after that.

 the actual write will happen once the reads are done */

 The following code is used for dev replace of a missing RAID 5/6 device. */

	/*

	 * This is a special bio which is used to hold the completion handler

	 * and make the scrub rbio is similar to the other types

	/*

	 * When we get bioc, we have already increased bio_counter, record it

	 * so we can free it at rbio_orig_end_io()

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/* returns NULL if the insertion worked, or it returns the node it did find

 * in the tree

/*

 * look for a given offset in the tree, and if it can't be found return the

 * first lesser offset

/*

 * look find the first ordered struct that has this offset, otherwise

 * the first one less than this offset

/*

 * Allocate and add a new ordered_extent into the per-inode tree.

 *

 * The tree is given a single reference on the ordered extent that was

 * inserted.

 For nocow write, we can release the qgroup rsv right now */

		/*

		 * The ordered extent has reserved qgroup space, release now

		 * and pass the reserved number for qgroup_record to free.

 one ref for the tree */

	/*

	 * We don't need the count_max_extents here, we can assume that all of

	 * that work has been done at higher layers, so this is truly the

	 * smallest the extent is going to get.

/*

 * Add a struct btrfs_ordered_sum into the list of checksums to be inserted

 * when an ordered extent is finished.  If the list covers more than one

 * ordered extent, it is split across multiples.

/*

 * Mark all ordered extents io inside the specified range finished.

 *

 * @page:	 The invovled page for the opeartion.

 *		 For uncompressed buffered IO, the page status also needs to be

 *		 updated to indicate whether the pending ordered io is finished.

 *		 Can be NULL for direct IO and compressed write.

 *		 For these cases, callers are ensured they won't execute the

 *		 endio function twice.

 * @finish_func: The function to be executed when all the IO of an ordered

 *		 extent are finished.

 *

 * This function is called for endio, thus the range must have ordered

 * extent(s) coveri it.

 No ordered extents at all */

		/*

		 * |<-- OE --->|  |

		 *		  cur

		 * Go to next OE.

 No more ordered extents, exit */

 Go to next ordered extent and continue */

		/*

		 * |	|<--- OE --->|

		 * cur

		 * Go to the start of OE.

		/*

		 * Now we are definitely inside one ordered extent.

		 *

		 * |<--- OE --->|

		 *	|

		 *	cur

			/*

			 * Ordered (Private2) bit indicates whether we still

			 * have pending io unfinished for the ordered extent.

			 *

			 * If there's no such bit, we need to skip to next range.

 Now we're fine to update the accounting */

		/*

		 * All the IO of the ordered extent is finished, we need to queue

		 * the finish_func to be executed.

/*

 * Finish IO for one ordered extent across a given range.  The range can only

 * contain one ordered extent.

 *

 * @cached:	 The cached ordered extent. If not NULL, we can skip the tree

 *               search and use the ordered extent directly.

 * 		 Will be also used to store the finished ordered extent.

 * @file_offset: File offset for the finished IO

 * @io_size:	 Length of the finish IO range

 *

 * Return true if the ordered extent is finished in the range, and update

 * @cached.

 * Return false otherwise.

 *

 * NOTE: The range can NOT cross multiple ordered extents.

 * Thus caller should ensure the range doesn't cross ordered extents.

		/*

		 * Ensure only one caller can set the flag and finished_ret

		 * accordingly

 test_and_set_bit implies a barrier */

/*

 * used to drop a reference on an ordered extent.  This will free

 * the extent if the last reference is dropped

/*

 * remove an ordered extent from the tree.  No references are dropped

 * and waiters are woken up.

 This is paired with btrfs_add_ordered_extent. */

	/*

	 * The current running transaction is waiting on us, we need to let it

	 * know that we're complete and wake it up.

		/*

		 * The checks for trans are just a formality, it should be set,

		 * but if it isn't we don't want to deref/assert under the spin

		 * lock, so be nice and check if trans is set, but ASSERT() so

		 * if it isn't set a developer will notice.

/*

 * wait for all the ordered extents in a root.  This is done when balancing

 * space between drives.

/*

 * Used to start IO or wait for a given ordered extent to finish.

 *

 * If wait is one, this effectively waits on page writeback for all the pages

 * in the extent, and it waits on the io completion code to insert

 * metadata into the btree corresponding to the extent

	/*

	 * pages in the range can be dirty, clean or writeback.  We

	 * start IO on any dirty ones so the wait doesn't stall waiting

	 * for the flusher thread to find them

/*

 * Used to wait on ordered extents across a large range of bytes.

	/* start IO across the range first to instantiate any delalloc

	 * extents

	/*

	 * If we have a writeback error don't return immediately. Wait first

	 * for any ordered extents that haven't completed yet. This is to make

	 * sure no one can dirty the same page ranges and call writepages()

	 * before the ordered extents complete - to avoid failures (-EEXIST)

	 * when adding the new ordered extents to the ordered tree.

		/*

		 * If the ordered extent had an error save the error but don't

		 * exit without waiting first for all other ordered extents in

		 * the range to complete.

/*

 * find an ordered extent corresponding to file_offset.  return NULL if

 * nothing is found, otherwise take a reference on the extent and return it

/* Since the DIO code tries to lock a wide area we need to look for any ordered

 * extents that exist in the range, rather than just the start of the range.

/*

 * Adds all ordered extents to the given list. The list ends up sorted by the

 * file_offset of the ordered extents.

/*

 * lookup and return any extent before 'file_offset'.  NULL is returned

 * if none is found

/*

 * Lookup the first ordered extent that overlaps the range

 * [@file_offset, @file_offset + @len).

 *

 * The difference between this and btrfs_lookup_first_ordered_extent() is

 * that this one won't return any ordered extent that does not overlap the range.

 * And the difference against btrfs_lookup_ordered_extent() is, this function

 * ensures the first ordered extent gets returned.

	/*

	 * Here we don't want to use tree_search() which will use tree->last

	 * and screw up the search order.

	 * And __tree_search() can't return the adjacent ordered extents

	 * either, thus here we do our own search.

			/*

			 * Direct hit, got an ordered extent that starts at

			 * @file_offset

 Empty tree */

 We got an entry around @file_offset, check adjacent entries */

 No ordered extent in the range */

/*

 * btrfs_flush_ordered_range - Lock the passed range and ensures all pending

 * ordered extents in it are run to completion.

 *

 * @inode:        Inode whose ordered tree is to be searched

 * @start:        Beginning of range to flush

 * @end:          Last byte of range to lock

 * @cached_state: If passed, will return the extent state responsible for the

 * locked range. It's the caller's responsibility to free the cached state.

 *

 * This function always returns with the given range locked, ensuring after it's

 * called no order extent can be pending.

			/*

			 * If no external cached_state has been passed then

			 * decrement the extra ref taken for cachedp since we

			 * aren't exposing it outside of this function

	/*

	 * The splitting extent is already counted and will be added again

	 * in btrfs_add_ordered_extent_*(). Subtract num_bytes to avoid

	 * double counting.

 Remove from tree once */

 Re-insert the node */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2011, 2012 STRATO.  All rights reserved.

/*

 * This is only the first step towards a full-features scrub. It reads all

 * extent and super block and verifies the checksums. In case a bad checksum

 * is found or the extent cannot be read, good data will be written back if

 * any can be found.

 *

 * Future enhancements:

 *  - In case an unrepairable extent is encountered, track which files are

 *    affected and report them

 *  - track and record media errors, throw out bad devices

 *  - add a mode to also read unallocated space

/*

 * the following three values only influence the performance.

 * The last one configures the number of parallel and outstanding I/O

 * operations. The first two values configure an upper limit for the number

 * of (dynamically allocated) pages that are added to a bio.

 128k per bio */

 128k per bio */

 8MB per device in flight */

/*

 * the following value times PAGE_SIZE needs to be large enough to match the

 * largest node/leaf/sector size that shall be supported.

 * Values larger than BTRFS_STRIPE_LEN are not supported.

 64k per node/leaf/sector */

 extent flags */

 free mem on transition to zero */

 also sets header_error */

 The following is for the data used to check parity */

 It is for the data with checksum */

 Used for the chunks with parity stripe such RAID5/6 */

 Work of parity check and repair */

 Mark the parity blocks which have data */

	/*

	 * Mark the parity blocks which have data, but errors happen when

	 * read data or check data

 State of IO submission throttling affecting the associated device */

 <= SCRUB_PAGES_PER_WR_BIO */

	/*

	 * statistics

	/*

	 * Use a ref counter to avoid use-after-free issues. Scrub workers

	 * decrement bios_in_flight and workers_pending and then do a wakeup

	 * on the list_wait wait queue. We must ensure the main scrub task

	 * doesn't free the scrub context before or while the workers are

	 * doing the wakeup() call.

/*

 * Insert new full stripe lock into full stripe locks tree

 *

 * Return pointer to existing or newly inserted full_stripe_lock structure if

 * everything works well.

 * Return ERR_PTR(-ENOMEM) if we failed to allocate memory

 *

 * NOTE: caller must hold full_stripe_locks_root->lock before calling this

 * function

	/*

	 * Insert new lock.

/*

 * Search for a full stripe lock of a block group

 *

 * Return pointer to existing full stripe lock if found

 * Return NULL if not found

/*

 * Helper to get full stripe logical from a normal bytenr.

 *

 * Caller must ensure @cache is a RAID56 block group.

	/*

	 * Due to chunk item size limit, full stripe length should not be

	 * larger than U32_MAX. Just a sanity check here.

	/*

	 * round_down() can only handle power of 2, while RAID56 full

	 * stripe length can be 64KiB * n, so we need to manually round down.

/*

 * Lock a full stripe to avoid concurrency of recovery and read

 *

 * It's only used for profiles with parities (RAID5/6), for other profiles it

 * does nothing.

 *

 * Return 0 if we locked full stripe covering @bytenr, with a mutex held.

 * So caller must call unlock_full_stripe() at the same context.

 *

 * Return <0 if encounters error.

 Profiles not based on parity don't need full stripe lock */

 Now insert the full stripe lock */

/*

 * Unlock a full stripe.

 *

 * NOTE: Caller must ensure it's the same context calling corresponding

 * lock_full_stripe().

 *

 * Return 0 if we unlock full stripe without problem.

 * Return <0 for error

 If we didn't acquire full stripe lock, no need to continue */

 Unpaired unlock_full_stripe() detected */

 this can happen when scrub is cancelled */

	/*

	 * this makes the path point to (inum INODE_ITEM ioff)

	/*

	 * init_path might indirectly call vmalloc, or use GFP_KERNEL. Scrub

	 * uses GFP_NOFS in this context, so we keep it consistent but it does

	 * not seem to be strictly necessary.

	/*

	 * we deliberately ignore the bit ipath might have been too small to

	 * hold all of the paths here

/*

 * scrub_handle_errored_block gets called when either verification of the

 * pages failed or the bio failed to read, e.g. with EIO. In the latter

 * case, this function handles all pages in the bio, even though only one

 * may be bad.

 * The goal of this function is to repair the errored block by using the

 * contents of one of the mirrors.

 holds one for each mirror */

		/*

		 * if we find an error in a super block, we just report it.

		 * They will get written with the next transaction commit

		 * anyway

	/*

	 * We must use GFP_NOFS because the scrub task might be waiting for a

	 * worker task executing this function and in turn a transaction commit

	 * might be waiting the scrub task to pause (which needs to wait for all

	 * the worker tasks to complete before pausing).

	 * We do allocations in the workers through insert_full_stripe_lock()

	 * and scrub_add_page_to_wr_bio(), which happens down the call chain of

	 * this function.

	/*

	 * For RAID5/6, race can happen for a different device scrub thread.

	 * For data corruption, Parity and Data threads will both try

	 * to recovery the data.

	 * Race can lead to doubly added csum error, or even unrecoverable

	 * error.

	/*

	 * read all mirrors one after the other. This includes to

	 * re-read the extent or metadata block that failed (that was

	 * the cause that this fixup code is called) another time,

	 * sector by sector this time in order to know which sectors

	 * caused I/O errors and which ones are good (for all mirrors).

	 * It is the goal to handle the situation when more than one

	 * mirror contains I/O errors, but the errors do not

	 * overlap, i.e. the data can be repaired by selecting the

	 * sectors from those mirrors without I/O error on the

	 * particular sectors. One example (with blocks >= 2 * sectorsize)

	 * would be that mirror #1 has an I/O error on the first sector,

	 * the second sector is good, and mirror #2 has an I/O error on

	 * the second sector, but the first sector is good.

	 * Then the first sector of the first mirror can be repaired by

	 * taking the first sector of the second mirror, and the

	 * second sector of the second mirror can be repaired by

	 * copying the contents of the 2nd sector of the 1st mirror.

	 * One more note: if the sectors of one mirror contain I/O

	 * errors, the checksum cannot be verified. In order to get

	 * the best data for repairing, the first attempt is to find

	 * a mirror without I/O errors and with a validated checksum.

	 * Only if this is not possible, the sectors are picked from

	 * mirrors with I/O errors without considering the checksum.

	 * If the latter is the case, at the end, the checksum of the

	 * repaired area is verified in order to correctly maintain

	 * the statistics.

 setup the context, map the logical blocks and alloc the pages */

 build and submit the bios for the failed mirror, check checksums */

		/*

		 * the error disappeared after reading page by page, or

		 * the area was part of a huge bio and other parts of the

		 * bio caused I/O errors, or the block layer merged several

		 * read requests into one and the error is caused by a

		 * different bio (usually one of the two latter cases is

		 * the cause)

	/*

	 * now build and submit the bios for the other mirrors, check

	 * checksums.

	 * First try to pick the mirror which is completely without I/O

	 * errors and also does not have a checksum error.

	 * If one is found, and if a checksum is present, the full block

	 * that is known to contain an error is rewritten. Afterwards

	 * the block is known to be corrected.

	 * If a mirror is found which is completely correct, and no

	 * checksum is present, only those pages are rewritten that had

	 * an I/O error in the block to be repaired, since it cannot be

	 * determined, which copy of the other pages is better (and it

	 * could happen otherwise that a correct page would be

	 * overwritten by a bad one).

 raid56's mirror can be more than BTRFS_MAX_MIRRORS */

 build and submit the bios, check checksums */

	/*

	 * In case of I/O errors in the area that is supposed to be

	 * repaired, continue by picking good copies of those sectors.

	 * Select the good sectors from mirrors to rewrite bad sectors from

	 * the area to fix. Afterwards verify the checksum of the block

	 * that is supposed to be repaired. This verification step is

	 * only done for the purpose of statistic counting and for the

	 * final scrub report, whether errors remain.

	 * A perfect algorithm could make use of the checksum and try

	 * all possible combinations of sectors from the different mirrors

	 * until the checksum verification succeeds. For example, when

	 * the 2nd sector of mirror #1 faces I/O errors, and the 2nd sector

	 * of mirror #2 is readable but the final checksum test fails,

	 * then the 2nd sector of mirror #3 could be tried, whether now

	 * the final checksum succeeds. But this would be a rare

	 * exception and is therefore not implemented. At least it is

	 * avoided that the good copy is overwritten.

	 * A more useful improvement would be to pick the sectors

	 * without I/O error based on sector sizes (512 bytes on legacy

	 * disks) instead of on sectorsize. Then maybe 512 byte of one

	 * mirror could be repaired by taking 512 byte of a different

	 * mirror, even if other 512 byte sectors in the same sectorsize

	 * area are unreadable.

 skip no-io-error page in scrub */

			/*

			 * In case of dev replace, if raid56 rebuild process

			 * didn't work out correct data, then copy the content

			 * in sblock_bad to make sure target device is identical

			 * to source device, instead of writing garbage data in

			 * sblock_for_recheck array to target device.

 try to find no-io-error page in mirrors */

			/*

			 * did not find a mirror to fetch the page

			 * from. scrub_write_page_to_dev_replace()

			 * handles this case (page->io_error), by

			 * filling the block with zeros before

			 * submitting the write request

			/*

			 * need to verify the checksum now that all

			 * sectors on disk are repaired (the write

			 * request for data to be repaired is on its way).

			 * Just be lazy and use scrub_recheck_block()

			 * which re-reads the data before the checksum

			 * is verified, but most likely the data comes out

			 * of the page cache.

 RAID5/6 */

 The other RAID type */

	/*

	 * note: the two members refs and outstanding_pages

	 * are not used (and not set) in the blocks that are used for

	 * the recheck procedure

		/*

		 * With a length of sectorsize, each returned stripe represents

		 * one mirror

 for missing devices, dev->bdev is NULL */

 All pages in sblock belong to the same stripe on the same device. */

/*

 * this function will check the on disk data for checksum errors, header

 * errors and read I/O errors. If any I/O errors happen, the exact pages

 * which are errored are marked as being bad. The goal is to enable scrub

 * to take those pages that are not errored from all the mirrors so that

 * the pages that are errored in the just handled mirror can be repaired.

 short cut for raid56 */

	/*

	 * This block is used for the check of the parity on the source device,

	 * so the data needn't be written into the destination device.

	/* process all writes in a single worker thread. Then the block layer

	 * orders the requests before sending them to the driver which

	 * doubled the write performance on spinning disks when measured

	/*

	 * No need to initialize these stats currently,

	 * because this function only use return value

	 * instead of these stats value.

	 *

	 * Todo:

	 * always use stats

	/*

	 * In scrub_pages() and scrub_pages_for_parity() we ensure each spage

	 * only contains one sector of data.

	/*

	 * This is done in sectorsize steps even for metadata as there's a

	 * constraint for nodesize to be aligned to sectorsize. This will need

	 * to change so we don't misuse data and metadata units like that.

 Each member in pagev is just one block, not a full page */

	/*

	 * we don't use the getter functions here, as we

	 * a) don't have an extent buffer and

	 * b) the page is already kmapped

		/*

		 * if we find an error in a super block, we just report it.

		 * They will get written with the next transaction commit

		 * anyway

/*

 * Throttling of IO submission, bandwidth-limit based, the timeslice is 1

 * second.  Limit can be set via /sys/fs/UUID/devinfo/devid/scrub_speed_max.

	/*

	 * Slice is divided into intervals when the IO is submitted, adjust by

	 * bwlimit and maximum of 64 intervals.

 Start new epoch, set deadline */

 Still in the time to send? */

 If current bio is within the limit, send it */

 We're over the limit, sleep until the rest of the slice */

 New request after deadline, start new epoch */

 Next call will start the deadline period */

	/*

	 * grab a fresh bio or wait for one to become available

 one for the page added to the bio */

		/*

		 * We shouldn't be scrubbing a missing device. Even for dev

		 * replace, we should only get here for RAID 5/6. We either

		 * managed to mount something with no mirrors remaining or

		 * there's a bug in scrub_remap_extent()/btrfs_map_block().

	/* one ref inside this function, plus one for each page added to

		/*

		 * Here we will allocate one page for one sector to scrub.

		 * This is fine if PAGE_SIZE == sectorsize, but will cost

		 * more memory for PAGE_SIZE > sectorsize case.

		/*

		 * This case should only be hit for RAID 5/6 device replace. See

		 * the comment in scrub_missing_raid56_pages() for details.

 last one frees, either here or in bio completion for last page */

 now complete the scrub_block items that have all pages completed */

		/*

		 * if has checksum error, write via repair mechanism in

		 * dev replace case, otherwise write here in dev replace

		 * case.

/*

 * Find the desired csum for range [logical, logical + sectorsize), and store

 * the csum into @csum.

 *

 * The search source is sctx->csum_list, which is a pre-populated list

 * storing bytenr ordered csum ranges.  We're responsible to cleanup any range

 * that is before @logical.

 *

 * Return 0 if there is no csum for the range.

 * Return 1 if there is csum for the range and copied to @csum.

 The current csum range is beyond our range, no csum found */

		/*

		 * The current sum is before our bytenr, since scrub is always

		 * done in bytenr order, the csum will never be used anymore,

		 * clean it up so that later calls won't bother with the range,

		 * and continue search the next range.

 Now the csum range covers our bytenr, copy the csum */

 Cleanup the range if we're at the end of the csum range */

 scrub extent tries to collect up to 64 kB for each bio */

 push csums to sbio */

	/* one ref inside this function, plus one for each page added to

 For scrub block */

 For scrub parity */

 Iterate over the stripe range in sectorsize steps */

 last one frees, either here or in bio completion for last page */

 push csums to sbio */

/*

 * Given a physical address, this will calculate it's

 * logical offset. if this is a parity stripe, it will return

 * the most left data stripe's logical offset.

 *

 * return 0 if it is a data stripe, 1 means parity stripe.

 Work out the disk rotation on this stripe-set */

 calculate which stripe this data locates */

 Check the comment in scrub_stripe() for why u32 is enough here */

	/*

	 * Unlike chunk length, extent length should never go beyond

	 * BTRFS_MAX_EXTENT_SIZE, thus u32 is enough here.

	/*

	 * work on commit root. The related disk blocks are static as

	 * long as COW is applied. This means, it is save to rewrite

	 * them to repair disk errors without any race conditions

	/*

	 * trigger the readahead for extent tree csum tree and wait for

	 * completion. During readahead, the scrub is officially paused

	 * to not hold off transaction commits

 FIXME it might be better to start readahead at commit root */

	/*

	 * collect all data csums for the stripe to avoid seeking during

	 * the scrub. This might currently (crc32) end up to be about 1MB

	/*

	 * now find all extents for each stripe and scrub them

		/*

		 * canceled?

		/*

		 * check to see if we have to pause

 push queued extents */

 it is parity strip */

				/* there's no smaller item, so stick with the

 out of this device extent */

			/*

			 * If our block group was removed in the meanwhile, just

			 * stop scrubbing since there is no point in continuing.

			 * Continuing would prevent reusing its device extents

			 * for new block groups for a long time.

			/*

			 * trim extent to this stripe

					/*

					 * loop until we find next data stripe

					 * or we have finished all stripes.

 push queued extents */

		/*

		 * Might have been an unused block group deleted by the cleaner

		 * kthread or relocation.

		/*

		 * get a reference on the corresponding block group to prevent

		 * the chunk from going away while we scrub it

		/* some chunks are removed but not committed to disk yet,

		/*

		 * Make sure that while we are scrubbing the corresponding block

		 * group doesn't get its logical address and its device extents

		 * reused for another block group, which can possibly be of a

		 * different type and different profile. We do this to prevent

		 * false error detections and crashes due to bogus attempts to

		 * repair extents.

		/*

		 * we need call btrfs_inc_block_group_ro() with scrubs_paused,

		 * to avoid deadlock caused by:

		 * btrfs_inc_block_group_ro()

		 * -> btrfs_wait_for_commit()

		 * -> btrfs_commit_transaction()

		 * -> btrfs_scrub_pause()

		/*

		 * Don't do chunk preallocation for scrub.

		 *

		 * This is especially important for SYSTEM bgs, or we can hit

		 * -EFBIG from btrfs_finish_chunk_alloc() like:

		 * 1. The only SYSTEM bg is marked RO.

		 *    Since SYSTEM bg is small, that's pretty common.

		 * 2. New SYSTEM bg will be allocated

		 *    Due to regular version will allocate new chunk.

		 * 3. New SYSTEM bg is empty and will get cleaned up

		 *    Before cleanup really happens, it's marked RO again.

		 * 4. Empty SYSTEM bg get scrubbed

		 *    We go back to 2.

		 *

		 * This can easily boost the amount of SYSTEM chunks if cleaner

		 * thread can't be triggered fast enough, and use up all space

		 * of btrfs_super_block::sys_chunk_array

		 *

		 * While for dev replace, we need to try our best to mark block

		 * group RO, to prevent race between:

		 * - Write duplication

		 *   Contains latest data

		 * - Scrub copy

		 *   Contains data from commit tree

		 *

		 * If target block group is not marked RO, nocow writes can

		 * be overwritten by scrub copy, causing data corruption.

		 * So for dev-replace, it's not allowed to continue if a block

		 * group is not RO.

			/*

			 * btrfs_inc_block_group_ro return -ENOSPC when it

			 * failed in creating new chunk for metadata.

			 * It is not a problem for scrub, because

			 * metadata are always cowed, and our scrub paused

			 * commit_transactions.

		/*

		 * Now the target block is marked RO, wait for nocow writes to

		 * finish before dev-replace.

		 * COW is fine, as COW never overwrites extents in commit tree.

		/*

		 * flush, submit all pending read and write bios, afterwards

		 * wait for them.

		 * Note that in the dev replace case, a read request causes

		 * write requests that are submitted in the read completion

		 * worker. Therefore in the current situation, it is required

		 * that all write requests are flushed, so that all read and

		 * write requests are really completed when bios_in_flight

		 * changes to 0.

		/*

		 * must be called before we decrease @scrub_paused.

		 * make sure we don't block transaction commit while

		 * we are waiting pending workers finished.

		/*

		 * We might have prevented the cleaner kthread from deleting

		 * this block group if it was already unused because we raced

		 * and set it to RO mode first. So add it back to the unused

		 * list, otherwise it might not ever be deleted unless a manual

		 * balance is triggered or it becomes used and unused again.

 Seed devices of a new filesystem has their own generation. */

/*

 * get a reference count on fs_info->scrub_workers. start worker if necessary

 Other thread raced in and created the workers for us */

		/*

		 * in this case scrub is unable to calculate the checksum

		 * the way scrub is implemented. Do not handle this

		 * situation at all because it won't ever happen.

		/*

		 * would exhaust the array bounds of pagev member in

		 * struct scrub_block

 Allocate outside of device_list_mutex */

	/*

	 * checking @scrub_pause_req here, we can avoid

	 * race between committing transaction and scrubbing.

	/*

	 * In order to avoid deadlock with reclaim when there is a transaction

	 * trying to pause scrub, make sure we use GFP_NOFS for all the

	 * allocations done at btrfs_scrub_pages() and scrub_pages_for_parity()

	 * invoked by our callees. The pausing request is done when the

	 * transaction commit starts, and it blocks the transaction until scrub

	 * is paused (done at specific points at scrub_stripe() or right above

	 * before incrementing fs_info->scrubs_running).

		/*

		 * by holding device list mutex, we can

		 * kick off writing super in log tree sync.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2014 Filipe David Borba Manana <fdmanana@gmail.com>

 Reset to defaults */

 Set NOCOMPRESS flag */

		/*

		 * This is not strictly necessary as the property should be

		 * valid, but in case it isn't, don't propagate it further.

		/*

		 * Currently callers should be reserving 1 item for properties,

		 * since we only have 1 property that we currently support.  If

		 * we add more in the future we need to try and reserve more

		 * space for them.  But we should also revisit how we do space

		 * reservations if we do add more properties in the future.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

	/*

	 * Search all extended backrefs in this item. We're only

	 * looking through any collisions so most of the time this is

	 * just going to compare against one buffer. If all is well,

	 * we'll return success and the inode ref object.

 Returns NULL if no extref found */

	/*

	 * Sanity check - did we find the right item for this name?

	 * This should always succeed so error here will make the FS

	 * readonly.

		/*

		 * Common case only one ref in the item, remove the

		 * whole item.

		/*

		 * No refs were found, or we could not find the

		 * name in our ref array. Find and remove the extended

		 * inode ref then.

/*

 * btrfs_insert_inode_extref() - Inserts an extended inode ref into a tree.

 *

 * The caller must have checked against BTRFS_LINK_MAX already.

 Will return 0, -ENOMEM, -EMLINK, or -EEXIST or anything from the CoW path */

		/* We ran out of space in the ref array. Need to

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) Qu Wenruo 2017.  All rights reserved.

/*

 * The module is used to catch unexpected/corrupted tree block data.

 * Such behavior can be caused either by a fuzzed image or bugs.

 *

 * The objective is to do leaf/node validation checks when tree block is read

 * from disk, and check *every* possible member, so other code won't

 * need to checking them again.

 *

 * Due to the potential and unwanted damage, every checker needs to be

 * carefully reviewed otherwise so it does not prevent mount of valid images.

/*

 * Error message should follow the following format:

 * corrupt <type>: <identifier>, <reason>[, <bad_value>]

 *

 * @type:	leaf or node

 * @identifier:	the necessary info to locate the leaf/node.

 * 		It's recommended to decode key.objecitd/offset if it's

 * 		meaningful.

 * @reason:	describe the error

 * @bad_value:	optional, it's recommended to output bad value and its

 *		expected value (range).

 *

 * Since comma is used to separate the components, only space is allowed

 * inside each component.

/*

 * Append generic "corrupt leaf/node root=%llu block=%llu slot=%d: " to @fmt.

 * Allows callers to customize the output.

/*

 * Customized reporter for extent data item, since its key objectid and

 * offset has its own meaning.

/*

 * Return 0 if the btrfs_file_extent_##name is aligned to @alignment

 * Else return 1

/*

 * Customized report for dir_item, the only new important information is

 * key->objectid, which represents inode number

/*

 * This functions checks prev_key->objectid, to ensure current key and prev_key

 * share the same objectid as inode number.

 *

 * This is to detect missing INODE_ITEM in subvolume trees.

 *

 * Return true if everything is OK or we don't need to check.

 * Return false if anything is wrong.

 No prev key, skip check */

 Only these key->types needs to be checked */

	/*

	 * Only subvolume trees along with their reloc trees need this check.

	 * Things like log tree doesn't follow this ino requirement.

 Error found */

	/*

	 * Previous key must have the same key->objectid (ino).

	 * It can be XATTR_ITEM, INODE_ITEM or just another EXTENT_DATA.

	 * But if objectids mismatch, it means we have a missing

	 * INODE_ITEM.

	/*

	 * Make sure the item contains at least inline header, so the file

	 * extent type is not some garbage.

	/*

	 * Support for new compression/encryption must introduce incompat flag,

	 * and must be caught in open_ctree().

 Inline extent must have 0 as key offset */

 Compressed inline extent has no on-disk size, skip it */

 Uncompressed inline extent size must match item size */

 Regular or preallocated extent has fixed item size */

 Catch extent end overflow */

	/*

	 * Check that no two consecutive file extent items, in the same leaf,

	 * present ranges that overlap each other.

 Inode item error output has the same format as dir_item_err() */

 For XATTR_ITEM, location key should be all 0 */

 No such tree id */

 DIR_ITEM/INDEX/INODE_REF is not allowed to point to non-fs trees */

	/*

	 * ROOT_ITEM with non-zero offset means this is a snapshot, created at

	 * @offset transid.

	 * Furthermore, for location key in DIR_ITEM, its offset is always -1.

	 *

	 * So here we only check offset for reloc tree whose key->offset must

	 * be a valid tree.

 header itself should not cross item boundary */

 Location key check */

 dir type check */

 Name/data length check */

 header and name/data should not cross item boundary */

		/*

		 * Special check for XATTR/DIR_ITEM, as key->offset is name

		 * hash, should match its name

	/*

	 * Here we don't really care about alignment since extent allocator can

	 * handle it.  We care more about the size.

 Only superblock eb is able to have such small offset */

		/*

		 * Get the slot number by iterating through all slots, this

		 * would provide better readability.

/*

 * The common chunk check which could also work on super block sys chunk array.

 *

 * Return -EUCLEAN if anything is corrupted.

 * Return 0 if everything is OK.

/*

 * Enhanced version of chunk item checker.

 *

 * The common btrfs_check_chunk_valid() doesn't check item size since it needs

 * to work on super block sys_chunk_array which doesn't have full item ptr.

 Let btrfs_check_chunk_valid() handle this error type */

	/*

	 * For device total_bytes, we don't have reliable way to check it, as

	 * it can be 0 for device removal. Device size check can only be done

	 * by dev extents check.

	/*

	 * Remaining members like io_align/type/gen/dev_group aren't really

	 * utilized.  Skip them to make later usage of them easier.

 Here we use super block generation + 1 to handle log tree */

 Note for ROOT_TREE_DIR_ITEM, mkfs could set its transid 0 */

	/*

	 * For size and nbytes it's better not to be too strict, as for dir

	 * item its size/nbytes can easily get wrong, but doesn't affect

	 * anything in the fs. So here we skip the check.

	/*

	 * S_IFMT is not bit mapped so we can't completely rely on

	 * is_power_of_2/has_single_bit_set, but it can save us from checking

	 * FIFO/CHR/DIR/REG.  Only needs to check BLK, LNK and SOCKS

	/*

	 * For legacy root item, the members starting at generation_v2 will be

	 * all filled with 0.

	 * And since we allow geneartion_v2 as 0, it will still pass the check.

 Generation related */

 Alignment and level check */

 Flags check */

 Current pointer inside inline refs */

 Extent item end */

 Total refs in btrfs_extent_item */

 found total inline refs */

 key->objectid is the bytenr for both key types */

 key->offset is tree level for METADATA_ITEM_KEY */

	/*

	 * EXTENT/METADATA_ITEM consists of:

	 * 1) One btrfs_extent_item

	 *    Records the total refs, type and generation of the extent.

	 *

	 * 2) One btrfs_tree_block_info (for EXTENT_ITEM and tree backref only)

	 *    Records the first key and level of the tree block.

	 *

	 * 2) Zero or more btrfs_extent_inline_ref(s)

	 *    Each inline ref has one btrfs_extent_inline_ref shows:

	 *    2.1) The ref type, one of the 4

	 *         TREE_BLOCK_REF	Tree block only

	 *         SHARED_BLOCK_REF	Tree block only

	 *         EXTENT_DATA_REF	Data only

	 *         SHARED_DATA_REF	Data only

	 *    2.2) Ref type specific data

	 *         Either using btrfs_extent_inline_ref::offset, or specific

	 *         data structure.

 Checks against extent_item */

 Check the special case of btrfs_tree_block_info */

 Check inline refs */

 inline_offset is subvolid of the owner, no need to check */

 Contains parent bytenr */

		/*

		 * Contains owner subvolid, owner key objectid, adjusted offset.

		 * The only obvious corruption can happen in that offset.

 Contains parent bytenr and ref count */

 No padding is allowed */

 Finally, check the inline refs against total refs */

		/*

		 * We cannot check the extent_data_ref hash due to possible

		 * overflow from the leaf due to hash collisions.

 namelen can't be 0, so item_size == sizeof() is also invalid */

		/*

		 * NOTE: In theory we should record all found index numbers

		 * to find any duplicated indexes, but that will be too time

		 * consuming for inodes with too many hard links.

/*

 * Common point to switch the item-specific validation.

 No valid key type is 0, so all key should be larger than this key */

	/*

	 * Extent buffers from a relocation tree have a owner field that

	 * corresponds to the subvolume tree they are based on. So just from an

	 * extent buffer alone we can not find out what is the id of the

	 * corresponding subvolume tree, so we can not figure out if the extent

	 * buffer corresponds to the root of the relocation tree or not. So

	 * skip this check for relocation trees.

 These trees must never be empty */

 Unknown tree */

	/*

	 * Check the following things to make sure this is a good leaf, and

	 * leaf users won't need to bother with similar sanity checks:

	 *

	 * 1) key ordering

	 * 2) item offset and size

	 *    No overlap, no hole, all inside the leaf.

	 * 3) item content

	 *    If possible, do comprehensive sanity check.

	 *    NOTE: All checks must only rely on the item data itself.

 Make sure the keys are in the right order */

		/*

		 * Make sure the offset and ends are right, remember that the

		 * item data starts at the end of the leaf and grows towards the

		 * front.

		/*

		 * Check to make sure that we don't point outside of the leaf,

		 * just in case all the items are consistent to each other, but

		 * all point outside of the leaf.

 Also check if the item pointer overlaps with btrfs item. */

			/*

			 * Check if the item size and content meet other

			 * criteria

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2008 Oracle.  All rights reserved.

/*

 * Btrfs LZO compression format

 *

 * Regular and inlined LZO compressed data extents consist of:

 *

 * 1.  Header

 *     Fixed size. LZO_LEN (4) bytes long, LE32.

 *     Records the total size (including the header) of compressed data.

 *

 * 2.  Segment(s)

 *     Variable size. Each segment includes one segment header, followed by data

 *     payload.

 *     One regular LZO compressed extent can have one or more segments.

 *     For inlined LZO compressed extent, only one segment is allowed.

 *     One segment represents at most one sector of uncompressed data.

 *

 * 2.1 Segment header

 *     Fixed size. LZO_LEN (4) bytes long, LE32.

 *     Records the total size of the segment (not including the header).

 *     Segment header never crosses sector boundary, thus it's possible to

 *     have at most 3 padding zeros at the end of the sector.

 *

 * 2.2 Data Payload

 *     Variable size. Size up limit should be lzo1x_worst_compress(sectorsize)

 *     which is 4419 for a 4KiB sectorsize.

 *

 * Example with 4K sectorsize:

 * Page 1:

 *          0     0x2   0x4   0x6   0x8   0xa   0xc   0xe     0x10

 * 0x0000   |  Header   | SegHdr 01 | Data payload 01 ...     |

 * ...

 * 0x0ff0   | SegHdr  N | Data payload  N     ...          |00|

 *                                                          ^^ padding zeros

 * Page 2:

 * 0x1000   | SegHdr N+1| Data payload N+1 ...                |

 where decompressed data goes */

 where compressed data goes */

/*

 * Will do:

 *

 * - Write a segment header into the destination

 * - Copy the compressed buffer into the destination

 * - Make sure we have enough space in the last sector to fit a segment header

 *   If not, we will pad at most (LZO_LEN (4)) - 1 bytes of zeros.

 *

 * Will allocate new pages when needed.

	/*

	 * We never allow a segment header crossing sector boundary, previous

	 * run should ensure we have enough space left inside the sector.

 Allocate a new page */

 Copy compressed data */

 Allocate a new page */

	/*

	 * Check if we can fit the next segment header into the remaining space

	 * of the sector.

 The remaining size is not enough, pad it with zeros */

 Points to the file offset of input data */

 Points to the current output byte */

	/*

	 * Skip the header for now, we will later come back and write the total

	 * compressed size

 Get the input page first */

 Compress at most one sector of data each time */

		/*

		 * Check if we're making it bigger after two sectors.  And if

		 * it is so, give up.

 Check if we have reached page boundary */

 Store the size of all chunks of compressed data */

/*

 * Copy the compressed segment payload into @dest.

 *

 * For the payload there will be no padding, just need to do page switching.

 Compressed data length, can be unaligned */

 Offset inside the compressed data */

 Bytes decompressed so far */

	/*

	 * LZO header length check

	 *

	 * The total length should not exceed the maximum extent length,

	 * and all sectors should be used.

	 * If this happens, it means the compressed extent is corrupted.

 Go through each lzo segment */

 Length of the compressed segment */

		/*

		 * We should always have enough space for one segment header

		 * inside current sector.

 Copy the compressed segment payload into workspace */

 Decompress the data */

 Copy the data into inode pages */

 All data read, exit */

 Check if the sector has enough space for a segment header */

 Skip the padding zeros */

	/*

	 * the caller is already checking against PAGE_SIZE, but lets

	 * move this check closer to the memcpy/memset

	/*

	 * btrfs_getblock is doing a zero on the tail of the page too,

	 * but this will cover anything missing from the decompressed

	 * data.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007,2008 Oracle.  All rights reserved.

	/*

	 * csum type is validated at mount time

 csum type is validated at mount time */

/*

 * Return driver name if defined, otherwise the name that's also a valid driver

 * name

 csum type is validated at mount time */

 this also releases the path */

/*

 * path release drops references on the extent buffers in the path

 * and it drops any locks held by this path

 *

 * It is safe to call this on paths that no locks or extent buffers held.

/*

 * safely gets a reference on the root node of a tree.  A lock

 * is not taken, so a concurrent writer may put a different node

 * at the root of the tree.  See btrfs_lock_root_node for the

 * looping required.

 *

 * The extent buffer returned by this has a reference taken, so

 * it won't disappear.  It may stop being the root of the tree

 * at any time because there are no locks held.

		/*

		 * RCU really hurts here, we could free up the root node because

		 * it was COWed but we may not get the new root node yet so do

		 * the inc_not_zero dance and if it doesn't work then

		 * synchronize_rcu and try again.

/*

 * Cowonly root (not-shareable trees, everything not subvolume or reloc roots),

 * just get put onto a simple dirty list.  Transaction walks this list to make

 * sure they get properly updated on disk.

 Want the extent tree to be the last on the list */

/*

 * used by snapshot creation to make a copy of a root for a tree with

 * a given objectid.  The buffer with the new root node is returned in

 * cow_ret, and this func returns zero on success or a negative error code.

/*

 * check if the tree block can be shared by multiple trees

	/*

	 * Tree blocks not in shareable trees and tree roots are never shared.

	 * If a block was allocated after the last snapshot and the block was

	 * not allocated by tree relocation, we know the block is not shared.

	/*

	 * Backrefs update rules:

	 *

	 * Always use full backrefs for extent pointers in tree block

	 * allocated by tree relocation.

	 *

	 * If a shared tree block is no longer referenced by its owner

	 * tree (btrfs_header_owner(buf) == root->root_key.objectid),

	 * use full backrefs for extent pointers in tree block.

	 *

	 * If a tree block is been relocating

	 * (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID),

	 * use full backrefs for extent pointers in tree block.

	 * The reason for this is some operations (such as drop tree)

	 * are only allowed for blocks use full backrefs.

/*

 * does the dirty work in cow of a single block.  The parent block (if

 * supplied) is updated to point to the new cow copy.  The new buffer is marked

 * dirty and returned locked.  If you modify the block it needs to be marked

 * dirty again.

 *

 * search_start -- an allocation hint for the new block

 *

 * empty_size -- a hint that you plan on doing more cow.  This is the size in

 * bytes the allocator should try to find free next to the block it returns.

 * This is just a hint and may be ignored by the allocator.

 cow is set to blocking by btrfs_init_new_buffer */

 Ensure we can see the FORCE_COW bit */

	/*

	 * We do not need to cow a block if

	 * 1) this block is not created or changed in this transaction;

	 * 2) this block does not belong to TREE_RELOC tree;

	 * 3) the root is not forced COW.

	 *

	 * What is forced COW:

	 *    when we create snapshot during committing the transaction,

	 *    after we've finished copying src root, we must COW the shared

	 *    block to ensure the metadata consistency.

/*

 * cows a single block, see __btrfs_cow_block for the real work.

 * This version of it has extra checks so that a block isn't COWed more than

 * once per transaction, as long as it hasn't been written yet

	/*

	 * Before CoWing this block for later modification, check if it's

	 * the subtree root and do the delayed subtree trace if needed.

	 *

	 * Also We don't care about the error, as it's handled internally.

/*

 * helper function for defrag to decide if two blocks pointed to by a

 * node are actually close by

/*

 * Compare two keys, on little-endian the disk order is same as CPU order and

 * we can avoid the conversion.

/*

 * compare two keys in a memcmp fashion

/*

 * same as comp_keys only with two btrfs_key's

/*

 * this is used by the defrag code to go through all the

 * leaves pointed to by a node and reallocate them so that

 * disk order is close to key order

/*

 * search for key in the extent_buffer.  The items start at offset p,

 * and they are item_size apart.

 *

 * the slot in the array is returned via slot, and it points to

 * the place where you would insert key if it is not found in

 * the array.

 *

 * Slot may point to total number of items if the key is bigger than

 * all of the keys

/*

 * simple bin_search frontend that does the right thing for

 * leaves vs nodes

/* given a node and slot number, this reads the blocks it points to.  The

 * extent buffer is returned with a reference taken (but unlocked).

/*

 * node level balancing, used to make sure nodes are in proper order for

 * item deletion.  We balance from the top down, so we have to make sure

 * that a deletion won't leave an node completely empty later on.

	/*

	 * deal with the case where there is only one pointer in the root

	 * by promoting the node below to a root

 promote the child to a root */

 once for the path */

 once for the root ptr */

 first, try to make some room in the middle buffer */

	/*

	 * then try to empty the right most buffer into the middle

		/*

		 * we're not allowed to leave a node with one item in the

		 * tree during a delete.  A deletion from lower in the tree

		 * could try to delete the only pointer in this node.

		 * So, pull some keys from the left.

		 * There has to be a left pointer at this point because

		 * otherwise we would have pulled some pointers from the

		 * right

 update the parent key to reflect our changes */

 update the path */

 left was locked after cow */

 double check we haven't messed things up */

/* Node balancing for insertion.  Here we only split or push nodes around

 * when they are completely full.  This is also done top down, so we

 * have to be pessimistic.

 first, try to make some room in the middle buffer */

	/*

	 * then try to empty the right most buffer into the middle

/*

 * readahead one full node of leaves, finding things that are close

 * to the block in 'slot', and triggering ra on them.

	/*

	 * Since the time between visiting leaves is much shorter than the time

	 * between visiting nodes, limit read ahead of nodes to 1, to avoid too

	 * much IO at once (possibly random).

/*

 * when we walk down the tree, it is usually safe to unlock the higher layers

 * in the tree.  The exceptions are when our path goes through slot 0, because

 * operations on the tree might require changing key pointers higher up in the

 * tree.

 *

 * callers might also have set path->keep_locks, which tells this code to keep

 * the lock if the path points to the last slot in the block.  This is part of

 * walking through the tree, and selecting the next slot in the higher block.

 *

 * lowest_unlock sets the lowest level in the tree we're allowed to unlock.  so

 * if lowest_unlock is 1, level 0 won't be unlocked

/*

 * helper function for btrfs_search_slot.  The goal is to find a block

 * in cache without setting the path to blocking.  If we find the block

 * we return zero and the path is unchanged.

 *

 * If we can't find the block, we set the path blocking and do some

 * reada.  -EAGAIN is returned and the search must be repeated.

 first we do an atomic uptodate check */

			/*

			 * Do extra check for first_key, eb can be stale due to

			 * being cached, read from scrub, or have multiple

			 * parents (shared tree blocks).

 now we're allowed to do a blocking uptodate check */

	/*

	 * reduce lock contention at high levels

	 * of the btree by dropping locks before

	 * we read.  Don't release the lock on the current

	 * level because we need to walk this node to figure

	 * out which blocks to read.

		/*

		 * If the read above didn't mark this buffer up to date,

		 * it will never end up being up to date.  Set ret to EIO now

		 * and give up so that our caller doesn't loop forever

		 * on our EAGAINs.

/*

 * helper function for btrfs_search_slot.  This does all of the checks

 * for node-level blocks and does any balancing required based on

 * the ins_len.

 *

 * If no extra work was required, zero is returned.  If we had to

 * drop the path, -EAGAIN is returned and btrfs_search_slot must

 * start over

 We try very hard to do read locks on the root */

		/*

		 * The commit roots are read only so we always do read locks,

		 * and we always must hold the commit_root_sem when doing

		 * searches on them, the only exception is send where we don't

		 * want to block transaction commits for a long time, so

		 * we need to clone the commit root in order to avoid races

		 * with transaction commits that create a snapshot of one of

		 * the roots used by a send operation.

		/*

		 * Ensure that all callers have set skip_locking when

		 * p->search_commit_root = 1.

	/*

	 * If the level is set to maximum, we can skip trying to get the read

	 * lock.

		/*

		 * We don't know the level of the root node until we actually

		 * have it read locked

 Whoops, must trade for write lock */

 The level might have changed, check again */

	/*

	 * Callers are responsible for dropping b's references.

/*

 * btrfs_search_slot - look for a key in a tree and perform necessary

 * modifications to preserve tree invariants.

 *

 * @trans:	Handle of transaction, used when modifying the tree

 * @p:		Holds all btree nodes along the search path

 * @root:	The root node of the tree

 * @key:	The key we are looking for

 * @ins_len:	Indicates purpose of search:

 *              >0  for inserts it's size of item inserted (*)

 *              <0  for deletions

 *               0  for plain searches, not modifying the tree

 *

 *              (*) If size of item inserted doesn't include

 *              sizeof(struct btrfs_item), then p->search_for_extension must

 *              be set.

 * @cow:	boolean should CoW operations be performed. Must always be 1

 *		when modifying the tree.

 *

 * If @ins_len > 0, nodes and leaves will be split as we walk down the tree.

 * If @ins_len < 0, nodes will be merged as we walk down the tree (if possible)

 *

 * If @key is found, 0 is returned and you can find the item in the leaf level

 * of the path (level 0)

 *

 * If @key isn't found, 1 is returned and the leaf level of the path (level 0)

 * points to the slot where it should be inserted

 *

 * If an error is encountered while searching the tree a negative error number

 * is returned

 everything at write_lock_level or lower must be write locked */

		/* when we are removing items, we might have to go up to level

		 * two as we update tree pointers  Make sure we keep write

		 * for those levels as well

		/*

		 * for inserting items, make sure we have a write lock on

		 * level 1 so we can update keys

			/*

			 * if we don't really need to cow this block

			 * then we don't want to set the path blocking,

			 * so we test it here

			/*

			 * must have write locks on this node and the

			 * parent

		/*

		 * Leave path with blocking locks to avoid massive

		 * lock context switch, this is made on purpose.

		/*

		 * we have a lock on b and as long as we aren't changing

		 * the tree, there is no way to for the items in b to change.

		 * It is safe to drop the lock on our parent before we

		 * go through the expensive btree search on b.

		 *

		 * If we're inserting or deleting (ins_len != 0), then we might

		 * be changing slot zero, which may require changing the parent.

		 * So, we can't drop the lock until after we know which slot

		 * we're operating on.

		/*

		 * If btrfs_bin_search returns an exact match (prev_cmp == 0)

		 * we can safely assume the target key will always be in slot 0

		 * on lower levels due to the invariants BTRFS' btree provides,

		 * namely that a btrfs_key_ptr entry always points to the

		 * lowest key in the child node, thus we can skip searching

		 * lower levels

			/*

			 * Item key already exists. In this case, if we are

			 * allowed to insert the item (for example, in dir_item

			 * case, item key collision is allowed), it will be

			 * merged with the original item. Only the item size

			 * grows, no new btrfs item will be added. If

			 * search_for_extension is not set, ins_len already

			 * accounts the size btrfs_item, deduct it here so leaf

			 * space check will be correct.

		/*

		 * Slot 0 is special, if we change the key we have to update

		 * the parent pointer which means we must have a write lock on

		 * the parent

/*

 * Like btrfs_search_slot, this looks for a key in the given tree. It uses the

 * current state of the tree together with the operations recorded in the tree

 * modification log to search for the key in a previous version of this tree, as

 * denoted by the time_seq parameter.

 *

 * Naturally, there is no support for insert, delete or cow operations.

 *

 * The resulting path and return value will be set up as if we called

 * btrfs_search_slot at that point in time with ins_len and cow both set to 0.

		/*

		 * we have a lock on b and as long as we aren't changing

		 * the tree, there is no way to for the items in b to change.

		 * It is safe to drop the lock on our parent before we

		 * go through the expensive btree search on b.

/*

 * helper to use instead of search slot if no exact match is needed but

 * instead the next or previous item should be returned.

 * When find_higher is true, the next higher item is returned, the next lower

 * otherwise.

 * When return_any and find_higher are both true, and no higher item is found,

 * return the next lower instead.

 * When return_any is true and find_higher is false, and no lower item is found,

 * return the next higher instead.

 * It returns 0 if any item is found, 1 if none is found (tree empty), and

 * < 0 on error

	/*

	 * a return value of 1 means the path is at the position where the

	 * item should be inserted. Normally this is the next bigger item,

	 * but in case the previous item is the last in a leaf, path points

	 * to the first free slot in the previous leaf, i.e. at an invalid

	 * item.

			/*

			 * no higher item found, return the next

			 * lower instead

			/*

			 * no lower item found, return the next

			 * higher instead

/*

 * Execute search and call btrfs_previous_item to traverse backwards if the item

 * was not found.

 *

 * Return 0 if found, 1 if not found and < 0 if error.

/*

 * adjust the pointers going up the tree, starting at level

 * making sure the right key of each node is points to 'key'.

 * This is used after shifting pointers to the left, so it stops

 * fixing up pointers when a given leaf/node is not in slot 0 of the

 * higher levels

 *

/*

 * update item key.

 *

 * This function isn't completely safe. It's the caller's responsibility

 * that the new key won't break the order

/*

 * Check key order of two sibling extent buffers.

 *

 * Return true if something is wrong.

 * Return false if everything is fine.

 *

 * Tree-checker only works inside one tree block, thus the following

 * corruption can not be detected by tree-checker:

 *

 * Leaf @left			| Leaf @right

 * --------------------------------------------------------------

 * | 1 | 2 | 3 | 4 | 5 | f6 |   | 7 | 8 |

 *

 * Key f6 in leaf @left itself is valid, but not valid when the next

 * key in leaf @right is 7.

 * This can only be checked at tree block merge time.

 * And since tree checker has ensured all key order in each tree block

 * is correct, we only need to bother the last key of @left and the first

 * key of @right.

 No key to check in one of the tree blocks */

/*

 * try to push data from one node into the next node left in the

 * tree.

 *

 * returns 0 if some ptrs were pushed left, < 0 if there was some horrible

 * error, and > 0 if there was no room in the left hand block.

			/* leave at least 8 pointers in the node if

			 * we aren't going to empty it

 dst is the left eb, src is the middle eb */

		/*

		 * Don't call btrfs_tree_mod_log_insert_move() here, key removal

		 * was already fully logged by btrfs_tree_mod_log_eb_copy() above.

/*

 * try to push data from one node into the next node right in the

 * tree.

 *

 * returns 0 if some ptrs were pushed, < 0 if there was some horrible

 * error, and > 0 if there was no room in the right hand block.

 *

 * this will  only push up to 1/2 the contents of the left node over

 don't try to empty the node */

 dst is the right eb, src is the middle eb */

/*

 * helper function to insert a new root level in the tree.

 * A new node is allocated, and a single item is inserted to

 * point to the existing root

 *

 * returns zero on success or < 0 on failure.

 the super has an extra ref to root->node */

/*

 * worker function to insert a single pointer in a node.

 * the node should have enough room for the pointer already

 *

 * slot and level indicate where you want the key to go, and

 * blocknr is the block the key points to.

/*

 * split the node at the specified level in path in two.

 * The path is corrected to point to the appropriate node after the split

 *

 * Before splitting this tries to make some room in the node by pushing

 * left and right, if either one works, it returns right away.

 *

 * returns 0 on success and < 0 on failure

		/*

		 * trying to split the root, lets make a new one

		 *

		 * tree mod log: We don't log_removal old root in

		 * insert_new_root, because that root buffer will be kept as a

		 * normal node. We are going to log removal of half of the

		 * elements below with btrfs_tree_mod_log_eb_copy(). We're

		 * holding a tree lock on the buffer, which is why we cannot

		 * race with other tree_mod_log users.

/*

 * how many bytes are required to store the items in a leaf.  start

 * and nr indicate which items in the leaf to check.  This totals up the

 * space used both by the item structs and the item data

/*

 * The space between the end of the leaf items and

 * the start of the leaf data.  IOW, how much room

 * the leaf has left for both items and data

/*

 * min slot controls the lowest index we're willing to push to the

 * right.  We'll push up to and including min_slot, but no lower

 push left to right */

 make room in the right data area */

 copy from the left data area */

 copy the items from left to right */

 update the item pointers */

 then fixup the leaf pointer in the path */

/*

 * push some data in the path leaf to the right, trying to free up at

 * least data_size bytes.  returns zero if the push worked, nonzero otherwise

 *

 * returns 1 if the push failed because the other node didn't have enough

 * room, 0 if everything worked out and < 0 if there were major errors.

 *

 * this will push starting from min_slot to the end of the leaf.  It won't

 * push any slot lower than min_slot

	/*

	 * slot + 1 is not valid or we fail to read the right node,

	 * no big deal, just return.

 cow and double check */

		/* Key greater than all keys in the leaf, right neighbor has

		 * enough room for it and we're not emptying our leaf to delete

		 * it, therefore use right neighbor to insert the new item and

/*

 * push some data in the path leaf to the left, trying to free up at

 * least data_size bytes.  returns zero if the push worked, nonzero otherwise

 *

 * max_slot can put a limit on how far into the leaf we'll push items.  The

 * item at 'max_slot' won't be touched.  Use (u32)-1 to make us do all the

 * items

 push data from right to left */

 fixup right node */

 then fixup the leaf pointer in the path */

/*

 * push some data in the path leaf to the left, trying to free up at

 * least data_size bytes.  returns zero if the push worked, nonzero otherwise

 *

 * max_slot can put a limit on how far into the leaf we'll push items.  The

 * item at 'max_slot' won't be touched.  Use (u32)-1 to make us push all the

 * items

	/*

	 * slot - 1 is not valid or we fail to read the left node,

	 * no big deal, just return.

 cow and double check */

 we hit -ENOSPC, but it isn't fatal here */

/*

 * split the path's leaf in two, making sure there is at least data_size

 * available for the resulting leaf level of the path.

/*

 * double splits happen when we need to insert a big item in the middle

 * of a leaf.  A double split can leave us with 3 mostly empty leaves:

 * leaf: [ slots 0 - N] [ our target ] [ N + 1 - total in leaf ]

 *          A                 B                 C

 *

 * We avoid this by trying to push the items on either side of our target

 * into the adjacent leaves.  If all goes well we can avoid the double split

 * completely.

	/*

	 * try to push all the items after our slot into the

	 * right leaf

	/*

	 * our goal is to get our slot at the start or end of a leaf.  If

	 * we've done so we're done

 try to push all the items before our slot into the next leaf */

/*

 * split the path's leaf in two, making sure there is at least data_size

 * available for the resulting leaf level of the path.

 *

 * returns 0 if all went well and < 0 on failure.

 first try to make some room by pushing left and right */

 did the pushes work? */

	/*

	 * We have to about BTRFS_NESTING_NEW_ROOT here if we've done a double

	 * split, because we're only allowed to have MAX_LOCKDEP_SUBCLASSES

	 * subclasses, which is 8 at the time of this patch, and we've maxed it

	 * out.  In the future we could add a

	 * BTRFS_NESTING_SPLIT_THE_SPLITTENING if we need to, but for now just

	 * use BTRFS_NESTING_NEW_ROOT.

		/*

		 * We create a new leaf 'right' for the required ins_len and

		 * we'll do btrfs_mark_buffer_dirty() on this leaf after copying

		 * the content of ins_len to 'right'.

 if our item isn't there, return now */

 the leaf has  changed, it now has room.  return now */

 shift the items */

 write the data for the start of the original item */

 write the data for the new item */

/*

 * This function splits a single item into two items,

 * giving 'new_key' to the new item and splitting the

 * old one at split_offset (from the start of the item).

 *

 * The path may be released by this operation.  After

 * the split, the path is pointing to the old item.  The

 * new item is going to be in the same node as the old one.

 *

 * Note, the item being split must be smaller enough to live alone on

 * a tree block with room for one extra struct btrfs_item

 *

 * This allows us to split the item in place, keeping a lock on the

 * leaf the entire time.

/*

 * make the item pointed to by the path smaller.  new_size indicates

 * how small to make it, and from_end tells us if we just chop bytes

 * off the end of the item or if we shift the item to chop bytes off

 * the front.

	/*

	 * item0..itemN ... dataN.offset..dataN.size .. data0.size

 first correct the data pointers */

 shift the data */

/*

 * make the item pointed to by the path bigger, data_size is the added size.

	/*

	 * item0..itemN ... dataN.offset..dataN.size .. data0.size

 first correct the data pointers */

 shift the data */

/**

 * setup_items_for_insert - Helper called before inserting one or more items

 * to a leaf. Main purpose is to save stack depth by doing the bulk of the work

 * in a function that doesn't call btrfs_search_slot

 *

 * @root:	root we are inserting items to

 * @path:	points to the leaf/slot where we are going to insert new items

 * @batch:      information about the batch of items to insert

	/*

	 * Before anything else, update keys in the parent and other ancestors

	 * if needed, then release the write locks on them, so that other tasks

	 * can use them while we modify the leaf.

		/*

		 * item0..itemN ... dataN.offset..dataN.size .. data0.size

 first correct the data pointers */

 shift the items */

 shift the data */

 setup the item for the new data */

/*

 * Insert a new item into a leaf.

 *

 * @root:      The root of the btree.

 * @path:      A path pointing to the target leaf and slot.

 * @key:       The key of the new item.

 * @data_size: The size of the data associated with the new key.

/*

 * Given a key and some data, insert items into the tree.

 * This does all the path init required, making room in the tree if needed.

/*

 * Given a key and some data, insert an item into the tree.

 * This does all the path init required, making room in the tree if needed.

/*

 * This function duplicates an item, giving 'new_key' to the new item.

 * It guarantees both items live in the same tree leaf and the new item is

 * contiguous with the original item.

 *

 * This allows us to split a file extent in place, keeping a lock on the leaf

 * the entire time.

/*

 * delete the pointer from a given node.

 *

 * the tree should have been previously balanced so the deletion does not

 * empty a node.

 just turn the root into a leaf and break */

/*

 * a helper function to delete the leaf pointed to by path->slots[1] and

 * path->nodes[1].

 *

 * This deletes the pointer in path->nodes[1] and frees the leaf

 * block extent.  zero is returned if it all worked out, < 0 otherwise.

 *

 * The path must have already been setup for deleting the leaf, including

 * all the proper balancing.  path->nodes[1] must be locked.

	/*

	 * btrfs_free_extent is expensive, we want to make sure we

	 * aren't holding any locks when we call it

/*

 * delete the item at the leaf level in path.  If that empties

 * the leaf, remove it from the tree

 delete the leaf if we've emptied it */

 delete the leaf if it is mostly empty */

			/* push_leaf_left fixes the path.

			 * make sure the path still points to our leaf

			 * for possible call to del_ptr below

				/* if we're still in the path, make sure

				 * we're dirty.  Otherwise, one of the

				 * push_leaf functions must have already

				 * dirtied this buffer

/*

 * search the tree again to find a leaf with lesser keys

 * returns 0 if it found something or 1 if there are no lesser leaves.

 * returns < 0 on io errors.

 *

 * This may release the path, and so you may lose any locks held at the

 * time you call it.

	/*

	 * We might have had an item with the previous key in the tree right

	 * before we released our path. And after we released our path, that

	 * item might have been pushed to the first slot (0) of the leaf we

	 * were holding due to a tree balance. Alternatively, an item with the

	 * previous key can exist as the only element of a leaf (big fat item).

	 * Therefore account for these 2 cases, so that our callers (like

	 * btrfs_previous_item) don't miss an existing item with a key matching

	 * the previous key we computed above.

/*

 * A helper function to walk down the tree starting at min_key, and looking

 * for nodes or leaves that are have a minimum transaction id.

 * This is used by the btree defrag code, and tree logging

 *

 * This does not cow, but it does stuff the starting key it finds back

 * into min_key, so you can call btrfs_search_slot with cow=1 on the

 * key and get a writable path.

 *

 * This honors path->lowest_level to prevent descent past a given level

 * of the tree.

 *

 * min_trans indicates the oldest transaction that you are interested

 * in walking through.  Any nodes or leaves older than min_trans are

 * skipped over (without reading them).

 *

 * returns zero if something useful was found, < 0 on error and 1 if there

 * was nothing in the tree that matched the search criteria.

 at the lowest level, we're done, setup the path and exit */

		/*

		 * check this node pointer against the min_trans parameters.

		 * If it is too old, skip to the next one.

		/*

		 * we didn't find a candidate key in this node, walk forward

		 * and find another one

 save our key for returning back */

/*

 * this is similar to btrfs_next_leaf, but does not try to preserve

 * and fixup the path.  It looks for and returns the next key in the

 * tree based on the current path and the min_trans parameters.

 *

 * 0 is returned if another key is found, < 0 if there are any errors

 * and 1 is returned if there are no higher keys in the tree

 *

 * path->keep_locks should be set to 1 on the search made before

 * calling this function.

	/*

	 * by releasing the path above we dropped all our locks.  A balance

	 * could have added more items next to the key that used to be

	 * at the very end of the block.  So, check again here and

	 * advance the path if there are now more items available.

	/*

	 * So the above check misses one case:

	 * - after releasing the path above, someone has removed the item that

	 *   used to be at the very end of the block, and balance between leafs

	 *   gets another one with bigger key.offset to replace it.

	 *

	 * This one should be returned as well, or we can get leaf corruption

	 * later(esp. in __btrfs_drop_extents()).

	 *

	 * And a bit more explanation about this check,

	 * with ret > 0, the key isn't found, the path points to the slot

	 * where it should be inserted, so the path->slots[0] item must be the

	 * bigger one.

		/*

		 * Our current level is where we're going to start from, and to

		 * make sure lockdep doesn't complain we need to drop our locks

		 * and nodes from 0 to our current level.

				/*

				 * If we don't get the lock, we may be racing

				 * with push_leaf_left, holding that lock while

				 * itself waiting for the leaf we've currently

				 * locked. To solve this situation, we give up

				 * on our lock and cycle.

/*

 * this uses btrfs_prev_leaf to walk backwards in the tree, and keeps

 * searching until it gets past min_objectid or finds an item of 'type'

 *

 * returns 0 if something is found, 1 if nothing was found and < 0 on error

/*

 * search in extent tree to find a previous Metadata/Data extent item with

 * min objecitd.

 *

 * returns 0 if something is found, 1 if nothing was found and < 0 on error

 SPDX-License-Identifier: GPL-2.0

/*

 * HOW DOES SPACE RESERVATION WORK

 *

 * If you want to know about delalloc specifically, there is a separate comment

 * for that with the delalloc code.  This comment is about how the whole system

 * works generally.

 *

 * BASIC CONCEPTS

 *

 *   1) space_info.  This is the ultimate arbiter of how much space we can use.

 *   There's a description of the bytes_ fields with the struct declaration,

 *   refer to that for specifics on each field.  Suffice it to say that for

 *   reservations we care about total_bytes - SUM(space_info->bytes_) when

 *   determining if there is space to make an allocation.  There is a space_info

 *   for METADATA, SYSTEM, and DATA areas.

 *

 *   2) block_rsv's.  These are basically buckets for every different type of

 *   metadata reservation we have.  You can see the comment in the block_rsv

 *   code on the rules for each type, but generally block_rsv->reserved is how

 *   much space is accounted for in space_info->bytes_may_use.

 *

 *   3) btrfs_calc*_size.  These are the worst case calculations we used based

 *   on the number of items we will want to modify.  We have one for changing

 *   items, and one for inserting new items.  Generally we use these helpers to

 *   determine the size of the block reserves, and then use the actual bytes

 *   values to adjust the space_info counters.

 *

 * MAKING RESERVATIONS, THE NORMAL CASE

 *

 *   We call into either btrfs_reserve_data_bytes() or

 *   btrfs_reserve_metadata_bytes(), depending on which we're looking for, with

 *   num_bytes we want to reserve.

 *

 *   ->reserve

 *     space_info->bytes_may_reserve += num_bytes

 *

 *   ->extent allocation

 *     Call btrfs_add_reserved_bytes() which does

 *     space_info->bytes_may_reserve -= num_bytes

 *     space_info->bytes_reserved += extent_bytes

 *

 *   ->insert reference

 *     Call btrfs_update_block_group() which does

 *     space_info->bytes_reserved -= extent_bytes

 *     space_info->bytes_used += extent_bytes

 *

 * MAKING RESERVATIONS, FLUSHING NORMALLY (non-priority)

 *

 *   Assume we are unable to simply make the reservation because we do not have

 *   enough space

 *

 *   -> __reserve_bytes

 *     create a reserve_ticket with ->bytes set to our reservation, add it to

 *     the tail of space_info->tickets, kick async flush thread

 *

 *   ->handle_reserve_ticket

 *     wait on ticket->wait for ->bytes to be reduced to 0, or ->error to be set

 *     on the ticket.

 *

 *   -> btrfs_async_reclaim_metadata_space/btrfs_async_reclaim_data_space

 *     Flushes various things attempting to free up space.

 *

 *   -> btrfs_try_granting_tickets()

 *     This is called by anything that either subtracts space from

 *     space_info->bytes_may_use, ->bytes_pinned, etc, or adds to the

 *     space_info->total_bytes.  This loops through the ->priority_tickets and

 *     then the ->tickets list checking to see if the reservation can be

 *     completed.  If it can the space is added to space_info->bytes_may_use and

 *     the ticket is woken up.

 *

 *   -> ticket wakeup

 *     Check if ->bytes == 0, if it does we got our reservation and we can carry

 *     on, if not return the appropriate error (ENOSPC, but can be EINTR if we

 *     were interrupted.)

 *

 * MAKING RESERVATIONS, FLUSHING HIGH PRIORITY

 *

 *   Same as the above, except we add ourselves to the

 *   space_info->priority_tickets, and we do not use ticket->wait, we simply

 *   call flush_space() ourselves for the states that are safe for us to call

 *   without deadlocking and hope for the best.

 *

 * THE FLUSHING STATES

 *

 *   Generally speaking we will have two cases for each state, a "nice" state

 *   and a "ALL THE THINGS" state.  In btrfs we delay a lot of work in order to

 *   reduce the locking over head on the various trees, and even to keep from

 *   doing any work at all in the case of delayed refs.  Each of these delayed

 *   things however hold reservations, and so letting them run allows us to

 *   reclaim space so we can make new reservations.

 *

 *   FLUSH_DELAYED_ITEMS

 *     Every inode has a delayed item to update the inode.  Take a simple write

 *     for example, we would update the inode item at write time to update the

 *     mtime, and then again at finish_ordered_io() time in order to update the

 *     isize or bytes.  We keep these delayed items to coalesce these operations

 *     into a single operation done on demand.  These are an easy way to reclaim

 *     metadata space.

 *

 *   FLUSH_DELALLOC

 *     Look at the delalloc comment to get an idea of how much space is reserved

 *     for delayed allocation.  We can reclaim some of this space simply by

 *     running delalloc, but usually we need to wait for ordered extents to

 *     reclaim the bulk of this space.

 *

 *   FLUSH_DELAYED_REFS

 *     We have a block reserve for the outstanding delayed refs space, and every

 *     delayed ref operation holds a reservation.  Running these is a quick way

 *     to reclaim space, but we want to hold this until the end because COW can

 *     churn a lot and we can avoid making some extent tree modifications if we

 *     are able to delay for as long as possible.

 *

 *   ALLOC_CHUNK

 *     We will skip this the first time through space reservation, because of

 *     overcommit and we don't want to have a lot of useless metadata space when

 *     our worst case reservations will likely never come true.

 *

 *   RUN_DELAYED_IPUTS

 *     If we're freeing inodes we're likely freeing checksums, file extent

 *     items, and extent tree items.  Loads of space could be freed up by these

 *     operations, however they won't be usable until the transaction commits.

 *

 *   COMMIT_TRANS

 *     This will commit the transaction.  Historically we had a lot of logic

 *     surrounding whether or not we'd commit the transaction, but this waits born

 *     out of a pre-tickets era where we could end up committing the transaction

 *     thousands of times in a row without making progress.  Now thanks to our

 *     ticketing system we know if we're not making progress and can error

 *     everybody out after a few commits rather than burning the disk hoping for

 *     a different answer.

 *

 * OVERCOMMIT

 *

 *   Because we hold so many reservations for metadata we will allow you to

 *   reserve more space than is currently free in the currently allocate

 *   metadata space.  This only happens with metadata, data does not allow

 *   overcommitting.

 *

 *   You can see the current logic for when we allow overcommit in

 *   btrfs_can_overcommit(), but it only applies to unallocated space.  If there

 *   is no unallocated space to be had, all reservations are kept within the

 *   free space in the allocated metadata chunks.

 *

 *   Because of overcommitting, you generally want to use the

 *   btrfs_can_overcommit() logic for metadata allocations, as it does the right

 *   thing with or without extra unallocated space.

/*

 * after adding space to the filesystem, we need to clear the full flags

 * on all the space infos.

	/*

	 * If we have dup, raid1 or raid10 then only half of the free

	 * space is actually usable.  For raid56, the space info used

	 * doesn't include the parity drive, so we don't have to

	 * change the math

	/*

	 * If we aren't flushing all things, let us overcommit up to

	 * 1/2th of the space. If we can flush, don't let us overcommit

	 * too much, let it overcommit up to 1/8 of the space.

 Don't overcommit when in mixed mode */

/*

 * This is for space we already have accounted in space_info->bytes_may_use, so

 * basically when we're returning space from block_rsv's.

 Check and see if our ticket can be satisfied now. */

 The free space could be negative in case of overcommit */

/*

 * shrink metadata reservation for delalloc

 Calc the number of the pages we need flush for space reservation */

		/*

		 * to_reclaim is set to however much metadata we need to

		 * reclaim, but reclaiming that much data doesn't really track

		 * exactly.  What we really want to do is reclaim full inode's

		 * worth of reservations, however that's not available to us

		 * here.  We will take a fraction of the delalloc bytes for our

		 * flushing loops and hope for the best.  Delalloc will expand

		 * the amount we write to cover an entire dirty extent, which

		 * will reclaim the metadata reservation for that range.  If

		 * it's not enough subsequent flush stages will be more

		 * aggressive.

	/*

	 * If we are doing more ordered than delalloc we need to just wait on

	 * ordered extents, otherwise we'll waste time trying to flush delalloc

	 * that likely won't give us the space back we need.

		/*

		 * We need to make sure any outstanding async pages are now

		 * processed before we continue.  This is because things like

		 * sync_inode() try to be smart and skip writing if the inode is

		 * marked clean.  We don't use filemap_fwrite for flushing

		 * because we want to control how many pages we write out at a

		 * time, thus this is the only safe way to make sure we've

		 * waited for outstanding compressed workers to have started

		 * their jobs and thus have ordered extents set up properly.

		 *

		 * This exists because we do not want to wait for each

		 * individual inode to finish its async work, we simply want to

		 * start the IO on everybody, and then come back here and wait

		 * for all of the async work to catch up.  Once we're done with

		 * that we know we'll have ordered extents for everything and we

		 * can decide if we wait for that or not.

		 *

		 * If we choose to replace this in the future, make absolutely

		 * sure that the proper waiting is being done in the async case,

		 * as there have been bugs in that area before.

		/*

		 * We don't want to wait forever, if we wrote less pages in this

		 * loop than we have outstanding, only wait for that number of

		 * pages, otherwise we can wait for all async pages to finish

		 * before continuing.

		/*

		 * If we are for preemption we just want a one-shot of delalloc

		 * flushing so we can stop flushing if we decide we don't need

		 * to anymore.

/*

 * Try to flush some data based on policy set by @state. This is only advisory

 * and may fail for various reasons. The caller is supposed to examine the

 * state of @space_info to detect the outcome.

		/*

		 * If we have pending delayed iputs then we could free up a

		 * bunch of pinned space, so make sure we run the iputs before

		 * we do our pinned bytes check below.

	/*

	 * We may be flushing because suddenly we have less space than we had

	 * before, and now we're well over-committed based on our current free

	 * space.  If that's the case add in our overage so we make sure to put

	 * appropriate pressure on the flushing state machine.

 If we're just plain full then async reclaim just slows us down. */

 The total flushable belongs to the global rsv, don't flush. */

	/*

	 * 128MiB is 1/4 of the maximum global rsv size.  If we have less than

	 * that devoted to other reservations then there's no sense in flushing,

	 * we don't have a lot of things that need flushing.

	/*

	 * We have tickets queued, bail so we don't compete with the async

	 * flushers.

	/*

	 * If we have over half of the free space occupied by reservations or

	 * pinned then we want to start flushing.

	 *

	 * We do not do the traditional thing here, which is to say

	 *

	 *   if (used >= ((total_bytes + avail) / 2))

	 *     return 1;

	 *

	 * because this doesn't quite work how we want.  If we had more than 50%

	 * of the space_info used by bytes_used and we had 0 available we'd just

	 * constantly run the background flusher.  Instead we want it to kick in

	 * if our reclaimable space exceeds our clamped free space.

	 *

	 * Our clamping range is 2^1 -> 2^8.  Practically speaking that means

	 * the following:

	 *

	 * Amount of RAM        Minimum threshold       Maximum threshold

	 *

	 *        256GiB                     1GiB                  128GiB

	 *        128GiB                   512MiB                   64GiB

	 *         64GiB                   256MiB                   32GiB

	 *         32GiB                   128MiB                   16GiB

	 *         16GiB                    64MiB                    8GiB

	 *

	 * These are the range our thresholds will fall in, corresponding to how

	 * much delalloc we need for the background flusher to kick in.

	/*

	 * If we have more ordered bytes than delalloc bytes then we're either

	 * doing a lot of DIO, or we simply don't have a lot of delalloc waiting

	 * around.  Preemptive flushing is only useful in that it can free up

	 * space before tickets need to wait for things to finish.  In the case

	 * of ordered extents, preemptively waiting on ordered extents gets us

	 * nothing, if our reservations are tied up in ordered extents we'll

	 * simply have to slow down writers by forcing them to wait on ordered

	 * extents.

	 *

	 * In the case that ordered is larger than delalloc, only include the

	 * block reserves that we would actually be able to directly reclaim

	 * from.  In this case if we're heavy on metadata operations this will

	 * clearly be heavy enough to warrant preemptive flushing.  In the case

	 * of heavy DIO or ordered reservations, preemptive flushing will just

	 * waste time and cause us to slow down.

	 *

	 * We want to make sure we truly are maxed out on ordered however, so

	 * cut ordered in half, and if it's still higher than delalloc then we

	 * can keep flushing.  This is to avoid the case where we start

	 * flushing, and now delalloc == ordered and we stop preemptively

	 * flushing when we could still have several gigs of delalloc to flush.

/*

 * maybe_fail_all_tickets - we've exhausted our flushing, start failing tickets

 * @fs_info - fs_info for this fs

 * @space_info - the space info we were flushing

 *

 * We call this when we've exhausted our flushing ability and haven't made

 * progress in satisfying tickets.  The reservation code handles tickets in

 * order, so if there is a large ticket first and then smaller ones we could

 * very well satisfy the smaller tickets.  This will attempt to wake up any

 * tickets in the list to catch this case.

 *

 * This function returns true if it was able to make progress by clearing out

 * other tickets, or if it stumbles across a ticket that was smaller than the

 * first ticket.

		/*

		 * We're just throwing tickets away, so more flushing may not

		 * trip over btrfs_try_granting_tickets, so we need to call it

		 * here to see if we can make progress with the next ticket in

		 * the list.

/*

 * This is for normal flushers, we can wait all goddamned day if we want to.  We

 * will loop and continuously try to flush as long as we are making progress.

 * We count progress as clearing off tickets each time we have to loop.

		/*

		 * We do not want to empty the system of delalloc unless we're

		 * under heavy pressure, so allow one trip through the flushing

		 * logic before we start doing a FLUSH_DELALLOC_FULL.

		/*

		 * We don't want to force a chunk allocation until we've tried

		 * pretty hard to reclaim space.  Think of the case where we

		 * freed up a bunch of space and so have a lot of pinned space

		 * to reclaim.  We would rather use that than possibly create a

		 * underutilized metadata chunk.  So if this is our first run

		 * through the flushing state machine skip ALLOC_CHUNK_FORCE and

		 * commit the transaction.  If nothing has changed the next go

		 * around then we can force a chunk allocation.

/*

 * This handles pre-flushing of metadata space before we get to the point that

 * we need to start blocking threads on tickets.  The logic here is different

 * from the other flush paths because it doesn't rely on tickets to tell us how

 * much we need to flush, instead it attempts to keep us below the 80% full

 * watermark of space by flushing whichever reservation pool is currently the

 * largest.

		/*

		 * We don't have a precise counter for the metadata being

		 * reserved for delalloc, so we'll approximate it by subtracting

		 * out the block rsv's space from the bytes_may_use.  If that

		 * amount is higher than the individual reserves, then we can

		 * assume it's tied up in delalloc reservations.

		/*

		 * We don't want to include the global_rsv in our calculation,

		 * because that's space we can't touch.  Subtract it from the

		 * block_rsv_size for the next checks.

		/*

		 * We really want to avoid flushing delalloc too much, as it

		 * could result in poor allocation patterns, so only flush it if

		 * it's larger than the rest of the pools combined.

		/*

		 * We don't want to reclaim everything, just a portion, so scale

		 * down the to_reclaim by 1/4.  If it takes us down to 0,

		 * reclaim 1 items worth.

 We only went through once, back off our clamping. */

/*

 * FLUSH_DELALLOC_WAIT:

 *   Space is freed from flushing delalloc in one of two ways.

 *

 *   1) compression is on and we allocate less space than we reserved

 *   2) we are overwriting existing space

 *

 *   For #1 that extra space is reclaimed as soon as the delalloc pages are

 *   COWed, by way of btrfs_add_reserved_bytes() which adds the actual extent

 *   length to ->bytes_reserved, and subtracts the reserved space from

 *   ->bytes_may_use.

 *

 *   For #2 this is trickier.  Once the ordered extent runs we will drop the

 *   extent in the range we are overwriting, which creates a delayed ref for

 *   that freed extent.  This however is not reclaimed until the transaction

 *   commits, thus the next stages.

 *

 * RUN_DELAYED_IPUTS

 *   If we are freeing inodes, we want to make sure all delayed iputs have

 *   completed, because they could have been on an inode with i_nlink == 0, and

 *   thus have been truncated and freed up space.  But again this space is not

 *   immediately re-usable, it comes in the form of a delayed ref, which must be

 *   run and then the transaction must be committed.

 *

 * COMMIT_TRANS

 *   This is where we reclaim all of the pinned space generated by running the

 *   iputs

 *

 * ALLOC_CHUNK_FORCE

 *   For data we start with alloc chunk force, however we could have been full

 *   before, and then the transaction commit could have freed new block groups,

 *   so if we now have space to allocate do the force chunk allocation.

 Something happened, fail everything and bail. */

 Something happened, fail everything and bail. */

			/*

			 * Delete us from the list. After we unlock the space

			 * info, we don't want the async reclaim job to reserve

			 * space for this ticket. If that would happen, then the

			 * ticket's task would not known that space was reserved

			 * despite getting an error, resulting in a space leak

			 * (bytes_may_use counter of our space_info).

/**

 * Do the appropriate flushing and waiting for a ticket

 *

 * @fs_info:    the filesystem

 * @space_info: space info for the reservation

 * @ticket:     ticket for the reservation

 * @start_ns:   timestamp when the reservation started

 * @orig_bytes: amount of bytes originally reserved

 * @flush:      how much we can flush

 *

 * This does the work of figuring out how to flush for the ticket, waiting for

 * the reservation, and returning the appropriate error if there is one.

		/*

		 * We were a priority ticket, so we need to delete ourselves

		 * from the list.  Because we could have other priority tickets

		 * behind us that require less space, run

		 * btrfs_try_granting_tickets() to see if their reservations can

		 * now be made.

	/*

	 * Check that we can't have an error set if the reservation succeeded,

	 * as that would confuse tasks and lead them to error out without

	 * releasing reserved space (if an error happens the expectation is that

	 * space wasn't reserved at all).

/*

 * This returns true if this flush state will go through the ordinary flushing

 * code.

	/*

	 * If we're heavy on ordered operations then clamping won't help us.  We

	 * need to clamp specifically to keep up with dirty'ing buffered

	 * writers, because there's not a 1:1 correlation of writing delalloc

	 * and freeing space, like there is with flushing delayed refs or

	 * delayed nodes.  If we're already more ordered than delalloc then

	 * we're keeping up, otherwise we aren't and should probably clamp.

/**

 * Try to reserve bytes from the block_rsv's space

 *

 * @fs_info:    the filesystem

 * @space_info: space info we want to allocate from

 * @orig_bytes: number of bytes we want

 * @flush:      whether or not we can flush to make our reservation

 *

 * This will reserve orig_bytes number of bytes from the space info associated

 * with the block_rsv.  If there is not enough space it will make an attempt to

 * flush out space to make room.  It will do this by flushing delalloc if

 * possible or committing the transaction.  If flush is 0 then no attempts to

 * regain reservations will be made and this will fail if there is not enough

 * space already.

	/*

	 * We don't want NO_FLUSH allocations to jump everybody, they can

	 * generally handle ENOSPC in a different way, so treat them the same as

	 * normal flushers when it comes to skipping pending tickets.

	/*

	 * Carry on if we have enough space (short-circuit) OR call

	 * can_overcommit() to ensure we can overcommit to continue.

	/*

	 * If we couldn't make a reservation then setup our reservation ticket

	 * and kick the async worker if it's not already running.

	 *

	 * If we are a priority flusher then we just need to add our ticket to

	 * the list and we will do our own flushing further down.

				/*

				 * We were forced to add a reserve ticket, so

				 * our preemptive flushing is unable to keep

				 * up.  Clamp down on the threshold for the

				 * preemptive flushing in order to keep up with

				 * the workload.

		/*

		 * We will do the space reservation dance during log replay,

		 * which means we won't have fs_info->fs_root set, so don't do

		 * the async reclaim as we will panic.

/**

 * Trye to reserve metadata bytes from the block_rsv's space

 *

 * @root:       the root we're allocating for

 * @block_rsv:  block_rsv we're allocating for

 * @orig_bytes: number of bytes we want

 * @flush:      whether or not we can flush to make our reservation

 *

 * This will reserve orig_bytes number of bytes from the space info associated

 * with the block_rsv.  If there is not enough space it will make an attempt to

 * flush out space to make room.  It will do this by flushing delalloc if

 * possible or committing the transaction.  If flush is 0 then no attempts to

 * regain reservations will be made and this will fail if there is not enough

 * space already.

/**

 * Try to reserve data bytes for an allocation

 *

 * @fs_info: the filesystem

 * @bytes:   number of bytes we need

 * @flush:   how we are allowed to flush

 *

 * This will reserve bytes from the data space info.  If there is not enough

 * space then we will attempt to flush space as specified by flush.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2011 Fujitsu.  All rights reserved.

 * Written by Miao Xie <miaox@cn.fujitsu.com>

 can be accessed */

		/*

		 * It's possible that we're racing into the middle of removing

		 * this node from the radix tree.  In this case, the refcount

		 * was zero and it should never go back to one.  Just return

		 * NULL like it was never in the radix at all; our release

		 * function is in the process of removing it.

		 *

		 * Some implementations of refcount_inc refuse to bump the

		 * refcount once it has hit zero.  If we don't do this dance

		 * here, refcount_inc() may decide to just WARN_ONCE() instead

		 * of actually bumping the refcount.

		 *

		 * If this node is properly in the radix, we want to bump the

		 * refcount twice, once for the inode and once for this get

		 * operation.

 Will return either the node or PTR_ERR(-ENOMEM) */

 cached in the btrfs inode and can be accessed */

/*

 * Call it when holding delayed_node->mutex

 *

 * If mod = 1, add this node into the prepared list.

 inserted into list */

 Call it when holding delayed_node->mutex */

 not in the list */

 not in the list */

		/*

		 * Once our refcount goes to zero, nobody is allowed to bump it

		 * back up.  We can delete it now.

/*

 * __btrfs_lookup_delayed_item - look up the delayed item by key

 * @delayed_node: pointer to the delayed node

 * @key:	  the key to look up

 * @prev:	  used to store the prev item if the right item isn't found

 * @next:	  used to store the next item if the right item isn't found

 *

 * Note: if we don't find the right item, we will return the prev item and

 * the next item.

 atomic_dec_return implies a barrier */

 Not associated with any delayed_node */

	/*

	 * Here we migrate space rsv from transaction rsv, since have already

	 * reserved space when starting a transaction.  So no need to reserve

	 * qgroup space here.

	/*

	 * Check btrfs_delayed_item_reserve_metadata() to see why we don't need

	 * to release/reserve qgroup space.

	/*

	 * btrfs_dirty_inode will update the inode under btrfs_join_transaction

	 * which doesn't reserve space for speed.  This is a problem since we

	 * still need to reserve space for this update, so try to reserve the

	 * space.

	 *

	 * Now if src_rsv == delalloc_block_rsv we'll let it just steal since

	 * we always reserve enough to update the inode item.

 NO_FLUSH could only fail with -ENOSPC */

/*

 * Insert a single delayed item or a batch of delayed items that have consecutive

 * keys if they exist.

	/*

	 * Now release our path before releasing the delayed items and their

	 * metadata reservations, so that we don't block other tasks for more

	 * time than needed.

 FIXME: Is errno suitable? */

	/*

	 * count the number of the dir index items that we can delete in batch

		/*

		 * can't find the item which the node points to, so this node

		 * is invalid, just drop it.

	/*

	 * Delayed iref deletion is for the inode who has only one link,

	 * so there is only one iref. The case that several irefs are

	 * in the same item doesn't exist.

	/*

	 * If we fail to update the delayed inode we need to abort the

	 * transaction, because we could leave the inode with the improper

	 * counts behind.

/*

 * Called when committing the transaction.

 * Returns 0 on success.

 * Returns < 0 on error and returns with an aborted transaction with any

 * outstanding delayed items cleaned up.

 Will return 0 or -ENOMEM */

	/*

	 * we have reserved enough space when we start a new transaction,

	 * so reserving metadata failure is impossible

	/*

	 * we have reserved enough space when we start a new transaction,

	 * so reserving metadata failure is impossible.

	/*

	 * Since we have held i_mutex of this directory, it is impossible that

	 * a new directory index is added into the delayed node and index_cnt

	 * is updated now. So we needn't lock the delayed node.

	/*

	 * We can only do one readdir with delayed items at a time because of

	 * item->readdir_list.

	/*

	 * This delayed node is still cached in the btrfs inode, so refs

	 * must be > 1 now, and we needn't check it is going to be freed

	 * or not.

	 *

	 * Besides that, this function is used to read dir, we do not

	 * insert/delete delayed items in this period. So we also needn't

	 * requeue or dequeue this delayed node.

	/*

	 * The VFS is going to do up_read(), so we need to downgrade back to a

	 * read lock.

/*

 * btrfs_readdir_delayed_dir_index - read dir info stored in the delayed tree

 *

	/*

	 * Changing the data of the delayed item is impossible. So

	 * we needn't lock them. And we have held i_mutex of the

	 * directory, nobody can delete any directory indexes now.

	/*

	 * we don't do delayed inode updates during log recovery because it

	 * leads to enospc problems.  This means we also can't do

	 * delayed inode refs

	/*

	 * We don't reserve space for inode ref deletion is because:

	 * - We ONLY do async inode ref deletion for the inode who has only

	 *   one link(i_nlink == 1), it means there is only one inode ref.

	 *   And in most case, the inode ref and the inode item are in the

	 *   same leaf, and we will deal with them at the same time.

	 *   Since we are sure we will reserve the space for the inode item,

	 *   it is unnecessary to reserve space for inode ref deletion.

	 * - If the inode ref and the inode item are not in the same leaf,

	 *   We also needn't worry about enospc problem, because we reserve

	 *   much more space for the inode update than it needs.

	 * - At the worst, we can steal some space from the global reservation.

	 *   It is very rare.

			/*

			 * Don't increase refs in case the node is dead and

			 * about to be removed from the tree in the loop below

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2009 Oracle.  All rights reserved.

/*

 * Relocation overview

 *

 * [What does relocation do]

 *

 * The objective of relocation is to relocate all extents of the target block

 * group to other block groups.

 * This is utilized by resize (shrink only), profile converting, compacting

 * space, or balance routine to spread chunks over devices.

 *

 * 		Before		|		After

 * ------------------------------------------------------------------

 *  BG A: 10 data extents	| BG A: deleted

 *  BG B:  2 data extents	| BG B: 10 data extents (2 old + 8 relocated)

 *  BG C:  1 extents		| BG C:  3 data extents (1 old + 2 relocated)

 *

 * [How does relocation work]

 *

 * 1.   Mark the target block group read-only

 *      New extents won't be allocated from the target block group.

 *

 * 2.1  Record each extent in the target block group

 *      To build a proper map of extents to be relocated.

 *

 * 2.2  Build data reloc tree and reloc trees

 *      Data reloc tree will contain an inode, recording all newly relocated

 *      data extents.

 *      There will be only one data reloc tree for one data block group.

 *

 *      Reloc tree will be a special snapshot of its source tree, containing

 *      relocated tree blocks.

 *      Each tree referring to a tree block in target block group will get its

 *      reloc tree built.

 *

 * 2.3  Swap source tree with its corresponding reloc tree

 *      Each involved tree only refers to new extents after swap.

 *

 * 3.   Cleanup reloc trees and data reloc tree.

 *      As old extents in the target block group are still referenced by reloc

 *      trees, we need to clean them up before really freeing the target block

 *      group.

 *

 * The main complexity is in steps 2.2 and 2.3.

 *

 * The entry point of relocation is relocate_block_group() function.

/*

 * map address of tree root to tree

 Use rb_simle_node for search/insert */

/*

 * present a tree block to process

 Use rb_simple_node for search/insert */

 block group to relocate */

 extent tree */

 inode for moving data */

 tree blocks have been processed */

 map start of tree root to corresponding reloc tree */

 list of reloc trees */

 list of subvolume trees that get relocated */

 size of metadata reservation for merging reloc trees */

 size of relocated tree nodes */

 reserved size for block group relocation*/

 stages of data relocation */

/*

 * walk up backref nodes until reach node presents tree root

/*

 * walk down backref nodes to find start of next reference path

/*

 * update backref cache after a transaction commit

	/*

	 * detached nodes are used to avoid unnecessary backref

	 * lookup. transaction commit changes the extent tree.

	 * so the detached nodes are no longer useful.

	/*

	 * some nodes can be left in the pending list if there were

	 * errors during processing the pending nodes.

	/*

	 * Pair with set_bit/clear_bit in clean_dirty_subvols and

	 * btrfs_update_reloc_root. We need to see the updated bit before

	 * trying to access reloc_root

/*

 * Check if this subvolume tree has valid reloc tree.

 *

 * Reloc tree after swap is considered dead, thus not considered as valid.

 * This is enough for most callers, as they don't distinguish dead reloc root

 * from no reloc root.  But btrfs_should_ignore_reloc_root() below is a

 * special case.

 This root has been merged with its reloc tree, we can ignore it */

	/*

	 * if there is reloc tree and it was created in previous

	 * transaction backref lookup can find the reloc tree,

	 * so backref node for the fs tree root is useless for

	 * relocation.

/*

 * find reloc tree by address of tree root

/*

 * For useless nodes, do two major clean ups:

 *

 * - Cleanup the children edges and nodes

 *   If child node is also orphan (no parent) during cleanup, then the child

 *   node will also be cleaned up.

 *

 * - Freeing up leaves (level 0), keeps nodes detached

 *   For nodes, the node is still cached as "detached"

 *

 * Return false if @node is not in the @useless_nodes list.

 * Return true if @node is in the @useless_nodes list.

 Only tree root nodes can be added to @useless_nodes */

 The node is the lowest node */

 Cleanup the lower edges */

 Child node is also orphan, queue for cleanup */

 Mark this block processed for relocation */

		/*

		 * Backref nodes for tree leaves are deleted from the cache.

		 * Backref nodes for upper level tree blocks are left in the

		 * cache to avoid unnecessary backref lookup.

/*

 * Build backref tree for a given tree block. Root of the backref tree

 * corresponds the tree block, leaves of the backref tree correspond roots of

 * b-trees that reference the tree block.

 *

 * The basic idea of this function is check backrefs of a given block to find

 * upper level blocks that reference the block, and then check backrefs of

 * these upper level blocks recursively. The recursion stops when tree root is

 * reached or backrefs for the block is cached.

 *

 * NOTE: if we find that backrefs for a block are cached, we know backrefs for

 * all upper level blocks that directly/indirectly reference the block are also

 * cached.

 For searching parent of TREE_BLOCK_REF */

 Breadth-first search to build backref cache */

		/*

		 * The pending list isn't empty, take the first block to

		 * process

 Finish the upper linkage of newly added edges/nodes */

/*

 * helper to add backref node for the newly created snapshot.

 * the backref node is created by cloning backref node that

 * corresponds to root of source tree

/*

 * helper to add 'address of tree root -> reloc tree' mapping

/*

 * helper to delete the 'address of tree root -> reloc tree'

 * mapping

	/*

	 * We only put the reloc root here if it's on the list.  There's a lot

	 * of places where the pattern is to splice the rc->reloc_roots, process

	 * the reloc roots, and then add the reloc root back onto

	 * rc->reloc_roots.  If we call __del_reloc_root while it's off of the

	 * list we don't want the reference being dropped, because the guy

	 * messing with the list is in charge of the reference.

/*

 * helper to update the 'address of tree root -> reloc tree'

 * mapping

 called by btrfs_init_reloc_root */

		/*

		 * Set the last_snapshot field to the generation of the commit

		 * root - like this ctree.c:btrfs_block_can_be_shared() behaves

		 * correctly (returns true) when the relocation root is created

		 * either inside the critical section of a transaction commit

		 * (through transaction.c:qgroup_account_snapshot()) and when

		 * it's created before the transaction commit is started.

		/*

		 * called by btrfs_reloc_post_snapshot_hook.

		 * the source tree is a reloc tree, all tree blocks

		 * modified after it was created have RELOC flag

		 * set in their headers. so it's OK to not update

		 * the 'last_snapshot'.

	/*

	 * We have changed references at this point, we must abort the

	 * transaction if anything fails.

/*

 * create reloc tree for a given fs tree. reloc tree is just a

 * snapshot of the fs tree with special root objectid.

 *

 * The reloc_root comes out of here with two references, one for

 * root->reloc_root, and another for being on the rc->reloc_roots list.

	/*

	 * The subvolume has reloc tree but the swap is finished, no need to

	 * create/update the dead reloc tree

	/*

	 * This is subtle but important.  We do not do

	 * record_root_in_transaction for reloc roots, instead we record their

	 * corresponding fs root, and then here we update the last trans for the

	 * reloc root.  This means that we have to do this for the entire life

	 * of the reloc root, regardless of which stage of the relocation we are

	 * in.

	/*

	 * We are merging reloc roots, we do not need new reloc trees.  Also

	 * reloc trees never need their own reloc tree.

 Pairs with create_reloc_root */

/*

 * update root item of reloc tree

	/*

	 * We are probably ok here, but __del_reloc_root() will drop its ref of

	 * the root.  We have the ref for root->reloc_root, but just in case

	 * hold it while we update the reloc root.

 root->reloc_root will stay until current relocation finished */

		/*

		 * Mark the tree as dead before we change reloc_root so

		 * have_reloc_root will not touch it from now on.

/*

 * helper to find first cached inode with inode number >= objectid

 * in a subvolume

/*

 * get new location of data

/*

 * update file extent items in the tree leaf to point to

 * the new locations.

 reloc trees always use full backref */

		/*

		 * if we are modifying block in fs tree, wait for readpage

		 * to complete and drop the extent cache

			/*

			 * Don't have to abort since we've not changed anything

			 * in the file extent yet.

/*

 * try to replace tree blocks in fs tree with the new blocks

 * in reloc tree. tree blocks haven't been modified since the

 * reloc tree was create can be replaced.

 *

 * if a block was replaced, level of the block + 1 is returned.

 * if no block got replaced, 0 is returned. if there are other

 * errors, a negative error number is returned.

		/*

		 * Info qgroup to trace both subtrees.

		 *

		 * We must trace both trees.

		 * 1) Tree reloc subtree

		 *    If not traced, we will leak data numbers

		 * 2) Fs subtree

		 *    If not traced, we will double count old data

		 *

		 * We don't scan the subtree right now, but only record

		 * the swapped tree blocks.

		 * The real subtree rescan is delayed until we have new

		 * CoW on the subtree root node before transaction commit.

		/*

		 * swap blocks in fs tree and reloc tree.

/*

 * helper to find next relocated block in reloc tree

/*

 * walk down reloc tree to find relocated block of lowest level

/*

 * invalidate extent cache for file extents whose key in range of

 * [min_key, max_key)

 the lock_extent waits for readpage to complete */

/*

 * Insert current subvolume into reloc_control::dirty_subvol_roots

 @root must be a subvolume tree root with a valid reloc tree */

 Merged subvolume, cleanup its reloc root */

			/*

			 * Need barrier to ensure clear_bit() only happens after

			 * root->reloc_root = NULL. Pairs with have_reloc_root.

				/*

				 * btrfs_drop_snapshot drops our ref we hold for

				 * ->reloc_root.  If it fails however we must

				 * drop the ref ourselves.

 Orphan reloc tree, just clean it up */

/*

 * merge the relocated tree blocks in reloc tree with corresponding

 * fs tree.

	/*

	 * In merge_reloc_root(), we modify the upper level pointer to swap the

	 * tree blocks between reloc tree and subvolume tree.  Thus for tree

	 * block COW, we COW at most from level 1 to root level for each tree.

	 *

	 * Thus the needed metadata size is at most root_level * nodesize,

	 * and * 2 since we have two trees to COW.

		/*

		 * At this point we no longer have a reloc_control, so we can't

		 * depend on btrfs_init_reloc_root to update our last_trans.

		 *

		 * But that's ok, we started the trans handle on our

		 * corresponding fs_root, which means it's been added to the

		 * dirty list.  At commit time we'll still call

		 * btrfs_update_reloc_root() and update our root item

		 * appropriately.

		/*

		 * save the merging progress in the drop_progress.

		 * this is OK since root refs == 1 in this case.

	/*

	 * handle the case only one block in the fs tree need to be

	 * relocated and the block is tree root.

			/*

			 * Even if we have an error we need this reloc root

			 * back on our list so we can clean up properly.

		/*

		 * set reference count to 1, so btrfs_recover_relocation

		 * knows it should resumes merging

		/*

		 * Even if we have an error we need this reloc root back on our

		 * list so we can clean up properly.

	/*

	 * this serializes us with btrfs_record_root_in_transaction,

	 * we have to make sure nobody is in the middle of

	 * adding their roots to the list while we are

	 * doing this splice

				/*

				 * For recovery we read the fs roots on mount,

				 * and if we didn't find the root then we marked

				 * the reloc root as a garbage root.  For normal

				 * relocation obviously the root should exist in

				 * memory.  However there's no reason we can't

				 * handle the error properly here just in case.

				/*

				 * This is actually impossible without something

				 * going really wrong (like weird race condition

				 * or cosmic rays).

 Don't forget to queue this reloc root for cleanup */

 new reloc root may be added */

	/*

	 * We used to have

	 *

	 * BUG_ON(!RB_EMPTY_ROOT(&rc->reloc_root_tree.rb_root));

	 *

	 * here, but it's wrong.  If we fail to start the transaction in

	 * prepare_to_merge() we will have only 0 ref reloc roots, none of which

	 * have actually been removed from the reloc_root_tree rb tree.  This is

	 * fine because we're bailing here, and we hold a reference on the root

	 * for the list that holds it, so these roots will be cleaned up when we

	 * do the reloc_dirty_list afterwards.  Meanwhile the root->reloc_root

	 * will be cleaned up on unmount.

	 *

	 * The remaining nodes will be cleaned up by free_reloc_control.

	/*

	 * This should succeed, since we can't have a reloc root without having

	 * already looked up the actual root and created the reloc root for this

	 * root.

	 *

	 * However if there's some sort of corruption where we have a ref to a

	 * reloc root without a corresponding root this could return ENOENT.

		/*

		 * If there is no root, then our references for this block are

		 * incomplete, as we should be able to walk all the way up to a

		 * block that is owned by a root.

		 *

		 * This path is only for SHAREABLE roots, so if we come upon a

		 * non-SHAREABLE root then we have backrefs that resolve

		 * improperly.

		 *

		 * Both of these cases indicate file system corruption, or a bug

		 * in the backref walking code.

		/*

		 * We could have raced with another thread which failed, so

		 * root->reloc_root may not be set, return ENOENT in this case.

			/*

			 * We just created the reloc root, so we shouldn't have

			 * ->new_bytenr set and this shouldn't be in the changed

			 *  list.  If it is then we have multiple roots pointing

			 *  at the same bytenr which indicates corruption, or

			 *  we've made a mistake in the backref walking code.

		/*

		 * This can happen if there's fs corruption or if there's a bug

		 * in the backref lookup code.

 setup backref node path for btrfs_reloc_cow_block */

/*

 * Select a tree root for relocation.

 *

 * Return NULL if the block is not shareable. We should use do_relocation() in

 * this case.

 *

 * Return a tree root pointer if the block is shareable.

 * Return -ENOENT if the block is root of reloc tree.

		/*

		 * This can occur if we have incomplete extent refs leading all

		 * the way up a particular path, in this case return -EUCLEAN.

 No other choice for non-shareable tree */

	/*

	 * We are under a transaction here so we can only do limited flushing.

	 * If we get an enospc just kick back -EAGAIN so we know to drop the

	 * transaction and try to refill when we can flush all the things.

		/*

		 * only one thread can access block_rsv at this point,

		 * so we don't need hold lock to protect block_rsv.

		 * we expand more reservation size here to allow enough

		 * space for relocation and we will return earlier in

		 * enospc case.

/*

 * relocate a block tree, and then update pointers in upper level

 * blocks that reference the block to point to the new location.

 *

 * if called by link_to_upper, the block has already been relocated.

 * in that case this function just updates pointers.

	/*

	 * If we are lowest then this is the first time we're processing this

	 * block, and thus shouldn't have an eb associated with it yet.

			/*

			 * We've just COWed this block, it should have updated

			 * the correct backref node entry.

	/*

	 * We should have allocated all of our space in the block rsv and thus

	 * shouldn't ENOSPC.

/*

 * mark a block and all blocks directly/indirectly reference the block

 * as processed.

/*

 * helper function to relocate a tree block

	/*

	 * If we fail here we want to drop our backref_node because we are going

	 * to start over and regenerate the tree for it.

 See explanation in select_one_root for the -EUCLEAN case. */

			/*

			 * This block was the root block of a root, and this is

			 * the first time we're processing the block and thus it

			 * should not have had the ->new_bytenr modified and

			 * should have not been included on the changed list.

			 *

			 * However in the case of corruption we could have

			 * multiple refs pointing to the same block improperly,

			 * and thus we would trip over these checks.  ASSERT()

			 * for the developer case, because it could indicate a

			 * bug in the backref code, however error out for a

			 * normal user in the case of corruption.

			/*

			 * Another thread could have failed, need to check if we

			 * have reloc_root actually set.

/*

 * relocate a list of blocks

 Kick in readahead for tree blocks with missing keys */

 Get first keys */

 Do tree relocation */

	/*

	 * For subpage case, previous i_size may not be aligned to PAGE_SIZE.

	 * This means the range [i_size, PAGE_END + 1) is filled with zeros by

	 * btrfs_do_readpage() call of previously relocated file cluster.

	 *

	 * If the current cluster starts in the above range, btrfs_do_readpage()

	 * will skip the read, and relocate_one_page() will later writeback

	 * the padding zeros as new data, causing data corruption.

	 *

	 * Here we have to manually invalidate the range (i_size, PAGE_END + 1).

		/*

		 * Subpage can't handle page with DIRTY but without UPTODATE

		 * bit as it can lead to the following deadlock:

		 *

		 * btrfs_readpage()

		 * | Page already *locked*

		 * |- btrfs_lock_and_flush_ordered_range()

		 *    |- btrfs_start_ordered_extent()

		 *       |- extent_write_cache_pages()

		 *          |- lock_page()

		 *             We try to lock the page we already hold.

		 *

		 * Here we just writeback the whole data reloc inode, so that

		 * we will be ensured to have no dirty range in the page, and

		 * are safe to clear the uptodate bits.

		 *

		 * This shouldn't cause too much overhead, as we need to write

		 * the data back anyway.

		/*

		 * If page is freed we don't need to do anything then, as we

		 * will re-read the whole page anyway.

/*

 * Allow error injection to test balance/relocation cancellation

 Last extent, use cluster end directly */

 Use next boundary start*/

	/*

	 * Start from the cluster, as for subpage case, the cluster can start

	 * inside the page.

 Reserve metadata for this range */

 Mark the range delalloc and dirty for later writeback */

		/*

		 * Set the boundary if it's inside the page.

		 * Data relocation requires the destination extents to have the

		 * same size as the source.

		 * EXTENT_BOUNDARY bit prevents current extent from being merged

		 * with previous extent.

 Crossed extent end, go to next extent */

 Just finished the last extent of the cluster, exit. */

/*

 * helper to add a tree block to the list.

 * the major work is getting the generation and level of the block

		/*

		 * We're reading random blocks without knowing their owner ahead

		 * of time.  This is ok most of the time, as all reloc roots and

		 * fs roots have the same lock type.  However normal trees do

		 * not, and the only way to know ahead of time is to read the

		 * inline ref offset.  We know it's an fs root if

		 *

		 * 1. There's more than one ref.

		 * 2. There's a SHARED_DATA_REF_KEY set.

		 * 3. FULL_BACKREF is set on the flags.

		 *

		 * Otherwise it's safe to assume that the ref offset == the

		 * owner of this block, so we can use that when calling

		 * read_tree_block.

/*

 * helper to add tree blocks for backref of type BTRFS_SHARED_DATA_REF_KEY

/*

 * Locate the free space cache EXTENT_DATA in root tree leaf and delete the

 * cache inode, to avoid free space cache data extent blocking data relocation.

/*

 * helper to find all tree blocks that reference a given data extent

/*

 * helper to find next unprocessed extent

		/*

		 * extent tree is not a ref_cow tree and has no reloc_root to

		 * cleanup.  And callers are responsible to free the above

		 * block rsv.

	/*

	 * Even in the case when the relocation is cancelled, we should all go

	 * through prepare_to_merge() and merge_reloc_roots().

	 *

	 * For error (including cancelled balance), prepare_to_merge() will

	 * mark all reloc trees orphan, then queue them for cleanup in

	 * merge_reloc_roots()

 get rid of pinned extents */

/*

 * helper to create inode for data relocation.

 * the inode is in data relocation tree and its link count is 0

/*

 * Mark start of chunk relocation that is cancellable. Check if the cancellation

 * has been requested meanwhile and don't start in that case.

 *

 * Return:

 *   0             success

 *   -EINPROGRESS  operation is already in progress, that's probably a bug

 *   -ECANCELED    cancellation request was set before the operation started

 *   -EAGAIN       can not start because there are ongoing send operations

 This should not happen */

		/*

		 * On cancel, clear all requests but let the caller mark

		 * the end after cleanup operations.

/*

 * Mark end of chunk relocation that is cancellable and wake any waiters.

 Requested after start, clear bit first so any waiters can continue */

/*

 * Print the block group being relocated

/*

 * function to relocate all extents in a block group.

		/*

		 * We may have gotten ENOSPC after we already dirtied some

		 * extents.  If writeout happens while we're relocating a

		 * different block group we could end up hitting the

		 * BUG_ON(rc->stage == UPDATE_DATA_PTRS) in

		 * btrfs_reloc_cow_block.  Make sure we write everything out

		 * properly so we don't trip over this problem, and then break

		 * out of the loop if we hit an error.

/*

 * recover relocation interrupted by system crash.

 *

 * this function resumes merging reloc trees with corresponding fs trees.

 * this is important for keeping the sharing of tree blocks

 cleanup orphan inode in data relocation tree */

/*

 * helper to add ordered checksum for data relocation.

 *

 * cloning checksum properly handles the nodatasum extents.

 * it also saves CPU time to re-calculate the checksum.

		/*

		 * We need to offset the new_bytenr based on where the csum is.

		 * We need to do this because we will read in entire prealloc

		 * extents but we may have written to say the middle of the

		 * prealloc extent, so we need to make sure the csum goes with

		 * the right disk offset.

		 *

		 * We can do this because the data reloc inode refers strictly

		 * to the on disk bytes, so we don't have to worry about

		 * disk_len vs real len like with real inodes since it's all

		 * disk length.

/*

 * called before creating snapshot. it calculates metadata reservation

 * required for relocating tree blocks in the snapshot

	/*

	 * relocation is in the stage of merging trees. the space

	 * used by merging a reloc tree is twice the size of

	 * relocated tree nodes in the worst case. half for cowing

	 * the reloc tree, half for cowing the fs tree. the space

	 * used by cowing the reloc tree will be freed after the

	 * tree is dropped. if we create snapshot, cowing the fs

	 * tree may use more space than it frees. so we need

	 * reserve extra space.

/*

 * called after snapshot is created. migrate block reservation

 * and create reloc root for the newly created snapshot

 *

 * This is similar to btrfs_init_reloc_root(), we come out of here with two

 * references held on the reloc_root, one for root->reloc_root and one for

 * rc->reloc_roots.

 Pairs with create_reloc_root */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/*

 * Structure name                       Path

 * --------------------------------------------------------------------------

 * btrfs_supported_static_feature_attrs /sys/fs/btrfs/features

 * btrfs_supported_feature_attrs	/sys/fs/btrfs/features and

 *					/sys/fs/btrfs/<uuid>/features

 * btrfs_attrs				/sys/fs/btrfs/<uuid>

 * devid_attrs				/sys/fs/btrfs/<uuid>/devinfo/<devid>

 * allocation_attrs			/sys/fs/btrfs/<uuid>/allocation

 * qgroup_attrs				/sys/fs/btrfs/<uuid>/qgroups/<level>_<qgroupid>

 * space_info_attrs			/sys/fs/btrfs/<uuid>/allocation/<bg-type>

 * raid_attrs				/sys/fs/btrfs/<uuid>/allocation/<bg-type>/<bg-profile>

 *

 * When built with BTRFS_CONFIG_DEBUG:

 *

 * btrfs_debug_feature_attrs		/sys/fs/btrfs/debug

 * btrfs_debug_mount_attrs		/sys/fs/btrfs/<uuid>/debug

 * discard_debug_attrs			/sys/fs/btrfs/<uuid>/debug/discard

 For raid type sysfs entries */

 Nothing to do */

	/*

	 * We don't want to do full transaction commit from inside sysfs

 Remove once support for zoned allocation is feature complete */

/*

 * Features which depend on feature bits and may differ between each fs.

 *

 * /sys/fs/btrfs/features      - all available features implemeted by this version

 * /sys/fs/btrfs/UUID/features - features of the fs which are enabled or

 *                               can be changed on a mounted filesystem.

		/*

		 * This "trick" only works as long as 'enum btrfs_csum_type' has

		 * no holes in it

 4K sector size is also supported with 64K page size */

 Only sectorsize == PAGE_SIZE is now supported */

/*

 * Features which only depend on kernel version.

 *

 * These are listed in /sys/fs/btrfs/features along with

 * btrfs_supported_feature_attrs.

/*

 * Discard statistics and tunables

/*

 * Per-filesystem debugging of discard (when mounted with discard=async).

 *

 * Path: /sys/fs/btrfs/<uuid>/debug/discard/

/*

 * Per-filesystem runtime debugging exported via sysfs.

 *

 * Path: /sys/fs/btrfs/UUID/debug/

/*

 * Runtime debugging exported via sysfs, applies to all mounted filesystems.

 *

 * Path: /sys/fs/btrfs/debug

/*

 * Allocation information about block group profiles.

 *

 * Path: /sys/fs/btrfs/<uuid>/allocation/<bg-type>/<bg-profile>/

/*

 * Allocation information about block group types.

 *

 * Path: /sys/fs/btrfs/<uuid>/allocation/<bg-type>/

/*

 * Allocation information about block groups.

 *

 * Path: /sys/fs/btrfs/<uuid>/allocation/

	/*

	 * p_len is the len until the first occurrence of either

	 * '\n' or '\0'

	/*

	 * We don't want to do full transaction commit from inside sysfs

/*

 * Look for an exact string @string in @buffer with possible leading or

 * trailing whitespace

 Skip leading whitespace */

 Match entire string, check if the rest is whitespace or empty */

/*

 * Per-filesystem information and stats.

 *

 * Path: /sys/fs/btrfs/<uuid>/

 when fs_devs is NULL it will remove all fsid kobject */

 safe max, 64 names * 64 bytes */

/*

 * Create a sysfs entry for a given block group type at path

 * /sys/fs/btrfs/UUID/allocation/data/TYPE

	/*

	 * Setup a NOFS context because kobject_add(), deep in its call chain,

	 * does GFP_KERNEL allocations, and we are often called in a context

	 * where if reclaim is triggered we can deadlock (we are either holding

	 * a transaction handle or some lock required for a transaction

	 * commit).

	/*

	 * We call this either on mount, or if we've created a block group for a

	 * new index type while running (i.e. when restriping).  The running

	 * case is tricky because we could race with other threads, so we need

	 * to have this check to make sure we didn't already init the kobject.

	 *

	 * We don't have to protect on the free side because it only happens on

	 * unmount.

/*

 * Remove sysfs directories for all block group types of a given space info and

 * the space info as well

/*

 * Create a sysfs entry for a space info type at path

 * /sys/fs/btrfs/UUID/allocation/TYPE

	/*

	 * Seed fs_devices devices_kobj aren't used, fetch kobject from the

	 * fs_info::fs_devices.

	/*

	 * Print all at once so we get a snapshot of all values from the same

	 * time. Keep them in sync and in order of definition of

	 * btrfs_dev_stat_values.

/*

 * Information about one device.

 *

 * Path: /sys/fs/btrfs/<uuid>/devinfo/<devid>/

	/*

	 * Make sure we use the fs_info::fs_devices to fetch the kobjects even

	 * for the seed fs_devices

	/*

	 * Sprouting changes fsid of the mounted filesystem, rename the fsid

	 * directory

 /sys/fs/btrfs/ entry */

/*

 * Creates:

 *		/sys/fs/btrfs/UUID

 *

 * Can be called by the device discovery thread.

 Discard directory */

/*

 * Qgroup information.

 *

 * Path: /sys/fs/btrfs/<uuid>/qgroups/<level>_<qgroupid>/

 Called when qgroups get initialized, thus there is no need for locking */

/*

 * Change per-fs features in /sys/fs/btrfs/UUID/features to match current

 * values in superblock. Call after any changes to incompat/compat_ro flags

	/*

	 * See 14e46e04958df74 and e410e34fad913dd, feature bit updates are not

	 * safe when called from some contexts (eg. balance)

	/*

	 * FIXME: this is too heavy to update just one value, ideally we'd like

	 * to use sysfs_update_group but some refactoring is needed first.

 SPDX-License-Identifier: GPL-2.0

 Key with offset of -1 found */

	/*

	 * have to add the null termination to make sure that reconnect_path

	 * gets the right len for strlen

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/*

 * Macro templates that define helpers to read/write extent buffer data of a

 * given size, that are also used via ctree.h for access to item members by

 * specialized helpers.

 *

 * Generic helpers:

 * - btrfs_set_8 (for 8/16/32/64)

 * - btrfs_get_8 (for 8/16/32/64)

 *

 * Generic helpers with a token (cached address of the most recently accessed

 * page):

 * - btrfs_set_token_8 (for 8/16/32/64)

 * - btrfs_get_token_8 (for 8/16/32/64)

 *

 * The set/get functions handle data spanning two pages transparently, in case

 * metadata block size is larger than page.  Every pointer to metadata items is

 * an offset into the extent buffer page array, cast to a specific type.  This

 * gives us all the type checking.

 *

 * The extent buffer pages stored in the array pages do not form a contiguous

 * phyusical range, but the API functions assume the linear offset to the range

 * from 0 to metadata node size.

 SPDX-License-Identifier: GPL-2.0

/*

 * Implementation of the interface defined in struct fsverity_operations.

 *

 * The main question is how and where to store the verity descriptor and the

 * Merkle tree. We store both in dedicated btree items in the filesystem tree,

 * together with the rest of the inode metadata. This means we'll need to do

 * extra work to encrypt them once encryption is supported in btrfs, but btrfs

 * has a lot of careful code around i_size and it seems better to make a new key

 * type than try and adjust all of our expectations for i_size.

 *

 * Note that this differs from the implementation in ext4 and f2fs, where

 * this data is stored as if it were in the file, but past EOF. However, btrfs

 * does not have a widespread mechanism for caching opaque metadata pages, so we

 * do pretend that the Merkle tree pages themselves are past EOF for the

 * purposes of caching them (as opposed to creating a virtual inode).

 *

 * fs verity items are stored under two different key types on disk.

 * The descriptor items:

 * [ inode objectid, BTRFS_VERITY_DESC_ITEM_KEY, offset ]

 *

 * At offset 0, we store a btrfs_verity_descriptor_item which tracks the

 * size of the descriptor item and some extra data for encryption.

 * Starting at offset 1, these hold the generic fs verity descriptor.

 * The latter are opaque to btrfs, we just read and write them as a blob for

 * the higher level verity code.  The most common descriptor size is 256 bytes.

 *

 * The merkle tree items:

 * [ inode objectid, BTRFS_VERITY_MERKLE_ITEM_KEY, offset ]

 *

 * These also start at offset 0, and correspond to the merkle tree bytes.

 * So when fsverity asks for page 0 of the merkle tree, we pull up one page

 * starting at offset 0 for this key type.  These are also opaque to btrfs,

 * we're blindly storing whatever fsverity sends down.

 *

 * Another important consideration is the fact that the Merkle tree data scales

 * linearly with the size of the file (with 4K pages/blocks and SHA-256, it's

 * ~1/127th the size) so for large files, writing the tree can be a lengthy

 * operation. For that reason, we guard the whole enable verity operation

 * (between begin_enable_verity and end_enable_verity) with an orphan item.

 * Again, because the data can be pretty large, it's quite possible that we

 * could run out of space writing it, so we try our best to handle errors by

 * stopping and rolling back rather than aborting the victim transaction.

/*

 * Compute the logical file offset where we cache the Merkle tree.

 *

 * @inode:  inode of the verity file

 *

 * For the purposes of caching the Merkle tree pages, as required by

 * fs-verity, it is convenient to do size computations in terms of a file

 * offset, rather than in terms of page indices.

 *

 * Use 64K to be sure it's past the last page in the file, even with 64K pages.

 * That rounding operation itself can overflow loff_t, so we do it in u64 and

 * check.

 *

 * Returns the file offset on success, negative error code on failure.

/*

 * Drop all the items for this inode with this key_type.

 *

 * @inode:     inode to drop items for

 * @key_type:  type of items to drop (BTRFS_VERITY_DESC_ITEM or

 *             BTRFS_VERITY_MERKLE_ITEM)

 *

 * Before doing a verity enable we cleanup any existing verity items.

 * This is also used to clean up if a verity enable failed half way through.

 *

 * Returns number of dropped items on success, negative error code on failure.

 1 for the item being dropped */

		/*

		 * Walk backwards through all the items until we find one that

		 * isn't from our key type or objectid

 No more keys of this type, we're done */

 No more keys of this type, we're done */

		/*

		 * This shouldn't be a performance sensitive function because

		 * it's not used as part of truncate.  If it ever becomes

		 * perf sensitive, change this to walk forward and bulk delete

		 * items

/*

 * Drop all verity items

 *

 * @inode:  inode to drop verity items for

 *

 * In most contexts where we are dropping verity items, we want to do it for all

 * the types of verity items, not a particular one.

 *

 * Returns: 0 on success, negative error code on failure.

/*

 * Insert and write inode items with a given key type and offset.

 *

 * @inode:     inode to insert for

 * @key_type:  key type to insert

 * @offset:    item offset to insert at

 * @src:       source data to write

 * @len:       length of source data to write

 *

 * Write len bytes from src into items of up to 2K length.

 * The inserted items will have key (ino, key_type, offset + off) where off is

 * consecutively increasing from 0 up to the last item ending at offset + len.

 *

 * Returns 0 on success and a negative error code on failure.

 1 for the new item being inserted */

		/*

		 * Insert 2K at a time mostly to be friendly for smaller leaf

		 * size filesystems

/*

 * Read inode items of the given key type and offset from the btree.

 *

 * @inode:      inode to read items of

 * @key_type:   key type to read

 * @offset:     item offset to read from

 * @dest:       Buffer to read into. This parameter has slightly tricky

 *              semantics.  If it is NULL, the function will not do any copying

 *              and will just return the size of all the items up to len bytes.

 *              If dest_page is passed, then the function will kmap_local the

 *              page and ignore dest, but it must still be non-NULL to avoid the

 *              counting-only behavior.

 * @len:        length in bytes to read

 * @dest_page:  copy into this page instead of the dest buffer

 *

 * Helper function to read items from the btree.  This returns the number of

 * bytes read or < 0 for errors.  We can return short reads if the items don't

 * exist on disk or aren't big enough to fill the desired length.  Supports

 * reading into a provided buffer (dest) or into the page cache

 *

 * Returns number of bytes read or a negative error code on failure.

			/*

			 * Once we've copied something, we want all of the items

			 * to be sequential

			/*

			 * Our initial offset might be in the middle of an

			 * item.  Make sure it all makes sense.

 desc = NULL to just sum all the item lengths */

 Number of bytes in this item we want to copy */

 Offset from the start of item for copying */

			/*

			 * We've reached the last slot in this leaf and we need

			 * to go to the next leaf.

/*

 * Delete an fsverity orphan

 *

 * @trans:  transaction to do the delete in

 * @inode:  inode to orphan

 *

 * Capture verity orphan specific logic that is repeated in the couple places

 * we delete verity orphans. Specifically, handling ENOENT and ignoring inodes

 * with 0 links.

 *

 * Returns zero on success or a negative error code on failure.

	/*

	 * If the inode has no links, it is either already unlinked, or was

	 * created with O_TMPFILE. In either case, it should have an orphan from

	 * that other operation. Rather than reference count the orphans, we

	 * simply ignore them here, because we only invoke the verity path in

	 * the orphan logic when i_nlink is 1.

/*

 * Rollback in-progress verity if we encounter an error.

 *

 * @inode:  inode verity had an error for

 *

 * We try to handle recoverable errors while enabling verity by rolling it back

 * and just failing the operation, rather than having an fs level error no

 * matter what. However, any error in rollback is unrecoverable.

 *

 * Returns 0 on success, negative error code on failure.

	/*

	 * 1 for updating the inode flag

	 * 1 for deleting the orphan

/*

 * Finalize making the file a valid verity file

 *

 * @inode:      inode to be marked as verity

 * @desc:       contents of the verity descriptor to write (not NULL)

 * @desc_size:  size of the verity descriptor

 *

 * Do the actual work of finalizing verity after successfully writing the Merkle

 * tree:

 *

 * - write out the descriptor items

 * - mark the inode with the verity flag

 * - delete the orphan item

 * - mark the ro compat bit

 * - clear the in progress bit

 *

 * Returns 0 on success, negative error code on failure.

 Write out the descriptor item */

 Write out the descriptor itself */

	/*

	 * 1 for updating the inode flag

	 * 1 for deleting the orphan

/*

 * fsverity op that begins enabling verity.

 *

 * @filp:  file to enable verity on

 *

 * Begin enabling fsverity for the file. We drop any existing verity items, add

 * an orphan and set the in progress bit.

 *

 * Returns 0 on success, negative error code on failure.

	/*

	 * This should almost never do anything, but theoretically, it's

	 * possible that we failed to enable verity on a file, then were

	 * interrupted or failed while rolling back, failed to cleanup the

	 * orphan, and finally attempt to enable verity again.

 1 for the orphan item */

/*

 * fsverity op that ends enabling verity.

 *

 * @filp:              file we are finishing enabling verity on

 * @desc:              verity descriptor to write out (NULL in error conditions)

 * @desc_size:         size of the verity descriptor (variable with signatures)

 * @merkle_tree_size:  size of the merkle tree in bytes

 *

 * If desc is null, then VFS is signaling an error occurred during verity

 * enable, and we should try to rollback. Otherwise, attempt to finish verity.

 *

 * Returns 0 on success, negative error code on error.

/*

 * fsverity op that gets the struct fsverity_descriptor.

 *

 * @inode:     inode to get the descriptor of

 * @buf:       output buffer for the descriptor contents

 * @buf_size:  size of the output buffer. 0 to query the size

 *

 * fsverity does a two pass setup for reading the descriptor, in the first pass

 * it calls with buf_size = 0 to query the size of the descriptor, and then in

 * the second pass it actually reads the descriptor off disk.

 *

 * Returns the size on success or a negative error code on failure.

/*

 * fsverity op that reads and caches a merkle tree page.

 *

 * @inode:         inode to read a merkle tree page for

 * @index:         page index relative to the start of the merkle tree

 * @num_ra_pages:  number of pages to readahead. Optional, we ignore it

 *

 * The Merkle tree is stored in the filesystem btree, but its pages are cached

 * with a logical position past EOF in the inode's mapping.

 *

 * Returns the page we read, or an ERR_PTR on error.

		/*

		 * We only insert uptodate pages, so !Uptodate has to be

		 * an error

	/*

	 * Merkle item keys are indexed from byte 0 in the merkle tree.

	 * They have the form:

	 *

	 * [ inode objectid, BTRFS_MERKLE_ITEM_KEY, offset in bytes ]

 Inserted and ready for fsverity */

 Did someone race us into inserting this page? */

/*

 * fsverity op that writes a Merkle tree block into the btree.

 *

 * @inode:          inode to write a Merkle tree block for

 * @buf:            Merkle tree data block to write

 * @index:          index of the block in the Merkle tree

 * @log_blocksize:  log base 2 of the Merkle tree block size

 *

 * Note that the block size could be different from the page size, so it is not

 * safe to assume that index is a page index.

 *

 * Returns 0 on success or negative error code on failure

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

 simple helper to search for an existing data extent at a given offset */

/*

 * helper function to lookup reference count and flags of a tree block.

 *

 * the head node for delayed ref is used to store the sum of all the

 * reference count modifications queued up in the rbtree. the head

 * node may also store the extent flags to set. This way you can check

 * to see what the reference count and extent flags would be if all of

 * the delayed refs are not processed.

	/*

	 * If we don't have skinny metadata, don't bother doing anything

	 * different

			/*

			 * Mutex was contended, block until it's released and try

			 * again

/*

 * Back reference rules.  Back refs have three main goals:

 *

 * 1) differentiate between all holders of references to an extent so that

 *    when a reference is dropped we can make sure it was a valid reference

 *    before freeing the extent.

 *

 * 2) Provide enough information to quickly find the holders of an extent

 *    if we notice a given block is corrupted or bad.

 *

 * 3) Make it easy to migrate blocks for FS shrinking or storage pool

 *    maintenance.  This is actually the same as #2, but with a slightly

 *    different use case.

 *

 * There are two kinds of back refs. The implicit back refs is optimized

 * for pointers in non-shared tree blocks. For a given pointer in a block,

 * back refs of this kind provide information about the block's owner tree

 * and the pointer's key. These information allow us to find the block by

 * b-tree searching. The full back refs is for pointers in tree blocks not

 * referenced by their owner trees. The location of tree block is recorded

 * in the back refs. Actually the full back refs is generic, and can be

 * used in all cases the implicit back refs is used. The major shortcoming

 * of the full back refs is its overhead. Every time a tree block gets

 * COWed, we have to update back refs entry for all pointers in it.

 *

 * For a newly allocated tree block, we use implicit back refs for

 * pointers in it. This means most tree related operations only involve

 * implicit back refs. For a tree block created in old transaction, the

 * only way to drop a reference to it is COW it. So we can detect the

 * event that tree block loses its owner tree's reference and do the

 * back refs conversion.

 *

 * When a tree block is COWed through a tree, there are four cases:

 *

 * The reference count of the block is one and the tree is the block's

 * owner tree. Nothing to do in this case.

 *

 * The reference count of the block is one and the tree is not the

 * block's owner tree. In this case, full back refs is used for pointers

 * in the block. Remove these full back refs, add implicit back refs for

 * every pointers in the new block.

 *

 * The reference count of the block is greater than one and the tree is

 * the block's owner tree. In this case, implicit back refs is used for

 * pointers in the block. Add full back refs for every pointers in the

 * block, increase lower level extents' reference counts. The original

 * implicit back refs are entailed to the new block.

 *

 * The reference count of the block is greater than one and the tree is

 * not the block's owner tree. Add implicit back refs for every pointer in

 * the new block, increase lower level extents' reference count.

 *

 * Back Reference Key composing:

 *

 * The key objectid corresponds to the first byte in the extent,

 * The key type is used to differentiate between types of back refs.

 * There are different meanings of the key offset for different types

 * of back refs.

 *

 * File extents can be referenced by:

 *

 * - multiple snapshots, subvolumes, or different generations in one subvol

 * - different files inside a single subvolume

 * - different offsets inside a file (bookend extents in file.c)

 *

 * The extent ref structure for the implicit back refs has fields for:

 *

 * - Objectid of the subvolume root

 * - objectid of the file holding the reference

 * - original offset in the file

 * - how many bookend extents

 *

 * The key offset for the implicit back refs is hash of the first

 * three fields.

 *

 * The extent ref structure for the full back refs has field for:

 *

 * - number of pointers in the tree leaf

 *

 * The key offset for the implicit back refs is the first byte of

 * the tree leaf

 *

 * When a file extent is allocated, The implicit back refs is used.

 * the fields are filled in:

 *

 *     (root_key.objectid, inode objectid, offset in file, 1)

 *

 * When a file extent is removed file truncation, we find the

 * corresponding implicit back refs and check the following fields:

 *

 *     (btrfs_header_owner(leaf), inode objectid, offset in file)

 *

 * Btree extents can be referenced by:

 *

 * - Different subvolumes

 *

 * Both the implicit back refs and the full back refs for tree blocks

 * only consist of key. The key offset for the implicit back refs is

 * objectid of block's owner tree. The key offset for the full back refs

 * is the first byte of parent block.

 *

 * When implicit back refs is used, information about the lowest key and

 * level of the tree block are required. These information are stored in

 * tree block info structure.

/*

 * is_data == BTRFS_REF_TYPE_BLOCK, tree block type is required,

 * is_data == BTRFS_REF_TYPE_DATA, data type is requiried,

 * is_data == BTRFS_REF_TYPE_ANY, either type is OK.

				/*

				 * Every shared one has parent tree block,

				 * which must be aligned to sector size.

				/*

				 * Every shared one has parent tree block,

				 * which must be aligned to sector size.

		/*

		 * If type is invalid, we should have bailed out earlier than

		 * this call.

/*

 * look for inline back ref. if back ref is found, *ref_ret is set

 * to the address of inline back ref, and 0 is returned.

 *

 * if back ref isn't found, *ref_ret is set to the address where it

 * should be inserted, and -ENOENT is returned.

 *

 * if insert is true and there are too many inline back refs, the path

 * points to the extent item, and -EAGAIN is returned.

 *

 * NOTE: inline back refs are ordered in the same way that back ref

 *	 items in the tree are ordered.

	/*

	 * Owner is our level, so we can just add one to get the level for the

	 * block we are interested in.

	/*

	 * We may be a newly converted file system which still has the old fat

	 * extent entries for metadata, so try and see if we have one of those.

		/*

		 * To add new inline back ref, we have to make sure

		 * there is no corresponding back ref item.

		 * For simplicity, we just do not add new inline back

		 * ref if there is any kind of item for this block

/*

 * helper to add new inline back ref

/*

 * helper to update/remove inline back ref

	/*

	 * If type is invalid, we should have bailed out after

	 * lookup_inline_extent_backref().

		/*

		 * We're adding refs to a tree block we already own, this

		 * should not happen at all.

 Skip any superblocks on this device. */

		/*

		 * Superblock spans beginning of range.  Adjust start and

		 * try again.

 Zone reset on a zoned filesystem */

 Send to replace target as well */

	/*

	 * Avoid races with device replace and make sure our bioc has devices

	 * associated to its stripes that don't go away while we are discarding.

 Tell the block device(s) that the sectors can be discarded */

		/*

		 * Error can be -ENOMEM, -ENOENT (no such chunk mapping) or

		 * -EOPNOTSUPP. For any such error, @num_bytes is not updated,

		 * thus we can't continue anyway.

				/*

				 * Logic errors or -ENOMEM, or -EIO, but

				 * unlikely to happen.

				 *

				 * And since there are two loops, explicitly

				 * go to out to avoid confusion.

			/*

			 * Just in case we get back EOPNOTSUPP for some reason,

			 * just ignore the return value so we don't screw up

			 * people calling discard_extent.

 Can return -ENOMEM */

/*

 * __btrfs_inc_extent_ref - insert backreference for a given extent

 *

 * The counterpart is in __btrfs_free_extent(), with examples and more details

 * how it works.

 *

 * @trans:	    Handle of transaction

 *

 * @node:	    The delayed ref node used to get the bytenr/length for

 *		    extent whose references are incremented.

 *

 * @parent:	    If this is a shared extent (BTRFS_SHARED_DATA_REF_KEY/

 *		    BTRFS_SHARED_BLOCK_REF_KEY) then it holds the logical

 *		    bytenr of the parent block. Since new extents are always

 *		    created with indirect references, this will only be the case

 *		    when relocating a shared extent. In that case, root_objectid

 *		    will be BTRFS_TREE_RELOC_OBJECTID. Otherwise, parent must

 *		    be 0

 *

 * @root_objectid:  The id of the root where this modification has originated,

 *		    this can be either one of the well-known metadata trees or

 *		    the subvolume id which references this extent.

 *

 * @owner:	    For data extents it is the inode number of the owning file.

 *		    For metadata extents this parameter holds the level in the

 *		    tree of the extent.

 *

 * @offset:	    For metadata extents the offset is ignored and is currently

 *		    always passed as 0. For data extents it is the fileoffset

 *		    this extent belongs to.

 *

 * @refs_to_add     Number of references to add

 *

 * @extent_op       Pointer to a structure, holding information necessary when

 *                  updating a tree block's flags

 *

 this will setup the path even if it fails to insert the back ref */

	/*

	 * Ok we had -EAGAIN which means we didn't have space to insert and

	 * inline extent ref, so just update the reference count and add a

	 * normal backref.

 now insert the actual backref */

 helper function to actually process a single delayed ref entry */

	/*

	 * Select a delayed ref of type BTRFS_ADD_DELAYED_REF first.

	 * This is to prevent a ref count from going down to zero, which deletes

	 * the extent item from the extent tree, when there still are references

	 * to add, which would fail because they would not find the extent item.

 Dropping this ref head update. */

	/*

	 * We had csum deletions accounted for in our delayed refs rsv, we need

	 * to drop the csum leaves for this update from our delayed_refs_rsv.

	/*

	 * Need to drop our head ref lock and re-acquire the delayed ref lock

	 * and then re-check to make sure nobody got added.

	/*

	 * Grab the lock that says we are going to process all the refs for

	 * this head

	/*

	 * We may have dropped the spin lock to get the head mutex lock, and

	 * that might have given someone else time to free the head.  If that's

	 * true, it has been removed from our list and we can move on.

		/*

		 * When we play the delayed ref, also correct the ref_mod on

		 * head

		/*

		 * Record the must_insert_reserved flag before we drop the

		 * spin lock.

/*

 * Returns 0 on success or if called with an already aborted transaction.

 * Returns -ENOMEM or -EIO on failure and will abort the transaction.

		/*

		 * We need to try and merge add/drops of the same ref since we

		 * can run into issues with relocate dropping the implicit ref

		 * and then it being added back again before the drop can

		 * finish.  If we merged anything we need to re-loop so we can

		 * get a good ref.

		 * Or we can get node references of the same type that weren't

		 * merged when created due to bumps in the tree mod seq, and

		 * we need to merge them to prevent adding an inline extent

		 * backref before dropping it (triggering a BUG_ON at

		 * insert_inline_extent_backref()).

			/*

			 * Error, btrfs_run_delayed_refs_for_head already

			 * unlocked everything so just bail out

			/*

			 * Success, perform the usual cleanup of a processed

			 * head

 We dropped our lock, we need to loop. */

		/*

		 * Either success case or btrfs_run_delayed_refs_for_head

		 * returned -EAGAIN, meaning we need to select another head

	/*

	 * We don't want to include ref heads since we can have empty ref heads

	 * and those will drastically skew our runtime down since we just do

	 * accounting, no actual extent tree updates.

		/*

		 * We weigh the current average higher than our current runtime

		 * to avoid large swings in the average.

 div by 4 */

/*

 * Normally delayed refs get processed in ascending bytenr order. This

 * correlates in most cases to the order added. To expose dependencies on this

 * order, we start to process the tree in the middle instead of the beginning

/*

 * this starts processing the delayed reference count updates and

 * extent insertions we have queued up so far.  count can be

 * 0, which means to process everything in the tree at the start

 * of the run (but not newly added entries), or it can be some target

 * number you'd like to process.

 *

 * Returns 0 on success or if called with an aborted transaction

 * Returns <0 on error and aborts the transaction

 We'll clean this up in btrfs_cleanup_transaction */

 Mutex was contended, block until it's released and retry. */

		/*

		 * Mutex was contended, block until it's released and let

		 * caller try again

	/*

	 * XXX: We should replace this with a proper search function in the

	 * future.

 If it's a shared ref we know a cross reference exists */

		/*

		 * If our ref doesn't match the one we're currently looking at

		 * then we have a cross reference.

 Corruption */

 If extent item has more than 1 inline ref then it's shared */

	/*

	 * If extent created before last snapshot => it's shared unless the

	 * snapshot has been deleted. Use the heuristic if strict is false.

 If this extent has SHARED_DATA_REF then it's shared */

 Logic error */

/*

 * this function must be called within transaction

	/*

	 * pull in the free space cache (if any) so that our pin

	 * removes the free space from the cache.  We have load_only set

	 * to one because the slow code to read in the free extents does check

	 * the pinned extents.

	/*

	 * Make sure we wait until the cache is completely built in case it is

	 * missing or is invalid and therefore needs to be rebuilt.

 remove us from the free space cache (if we're there at all) */

	/*

	 * Make sure we wait until the cache is completely built in case it is

	 * missing or is invalid and therefore needs to be rebuilt.

/*

 * Returns the free cluster for the given space info and sets empty_cluster to

 * what it should be based on the mount options.

 Logic error */

		/*

		 * If this space cluster has been marked as fragmented and we've

		 * unpinned enough in this block group to potentially allow a

		 * cluster to be created inside of it go ahead and clear the

		 * fragmented check.

 Need reset before reusing in a zoned block group */

 Add to any tickets we may have */

	/*

	 * Transaction is finished.  We don't need the lock anymore.  We

	 * do need to clean up the block groups in case of a transaction

	 * abort.

/*

 * Drop one or more refs of @node.

 *

 * 1. Locate the extent refs.

 *    It's either inline in EXTENT/METADATA_ITEM or in keyed SHARED_* item.

 *    Locate it, then reduce the refs number or remove the ref line completely.

 *

 * 2. Update the refs count in EXTENT/METADATA_ITEM

 *

 * Inline backref case:

 *

 * in extent tree we have:

 *

 * 	item 0 key (13631488 EXTENT_ITEM 1048576) itemoff 16201 itemsize 82

 *		refs 2 gen 6 flags DATA

 *		extent data backref root FS_TREE objectid 258 offset 0 count 1

 *		extent data backref root FS_TREE objectid 257 offset 0 count 1

 *

 * This function gets called with:

 *

 *    node->bytenr = 13631488

 *    node->num_bytes = 1048576

 *    root_objectid = FS_TREE

 *    owner_objectid = 257

 *    owner_offset = 0

 *    refs_to_drop = 1

 *

 * Then we should get some like:

 *

 * 	item 0 key (13631488 EXTENT_ITEM 1048576) itemoff 16201 itemsize 82

 *		refs 1 gen 6 flags DATA

 *		extent data backref root FS_TREE objectid 258 offset 0 count 1

 *

 * Keyed backref case:

 *

 * in extent tree we have:

 *

 *	item 0 key (13631488 EXTENT_ITEM 1048576) itemoff 3971 itemsize 24

 *		refs 754 gen 6 flags DATA

 *	[...]

 *	item 2 key (13631488 EXTENT_DATA_REF <HASH>) itemoff 3915 itemsize 28

 *		extent data backref root FS_TREE objectid 866 offset 0 count 1

 *

 * This function get called with:

 *

 *    node->bytenr = 13631488

 *    node->num_bytes = 1048576

 *    root_objectid = FS_TREE

 *    owner_objectid = 866

 *    owner_offset = 0

 *    refs_to_drop = 1

 *

 * Then we should get some like:

 *

 *	item 0 key (13631488 EXTENT_ITEM 1048576) itemoff 3971 itemsize 24

 *		refs 753 gen 6 flags DATA

 *

 * And that (13631488 EXTENT_DATA_REF <HASH>) gets removed.

		/*

		 * Either the inline backref or the SHARED_DATA_REF/

		 * SHARED_BLOCK_REF is found

		 *

		 * Here is a quick path to locate EXTENT/METADATA_ITEM.

		 * It's possible the EXTENT/METADATA_ITEM is near current slot.

 Quick path didn't find the EXTEMT/METADATA_ITEM */

 Must be SHARED_* item, remove the backref first */

 Slow path to locate EXTENT/METADATA_ITEM */

				/*

				 * Couldn't find our skinny metadata item,

				 * see if we have ye olde extent item.

		/*

		 * In the case of inline back ref, reference count will

		 * be updated by remove_extent_backref

 In this branch refs == 1 */

				/*

				 * No inline ref, we must be at SHARED_* item,

				 * And it's single ref, it must be:

				 * |	extent_slot	  ||extent_slot + 1|

				 * [ EXTENT/METADATA_ITEM ][ SHARED_* ITEM ]

	/*

	 * Leaf dump can take up a lot of log buffer, so we only do full leaf

	 * dump for debug build.

/*

 * when we free an block, it is possible (and likely) that we free the last

 * delayed ref for that extent as well.  This searches the delayed ref tree for

 * a given extent, and if there are no other delayed refs to be processed, it

 * removes it from the tree.

	/*

	 * waiting for the lock here would deadlock.  If someone else has it

	 * locked they are already in the process of dropping it anyway

 -ENOMEM */

		/*

		 * If this is a leaf and there are tree mod log users, we may

		 * have recorded mod log operations that point to this leaf.

		 * So we must make sure no one reuses this leaf's extent before

		 * mod log operations are applied to a node, otherwise after

		 * rewinding a node using the mod log operations we get an

		 * inconsistent btree, as the leaf's extent may now be used as

		 * a node or leaf for another different btree.

		 * We are safe from races here because at this point no other

		 * node or root points to this extent buffer, so if after this

		 * check a new tree mod log user joins, it will not be able to

		 * find a node pointing to this leaf and record operations that

		 * point to this leaf.

		/*

		 * Deleting the buffer, clear the corrupt flag since it doesn't

		 * matter anymore.

 Can return -ENOMEM */

	/*

	 * tree log blocks never actually go into the extent allocation

	 * tree, just update pinning info and exit early.

 unlocks the pinned mutex */

 We should only have one-level nested. */

/*

 * Structure used internally for find_free_extent() function.  Wraps needed

 * parameters.

 Basic allocation info */

 Where to start the search inside the bg */

 For clustered allocation */

 Allocation is called for tree-log */

 Allocation is called for data relocation */

 RAID index, converted from flags */

	/*

	 * Current loop number, check find_free_extent_update_loop() for details

	/*

	 * Whether we're refilling a cluster, if true we need to re-search

	 * current block group but don't try to refill the cluster again.

	/*

	 * Whether we're updating free space cache, if true we need to re-search

	 * current block group but don't try updating free space cache again.

 If current block group is cached */

 Max contiguous hole found */

 Total free space from free space cache, not always contiguous */

 Found result */

 Hint where to start looking for an empty space */

 Allocation policy */

/*

 * Helper function for find_free_extent().

 *

 * Return -ENOENT to inform caller that we need fallback to unclustered mode.

 * Return -EAGAIN to inform caller that we need to re-search this block group

 * Return >0 to inform caller that we find nothing

 * Return 0 means we have found a location and set ffe_ctl->found_offset.

 We have a block, we're done */

	/*

	 * If we are on LOOP_NO_EMPTY_SIZE, we can't set up a new clusters, so

	 * lets just skip it and let the allocator find whatever block it can

	 * find. If we reach this point, we will have tried the cluster

	 * allocator plenty of times and not have found anything, so we are

	 * likely way too fragmented for the clustering stuff to find anything.

	 *

	 * However, if the cluster is taken from the current block group,

	 * release the cluster first, so that we stand a better chance of

	 * succeeding in the unclustered allocation.

 This cluster didn't work out, free it and start over */

 Now pull our allocation out of this cluster */

 We found one, proceed */

	/*

	 * At this point we either didn't find a cluster or we weren't able to

	 * allocate a block from our cluster.  Free the cluster we've been

	 * trying to use, and go to the next block group.

/*

 * Return >0 to inform caller that we find nothing

 * Return 0 when we found an free extent and set ffe_ctrl->found_offset

 * Return -EAGAIN to inform caller that we need to re-search this block group

	/*

	 * We are doing an unclustered allocation, set the fragmented flag so

	 * we don't bother trying to setup a cluster again until we get more

	 * space.

	/*

	 * If we didn't find a chunk, and we haven't failed on this block group

	 * before, and this block group is in the middle of caching and we are

	 * ok with waiting, then go ahead and wait for progress to be made, and

	 * set @retry_unclustered to true.

	 *

	 * If @retry_unclustered is true then we've already waited on this

	 * block group once and should move on to the next block group.

 We want to try and use the cluster allocator, so lets look there */

 ret == -ENOENT case falls through */

/*

 * Tree-log block group locking

 * ============================

 *

 * fs_info::treelog_bg_lock protects the fs_info::treelog_bg which

 * indicates the starting address of a block group, which is reserved only

 * for tree-log metadata.

 *

 * Lock nesting

 * ============

 *

 * space_info::lock

 *   block_group::lock

 *     fs_info::treelog_bg_lock

/*

 * Simple allocator for sequential-only block group. It only allows sequential

 * allocation. No need to play with trees. This function also reserves the

 * bytes as in btrfs_add_reserved_bytes.

	/*

	 * Do not allow non-tree-log blocks in the dedicated tree-log block

	 * group, and vice versa.

	/*

	 * Do not allow non-relocation blocks in the dedicated relocation block

	 * group, and vice versa.

 Check RO and no space case before trying to activate it */

	/*

	 * Do not allow currently using block group to be tree-log dedicated

	 * block group.

	/*

	 * Do not allow currently used block group to be the data relocation

	 * dedicated block group.

			/*

			 * With sequential allocator, free space is always

			 * contiguous

	/*

	 * We do not check if found_offset is aligned to stripesize. The

	 * address is anyway rewritten when using zone append writing.

 Nothing to do */

 Nothing to do */

		/*

		 * If we can't allocate a new chunk we've already looped through

		 * at least once, move on to the NO_EMPTY_SIZE case.

 Give up here */

/*

 * Return >0 means caller needs to re-search for free extent

 * Return 0 means we have the needed free extent.

 * Return <0 means we failed to locate any free extent.

		/*

		 * If we have enough free space left in an already active block

		 * group and we can't activate any other zone now, retry the

		 * active ones with a smaller allocation size.  Returning early

		 * from here will tell btrfs_reserve_extent() to haven the

		 * size.

	/*

	 * LOOP_CACHING_NOWAIT, search partially cached block groups, kicking

	 *			caching kthreads as we move along

	 * LOOP_CACHING_WAIT, search everything, and wait if our bg is caching

	 * LOOP_ALLOC_CHUNK, force a chunk allocation and try again

	 * LOOP_NO_EMPTY_SIZE, set empty_size and empty_cluster to 0 and try

	 *		       again

			/*

			 * We want to skip the LOOP_CACHING_WAIT step if we

			 * don't have any uncached bgs and we've already done a

			 * full search through.

 Do not bail out on ENOSPC since we can do more. */

			/*

			 * Don't loop again if we already have no empty_size and

			 * no empty_cluster.

	/*

	 * If our free space is heavily fragmented we may not be able to make

	 * big contiguous allocations, so instead of doing the expensive search

	 * for free space, simply return ENOSPC with our max_extent_size so we

	 * can go ahead and search for a more manageable chunk.

	 *

	 * If our max_extent_size is large enough for our allocation simply

	 * disable clustering since we will likely not be able to find enough

	 * space to create a cluster and induce latency trying.

			/*

			 * We still set window_start so we can keep track of the

			 * last place we found an allocation to try and save

			 * some time.

/*

 * walks the btree of allocated extents and find a hole of a given size.

 * The key ins is changed to record the hole:

 * ins->objectid == start position

 * ins->flags = BTRFS_EXTENT_ITEM_KEY

 * ins->offset == the size of the hole.

 * Any available blocks before search_start are skipped.

 *

 * If there is no suitable free space, we will record the max size of

 * the free space extent currently.

 *

 * The overall logic and call chain:

 *

 * find_free_extent()

 * |- Iterate through all block groups

 * |  |- Get a valid block group

 * |  |- Try to do clustered allocation in that block group

 * |  |- Try to do unclustered allocation in that block group

 * |  |- Check if the result is valid

 * |  |  |- If valid, then exit

 * |  |- Jump to next block group

 * |

 * |- Push harder to find free extents

 *    |- If not found, re-iterate all block groups

 For clustered allocation */

 For clustered allocation */

		/*

		 * we don't want to use the block group if it doesn't match our

		 * allocation bits, or if its not cached.

		 *

		 * However if we are re-searching with an ideal block group

		 * picked out then we don't care that the block group is cached.

				/*

				 * someone is removing this block group,

				 * we can't jump into the have_block_group

				 * target because our list pointers are not

				 * valid

 If the block group is read-only, we can skip it entirely. */

		/*

		 * this can happen if we end up cycling through all the

		 * raid types, but we want to make sure we only allocate

		 * for the proper type.

			/*

			 * if they asked for extra copies and this block group

			 * doesn't provide them, bail.  This does allow us to

			 * fill raid0 from raid1.

			/*

			 * This block group has different flags than we want.

			 * It's possible that we have MIXED_GROUP flag but no

			 * block group is mixed.  Just skip such block group.

			/*

			 * If we get ENOMEM here or something else we want to

			 * try other block groups, because it may not be fatal.

			 * However if we can't find anything else we need to

			 * save our return here so that we return the actual

			 * error that caused problems, not ENOSPC.

 Checks */

 move on to the next group */

 we are all good, lets return */

		/*

		 * Use ffe_ctl->total_free_space as fallback if we can't find

		 * any contiguous hole.

/*

 * btrfs_reserve_extent - entry point to the extent allocator. Tries to find a

 *			  hole that is at least as big as @num_bytes.

 *

 * @root           -	The root that will contain this extent

 *

 * @ram_bytes      -	The amount of space in ram that @num_bytes take. This

 *			is used for accounting purposes. This value differs

 *			from @num_bytes only in the case of compressed extents.

 *

 * @num_bytes      -	Number of bytes to allocate on-disk.

 *

 * @min_alloc_size -	Indicates the minimum amount of space that the

 *			allocator should try to satisfy. In some cases

 *			@num_bytes may be larger than what is required and if

 *			the filesystem is fragmented then allocation fails.

 *			However, the presence of @min_alloc_size gives a

 *			chance to try and satisfy the smaller allocation.

 *

 * @empty_size     -	A hint that you plan on doing more COW. This is the

 *			size in bytes the allocator should try to find free

 *			next to the block it returns.  This is just a hint and

 *			may be ignored by the allocator.

 *

 * @hint_byte      -	Hint to the allocator to start searching above the byte

 *			address passed. It might be ignored.

 *

 * @ins            -	This key is modified to record the found hole. It will

 *			have the following values:

 *			ins->objectid == start position

 *			ins->flags = BTRFS_EXTENT_ITEM_KEY

 *			ins->offset == the size of the hole.

 *

 * @is_data        -	Boolean flag indicating whether an extent is

 *			allocated for data (true) or metadata (false)

 *

 * @delalloc       -	Boolean flag indicating whether this allocation is for

 *			delalloc or not. If 'true' data_rwsem of block groups

 *			is going to be acquired.

 *

 *

 * Returns 0 when an allocation succeeded or < 0 when an error occurred. In

 * case -ENOSPC is returned then @ins->offset will contain the size of the

 * largest available hole the allocator managed to find.

 -ENOENT, logic error */

 -ENOENT, logic error */

/*

 * this is used by the tree logging recovery code.  It records that

 * an extent has been allocated and makes sure to clear the free

 * space cache bits as well

	/*

	 * Mixed block groups will exclude before processing the log so we only

	 * need to do the exclude dance if this fs isn't mixed.

	/*

	 * Extra safety check in case the extent tree is corrupted and extent

	 * allocator chooses to use a tree block which is already used and

	 * locked.

	/*

	 * This needs to stay, because we could allocate a freed block from an

	 * old tree into a new tree, so we need to make sure this new block is

	 * set to the appropriate level and owner.

		/*

		 * we allow two log transactions at a time, use different

		 * EXTENT bit to differentiate dirty pages.

 this returns a buffer locked for blocking */

/*

 * finds a free extent and does all the dirty work required for allocation

 * returns the tree buffer or an ERR_PTR on error.

 We don't lock the tree block, it's OK to be racy here */

 We don't care about errors in readahead. */

/*

 * helper to process tree block while walking down the tree.

 *

 * when wc->stage == UPDATE_BACKREF, this function updates

 * back refs for pointers in the block.

 *

 * NOTE: return value 1 means we should stop walking down.

	/*

	 * when reference count of tree block is 1, it won't increase

	 * again. once full backref flag is set, we never clear it.

 wc->stage == UPDATE_BACKREF */

 -ENOMEM */

 -ENOMEM */

 -ENOMEM */

	/*

	 * the block is shared by multiple trees, so it's not good to

	 * keep the tree lock

/*

 * This is used to verify a ref exists for this root to deal with a bug where we

 * would have a drop_progress key that hadn't been updated properly.

/*

 * helper to process tree block pointer.

 *

 * when wc->stage == DROP_REFERENCE, this function checks

 * reference count of the block pointed to. if the block

 * is shared and we need update back refs for the subtree

 * rooted at the block, this function changes wc->stage to

 * UPDATE_BACKREF. if the block is shared and there is no

 * need to update back, this function drops the reference

 * to the block.

 *

 * NOTE: return value 1 means we should stop walking down.

	/*

	 * if the lower level block was created before the snapshot

	 * was created, we know there is no need to update back refs

	 * for the subtree

		/*

		 * If we had a drop_progress we need to verify the refs are set

		 * as expected.  If we find our ref then we know that from here

		 * on out everything should be correct, and we can clear the

		 * ->restarted flag.

		/*

		 * Reloc tree doesn't contribute to qgroup numbers, and we have

		 * already accounted them at merge time (replace_path),

		 * thus we could skip expensive subtree trace here.

		/*

		 * We need to update the next key in our walk control so we can

		 * update the drop_progress key accordingly.  We don't care if

		 * find_next_key doesn't find a key because that means we're at

		 * the end and are going to clean up now.

/*

 * helper to process tree block while walking up the tree.

 *

 * when wc->stage == DROP_REFERENCE, this function drops

 * reference count on the block.

 *

 * when wc->stage == UPDATE_BACKREF, this function changes

 * wc->stage back to DROP_REFERENCE if we changed wc->stage

 * to UPDATE_BACKREF previously while processing the block.

 *

 * NOTE: return value 1 means we should stop walking up.

		/*

		 * check reference count again if the block isn't locked.

		 * we should start walking down the tree again if reference

		 * count is one.

 wc->stage == DROP_REFERENCE */

 -ENOMEM */

 make block locked assertion in btrfs_clean_tree_block happy */

/*

 * drop a subvolume tree.

 *

 * this function traverses the tree freeing any blocks that only

 * referenced by the tree.

 *

 * when a shared tree block is found. this function decreases its

 * reference count by one. if update_ref is true, this function

 * also make sure backrefs for the shared block and all lower level

 * blocks are properly updated.

 *

 * If called with for_reloc == 0, may exit early with -EAGAIN

	/*

	 * Use join to avoid potential EINTR from transaction start. See

	 * wait_reserve_ticket and the whole reservation callchain.

	/*

	 * This will help us catch people modifying the fs tree while we're

	 * dropping it.  It is unsafe to mess with the fs tree while it's being

	 * dropped as we unlock the root node and parent nodes as we walk down

	 * the tree, assuming nothing will change.  If something does change

	 * then we'll have stale information and drop references to blocks we've

	 * already dropped.

		/*

		 * unlock our path, this is safe because only this

		 * function is allowed to delete this snapshot

		       /*

			* Use join to avoid potential EINTR from transaction

			* start. See wait_reserve_ticket and the whole

			* reservation callchain.

			/* if we fail to delete the orphan item this time

			 * around, it'll get picked up the next time.

			 *

			 * The most common failure here is just -ENOENT.

	/*

	 * This subvolume is going to be completely dropped, and won't be

	 * recorded as dirty roots, thus pertrans meta rsv will not be freed at

	 * commit transaction time.  So free it here manually.

	/*

	 * So if we need to stop dropping the snapshot for whatever reason we

	 * need to make sure to add it back to the dead root list so that we

	 * keep trying to do the work later.  This also cleans up roots if we

	 * don't have it in the radix (like when we recover after a power fail

	 * or unmount) so we don't leak memory.

/*

 * drop subtree rooted at tree block 'node'.

 *

 * NOTE: this function will unlock and release tree block 'node'

 * only used by relocation code

/*

 * helper to account the unused space of all the readonly block group in the

 * space_info. takes mirrors into account.

 It's df, we don't care if it's racy */

/*

 * It used to be that old block groups would be left around forever.

 * Iterating over them would be enough to trim unused space.  Since we

 * now automatically remove them, we also need to iterate over unallocated

 * space.

 *

 * We don't want a transaction for this since the discard may take a

 * substantial amount of time.  We don't require that a transaction be

 * running, but we do need to take a running transaction into account

 * to ensure that we're not discarding chunks that were released or

 * allocated in the current transaction.

 *

 * Holding the chunks lock will prevent other threads from allocating

 * or releasing chunks, but it won't prevent a running transaction

 * from committing and releasing the memory that the pending chunks

 * list head uses.  For that, we need to take a reference to the

 * transaction and hold the commit root sem.  We only need to hold

 * it while performing the free space search since we have already

 * held back allocations.

 Discard not supported = nothing to do. */

 Not writable = nothing to do. */

 No free space = nothing to do. */

 Check if there are any CHUNK_* bits left */

 Ensure we skip the reserved area in the first 1M */

		/*

		 * If find_first_clear_extent_bit find a range that spans the

		 * end of the device it will set end to -1, in this case it's up

		 * to the caller to trim the value to the size of the device.

 We didn't find any extents */

/*

 * Trim the whole filesystem by:

 * 1) trimming the free space in each block group

 * 2) trimming the unallocated space on each device

 *

 * This will also continue trimming even if a block group or device encounters

 * an error.  The return value will be the last error, or 0 if nothing bad

 * happens.

	/*

	 * Check range overflow if range->len is set.

	 * The default range->len is U64_MAX.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/**

 * Set inode's size according to filesystem options

 *

 * @inode:      inode we want to update the disk_i_size for

 * @new_i_size: i_size we want to set to, 0 if we use i_size

 *

 * With NO_HOLES set this simply sets the disk_is_size to whatever i_size_read()

 * returns as it is perfectly fine with a file that has holes without hole file

 * extent items.

 *

 * However without NO_HOLES we need to only return the area that is contiguous

 * from the 0 offset of the file.  Otherwise we could end up adjust i_size up

 * to an extent that has a gap in between.

 *

 * Finally new_i_size should only be set in the case of truncate where we're not

 * ready to use i_size_read() as the limiter yet.

/**

 * Mark range within a file as having a new extent inserted

 *

 * @inode: inode being modified

 * @start: start file offset of the file extent we've inserted

 * @len:   logical length of the file extent item

 *

 * Call when we are inserting a new file extent where there was none before.

 * Does not need to call this in the case where we're replacing an existing file

 * extent, however if not sure it's fine to call this multiple times.

 *

 * The start and len must match the file extent item, so thus must be sectorsize

 * aligned.

/**

 * Marks an inode range as not having a backing extent

 *

 * @inode: inode being modified

 * @start: start file offset of the file extent we've inserted

 * @len:   logical length of the file extent item

 *

 * Called when we drop a file extent, for example when we truncate.  Doesn't

 * need to be called for cases where we're replacing a file extent, like when

 * we've COWed a file extent.

 *

 * The start and len must match the file extent item, so thus must be sectorsize

 * aligned.

 Can't happen */

/*

 * Find checksums for logical bytenr range [disk_bytenr, disk_bytenr + len) and

 * estore the result to @dst.

 *

 * Return >0 for the number of sectors we found.

 * Return 0 for the range [disk_bytenr, disk_bytenr + sectorsize) has no csum

 * for it. Caller may want to try next sector until one range is hit.

 * Return <0 for fatal error.

 Check if the current csum item covers disk_bytenr */

 Current item doesn't contain the desired range, search again */

/*

 * Locate the file_offset of @cur_disk_bytenr of a @bio.

 *

 * Bio of btrfs represents read range of

 * [bi_sector << 9, bi_sector << 9 + bi_size).

 * Knowing this, we can iterate through each bvec to locate the page belong to

 * @cur_disk_bytenr and get the file offset.

 *

 * @inode is used to determine if the bvec page really belongs to @inode.

 *

 * Return 0 if we can't find the file offset

 * Return >0 if we find the file offset and restore it to @file_offset_ret

/**

 * Lookup the checksum for the read bio in csum tree.

 *

 * @inode: inode that the bio is for.

 * @bio: bio to look up.

 * @dst: Buffer of size nblocks * btrfs_super_csum_size() used to return

 *       checksum (nblocks = bio->bi_iter.bi_size / fs_info->sectorsize). If

 *       NULL, the checksum buffer is allocated and returned in

 *       btrfs_bio(bio)->csum instead.

 *

 * Return: BLK_STS_RESOURCE if allocating memory fails, BLK_STS_OK otherwise.

	/*

	 * This function is only called for read bio.

	 *

	 * This means two things:

	 * - All our csums should only be in csum tree

	 *   No ordered extents csums, as ordered extents are only for write

	 *   path.

	 * - No need to bother any other info from bvec

	 *   Since we're looking up csums, the only important info is the

	 *   disk_bytenr and the length, which can be extracted from bi_iter

	 *   directly.

	/*

	 * If requested number of sectors is larger than one leaf can contain,

	 * kick the readahead for csum tree.

	/*

	 * the free space stuff is only read when it hasn't been

	 * updated in the current transaction.  So, we can safely

	 * read from the commit root and sidestep a nasty deadlock

	 * between reading the free space cache and updating the csum tree.

		/*

		 * Although both cur_disk_bytenr and orig_disk_bytenr is u64,

		 * we're calculating the offset to the bio start.

		 *

		 * Bio size is limited to UINT_MAX, thus unsigned int is large

		 * enough to contain the raw result, not to mention the right

		 * shifted result.

			/*

			 * Either we hit a critical error or we didn't find

			 * the csum.

			 * Either way, we put zero into the csums dst, and skip

			 * to the next sector.

			/*

			 * For data reloc inode, we need to mark the range

			 * NODATASUM so that balance won't report false csum

			 * error.

/*

 * btrfs_csum_one_bio - Calculates checksums of the data contained inside a bio

 * @inode:	 Owner of the data inside the bio

 * @bio:	 Contains the data to be checksummed

 * @file_start:  offset in file this bio begins to describe

 * @contig:	 Boolean. If true/1 means all bio vecs in this bio are

 *		 contiguous and they begin at @file_start in the file. False/0

 *		 means this bio can contain potentially discontiguous bio vecs

 *		 so the logical offset of each should be calculated separately.

 shut up gcc */

			/*

			 * The bio range is not covered by any ordered extent,

			 * must be a code logic error.

 -ENOMEM */

 Logic error */

/*

 * helper function for csum removal, this expects the

 * key to describe the csum pointed to by the path, and it expects

 * the csum to overlap the range [bytenr, len]

 *

 * The csum should not be entirely contained in the range and the

 * range should not be entirely contained in the csum.

 *

 * This calls btrfs_truncate_item with the correct args based on the

 * overlap, and fixes up the key as required.

		/*

		 *         [ bytenr - len ]

		 *         [   ]

		 *   [csum     ]

		 *   A simple truncate off the end of the item

		/*

		 *         [ bytenr - len ]

		 *                 [ ]

		 *                 [csum     ]

		 * we need to truncate from the beginning of the csum

/*

 * deletes the csum items from the csum tree for a given

 * range of bytes.

 this csum ends before we start, we're done */

 delete the entire item, it is inside our range */

			/*

			 * Check how many csum items preceding this one in this

			 * leaf correspond to our range and then delete them all

			 * at once.

			/*

			 *        [ bytenr - len ]

			 *     [csum                ]

			 *

			 * Our bytes are in the middle of the csum,

			 * we need to split this item and insert a new one.

			 *

			 * But we can't drop the path because the

			 * csum could change, get removed, extended etc.

			 *

			 * The trick here is the max size of a csum item leaves

			 * enough room in the tree block for a single

			 * item header.  So, we split the item in place,

			 * adding a new header pointing to the existing

			 * bytes.  Then we loop around again and we have

			 * a nicely formed csum item that we can neatly

			 * truncate.

			/*

			 * btrfs_split_item returns -EAGAIN when the

			 * item changed size or key

 we found one, but it isn't big enough yet */

 already at max size, make a new one */

 We didn't find a csum item, insert one. */

	/*

	 * At this point, we know the tree has a checksum item that ends at an

	 * offset matching the start of the checksum range we want to insert.

	 * We try to extend that item as much as possible and then add as many

	 * checksums to it as they fit.

	 *

	 * First check if the leaf has enough free space for at least one

	 * checksum. If it has go directly to the item extension code, otherwise

	 * release the path and do a search for insertion before the extension.

		/*

		 * A log tree can already have checksum items with a subset of

		 * the checksums we are trying to log. This can happen after

		 * doing a sequence of partial writes into prealloc extents and

		 * fsyncs in between, with a full fsync logging a larger subrange

		 * of an extent for which a previous fast fsync logged a smaller

		 * subrange. And this happens in particular due to merging file

		 * extent items when we complete an ordered extent for a range

		 * covered by a prealloc extent - this is done at

		 * btrfs_mark_extent_written().

		 *

		 * So if we try to extend the previous checksum item, which has

		 * a range that ends at the start of the range we want to insert,

		 * make sure we don't extend beyond the start offset of the next

		 * checksum item. If we are at the last item in the leaf, then

		 * forget the optimization of extending and add a new checksum

		 * item - it is not worth the complexity of releasing the path,

		 * getting the first key for the next leaf, repeat the btree

		 * search, etc, because log trees are temporary anyway and it

		 * would only save a few bytes of leaf space.

		/*

		 * Initialize orig_start and block_len with the same values

		 * as in inode.c:btrfs_get_extent().

/*

 * Returns the end offset (non inclusive) of the file extent item the given path

 * points to. If it points to an inline extent, the returned offset is rounded

 * up to the sector size.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/*

 * Read a root item from the tree. In case we detect a root item smaller then

 * sizeof(root_item), we know it's an old version of the root structure and

 * initialize all new fields to zero. The same happens if we detect mismatching

 * generation numbers as then we know the root was once mounted with an older

 * kernel that was not aware of the root item structure change.

 Clear all members from generation_v2 onwards. */

/*

 * btrfs_find_root - lookup the root by the key.

 * root: the root of the root tree

 * search_key: the key to search

 * path: the path we search

 * root_item: the root item of the tree we look for

 * root_key: the root key of the tree we look for

 *

 * If ->offset of 'search_key' is -1ULL, it means we are not sure the offset

 * of the search key, just lookup the root with the highest offset for a

 * given objectid.

 *

 * If we find something return 0, otherwise > 0, < 0 on error.

 the search key is exact */

 Logical error */

/*

 * copy the data in 'item' into the btree

	/*

	 * If this is the first time we update the root item which originated

	 * from an older kernel, we need to enlarge the item size to make room

	 * for the added fields.

	/*

	 * Update generation_v2 so at the next mount we know the new root

	 * fields are valid.

	/*

	 * Make sure generation v1 and v2 match. See update_root for details.

 drop the root item for 'key' from the tree root */

/*

 * add a btrfs_root_ref item.  type is either BTRFS_ROOT_REF_KEY

 * or BTRFS_ROOT_BACKREF_KEY.

 *

 * The dirid, sequence, name and name_len refer to the directory entry

 * that is referencing the root.

 *

 * For a forward ref, the root_id is the id of the tree referencing

 * the root and ref_id is the id of the subvol  or snapshot.

 *

 * For a back ref the root_id is the id of the subvol or snapshot and

 * ref_id is the id of the tree referencing it.

 *

 * Will return 0, -ENOMEM, or anything from the CoW path

/*

 * Old btrfs forgets to init root_item->flags and root_item->byte_limit

 * for subvolumes. To work around this problem, we steal a bit from

 * root_item->inode_item->flags, and use it to indicate if those fields

 * have been properly initialized.

/*

 * btrfs_subvolume_reserve_metadata() - reserve space for subvolume operation

 * root: the root of the parent directory

 * rsv: block reservation

 * items: the number of items that we need do reservation

 * use_global_rsv: allow fallback to the global block reservation

 *

 * This function is used to reserve the space for snapshot/subvolume

 * creation and deletion. Those operations are different with the

 * common file/directory operations, they change two fs/file trees

 * and root tree, the number of items that the qgroup reserves is

 * different with the free space reservation. So we can not use

 * the space reservation mechanism in start_transaction().

 One for parent inode, two for dir entries */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2008 Red Hat.  All rights reserved.

	/*

	 * We are often under a trans handle at this point, so we need to make

	 * sure NOFS is set to keep us from deadlocking.

 We inline CRCs for the free disk space cache */

/*

 * inode is an optional sink: if it is NULL, btrfs_remove_free_space_inode

 * handles lookup, otherwise it takes ownership and iputs the inode.

 * Don't reuse an inode pointer after passing it into this function.

 One for the block groups ref */

 One for the lookup ref */

 1 for slack space, 1 for updating the inode */

		/*

		 * now that we've truncated the cache away, its no longer

		 * setup or written

	/*

	 * We skip the throttling logic for free space cache inodes, so we don't

	 * need to check for -EAGAIN.

 Make sure we can fit our crcs and generation into the first page */

	/*

	 * Skip the csum areas.  If we don't check crcs then we just have a

	 * 64bit chunk at the front of the first page.

	/*

	 * Skip the crc area.  If we don't check crcs then we just have a 64bit

	 * chunk at the front of the first page.

 No more pages to map */

 map the next page */

	/*

	 * If we aren't at the start of the current page, unmap this one and

	 * map the next one if there is any left.

	/*

	 * If we're not on the boundary we know we've modified the page and we

	 * need to crc the page.

	/*

	 * We are trying to keep the total amount of memory used per 1GiB of

	 * space to be MAX_CACHE_BYTES_PER_GIG.  However, with a reclamation

	 * mechanism of pulling extents >= FORCE_EXTENT_THRESHOLD out of

	 * bitmaps, we may end up using more memory than this.

	/*

	 * we want the extent entry threshold to always be at most 1/2 the max

	 * bytes we can have, or whatever is less than that.

 Nothing in the space cache, goodbye */

	/*

	 * We add the bitmaps at the end of the entries in order that

	 * the bitmap entries are added to the cache.

	/*

	 * Because we could potentially discard our loaded free space, we want

	 * to load everything into a temporary structure first, and then if it's

	 * valid copy it all into the actual free space ctl.

	/*

	 * If this block group has been marked to be cleared for one reason or

	 * another then we can't trust the on disk cache, so just return.

	/*

	 * We must pass a path with search_commit_root set to btrfs_iget in

	 * order to avoid a deadlock when allocating extents for the tree root.

	 *

	 * When we are COWing an extent buffer from the tree root, when looking

	 * for a free extent, at extent-tree.c:find_free_extent(), we can find

	 * block group without its free space cache loaded. When we find one

	 * we must load its space cache which requires reading its free space

	 * cache's inode item from the root tree. If this inode item is located

	 * in the same leaf that we started COWing before, then we end up in

	 * deadlock on the extent buffer (trying to read lock it when we

	 * previously write locked it).

	 *

	 * It's safe to read the inode item using the commit root because

	 * block groups, once loaded, stay in memory forever (until they are

	 * removed) as well as their space caches once loaded. New block groups

	 * once created get their ->cached field set to BTRFS_CACHE_FINISHED so

	 * we will never try to read their inode item while the fs is mounted.

 We may have converted the inode and made the cache invalid. */

		/*

		 * ret == 1 means we successfully loaded the free space cache,

		 * so we need to re-set it here.

 This cache is bogus, make sure it gets cleared */

 Get the cluster for this block_group if it exists */

 Write out the extent entries */

	/*

	 * Make sure we don't miss any range that was removed from our rbtree

	 * because trimming is running. Otherwise after a umount+mount (or crash

	 * after committing the transaction) we would leak free space and get

	 * an inconsistent free space cache report from fsck.

	/*

	 * We want to add any pinned extents to our free space cache

	 * so we don't leak the space

	 *

	 * We shouldn't have switched the pinned extents yet so this is the

	 * right one

 This pinned extent is out of our range */

 Write out the bitmaps */

 Flush the dirty pages in the cache file. */

 Update the cache item to tell everyone this cache file is valid. */

 the dirty list is protected by the dirty_bgs_lock */

 the disk_cache_state is protected by the block group lock */

		/*

		 * only mark this as written if we didn't get put back on

		 * the dirty list while waiting for IO.   Otherwise our

		 * cache state won't be right, and we won't get written again

/**

 * Write out cached info to an inode

 *

 * @root:        root the inode belongs to

 * @inode:       freespace inode we are writing out

 * @ctl:         free space cache we are going to write out

 * @block_group: block_group for this cache if it belongs to a block_group

 * @io_ctl:      holds context for the io

 * @trans:       the trans handle

 *

 * This function writes out a free space cache struct to disk for quick recovery

 * on mount.  This will return 0 if it was successful in writing the cache out,

 * or an errno if it was not.

 Lock all pages first so we can lock the extent safely. */

 Write out the extent entries in the free space cache */

	/*

	 * Some spaces that are freed in the current transaction are pinned,

	 * they will be added into free space cache after the transaction is

	 * committed, we shouldn't lose them.

	 *

	 * If this changes while we are working we'll get added back to

	 * the dirty list and redo it.  No locking needed

	/*

	 * At last, we write out all the bitmaps and keep cache_writeout_mutex

	 * locked while doing it because a concurrent trim can be manipulating

	 * or freeing the bitmap.

 Zero out the rest of the pages just to make sure */

 Everything is written out, now we dirty the pages in the file. */

	/*

	 * Release the pages and unlock the extent, we will flush

	 * them out later

	/*

	 * at this point the pages are under IO and we're happy,

	 * The caller is responsible for waiting on them and updating

	 * the cache and the inode

	/*

	 * if ret == 0 the caller is expected to call btrfs_wait_cache_io

	 * to wait for IO and put the inode

			/*

			 * we could have a bitmap entry and an extent entry

			 * share the same offset.  If this is the case, we want

			 * the extent entry to always be found first if we do a

			 * linear search through the tree, since we want to have

			 * the quickest allocation time, and allocating from an

			 * extent is faster than allocating from a bitmap.  So

			 * if we're inserting a bitmap and we find an entry at

			 * this offset, we want to go right, or after this entry

			 * logically.  If we are inserting an extent and we've

			 * found a bitmap, we want to go left, or before

			 * logically.

/*

 * searches the tree for the given offset.

 *

 * fuzzy - If this is set, then we are trying to make an allocation, and we just

 * want a section that has at least bytes size and comes at or after the given

 * offset.

 find entry that is closest to the 'offset' */

		/*

		 * bitmap entry and extent entry may share same offset,

		 * in that case, bitmap entry comes after extent entry.

			/*

			 * if previous extent entry covers the offset,

			 * we should return it instead of the bitmap entry

 find last entry before the 'offset' */

/*

 * If we can not find suitable extent, we will use bytes to record

 * the size of the max extent.

	/*

	 * Skip searching the bitmap if we don't have a contiguous section that

	 * is large enough for this allocation.

 Cache the size of the max extent in bytes */

		/* make sure the space returned is big enough

		 * to match our requested alignment

	/*

	 * Normally when this is called, the bitmap is completely empty. However,

	 * if we are blowing up the free space cache for one reason or another

	 * via __btrfs_remove_free_space_cache(), then it may not be freed and

	 * we may leave stats on the table.

	/*

	 * We need to search for bits in this bitmap.  We could only cover some

	 * of the extent in this bitmap thanks to how we add space, so we need

	 * to search for as much as it as we can and clear that amount, and then

	 * go searching for the next bit.

 We may have found more bits than what we need */

 Cannot clear past the end of the bitmap */

		/*

		 * no entry after this bitmap, but we still have bytes to

		 * remove, so something has gone wrong.

		/*

		 * if the next entry isn't a bitmap we need to return to let the

		 * extent stuff do its work.

		/*

		 * Ok the next item is a bitmap, but it may not actually hold

		 * the information for the rest of this free space stuff, so

		 * look for it, and if we don't find it return so we can try

		 * everything over again.

	/*

	 * This is a tradeoff to make bitmap trim state minimal.  We mark the

	 * whole bitmap untrimmed if at any point we add untrimmed regions.

	/*

	 * We set some bytes, we have no idea what the max extent size is

	 * anymore.

 This is a way to reclaim large regions from the bitmaps. */

	/*

	 * If we are below the extents threshold then we can add this as an

	 * extent, and don't have to deal with the bitmap

		/*

		 * If this block group has some small extents we don't want to

		 * use up all of our free slots in the cache with them, we want

		 * to reserve them to larger extents, however if we have plenty

		 * of cache left then go ahead an dadd them, no sense in adding

		 * the overhead of a bitmap if we don't have to.

	/*

	 * The original block groups from mkfs can be really small, like 8

	 * megabytes, so don't bother with a bitmap for those entries.  However

	 * some block groups can be smaller than what a bitmap would cover but

	 * are still large enough that they could overflow the 32k memory limit,

	 * so allow those block groups to still be allowed to have a bitmap

	 * entry.

	/*

	 * Since we link bitmaps right into the cluster we need to see if we

	 * have a cluster here, and if so and it has our bitmap we need to add

	 * the free space to that bitmap.

 no pre-allocated info, allocate a new one */

 allocate the bitmap */

/*

 * Free space merging rules:

 *  1) Merge trimmed areas together

 *  2) Let untrimmed areas coalesce with trimmed areas

 *  3) Always pull neighboring regions from bitmaps

 *

 * The above rules are for when we merge free space based on btrfs_trim_state.

 * Rules 2 and 3 are subtle because they are suboptimal, but are done for the

 * same reason: to promote larger extent regions which makes life easier for

 * find_free_extent().  Rule 2 enables coalescing based on the common path

 * being returning free space from btrfs_finish_extent_commit().  So when free

 * space is trimmed, it will prevent aggregating trimmed new region and

 * untrimmed regions in the rb_tree.  Rule 3 is purely to obtain larger extents

 * and provide find_free_extent() with the largest extents possible hoping for

 * the reuse path.

	/*

	 * first we want to see if there is free space adjacent to the range we

	 * are adding, if there is remove that struct and add a new one to

	 * cover the entire range

 See try_merge_free_space() comment. */

 See try_merge_free_space() comment. */

 See try_merge_free_space() comment. */

 If we're on a boundary, try the previous logical bitmap. */

 See try_merge_free_space() comment. */

/*

 * We prefer always to allocate from extent entries, both for clustered and

 * non-clustered allocation requests. So when attempting to add a new extent

 * entry, try to see if there's adjacent free space in bitmap entries, and if

 * there is, migrate that space from the bitmaps to the extent.

 * Like this we get better chances of satisfying space allocation requests

 * because we attempt to satisfy them based on a single cache entry, and never

 * on 2 or more entries - even if the entries represent a contiguous free space

 * region (e.g. 1 extent entry + 1 bitmap entry starting where the extent entry

 * ends).

	/*

	 * Only work with disconnected entries, as we can change their offset,

	 * and must be extent entries.

	/*

	 * There was no extent directly to the left or right of this new

	 * extent then we know we're going to have to allocate a new extent, so

	 * before we do that see if we need to drop this into a bitmap

	/*

	 * Only steal free space from adjacent bitmaps if we're sure we're not

	 * going to add the new free space to existing bitmap entries - because

	 * that would mean unnecessary work that would be reverted. Therefore

	 * attempt to steal space from bitmaps if we're adding an extent entry.

	/*

	 * If the block group is read-only, we should account freed space into

	 * bytes_readonly.

 All the region is now unusable. Mark it as unused and reclaim */

/*

 * This is a subtle distinction because when adding free space back in general,

 * we want it to be added as untrimmed for async. But in the case where we add

 * it on loading of a block group, we want to consider it trimmed.

		/*

		 * This can happen with conventional zones when replaying log.

		 * Since the allocation info of tree-log nodes are not recorded

		 * to the extent-tree, calculate_alloc_pointer() failed to

		 * advance the allocation pointer after last allocated tree log

		 * node blocks.

		 *

		 * This function is called from

		 * btrfs_pin_extent_for_log_replay() when replaying the log.

		 * Advance the pointer not to overwrite the tree-log nodes.

		/*

		 * oops didn't find an extent that matched the space we wanted

		 * to remove, look for a bitmap instead

			/*

			 * If we found a partial bit of our free space in a

			 * bitmap but then couldn't find the other part this may

			 * be a problem, so WARN about it.

 Not enough bytes in this entry to satisfy us */

 all done */

	/*

	 * Zoned btrfs does not use free space tree and cluster. Just print

	 * out the free space after the allocation offset.

	/*

	 * we only want to have 32k of ram per block group for keeping

	 * track of free space, and if we pass 1/2 of that we want to

	 * start converting things over to using bitmaps

/*

 * for a given cluster, put all of its extents back into the free

 * space cache.  If the block group passed doesn't match the block group

 * pointed to by the cluster, someone else raced in and freed the

 * cluster already.  In that case, we just return without changing anything

 Merging treats extents as if they were new */

 As we insert directly, update these statistics */

/**

 * btrfs_is_free_space_trimmed - see if everything is trimmed

 * @block_group: block_group of interest

 *

 * Walk @block_group's free space rb_tree to determine if everything is trimmed.

/*

 * given a cluster, put all of its extents back into the free space

 * cache.  If a block group is passed, this function will only free

 * a cluster that belongs to the passed block group.

 *

 * Otherwise, it'll get a reference on the block group pointed to by the

 * cluster and remove the cluster from it.

 first, get a safe pointer to the block group */

 someone else has already freed it don't redo their work */

 now return any extents the cluster had on it */

 finally drop our ref */

/*

 * given a cluster, try to allocate 'bytes' from it, returns 0

 * if it couldn't find anything suitably large, or a logical disk offset

 * if things worked out

	/*

	 * Don't bother looking for a cluster in this bitmap if it's heavily

	 * fragmented.

 -EEXIST; Logic error */

/*

 * This searches the block group for just extents to fill the cluster with.

 * Try to find a cluster with at least bytes total bytes, at least one

 * extent of cont1_bytes, and other clusters of at least min_bytes.

	/*

	 * We don't want bitmaps, so just move along until we find a normal

	 * extent entry.

	/*

	 * now we've found our entries, pull them out of the free space

	 * cache and put them into the cluster rbtree

 -EEXIST; Logic error */

/*

 * This specifically looks for bitmaps that may work in the cluster, we assume

 * that we have already failed to find extents that will work.

	/*

	 * The bitmap that covers offset won't be in the list unless offset

	 * is just its start offset.

	/*

	 * The bitmaps list has all the bitmaps that record free space

	 * starting after offset, so no more search is required.

/*

 * here we try to find a cluster of blocks in a block group.  The goal

 * is to find at least bytes+empty_size.

 * We might not find them all in one contiguous area.

 *

 * returns zero and sets up cluster if things worked out, otherwise

 * it returns -enospc

	/*

	 * Choose the minimum extent size we'll require for this

	 * cluster.  For SSD_SPREAD, don't allow any fragmentation.

	 * For metadata, allow allocates with smaller extents.  For

	 * data, keep it dense.

	/*

	 * If we know we don't have enough space to make a cluster don't even

	 * bother doing all the work to try and find one.

 someone already found a cluster, hooray */

 Clear our temporary list */

/*

 * simple code to zero out a cluster

/*

 * If @async is set, then we will trim 1 region and return.

 Skip bitmaps and if async, already trimmed entries */

			/*

			 * Let bytes = BTRFS_MAX_DISCARD_SIZE + X.

			 * If X < BTRFS_ASYNC_DISCARD_MIN_FILTER, we won't trim

			 * X when we come back around.  So trim it now.

/*

 * If we break out of trimming a bitmap prematurely, we should reset the

 * trimming bit.  In a rather contrieved case, it's possible to race here so

 * reset the state to BTRFS_TRIM_STATE_UNTRIMMED.

 *

 * start = start of bitmap

 * end = near end of bitmap

 *

 * Thread 1:			Thread 2:

 * trim_bitmaps(start)

 *				trim_bitmaps(end)

 *				end_trimming_bitmap()

 * reset_trimming_bitmap()

/*

 * If @async is set, then we will trim 1 region and return.

		/*

		 * Bitmaps are marked trimmed lossily now to prevent constant

		 * discarding of the same bitmap (the reason why we are bound

		 * by the filters).  So, retrim the block group bitmaps when we

		 * are preparing to punt to the unused_bgs list.  This uses

		 * @minlen to determine if we are in BTRFS_DISCARD_INDEX_UNUSED

		 * which is the only discard index which sets minlen to 0.

		/*

		 * Async discard bitmap trimming begins at by setting the start

		 * to be key.objectid and the offset_to_bitmap() aligns to the

		 * start of the bitmap.  This lets us know we are fully

		 * scanning the bitmap rather than only some portion of it.

			/*

			 * We lossily consider a bitmap trimmed if we only skip

			 * over regions <= BTRFS_ASYNC_DISCARD_MIN_FILTER.

		/*

		 * We already trimmed a region, but are using the locking above

		 * to reset the trim_state.

		/*

		 * Let bytes = BTRFS_MAX_DISCARD_SIZE + X.

		 * If X < @minlen, we won't trim X when we come back around.

		 * So trim it now.  We differ here from trimming extents as we

		 * don't keep individual state per bit.

 If we ended in the middle of a bitmap, reset the trimming flag */

	/*

	 * update_super_roots will appropriately set or unset

	 * super_copy->cache_generation based on SPACE_CACHE and

	 * BTRFS_FS_CLEANUP_SPACE_CACHE_V1. For this reason, we need a

	 * transaction commit whether we are enabling space cache v1 and don't

	 * have any other work to do, or are disabling it and removing free

	 * space inodes.

/*

 * Use this if you need to make a bitmap or extent entry specifically, it

 * doesn't do any of the merging that add_free_space does, this acts a lot like

 * how the free space cache loading stuff works, so you can get really weird

 * configurations.

/*

 * Checks to see if the given range is in the free space cache.  This is really

 * just used to check the absence of space, so if there is free space in the

 * range at all we will return 1.

 CONFIG_BTRFS_FS_RUN_SANITY_TESTS */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

			/*

			 * offset is supposed to be a tree block which

			 * must be aligned to nodesize.

			/*

			 * offset is supposed to be a tree block which

			 * must be aligned to nodesize.

/*

 * Helper to output refs and locking status of extent buffer.  Useful to debug

 * race condition related problems.

 SPDX-License-Identifier: GPL-2.0

	/*

	 * We round up to the block size at eof when determining which

	 * extents to clone above, but shouldn't round up the file size.

	/*

	 * We have flushed and locked the ranges of the source and destination

	 * inodes, we also have locked the inodes, so we are safe to do a

	 * reservation here. Also we must not do the reservation while holding

	 * a transaction open, otherwise we would deadlock.

	/*

	 * After dirtying the page our caller will need to start a transaction,

	 * and if we are low on metadata free space, that can cause flushing of

	 * delalloc for all inodes in order to get metadata space released.

	 * However we are holding the range locked for the whole duration of

	 * the clone/dedupe operation, so we may deadlock if that happens and no

	 * other task releases enough space. So mark this inode as not being

	 * possible to flush to avoid such deadlock. We will clear that flag

	 * when we finish cloning all extents, since a transaction is started

	 * after finding each extent to clone.

	/*

	 * If our inline data is smaller then the block/page size, then the

	 * remaining of the block/page is equivalent to zeroes. We had something

	 * like the following done:

	 *

	 * $ xfs_io -f -c "pwrite -S 0xab 0 500" file

	 * $ sync  # (or fsync)

	 * $ xfs_io -c "falloc 0 4K" file

	 * $ xfs_io -c "pwrite -S 0xcd 4K 4K"

	 *

	 * So what's in the range [500, 4095] corresponds to zeroes.

/*

 * Deal with cloning of inline extents. We try to copy the inline extent from

 * the source inode to destination inode when possible. When not possible we

 * copy the inline extent's data into the respective page of the inode.

			/*

			 * There's an implicit hole at file offset 0, copy the

			 * inline extent's data to the page.

		/*

		 * If it's an inline extent replace it with the source inline

		 * extent, otherwise copy the source inline extent data into

		 * the respective page at the destination inode.

	/*

	 * We have no extent items, or we have an extent at offset 0 which may

	 * or may not be inlined. All these cases are dealt the same way.

		/*

		 * At the destination offset 0 we have either a hole, a regular

		 * extent or an inline extent larger then the one we want to

		 * clone. Deal with all these cases by copying the inline extent

		 * data into the respective page at the destination inode.

	/*

	 * Release path before starting a new transaction so we don't hold locks

	 * that would confuse lockdep.

	/*

	 * If we end up here it means were copy the inline extent into a leaf

	 * of the destination inode. We know we will drop or adjust at most one

	 * extent item in the destination root.

	 *

	 * 1 unit - adjusting old extent (we may have to split it)

	 * 1 unit - add new extent

	 * 1 unit - inode update

		/*

		 * No transaction here means we copied the inline extent into a

		 * page of the destination inode.

		 *

		 * 1 unit to update inode item

	/*

	 * Release our path because we don't need it anymore and also because

	 * copy_inline_to_page() needs to reserve data and metadata, which may

	 * need to flush delalloc when we are low on available space and

	 * therefore cause a deadlock if writeback of an inline extent needs to

	 * write to the same leaf or an ordered extent completion needs to write

	 * to the same leaf.

/**

 * btrfs_clone() - clone a range from inode file to another

 *

 * @src: Inode to clone from

 * @inode: Inode to clone to

 * @off: Offset within source to start clone from

 * @olen: Original length, passed by user, of range to clone

 * @olen_aligned: Block-aligned value of olen

 * @destoff: Offset within @inode to start clone

 * @no_time_update: Whether to update mtime/ctime on the target inode

 Clone data */

 Note the key will change type as we walk through the tree */

		/*

		 * First search, if no extent item that starts at offset off was

		 * found but the previous item is an extent item, it's possible

		 * it might overlap our target range, therefore process it.

 Take upper bound, may be compressed */

		/*

		 * The first search might have left us at an extent item that

		 * ends before our target range's start, can happen if we have

		 * holes and NO_HOLES feature enabled.

		/*

		 * Deal with a hole that doesn't have an extent item that

		 * represents it (NO_HOLES feature enabled).

		 * This hole is either in the middle of the cloning range or at

		 * the beginning (fully overlaps it or partially overlaps it).

			/*

			 *    a  | --- range to clone ---|  b

			 * | ------------- extent ------------- |

 Subtract range b */

 Subtract range a */

			/*

			 * Inline extents always have to start at file offset 0

			 * and can never be bigger then the sector size. We can

			 * never clone only parts of an inline extent, since all

			 * reflink operations must start at a sector size aligned

			 * offset, and the length must be aligned too or end at

			 * the i_size (which implies the whole inlined data).

		/*

		 * If this is a new extent update the last_reflink_trans of both

		 * inodes. This is used by fsync to make sure it does not log

		 * multiple checksum items with overlapping ranges. For older

		 * extents we don't need to do it since inode logging skips the

		 * checksums for older extents. Also ignore holes and inline

		 * extents because they don't have checksums in the csum tree.

		/*

		 * We have an implicit hole that fully or partially overlaps our

		 * cloning range at its end. This means that we either have the

		 * NO_HOLES feature enabled or the implicit hole happened due to

		 * mixing buffered and direct IO writes against this file.

		/*

		 * When using NO_HOLES and we are cloning a range that covers

		 * only a hole (no extents) into a range beyond the current

		 * i_size, punching a hole in the target range will not create

		 * an extent map defining a hole, because the range starts at or

		 * beyond current i_size. If the file previously had an i_size

		 * greater than the new i_size set by this clone operation, we

		 * need to make sure the next fsync is a full fsync, so that it

		 * detects and logs a hole covering a range from the current

		 * i_size to the new i_size. If the clone range covers extents,

		 * besides a hole, then we know the full sync flag was already

		 * set by previous calls to btrfs_replace_file_extents() that

		 * replaced file extent items.

	/*

	 * Lock destination range to serialize with concurrent readpages() and

	 * source range to serialize with relocation.

	/*

	 * VFS's generic_remap_file_range_prep() protects us from cloning the

	 * eof block into the middle of a file, which would result in corruption

	 * if the file size is not blocksize aligned. So we don't need to check

	 * for that case here.

		/*

		 * We may have truncated the last block if the inode's size is

		 * not sector size aligned, so we need to wait for writeback to

		 * complete before proceeding further, otherwise we can race

		 * with cloning and attempt to increment a reference to an

		 * extent that no longer exists (writeback completed right after

		 * we found the previous extent covering eof and before we

		 * attempted to increment its reference count).

	/*

	 * Lock destination range to serialize with concurrent readpages() and

	 * source range to serialize with relocation.

	/*

	 * We may have copied an inline extent into a page of the destination

	 * range, so wait for writeback to complete before truncating pages

	 * from the page cache. This is a rare case.

	/*

	 * Truncate page cache pages so that future reads will see the cloned

	 * data immediately and not the previous data.

 Don't make the dst file partly checksummed */

	/*

	 * Now that the inodes are locked, we need to start writeback ourselves

	 * and can not rely on the writeback from the VFS's generic helper

	 * generic_remap_file_range_prep() because:

	 *

	 * 1) For compression we must call filemap_fdatawrite_range() range

	 *    twice (btrfs_fdatawrite_range() does it for us), and the generic

	 *    helper only calls it once;

	 *

	 * 2) filemap_fdatawrite_range(), called by the generic helper only

	 *    waits for the writeback to complete, i.e. for IO to be done, and

	 *    not for the ordered extents to complete. We need to wait for them

	 *    to complete so that new file extent items are in the fs tree.

	/*

	 * Since we don't lock ranges, wait for ongoing lockless dio writes (as

	 * any in progress could create its ordered extents after we wait for

	 * existing ordered extents below).

	/*

	 * Workaround to make sure NOCOW buffered write reach disk as NOCOW.

	 *

	 * Btrfs' back references do not have a block level granularity, they

	 * work at the whole extent level.

	 * NOCOW buffered write without data space reserved may not be able

	 * to fall back to CoW due to lack of data space, thus could cause

	 * data loss.

	 *

	 * Here we take a shortcut by flushing the whole inode, so that all

	 * nocow write should reach disk as nocow before we increase the

	 * reference of the extent. We could do better by only flushing NOCOW

	 * data, but that needs extra accounting.

	 *

	 * Also we don't need to check ASYNC_EXTENT, as async extent will be

	 * CoWed anyway, not affecting nocow part.

	/*

	 * If either the source or the destination file was opened with O_SYNC,

	 * O_DSYNC or has the S_SYNC attribute, fsync both the destination and

	 * source files/ranges, so that after a successful return (0) followed

	 * by a power failure results in the reflinked data to be readable from

	 * both files/ranges.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Oracle.  All rights reserved.

/*

 * btrfs_end_io_wq structs are used to do processing in task context when an IO

 * is complete.  This is used during reads to verify checksums, and it is used

 * by writes to insert metadata for new file extents after IO is complete.

/*

 * async submit bios are used to offload expensive checksumming

 * onto the worker threads.  They checksum file and metadata bios

 * just before they are sent down the IO stack.

 Optional parameter for submit_bio_start used by direct io */

/*

 * Lockdep class keys for extent_buffer->lock's in this root.  For a given

 * eb, the lockdep key is determined by the btrfs_root it belongs to and

 * the level the eb occupies in the tree.

 *

 * Different roots are used for different purposes and may nest inside each

 * other and they require separate keysets.  As lockdep keys should be

 * static, assign keysets according to the purpose of the root as indicated

 * by btrfs_root->root_key.objectid.  This ensures that all special purpose

 * roots have separate keysets.

 *

 * Lock-nesting across peer nodes is always done with the immediate parent

 * node locked thus preventing deadlock.  As lockdep doesn't know this, use

 * subclass to avoid triggering lockdep warning in such cases.

 *

 * The key is set by the readpage_end_io_hook after the buffer has passed

 * csum validation but before the pages are unlocked.  It is also set by

 * btrfs_init_new_buffer on freshly allocated blocks.

 *

 * We also add a check to make sure the highest level of the tree is the

 * same as our lockdep setup here.  If BTRFS_MAX_LEVEL changes, this code

 * needs update as well.

 root objectid */

 Longest entry: btrfs-free-space-00 */

 find the matching keyset, id 0 is the default entry */

/*

 * Compute the csum of a btree block and store the result to provided buffer.

/*

 * we can't consider a given block up to date unless the transid of the

 * block matches the transid in the parent node's pointer.  This is how we

 * detect blocks that either didn't get written at all or got written

 * in the wrong place.

/*

 * Return 0 if the superblock checksum type matches the checksum value of that

 * algorithm. Pass the raw disk superblock data.

	/*

	 * The super_block structure does not span the whole

	 * BTRFS_SUPER_INFO_SIZE range, we expect that the unused space is

	 * filled with zeros and is included in the checksum.

	/*

	 * For live tree block (new tree blocks in current transaction),

	 * we need proper lock context to avoid race, which is impossible here.

	 * So we only checks tree blocks which is read from disk, whose

	 * generation <= fs_info->last_trans_committed.

 We have @first_key, so this @eb must have at least one item */

/*

 * helper to read a given tree block, doing retries as required when

 * the checksums don't match and we have alternate mirrors to try.

 *

 * @parent_transid:	expected transid, skip check if 0

 * @level:		expected level, mandatory check

 * @first_key:		expected key of first slot, skip check if NULL

 Checksum all dirty extent buffers in one bio_vec */

 A dirty eb shouldn't disappear from buffer_radix */

/*

 * Checksum a dirty tree block before IO.  This has extra checks to make sure

 * we only fill in the checksum field in the first page of a multi-page block.

 * For subpage extent buffers we need bvec to also read the offset in the page.

	/*

	 * Please do not consolidate these warnings into a single if.

	 * It is useful to know what went wrong.

	/*

	 * Checking the incompat flag is only valid for the current fs. For

	 * seed devices it's forbidden to have their uuid changed so reading

	 * ->fsid in this case is fine

 Do basic extent buffer checks at read time */

	/*

	 * If this is a leaf block and it is corrupt, set the corrupt bit so

	 * that we don't try and read the other copies of this block, just

	 * return -EIO.

	/*

	 * We don't allow bio merge for subpage metadata read, so we should

	 * only get one eb for each endio hook.

	/*

	 * When we are reading one tree block, eb must have been inserted into

	 * the radix tree. If not, something is wrong.

 Subpage read must finish in page read */

	/*

	 * end_bio_extent_readpage decrements io_pages in case of error,

	 * make sure it has something to decrement.

	/*

	 * The pending IO might have been the only thing that kept this buffer

	 * in memory.  Make sure we have a ref for all this other checks

		/*

		 * our io error hook is going to dec the io pages

		 * again, we have to make sure it has something

		 * to decrement

/*

 * In order to insert checksums into the metadata in large chunks, we wait

 * until bio submission time.   All the pages in the bio are checksummed and

 * sums are attached onto the ordered extent record.

 *

 * At IO completion time the csums attached on the ordered extent record are

 * inserted into the tree.

 If an error occurred we just want to clean up the bio and move on */

	/*

	 * All of the bios that pass through here are from async helpers.

	 * Use REQ_CGROUP_PUNT to issue them from the owning cgroup's context.

	 * This changes nothing when cgroups aren't in use.

	/*

	 * when we're called for a write, we're already in the async

	 * submission context.  Just jump into btrfs_map_bio

		/*

		 * called for a read, do the setup so that checksum validation

		 * can happen in the async kernel threads

		/*

		 * kthread helpers are used to submit writes so that

		 * checksumming can happen in parallel across all CPUs

	/*

	 * we can't safely write a btree page from here,

	 * we haven't done the locking hook

	/*

	 * Buffers may be managed in a filesystem specific way.

	 * We must have no buffers or drop them.

 this is a bit racy, but that's ok */

/*

 * Read tree block at logical address @bytenr and do variant basic but critical

 * verification.

 *

 * @owner_root:		the objectid of the root owner for this block.

 * @parent_transid:	expected transid of this tree block, skip check if 0

 * @level:		expected level, mandatory check

 * @first_key:		expected key in slot 0, skip check if NULL

 Should only be used by the testing infrastructure */

 We don't use the stripesize in selftest, set it as sectorsize */

	/*

	 * We're holding a transaction handle, so use a NOFS memory allocation

	 * context to avoid deadlock if reclaim happens.

	/*

	 * DON'T set SHAREABLE bit for log trees.

	 *

	 * Log trees are not exposed to user space thus can't be snapshotted,

	 * and they go away before a real commit is actually done.

	 *

	 * They do store pointers to file data extents, and those reference

	 * counts still get updated (along with back refs to the log tree).

/*

 * Initialize subvolume root in-memory structure

 *

 * @anon_dev:	anonymous device to attach to the root, if zero, allocate new

	/*

	 * We might be called under a transaction (e.g. indirect backref

	 * resolution) which could deadlock if it triggers memory reclaim

	/*

	 * Don't assign anonymous block device to roots that are not exposed to

	 * userspace, the id pool is limited to 1M

 The caller is responsible to call btrfs_free_fs_root */

/*

 * Get an in-memory reference of a root structure.

 *

 * For essential trees like root/extent tree, we grab it from fs_info directly.

 * For subvolume trees, we check the cached filesystem roots first. If not

 * found, then read it from disk and add it to cached fs roots.

 *

 * Caller should release the root by calling btrfs_put_root() after the usage.

 *

 * NOTE: Reloc and log trees can't be read by this function as they share the

 *	 same root objectid.

 *

 * @objectid:	root id

 * @anon_dev:	preallocated anonymous block device number for new roots,

 * 		pass 0 for new allocation.

 * @check_ref:	whether to check root item references, If true, return -ENOENT

 *		for orphan roots

 Shouldn't get preallocated anon_dev for cached roots */

/*

 * Get in-memory reference of a root structure

 *

 * @objectid:	tree objectid

 * @check_ref:	if set, verify that the tree exists and the item has at least

 *		one reference

/*

 * Get in-memory reference of a root structure, created as new, optionally pass

 * the anonymous block device id

 *

 * @objectid:	tree objectid

 * @anon_dev:	if zero, allocate a new anonymous block device or use the

 *		parameter value

/*

 * btrfs_get_fs_root_commit_root - return a root for the given objectid

 * @fs_info:	the fs_info

 * @objectid:	the objectid we need to lookup

 *

 * This is exclusively used for backref walking, and exists specifically because

 * of how qgroups does lookups.  Qgroups will do a backref lookup at delayed ref

 * creation time, which means we may have to read the tree_root in order to look

 * up a fs root that is not in memory.  If the root is not in memory we will

 * read the tree root commit root and look up the fs root from there.  This is a

 * temporary root, it will not be inserted into the radix tree as it doesn't

 * have the most uptodate information, it'll simply be discarded once the

 * backref code is finished using the root.

	/*

	 * This can return -ENOENT if we ask for a root that doesn't exist, but

	 * since this is called via the backref walking code we won't be looking

	 * up a root that doesn't exist, unless there's corruption.  So if root

	 * != NULL just return it.

/*

 * called by the kthread helper functions to finally call the bio end_io

 * functions.  This is where read checksum verification actually happens

 Make the cleaner go to sleep early. */

		/*

		 * Do not do anything if we might cause open_ctree() to block

		 * before we have finished mounting the filesystem.

		/*

		 * Avoid the problem that we change the status of the fs

		 * during the above check and trylock.

		/*

		 * The defragger has dealt with the R/O remount and umount,

		 * needn't do anything special here.

		/*

		 * Acquires fs_info->reclaim_bgs_lock to avoid racing

		 * with relocation (btrfs_relocate_chunk) and relocation

		 * acquires fs_info->cleaner_mutex (btrfs_relocate_block_group)

		 * after acquiring fs_info->reclaim_bgs_lock. So we

		 * can't hold, nor need to, fs_info->cleaner_mutex when deleting

		 * unused block groups.

		/*

		 * Reclaim block groups in the reclaim_bgs list after we deleted

		 * all unused block_groups. This possibly gives us some more free

		 * space.

 If the file system is aborted, this will always fail. */

/*

 * This will find the highest generation in the array of root backups.  The

 * index of the highest array is returned, or -EINVAL if we can't find

 * anything.

 *

 * We check to make sure the array is valid by comparing the

 * generation of the latest  root in the array with the generation

 * in the super block.  If they don't match we pitch it.

/*

 * copy all the root pointers into the super backup array.

 * this will bump the backup pointer by one when it is

 * done

	/*

	 * make sure all of our padding and empty slots get zero filled

	 * regardless of which ones we use today

	/*

	 * we might commit during log recovery, which happens before we set

	 * the fs_root.  Make sure it is valid before we fill it in.

	/*

	 * if we don't copy this out to the super_copy, it won't get remembered

	 * for the next commit

/*

 * read_backup_root - Reads a backup root based on the passed priority. Prio 0

 * is the newest, prio 1/2/3 are 2nd newest/3rd newest/4th (oldest) backup roots

 *

 * fs_info - filesystem whose backup roots need to be read

 * priority - priority of backup root required

 *

 * Returns backup root index on success and -EINVAL otherwise.

	/*

	 * Fixme: the total bytes and num_devices need to match or we should

	 * need a fsck

 helper to cleanup workers */

	/*

	 * Now that all other work queues are destroyed, we can safely destroy

	 * the queues used for metadata I/O, since tasks from those other work

	 * queues can do metadata I/O operations.

 helper to cleanup tree roots */

	/*

	 * we set the i_size on the btree inode to the max possible int.

	 * the real end of the address space is determined by all of

	 * the devices in the system

	/*

	 * endios are largely parallel and should have a very

	 * low idle thresh

 returns with log_tree_root freed on success */

 Initialize fs_info for all devices in any case */

 If IGNOREDATACSUMS is set don't bother reading the csum root. */

	/*

	 * This tree can share blocks with some other fs tree during relocation

	 * and we need a proper setup by btrfs_get_fs_root

/*

 * Real super block validation

 * NOTE: super csum type and incompat features will not be checked here.

 *

 * @sb:		super block to check

 * @mirror_num:	the super block number to check its bytenr:

 * 		0	the primary (1st) sb

 * 		1, 2	2nd and 3rd backup copy

 * 	       -1	skip bytenr check

	/*

	 * Check sectorsize and nodesize first, other check will need it.

	 * Check all possible sectorsize(4K, 8K, 16K, 32K, 64K) here.

	/*

	 * For 4K page size, we only support 4K sector size.

	 * For 64K page size, we support 64K and 4K sector sizes.

 Root alignment check */

	/*

	 * Hint to catch really bogus numbers, bitflips or so, more exact checks are

	 * done later

	/*

	 * Obvious sys_chunk_array corruptions, it must hold at least one key

	 * and one chunk

	/*

	 * The generation is a global counter, we'll trust it more than the others

	 * but it's still possible that it's the one that's wrong.

/*

 * Validation of super block at mount time.

 * Some checks already done early at mount time, like csum type and incompat

 * flags will be skipped.

/*

 * Validation of super block at write time.

 * Some checks like bytenr check will be skipped as their values will be

 * overwritten soon.

 * Extra checks like csum type and incompat flags will be done here.

			/*

			 * Don't use the log in recovery mode, it won't be

			 * valid

 We can't trust the free space cache either */

		/*

		 * No need to hold btrfs_root::objectid_mutex since the fs

		 * hasn't been fully initialised and we are the only user

 All successful */

 Always begin writing backup roots after the one being used */

 div by 64 */

 readahead state */

 Usable values until the real ones are cached from the superblock */

	/*

	 * 1st step is to iterate through the existing UUID tree and

	 * to delete all entries that contain outdated data.

	 * 2nd step is to add all missing entries to the UUID tree.

 fs_info->update_uuid_tree_gen remains 0 in all error case */

/*

 * Some options only have meaning at mount time and shouldn't persist across

 * remounts, or be displayed. Clear these at the end of mount and remount

 * code paths.

/*

 * Mounting logic specific to read-write file systems. Shared by open_ctree

 * and btrfs_remount when remounting from read-only to read-write.

	/*

	 * btrfs_find_orphan_roots() is responsible for finding all the dead

	 * roots (with 0 refs), flag them with BTRFS_ROOT_DEAD_TREE and load

	 * them into the fs_info->fs_roots_radix tree. This must be done before

	 * calling btrfs_orphan_cleanup() on the tree root. If we don't do it

	 * first, then btrfs_orphan_cleanup() will delete a dead root's orphan

	 * item before the root's tree is deleted - this means that if we unmount

	 * or crash before the deletion completes, on the next mount we will not

	 * delete what remains of the tree because the orphan item does not

	 * exists anymore, which is what tells us we have a pending deletion.

 These need to be init'ed before we start creating inodes and such. */

	/*

	 * Read super block and check the signature bytes only

	/*

	 * Verify the type first, if that or the checksum value are

	 * corrupted, we'll find out

	/*

	 * We want to check superblock checksum, the type is stored inside.

	 * Pass the whole disk block of size BTRFS_SUPER_INFO_SIZE (4k).

	/*

	 * super_copy is zeroed at allocation time and we never touch the

	 * following bytes up to INFO_SIZE, the checksum is calculated from

	 * the whole block of INFO_SIZE

 check FS state, whether FS is broken. */

	/*

	 * In the long term, we'll store the compression type in the super

	 * block, and it'll be used for per file compression control.

	/*

	 * Flag our filesystem as having big metadata blocks if they are bigger

	 * than the page size.

 Set up fs_info before parsing mount options */

	/*

	 * mixed block groups end up with duplicate but slightly offset

	 * extent buffers for the same range.  It leads to corruptions

	/*

	 * Needn't use the lock because there is no other task which will

	 * update the flag.

	/*

	 * At this point we know all the devices that make this filesystem,

	 * including the seed devices but we don't know yet if the replace

	 * target is required. So free devices that are not part of this

	 * filesystem but skip the replace target device which is checked

	 * below in btrfs_init_dev_replace().

	/*

	 * Get zone type information of zoned block devices. This will also

	 * handle emulation of a zoned filesystem if a regular device has the

	 * zoned incompat feature flag set.

	/*

	 * If we have a uuid root and we're not being told to rescan we need to

	 * check the generation here so we can set the

	 * BTRFS_FS_UPDATE_UUID_TREE_GEN bit.  Otherwise we could commit the

	 * transaction during a balance or the log replay without updating the

	 * uuid generation, and then if we crash we would rescan the uuid tree,

	 * even though it was perfectly fine.

	/*

	 * Mount does not set all options immediately, we can do it now and do

	 * not have to wait for transaction commit

 do not make disk changes in broken FS or nologreplay is given */

	/*

	 * make sure we're done with the btree inode before we stop our

	 * kthreads

	/* we would like to check all the supers, but that would make

	 * a btrfs mount succeed after a mkfs from a different FS.

	 * So, we need to add a special mount option to scan for

	 * later supers, using BTRFS_SUPER_MIRROR_MAX instead

/*

 * Write superblock @sb to the @device. Do not wait for completion, all the

 * pages we use for writing are locked.

 *

 * Write @max_mirrors copies of the superblock, where 0 means default that fit

 * the expected device size at commit time. Note that max_mirrors must be

 * same for write and wait phases.

 *

 * Return number of errors when page is not found or submission fails.

 Bump the refcount for wait_dev_supers() */

		/*

		 * Directly use bios here instead of relying on the page cache

		 * to do I/O, so we don't lose the ability to do integrity

		 * checking.

		/*

		 * We FUA only the first super block.  The others we allow to

		 * go down lazy and there's a short window where the on-disk

		 * copies might still contain the older version.

/*

 * Wait for write completion of superblocks done by write_dev_supers,

 * @max_mirrors same for write and wait phases.

 *

 * Return number of errors when page is not found or not marked up to

 * date.

 Page is submitted locked and unlocked once the IO completes */

 Drop our reference */

 Drop the reference from the writing run */

 log error, force error return */

/*

 * endio for the write_dev_flush, this will wake anyone waiting

 * for the barrier when it is done

/*

 * Submit a flush request to the device if it supports it. Error handling is

 * done in the waiting counterpart.

	/*

	 * When a disk has write caching disabled, we skip submission of a bio

	 * with flush and sync requests before writing the superblock, since

	 * it's not needed. However when the integrity checker is enabled, this

	 * results in reports that there are metadata blocks referred by a

	 * superblock that were not properly flushed. So don't skip the bio

	 * submission only when the integrity checker is enabled for the sake

	 * of simplicity, since this is a debug tool and not meant for use in

	 * non-debug builds.

/*

 * If the flush bio has been submitted by write_dev_flush, wait for it.

/*

 * send an empty flush down to each device in parallel,

 * then wait for them

 send down all the barriers */

 wait for all the barriers */

		/*

		 * At some point we need the status of all disks

		 * to arrive at the volume status. So error checking

		 * is being pushed to a separate loop.

	/*

	 * max_mirrors == 0 indicates we're from commit_transaction,

	 * not from fsync where the tree roots in fs_info have not

	 * been consistent on disk.

 FUA is masked off if unsupported and can't be the reason */

 Drop a fs root from the radix tree and free it. */

 Avoid to grab roots in dead_roots */

 grab all the search result for later use */

 release the uncleaned roots due to error */

 wait until ongoing cleanup work done */

	/*

	 * We don't want the cleaner to start new transactions, add more delayed

	 * iputs, etc. while we're closing. We can't use kthread_stop() yet

	 * because that frees the task_struct, and the transaction kthread might

	 * still try to wake up the cleaner.

 wait for the qgroup rescan worker to stop */

 wait for the uuid_scan task to finish */

 avoid complains from lockdep et al., set sem back to initial state */

 pause restriper - we want to resume on mount */

 wait for any defraggers to finish */

 clear out the rbtree of defraggable inodes */

 Cancel or finish ongoing discard work */

		/*

		 * The cleaner kthread is stopped, so do one final pass over

		 * unused block groups.

		/*

		 * There might be existing delayed inode workers still running

		 * and holding an empty delayed inode item. We must wait for

		 * them to complete first because they can create a transaction.

		 * This happens when someone calls btrfs_balance_delayed_items()

		 * and then a transaction commit runs the same delayed nodes

		 * before any delayed worker has done something with the nodes.

		 * We must wait for any worker here and not at transaction

		 * commit time since that could cause a deadlock.

		 * This is a very rare case.

	/*

	 * we must make sure there is not any read request to

	 * submit after we stopping all workers.

 We shouldn't have any transaction open at this point */

	/*

	 * We must free the block groups after dropping the fs_roots as we could

	 * have had an IO error and have left over tree log blocks that aren't

	 * cleaned up until the fs roots are freed.  This makes the block group

	 * accounting appear to be wrong because there's pending reserved bytes,

	 * so make sure we do the block group cleanup afterwards.

	/*

	 * This is a fast path so only do this check if we have sanity tests

	 * enabled.  Normal people shouldn't be using unmapped buffers as dirty

	 * outside of the sanity tests.

	/*

	 * Since btrfs_mark_buffer_dirty() can be called with item pointer set

	 * but item data not updated.

	 * So here we should only check item pointers, not item data.

	/*

	 * looks as though older kernels can get into trouble with

	 * this code, they end up stuck in balance_dirty_pages forever

 cleanup FS via transaction */

	/*

	 * This will just short circuit the ordered completion stuff which will

	 * make sure the ordered extent gets properly cleaned up.

	/*

	 * We need this here because if we've been flipped read-only we won't

	 * get sync() from the umount, so we need to make sure any ordered

	 * extents that haven't had their dirty pages IO start writeout yet

	 * actually get run and error out properly.

		/*

		 * Make sure we get a live inode and that it'll not disappear

		 * meanwhile.

		/*

		 * The btrfs_finish_extent_commit() may get the same range as

		 * ours between find_first_extent_bit and clear_extent_dirty.

		 * Hence, hold the unused_bg_unpin_mutex to avoid double unpin

		 * the same extent range.

	/*

	 * Refer to the definition of io_bgs member for details why it's safe

	 * to use it without any locking

			/*

			 * We wait for 0 num_writers since we don't hold a trans

			 * handle open currently for this transaction.

 Corruption */

 SPDX-License-Identifier: GPL-2.0

/*

 * This contains the logic to handle async discard.

 *

 * Async discard manages trimming of free space outside of transaction commit.

 * Discarding is done by managing the block_groups on a LRU list based on free

 * space recency.  Two passes are used to first prioritize discarding extents

 * and then allow for trimming in the bitmap the best opportunity to coalesce.

 * The block_groups are maintained on multiple lists to allow for multiple

 * passes with different discard filter requirements.  A delayed work item is

 * used to manage discarding with timeout determined by a max of the delay

 * incurred by the iops rate limit, the byte rate limit, and the max delay of

 * BTRFS_DISCARD_MAX_DELAY.

 *

 * Note, this only keeps track of block_groups that are explicitly for data.

 * Mixed block_groups are not supported.

 *

 * The first list is special to manage discarding of fully free block groups.

 * This is necessary because we issue a final trim for a full free block group

 * after forgetting it.  When a block group becomes unused, instead of directly

 * being added to the unused_bgs list, we add it to this first list.  Then

 * from there, if it becomes fully discarded, we place it onto the unused_bgs

 * list.

 *

 * The in-memory free space cache serves as the backing state for discard.

 * Consequently this means there is no persistence.  We opt to load all the

 * block groups in as not discarded, so the mount case degenerates to the

 * crashing case.

 *

 * As the free space cache uses bitmaps, there exists a tradeoff between

 * ease/efficiency for find_free_extent() and the accuracy of discard state.

 * Here we opt to let untrimmed regions merge with everything while only letting

 * trimmed regions merge with other trimmed regions.  This can cause

 * overtrimming, but the coalescing benefit seems to be worth it.  Additionally,

 * bitmap state is tracked as a whole.  If we're able to fully trim a bitmap,

 * the trimmed flag is set on the bitmap.  Otherwise, if an allocation comes in,

 * this resets the state and we will retry trimming the whole bitmap.  This is a

 * tradeoff between discard state accuracy and the cost of accounting.

 This is an initial delay to give some chance for block reuse */

 Target completion latency of discarding all discardable extents */

 Montonically decreasing minimum length filters after index 0 */

/**

 * find_next_block_group - find block_group that's up next for discarding

 * @discard_ctl: discard control

 * @now: current time

 *

 * Iterate over the discard lists to find the next block_group up for

 * discarding checking the discard_eligible_time of block_group.

/**

 * Wrap find_next_block_group()

 *

 * @discard_ctl:   discard control

 * @discard_state: the discard_state of the block_group after state management

 * @discard_index: the discard_index of the block_group after state management

 * @now:           time when discard was invoked, in ns

 *

 * This wraps find_next_block_group() and sets the block_group to be in use.

 * discard_state's control flow is managed here.  Variables related to

 * discard_state are reset here as needed (eg discard_cursor).  @discard_state

 * and @discard_index are remembered as it may change while we're discarding,

 * but we want the discard to execute in the context determined here.

/**

 * btrfs_discard_check_filter - updates a block groups filters

 * @block_group: block group of interest

 * @bytes: recently freed region size after coalescing

 *

 * Async discard maintains multiple lists with progressively smaller filters

 * to prioritize discarding based on size.  Should a free space that matches

 * a larger filter be returned to the free_space_cache, prioritize that discard

 * by moving @block_group to the proper filter.

/**

 * btrfs_update_discard_index - moves a block group along the discard lists

 * @discard_ctl: discard control

 * @block_group: block_group of interest

 *

 * Increment @block_group's discard_index.  If it falls of the list, let it be.

 * Otherwise add it back to the appropriate list.

/**

 * btrfs_discard_cancel_work - remove a block_group from the discard lists

 * @discard_ctl: discard control

 * @block_group: block_group of interest

 *

 * This removes @block_group from the discard lists.  If necessary, it waits on

 * the current work and then reschedules the delayed work.

/**

 * btrfs_discard_queue_work - handles queuing the block_groups

 * @discard_ctl: discard control

 * @block_group: block_group of interest

 *

 * This maintains the LRU order of the discard lists.

		/*

		 * A single delayed workqueue item is responsible for

		 * discarding, so we can manage the bytes rate limit by keeping

		 * track of the previous discard.

		/*

		 * This timeout is to hopefully prevent immediate discarding

		 * in a recently allocated block group.

/*

 * btrfs_discard_schedule_work - responsible for scheduling the discard work

 * @discard_ctl:  discard control

 * @override:     override the current timer

 *

 * Discards are issued by a delayed workqueue item.  @override is used to

 * update the current delay as the baseline delay interval is reevaluated on

 * transaction commit.  This is also maxed with any other rate limit.

/**

 * btrfs_finish_discard_pass - determine next step of a block_group

 * @discard_ctl: discard control

 * @block_group: block_group of interest

 *

 * This determines the next step for a block group after it's finished going

 * through a pass on a discard list.  If it is unused and fully trimmed, we can

 * mark it unused and send it to the unused_bgs path.  Otherwise, pass it onto

 * the appropriate filter list or let it fall off.

/**

 * btrfs_discard_workfn - discard work function

 * @work: work

 *

 * This finds the next block_group to start discarding and then discards a

 * single region.  It does this in a two-pass fashion: first extents and second

 * bitmaps.  Completely discarded block groups are sent to the unused_bgs path.

 Perform discarding */

		/*

		 * Use the previous levels minimum discard length as the max

		 * length filter.  In the case something is added to make a

		 * region go beyond the max filter, the entire bitmap is set

		 * back to BTRFS_TRIM_STATE_UNTRIMMED.

 Determine next steps for a block_group */

/**

 * btrfs_run_discard_work - determines if async discard should be running

 * @discard_ctl: discard control

 *

 * Checks if the file system is writeable and BTRFS_FS_DISCARD_RUNNING is set.

/**

 * btrfs_discard_calc_delay - recalculate the base delay

 * @discard_ctl: discard control

 *

 * Recalculate the base delay which is based off the total number of

 * discardable_extents.  Clamp this between the lower_limit (iops_limit or 1ms)

 * and the upper_limit (BTRFS_DISCARD_MAX_DELAY_MSEC).

	/*

	 * The following is to fix a potential -1 discrepenancy that we're not

	 * sure how to reproduce. But given that this is the only place that

	 * utilizes these numbers and this is only called by from

	 * btrfs_finish_extent_commit() which is synchronized, we can correct

	 * here.

/**

 * btrfs_discard_update_discardable - propagate discard counters

 * @block_group: block_group of interest

 *

 * This propagates deltas of counters up to the discard_ctl.  It maintains a

 * current counter and a previous counter passing the delta up to the global

 * stat.  Then the current counter value becomes the previous counter value.

/**

 * btrfs_discard_punt_unused_bgs_list - punt unused_bgs list to discard lists

 * @fs_info: fs_info of interest

 *

 * The unused_bgs list needs to be punted to the discard lists because the

 * order of operations is changed.  In the normal synchronous discard path, the

 * block groups are trimmed via a single large trim in transaction commit.  This

 * is ultimately what we are trying to avoid with asynchronous discard.  Thus,

 * it must be done before going down the unused_bgs path.

 We enabled async discard, so punt all to the queue */

/**

 * btrfs_discard_purge_list - purge discard lists

 * @discard_ctl: discard control

 *

 * If we are disabling async discard, we may have intercepted block groups that

 * are completely free and ready for the unused_bgs path.  As discarding will

 * now happen in transaction commit or not at all, we can safely mark the

 * corresponding block groups as unused and they will be sent on their merry

 * way to the unused_bgs list.

 SPDX-License-Identifier: GPL-2.0

	/*

	 * This is used for BTRFS_MOD_LOG_KEY_* and BTRFS_MOD_LOG_MOVE_KEYS

	 * operations.

 This is used for BTRFS_MOD_LOG_KEY* and BTRFS_MOD_LOG_ROOT_REPLACE. */

 Those are used for op == BTRFS_MOD_LOG_KEY_{REPLACE,REMOVE}. */

 This is used for op == BTRFS_MOD_LOG_MOVE_KEYS. */

 This is used for op == BTRFS_MOD_LOG_ROOT_REPLACE. */

/*

 * Pull a new tree mod seq number for our operation.

/*

 * This adds a new blocker to the tree mod log's blocker list if the @elem

 * passed does not already have a sequence number set. So when a caller expects

 * to record tree modifications, it should ensure to set elem->seq to zero

 * before calling btrfs_get_tree_mod_seq.

 * Returns a fresh, unused tree log modification sequence number, even if no new

 * blocker was added.

			/*

			 * Blocker with lower sequence number exists, we cannot

			 * remove anything from the log.

	/*

	 * Anything that's lower than the lowest existing (read: blocked)

	 * sequence number can be removed from the tree.

/*

 * Key order of the log:

 *       node/leaf start address -> sequence

 *

 * The 'start address' is the logical address of the *new* root node for root

 * replace operations, or the logical address of the affected block for all

 * other operations.

/*

 * Determines if logging can be omitted. Returns true if it can. Otherwise, it

 * returns false with the tree_mod_log_lock acquired. The caller must hold

 * this until all tree mod log insertions are recorded in the rb tree and then

 * write unlock fs_info::tree_mod_log_lock.

 Similar to tree_mod_dont_log, but doesn't acquire any locks. */

	/*

	 * When we override something during the move, we log these removals.

	 * This can only happen when we move towards the beginning of the

	 * buffer, i.e. dst_slot < src_slot.

 We want the node with the highest seq */

 We want the node with the smallest seq */

/*

 * This returns the element from the log with the smallest time sequence

 * value that's in the log (the oldest log item). Any element with a time

 * sequence lower than min_seq will be ignored.

/*

 * This returns the element from the log with the largest time sequence

 * value that's in the log (the most recent log item). Any element with

 * a time sequence lower than min_seq will be ignored.

/*

 * Returns the logical address of the oldest predecessor of the given root.

 * Entries older than time_seq are ignored.

	/*

	 * The very last operation that's logged for a root is the replacement

	 * operation (if it is replaced at all). This has the logical address

	 * of the *new* root, making it the very first operation that's logged

	 * for this root.

		/*

		 * If there are no tree operation for the oldest root, we simply

		 * return it. This should only happen if that (old) root is at

		 * level 0.

		/*

		 * If there's an operation that's not a root replacement, we

		 * found the oldest version of our root. Normally, we'll find a

		 * BTRFS_MOD_LOG_KEY_REMOVE_WHILE_FREEING operation here.

 If there's no old root to return, return what we found instead */

/*

 * tm is a pointer to the first operation to rewind within eb. Then, all

 * previous operations will be rewound (until we reach something older than

 * time_seq).

		/*

		 * All the operations are recorded with the operator used for

		 * the modification. As we're going backwards, we do the

		 * opposite of each operation here.

 if a move operation is needed it's in the log */

			/*

			 * This operation is special. For roots, this must be

			 * handled explicitly before rewinding.

			 * For non-roots, this operation may exist if the node

			 * was a root: root A -> child B; then A gets empty and

			 * B is promoted to the new root. In the mod log, we'll

			 * have a root-replace operation for B, a tree block

			 * that is no root. We simply ignore that operation.

/*

 * Called with eb read locked. If the buffer cannot be rewound, the same buffer

 * is returned. If rewind operations happen, a fresh buffer is returned. The

 * returned buffer is always read-locked. If the returned buffer is not the

 * input buffer, the lock on the input buffer is released and the input buffer

 * is freed (its refcount is decremented).

/*

 * Rewind the state of @root's root node to the given @time_seq value.

 * If there are no changes, the current root->root_node is returned. If anything

 * changed in between, there's a fresh buffer allocated on which the rewind

 * operations are done. In any case, the returned buffer is read locked.

 * Returns NULL on error (with no locks held).

			/*

			 * After the lookup for the most recent tree mod operation

			 * above and before we locked and cloned the extent buffer

			 * 'old', a new tree mod log operation may have been added.

			 * So lookup for a more recent one to make sure the number

			 * of mod log operations we replay is consistent with the

			 * number of items we have in the cloned extent buffer,

			 * otherwise we can hit a BUG_ON when rewinding the extent

			 * buffer.

/*

 * Return the lowest sequence number in the tree modification log.

 *

 * Return the sequence number of the oldest tree modification log user, which

 * corresponds to the lowest sequence number of all existing users. If there are

 * no users it returns 0.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) STRATO AG 2011.  All rights reserved.

/*

 * This module can be used to catch cases when the btrfs kernel

 * code executes write requests to the disk that bring the file

 * system in an inconsistent state. In such a state, a power-loss

 * or kernel panic event would cause that the data on disk is

 * lost or at least damaged.

 *

 * Code is added that examines all block write requests during

 * runtime (including writes of the super block). Three rules

 * are verified and an error is printed on violation of the

 * rules:

 * 1. It is not allowed to write a disk block which is

 *    currently referenced by the super block (either directly

 *    or indirectly).

 * 2. When a super block is written, it is verified that all

 *    referenced (directly or indirectly) blocks fulfill the

 *    following requirements:

 *    2a. All referenced blocks have either been present when

 *        the file system was mounted, (i.e., they have been

 *        referenced by the super block) or they have been

 *        written since then and the write completion callback

 *        was called and no write error was indicated and a

 *        FLUSH request to the device where these blocks are

 *        located was received and completed.

 *    2b. All referenced blocks need to have a generation

 *        number which is equal to the parent's number.

 *

 * One issue that was found using this module was that the log

 * tree on disk became temporarily corrupted because disk blocks

 * that had been in use for the log tree had been freed and

 * reused too early, while being referenced by the written super

 * block.

 *

 * The search term in the kernel log that can be used to filter

 * on the existence of detected integrity issues is

 * "btrfs: attempt".

 *

 * The integrity check is enabled via mount options. These

 * mount options are only supported if the integrity check

 * tool is compiled by defining BTRFS_FS_CHECK_INTEGRITY.

 *

 * Example #1, apply integrity checks to all metadata:

 * mount /dev/sdb1 /mnt -o check_int

 *

 * Example #2, apply integrity checks to all metadata and

 * to data extents:

 * mount /dev/sdb1 /mnt -o check_int_data

 *

 * Example #3, apply integrity checks to all metadata and dump

 * the tree that the super block references to kernel messages

 * each time after a super block was written:

 * mount /dev/sdb1 /mnt -o check_int,check_int_print_mask=263

 *

 * If the integrity check tool is included and activated in

 * the mount options, plenty of kernel memory is used, and

 * plenty of additional CPU cycles are spent. Enabling this

 * functionality is not intended for normal use. In most

 * cases, unless you are a btrfs developer who needs to verify

 * the integrity of (super)-block write requests, do not

 * enable the config option BTRFS_FS_CHECK_INTEGRITY to

 * include and compile the integrity check tool.

 *

 * Expect millions of lines of information in the kernel log with an

 * enabled check_int_print_mask. Therefore set LOG_BUF_SHIFT in the

 * kernel config to at least 26 (which is 64MB). Usually the value is

 * limited to 21 (which is 2MB) in init/Kconfig. The file needs to be

 * changed like this before LOG_BUF_SHIFT can be set to a high value:

 * config LOG_BUF_SHIFT

 *       int "Kernel log buffer size (16 => 64KB, 17 => 128KB)"

 *       range 12 30

#define BTRFSIC_TREE_DUMP_MAX_INDENT_LEVEL (200 - 6)	/* in characters,

/*

 * The definition of the bitmask fields for the print_mask.

 * They are specified with the mount option check_integrity_print_mask.

 only used for debug purposes */

 if it is meta-data, not data-data */

 if it is one of the superblocks */

 if is done by lower subsystem */

 error was indicated to endio */

	unsigned int never_written:1;	/* block was added because it was

					 * referenced, not because it was

	unsigned int mirror_num;	/* large enough to hold

 key, physical byte num on disk */

 logical byte num on disk */

	struct btrfs_disk_key disk_key;	/* extra info to print in case of

 list node */

 list node */

 the following two lists contain block_link items */

 list */

 list */

 only valid if !never_written */

/*

 * Elements of this type are allocated dynamically and required because

 * each block object can refer to and can be ref from multiple blocks.

 * The key to lookup them in the hashtable is the dev_bytenr of

 * the block ref to plus the one from the block referred from.

 * The fact that they are searchable via a hashtable and that a

 * ref_cnt is maintained is not required for the btrfs integrity

 * check algorithm itself, it is only used to make the output more

 * beautiful in case that an error is detected (an error is defined

 * as a write operation to a block while that block is still referenced).

 only used for debug purposes */

 list node */

 list node */

 list node */

 only used for debug purposes */

 list node */

 virtual bytenr */

 physical bytenr on device */

/* This structure is used to implement recursion without occupying

 Some state per mounted filesystem */

 super block bytenr is always the unmapped device bytenr */

 for superblock, only the dev_bytenr makes sense */

 select the one with the highest generation field */

 the one for the initial block is freed in the caller */

 Pages must be unmapped in reverse order */

/*

 * Test whether the disk block contains a tree block (leaf or node)

 * (note that this test fails for the super block)

 not metadata */

 is metadata */

 it would not be safe to go on */

		/*

		 * Clear all references of this block. Do not free

		 * the block itself even if is not referenced anymore

		 * because it still carries valuable information

		 * like whether it was ever written and IO completed.

 unknown */

 unknown */

				/*

				 * disk block is overwritten with extent

				 * data (not meta data) and we are configured

				 * to not include extent data: take the

				 * chance and free the block's memory

 block has not been found in hash table */

 ignore that written D block */

			/* this is getting ugly for the

 unknown */

 unknown */

	/* mutex is not held! This is not save if IO is not yet completed

			block->flush_gen = 0; /* FUA completed means block is

 for FLUSH, this releases the block */

		/*

		 * Note that this situation can happen and does not

		 * indicate an error in regular cases. It happens

		 * when disk blocks are freed and later reused.

		 * The check-integrity module is not aware of any

		 * block free operations, it just recognizes block

		 * write operations. Therefore it keeps the linkage

		 * information for a block until a block is

		 * rewritten. This can temporarily cause incorrect

		 * and even circular linkage information. This

		 * causes no harm unless such blocks are referenced

		 * by the most recent super block.

	/*

	 * This algorithm is recursive because the amount of used stack

	 * space is very small and the max recursion depth is limited.

 refer to comment at "abort cyclic linkage (case 1)" */

	/*

	 * This algorithm is recursive because the amount of used stack space

	 * is very small and the max recursion depth is limited.

	/*

	 * Should better fill an on-stack buffer with a complete line and

	 * dump it at once when it is time to print a newline character.

	/*

	 * This algorithm is recursive because the amount of used stack space

	 * is very small and the max recursion depth is limited.

	/* since btrfsic_submit_bio() is also called before

	/*

	 * Don't care about keeping the lists' state up to date,

	 * just free all memory that was allocated dynamically.

	 * Free the blocks and the block_links.

 SPDX-License-Identifier: GPL-2.0

	/*

	 * If we didn't get into open_ctree our allocated_ebs will not be

	 * initialized, so just skip this.

	/* tells writepage not to lock the state bits for this range

	 * it still does the unlocking

 tells the submit_bio code to use REQ_SYNC */

 Caller should ensure the bio has at least some range added */

 Cleanup unsubmitted bios */

/*

 * Submit bio from extent page data via submit_one_bio

 *

 * Return 0 if everything is OK.

 * Return <0 for error.

		/*

		 * Clean up of epd->bio is handled by its endio function.

		 * And endio is either triggered by successful bio execution

		 * or the error handler of submit bio hook.

		 * So at this point, no matter what happened, we don't need

		 * to clean up epd->bio.

	/*

	 * Make sure all delayed rcu free are flushed before we

	 * destroy caches.

/*

 * For the file_extent_tree, we want to hold the inode lock when we lookup and

 * update the disk_i_size, but lockdep will complain because our io_tree we hold

 * the tree lock and get the inode lock when setting delalloc.  These two things

 * are unrelated, so make a class for the file_extent_tree so we don't get the

 * two locking patterns mixed up.

	/*

	 * Do a single barrier for the waitqueue_active check here, the state

	 * of the waitqueue should not change once extent_io_tree_release is

	 * called.

		/*

		 * btree io trees aren't supposed to have tasks waiting for

		 * changes in the flags of extent states ever.

	/*

	 * The given mask might be not appropriate for the slab allocator,

	 * drop the unsupported bits

/**

 * Search @tree for an entry that contains @offset. Such entry would have

 * entry->start <= offset && entry->end >= offset.

 *

 * @tree:       the tree to search

 * @offset:     offset that should fall within an entry in @tree

 * @next_ret:   pointer to the first entry whose range ends after @offset

 * @prev_ret:   pointer to the first entry whose range begins before @offset

 * @p_ret:      pointer where new node should be anchored (used when inserting an

 *	        entry in the tree)

 * @parent_ret: points to entry which would have been the parent of the entry,

 *               containing @offset

 *

 * This function returns a pointer to the entry that contains @offset byte

 * address. If no such entry exists, then NULL is returned and the other

 * pointer arguments to the function are filled, otherwise the found entry is

 * returned and other pointers are left untouched.

/*

 * utility function to look for merge candidates inside a given range.

 * Any extents with matching state are merged together into a single

 * extent in the tree.  Extents with EXTENT_IO in their state field

 * are not merged because the end_io handlers need to be able to do

 * operations on them without sleeping (or doing allocations/splits).

 *

 * This should be called with the tree lock held.

/*

 * insert an extent_state struct into the tree.  'bits' are set on the

 * struct before it is inserted.

 *

 * This may return -EEXIST if the extent is already there, in which case the

 * state struct is freed.

 *

 * The tree lock is not taken internally.  This is a utility function and

 * probably isn't what you want to call (see set/clear_extent_bit).

/*

 * split a given extent state struct in two, inserting the preallocated

 * struct 'prealloc' as the newly created second half.  'split' indicates an

 * offset inside 'orig' where it should be split.

 *

 * Before calling,

 * the tree has 'orig' at [orig->start, orig->end].  After calling, there

 * are two extent state structs in the tree:

 * prealloc: [orig->start, split - 1]

 * orig: [ split, orig->end ]

 *

 * The tree locks are not taken by this function. They need to be held

 * by the caller.

/*

 * utility function to clear some bits in an extent state struct.

 * it will optionally wake up anyone waiting on this state (wake == 1).

 *

 * If no bits are set on the state struct after clearing things, the

 * struct is freed and removed from the tree

/*

 * clear some bits on a range in the tree.  This may require splitting

 * or inserting elements in the tree, so the gfp mask is used to

 * indicate which allocations or sleeping are allowed.

 *

 * pass 'wake' == 1 to kick any sleepers, and 'delete' == 1 to remove

 * the given range from the tree regardless of state (ie for truncate).

 *

 * the range [start, end] is inclusive.

 *

 * This takes the tree lock, and returns 0 on success and < 0 on error.

		/*

		 * Don't care for allocation failure here because we might end

		 * up not needing the pre-allocated extent state at all, which

		 * is the case if we only have in the tree extent states that

		 * cover our input range and don't cover too any other range.

		 * If we end up needing a new extent state we allocate it later.

	/*

	 * this search will find the extents that end after

	 * our range starts

 the state doesn't have the wanted bits, go ahead */

	/*

	 *     | ---- desired range ---- |

	 *  | state | or

	 *  | ------------- state -------------- |

	 *

	 * We need to split the extent we found, and may flip

	 * bits on second half.

	 *

	 * If the extent we found extends past our range, we

	 * just split and search again.  It'll get split again

	 * the next time though.

	 *

	 * If the extent we found is inside our range, we clear

	 * the desired bit on it.

	/*

	 * | ---- desired range ---- |

	 *                        | state |

	 * We need to split the extent, and clear the bit

	 * on the first half

/*

 * waits for one or more bits to clear on a range in the state tree.

 * The range [start, end] is inclusive.

 * The tree lock is taken by this function

		/*

		 * this search will find all the extents that end after

		 * our range starts

/*

 * set some bits on a range in the tree.  This may require allocations or

 * sleeping, so the gfp mask is used to indicate what is allowed.

 *

 * If any of the exclusive bits are set, this will fail with -EEXIST if some

 * part of the range already has the desired bits set.  The start of the

 * existing range is returned in failed_start in this case.

 *

 * [start, end] is inclusive This takes the tree lock.

		/*

		 * Don't care for allocation failure here because we might end

		 * up not needing the pre-allocated extent state at all, which

		 * is the case if we only have in the tree extent states that

		 * cover our input range and don't cover too any other range.

		 * If we end up needing a new extent state we allocate it later.

	/*

	 * this search will find all the extents that end after

	 * our range starts.

	/*

	 * | ---- desired range ---- |

	 * | state |

	 *

	 * Just lock what we found and keep going

	/*

	 *     | ---- desired range ---- |

	 * | state |

	 *   or

	 * | ------------- state -------------- |

	 *

	 * We need to split the extent we found, and may flip bits on

	 * second half.

	 *

	 * If the extent we found extends past our

	 * range, we just split and search again.  It'll get split

	 * again the next time though.

	 *

	 * If the extent we found is inside our range, we set the

	 * desired bit on it.

		/*

		 * If this extent already has all the bits we want set, then

		 * skip it, not necessary to split it or do anything with it.

	/*

	 * | ---- desired range ---- |

	 *     | state | or               | state |

	 *

	 * There's a hole, we need to insert something in it and

	 * ignore the extent we found.

		/*

		 * Avoid to free 'prealloc' if it can be merged with

		 * the later extent.

	/*

	 * | ---- desired range ---- |

	 *                        | state |

	 * We need to split the extent, and set the bit

	 * on the first half

/**

 * convert_extent_bit - convert all bits in a given range from one bit to

 * 			another

 * @tree:	the io tree to search

 * @start:	the start offset in bytes

 * @end:	the end offset in bytes (inclusive)

 * @bits:	the bits to set in this range

 * @clear_bits:	the bits to clear in this range

 * @cached_state:	state that we're going to cache

 *

 * This will go through and set bits for the given range.  If any states exist

 * already in this range they are set with the given bit and cleared of the

 * clear_bits.  This is only meant to be used by things that are mergeable, ie

 * converting from say DELALLOC to DIRTY.  This is not meant to be used with

 * boundary bits like LOCK.

 *

 * All allocations are done with GFP_NOFS.

		/*

		 * Best effort, don't worry if extent state allocation fails

		 * here for the first iteration. We might have a cached state

		 * that matches exactly the target range, in which case no

		 * extent state allocations are needed. We'll only know this

		 * after locking the tree.

	/*

	 * this search will find all the extents that end after

	 * our range starts.

	/*

	 * | ---- desired range ---- |

	 * | state |

	 *

	 * Just lock what we found and keep going

	/*

	 *     | ---- desired range ---- |

	 * | state |

	 *   or

	 * | ------------- state -------------- |

	 *

	 * We need to split the extent we found, and may flip bits on

	 * second half.

	 *

	 * If the extent we found extends past our

	 * range, we just split and search again.  It'll get split

	 * again the next time though.

	 *

	 * If the extent we found is inside our range, we set the

	 * desired bit on it.

	/*

	 * | ---- desired range ---- |

	 *     | state | or               | state |

	 *

	 * There's a hole, we need to insert something in it and

	 * ignore the extent we found.

		/*

		 * Avoid to free 'prealloc' if it can be merged with

		 * the later extent.

	/*

	 * | ---- desired range ---- |

	 *                        | state |

	 * We need to split the extent, and set the bit

	 * on the first half

 wrappers around set/clear extent bit */

	/*

	 * We don't support EXTENT_LOCKED yet, as current changeset will

	 * record any bits changed, so for EXTENT_LOCKED case, it will

	 * either fail with -EEXIST or changeset will record the whole

	 * range.

	/*

	 * Don't support EXTENT_LOCKED case, same reason as

	 * set_record_extent_bits().

/*

 * either insert or lock state struct between start and end use mask to tell

 * us if waiting is desired.

 Pages should be in the extent_io_tree */

 Pages should be in the extent_io_tree */

/* find the first state struct with 'bits' set after 'start', and

 * return it.  tree->lock must be held.  NULL will returned if

 * nothing was found after 'start'

	/*

	 * this search will find all the extents that end after

	 * our range starts.

/*

 * Find the first offset in the io tree with one or more @bits set.

 *

 * Note: If there are multiple bits set in @bits, any of them will match.

 *

 * Return 0 if we find something, and update @start_ret and @end_ret.

 * Return 1 if we found nothing.

/**

 * Find a contiguous area of bits

 *

 * @tree:      io tree to check

 * @start:     offset to start the search from

 * @start_ret: the first offset we found with the bits set

 * @end_ret:   the final contiguous range of the bits that were set

 * @bits:      bits to look for

 *

 * set_extent_bit and clear_extent_bit can temporarily split contiguous ranges

 * to set bits appropriately, and then merge them again.  During this time it

 * will drop the tree->lock, so use this helper if you want to find the actual

 * contiguous area for given bits.  We will search to the first bit we find, and

 * then walk down the tree until we find a non-contiguous area.  The area

 * returned will be the full contiguous area with the bits set.

/**

 * Find the first range that has @bits not set. This range could start before

 * @start.

 *

 * @tree:      the tree to search

 * @start:     offset at/after which the found extent should start

 * @start_ret: records the beginning of the range

 * @end_ret:   records the end of the range (inclusive)

 * @bits:      the set of bits which must be unset

 *

 * Since unallocated range is also considered one which doesn't have the bits

 * set it's possible that @end_ret contains -1, this happens in case the range

 * spans (last_range_end, end of device]. In this case it's up to the caller to

 * trim @end_ret to the appropriate size.

 Find first extent with bits cleared */

			/*

			 * Tree is completely empty, send full range and let

			 * caller deal with it

			/*

			 * We are past the last allocated chunk, set start at

			 * the end of the last extent.

		/*

		 * At this point 'node' either contains 'start' or start is

		 * before 'node'

				/*

				 * |--range with bits sets--|

				 *    |

				 *    start

				/*

				 * 'start' falls within a range that doesn't

				 * have the bits set, so take its start as

				 * the beginning of the desired range

				 *

				 * |--range with bits cleared----|

				 *      |

				 *      start

			/*

			 * |---prev range---|---hole/unset---|---node range---|

			 *                          |

			 *                        start

			 *

			 *                        or

			 *

			 * |---hole/unset--||--first node--|

			 * 0   |

			 *    start

	/*

	 * Find the longest stretch from start until an entry which has the

	 * bits set

/*

 * find a contiguous range of bytes in the file marked as delalloc, not

 * more than 'max_bytes'.  start and end are used to return the range,

 *

 * true is returned if we find something, false if nothing was in the tree

	/*

	 * this search will find all the extents that end after

	 * our range starts.

/*

 * Process one page for __process_pages_contig().

 *

 * Return >0 if we hit @page == @locked_page.

 * Return 0 if we updated the page status.

 * Return -EGAIN if the we need to try again.

 * (For PAGE_LOCK case but got dirty page or page not belong to mapping)

			/*

			 * Only if we're going to lock these pages, we can find

			 * nothing at @index.

		/*

		 * Update @processed_end. I know this is awful since it has

		 * two different return value patterns (inclusive vs exclusive).

		 *

		 * But the exclusive pattern is necessary if @start is 0, or we

		 * underflow and check against processed_end won't work as

		 * expected.

/*

 * Find and lock a contiguous range of bytes in the file marked as delalloc, no

 * more than @max_bytes.

 *

 * @start:	The original start bytenr to search.

 *		Will store the extent range start bytenr.

 * @end:	The original end bytenr of the search range

 *		Will store the extent range end bytenr.

 *

 * Return true if we find a delalloc range which starts inside the original

 * range, and @start/@end will store the delalloc range start/end.

 *

 * Return false if we can't find any delalloc range which starts inside the

 * original range, and @start/@end will be the non-delalloc range start/end.

 Caller should pass a valid @end to indicate the search range end */

 The range should at least cover part of the page */

 step one, find a bunch of delalloc bytes starting at start */

 @delalloc_end can be -1, never go beyond @orig_end */

	/*

	 * start comes from the offset of locked_page.  We have to lock

	 * pages in order, so we can't process delalloc bytes before

	 * locked_page

	/*

	 * make sure to limit the number of pages we try to lock down

 step two, lock all the pages after the page that has start */

		/* some of the pages are gone, lets avoid looping by

		 * shortening the size of the delalloc range we're searching

 step three, lock the state bits for the whole range */

 then test to make sure it is all still delalloc */

/*

 * count the number of bytes in the tree that have a given bit(s)

 * set.  This can be fairly slow, except for EXTENT_DIRTY which is

 * cached.  The total number found is returned.

	/*

	 * this search will find all the extents that end after

	 * our range starts.

/*

 * set the private field for a given byte offset in the tree.  If there isn't

 * an extent_state there already, this does nothing.

	/*

	 * this search will find all the extents that end after

	 * our range starts.

	/*

	 * this search will find all the extents that end after

	 * our range starts.

/*

 * searches a range in the state tree for a given mask.

 * If 'filled' == 1, this returns 1 only if every extent in the tree

 * has the bits set.  Otherwise, 1 is returned if any bit in the

 * range is found set.

/*

 * this bypasses the standard btrfs submit functions deliberately, as

 * the standard behavior is to write all copies in a raid setup. here we only

 * want to write the one bad copy. so we do the mapping for ourselves and issue

 * submit_bio directly.

 * to avoid any synchronization issues, wait for the data after writing, which

 * actually prevents the read that triggered the error from finishing.

 * currently, there can be no more than two copies of every data bit. thus,

 * exactly one rewrite is required.

	/*

	 * Avoid races with device replace and make sure our bioc has devices

	 * associated to its stripes that don't go away while we are doing the

	 * read repair operation.

		/*

		 * Note that we don't use BTRFS_MAP_WRITE because it's supposed

		 * to update all raid stripes, but here we just want to correct

		 * bad stripe, thus BTRFS_MAP_READ is abused to only get the bad

		 * stripe's dev and sector.

 try to remap that extent elsewhere? */

/*

 * each time an IO finishes, we do a fast check in the IO failure tree

 * to see if we need to process or clean up an io_failure_record

/*

 * Can be called when

 * - hold extent lock

 * - under ordered extent

 * - the inode is freeing

		/*

		 * when data can be on disk more than twice, add to failrec here

		 * (e.g. with a list for failed_mirror) to make

		 * clean_io_failure() clean all those errors at once.

 Set the bits in the private failure tree */

 Set the bits in the inode's tree */

		/*

		 * we only have a single copy of the data, so don't bother with

		 * all the retry and error correction code that follows. no

		 * matter what the error is, it is very likely to persist.

 The failure record should only contain one sector */

	/*

	 * There are two premises:

	 * a) deliver good data to the caller

	 * b) correct the bad sectors on disk

	 *

	 * Since we're only doing repair for one sector, we only need to get

	 * a good copy of the failed sector and if we succeed, we have setup

	 * everything for repair_io_failure to do the rest for us.

 We're here because we had some read errors or csum mismatch */

	/*

	 * We only get called on buffered IO, thus page must be mapped and bio

	 * must not be cloned.

 Iterate through all the sectors in the range */

			/*

			 * This sector has no error, just end the page read

			 * and unlock the range.

			/*

			 * We have submitted the read repair, the page release

			 * will be handled by the endio function of the

			 * submitted repair bio.

			 * Thus we don't need to do any thing here.

		/*

		 * Repair failed, just record the error but still continue.

		 * Or the remaining sectors will not be properly unlocked.

 lots and lots of room for performance fixes in the end_bio funcs */

/*

 * after a writepage IO is done, we need to:

 * clear the uptodate bits on error

 * clear the writeback bits in the extent tree for this IO

 * end_page_writeback if the page has no more pending IO

 *

 * Scheduling is not allowed, so the extent state tree is expected

 * to have one and only one object corresponding to this IO.

 Our read/write should always be sector aligned. */

/*

 * Record previously processed extent range

 *

 * For endio_readpage_release_extent() to handle a full extent range, reducing

 * the extent io operations.

 Start of the range in @inode */

 End of the range in @inode */

/*

 * Try to release processed extent range

 *

 * May not release the extent range right now if the current range is

 * contiguous to processed extent.

 *

 * Will release processed extent when any of @inode, @uptodate, the range is

 * no longer contiguous to the processed range.

 *

 * Passing @inode == NULL will force processed extent to be released.

 The first extent, initialize @processed */

	/*

	 * Contiguous to processed extent, just uptodate the end.

	 *

	 * Several things to notice:

	 *

	 * - bio can be merged as long as on-disk bytenr is contiguous

	 *   This means we can have page belonging to other inodes, thus need to

	 *   check if the inode still matches.

	 * - bvec can contain range beyond current page for multi-page bvec

	 *   Thus we need to do processed->end + 1 >= start check

	/*

	 * Now we don't have range contiguous to the processed range, release

	 * the processed range now.

 Update processed to current range */

/*

 * Find extent buffer for a givne bytenr.

 *

 * This is for end_bio_extent_readpage(), thus we can't do any unsafe locking

 * in endio context.

	/*

	 * For regular sectorsize, we can use page->private to grab extent

	 * buffer

 For subpage case, we need to lookup buffer radix tree */

/*

 * after a readpage IO is done, we need to:

 * clear the uptodate bits on error

 * set the uptodate bits if things worked

 * set the page up to date if all extents in the tree are uptodate

 * clear the lock bit in the extent tree

 * unlock the page if there are no other extents locked for it

 *

 * Scheduling is not allowed, so the extent state tree is expected

 * to have one and only one object corresponding to this IO.

	/*

	 * The offset to the beginning of a bio, since one bio can never be

	 * larger than UINT_MAX, u32 here is enough.

		/*

		 * We always issue full-sector reads, but if some block in a

		 * page fails to read, blk_update_request() will advance

		 * bv_offset and adjust bv_len to compensate.  Print a warning

		 * for unaligned offsets, and an error if they don't add up to

		 * a full sector.

			/*

			 * btrfs_submit_read_repair() will handle all the good

			 * and bad sectors, we just continue to the next bvec.

			/*

			 * Zero out the remaining part if this range straddles

			 * i_size.

			 *

			 * Here we should only zero the range inside the bvec,

			 * not touch anything else.

			 *

			 * NOTE: i_size is exclusive while end is inclusive.

 Update page status and unlock */

 Release the last extent */

/*

 * Initialize the members up to but not including 'bio'. Use after allocating a

 * new bio by bio_alloc_bioset as it does not initialize the bytes outside of

 * 'bio' because use of __GFP_ZERO is not supported.

/*

 * Allocate a btrfs_io_bio, with @nr_iovecs as maximum number of iovecs.

 *

 * The bio allocation is backed by bioset and does not fail.

 Bio allocation backed by a bioset does not fail */

 this will never fail when it's backed by a bioset */

/**

 * Attempt to add a page to bio

 *

 * @bio:	destination bio

 * @page:	page to add to the bio

 * @disk_bytenr:  offset of the new bio or to check whether we are adding

 *                a contiguous page to the previous one

 * @pg_offset:	starting offset in the page

 * @size:	portion of page that we want to write

 * @prev_bio_flags:  flags of previous bio to see if we can merge the current one

 * @bio_flags:	flags of the current bio to see if we can merge them

 *

 * Attempt to add a page to bio considering stripe alignment etc.

 *

 * Return >= 0 for the number of bytes added to the bio.

 * Can return 0 if the current bio is already at stripe/zone boundary.

 * Return <0 for error.

 The limit should be calculated when bio_ctrl->bio is allocated */

	/*

	 * If real_size is 0, never call bio_add_*_page(), as even size is 0,

	 * bio will still execute its endio function on the page!

	/*

	 * Pages for compressed extent are never submitted to disk directly,

	 * thus it has no real boundary, just set them to U32_MAX.

	 *

	 * The split happens for real compressed bio, which happens in

	 * btrfs_submit_compressed_read/write().

 Ordered extent not yet created, so we're good */

	/*

	 * For compressed page range, its disk_bytenr is always @disk_bytenr

	 * passed in, no matter if we have added any range into previous bio.

/*

 * @opf:	bio REQ_OP_* and REQ_* flags as one value

 * @wbc:	optional writeback control for io accounting

 * @page:	page to add to the bio

 * @disk_bytenr: logical bytenr where the write will be

 * @size:	portion of page that we want to write to

 * @pg_offset:	offset of the new bio or to check whether we are adding

 *              a contiguous page to the previous one

 * @bio_ret:	must be valid pointer, newly allocated bio will be stored there

 * @end_io_func:     end_io callback for new bio

 * @mirror_num:	     desired mirror to read/write

 * @prev_bio_flags:  flags of previous bio to see if we can merge the current one

 * @bio_flags:	flags of the current bio to see if we can merge them

 Allocate new bio if needed */

		/*

		 * We must go through btrfs_bio_add_page() to ensure each

		 * page range won't cross various boundaries.

 Metadata page range should never be split */

 At least we added some page, update the account */

 We have reached boundary, submit right now */

 The bio should contain some page(s) */

	/*

	 * If the page is mapped to btree inode, we should hold the private

	 * lock to prevent race.

	 * For cloned or dummy extent buffers, their pages are not mapped and

	 * will not race with any other ebs.

 Already mapped, just free prealloc */

 Has preallocated memory for subpage */

 Do new allocation to attach subpage */

/*

 * basic readpage implementation.  Locked extent state structs are inserted

 * into the tree that are removed when the IO is done (by the end_io

 * handlers)

 * XXX JDM: This needs looking at to ensure proper page locking

 * return 0 on success, otherwise return error

		/*

		 * If we have a file range that points to a compressed extent

		 * and it's followed by a consecutive file range that points

		 * to the same compressed extent (possibly with a different

		 * offset and/or length, so it either points to the whole extent

		 * or only part of it), we must make sure we do not submit a

		 * single bio to populate the pages for the 2 ranges because

		 * this makes the compressed extent read zero out the pages

		 * belonging to the 2nd range. Imagine the following scenario:

		 *

		 *  File layout

		 *  [0 - 8K]                     [8K - 24K]

		 *    |                               |

		 *    |                               |

		 * points to extent X,         points to extent X,

		 * offset 4K, length of 8K     offset 0, length 16K

		 *

		 * [extent X, compressed length = 4K uncompressed length = 16K]

		 *

		 * If the bio to read the compressed extent covers both ranges,

		 * it will decompress extent X into the pages belonging to the

		 * first range and then it will stop, zeroing out the remaining

		 * pages that belong to the other range that points to extent X.

		 * So here we make sure we submit 2 bios, one for the first

		 * range and another one for the third range. Both will target

		 * the same physical extent from disk, but we can't currently

		 * make the compressed bio endio callback populate the pages

		 * for both ranges because each compressed bio is tightly

		 * coupled with a single extent map, and each range can have

		 * an extent map with a different offset value relative to the

		 * uncompressed data of our extent and different lengths. This

		 * is a corner case so we prioritize correctness over

		 * non-optimal behavior (submitting 2 bios for the same extent).

 we've found a hole, just zero and go on */

 the get_extent function already copied into the page */

		/* we have an inline extent but it didn't get marked up

		 * to date.  Error out

/*

 * helper for __extent_writepage, doing all of the delayed allocation setup.

 *

 * This returns 1 if btrfs_run_delalloc_range function did all the work required

 * to write the page (copy into inline extent).  In this case the IO has

 * been started and the page is already unlocked.

 *

 * This returns 0 if all went well (page still locked)

 * This returns < 0 if there were errors (page still locked)

		/*

		 * delalloc_end is already one less than the total length, so

		 * we don't subtract one from PAGE_SIZE

	/* did the fill delalloc function already unlock and start

	 * the IO?

		/*

		 * we've unlocked the page, so we can't update

		 * the mapping's writeback index, just update

		 * nr_to_write.

/*

 * Find the first byte we need to write.

 *

 * For subpage, one page can contain several sectors, and

 * __extent_writepage_io() will just grab all extent maps in the page

 * range and try to submit all non-inline/non-compressed extents.

 *

 * This is a big problem for subpage, we shouldn't re-submit already written

 * data at all.

 * This function will lookup subpage dirty bit to find which range we really

 * need to submit.

 *

 * Return the next dirty range in [@start, @end).

 * If no dirty range is found, @start will be page_offset(page) + PAGE_SIZE.

 Declare as unsigned long so we can use bitmap ops */

	/*

	 * For regular sector size == page size case, since one page only

	 * contains one sector, we return the page offset directly.

 We should have the page locked, but just in case */

/*

 * helper for __extent_writepage.  This calls the writepage start hooks,

 * and does the loop to map the page into extents and bios.

 *

 * We return 1 if the IO is started and the page is unlocked,

 * 0 if all went well (page still locked)

 * < 0 if there were errors (page still locked)

 Fixup worker will requeue */

	/*

	 * we don't want to touch the inode after unlocking the page,

	 * so we update the mapping writeback index now

			/*

			 * This range is beyond i_size, thus we don't need to

			 * bother writing back.

			 * But we still need to clear the dirty subpage bit, or

			 * the next time the page gets dirtied, we will try to

			 * writeback the sectors with subpage dirty bits,

			 * causing writeback without ordered extent.

		/*

		 * Note that em_end from extent_map_end() and dirty_range_end from

		 * find_next_dirty_byte() are all exclusive

		/*

		 * compressed and inline extents are written through other

		 * paths in the FS

		/*

		 * Although the PageDirty bit is cleared before entering this

		 * function, subpage dirty bit is not cleared.

		 * So clear subpage dirty bit here so next time we won't submit

		 * page for range already written to disk.

	/*

	 * If we finish without problem, we should not only clear page dirty,

	 * but also empty subpage dirty bits

/*

 * the writepage semantics are similar to regular writepage.  extent

 * records are inserted to lock ranges in the tree, and as dirty areas

 * are found, they are marked writeback.  Then the lock bits are removed

 * and the end_io handler clears the writeback ranges

 *

 * Return 0 if everything goes well.

 * Return <0 for error.

 make sure the mapping tag for page dirty gets cleared */

	/*

	 * Here we used to have a check for PageError() and then set @ret and

	 * call end_extent_writepage().

	 *

	 * But in fact setting @ret here will cause different error paths

	 * between subpage and regular sectorsize.

	 *

	 * For regular page size, we never submit current page, but only add

	 * current page to current bio.

	 * The bio submission can only happen in next page.

	 * Thus if we hit the PageError() branch, @ret is already set to

	 * non-zero value and will not get updated for regular sectorsize.

	 *

	 * But for subpage case, it's possible we submit part of current page,

	 * thus can get PageError() set by submitted bio of the same page,

	 * while our @ret is still 0.

	 *

	 * So here we unify the behavior and don't set @ret.

	 * Error can still be properly passed to higher layer as page will

	 * be set error, here we just don't handle the IO failure.

	 *

	 * NOTE: This is just a hotfix for subpage.

	 * The root fix will be properly ending ordered extent when we hit

	 * an error during writeback.

	 *

	 * But that needs a bigger refactoring, as we not only need to grab the

	 * submitted OE, but also need to know exactly at which bytenr we hit

	 * the error.

	 * Currently the full page based __extent_writepage_io() is not

	 * capable of that.

		/*

		 * If epd->extent_locked, it's from extent_write_locked_range(),

		 * the page can either be locked by lock_page() or

		 * process_one_page().

		 * Let btrfs_page_unlock_writer() handle both cases.

/*

 * Lock extent buffer status and pages for writeback.

 *

 * May try to flush write bio if we can't get the lock.

 *

 * Return  0 if the extent buffer doesn't need to be submitted.

 *           (E.g. the extent buffer is not dirty)

 * Return >0 is the extent buffer is submitted to bio.

 * Return <0 if something went wrong, no page is locked.

	/*

	 * We need to do this to prevent races in people who check if the eb is

	 * under IO since we can end up having no IO bits set for a short period

	 * of time.

	/*

	 * Either we don't need to submit any tree block, or we're submitting

	 * subpage eb.

	 * Subpage metadata doesn't use page locking at all, so we can skip

	 * the page locking.

 Unlock already locked pages */

	/*

	 * Clear EXTENT_BUFFER_WRITEBACK and wake up anyone waiting on it.

	 * Also set back EXTENT_BUFFER_DIRTY so future attempts to this eb can

	 * be made and undo everything done before.

	/*

	 * If we error out, we should add back the dirty_metadata_bytes

	 * to make it consistent.

	/*

	 * If writeback for a btree extent that doesn't belong to a log tree

	 * failed, increment the counter transaction->eb_write_errors.

	 * We do this because while the transaction is running and before it's

	 * committing (when we call filemap_fdata[write|wait]_range against

	 * the btree inode), we might have

	 * btree_inode->i_mapping->a_ops->writepages() called by the VM - if it

	 * returns an error or an error happens during writeback, when we're

	 * committing the transaction we wouldn't know about it, since the pages

	 * can be no longer dirty nor marked anymore for writeback (if a

	 * subsequent modification to the extent buffer didn't happen before the

	 * transaction commit), which makes filemap_fdata[write|wait]_range not

	 * able to find the pages tagged with SetPageError at transaction

	 * commit time. So if this happens we must abort the transaction,

	 * otherwise we commit a super block with btree roots that point to

	 * btree nodes/leafs whose content on disk is invalid - either garbage

	 * or the content of some node/leaf from a past generation that got

	 * cowed or deleted and is no longer valid.

	 *

	 * Note: setting AS_EIO/AS_ENOSPC in the btree inode's i_mapping would

	 * not be enough - we need to distinguish between log tree extents vs

	 * non-log tree extents, and the next filemap_fdatawait_range() call

	 * will catch and clear such errors in the mapping - and that call might

	 * be from a log sync and not from a transaction commit. Also, checking

	 * for the eb flag EXTENT_BUFFER_WRITE_ERR at transaction commit time is

	 * not done and would not be reliable - the eb might have been released

	 * from memory and reading it back again means that flag would not be

	 * set (since it's a runtime flag, not persisted on disk).

	 *

	 * Using the flags below in the btree inode also makes us achieve the

	 * goal of AS_EIO/AS_ENOSPC when writepages() returns success, started

	 * writeback for all dirty pages and before filemap_fdatawait_range()

	 * is called, the writeback for all dirty pages had already finished

	 * with errors - because we were not using AS_EIO/AS_ENOSPC,

	 * filemap_fdatawait_range() would return success, as it could not know

	 * that writeback errors happened (the pages were no longer tagged for

	 * writeback).

 unexpected, logic error */

/*

 * The endio specific version which won't touch any unsafe spinlock in endio

 * context.

/*

 * The endio function for subpage extent buffer write.

 *

 * Unlike end_bio_extent_buffer_writepage(), we only call end_page_writeback()

 * after all extent buffers in the page has finished their writeback.

 Iterate through all extent buffers in the range */

			/*

			 * Here we can't use find_extent_buffer(), as it may

			 * try to lock eb->refs_lock, which is not safe in endio

			 * context.

			/*

			 * free_extent_buffer() will grab spinlock which is not

			 * safe in endio context. Thus here we manually dec

			 * the ref.

 Set btree blocks beyond nritems with 0 to avoid stale content */

		/*

		 * Leaf:

		 * header 0 1 2 .. N ... data_N .. data_2 data_1 data_0

/*

 * Unlike the work in write_one_eb(), we rely completely on extent locking.

 * Page locking is only utilized at minimum to keep the VMM code happy.

 clear_page_dirty_for_io() in subpage helper needs page locked */

 Check if this is the last dirty bit to update nr_written */

	/*

	 * Submission finished without problem, if no range of the page is

	 * dirty anymore, we have submitted a page.  Update nr_written in wbc.

/*

 * Submit one subpage btree page.

 *

 * The main difference to submit_eb_page() is:

 * - Page locking

 *   For subpage, we don't rely on page locking at all.

 *

 * - Flush write bio

 *   We only flush bio if we may be unable to fit current extent buffers into

 *   current bio.

 *

 * Return >=0 for the number of submitted extent buffers.

 * Return <0 for fatal error.

 Lock and write each dirty extent buffers in the range */

		/*

		 * Take private lock to ensure the subpage won't be detached

		 * in the meantime.

		/*

		 * Here we just want to grab the eb without touching extra

		 * spin locks, so call find_extent_buffer_nolock().

		/*

		 * The eb has already reached 0 refs thus find_extent_buffer()

		 * doesn't return it. We don't need to write back such eb

		 * anyway.

 We hit error, end bio for the submitted extent buffers */

/*

 * Submit all page(s) of one extent buffer.

 *

 * @page:	the page of one extent buffer

 * @eb_context:	to determine if we need to submit this page, if current page

 *		belongs to this eb, we don't need to submit

 *

 * The caller should pass each page in their bytenr order, and here we use

 * @eb_context to determine if we have submitted pages of one extent buffer.

 *

 * If we have, we just skip until we hit a new page that doesn't belong to

 * current @eb_context.

 *

 * If not, we submit all the page(s) of the extent buffer.

 *

 * Return >0 if we have submitted the extent buffer successfully.

 * Return 0 if we don't need to submit the page, as it's already submitted by

 * previous call.

 * Return <0 for fatal error.

	/*

	 * Shouldn't happen and normally this would be a BUG_ON but no point

	 * crashing the machine for something we can survive anyway.

		/*

		 * If for_sync, this hole will be filled with

		 * trasnsaction commit.

 Impiles write in zoned mode */

 Mark the last eb in a block group */

 Inclusive */

 Start from prev offset */

		/*

		 * Start from the beginning does not need to cycle over the

		 * range, mark it as scanned.

			/*

			 * the filesystem may choose to bump up nr_to_write.

			 * We have to make sure to honor the new nr_to_write

			 * at any time

		/*

		 * We hit the last page and there is more work to be done: wrap

		 * back to the start of the file

	/*

	 * If something went wrong, don't allow any metadata write bio to be

	 * submitted.

	 *

	 * This would prevent use-after-free if we had dirty pages not

	 * cleaned up, which can still happen by fuzzed images.

	 *

	 * - Bad extent tree

	 *   Allowing existing tree block to be allocated for other trees.

	 *

	 * - Log tree operations

	 *   Exiting tree blocks get allocated to log tree, bumps its

	 *   generation, then get cleaned in tree re-balance.

	 *   Such tree block will not be written back, since it's clean,

	 *   thus no WRITTEN flag set.

	 *   And after log writes back, this tree block is not traced by

	 *   any dirty extent_io_tree.

	 *

	 * - Offending tree block gets re-dirtied from its original owner

	 *   Since it has bumped generation, no WRITTEN flag, it can be

	 *   reused without COWing. This tree block will not be traced

	 *   by btrfs_transaction::dirty_pages.

	 *

	 *   Now such dirty tree block will not be cleaned by any dirty

	 *   extent io tree. Thus we don't want to submit such wild eb

	 *   if the fs already has error.

/**

 * Walk the list of dirty pages of the given address space and write all of them.

 *

 * @mapping: address space structure to write

 * @wbc:     subtract the number of written pages from *@wbc->nr_to_write

 * @epd:     holds context for the write, namely the bio

 *

 * If a page is already under I/O, write_cache_pages() skips it, even

 * if it's dirty.  This is desirable behaviour for memory-cleaning writeback,

 * but it is INCORRECT for data-integrity system calls such as fsync().  fsync()

 * and msync() need to guarantee that all the data which was dirty at the time

 * the call was made get new I/O started against them.  If wbc->sync_mode is

 * WB_SYNC_ALL then we were called for data integrity and we must wait for

 * existing IO to complete.

 Inclusive */

	/*

	 * We have to hold onto the inode so that ordered extents can do their

	 * work when the IO finishes.  The alternative to this is failing to add

	 * an ordered extent if the igrab() fails there and that is a huge pain

	 * to deal with, so instead just hold onto the inode throughout the

	 * writepages operation.  If it fails here we are freeing up the inode

	 * anyway and we'd rather not waste our time writing out stuff that is

	 * going to be truncated anyway.

 Start from prev offset */

		/*

		 * Start from the beginning does not need to cycle over the

		 * range, mark it as scanned.

	/*

	 * We do the tagged writepage as long as the snapshot flush bit is set

	 * and we are the first one who do the filemap_flush() on this inode.

	 *

	 * The nr_to_write == LONG_MAX is needed to make sure other flushers do

	 * not race in and drop the bit.

			/*

			 * At this point we hold neither the i_pages lock nor

			 * the page lock: the page may be truncated or

			 * invalidated (changing page->mapping to NULL),

			 * or even swizzled back from swapper_space to

			 * tmpfs file mapping

			/*

			 * the filesystem may choose to bump up nr_to_write.

			 * We have to make sure to honor the new nr_to_write

			 * at any time

		/*

		 * We hit the last page and there is more work to be done: wrap

		 * back to the start of the file

		/*

		 * If we're looping we could run into a page that is locked by a

		 * writer and that writer could be waiting on writeback for a

		 * page in our current bio, and thus deadlock, so flush the

		 * write bio here.

/*

 * Submit the pages in the range to bio for call sites which delalloc range has

 * already been ran (aka, ordered extent inserted) and all pages are still

 * locked.

 We're called from an async helper function */

		/*

		 * All pages in the range are locked since

		 * btrfs_run_delalloc_range(), thus there is no way to clear

		 * the page dirty flag.

	/*

	 * Allow only a single thread to do the reloc work in zoned mode to

	 * protect the write pointer updates.

/*

 * basic invalidatepage code, this waits on any locked or writeback

 * ranges corresponding to the page, and then deletes any extent state

 * records from the tree

 This function is only called for the btree inode */

	/*

	 * Currently for btree io tree, only EXTENT_LOCKED is utilized,

	 * so here we only need to unlock the extent range to free any

	 * existing extent state.

/*

 * a helper for releasepage, this tests for areas of the page that

 * are locked or under IO and drops the related state bits if it is safe

 * to drop the page.

		/*

		 * At this point we can safely clear everything except the

		 * locked bit, the nodatasum bit and the delalloc new bit.

		 * The delalloc new bit will be cleared by ordered extent

		 * completion.

		/* if clear_extent_bit failed for enomem reasons,

		 * we can't allow the release to continue.

/*

 * a helper for releasepage.  As long as there are no locked extents

 * in the range corresponding to the page, both state records and extent

 * map records are removed

			/*

			 * If it's not in the list of modified extents, used

			 * by a fast fsync, we can remove it. If it's being

			 * logged we can safely remove it since fsync took an

			 * extra reference on the em.

			/*

			 * If it's in the list of modified extents, remove it

			 * only if its generation is older then the current one,

			 * in which case we don't need it for a fast fsync.

			 * Otherwise don't remove it, we could be racing with an

			 * ongoing fast fsync that could miss the new extent.

			/*

			 * We only remove extent maps that are not in the list of

			 * modified extents or that are in the list but with a

			 * generation lower then the current generation, so there

			 * is no need to set the full fsync flag on the inode (it

			 * hurts the fsync performance for workloads with a data

			 * size that exceeds or is close to the system's memory).

 once for the rb tree */

 once for us */

 Allow large-extent preemption. */

/*

 * helper function for fiemap, which doesn't want to see any holes.

 * This maps until we find something past 'last'

 if this isn't a hole return it */

 this is a hole, advance to the next extent */

/*

 * To cache previous fiemap extent

 *

 * Will be used for merging fiemap extent

/*

 * Helper to submit fiemap extent.

 *

 * Will try to merge current fiemap extent specified by @offset, @phys,

 * @len and @flags with cached one.

 * And only when we fails to merge, cached one will be submitted as

 * fiemap extent.

 *

 * Return value is the same as fiemap_fill_next_extent().

	/*

	 * Sanity check, extent_fiemap() should have ensured that new

	 * fiemap extent won't overlap with cached one.

	 * Not recoverable.

	 *

	 * NOTE: Physical address can overlap, due to compression

	/*

	 * Only merges fiemap extents if

	 * 1) Their logical addresses are continuous

	 *

	 * 2) Their physical addresses are continuous

	 *    So truly compressed (physical size smaller than logical size)

	 *    extents won't get merged with each other

	 *

	 * 3) Share same flags except FIEMAP_EXTENT_LAST

	 *    So regular extent won't get merged with prealloc extent

 Not mergeable, need to submit cached one */

/*

 * Emit last fiemap cache

 *

 * The last fiemap cache may still be cached in the following case:

 * 0		      4k		    8k

 * |<- Fiemap range ->|

 * |<------------  First extent ----------->|

 *

 * In this case, the first extent range will be cached but not emitted.

 * So we must emit it before ending extent_fiemap().

	/*

	 * We can't initialize that to 'start' as this could miss extents due

	 * to extent item merging

	/*

	 * lookup the last file extent.  We're not using i_size here

	 * because there might be preallocation past i_size

 No extents, but there might be delalloc bits */

 have to trust i_size as the end */

		/*

		 * remember the start of the last extent.  There are a

		 * bunch of different factors that go into the length of the

		 * extent, so its much less complex to remember where it started

	/*

	 * we might have some extents allocated but more delalloc past those

	 * extents.  so, we trust isize unless the start of the last extent is

	 * beyond isize

 break if the extent we found is outside the range */

		/*

		 * get_extent may return an extent that starts before our

		 * requested range.  We have to make sure the ranges

		 * we return to fiemap always move forward and don't

		 * overlap, so adjust the offsets here

		/*

		 * record the offset from the start of the extent

		 * for adjusting the disk offset below.  Only do this if the

		 * extent isn't compressed since our in ram offset may be past

		 * what we have actually allocated on disk.

		/*

		 * bump off for our next call to get_extent

			/*

			 * As btrfs supports shared space, this information

			 * can be exported to userspace tools via

			 * flag FIEMAP_EXTENT_SHARED.  If fi_extents_max == 0

			 * then we're just getting a count and we can skip the

			 * lookup stuff.

 now scan forward to see if this is really the last extent. */

		/*

		 * Even there is no eb refs here, we may still have

		 * end_page_read() call relying on page::private.

	/*

	 * For mapped eb, we're going to change the page private, which should

	 * be done under the private_lock.

		/*

		 * We do this since we'll remove the pages after we've

		 * removed the eb from the radix tree, so we could race

		 * and have this page now attached to the new eb.  So

		 * only clear page_private if it's still connected to

		 * this eb.

			/*

			 * We need to make sure we haven't be attached

			 * to a new eb.

	/*

	 * For subpage, we can have dummy eb with page private.  In this case,

	 * we can directly detach the private as such page is only attached to

	 * one dummy eb, no sharing.

	/*

	 * We can only detach the page private if there are no other ebs in the

	 * page range and no unfinished IO.

 Release all pages attached to the extent buffer */

 One for when we allocated the page */

/*

 * Helper for releasing the extent buffer.

	/*

	 * Set UNMAPPED before calling btrfs_release_extent_buffer(), as

	 * btrfs_release_extent_buffer() have different behavior for

	 * UNMAPPED subpage extent buffer.

	/*

	 * The TREE_REF bit is first set when the extent_buffer is added

	 * to the radix tree. It is also reset, if unset, when a new reference

	 * is created by find_extent_buffer.

	 *

	 * It is only cleared in two cases: freeing the last non-tree

	 * reference to the extent_buffer when its STALE bit is set or

	 * calling releasepage when the tree reference is the only reference.

	 *

	 * In both cases, care is taken to ensure that the extent_buffer's

	 * pages are not under io. However, releasepage can be concurrently

	 * called with creating new references, which is prone to race

	 * conditions between the calls to check_buffer_tree_ref in those

	 * codepaths and clearing TREE_REF in try_release_extent_buffer.

	 *

	 * The actual lifetime of the extent_buffer in the radix tree is

	 * adequately protected by the refcount, but the TREE_REF bit and

	 * its corresponding reference are not. To protect against this

	 * class of races, we call check_buffer_tree_ref from the codepaths

	 * which trigger io after they set eb->io_pages. Note that once io is

	 * initiated, TREE_REF can no longer be cleared, so that is the

	 * moment at which any such race is best fixed.

	/*

	 * Lock our eb's refs_lock to avoid races with free_extent_buffer().

	 * When we get our eb it might be flagged with EXTENT_BUFFER_STALE and

	 * another task running free_extent_buffer() might have seen that flag

	 * set, eb->refs == 2, that the buffer isn't under IO (dirty and

	 * writeback flags not set) and it's still in the tree (flag

	 * EXTENT_BUFFER_TREE_REF set), therefore being in the process of

	 * decrementing the extent buffer's reference count twice.  So here we

	 * could race and increment the eb's reference count, clear its stale

	 * flag, mark it as dirty and drop our reference before the other task

	 * finishes executing free_extent_buffer, which would later result in

	 * an attempt to free an extent buffer that is dirty.

	/*

	 * For subpage case, we completely rely on radix tree to ensure we

	 * don't try to insert two ebs for the same bytenr.  So here we always

	 * return NULL and just continue.

 Page not yet attached to an extent buffer */

	/*

	 * We could have already allocated an eb for this page and attached one

	 * so lets see if we can get a ref on the existing eb, and if we can we

	 * know it's good and we can just return that one, else we know we can

	 * just overwrite page->private.

		/*

		 * Preallocate page->private for subpage case, so that we won't

		 * allocate memory with private_lock hold.  The memory will be

		 * freed by attach_extent_buffer_page() or freed manually if

		 * we exit earlier.

		 *

		 * Although we have ensured one subpage eb can only have one

		 * page, but it may change in the future for 16K page size

		 * support, so we still preallocate the memory in the loop.

 Should not fail, as we have preallocated the memory */

		/*

		 * To inform we have extra eb under allocation, so that

		 * detach_extent_buffer_page() won't release the page private

		 * when the eb hasn't yet been inserted into radix tree.

		 *

		 * The ref will be decreased when the eb released the page, in

		 * detach_extent_buffer_page().

		 * Thus needs no special handling in error path.

		/*

		 * We can't unlock the pages just yet since the extent buffer

		 * hasn't been properly inserted in the radix tree, this

		 * opens a race with btree_releasepage which can free a page

		 * while we are still filling in all pages for the buffer and

		 * we could crash.

 add one reference for the tree */

	/*

	 * Now it's safe to unlock the pages because any calls to

	 * btree_releasepage will correctly detect that a page belongs to a

	 * live buffer and won't free them prematurely.

 Should be safe to release our pages at this point */

	/*

	 * I know this is terrible, but it's temporary until we stop tracking

	 * the uptodate bits and such for the extent buffers.

 btree_clear_page_dirty() needs page locked */

		/*

		 * For subpage case, we can have other extent buffers in the

		 * same page, and in clear_subpage_extent_buffer_dirty() we

		 * have to clear page dirty without subpage lock held.

		 * This can cause race where our page gets dirty cleared after

		 * we just set it.

		 *

		 * Thankfully, clear_subpage_extent_buffer_dirty() has locked

		 * its page for other reasons, we can use page lock to prevent

		 * the above race.

		/*

		 * In the endio function, if we hit something wrong we will

		 * increase the io_pages, so here we need to decrease it for

		 * error path.

			/*

			 * WAIT_NONE is only utilized by readahead. If we can't

			 * acquire the lock atomically it means either the eb

			 * is being read out or under modification.

			 * Either way the eb will be or has been cached,

			 * readahead can exit safely.

	/*

	 * We need to firstly lock all pages to make sure that

	 * the uptodate bit of our pages won't be affected by

	 * clear_extent_buffer_uptodate().

	/*

	 * It is possible for releasepage to clear the TREE_REF bit before we

	 * set io_pages. See check_buffer_tree_ref for a more detailed comment.

				/*

				 * We failed to submit the bio so it's the

				 * caller's responsibility to perform cleanup

				 * i.e unlock page/set error bit.

/*

 * Check if the [start, start + len) range is valid before reading/writing

 * the eb.

 * NOTE: @start and @len are offset inside the eb, not logical address.

 *

 * Caller should not touch the dst/src memory if this function returns error.

 start, start + len should not go beyond eb->len nor overflow */

/*

 * Check that the extent buffer is uptodate.

 *

 * For regular sector size == PAGE_SIZE case, check if @page is uptodate.

 * For subpage case, check if the range covered by the eb has EXTENT_UPTODATE.

/*

 * eb_bitmap_offset() - calculate the page and offset of the byte containing the

 * given bit number

 * @eb: the extent buffer

 * @start: offset of the bitmap item in the extent buffer

 * @nr: bit number

 * @page_index: return index of the page in the extent buffer that contains the

 * given bit number

 * @page_offset: return offset into the page given by page_index

 *

 * This helper hides the ugliness of finding the byte in an extent buffer which

 * contains a given bit.

	/*

	 * The byte we want is the offset of the extent buffer + the offset of

	 * the bitmap item in the extent buffer + the offset of the byte in the

	 * bitmap item.

/**

 * extent_buffer_test_bit - determine whether a bit in a bitmap item is set

 * @eb: the extent buffer

 * @start: offset of the bitmap item in the extent buffer

 * @nr: bit number to test

/**

 * extent_buffer_bitmap_set - set an area of a bitmap

 * @eb: the extent buffer

 * @start: offset of the bitmap item in the extent buffer

 * @pos: bit number of the first bit

 * @len: number of bits to set

/**

 * extent_buffer_bitmap_clear - clear an area of a bitmap

 * @eb: the extent buffer

 * @start: offset of the bitmap item in the extent buffer

 * @pos: bit number of the first bit

 * @len: number of bits to clear

 Already beyond page end */

 Found one */

		/*

		 * Unlike try_release_extent_buffer() which uses page->private

		 * to grab buffer, for subpage case we rely on radix tree, thus

		 * we need to ensure radix tree consistency.

		 *

		 * We also want an atomic snapshot of the radix tree, thus go

		 * with spinlock rather than RCU.

 No more eb in the page range after or at cur */

		/*

		 * The same as try_release_extent_buffer(), to ensure the eb

		 * won't disappear out from under us.

		/*

		 * If tree ref isn't set then we know the ref on this eb is a

		 * real ref, so just return, this eb will likely be freed soon

		 * anyway.

		/*

		 * Here we don't care about the return value, we will always

		 * check the page private at the end.  And

		 * release_extent_buffer() will release the refs_lock.

	/*

	 * Finally to check if we have cleared page private, as if we have

	 * released all ebs in the page, the page private should be cleared now.

	/*

	 * We need to make sure nobody is changing page->private, as we rely on

	 * page->private as the pointer to extent buffer.

	/*

	 * This is a little awful but should be ok, we need to make sure that

	 * the eb doesn't disappear out from under us while we're looking at

	 * this page.

	/*

	 * If tree ref isn't set then we know the ref on this eb is a real ref,

	 * so just return, this page will likely be freed soon anyway.

/*

 * btrfs_readahead_tree_block - attempt to readahead a child block

 * @fs_info:	the fs_info

 * @bytenr:	bytenr to read

 * @owner_root: objectid of the root that owns this eb

 * @gen:	generation for the uptodate check, can be 0

 * @level:	level for the eb

 *

 * Attempt to readahead a tree block at @bytenr.  If @gen is 0 then we do a

 * normal uptodate check of the eb, without checking the generation.  If we have

 * to read the block we will not block on anything.

/*

 * btrfs_readahead_node_child - readahead a node's child block

 * @node:	parent node we're reading from

 * @slot:	slot in the parent node for the child we want to read

 *

 * A helper for btrfs_readahead_tree_block, we simply read the bytenr pointed at

 * the slot in the node provided.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2009 Oracle.  All rights reserved.

/*

 * delayed back reference update tracking.  For subvolume trees

 * we queue up extent allocations and backref maintenance for

 * delayed processing.   This avoids deep call chains where we

 * add extents in the middle of btrfs_search_slot, and it allows

 * us to buffer up frequently modified backrefs in an rb tree instead

 * of hammering updates on the extent allocation tree.

	/*

	 * Since the global reserve is just kind of magic we don't really want

	 * to rely on it to save our bacon, so if our size is more than the

	 * delayed_refs_rsv and the global rsv then it's time to think about

	 * bailing.

/**

 * Release a ref head's reservation

 *

 * @fs_info:  the filesystem

 * @nr:       number of items to drop

 *

 * This drops the delayed ref head's count from the delayed refs rsv and frees

 * any excess reservation we had.

/*

 * btrfs_update_delayed_refs_rsv - adjust the size of the delayed refs rsv

 * @trans - the trans that may have generated delayed refs

 *

 * This is to be called anytime we may have adjusted trans->delayed_ref_updates,

 * it'll calculate the additional size and add it to the delayed_refs_rsv.

/**

 * Transfer bytes to our delayed refs rsv

 *

 * @fs_info:   the filesystem

 * @src:       source block rsv to transfer from

 * @num_bytes: number of bytes to transfer

 *

 * This transfers up to the num_bytes amount from the src rsv to the

 * delayed_refs_rsv.  Any extra bytes are returned to the space info.

/**

 * Refill based on our delayed refs usage

 *

 * @fs_info: the filesystem

 * @flush:   control how we can flush for this reservation.

 *

 * This will refill the delayed block_rsv up to 1 items size worth of space and

 * will return -ENOSPC if we can't make the reservation.

/*

 * compare two delayed tree backrefs with same bytenr and type

/*

 * compare two delayed data backrefs with same bytenr and type

 insert a new ref to head ref rbtree */

/*

 * Find a head entry based on bytenr. This returns the delayed ref head if it

 * was able to find one, or NULL if nothing was in that spot.  If return_bigger

 * is given, the next bigger entry is returned if no exact match is found.

			/*

			 * Can't have multiples of the same ref on a tree block.

 We don't have too many refs to merge for data. */

/*

 * Helper to insert the ref_node to the tail or merge with tail.

 *

 * Return 0 for insert.

 * Return >0 for merge.

 Now we are sure we can merge */

 Need to change action */

 remove existing tail if its ref_mod is zero */

/*

 * helper function to update the accounting in the head ref

 * existing and update must have the same bytenr

		/* if the extent was freed and then

		 * reallocated before the delayed ref

		 * entries were processed, we can end up

		 * with an existing head ref without

		 * the must_insert_reserved flag set.

		 * Set it again here

		/*

		 * update the num_bytes so we make sure the accounting

		 * is done correctly

	/*

	 * update the reference mod on the head to reflect this new operation,

	 * only need the lock for this case cause we could be processing it

	 * currently, for refs we just added we know we're a-ok.

	/*

	 * If we are going to from a positive ref mod to a negative or vice

	 * versa we need to make sure to adjust pending_csums accordingly.

 If reserved is provided, it must be a data extent. */

	/*

	 * The head node stores the sum of all the mods, so dropping a ref

	 * should drop the sum in the head node by one.

	/*

	 * BTRFS_ADD_DELAYED_EXTENT means that we need to update the reserved

	 * accounting when the extent is finally added, or if a later

	 * modification deletes the delayed ref without ever inserting the

	 * extent into the extent allocation tree.  ref->must_insert_reserved

	 * is the flag used to record that accounting mods are required.

	 *

	 * Once we record must_insert_reserved, switch the action to

	 * BTRFS_ADD_DELAYED_REF because other special casing is not required.

/*

 * helper function to actually insert a head node into the rbtree.

 * this does all the dirty work in terms of maintaining the correct

 * overall modification count.

 Record qgroup extent info if provided */

		/*

		 * we've updated the existing ref, free the newly

		 * allocated ref

/*

 * init_delayed_ref_common - Initialize the structure which represents a

 *			     modification to a an extent.

 *

 * @fs_info:    Internal to the mounted filesystem mount structure.

 *

 * @ref:	The structure which is going to be initialized.

 *

 * @bytenr:	The logical address of the extent for which a modification is

 *		going to be recorded.

 *

 * @num_bytes:  Size of the extent whose modification is being recorded.

 *

 * @ref_root:	The id of the root where this modification has originated, this

 *		can be either one of the well-known metadata trees or the

 *		subvolume id which references this extent.

 *

 * @action:	Can be one of BTRFS_ADD_DELAYED_REF/BTRFS_DROP_DELAYED_REF or

 *		BTRFS_ADD_DELAYED_EXTENT

 *

 * @ref_type:	Holds the type of the extent which is being recorded, can be

 *		one of BTRFS_SHARED_BLOCK_REF_KEY/BTRFS_TREE_BLOCK_REF_KEY

 *		when recording a metadata extent or BTRFS_SHARED_DATA_REF_KEY/

 *		BTRFS_EXTENT_DATA_REF_KEY when recording data extent

/*

 * add a delayed tree ref.  This does all of the accounting required

 * to make sure the delayed ref is eventually processed before this

 * transaction commits.

	/*

	 * insert both the head node and the new ref without dropping

	 * the spin lock

	/*

	 * Need to update the delayed_refs_rsv with any changes we may have

	 * made.

/*

 * add a delayed data ref. it's similar to btrfs_add_delayed_tree_ref.

	/*

	 * insert both the head node and the new ref without dropping

	 * the spin lock

	/*

	 * Need to update the delayed_refs_rsv with any changes we may have

	 * made.

	/*

	 * Need to update the delayed_refs_rsv with any changes we may have

	 * made.

/*

 * This does a simple search for the head node for a given extent.  Returns the

 * head node if found, or NULL if not.

 SPDX-License-Identifier: GPL-2.0

 Maximum number of zones to report per blkdev_report_zones() call */

 Invalid allocation pointer value for missing devices */

 Pseudo write pointer value for conventional zone */

/*

 * Location of the first zone of superblock logging zone pairs.

 *

 * - primary superblock:    0B (zone 0)

 * - first copy:          512G (zone starting at that offset)

 * - second copy:           4T (zone starting at that offset)

 Number of superblock log zones */

/*

 * Minimum of active zones we need:

 *

 * - BTRFS_SUPER_MIRROR_MAX zones for superblock mirrors

 * - 3 zones to ensure at least one zone per SYSTEM, META and DATA block group

 * - 1 zone for tree-log dedicated block group

 * - 1 zone for relocation

/*

 * Maximum supported zone size. Currently, SMR disks have a zone size of

 * 256MiB, and we are expecting ZNS drives to be in the 1-4GiB range. We do not

 * expect the zone size to become larger than 8GiB in the near future.

	/*

	 * Possible states of log buffer zones

	 *

	 *           Empty[0]  In use[0]  Full[0]

	 * Empty[1]         *          x        0

	 * In use[1]        0          x        0

	 * Full[1]          1          1        C

	 *

	 * Log position:

	 *   *: Special case, no superblock is written

	 *   0: Use write pointer of zones[0]

	 *   1: Use write pointer of zones[1]

	 *   C: Compare super blocks from zones[0] and zones[1], use the latest

	 *      one determined by generation

	 *   x: Invalid state

 Special case to distinguish no superblock to read */

 Compare two super blocks */

/*

 * Get the first zone number of the superblock mirror

/*

 * Emulate blkdev_report_zones() for a non-zoned device. It slices up the block

 * device into static sized chunks and fake a conventional zone on each of

 * them.

 The emulated zone size is determined from the size of device extent */

 No dev extents at all? Not good */

 fs_info->zone_size might not set yet. Use the incomapt flag here. */

 We can skip reading of zone info for missing devices */

	/*

	 * Cannot use btrfs_is_zoned here, since fs_info::zone_size might not

	 * yet be set.

 Check if it's power of 2 (see is_power_of_2) */

 We reject devices with a zone size larger than 8GB */

 Get zones type */

 Validate superblock log */

		/*

		 * If zones[0] is conventional, always use the beginning of the

		 * zone to record superblock. No need to validate in that case.

 Just in case */

 Count zoned devices */

		/*

		 * A Host-Managed zoned device must be used as a zoned device.

		 * A Host-Aware zoned device and a non-zoned devices can be

		 * treated as a zoned device, if ZONED flag is enabled in the

		 * superblock.

 No zoned block device found on ZONED filesystem */

	/*

	 * stripe_size is always aligned to BTRFS_STRIPE_LEN in

	 * btrfs_create_chunk(). Since we want stripe_len == zone_size,

	 * check the alignment here.

	/*

	 * Check mount options here, because we might change fs_info->zoned

	 * from fs_info->zone_size.

	/*

	 * Space cache writing is not COWed. Disable that to avoid write errors

	 * in sequential zones.

		/*

		 * For READ, we want the previous one. Move write pointer to

		 * the end of a zone, if it is at the head of a zone.

	/*

	 * For a zoned filesystem on a non-zoned block device, use the same

	 * super block locations as regular filesystem. Doing so, the super

	 * block can always be retrieved and the zoned flag of the volume

	 * detected from the super block information.

 Advance the next zone */

			/*

			 * No room left to write new superblock. Since

			 * superblock is written with REQ_SYNC, it is safe to

			 * finish the zone now.

			 *

			 * If the write pointer is exactly at the capacity,

			 * explicit ZONE_FINISH is not necessary.

 All the zones are FULL. Should not reach here. */

/**

 * btrfs_find_allocatable_zones - find allocatable zones within a given region

 *

 * @device:	the device to allocate a region on

 * @hole_start: the position of the hole to allocate the region

 * @num_bytes:	size of wanted region

 * @hole_end:	the end of the hole

 * @return:	position of allocatable zones

 *

 * Allocatable region should not contain any superblock locations.

 Check if zones in the region are all empty */

 We also need to exclude regular superblock positions */

 We can use any number of zones */

 Active zone left? */

 Someone already set the bit */

 We can use any number of zones */

 All the zones are conventional */

 All the zones are sequential and empty */

 Free regions should be empty */

/*

 * Calculate an allocation pointer from the extent allocation information

 * for a block group consist of conventional zones. It is pointed to the

 * end of the highest addressed extent in the block group as an allocation

 * offset.

 We should not find the exact match */

 Sanity check */

 Get the chunk mapping */

		/*

		 * This zone will be used for allocation, so mark this zone

		 * non-empty.

		/*

		 * The group is mapped to a sequential zone. Get the zone write

		 * pointer to determine the allocation offset within the zone.

 Partially used zone */

		/*

		 * Consider a zone as active if we can allow any number of

		 * active zones.

		/*

		 * Avoid calling calculate_alloc_pointer() for new BG. It

		 * is no use for new BG. It must be always 0.

		 *

		 * Also, we have a lock chain of extent buffer lock ->

		 * chunk mutex.  For new BG, this function is called from

		 * btrfs_make_block_group() which is already taking the

		 * chunk mutex. Thus, we cannot call

		 * calculate_alloc_pointer() which takes extent buffer

		 * locks to avoid deadlock.

 Zone capacity is always zone size in emulation */

 single */

 non-single profiles are not supported yet */

 An extent is allocated after the write pointer */

 We only need ->free_space in ALLOC_SEQ block groups */

	/*

	 * Using REQ_OP_ZONE_APPNED for relocation can break assumptions on the

	 * extent layout the relocation code has.

	 * Furthermore we have set aside own block-group from which only the

	 * relocation "process" can allocate and make sure only one process at a

	 * time can add pages to an extent that gets relocated, so it's safe to

	 * use regular REQ_OP_WRITE for this special case.

 Zoned devices should not have partitions. So, we can assume it is 0 */

 Missing device */

 Failing device */

/*

 * Synchronize write pointer in a zone at @physical_start on @tgt_dev, by

 * filling zeros between @physical_pos to a write pointer of dev-replace

 * source device.

 We only support single profile for now */

/**

 * Activate block group and underlying device zones

 *

 * @block_group: the block group to activate

 *

 * Return: true on success, false otherwise

 Currently support SINGLE profile only */

 No space left */

 Cannot activate the zone */

 Successfully activated all the zones */

 For the active block group list */

 Currently support SINGLE profile only */

 Check if we have unwritten allocated space */

 Ensure all writes in this block group finish */

 No need to wait for NOCOW writers. Zoned mode does not allow that. */

	/*

	 * Bail out if someone already deactivated the block group, or

	 * allocated space is left in the block group.

 For active_bg_list */

 Non-single profiles are not supported yet */

 Check if there is a device with active zones left */

 We should have consumed all the free space */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2015 Facebook.  All rights reserved.

 Flip it to the other format and check that for good measure. */

	/*

	 * Align some operations to a page to flush out bugs in the extent

	 * buffer bitmap handling of highmem.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2013 Fusion IO.  All rights reserved.

/*

 * Build the most complicated map of extents the earth has ever seen.  We want

 * this so we can test all of the corner cases of btrfs_get_extent.  Here is a

 * diagram of how the extents will look though this may not be possible we still

 * want to make sure everything acts normally (the last number is not inclusive)

 *

 * [0 - 5][5 -  6][     6 - 4096     ][ 4096 - 4100][4100 - 8195][8195 - 12291]

 * [hole ][inline][hole but no extent][  hole   ][   regular ][regular1 split]

 *

 * [12291 - 16387][16387 - 24579][24579 - 28675][ 28675 - 32771][32771 - 36867 ]

 * [    hole    ][regular1 split][   prealloc ][   prealloc1  ][prealloc1 written]

 *

 * [36867 - 45059][45059 - 53251][53251 - 57347][57347 - 61443][61443- 69635]

 * [  prealloc1  ][ compressed  ][ compressed1 ][    regular  ][ compressed1]

 *

 * [69635-73731][   73731 - 86019   ][86019-90115]

 * [  regular  ][ hole but no extent][  regular  ]

 First we want a hole */

	/*

	 * Now we want an inline extent, I don't think this is possible but hey

	 * why not?  Also keep in mind if we have an inline extent it counts as

	 * the whole first page.  If we were to expand it we would have to cow

	 * and we wouldn't have an inline extent anymore.

 Now another hole */

 Now for a regular extent */

	/*

	 * Now for 3 extents that were split from a hole punch so we test

	 * offsets properly.

 Now for a unwritten prealloc extent */

	/*

	 * We want to jack up disk_bytenr a little more so the em stuff doesn't

	 * merge our records.

	/*

	 * Now for a partially written prealloc extent, basically the same as

	 * the hole punch example above.  Ram_bytes never changes when you mark

	 * extents written btw.

 Now a normal compressed extent */

 No merges */

 Now a split compressed extent */

 Now extents that have a hole but no hole extent */

 First with no extents */

	/*

	 * All of the magic numbers are based on the mapping setup in

	 * setup_file_extents, so if you change anything there you need to

	 * update the comment and update the expected values below.

	/*

	 * We don't test anything else for inline since it doesn't get set

	 * unless we have a page for it to write into.  Maybe we should change

	 * this?

 Regular extent */

 The next 3 are split extents */

 Prealloc extent */

 The next 3 are a half written prealloc extent */

 Now for the compressed extent */

 Split compressed extent */

 A hole between regular extents but no hole extent */

	/*

	 * Currently we just return a length that we requested rather than the

	 * length of the actual hole, if this changes we'll have to change this

	 * test.

	/*

	 * Need a blank inode item here just so we don't confuse

	 * btrfs_get_extent.

 [BTRFS_MAX_EXTENT_SIZE] */

 [BTRFS_MAX_EXTENT_SIZE][sectorsize] */

 [BTRFS_MAX_EXTENT_SIZE/2][sectorsize HOLE][the rest] */

 [BTRFS_MAX_EXTENT_SIZE][sectorsize] */

	/*

	 * [BTRFS_MAX_EXTENT_SIZE+sectorsize][sectorsize HOLE][BTRFS_MAX_EXTENT_SIZE+sectorsize]

	/*

	* [BTRFS_MAX_EXTENT_SIZE+sectorsize][sectorsize][BTRFS_MAX_EXTENT_SIZE+sectorsize]

 [BTRFS_MAX_EXTENT_SIZE+4k][4K HOLE][BTRFS_MAX_EXTENT_SIZE+4k] */

	/*

	 * Refill the hole again just for good measure, because I thought it

	 * might fail and I'd rather satisfy my paranoia at this point.

 Empty */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2013 Fusion IO.  All rights reserved.

/*

 * This test just does basic sanity checking, making sure we can add an extent

 * entry and remove space from either end and the middle, and make sure we can

 * remove space that covers adjacent extent entries.

 First just make sure we can remove an entire entry */

 Ok edge and middle cases now */

 Cleanup */

	/*

	 * The first bitmap we have starts at offset 0 so the next one is just

	 * at the end of the first bitmap.

 Test a bit straddling two bitmaps */

 This is the high grade jackassery */

	/*

	 * First let's do something simple, an extent at the same offset as the

	 * bitmap, but the free space completely in the extent and then

	 * completely in the bitmap.

 Now to add back the extent entry and remove from the bitmap */

	/*

	 * Ok so a little more evil, extent entry and bitmap at the same offset,

	 * removing an overlapping chunk.

 Now with the extent entry offset into the bitmap */

	/*

	 * This has blown up in the past, the extent entry starts before the

	 * bitmap entry, but we're trying to remove an offset that falls

	 * completely within the bitmap range and is in both the extent entry

	 * and the bitmap entry, looks like this

	 *

	 *   [ extent ]

	 *      [ bitmap ]

	 *        [ del ]

	/*

	 * This blew up before, we have part of the free space in a bitmap and

	 * then the entirety of the rest of the space in an extent.  This used

	 * to return -EAGAIN back from btrfs_remove_extent, make sure this

	 * doesn't happen.

 Used by test_steal_space_from_bitmap_to_extent(). */

 Used by test_steal_space_from_bitmap_to_extent(). */

 Used by test_steal_space_from_bitmap_to_extent(). */

	/*

	 * Now lets confirm that there's absolutely no free space left to

	 * allocate.

 And any allocation request, no matter how small, should fail now. */

 And no extent nor bitmap entries in the cache anymore. */

/*

 * Before we were able to steal free space from a bitmap entry to an extent

 * entry, we could end up with 2 entries representing a contiguous free space.

 * One would be an extent entry and the other a bitmap entry. Since in order

 * to allocate space to a caller we use only 1 entry, we couldn't return that

 * whole range to the caller if it was requested. This forced the caller to

 * either assume ENOSPC or perform several smaller space allocations, which

 * wasn't optimal as they could be spread all over the block group while under

 * concurrency (extra overhead and fragmentation).

 *

 * This stealing approach is beneficial, since we always prefer to allocate

 * from extent entries, both for clustered and non-clustered allocation

 * requests.

	/*

	 * For this test, we want to ensure we end up with an extent entry

	 * immediately adjacent to a bitmap entry, where the bitmap starts

	 * at an offset where the extent entry ends. We keep adding and

	 * removing free space to reach into this state, but to get there

	 * we need to reach a point where marking new free space doesn't

	 * result in adding new extent entries or merging the new space

	 * with existing extent entries - the space ends up being marked

	 * in an existing bitmap that covers the new free space range.

	 *

	 * To get there, we need to reach the threshold defined set at

	 * cache->free_space_ctl->extents_thresh, which currently is

	 * 256 extents on a x86_64 system at least, and a few other

	 * conditions (check free_space_cache.c). Instead of making the

	 * test much longer and complicated, use a "use_bitmap" operation

	 * that forces use of bitmaps as soon as we have at least 1

	 * extent entry.

	/*

	 * Extent entry covering free space range [128Mb - 256Kb, 128Mb - 128Kb[

 Bitmap entry covering free space range [128Mb + 512Kb, 256Mb[ */

	/*

	 * Now make only the first 256Kb of the bitmap marked as free, so that

	 * we end up with only the following ranges marked as free space:

	 *

	 * [128Mb - 256Kb, 128Mb - 128Kb[

	 * [128Mb + 512Kb, 128Mb + 768Kb[

 Confirm that only those 2 ranges are marked as free. */

	/*

	 * Confirm that the bitmap range [128Mb + 768Kb, 256Mb[ isn't marked

	 * as free anymore.

	/*

	 * Confirm that the region [128Mb + 256Kb, 128Mb + 512Kb[, which is

	 * covered by the bitmap, isn't marked as free.

	/*

	 * Confirm that the region [128Mb, 128Mb + 256Kb[, which is covered

	 * by the bitmap too, isn't marked as free either.

	/*

	 * Now lets mark the region [128Mb, 128Mb + 512Kb[ as free too. But,

	 * lets make sure the free space cache marks it as free in the bitmap,

	 * and doesn't insert a new extent entry to represent this region.

 Confirm the region is marked as free. */

	/*

	 * Confirm that no new extent entries or bitmap entries were added to

	 * the cache after adding that free space region.

	/*

	 * Now lets add a small free space region to the right of the previous

	 * one, which is not contiguous with it and is part of the bitmap too.

	 * The goal is to test that the bitmap entry space stealing doesn't

	 * steal this space region.

	/*

	 * Confirm that no new extent entries or bitmap entries were added to

	 * the cache after adding that free space region.

	/*

	 * Now mark the region [128Mb - 128Kb, 128Mb[ as free too. This will

	 * expand the range covered by the existing extent entry that represents

	 * the free space [128Mb - 256Kb, 128Mb - 128Kb[.

 Confirm the region is marked as free. */

	/*

	 * Confirm that our extent entry didn't stole all free space from the

	 * bitmap, because of the small 4Kb free space region.

	/*

	 * So now we have the range [128Mb - 256Kb, 128Mb + 768Kb[ as free

	 * space. Without stealing bitmap free space into extent entry space,

	 * we would have all this free space represented by 2 entries in the

	 * cache:

	 *

	 * extent entry covering range: [128Mb - 256Kb, 128Mb[

	 * bitmap entry covering range: [128Mb, 128Mb + 768Kb[

	 *

	 * Attempting to allocate the whole free space (1Mb) would fail, because

	 * we can't allocate from multiple entries.

	 * With the bitmap free space stealing, we get a single extent entry

	 * that represents the 1Mb free space, and therefore we're able to

	 * allocate the whole free space at once.

	/*

	 * All that remains is a sectorsize free space region in a bitmap.

	 * Confirm.

	/*

	 * Now test a similar scenario, but where our extent entry is located

	 * to the right of the bitmap entry, so that we can check that stealing

	 * space from a bitmap to the front of an extent entry works.

	/*

	 * Extent entry covering free space range [128Mb + 128Kb, 128Mb + 256Kb[

 Bitmap entry covering free space range [0, 128Mb - 512Kb[ */

	/*

	 * Now make only the last 256Kb of the bitmap marked as free, so that

	 * we end up with only the following ranges marked as free space:

	 *

	 * [128Mb + 128b, 128Mb + 256Kb[

	 * [128Mb - 768Kb, 128Mb - 512Kb[

 Confirm that only those 2 ranges are marked as free. */

	/*

	 * Confirm that the bitmap range [0, 128Mb - 768Kb[ isn't marked

	 * as free anymore.

	/*

	 * Confirm that the region [128Mb - 512Kb, 128Mb[, which is

	 * covered by the bitmap, isn't marked as free.

	/*

	 * Now lets mark the region [128Mb - 512Kb, 128Mb[ as free too. But,

	 * lets make sure the free space cache marks it as free in the bitmap,

	 * and doesn't insert a new extent entry to represent this region.

 Confirm the region is marked as free. */

	/*

	 * Confirm that no new extent entries or bitmap entries were added to

	 * the cache after adding that free space region.

	/*

	 * Now lets add a small free space region to the left of the previous

	 * one, which is not contiguous with it and is part of the bitmap too.

	 * The goal is to test that the bitmap entry space stealing doesn't

	 * steal this space region.

	/*

	 * Now mark the region [128Mb, 128Mb + 128Kb[ as free too. This will

	 * expand the range covered by the existing extent entry that represents

	 * the free space [128Mb + 128Kb, 128Mb + 256Kb[.

 Confirm the region is marked as free. */

	/*

	 * Confirm that our extent entry didn't stole all free space from the

	 * bitmap, because of the small 2 * sectorsize free space region.

	/*

	 * So now we have the range [128Mb - 768Kb, 128Mb + 256Kb[ as free

	 * space. Without stealing bitmap free space into extent entry space,

	 * we would have all this free space represented by 2 entries in the

	 * cache:

	 *

	 * extent entry covering range: [128Mb, 128Mb + 256Kb[

	 * bitmap entry covering range: [128Mb - 768Kb, 128Mb[

	 *

	 * Attempting to allocate the whole free space (1Mb) would fail, because

	 * we can't allocate from multiple entries.

	 * With the bitmap free space stealing, we get a single extent entry

	 * that represents the 1Mb free space, and therefore we're able to

	 * allocate the whole free space at once.

	/*

	 * All that remains is 2 * sectorsize free space region

	 * in a bitmap. Confirm.

	/*

	 * For ppc64 (with 64k page size), bytes per bitmap might be

	 * larger than 1G.  To make bitmap test available in ppc64,

	 * alloc dummy block group whose size cross bitmaps.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2013 Fusion IO.  All rights reserved.

 Shouldn't happen but that kind of thinking creates CVE's */

 Will be freed by btrfs_free_fs_roots */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2017 Oracle.  All rights reserved.

/*

 * Test scenario:

 *

 * Suppose that no extent map has been loaded into memory yet, there is a file

 * extent [0, 16K), followed by another file extent [16K, 20K), two dio reads

 * are entering btrfs_get_extent() concurrently, t1 is reading [8K, 16K), t2 is

 * reading [0, 8K)

 *

 *     t1                            t2

 *  btrfs_get_extent()              btrfs_get_extent()

 *    -> lookup_extent_mapping()      ->lookup_extent_mapping()

 *    -> add_extent_mapping(0, 16K)

 *    -> return em

 *                                    ->add_extent_mapping(0, 16K)

 *                                    -> #handle -EEXIST

 Add [0, 16K) */

 Add [16K, 20K) following [0, 16K)  */

 avoid merging */

 Add [0, 8K), should return [0, 16K) instead. */

/*

 * Test scenario:

 *

 * Reading the inline ending up with EEXIST, ie. read an inline

 * extent and discard page cache and read it again.

 Add [0, 1K) */

 Add [4K, 8K) following [0, 1K)  */

 Add [0, 1K) */

 Add [4K, 8K) */

 Add [0, 16K) */

	/*

	 * Since bytes within em are contiguous, em->block_start is identical to

	 * em->start.

/*

 * Test scenario:

 *

 * Suppose that no extent map has been loaded into memory yet.

 * There is a file extent [0, 16K), two jobs are running concurrently

 * against it, t1 is buffered writing to [4K, 8K) and t2 is doing dio

 * read from [0, 4K) or [8K, 12K) or [12K, 16K).

 *

 * t1 goes ahead of t2 and adds em [4K, 8K) into tree.

 *

 *         t1                       t2

 *  cow_file_range()	     btrfs_get_extent()

 *                            -> lookup_extent_mapping()

 *   -> add_extent_mapping()

 *                            -> add_extent_mapping()

 Add [0K, 8K) */

 Add [8K, 32K) */

 avoid merging */

 Add [0K, 32K) */

/*

 * Test scenario:

 *

 * Suppose that no extent map has been loaded into memory yet.

 * There is a file extent [0, 32K), two jobs are running concurrently

 * against it, t1 is doing dio write to [8K, 32K) and t2 is doing dio

 * read from [0, 4K) or [4K, 8K).

 *

 * t1 goes ahead of t2 and splits em [0, 32K) to em [0K, 8K) and [8K 32K).

 *

 *         t1                                t2

 *  btrfs_get_blocks_direct()	       btrfs_get_blocks_direct()

 *   -> btrfs_get_extent()              -> btrfs_get_extent()

 *       -> lookup_extent_mapping()

 *       -> add_extent_mapping()            -> lookup_extent_mapping()

 *          # load [0, 32K)

 *   -> btrfs_new_extent_direct()

 *       -> btrfs_drop_extent_cache()

 *          # split [0, 32K)

 *       -> add_extent_mapping()

 *          # add [8K, 32K)

 *                                          -> add_extent_mapping()

 *                                             # handle -EEXIST when adding

 *                                             # [0, 32K)

 Assume we won't have more than 5 physical stripes */

 Physical to logical addresses */

 Start at 4GiB logical address */

 For us */

 For the tree */

			/*

			 * Test a chunk with 2 data stripes one of which

			 * intersects the physical address of the super block

			 * is correctly recognised.

			/*

			 * Test that out-of-range physical addresses are

			 * ignored

 SINGLE chunk type */

	/*

	 * Note: the fs_info is not set up completely, we only need

	 * fs_info::fsid for the tracepoint.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2013 Fusion IO.  All rights reserved.

 In this test we need at least 2 file extents at its maximum size */

	/*

	 * Passing NULL as we don't have fs_info but tracepoints are not used

	 * at this point

	/*

	 * First go through and create and mark all of our pages dirty, we pin

	 * everything to make sure our pages don't get evicted and screw up our

	 * test.

	/* Test this scenario

	 * |--- delalloc ---|

	 * |---  search  ---|

	/*

	 * Test this scenario

	 *

	 * |--- delalloc ---|

	 *           |--- search ---|

 locked_page was unlocked above */

	/*

	 * Test this scenario

	 * |--- delalloc ---|

	 *                    |--- search ---|

	/*

	 * Test this scenario

	 * [------- delalloc -------|

	 * [max_bytes]|-- search--|

	 *

	 * We are re-using our test_start from above since it works out well.

	/*

	 * Now to test where we run into a page that is no longer dirty in the

	 * range we want to find.

 We unlocked it in the previous test */

	/*

	 * Currently if we fail to find dirty pages in the delalloc range we

	 * will adjust max_bytes down to PAGE_SIZE and then re-search.  If

	 * this changes at any point in the future we will need to fix this

	 * tests expected behavior.

 Straddling pages test */

	/*

	 * Generate a wonky pseudo-random bit pattern for the sake of not using

	 * something repetitive that could miss some hypothetical off-by-n bug.

	/*

	 * Test again for case where the tree block is sectorsize aligned but

	 * not nodesize aligned.

 Test correct handling of empty tree */

	/*

	 * Set 1M-4M alloc/discard and 32M-64M thus leaving a hole between

	 * 4M-32M

 Now add 32M-64M so that we have a hole between 4M-32M */

	/*

	 * Request first hole starting at 12M, we should get 4M-32M

	/*

	 * Search in the middle of allocated range, should get the next one

	 * available, which happens to be unallocated -> 4M-32M

	/*

	 * Set 64M-72M with CHUNK_ALLOC flag, then search for CHUNK_TRIMMED flag

	 * being unset in this range, we should get the entry in range 64M-72M

	/*

	 * Search in the middle of set range whose immediate neighbour doesn't

	 * have the bits set so it must be returned

	/*

	 * Search beyond any known range, shall return after last known range

	 * and end should be -1

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2013 Facebook.  All rights reserved.

	/*

	 * Since the test trans doesn't have the complicated delayed refs,

	 * we can only call btrfs_qgroup_account_extent() directly to test

	 * quota.

/*

 * Add a ref for two different roots to make sure the shared value comes out

 * right, also remove one of the roots and make sure the exclusive count is

 * adjusted properly.

	/*

	 * We have BTRFS_FS_TREE_OBJECTID created already from the

	 * previous test.

 We are using this root as our extent root */

	/*

	 * Some of the paths we test assume we have a filled out fs_info, so we

	 * just need to add the root in there so we don't panic.

	/*

	 * Can't use bytenr 0, some things freak out

	 * *cough*backref walking code*cough*

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2013 Fusion IO.  All rights reserved.

	/*

	 * Passing NULL trans here should be safe because we have plenty of

	 * space in this leaf to split the item without having to split the

	 * leaf.

	/*

	 * Read the first slot, it should have the original key and contain only

	 * 'mary had a little'

 Do it again so we test memmoving the other items in the leaf */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Novell Inc.

 * Copyright (C) 2016 Red Hat, Inc.

		/*

		 * One of the ancestor path elements in an absolute path

		 * lookup in ovl_lookup_layer() could have been opaque and

		 * that will stop further lookup in lower layers (d->stop=true)

		 * But we have found an absolute redirect in decendant path

		 * element and that should force continue lookup in lower

		 * layers (reset d->stop).

	/*

	 * A non-dir origin may be disconnected, which is fine, because

	 * we only need it for its unique inode number.

 Don't decode a deleted empty directory */

 Check if directory belongs to the layer we are decoding from */

/*

 * Check validity of an overlay file handle buffer.

 *

 * Return 0 for a valid file handle.

 * Return -ENODATA for "origin unknown".

 * Return <0 for an invalid file handle.

 Treat larger version and unknown flags as "origin unknown" */

 Treat endianness mismatch as "origin unknown" */

 Zero size value means "copied up but origin unknown" */

	/*

	 * Make sure that the stored uuid matches the uuid of the lower

	 * layer where file handle will be decoded.

	 * In case of uuid=off option just make sure that stored uuid is null.

		/*

		 * Treat stale file handle to lower file as "origin unknown".

		 * upper file handle could become stale when upper file is

		 * unlinked and this information is needed to handle stale

		 * index entries correctly.

 Recheck condition under lock */

 Don't support traversing automounts and other weirdness */

	/*

	 * This dentry should be a regular file if previous layer lookup

	 * found a metacopy dentry.

 Caught in a trap of overlapping layers */

 Counting down from the end, since the prefix can change */

 Verify we did not go off the rails */

		/*

		 * If lower fs uuid is not unique among lower fs we cannot match

		 * fh->uuid to layer.

/*

 * Verify that @fh matches the file handle stored in xattr @name.

 * Return 0 on match, -ESTALE on mismatch, < 0 on error.

/*

 * Verify that @real dentry matches the file handle stored in xattr @name.

 *

 * If @set is true and there is no stored file handle, encode @real and store

 * file handle in xattr @name.

 *

 * Return 0 on match, -ESTALE on mismatch, -ENODATA on no xattr, < 0 on error.

 Get upper dentry from index */

/*

 * Verify that an index entry name matches the origin file handle stored in

 * OVL_XATTR_ORIGIN and that origin file handle can be decoded to lower path.

 * Return 0 on match, -ESTALE on mismatch or stale origin, < 0 on error.

	/*

	 * Whiteout index entries are used as an indication that an exported

	 * overlay file handle should be treated as stale (i.e. after unlink

	 * of the overlay inode). These entries contain no origin xattr.

	/*

	 * Verifying directory index entries are not stale is expensive, so

	 * only verify stale dir index if NFS export is enabled.

	/*

	 * Directory index entries should have 'upper' xattr pointing to the

	 * real upper dir. Non-dir index entries are hardlinks to the upper

	 * real inode. For non-dir index, we can read the copy up origin xattr

	 * directly from the index dentry, but for dir index we first need to

	 * decode the upper directory.

		/*

		 * Directory index entries with no 'upper' xattr need to be

		 * removed. When dir index entry has a stale 'upper' xattr,

		 * we assume that upper dir was removed and we treat the dir

		 * index as orphan entry that needs to be whited out.

 Check if non-dir index is orphan and don't warn before cleaning it */

/*

 * Lookup in indexdir for the index entry of a lower real inode or a copy up

 * origin inode. The index entry name is the hex representation of the lower

 * inode file handle.

 *

 * If the index dentry in negative, then either no lower aliases have been

 * copied up yet, or aliases have been copied up in older kernels and are

 * not indexed.

 *

 * If the index dentry for a copy up origin inode is positive, but points

 * to an inode different than the upper inode, then either the upper inode

 * has been copied up and not indexed or it was indexed, but since then

 * index dir was cleared. Either way, that index cannot be used to indentify

 * the overlay inode.

 Lookup index by file handle for NFS export */

		/*

		 * When index lookup is called with !verify for decoding an

		 * overlay file handle, a whiteout index implies that decode

		 * should treat file handle as stale and no need to print a

		 * warning about it.

		/*

		 * Index should always be of the same file type as origin

		 * except for the case of a whiteout index. A whiteout

		 * index should only exist if all lower aliases have been

		 * unlinked, which means that finding a lower origin on lookup

		 * whose index is a whiteout should be treated as an error.

 Verify that dir index 'upper' xattr points to upper dir */

/*

 * Returns next layer in stack starting from top.

 * Returns -1 if this is the last layer.

 Fix missing 'origin' xattr */

			/*

			 * Lookup copy up origin by decoding origin file handle.

			 * We may get a disconnected dentry, which is fine,

			 * because we only need to hold the origin inode in

			 * cache and use its inode number.  We may even get a

			 * connected dentry, that is not under any of the lower

			 * layers root.  That is also fine for using it's inode

			 * number - it's the same as if we held a reference

			 * to a dentry in lower layer that was moved under us.

		/*

		 * If no origin fh is stored in upper of a merge dir, store fh

		 * of lower dir and set upper parent "impure".

		/*

		 * When "verify_lower" feature is enabled, do not merge with a

		 * lower dir that does not match a stored origin xattr. In any

		 * case, only verified origin is used for index lookup.

		 *

		 * For non-dir dentry, if index=on, then ensure origin

		 * matches the dentry found using path based lookup,

		 * otherwise error out.

			/*

			 * Do not store intermediate metacopy dentries in

			 * lower chain, except top most lower metacopy dentry.

			 * Continue the loop so that if there is an absolute

			 * redirect on this dentry, poe can be reset to roe.

		/*

		 * Following redirects can have security consequences: it's like

		 * a symlink into the lower layer without the permission checks.

		 * This is only a problem if the upper layer is untrusted (e.g

		 * comes from an USB drive).  This can allow a non-readable file

		 * or directory to become readable.

		 *

		 * Only following redirects when redirects are enabled disables

		 * this attack vector when not necessary.

 Find the current layer on the root dentry */

	/*

	 * For regular non-metacopy upper dentries, there is no lower

	 * path based lookup, hence ctr will be zero. If a dentry is found

	 * using ORIGIN xattr on upper, install it in stack.

	 *

	 * For metacopy dentry, path based lookup will find lower dentries.

	 * Just make sure a corresponding data dentry has been found.

	/*

	 * Always lookup index if there is no-upperdentry.

	 *

	 * For the case of upperdentry, we have set origin by now if it

	 * needed to be set. There are basically three cases.

	 *

	 * For directories, lookup index by lower inode and verify it matches

	 * upper inode. We only trust dir index if we verified that lower dir

	 * matches origin, otherwise dir index entries may be inconsistent

	 * and we ignore them.

	 *

	 * For regular upper, we already set origin if upper had ORIGIN

	 * xattr. There is no verification though as there is no path

	 * based dentry lookup in lower in this case.

	 *

	 * For metacopy upper, we set a verified origin already if index

	 * is enabled and if upper had an ORIGIN xattr.

	 *

	/*

	 * If dentry is negative, then lower is positive iff this is a

	 * whiteout.

 Negative upper -> positive lower */

 Positive upper -> have to look up lower to see whether it exists */

				/*

				 * Assume something is there, we just couldn't

				 * access it.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2011 Novell Inc.

 It's an overlay file */

 Handle recursion */

 Hack!  Reuse ofs->layers as a vfsmount array before freeing it */

 Sync real dirty inodes in upper filesystem (if it exists) */

	/*

	 * We have to always set the err, because the return value isn't

	 * checked in syncfs, and instead indirectly return an error via

	 * the sb's writeback errseq, which VFS inspects after this call.

	/*

	 * Not called for sync(2) call or an emergency sync (SB_I_SKIP_SYNC).

	 * All the super blocks will be iterated, including upper_sb.

	 *

	 * If this is a syncfs(2) call, then we do need to call

	 * sync_filesystem() on upper_sb, but enough if we do it when being

	 * called with wait == 1.

/**

 * ovl_statfs

 * @sb: The overlayfs super block

 * @buf: The struct kstatfs to fill in with stats

 *

 * Get the filesystem statistics.  As writes always target the upper layer

 * filesystem pass the statfs to the upper filesystem (if it exists)

 Will this overlay be forced to mount/remount ro? */

/**

 * ovl_show_options

 *

 * Prints the mount options for a given superblock.

 * Returns zero; does not fail.

		/*

		 * Does not make sense to have redirect creation without

		 * redirect following.

 Workdir/index are useless in non-upper mount */

	/*

	 * This is to make the logic below simpler.  It doesn't make any other

	 * difference, since config->redirect_dir is only used for upper.

 Resolve metacopy -> redirect_dir dependency */

			/*

			 * There was an explicit redirect_dir=... that resulted

			 * in this conflict.

 Automatically enable redirect otherwise. */

 Resolve nfs_export -> index dependency */

			/*

			 * There was an explicit index=off that resulted

			 * in this conflict.

 Automatically enable index otherwise. */

 Resolve nfs_export -> !metacopy dependency */

			/*

			 * There was an explicit metacopy=on that resulted

			 * in this conflict.

			/*

			 * There was an explicit nfs_export=on that resulted

			 * in this conflict.

 Resolve userxattr -> !redirect && !metacopy dependency */

		/*

		 * Silently disable default setting of redirect and metacopy.

		 * This shall be the default in the future as well: these

		 * options must be explicitly enabled if used together with

		 * userxattr.

 Weird filesystem returning with hashed negative (kernfs)? */

		/*

		 * Try to remove POSIX ACL xattrs from workdir.  We are good if:

		 *

		 * a) success (there was a POSIX ACL xattr and was removed)

		 * b) -ENODATA (there was no POSIX ACL xattr)

		 * c) -EOPNOTSUPP (POSIX ACL xattrs are not supported)

		 *

		 * There are various other error values that could effectively

		 * mean that the xattr doesn't exist (e.g. -ERANGE is returned

		 * if the xattr name is too long), but the set of filesystems

		 * allowed as upper are limited to "normal" ones, where checking

		 * for the above two errors is sufficient.

 Clear any inherited mode bits */

	/*

	 * The inodes index feature and NFS export need to encode and decode

	 * file handles, so they require that all layers support them.

	/*

	 * Decoding origin file handle is required for persistent st_ino.

	 * Without persistent st_ino, xino=auto falls back to xino=off.

 Check if lower fs has 32bit inode numbers */

 Workdir should not be subdir of upperdir and vice versa */

 Check that everything is OK before copy-up */

	/*

	 * Check if sgid bit needs to be cleared (actual setacl operation will

	 * be done with mounter's capabilities and so that won't do it for us).

 catch all */

/*

 * Determine how we treat concurrent use of upperdir/workdir based on the

 * index feature. This is papering over mount leaks of container runtimes,

 * for example, an old overlay mount is leaked and now its upperdir is

 * attempted to be used as a lower layer in a new overlay mount.

 Upperdir path should not be r/o */

 Don't inherit atime flags */

	/*

	 * Inherit SB_NOSEC flag from upperdir.

	 *

	 * This optimization changes behavior when a security related attribute

	 * (suid/sgid/security.*) is changed on an underlying layer.  This is

	 * okay because we don't yet have guarantees in that case, but it will

	 * need careful treatment once we want to honour changes to underlying

	 * filesystems.

/*

 * Returns 1 if RENAME_WHITEOUT is supported, 0 if not supported and

 * negative values if error is encountered.

 Name is inline and stable - using snapshot as a copy helper */

 Best effort cleanup of whiteout and temp file */

/*

 * Creates $workdir/work/incompat/volatile/dirty file if it is not already

 * present.

	/*

	 * Upper should support d_type, else whiteouts are visible.  Given

	 * workdir and upper are on same fs, we can do iterate_dir() on

	 * workdir. This check requires successful creation of workdir in

	 * previous step.

 Check if upper/work fs supports O_TMPFILE */

 Check if upper/work fs supports RENAME_WHITEOUT */

	/*

	 * Check if upper/work fs supports (trusted|user).overlay.* xattr

		/*

		 * xattr support is required for persistent st_ino.

		 * Without persistent st_ino, xino=auto falls back to xino=off.

	/*

	 * We allowed sub-optimal upper fs configuration and don't want to break

	 * users over kernel upgrade, but we never allowed remote upper fs, so

	 * we can enforce strict requirements for remote upper fs.

	/*

	 * For volatile mount, create a incompat/volatile/dirty file to keep

	 * track of it.

 Check if upper/work fs supports file handles */

 Check if upper fs has 32bit inode numbers */

 NFS export of r/w mount depends on index */

 Verify lower root is upper root origin */

 index dir will act also as workdir */

		/*

		 * Verify upper root is exclusively associated with index dir.

		 * Older kernels stored upper fh in ".overlay.origin"

		 * xattr. If that xattr exists, verify that it is a match to

		 * upper dir file handle. In any case, verify or set xattr

		 * ".overlay.upper" to indicate that index may have

		 * directory entries.

 Cleanup bad/stale/orphan index entries */

	/*

	 * We allow using single lower with null uuid for index and nfs_export

	 * for example to support those features with single lower squashfs.

	 * To avoid regressions in setups of overlay with re-formatted lower

	 * squashfs, do not allow decoding origin with lower null uuid unless

	 * user opted-in to one of the new features that require following the

	 * lower inode of non-dir upper.

		/*

		 * We use uuid to associate an overlay lower file handle with a

		 * lower layer, so we can accept lower fs with null uuid as long

		 * as all lower layers with null uuid are on the same fs.

		 * if we detect multiple lower fs with the same uuid, we

		 * disable lower file handle decoding on all of them.

 Get a unique fsid for the layer */

 idx/fsid 0 are reserved for upper fs even with lower only overlay */

	/*

	 * All lower layers that share the same fs as upper layer, use the same

	 * pseudo_dev as upper layer.  Allocate fs[0].pseudo_dev even for lower

	 * only overlay to simplify ovl_fs_free().

	 * is_lower will be set if upper fs is shared with a lower layer.

		/*

		 * Check if lower root conflicts with this overlay layers before

		 * checking if it is in-use as upperdir/workdir of "another"

		 * mount, because we do not bother to check in ovl_is_inuse() if

		 * the upperdir/workdir is in fact in-use by our

		 * upperdir/workdir.

		/*

		 * Make lower layers R/O.  That way fchmod/fchown on lower file

		 * will fail instead of modifying lower fs.

	/*

	 * When all layers on same fs, overlay can use real inode numbers.

	 * With mount option "xino=<on|auto>", mounter declares that there are

	 * enough free high bits in underlying fs to hold the unique fsid.

	 * If overlayfs does encounter underlying inodes using the high xino

	 * bits reserved for fsid, it emits a warning and uses the original

	 * inode number or a non persistent inode number allocated from a

	 * dedicated range.

		/*

		 * This is a roundup of number of bits needed for encoding

		 * fsid, where fsid 0 is reserved for upper fs (even with

		 * lower only overlay) +1 extra bit is reserved for the non

		 * persistent inode number range that is used for resolving

		 * xino lower bits overflow.

/*

 * Check if this layer root is a descendant of:

 * - another layer of this overlayfs instance

 * - upper/work dir of any overlayfs instance

 Walk back ancestors to root (inclusive) looking for traps */

/*

 * Check if any of the layers or work dirs overlap.

		/*

		 * Checking workbasedir avoids hitting ovl_is_inuse(parent) of

		 * this instance and covers overlapping work and index dirs,

		 * unless work or index dir have been moved since created inside

		 * workbasedir.  In that case, we already have their traps in

		 * inode cache and we will catch that case on lookup.

 Root inode uses upper st_ino/i_ino */

 Root is always merge -> can have whiteouts */

 Is there a reason anyone would want not to share whiteouts? */

 Layer 0 is reserved for upper even if there's no upper */

 Assume underlaying fs uses 32bit inodes unless proven otherwise */

 alloc/destroy_inode needed for setting up traps in inode cache */

 If the upper fs is nonexistent, we mark overlayfs r/o too */

 Force r/o mount with no index dir */

 Show index=off in /proc/mounts for forced r/o mount */

 Never override disk quota limits or use reserved space */

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2011 Novell Inc.

 Truncate should trigger data copy up as well */

		/*

		 * We might have to translate ovl file into real file object

		 * once use cases emerge.  For now, simply don't let underlying

		 * filesystem rely on attr->ia_file

		/*

		 * If open(O_TRUNC) is done, VFS calls ->setattr with ATTR_OPEN

		 * set.  Overlayfs does not pass O_TRUNC flag to underlying

		 * filesystem during open -> do not pass ATTR_OPEN.  This

		 * disables optimization in fuse which assumes open(O_TRUNC)

		 * already set file size to 0.  But we never passed O_TRUNC to

		 * fuse.  So by clearing ATTR_OPEN, fuse will be forced to send

		 * setattr request to server.

		/*

		 * When all layers are on the same fs, all real inode

		 * number are unique, so we use the overlay st_dev,

		 * which is friendly to du -x.

		/*

		 * All inode numbers of underlying fs should not be using the

		 * high xinobits, so we use high xinobits to partition the

		 * overlay st_ino address space. The high bits holds the fsid

		 * (upper fsid is 0). The lowest xinobit is reserved for mapping

		 * the non-persistent inode numbers range in case of overflow.

		 * This way all overlay inode numbers are unique and use the

		 * overlay st_dev.

 The inode could not be mapped to a unified st_ino address space */

		/*

		 * Always use the overlay st_dev for directories, so 'find

		 * -xdev' will scan the entire overlay mount and won't cross the

		 * overlay mount boundaries.

		 *

		 * If not all layers are on the same fs the pair {real st_ino;

		 * overlay st_dev} is not unique, so use the non persistent

		 * overlay st_ino for directories.

		/*

		 * For non-samefs setup, if we cannot map all layers st_ino

		 * to a unified address space, we need to make sure that st_dev

		 * is unique per underlying fs, so we use the unique anonymous

		 * bdev assigned to the underlying fs.

 Report the effective immutable/append-only STATX flags */

	/*

	 * For non-dir or same fs, we use st_ino of the copy up origin.

	 * This guaranties constant st_dev/st_ino across copy up.

	 * With xino feature and non-samefs, we use st_ino of the copy up

	 * origin masked with high bits that represent the layer id.

	 *

	 * If lower filesystem supports NFS file handles, this also guaranties

	 * persistent st_ino across mount cycle.

			/*

			 * Lower hardlinks may be broken on copy up to different

			 * upper files, so we cannot use the lower origin st_ino

			 * for those different files, even for the same fs case.

			 *

			 * Similarly, several redirected dirs can point to the

			 * same dir on a lower layer. With the "verify_lower"

			 * feature, we do not use the lower origin st_ino, if

			 * we haven't verified that this redirect is unique.

			 *

			 * With inodes index enabled, it is safe to use st_ino

			 * of an indexed origin. The index validates that the

			 * upper hardlink is not broken and that a redirected

			 * dir is the only redirect to that origin.

			/*

			 * If we are querying a metacopy dentry and lower

			 * dentry is data dentry, then use the blocks we

			 * queried just now. We don't have to do additional

			 * vfs_getattr(). If lower itself is metacopy, then

			 * additional vfs_getattr() is unavoidable.

			/*

			 * If lower is not same as lowerdata or if there was

			 * no origin on upper, we can end up here.

	/*

	 * It's probably not worth it to count subdirs to get the

	 * correct link count.  nlink=1 seems to pacify 'find' and

	 * other utilities.

	/*

	 * Return the overlay inode nlinks for indexed upper inodes.

	 * Overlay inode nlink counts the union of the upper hardlinks

	 * and non-covered lower hardlinks. It does not include the upper

	 * index hardlink.

 Careful in RCU walk mode */

	/*

	 * Check overlay inode with the creds of task and underlying inode

	 * with creds of mounter

 Make sure mounter can read file for copy up later */

 copy c/mtime */

 Never list private (.overlay) */

 List all non-trusted xattrs */

 list other trusted for superuser only */

 filter out private xattrs */

 underlying fs providing us with an broken xattr list? */

/*

 * Work around the fact that security_file_ioctl() takes a file argument.

 * Introducing security_inode_fileattr_get/set() hooks would solve this issue

 * properly.

		/*

		 * Store immutable/append-only flags in xattr and clear them

		 * in upper fileattr (in case they were set by older kernel)

		 * so children of "ovl-immutable" directories lower aliases of

		 * "ovl-immutable" hardlinks could be copied up.

		 * Clear xattr when flags are cleared.

		/*

		 * Merge real inode flags with inode flags read from

		 * overlay.protattr xattr

 Update ctime */

 Convert inode protection flags to fileattr flags */

 For O_DIRECT dentry_open() checks f_mapping->a_ops->direct_IO */

/*

 * It is possible to stack overlayfs instance on top of another

 * overlayfs instance as lower layer. We need to annotate the

 * stackable i_mutex locks according to stack level of the super

 * block instance. An overlayfs instance can never be in stack

 * depth 0 (there is always a real fs below it).  An overlayfs

 * inode lock will use the lockdep annotation ovl_i_mutex_key[depth].

 *

 * For example, here is a snip from /proc/lockdep_chains after

 * dir_iterate of nested overlayfs:

 *

 * [...] &ovl_i_mutex_dir_key[depth]   (stack_depth=2)

 * [...] &ovl_i_mutex_dir_key[depth]#2 (stack_depth=1)

 * [...] &type->i_mutex_dir_key        (stack_depth=0)

 *

 * Locking order w.r.t ovl_want_write() is important for nested overlayfs.

 *

 * This chain is valid:

 * - inode->i_rwsem			(inode_lock[2])

 * - upper_mnt->mnt_sb->s_writers	(ovl_want_write[0])

 * - OVL_I(inode)->lock			(ovl_inode_lock[2])

 * - OVL_I(lowerinode)->lock		(ovl_inode_lock[1])

 *

 * And this chain is valid:

 * - inode->i_rwsem			(inode_lock[2])

 * - OVL_I(inode)->lock			(ovl_inode_lock[2])

 * - lowerinode->i_rwsem		(inode_lock[1])

 * - OVL_I(lowerinode)->lock		(ovl_inode_lock[1])

 *

 * But lowerinode->i_rwsem SHOULD NOT be acquired while ovl_want_write() is

 * held, because it is in reverse order of the non-nested case using the same

 * upper fs:

 * - inode->i_rwsem			(inode_lock[1])

 * - upper_mnt->mnt_sb->s_writers	(ovl_want_write[0])

 * - OVL_I(inode)->lock			(ovl_inode_lock[1])

	/*

	 * When d_ino is consistent with st_ino (samefs or i_ino has enough

	 * bits to encode layer), set the same value used for st_ino to i_ino,

	 * so inode number exposed via /proc/locks and a like will be

	 * consistent with d_ino and st_ino values. An i_ino value inconsistent

	 * with d_ino also causes nfsd readdirplus to fail.

	/*

	 * For directory inodes on non-samefs with xino disabled or xino

	 * overflow, we allocate a non-persistent inode number, to be used for

	 * resolving st_ino collisions in ovl_map_dev_ino().

	 *

	 * To avoid ino collision with legitimate xino values from upper

	 * layer (fsid 0), use the lowest xinobit to map the non

	 * persistent inode numbers to the unified st_ino address space.

/*

 * With inodes index enabled, an overlay inode nlink counts the union of upper

 * hardlinks and non-covered lower hardlinks. During the lifetime of a non-pure

 * upper inode, the following nlink modifying operations can happen:

 *

 * 1. Lower hardlink copy up

 * 2. Upper hardlink created, unlinked or renamed over

 * 3. Lower hardlink whiteout or renamed over

 *

 * For the first, copy up case, the union nlink does not change, whether the

 * operation succeeds or fails, but the upper inode nlink may change.

 * Therefore, before copy up, we store the union nlink value relative to the

 * lower inode nlink in the index inode xattr .overlay.nlink.

 *

 * For the second, upper hardlink case, the union nlink should be incremented

 * or decremented IFF the operation succeeds, aligned with nlink change of the

 * upper inode. Therefore, before link/unlink/rename, we store the union nlink

 * value relative to the upper inode nlink in the index inode.

 *

 * For the last, lower cover up case, we simplify things by preceding the

 * whiteout or cover up with copy up. This makes sure that there is an index

 * upper inode where the nlink xattr can be stored before the copied up upper

 * entry is unlink.

/*

 * On-disk format for indexed nlink:

 *

 * nlink relative to the upper inode - "U[+-]NUM"

 * nlink relative to the lower inode - "L[+-]NUM"

	/*

	 * For directories, @strict verify from lookup path performs consistency

	 * checks, so NULL lower/upper in dentry must match NULL lower/upper in

	 * inode. Non @strict verify from NFS handle decode path passes NULL for

	 * 'unknown' lower/upper.

 Real lower dir moved to upper layer under us? */

 Lookup of an uncovered redirect origin? */

	/*

	 * Allow non-NULL lower inode in ovl_inode even if lowerdentry is NULL.

	 * This happens when finding a copied up overlay inode for a renamed

	 * or hardlinked overlay dentry and lower dentry cannot be followed

	 * by origin because lower fs does not support file handles.

	/*

	 * Allow non-NULL __upperdentry in inode even if upperdentry is NULL.

	 * This happens when finding a lower alias for a copied up hard link.

/*

 * Create an inode cache entry for layer root dir, that will intentionally

 * fail ovl_verify_inode(), so any lookup that will find some layer root

 * will fail.

 Conflicting layer roots? */

/*

 * Does overlay inode need to be hashed by lower inode?

 No, if pure upper */

 Yes, if already indexed */

 Yes, if won't be copied up */

 No, if lower hardlink is or will be broken on copy up */

 No, if non-indexed upper with NFS export */

 Otherwise, hash by lower inode for fsnotify */

	/*

	 * Copy up origin (lower) may exist for non-indexed upper, but we must

	 * not use lower as hash key if this is a broken hardlink.

			/*

			 * Verify that the underlying files stored in the inode

			 * match those in the dentry.

 Recalculate nlink for non-dir due to indexing */

 Lower hardlink that will be broken on copy up */

 Check for non-merge dir that may have whiteouts */

 Check for immutable/append-only inode flags in xattr */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2011 Novell Inc.

 counter is allowed to wrap, since temp dentries are ephemeral */

 caller holds i_mutex on workdir */

 Caller must hold i_mutex on both workdir and dir */

	/*

	 * vfs_mkdir() may succeed and leave the dentry passed

	 * to it unhashed and negative. If that happens, try to

	 * lookup a new hashed and positive dentry.

 mkdir is special... */

		/*

		 * Not quite sure if non-instantiated dentry is legal or not.

		 * VFS doesn't seem to care so check and warn here.

	/*

	 * Fail with -EIO when trying to create opaque dir and upper doesn't

	 * support xattrs. ovl_rename() calls ovl_set_opaque_xerr(-EXDEV) to

	 * return a specific error for noxattr case.

/*

 * Common operations required to be done after creation of file on upper.

 * If @hardlink is false, then @inode is a pre-allocated inode, we may or

 * may not use to instantiate the new dentry.

		/*

		 * ovl_obtain_alias() can be called after ovl_create_real()

		 * and before we get here, so we may get an inode from cache

		 * with the same real upperdentry that is not the inode we

		 * pre-allocated.  In this case we will use the cached inode

		 * to instantiate the new dentry.

		 *

		 * XXX: if we ever use ovl_obtain_alias() to decode directory

		 * file handles, need to use ovl_get_inode_locked() and

		 * d_instantiate_new() here to prevent from creating two

		 * hashed directory inode aliases.

 Force lookup of new upper hardlink to find its lower */

 Setting opaque here is just an optimization, allow to fail */

 dentry's upper doesn't match now, get rid of it */

	/*

	 * mode could have been mutilated due to umask (e.g. sgid directory)

	/*

	 * When linking a file with copy up origin into a new parent, mark the

	 * new parent dir "impure".

 Preallocate inode to be used by ovl_get_inode() */

 Did we end up using the preallocated inode? */

 Don't allow creation of "whiteout" on overlay */

	/*

	 * Keeping this dentry hashed would mean having to release

	 * upperpath/lowerpath, which could only be done if we are the

	 * sole user of this dentry.  Too tricky...  Just unhash for

	 * now.

 Try to find another, hashed alias */

	/*

	 * Changes to underlying layers may cause i_nlink to lose sync with

	 * reality.  In this case prevent the link count from going to zero

	 * prematurely.

 No need to clean pure upper removed by vfs_rmdir() */

	/*

	 * Copy ctime

	 *

	 * Note: we fail to update ctime if there was no copy-up, only a

	 * whiteout

 If path is too long, fall back to userspace move */

 Absolute redirect: finished */

	/*

	 * For non-dir hardlinked files, we need absolute redirects

	 * in general as two upper hardlinks could be in different

	 * dirs. We could put a relative redirect now and convert

	 * it to absolute redirect later. But when nlink > 1 and

	 * indexing is on, that means relative redirect needs to be

	 * converted to absolute during copy up of another lower

	 * hardllink as well.

	 *

	 * So without optimizing too much, just check if lower is

	 * a hard link or not. If lower is hard link, put absolute

	 * redirect.

 Fall back to userspace copy-up */

 Don't copy up directory trees */

 Whiteout source */

 Switch whiteouts */

		/*

		 * When moving a merge dir or non-dir with copy up origin into

		 * a new parent, we are marking the new parent dir "impure".

		 * When ovl_iterate() iterates an "impure" upper dir, it will

		 * lookup the origin inodes of the entries to fill d_ino.

 copy ctime: */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 Red Hat, Inc.

 No atime modificaton nor notify on underlying */

 Has it been copied up since we'd opened it? */

 Did the flags change since open? */

 No longer need these flags, so don't pass them on to underlying fs */

	/*

	 * The two special cases below do not need to involve real fs,

	 * so we can optimizing concurrent callers.

	/*

	 * Overlay file f_pos is the master copy that is preserved

	 * through copy up and modified on read/write, but only real

	 * fs knows how to SEEK_HOLE/SEEK_DATA and real fs may impose

	 * limitations that are more strict than ->s_maxbytes for specific

	 * files, so we use the real file to perform seeks.

 Actually acquired in ovl_write_iter() */

 Update mode */

 Update size */

 Pacify lockdep, same trick as done in aio_write() */

/*

 * Calling iter_file_splice_write() directly from overlay's f_op may deadlock

 * due to lock order inversion between pipe->mutex in iter_file_splice_write()

 * and file_start_write(real.file) in ovl_write_iter().

 *

 * So do everything ovl_write_iter() does and call iter_file_splice_write() on

 * the real file.

 Update mode */

 Update size */

 Don't sync lower file for fear of receiving EROFS error */

 Update size */

 Update size */

	/*

	 * Don't copy up because of a dedupe request, this wouldn't make sense

	 * most of the time (data would be duplicated instead of deduplicated).

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Novell Inc.

 * Copyright (C) 2016 Red Hat, Inc.

/*

 * Check if underlying fs supports file handles and try to determine encoding

 * type, in order to deduce maximum inode number used by fs.

 *

 * Return 0 if file handles are not supported.

 * Return 1 (FILEID_INO32_GEN) if fs uses the default 32bit inode encoding.

 * Return -1 if fs uses a non default encoding with unknown inode size.

 Index all files on copy up. For now only enabled for NFS export */

 Verify lower origin on lookup. For now only enabled for NFS export */

		/*

		 * Non-dir dentry can hold lower dentry of its copy up origin.

/*

 * ovl_dentry_lower() could return either a data dentry or metacopy dentry

 * depending on what is stored in lowerstack[0]. At times we need to find

 * lower dentry which has data (and not metacopy dentry). This helper

 * returns the lower data dentry.

 Return inode which contains lower data. Do not return metacopy */

 Return real inode which contains data. Does not return metacopy inode */

/*

 * For hard links and decoded file handles, it's possible for ovl_dentry_upper()

 * to return positive, while there's no actual upper alias for the inode.

 * Copy up code needs to know about the existence of the upper alias, so it

 * can't use ovl_dentry_upper().

	/*

	 * Pairs with smp_wmb() in ovl_set_upperdata(). Main user of

	 * ovl_has_upperdata() is ovl_copy_up_meta_inode_data(). Make sure

	 * if setting of OVL_UPPERDATA is visible, then effects of writes

	 * before that are visible too.

	/*

	 * Pairs with smp_rmb() in ovl_has_upperdata(). Make sure

	 * if OVL_UPPERDATA flag is visible, then effects of write operations

	 * before it are visible as well.

 Caller should hold ovl_inode->lock */

	/*

	 * Make sure upperdentry is consistent before making it visible

	/*

	 * Version is used by readdir code to keep cache consistent.

	 * For merge dirs (or dirs with origin) all changes need to be noted.

	 * For non-merge dirs, cache contains only impure entries (i.e. ones

	 * which have been copied up and have origins), so only need to note

	 * changes to impure entries.

 Copy mtime/ctime */

 O_NOATIME is an optimization, don't fail if not permitted */

 Caller should hold ovl_inode->lock */

	/*

	 * Check if copy-up has happened as well as for upper alias (in

	 * case of hard links) is there.

	 *

	 * Both checks are lockless:

	 *  - false negatives: will recheck under oi->lock

	 *  - false positives:

	 *    + ovl_dentry_upper() uses memory barriers to ensure the

	 *      upper dentry is up-to-date

	 *    + ovl_dentry_has_upper_alias() relies on locking of

	 *      upper parent i_rwsem to prevent reordering copy-up

	 *      with rename.

 Already copied up */

 Zero size value means "copied up but origin unknown" */

	/*

	 * Do not fail when upper doesn't support xattrs.

	 * Upper inodes won't have origin nor redirect xattr anyway.

 Reserved for future flags */

	/*

	 * Initialize inode flags from overlay.protattr xattr and upper inode

	 * flags.  If upper inode has those fileattr flags set (i.e. from old

	 * kernel), we do not clear them on ovl_get_inode(), but we will clear

	 * them on next fileattr_set().

	/*

	 * Do not allow to set protection flags when upper doesn't support

	 * xattrs, because we do not set those fileattr flags on upper inode.

	 * Remove xattr if it exist and all protection flags are cleared.

 Mask out the fileattr flags that should not be set in upper inode */

/**

 * Caller must hold a reference to inode to prevent it from being freed while

 * it is marked inuse.

/*

 * Does this overlay dentry need to be indexed on copy up?

 Index all files for NFS export and consistency verification */

 Index only lower hardlinks on copy up */

 Caller must hold OVL_I(inode)->lock */

		/*

		 * We either have a bug with persistent union nlink or a lower

		 * hardlink was added while overlay is mounted. Adding a lower

		 * hardlink and then unlinking all overlay hardlinks would drop

		 * overlay nlink to zero before all upper inodes are unlinked.

		 * As a safety measure, when that situation is detected, set

		 * the overlay nlink to the index inode nlink minus one for the

		 * index entry itself.

 Whiteout orphan index to block future open by handle */

 Cleanup orphan index entries */

/*

 * Operations that change overlay inode and upper inode nlink need to be

 * synchronized with copy up for persistent nlink accounting.

	/*

	 * With inodes index is enabled, we store the union overlay nlink

	 * in an xattr on the index inode. When whiting out an indexed lower,

	 * we need to decrement the overlay persistent nlink, but before the

	 * first copy up, we have no upper index inode to store the xattr.

	 *

	 * As a workaround, before whiteout/rename over an indexed lower,

	 * copy up to create the upper index. Creating the upper index will

	 * initialize the overlay nlink, so it could be dropped if unlink

	 * or rename succeeds.

	 *

	 * TODO: implement metadata only index copy up when called with

	 *       ovl_copy_up_flags(dentry, O_PATH).

	/*

	 * The overlay inode nlink should be incremented/decremented IFF the

	 * upper operation succeeds, along with nlink change of upper inode.

	 * Therefore, before link/unlink/rename, we store the union nlink

	 * value relative to the upper inode nlink in an upper inode xattr.

 Workdir should not be the same as upperdir */

 Workdir should not be subdir of upperdir and vice versa */

 err < 0, 0 if no metacopy xattr, 1 if metacopy xattr found */

 Only regular files can have metacopy xattr */

		/*

		 * getxattr on user.* may fail with EACCES in case there's no

		 * read permission on the inode.  Not much we can do, other than

		 * tell the caller that this is not a metacopy inode.

/*

 * ovl_sync_status() - Check fs sync status for volatile mounts

 *

 * Returns 1 if this is not a volatile mount and a real sync is required.

 *

 * Returns 0 if syncing can be skipped because mount is volatile, and no errors

 * have occurred on the upperdir since the mount.

 *

 * Returns -errno if it is a volatile mount, and the error that occurred since

 * the last mount. If the error code changes, it'll return the latest error

 * code.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2011 Novell Inc.

 underlying fs providing us with an broken xattr list? */

 Discard */

 Ignore failure to copy unknown xattrs */

 Ntfs-3g returns -EINVAL for "no fileattr support" */

	/*

	 * We cannot set immutable and append-only flags on upper inode,

	 * because we would not be able to link upper inode to upper dir

	 * not set overlay private xattr on upper inode.

	 * Store these flags in overlay.protattr xattr instead.

 Don't bother copying flags if none are set */

 Try to use clone_file_range to clone up within the same fs */

 Couldn't clone, so now we try to copy the data */

 Check if lower fs supports seek operation */

		/*

		 * Fill zero for hole will cost unnecessary disk space

		 * and meanwhile slow down the copy-up speed, so we do

		 * an optimization for hole during copy-up, it relies

		 * on SEEK_DATA implementation in lower fs so if lower

		 * fs does not support it, copy-up will behave as before.

		 *

		 * Detail logic of hole detection as below:

		 * When we detect next data position is larger than current

		 * position we will skip that hole, otherwise we copy

		 * data in the size of OVL_COPY_UP_CHUNK_SIZE. Actually,

		 * it may not recognize all kind of holes and sometimes

		 * only skips partial of hole area. However, it will be

		 * enough for most of the use cases.

 Make sure the real fid stays 32bit aligned */

	/*

	 * We encode a non-connectable file handle for non-dir, because we

	 * only need to find the lower inode number and we don't want to pay

	 * the price or reconnecting the dentry.

	/*

	 * When we will want to decode an overlay dentry from this handle

	 * and all layers are on the same fs, if we get a disconncted real

	 * dentry when we decode fid, the only way to tell if we should assign

	 * it to upperdentry or to lowerstack is by checking this flag.

	/*

	 * When lower layer doesn't support export operations store a 'null' fh,

	 * so we can use the overlay.origin xattr to distignuish between a copy

	 * up and a pure upper inode.

	/*

	 * Do not fail when upper doesn't support xattrs.

 Ignore -EPERM from setting "user.*" on symlink/special */

 Store file handle of @upper dir in @index dir entry */

/*

 * Create and install index entry.

 *

 * Caller must hold i_mutex on indexdir.

	/*

	 * For now this is only used for creating index entry for directories,

	 * because non-dir are copied up directly to index and then hardlinked

	 * to upper dir.

	 *

	 * TODO: implement create index for non-dir, so we can call it when

	 * encoding file handle for non-dir in case index does not exist.

 Directory not expected to be indexed before copy up */

 Mark parent "impure" because it may now contain non-pure upper */

 Restore timestamps on parent (best effort) */

	/*

	 * Copy up data first and then xattrs. Writing data after

	 * xattrs will remove security.capability xattr automatically.

		/*

		 * Copy the fileattr inode flags that are the source of already

		 * copied i_flags

	/*

	 * Store identifier of lower inode in upper inode xattr to

	 * allow lookup of the copy up origin inode.

	 *

	 * Don't set origin when we are breaking the association with a lower

	 * hard link.

/*

 * Copyup using workdir to prepare temp file.  Used when copying up directories,

 * special files or when upper fs doesn't support O_TMPFILE.

 Can't properly set mode on creation because of the umask */

 workdir and destdir could be the same when copying up to indexdir */

 Copyup using O_TMPFILE which does not require cross dir locking */

/*

 * Copy up a single dentry

 *

 * All renames start with copy up of source if necessary.  The actual

 * rename will only proceed once the copy up was successful.  Copy up uses

 * upper parent i_mutex for exclusion.  Since rename can change d_parent it

 * is possible that the copy up will lock the old parent.  At that point

 * the file will have already been copied up anyway.

	/*

	 * Indexed non-dir is copied up directly to the index entry and then

	 * hardlinked to upper dir. Indexed dir is copied up to indexdir,

	 * then index entry is created and then copied up dir installed.

	 * Copying dir up to indexdir instead of workdir simplifies locking.

 Disconnected dentry must be copied up to index dir */

		/*

		 * Mark parent "impure" because it may now contain non-pure

		 * upper

 Should we copyup with O_TMPFILE or with workdir? */

 Initialize nlink for copy up of disconnected dentry */

 Restore timestamps on parent (best effort) */

 Copy up data of an inode which was copied up metadata only in the past. */

	/*

	 * Writing to upper file will clear security.capability xattr. We

	 * don't want that to happen for normal copy-up operation.

 maybe truncate regular file. this has no effect on dirs */

 err < 0: interrupted, err > 0: raced with another copy-up */

	/*

	 * With NFS export, copy up can get called for a disconnected non-dir.

	 * In this case, we will copy up lower inode to index dir without

	 * linking it to upper dir.

 find the topmost dentry not yet copied up */

 Copy up of disconnected dentry does not set upper alias */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Overlayfs NFS export support.

 *

 * Amir Goldstein <amir73il@gmail.com>

 *

 * Copyright (C) 2017-2018 CTERA Networks. All Rights Reserved.

/*

 * Before encoding a non-upper directory file handle from real layer N, we need

 * to check if it will be possible to reconnect an overlay dentry from the real

 * lower decoded dentry. This is done by following the overlay ancestry up to a

 * "layer N connected" ancestor and verifying that all parents along the way are

 * "layer N connectable". If an ancestor that is NOT "layer N connectable" is

 * found, we need to copy up an ancestor, which is "layer N connectable", thus

 * making that ancestor "layer N connected". For example:

 *

 * layer 1: /a

 * layer 2: /a/b/c

 *

 * The overlay dentry /a is NOT "layer 2 connectable", because if dir /a is

 * copied up and renamed, upper dir /a will be indexed by lower dir /a from

 * layer 1. The dir /a from layer 2 will never be indexed, so the algorithm (*)

 * in ovl_lookup_real_ancestor() will not be able to lookup a connected overlay

 * dentry from the connected lower dentry /a/b/c.

 *

 * To avoid this problem on decode time, we need to copy up an ancestor of

 * /a/b/c, which is "layer 2 connectable", on encode time. That ancestor is

 * /a/b. After copy up (and index) of /a/b, it will become "layer 2 connected"

 * and when the time comes to decode the file handle from lower dentry /a/b/c,

 * ovl_lookup_real_ancestor() will find the indexed ancestor /a/b and decoding

 * a connected overlay dentry will be accomplished.

 *

 * (*) the algorithm in ovl_lookup_real_ancestor() can be improved to lookup an

 * entry /a in the lower layers above layer N and find the indexed dir /a from

 * layer 1. If that improvement is made, then the check for "layer N connected"

 * will need to verify there are no redirects in lower layers above N. In the

 * example above, /a will be "layer 2 connectable". However, if layer 2 dir /a

 * is a target of a layer 1 redirect, then /a will NOT be "layer 2 connectable":

 *

 * layer 1: /A (redirect = /a)

 * layer 2: /a/b/c

 Return the lowest layer for encoding a connectable file handle */

 We can get overlay root from root of any layer */

	/*

	 * If it's an unindexed merge dir, then it's not connectable with any

	 * lower layer

 We can get upper/overlay path from indexed/lower dentry */

/*

 * @dentry is "connected" if all ancestors up to root or a "connected" ancestor

 * have the same uppermost lower layer as the origin's layer. We may need to

 * copy up a "connectable" ancestor to make it "connected". A "connected" dentry

 * cannot become non "connected", so cache positive result in dentry flags.

 *

 * Return the connected origin layer or < 0 on error.

 Find the topmost origin layer connectable ancestor of @dentry */

		/*

		 * If @parent is not origin layer connectable, then copy up

		 * @next which is origin layer connectable and we are done.

 If @parent is connected or indexed we are done */

/*

 * We only need to encode origin if there is a chance that the same object was

 * encoded pre copy up and then we need to stay consistent with the same

 * encoding also after copy up. If non-pure upper is not indexed, then it was

 * copied up before NFS export was enabled. In that case we don't need to worry

 * about staying consistent with pre copy up encoding and we encode an upper

 * file handle. Overlay root dentry is a private case of non-indexed upper.

 *

 * The following table summarizes the different file handle encodings used for

 * different overlay object types:

 *

 *  Object type		| Encoding

 * --------------------------------

 *  Pure upper		| U

 *  Non-indexed upper	| U

 *  Indexed upper	| L (*)

 *  Non-upper		| L (*)

 *

 * U = upper file handle

 * L = lower file handle

 *

 * (*) Connecting an overlay dir from real lower dentry is not always

 * possible when there are redirects in lower layers and non-indexed merge dirs.

 * To mitigate those case, we may copy up the lower dir ancestor before encode

 * a lower dir file handle.

 *

 * Return 0 for upper file handle, > 0 for lower file handle or < 0 on error.

 Upper file handle for pure upper */

	/*

	 * Upper file handle for non-indexed upper.

	 *

	 * Root is never indexed, so if there's an upper layer, encode upper for

	 * root.

	/*

	 * Decoding a merge dir, whose origin's ancestor is under a redirected

	 * lower dir or under a non-indexed upper is not always possible.

	 * ovl_connect_layer() will try to make origin's layer "connected" by

	 * copying up a "connectable" ancestor.

 Lower file handle for indexed and non-upper dir/non-dir */

	/*

	 * Check if we should encode a lower or upper file handle and maybe

	 * copy up an ancestor to make lower file handle connectable.

 Encode an upper or lower file handle */

 TODO: encode connectable file handles */

/*

 * Find or instantiate an overlay dentry from real dentries and index.

 We get overlay directory dentries with ovl_lookup_real() */

 Get the upper or lower dentry in stach whose on layer @idx */

/*

 * Lookup a child overlay dentry to get a connected overlay dentry whose real

 * dentry is @real. If @real is on upper layer, we lookup a child overlay

 * dentry with the same name as the real dentry. Otherwise, we need to consult

 * index for lookup.

	/*

	 * Lookup child overlay dentry by real name. The dir mutex protects us

	 * from racing with overlay rename. If the overlay dentry that is above

	 * real has already been moved to a parent that is not under the

	 * connected overlay dir, we return -ECHILD and restart the lookup of

	 * connected real path from the top.

	/*

	 * We also need to take a snapshot of real dentry name to protect us

	 * from racing with underlying layer rename. In this case, we don't

	 * care about returning ESTALE, only from dereferencing a free name

	 * pointer because we hold no lock on the real dentry.

/*

 * Lookup an indexed or hashed overlay dentry by real inode.

	/*

	 * Decoding upper dir from index is expensive, so first try to lookup

	 * overlay dentry in inode/dcache.

	/*

	 * For decoded lower dir file handle, lookup index by origin to check

	 * if lower dir was copied up and and/or removed.

 Get connected upper overlay dir from index */

		/*

		 * ovl_lookup_real() in lower layer may call recursively once to

		 * ovl_lookup_real() in upper layer. The first level call walks

		 * back lower parents to the topmost indexed parent. The second

		 * recursive call walks back from indexed upper to the topmost

		 * connected/hashed upper parent (or up to root).

/*

 * Lookup an indexed or hashed overlay dentry, whose real dentry is an

 * ancestor of @real.

 Find the topmost indexed or hashed ancestor */

		/*

		 * Lookup a matching overlay dentry in inode/dentry

		 * cache or in index by real inode.

		/*

		 * If @real has been moved out of the layer root directory,

		 * we will eventully hit the real fs root. This cannot happen

		 * by legit overlay rename, so we return error in that case.

/*

 * Lookup a connected overlay dentry whose real dentry is @real.

 * If @real is on upper layer, we lookup a child overlay dentry with the same

 * path the real dentry. Otherwise, we need to consult index for lookup.

 Find the topmost dentry not yet connected */

			/*

			 * If real has been moved out of 'real_connected',

			 * we will not find 'real_connected' and hit the layer

			 * root. In that case, we need to restart connecting.

			 * This game can go on forever in the worst case. We

			 * may want to consider taking s_vfs_rename_mutex if

			 * this happens more than once.

			/*

			 * If real file has been moved out of the layer root

			 * directory, we will eventully hit the real fs root.

			 * This cannot happen by legit overlay rename, so we

			 * return error in that case.

			/*

			 * Lookup of child in overlay can fail when racing with

			 * overlay rename of child away from 'connected' parent.

			 * In this case, we need to restart the lookup from the

			 * top, because we cannot trust that 'real_connected' is

			 * still an ancestor of 'real'. There is a good chance

			 * that the renamed overlay ancestor is now in cache, so

			 * ovl_lookup_real_ancestor() will find it and we can

			 * continue to connect exactly from where lookup failed.

/*

 * Get an overlay dentry from upper/lower real dentries and index.

	/*

	 * Obtain a disconnected overlay dentry from a non-dir real dentry

	 * and index.

 Removed empty directory? */

	/*

	 * If real dentry is connected and hashed, get a connected overlay

	 * dentry whose real dentry is @real.

 First lookup overlay inode in inode cache by origin fh */

 Then lookup indexed upper/whiteout by origin fh */

 Then try to get a connected upper dir by index */

 Find origin.dentry again with ovl_acceptable() layer check */

 Get a connected non-upper dir or disconnected non-dir */

 If on-wire inner fid is aligned - nothing to do */

 Copy unaligned inner fh into aligned buffer */

 We may have needed to re-align OVL_FILEID_V0 */

	/*

	 * ovl_fh_to_dentry() returns connected dir overlay dentries and

	 * ovl_fh_to_parent() is not implemented, so we should not get here.

	/*

	 * ovl_fh_to_dentry() returns connected dir overlay dentries, so we

	 * should not get here.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2011 Novell Inc.

 Don't care if not doing ovl_iter() */

 Always recalc d_ino when remapping lower inode numbers */

 Always recalc d_ino for parent */

 If this is lower, then native d_ino will do */

	/*

	 * Recalc d_ino for '.' and for all entries if dir is impure (contains

	 * copied up entries)

 Defer setting d_ino for upper entry to ovl_iterate() */

 is_real can only become false when dir is copied up */

			/*

			 * Insert lowest layer entries before upper ones, this

			 * allows offsets to be reasonably constant

 Cursor is safe since the cache is stable */

 Map inode number to lower fs unique range */

	/*

	 * The lowest xinobit is reserved for mapping the non-peresistent inode

	 * numbers range, but this range is only exposed via st_ino, not here.

/*

 * Set d_ino for upper entries. Non-upper entries should always report

 * the uppermost real inode ino and should not call this function.

 *

 * When not all layer are on same fs, report real ino also for upper.

 *

 * When all layers are on the same fs, and upper has a reference to

 * copy up origin, call vfs_getattr() on the overlay entry to make

 * sure that d_ino will be consistent with st_ino from stat(2).

 we shall not be moved */

 Mark a stale entry */

		/*

		 * Directory inode is always on overlay st_dev.

		 * Non-dir with ovl_same_dev() could be on pseudo st_dev in case

		 * of xino bits overflow.

 Impure cache is not refcounted, free it here */

		/*

		 * A good opportunity to get rid of an unneeded "impure" flag.

		 * Removing the "impure" xattr is best effort.

	/*

	 * Only upper dir can be impure, but if we are in the middle of

	 * iterating a lower real dir, dir could be copied up and marked

	 * impure. We only want the impure cache if we started iterating

	 * a real upper dir to begin with.

		/*

		 * If parent is merge, then need to adjust d_ino for '..', if

		 * dir is impure then need to adjust d_ino for copied up

		 * entries.

 ovl_cache_update_ino() sets is_whiteout on stale entry */

/*

 * Like ovl_real_fdget(), returns upperfile if dir was copied up since open.

 * Unlike ovl_real_fdget(), this caches upperfile in file->private_data.

 *

 * TODO: use same abstract type for file->private_data of dir and file so

 * upperfile could also be cached for files as well.

	/*

	 * Need to check if we started out being a lower dir, but got copied up

 Nothing to sync for lower */

		/*

		 * Select whiteouts in upperdir, they should

		 * be cleared when deleting this directory.

 Even if d_type is not supported, DT_DIR is returned for . and .. */

/*

 * Returns 1 if d_type is supported, 0 not supported/unknown. Negative values

 * if error is encountered.

	/*

	 * The "work/incompat" directory is treated specially - if it is not

	 * empty, instead of printing a generic error and mounting read-only,

	 * we will error about incompat features and fail the mount.

	 *

	 * When called from ovl_indexdir_cleanup(), path->dentry->d_name.name

	 * starts with '#'.

 Cleanup leftover from index create/cleanup attempt */

 Cleanup stale index entries */

			/*

			 * Abort mount to avoid corrupting the index if

			 * an incompatible index entry was found or on out

			 * of memory.

			/*

			 * Whiteout orphan index to block future open by

			 * handle after overlay nlink dropped to zero.

 Cleanup orphan index entries */

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS ioctl operations.

 *

 * Copyright (C) 2007, 2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Koji Sato.

 capable() */

 copy_from_user(), copy_to_user() */

 compat_ptr() */

 mnt_want_write_file(), mnt_drop_write_file() */

/**

 * nilfs_ioctl_wrap_copy - wrapping function of get/set metadata info

 * @nilfs: nilfs object

 * @argv: vector of arguments from userspace

 * @dir: set of direction flags

 * @dofunc: concrete function of get/set metadata info

 *

 * Description: nilfs_ioctl_wrap_copy() gets/sets metadata info by means of

 * calling dofunc() function on the basis of @argv argument.

 *

 * Return Value: On success, 0 is returned and requested metadata info

 * is copied into userspace. On error, one of the following

 * negative error codes is returned.

 *

 * %-EINVAL - Invalid arguments from userspace.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-EFAULT - Failure during execution of requested operation.

	/*

	 * Reject pairs of a start item position (argv->v_index) and a

	 * total count (argv->v_nmembs) which leads position 'pos' to

	 * overflow by the increment at the end of the loop.

/**

 * nilfs_fileattr_get - ioctl to support lsattr

/**

 * nilfs_fileattr_set - ioctl to support chattr

/**

 * nilfs_ioctl_getversion - get info about a file's version (generation number)

/**

 * nilfs_ioctl_change_cpmode - change checkpoint mode (checkpoint/snapshot)

 * @inode: inode object

 * @filp: file object

 * @cmd: ioctl's request code

 * @argp: pointer on argument from userspace

 *

 * Description: nilfs_ioctl_change_cpmode() function changes mode of

 * given checkpoint between checkpoint and snapshot state. This ioctl

 * is used in chcp and mkcp utilities.

 *

 * Return Value: On success, 0 is returned and mode of a checkpoint is

 * changed. On error, one of the following negative error codes

 * is returned.

 *

 * %-EPERM - Operation not permitted.

 *

 * %-EFAULT - Failure during checkpoint mode changing.

 never fails */

/**

 * nilfs_ioctl_delete_checkpoint - remove checkpoint

 * @inode: inode object

 * @filp: file object

 * @cmd: ioctl's request code

 * @argp: pointer on argument from userspace

 *

 * Description: nilfs_ioctl_delete_checkpoint() function removes

 * checkpoint from NILFS2 file system. This ioctl is used in rmcp

 * utility.

 *

 * Return Value: On success, 0 is returned and a checkpoint is

 * removed. On error, one of the following negative error codes

 * is returned.

 *

 * %-EPERM - Operation not permitted.

 *

 * %-EFAULT - Failure during checkpoint removing.

 never fails */

/**

 * nilfs_ioctl_do_get_cpinfo - callback method getting info about checkpoints

 * @nilfs: nilfs object

 * @posp: pointer on array of checkpoint's numbers

 * @flags: checkpoint mode (checkpoint or snapshot)

 * @buf: buffer for storing checkponts' info

 * @size: size in bytes of one checkpoint info item in array

 * @nmembs: number of checkpoints in array (numbers and infos)

 *

 * Description: nilfs_ioctl_do_get_cpinfo() function returns info about

 * requested checkpoints. The NILFS_IOCTL_GET_CPINFO ioctl is used in

 * lscp utility and by nilfs_cleanerd daemon.

 *

 * Return value: count of nilfs_cpinfo structures in output buffer.

/**

 * nilfs_ioctl_get_cpstat - get checkpoints statistics

 * @inode: inode object

 * @filp: file object

 * @cmd: ioctl's request code

 * @argp: pointer on argument from userspace

 *

 * Description: nilfs_ioctl_get_cpstat() returns information about checkpoints.

 * The NILFS_IOCTL_GET_CPSTAT ioctl is used by lscp, rmcp utilities

 * and by nilfs_cleanerd daemon.

 *

 * Return Value: On success, 0 is returned, and checkpoints information is

 * copied into userspace pointer @argp. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-EFAULT - Failure during getting checkpoints statistics.

/**

 * nilfs_ioctl_do_get_suinfo - callback method getting segment usage info

 * @nilfs: nilfs object

 * @posp: pointer on array of segment numbers

 * @flags: *not used*

 * @buf: buffer for storing suinfo array

 * @size: size in bytes of one suinfo item in array

 * @nmembs: count of segment numbers and suinfos in array

 *

 * Description: nilfs_ioctl_do_get_suinfo() function returns segment usage

 * info about requested segments. The NILFS_IOCTL_GET_SUINFO ioctl is used

 * in lssu, nilfs_resize utilities and by nilfs_cleanerd daemon.

 *

 * Return value: count of nilfs_suinfo structures in output buffer.

/**

 * nilfs_ioctl_get_sustat - get segment usage statistics

 * @inode: inode object

 * @filp: file object

 * @cmd: ioctl's request code

 * @argp: pointer on argument from userspace

 *

 * Description: nilfs_ioctl_get_sustat() returns segment usage statistics.

 * The NILFS_IOCTL_GET_SUSTAT ioctl is used in lssu, nilfs_resize utilities

 * and by nilfs_cleanerd daemon.

 *

 * Return Value: On success, 0 is returned, and segment usage information is

 * copied into userspace pointer @argp. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-EFAULT - Failure during getting segment usage statistics.

/**

 * nilfs_ioctl_do_get_vinfo - callback method getting virtual blocks info

 * @nilfs: nilfs object

 * @posp: *not used*

 * @flags: *not used*

 * @buf: buffer for storing array of nilfs_vinfo structures

 * @size: size in bytes of one vinfo item in array

 * @nmembs: count of vinfos in array

 *

 * Description: nilfs_ioctl_do_get_vinfo() function returns information

 * on virtual block addresses. The NILFS_IOCTL_GET_VINFO ioctl is used

 * by nilfs_cleanerd daemon.

 *

 * Return value: count of nilfs_vinfo structures in output buffer.

/**

 * nilfs_ioctl_do_get_bdescs - callback method getting disk block descriptors

 * @nilfs: nilfs object

 * @posp: *not used*

 * @flags: *not used*

 * @buf: buffer for storing array of nilfs_bdesc structures

 * @size: size in bytes of one bdesc item in array

 * @nmembs: count of bdescs in array

 *

 * Description: nilfs_ioctl_do_get_bdescs() function returns information

 * about descriptors of disk block numbers. The NILFS_IOCTL_GET_BDESCS ioctl

 * is used by nilfs_cleanerd daemon.

 *

 * Return value: count of nilfs_bdescs structures in output buffer.

/**

 * nilfs_ioctl_get_bdescs - get disk block descriptors

 * @inode: inode object

 * @filp: file object

 * @cmd: ioctl's request code

 * @argp: pointer on argument from userspace

 *

 * Description: nilfs_ioctl_do_get_bdescs() function returns information

 * about descriptors of disk block numbers. The NILFS_IOCTL_GET_BDESCS ioctl

 * is used by nilfs_cleanerd daemon.

 *

 * Return Value: On success, 0 is returned, and disk block descriptors are

 * copied into userspace pointer @argp. On error, one of the following

 * negative error codes is returned.

 *

 * %-EINVAL - Invalid arguments from userspace.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-EFAULT - Failure during getting disk block descriptors.

/**

 * nilfs_ioctl_move_inode_block - prepare data/node block for moving by GC

 * @inode: inode object

 * @vdesc: descriptor of virtual block number

 * @buffers: list of moving buffers

 *

 * Description: nilfs_ioctl_move_inode_block() function registers data/node

 * buffer in the GC pagecache and submit read request.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - Requested block doesn't exist.

 *

 * %-EEXIST - Blocks conflict is detected.

/**

 * nilfs_ioctl_move_blocks - move valid inode's blocks during garbage collection

 * @sb: superblock object

 * @argv: vector of arguments from userspace

 * @buf: array of nilfs_vdesc structures

 *

 * Description: nilfs_ioctl_move_blocks() function reads valid data/node

 * blocks that garbage collector specified with the array of nilfs_vdesc

 * structures and stores them into page caches of GC inodes.

 *

 * Return Value: Number of processed nilfs_vdesc structures or

 * error code, otherwise.

			/*

			 * Add the inode to GC inode list. Garbage Collection

			 * is serialized and no two processes manipulate the

			 * list simultaneously.

 The inode still remains in GC inode list */

/**

 * nilfs_ioctl_delete_checkpoints - delete checkpoints

 * @nilfs: nilfs object

 * @argv: vector of arguments from userspace

 * @buf: array of periods of checkpoints numbers

 *

 * Description: nilfs_ioctl_delete_checkpoints() function deletes checkpoints

 * in the period from p_start to p_end, excluding p_end itself. The checkpoints

 * which have been already deleted are ignored.

 *

 * Return Value: Number of processed nilfs_period structures or

 * error code, otherwise.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-EINVAL - invalid checkpoints.

/**

 * nilfs_ioctl_free_vblocknrs - free virtual block numbers

 * @nilfs: nilfs object

 * @argv: vector of arguments from userspace

 * @buf: array of virtual block numbers

 *

 * Description: nilfs_ioctl_free_vblocknrs() function frees

 * the virtual block numbers specified by @buf and @argv->v_nmembs.

 *

 * Return Value: Number of processed virtual block numbers or

 * error code, otherwise.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - The virtual block number have not been allocated.

/**

 * nilfs_ioctl_mark_blocks_dirty - mark blocks dirty

 * @nilfs: nilfs object

 * @argv: vector of arguments from userspace

 * @buf: array of block descriptors

 *

 * Description: nilfs_ioctl_mark_blocks_dirty() function marks

 * metadata file or data blocks as dirty.

 *

 * Return Value: Number of processed block descriptors or

 * error code, otherwise.

 *

 * %-ENOMEM - Insufficient memory available.

 *

 * %-EIO - I/O error

 *

 * %-ENOENT - the specified block does not exist (hole block)

 XXX: use macro or inline func to check liveness */

 skip dead block */

		/*

		 * can safely abort because checkpoints can be removed

		 * independently.

		/*

		 * can safely abort because DAT file is updated atomically

		 * using a copy-on-write technique.

		/*

		 * can safely abort because the operation is nondestructive.

/**

 * nilfs_ioctl_clean_segments - clean segments

 * @inode: inode object

 * @filp: file object

 * @cmd: ioctl's request code

 * @argp: pointer on argument from userspace

 *

 * Description: nilfs_ioctl_clean_segments() function makes garbage

 * collection operation in the environment of requested parameters

 * from userspace. The NILFS_IOCTL_CLEAN_SEGMENTS ioctl is used by

 * nilfs_cleanerd daemon.

 *

 * Return Value: On success, 0 is returned or error code, otherwise.

	/*

	 * argv[4] points to segment numbers this ioctl cleans.  We

	 * use kmalloc() for its buffer because memory used for the

	 * segment numbers is enough small.

	/*

	 * nilfs_ioctl_move_blocks() will call nilfs_iget_for_gc(),

	 * which will operates an inode list without blocking.

	 * To protect the list from concurrent operations,

	 * nilfs_ioctl_move_blocks should be atomic operation.

/**

 * nilfs_ioctl_sync - make a checkpoint

 * @inode: inode object

 * @filp: file object

 * @cmd: ioctl's request code

 * @argp: pointer on argument from userspace

 *

 * Description: nilfs_ioctl_sync() function constructs a logical segment

 * for checkpointing.  This function guarantees that all modified data

 * and metadata are written out to the device when it successfully

 * returned.

 *

 * Return Value: On success, 0 is retured. On errors, one of the following

 * negative error code is returned.

 *

 * %-EROFS - Read only filesystem.

 *

 * %-EIO - I/O error

 *

 * %-ENOSPC - No space left on device (only in a panic state).

 *

 * %-ERESTARTSYS - Interrupted.

 *

 * %-ENOMEM - Insufficient memory available.

 *

 * %-EFAULT - Failure during execution of requested operation.

/**

 * nilfs_ioctl_resize - resize NILFS2 volume

 * @inode: inode object

 * @filp: file object

 * @argp: pointer on argument from userspace

 *

 * Return Value: On success, 0 is returned or error code, otherwise.

/**

 * nilfs_ioctl_trim_fs() - trim ioctl handle function

 * @inode: inode object

 * @argp: pointer on argument from userspace

 *

 * Description: nilfs_ioctl_trim_fs is the FITRIM ioctl handle function. It

 * checks the arguments from userspace and calls nilfs_sufile_trim_fs, which

 * performs the actual trim operation.

 *

 * Return Value: On success, 0 is returned or negative error code, otherwise.

/**

 * nilfs_ioctl_set_alloc_range - limit range of segments to be allocated

 * @inode: inode object

 * @argp: pointer on argument from userspace

 *

 * Description: nilfs_ioctl_set_alloc_range() function defines lower limit

 * of segments in bytes and upper limit of segments in bytes.

 * The NILFS_IOCTL_SET_ALLOC_RANGE is used by nilfs_resize utility.

 *

 * Return Value: On success, 0 is returned or error code, otherwise.

/**

 * nilfs_ioctl_get_info - wrapping function of get metadata info

 * @inode: inode object

 * @filp: file object

 * @cmd: ioctl's request code

 * @argp: pointer on argument from userspace

 * @membsz: size of an item in bytes

 * @dofunc: concrete function of getting metadata info

 *

 * Description: nilfs_ioctl_get_info() gets metadata info by means of

 * calling dofunc() function.

 *

 * Return Value: On success, 0 is returned and requested metadata info

 * is copied into userspace. On error, one of the following

 * negative error codes is returned.

 *

 * %-EINVAL - Invalid arguments from userspace.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-EFAULT - Failure during execution of requested operation.

/**

 * nilfs_ioctl_set_suinfo - set segment usage info

 * @inode: inode object

 * @filp: file object

 * @cmd: ioctl's request code

 * @argp: pointer on argument from userspace

 *

 * Description: Expects an array of nilfs_suinfo_update structures

 * encapsulated in nilfs_argv and updates the segment usage info

 * according to the flags in nilfs_suinfo_update.

 *

 * Return Value: On success, 0 is returned. On error, one of the

 * following negative error codes is returned.

 *

 * %-EPERM - Not enough permissions

 *

 * %-EFAULT - Error copying input data

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-EINVAL - Invalid values in input (segment number, flags or nblocks)

 never fails */

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS inode file

 *

 * Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Amagai Yoshiji.

 * Revised by Ryusuke Konishi.

 *

/**

 * struct nilfs_ifile_info - on-memory private data of ifile

 * @mi: on-memory private data of metadata file

 * @palloc_cache: persistent object allocator cache of ifile

/**

 * nilfs_ifile_create_inode - create a new disk inode

 * @ifile: ifile inode

 * @out_ino: pointer to a variable to store inode number

 * @out_bh: buffer_head contains newly allocated disk inode

 *

 * Return Value: On success, 0 is returned and the newly allocated inode

 * number is stored in the place pointed by @ino, and buffer_head pointer

 * that contains newly allocated disk inode structure is stored in the

 * place pointed by @out_bh

 * On error, one of the following negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOSPC - No inode left.

	req.pr_entry_nr = 0;  /*

			       * 0 says find free inode from beginning

			       * of a group. dull code!!

/**

 * nilfs_ifile_delete_inode - delete a disk inode

 * @ifile: ifile inode

 * @ino: inode number

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - The inode number @ino have not been allocated.

/**

 * nilfs_ifile_count_free_inodes - calculate free inodes count

 * @ifile: ifile inode

 * @nmaxinodes: current maximum of available inodes count [out]

 * @nfreeinodes: free inodes count [out]

/**

 * nilfs_ifile_read - read or get ifile inode

 * @sb: super block instance

 * @root: root object

 * @inode_size: size of an inode

 * @raw_inode: on-disk ifile inode

 * @inodep: buffer to store the inode

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS pathname lookup operations.

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Modified for NILFS by Amagai Yoshiji and Ryusuke Konishi.

/*

 *  linux/fs/ext2/namei.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/namei.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

/*

 * Methods themselves.

/*

 * By the time this is called, we already have created

 * the directory cache entry for the new file, but it

 * is so far negative - it has no inode.

 *

 * If the create succeeds, we fill in the inode information

 * with d_instantiate().

 slow symlink */

 mark_inode_dirty(inode); */

 page_symlink() do this */

	/*

	 * Like most other Unix systems, set the ctime for inodes on a

	 * rename.

/*

 * Export operations

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS module and super block management.

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Ryusuke Konishi.

/*

 *  linux/fs/ext2/super.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/inode.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

 nilfs_sufile_resize(), nilfs_sufile_set_alloc_range() */

/**

 * __nilfs_error() - report failure condition on a filesystem

 *

 * __nilfs_error() sets an ERROR_FS flag on the superblock as well as

 * reporting an error message.  This function should be called when

 * NILFS detects incoherences or defects of meta data on disk.

 *

 * This implements the body of nilfs_error() macro.  Normally,

 * nilfs_error() should be used.  As for sustainable errors such as a

 * single-shot I/O error, nilfs_err() should be used instead.

 *

 * Callers should not add a trailing newline since this will do it.

			/*

			 * sbp[0] points to newer log than sbp[1],

			 * so copy sbp[0] to sbp[1] to take over sbp[0].

		/*

		 * The latest segment becomes trailable from the position

		 * written in superblock.

 update GC protection for recent segments */

 nilfs->ns_sem must be locked by the caller. */

 nilfs->ns_sem must be locked by the caller. */

 nilfs->ns_sem must be locked by the caller. */

 make sure store to ns_flushed_device cannot be reordered */

/**

 * nilfs_cleanup_super() - write filesystem state for cleanup

 * @sb: super block instance to be unmounted or degraded to read-only

 *

 * This function restores state flags in the on-disk super block.

 * This will set "clean" flag (i.e. NILFS_VALID_FS) unless the

 * filesystem was not clean previously.

			/*

			 * make the "clean" flag also to the opposite

			 * super block if both super blocks point to

			 * the same checkpoint.

/**

 * nilfs_move_2nd_super - relocate secondary super block

 * @sb: super block instance

 * @sb2off: new offset of the secondary super block (in bytes)

 array index of the secondary superblock */

 nilfs->ns_sem must be locked by the caller. */

 super block location is unchanged */

 Get new super block buffer */

 secondary super block will be restored to index 1 */

/**

 * nilfs_resize_fs - resize the filesystem

 * @sb: super block instance

 * @newsize: new size of the filesystem (in bytes)

	/*

	 * Write lock is required to protect some functions depending

	 * on the number of segments, the number of reserved segments,

	 * and so forth.

		/*

		 * Drop NILFS_RESIZE_FS flag for compatibility with

		 * mount-time resize which may be implemented in a

		 * future release.

	/*

	 * Reset the range of allocatable segments last.  This order

	 * is important in the case of expansion because the secondary

	 * superblock must be protected from log write until migration

	 * completes.

 This function is called when super block should be written back */

 already attached checkpoint */

 Mark super block clean */

	/*

	 * Compute all of the segment blocks

	 *

	 * The blocks before first segment and after last segment

	 * are excluded.

	/*

	 * Compute the overhead

	 *

	 * When distributing meta data blocks outside segment structure,

	 * We must count them as the overhead.

			/*

			 * If nilfs_palloc_count_max_entries() returns

			 * -ERANGE error code then we simply treat

			 * curent inodes count as maximum possible and

			 * zero as free inodes value.

 Ordered data semantics */

 Strict in-order semantics */

 nilfs->ns_sem must be locked by the caller. */

 synchronize sbp[1] with sbp[0] */

 FS independent flags */

/**

 * nilfs_tree_is_busy() - try to shrink dentries of a checkpoint

 * @root_dentry: root dentry of the tree to be shrunk

 *

 * This function returns true if the tree was in-use.

 protect recent checkpoints */

/**

 * nilfs_fill_super() - initialize a super block instance

 * @sb: super_block

 * @data: mount options

 * @silent: silent mode flag

 *

 * This function is called exclusively by nilfs->ns_mount_mutex.

 * So, the recovery process is protected from other simultaneous mounts.

 Shutting down log writer */

		/*

		 * Remounting a valid RW partition RDONLY, so set

		 * the RDONLY flag and then mark the partition as valid again.

		/*

		 * Mounting a RDONLY partition read-write, so reread and

		 * store the current valid flag.  (It may have been changed

		 * by fsck since we originally mounted the partition.)

/**

 * nilfs_identify - pre-read mount options needed to identify mount instance

 * @data: mount options

 * @sd: nilfs_super_data

	/*

	 * once the super is inserted into the list by sget, s_umount

	 * will protect the lockfs code from trying to start a snapshot

	 * while we are mounting

 New superblock instance created */

			/*

			 * Try remount to setup mount states if the current

			 * tree is not mounted and only snapshots use this sb.

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS inode operations.

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Ryusuke Konishi.

 *

/**

 * struct nilfs_iget_args - arguments used during comparison between inodes

 * @ino: inode number

 * @cno: checkpoint number

 * @root: pointer on NILFS root object (mounted checkpoint)

 * @for_gc: inode for GC flag

/**

 * nilfs_get_block() - get a file block on the filesystem (callback function)

 * @inode - inode struct of the target file

 * @blkoff - file block number

 * @bh_result - buffer head to be mapped on

 * @create - indicate whether allocating the block or not when it has not

 *      been allocated yet.

 *

 * This function does not issue actual read request of the specified data

 * block. It is done by VFS.

 found */

 data block was not found */

				/*

				 * The get_block() function could be called

				 * from multiple callers for an inode.

				 * However, the page having this block must

				 * be locked in this case.

 never fails */

 Error handling should be detailed */

 Disk block number must be changed to proper value */

		/*

		 * not found is not error (e.g. hole); must return without

		 * the mapped state flag.

/**

 * nilfs_readpage() - implement readpage() method of nilfs_aops {}

 * address_space_operations.

 * @file - file struct of the file to be read

 * @page - the page to be read

		/*

		 * It means that filesystem was remounted in read-only

		 * mode because of error or metadata corruption. But we

		 * have dirty pages that try to be flushed in background.

		 * So, here we simply discard this dirty page.

		/*

		 * This page is locked by callers, and no other thread

		 * concurrently marks its buffers dirty since they are

		 * only dirtied through routines in fs/buffer.c in

		 * which call sites of mark_buffer_dirty are protected

		 * by page lock.

 Do not mark hole blocks dirty */

 Needs synchronization with the cleaner */

 .releasepage		= nilfs_releasepage, */

 reference count of i_bh inherits from nilfs_mdt_read_block() */

 No lock is needed; iget() ensures it. */

 ii->i_file_acl = 0; */

 ii->i_dir_acl = 0; */

		/*

		 * Never occur.  When supporting nilfs_init_acl(),

		 * proper cancellation of above jobs should be considered.

	iput(inode);  /*

		       * raw_inode will be deleted through

		       * nilfs_evict_inode().

 this inode is deleted */

 No lock is needed; iget() ensures it. */

 zero-fill unused portion in the case of super root block */

	/*

	 * When extending inode, nilfs->ns_inode_size should be checked

	 * for substitutions of appended fields.

		/*

		 * XXX: call with has_bmap = 0 is a workaround to avoid

		 * deadlock of bmap.  This delays update of i_bmap to just

		 * before writing.

 64MB for 4KB block */

 never fails */

	/*

	 * May construct a logical segment and may fail in sync mode.

	 * But truncate has no return value.

	/*

	 * Free resources allocated in nilfs_read_inode(), here.

 never fails */

 TODO: some of the following operations may fail.  */

	/*

	 * May construct a logical segment and may fail in sync mode.

	 * But delete_inode has no return value.

 snapshot is not writable */

		/*

		 * Because this routine may race with nilfs_dispose_list(),

		 * we have to check NILFS_I_QUEUED here, too.

			/*

			 * This will happen when somebody is freeing

			 * this inode.

			return -EINVAL; /*

					 * NILFS_I_DIRTY may remain for

					 * freeing inode.

/**

 * nilfs_dirty_inode - reflect changes on given inode to an inode block.

 * @inode: inode of the file to be registered.

 *

 * nilfs_dirty_inode() loads a inode block containing the specified

 * @inode and copies data from a nilfs_inode to a corresponding inode

 * entry in the inode block. This operation is excluded from the segment

 * construction. This function can be called both as a single operation

 * and as a part of indivisible file operations.

 never fails */

 End of the current extent */

		/*

		 * Limit the number of blocks that we look up so as

		 * not to get into the next delayed allocation extent.

 error */

 HOLE */

 End of the current extent */

 The current extent goes on */

 Terminate the current extent */

 Start another extent */

 Start a new extent */

 If ret is 1 then we just hit the end of the extent array */

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS segment constructor.

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Ryusuke Konishi.

 *

/*

 * Segment constructor

 Size of locally allocated inode vector */

#define SC_MAX_SEGDELTA 64   /*

			      * Upper limit of the number of segments

			      * appended in collection retry loop

 Construction mode */

 Make a logical segment having a super root */

	SC_LSEG_DSYNC,	/*

			 * Flush data blocks of a given file and make

			 * a logical segment without a super root.

	SC_FLUSH_FILE,	/*

			 * Flush data files, leads to segment writes without

			 * creating a checkpoint.

	SC_FLUSH_DAT,	/*

			 * Flush DAT file.  This also creates segments

			 * without a checkpoint.

 Stage numbers of dirty block collection */

 Collecting dirty blocks for GC */

 Super root */

 Data sync blocks */

/*

 * nilfs_sc_cstage_inc(), nilfs_sc_cstage_set(), nilfs_sc_cstage_get() are

 * wrapper functions of stage count (nilfs_sc_info->sc_stage.scnt). Users of

 * the variable must use them because transition of stage count must involve

 * trace events (trace_nilfs2_collection_stage_transition).

 *

 * nilfs_sc_cstage_get() isn't required for the above purpose because it doesn't

 * produce tracepoint events. It is provided just for making the intention

 * clear.

 State flags of collection */

 Collecting node blocks */

 IFILE stage has started */

 segment usages has been freed */

 Operations depending on the construction mode and file type */

/*

 * Other definitions

		/*

		 * If journal_info field is occupied by other FS,

		 * it is saved and will be restored on

		 * nilfs_transaction_commit().

/**

 * nilfs_transaction_begin - start indivisible file operations.

 * @sb: super block

 * @ti: nilfs_transaction_info

 * @vacancy_check: flags for vacancy rate checks

 *

 * nilfs_transaction_begin() acquires a reader/writer semaphore, called

 * the segment semaphore, to make a segment construction and write tasks

 * exclusive.  The function is used with nilfs_transaction_commit() in pairs.

 * The region enclosed by these two functions can be nested.  To avoid a

 * deadlock, the semaphore is only acquired or released in the outermost call.

 *

 * This function allocates a nilfs_transaction_info struct to keep context

 * information on it.  It is initialized and hooked onto the current task in

 * the outermost call.  If a pre-allocated struct is given to @ti, it is used

 * instead; otherwise a new struct is assigned from a slab.

 *

 * When @vacancy_check flag is set, this function will check the amount of

 * free space, and will wait for the GC to reclaim disk space if low capacity.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error code is returned.

 *

 * %-ENOMEM - Insufficient memory available.

 *

 * %-ENOSPC - No space left on device

/**

 * nilfs_transaction_commit - commit indivisible file operations.

 * @sb: super block

 *

 * nilfs_transaction_commit() releases the read semaphore which is

 * acquired by nilfs_transaction_begin(). This is only performed

 * in outermost call of this function.  If a commit flag is set,

 * nilfs_transaction_commit() sets a timer to start the segment

 * constructor.  If a sync flag is set, it starts construction

 * directly.

/**

 * nilfs_segctor_reset_segment_buffer - reset the current segment buffer

 * @sci: nilfs_sc_info

		return -E2BIG; /*

				* The current segment is filled up

				* (internal code)

/*

 * Functions for making segment summary and payloads

 Size of finfo and binfo is enough small against blocksize */

 skip finfo */

 Substitution to vblocknr is delayed until update_blocknr() */

/*

 * Callback functions that enumerate, mark, and collect dirty blocks

		/*

		 * A valid range is given for sync-ing data pages. The

		 * range is rounded to per-page; extra dirty buffers

		 * may be included if blocksize < pagesize.

 XXX: this interface will be changed */

		/*

		 * The following code is duplicated with cpfile.  But, it is

		 * needed to collect the checkpoint even if it was not newly

		 * created.

 Remaining number of blocks within segment buffer */

 always receive -E2BIG or true error */

 dispose node list */

 Collect node */

 always receive -E2BIG or true error if n > rest */

 Pre-processes */

 sci->sc_stage.dirty_file_ptr = NILFS_I(inode); */

 XXX: required ? */

 Creating a checkpoint */

 Appending a super root */

 End of a logical segment */

/**

 * nilfs_segctor_begin_construction - setup segment buffer to make a new log

 * @sci: nilfs_sc_info

 * @nilfs: nilfs object

 Start from the head of a new full segment */

 Continue logs */

	/*

	 * Since the segment specified with nextnum might be allocated during

	 * the previous construction, the buffer including its segusage may

	 * not be dirty.  The following call ensures that the buffer is dirty

	 * and will pin the buffer on memory until the sufile is written.

 extend segment info */

 map this buffer to region of segment on-disk */

 allocate the next next full segment */

 never fails */

 never fails */

 Case 1: The first segment failed */

			/*

			 * Case 1a:  Partial segment appended into an existing

			 * segment

 Case 1b:  New full segment */

 never fails */

 Case 2: extended segment (!= next) failed */

 always succeed because the segusage is dirty */

 always succeed because the segusage is dirty */

 always succeed */

 Collection retry loop */

 The current segment is filled up */

 do not happen */

 The caller must release old_bh */

 file blocks */

		/*

		 * For split b-tree node pages, this function may be called

		 * twice.  We ignore the 2nd or later calls by this check.

		/*

		 * For b-tree node pages, this function may be called twice

		 * or more because they might be split in a segment.

			/*

			 * For pages holding split b-tree node buffers, dirty

			 * flag on the buffers may be cleared discretely.

			 * In that case, the page is once redirtied for

			 * remaining buffers, and it must be cancelled if

			 * all the buffers get cleaned later.

 do not happen */

		/*

		 * We assume that the buffers which belong to the same page

		 * continue over the buffer list.

		 * Under this assumption, the last BHs of pages is

		 * identifiable by the discontinuity of bh->b_page

		 * (page != fs_page).

		 *

		 * For B-tree node blocks, however, this assumption is not

		 * guaranteed.  The cleanup code of B-tree node pages needs

		 * special care.

	/*

	 * Since pages may continue over multiple segment buffers,

	 * end of the last page must be checked outside of the loop.

 Always redirty the buffer to avoid race condition

			/*

			 * Defer calling iput() to avoid deadlocks if

			 * i_nlink == 0 or mount is not yet finished.

/*

 * Main procedure of segment constructor

 Update time stamp */

 Avoid empty segment */

 Write partial segments */

			/*

			 * At this point, we avoid double buffering

			 * for blocksize < pagesize because page dirty

			 * flag is turned off during write and dirty

			 * buffers are not properly collected for

			 * pages crossing over segments.

/**

 * nilfs_segctor_start_timer - set timer of background write

 * @sci: nilfs_sc_info

 *

 * If the timer has already been set, it ignores the new request.

 * This function MUST be called within a section locking the segment

 * semaphore.

/**

 * nilfs_flush_segment - trigger a segment construction for resource control

 * @sb: super block

 * @ino: inode number of the file to be flushed out.

 assign bit 0 to data files */

/**

 * nilfs_construct_segment - construct a logical segment

 * @sb: super block

 *

 * Return Value: On success, 0 is returned. On errors, one of the following

 * negative error code is returned.

 *

 * %-EROFS - Read only filesystem.

 *

 * %-EIO - I/O error

 *

 * %-ENOSPC - No space left on device (only in a panic state).

 *

 * %-ERESTARTSYS - Interrupted.

 *

 * %-ENOMEM - Insufficient memory available.

 A call inside transactions causes a deadlock. */

/**

 * nilfs_construct_dsync_segment - construct a data-only logical segment

 * @sb: super block

 * @inode: inode whose data blocks should be written out

 * @start: start byte offset

 * @end: end byte offset (inclusive)

 *

 * Return Value: On success, 0 is returned. On errors, one of the following

 * negative error code is returned.

 *

 * %-EROFS - Read only filesystem.

 *

 * %-EIO - I/O error

 *

 * %-ENOSPC - No space left on device (only in a panic state).

 *

 * %-ERESTARTSYS - Interrupted.

 *

 * %-ENOMEM - Insufficient memory available.

 data file only */

 DAT only */

/**

 * nilfs_segctor_accept - record accepted sequence count of log-write requests

 * @sci: segment constructor object

/**

 * nilfs_segctor_notify - notify the result of request to caller threads

 * @sci: segment constructor object

 * @mode: mode of log forming

 * @err: error code to be notified

 Clear requests (even when the construction failed) */

 re-enable timer if checkpoint creation was not done */

/**

 * nilfs_segctor_construct - form logs and write them to disk

 * @sci: segment constructor object

 * @mode: mode of log forming

	/*

	 * Unclosed segment should be retried.  We do this using sc_timer.

	 * Timeout of sc_timer will invoke complete construction which leads

	 * to close the current logical segment.

/**

 * nilfs_segctor_thread - main loop of the segment constructor thread.

 * @arg: pointer to a struct nilfs_sc_info.

 *

 * nilfs_segctor_thread() initializes a timer and serves as a daemon

 * to execute segment constructions.

 start sync. */

 for nilfs_segctor_start_thread() */

 end sync. */

 for nilfs_segctor_kill_thread() */

/*

 * Setup & clean-up functions

	/*

	 * The segctord thread was stopped and its timer was removed.

	 * But some tasks remain.

/**

 * nilfs_segctor_destroy - destroy the segment constructor.

 * @sci: nilfs_sc_info

 *

 * nilfs_segctor_destroy() kills the segctord thread and frees

 * the nilfs_sc_info struct.

 * Caller must hold the segment semaphore.

/**

 * nilfs_attach_log_writer - attach log writer

 * @sb: super block instance

 * @root: root object of the current filesystem tree

 *

 * This allocates a log writer object, initializes it, and starts the

 * log writer.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error code is returned.

 *

 * %-ENOMEM - Insufficient memory available.

		/*

		 * This happens if the filesystem was remounted

		 * read/write after nilfs_error degenerated it into a

		 * read-only mount.

/**

 * nilfs_detach_log_writer - destroy log writer

 * @sb: super block instance

 *

 * This kills log writer daemon, frees the log writer object, and

 * destroys list of dirty files.

 Force to free the list of dirty files */

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS directory entry operations

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Modified for NILFS by Amagai Yoshiji.

/*

 *  linux/fs/ext2/dir.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/dir.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  ext2 directory handling functions

 *

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

 *

 * All code that works with directory layout had been switched to pagecache

 * and moved here. AV

/*

 * nilfs uses block-sized chunks. Arguably, sector-sized ones would be

 * more robust, but we have what we have

/*

 * Return the offset into page `page_nr' of the last valid

 * byte in that page, plus one.

 do not happen */

 Too bad, we had an error */

/*

 * NOTE! unlike strncmp, nilfs_match returns 1 for success, 0 for failure.

 *

 * len <= NILFS_NAME_LEN and de != NULL are guaranteed by caller.

/*

 * p is at least 6 bytes before the end of page

/*

 *	nilfs_find_entry()

 *

 * finds an entry in the specified directory with the wanted name. It

 * returns the page in which the entry was found, and the entry itself

 * (as a parameter - res_dir). Page is returned mapped and unlocked.

 * Entry is guaranteed to be valid.

 OFFSET_CACHE */

 next page is past the blocks we've got */

 Releases the page */

/*

 *	Parent is locked.

	/*

	 * We take care of directory expansion in the same loop.

	 * This code plays outside i_size, so it locks the page

	 * to protect that region.

 We hit i_size */

 OFFSET_CACHE */

/*

 * nilfs_delete_entry deletes a directory entry by merging it with the

 * previous entry. Page is up-to-date. Releases the page.

/*

 * Set the first fragment of directory.

/*

 * routine to check that the specified directory is empty (for rmdir)

 check for . and .. */

 CONFIG_COMPAT */

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS segment usage file.

 *

 * Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Koji Sato.

 * Revised by Ryusuke Konishi.

/**

 * struct nilfs_sufile_info - on-memory private data of sufile

 * @mi: on-memory private data of metadata file

 * @ncleansegs: number of clean segments

 * @allocmin: lower limit of allocatable segment range

 * @allocmax: upper limit of allocatable segment range

 number of clean segments */

 lower limit of allocatable segment range */

 upper limit of allocatable segment range */

/**

 * nilfs_sufile_get_ncleansegs - return the number of clean segments

 * @sufile: inode of segment usage file

/**

 * nilfs_sufile_updatev - modify multiple segment usages at a time

 * @sufile: inode of segment usage file

 * @segnumv: array of segment numbers

 * @nsegs: size of @segnumv array

 * @create: creation flag

 * @ndone: place to store number of modified segments on @segnumv

 * @dofunc: primitive operation for the update

 *

 * Description: nilfs_sufile_updatev() repeatedly calls @dofunc

 * against the given array of segments.  The @dofunc is called with

 * buffers of a header block and the sufile block in which the target

 * segment usage entry is contained.  If @ndone is given, the number

 * of successfully modified segments from the head is stored in the

 * place @ndone points to.

 *

 * Return Value: On success, zero is returned.  On error, one of the

 * following negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - Given segment usage is in hole block (may be returned if

 *            @create is zero)

 *

 * %-EINVAL - Invalid segment usage number

 get different block */

/**

 * nilfs_sufile_set_alloc_range - limit range of segment to be allocated

 * @sufile: inode of segment usage file

 * @start: minimum segment number of allocatable region (inclusive)

 * @end: maximum segment number of allocatable region (inclusive)

 *

 * Return Value: On success, 0 is returned.  On error, one of the

 * following negative error codes is returned.

 *

 * %-ERANGE - invalid segment region

/**

 * nilfs_sufile_alloc - allocate a segment

 * @sufile: inode of segment usage file

 * @segnump: pointer to segment number

 *

 * Description: nilfs_sufile_alloc() allocates a clean segment.

 *

 * Return Value: On success, 0 is returned and the segment number of the

 * allocated segment is stored in the place pointed by @segnump. On error, one

 * of the following negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOSPC - No clean segment left.

				/*

				 * wrap around in the limited region.

				 * if allocation started from

				 * sui->allocmin, this never happens.

 never happens */

 found a clean segment */

 no segments left */

 make the segment garbage */

/**

 * nilfs_sufile_mark_dirty - mark the buffer having a segment usage dirty

 * @sufile: inode of segment usage file

 * @segnum: segment number

/**

 * nilfs_sufile_set_segment_usage - set usage of a segment

 * @sufile: inode of segment usage file

 * @segnum: segment number

 * @nblocks: number of live blocks in the segment

 * @modtime: modification time (option)

/**

 * nilfs_sufile_get_stat - get segment usage statistics

 * @sufile: inode of segment usage file

 * @sustat: pointer to a structure of segment usage statistics

 *

 * Description: nilfs_sufile_get_stat() returns information about segment

 * usage.

 *

 * Return Value: On success, 0 is returned, and segment usage information is

 * stored in the place pointed by @sustat. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

/**

 * nilfs_sufile_truncate_range - truncate range of segment array

 * @sufile: inode of segment usage file

 * @start: start segment number (inclusive)

 * @end: end segment number (inclusive)

 *

 * Return Value: On success, 0 is returned.  On error, one of the

 * following negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-EINVAL - Invalid number of segments specified

 *

 * %-EBUSY - Dirty or active segments are present in the range

 hole */

 make hole */

/**

 * nilfs_sufile_resize - resize segment array

 * @sufile: inode of segment usage file

 * @newnsegs: new number of segments

 *

 * Return Value: On success, 0 is returned.  On error, one of the

 * following negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOSPC - Enough free space is not left for shrinking

 *

 * %-EBUSY - Dirty or active segments exist in the region to be truncated

 newnsegs < nsegs */ {

/**

 * nilfs_sufile_get_suinfo -

 * @sufile: inode of segment usage file

 * @segnum: segment number to start looking

 * @buf: array of suinfo

 * @sisz: byte size of suinfo

 * @nsi: size of suinfo array

 *

 * Description:

 *

 * Return Value: On success, 0 is returned and .... On error, one of the

 * following negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 hole */

/**

 * nilfs_sufile_set_suinfo - sets segment usage info

 * @sufile: inode of segment usage file

 * @buf: array of suinfo_update

 * @supsz: byte size of suinfo_update

 * @nsup: size of suinfo_update array

 *

 * Description: Takes an array of nilfs_suinfo_update structs and updates

 * segment usage accordingly. Only the fields indicated by the sup_flags

 * are updated.

 *

 * Return Value: On success, 0 is returned. On error, one of the

 * following negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-EINVAL - Invalid values in input (segment number, flags or nblocks)

			/*

			 * Active flag is a virtual flag projected by running

			 * nilfs kernel code - drop it not to write it to

			 * disk.

 get different block */

/**

 * nilfs_sufile_trim_fs() - trim ioctl handle function

 * @sufile: inode of segment usage file

 * @range: fstrim_range structure

 *

 * start:	First Byte to trim

 * len:		number of Bytes to trim from start

 * minlen:	minimum extent length in Bytes

 *

 * Decription: nilfs_sufile_trim_fs goes through all segments containing bytes

 * from start to start+len. start is rounded up to the next block boundary

 * and start+len is rounded down. For each clean segment blkdev_issue_discard

 * function is invoked.

 *

 * Return Value: On success, 0 is returned or negative error code, otherwise.

	/*

	 * range->len can be very large (actually, it is set to

	 * ULLONG_MAX by default) - truncate upper end of the range

	 * carefully so as not to overflow.

 hole */

 start new extent */

 add to previous extent */

 discard previous extent */

 start new extent */

 discard last extent */

/**

 * nilfs_sufile_read - read or get sufile inode

 * @sb: super block instance

 * @susize: size of a segment usage entry

 * @raw_inode: on-disk sufile inode

 * @inodep: buffer to store the inode

 SPDX-License-Identifier: GPL-2.0+

/*

 * Buffer/page management specific to NILFS

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Ryusuke Konishi and Seiji Kihara.

/**

 * nilfs_forget_buffer - discard dirty state

 * @bh: buffer head of the buffer to be discarded

/**

 * nilfs_copy_buffer -- copy buffer data and flags

 * @dbh: destination buffer

 * @sbh: source buffer

/**

 * nilfs_page_buffers_clean - check if a page has dirty buffers or not.

 * @page: page to be checked

 *

 * nilfs_page_buffers_clean() returns zero if the page has dirty buffers.

 * Otherwise, it returns non-zero value.

/**

 * nilfs_copy_page -- copy the page with buffers

 * @dst: destination page

 * @src: source page

 * @copy_dirty: flag whether to copy dirty states on the page's buffer heads.

 *

 * This function is for both data pages and btnode pages.  The dirty flag

 * should be treated by caller.  The page must not be under i/o.

 * Both src and dst page must be locked

 No empty page is added to the page cache */

/**

 * nilfs_copy_back_pages -- copy back pages to original cache from shadow cache

 * @dmap: destination page cache

 * @smap: source page cache

 *

 * No pages must be added to the cache during this process.

 * This must be ensured by the caller.

 overwrite existing page in the destination cache */

 Do we not need to remove page from smap here? */

 move the page to the destination cache */

 Probably -ENOMEM */

/**

 * nilfs_clear_dirty_pages - discard dirty pages in address space

 * @mapping: address space with dirty pages for discarding

 * @silent: suppress [true] or print [false] warning messages

/**

 * nilfs_clear_dirty_page - discard dirty page

 * @page: dirty page that will be discarded

 * @silent: suppress [true] or print [false] warning messages

/*

 * NILFS2 needs clear_page_dirty() in the following two cases:

 *

 * 1) For B-tree node pages and data pages of the dat/gcdat, NILFS2 clears

 *    page dirty flags when it copies back pages from the shadow cache

 *    (gcdat->{i_mapping,i_btnode_cache}) to its original cache

 *    (dat->{i_mapping,i_btnode_cache}).

 *

 * 2) Some B-tree operations like insertion or deletion may dispose buffers

 *    in dirty state, and this needs to cancel the dirty state of their pages.

/**

 * nilfs_find_uncommitted_extent - find extent of uncommitted data

 * @inode: inode

 * @start_blk: start block offset (in)

 * @blkoff: start offset of the found extent (out)

 *

 * This function searches an extent of buffers marked "delayed" which

 * starts from a block offset equal to or larger than @start_blk.  If

 * such an extent was found, this will store the start offset in

 * @blkoff and return its length in blocks.  Otherwise, zero is

 * returned.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Dummy inodes to buffer blocks for garbage collection

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Seiji Kihara, Amagai Yoshiji, and Ryusuke Konishi.

 * Revised by Ryusuke Konishi.

 *

/*

 * This file adds the cache of on-disk blocks to be moved in garbage

 * collection.  The disk blocks are held with dummy inodes (called

 * gcinodes), and this file provides lookup function of the dummy

 * inodes and their buffer read function.

 *

 * Buffers and pages held by the dummy inodes will be released each

 * time after they are copied to a new log.  Dirty blocks made on the

 * current generation and the blocks to be moved by GC never overlap

 * because the dirty blocks make a new generation; they rather must be

 * written individually.

/*

 * nilfs_gccache_submit_read_data() - add data buffer and submit read request

 * @inode - gc inode

 * @blkoff - dummy offset treated as the key for the page cache

 * @pbn - physical block number of the block

 * @vbn - virtual block number of the block, 0 for non-virtual block

 * @out_bh - indirect pointer to a buffer_head struct to receive the results

 *

 * Description: nilfs_gccache_submit_read_data() registers the data buffer

 * specified by @pbn to the GC pagecache with the key @blkoff.

 * This function sets @vbn (@pbn if @vbn is zero) in b_blocknr of the buffer.

 *

 * Return Value: On success, 0 is returned. On Error, one of the following

 * negative error code is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - The block specified with @pbn does not exist.

 -EIO, -ENOMEM, -ENOENT */

/*

 * nilfs_gccache_submit_read_node() - add node buffer and submit read request

 * @inode - gc inode

 * @pbn - physical block number for the block

 * @vbn - virtual block number for the block

 * @out_bh - indirect pointer to a buffer_head struct to receive the results

 *

 * Description: nilfs_gccache_submit_read_node() registers the node buffer

 * specified by @vbn to the GC pagecache.  @pbn can be supplied by the

 * caller to avoid translation of the disk block address.

 *

 * Return Value: On success, 0 is returned. On Error, one of the following

 * negative error code is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 internal code (cache hit) */

/**

 * nilfs_remove_all_gcinodes() - remove all unprocessed gc inodes

 SPDX-License-Identifier: GPL-2.0+

/*

 * Meta data file for NILFS

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Ryusuke Konishi.

 nilfs_palloc_destroy_cache() */

 Caller exclude read accesses using page lock */

 set_buffer_new(bh); */

 internal code */

 mode == READ */

 internal code */

 abort readahead if bmap lookup failed */

/**

 * nilfs_mdt_get_block - read or create a buffer on meta data file.

 * @inode: inode of the meta data file

 * @blkoff: block offset

 * @create: create flag

 * @init_block: initializer used for newly allocated block

 * @out_bh: output of a pointer to the buffer_head

 *

 * nilfs_mdt_get_block() looks up the specified buffer and tries to create

 * a new buffer if @create is not zero.  On success, the returned buffer is

 * assured to be either existing or formatted using a buffer lock on success.

 * @out_bh is substituted only when zero is returned.

 *

 * Return Value: On success, it returns 0. On error, the following negative

 * error code is returned.

 *

 * %-ENOMEM - Insufficient memory available.

 *

 * %-EIO - I/O error

 *

 * %-ENOENT - the specified block does not exist (hole block)

 *

 * %-EROFS - Read only filesystem (for create mode)

 Should be rewritten with merging nilfs_mdt_read_block() */

 create = 0; */  
/**

 * nilfs_mdt_find_block - find and get a buffer on meta data file.

 * @inode: inode of the meta data file

 * @start: start block offset (inclusive)

 * @end: end block offset (inclusive)

 * @blkoff: block offset

 * @out_bh: place to store a pointer to buffer_head struct

 *

 * nilfs_mdt_find_block() looks up an existing block in range of

 * [@start, @end] and stores pointer to a buffer head of the block to

 * @out_bh, and block offset to @blkoff, respectively.  @out_bh and

 * @blkoff are substituted only when zero is returned.

 *

 * Return Value: On success, it returns 0. On error, the following negative

 * error code is returned.

 *

 * %-ENOMEM - Insufficient memory available.

 *

 * %-EIO - I/O error

 *

 * %-ENOENT - no block was found in the range

/**

 * nilfs_mdt_delete_block - make a hole on the meta data file.

 * @inode: inode of the meta data file

 * @block: block offset

 *

 * Return Value: On success, zero is returned.

 * On error, one of the following negative error code is returned.

 *

 * %-ENOMEM - Insufficient memory available.

 *

 * %-EIO - I/O error

/**

 * nilfs_mdt_forget_block - discard dirty state and try to remove the page

 * @inode: inode of the meta data file

 * @block: block offset

 *

 * nilfs_mdt_forget_block() clears a dirty flag of the specified buffer, and

 * tries to release the page including the buffer from a page cache.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error code is returned.

 *

 * %-EBUSY - page has an active buffer.

 *

 * %-ENOENT - page cache has no page addressed by the offset.

		/*

		 * It means that filesystem was remounted in read-only

		 * mode because of error or metadata corruption. But we

		 * have dirty pages that try to be flushed in background.

		 * So, here we simply discard this dirty page.

/**

 * nilfs_mdt_clear - do cleanup for the metadata file

 * @inode: inode of the metadata file

/**

 * nilfs_mdt_destroy - release resources used by the metadata file

 * @inode: inode of the metadata file

 kfree(NULL) is safe */

/**

 * nilfs_mdt_setup_shadow_map - setup shadow map and bind it to metadata file

 * @inode: inode of the metadata file

 * @shadow: shadow mapping

/**

 * nilfs_mdt_save_to_shadow_map - copy bmap and dirty pages to shadow map

 * @inode: inode of the metadata file

 already frozen */

 drop ref-count to make it releasable */

/**

 * nilfs_mdt_restore_from_shadow_map - restore dirty pages and bmap state

 * @inode: inode of the metadata file

/**

 * nilfs_mdt_clear_shadow_map - truncate pages in shadow map caches

 * @inode: inode of the metadata file

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS B-tree node cache

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Originally written by Seiji Kihara.

 * Fully revised by Ryusuke Konishi for stabilization and simplification.

 *

 internal code */

 blocknr is a virtual block number */

 internal code */

 mode == READ */

 internal code */

 set block address for read */

 set back to the given block address */

/**

 * nilfs_btnode_delete - delete B-tree node buffer

 * @bh: buffer to be deleted

 *

 * nilfs_btnode_delete() invalidates the specified buffer and delete the page

 * including the buffer if the page gets unbusy.

/**

 * nilfs_btnode_prepare_change_key

 *  prepare to move contents of the block for old key to one of new key.

 *  the old buffer will not be removed, but might be reused for new buffer.

 *  it might return -ENOMEM because of memory allocation errors,

 *  and might return -EIO because of disk read errors.

 BUG_ON(oldkey != obh->b_page->index); */

		/*

		 * Note: page->index will not change to newkey until

		 * nilfs_btnode_commit_change_key() will be called.

		 * To protect the page in intermediate state, the page lock

		 * is held.

 fallback to copy mode */

/**

 * nilfs_btnode_commit_change_key

 *  commit the change_key operation prepared by prepare_change_key().

 blocksize == pagesize */

 will decrement bh->b_count */

/**

 * nilfs_btnode_abort_change_key

 *  abort the change_key operation prepared by prepare_change_key().

 blocksize == pagesize */

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS regular file handling primitives including fsync().

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Amagai Yoshiji and Ryusuke Konishi.

	/*

	 * Called from fsync() system call

	 * This is the only entry point that can catch write and synch

	 * timing for both data blocks and intermediate blocks.

	 *

	 * This function should be implemented when the writeback function

	 * will be implemented.

 -ENOSPC */

 make the VM retry the fault */

	/*

	 * check to see if the page is mapped already (no holes)

	/*

	 * fill hole blocks

 never returns -ENOMEM, but may return -ENOSPC */

/*

 * We have mostly NULL's here: the current defaults are ok for

 * the nilfs filesystem.

 CONFIG_COMPAT */

 .release	= nilfs_release_file, */

 end of file */

 SPDX-License-Identifier: GPL-2.0+

/*

 * the_nilfs shared structure.

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Ryusuke Konishi.

 *

/**

 * alloc_nilfs - allocate a nilfs object

 * @sb: super block instance

 *

 * Return Value: On success, pointer to the_nilfs is returned.

 * On error, NULL is returned.

/**

 * destroy_nilfs - destroy nilfs object

 * @nilfs: nilfs object to be released

/**

 * nilfs_store_log_cursor - load log cursor from a super block

 * @nilfs: nilfs object

 * @sbp: buffer storing super block to be read

 *

 * nilfs_store_log_cursor() reads the last position of the log

 * containing a super root from a given super block, and initializes

 * relevant information on the nilfs object preparatory for log

 * scanning and recovery.

/**

 * load_nilfs - load and recover the nilfs

 * @nilfs: the_nilfs structure to be released

 * @sb: super block instance used to recover past segment

 *

 * load_nilfs() searches and load the latest super root,

 * attaches the last segment, and does recovery if needed.

 * The caller must call this exclusively for simultaneous mounts.

		/*

		 * restore super block with its spare and reconfigure

		 * relevant states of the nilfs object.

 verify consistency between two super blocks */

 drop clean flag to allow roll-forward and recovery */

 set "clean" flag */

 page cache limit */

 bmap size limit */

/**

 * nilfs_nrsvsegs - calculate the number of reserved segments

 * @nilfs: nilfs object

 * @nsegs: total number of segments

	/*

	 * Compare two super blocks and set 1 in swp if the secondary

	 * super block is valid and newer.  Otherwise, set 0 in swp.

/**

 * init_nilfs - initialize a NILFS instance.

 * @nilfs: the_nilfs structure

 * @sb: super block

 * @data: mount options

 *

 * init_nilfs() performs common initialization per block device (e.g.

 * reading the super block, getting disk layout information, initializing

 * shared fields in the_nilfs).

 *

 * Return Value: On success, 0 is returned. On error, a negative error

 * code is returned.

			/*

			 * Not to failed_sbh; sbh is released automatically

			 * when reloading fails.

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS recovery logic

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Ryusuke Konishi.

/*

 * Segment check result

 work structure for recovery */

	ino_t ino;		/*

				 * Inode number of the file that this block

				 * belongs to

 block number */

 virtual block number */

 File offset of the data block (per block) */

/**

 * nilfs_compute_checksum - compute checksum of blocks continuously

 * @nilfs: nilfs object

 * @bhs: buffer head of start block

 * @sum: place to store result

 * @offset: offset bytes in the first block

 * @check_bytes: number of bytes to be checked

 * @start: DBN of start block

 * @nblock: number of blocks to be checked

/**

 * nilfs_read_super_root_block - read super root block

 * @nilfs: nilfs object

 * @sr_block: disk block number of the super root block

 * @pbh: address of a buffer_head pointer to return super root buffer

 * @check: CRC check flag

/**

 * nilfs_read_log_header - read summary header of the specified log

 * @nilfs: nilfs object

 * @start_blocknr: start block number of the log

 * @sum: pointer to return segment summary structure

/**

 * nilfs_validate_log - verify consistency of log

 * @nilfs: nilfs object

 * @seg_seq: sequence number of segment

 * @bh_sum: buffer head of summary block

 * @sum: segment summary struct

 This limits the number of blocks read in the CRC check */

/**

 * nilfs_read_summary_info - read an item on summary blocks of a log

 * @nilfs: nilfs object

 * @pbh: the current buffer head on summary blocks [in, out]

 * @offset: the current byte offset on summary blocks [in, out]

 * @bytes: byte size of the item to be read

/**

 * nilfs_skip_summary_info - skip items on summary blocks of a log

 * @nilfs: nilfs object

 * @pbh: the current buffer head on summary blocks [in, out]

 * @offset: the current byte offset on summary blocks [in, out]

 * @bytes: byte size of the item to be skipped

 * @count: number of items to be skipped

/**

 * nilfs_scan_dsync_log - get block information of a log written for data sync

 * @nilfs: nilfs object

 * @start_blocknr: start block number of the log

 * @sum: log summary information

 * @head: list head to add nilfs_recovery_block struct

 INIT_LIST_HEAD(&rb->list); */

 always 0 for data sync logs */

 brelse(NULL) is just ignored */

	/*

	 * Releasing the next segment of the latest super root.

	 * The next segment is invalidated by this recovery.

	/*

	 * Collecting segments written after the latest super root.

	 * These are marked dirty to avoid being reallocated in the next write.

 Allocate new segments for recovery */

 No need to recover sufile because it will be destroyed on error */

 iput(NULL) is just ignored */

/**

 * nilfs_do_roll_forward - salvage logical segments newer than the latest

 * checkpoint

 * @nilfs: nilfs object

 * @sb: super block instance

 * @ri: pointer to a nilfs_recovery_info

 Starting/ending DBN of full segment */

 list of data blocks to be recovered */

 scanning data-sync segments */

 Found a valid partial segment; do recovery actions */

 Fall through to try_next_pseg */

 Looking to the next full segment */

/**

 * nilfs_salvage_orphan_logs - salvage logs written after the latest checkpoint

 * @nilfs: nilfs object

 * @sb: super block instance

 * @ri: pointer to a nilfs_recovery_info struct to store search results.

 *

 * Return Value: On success, 0 is returned.  On error, one of the following

 * negative error code is returned.

 *

 * %-EINVAL - Inconsistent filesystem state.

 *

 * %-EIO - I/O error

 *

 * %-ENOSPC - No space left on device (only in a panic state).

 *

 * %-ERESTARTSYS - Interrupted.

 *

 * %-ENOMEM - Insufficient memory available.

/**

 * nilfs_search_super_root - search the latest valid super root

 * @nilfs: the_nilfs

 * @ri: pointer to a nilfs_recovery_info struct to store search results.

 *

 * nilfs_search_super_root() looks for the latest super-root from a partial

 * segment pointed by the superblock.  It sets up struct the_nilfs through

 * this search. It fills nilfs_recovery_info (ri) required for recovery.

 *

 * Return Value: On success, 0 is returned.  On error, one of the following

 * negative error code is returned.

 *

 * %-EINVAL - No valid segment found

 *

 * %-EIO - I/O error

 *

 * %-ENOMEM - Insufficient memory available.

 range of full segment (block number) */

 Calculate range of segment */

 Read ahead segment */

 A valid partial segment */

			/*

			 * This will never happen because a superblock

			 * (last_segment) always points to a pseg with

			 * a super root.

 A valid super root was found. */

 nilfs->ns_cno = ri->ri_cno + 1 */

 Standing on a course, or met an inconsistent state */

 Off the trail */

			/*

			 * This can happen if a checkpoint was written without

			 * barriers, or as a result of an I/O failure.

 Looking to the next full segment */

 found a valid super root */

 Updating pointers relating to the latest checkpoint */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Sysfs support implementation.

 *

 * Copyright (C) 2005-2014 Nippon Telegraph and Telephone Corporation.

 * Copyright (C) 2014 HGST, Inc., a Western Digital Company.

 *

 * Written by Vyacheslav Dubeyko <Vyacheslav.Dubeyko@hgst.com>

 /sys/fs/<nilfs>/ */

/************************************************************************

 *                        NILFS snapshot attrs                          *

/************************************************************************

 *                    NILFS mounted snapshots attrs                     *

/************************************************************************

 *                      NILFS checkpoints attrs                         *

/************************************************************************

 *                        NILFS segments attrs                          *

/************************************************************************

 *                        NILFS segctor attrs                           *

/************************************************************************

 *                        NILFS superblock attrs                        *

/************************************************************************

 *                        NILFS device attrs                            *

/************************************************************************

 *                        NILFS feature attrs                           *

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS direct block pointer.

 *

 * Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Koji Sato.

 sequential access */

 block group */

 ptr must be a pointer to a buffer head. */

 no need to allocate any resource for conversion */

 delete */

 free resources */

 convert */

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS segment buffer

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Ryusuke Konishi.

 *

 The region to be submitted */

/**

 * nilfs_segbuf_map_cont - map a new log behind a given log

 * @segbuf: new segment buffer

 * @prev: segment buffer containing a log to be continued

/*

 * Setup segment summary

/*

 * CRC calculation routines

/*

 * Iterators for segment buffers

/**

 * nilfs_add_checksums_on_logs - add checksums on the logs

 * @logs: list of segment buffers storing target logs

 * @seed: checksum seed value

/*

 * BIO operations

/**

 * nilfs_alloc_seg_bio - allocate a new bio for writing log

 * @nilfs: nilfs object

 * @start: start block number of the bio

 * @nr_vecs: request size of page vector.

 *

 * Return Value: On success, pointer to the struct bio is returned.

 * On error, NULL is returned.

 bio is FULL */

 never submit current bh */

/**

 * nilfs_segbuf_write - submit write requests of a log

 * @segbuf: buffer storing a log to be written

 * @nilfs: nilfs object

 *

 * Return Value: On Success, 0 is returned. On Error, one of the following

 * negative error code is returned.

 *

 * %-EIO - I/O error

 *

 * %-ENOMEM - Insufficient memory available.

		/*

		 * Last BIO is always sent through the following

		 * submission.

/**

 * nilfs_segbuf_wait - wait for completion of requested BIOs

 * @segbuf: segment buffer

 *

 * Return Value: On Success, 0 is returned. On Error, one of the following

 * negative error code is returned.

 *

 * %-EIO - I/O error

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS B-tree.

 *

 * Copyright (C) 2005-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Koji Sato.

/*

 * B-tree node operations

 Assume the buffer heads corresponding to left and right are locked. */

 Assume that the buffer heads corresponding to left and right are locked. */

 Assume that the buffer head corresponding to node is locked. */

 Assume that the buffer head corresponding to node is locked. */

 binary search */

 adjust index */

/**

 * nilfs_btree_node_broken - verify consistency of btree node

 * @node: btree node block to be examined

 * @size: node size (in bytes)

 * @inode: host inode of btree

 * @blocknr: block number

 *

 * Return Value: If node is broken, 1 is returned. Otherwise, 0 is returned.

/**

 * nilfs_btree_root_broken - verify consistency of btree root node

 * @node: btree root node to be examined

 * @inode: host inode of btree

 *

 * Return Value: If node is broken, 1 is returned. Otherwise, 0 is returned.

 parent node */

 max nof blocks to read ahead */

 current index on the parent node */

 nof children in the parent node */

 read ahead sibling nodes */

 insert */

/**

 * nilfs_btree_get_next_key - get next valid key from btree path array

 * @btree: bmap struct of btree

 * @path: array of nilfs_btree_path struct

 * @minlevel: start level

 * @nextkey: place to store the next valid key

 *

 * Return Value: If a next key was found, 0 is returned. Otherwise,

 * -ENOENT is returned.

 Next index is already set to bp_index for leaf nodes. */

 Next key is in this node */

 For non-leaf nodes, next index is stored at bp_index + 1. */

 look-up right sibling node */

 root */

 move insert point */

 move insert point */

 left sibling */

 parent */

 sequential access */

 near */

 block group */

 allocate a new ptr for data block */

 left sibling */

 right sibling */

 split */

 root */

 grow */

 a newly-created node block and a data block are added */

 success */

 error */

 left sibling */

 continue; */

 right sibling */

				/*

				 * When merging right sibling node

				 * into the current node, pointer to

				 * the right sibling node must be

				 * terminated instead.  The adjustment

				 * below is required for that.

 continue; */

 no siblings */

 the only child of the root node */

 child of the root node is deleted */

 success */

 error */

 for data */

 cannot find near ptr */

 success */

 error */

 free resources */

 ptr must be a pointer to a buffer head. */

 convert and insert */

 create child node at level 1 */

 create root node at level 2 */

 create root node at level 1 */

/**

 * nilfs_btree_convert_and_insert -

 * @bmap:

 * @key:

 * @ptr:

 * @keys:

 * @ptrs:

 * @n:

 success */

 error */

 on-disk format */

 on-disk format */

 on-disk format */

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS block mapping.

 *

 * Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Koji Sato.

/**

 * nilfs_bmap_lookup_at_level - find a data block or node block

 * @bmap: bmap

 * @key: key

 * @level: level

 * @ptrp: place to store the value associated to @key

 *

 * Description: nilfs_bmap_lookup_at_level() finds a record whose key

 * matches @key in the block at @level of the bmap.

 *

 * Return Value: On success, 0 is returned and the record associated with @key

 * is stored in the place pointed by @ptrp. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - A record associated with @key does not exist.

/**

 * nilfs_bmap_insert - insert a new key-record pair into a bmap

 * @bmap: bmap

 * @key: key

 * @rec: record

 *

 * Description: nilfs_bmap_insert() inserts the new key-record pair specified

 * by @key and @rec into @bmap.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-EEXIST - A record associated with @key already exist.

/**

 * nilfs_bmap_seek_key - seek a valid entry and return its key

 * @bmap: bmap struct

 * @start: start key number

 * @keyp: place to store valid key

 *

 * Description: nilfs_bmap_seek_key() seeks a valid key on @bmap

 * starting from @start, and stores it to @keyp if found.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - No valid entry was found

/**

 * nilfs_bmap_delete - delete a key-record pair from a bmap

 * @bmap: bmap

 * @key: key

 *

 * Description: nilfs_bmap_delete() deletes the key-record pair specified by

 * @key from @bmap.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - A record associated with @key does not exist.

/**

 * nilfs_bmap_truncate - truncate a bmap to a specified key

 * @bmap: bmap

 * @key: key

 *

 * Description: nilfs_bmap_truncate() removes key-record pairs whose keys are

 * greater than or equal to @key from @bmap.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

/**

 * nilfs_bmap_clear - free resources a bmap holds

 * @bmap: bmap

 *

 * Description: nilfs_bmap_clear() frees resources associated with @bmap.

/**

 * nilfs_bmap_propagate - propagate dirty state

 * @bmap: bmap

 * @bh: buffer head

 *

 * Description: nilfs_bmap_propagate() marks the buffers that directly or

 * indirectly refer to the block specified by @bh dirty.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

/**

 * nilfs_bmap_lookup_dirty_buffers -

 * @bmap: bmap

 * @listp: pointer to buffer head list

/**

 * nilfs_bmap_assign - assign a new block number to a block

 * @bmap: bmap

 * @bh: pointer to buffer head

 * @blocknr: block number

 * @binfo: block information

 *

 * Description: nilfs_bmap_assign() assigns the block number @blocknr to the

 * buffer specified by @bh.

 *

 * Return Value: On success, 0 is returned and the buffer head of a newly

 * create buffer and the block information associated with the buffer are

 * stored in the place pointed by @bh and @binfo, respectively. On error, one

 * of the following negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

/**

 * nilfs_bmap_mark - mark block dirty

 * @bmap: bmap

 * @key: key

 * @level: level

 *

 * Description: nilfs_bmap_mark() marks the block specified by @key and @level

 * as dirty.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

/**

 * nilfs_bmap_test_and_clear_dirty - test and clear a bmap dirty state

 * @bmap: bmap

 *

 * Description: nilfs_test_and_clear() is the atomic operation to test and

 * clear the dirty state of @bmap.

 *

 * Return Value: 1 is returned if @bmap is dirty, or 0 if clear.

/*

 * Internal use only

/**

 * nilfs_bmap_read - read a bmap from an inode

 * @bmap: bmap

 * @raw_inode: on-disk inode

 *

 * Description: nilfs_bmap_read() initializes the bmap @bmap.

 *

 * Return Value: On success, 0 is returned. On error, the following negative

 * error code is returned.

 *

 * %-ENOMEM - Insufficient amount of memory available.

/**

 * nilfs_bmap_write - write back a bmap to an inode

 * @bmap: bmap

 * @raw_inode: on-disk inode

 *

 * Description: nilfs_bmap_write() stores @bmap in @raw_inode.

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS disk address translation.

 *

 * Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Koji Sato.

/**

 * struct nilfs_dat_info - on-memory private data of DAT file

 * @mi: on-memory private data of metadata file

 * @palloc_cache: persistent object allocator cache of DAT file

 * @shadow: shadow map of DAT file

/**

 * nilfs_dat_mark_dirty -

 * @dat: DAT file inode

 * @vblocknr: virtual block number

 *

 * Description:

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

/**

 * nilfs_dat_freev - free virtual block numbers

 * @dat: DAT file inode

 * @vblocknrs: array of virtual block numbers

 * @nitems: number of virtual block numbers

 *

 * Description: nilfs_dat_freev() frees the virtual block numbers specified by

 * @vblocknrs and @nitems.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - The virtual block number have not been allocated.

/**

 * nilfs_dat_move - change a block number

 * @dat: DAT file inode

 * @vblocknr: virtual block number

 * @blocknr: block number

 *

 * Description: nilfs_dat_move() changes the block number associated with

 * @vblocknr to @blocknr.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

	/*

	 * The given disk block number (blocknr) is not yet written to

	 * the device at this point.

	 *

	 * To prevent nilfs_dat_translate() from returning the

	 * uncommitted block number, this makes a copy of the entry

	 * buffer and redirects nilfs_dat_translate() to the copy.

/**

 * nilfs_dat_translate - translate a virtual block number to a block number

 * @dat: DAT file inode

 * @vblocknr: virtual block number

 * @blocknrp: pointer to a block number

 *

 * Description: nilfs_dat_translate() maps the virtual block number @vblocknr

 * to the corresponding block number.

 *

 * Return Value: On success, 0 is returned and the block number associated

 * with @vblocknr is stored in the place pointed by @blocknrp. On error, one

 * of the following negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - A block number associated with @vblocknr does not exist.

 last virtual block number in this block */

/**

 * nilfs_dat_read - read or get dat inode

 * @sb: super block instance

 * @entry_size: size of a dat entry

 * @raw_inode: on-disk dat inode

 * @inodep: buffer to store the inode

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS checkpoint file.

 *

 * Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Written by Koji Sato.

 block number from the beginning of the file */

 offset in block */

/**

 * nilfs_cpfile_find_checkpoint_block - find and get a buffer on cpfile

 * @cpfile: inode of cpfile

 * @start_cno: start checkpoint number (inclusive)

 * @end_cno: end checkpoint number (inclusive)

 * @cnop: place to store the next checkpoint number

 * @bhp: place to store a pointer to buffer_head struct

 *

 * Return Value: On success, it returns 0. On error, the following negative

 * error code is returned.

 *

 * %-ENOMEM - Insufficient memory available.

 *

 * %-EIO - I/O error

 *

 * %-ENOENT - no block exists in the range.

/**

 * nilfs_cpfile_get_checkpoint - get a checkpoint

 * @cpfile: inode of checkpoint file

 * @cno: checkpoint number

 * @create: create flag

 * @cpp: pointer to a checkpoint

 * @bhp: pointer to a buffer head

 *

 * Description: nilfs_cpfile_get_checkpoint() acquires the checkpoint

 * specified by @cno. A new checkpoint will be created if @cno is the current

 * checkpoint number and @create is nonzero.

 *

 * Return Value: On success, 0 is returned, and the checkpoint and the

 * buffer head of the buffer on which the checkpoint is located are stored in

 * the place pointed by @cpp and @bhp, respectively. On error, one of the

 * following negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - No such checkpoint.

 *

 * %-EINVAL - invalid checkpoint.

 a newly-created checkpoint */

/**

 * nilfs_cpfile_put_checkpoint - put a checkpoint

 * @cpfile: inode of checkpoint file

 * @cno: checkpoint number

 * @bh: buffer head

 *

 * Description: nilfs_cpfile_put_checkpoint() releases the checkpoint

 * specified by @cno. @bh must be the buffer head which has been returned by

 * a previous call to nilfs_cpfile_get_checkpoint() with @cno.

/**

 * nilfs_cpfile_delete_checkpoints - delete checkpoints

 * @cpfile: inode of checkpoint file

 * @start: start checkpoint number

 * @end: end checkpoint number

 *

 * Description: nilfs_cpfile_delete_checkpoints() deletes the checkpoints in

 * the period from @start to @end, excluding @end itself. The checkpoints

 * which have been already deleted are ignored.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-EINVAL - invalid checkpoints.

 skip hole */

 make hole */

 checkpoint number 0 is invalid */

 No snapshots (started from a hole block) */

 Terminator */

 reach end of the snapshot list */

/**

 * nilfs_cpfile_get_cpinfo -

 * @cpfile:

 * @cno:

 * @ci:

 * @nci:

/**

 * nilfs_cpfile_delete_checkpoint -

 * @cpfile:

 * @cno:

 checkpoint number 0 is invalid */

 checkpoint number 0 is invalid */

/**

 * nilfs_cpfile_is_snapshot -

 * @cpfile: inode of checkpoint file

 * @cno: checkpoint number

 *

 * Description:

 *

 * Return Value: On success, 1 is returned if the checkpoint specified by

 * @cno is a snapshot, or 0 if not. On error, one of the following negative

 * error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - No such checkpoint.

	/*

	 * CP number is invalid if it's zero or larger than the

	 * largest existing one.

/**

 * nilfs_cpfile_change_cpmode - change checkpoint mode

 * @cpfile: inode of checkpoint file

 * @cno: checkpoint number

 * @mode: mode of checkpoint

 *

 * Description: nilfs_change_cpmode() changes the mode of the checkpoint

 * specified by @cno. The mode @mode is NILFS_CHECKPOINT or NILFS_SNAPSHOT.

 *

 * Return Value: On success, 0 is returned. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

 *

 * %-ENOENT - No such checkpoint.

			/*

			 * Current implementation does not have to protect

			 * plain read-only mounts since they are exclusive

			 * with a read/write mount and are protected from the

			 * cleaner.

/**

 * nilfs_cpfile_get_stat - get checkpoint statistics

 * @cpfile: inode of checkpoint file

 * @cpstat: pointer to a structure of checkpoint statistics

 *

 * Description: nilfs_cpfile_get_stat() returns information about checkpoints.

 *

 * Return Value: On success, 0 is returned, and checkpoints information is

 * stored in the place pointed by @cpstat. On error, one of the following

 * negative error codes is returned.

 *

 * %-EIO - I/O error.

 *

 * %-ENOMEM - Insufficient amount of memory available.

/**

 * nilfs_cpfile_read - read or get cpfile inode

 * @sb: super block instance

 * @cpsize: size of a checkpoint entry

 * @raw_inode: on-disk cpfile inode

 * @inodep: buffer to store the inode

 SPDX-License-Identifier: GPL-2.0+

/*

 * NILFS dat/inode allocator

 *

 * Copyright (C) 2006-2008 Nippon Telegraph and Telephone Corporation.

 *

 * Originally written by Koji Sato.

 * Two allocators were unified by Ryusuke Konishi and Amagai Yoshiji.

/**

 * nilfs_palloc_groups_per_desc_block - get the number of groups that a group

 *					descriptor block can maintain

 * @inode: inode of metadata file using this allocator

/**

 * nilfs_palloc_groups_count - get maximum number of groups

 * @inode: inode of metadata file using this allocator

 log2(8) */));

/**

 * nilfs_palloc_init_blockgroup - initialize private variables for allocator

 * @inode: inode of metadata file using this allocator

 * @entry_size: size of the persistent object

		/*

		 * Number of blocks in a group including entry blocks

		 * and a bitmap block

		/*

		 * Number of blocks per descriptor including the

		 * descriptor block

/**

 * nilfs_palloc_group - get group number and offset from an entry number

 * @inode: inode of metadata file using this allocator

 * @nr: serial number of the entry (e.g. inode number)

 * @offset: pointer to store offset number in the group

/**

 * nilfs_palloc_desc_blkoff - get block offset of a group descriptor block

 * @inode: inode of metadata file using this allocator

 * @group: group number

 *

 * nilfs_palloc_desc_blkoff() returns block offset of the descriptor

 * block which contains a descriptor of the specified group.

/**

 * nilfs_palloc_bitmap_blkoff - get block offset of a bitmap block

 * @inode: inode of metadata file using this allocator

 * @group: group number

 *

 * nilfs_palloc_bitmap_blkoff() returns block offset of the bitmap

 * block used to allocate/deallocate entries in the specified group.

/**

 * nilfs_palloc_group_desc_nfrees - get the number of free entries in a group

 * @desc: pointer to descriptor structure for the group

 * @lock: spin lock protecting @desc

/**

 * nilfs_palloc_group_desc_add_entries - adjust count of free entries

 * @desc: pointer to descriptor structure for the group

 * @lock: spin lock protecting @desc

 * @n: delta to be added

/**

 * nilfs_palloc_entry_blkoff - get block offset of an entry block

 * @inode: inode of metadata file using this allocator

 * @nr: serial number of the entry (e.g. inode number)

/**

 * nilfs_palloc_desc_block_init - initialize buffer of a group descriptor block

 * @inode: inode of metadata file

 * @bh: buffer head of the buffer to be initialized

 * @kaddr: kernel address mapped for the page including the buffer

		/*

		 * The following code must be safe for change of the

		 * cache contents during the get block call.

/**

 * nilfs_palloc_delete_block - delete a block on the persistent allocator file

 * @inode: inode of metadata file using this allocator

 * @blkoff: block offset

 * @prev: nilfs_bh_assoc struct of the last used buffer

 * @lock: spin lock protecting @prev

/**

 * nilfs_palloc_get_desc_block - get buffer head of a group descriptor block

 * @inode: inode of metadata file using this allocator

 * @group: group number

 * @create: create flag

 * @bhp: pointer to store the resultant buffer head

/**

 * nilfs_palloc_get_bitmap_block - get buffer head of a bitmap block

 * @inode: inode of metadata file using this allocator

 * @group: group number

 * @create: create flag

 * @bhp: pointer to store the resultant buffer head

/**

 * nilfs_palloc_delete_bitmap_block - delete a bitmap block

 * @inode: inode of metadata file using this allocator

 * @group: group number

/**

 * nilfs_palloc_get_entry_block - get buffer head of an entry block

 * @inode: inode of metadata file using this allocator

 * @nr: serial number of the entry (e.g. inode number)

 * @create: create flag

 * @bhp: pointer to store the resultant buffer head

/**

 * nilfs_palloc_delete_entry_block - delete an entry block

 * @inode: inode of metadata file using this allocator

 * @nr: serial number of the entry

/**

 * nilfs_palloc_block_get_group_desc - get kernel address of a group descriptor

 * @inode: inode of metadata file using this allocator

 * @group: group number

 * @bh: buffer head of the buffer storing the group descriptor block

 * @kaddr: kernel address mapped for the page including the buffer

/**

 * nilfs_palloc_block_get_entry - get kernel address of an entry

 * @inode: inode of metadata file using this allocator

 * @nr: serial number of the entry (e.g. inode number)

 * @bh: buffer head of the buffer storing the entry block

 * @kaddr: kernel address mapped for the page including the buffer

/**

 * nilfs_palloc_find_available_slot - find available slot in a group

 * @bitmap: bitmap of the group

 * @target: offset number of an entry in the group (start point)

 * @bsize: size in bits

 * @lock: spin lock protecting @bitmap

 wrap around */

/**

 * nilfs_palloc_rest_groups_in_desc_block - get the remaining number of groups

 *					    in a group descriptor block

 * @inode: inode of metadata file using this allocator

 * @curr: current group number

 * @max: maximum number of groups

/**

 * nilfs_palloc_count_desc_blocks - count descriptor blocks number

 * @inode: inode of metadata file using this allocator

 * @desc_blocks: descriptor blocks number [out]

/**

 * nilfs_palloc_mdt_file_can_grow - check potential opportunity for

 *					MDT file growing

 * @inode: inode of metadata file using this allocator

 * @desc_blocks: known current descriptor blocks count

/**

 * nilfs_palloc_count_max_entries - count max number of entries that can be

 *					described by descriptor blocks count

 * @inode: inode of metadata file using this allocator

 * @nused: current number of used entries

 * @nmaxp: max number of entries [out]

/**

 * nilfs_palloc_prepare_alloc_entry - prepare to allocate a persistent object

 * @inode: inode of metadata file using this allocator

 * @req: nilfs_palloc_req structure exchanged for the allocation

 wrap around */

 found a free entry */

 no entries left */

/**

 * nilfs_palloc_commit_alloc_entry - finish allocation of a persistent object

 * @inode: inode of metadata file using this allocator

 * @req: nilfs_palloc_req structure exchanged for the allocation

/**

 * nilfs_palloc_commit_free_entry - finish deallocating a persistent object

 * @inode: inode of metadata file using this allocator

 * @req: nilfs_palloc_req structure exchanged for the removal

/**

 * nilfs_palloc_abort_alloc_entry - cancel allocation of a persistent object

 * @inode: inode of metadata file using this allocator

 * @req: nilfs_palloc_req structure exchanged for the allocation

/**

 * nilfs_palloc_prepare_free_entry - prepare to deallocate a persistent object

 * @inode: inode of metadata file using this allocator

 * @req: nilfs_palloc_req structure exchanged for the removal

/**

 * nilfs_palloc_abort_free_entry - cancel deallocating a persistent object

 * @inode: inode of metadata file using this allocator

 * @req: nilfs_palloc_req structure exchanged for the removal

/**

 * nilfs_palloc_freev - deallocate a set of persistent objects

 * @inode: inode of metadata file using this allocator

 * @entry_nrs: array of entry numbers to be deallocated

 * @nitems: number of entries stored in @entry_nrs

 Get the first entry number of the group */

 This entry is in the same block */

 Test if the entry block is empty or not */

 Go on to the next entry block */

 SPDX-License-Identifier: GPL-2.0

/*

 * Process version 2 NFS requests.

 *

 * Copyright (C) 1995-1997 Olaf Kirch <okir@monad.swb.de>

/*

 * Get a file's attributes

 * N.B. After this call resp->fh needs an fh_put

/*

 * Set a file's attributes

 * N.B. After this call resp->fh needs an fh_put

	/*

	 * NFSv2 does not differentiate between "set-[ac]time-to-now"

	 * which only requires access, and "set-[ac]time-to-X" which

	 * requires ownership.

	 * So if it looks like it might be "set both to the same time which

	 * is close to now", and if setattr_prepare fails, then we

	 * convert to "set to now" instead of "set to explicit time"

	 *

	 * We only call setattr_prepare as the last test as technically

	 * it is not an interface that we should be using.

		/*

		 * Looks probable.

		 *

		 * Now just make sure time is in the right ballpark.

		 * Solaris, at least, doesn't seem to care what the time

		 * request is.  We require it be within 30 minutes of now.

			/*

			 * Turn off ATTR_[AM]TIME_SET but leave ATTR_[AM]TIME.

			 * This will cause notify_change to set these times

			 * to "now"

 Obsolete, replaced by MNTPROC_MNT. */

/*

 * Look up a path name component

 * Note: the dentry in the resp->fh may be negative if the file

 * doesn't exist yet.

 * N.B. After this call resp->fh needs an fh_put

/*

 * Read a symlink.

 Read the symlink. */

/*

 * Read a portion of a file.

 * N.B. After this call resp->fh needs an fh_put

	/* Obtain buffer pointer for payload. 19 is 1 word for

	 * status, 17 words for fattr, and 1 word for the byte count.

 Reserved */

/*

 * Write data to a file

 * N.B. After this call resp->fh needs an fh_put

/*

 * CREATE processing is complicated. The keyword here is `overloaded.'

 * The parent directory is kept locked between the check for existence

 * and the actual create() call in compliance with VFS protocols.

 * N.B. After this call _both_ argp->fh and resp->fh need an fh_put

 First verify the parent file handle */

 must fh_put dirfhp even on error */

 Check for NFSD_MAY_WRITE in nfsd_create if necessary */

		/*

		 * If the new file handle wasn't verified, we can't tell

		 * whether the file exists or not. Time to bail ...

 Unfudge the mode bits */

			/* no type, so if target exists, assume same as that,

 reserve rdev for later checking */

					/* this is probably a permission check..

					 * at least IRIX implements perm checking on

					 *   echo thing > device-special-file-or-pipe

					 * by doing a CREATE with type==0

 ??? */

	/* Special treatment for non-regular files according to the

	 * gospel of sun micro

 If you think you've seen the worst, grok this. */

 Okay, char or block special */

 we've used the SIZE information, so discard it */

 Make sure the type and device matches */

 File doesn't exist. Create it and set attrs */

		/* File already exists. We ignore all attributes except

		 * size, so that creat() behaves exactly like

		 * open(..., O_CREAT|O_TRUNC|O_WRONLY).

 We don't really need to unlock, as fh_put does it. */

 Unlink. -SIFDIR means file must not be a directory */

/*

 * Make directory. This operation is not idempotent.

 * N.B. After this call resp->fh needs an fh_put

/*

 * Remove a directory

 Reserve room for the NULL ptr & eof flag (-2 words) */

	/* This is xdr_init_encode(), but it assumes that

/*

 * Read a portion of a directory.

/*

 * Get file system info

/*

 * NFSv2 Server procedures.

 * Only the results of non-idempotent operations are cached.

 status */

 filehandle */

 attributes */

/*

 * Map errnos to NFS errnos.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de> */

 discard any old override before preparing the new set */

 Each thread allocates its own gi, no race */

/*

*  Copyright (c) 2004 The Regents of the University of Michigan.

*  Copyright (c) 2012 Jeff Layton <jlayton@redhat.com>

*  All rights reserved.

*

*  Andy Adamson <andros@citi.umich.edu>

*

*  Redistribution and use in source and binary forms, with or without

*  modification, are permitted provided that the following conditions

*  are met:

*

*  1. Redistributions of source code must retain the above copyright

*     notice, this list of conditions and the following disclaimer.

*  2. Redistributions in binary form must reproduce the above copyright

*     notice, this list of conditions and the following disclaimer in the

*     documentation and/or other materials provided with the distribution.

*  3. Neither the name of the University nor the names of its

*     contributors may be used to endorse or promote products derived

*     from this software without specific prior written permission.

*

*  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

*  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF

*  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE

*  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE

*  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR

*  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF

*  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR

*  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF

*  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING

*  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS

*  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

*

 Declarations */

 Globals */

/*

 * If we had an error generating the recdir name for the legacy tracker

 * then warn the admin. If the error doesn't appear to be transient,

 * then disable recovery tracking.

	/*

	 * if the algorithm just doesn't exist, then disable the recovery

	 * tracker altogether. The crypto libs will generally return this if

	 * FIPS is enabled as well.

 lock the parent */

		/*

		 * In the 4.1 case, where we're called from

		 * reclaim_complete(), records from the previous reboot

		 * may still be left, so this is OK.

		 *

		 * In the 4.0 case, we should never get here; but we may

		 * as well be forgiving and just succeed silently.

 Keep trying; maybe the others are OK: */

 Keep trying, success or failure: */

 Keep trying; maybe the others are OK: */

/*

 * Hold reference to the recovery directory.

 XXX: The legacy code won't work in a container */

/*

 * Change the NFSv4 recovery directory to recdir.

 did we already find that this client is stable? */

 look for it in the reclaim hashtable otherwise */

 Globals */

 per-net-ns structure for holding cld upcall info */

	/*

	 * -EAGAIN occurs when pipe is closed and reopened while there are

	 *  upcalls queued.

 copy just the xid so we can try to find that */

	/*

	 * copy the status so we know whether to remove the upcall from the

	 * list (for -EINPROGRESS, we just want to make sure the xid is

	 * valid, not remove the upcall from the list)

 walk the list and find corresponding xid */

 couldn't find upcall? */

 errno >= 0 means we got a downcall */

 Initialize rpc_pipefs pipe for communication with client tracking daemon */

 FIXME: hard cap on number in flight? */

 Ask daemon to create a new record */

 Don't upcall if it's already stored */

 Ask daemon to create a new record */

 Don't upcall if it's already stored */

 Ask daemon to create a new record */

 Don't upcall if it's already removed */

/*

 * For older nfsdcld's that do not allow us to "slurp" the clients

 * from the tracking database during startup.

 *

 * Check for presence of a record, and update its timestamp

 Don't upcall if one was already stored during this grace pd */

/*

 * For newer nfsdcld's that allow us to "slurp" the clients

 * from the tracking database during startup.

 *

 * Check for presence of a record in the reclaim_str_hashtbl

 did we already find that this client is stable? */

 look for it in the reclaim hashtable otherwise */

 did we already find that this client is stable? */

 look for it in the reclaim hashtable otherwise */

 For older nfsdcld's that need cm_gracetime */

/*

 * For newer nfsdcld's that do not need cm_gracetime.  We also need to call

 * nfs4_release_reclaim() to clear out the reclaim_str_hashtbl.

	/*

	 * rpc pipe upcalls take 30 seconds to time out, so we don't want to

	 * queue an upcall unless we know that nfsdcld is running (because we

	 * want this to fail fast so that nfsd4_client_tracking_init() can try

	 * the next client tracking method).  nfsdcld should already be running

	 * before nfsd is started, so the wait here is for nfsdcld to open the

	 * pipefs file we just created.

 For older nfsdcld's */

 For newer nfsdcld's */

 v2 create/check ops include the principal, if available */

 upcall via usermodehelper */

 just return nothing if output was truncated */

 +1 is for '/' between "topdir" and "recdir" */

 just return nothing if output will be truncated */

 prefix + Y/N character + terminating NULL */

 just return nothing if output was truncated */

 prefix + max width of int64_t string + terminating NULL */

 just return nothing if output was truncated */

	/*

	 * Disable the upcall mechanism if we're getting an ENOENT or EACCES

	 * error. The admin can re-enable it on the fly by using sysfs

	 * once the problem has been fixed.

 +1 for terminating NULL */

 XXX: The usermode helper s not working in container yet. */

	/*

	 * With v4.0 clients, there's little difference in outcome between a

	 * create and check operation, and we can end up calling into this

	 * function multiple times per client (once for each openowner). So,

	 * for v4.0 clients skip upcalling once the client has been recorded

	 * on stable storage.

	 *

	 * For v4.1+ clients, the outcome of the two operations is different,

	 * so we must ensure that we upcall for the create operation. v4.1+

	 * clients call this on RECLAIM_COMPLETE though, so we should only end

	 * up doing a single create upcall per client.

 FIXME: better way to determine max size? */

 just run the init if it the method is already decided */

 First, try to use nfsdcld */

	/*

	 * Next, try the UMH upcall.

	/*

	 * Finally, See if the recoverydir exists and is a directory.

	 * If it is, then use the legacy ops.

 SPDX-License-Identifier: GPL-2.0

/*

 * File operations used by nfsd. Some of these have been ripped from

 * other parts of the kernel because they weren't exported, others

 * are partial duplicates with added or changed functionality.

 *

 * Note that several functions dget() the dentry upon which they want

 * to act, most notably those that create directory entries. Response

 * dentry's are dput()'d if necessary in the release callback.

 * So if you notice code paths that apparently fail to dput() the

 * dentry, don't worry--they have been taken care of.

 *

 * Copyright (C) 1995-1999 Olaf Kirch <okir@monad.swb.de>

 * Zerocpy NFS support (C) 2002 Hirokazu Takahashi <taka@valinux.co.jp>

 CONFIG_NFSD_V3 */

 CONFIG_NFSD_V4 */

/* 

 * Called from nfsd_lookup and encode_dirent. Check if we have crossed 

 * a mount point.

 * Returns -EAGAIN or -ETIMEDOUT leaving *dpp and *expp unchanged,

 *  or nfs_ok having possibly changed *dpp and *expp

 This is only a mountpoint in some other namespace */

		/*

		 * We normally allow NFS clients to continue

		 * "underneath" a mountpoint that is not exported.

		 * The exception is V4ROOT, where no traversal is ever

		 * allowed without an explicit export of the new

		 * directory.

 successfully crossed mount point */

		/*

		 * This is subtle: path.dentry is *not* on path.mnt

		 * at this point.  The only reason we are safe is that

		 * original mnt is pinned down by exp, so we should

		 * put path *before* putting exp

/*

 * For nfsd purposes, we treat V4ROOT exports as though there was an

 * export at *every* directory.

 * We return:

 * '1' if this dentry *must* be an export point,

 * '2' if it might be, if there is really a mount here, and

 * '0' if there is no chance of an export point here.

		/*

		 * Might only be a mountpoint in a different namespace,

		 * but we need to check.

 Lookup the name, but don't follow links */

 .. == . just like at / */

 checking mountpoint crossing is very different when stepping up */

		/*

		 * In the nfsd4_open() case, this may be held across

		 * subsequent open and delegation acquisition which may

		 * need to take the child's i_mutex:

			/*

			 * We don't need the i_mutex after all.  It's

			 * still possible we could open this (regular

			 * files can be mountpoints too), but the

			 * i_mutex is just there to prevent renames of

			 * something that we might be about to delegate,

			 * and a mountpoint won't be renamed:

/*

 * Look up one component of a pathname.

 * N.B. After this call _both_ fhp and resfh need an fh_put

 *

 * If the lookup would cross a mountpoint, and the mounted filesystem

 * is exported to the client with NFSEXP_NOHIDE, then the lookup is

 * accepted as it stands and the mounted directory is

 * returned. Otherwise the covered directory is returned.

 * NOTE: this mountpoint crossing is not supported properly by all

 *   clients and is explicitly disallowed for NFSv3

 *      NeilBrown <neilb@cse.unsw.edu.au>

	/*

	 * Note: we compose the file handle now, but as the

	 * dentry may be negative, it may need to be updated.

/*

 * Commit metadata changes to stable storage.

/*

 * Go over the attributes and take care of the small differences between

 * NFS semantics and what Linux expects.

 sanitize the mode change */

 Revoke setuid/setgid on chown */

 we're setting mode too, just clear the s*id bits */

 set ATTR_KILL_* bits and let VFS handle it */

/*

 * Set various file attributes.  After this call fhp needs an fh_put.

	/*

	 * If utimes(2) and friends are called with times not NULL, we should

	 * not set NFSD_MAY_WRITE bit. Otherwise fh_verify->nfsd_permission

	 * will return EACCES, when the caller's effective UID does not match

	 * the owner of the file, and the caller is not privileged. In this

	 * situation, we should return EPERM(notify_change will return this).

 Callers that do fh_verify should do the fh_want_write: */

 Get inode */

 Ignore any mode updates on symlinks */

	/*

	 * The size case is special, it changes the file in addition to the

	 * attributes, and file systems don't expect it to be mixed with

	 * "random" attribute changes.  We thus split out the size change

	 * into a separate call to ->setattr, and do the rest as a separate

	 * setattr call.

		/*

		 * RFC5661, Section 18.30.4:

		 *   Changing the size of a file with SETATTR indirectly

		 *   changes the time_modify and change attributes.

		 *

		 * (and similar for the older RFCs)

		/*

		 * Avoid the additional setattr call below if the only other

		 * attribute that the client sends is the mtime, as we update

		 * it as part of the size change above.

/*

 * NFS junction information is stored in an extended attribute.

/**

 * nfsd4_is_junction - Test if an object could be an NFS junction

 *

 * @dentry: object to test

 *

 * Returns 1 if "dentry" appears to contain NFS junction information.

 * Otherwise 0 is returned.

 S_IFREG */, NFSD_MAY_SATTR);

	/*

	 * Limit copy to 4MB to prevent indefinitely blocking an nfsd

	 * thread and client rpc slot.  The choice of 4MB is somewhat

	 * arbitrary.  We might instead base this on r/wsize, or make it

	 * tunable, or use a time instead of a byte limit, or implement

	 * asynchronous copy.  In theory a client could also recognize a

	 * limit like this and pipeline multiple COPY requests.

 defined(CONFIG_NFSD_V4) */

/*

 * Check server access rights to a file system object

	/* Some clients - Solaris 2.6 at least, make an access call

	 * to the server to check for access for things like /dev/null

	 * (which really, the server doesn't care about).  So

	 * We provide simple access checking for them, looking

	 * mainly at mode bits, and we make sure to ignore read-only

	 * filesystem checks

			/* the following error codes just mean the access was not allowed,

 simply don't "or" in the access bit. */

 CONFIG_NFSD_V3 */

/*

 * Open an existing file or directory.

 * The may_flags argument indicates the type of open (read/write/lock)

 * and additional flags.

 * N.B. After this call fhp needs an fh_put

 NOMEM or WOULDBLOCK */

	/*

	 * If we get here, then the client has already done an "open",

	 * and (hopefully) checked permission - so allow OWNER_OVERRIDE

	 * in case a chmod has now revoked permission.

	 *

	 * Arguably we should also allow the owner override for

	 * directories, but we never have and it doesn't seem to have

	 * caused anyone a problem.  If we were to change this, note

	 * also that our filldir callbacks would need a variant of

	 * lookup_one_len that doesn't check permissions.

/*

 * Grab and keep cached pages associated with a file in the svc_rqst

 * so that they can be passed to the network sendmsg/sendpage routines

 * directly. They will be released after the sending has completed.

/*

 * Gathered writes: If another process is currently writing to the file,

 * there's a high chance this is another nfsd (triggered by a bulk write

 * from a client's biod). Rather than syncing the file with each write

 * request, we sleep for 10 msec.

 *

 * I don't know if this roughly approximates C. Juszak's idea of

 * gathered writes, but it's a nice and simple solution (IMHO), and it

 * seems to work:-)

 *

 * Note: we do this only in the NFSv2 case, since v3 and higher have a

 * better tool (separate unstable writes and commits) for solving this

 * problem.

		/*

		 * We want throttling in balance_dirty_pages()

		 * and shrink_inactive_list() to only consider

		 * the backingdev we are writing to, so that nfs to

		 * localhost doesn't cause nfsd to lock up due to all

		 * the client's dirty pages or its congested queue.

/*

 * Read data from a file. count must contain the requested read count

 * on entry. On return, *count contains the number of bytes actually read.

 * N.B. After this call fhp needs an fh_put

/*

 * Write data to a file.

 * The stable flag requests synchronous writes.

 * N.B. After this call fhp needs an fh_put

/*

 * Commit all pending writes to stable storage.

 *

 * Note: we only guarantee that data that lies within the range specified

 * by the 'offset' and 'count' parameters will be synced.

 *

 * Unfortunately we cannot lock the file to make sure we return full WCC

 * data to the client, as locking happens lower down in the filesystem.

 CONFIG_NFSD_V3 */

	/*

	 * Mode has already been set earlier in create:

	/*

	 * Setting uid/gid works only for root.  Irix appears to

	 * send along the gid on create when it tries to implement

	 * setgid directories via NFS:

 Callers expect file metadata to be committed here */

/* HPUX client sometimes creates a file in mode 000, and sets size to 0.

 * setting size to 0 may fail for some specific file systems by the permission

 * checking which requires WRITE permission but the mode is 000.

 * we ignore the resizing(to 0) on the just new created file, since the size is

 * 0 after file created.

 *

 * call this only after vfs_create() is called.

 The parent directory should already be locked: */

	/*

	 * nfsd_create_setattr already committed the child.  Transactional

	 * filesystems had a chance to commit changes for both parent and

	 * child simultaneously making the following commit_metadata a

	 * noop.

	/*

	 * Update the file handle to get the new inode info.

/*

 * Create a filesystem object (regular, directory, special).

 * Note that the parent directory is left locked.

 *

 * N.B. Every call to nfsd_create needs an fh_put for _both_ fhp and resfhp

	/*

	 * We unconditionally drop our ref to dchild as fh_compose will have

	 * already grabbed its own ref for it.

/*

 * NFSv3 and NFSv4 version of nfsd_create

	/*

	 * Compose the response file handle.

 If file doesn't exist, check for permissions to create one */

		/* solaris7 gets confused (bugid 4218508) if these have

		 * the high bit set, as do xfs filesystems without the

		 * "bigtime" feature.  So just clear the high bits. If this is

		 * ever changed to use different attrs for storing the

		 * verifier, then do_open_lookup() will also need to be fixed

		 * accordingly.

				/* in nfsv4, we need to treat this case a little

				 * differently.  we don't want to truncate the

				 * file now; this would be wrong if the OPEN

				 * fails for some other reason.  furthermore,

				 * if the size is nonzero, we should ignore it

				 * according to spec!

 Cram the verifier into atime/mtime */

 XXX someone who knows this better please fix it for nsec */ 

	/*

	 * nfsd_create_setattr already committed the child

	 * (and possibly also the parent).

	/*

	 * Update the filehandle to get the new inode info.

 CONFIG_NFSD_V3 */

/*

 * Read a symlink. On entry, *lenp must contain the maximum path length that

 * fits into the buffer. On return, it contains the true length.

 * N.B. After this call fhp needs an fh_put

/*

 * Create a symlink and look up its inode

 * N.B. After this call _both_ fhp and resfhp need an fh_put

/*

 * Create a hardlink

 * N.B. After this call _both_ ffhp and tfhp need an fh_put

/*

 * Rename a file

 * N.B. After this call _both_ ffhp and tfhp need an fh_put

	/* cannot use fh_lock as we need deadlock protective ordering

	/*

	 * We cannot rely on fh_unlock on the two filehandles,

	 * as that would do the wrong thing if the two directories

	 * were the same, so again we do it by hand.

	/*

	 * If the target dentry has cached open files, then we need to try to

	 * close them prior to doing the rename. Flushing delayed fput

	 * shouldn't be done with locks held however, so we delay it until this

	 * point and then reattempt the whole shebang.

/*

 * Unlink a file or directory

 * N.B. After this call fhp needs an fh_put

 truncate the inode here */

		/* name is mounted-on. There is no perfect

		 * error status.

/*

 * We do this buffering because we must not call back into the file

 * system's ->lookup() method from the filldir callback. That may well

 * deadlock a number of file systems.

 *

 * This is based heavily on the implementation of same in XFS.

 will be cleared on successful read */

 We bailed out early */

/*

 * Read entries from a directory.

 * The  NFSv3/4 verifier we ignore for now.

 NFSv2 only supports 32 bit cookies */

 can still be found in ->err */

/*

 * Get file system stats

 * N.B. After this call fhp needs an fh_put

/*

 * Helper function to translate error numbers. In the case of xattr operations,

 * some error codes need to be translated outside of the standard translations.

 *

 * ENODATA needs to be translated to nfserr_noxattr.

 * E2BIG to nfserr_xattr2big.

 *

 * Additionally, vfs_listxattr can return -ERANGE. This means that the

 * file has too many extended attributes to retrieve inside an

 * XATTR_LIST_MAX sized buffer. This is a bug in the xattr implementation:

 * filesystems will allow the adding of extended attributes until they hit

 * their own internal limit. This limit may be larger than XATTR_LIST_MAX.

 * So, at that point, the attributes are present and valid, but can't

 * be retrieved using listxattr, since the upper level xattr code enforces

 * the XATTR_LIST_MAX limit.

 *

 * This bug means that we need to deal with listxattr returning -ERANGE. The

 * best mapping is to return TOOSMALL.

/*

 * Retrieve the specified user extended attribute. To avoid always

 * having to allocate the maximum size (since we are not getting

 * a maximum size from the RPC), do a probe + alloc. Hold a reader

 * lock on i_rwsem to prevent the extended attribute from changing

 * size while we're doing this.

	/*

	 * Zero-length attribute, just return.

/*

 * Retrieve the xattr names. Since we can't know how many are

 * user extended attributes, we must get all attributes here,

 * and have the XDR encode filter out the "user." ones.

 *

 * While this could always just allocate an XATTR_LIST_MAX

 * buffer, that's a waste, so do a probe + allocate. To

 * avoid any changes between the probe and allocate, wrap

 * this in inode_lock.

	/*

	 * We're holding i_rwsem - use GFP_NOFS.

/*

 * Removexattr and setxattr need to call fh_lock to both lock the inode

 * and set the change attribute. Since the top-level vfs_removexattr

 * and vfs_setxattr calls already do their own inode_lock calls, call

 * the _locked variant. Pass in a NULL pointer for delegated_inode,

 * and let the client deal with NFS4ERR_DELAY (same as with e.g.

 * setattr and remove).

/*

 * Check for a user's access permissions to this inode.

	/* Normally we reject any write/sattr etc access on a read-only file

	 * system.  But if it is IRIX doing check on write-access for a 

	 * device special file, we ignore rofs.

 (acc & NFSD_MAY_WRITE) && */ IS_IMMUTABLE(inode))

		/* If we cannot rely on authentication in NLM requests,

		 * just allow locks, otherwise require read permission, or

		 * ownership

	/*

	 * The file owner always gets access permission for accesses that

	 * would normally be checked at open time. This is to make

	 * file access work even when the client has done a fchmod(fd, 0).

	 *

	 * However, `cp foo bar' should fail nevertheless when bar is

	 * readonly. A sensible way to do this might be to reject all

	 * attempts to truncate a read-only file, because a creat() call

	 * always implies file truncation.

	 * ... but this isn't really fair.  A process may reasonably call

	 * ftruncate on an open file descriptor on a file with perm 000.

	 * We must trust the client to do permission checking - using "ACCESS"

	 * with NFSv3.

 This assumes  NFSD_MAY_{READ,WRITE,EXEC} == MAY_{READ,WRITE,EXEC} */

 Allow read access to binaries even when mode 111 */

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains all the stubs needed when communicating with lockd.

 * This level of indirection is necessary so we can run nfsd+lockd without

 * requiring the nfs client to be compiled in/loaded, and vice versa.

 *

 * Copyright (C) 1996, Olaf Kirch <okir@monad.swb.de>

/*

 * Note: we hold the dentry use count while the file is open.

 must initialize before using! but maxsize doesn't matter */

 	/* We return nlm error codes as nlm doesn't know

	 * about nfsd, but nfsd does know about nlm..

 open file for locking */

 close file */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Syscall interface to knfsd.

 *

 * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>

/*

 *	We have a single directory with several nodes in it.

	/*

	 * The below MUST come last.  Otherwise we leave a hole in nfsd_files[]

	 * with !CONFIG_NFSD_V4 and simple_fill_super() goes oops

/*

 * write() for these nodes.

		/* An attempt to read a transaction file without writing

		 * causes a 0-byte write so that the file can return

		 * state information

 CONFIG_SUNRPC_GSS or CONFIG_SUNRPC_GSS_MODULE */

----------------------------------------------------------------------------*/

/*

 * payload - write methods

/*

 * write_unlock_ip - Release all locks used by a client

 *

 * Experimental.

 *

 * Input:

 *			buf:	'\n'-terminated C string containing a

 *				presentation format IP address

 *			size:	length of C string in @buf

 * Output:

 *	On success:	returns zero if all specified locks were released;

 *			returns one if one or more locks were not released

 *	On error:	return code is negative errno value

 sanity check */

/*

 * write_unlock_fs - Release all locks on a local file system

 *

 * Experimental.

 *

 * Input:

 *			buf:	'\n'-terminated C string containing the

 *				absolute pathname of a local file system

 *			size:	length of C string in @buf

 * Output:

 *	On success:	returns zero if all specified locks were released;

 *			returns one if one or more locks were not released

 *	On error:	return code is negative errno value

 sanity check */

	/*

	 * XXX: Needs better sanity checking.  Otherwise we could end up

	 * releasing locks on the wrong file system.

	 *

	 * For example:

	 * 1.  Does the path refer to a directory?

	 * 2.  Is that directory a mount point, or

	 * 3.  Is that directory the root of an exported file system?

/*

 * write_filehandle - Get a variable-length NFS file handle by path

 *

 * On input, the buffer contains a '\n'-terminated C string comprised of

 * three alphanumeric words separated by whitespace.  The string may

 * contain escape sequences.

 *

 * Input:

 *			buf:

 *				domain:		client domain name

 *				path:		export pathname

 *				maxsize:	numeric maximum size of

 *						@buf

 *			size:	length of C string in @buf

 * Output:

 *	On success:	passed-in buffer filled with '\n'-terminated C

 *			string containing a ASCII hex text version

 *			of the NFS file handle;

 *			return code is the size in bytes of the string

 *	On error:	return code is negative errno value

 we have all the words, they are in buf.. */

/*

 * write_threads - Start NFSD, or report the current number of running threads

 *

 * Input:

 *			buf:		ignored

 *			size:		zero

 * Output:

 *	On success:	passed-in buffer filled with '\n'-terminated C

 *			string numeric value representing the number of

 *			running NFSD threads;

 *			return code is the size in bytes of the string

 *	On error:	return code is zero

 *

 * OR

 *

 * Input:

 *			buf:		C string containing an unsigned

 *					integer value representing the

 *					number of NFSD threads to start

 *			size:		non-zero length of C string in @buf

 * Output:

 *	On success:	NFS service is started;

 *			passed-in buffer filled with '\n'-terminated C

 *			string numeric value representing the number of

 *			running NFSD threads;

 *			return code is the size in bytes of the string

 *	On error:	return code is zero or a negative errno value

/*

 * write_pool_threads - Set or report the current number of threads per pool

 *

 * Input:

 *			buf:		ignored

 *			size:		zero

 *

 * OR

 *

 * Input:

 * 			buf:		C string containing whitespace-

 * 					separated unsigned integer values

 *					representing the number of NFSD

 *					threads to start in each pool

 *			size:		non-zero length of C string in @buf

 * Output:

 *	On success:	passed-in buffer filled with '\n'-terminated C

 *			string containing integer values representing the

 *			number of NFSD threads in each pool;

 *			return code is the size in bytes of the string

 *	On error:	return code is zero or a negative errno value

	/* if size > 0, look for an array of number of threads per node

	 * and apply them  then write out number of threads per node as reply

		/*

		 * NFS is shut down.  The admin can start it by

		 * writing to the threads file but NOT the pool_threads

		 * file, sorry.  Report zero threads.

 fewer numbers than pools */

 syntax error */

		/*

		 * special case for backward compatability.

		 * +4.0 is never reported, it is implied by

		 * +4, unless -4.0 is present.

			/* Cannot change versions without updating

			 * nn->nfsd_serv->sv_xdrsize, and reallocing

			 * rq_argp and rq_resp

					/*

					 * Either we have +4 and no minors are enabled,

					 * or we have -4 and at least one minor is enabled.

					 * In either case, propagate 'cmd' to all minors.

		/* If all get turned off, turn them back on, as

		 * having no versions is BAD

 Now write current state into reply buffer */

/*

 * write_versions - Set or report the available NFS protocol versions

 *

 * Input:

 *			buf:		ignored

 *			size:		zero

 * Output:

 *	On success:	passed-in buffer filled with '\n'-terminated C

 *			string containing positive or negative integer

 *			values representing the current status of each

 *			protocol version;

 *			return code is the size in bytes of the string

 *	On error:	return code is zero or a negative errno value

 *

 * OR

 *

 * Input:

 * 			buf:		C string containing whitespace-

 * 					separated positive or negative

 * 					integer values representing NFS

 * 					protocol versions to enable ("+n")

 * 					or disable ("-n")

 *			size:		non-zero length of C string in @buf

 * Output:

 *	On success:	status of zero or more protocol versions has

 *			been updated; passed-in buffer filled with

 *			'\n'-terminated C string containing positive

 *			or negative integer values representing the

 *			current status of each protocol version;

 *			return code is the size in bytes of the string

 *	On error:	return code is zero or a negative errno value

/*

 * Zero-length write.  Return a list of NFSD's current listener

 * transports.

/*

 * A single 'fd' number was written, in which case it must be for

 * a socket of a supported family/protocol, and we use it as an

 * nfsd listener.

 Decrease the count, but don't shut down the service */

/*

 * A transport listener is added by writing it's transport name and

 * a port number.

 Decrease the count, but don't shut down the service */

/*

 * write_ports - Pass a socket file descriptor or transport name to listen on

 *

 * Input:

 *			buf:		ignored

 *			size:		zero

 * Output:

 *	On success:	passed-in buffer filled with a '\n'-terminated C

 *			string containing a whitespace-separated list of

 *			named NFSD listeners;

 *			return code is the size in bytes of the string

 *	On error:	return code is zero or a negative errno value

 *

 * OR

 *

 * Input:

 *			buf:		C string containing an unsigned

 *					integer value representing a bound

 *					but unconnected socket that is to be

 *					used as an NFSD listener; listen(3)

 *					must be called for a SOCK_STREAM

 *					socket, otherwise it is ignored

 *			size:		non-zero length of C string in @buf

 * Output:

 *	On success:	NFS service is started;

 *			passed-in buffer filled with a '\n'-terminated C

 *			string containing a unique alphanumeric name of

 *			the listener;

 *			return code is the size in bytes of the string

 *	On error:	return code is a negative errno value

 *

 * OR

 *

 * Input:

 *			buf:		C string containing a transport

 *					name and an unsigned integer value

 *					representing the port to listen on,

 *					separated by whitespace

 *			size:		non-zero length of C string in @buf

 * Output:

 *	On success:	returns zero; NFS service is started

 *	On error:	return code is a negative errno value

/*

 * write_maxblksize - Set or report the current NFS blksize

 *

 * Input:

 *			buf:		ignored

 *			size:		zero

 *

 * OR

 *

 * Input:

 * 			buf:		C string containing an unsigned

 * 					integer value representing the new

 * 					NFS blksize

 *			size:		non-zero length of C string in @buf

 * Output:

 *	On success:	passed-in buffer filled with '\n'-terminated C string

 *			containing numeric value of the current NFS blksize

 *			setting;

 *			return code is the size in bytes of the string

 *	On error:	return code is zero or a negative errno value

		/* force bsize into allowed range and

		 * required alignment.

/*

 * write_maxconn - Set or report the current max number of connections

 *

 * Input:

 *			buf:		ignored

 *			size:		zero

 * OR

 *

 * Input:

 * 			buf:		C string containing an unsigned

 * 					integer value representing the new

 * 					number of max connections

 *			size:		non-zero length of C string in @buf

 * Output:

 *	On success:	passed-in buffer filled with '\n'-terminated C string

 *			containing numeric value of max_connections setting

 *			for this net namespace;

 *			return code is the size in bytes of the string

 *	On error:	return code is zero or a negative errno value

		/*

		 * Some sanity checking.  We don't have a reason for

		 * these particular numbers, but problems with the

		 * extremes are:

		 *	- Too short: the briefest network outage may

		 *	  cause clients to lose all their locks.  Also,

		 *	  the frequent polling may be wasteful.

		 *	- Too long: do you really want reboot recovery

		 *	  to take more than an hour?  Or to make other

		 *	  clients wait an hour before being able to

		 *	  revoke a dead client's locks?

/*

 * write_leasetime - Set or report the current NFSv4 lease time

 *

 * Input:

 *			buf:		ignored

 *			size:		zero

 *

 * OR

 *

 * Input:

 *			buf:		C string containing an unsigned

 *					integer value representing the new

 *					NFSv4 lease expiry time

 *			size:		non-zero length of C string in @buf

 * Output:

 *	On success:	passed-in buffer filled with '\n'-terminated C

 *			string containing unsigned integer value of the

 *			current lease expiry time;

 *			return code is the size in bytes of the string

 *	On error:	return code is zero or a negative errno value

/*

 * write_gracetime - Set or report current NFSv4 grace period time

 *

 * As above, but sets the time of the NFSv4 grace period.

 *

 * Note this should never be set to less than the *previous*

 * lease-period time, but we don't try to enforce this.  (In the common

 * case (a new boot), we don't know what the previous lease time was

 * anyway.)

/*

 * write_recoverydir - Set or report the pathname of the recovery directory

 *

 * Input:

 *			buf:		ignored

 *			size:		zero

 *

 * OR

 *

 * Input:

 *			buf:		C string containing the pathname

 *					of the directory on a local file

 *					system containing permanent NFSv4

 *					recovery data

 *			size:		non-zero length of C string in @buf

 * Output:

 *	On success:	passed-in buffer filled with '\n'-terminated C string

 *			containing the current recovery pathname setting;

 *			return code is the size in bytes of the string

 *	On error:	return code is zero or a negative errno value

/*

 * write_v4_end_grace - release grace period for nfsd's v4.x lock manager

 *

 * Input:

 *			buf:		ignored

 *			size:		zero

 * OR

 *

 * Input:

 * 			buf:		any value

 *			size:		non-zero length of C string in @buf

 * Output:

 *			passed-in buffer filled with "Y" or "N" with a newline

 *			and NULL-terminated C string. This indicates whether

 *			the grace period has ended in the current net

 *			namespace. Return code is the size in bytes of the

 *			string. Writing a string that starts with 'Y', 'y', or

 *			'1' to the file will end the grace period for nfsd's v4

 *			lock manager.

----------------------------------------------------------------------------*/

/*

 *	populating the filesystem.

 Basically copying rpc_get_inode. */

 Following advice from simple_fill_super documentation: */

 from __rpc_unlink */

 I think this can't happen? */

/* XXX: cut'n'paste from simple_fill_super; figure out if we could share

 on success, returns positive number unique to that client. */

 XXX: tossing errors? */

 Taken from __rpc_rmdir: */

 Per-export io stats use same ops as exports file */

 CONFIG_SUNRPC_GSS or CONFIG_SUNRPC_GSS_MODULE */

 last one */ {""}

 CONFIG_PROC_FS */

 default lease time */

 Statistics */

 lockd->nfsd callbacks */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2016 Tom Haynes <loghyr@primarydata.com>

 *

 * The following implements a super-simple flex-file server

 * where the NFSv4.1 mds is also the ds. And the storage is

 * the same. I.e., writing to the mds via a NFSv4.1 WRITE

 * goes to the same location as the NFSv3 WRITE.

	/*

	 * The super simple flex file server has 1 mirror, 1 data server,

	 * and 1 file handle. So instead of 4 allocs, do 1 for now.

	 * Zero it out for the stateid - don't want junk in there!

	/*

	 * Avoid layout commit, try to force the I/O to the DS,

	 * and for fun, cause all IOMODE_RW layout segments to

	 * effectively be WRITE only.

 Do not allow a IOMODE_READ segment to have write pemissions */

 Give whole file layout segments */

 SPDX-License-Identifier: GPL-2.0

/*

 * XDR support for nfsd

 *

 * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>

/*

 * Mapping of S_IF* types to NFS file types

/*

 * Basic NFSv2 data types (RFC 1094 Section 2.3)

/**

 * svcxdr_encode_stat - Encode an NFSv2 status code

 * @xdr: XDR stream

 * @status: status value to encode

 *

 * Return values:

 *   %false: Send buffer space was exhausted

 *   %true: Success

/**

 * svcxdr_decode_fhandle - Decode an NFSv2 file handle

 * @xdr: XDR stream positioned at an encoded NFSv2 FH

 * @fhp: OUT: filled-in server file handle

 *

 * Return values:

 *  %false: The encoded file handle was not valid

 *  %true: @fhp has been initialized

	/*

	 * Some Sun clients put 0xffff in the mode field when they

	 * mean 0xffffffff.

		/*

		 * Passing the invalid value useconds=1000000 for mtime

		 * is a Sun convention for "set both mtime and atime to

		 * current server time".  It's needed to make permissions

		 * checks for the "touch" program across v2 mounts to

		 * Solaris and Irix boxes work correctly. See description of

		 * sattr in section 6.1 of "NFS Illustrated" by

		 * Brent Callaghan, Addison-Wesley, ISBN 0-201-32750-5

/**

 * svcxdr_encode_fattr - Encode NFSv2 file attributes

 * @rqstp: Context of a completed RPC transaction

 * @xdr: XDR stream

 * @fhp: File handle to encode

 * @stat: Attributes to encode

 *

 * Return values:

 *   %false: Send buffer space was exhausted

 *   %true: Success

/*

 * XDR decode functions

 totalcount is ignored */

 beginoffset is ignored */

 totalcount is ignored */

 opaque data */

/*

 * XDR encode functions

 no more entries */

/**

 * nfssvc_encode_nfscookie - Encode a directory offset cookie

 * @resp: readdir result context

 * @offset: offset cookie to encode

 *

 * The buffer space for the offset cookie has already been reserved

 * by svcxdr_encode_entry_common().

 fileid */

 name */

 cookie */

/**

 * nfssvc_encode_entry - encode one NFSv2 READDIR entry

 * @data: directory context

 * @name: name of the object to be encoded

 * @namlen: length of that name, in bytes

 * @offset: the offset of the previous entry

 * @ino: the fileid of this entry

 * @d_type: unused

 *

 * Return values:

 *   %0: Entry was successfully encoded.

 *   %-EINVAL: An encoding problem occured, secondary status code in resp->common.err

 *

 * On exit, the following fields are updated:

 *   - resp->xdr

 *   - resp->common.err

 *   - resp->cookie_offset

 The offset cookie for the previous entry */

/*

 * XDR release functions

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2016 Tom Haynes <loghyr@primarydata.com>

	/*

	 * Unlike nfsd4_encode_user, we know these will

	 * always be stringified.

 8 + len for recording the length, name, and padding */

 The layout segment */

 stripe unit of 1 */

 single mirror */

 single data server */

 efficiency */

 single file handle */

 No stats collect hint */

 len + padding for two strings */

	/*

	 * Fill in the overall length and number of volumes at the beginning

	 * of the layout.

 1 netaddr */

 1 versions */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2014 Christoph Hellwig.

 pNFS device ID to export fsid mapping */

 retain the whole layout segment on a split. */

		/*

		 * Anything left? If not, then call it done. Note that we don't

		 * take the spinlock since this is an optimization and nothing

		 * should get added until the cb counter goes to zero.

 Poll the client until it's done with the layout */

 Client gets 2 lease periods to return it */

 10 mili-seconds */

		/*

		 * Unknown error or non-responding client, we'll need to fence.

	/*

	 * We don't want the locks code to timeout the lease for us;

	 * we'll remove it ourself if a layout isn't returned

	 * in time:

/*

 *  Common NFSv4 ACL handling code.

 *

 *  Copyright (c) 2002, 2003 The Regents of the University of Michigan.

 *  All rights reserved.

 *

 *  Marius Aamodt Eriksen <marius@umich.edu>

 *  Jeff Sedlak <jsedlak@umich.edu>

 *  J. Bruce Fields <bfields@umich.edu>

 *

 *  Redistribution and use in source and binary forms, with or without

 *  modification, are permitted provided that the following conditions

 *  are met:

 *

 *  1. Redistributions of source code must retain the above copyright

 *     notice, this list of conditions and the following disclaimer.

 *  2. Redistributions in binary form must reproduce the above copyright

 *     notice, this list of conditions and the following disclaimer in the

 *     documentation and/or other materials provided with the distribution.

 *  3. Neither the name of the University nor the names of its

 *     contributors may be used to endorse or promote products derived

 *     from this software without specific prior written permission.

 *

 *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

 *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE

 *  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE

 *  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR

 *  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF

 *  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR

 *  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF

 *  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING

 *  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS

 *  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 mode bit translations: */

 flags used to simulate posix default ACLs */

/* XXX: modify functions to return NFS errors; they're only ever

/* We only map from NFSv4 to POSIX ACLs when setting ACLs, when we err on the

 * side of being more restrictive, so the mode bit mapping below is

 * pessimistic.  An optimistic version would be needed to handle DENY's,

 * but we expect to coalesce all ALLOWs and DENYs before mapping to mode

 allocate for worst case: one (deny, allow) pair each: */

	/*

	 * Only pas.users and pas.groups need initialization; previous

	 * posix_acl_valid() calls ensure that the other fields will be

	 * initialized in the following loop.  But, just to placate gcc:

 We'll only care about effective permissions: */

 We assume the acl has been verified with posix_acl_valid. */

 We could deny everything not granted by the owner: */

	/*

	 * but it is equivalent (and simpler) to deny only what is not

	 * granted by later entries:

	/* In the case of groups, we apply allow ACEs first, then deny ACEs,

 allow ACEs */

 deny ACEs */

	/* We just do a bubble sort; easy to do in place, and we're not

	/* posix_acl_valid requires that users and groups be in order

 no users or groups */

/*

 * While processing the NFSv4 ACE, this maintains bitmasks representing

 * which permission bits have been allowed and which denied to a given

/*

 * While processing the NFSv4 ACE, this maintains the partial permissions

 Deny unused in this case */

	/*

	 * In the worst case, each individual acl could be for a distinct

	 * named user or group, but we don't know which, so we allocate

	 * enough space for either:

	/*

	 * ACLs with no ACEs are treated differently in the inheritable

	 * and effective cases: when there are no inheritable ACEs,

	 * calls ->set_acl with a NULL ACL structure.

	/*

	 * When there are no effective ACEs, the following will end

	 * up setting a 3-element effective posix ACL with all

	 * permissions zero.

 Note we also include a MASK ACE in this case: */

 Allow all bits in the mask not already denied: */

 Deny all bits in the mask not already allowed: */

 Not found: */

 Not found: */

		/*

		 * Note that when only one of FILE_INHERIT or DIRECTORY_INHERIT

		 * is set, we're effectively turning on the other.  That's OK,

		 * according to rfc 3530.

 Get inode */

/*

 * return the size of the struct nfs4_acl required to represent an acl

 * with @entries entries.

 SPDX-License-Identifier: GPL-2.0

/*

 * Request reply cache. This is currently a global cache, but this may

 * change in the future and be a per-client cache.

 *

 * This code is heavily inspired by the 44BSD implementation, although

 * it does things a bit differently.

 *

 * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>

/*

 * We use this value to determine the number of hash buckets from the max

 * cache size, the idea being that when the cache is at its maximum number

 * of entries, then this should be the average number of entries per bucket.

/*

 * Put a cap on the size of the DRC based on the amount of available

 * low memory in the machine.

 *

 *  64MB:    8192

 * 128MB:   11585

 * 256MB:   16384

 * 512MB:   23170

 *   1GB:   32768

 *   2GB:   46340

 *   4GB:   65536

 *   8GB:   92681

 *  16GB:  131072

 *

 * ...with a hard cap of 256k entries. In the worst case, each entry will be

 * ~1k, so the above numbers should give a rough max of the amount of memory

 * used in k.

 *

 * XXX: these limits are per-container, so memory used will increase

 * linearly with number of containers.  Maybe that's OK.

/*

 * Compute the number of hash buckets we need. Divide the max cachesize by

 * the "target" max bucket size, and round up to next power of two.

/*

 * Move cache entry to end of LRU list, and queue the cleaner to run if it's

 * not already scheduled.

		/*

		 * Don't free entries attached to calls that are still

		 * in-progress, but do keep scanning the list.

/*

 * Walk the LRU list and prune off entries that are older than RC_EXPIRE.

 * Also prune the oldest ones when the total exceeds the max number of entries.

/*

 * Walk an xdr_buf and get a CRC for at most the first RC_CSUMLEN bytes

 rq_arg.head first */

 Continue into page array */

/*

 * Search the request hash for an entry that matches the given rqstp.

 * Must be called with cache_lock held. Returns the found entry or

 * inserts an empty key on failure.

 tally hash chain length stats */

 prefer to keep the smallest cachesize possible here */

/**

 * nfsd_cache_lookup - Find an entry in the duplicate reply cache

 * @rqstp: Incoming Call to find

 *

 * Try to find an entry matching the current call in the cache. When none

 * is found, we try to grab the oldest expired entry off the LRU list. If

 * a suitable one isn't there, then drop the cache_lock and allocate a

 * new one, then search again in case one got inserted while this thread

 * didn't hold the lock.

 *

 * Return values:

 *   %RC_DOIT: Process the request normally

 *   %RC_REPLY: Reply from cache

 *   %RC_DROPIT: Do not process the request further

	/*

	 * Since the common case is a cache miss followed by an insert,

	 * preallocate an entry.

 We found a matching entry which is either in progress or done. */

 Request being processed */

	/* From the hall of fame of impractical attacks:

 Compose RPC reply header */

 should not happen */

/**

 * nfsd_cache_update - Update an entry in the duplicate reply cache.

 * @rqstp: svc_rqst with a finished Reply

 * @cachetype: which cache to update

 * @statp: Reply's status code

 *

 * This is called from nfsd_dispatch when the procedure has been

 * executed and the complete reply is in rqstp->rq_res.

 *

 * We're copying around data here rather than swapping buffers because

 * the toplevel loop requires max-sized buffers, which would be a waste

 * of memory for a cache with a max reply size of 100 bytes (diropokres).

 *

 * If we should start to use different types of cache entries tailored

 * specifically for attrstat and fh's, we may save even more space.

 *

 * Also note that a cachetype of RC_NOCACHE can legally be passed when

 * nfsd failed to encode a reply that otherwise would have been cached.

 * In this case, nfsd_cache_update is called with statp == NULL.

 Don't cache excessive amounts of data and XDR failures */

/*

 * Copy cached reply to current reply buffer. Should always fit.

 * FIXME as reply is in a page, we should just attach the page, and

 * keep a refcount....

/*

 * Note that fields may be added, removed or reordered in the future. Programs

 * scraping this file for info should test the labels to ensure they're

 * getting the correct field.

 SPDX-License-Identifier: GPL-2.0

/*

 * XDR support for nfsd/protocol version 3.

 *

 * Copyright (C) 1995, 1996, 1997 Olaf Kirch <okir@monad.swb.de>

 *

 * 2003-08-09 Jamie Lokier: Use htonl() for nanoseconds, not htons()!

/*

 * Force construction of an empty post-op attr

/*

 * time_delta. {1, 0} means the server is accurate only

 * to the nearest second.

/*

 * Mapping of S_IF* types to NFS file types

/*

 * Basic NFSv3 data types (RFC 1813 Sections 2.5 and 2.6)

/**

 * svcxdr_decode_nfs_fh3 - Decode an NFSv3 file handle

 * @xdr: XDR stream positioned at an undecoded NFSv3 FH

 * @fhp: OUT: filled-in server file handle

 *

 * Return values:

 *  %false: The encoded file handle was not valid

 *  %true: @fhp has been initialized

/**

 * svcxdr_encode_nfsstat3 - Encode an NFSv3 status code

 * @xdr: XDR stream

 * @status: status value to encode

 *

 * Return values:

 *   %false: Send buffer space was exhausted

 *   %true: Success

 used */

 rdev */

 fileid */

/**

 * svcxdr_encode_post_op_attr - Encode NFSv3 post-op attributes

 * @rqstp: Context of a completed RPC transaction

 * @xdr: XDR stream

 * @fhp: File handle to encode

 *

 * Return values:

 *   %false: Send buffer space was exhausted

 *   %true: Success

	/*

	 * The inode may be NULL if the call failed because of a

	 * stale file handle. In this case, no attributes are

	 * returned.

/*

 * Encode weak cache consistency data

 before */

 after */

/*

 * Fill in the pre_op attr for the wcc data

 Grab the times from inode anyway */

/*

 * Fill in the post_op attr for the wcc data

/*

 * XDR decode functions

 opaque data */

 request sanity */

 request sanity */

 Valid XDR but illegal file types */

 dircount is ignored */

/*

 * XDR encode functions

 GETATTR */

 SETATTR, REMOVE, RMDIR */

 LOOKUP */

 ACCESS */

 READLINK */

 READ */

 WRITE */

 CREATE, MKDIR, SYMLINK, MKNOD */

 RENAME */

 LINK */

 READDIR */

 no more entries */

			/*

			 * Don't return filehandle for ".." if we're at

			 * the filesystem or export root:

/**

 * nfs3svc_encode_cookie3 - Encode a directory offset cookie

 * @resp: readdir result context

 * @offset: offset cookie to encode

 *

 * The buffer space for the offset cookie has already been reserved

 * by svcxdr_encode_entry3_common().

 fileid */

 name */

 cookie */

/**

 * nfs3svc_encode_entry3 - encode one NFSv3 READDIR entry

 * @data: directory context

 * @name: name of the object to be encoded

 * @namlen: length of that name, in bytes

 * @offset: the offset of the previous entry

 * @ino: the fileid of this entry

 * @d_type: unused

 *

 * Return values:

 *   %0: Entry was successfully encoded.

 *   %-EINVAL: An encoding problem occured, secondary status code in resp->common.err

 *

 * On exit, the following fields are updated:

 *   - resp->xdr

 *   - resp->common.err

 *   - resp->cookie_offset

 The offset cookie for the previous entry */

/**

 * nfs3svc_encode_entryplus3 - encode one NFSv3 READDIRPLUS entry

 * @data: directory context

 * @name: name of the object to be encoded

 * @namlen: length of that name, in bytes

 * @offset: the offset of the previous entry

 * @ino: the fileid of this entry

 * @d_type: unused

 *

 * Return values:

 *   %0: Entry was successfully encoded.

 *   %-EINVAL: An encoding problem occured, secondary status code in resp->common.err

 *

 * On exit, the following fields are updated:

 *   - resp->xdr

 *   - resp->common.err

 *   - resp->cookie_offset

 The offset cookie for the previous entry */

 total bytes */

 free bytes */

 user available bytes */

 total inodes */

 free inodes */

 user available inodes */

 mean unchanged time */

 FSSTAT */

 FSINFO */

 PATHCONF */

 COMMIT */

/*

 * XDR release functions

 SPDX-License-Identifier: GPL-2.0

/*

 * Process version 3 NFS requests.

 *

 * Copyright (C) 1996, 1997, 1998 Olaf Kirch <okir@monad.swb.de>

 NF3NON */

 NF3REG */

 NF3DIR */

 NF3BLK */

 NF3CHR */

 NF3LNK */

 NF3SOCK */

 NF3FIFO */

/*

 * NULL call.

/*

 * Get a file's attributes

/*

 * Set a file's attributes

/*

 * Look up a path name component

/*

 * Check file access

/*

 * Read a symlink.

 Read the symlink. */

/*

 * Read a portion of a file.

	/* Obtain buffer pointer for payload.

	 * 1 (status) + 22 (post_op_attr) + 1 (count) + 1 (eof)

	 * + 1 (xdr opaque byte count) = 26

/*

 * Write data to a file

/*

 * With NFSv3, CREATE processing is a lot easier than with NFSv2.

 * At least in theory; we'll see how it fares in practice when the

 * first reports about SunOS compatibility problems start to pour in...

 Unfudge the mode bits */

 Now create the file and set attributes */

/*

 * Make directory. This operation is not idempotent.

/*

 * Make socket/fifo/device.

/*

 * Remove file/fifo/socket etc.

 Unlink. -S_IFDIR means file must not be a directory */

/*

 * Remove a directory

 Reserve room for the NULL ptr & eof flag (-2 words) */

	/* This is xdr_init_encode(), but it assumes that

/*

 * Read a portion of a directory.

 Recycle only pages that were part of the reply */

/*

 * Read a portion of a directory, including file handles and attrs.

 * For now, we choose to ignore the dircount parameter.

 Recycle only pages that were part of the reply */

/*

 * Get file system stats

/*

 * Get file system info

	/* Check special features of the file system. May request

	 * different read/write sizes for file systems known to have

 Note that we don't care for remote fs's here */

/*

 * Get pathconf info for the specified file

 Set default pathconf */

 at least */

 at least */

 Note that we don't care for remote fs's here */

/*

 * Commit a file (range) to stable storage.

/*

 * NFSv3 Server procedures.

 * Only the results of non-idempotent operations are cached.

 status*/

 filehandle with length */

 attributes */

 post attributes - conditional */

 WCC attributes */

/*

 *  Copyright (c) 2001 The Regents of the University of Michigan.

 *  All rights reserved.

 *

 *  Kendrick Smith <kmsmith@umich.edu>

 *  Andy Adamson <andros@umich.edu>

 *

 *  Redistribution and use in source and binary forms, with or without

 *  modification, are permitted provided that the following conditions

 *  are met:

 *

 *  1. Redistributions of source code must retain the above copyright

 *     notice, this list of conditions and the following disclaimer.

 *  2. Redistributions in binary form must reproduce the above copyright

 *     notice, this list of conditions and the following disclaimer in the

 *     documentation and/or other materials provided with the distribution.

 *  3. Neither the name of the University nor the names of its

 *     contributors may be used to endorse or promote products derived

 *     from this software without specific prior written permission.

 *

 *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

 *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE

 *  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE

 *  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR

 *  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF

 *  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR

 *  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF

 *  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING

 *  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS

 *  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 Index of predefined Linux callback client operations */

 args */

 minorversion 0 only */

 res */

/*

 * Encode/decode NFSv4 CB basic data types

 *

 * Basic NFSv4 callback data types are defined in section 15 of RFC

 * 3530: "Network File System (NFS) version 4 Protocol" and section

 * 20 of RFC 5661: "Network File System (NFS) Version 4 Minor Version

 * 1 Protocol"

/*

 *	nfs_cb_opnum4

 *

 *	enum nfs_cb_opnum4 {

 *		OP_CB_GETATTR		= 3,

 *		  ...

 *	};

/*

 * nfs_fh4

 *

 *	typedef opaque nfs_fh4<NFS4_FHSIZE>;

/*

 * stateid4

 *

 *	struct stateid4 {

 *		uint32_t	seqid;

 *		opaque		other[12];

 *	};

/*

 * sessionid4

 *

 *	typedef opaque sessionid4[NFS4_SESSIONID_SIZE];

/*

 * nfsstat4

/*

 * If we cannot translate the error, the recovery routines should

 * handle it.

 *

 * Note: remaining NFSv4 error codes have values > 10000, so should

 * not conflict with native Linux error codes.

/*

 * CB_COMPOUND4args

 *

 *	struct CB_COMPOUND4args {

 *		utf8str_cs	tag;

 *		uint32_t	minorversion;

 *		uint32_t	callback_ident;

 *		nfs_cb_argop4	argarray<>;

 *	};

 empty tag */

 argarray element count */

/*

 * Update argarray element count

/*

 * CB_COMPOUND4res

 *

 *	struct CB_COMPOUND4res {

 *		nfsstat4	status;

 *		utf8str_cs	tag;

 *		nfs_cb_resop4	resarray<>;

 *	};

 Ignore the tag */

/*

 * CB_RECALL4args

 *

 *	struct CB_RECALL4args {

 *		stateid4	stateid;

 *		bool		truncate;

 *		nfs_fh4		fh;

 *	};

 truncate */

/*

 * CB_SEQUENCE4args

 *

 *	struct CB_SEQUENCE4args {

 *		sessionid4		csa_sessionid;

 *		sequenceid4		csa_sequenceid;

 *		slotid4			csa_slotid;

 *		slotid4			csa_highest_slotid;

 *		bool			csa_cachethis;

 *		referring_call_list4	csa_referring_call_lists<>;

 *	};

 csa_sequenceid */

 csa_slotid */

 csa_highest_slotid */

 csa_cachethis */

 csa_referring_call_lists */

/*

 * CB_SEQUENCE4resok

 *

 *	struct CB_SEQUENCE4resok {

 *		sessionid4	csr_sessionid;

 *		sequenceid4	csr_sequenceid;

 *		slotid4		csr_slotid;

 *		slotid4		csr_highest_slotid;

 *		slotid4		csr_target_highest_slotid;

 *	};

 *

 *	union CB_SEQUENCE4res switch (nfsstat4 csr_status) {

 *	case NFS4_OK:

 *		CB_SEQUENCE4resok	csr_resok4;

 *	default:

 *		void;

 *	};

 *

 * Our current back channel implmentation supports a single backchannel

 * with a single slot.

	/*

	 * If the server returns different values for sessionID, slotID or

	 * sequence number, the server is looney tunes.

	/*

	 * FIXME: process highest slotid and target highest slotid

/*

 * NFSv4.0 and NFSv4.1 XDR encode functions

 *

 * NFSv4.0 callback argument types are defined in section 15 of RFC

 * 3530: "Network File System (NFS) version 4 Protocol" and section 20

 * of RFC 5661:  "Network File System (NFS) Version 4 Minor Version 1

 * Protocol".

/*

 * NB: Without this zero space reservation, callbacks over krb5p fail

/*

 * 20.2. Operation 4: CB_RECALL - Recall a Delegation

/*

 * NFSv4.0 and NFSv4.1 XDR decode functions

 *

 * NFSv4.0 callback result types are defined in section 15 of RFC

 * 3530: "Network File System (NFS) version 4 Protocol" and section 20

 * of RFC 5661:  "Network File System (NFS) Version 4 Minor Version 1

 * Protocol".

/*

 * 20.2. Operation 4: CB_RECALL - Recall a Delegation

/*

 * CB_LAYOUTRECALL4args

 *

 *	struct layoutrecall_file4 {

 *		nfs_fh4         lor_fh;

 *		offset4         lor_offset;

 *		length4         lor_length;

 *		stateid4        lor_stateid;

 *	};

 *

 *	union layoutrecall4 switch(layoutrecall_type4 lor_recalltype) {

 *	case LAYOUTRECALL4_FILE:

 *		layoutrecall_file4 lor_layout;

 *	case LAYOUTRECALL4_FSID:

 *		fsid4              lor_fsid;

 *	case LAYOUTRECALL4_ALL:

 *		void;

 *	};

 *

 *	struct CB_LAYOUTRECALL4args {

 *		layouttype4             clora_type;

 *		layoutiomode4           clora_iomode;

 *		bool                    clora_changed;

 *		layoutrecall4           clora_recall;

 *	};

 CONFIG_NFSD_PNFS */

/*

 * struct write_response4 {

 *	stateid4	wr_callback_id<1>;

 *	length4		wr_count;

 *	stable_how4	wr_committed;

 *	verifier4	wr_writeverf;

 * };

 * union offload_info4 switch (nfsstat4 coa_status) {

 *	case NFS4_OK:

 *		write_response4	coa_resok4;

 *	default:

 *	length4		coa_bytes_copied;

 * };

 * struct CB_OFFLOAD4args {

 *	nfs_fh4		coa_fh;

 *	stateid4	coa_stateid;

 *	offload_info4	coa_offload_info;

 * };

 We always return success if bytes were written */

/*

 * RPC procedure tables

/*

 * Note on the callback rpc program version number: despite language in rfc

 * 5661 section 18.36.3 requiring servers to use 4 in this field, the

 * official xdr descriptions for both 4.0 and 4.1 specify version 1, and

 * in practice that appears to be what implementations use.  The section

 * 18.36.3 language is expected to be fixed in an erratum.

	/*

	 * nfsd4_lease is set to at most one hour in __nfsd4_write_time,

	 * so we can use 32-bit math on it. Warn if that assumption

	 * ever stops being true.

 Create RPC client */

	/* XXX: release method to ensure we set the cb channel down if

/*

 * Poke the callback thread to process any updates to the callback

 * parameters, and send a null probe.

/*

 * There's currently a single callback channel slot.

 * If the slot is available, then mark it busy.  Otherwise, set the

 * thread for sleeping on the callback RPC wait queue.

 Race breaker */

/*

 * TODO: cb_sequence should support referring call lists, cachethis, multiple

 * slots, and mark callback channel down on communication errors.

	/*

	 * cb_seq_status is only set in decode_cb_sequence4res,

	 * and so will remain 1 if an rpc level failure occurs.

		/*

		 * If the backchannel connection was shut down while this

		 * task was queued, we need to resubmit it after setting up

		 * a new backchannel connection.

		 *

		 * Note that if we lost our callback connection permanently

		 * the submission code will error out, so we don't need to

		 * handle that case here.

		/*

		 * No need for lock, access serialized in nfsd4_cb_prepare

		 *

		 * RFC5661 20.9.3

		 * If CB_SEQUENCE returns an error, then the state of the slot

		 * (sequence ID, cached reply) MUST NOT change.

 must be called under the state lock */

	/*

	 * Note this won't actually result in a null callback;

	 * instead, nfsd4_run_cb_null() will detect the killed

	 * client, destroy the rpc client, and stop:

 requires cl_lock: */

/*

 * Note there isn't a lot of locking in this code; instead we depend on

 * the fact that it is run from the callback_wq, which won't run two

 * work items at once.  So, for example, callback_wq handles all access

 * of cl_cb_client and all calls to rpc_create or rpc_shutdown_client.

	/*

	 * This is either an update, or the client dying; in either case,

	 * kill the old client:

	/*

	 * Only serialized callback code is allowed to clear these

	 * flags; main nfsd code can only set them:

 Callback channel broken, or client killed; give up: */

	/*

	 * Don't send probe messages for 4.1 or later.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2011 Bryan Schumaker <bjschuma@netapp.com>

 *

 * Uses debugfs to create fault injection points for client testing

 Deal with any embedded newlines in the string */

 on success, claim we got the whole input */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2014-2016 Christoph Hellwig.

	/*

	 * Some clients barf on non-zero block numbers for NONE or INVALID

	 * layouts, so make sure to zero the whole structure.

			/*

			 * Crack monkey special case from section 2.3.1.

	/*

	 * Pretend that we send notification to the client.  This is a blatant

	 * lie to force recent Linux clients to cache our device IDs.

	 * We rarely ever change the device ID, so the harm of leaking deviceids

	 * for a while isn't too bad.  Unfortunately RFC5661 is a complete mess

	 * in this regard, but I filed errata 4119 for this a while ago, and

	 * hopefully the Linux client will eventually start caching deviceids

	 * without this again.

 CONFIG_NFSD_BLOCKLAYOUT */

/*

 * We use the client ID as a unique key for the reservations.

 * This allows us to easily fence a client when recalls fail.

	/*

	 * Pretend that we send notification to the client.  This is a blatant

	 * lie to force recent Linux clients to cache our device IDs.

	 * We rarely ever change the device ID, so the harm of leaking deviceids

	 * for a while isn't too bad.  Unfortunately RFC5661 is a complete mess

	 * in this regard, but I filed errata 4119 for this a while ago, and

	 * hopefully the Linux client will eventually start caching deviceids

	 * without this again.

 CONFIG_NFSD_SCSILAYOUT */

 SPDX-License-Identifier: GPL-2.0

/*

 * NFS server file handle treatment.

 *

 * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>

 * Portions Copyright (C) 1999 G. Allen Morris III <gam3@acm.org>

 * Extensive rewrite by Neil Brown <neilb@cse.unsw.edu.au> Southern-Spring 1999

 * ... and again Southern-Winter 2001 to support export_operations

/*

 * our acceptability function.

 * if NOSUBTREECHECK, accept anything

 * if not, require that we can walk up to exp->ex_dentry

 * doing some checks on the 'x' bits

 make sure parents give x permission to user */

/* Type check. The correct error return for type mismatches does not seem to be

 * generally agreed upon. SunOS seems to use EISDIR if file isn't S_IFREG; a

 * comment in the NFSv3 spec says this is incorrect (implementation notes for

 * the write call).

 the caller doesn't care */

	/*

	 * v4 has an error more specific than err_notdir which we should

	 * return in preference to err_notdir:

 We don't require gss requests to use low ports: */

 Check if the request originated from a secure port. */

 Set user creds for this exportpoint */

	/*

	 * v2/v3 clients have no need for the V4ROOT export--they use

	 * the mount protocl instead; also, further V4ROOT checks may be

	 * in v4-specific code, in which case v2/v3 clients could bypass

	 * them.

	/*

	 * We're exposing only the directories and symlinks that have to be

	 * traversed on the way to real exports:

	/*

	 * A pseudoroot export gives permission to access only one

	 * single directory; the kernel has to make another upcall

	 * before granting access to anything else under it:

/*

 * Use the given filehandle to look up the corresponding export and

 * dentry.  On success, the results are used to set fh_export and

 * fh_dentry.

 deprecated, convert to type 3 */

		/*

		 * struct knfsd_fh uses host-endian fields, which are

		 * sometimes used to hold net-endian values. This

		 * confuses sparse, so we must use __force here to

		 * keep it from complaining.

		/* Elevate privileges so that the lack of 'r' or 'x'

		 * permission on some parent directory will

		 * not stop exportfs_decode_fh from being able

		 * to reconnect a directory into the dentry cache.

		 * The same problem can affect "SUBTREECHECK" exports,

		 * but as nfsd_acceptable depends on correct

		 * access control settings being in effect, we cannot

		 * fix that case easily.

	/*

	 * Look up the dentry using the NFS file handle.

/**

 * fh_verify - filehandle lookup and access checking

 * @rqstp: pointer to current rpc request

 * @fhp: filehandle to be verified

 * @type: expected type of object pointed to by filehandle

 * @access: type of access needed to object

 *

 * Look up a dentry from the on-the-wire filehandle, check the client's

 * access to the export, and set the current task's credentials.

 *

 * Regardless of success or failure of fh_verify(), fh_put() should be

 * called on @fhp when the caller is finished with the filehandle.

 *

 * fh_verify() may be called multiple times on a given filehandle, for

 * example, when processing an NFSv4 compound.  The first call will look

 * up a dentry using the on-the-wire filehandle.  Subsequent calls will

 * skip the lookup and just perform the other checks and possibly change

 * the current task's credentials.

 *

 * @type specifies the type of object expected using one of the S_IF*

 * constants defined in include/linux/stat.h.  The caller may use zero

 * to indicate that it doesn't care, or a negative integer to indicate

 * that it expects something not of the given type.

 *

 * @access is formed from the NFSD_MAY_* constants defined in

 * fs/nfsd/vfs.h.

	/*

	 * We still have to do all these permission checks, even when

	 * fh_dentry is already set:

	 * 	- fh_verify may be called multiple times with different

	 * 	  "access" arguments (e.g. nfsd_proc_create calls

	 * 	  fh_verify(...,NFSD_MAY_EXEC) first, then later (in

	 * 	  nfsd_create) calls fh_verify(...,NFSD_MAY_CREATE).

	 *	- in the NFSv4 case, the filehandle may have been filled

	 *	  in by fh_compose, and given a dentry, but further

	 *	  compound operations performed with that filehandle

	 *	  still need permissions checks.  In the worst case, a

	 *	  mountpoint crossing may have changed the export

	 *	  options, and we may now need to use a different uid

	 *	  (for example, if different id-squashing options are in

	 *	  effect on the new filesystem).

	/*

	 * pseudoflavor restrictions are not enforced on NLM,

	 * which clients virtually always use auth_sys for,

	 * even while using RPCSEC_GSS for NFS.

	/*

	 * Clients may expect to be able to use auth_sys during mount,

	 * even if they use gss for everything else; see section 2.3.2

	 * of rfc 2623.

 Finally, check access permissions. */

/*

 * Compose a file handle for an NFS reply.

 *

 * Note that when first composed, the dentry may not yet have

 * an inode.  In this case a call to fh_update should be made

 * before the fh goes out on the wire ...

		/*

		 * As the fsid -> filesystem mapping was guided by

		 * user-space, there is no guarantee that the filesystem

		 * actually supports that fsid type. If it doesn't we

		 * loop around again without ref_fh set.

 for newer device numbers, we must use a newer fsid format */

	/* ref_fh is a reference file handle.

	 * if it is non-null and for the same filesystem, then we should compose

	 * a filehandle which is of the same version, where possible.

	/* Choose filehandle version and fsid type based on

	 * the reference filehandle (if it is in the same export)

	 * or the export options.

 If we have a ref_fh, then copy the fh_no_wcc setting from it. */

 our internal copy */

/*

 * Update file handle information after changing a dentry.

 * This is only called by nfsd_create, nfsd_create_v3 and nfsd_proc_create

/*

 * Release a file handle.

/*

 * Shorthand for dprintk()'s

	/* either a UUID type filehandle, or the filehandle doesn't

	 * match the export.

 SPDX-License-Identifier: GPL-2.0

/*

 * Process version 3 NFSACL requests.

 *

 * Copyright (C) 2002-2003 Andreas Gruenbacher <agruen@suse.de>

 FIXME: nfsacl.h is a broken header */

/*

 * NULL call.

/*

 * Get the Access and/or Default ACL of a file.

 Solaris returns the inode's minimum ACL. */

		/* Check how Solaris handles requests for the Default ACL

 resp->acl_{access,default} are released in nfs3svc_release_getacl. */

/*

 * Set the Access and/or Default ACL of a file.

	/* argp->acl_{access,default} may have been allocated in

/*

 * XDR decode functions

/*

 * XDR encode functions

 GETACL */

 SETACL */

/*

 * XDR release functions

 status*/

 attributes */

 post attributes - conditional */

 Access Control List */

 SPDX-License-Identifier: GPL-2.0

/*

 * Central processing for nfsd.

 *

 * Authors:	Olaf Kirch (okir@monad.swb.de)

 *

 * Copyright (C) 1995, 1996, 1997 Olaf Kirch <okir@monad.swb.de>

/*

 * nfsd_mutex protects nn->nfsd_serv -- both the pointer itself and the members

 * of the svc_serv struct. In particular, ->sv_nrthreads but also to some

 * extent ->sv_temp_socks and ->sv_permsocks. It also protects nfsdstats.th_cnt

 *

 * If (out side the lock) nn->nfsd_serv is non-NULL, then it must point to a

 * properly initialised 'struct svc_serv' with ->sv_nrthreads > 0. That number

 * of nfsd threads must exist and each must listed in ->sp_all_threads in each

 * entry of ->sv_pools[].

 *

 * Transitions of the thread count between zero and non-zero are of particular

 * interest since the svc_serv needs to be created and initialized at that

 * point, or freed.

 *

 * Finally, the nfsd_mutex also protects some of the global variables that are

 * accessed when nfsd starts and that are settable via the write_* routines in

 * nfsctl.c. In particular:

 *

 *	user_recovery_dirname

 *	user_lease_time

 *	nfsd_versions

/*

 * nfsd_drc_lock protects nfsd_drc_max_pages and nfsd_drc_pages_used.

 * nfsd_drc_max_pages limits the total amount of memory available for

 * version 4.1 DRC caches.

 * nfsd_drc_pages_used tracks the current version 4.1 DRC memory usage.

 defined(CONFIG_NFSD_V2_ACL) || defined(CONFIG_NFSD_V3_ACL) */

 program number */

 nr of entries in nfsd_version */

 version table */

 program name */

 authentication class */

 version table */

 export authentication */

 All compiled versions are enabled by default */

 All minor versions are enabled by default */

/*

 * Maximum number of nfsd processes

		/*

		 * This is opaque to client, so no need to byte-swap. Use

		 * __force to keep sparse happy. y2038 time_t overflow is

		 * irrelevant in this usage

 Only used under nfsd_mutex, so this atomic may be overkill: */

 check if the notifier still has clients */

	/*

	 * write_ports can create the server without actually starting

	 * any threads--if we get shut down before any threads are

	 * started, then nfsd_last_thread will be run before any of this

	 * other initialization has been done except the rpcb information.

/*

 * Each session guarantees a negotiated per slot memory cache for replies

 * which in turn consumes memory beyond the v2/v3/v4.0 server. A dedicated

 * NFSv4.1 server might want to use more memory for a DRC than a machine

 * with mutiple services.

 *

 * Impose a hard limit on the number of pages for the DRC which varies

 * according to the machines free pages. This is of course only a default.

 *

 * For now this is a #defined shift which could be under admin control

 * in the future.

	/*

	 * Aim for 1/4096 of memory per thread This gives 1MB on 4Gig

	 * machines, but only uses 32K on 128M machines.  Bottom out at

	 * 8K on 32M and smaller.  Of course, this is only a default.

 Kill outstanding nfsd threads */

 Wait for shutdown of nfsd_serv to complete */

 check if the notifier is already set */

 enforce a global maximum number of threads */

 total too large: scale down requested numbers */

	/*

	 * There must always be a thread in pool 0; the admin

	 * can't shut down NFS completely using pool_threads.

 apply the new numbers */

/*

 * Adjust the number of threads and return the new number of threads.

 * This is also the function that starts the server if necessary, if

 * this is the first time nrservs is nonzero.

	/* We are holding a reference to nn->nfsd_serv which

	 * we don't want to count in the return value,

	 * so subtract 1

 Release server */

/*

 * This is the NFS server kernel thread

 Lock module and set up kernel thread */

	/* At this point, the thread shares current->fs

	 * with the init process. We need to create files with the

	/*

	 * thread is spawned with all signals set to SIG_IGN, re-enable

	 * the ones that will bring down the thread

	/*

	 * The main request loop

 Update sv_maxconn if it has changed */

		/*

		 * Find a socket with data available and call its

		 * recvfrom routine.

 Clear signals before calling svc_exit_thread() */

 Release the thread */

 Release module */

/**

 * nfsd_dispatch - Process an NFS or NFSACL Request

 * @rqstp: incoming request

 * @statp: pointer to location of accept_stat field in RPC Reply buffer

 *

 * This RPC dispatcher integrates the NFS server's duplicate reply cache.

 *

 * Return values:

 *  %0: Processing complete; do not send a Reply

 *  %1: Processing complete; send Reply in rqstp->rq_res

	/*

	 * Give the xdr decoder a chance to change this if it wants

	 * (necessary in the NFSv4.0 compound case)

	/*

	 * Need to grab the location to store the status, as

	 * NFSv4 does some encoding while processing

/**

 * nfssvc_decode_voidarg - Decode void arguments

 * @rqstp: Server RPC transaction context

 * @xdr: XDR stream positioned at arguments to decode

 *

 * Return values:

 *   %false: Arguments were not valid

 *   %true: Decoding was successful

/**

 * nfssvc_encode_voidres - Encode void results

 * @rqstp: Server RPC transaction context

 * @xdr: XDR stream into which to encode results

 *

 * Return values:

 *   %false: Local error while encoding

 *   %true: Encoding was successful

 bump up the psudo refcount while traversing */

 this function really, really should have been called svc_put() */

 SPDX-License-Identifier: GPL-2.0

/*

 * procfs-based user access to knfsd statistics

 *

 * /proc/net/rpc/nfsd

 *

 * Format:

 *	rc <hits> <misses> <nocache>

 *			Statistsics for the reply cache

 *	fh <stale> <deprecated filehandle cache stats>

 *			statistics for filehandle lookup

 *	io <bytes-read> <bytes-written>

 *			statistics for IO throughput

 *	th <threads> <deprecated thread usage histogram stats>

 *			number of threads

 *	ra <deprecated ra-cache stats>

 *

 *	plus generic RPC stats (see net/sunrpc/stats.c)

 *

 * Copyright (C) 1995, 1996, 1997 Olaf Kirch <okir@monad.swb.de>

 thread usage: */

 deprecated thread usage histogram stats */

 deprecated ra-cache stats */

 show my rpc info */

 Show count for individual nfsv4 operations */

 Writing operation numbers 0 1 2 also for maintaining uniformity */

/*

 * Open file cache.

 *

 * (c) 2015 - Jeff Layton <jeff.layton@primarydata.com>

 FIXME: dynamically size this for the machine somehow? */

 We only care about NFSD_MAY_READ/WRITE for this cache */

 Avoid soft lockup race with nfsd_file_mark_put() */

 allocate a new nfm */

		/*

		 * If the add was successful, then return the object.

		 * Otherwise, we need to put the reference we hold on the

		 * nfm_mark. The fsnotify code will take a reference and put

		 * it on failure, so we can't just free it directly. It's also

		 * not safe to call fsnotify_destroy_mark on it as the

		 * mark->group will be NULL. Thus, we can't let the nfm_ref

		 * counter drive the destruction at this point.

/*

 * Return true if the file was unhashed.

 keep final reference for nfsd_file_lru_dispose */

/*

 * Note this can deadlock with nfsd_file_cache_purge.

	/*

	 * Do a lockless refcount check. The hashtable holds one reference, so

	 * we look to see if anything else has a reference, or if any have

	 * been put since the shrinker last ran. Those don't get unhashed and

	 * released.

	 *

	 * Note that in the put path, we set the flag and then decrement the

	 * counter. Here we check the counter and then test and clear the flag.

	 * That order is deliberate to ensure that we can do this locklessly.

	/*

	 * Don't throw out files that are still undergoing I/O or

	 * that have uncleared errors pending.

/**

 * nfsd_file_close_inode_sync - attempt to forcibly close a nfsd_file

 * @inode: inode of the file to attempt to remove

 *

 * Walk the whole hash bucket, looking for any files that correspond to "inode".

 * If any do, then unhash them and put the hashtable reference to them and

 * destroy any that had their last reference put. Also ensure that any of the

 * fputs also have their final __fput done as well.

/**

 * nfsd_file_close_inode - attempt a delayed close of a nfsd_file

 * @inode: inode of the file to attempt to remove

 *

 * Walk the whole hash bucket, looking for any files that correspond to "inode".

 * If any do, then unhash them and put the hashtable reference to them and

 * destroy any that had their last reference put.

/**

 * nfsd_file_delayed_close - close unused nfsd_files

 * @work: dummy

 *

 * Walk the LRU list and close any entries that have not been used since

 * the last scan.

 *

 * Note this can deadlock with nfsd_file_cache_purge.

 Only close files for F_SETLEASE leases */

 Should be no marks on non-regular files */

 don't close files if this was not the last link */

/*

 * Note this can deadlock with nfsd_file_lru_cb.

			/*

			 * Deadlock detected! Something marked this entry as

			 * unhased, but hasn't removed it from the hash list.

	/*

	 * make sure all callers of nfsd_file_lru_cb are done before

	 * calling nfsd_file_cache_purge

/**

 * nfsd_file_is_cached - are there any cached open files for this fh?

 * @inode: inode of the file to check

 *

 * Scan the hashtable for open files that match this fh. Returns true if there

 * are any, and false if not.

 FIXME: skip this if fh_dentry is already set? */

 Did construction of this file fail? */

 Take reference for the hashtable */

	/*

	 * If construction failed, or we raced with a call to unlink()

	 * then unhash.

/*

 * Note that fields may be added, removed or reordered in the future. Programs

 * scraping this file for info should test the labels to ensure they're

 * getting the correct field.

	/*

	 * No need for spinlocks here since we're not terribly interested in

	 * accuracy. We do take the nfsd_mutex simply to ensure that we

	 * don't end up racing with server shutdown

/*

 *  Server-side procedures for NFSv4.

 *

 *  Copyright (c) 2002 The Regents of the University of Michigan.

 *  All rights reserved.

 *

 *  Kendrick Smith <kmsmith@umich.edu>

 *  Andy Adamson   <andros@umich.edu>

 *

 *  Redistribution and use in source and binary forms, with or without

 *  modification, are permitted provided that the following conditions

 *  are met:

 *

 *  1. Redistributions of source code must retain the above copyright

 *     notice, this list of conditions and the following disclaimer.

 *  2. Redistributions in binary form must reproduce the above copyright

 *     notice, this list of conditions and the following disclaimer in the

 *     documentation and/or other materials provided with the distribution.

 *  3. Neither the name of the University nor the names of its

 *     contributors may be used to endorse or promote products derived

 *     from this software without specific prior written permission.

 *

 *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

 *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE

 *  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE

 *  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR

 *  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF

 *  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR

 *  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF

 *  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING

 *  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS

 *  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 default to 15 mins */

		/*

		 * XXX: We should really fail the whole open, but we may

		 * already have created a new file, so it may be too

		 * late.  For now this seems the least of evils:

/*

 * if error occurs when setting the acl, just clear the acl bit

 * in the returned attr bitmap.

		/*

		 * We should probably fail the whole open at this point,

		 * but we've already created the file, so it's too late;

		 * So this seems the least of evils:

	/*

	 * Using err_symlink as our catch-all case may look odd; but

	 * there's no other obvious error for this case in 4.0, and we

	 * happen to know that it will cause the linux v4 client to do

	 * the right thing on attempts to open something other than a

	 * regular file.

		/* FIXME: check session persistence and pnfs flags.

		 * The nfsv4.1 spec requires the following semantics:

		 *

		 * Persistent   | pNFS   | Server REQUIRED | Client Allowed

		 * Reply Cache  | server |                 |

		 * -------------+--------+-----------------+--------------------

		 * no           | no     | EXCLUSIVE4_1    | EXCLUSIVE4_1

		 *              |        |                 | (SHOULD)

		 *              |        | and EXCLUSIVE4  | or EXCLUSIVE4

		 *              |        |                 | (SHOULD NOT)

		 * no           | yes    | EXCLUSIVE4_1    | EXCLUSIVE4_1

		 * yes          | no     | GUARDED4        | GUARDED4

		 * yes          | yes    | GUARDED4        | GUARDED4

		/*

		 * Note: create modes (UNCHECKED,GUARDED...) are the same

		 * in NFSv4 as in v3 except EXCLUSIVE4_1.

		/*

		 * Following rfc 3530 14.2.16, and rfc 5661 18.16.4

		 * use the returned bitmask to indicate which attributes

		 * we used to store the verifier:

		/*

		 * Note this may exit with the parent still locked.

		 * We will hold the lock until nfsd4_open's final

		 * lookup, to prevent renames or unlinks until we've had

		 * a chance to an acquire a delegation if appropriate.

	/* We don't know the target directory, and therefore can not

	* set the change info

	/*

	 * In the delegation case, the client is telling us about an

	 * open that it *already* performed locally, some time ago.  We

	 * should let it succeed now if possible.

	 *

	 * In the case of a CLAIM_FH open, on the other hand, the client

	 * may be counting on us to enforce permissions (the Linux 4.1

	 * client uses this for normal opens, for example).

 This check required by spec. */

	/*

	 * RFC5661 18.51.3

	 * Before RECLAIM_COMPLETE done, server should deny new lock

 check seqid for replay. set nfs4_owner */

	/* Openowner is now set, so sequence id will get bumped.  Now we need

	/*

	 * nfsd4_process_open2() does the actual opening of the file.  If

	 * successful, it (1) truncates the file if open->op_truncate was

	 * set, (2) sets open->op_stateid, (3) sets open->op_delegation.

/*

 * OPEN is the only seqid-mutating operation whose decoding can fail

 * with a seqid-mutating error (specifically, decoding of user names in

 * the attributes).  Therefore we have to do some processing to look up

 * the stateowner so that we can bump the seqid.

/*

 * filehandle-manipulating ops.

/*

 * misc nfsv4 ops

	/*

	 * If we do a zero copy read, then a client will see read data

	 * that reflects the state of the file *after* performing the

	 * following compound.

	 *

	 * To ensure proper ordering, we therefore turn off zero copy if

	 * the client wants us to do more in this compound:

 check stateid */

 no need to check permission - this will be done in nfsd_readdir() */

 See rfc 5661 section 2.6.3.1.1.8 */

 fix up for NFS-specific error code */

 only 1 thread should stop the copy */

/*

 * setup a work entry in the ssc delayed unmount list.

 found a match */

  wait - and try again */

 allow 20secs for mount/unmount for now - revisit */

 return vfsmount in ss_mnt */

 set nsui_vfsmount, clear busy flag and wakeup waiters */

/*

 * Support one copy source server for now.

 Construct the raw data for the vfs_kern_mount call */

 2 for ipv6 endsep and startsep. 3 for ":/" and trailing '/0'*/

 Set the server:<export> for the vfs_kern_mount call */

 Use an 'internal' mount: SB_KERNMOUNT -> MNT_INTERNAL */

/*

 * Verify COPY destination stateid.

 *

 * Connect to the source server with NFSv4.1.

 * Create the source struct file for nfsd_copy_range.

 * Called with COPY cstate:

 *    SAVED_FH: source filehandle

 *    CURRENT_FH: destination filehandle

 Verify the destination stateid and set dst struct file*/

			/*

			 * vfsmount can be shared by multiple exports,

			 * decrement refcnt. If the count drops to 1 it

			 * will be unmounted when nsui_expire expires.

 CONFIG_NFSD_V4_2_INTER_SSC */

 CONFIG_NFSD_V4_2_INTER_SSC */

 See RFC 7862 p.67: */

 for a non-zero asynchronous copy do a commit of data */

	/* for async copy, we ignore the error, client can always retry

	 * to get the error

 Inter server SSC */

 for inter, file_src doesn't exist yet */

 Inter server SSC */

 Inter server SSC */

	/* For now, only return one server address in cpn_src, the

	 * address used by the client to connect to this server.

	/*

	 * Note:  This call does change file->f_pos, but nothing in NFSD

	 *        should ever file->f_pos.

/* This routine never returns NFS_OK!  If there are no other errors, it

 * will return NFSERR_SAME or NFSERR_NOT_SAME depending on whether the

 * attributes matched.  VERIFY is implemented by mapping NFSERR_SAME

 * to NFS_OK after the call; NVERIFY by mapping NFSERR_NOT_SAME to NFS_OK.

	/* count in words:

	 *   bitmap_len(1) + bitmap(2) + attr_len(1) = 4

	/*

	 * If nfsd4_encode_fattr() ran out of space, assume that's because

	 * the attributes are longer (hence different) than those given:

 skip bitmap */

	/*

	 * Verify minlength and range as per RFC5661:

	 *  o  If loga_length is less than loga_minlength,

	 *     the metadata server MUST return NFS4ERR_INVAL.

	 *  o  If the sum of loga_offset and loga_minlength exceeds

	 *     NFS4_UINT64_MAX, and loga_minlength is not

	 *     NFS4_UINT64_MAX, the error NFS4ERR_INVAL MUST result.

	 *  o  If the sum of loga_offset and loga_length exceeds

	 *     NFS4_UINT64_MAX, and loga_length is not NFS4_UINT64_MAX,

	 *     the error NFS4ERR_INVAL MUST result.

 fixup error code as per RFC5661 */

 LAYOUTCOMMIT does not require any serialization */

 CONFIG_NFSD_PNFS */

	/*

	 * Get the entire list, then copy out only the user attributes

	 * in the encode function.

/*

 * NULL call.

/*

 * Enforce NFSv4.1 COMPOUND ordering rules:

 *

 * Also note, enforced elsewhere:

 *	- SEQUENCE other than as first op results in

 *	  NFS4ERR_SEQUENCE_POS. (Enforced in nfsd4_sequence().)

 *	- BIND_CONN_TO_SESSION must be the only op in its compound.

 *	  (Enforced in nfsd4_bind_conn_to_session().)

 *	- DESTROY_SESSION must be the final operation in a compound, if

 *	  sessionid's in SEQUENCE and DESTROY_SESSION are the same.

 *	  (Enforced in nfsd4_destroy_session().)

 These ordering requirements don't apply to NFSv4.0: */

 This is weird, but OK, not our problem: */

	/*

	 * So first_op is something allowed outside a session, like

	 * EXCHANGE_ID; but then it has to be the only op in the

	 * compound:

	/*

	 * Most ops check wronsec on our own; only the putfh-like ops

	 * have special rules.

	/*

	 * rfc 5661 2.6.3.1.1.6: don't bother erroring out a

	 * put-filehandle operation if we're not going to use the

	 * result:

	/*

	 * Rest of 2.6.3.1.1: certain operations will return WRONGSEC

	 * errors themselves as necessary; others should check for them

	 * now:

	/* traverse all operation and if it's a COPY compound, mark the

	 * source filehandle to skip verification

/*

 * COMPOUND call.

 reserve space for: NFS status code */

 reserve space for: taglen, tag, and opcnt */

	/*

	 * Don't use the deferral mechanism for NFSv4; compounds make it

	 * too hard to avoid non-idempotency problems.

	/*

	 * According to RFC3010, this takes precedence over all other errors.

		/*

		 * The XDR decode routines may have pre-set op->status;

		 * for example, if there is a miscellaneous XDR error

		 * it will be set to nfserr_bad_xdr.

 If op is non-idempotent */

			/*

			 * Don't execute this op if we couldn't encode a

			 * succesful reply:

			/*

			 * Plus if there's another operation, make sure

			 * we'll have space to at least encode an error:

 Only from SEQUENCE */

 Reset deferral mechanism for RPC deferrals */

 We'll fall back on returning no lockowner if run out of space: */

 ac_supported, ac_resp_access */

/*

 * Note since this is an idempotent operation we won't insist on failing

 * the op prematurely if the estimate is too large.  We may turn off splice

 * reads unnecessarily.

	/*

	 * Largest of remaining attributes are 16 bytes (e.g.,

	 * supported_attributes)

 bitmask, length */

	/*

	 * If we detect that the file changed during hole encoding, then we

	 * recover by encoding the remaining reply as data. This means we need

	 * to set aside enough room to encode two data segments.

 eir_clientid, eir_sequenceid */\

 eir_flags, spr_how */\

 spo_must_enforce & _allow with bitmap */\

eir_server_owner.so_minor_id */\

 eir_server_owner.so_major_id<> */\

 eir_server_scope<> */\

 eir_server_impl_id array length */\

 ignored eir_server_impl_id contents */) * sizeof(__be32);

 bctsr_sessid */\

 bctsr_dir, use_conn_in_rdma_mode */) * sizeof(__be32);

 sessionid */\

 csr_sequence, csr_flags */\

 wr_callback */ +

 wr_callback */ +

 wr_count */ +

 wr_committed */ +

 cr_consecutive */ +

 cr_synchronous */) * sizeof(__be32);

 osr_count */ +

 osr_complete<1> optional 0 for now */) * sizeof(__be32);

 cnr_lease_time */ +

 We support one cnr_source_server */ +

 cnr_stateid seq */ +

 cnr_stateid */ +

 num cnr_source_server*/ +

 nl4_type */ +

 nl4 size */ +

nl4_loc + nl4_loc_sz */)

 gd_layout_type*/ +

 gd_notify_types */) * sizeof(__be32);

/*

 * At this stage we don't really know what layout driver will handle the request,

 * so we need to define an arbitrary upper bound here.

 logr_return_on_close */ +

 nr of layouts */ +

 locr_newsize */ +

 ns_size */) * sizeof(__be32);

 lrs_stateid */ +

 CONFIG_NFSD_PNFS */

 NFSv4.1 operations */

 CONFIG_NFSD_PNFS */

 NFSv4.2 operations */

/**

 * nfsd4_spo_must_allow - Determine if the compound op contains an

 * operation that is allowed to be sent with machine credentials

 *

 * @rqstp: a pointer to the struct svc_rqst

 *

 * Checks to see if the compound contains a spo_must_allow op

 * and confirms that it was sent with the proper machine creds.

/*

 *  Mapping of UID/GIDs to name and vice versa.

 *

 *  Copyright (c) 2002, 2003 The Regents of the University of

 *  Michigan.  All rights reserved.

 *

 *  Marius Aamodt Eriksen <marius@umich.edu>

 *

 *  Redistribution and use in source and binary forms, with or without

 *  modification, are permitted provided that the following conditions

 *  are met:

 *

 *  1. Redistributions of source code must retain the above copyright

 *     notice, this list of conditions and the following disclaimer.

 *  2. Redistributions in binary form must reproduce the above copyright

 *     notice, this list of conditions and the following disclaimer in the

 *     documentation and/or other materials provided with the distribution.

 *  3. Neither the name of the University nor the names of its

 *     contributors may be used to endorse or promote products derived

 *     from this software without specific prior written permission.

 *

 *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

 *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE

 *  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE

 *  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR

 *  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF

 *  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR

 *  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF

 *  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING

 *  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS

 *  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

/*

 * Turn off idmapping when using AUTH_SYS.

/*

 * Cache entry

/*

 * XXX we know that IDMAP_NAMESZ < PAGE_SIZE, but it's ugly to rely on

 * that.

 User / Group */

 Common entry handling */

/*

 * ID -> Name cache

 Flip LSB for user/group */

 Authentication name */

 Type */

 ID */

 expiry */

 Name */

/*

 * Name -> ID cache

 Authentication name */

 Type */

 Name */

 expiry */

 ID */

/*

 * Exported API

 too long to represent a 32-bit id: */

 Just to make sure it's null-terminated: */

		/*

		 * otherwise, fall through and try idmapping, for

		 * backwards compatibility with clients sending names:

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2014-2016 Christoph Hellwig.

 we always return a single extent */

 single signature */

	/*

	 * Fill in the overall length and number of volumes at the beginning

	 * of the layout.

 SPDX-License-Identifier: GPL-2.0

/*

 * NFS exporting and validation.

 *

 * We maintain a list of clients, each of which has a list of

 * exports. To export an fs to a given client, you first have

 * to create the client entry with NFSCTL_ADDCLIENT, which

 * creates a client control block and adds it to the hash

 * table. Then, you call NFSCTL_EXPORT for each fs.

 *

 *

 * Copyright (C) 1995, 1996 Olaf Kirch, <okir@monad.swb.de>

/*

 * We have two caches.

 * One maps client+vfsmnt+dentry to export options - the export map

 * The other maps client+filehandle-fragment to export options. - the expkey map

 *

 * The export options are actually stored in the first map, and the

 * second map contains a reference to the entry in the first map.

 client fsidtype \xfsid */

 client fsidtype fsid expiry [path] */

 invalid type */

 OK, we seem to have a valid key */

 now we want a pathname, or empty meaning NEGATIVE  */

	/*

	 * Take the nfsd_mutex here to ensure that the file cache is not

	 * destroyed while we're in the middle of flushing.

  client path */

 is this correct? */

	/*

	 * We currently export only dirs, regular files, and (for v4

	 * pseudoroot) symlinks.

	/*

	 * Mountd should never pass down a writeable V4ROOT export, but,

	 * just to make sure:

	/* There are two requirements on a filesystem to be exportable.

	 * 1:  We must be able to identify the filesystem from a number.

	 *       either a device number (so FS_REQUIRES_DEV needed)

	 *       or an FSID number (so NFSEXP_FSID or ->uuid is needed).

	 * 2:  We must be able to find an inode from a filehandle.

	 *       This means that s_export_op must be set.

	 * 3: We must not currently be on an idmapped mount.

 more than one fsloc */

 listsize */

 colon separated host list */

 slash separated path component list */

 migrated */

 more than one secinfo */

		/*

		 * XXX: It would be nice to also check whether this

		 * pseudoflavor is supported, so we can discover the

		 * problem at export time instead of when a client fails

		 * to authenticate.

 Only some flags are allowed to differ between flavors: */

 CONFIG_NFSD_V4 */

 more than one uuid */

 expect a 16 byte uuid encoded as \xXXXX... */

 client path expiry [flags anonuid anongid fsid] */

 client */

 path */

 expiry */

 flags */

 anon uid */

 anon gid */

 fsid */

				/* quietly ignore unknown words and anything

				 * following. Newer user-space can try to set

				 * new values, then see what the result was.

		/*

		 * No point caching this if it would immediately expire.

		 * Also, this protects exportfs's dummy export from the

		 * anon_uid/anon_gid checks:

		/*

		 * For some reason exportfs has been passing down an

		 * invalid (-1) uid & gid on the "dummy" export which it

		 * uses to test export support.  To make sure exportfs

		 * sees errors from check_export we therefore need to

		 * delay these checks till after check_export:

	/*

	 * The export_stats file uses the same ops as the exports file.

	 * We use the file's name to determine the reported info per export.

	 * There is no rename in nsfdfs, so d_name.name is stable.

/*

 * Find the export entry for a given dentry.

/*

 * Obtain the root fh on behalf of a client.

 * This could be done in user space, but I feel that it adds some safety

 * since its harder to fool a kernel module than a user space program.

 NB: we probably ought to check that it's NUL-terminated */

	/*

	 * fh must be initialized before calling fh_compose

 legacy gss-only clients are always OK: */

 ip-address based client; check sec= export option: */

 defaults in absence of sec= options: */

	/* If the compound op contains a spo_must_allowed op,

	 * it will be sent with integrity/protection which

	 * will have to be expressly allowed on mounts that

	 * don't support it

/*

 * Uses rq_client and rq_gssclient to find an export; uses rq_client (an

 * auth_unix client) if it's available and has secinfo information;

 * otherwise, will try to use rq_gssclient.

 *

 * Called from functions that handle requests; functions that do work on

 * behalf of mountd are passed a single client name to use, and should

 * use exp_get_by_name() or exp_find().

 First try the auth_unix client: */

 If it has secinfo, assume there are no gss/... clients */

 Otherwise, try falling back on gss client */

 First try the auth_unix client: */

 If it has secinfo, assume there are no gss/... clients */

 Otherwise, try falling back on gss client */

/*

 * Called when we need the filehandle for the root of the pseudofs,

 * for a given NFSv4 client.   The root is defined to be the

 * export point with fsid==0

/*

 * Initialize the exports module.

/*

 * Flush exports table - called when last nfsd thread is killed

/*

 * Shutdown the exports module.

/*

*  Copyright (c) 2001 The Regents of the University of Michigan.

*  All rights reserved.

*

*  Kendrick Smith <kmsmith@umich.edu>

*  Andy Adamson <kandros@umich.edu>

*

*  Redistribution and use in source and binary forms, with or without

*  modification, are permitted provided that the following conditions

*  are met:

*

*  1. Redistributions of source code must retain the above copyright

*     notice, this list of conditions and the following disclaimer.

*  2. Redistributions in binary form must reproduce the above copyright

*     notice, this list of conditions and the following disclaimer in the

*     documentation and/or other materials provided with the distribution.

*  3. Neither the name of the University nor the names of its

*     contributors may be used to endorse or promote products derived

*     from this software without specific prior written permission.

*

*  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

*  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF

*  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE

*  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE

*  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR

*  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF

*  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR

*  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF

*  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING

*  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS

*  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

*

 all fields zero */

 forward declarations */

 Locking: */

/*

 * Currently used for the del_recall_lru and file hash table.  In an

 * effort to decrease the scope of the client_mutex, this spinlock may

 * eventually cover more:

/*

 * A waitqueue for all in-progress 4.0 CLOSE operations that are waiting for

 * the refcount on the open stateid to drop.

/*

 * A waitqueue where a writer to clients/#/ctl destroying a client can

 * wait for cl_rpc_users to drop to 0 and then for the client to be

 * unhashed.

 must be called under the client_lock */

 Dequeue all blocked locks */

 Now free them */

	/*

	 * Since this is just an optimization, we don't try very hard if it

	 * turns out not to succeed. We'll requeue it on NFS4ERR_DELAY, and

	 * just quit trying on anything else.

/*

 * We store the NONE, READ, WRITE, and BOTH bits separately in the

 * st_{access,deny}_bmap field of the stateid, in order to track not

 * only what share bits are currently in force, but also what

 * combinations of share bits previous opens have used.  This allows us

 * to enforce the recommendation of rfc 3530 14.2.19 that the server

 * return an error if the client attempt to downgrade to a combination

 * of share bits not explicable by closing some of its previous opens.

 *

 * XXX: This enforcement is actually incomplete, since we don't keep

 * track of access/deny bit combinations; so, e.g., we allow:

 *

 *	OPEN allow read, deny write

 *	OPEN allow both, deny none

 *	DOWNGRADE allow read, deny none

 *

 * which we should reject.

 set share access for a given stateid */

 clear share access for a given stateid */

 test whether a given stateid has access */

 set share deny for a given stateid */

 clear share deny for a given stateid */

 test whether a given stateid is denying specific access */

/*

 * Open owner state (share locks)

 hash tables for lock and open owners */

 hash table for nfs4_file */

 XXX: why not (here & in file cache) use inode? */

 Does this access mode make sense? */

 Does it conflict with a deny mode already set? */

 Common case is that there is no deny mode. */

 Does this deny mode make sense? */

/*

 * Allocate a new open/delegation state counter. This is needed for

 * pNFS for proper return on close semantics.

 *

 * Note that we only allocate it for pNFS-enabled exports, otherwise

 * all pointers to struct nfs4_clnt_odstate are always NULL.

 Reserving 0 for start of file in nfsdfs "states" file: */

 Will be incremented before return to client: */

	/*

	 * It shouldn't be a problem to reuse an opaque stateid value.

	 * I don't think it is for 4.1.  But with 4.0 I worry that, for

	 * example, a stray write retransmission could be accepted by

	 * the server when it should have been rejected.  Therefore,

	 * adopt a trick from the sctp code to attempt to maximize the

	 * amount of time until an id is reused, by ensuring they always

	 * "increase" (mod INT_MAX):

/*

 * Create a unique stateid_t to represent each COPY.

/*

 * When we recall a delegation, we should be careful not to hand it

 * out again straight away.

 * To ensure this we keep a pair of bloom filters ('new' and 'old')

 * in which the filehandles of recalled delegations are "stored".

 * If a filehandle appear in either filter, a delegation is blocked.

 * When a delegation is recalled, the filehandle is stored in the "new"

 * filter.

 * Every 30 seconds we swap the filters and clear the "new" one,

 * unless both are empty of course.

 *

 * Each filter is 256 bits.  We hash the filehandle to 32bit and use the

 * low 3 bytes as hash-table indices.

 *

 * 'blocked_delegations_lock', which is always taken in block_delegations(),

 * is used to manage concurrent access.  Testing does not need the lock

 * except when swapping the two filters.

 index into 'set' */

	/*

	 * delegation seqid's are never incremented.  The 4.1 special

	 * meaning of seqid 0 isn't meaningful, really, but let's avoid

	 * 0 anyway just for consistency and use 1:

/**

 * nfs4_delegation_exists - Discover if this delegation already exists

 * @clp:     a pointer to the nfs4_client we're granting a delegation to

 * @fp:      a pointer to the nfs4_file we're granting a delegation on

 *

 * Return:

 *      On success: true iff an existing delegation is found

/**

 * hash_delegation_locked - Add a delegation to the appropriate lists

 * @dp:     a pointer to the nfs4_delegation we are adding.

 * @fp:     a pointer to the nfs4_file we're granting a delegation on

 *

 * Return:

 *      On success: NULL if the delegation was successfully hashed.

 *

 *      On error: -EAGAIN if one was previously granted to this

 *                 nfs4_client for this nfs4_file. Delegation is not hashed.

 *

 Ensure that deleg break won't try to requeue it */

/* 

 * SETCLIENTID state 

/*

 * A stateid that had a deny mode associated with it is being released

 * or downgraded. Recalculate the deny mode on the file.

 Recalculate per-file deny mode if there was a change */

 release all access and file references for a given stateid */

/*

 * Put the persistent reference to an already unhashed generic stateid, while

 * holding the cl_lock. If it's the last reference, then put it onto the

 * reaplist for later destruction.

/*

 * Free a list of generic stateids that were collected earlier after being

 * fully unhashed.

/*

 * Bump the seqid on cstate->replay_owner, and clear replay_owner if it

 * won't be used for replay.

/*

 * The protocol defines ca_maxresponssize_cached to include the size of

 * the rpc header, but all we need to cache is the data starting after

 * the end of the initial SEQUENCE operation--the rest we regenerate

 * each time.  Therefore we can advertise a ca_maxresponssize_cached

 * value that is the number of bytes in our cache plus a few additional

 * bytes.  In order to stay on the safe side, and not promise more than

 * we can cache, those additional bytes must be the minimum possible: 24

 * bytes of rpc header (xid through accept state, with AUTH_NULL

 * verifier), 12 for the compound header (with zero-length tag), and 44

 * for the SEQUENCE op response:

/*

 * We don't actually need to cache the rpc and session headers, so we

 * can allocate a little less for each slot:

/*

 * XXX: If we run out of reserved DRC memory we could (up to a point)

 * re-negotiate active sessions and reduce their slot usage to make

 * room for new connections. For now we just fail the create session.

		/* We have handed out more space than we chose in

		 * set_max_drc() to allow.  That isn't really a

		 * problem as long as that doesn't make us think we

		 * have lots more due to integer overflow.

	/*

	 * Never use more than a fraction of the remaining memory,

	 * unless it's the only way to give this client a slot.

	 * The chosen fraction is either 1/8 or 1/number of threads,

	 * whichever is smaller.  This ensures there are adequate

	 * slots to support multiple clients per thread.

	 * Give the client one slot even if that would require

	 * over-allocation--it is better than failure.

 allocate each struct nfsd4_slot and data cache in one piece */

 oops; xprt is already down: */

 We may have gained or lost a callback channel: */

 must be called under client_lock */

		/*

		 * This is a little silly; with sessions there's no real

		 * use for the callback address.  Use the peer address

		 * as a reasonable default for now, but consider fixing

		 * the rpc client not to require an address in the

		 * future:

 caller must hold client_lock */

 Search in the appropriate list */

 caller must hold client_lock */

 SETCLIENTID and SETCLIENTID_CONFIRM Helper functions */

	/*

	 * We're assuming the clid was not given out from a boot

	 * precisely 2^32 (about 136 years) before this one.  That seems

	 * a safe assumption:

/* 

 * XXX Should we use a slab cache ?

 * This type of memory management is somewhat inefficient, but we use it

 * anyway since SETCLIENTID is not a common operation.

 must be called under the client_lock */

 Mark the client as expired! */

 Make it invisible */

 Should be no openowners at this point */

/*

 * RFC 3530 language requires clid_inuse be returned when the

 * "principal" associated with a requests differs from that previously

 * used.  We use uid, gid's, and gss principal string as our best

 * approximation.  We also don't want to allow non-gss use of a client

 * established using gss: in theory cr_principal should catch that

 * change, but in practice cr_principal can be null even in the gss case

 * since gssd doesn't always pass down a principal string.

 Is cr_flavor one of the gss "pseudoflavors"?: */

 XXX: check that cr_targ_princ fields match ? */

	/*

	 * This is opaque to client, so no need to byte-swap. Use

	 * __force to keep sparse happy

 XXX: or SEQ_SKIP? */

	/*

	 * Note: a lock stateid isn't really the same thing as a lock,

	 * it's the locking state held by one owner on a file, and there

	 * may be multiple (or no) lock ranges associated with it.

	 * (Same for the matter is true of open stateids.)

 XXX: open stateid? */

 Kinda dead code as long as we only support read delegs: */

 XXX: lease time, whether it's being recalled. */

 XXX: What else would be useful? */

 XXX: or SEQ_SKIP? */

 XXX: copy stateids? */

 XXX: alternatively, we could get/drop in seq start/stop */

/*

 * Normally we refuse to destroy clients that are in use, but here the

 * administrator is telling us to just do it.  We also want to wait

 * so the caller has a guarantee that the client's locks are gone by

 * the time the write returns:

 Currently, we only support tcp and tcp6 for the callback channel */

/*

 * Cache a reply. nfsd4_check_resp_size() has bounded the cache size.

/*

 * Encode the replay sequence operation from the slot values.

 * If cachethis is FALSE encode the uncached rep error on the next

 * operation which sets resp->p and increments resp->opcnt for

 * nfs4svc_encode_compoundres.

 *

 Encode the replayed sequence operation */

		/*

		 * The original operation wasn't a solo sequence--we

		 * always cache those--so this retry must not match the

		 * original:

/*

 * The sequence operation is not cached because we can use the slot and

 * session values.

/*

 * Set the exchange_id flags returned by the server.

 Referrals are supported, Migration is not. */

 set the wire flags to return to client. */

		/*

		 * Sometimes userspace doesn't give us a principal.

		 * Which is a bug, really.  Anyway, we can't enforce

		 * MACH_CRED in that case, better to give up now:

 checked by xdr code */

 Cases below refer to rfc 5661 section 18.35.4: */

 buggy client */

 case 9 */

 case 8 */

 case 6 */

 case 3 */

 case 2 */

 case 5, client reboot */

 case 7 */

 case 4, possible retry or client restart */

 case 1, new owner ID */

 The slot is in use, and no response has been sent. */

 Note unsigned 32-bit arithmetic handles wraparound: */

/*

 * Cache the create session result into the create session single DRC

 * slot cache by saving the xdr structure. sl_seqid has been set.

 * Do this for solo or embedded create session operations.

 credential,verifier: AUTH_NULL, length 0 */ \

 MIN tag is length with zero, only length */ \

 version, opcount, opcode */ \

 seqid, slotID, slotID, cache */ \

 verifier: AUTH_NULL, length 0 */\

 status */ \

 MIN tag is length with zero, only length */ \

 opcount, opcode, opstatus*/ \

 seqid, slotID, slotID, slotID, status */ \

	/*

	 * Note decreasing slot size below client's request may make it

	 * difficult for client to function correctly, whereas

	 * decreasing the number of slots will (just?) affect

	 * performance.  When short on memory we therefore prefer to

	 * decrease number of slots instead of their size.  Clients that

	 * request larger slots than they need will get poor results:

	 * Note that we always allow at least one slot, because our

	 * accounting is soft and provides no guarantees either way.

/*

 * Server's NFSv4.1 backchannel support is AUTH_SYS-only for now.

 * These are based on similar macros in linux/sunrpc/msg_prot.h .

		/*

		 * GSS case: the spec doesn't allow us to return this

		 * error.  But it also doesn't allow us not to support

		 * GSS.

		 * I'd rather this fail hard than return some error the

		 * client might think it can already handle:

 an unconfirmed replay returns misordered */

 Persistent sessions are not supported */

 Upshifting from TCP to RDMA is not supported */

 cache solo and embedded create sessions under the client_lock */

 init connection and backchannel */

 Following the last paragraph of RFC 5661 Section 18.34.3: */

 oops; xprt is already down: */

	/*

	 * If there's an error then the reply can have fewer ops than

	 * the call.

	/*

	 * But if we cached a reply with *more* ops than the call you're

	 * sending us now, then this new call is clearly not really a

	 * replay of the old one:

 This is the only check explicitly called by spec: */

	/*

	 * There may be more comparisons we could actually do, but the

	 * spec doesn't require us to catch every case where the calls

	 * don't match (that would require caching the call as well as

	 * the reply), so we don't bother.

	/*

	 * Will be either used or freed by nfsd4_sequence_check_conn

	 * below.

	/* We do not negotiate the number of slots yet, so set the

	 * maxslots to the session maxreqs which is used to encode

		/* Return the cached reply status and set cstate->status

 Success! bump slot seqid */

 Drop session reference that was taken in nfsd4_sequence() */

		/*

		 * We don't take advantage of the rca_one_fs case.

		 * That's OK, it's optional, we can safely ignore it.

		/*

		 * The following error isn't really legal.

		 * But we only get here if the client just explicitly

		 * destroyed the client.  Surely it no longer cares what

		 * error it gets back on an operation for the dead

		 * client.

	/*

	 * We try hard to give out unique clientid's, so if we get an

	 * attempt to confirm the same clientid with a different cred,

	 * the client may be buggy; this should never happen.

	 *

	 * Nevertheless, RFC 7530 recommends INUSE for this case:

 OPEN Share state helper functions */

 ignore lock owners */

 Lock the stateid st_mutex, and deal with races with CLOSE */

 We are moving these outside of the spinlocks to avoid the warnings */

 Handle races with CLOSE */

 To keep mutex tracking happy */

/*

 * In the 4.0 case we need to keep the owners around a little while to handle

 * CLOSE replay. We still do need to release any file access that is held by

 * them before returning however.

	/*

	 * We know that we hold one reference via nfsd4_close, and another

	 * "persistent" reference for the client. If the refcount is higher

	 * than 2, then there are still calls in progress that are using this

	 * stateid. We can't put the sc_file reference until they are finished.

	 * Wait for the refcount to drop to 2. Since it has been unhashed,

	 * there should be no danger of the refcount going back up again at

	 * this point.

 search file_hashtbl[] for file */

/*

 * Called to check deny when READ with all zero stateid or

 * WRITE with all zero or all one stateid

 Check for conflicting share reservations */

	/*

	 * We can't do this in nfsd_break_deleg_cb because it is

	 * already holding inode->i_lock.

	 *

	 * If the dl_time != 0, then we know that it has already been

	 * queued for a lease break. Don't queue it again.

		/*

		 * Race: client probably got cb_recall before open reply

		 * granting delegation.

	/*

	 * We're assuming the state code never drops its reference

	 * without first removing the lease.  Since we're in this lease

	 * callback (and since the lease code is serialized by the

	 * i_lock) we know the server hasn't removed the lease yet, and

	 * we know it's safe to take a reference.

 Called from break_lease() with i_lock held. */

	/*

	 * We don't want the locks code to timeout the lease for us;

	 * we'll remove it ourself if a delegation isn't returned

	 * in time:

 Note rq_prog == NFS_ACL_PROGRAM is also possible: */

	/*

	 * We're in the 4.0 case (otherwise the SEQUENCE op would have

	 * set cstate->clp), so session = false:

	/*

	 * In case we need it later, after we've already created the

	 * file and don't want to risk a further failure:

 Replace unconfirmed owners without checking for replay. */

	/*

	 * Are we trying to set a deny mode that would conflict with

	 * current access?

 set access to the file */

 Set access bits in stateid */

 Set new deny mask */

 test and set deny mode */

 Should we give out recallable state?: */

	/*

	 * In the sessions case, since we don't have to establish a

	 * separate connection for callbacks, we assume it's OK

	 * until we hear otherwise:

	/*

	 * There could be multiple filehandles (hence multiple

	 * nfs4_files) referencing this file, but that's not too

	 * common; let's just give up in that case rather than

	 * trying to go look up all the clients using that other

	 * nfs4_file as well:

	/*

	 * If there's a close in progress, make sure that we see it

	 * clear any fi_fds[] entries before we see it decrement

	 * i_writecount:

 There may be non-NFSv4 writers */

	/*

	 * It's possible there are non-NFSv4 write opens in progress,

	 * but if they haven't incremented i_writecount yet then they

	 * also haven't called break lease yet; so, they'll break this

	 * lease soon enough.  So, all that's left to check for is NFSv4

	 * opens:

 it's an open */ &&

	/*

	 * There's a small chance that we could be racing with another

	 * NFSv4 open.  However, any open that hasn't added itself to

	 * the fi_stateids list also hasn't called break_lease yet; so,

	 * they'll break this lease soon enough.

	/*

	 * The fi_had_conflict and nfs_get_existing_delegation checks

	 * here are just optimizations; we'll need to recheck them at

	 * the end:

		/*

		 * We probably could attempt another open and get a read

		 * delegation, but for now, don't bother until the

		 * client actually sends us one.

		/* increment early to prevent fi_deleg_file from being

/*

 * Attempt to hand out a delegation.

 *

 * Note we don't support write delegations, and won't until the vfs has

 * proper support for them.

			/*

			 * Let's not give out any delegations till everyone's

			 * had the chance to reclaim theirs, *and* until

			 * NLM locks have all been reclaimed:

 4.1 client asking for a delegation? */

	/* Otherwise the client must be confused wanting a delegation

	 * it already has, therefore we don't return

	 * NFS4_OPEN_DELEGATE_NONE_EXT and reason.

	/*

	 * Lookup file; if found, lookup stateid and check open request,

	 * and check for delegations in the process of being recalled.

	 * If not found, create the nfs4_file struct

	/*

	 * OPEN the file, or upgrade an existing OPEN.

	 * If truncate fails, the OPEN fails.

	 *

	 * stp is already locked.

 Stateid was found, this is an OPEN upgrade */

	/*

	* Attempt to hand out a delegation. No error return, because the

	* OPEN succeeds even if we fail.

 4.1 client trying to upgrade/downgrade delegation? */

	/*

	* To finish the open response, we just need to set the rflags.

 do nothing if grace period already ended */

	/*

	 * If the server goes down again right now, an NFSv4

	 * client will still be allowed to reclaim after it comes back up,

	 * even if it hasn't yet had a chance to reclaim state this time.

	 *

	/*

	 * At this point, NFSv4 clients can still reclaim.  But if the

	 * server crashes, any that have not yet reclaimed will be out

	 * of luck on the next boot.

	 *

	 * (NFSv4.1+ clients are considered to have reclaimed once they

	 * call RECLAIM_COMPLETE.  NFSv4.0 clients are considered to

	 * have reclaimed after their first OPEN.)

	/*

	 * At this point, and once lockd and/or any other containers

	 * exit their grace period, further reclaims will fail and

	 * regular locking can resume.

/*

 * If we've waited a lease period but there are still clients trying to

 * reclaim, wait a little longer to give them a chance to finish.

	/*

	 * If we've given them *two* lease times to reclaim, and they're

	 * still not done, give up:

/*

 * This is called when nfsd is being shutdown, after all inter_ssc

 * cleanup were done, to destroy the ssc delayed unmount list.

 mark being unmount */

 waiters need to start from begin of list */

 wakeup ssc_connect waiters */

	/*

	 * It's possible for a client to try and acquire an already held lock

	 * that is being held for a long time, and then lose interest in it.

	 * So, we clean out any un-revisited request after a lease period

	 * under the assumption that the client is no longer interested.

	 *

	 * RFC5661, sec. 9.6 states that the client must not rely on getting

	 * notifications and must continue to poll for locks, even when the

	 * server supports them. Thus this shouldn't lead to clients blocking

	 * indefinitely once the lock does become free.

 service the server-to-server copy delayed unmount list */

 For lock stateid's, we test the parent open, not the lock: */

		/* Answer in remaining cases depends on existence of

 (flags & RD_STATE) && ZERO_STATEID(stateid) */

	/*

	 * When sessions are used the stateid generation number is ignored

	 * when it is zero.

 If the client sends us a stateid from the future, it's buggy: */

	/*

	 * However, we could see a stateid from the past, even from a

	 * non-buggy client.  For example, if the client sends a lock

	 * while some IO is outstanding, the lock may bump si_generation

	 * while the IO is still in flight.  The client could avoid that

	 * situation by waiting for responses on all the IO requests,

	 * but better performance may result in retrying IO that

	 * receives an old_stateid error if requests are rarely

	 * reordered in flight:

	/*

	 *  only return revoked delegations if explicitly asked.

	 *  otherwise we report revoked or bad_stateid status.

/*

 * A READ from an inter server to server COPY will have a

 * copy stateid. Look up the copy notify stateid from the

 * idr structure and take a reference on it.

/*

 * Checks for stateid operations

/*

 * Test if the stateid is valid

 Default falls through and returns nfserr_bad_stateid */

/* 

 * Checks for sequence id mutating operations. 

 We don't yet support WANT bits: */

/*

 * nfs4_unlock_state() called after encode

	/*

	 * Technically we don't _really_ have to increment or copy it, since

	 * it should just be gone after this operation and we clobber the

	 * copied value below, but we continue to do so here just to ensure

	 * that racing ops see that there was a state change.

	/* v4.1+ suggests that we send a special stateid in here, since the

	 * clients should just ignore this anyway. Since this is not useful

	 * for v4.0 clients either, we set it to the special close_stateid

	 * universally.

	 *

	 * See RFC5661 section 18.2.4, and RFC7530 section 16.2.5

 put reference from nfs4_preprocess_seqid_op */

 last octet in a range */

/*

 * TODO: Linux file offsets are _signed_ 64-bit quantities, which means that

 * we can't properly handle lock requests that go beyond the (2^63 - 1)-th

 * byte, because of sign extension problems.  Since NFSv4 calls for 64-bit

 * locking, this prevents us from being completely protocol-compliant.  The

 * real solution to this problem is to start using unsigned file offsets in

 * the VFS, but this is a very deep change!

 An empty list means that something else is going to be using it */

 We just don't care that much */

/*

 * Alloc a lock owner structure.

 * Called in nfsd4_lock - therefore, OPEN and OPEN_CONFIRM (if needed) has 

 * occurred. 

 *

 * strhashval = ownerstr_hashval

 If ost is not hashed, ost->st_locks will not be valid */

 To keep mutex tracking happy */

 with an existing lockowner, seqids must be the same */

/*

 *  LOCK operation 

 See rfc 5661 18.10.3: given clientid is ignored: */

 validate and update open stateid and open seqid */

 success! */

 conflock holds conflicting lock */

 dequeue it if we queued it before */

 Bump seqid manually if the 4.0 replay owner is openowner */

		/*

		 * If this is a new, never-before-used stateid, and we are

		 * returning an error, then just go ahead and release it.

/*

 * The NFSv4 spec allows a client to do a LOCKT without holding an OPEN,

 * so we do a temporary open here just to get an open file to pass to

 * vfs_test_lock.

 to block new leases till after test_lock: */

/*

 * LOCKT operation

/*

 * returns

 * 	true:  locks held by lockowner

 * 	false: no locks held by lockowner

 Any valid lock stateid should have some sort of access */

 Find the matching lock stateowner */

 see if there are still any locks associated with it */

/*

 * failure => all reset bets are off, nfserr_no_grace...

 *

 * The caller is responsible for freeing name.data if NULL is returned (it

 * will be freed in nfs4_remove_reclaim_record in the normal case).

/*

/*

 * Since the lifetime of a delegation isn't limited to that of an open, a

 * client may quite reasonably hang on to a delegation as long as it has

 * the inode cached.  This becomes an obvious problem the first time a

 * client's inode cache approaches the size of the server's total memory.

 *

 * For now we avoid this problem by imposing a hard limit on the number

 * of delegations, which varies according to the server's memory size.

	/*

	 * Allow at most 4 delegations per megabyte of RAM.  Quick

	 * estimates suggest that in the worst case (where every delegation

	 * is for a different inode), a delegation could take about 1.5K,

	 * giving a worst case usage of about 6% of memory.

 initialization to perform when the nfsd service is started: */

/*

 * functions to set current state id

/*

 * functions to consume current state id

/*

 *  Server-side XDR for NFSv4

 *

 *  Copyright (c) 2002 The Regents of the University of Michigan.

 *  All rights reserved.

 *

 *  Kendrick Smith <kmsmith@umich.edu>

 *  Andy Adamson   <andros@umich.edu>

 *

 *  Redistribution and use in source and binary forms, with or without

 *  modification, are permitted provided that the following conditions

 *  are met:

 *

 *  1. Redistributions of source code must retain the above copyright

 *     notice, this list of conditions and the following disclaimer.

 *  2. Redistributions in binary form must reproduce the above copyright

 *     notice, this list of conditions and the following disclaimer in the

 *     documentation and/or other materials provided with the distribution.

 *  3. Neither the name of the University nor the names of its

 *     contributors may be used to endorse or promote products derived

 *     from this software without specific prior written permission.

 *

 *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

 *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE

 *  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE

 *  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR

 *  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF

 *  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR

 *  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF

 *  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING

 *  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS

 *  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

/*

 * As per referral draft, the fsid for a referral MUST be different from the fsid of the containing

 * directory in order to indicate to the client that a filesystem boundary is present

 * We use a fixed fsid for a referral

/**

 * svcxdr_tmpalloc - allocate memory to be freed after compound processing

 * @argp: NFSv4 compound argument structure

 * @len: length of buffer to allocate

 *

 * Allocates a buffer of size @len to be freed when processing the compound

 * operation described in @argp finishes.

/*

 * For xdr strings that need to be passed to other kernel api's

 * as null-terminated strings.

 *

 * Note null-terminating in place usually isn't safe since the

 * buffer might end on a page boundary.

	/*

	 * The location of the decoded data item is stable,

	 * so @p is OK to use. This is the common case.

/*

 * NFSv4 basic data type decoders

/*

 * This helper handles variable-length opaques which belong to protocol

 * elements that this implementation does not support.

/**

 * nfsd4_decode_bitmap4 - Decode an NFSv4 bitmap4

 * @argp: NFSv4 compound argument structure

 * @bmval: pointer to an array of u32's to decode into

 * @bmlen: size of the @bmval array

 *

 * The server needs to return nfs_ok rather than nfserr_bad_xdr when

 * encountering bitmaps containing bits it does not recognize. This

 * includes bits in bitmap words past WORDn, where WORDn is the last

 * bitmap WORD the implementation currently supports. Thus we are

 * careful here to simply ignore bits in bitmap words that this

 * implementation has yet to support explicitly.

 *

 * Return values:

 *   %nfs_ok: @bmval populated successfully

 *   %nfserr_bad_xdr: the encoded bitmap was invalid

 request sanity */

 A counted array of nfsace4's */

		/*

		 * Even with 4-byte names there wouldn't be

		 * space for that many aces; something fishy is

		 * going on:

 request sanity: did attrlist4 contain the expected number of words? */

 CONFIG_NFSD_PNFS */

 Defined in Appendix A of RFC 5531 */

 machine name */

 gcbp_handle_from_server */

 gcbp_handle_from_client */

 a counted array of callback_sec_parms4 items */

 callback_sec_params4 */

 Is this legal? Be generous, take it to mean AUTH_NONE: */

 void */

/*

 * NFSv4 operation argument decoders

 open_downgrade */

 Note: unlike access bits, deny bits may be zero. */

 void */

 deleg_want is ignored */

 deleg_want is ignored */

 Also used for NVERIFY */

	/* For convenience's sake, we compare raw xdr'd attributes in

/*

 * This implementation currently does not support SP4_SSV.

 * This decoder simply skips over these arguments.

 ssp_ops */

 ssp_hash_algs<> */

 ssp_encr_algs<> */

		/* Note that RFC 8881 places no length limit on

		 * nii_domain, but this implementation permits no

		/* Note that RFC 8881 places no length limit on

		 * nii_name, but this implementation permits no

 headerpadsz is ignored */

 CONFIG_NFSD_PNFS */

 XXX: not jukebox? */

 currently support for 1 inter-server source server */

 ca_consecutive: we always do consecutive copies */

 intra-server copy */

 decode all the supplied server addresses but use only the first */

 XXX: jukebox? */

/*

 * XDR data that is more than PAGE_SIZE in size is normally part of a

 * read or write. However, the size of extended attributes is limited

 * by the maximum request size, and then further limited by the underlying

 * filesystem limits. This can exceed PAGE_SIZE (currently, XATTR_SIZE_MAX

 * is 64k). Since there is no kvec- or page-based interface to xattrs,

 * and we're not dealing with contiguous pages, we need to do some copying.

/*

 * Decode data into buffer.

		/*

		 * We're in luck, the head has enough space. Just return

		 * the head, no need for copying.

/*

 * Get a user extended attribute name from the XDR buffer.

 * It will not have the "user." prefix, so prepend it.

 * Lastly, check for nul characters in the name.

	/*

	 * Copy the extended attribute name over while checking for 0

	 * characters.

/*

 * A GETXATTR op request comes without a length specifier. We just set the

 * maximum length for the reply based on XATTR_SIZE_MAX and the maximum

 * channel reply size. nfsd_getxattr will probe the length of the xattr,

 * check it against getxa_len, and allocate + return the value.

	/*

	 * If the cookie  is too large to have even one user.x attribute

	 * plus trailing '\0' left in a maximum size buffer, it's invalid.

 Always need at least 2 words (length and one character) */

 new operations for NFSv4.1 */

 new operations for NFSv4.2 */

 RFC 8276 extended atributes operations */

 opcnt, status */

	/*

	 * NFS4ERR_RESOURCE is a more helpful error than GARBAGE_ARGS

	 * here, so we return success at the xdr level so that

	 * nfsd4_proc can handle this is an NFS-level error.

		/*

		 * We'll try to cache the result in the DRC if any one

		 * op in the compound wants to be cached:

		/*

		 * OP_LOCK and OP_LOCKT may return a conflicting lock.

		 * (Special case because it will just skip encoding this

		 * if it runs out of xdr buffer space, and it is the only

		 * operation that behaves this way.)

 Sessions make the DRC unnecessary: */

/*

 * ctime (in NFSv4, time_metadata) is not writeable, and the client

 * doesn't really care what resolution could theoretically be stored by

 * the filesystem.

 *

 * The client cares how close together changes can be while still

 * guaranteeing ctime changes.  For most filesystems (which have

 * timestamps with nanosecond fields) that is limited by the resolution

 * of the time returned from current_time() (which I'm assuming to be

 * 1/HZ).

/* Encode as an array of strings the string given with components

 * separated @sep, escaped with esc_enter and esc_exit.

 We will fill this in with @count later */

 try to parse as esc_start, ..., esc_end, sep */

 find esc_exit or end of string */;

 find sep or end of string */;

/* Encode as an array of strings the string given with components

 * separated @sep.

/*

 * encode a location element of a fs_locations structure

/*

 * Encode a path in RFC3530 'pathname4' format

	/* First walk the path up to the nfsd root, and store the

	 * dentries/path components in an array.

/*

 *  encode a fs_locations structure

	/*

	 * For now we use a 0 here to indicate the null translation; in

	 * the future we may place a call to translation code here.

 lfs */

 pi */

 As per referral draft:  */

/*

 * Note: @fhp can be NULL; in this case, we might have to compose the filehandle

 * ourselves.

 CONFIG_NFSD_V4_SECURITY_LABEL */

 to be backfilled later */

		/*

		 * Get parent's attributes if not ignoring crossmount

		 * and this is the root of a cross-mounted filesystem.

 CONFIG_NFSD_PNFS */

 CONFIG_NFSD_V4_SECURITY_LABEL */

	/*

	 * In the case of a mountpoint, the client may be asking for

	 * attributes that are only properties of the underlying filesystem

	 * as opposed to the cross-mounted file system. In such a case,

	 * we will not follow the cross mount and will fill the attribtutes

	 * directly from the mountpoint dentry.

		/*

		 * Why the heck aren't we just using nfsd_lookup??

		 * Different "."/".." handling?  Something else?

		 * At least, add a comment here to explain....

 bmval0 */

 bmval1 */

 attribute length */

 no htonl */

 In nfsv4, "." and ".." never make it onto the wire.. */

 mark entry present */

 offset of next entry */

 name length & name */

		/*

		 * If the client requested the RDATTR_ERROR attribute,

		 * we stuff the error code into this attribute

		 * and continue.  If this attribute was not requested,

		 * then in accordance with the spec, we fail the

		 * entire READDIR operation(!)

	/*

	 * RFC 3530 14.2.24 describes rd_dircount as only a "hint", and

	 * notes that it could be zero. If it is zero, then the server

	 * should enforce only the rd_maxcount value.

 Upshifting from TCP to RDMA is not supported */

/*

* Including all fields other than the name, a LOCK4denied structure requires

*   8(clientid) + 4(namelen) + 8(offset) + 8(length) + 4(type) = 32 bytes.

		/*

		 * Don't fail to return the result just because we can't

		 * return the conflicting open:

 non - nfsv4 lock in conflict, no clientid nor owner */

 clientid */

 length of owner name */

		/*

		 * TODO: ACE's in delegations

 XXX: is NULL principal ok? */

		/*

		 * TODO: space_limit's in delegations

		/*

		 * TODO: ACE's in delegations

 XXX: is NULL principal ok? */

 4.1 */

 deleg signaling not supported yet: */

 XXX save filehandle here */

 Make sure there will be room for padding if needed */

 Use rest of head for padding and remaining ops: */

	/*

	 * nfsd_splice_actor may have already messed with the

	 * page length; reset it so as not to confuse

	 * xdr_truncate_encode in our caller.

 eof flag and byte count */

	/*

	 * XXX: By default, vfs_readlink() will truncate symlinks if they

	 * would overflow the buffer.  Is this kosher in NFSv4?  If not, one

	 * easy fix is: if vfs_readlink() precisely fills the buffer, assume

	 * that truncation occurred, and return NFS4ERR_RESOURCE.

 XXX: Following NFSv3, we ignore the READDIR verifier for now. */

	/*

	 * Number of bytes left for directory entries allowing for the

	 * final 8 bytes of the readdir and a following failed op:

	/*

	 * Note the rfc defines rd_maxcount as the size of the

	 * READDIR4resok structure, which includes the verifier above

	 * and the 8 bytes encoded at the end of this function:

 RFC 3530 14.2.24 allows us to ignore dircount when it's 0: */

 nothing encoded; which limit did we hit?: */

 It was the fault of rd_maxcount: */

 We ran out of buffer space: */

 no more entries */

 Handling of some defaults in absence of real secinfo: */

 to be backfilled later */

/*

 * The SETATTR encode routine is special -- it always encodes a bitmap,

 * regardless of the error status.

 eir_clientid */ +

 eir_sequenceid */ +

 eir_flags */ +

 spr_how */);

 spo_must_enforce bitmap: */

 spo_must_allow bitmap: */

 so_minor_id */ +

 so_major_id.len */ +

 eir_server_scope.len */ +

 eir_server_impl_id.count (0) */);

 The server_owner struct */

 Minor id */

 major id */

 Server scope */

 Implementation id */

 zero length nfs_impl_id4 array */

 headerpadsz */

 headerpadsz */

 Note slotid's are numbered from zero: */

 sr_highest_slotid */

 sr_target_highest_slotid */

 DRC cache data pointer */

 If maxcount is 0 then just update notifications */

			/*

			 * We don't bother to burden the layout drivers with

			 * enforcing gd_maxcount, just tell the client to

			 * come back with a bigger buffer if it's not enough.

 bitmap length */

 notifications */;

 we always set return-on-close */

 we always return a single layout */

 CONFIG_NFSD_PNFS */

		/* netid_len, netid, uaddr_len, uaddr (port included

		 * in RPCBIND_MAXUADDRLEN)

 netid len */ +

 uaddr len */ +

 cr_consecutive */

 Content type, offset, byte count */

 Content type, offset, byte count */

 eof flag, segment count */

 8 sec, 4 nsec */

 cnr_lease_time */

 cnr_stateid */

 cnr_src.nl_nsvr */

/*

 * Encode kmalloc-ed buffer in to XDR stream.

			/*

			 * We're done, with a length that wasn't page

			 * aligned, so possibly not word aligned. Pad

			 * any trailing bytes with 0.

/*

 * See if there are cookie values that can be rejected outright.

	/*

	 * If the cookie is larger than the maximum number we can fit

	 * in either the buffer we just got back from vfs_listxattr, or,

	 * XDR-encoded, in the return buffer, it's invalid.

	/*

	 * Reserve space for the cookie and the name array count. Record

	 * the offsets to save them later.

		/*

		 * Check if this is a "user." attribute, skip it if not.

				/*

				 * Can't even fit the first attribute name.

	/*

	 * If there were user attributes to copy, but we didn't copy

	 * any, the offset was too large (e.g. the cookie was invalid).

/*

 * Note: nfsd4_enc_ops vector is shared for v4.0 and v4.1

 * since we don't need to filter out obsolete ops as this is

 * done in the decoding phase.

 NFSv4.1 operations */

 NFSv4.2 operations */

 RFC 8276 extended atributes operations */

/*

 * Calculate whether we still have space to encode repsize bytes.

 * There are two considerations:

 *     - For NFS versions >=4.1, the size of the reply must stay within

 *       session limits

 *     - For all NFS versions, we must stay within limited preallocated

 *       buffer space.

 *

 * This is called before the operation is processed, so can only provide

 * an upper estimate.  For some nonidempotent operations (such as

 * getattr), it's not necessarily a problem if that estimate is wrong,

 * as we can fail it after processing without significant side effects.

 nfsd4_check_resp_size guarantees enough room for error status */

		/*

		 * The operation may have already been encoded or

		 * partially encoded.  No op returns anything additional

		 * in the case of one of these three errors, so we can

		 * just truncate back to after the status.  But it's a

		 * bug if we had to do this on a non-idempotent op:

 Note that op->status is already in network byte order: */

/* 

 * Encode the reply stored in the stateowner reply cache 

 * 

 * XDR note: do not encode rp->rp_buflen: the buffer contains the

 * previously sent already encoded operation.

 already xdr'ed */

 svcxdr_tmp_alloc */

	/*

	 * Send buffer space for the following items is reserved

	 * at the top of nfsd4_proc_compound().

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Process version 2 NFSACL requests.

 *

 * Copyright (C) 2002-2003 Andreas Gruenbacher <agruen@suse.de>

 FIXME: nfsacl.h is a broken header */

/*

 * NULL call.

/*

 * Get the Access and/or Default ACL of a file.

 Solaris returns the inode's minimum ACL. */

		/* Check how Solaris handles requests for the Default ACL

 resp->acl_{access,default} are released in nfssvc_release_getacl. */

/*

 * Set the Access and/or Default ACL of a file.

	/* argp->acl_{access,default} may have been allocated in

/*

 * Check file attributes

/*

 * Check file access

/*

 * XDR decode functions

/*

 * XDR encode functions

 GETACL */

 ACCESS */

/*

 * XDR release functions

 status*/

 attributes */

 post attributes - conditional */

 Access Control List */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017-2018 HUAWEI, Inc.

 *             https://www.huawei.com/

 based on the end of qn is accurate and it must have the trailing '\0' */

	/*

	 * on-disk error, let's only BUG_ON in the debugging mode.

	 * otherwise, it will return 1 to just skip the invalid name

	 * and go on (in consideration of the lookup performance).

 qd could not have trailing '\0' */

 However it is absolutely safe if < qd->end */

 See comments in __d_alloc on the terminating NUL character */

 since the 1st dirent has been evaluated previously */

 string comparison without already matched prefix */

 string comparison without already matched prefix */

 free if the candidate is valid */

 the target page has been mapped */

 NOTE: i_mutex is already held by vfs */

 dentry must be unhashed in lookup, no need to worry about */

 file name exceeds fs limit */

 false uninitialized warnings on gcc 4.8.x */

 negative dentry */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017-2018 HUAWEI, Inc.

 *             https://www.huawei.com/

 * Copyright (C) 2021, Alibaba Cloud

 should already be PageUptodate */

 there is no hole in flatmode */

 2 - inode inline B: inode, [xattrs], inline last blk... */

 inline data should be located in one meta block */

 leave out-of-bound access unmapped */

 chunk index */

 block map */

 handle block map */

 parse chunk indexes */

 primary device by default */

/*

 * since we dont have write or truncate flows, so no inode

 * locking needs to be held at the moment.

 no need taking (shared) inode lock since it's a ro filesystem */

 for uncompressed (aligned) files and raw access for other files */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017-2018 HUAWEI, Inc.

 *             https://www.huawei.com/

 to allow for x86 boot sectors and other oddities. */

 zero out everything except vfs_inode */

 be careful of RCU symlink path */

 check if current kernel meets all mandatory requirements */

 read variable-sized metadata, offset will be aligned by 4-byte */

 9(512 bytes) + LOG_SECTORS_PER_BLOCK == LOG_BLOCK_SIZE */

 -E2BIG */

 parse on-disk compression configurations */

 handle multiple devices */

 set up default EROFS parameters */

 0 - busy */

 Check for potential overflow in debug mode */

 get the root inode */

 sb->s_umount is already locked, SB_ACTIVE and SB_BORN are not set */

/*

 * could be triggered after deactivate_locked_super()

 * is called, thus including umount and failed to initialize.

 called when ->s_root is non-NULL */

 Ensure all RCU free inodes / pclusters are safe to be destroyed. */

 get filesystem statistics */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017-2018 HUAWEI, Inc.

 *             https://www.huawei.com/

 the only user of kunmap() is 'init_inode_xattrs' */

 the most case is that xattrs of this inode are initialized. */

		/*

		 * paired with smp_mb() at the end of the function to ensure

		 * fields will only be observed after the bit is set.

 someone has initialized xattrs for us? */

	/*

	 * bypass all xattr operations if ->xattr_isize is not greater than

	 * sizeof(struct erofs_xattr_ibody_header), in detail:

	 * 1) it is not enough to contain erofs_xattr_ibody_header then

	 *    ->xattr_isize should be 0 (it means no xattr);

	 * 2) it is just to contain erofs_xattr_ibody_header, which is on-disk

	 *    undefined right now (maybe use later with some new sb feature).

 xattr ondisk layout error */

 read in shared xattr array (non-atomic, see kmalloc below) */

 let's skip ibody header */

 cannot be unaligned */

 paired with smp_mb() at the beginning of the function. */

/*

 * the general idea for these return values is

 * if    0 is returned, go on processing the current xattr;

 *       1 (> 0) is returned, skip this round to process the next xattr;

 *    -err (< 0) is returned, an error (maybe ENOXATTR) occurred

 *                            and need to be handled

/*

 * Regardless of success or failure, `xattr_foreach' will end up with

 * `ofs' pointing to the next xattr item rather than an arbitrary position.

 0. fixup blkaddr, ofs, ipage */

	/*

	 * 1. read xattr entry to the memory,

	 *    since we do EROFS_XATTR_ALIGN

	 *    therefore entry should be in the page

 xattr on-disk corruption: xattr entry beyond xattr_isize */

 handle entry */

 2. handle xattr name (ofs will finally be at the end of name) */

 handle name */

 3. handle xattr value */

 xattrs should be 4-byte aligned (on-disk constraint) */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017-2018 HUAWEI, Inc.

 *             https://www.huawei.com/

 * Copyright (C) 2021, Alibaba Cloud

/*

 * if inode is successfully read, return its inode page (or sometimes

 * the inode payload page if it's an extended inode) in order to fill

 * inline data if possible.

 check if the inode acrosses page boundary */

 extended inode has its own timestamp */

 total blocks for compressed files */

 fill chunked inode summary info */

 use build time for compact inodes */

 measure inode.i_blocks as generic filesystems */

 if it cannot be handled with fast symlink scheme */

 inline symlink data shouldn't cross page boundary as well */

 read inode base data from disk */

 setup the new inode */

/*

 * erofs nid is 64bits, but i_ino is 'unsigned long', therefore

 * we should do more for 32-bit platform to find the right inode.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017-2018 HUAWEI, Inc.

 *             https://www.huawei.com/

 since the on-disk name could not have the trailing '\0' */

 the last dirent in the block? */

 a corrupted entry is found */

 stopped by some reason */

 search dirents at the arbitrary position */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2018 HUAWEI, Inc.

 *             https://www.huawei.com/

 global shrink count (for all mounted EROFS instances) */

 decrease refcount paired by erofs_workgroup_put */

 prefer to relax rcu read side */

	/*

	 * Bump up a reference count before making this visible

	 * to others for the XArray in order to avoid potential

	 * UAF without serialized by xa_lock.

 try to legitimize the current in-tree one */

	/*

	 * If managed cache is on, refcount of workgroups

	 * themselves could be < 0 (freezed). In other words,

	 * there is no guarantee that all refcounts > 0.

	/*

	 * Note that all cached pages should be unattached

	 * before deleted from the XArray. Otherwise some

	 * cached pages could be still attached to the orphan

	 * old workgroup when the new one is available in the tree.

	/*

	 * It's impossible to fail after the workgroup is freezed,

	 * however in order to avoid some race conditions, add a

	 * DBG_BUGON to observe this in advance.

 last refcount should be connected with its managed pslot.  */

 try to shrink each valid workgroup */

 protected by 'erofs_sb_list_lock' */

 protects the mounted 'erofs_sb_list' */

 clean up all remaining workgroups in memory */

 Iterate over all mounted superblocks and try to shrink them */

		/*

		 * We move the ones we do to the end of the list, so we stop

		 * when we see one we have already done.

 Get the next list element before we move this one */

		/*

		 * Move this one to the end of the list to provide some

		 * fairness.

 !CONFIG_EROFS_FS_ZIP */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2018 HUAWEI, Inc.

 *             https://www.huawei.com/

/*

 * since pclustersize is variable for big pcluster feature, introduce slab

 * pools implementation for different pcluster sizes.

 how to allocate cached pages for a pcluster */

 don't allocate any cached pages */

	/*

	 * try to use cached I/O if page allocation succeeds or fallback

	 * to in-place I/O instead to avoid any direct reclaim.

/*

 * tagged pointer with 1-bit tag for all compressed pages

 * tag 0 - the page is just found with an extra page reference

	/*

	 * no need to spawn too many threads, limiting threads could minimum

	 * scheduling overhead, perhaps per-CPU threads should be better?

	/*

	 * The current collection was the tail of an exist chain, in addition

	 * that the previous processed chained collections are all decided to

	 * be hooked up to it.

	 * A new chain will be created for the remaining collections which are

	 * not processed yet, therefore different from COLLECT_PRIMARY_FOLLOWED,

	 * the next collection cannot reuse the whole page safely in

	 * the following scenario:

	 *  ________________________________________________________________

	 * |      tail (partial) page     |       head (partial) page       |

	 * |   (belongs to the next cl)   |   (belongs to the current cl)   |

	 * |_______PRIMARY_FOLLOWED_______|________PRIMARY_HOOKED___________|

	/*

	 * a weak form of COLLECT_PRIMARY_FOLLOWED, the difference is that it

	 * could be dispatched into bypass queue later due to uptodated managed

	 * pages. All related online pages cannot be reused for inplace I/O (or

	 * pagevec) since it can be directly decoded without I/O submission.

	/*

	 * The current collection has been linked with the owned chain, and

	 * could also be linked with the remaining collections, which means

	 * if the processing page is the tail page of the collection, thus

	 * the current collection can safely use the whole page (since

	 * the previous collection is under control) for in-place I/O, as

	 * illustrated below:

	 *  ________________________________________________________________

	 * |  tail (partial) page |          head (partial) page           |

	 * |  (of the current cl) |      (of the previous collection)      |

	 * |  PRIMARY_FOLLOWED or |                                        |

	 * |_____PRIMARY_HOOKED___|____________PRIMARY_FOLLOWED____________|

	 *

	 * [  (*) the above page can be used as inplace I/O.               ]

 a pointer used to pick up inplace I/O pages */

 used for applying cache strategy on the fly */

 the compressed page was loaded before */

 I/O is needed, no possible to decompress directly */

 DONTALLOC */

	/*

	 * don't do inplace I/O if all compressed pages are available in

	 * managed cache since it can be moved to the bypass queue instead.

 called by erofs_shrinker to get rid of all compressed_pages */

	/*

	 * refcount of workgroup is now freezed as 1,

	 * therefore no need to worry about available decompression users.

 block other users from reclaiming or migrating the page */

 barrier is implied in the following 'unlock_page' */

 0 - busy */

 page_type must be Z_EROFS_PAGE_TYPE_EXCLUSIVE */

 callers must be with collection lock held */

 give priority for inplaceio */

 type 1, nil pcluster (this pcluster doesn't belong to any chain.) */

 so we can attach this pcluster to our submission chain. */

	/*

	 * type 2, link to the end of an existing open chain, be careful

	 * that its submission is controlled by the original attached chain.

 type 3, it belongs to a chain, but it isn't the end of the chain */

 to avoid unexpected loop formed by corrupted images */

 used to check tail merging loop due to corrupted images */

 no available pcluster, let's allocate one */

 new pclusters should be claimed as type 1, primary and followed */

	/*

	 * lock all primary followed works before visible to others

	 * and mutex_trylock *never* fails for a new pcluster.

 used to check tail merging loop due to corrupted images */

 must be Z_EROFS_PCLUSTER_TAIL or pointed to previous collection */

 since file-backed online pages are traversed in reverse order */

/*

 * keep in mind that no referenced pclusters will be freed

 * only after a RCU grace period.

	/*

	 * if all pending pages are added, don't hold its reference

	 * any longer if the pcluster isn't hosted by ourselves.

 register locked file pages as online pages in pack */

 lucky, within the range of the current map_blocks */

 didn't get a valid collection previously (very rare) */

 go ahead the next map_blocks */

 preload all compressed pages (maybe downgrade role if necessary) */

	/*

	 * Ensure the current partial page belongs to this submit chain rather

	 * than other concurrent submit chains or the noio(bypass) chain since

	 * those chains are handled asynchronously thus the page cannot be used

	 * for inplace I/O or pagevec (should be processed in strict order.)

 let's derive page type */

 should allocate an additional short-lived page for pagevec */

 bump up the number of spiltted parts of a page */

 also update nr_pages */

 can be used for verification */

 if some error occurred while processing this page */

 wake up the caller thread for sync decompression */

 Use workqueue and sync decompression for atomic contexts only */

 fallback to global pagemap for the lowmem scenario */

 all pages in pagevec ought to be valid */

		/*

		 * currently EROFS doesn't support multiref(dedup),

		 * so here erroring out one multiref page.

 all compressed pages ought to be valid */

			/*

			 * only if non-head page can be selected

			 * for inplace decompression

 PG_error needs checking for all non-managed pages */

 must handle all compressed pages before ending pages */

 recycle all individual short-lived pages */

 recycle all individual short-lived pages */

 all cl locks MUST be taken before the following line */

 all cl locks SHOULD be released right now */

 no possible that 'owned' equals Z_EROFS_WORK_TPTR_TAIL */

 no possible that 'owned' equals NULL */

 process the target tagged pointer */

	/*

	 * preallocated cached pages, which is used to avoid direct reclaim

	 * otherwise, it will go inplace I/O path instead.

	/*

	 * file-backed online pages in plcuster are all locked steady,

	 * therefore it is impossible for `mapping' to be NULL.

 ought to be unmanaged pages */

 directly return for shortlived page as well */

 only true if page reclaim goes wrong, should never happen */

 the page is still in manage cache */

			/*

			 * impossible to be !PagePrivate(page) for

			 * the current restriction as well if

			 * the page is already in compressed_pages[].

 no need to submit io if it is already up-to-date */

	/*

	 * the managed page has been truncated, it's unsafe to

	 * reuse this one, let's allocate a new cache-managed page.

 turn into temporary page if fails (1 ref) */

 drop a refcount added by allocpage (then we have 2 refs here) */

 the only exit (for tracing and debugging) */

 define decompression jobqueue types */

	/*

	 * if managed cache is enabled, bypass jobqueue is needed,

	 * no need to read from device for all pclusters in this queue.

 bio is NULL initially, so no need to initialize last_{index,bdev} */

 by default, all need io submission */

 no possible 'owned_head' equals the following */

 no device id here, thus it will always succeed */

 close the main owned chain at first */

	/*

	 * although background is preferred, no one is pending for submission.

	 * don't issue workqueue for decompression but drop it directly instead.

 handle bypass queue (no i/o pclusters) immediately */

 wait until all bios are completed */

 handle synchronous decompress queue in the caller context */

/*

 * Since partial uptodate is still unimplemented for now, we have to use

 * approximate readmore strategies as a start.

 expend ra for the trailing edge if readahead */

 if some compressed cluster ready, need submit them anyway */

 traversal in reverse order */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2018-2019 HUAWEI, Inc.

 *             https://www.huawei.com/

		/*

		 * paired with smp_mb() at the end of the function to ensure

		 * fields will only be observed after the bit is set.

 paired with smp_mb() at the beginning of the function */

 compression extent information gathered */

 vcnt - 1 (Z_EROFS_VLE_CLUSTER_TYPE_NONHEAD) item */

 figure out lookahead_distance: delta[1] if needed */

		/*

		 * since the last lcluster in the pack is special,

		 * of which lo saves delta[1] rather than delta[0].

		 * Hence, get delta[0] by the previous lcluster indirectly.

 figout out blkaddr (pblk) for HEAD lclusters */

 bigpcluster shouldn't have plain d0 == 1 */

 used to align to 32-byte (compacted_2b) alignment */

 load extent head logical cluster if needed */

	/*

	 * If the 1st NONHEAD lcluster has already been handled initially w/o

	 * valid compressedlcs, which means at least it mustn't be CBLKCNT, or

	 * an internal implemenatation error is detected.

	 *

	 * The following code can also handle it properly anyway, but let's

	 * BUG_ON in the debugging mode only for developers to notice that.

		/*

		 * if the 1st NONHEAD lcluster is actually PLAIN or HEAD type

		 * rather than CBLKCNT, it's a 1 lcluster-sized pcluster.

 handle the last EOF pcluster (no next HEAD lcluster) */

 go on until the next HEAD lcluster */

 when trying to read beyond EOF, leave it unmapped */

 m.lcn should be >= 1 if endoff < m.clusterofs */

 get the corresponding first chunk */

 aggressively BUG_ON iff CONFIG_EROFS_FS_DEBUG is on */

		/*

		 * No strict rule how to describe extents for post EOF, yet

		 * we need do like below. Otherwise, iomap itself will get

		 * into an endless loop on post EOF.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2019 HUAWEI, Inc.

 *             https://www.huawei.com/

 history window size */

 set to maximum value by default */

 reserved case */

/*

 * Fill all gaps with bounce pages if it's a sparse page list. Also check if

 * all physical pages are consecutive, which can be seen for moderate CR.

 'valid' bounced can only be tested after a complete round */

 Or copy compressed data which can be overlapped to per-CPU buffer */

 decompression inplace is only safe when 0padding is enabled */

 legacy format could compress extra data in a pcluster. */

 one optimized fast path only for non bigpcluster cases yet */

 general decoding path which can be used for all cases */

 SPDX-License-Identifier: GPL-2.0-or-later

 considering the LZMA performance, no need to use a lockless list for now */

 there should be no running fs instance */

 by default, use # of possible CPUs instead */

 in case 2 z_erofs_load_lzma_config() race to avoid deadlock */

 1. collect/isolate all streams for the following check */

 2. walk each isolated stream and grow max dict_size if needed */

 3. push back all to the global list and update max dict_size */

 1. get the exact LZMA compressed size */

 2. get an available lzma context */

 3. multi-call decompress */

		/*

		 * Handle overlapping: Use bounced buffer if the compressed

		 * data is under processing; Otherwise, Use short-lived pages

		 * from the on-stack pagepool where pages share with the same

		 * request.

 4. push back LZMA stream context to the global list */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) Gao Xiang <xiang@kernel.org>

 *

 * For low-latency decompression algorithms (e.g. lz4), reserve consecutive

 * per-CPU virtual memory (in pages) in advance to store such inplace I/O

 * data if inplace decompression is failed (due to unmet inplace margin for

 * example).

 check if the per-CPU buffer is too small */

 (for sparse checker) pretend pcb->lock is still taken */

 the next step: support per-CPU page buffers hotplug */

 avoid shrinking pcpubuf, since no idea how many fses rely on */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/ext2/acl.c

 *

 * Copyright (C) 2001-2003 Andreas Gruenbacher, <agruen@suse.de>

/*

 * Convert from filesystem to in-memory representation.

/*

 * Convert from in-memory to filesystem representation.

/*

 * inode->i_mutex: don't care

/*

 * inode->i_mutex: down

/*

 * Initialize the ACLs of a new inode. Called from ext2_new_inode.

 *

 * dir->i_mutex: down

 * inode->i_mutex: up (access to inode is still exclusive)

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/ext2/xattr_user.c

 * Handler for extended user attributes.

 *

 * Copyright (C) 2001 by Andreas Gruenbacher, <a.gruenbacher@computer.org>

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/ext2/ioctl.c

 *

 * Copyright (C) 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 Is it quota file? Do not allow user to mess with it */

		/*

		 * need to allocate reservation structure for this inode

		 * before set the window size

		/*

		 * XXX What lock should protect the rsv_goal_size?

		 * Accessed in ext2_get_block only.  ext3 uses i_truncate.

 These are just misnamed, they actually get/put from/to user an int */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/ext2/namei.c

 *

 * Rewrite to pagecache. Almost all code had been changed, so blame me

 * if the things go wrong. Please, send bug reports to

 * viro@parcelfarce.linux.theplanet.co.uk

 *

 * Stuff here is basically a glue between the VFS and generic UNIXish

 * filesystem that keeps everything in pagecache. All knowledge of the

 * directory layout is in fs/ext2/dir.c - it turned out to be easily separatable

 * and it's easier to debug that way. In principle we might want to

 * generalize that a bit and turn it into a library. Or not.

 *

 * The only non-static object here is ext2_dir_inode_operations.

 *

 * TODO: get rid of kmap() use, add readahead.

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/namei.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

/*

 * Methods themselves.

/*

 * By the time this is called, we already have created

 * the directory cache entry for the new file, but it

 * is so far negative - it has no inode.

 *

 * If the create succeeds, we fill in the inode information

 * with d_instantiate(). 

 slow symlink */

 fast symlink */

	/*

	 * Like most other Unix systems, set the ctime for inodes on a

 	 * rename.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/fs/ext2/super.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/inode.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

/*

 * This must be called with sbi->s_lock held.

 leave es->s_feature_*compat flags alone */

 es->s_uuid will be set by e2fsck if empty */

	/*

	 * The rest of the superblock fields should be zero, and if not it

	 * means they are likely already in use, so leave them alone.  We

	 * can leave it up to e2fsck to clean up any inconsistencies there.

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

	/*

	 * ext2_iget isn't quite right if the inode is currently unallocated!

	 * However ext2_iget currently does appropriate checks to handle stale

	 * inodes so everything is OK.

 we didn't find the right inode.. */

 Default location */

 handled by get_sb_block() instead of here */

 *sb_block = match_int(&args[0]); */

/*

 * Maximal file size.  There is a direct, and {,double-,triple-}indirect

 * block limit, and also a limit of (2^32 - 1) 512-byte sectors in i_blocks.

 * We need to be 1 filesystem block less than the 2^32 sector limit.

	/* This is calculated to be the largest file size for a

	 * dense, file such that the total number of

	 * sectors in the file, including data and all indirect blocks,

	 * does not exceed 2^32 -1

	 * __u32 i_blocks representing the total number of

	 * 512 bytes blocks of the file

 total blocks in file system block size */

 Compute how many blocks we can address by block tree */

 Does block tree limit file size? */

 How many metadata blocks are needed for addressing upper_limit? */

 indirect blocks */

 double indirect blocks */

 tripple indirect blocks for the rest */

	/*

	 * See what the current blocksize for the device is, and

	 * use that as the blocksize.  Otherwise (or if the blocksize

	 * is smaller than the default) use the default.

	 * This is important for devices that have a hardware

	 * sectorsize that is larger than the default.

	/*

	 * If the superblock doesn't start on a hardware sector boundary,

	 * calculate the offset.  

	/*

	 * Note: s_es must be initialized as soon as possible because

	 *       some ext2 macro-instructions depend on its value

 Set defaults before we parse the mount options */

	/*

	 * Check feature flags regardless of the revision level, since we

	 * previously didn't change the revision level when setting the flags,

	 * so there is a chance incompat flags are set on a rev 0 filesystem.

 If the blocksize doesn't match, re-read the thing.. */

 per filesystem reservation list head & lock */

	/*

	 * Add a single, static dummy reservation to the start of the

	 * reservation window list --- it gives us a placeholder for

	 * append-at-start-of-list which makes the allocation logic

	 * _much_ simpler.

	/*

	 * set up enough so that it can read an inode

		/*

		 * Oh, dear.  A previous attempt to write the

		 * superblock failed.  This could happen because the

		 * USB device was yanked out.  Or it could happen to

		 * be a transient write error and maybe the block will

		 * be remapped.  Nothing we can do but to retry the

		 * write and hope for the best.

 unlock before we do IO */

/*

 * In the second extended file system, it is not necessary to

 * write the super block since we use a mapping of the

 * disk super block in a buffer.

 *

 * However, this function is still used to set the fs valid

 * flags to 0.  We need to set this flag to 0 since the fs

 * may have been checked while mounted and e2fsck may have

 * set s_state to EXT2_VALID_FS after some corrections.

	/*

	 * Write quota structures to quota file, sync_blockdev() will write

	 * them to disk later

	/*

	 * Open but unlinked files present? Keep EXT2_VALID_FS flag cleared

	 * because we have unattached inodes and thus filesystem is not fully

	 * consistent.

 Set EXT2_FS_VALID flag */

 Just write sb to clear EXT2_VALID_FS flag */

		/*

		 * OK, we are remounting a valid rw partition rdonly, so set

		 * the rdonly flag and then mark the partition as valid again.

		/*

		 * Mounting a RDONLY partition read-write, so reread and

		 * store the current valid flag.  (It may have been changed

		 * by e2fsck since we originally mounted the partition.)

		/*

		 * Compute the overhead (FS structures). This is constant

		 * for a given filesystem unless the number of block groups

		 * changes so we cache the previous value until it does.

		/*

		 * All of the blocks before first_data_block are

		 * overhead

		/*

		 * Add the overhead attributed to the superblock and

		 * block group descriptors.  If the sparse superblocks

		 * feature is turned on, then not all groups have this.

		/*

		 * Every block group has an inode bitmap, a block

		 * bitmap, and an inode table.

/* Read data from quotafile - avoid pagecache and such because we cannot afford

 * acquiring the locks... As quota files are never truncated and quota code

 * itself serializes the operations (and no one else should touch the files)

 A hole? */

 Write to quotafile */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ext2/ialloc.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  BSD ufs-inspired inode and directory allocation by 

 *  Stephen Tweedie (sct@dcs.ed.ac.uk), 1993

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

/*

 * ialloc.c contains the inodes allocation and deallocation routines

/*

 * The free inodes are managed by bitmaps.  A file system contains several

 * blocks groups.  Each group contains 1 bitmap block for blocks, 1 bitmap

 * block for inodes, N blocks for the inode table and data blocks.

 *

 * The file system contains group descriptors which are located after the

 * super block.  Each descriptor contains the number of the bitmap block and

 * the free blocks count in the block.

/*

 * Read the inode allocation bitmap for a given block_group, reading

 * into the specified slot in the superblock's bitmap cache.

 *

 * Return buffer_head of bitmap on success or NULL.

/*

 * NOTE! When we get the inode, we're the only people

 * that have access to it, and as such there are no

 * race conditions we have to worry about. The inode

 * is not on the hash-lists, and it cannot be reached

 * through the filesystem because the directory entry

 * has been deleted earlier.

 *

 * HOWEVER: we must make sure that we get no aliases,

 * which means that we have to call "clear_inode()"

 * _before_ we mark the inode not in use in the inode

 * bitmaps. Otherwise a newly created file might use

 * the same inode number (not actually the same pointer

 * though), and then we'd have two inodes sharing the

 * same inode number and space on the harddisk.

	/*

	 * Note: we must free any quota before locking the superblock,

	 * as writing the quota to disk may need the lock as well.

 Quota is already initialized in iput() */

 Ok, now we can actually update the inode bitmaps.. */

/*

 * We perform asynchronous prereading of the new inode's inode block when

 * we create the inode, in the expectation that the inode will be written

 * back soon.  There are two reasons:

 *

 * - When creating a large number of files, the async prereads will be

 *   nicely merged into large reads

 * - When writing out a large number of inodes, we don't need to keep on

 *   stalling the writes while we read the inode block.

 *

 * FIXME: ext2_get_group_desc() needs to be simplified.

	/*

	 * Figure out the offset within the block group inode table

/*

 * There are two policies for allocating an inode.  If the new inode is

 * a directory, then a forward search is made for a block group with both

 * free space and a low directory-to-inode ratio; if that fails, then of

 * the groups with above-average free space, that group with the fewest

 * directories already is chosen.

 *

 * For other inodes, search forward from the parent directory\'s block

 * group to find a free inode.

/* 

 * Orlov's allocator for directories. 

 * 

 * We always try to spread first-level directories.

 *

 * If there are blockgroups with both free inodes and free blocks counts 

 * not worse than average we return one with smallest directory count. 

 * Otherwise we simply return a random group. 

 * 

 * For the rest rules look so: 

 * 

 * It's OK to put directory into a group unless 

 * it has too many directories already (max_dirs) or 

 * it has too few free inodes left (min_inodes) or 

 * it has too few free blocks left (min_blocks) or 

 * it's already running too large debt (max_debt). 

 * Parent's group is preferred, if it doesn't satisfy these 

 * conditions we search cyclically through the rest. If none 

 * of the groups look good we just look for a group with more 

 * free inodes than average (starting at parent's group). 

 * 

 * Debt is incremented each time we allocate a directory and decremented 

 * when we allocate an inode, within 0--255. 

 percpu_counters are approximate... */

		/*

		 * The free-inodes counter is approximate, and for really small

		 * filesystems the above test can fail to find any blockgroups

	/*

	 * Try to place the inode in its parent directory

	/*

	 * We're going to place this inode in a different blockgroup from its

	 * parent.  We want to cause files in a common directory to all land in

	 * the same blockgroup.  But we want files which are in a different

	 * directory which shares a blockgroup with our parent to land in a

	 * different blockgroup.

	 *

	 * So add our directory's i_ino into the starting point for the hash.

	/*

	 * Use a quadratic hash to find a group with a free inode and some

	 * free blocks.

	/*

	 * That failed: try linear search for a free inode, even if that group

	 * has no free blocks.

			/*

			 * Rare race: find_group_xx() decided that there were

			 * free inodes in this group, but by the time we tried

			 * to allocate one, they're all gone.  This can also

			 * occur because the counters which find_group_orlov()

			 * uses are approximate.  So just go and search the

			 * next block group.

 we lost this inode */

 this group is exhausted, try next group */

 try to find free inode in the same group */

	/*

	 * Scanned all blockgroups.

 Called at mount-time, super-block is locked */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/ext2/xattr.c

 *

 * Copyright (C) 2001-2003 Andreas Gruenbacher <agruen@suse.de>

 *

 * Fix by Harrison Xing <harrison@mountainviewdata.com>.

 * Extended attributes for symlinks and special files added per

 *  suggestion of Luka Renko <luka.renko@hermes.si>.

 * xattr consolidation Copyright (c) 2004 James Morris <jmorris@redhat.com>,

 *  Red Hat Inc.

 *

/*

 * Extended attributes are stored on disk blocks allocated outside of

 * any inode. The i_file_acl field is then made to point to this allocated

 * block. If all extended attributes of an inode are identical, these

 * inodes may share the same extended attribute block. Such situations

 * are automatically detected by keeping a cache of recent attribute block

 * numbers and hashes over the block's contents in memory.

 *

 *

 * Extended attribute block layout:

 *

 *   +------------------+

 *   | header           |

 *   | entry 1          | |

 *   | entry 2          | | growing downwards

 *   | entry 3          | v

 *   | four null bytes  |

 *   | . . .            |

 *   | value 1          | ^

 *   | value 3          | | growing upwards

 *   | value 2          | |

 *   +------------------+

 *

 * The block header is followed by multiple entry descriptors. These entry

 * descriptors are variable in size, and aligned to EXT2_XATTR_PAD

 * byte boundaries. The entry descriptors are sorted by attribute name,

 * so that two extended attribute blocks can be compared efficiently.

 *

 * Attribute values are aligned to the end of the block, stored in

 * no specific order. They are also padded to EXT2_XATTR_PAD byte

 * boundaries. No additional gaps are left between them.

 *

 * Locking strategy

 * ----------------

 * EXT2_I(inode)->i_file_acl is protected by EXT2_I(inode)->xattr_sem.

 * EA blocks are only changed if they are exclusive to an inode, so

 * holding xattr_sem also means that nothing but the EA block's reference

 * count will change. Multiple writers to an EA block are synchronized

 * by the bh lock. No more than a single bh lock is held at any time

 * to avoid deadlocks.

/*

 * ext2_xattr_get()

 *

 * Copy an extended attribute into the buffer

 * provided, or compute the buffer size required.

 * Buffer is NULL to compute the size of the buffer required.

 *

 * Returns a negative error number on failure, or the number of bytes

 * used / required on success.

 find named attribute */

 return value of attribute */

/*

 * ext2_xattr_list()

 *

 * Copy a list of attribute names into the buffer

 * provided, or compute the buffer size required.

 * Buffer is NULL to compute the size of the buffer required.

 *

 * Returns a negative error number on failure, or the number of bytes

 * used / required on success.

 check the on-disk data structure */

 list the attribute names */

 total size */

/*

 * Inode operation listxattr()

 *

 * d_inode(dentry)->i_mutex: don't care

/*

 * If the EXT2_FEATURE_COMPAT_EXT_ATTR feature of this file system is

 * not set, set it.

/*

 * ext2_xattr_set()

 *

 * Create, replace or remove an extended attribute for this inode.  Value

 * is NULL to remove an existing extended attribute, and non-NULL to

 * either replace an existing extended attribute, or create a new extended

 * attribute. The flags XATTR_REPLACE and XATTR_CREATE

 * specify that an extended attribute must exist and must not exist

 * previous to the call, respectively.

 *

 * Returns 0, or a negative error number on failure.

	/*

	 * header -- Points either into bh, or to a temporarily

	 *           allocated buffer.

	 * here -- The named entry found, or the place for inserting, within

	 *         the block pointed to by header.

	 * last -- Points right after the last named entry within the block

	 *         pointed to by header.

	 * min_offs -- The offset of the first value (values are aligned

	 *             towards the end of the block).

	 * end -- Points right after the block pointed to by header.

 The inode already has an extended attribute block. */

		/*

		 * Find the named attribute. If not found, 'here' will point

		 * to entry where the new attribute should be inserted to

		 * maintain sorting.

 Check whether we have enough space left. */

 We will use a new extended attribute block. */

 Request to remove a nonexistent attribute? */

 Request to create an existing attribute? */

 Here we know that we can set the new attribute. */

 assert(header == HDR(bh)); */

			/*

			 * This must happen under buffer lock for

			 * ext2_xattr_set2() to reliably detect modified block

 keep the buffer locked while modifying it. */

 Allocate a buffer where we construct the new block. */

 Iff we are modifying the block in-place, bh is locked here. */

 Insert the new name. */

				/* The old and the new value have the same

 Clear pad bytes. */

 Remove the old value. */

 Adjust all value offsets. */

 Remove the old name. */

 Insert the new value. */

 Clear the pad bytes. */

 This block is now empty. */

 we were modifying in-place. */

 we were modifying in-place. */

/*

 * Second half of ext2_xattr_set(): Update the file system.

 We found an identical block in the cache. */

				/* The old block is released after updating

			/* Keep this block. No need to lock the block as we

 We need to allocate a new block */

 Update the inode. */

		/* In case sync failed due to ENOSPC the inode was actually

		 * written (only some dirty data were not) so we just proceed

		/*

		 * If there was an old block and we are no longer using it,

		 * release the old block.

			/*

			 * This must happen under buffer lock for

			 * ext2_xattr_set2() to reliably detect freed block

 Free the old block. */

			/* We let our caller release old_bh, so we

 Decrement the refcount only. */

/*

 * ext2_xattr_delete_inode()

 *

 * Free extended attribute resources associated with this inode. This

 * is called immediately before an inode is freed.

	/*

	 * We are the only ones holding inode reference. The xattr_sem should

	 * better be unlocked! We could as well just not acquire xattr_sem at

	 * all but this makes the code more futureproof. OTOH we need trylock

	 * here to avoid false-positive warning from lockdep about reclaim

	 * circular dependency.

		/*

		 * This must happen under buffer lock for ext2_xattr_set2() to

		 * reliably detect freed block

/*

 * ext2_xattr_cache_insert()

 *

 * Create a new entry in the extended attribute cache, and insert

 * it unless such an entry is already in the cache.

 *

 * Returns 0, or a negative error number on failure.

/*

 * ext2_xattr_cmp()

 *

 * Compare two extended attribute blocks for equality.

 *

 * Returns 0 if the blocks are equal, 1 if they differ, and

 * a negative error number on errors.

/*

 * ext2_xattr_cache_find()

 *

 * Find an identical extended attribute block.

 *

 * Returns a locked buffer head to the block found, or NULL if such

 * a block was not found or an error occurred.

 never share */

			/*

			 * We have to be careful about races with freeing or

			 * rehashing of xattr block. Once we hold buffer lock

			 * xattr block's state is stable so we can check

			 * whether the block got freed / rehashed or not.

			 * Since we unhash mbcache entry under buffer lock when

			 * freeing / rehashing xattr block, checking whether

			 * entry is still hashed is reliable.

/*

 * ext2_xattr_hash_entry()

 *

 * Compute the hash of an extended attribute.

/*

 * ext2_xattr_rehash()

 *

 * Re-compute the extended attribute hash value after an entry has changed.

 Block is not shared if an entry's hash value == 0 */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ext2/inode.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/inode.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  Goal-directed block allocation by Stephen Tweedie

 * 	(sct@dcs.ed.ac.uk), 1993, 1998

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

 *  64-bit file support on 64-bit platforms by Jakub Jelinek

 * 	(jj@sunsite.ms.mff.cuni.cz)

 *

 *  Assorted race fixes, rewrite of ext2_get_block() by Al Viro, 2000

/*

 * Test whether an inode is a fast symlink.

/*

 * Called at the last iput() if i_nlink is zero.

 set dtime */

 truncate to 0 */

/**

 *	ext2_block_to_path - parse the block number into array of offsets

 *	@inode: inode in question (we are only interested in its superblock)

 *	@i_block: block number to be parsed

 *	@offsets: array to store the offsets in

 *      @boundary: set this non-zero if the referred-to block is likely to be

 *             followed (on disk) by an indirect block.

 *	To store the locations of file's data ext2 uses a data structure common

 *	for UNIX filesystems - tree of pointers anchored in the inode, with

 *	data blocks at leaves and indirect blocks in intermediate nodes.

 *	This function translates the block number into path in that tree -

 *	return value is the path length and @offsets[n] is the offset of

 *	pointer to (n+1)th node in the nth one. If @block is out of range

 *	(negative or too large) warning is printed and zero returned.

 *

 *	Note: function doesn't find node addresses, so no IO is needed. All

 *	we need to know is the capacity of indirect blocks (taken from the

 *	inode->i_sb).

/*

 * Portability note: the last comparison (check that we fit into triple

 * indirect block) is spelled differently, because otherwise on an

 * architecture with 32-bit longs and 8Kb pages we might get into trouble

 * if our filesystem had 8Kb blocks. We might use long long, but that would

 * kill us on x86. Oh, well, at least the sign propagation does not matter -

 * i_block would have to be negative in the very beginning, so we would not

 * get there at all.

/**

 *	ext2_get_branch - read the chain of indirect blocks leading to data

 *	@inode: inode in question

 *	@depth: depth of the chain (1 - direct pointer, etc.)

 *	@offsets: offsets of pointers in inode/indirect blocks

 *	@chain: place to store the result

 *	@err: here we store the error value

 *

 *	Function fills the array of triples <key, p, bh> and returns %NULL

 *	if everything went OK or the pointer to the last filled triple

 *	(incomplete one) otherwise. Upon the return chain[i].key contains

 *	the number of (i+1)-th block in the chain (as it is stored in memory,

 *	i.e. little-endian 32-bit), chain[i].p contains the address of that

 *	number (it points into struct inode for i==0 and into the bh->b_data

 *	for i>0) and chain[i].bh points to the buffer_head of i-th indirect

 *	block for i>0 and NULL for i==0. In other words, it holds the block

 *	numbers of the chain, addresses they were taken from (and where we can

 *	verify that chain did not change) and buffer_heads hosting these

 *	numbers.

 *

 *	Function stops when it stumbles upon zero pointer (absent block)

 *		(pointer to last triple returned, *@err == 0)

 *	or when it gets an IO error reading an indirect block

 *		(ditto, *@err == -EIO)

 *	or when it notices that chain had been changed while it was reading

 *		(ditto, *@err == -EAGAIN)

 *	or when it reads all @depth-1 indirect blocks successfully and finds

 *	the whole chain, all way to the data (returns %NULL, *err == 0).

 i_data is not going away, no lock needed */

/**

 *	ext2_find_near - find a place for allocation with sufficient locality

 *	@inode: owner

 *	@ind: descriptor of indirect block.

 *

 *	This function returns the preferred place for block allocation.

 *	It is used when heuristic for sequential allocation fails.

 *	Rules are:

 *	  + if there is a block to the left of our position - allocate near it.

 *	  + if pointer will live in indirect block - allocate near that block.

 *	  + if pointer will live in inode - allocate in the same cylinder group.

 *

 * In the latter case we colour the starting block by the callers PID to

 * prevent it from clashing with concurrent allocations for a different inode

 * in the same block group.   The PID is used here so that functionally related

 * files will be close-by on-disk.

 *

 *	Caller must make sure that @ind is valid and will stay that way.

 Try to find previous block */

 No such thing, so let's try location of indirect block */

	/*

	 * It is going to be referred from inode itself? OK, just put it into

	 * the same cylinder group then.

/**

 *	ext2_find_goal - find a preferred place for allocation.

 *	@inode: owner

 *	@block:  block we want

 *	@partial: pointer to the last triple within a chain

 *

 *	Returns preferred place for a block (the goal).

	/*

	 * try the heuristic for sequential allocation,

	 * failing that at least try to get decent locality.

/**

 *	ext2_blks_to_allocate: Look up the block map and count the number

 *	of direct blocks need to be allocated for the given branch.

 *

 * 	@branch: chain of indirect blocks

 *	@k: number of blocks need for indirect blocks

 *	@blks: number of data blocks to be mapped.

 *	@blocks_to_boundary:  the offset in the indirect block

 *

 *	return the number of direct blocks to allocate.

	/*

	 * Simple case, [t,d]Indirect block(s) has not allocated yet

	 * then it's clear blocks on that path have not allocated

 right now don't hanel cross boundary allocation */

/**

 *	ext2_alloc_blocks: multiple allocate blocks needed for a branch

 *	@indirect_blks: the number of blocks need to allocate for indirect

 *			blocks

 *	@blks: the number of blocks need to allocate for direct blocks

 *	@new_blocks: on return it will store the new block numbers for

 *	the indirect blocks(if needed) and the first direct block,

	/*

	 * Here we try to allocate the requested multiple blocks at once,

	 * on a best-effort basis.

	 * To build a branch, we should allocate blocks for

	 * the indirect blocks(if not allocated yet), and at least

	 * the first direct block of this branch.  That's the

	 * minimum number of blocks need to allocate(required)

 allocating blocks for indirect blocks and direct blocks */

 allocate blocks for indirect blocks */

 save the new block number for the first direct block */

 total number of blocks allocated for direct blocks */

/**

 *	ext2_alloc_branch - allocate and set up a chain of blocks.

 *	@inode: owner

 *	@indirect_blks: depth of the chain (number of blocks to allocate)

 *	@blks: number of allocated direct blocks

 *	@goal: preferred place for allocation

 *	@offsets: offsets (in the blocks) to store the pointers to next.

 *	@branch: place to store the chain in.

 *

 *	This function allocates @num blocks, zeroes out all but the last one,

 *	links them into chain and (if we are synchronous) writes them to disk.

 *	In other words, it prepares a branch that can be spliced onto the

 *	inode. It stores the information about that chain in the branch[], in

 *	the same format as ext2_get_branch() would do. We are calling it after

 *	we had read the existing part of chain and partial points to the last

 *	triple of that (one with zero ->key). Upon the exit we have the same

 *	picture as after the successful ext2_get_block(), except that in one

 *	place chain is disconnected - *branch->p is still zero (we did not

 *	set the last link), but branch->key contains the number that should

 *	be placed into *branch->p to fill that gap.

 *

 *	If allocation fails we free all blocks we've allocated (and forget

 *	their buffer_heads) and return the error value the from failed

 *	ext2_alloc_block() (normally -ENOSPC). Otherwise we set the chain

 *	as described above and return 0.

	/*

	 * metadata blocks and data blocks are allocated.

		/*

		 * Get buffer_head for parent block, zero it out

		 * and set the pointer to new one, then send

		 * parent to disk.

			/*

			 * End of chain, update the last new metablock of

			 * the chain to point to the new allocated

			 * data blocks numbers

		/* We used to sync bh here if IS_SYNC(inode).

		 * But we now rely upon generic_write_sync()

		 * and b_inode_buffers.  But not for directories.

/**

 * ext2_splice_branch - splice the allocated branch onto inode.

 * @inode: owner

 * @block: (logical) number of block we are adding

 * @where: location of missing link

 * @num:   number of indirect blocks we are adding

 * @blks:  number of direct blocks we are adding

 *

 * This function fills the missing link and does all housekeeping needed in

 * inode (->i_blocks, etc.). In case of success we end up with the full

 * chain to new block and return 0.

 XXX LOCKING probably should have i_meta_lock ?*/

 That's it */

	/*

	 * Update the host buffer_head or inode to point to more just allocated

	 * direct blocks blocks

	/*

	 * update the most recently allocated logical & physical block

	 * in i_block_alloc_info, to assist find the proper goal block for next

	 * allocation

 We are done with atomic stuff, now do the rest of housekeeping */

 had we spliced it onto indirect block? */

/*

 * Allocation strategy is simple: if we have to allocate something, we will

 * have to go the whole way to leaf. So let's do it before attaching anything

 * to tree, set linkage between the newborn blocks, write them if sync is

 * required, recheck the path, free and repeat if check fails, otherwise

 * set the last missing link (that will protect us from any truncate-generated

 * removals - all blocks on the path are immune now) and possibly force the

 * write on the parent block.

 * That has a nice additional property: no special recovery from the failed

 * allocations is needed - we simply release blocks and do not touch anything

 * reachable from inode.

 *

 * `handle' can be NULL if create == 0.

 *

 * return > 0, # of blocks mapped or allocated.

 * return = 0, if plain lookup failed.

 * return < 0, error case.

 Simplest case - block found, no allocation needed */

map more blocks*/

				/*

				 * Indirect block might be removed by

				 * truncate while we were reading it.

				 * Handling of that case: forget what we've

				 * got now, go to reread.

 Next simple case - plain lookup or failed read of indirect block */

	/*

	 * If the indirect block is missing while we are reading

	 * the chain(ext2_get_branch() returns -EAGAIN err), or

	 * if the chain has been changed after we grab the semaphore,

	 * (either because another process truncated this branch, or

	 * another get_block allocated this branch) re-grab the chain to see if

	 * the request block has been allocated or not.

	 *

	 * Since we already block the truncate/other get_block

	 * at this point, we will have the current copy of the chain when we

	 * splice the branch into the tree.

	/*

	 * Okay, we need to do block allocation.  Lazily initialize the block

	 * allocation info here if necessary

 the number of blocks need to allocate for [d,t]indirect blocks */

	/*

	 * Next look up the indirect map to count the total number of

	 * direct blocks to allocate for this branch.

	/*

	 * XXX ???? Block out ext2_truncate while we alter the tree

		/*

		 * We must unmap blocks before zeroing so that writeback cannot

		 * overwrite zeros with stale data from block device page cache.

		/*

		 * block must be initialised before we put it in the tree

		 * so that it's not found by another thread before it's

		 * initialised

 Clean up and exit */

 the whole chain */

/*

 * Probably it should be a library function... search for first non-zero word

 * or memcmp with zero_page, whatever is better for particular architecture.

 * Linus?

/**

 *	ext2_find_shared - find the indirect blocks for partial truncation.

 *	@inode:	  inode in question

 *	@depth:	  depth of the affected branch

 *	@offsets: offsets of pointers in that branch (see ext2_block_to_path)

 *	@chain:	  place to store the pointers to partial indirect blocks

 *	@top:	  place to the (detached) top of branch

 *

 *	This is a helper function used by ext2_truncate().

 *

 *	When we do truncate() we may have to clean the ends of several indirect

 *	blocks but leave the blocks themselves alive. Block is partially

 *	truncated if some data below the new i_size is referred from it (and

 *	it is on the path to the first completely truncated data block, indeed).

 *	We have to free the top of that path along with everything to the right

 *	of the path. Since no allocation past the truncation point is possible

 *	until ext2_truncate() finishes, we may safely do the latter, but top

 *	of branch may require special attention - pageout below the truncation

 *	point might try to populate it.

 *

 *	We atomically detach the top of branch from the tree, store the block

 *	number of its root in *@top, pointers to buffer_heads of partially

 *	truncated blocks - in @chain[].bh and pointers to their last elements

 *	that should not be removed - in @chain[].p. Return value is the pointer

 *	to last filled element of @chain.

 *

 *	The work left to caller to do the actual freeing of subtrees:

 *		a) free the subtree starting from *@top

 *		b) free the subtrees whose roots are stored in

 *			(@chain[i].p+1 .. end of @chain[i].bh->b_data)

 *		c) free the subtrees growing from the inode past the @chain[0].p

 *			(no partially truncated stuff there).

	/*

	 * If the branch acquired continuation since we've looked at it -

	 * fine, it should all survive and (new) top doesn't belong to us.

	/*

	 * OK, we've found the last block that must survive. The rest of our

	 * branch should be detached before unlocking. However, if that rest

	 * of branch is all ours and does not grow immediately from the inode

	 * it's easier to cheat and just decrement partial->p.

/**

 *	ext2_free_data - free a list of data blocks

 *	@inode:	inode we are dealing with

 *	@p:	array of block numbers

 *	@q:	points immediately past the end of array

 *

 *	We are freeing all blocks referred from that array (numbers are

 *	stored as little-endian 32-bit) and updating @inode->i_blocks

 *	appropriately.

 accumulate blocks to free if they're contiguous */

/**

 *	ext2_free_branches - free an array of branches

 *	@inode:	inode we are dealing with

 *	@p:	array of block numbers

 *	@q:	pointer immediately past the end of array

 *	@depth:	depth of the branches to free

 *

 *	We are freeing all blocks referred from these branches (numbers are

 *	stored as little-endian 32-bit) and updating @inode->i_blocks

 *	appropriately.

			/*

			 * A read failure? Report error and clear slot

			 * (should be rare).

 mapping->invalidate_lock must be held when calling this function */

	/*

	 * From here we block out all ext2_get_block() callers who want to

	 * modify the block allocation tree.

 Kill the top of shared branch (already detached) */

 Clear the ends of indirect blocks on the shared branch */

 Kill the remaining (whole) subtrees */

	/*

	 * Figure out the offset within the block group inode table

	/* We now have enough fields to check if the inode was active or not.

	 * This is needed because nfsd might try to access dead inodes

	 * the test is that same one that e2fsck uses

	 * NeilBrown 1999oct15

 this inode is deleted */

	/*

	 * NOTE! The in-memory inode i_data array is in little-endian order

	 * even on big-endian machines: we do NOT byteswap the block numbers!

	/* For fields not not tracking in the in-memory inode,

/*

 * Fix up interoperability with old kernels. Otherwise, old inodes get

 * re-used with the upper 16 bits of the uid/gid intact

			       /* If this is the first large file

				* created, add a flag to the superblock.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ext2/dir.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/dir.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  ext2 directory handling functions

 *

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

 *

 * All code that works with directory layout had been switched to pagecache

 * and moved here. AV

/*

 * Tests against MAX_REC_LEN etc were put in place for 64k block

 * sizes; if that is not possible on this arch, we can skip

 * those tests and speed things up.

/*

 * ext2 uses block-sized chunks. Arguably, sector-sized ones would be

 * more robust, but we have what we have

/*

 * Return the offset into page `page_nr' of the last valid

 * byte in that page, plus one.

 Too bad, we had an error */

/*

 * Calls to ext2_get_page()/ext2_put_page() must be nested according to the

 * rules documented in kmap_local_page()/kunmap_local().

 *

 * NOTE: ext2_find_entry() and ext2_dotdot() act as a call to ext2_get_page()

 * and should be treated as a call to ext2_get_page() for nesting purposes.

/*

 * NOTE! unlike strncmp, ext2_match returns 1 for success, 0 for failure.

 *

 * len <= EXT2_NAME_LEN and de != NULL are guaranteed by caller.

/*

 * p is at least 6 bytes before the end of page

/*

 *	ext2_find_entry()

 *

 * finds an entry in the specified directory with the wanted name. It

 * returns the page in which the entry was found (as a parameter - res_page),

 * and the entry itself. Page is returned mapped and unlocked.

 * Entry is guaranteed to be valid.

 *

 * On Success ext2_put_page() should be called on *res_page.

 *

 * NOTE: Calls to ext2_get_page()/ext2_put_page() must be nested according to

 * the rules documented in kmap_local_page()/kunmap_local().

 *

 * ext2_find_entry() and ext2_dotdot() act as a call to ext2_get_page() and

 * should be treated as a call to ext2_get_page() for nesting purposes.

 OFFSET_CACHE */

 next page is past the blocks we've got */

/**

 * Return the '..' directory entry and the page in which the entry was found

 * (as a parameter - p).

 *

 * On Success ext2_put_page() should be called on *p.

 *

 * NOTE: Calls to ext2_get_page()/ext2_put_page() must be nested according to

 * the rules documented in kmap_local_page()/kunmap_local().

 *

 * ext2_find_entry() and ext2_dotdot() act as a call to ext2_get_page() and

 * should be treated as a call to ext2_get_page() for nesting purposes.

/*

 *	Parent is locked.

	/*

	 * We take care of directory expansion in the same loop.

	 * This code plays outside i_size, so it locks the page

	 * to protect that region.

 We hit i_size */

 OFFSET_CACHE */

/*

 * ext2_delete_entry deletes a directory entry by merging it with the

 * previous entry. Page is up-to-date.

/*

 * Set the first fragment of directory.

/*

 * routine to check that the specified directory is empty (for rmdir)

 check for . and .. */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ext2/balloc.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  Enhanced block allocation by Stephen Tweedie (sct@redhat.com), 1993

 *  Big-endian to little-endian byte-swapping/bitmaps by

 *        David S. Miller (davem@caip.rutgers.edu), 1995

/*

 * balloc.c contains the blocks allocation and deallocation routines

/*

 * The free blocks are managed by bitmaps.  A file system contains several

 * blocks groups.  Each group contains 1 bitmap block for blocks, 1 bitmap

 * block for inodes, N blocks for the inode table and data blocks.

 *

 * The file system contains group descriptors which are located after the

 * super block.  Each descriptor contains the number of the bitmap block and

 * the free blocks count in the block.  The descriptors are loaded in memory

 * when a file system is mounted (see ext2_fill_super).

 check whether block bitmap block number is set */

 bad block bitmap */

 check whether the inode bitmap block number is set */

 bad block bitmap */

 check whether the inode table block number is set */

 good bitmap for inode tables */

/*

 * Read the bitmap for a given block_group,and validate the

 * bits for block/inode/inode tables are set in the bitmaps

 *

 * Return buffer_head on success or NULL in case of failure.

	/*

	 * file system mounted not to panic on error, continue with corrupt

	 * bitmap

/*

 * The reservation window structure operations

 * --------------------------------------------

 * Operations include:

 * dump, find, add, remove, is_empty, find_next_reservable_window, etc.

 *

 * We use a red-black tree to represent per-filesystem reservation

 * windows.

 *

/**

 * __rsv_window_dump() -- Dump the filesystem block allocation reservation map

 * @root:		root of per-filesystem reservation rb tree

 * @verbose:		verbose mode

 * @fn:			function which wishes to dump the reservation map

 *

 * If verbose is turned on, it will print the whole block reservation

 * windows(start, end). Otherwise, it will only print out the "bad" windows,

 * those windows that overlap with their immediate neighbors.

/**

 * goal_in_my_reservation()

 * @rsv:		inode's reservation window

 * @grp_goal:		given goal block relative to the allocation block group

 * @group:		the current allocation block group

 * @sb:			filesystem super block

 *

 * Test if the given goal block (group relative) is within the file's

 * own block reservation window range.

 *

 * If the reservation window is outside the goal allocation group, return 0;

 * grp_goal (given goal block) could be -1, which means no specific

 * goal block. In this case, always return 1.

 * If the goal block is within the reservation window, return 1;

 * otherwise, return 0;

/**

 * search_reserve_window()

 * @root:		root of reservation tree

 * @goal:		target allocation block

 *

 * Find the reserved window which includes the goal, or the previous one

 * if the goal is not in any window.

 * Returns NULL if there are no windows or if all windows start after the goal.

	/*

	 * We've fallen off the end of the tree: the goal wasn't inside

	 * any particular node.  OK, the previous node must be to one

	 * side of the interval containing the goal.  If it's the RHS,

	 * we need to back up one.

/*

 * ext2_rsv_window_add() -- Insert a window to the block reservation rb tree.

 * @sb:			super block

 * @rsv:		reservation window to add

 *

 * Must be called with rsv_lock held.

/**

 * rsv_window_remove() -- unlink a window from the reservation rb tree

 * @sb:			super block

 * @rsv:		reservation window to remove

 *

 * Mark the block reservation window as not allocated, and unlink it

 * from the filesystem reservation window rb tree. Must be called with

 * rsv_lock held.

/*

 * rsv_is_empty() -- Check if the reservation window is allocated.

 * @rsv:		given reservation window to check

 *

 * returns 1 if the end block is EXT2_RESERVE_WINDOW_NOT_ALLOCATED.

 a valid reservation end block could not be 0 */

/**

 * ext2_init_block_alloc_info()

 * @inode:		file inode structure

 *

 * Allocate and initialize the  reservation window structure, and

 * link the window to the ext2 inode structure at last

 *

 * The reservation window structure is only dynamically allocated

 * and linked to ext2 inode the first time the open file

 * needs a new block. So, before every ext2_new_block(s) call, for

 * regular files, we should check whether the reservation window

 * structure exists or not. In the latter case, this function is called.

 * Fail to do so will result in block reservation being turned off for that

 * open file.

 *

 * This function is called from ext2_get_blocks_handle(), also called

 * when setting the reservation window size through ioctl before the file

 * is open for write (needs block allocation).

 *

 * Needs truncate_mutex protection prior to calling this function.

	 	/*

		 * if filesystem is mounted with NORESERVATION, the goal

		 * reservation window size is set to zero to indicate

		 * block reservation is off

/**

 * ext2_discard_reservation()

 * @inode:		inode

 *

 * Discard(free) block reservation window on last file close, or truncate

 * or at last iput().

 *

 * It is being called in three cases:

 * 	ext2_release_file(): last writer closes the file

 * 	ext2_clear_inode(): last iput(), when nobody links to this file.

 * 	ext2_truncate(): when the block indirect map is about to change.

/**

 * ext2_free_blocks() -- Free given blocks and update quota and i_blocks

 * @inode:		inode

 * @block:		start physical block to free

 * @count:		number of blocks to free

	/*

	 * Check to see if we are freeing blocks across a group

	 * boundary.

/**

 * bitmap_search_next_usable_block()

 * @start:		the starting block (group relative) of the search

 * @bh:			bufferhead contains the block group bitmap

 * @maxblocks:		the ending block (group relative) of the reservation

 *

 * The bitmap search --- search forward through the actual bitmap on disk until

 * we find a bit free.

/**

 * find_next_usable_block()

 * @start:		the starting block (group relative) to find next

 * 			allocatable block in bitmap.

 * @bh:			bufferhead contains the block group bitmap

 * @maxblocks:		the ending block (group relative) for the search

 *

 * Find an allocatable block in a bitmap.  We perform the "most

 * appropriate allocation" algorithm of looking for a free block near

 * the initial goal; then for a free byte somewhere in the bitmap;

 * then for any free bit in the bitmap.

		/*

		 * The goal was occupied; search forward for a free 

		 * block within the next XX blocks.

		 *

		 * end_goal is more or less random, but it has to be

		 * less than EXT2_BLOCKS_PER_GROUP. Aligning up to the

		 * next 64-bit boundary is simple..

/**

 * ext2_try_to_allocate()

 * @sb:			superblock

 * @group:		given allocation block group

 * @bitmap_bh:		bufferhead holds the block bitmap

 * @grp_goal:		given target block within the group

 * @count:		target number of blocks to allocate

 * @my_rsv:		reservation window

 *

 * Attempt to allocate blocks within a give range. Set the range of allocation

 * first, then find the first free bit(s) from the bitmap (within the range),

 * and at last, allocate the blocks by claiming the found free bit as allocated.

 *

 * To set the range of this allocation:

 * 	if there is a reservation window, only try to allocate block(s)

 * 	from the file's own reservation window;

 * 	Otherwise, the allocation range starts from the give goal block,

 * 	ends at the block group's last block.

 *

 * If we failed to allocate the desired block then we may end up crossing to a

 * new bitmap.

 we do allocation within the reservation window if we have a window */

/**

 * 	find_next_reservable_window():

 *		find a reservable space within the given range.

 *		It does not allocate the reservation window for now:

 *		alloc_new_reservation() will do the work later.

 *

 * 	@search_head: the head of the searching list;

 *		This is not necessarily the list head of the whole filesystem

 *

 *		We have both head and start_block to assist the search

 *		for the reservable space. The list starts from head,

 *		but we will shift to the place where start_block is,

 *		then start from there, when looking for a reservable space.

 *

 *	@sb: the super block.

 *

 * 	@start_block: the first block we consider to start the real search from

 *

 * 	@last_block:

 *		the maximum block number that our goal reservable space

 *		could start from. This is normally the last block in this

 *		group. The search will end when we found the start of next

 *		possible reservable space is out of this boundary.

 *		This could handle the cross boundary reservation window

 *		request.

 *

 * 	basically we search from the given range, rather than the whole

 * 	reservation double linked list, (start_block, last_block)

 * 	to find a free region that is of my size and has not

 * 	been reserved.

 *

 TODO: make the start of the reservation window byte-aligned */

 cur = *start_block & ~7;*/

		/* TODO?

		 * in the case we could not find a reservable space

		 * that is what is expected, during the re-search, we could

		 * remember what's the largest reservable space we could have

		 * and return that one.

		 *

		 * For now it will fail if we could not find the reservable

		 * space with expected-size (or more)...

 fail */

		/*

		 * Reached the last reservation, we can just append to the

		 * previous one.

			/*

			 * Found a reserveable space big enough.  We could

			 * have a reservation across the group boundary here

	/*

	 * we come here either :

	 * when we reach the end of the whole list,

	 * and there is empty reservable space after last entry in the list.

	 * append it to the end of the list.

	 *

	 * or we found one reservable space in the middle of the list,

	 * return the reservation window that we could append to.

	 * succeed.

	/*

	 * Let's book the whole available window for now.  We will check the

	 * disk bitmap later and then, if there are free blocks then we adjust

	 * the window size if it's larger than requested.

	 * Otherwise, we will remove this node from the tree next time

	 * call find_next_reservable_window.

/**

 * 	alloc_new_reservation()--allocate a new reservation window

 *

 *		To make a new reservation, we search part of the filesystem

 *		reservation list (the list that inside the group). We try to

 *		allocate a new reservation window near the allocation goal,

 *		or the beginning of the group, if there is no goal.

 *

 *		We first find a reservable space after the goal, then from

 *		there, we check the bitmap for the first free block after

 *		it. If there is no free block until the end of group, then the

 *		whole group is full, we failed. Otherwise, check if the free

 *		block is inside the expected reservable space, if so, we

 *		succeed.

 *		If the first free block is outside the reservable space, then

 *		start from the first free block, we search for next available

 *		space, and go on.

 *

 *	on succeed, a new reservation will be found and inserted into the list

 *	It contains at least one free block, and it does not overlap with other

 *	reservation windows.

 *

 *	failed: we failed to find a reservation window in this group

 *

 *	@my_rsv: the reservation

 *

 *	@grp_goal: The goal (group-relative).  It is where the search for a

 *		free reservable space should start from.

 *		if we have a goal(goal >0 ), then start from there,

 *		no goal(goal = -1), we start from the first block

 *		of the group.

 *

 *	@sb: the super block

 *	@group: the group we are trying to allocate in

 *	@bitmap_bh: the block group block bitmap

 *

		/*

		 * if the old reservation is cross group boundary

		 * and if the goal is inside the old reservation window,

		 * we will come here when we just failed to allocate from

		 * the first part of the window. We still have another part

		 * that belongs to the next group. In this case, there is no

		 * point to discard our window and try to allocate a new one

		 * in this group(which will fail). we should

		 * keep the reservation window, just simply move on.

		 *

		 * Maybe we could shift the start block of the reservation

		 * window to the first block of next group.

			/*

			 * if the previously allocation hit ratio is

			 * greater than 1/2, then we double the size of

			 * the reservation window the next time,

			 * otherwise we keep the same size window

	/*

	 * shift the search start to the window near the goal block

	/*

	 * find_next_reservable_window() simply finds a reservable window

	 * inside the given range(start_block, group_end_block).

	 *

	 * To make sure the reservation window has a free bit inside it, we

	 * need to check the bitmap after we found a reservable window.

	/*

	 * On success, find_next_reservable_window() returns the

	 * reservation window where there is a reservable space after it.

	 * Before we reserve this reservable space, we need

	 * to make sure there is at least a free block inside this region.

	 *

	 * Search the first free bit on the block bitmap.  Search starts from

	 * the start block of the reservable space we just found.

		/*

		 * no free block left on the bitmap, no point

		 * to reserve the space. return failed.

 failed */

	/*

	 * check if the first free block is within the

	 * free space we just reserved

 success */

	/*

	 * if the first free bit we found is out of the reservable space

	 * continue search for next reservable space,

	 * start from where the free block is,

	 * we also shift the list head to where we stopped last time

/**

 * try_to_extend_reservation()

 * @my_rsv:		given reservation window

 * @sb:			super block

 * @size:		the delta to extend

 *

 * Attempt to expand the reservation window large enough to have

 * required number of free blocks

 *

 * Since ext2_try_to_allocate() will always allocate blocks within

 * the reservation window range, if the window size is too small,

 * multiple blocks allocation has to stop at the end of the reservation

 * window. To make this more efficient, given the total number of

 * blocks needed and the current size of the window, we try to

 * expand the reservation window size if necessary on a best-effort

 * basis before ext2_new_blocks() tries to allocate blocks.

/**

 * ext2_try_to_allocate_with_rsv()

 * @sb:			superblock

 * @group:		given allocation block group

 * @bitmap_bh:		bufferhead holds the block bitmap

 * @grp_goal:		given target block within the group

 * @count:		target number of blocks to allocate

 * @my_rsv:		reservation window

 *

 * This is the main function used to allocate a new block and its reservation

 * window.

 *

 * Each time when a new block allocation is need, first try to allocate from

 * its own reservation.  If it does not have a reservation window, instead of

 * looking for a free bit on bitmap first, then look up the reservation list to

 * see if it is inside somebody else's reservation window, we try to allocate a

 * reservation window for it starting from the goal first. Then do the block

 * allocation within the reservation window.

 *

 * This will avoid keeping on searching the reservation list again and

 * again when somebody is looking for a free block (without

 * reservation), and there are lots of free blocks, but they are all

 * being reserved.

 *

 * We use a red-black tree for the per-filesystem reservation list.

	/*

	 * we don't deal with reservation when

	 * filesystem is mounted without reservation

	 * or the file is not a regular file

	 * or last attempt to allocate a block with reservation turned on failed

	/*

	 * grp_goal is a group relative block number (if there is a goal)

	 * 0 <= grp_goal < EXT2_BLOCKS_PER_GROUP(sb)

	 * first block is a filesystem wide block number

	 * first block is the block number of the first block in this group

	/*

	 * Basically we will allocate a new block from inode's reservation

	 * window.

	 *

	 * We need to allocate a new reservation window, if:

	 * a) inode does not have a reservation window; or

	 * b) last attempt to allocate a block from existing reservation

	 *    failed; or

	 * c) we come here with a goal and with a reservation window

	 *

	 * We do not need to allocate a new reservation window if we come here

	 * at the beginning with a goal and the goal is inside the window, or

	 * we don't have a goal but already have a reservation window.

	 * then we could go to allocate from the reservation window directly.

 failed */

 succeed */

/**

 * ext2_has_free_blocks()

 * @sbi:		in-core super block structure.

 *

 * Check if filesystem has at least 1 free block available for allocation.

/*

 * Returns 1 if the passed-in block region is valid; 0 if some part overlaps

 * with filesystem metadata blocks.

 Ensure we do not step over superblock */

/*

 * ext2_new_blocks() -- core block(s) allocation function

 * @inode:		file inode

 * @goal:		given target block(filesystem wide)

 * @count:		target number of blocks to allocate

 * @errp:		error code

 *

 * ext2_new_blocks uses a goal block to assist allocation.  If the goal is

 * free, or there is a free block within 32 blocks of the goal, that block

 * is allocated.  Otherwise a forward search is made for a free block; within 

 * each block group the search first looks for an entire free byte in the block

 * bitmap, and then for any free bit if that fails.

 * This function also updates quota and i_blocks field.

 blockgroup relative goal block */

 blockgroup-relative allocated block*/

 filesyetem-wide allocated block */

 blockgroup iteration index */

 number of free blocks in a group */

	/*

	 * Check quota for allocation of this block.

	/*

	 * Allocate a block from reservation only when

	 * filesystem is mounted with reservation(default,-o reservation), and

	 * it's a regular file, and

	 * the desired window size is greater than 0 (One could use ioctl

	 * command EXT2_IOC_SETRSVSZ to set the window size to 0 to turn off

	 * reservation on that particular file)

	/*

	 * First, test whether the goal block is free.

	/*

	 * if there is not enough free blocks to make a new resevation

	 * turn off reservation for this allocation

		/*

		 * In case we retry allocation (due to fs reservation not

		 * working out or fs corruption), the bitmap_bh is non-null

		 * pointer and we have to release it before calling

		 * read_block_bitmap().

	/*

	 * Now search the rest of the groups.  We assume that

	 * group_no and gdp correctly point to the last group visited.

		/*

		 * skip this group (and avoid loading bitmap) if there

		 * are no free blocks

		/*

		 * skip this group if the number of

		 * free blocks is less than half of the reservation

		 * window size.

		/*

		 * try to allocate block(s) from this group, without a goal(-1).

	/*

	 * We may end up a bogus earlier ENOSPC error due to

	 * filesystem is "full" of reservations, but

	 * there maybe indeed free blocks available on disk

	 * In this case, we just forget about the reservations

	 * just do block allocation as without reservations.

 No space left on the device */

		/*

		 * ext2_try_to_allocate marked the blocks we allocated as in

		 * use.  So we may want to selectively mark some of the blocks

		 * as free

	/*

	 * Undo the block allocation

  EXT2FS_DEBUG  */

/**

 *	ext2_bg_has_super - number of blocks used by the superblock in group

 *	@sb: superblock for filesystem

 *	@group: group number to check

 *

 *	Return the number of blocks used by the superblock (primary or backup)

 *	in this group.  Currently this will be only 0 or 1.

/**

 *	ext2_bg_num_gdb - number of blocks used by the group table in group

 *	@sb: superblock for filesystem

 *	@group: group number to check

 *

 *	Return the number of blocks used by the group descriptor table

 *	(primary or backup) in this group.  In the future there may be a

 *	different number of descriptor blocks in each group.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ext2/file.c

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/file.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  ext2 fs regular file handling primitives

 *

 *  64-bit file support on 64-bit platforms by Jakub Jelinek

 * 	(jj@sunsite.ms.mff.cuni.cz)

 skip atime */

/*

 * The lock ordering for ext2 DAX fault paths is:

 *

 * mmap_lock (MM)

 *   sb_start_pagefault (vfs, freeze)

 *     address_space->invalidate_lock

 *       address_space->i_mmap_rwsem or page_lock (mutually exclusive in DAX)

 *         ext2_inode_info->truncate_mutex

 *

 * The default page_lock and i_size verification done by non-DAX fault paths

 * is sufficient because ext2 doesn't support hole punching.

	/*

	 * .huge_fault is not supported for DAX because allocation in ext2

	 * cannot be reliably aligned to huge page sizes and so pmd faults

	 * will always fail and fail back to regular faults.

/*

 * Called when filp is released. This happens when all file descriptors

 * for a single struct file are closed. Note that different open() calls

 * for the same file yield different struct file structures.

 We don't really know where the IO error happened... */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/ext2/symlink.c

 *

 * Only fast symlinks left here - the rest is done by generic code. AV, 1999

 *

 * Copyright (C) 1992, 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 *  from

 *

 *  linux/fs/minix/symlink.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  ext2 symlink handling code

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/ext2/xattr_trusted.c

 * Handler for trusted extended attributes.

 *

 * Copyright (C) 2003 by Andreas Gruenbacher, <a.gruenbacher@computer.org>

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/ext2/xattr_security.c

 * Handler for storing security labels as extended attributes.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/hfsplus/xattr_user.c

 *

 * Vyacheslav Dubeyko <slava@dubeyko.com>

 *

 * Handler for user extended attributes.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/ioctl.c

 *

 * Copyright (C) 2003

 * Ethan Benson <erbenson@alaska.net>

 * partially derived from linux/fs/ext2/ioctl.c

 * Copyright (C) 1993, 1994, 1995

 * Remy Card (card@masi.ibp.fr)

 * Laboratoire MASI - Institut Blaise Pascal

 * Universite Pierre et Marie Curie (Paris VI)

 *

 * hfsplus ioctls

/*

 * "Blessing" an HFS+ filesystem writes metadata to the superblock informing

 * the platform firmware which file to boot from

 Directory containing the bootable system */

	/*

	 * Bootloader. Just using the inode here breaks in the case of

	 * hard links - the firmware wants the ID of the hard link file,

	 * but the inode points at the indirect inode

 Per spec, the OS X system folder - same as finder_info[0] here */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/wrapper.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

 * Handling of HFS wrappers around HFS+ volumes

/**

 * hfsplus_submit_bio - Perform block I/O

 * @sb: super block of volume for I/O

 * @sector: block to read or write, for blocks of HFSPLUS_SECTOR_SIZE bytes

 * @buf: buffer for I/O

 * @data: output pointer for location of requested data

 * @op: direction of I/O

 * @op_flags: request op flags

 *

 * The unit of I/O is hfsplus_min_io_size(sb), which may be bigger than

 * HFSPLUS_SECTOR_SIZE, and @buf must be sized accordingly. On reads

 * @data will return a pointer to the start of the requested sector,

 * which may not be the same location as @buf.

 *

 * If @sector is not aligned to the bdev logical block size it will

 * be rounded down. For writes this means that @buf should contain data

 * that starts at the rounded-down address. As long as the data was

 * read using hfsplus_submit_bio() and the same buffer is used things

 * will work correctly.

	/*

	 * Align sector to hardware sector size and find offset. We

	 * assume that io_size is a power of two, which _should_

	 * be true.

 default values */

 Find the volume header and fill in some minimum bits in superblock */

 Takes in super block, returns true if good data read */

		/*

		 * Check for a partition block.

		 *

		 * (should do this only for cdrom/loop though)

	/*

	 * Block size must be at least as large as a sector and a multiple of 2.

	/*

	 * Align block size to block offset.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/fs/hfsplus/super.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

	/*

	 * Explicitly write out the special metadata inodes.

	 *

	 * While these special inodes are marked as hashed and written

	 * out peridocically by the flusher threads we redirty them

	 * during writeout of normal inodes, and thus the life lock

	 * prevents us from getting the latest state to disk.

 nothing */

 temporarily use utf8 to correctly find the hidden dir below */

 Grab the volume header */

 Copy parts of the volume header into the superblock */

 Set up operations so we can load metadata */

 nothing */

 Load metadata objects (B*Trees) */

 Load the root directory */

		/*

		 * H+LX == hfsplusutils, H+Lx == this driver, H+lx is unused

		 * all three are registered with Apple for our use

 Operation is not supported. */

				/*

				 * Try to delete anyway without

				 * error analysis.

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/hfsplus/xattr.c

 *

 * Vyacheslav Dubeyko <slava@dubeyko.com>

 *

 * Logic of processing extended attributes

 The end of the node contains list of record offsets */

		/*

		 * This state means that another thread is in process

		 * of AttributesFile creation. Theoretically, it is

		 * possible to be here. But really __setxattr() method

		 * first of all calls hfs_find_init() for lookup in

		 * B-tree of CatalogFile. This method locks mutex of

		 * CatalogFile's B-tree. As a result, if some thread

		 * is inside AttributedFile creation operation then

		 * another threads will be waiting unlocking of

		 * CatalogFile's B-tree's mutex. However, if code will

		 * change then we will return error code (-EAGAIN) from

		 * here. Really, it means that first try to set of xattr

		 * fails with error but second attempt will have success.

	/*

	 * Don't allow retrieving properly prefixed attributes

	 * by prepending them with "osx."

	/*

	 * osx is the namespace we use to indicate an unprefixed

	 * attribute on the filesystem (like the ones that OS X

	 * creates), so we pass the name through unmodified (after

	 * ensuring it doesn't conflict with another namespace).

	/*

	 * Don't allow setting properly prefixed attributes

	 * by prepending them with "osx."

	/*

	 * osx is the namespace we use to indicate an unprefixed

	 * attribute on the filesystem (like the ones that OS X

	 * creates), so we pass the name through unmodified (after

	 * ensuring it doesn't conflict with another namespace).

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/hfsplus/tables.c

 *

 * Various data tables

/*

 *  Unicode case folding table taken from Apple Technote #1150

 *  (HFS Plus Volume Format)

/*

 *  The lower case table consists of a 256-entry high-byte table followed by

 *  some number of 256-entry subtables. The high-byte table contains either an

 *  offset to the subtable for characters with that high byte or zero, which

 *  means that there are no case mappings or ignored characters in that block.

 *  Ignored characters are mapped to zero.

 High-byte indices ( == 0 iff no case mapping and no ignorables )

 0 */ 0x0100, 0x0200, 0x0000, 0x0300, 0x0400, 0x0500, 0x0000, 0x0000,

 1 */ 0x0600, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 2 */ 0x0700, 0x0800, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 3 */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 4 */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 5 */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 6 */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 7 */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 8 */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 9 */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 A */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 B */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 C */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 D */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 E */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 F */ 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,

 Table 1 (for high byte 0x00)

 0 */ 0xFFFF, 0x0001, 0x0002, 0x0003, 0x0004, 0x0005, 0x0006, 0x0007,

 1 */ 0x0010, 0x0011, 0x0012, 0x0013, 0x0014, 0x0015, 0x0016, 0x0017,

 2 */ 0x0020, 0x0021, 0x0022, 0x0023, 0x0024, 0x0025, 0x0026, 0x0027,

 3 */ 0x0030, 0x0031, 0x0032, 0x0033, 0x0034, 0x0035, 0x0036, 0x0037,

 4 */ 0x0040, 0x0061, 0x0062, 0x0063, 0x0064, 0x0065, 0x0066, 0x0067,

 5 */ 0x0070, 0x0071, 0x0072, 0x0073, 0x0074, 0x0075, 0x0076, 0x0077,

 6 */ 0x0060, 0x0061, 0x0062, 0x0063, 0x0064, 0x0065, 0x0066, 0x0067,

 7 */ 0x0070, 0x0071, 0x0072, 0x0073, 0x0074, 0x0075, 0x0076, 0x0077,

 8 */ 0x0080, 0x0081, 0x0082, 0x0083, 0x0084, 0x0085, 0x0086, 0x0087,

 9 */ 0x0090, 0x0091, 0x0092, 0x0093, 0x0094, 0x0095, 0x0096, 0x0097,

 A */ 0x00A0, 0x00A1, 0x00A2, 0x00A3, 0x00A4, 0x00A5, 0x00A6, 0x00A7,

 B */ 0x00B0, 0x00B1, 0x00B2, 0x00B3, 0x00B4, 0x00B5, 0x00B6, 0x00B7,

 C */ 0x00C0, 0x00C1, 0x00C2, 0x00C3, 0x00C4, 0x00C5, 0x00E6, 0x00C7,

 D */ 0x00F0, 0x00D1, 0x00D2, 0x00D3, 0x00D4, 0x00D5, 0x00D6, 0x00D7,

 E */ 0x00E0, 0x00E1, 0x00E2, 0x00E3, 0x00E4, 0x00E5, 0x00E6, 0x00E7,

 F */ 0x00F0, 0x00F1, 0x00F2, 0x00F3, 0x00F4, 0x00F5, 0x00F6, 0x00F7,

 Table 2 (for high byte 0x01)

 0 */ 0x0100, 0x0101, 0x0102, 0x0103, 0x0104, 0x0105, 0x0106, 0x0107,

 1 */ 0x0111, 0x0111, 0x0112, 0x0113, 0x0114, 0x0115, 0x0116, 0x0117,

 2 */ 0x0120, 0x0121, 0x0122, 0x0123, 0x0124, 0x0125, 0x0127, 0x0127,

 3 */ 0x0130, 0x0131, 0x0133, 0x0133, 0x0134, 0x0135, 0x0136, 0x0137,

 4 */ 0x0140, 0x0142, 0x0142, 0x0143, 0x0144, 0x0145, 0x0146, 0x0147,

 5 */ 0x0150, 0x0151, 0x0153, 0x0153, 0x0154, 0x0155, 0x0156, 0x0157,

 6 */ 0x0160, 0x0161, 0x0162, 0x0163, 0x0164, 0x0165, 0x0167, 0x0167,

 7 */ 0x0170, 0x0171, 0x0172, 0x0173, 0x0174, 0x0175, 0x0176, 0x0177,

 8 */ 0x0180, 0x0253, 0x0183, 0x0183, 0x0185, 0x0185, 0x0254, 0x0188,

 9 */ 0x025B, 0x0192, 0x0192, 0x0260, 0x0263, 0x0195, 0x0269, 0x0268,

 A */ 0x01A0, 0x01A1, 0x01A3, 0x01A3, 0x01A5, 0x01A5, 0x01A6, 0x01A8,

 B */ 0x01B0, 0x028A, 0x028B, 0x01B4, 0x01B4, 0x01B6, 0x01B6, 0x0292,

 C */ 0x01C0, 0x01C1, 0x01C2, 0x01C3, 0x01C6, 0x01C6, 0x01C6, 0x01C9,

 D */ 0x01D0, 0x01D1, 0x01D2, 0x01D3, 0x01D4, 0x01D5, 0x01D6, 0x01D7,

 E */ 0x01E0, 0x01E1, 0x01E2, 0x01E3, 0x01E5, 0x01E5, 0x01E6, 0x01E7,

 F */ 0x01F0, 0x01F3, 0x01F3, 0x01F3, 0x01F4, 0x01F5, 0x01F6, 0x01F7,

 Table 3 (for high byte 0x03)

 0 */ 0x0300, 0x0301, 0x0302, 0x0303, 0x0304, 0x0305, 0x0306, 0x0307,

 1 */ 0x0310, 0x0311, 0x0312, 0x0313, 0x0314, 0x0315, 0x0316, 0x0317,

 2 */ 0x0320, 0x0321, 0x0322, 0x0323, 0x0324, 0x0325, 0x0326, 0x0327,

 3 */ 0x0330, 0x0331, 0x0332, 0x0333, 0x0334, 0x0335, 0x0336, 0x0337,

 4 */ 0x0340, 0x0341, 0x0342, 0x0343, 0x0344, 0x0345, 0x0346, 0x0347,

 5 */ 0x0350, 0x0351, 0x0352, 0x0353, 0x0354, 0x0355, 0x0356, 0x0357,

 6 */ 0x0360, 0x0361, 0x0362, 0x0363, 0x0364, 0x0365, 0x0366, 0x0367,

 7 */ 0x0370, 0x0371, 0x0372, 0x0373, 0x0374, 0x0375, 0x0376, 0x0377,

 8 */ 0x0380, 0x0381, 0x0382, 0x0383, 0x0384, 0x0385, 0x0386, 0x0387,

 9 */ 0x0390, 0x03B1, 0x03B2, 0x03B3, 0x03B4, 0x03B5, 0x03B6, 0x03B7,

 A */ 0x03C0, 0x03C1, 0x03A2, 0x03C3, 0x03C4, 0x03C5, 0x03C6, 0x03C7,

 B */ 0x03B0, 0x03B1, 0x03B2, 0x03B3, 0x03B4, 0x03B5, 0x03B6, 0x03B7,

 C */ 0x03C0, 0x03C1, 0x03C2, 0x03C3, 0x03C4, 0x03C5, 0x03C6, 0x03C7,

 D */ 0x03D0, 0x03D1, 0x03D2, 0x03D3, 0x03D4, 0x03D5, 0x03D6, 0x03D7,

 E */ 0x03E0, 0x03E1, 0x03E3, 0x03E3, 0x03E5, 0x03E5, 0x03E7, 0x03E7,

 F */ 0x03F0, 0x03F1, 0x03F2, 0x03F3, 0x03F4, 0x03F5, 0x03F6, 0x03F7,

 Table 4 (for high byte 0x04)

 0 */ 0x0400, 0x0401, 0x0452, 0x0403, 0x0454, 0x0455, 0x0456, 0x0407,

 1 */ 0x0430, 0x0431, 0x0432, 0x0433, 0x0434, 0x0435, 0x0436, 0x0437,

 2 */ 0x0440, 0x0441, 0x0442, 0x0443, 0x0444, 0x0445, 0x0446, 0x0447,

 3 */ 0x0430, 0x0431, 0x0432, 0x0433, 0x0434, 0x0435, 0x0436, 0x0437,

 4 */ 0x0440, 0x0441, 0x0442, 0x0443, 0x0444, 0x0445, 0x0446, 0x0447,

 5 */ 0x0450, 0x0451, 0x0452, 0x0453, 0x0454, 0x0455, 0x0456, 0x0457,

 6 */ 0x0461, 0x0461, 0x0463, 0x0463, 0x0465, 0x0465, 0x0467, 0x0467,

 7 */ 0x0471, 0x0471, 0x0473, 0x0473, 0x0475, 0x0475, 0x0476, 0x0477,

 8 */ 0x0481, 0x0481, 0x0482, 0x0483, 0x0484, 0x0485, 0x0486, 0x0487,

 9 */ 0x0491, 0x0491, 0x0493, 0x0493, 0x0495, 0x0495, 0x0497, 0x0497,

 A */ 0x04A1, 0x04A1, 0x04A3, 0x04A3, 0x04A5, 0x04A5, 0x04A7, 0x04A7,

 B */ 0x04B1, 0x04B1, 0x04B3, 0x04B3, 0x04B5, 0x04B5, 0x04B7, 0x04B7,

 C */ 0x04C0, 0x04C1, 0x04C2, 0x04C4, 0x04C4, 0x04C5, 0x04C6, 0x04C8,

 D */ 0x04D0, 0x04D1, 0x04D2, 0x04D3, 0x04D4, 0x04D5, 0x04D6, 0x04D7,

 E */ 0x04E0, 0x04E1, 0x04E2, 0x04E3, 0x04E4, 0x04E5, 0x04E6, 0x04E7,

 F */ 0x04F0, 0x04F1, 0x04F2, 0x04F3, 0x04F4, 0x04F5, 0x04F6, 0x04F7,

 Table 5 (for high byte 0x05)

 0 */ 0x0500, 0x0501, 0x0502, 0x0503, 0x0504, 0x0505, 0x0506, 0x0507,

 1 */ 0x0510, 0x0511, 0x0512, 0x0513, 0x0514, 0x0515, 0x0516, 0x0517,

 2 */ 0x0520, 0x0521, 0x0522, 0x0523, 0x0524, 0x0525, 0x0526, 0x0527,

 3 */ 0x0530, 0x0561, 0x0562, 0x0563, 0x0564, 0x0565, 0x0566, 0x0567,

 4 */ 0x0570, 0x0571, 0x0572, 0x0573, 0x0574, 0x0575, 0x0576, 0x0577,

 5 */ 0x0580, 0x0581, 0x0582, 0x0583, 0x0584, 0x0585, 0x0586, 0x0557,

 6 */ 0x0560, 0x0561, 0x0562, 0x0563, 0x0564, 0x0565, 0x0566, 0x0567,

 7 */ 0x0570, 0x0571, 0x0572, 0x0573, 0x0574, 0x0575, 0x0576, 0x0577,

 8 */ 0x0580, 0x0581, 0x0582, 0x0583, 0x0584, 0x0585, 0x0586, 0x0587,

 9 */ 0x0590, 0x0591, 0x0592, 0x0593, 0x0594, 0x0595, 0x0596, 0x0597,

 A */ 0x05A0, 0x05A1, 0x05A2, 0x05A3, 0x05A4, 0x05A5, 0x05A6, 0x05A7,

 B */ 0x05B0, 0x05B1, 0x05B2, 0x05B3, 0x05B4, 0x05B5, 0x05B6, 0x05B7,

 C */ 0x05C0, 0x05C1, 0x05C2, 0x05C3, 0x05C4, 0x05C5, 0x05C6, 0x05C7,

 D */ 0x05D0, 0x05D1, 0x05D2, 0x05D3, 0x05D4, 0x05D5, 0x05D6, 0x05D7,

 E */ 0x05E0, 0x05E1, 0x05E2, 0x05E3, 0x05E4, 0x05E5, 0x05E6, 0x05E7,

 F */ 0x05F0, 0x05F1, 0x05F2, 0x05F3, 0x05F4, 0x05F5, 0x05F6, 0x05F7,

 Table 6 (for high byte 0x10)

 0 */ 0x1000, 0x1001, 0x1002, 0x1003, 0x1004, 0x1005, 0x1006, 0x1007,

 1 */ 0x1010, 0x1011, 0x1012, 0x1013, 0x1014, 0x1015, 0x1016, 0x1017,

 2 */ 0x1020, 0x1021, 0x1022, 0x1023, 0x1024, 0x1025, 0x1026, 0x1027,

 3 */ 0x1030, 0x1031, 0x1032, 0x1033, 0x1034, 0x1035, 0x1036, 0x1037,

 4 */ 0x1040, 0x1041, 0x1042, 0x1043, 0x1044, 0x1045, 0x1046, 0x1047,

 5 */ 0x1050, 0x1051, 0x1052, 0x1053, 0x1054, 0x1055, 0x1056, 0x1057,

 6 */ 0x1060, 0x1061, 0x1062, 0x1063, 0x1064, 0x1065, 0x1066, 0x1067,

 7 */ 0x1070, 0x1071, 0x1072, 0x1073, 0x1074, 0x1075, 0x1076, 0x1077,

 8 */ 0x1080, 0x1081, 0x1082, 0x1083, 0x1084, 0x1085, 0x1086, 0x1087,

 9 */ 0x1090, 0x1091, 0x1092, 0x1093, 0x1094, 0x1095, 0x1096, 0x1097,

 A */ 0x10D0, 0x10D1, 0x10D2, 0x10D3, 0x10D4, 0x10D5, 0x10D6, 0x10D7,

 B */ 0x10E0, 0x10E1, 0x10E2, 0x10E3, 0x10E4, 0x10E5, 0x10E6, 0x10E7,

 C */ 0x10F0, 0x10F1, 0x10F2, 0x10F3, 0x10F4, 0x10F5, 0x10C6, 0x10C7,

 D */ 0x10D0, 0x10D1, 0x10D2, 0x10D3, 0x10D4, 0x10D5, 0x10D6, 0x10D7,

 E */ 0x10E0, 0x10E1, 0x10E2, 0x10E3, 0x10E4, 0x10E5, 0x10E6, 0x10E7,

 F */ 0x10F0, 0x10F1, 0x10F2, 0x10F3, 0x10F4, 0x10F5, 0x10F6, 0x10F7,

 Table 7 (for high byte 0x20)

 0 */ 0x2000, 0x2001, 0x2002, 0x2003, 0x2004, 0x2005, 0x2006, 0x2007,

 1 */ 0x2010, 0x2011, 0x2012, 0x2013, 0x2014, 0x2015, 0x2016, 0x2017,

 2 */ 0x2020, 0x2021, 0x2022, 0x2023, 0x2024, 0x2025, 0x2026, 0x2027,

 3 */ 0x2030, 0x2031, 0x2032, 0x2033, 0x2034, 0x2035, 0x2036, 0x2037,

 4 */ 0x2040, 0x2041, 0x2042, 0x2043, 0x2044, 0x2045, 0x2046, 0x2047,

 5 */ 0x2050, 0x2051, 0x2052, 0x2053, 0x2054, 0x2055, 0x2056, 0x2057,

 6 */ 0x2060, 0x2061, 0x2062, 0x2063, 0x2064, 0x2065, 0x2066, 0x2067,

 7 */ 0x2070, 0x2071, 0x2072, 0x2073, 0x2074, 0x2075, 0x2076, 0x2077,

 8 */ 0x2080, 0x2081, 0x2082, 0x2083, 0x2084, 0x2085, 0x2086, 0x2087,

 9 */ 0x2090, 0x2091, 0x2092, 0x2093, 0x2094, 0x2095, 0x2096, 0x2097,

 A */ 0x20A0, 0x20A1, 0x20A2, 0x20A3, 0x20A4, 0x20A5, 0x20A6, 0x20A7,

 B */ 0x20B0, 0x20B1, 0x20B2, 0x20B3, 0x20B4, 0x20B5, 0x20B6, 0x20B7,

 C */ 0x20C0, 0x20C1, 0x20C2, 0x20C3, 0x20C4, 0x20C5, 0x20C6, 0x20C7,

 D */ 0x20D0, 0x20D1, 0x20D2, 0x20D3, 0x20D4, 0x20D5, 0x20D6, 0x20D7,

 E */ 0x20E0, 0x20E1, 0x20E2, 0x20E3, 0x20E4, 0x20E5, 0x20E6, 0x20E7,

 F */ 0x20F0, 0x20F1, 0x20F2, 0x20F3, 0x20F4, 0x20F5, 0x20F6, 0x20F7,

 Table 8 (for high byte 0x21)

 0 */ 0x2100, 0x2101, 0x2102, 0x2103, 0x2104, 0x2105, 0x2106, 0x2107,

 1 */ 0x2110, 0x2111, 0x2112, 0x2113, 0x2114, 0x2115, 0x2116, 0x2117,

 2 */ 0x2120, 0x2121, 0x2122, 0x2123, 0x2124, 0x2125, 0x2126, 0x2127,

 3 */ 0x2130, 0x2131, 0x2132, 0x2133, 0x2134, 0x2135, 0x2136, 0x2137,

 4 */ 0x2140, 0x2141, 0x2142, 0x2143, 0x2144, 0x2145, 0x2146, 0x2147,

 5 */ 0x2150, 0x2151, 0x2152, 0x2153, 0x2154, 0x2155, 0x2156, 0x2157,

 6 */ 0x2170, 0x2171, 0x2172, 0x2173, 0x2174, 0x2175, 0x2176, 0x2177,

 7 */ 0x2170, 0x2171, 0x2172, 0x2173, 0x2174, 0x2175, 0x2176, 0x2177,

 8 */ 0x2180, 0x2181, 0x2182, 0x2183, 0x2184, 0x2185, 0x2186, 0x2187,

 9 */ 0x2190, 0x2191, 0x2192, 0x2193, 0x2194, 0x2195, 0x2196, 0x2197,

 A */ 0x21A0, 0x21A1, 0x21A2, 0x21A3, 0x21A4, 0x21A5, 0x21A6, 0x21A7,

 B */ 0x21B0, 0x21B1, 0x21B2, 0x21B3, 0x21B4, 0x21B5, 0x21B6, 0x21B7,

 C */ 0x21C0, 0x21C1, 0x21C2, 0x21C3, 0x21C4, 0x21C5, 0x21C6, 0x21C7,

 D */ 0x21D0, 0x21D1, 0x21D2, 0x21D3, 0x21D4, 0x21D5, 0x21D6, 0x21D7,

 E */ 0x21E0, 0x21E1, 0x21E2, 0x21E3, 0x21E4, 0x21E5, 0x21E6, 0x21E7,

 F */ 0x21F0, 0x21F1, 0x21F2, 0x21F3, 0x21F4, 0x21F5, 0x21F6, 0x21F7,

 Table 9 (for high byte 0xFE)

 0 */ 0xFE00, 0xFE01, 0xFE02, 0xFE03, 0xFE04, 0xFE05, 0xFE06, 0xFE07,

 1 */ 0xFE10, 0xFE11, 0xFE12, 0xFE13, 0xFE14, 0xFE15, 0xFE16, 0xFE17,

 2 */ 0xFE20, 0xFE21, 0xFE22, 0xFE23, 0xFE24, 0xFE25, 0xFE26, 0xFE27,

 3 */ 0xFE30, 0xFE31, 0xFE32, 0xFE33, 0xFE34, 0xFE35, 0xFE36, 0xFE37,

 4 */ 0xFE40, 0xFE41, 0xFE42, 0xFE43, 0xFE44, 0xFE45, 0xFE46, 0xFE47,

 5 */ 0xFE50, 0xFE51, 0xFE52, 0xFE53, 0xFE54, 0xFE55, 0xFE56, 0xFE57,

 6 */ 0xFE60, 0xFE61, 0xFE62, 0xFE63, 0xFE64, 0xFE65, 0xFE66, 0xFE67,

 7 */ 0xFE70, 0xFE71, 0xFE72, 0xFE73, 0xFE74, 0xFE75, 0xFE76, 0xFE77,

 8 */ 0xFE80, 0xFE81, 0xFE82, 0xFE83, 0xFE84, 0xFE85, 0xFE86, 0xFE87,

 9 */ 0xFE90, 0xFE91, 0xFE92, 0xFE93, 0xFE94, 0xFE95, 0xFE96, 0xFE97,

 A */ 0xFEA0, 0xFEA1, 0xFEA2, 0xFEA3, 0xFEA4, 0xFEA5, 0xFEA6, 0xFEA7,

 B */ 0xFEB0, 0xFEB1, 0xFEB2, 0xFEB3, 0xFEB4, 0xFEB5, 0xFEB6, 0xFEB7,

 C */ 0xFEC0, 0xFEC1, 0xFEC2, 0xFEC3, 0xFEC4, 0xFEC5, 0xFEC6, 0xFEC7,

 D */ 0xFED0, 0xFED1, 0xFED2, 0xFED3, 0xFED4, 0xFED5, 0xFED6, 0xFED7,

 E */ 0xFEE0, 0xFEE1, 0xFEE2, 0xFEE3, 0xFEE4, 0xFEE5, 0xFEE6, 0xFEE7,

 F */ 0xFEF0, 0xFEF1, 0xFEF2, 0xFEF3, 0xFEF4, 0xFEF5, 0xFEF6, 0xFEF7,

 Table 10 (for high byte 0xFF)

 0 */ 0xFF00, 0xFF01, 0xFF02, 0xFF03, 0xFF04, 0xFF05, 0xFF06, 0xFF07,

 1 */ 0xFF10, 0xFF11, 0xFF12, 0xFF13, 0xFF14, 0xFF15, 0xFF16, 0xFF17,

 2 */ 0xFF20, 0xFF41, 0xFF42, 0xFF43, 0xFF44, 0xFF45, 0xFF46, 0xFF47,

 3 */ 0xFF50, 0xFF51, 0xFF52, 0xFF53, 0xFF54, 0xFF55, 0xFF56, 0xFF57,

 4 */ 0xFF40, 0xFF41, 0xFF42, 0xFF43, 0xFF44, 0xFF45, 0xFF46, 0xFF47,

 5 */ 0xFF50, 0xFF51, 0xFF52, 0xFF53, 0xFF54, 0xFF55, 0xFF56, 0xFF57,

 6 */ 0xFF60, 0xFF61, 0xFF62, 0xFF63, 0xFF64, 0xFF65, 0xFF66, 0xFF67,

 7 */ 0xFF70, 0xFF71, 0xFF72, 0xFF73, 0xFF74, 0xFF75, 0xFF76, 0xFF77,

 8 */ 0xFF80, 0xFF81, 0xFF82, 0xFF83, 0xFF84, 0xFF85, 0xFF86, 0xFF87,

 9 */ 0xFF90, 0xFF91, 0xFF92, 0xFF93, 0xFF94, 0xFF95, 0xFF96, 0xFF97,

 A */ 0xFFA0, 0xFFA1, 0xFFA2, 0xFFA3, 0xFFA4, 0xFFA5, 0xFFA6, 0xFFA7,

 B */ 0xFFB0, 0xFFB1, 0xFFB2, 0xFFB3, 0xFFB4, 0xFFB5, 0xFFB6, 0xFFB7,

 C */ 0xFFC0, 0xFFC1, 0xFFC2, 0xFFC3, 0xFFC4, 0xFFC5, 0xFFC6, 0xFFC7,

 D */ 0xFFD0, 0xFFD1, 0xFFD2, 0xFFD3, 0xFFD4, 0xFFD5, 0xFFD6, 0xFFD7,

 E */ 0xFFE0, 0xFFE1, 0xFFE2, 0xFFE3, 0xFFE4, 0xFFE5, 0xFFE6, 0xFFE7,

 F */ 0xFFF0, 0xFFF1, 0xFFF2, 0xFFF3, 0xFFF4, 0xFFF5, 0xFFF6, 0xFFF7,

 base table */

 char table 0x0___ */

 char table 0x00__ */

 char values 0x00c_ */

 char values 0x00d_ */

 char values 0x00e_ */

 char values 0x00f_ */

 char table 0x01__ */

 char values 0x010_ */

 char values 0x011_ */

 char values 0x012_ */

 char values 0x013_ */

 char values 0x014_ */

 char values 0x015_ */

 char values 0x016_ */

 char values 0x017_ */

 char values 0x01a_ */

 char values 0x01b_ */

 char values 0x01c_ */

 char values 0x01d_ */

 char values 0x01e_ */

 char values 0x01f_ */

 char table 0x02__ */

 char values 0x020_ */

 char values 0x021_ */

 char table 0x03__ */

 char values 0x031_ */

 char values 0x034_ */

 char values 0x037_ */

 char values 0x038_ */

 char values 0x039_ */

 char values 0x03a_ */

 char values 0x03b_ */

 char values 0x03c_ */

 char values 0x03d_ */

 char table 0x04__ */

 char values 0x040_ */

 char values 0x041_ */

 char values 0x043_ */

 char values 0x045_ */

 char values 0x047_ */

 char values 0x04c_ */

 char values 0x04d_ */

 char values 0x04e_ */

 char values 0x04f_ */

 char table 0x09__ */

 char values 0x092_ */

 char values 0x093_ */

 char values 0x095_ */

 char values 0x09b_ */

 char values 0x09c_ */

 char values 0x09d_ */

 char table 0x0a__ */

 char values 0x0a5_ */

 char table 0x0b__ */

 char values 0x0b4_ */

 char values 0x0b5_ */

 char values 0x0b9_ */

 char values 0x0bc_ */

 char table 0x0c__ */

 char values 0x0c4_ */

 char values 0x0cc_ */

 char table 0x0d__ */

 char values 0x0d4_ */

 char table 0x0e__ */

 char values 0x0e3_ */

 char values 0x0eb_ */

 char table 0x0f__ */

 char values 0x0f4_ */

 char values 0x0f5_ */

 char values 0x0f6_ */

 char values 0x0f7_ */

 char values 0x0f8_ */

 char values 0x0f9_ */

 char values 0x0fa_ */

 char values 0x0fb_ */

 char table 0x1___ */

 char table 0x1e__ */

 char values 0x1e0_ */

 char values 0x1e1_ */

 char values 0x1e2_ */

 char values 0x1e3_ */

 char values 0x1e4_ */

 char values 0x1e5_ */

 char values 0x1e6_ */

 char values 0x1e7_ */

 char values 0x1e8_ */

 char values 0x1e9_ */

 char values 0x1ea_ */

 char values 0x1eb_ */

 char values 0x1ec_ */

 char values 0x1ed_ */

 char values 0x1ee_ */

 char values 0x1ef_ */

 char table 0x1f__ */

 char values 0x1f0_ */

 char values 0x1f1_ */

 char values 0x1f2_ */

 char values 0x1f3_ */

 char values 0x1f4_ */

 char values 0x1f5_ */

 char values 0x1f6_ */

 char values 0x1f7_ */

 char values 0x1f8_ */

 char values 0x1f9_ */

 char values 0x1fa_ */

 char values 0x1fb_ */

 char values 0x1fc_ */

 char values 0x1fd_ */

 char values 0x1fe_ */

 char values 0x1ff_ */

 char table 0x3___ */

 char table 0x30__ */

 char values 0x304_ */

 char values 0x305_ */

 char values 0x306_ */

 char values 0x307_ */

 char values 0x309_ */

 char values 0x30a_ */

 char values 0x30b_ */

 char values 0x30c_ */

 char values 0x30d_ */

 char values 0x30f_ */

 char table 0xf___ */

 char table 0xfb__ */

 char values 0xfb1_ */

 char values 0xfb2_ */

 char values 0xfb3_ */

 char values 0xfb4_ */

 decomposed characters */

 base */

 hangul marker */

 0x0300 */

 0x0301 */

 0x0302 */

 0x0303 */

 0x0304 */

 0x0306 */

 0x0307 */

 0x0308 */

 0x0309 */

 0x030a */

 0x030b */

 0x030c */

 0x030d */

 0x030f */

 0x0311 */

 0x0313 */

 0x0314 */

 0x031b */

 0x0323 */

 0x0324 */

 0x0325 */

 0x0327 */

 0x0328 */

 0x032d */

 0x032e */

 0x0330 */

 0x0331 */

 0x0342 */

 0x0345 */

 0x05b7 */

 0x05b8 */

 0x05b9 */

 0x05bc */

 0x05bf */

 0x05c1 */

 0x05c2 */

 0x093c */

 0x09bc */

 0x09be */

 0x09d7 */

 0x0a3c */

 0x0b3c */

 0x0b3e */

 0x0b56 */

 0x0b57 */

 0x0bbe */

 0x0bd7 */

 0x0c56 */

 0x0cc2 */

 0x0cd5 */

 0x0cd6 */

 0x0d3e */

 0x0d57 */

 0x0e32 */

 0x0eb2 */

 0x0f71 */

 0x0f80 */

 0x0fb5 */

 0x0fb7 */

 0x3099 */

 0x309a */

 0x0041 0x0300 */

 0x0045 0x0300 */

 0x0049 0x0300 */

 0x004f 0x0300 */

 0x0055 0x0300 */

 0x0057 0x0300 */

 0x0059 0x0300 */

 0x0061 0x0300 */

 0x0065 0x0300 */

 0x0069 0x0300 */

 0x006f 0x0300 */

 0x0075 0x0300 */

 0x0077 0x0300 */

 0x0079 0x0300 */

 0x00a8 0x0300 */

 0x0391 0x0300 */

 0x0395 0x0300 */

 0x0397 0x0300 */

 0x0399 0x0300 */

 0x039f 0x0300 */

 0x03a5 0x0300 */

 0x03a9 0x0300 */

 0x03b1 0x0300 */

 0x03b5 0x0300 */

 0x03b7 0x0300 */

 0x03b9 0x0300 */

 0x03bf 0x0300 */

 0x03c5 0x0300 */

 0x03c9 0x0300 */

 0x1fbf 0x0300 */

 0x1ffe 0x0300 */

 0x0041 0x0301 */

 0x0043 0x0301 */

 0x0045 0x0301 */

 0x0047 0x0301 */

 0x0049 0x0301 */

 0x004b 0x0301 */

 0x004c 0x0301 */

 0x004d 0x0301 */

 0x004e 0x0301 */

 0x004f 0x0301 */

 0x0050 0x0301 */

 0x0052 0x0301 */

 0x0053 0x0301 */

 0x0055 0x0301 */

 0x0057 0x0301 */

 0x0059 0x0301 */

 0x005a 0x0301 */

 0x0061 0x0301 */

 0x0063 0x0301 */

 0x0065 0x0301 */

 0x0067 0x0301 */

 0x0069 0x0301 */

 0x006b 0x0301 */

 0x006c 0x0301 */

 0x006d 0x0301 */

 0x006e 0x0301 */

 0x006f 0x0301 */

 0x0070 0x0301 */

 0x0072 0x0301 */

 0x0073 0x0301 */

 0x0075 0x0301 */

 0x0077 0x0301 */

 0x0079 0x0301 */

 0x007a 0x0301 */

 0x00a8 0x0301 */

 0x00c6 0x0301 */

 0x00d8 0x0301 */

 0x00e6 0x0301 */

 0x00f8 0x0301 */

 0x0391 0x0301 */

 0x0395 0x0301 */

 0x0397 0x0301 */

 0x0399 0x0301 */

 0x039f 0x0301 */

 0x03a5 0x0301 */

 0x03a9 0x0301 */

 0x03b1 0x0301 */

 0x03b5 0x0301 */

 0x03b7 0x0301 */

 0x03b9 0x0301 */

 0x03bf 0x0301 */

 0x03c5 0x0301 */

 0x03c9 0x0301 */

 0x0413 0x0301 */

 0x041a 0x0301 */

 0x0433 0x0301 */

 0x043a 0x0301 */

 0x1fbf 0x0301 */

 0x1ffe 0x0301 */

 0x0041 0x0302 */

 0x0043 0x0302 */

 0x0045 0x0302 */

 0x0047 0x0302 */

 0x0048 0x0302 */

 0x0049 0x0302 */

 0x004a 0x0302 */

 0x004f 0x0302 */

 0x0053 0x0302 */

 0x0055 0x0302 */

 0x0057 0x0302 */

 0x0059 0x0302 */

 0x005a 0x0302 */

 0x0061 0x0302 */

 0x0063 0x0302 */

 0x0065 0x0302 */

 0x0067 0x0302 */

 0x0068 0x0302 */

 0x0069 0x0302 */

 0x006a 0x0302 */

 0x006f 0x0302 */

 0x0073 0x0302 */

 0x0075 0x0302 */

 0x0077 0x0302 */

 0x0079 0x0302 */

 0x007a 0x0302 */

 0x0041 0x0303 */

 0x0045 0x0303 */

 0x0049 0x0303 */

 0x004e 0x0303 */

 0x004f 0x0303 */

 0x0055 0x0303 */

 0x0056 0x0303 */

 0x0059 0x0303 */

 0x0061 0x0303 */

 0x0065 0x0303 */

 0x0069 0x0303 */

 0x006e 0x0303 */

 0x006f 0x0303 */

 0x0075 0x0303 */

 0x0076 0x0303 */

 0x0079 0x0303 */

 0x0041 0x0304 */

 0x0045 0x0304 */

 0x0047 0x0304 */

 0x0049 0x0304 */

 0x004f 0x0304 */

 0x0055 0x0304 */

 0x0061 0x0304 */

 0x0065 0x0304 */

 0x0067 0x0304 */

 0x0069 0x0304 */

 0x006f 0x0304 */

 0x0075 0x0304 */

 0x00c6 0x0304 */

 0x00e6 0x0304 */

 0x0391 0x0304 */

 0x0399 0x0304 */

 0x03a5 0x0304 */

 0x03b1 0x0304 */

 0x03b9 0x0304 */

 0x03c5 0x0304 */

 0x0418 0x0304 */

 0x0423 0x0304 */

 0x0438 0x0304 */

 0x0443 0x0304 */

 0x0041 0x0306 */

 0x0045 0x0306 */

 0x0047 0x0306 */

 0x0049 0x0306 */

 0x004f 0x0306 */

 0x0055 0x0306 */

 0x0061 0x0306 */

 0x0065 0x0306 */

 0x0067 0x0306 */

 0x0069 0x0306 */

 0x006f 0x0306 */

 0x0075 0x0306 */

 0x0391 0x0306 */

 0x0399 0x0306 */

 0x03a5 0x0306 */

 0x03b1 0x0306 */

 0x03b9 0x0306 */

 0x03c5 0x0306 */

 0x0410 0x0306 */

 0x0415 0x0306 */

 0x0416 0x0306 */

 0x0418 0x0306 */

 0x0423 0x0306 */

 0x0430 0x0306 */

 0x0435 0x0306 */

 0x0436 0x0306 */

 0x0438 0x0306 */

 0x0443 0x0306 */

 0x0041 0x0307 */

 0x0042 0x0307 */

 0x0043 0x0307 */

 0x0044 0x0307 */

 0x0045 0x0307 */

 0x0046 0x0307 */

 0x0047 0x0307 */

 0x0048 0x0307 */

 0x0049 0x0307 */

 0x004d 0x0307 */

 0x004e 0x0307 */

 0x0050 0x0307 */

 0x0052 0x0307 */

 0x0053 0x0307 */

 0x0054 0x0307 */

 0x0057 0x0307 */

 0x0058 0x0307 */

 0x0059 0x0307 */

 0x005a 0x0307 */

 0x0061 0x0307 */

 0x0062 0x0307 */

 0x0063 0x0307 */

 0x0064 0x0307 */

 0x0065 0x0307 */

 0x0066 0x0307 */

 0x0067 0x0307 */

 0x0068 0x0307 */

 0x006d 0x0307 */

 0x006e 0x0307 */

 0x0070 0x0307 */

 0x0072 0x0307 */

 0x0073 0x0307 */

 0x0074 0x0307 */

 0x0077 0x0307 */

 0x0078 0x0307 */

 0x0079 0x0307 */

 0x007a 0x0307 */

 0x017f 0x0307 */

 0x0306 0x0307 */

 0x0041 0x0308 */

 0x0045 0x0308 */

 0x0048 0x0308 */

 0x0049 0x0308 */

 0x004f 0x0308 */

 0x0055 0x0308 */

 0x0057 0x0308 */

 0x0058 0x0308 */

 0x0059 0x0308 */

 0x0061 0x0308 */

 0x0065 0x0308 */

 0x0068 0x0308 */

 0x0069 0x0308 */

 0x006f 0x0308 */

 0x0074 0x0308 */

 0x0075 0x0308 */

 0x0077 0x0308 */

 0x0078 0x0308 */

 0x0079 0x0308 */

 0x018f 0x0308 */

 0x019f 0x0308 */

 0x0259 0x0308 */

 0x0275 0x0308 */

 0x0399 0x0308 */

 0x03a5 0x0308 */

 0x03b9 0x0308 */

 0x03c5 0x0308 */

 0x03d2 0x0308 */

 0x0406 0x0308 */

 0x0410 0x0308 */

 0x0415 0x0308 */

 0x0416 0x0308 */

 0x0417 0x0308 */

 0x0418 0x0308 */

 0x041e 0x0308 */

 0x0423 0x0308 */

 0x0427 0x0308 */

 0x042b 0x0308 */

 0x0430 0x0308 */

 0x0435 0x0308 */

 0x0436 0x0308 */

 0x0437 0x0308 */

 0x0438 0x0308 */

 0x043e 0x0308 */

 0x0443 0x0308 */

 0x0447 0x0308 */

 0x044b 0x0308 */

 0x0456 0x0308 */

 0x0041 0x0309 */

 0x0045 0x0309 */

 0x0049 0x0309 */

 0x004f 0x0309 */

 0x0055 0x0309 */

 0x0059 0x0309 */

 0x0061 0x0309 */

 0x0065 0x0309 */

 0x0069 0x0309 */

 0x006f 0x0309 */

 0x0075 0x0309 */

 0x0079 0x0309 */

 0x0041 0x030a */

 0x0055 0x030a */

 0x0061 0x030a */

 0x0075 0x030a */

 0x0077 0x030a */

 0x0079 0x030a */

 0x004f 0x030b */

 0x0055 0x030b */

 0x006f 0x030b */

 0x0075 0x030b */

 0x0423 0x030b */

 0x0443 0x030b */

 0x0041 0x030c */

 0x0043 0x030c */

 0x0044 0x030c */

 0x0045 0x030c */

 0x0047 0x030c */

 0x0049 0x030c */

 0x004b 0x030c */

 0x004c 0x030c */

 0x004e 0x030c */

 0x004f 0x030c */

 0x0052 0x030c */

 0x0053 0x030c */

 0x0054 0x030c */

 0x0055 0x030c */

 0x005a 0x030c */

 0x0061 0x030c */

 0x0063 0x030c */

 0x0064 0x030c */

 0x0065 0x030c */

 0x0067 0x030c */

 0x0069 0x030c */

 0x006a 0x030c */

 0x006b 0x030c */

 0x006c 0x030c */

 0x006e 0x030c */

 0x006f 0x030c */

 0x0072 0x030c */

 0x0073 0x030c */

 0x0074 0x030c */

 0x0075 0x030c */

 0x007a 0x030c */

 0x01b7 0x030c */

 0x0292 0x030c */

 0x00a8 0x030d */

 0x0308 0x030d */

 0x0391 0x030d */

 0x0395 0x030d */

 0x0397 0x030d */

 0x0399 0x030d */

 0x039f 0x030d */

 0x03a5 0x030d */

 0x03a9 0x030d */

 0x03b1 0x030d */

 0x03b5 0x030d */

 0x03b7 0x030d */

 0x03b9 0x030d */

 0x03bf 0x030d */

 0x03c5 0x030d */

 0x03c9 0x030d */

 0x03d2 0x030d */

 0x0041 0x030f */

 0x0045 0x030f */

 0x0049 0x030f */

 0x004f 0x030f */

 0x0052 0x030f */

 0x0055 0x030f */

 0x0061 0x030f */

 0x0065 0x030f */

 0x0069 0x030f */

 0x006f 0x030f */

 0x0072 0x030f */

 0x0075 0x030f */

 0x0474 0x030f */

 0x0475 0x030f */

 0x0041 0x0311 */

 0x0045 0x0311 */

 0x0049 0x0311 */

 0x004f 0x0311 */

 0x0052 0x0311 */

 0x0055 0x0311 */

 0x0061 0x0311 */

 0x0065 0x0311 */

 0x0069 0x0311 */

 0x006f 0x0311 */

 0x0072 0x0311 */

 0x0075 0x0311 */

 0x0391 0x0313 */

 0x0395 0x0313 */

 0x0397 0x0313 */

 0x0399 0x0313 */

 0x039f 0x0313 */

 0x03a9 0x0313 */

 0x03b1 0x0313 */

 0x03b5 0x0313 */

 0x03b7 0x0313 */

 0x03b9 0x0313 */

 0x03bf 0x0313 */

 0x03c1 0x0313 */

 0x03c5 0x0313 */

 0x03c9 0x0313 */

 0x0391 0x0314 */

 0x0395 0x0314 */

 0x0397 0x0314 */

 0x0399 0x0314 */

 0x039f 0x0314 */

 0x03a1 0x0314 */

 0x03a5 0x0314 */

 0x03a9 0x0314 */

 0x03b1 0x0314 */

 0x03b5 0x0314 */

 0x03b7 0x0314 */

 0x03b9 0x0314 */

 0x03bf 0x0314 */

 0x03c1 0x0314 */

 0x03c5 0x0314 */

 0x03c9 0x0314 */

 0x004f 0x031b */

 0x0055 0x031b */

 0x006f 0x031b */

 0x0075 0x031b */

 0x0041 0x0323 */

 0x0042 0x0323 */

 0x0044 0x0323 */

 0x0045 0x0323 */

 0x0048 0x0323 */

 0x0049 0x0323 */

 0x004b 0x0323 */

 0x004c 0x0323 */

 0x004d 0x0323 */

 0x004e 0x0323 */

 0x004f 0x0323 */

 0x0052 0x0323 */

 0x0053 0x0323 */

 0x0054 0x0323 */

 0x0055 0x0323 */

 0x0056 0x0323 */

 0x0057 0x0323 */

 0x0059 0x0323 */

 0x005a 0x0323 */

 0x0061 0x0323 */

 0x0062 0x0323 */

 0x0064 0x0323 */

 0x0065 0x0323 */

 0x0068 0x0323 */

 0x0069 0x0323 */

 0x006b 0x0323 */

 0x006c 0x0323 */

 0x006d 0x0323 */

 0x006e 0x0323 */

 0x006f 0x0323 */

 0x0072 0x0323 */

 0x0073 0x0323 */

 0x0074 0x0323 */

 0x0075 0x0323 */

 0x0076 0x0323 */

 0x0077 0x0323 */

 0x0079 0x0323 */

 0x007a 0x0323 */

 0x0055 0x0324 */

 0x0075 0x0324 */

 0x0041 0x0325 */

 0x0061 0x0325 */

 0x0043 0x0327 */

 0x0044 0x0327 */

 0x0045 0x0327 */

 0x0047 0x0327 */

 0x0048 0x0327 */

 0x004b 0x0327 */

 0x004c 0x0327 */

 0x004e 0x0327 */

 0x0052 0x0327 */

 0x0053 0x0327 */

 0x0054 0x0327 */

 0x0063 0x0327 */

 0x0064 0x0327 */

 0x0065 0x0327 */

 0x0067 0x0327 */

 0x0068 0x0327 */

 0x006b 0x0327 */

 0x006c 0x0327 */

 0x006e 0x0327 */

 0x0072 0x0327 */

 0x0073 0x0327 */

 0x0074 0x0327 */

 0x0041 0x0328 */

 0x0045 0x0328 */

 0x0049 0x0328 */

 0x004f 0x0328 */

 0x0055 0x0328 */

 0x0061 0x0328 */

 0x0065 0x0328 */

 0x0069 0x0328 */

 0x006f 0x0328 */

 0x0075 0x0328 */

 0x0044 0x032d */

 0x0045 0x032d */

 0x004c 0x032d */

 0x004e 0x032d */

 0x0054 0x032d */

 0x0055 0x032d */

 0x0064 0x032d */

 0x0065 0x032d */

 0x006c 0x032d */

 0x006e 0x032d */

 0x0074 0x032d */

 0x0075 0x032d */

 0x0048 0x032e */

 0x0068 0x032e */

 0x0045 0x0330 */

 0x0049 0x0330 */

 0x0055 0x0330 */

 0x0065 0x0330 */

 0x0069 0x0330 */

 0x0075 0x0330 */

 0x0042 0x0331 */

 0x0044 0x0331 */

 0x004b 0x0331 */

 0x004c 0x0331 */

 0x004e 0x0331 */

 0x0052 0x0331 */

 0x0054 0x0331 */

 0x005a 0x0331 */

 0x0062 0x0331 */

 0x0064 0x0331 */

 0x0068 0x0331 */

 0x006b 0x0331 */

 0x006c 0x0331 */

 0x006e 0x0331 */

 0x0072 0x0331 */

 0x0074 0x0331 */

 0x007a 0x0331 */

 0x00a8 0x0342 */

 0x03b1 0x0342 */

 0x03b7 0x0342 */

 0x03b9 0x0342 */

 0x03c5 0x0342 */

 0x03c9 0x0342 */

 0x1fbf 0x0342 */

 0x1ffe 0x0342 */

 0x0391 0x0345 */

 0x0397 0x0345 */

 0x03a9 0x0345 */

 0x03b1 0x0345 */

 0x03b7 0x0345 */

 0x03bf 0x0345 */

 0x03c9 0x0345 */

 0x05d0 0x05b7 */

 0x05f2 0x05b7 */

 0x05d0 0x05b8 */

 0x05d5 0x05b9 */

 0x05d0 0x05bc */

 0x05d1 0x05bc */

 0x05d2 0x05bc */

 0x05d3 0x05bc */

 0x05d4 0x05bc */

 0x05d5 0x05bc */

 0x05d6 0x05bc */

 0x05d8 0x05bc */

 0x05d9 0x05bc */

 0x05da 0x05bc */

 0x05db 0x05bc */

 0x05dc 0x05bc */

 0x05de 0x05bc */

 0x05e0 0x05bc */

 0x05e1 0x05bc */

 0x05e3 0x05bc */

 0x05e4 0x05bc */

 0x05e6 0x05bc */

 0x05e7 0x05bc */

 0x05e8 0x05bc */

 0x05e9 0x05bc */

 0x05ea 0x05bc */

 0x05d1 0x05bf */

 0x05db 0x05bf */

 0x05e4 0x05bf */

 0x05e9 0x05c1 */

 0x05e9 0x05c2 */

 0x0915 0x093c */

 0x0916 0x093c */

 0x0917 0x093c */

 0x091c 0x093c */

 0x0921 0x093c */

 0x0922 0x093c */

 0x0928 0x093c */

 0x092b 0x093c */

 0x092f 0x093c */

 0x0930 0x093c */

 0x0933 0x093c */

 0x09a1 0x09bc */

 0x09a2 0x09bc */

 0x09ac 0x09bc */

 0x09af 0x09bc */

 0x09c7 0x09be */

 0x09c7 0x09d7 */

 0x0a16 0x0a3c */

 0x0a17 0x0a3c */

 0x0a1c 0x0a3c */

 0x0a21 0x0a3c */

 0x0a2b 0x0a3c */

 0x0b21 0x0b3c */

 0x0b22 0x0b3c */

 0x0b2f 0x0b3c */

 0x0b47 0x0b3e */

 0x0b47 0x0b56 */

 0x0b47 0x0b57 */

 0x0bc6 0x0bbe */

 0x0bc7 0x0bbe */

 0x0b92 0x0bd7 */

 0x0bc6 0x0bd7 */

 0x0c46 0x0c56 */

 0x0cc6 0x0cc2 */

 0x0cbf 0x0cd5 */

 0x0cc6 0x0cd5 */

 0x0cc6 0x0cd6 */

 0x0d46 0x0d3e */

 0x0d47 0x0d3e */

 0x0d46 0x0d57 */

 0x0e4d 0x0e32 */

 0x0ecd 0x0eb2 */

 0x0f72 0x0f71 */

 0x0f74 0x0f71 */

 0x0f80 0x0f71 */

 0x0fb2 0x0f80 */

 0x0fb3 0x0f80 */

 0x0f40 0x0fb5 */

 0x0f90 0x0fb5 */

 0x0f42 0x0fb7 */

 0x0f4c 0x0fb7 */

 0x0f51 0x0fb7 */

 0x0f56 0x0fb7 */

 0x0f5b 0x0fb7 */

 0x0f92 0x0fb7 */

 0x0f9c 0x0fb7 */

 0x0fa1 0x0fb7 */

 0x0fa6 0x0fb7 */

 0x0fab 0x0fb7 */

 0x3046 0x3099 */

 0x304b 0x3099 */

 0x304d 0x3099 */

 0x304f 0x3099 */

 0x3051 0x3099 */

 0x3053 0x3099 */

 0x3055 0x3099 */

 0x3057 0x3099 */

 0x3059 0x3099 */

 0x305b 0x3099 */

 0x305d 0x3099 */

 0x305f 0x3099 */

 0x3061 0x3099 */

 0x3064 0x3099 */

 0x3066 0x3099 */

 0x3068 0x3099 */

 0x306f 0x3099 */

 0x3072 0x3099 */

 0x3075 0x3099 */

 0x3078 0x3099 */

 0x307b 0x3099 */

 0x309d 0x3099 */

 0x30a6 0x3099 */

 0x30ab 0x3099 */

 0x30ad 0x3099 */

 0x30af 0x3099 */

 0x30b1 0x3099 */

 0x30b3 0x3099 */

 0x30b5 0x3099 */

 0x30b7 0x3099 */

 0x30b9 0x3099 */

 0x30bb 0x3099 */

 0x30bd 0x3099 */

 0x30bf 0x3099 */

 0x30c1 0x3099 */

 0x30c4 0x3099 */

 0x30c6 0x3099 */

 0x30c8 0x3099 */

 0x30cf 0x3099 */

 0x30d2 0x3099 */

 0x30d5 0x3099 */

 0x30d8 0x3099 */

 0x30db 0x3099 */

 0x30ef 0x3099 */

 0x30f0 0x3099 */

 0x30f1 0x3099 */

 0x30f2 0x3099 */

 0x30fd 0x3099 */

 0x306f 0x309a */

 0x3072 0x309a */

 0x3075 0x309a */

 0x3078 0x309a */

 0x307b 0x309a */

 0x30cf 0x309a */

 0x30d2 0x309a */

 0x30d5 0x309a */

 0x30d8 0x309a */

 0x30db 0x309a */

 0x0307 0x0053 0x0301 */

 0x0307 0x0073 0x0301 */

 0x0300 0x0041 0x0302 */

 0x0301 0x0041 0x0302 */

 0x0303 0x0041 0x0302 */

 0x0309 0x0041 0x0302 */

 0x0300 0x0045 0x0302 */

 0x0301 0x0045 0x0302 */

 0x0303 0x0045 0x0302 */

 0x0309 0x0045 0x0302 */

 0x0300 0x004f 0x0302 */

 0x0301 0x004f 0x0302 */

 0x0303 0x004f 0x0302 */

 0x0309 0x004f 0x0302 */

 0x0300 0x0061 0x0302 */

 0x0301 0x0061 0x0302 */

 0x0303 0x0061 0x0302 */

 0x0309 0x0061 0x0302 */

 0x0300 0x0065 0x0302 */

 0x0301 0x0065 0x0302 */

 0x0303 0x0065 0x0302 */

 0x0309 0x0065 0x0302 */

 0x0300 0x006f 0x0302 */

 0x0301 0x006f 0x0302 */

 0x0303 0x006f 0x0302 */

 0x0309 0x006f 0x0302 */

 0x0301 0x004f 0x0303 */

 0x0308 0x004f 0x0303 */

 0x0301 0x0055 0x0303 */

 0x0301 0x006f 0x0303 */

 0x0308 0x006f 0x0303 */

 0x0301 0x0075 0x0303 */

 0x0300 0x0045 0x0304 */

 0x0301 0x0045 0x0304 */

 0x0300 0x004f 0x0304 */

 0x0301 0x004f 0x0304 */

 0x0308 0x0055 0x0304 */

 0x0300 0x0065 0x0304 */

 0x0301 0x0065 0x0304 */

 0x0300 0x006f 0x0304 */

 0x0301 0x006f 0x0304 */

 0x0308 0x0075 0x0304 */

 0x0300 0x0041 0x0306 */

 0x0301 0x0041 0x0306 */

 0x0303 0x0041 0x0306 */

 0x0309 0x0041 0x0306 */

 0x0300 0x0061 0x0306 */

 0x0301 0x0061 0x0306 */

 0x0303 0x0061 0x0306 */

 0x0309 0x0061 0x0306 */

 0x0304 0x0041 0x0307 */

 0x0304 0x0061 0x0307 */

 0x0304 0x0041 0x0308 */

 0x0301 0x0049 0x0308 */

 0x0300 0x0055 0x0308 */

 0x0301 0x0055 0x0308 */

 0x0304 0x0055 0x0308 */

 0x030c 0x0055 0x0308 */

 0x0304 0x0061 0x0308 */

 0x0301 0x0069 0x0308 */

 0x0300 0x0075 0x0308 */

 0x0301 0x0075 0x0308 */

 0x0304 0x0075 0x0308 */

 0x030c 0x0075 0x0308 */

 0x0300 0x03b9 0x0308 */

 0x0301 0x03b9 0x0308 */

 0x030d 0x03b9 0x0308 */

 0x0342 0x03b9 0x0308 */

 0x0300 0x03c5 0x0308 */

 0x0301 0x03c5 0x0308 */

 0x030d 0x03c5 0x0308 */

 0x0342 0x03c5 0x0308 */

 0x0301 0x0041 0x030a */

 0x0301 0x0061 0x030a */

 0x0307 0x0053 0x030c */

 0x0307 0x0073 0x030c */

 0x0300 0x0391 0x0313 */

 0x0301 0x0391 0x0313 */

 0x0342 0x0391 0x0313 */

 0x0300 0x0395 0x0313 */

 0x0301 0x0395 0x0313 */

 0x0300 0x0397 0x0313 */

 0x0301 0x0397 0x0313 */

 0x0342 0x0397 0x0313 */

 0x0300 0x0399 0x0313 */

 0x0301 0x0399 0x0313 */

 0x0342 0x0399 0x0313 */

 0x0300 0x039f 0x0313 */

 0x0301 0x039f 0x0313 */

 0x0300 0x03a9 0x0313 */

 0x0301 0x03a9 0x0313 */

 0x0342 0x03a9 0x0313 */

 0x0300 0x03b1 0x0313 */

 0x0301 0x03b1 0x0313 */

 0x0342 0x03b1 0x0313 */

 0x0300 0x03b5 0x0313 */

 0x0301 0x03b5 0x0313 */

 0x0300 0x03b7 0x0313 */

 0x0301 0x03b7 0x0313 */

 0x0342 0x03b7 0x0313 */

 0x0300 0x03b9 0x0313 */

 0x0301 0x03b9 0x0313 */

 0x0342 0x03b9 0x0313 */

 0x0300 0x03bf 0x0313 */

 0x0301 0x03bf 0x0313 */

 0x0300 0x03c5 0x0313 */

 0x0301 0x03c5 0x0313 */

 0x0342 0x03c5 0x0313 */

 0x0300 0x03c9 0x0313 */

 0x0301 0x03c9 0x0313 */

 0x0342 0x03c9 0x0313 */

 0x0300 0x0391 0x0314 */

 0x0301 0x0391 0x0314 */

 0x0342 0x0391 0x0314 */

 0x0300 0x0395 0x0314 */

 0x0301 0x0395 0x0314 */

 0x0300 0x0397 0x0314 */

 0x0301 0x0397 0x0314 */

 0x0342 0x0397 0x0314 */

 0x0300 0x0399 0x0314 */

 0x0301 0x0399 0x0314 */

 0x0342 0x0399 0x0314 */

 0x0300 0x039f 0x0314 */

 0x0301 0x039f 0x0314 */

 0x0300 0x03a5 0x0314 */

 0x0301 0x03a5 0x0314 */

 0x0342 0x03a5 0x0314 */

 0x0300 0x03a9 0x0314 */

 0x0301 0x03a9 0x0314 */

 0x0342 0x03a9 0x0314 */

 0x0300 0x03b1 0x0314 */

 0x0301 0x03b1 0x0314 */

 0x0342 0x03b1 0x0314 */

 0x0300 0x03b5 0x0314 */

 0x0301 0x03b5 0x0314 */

 0x0300 0x03b7 0x0314 */

 0x0301 0x03b7 0x0314 */

 0x0342 0x03b7 0x0314 */

 0x0300 0x03b9 0x0314 */

 0x0301 0x03b9 0x0314 */

 0x0342 0x03b9 0x0314 */

 0x0300 0x03bf 0x0314 */

 0x0301 0x03bf 0x0314 */

 0x0300 0x03c5 0x0314 */

 0x0301 0x03c5 0x0314 */

 0x0342 0x03c5 0x0314 */

 0x0300 0x03c9 0x0314 */

 0x0301 0x03c9 0x0314 */

 0x0342 0x03c9 0x0314 */

 0x0300 0x004f 0x031b */

 0x0301 0x004f 0x031b */

 0x0303 0x004f 0x031b */

 0x0309 0x004f 0x031b */

 0x0323 0x004f 0x031b */

 0x0300 0x0055 0x031b */

 0x0301 0x0055 0x031b */

 0x0303 0x0055 0x031b */

 0x0309 0x0055 0x031b */

 0x0323 0x0055 0x031b */

 0x0300 0x006f 0x031b */

 0x0301 0x006f 0x031b */

 0x0303 0x006f 0x031b */

 0x0309 0x006f 0x031b */

 0x0323 0x006f 0x031b */

 0x0300 0x0075 0x031b */

 0x0301 0x0075 0x031b */

 0x0303 0x0075 0x031b */

 0x0309 0x0075 0x031b */

 0x0323 0x0075 0x031b */

 0x0302 0x0041 0x0323 */

 0x0306 0x0041 0x0323 */

 0x0302 0x0045 0x0323 */

 0x0304 0x004c 0x0323 */

 0x0302 0x004f 0x0323 */

 0x0304 0x0052 0x0323 */

 0x0307 0x0053 0x0323 */

 0x0302 0x0061 0x0323 */

 0x0306 0x0061 0x0323 */

 0x0302 0x0065 0x0323 */

 0x0304 0x006c 0x0323 */

 0x0302 0x006f 0x0323 */

 0x0304 0x0072 0x0323 */

 0x0307 0x0073 0x0323 */

 0x0301 0x0043 0x0327 */

 0x0306 0x0045 0x0327 */

 0x0301 0x0063 0x0327 */

 0x0306 0x0065 0x0327 */

 0x0304 0x004f 0x0328 */

 0x0304 0x006f 0x0328 */

 0x0313 0x0391 0x0345 */

 0x0314 0x0391 0x0345 */

 0x0313 0x0397 0x0345 */

 0x0314 0x0397 0x0345 */

 0x0313 0x03a9 0x0345 */

 0x0314 0x03a9 0x0345 */

 0x0300 0x03b1 0x0345 */

 0x0301 0x03b1 0x0345 */

 0x0313 0x03b1 0x0345 */

 0x0314 0x03b1 0x0345 */

 0x0342 0x03b1 0x0345 */

 0x0300 0x03b7 0x0345 */

 0x0301 0x03b7 0x0345 */

 0x0313 0x03b7 0x0345 */

 0x0314 0x03b7 0x0345 */

 0x0342 0x03b7 0x0345 */

 0x0301 0x03bf 0x0345 */

 0x0300 0x03c9 0x0345 */

 0x0313 0x03c9 0x0345 */

 0x0314 0x03c9 0x0345 */

 0x0342 0x03c9 0x0345 */

 0x05c1 0x05e9 0x05bc */

 0x05c2 0x05e9 0x05bc */

 0x0cd5 0x0cc6 0x0cc2 */

 0x0f71 0x0fb2 0x0f80 */

 0x0f71 0x0fb3 0x0f80 */

 0x0300 0x0313 0x0391 0x0345 */

 0x0301 0x0313 0x0391 0x0345 */

 0x0342 0x0313 0x0391 0x0345 */

 0x0300 0x0314 0x0391 0x0345 */

 0x0301 0x0314 0x0391 0x0345 */

 0x0342 0x0314 0x0391 0x0345 */

 0x0300 0x0313 0x0397 0x0345 */

 0x0301 0x0313 0x0397 0x0345 */

 0x0342 0x0313 0x0397 0x0345 */

 0x0300 0x0314 0x0397 0x0345 */

 0x0301 0x0314 0x0397 0x0345 */

 0x0342 0x0314 0x0397 0x0345 */

 0x0300 0x0313 0x03a9 0x0345 */

 0x0301 0x0313 0x03a9 0x0345 */

 0x0342 0x0313 0x03a9 0x0345 */

 0x0300 0x0314 0x03a9 0x0345 */

 0x0301 0x0314 0x03a9 0x0345 */

 0x0342 0x0314 0x03a9 0x0345 */

 0x0300 0x0313 0x03b1 0x0345 */

 0x0301 0x0313 0x03b1 0x0345 */

 0x0342 0x0313 0x03b1 0x0345 */

 0x0300 0x0314 0x03b1 0x0345 */

 0x0301 0x0314 0x03b1 0x0345 */

 0x0342 0x0314 0x03b1 0x0345 */

 0x0300 0x0313 0x03b7 0x0345 */

 0x0301 0x0313 0x03b7 0x0345 */

 0x0342 0x0313 0x03b7 0x0345 */

 0x0300 0x0314 0x03b7 0x0345 */

 0x0301 0x0314 0x03b7 0x0345 */

 0x0342 0x0314 0x03b7 0x0345 */

 0x0300 0x0313 0x03c9 0x0345 */

 0x0301 0x0313 0x03c9 0x0345 */

 0x0342 0x0313 0x03c9 0x0345 */

 0x0300 0x0314 0x03c9 0x0345 */

 0x0301 0x0314 0x03c9 0x0345 */

 0x0342 0x0314 0x03c9 0x0345 */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/extents.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

 * Handling of Extents both in catalog and extents overflow trees

 Compare two extents keys, returns 0 on same, pos/neg for difference */

 panic? */

 Fail early and avoid ENOSPC during the btree operation */

	/*

	 * We can't just use hfsplus_mark_inode_dirty here, because we

	 * also get called from hfsplus_write_inode, which should not

	 * redirty the inode.  Instead the callers have to be careful

	 * to explicily mark the inode dirty, too.

 Get a block at iblock for inode, possibly allocating if create */

 Convert inode block to disk allocation block */

	/*

	 * hfsplus_ext_read_extent will write out a cached extent into

	 * the extents btree.  In that case we may have to mark the inode

	 * dirty even for a pure read of an extent here.

 panic? */

 Mapping the allocation file may lock the extent tree */

 panic? */

			/*

			 * Try to free all extents and

			 * return only last error

 extend alloc file */

 no extents yet */

 try to append to extents in inode */

 XXX: We lack error handling of hfsplus_file_truncate() */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/inode.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

 * Inode handling routines

	/*

	 * In case of error extending write may have instantiated a few

	 * blocks outside i_size. Trim these off again.

	/*

	 * Sync inode metadata into the catalog and extent trees.

	/*

	 * And explicitly write out the btrees.

 panic? */

 panic? */

 simple node checks? */

 don't silently ignore unsupported ext2 flags */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/hfsplus/attributes.c

 *

 * Vyacheslav Dubeyko <slava@dubeyko.com>

 *

 * Handling of records in attributes tree

	/* The length of the key, as stored in key_len field, does not include

	 * the size of the key_len field itself.

	 * So, offsetof(hfsplus_attr_key, key_name) is a trick because

	 * it takes into consideration key_len field (__be16) of

	 * hfsplus_attr_key structure instead of length field (__be16) of

	 * hfsplus_attr_unistr structure.

		/*

		 * Mac OS X supports only inline data attributes.

		 * Do nothing

		/*

		 * Mac OS X supports only inline data attributes.

		 * Do nothing.

		/*

		 * Align len on two-byte boundary.

		 * It needs to add pad byte if we have odd len.

 invalid input */

 Fail early and avoid ENOSPC during the btree operation */

 Mac OS X supports only inline data attributes. */

 All is OK. Do nothing. */

 Avoid btree corruption */

 Fail early and avoid ENOSPC during the btree operation */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/dir.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

 * Handling of directories

 Find the entry inside dir named dentry->d_name */

 No such entry */

				/*

				 * We found a link pointing to another link,

				 * so ignore it and treat it as regular file.

 This is completely artificial... */

	/*

	 * Can be done after the list insertion; exclusion with

	 * hfsplus_delete_cat() is provided by directory lock.

 panic? */

 Operation is not supported. */

 Try to delete anyway without error analysis. */

 Operation is not supported. */

 Try to delete anyway without error analysis. */

 Unlink destination if it already exists */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/catalog.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

 * Handling of catalog records

 Generates key for catalog file/folders record. */

 Generates key for catalog thread record. */

 invisible and namelocked */

 Try to get a catalog entry for given catalog id */

		/*

		 * Increment subfolder count. Note, the value is only meaningful

		 * for folders with HFSPLUS_HAS_FOLDER_COUNT flag set.

		/*

		 * Decrement subfolder count. Note, the value is only meaningful

		 * for folders with HFSPLUS_HAS_FOLDER_COUNT flag set.

		 *

		 * Check for zero. Some subfolders may have been created

		 * by an implementation ignorant of this counter.

	/*

	 * Fail early and avoid ENOSPC during the btree operations. We may

	 * have to split the root node at most once.

 panic? */

	/*

	 * Fail early and avoid ENOSPC during the btree operations. We may

	 * have to split the root node at most once.

 we only need to take spinlock for exclusion with ->release() */

	/*

	 * Fail early and avoid ENOSPC during the btree operations. We may

	 * have to split the root node at most twice.

 find the old dir entry and read the data */

 create new dir entry with the data from the old entry */

 finally remove the old entry */

 remove old thread entry */

 create new thread entry */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/bnode.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

 * Handle basic btree node operations

 Copy a specified range of bytes from the raw data of a node */

 TODO: optimize later... */

 TODO: optimize later... */

 TODO: optimize later... */

 move down? */

 Load a particular node out of a tree */

 Dispose of resources used by a node */

/*

 * Unused nodes have to be zeroed if this is the catalog tree and

 * a corresponding flag in the volume header is set.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/unicode.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

 * Handler routines for unicode strings

 Fold the case of a unicode char, given the 16 bit value */

 Returns folded char, or 0 if ignorable */

 Compare unicode strings, return values like normal strcmp */

 Compare names as a sequence of 16-bit unsigned integers */

 search for single decomposed char */

 start of a possibly decomposed Hangul char */

 compose the Hangul char */

 main loop for common case of not composed chars */

/*

 * Convert one or more ASCII characters into a single unicode character.

 * Returns the number of ASCII characters corresponding to the unicode char.

 Decomposes a non-Hangul unicode character. */

/*

 * Try to decompose a unicode character as Hangul. Return 0 if @uc is not

 * precomposed Hangul, otherwise return the length of the decomposition.

 *

 * This function was adapted from sample code from the Unicode Standard

 * Annex #15: Unicode Normalization Forms, version 3.2.0.

 *

 * Copyright (C) 1991-2018 Unicode, Inc.  All rights reserved.  Distributed

 * under the Terms of Use in http://www.unicode.org/copyright.html.

 Decomposes a single unicode character. */

 Hangul is handled separately */

/*

 * Hash a string to an integer as appropriate for the HFS+ filesystem.

 * Composed unicode characters are decomposed and case-folding is performed

 * if the appropriate bits are (un)set on the superblock.

/*

 * Compare strings with HFS+ filename ordering.

 * Composed unicode characters are decomposed and case-folding is performed

 * if the appropriate bits are (un)set on the superblock.

/*

 * linux/fs/hfsplus/part_tbl.c

 *

 * Copyright (C) 1996-1997  Paul H. Hargrove

 * This file may be distributed under the terms of

 * the GNU General Public License.

 *

 * Original code to handle the new style Mac partition table based on

 * a patch contributed by Holger Schemel (aeglos@valinor.owl.de).

 *

 * In function preconditions the term "valid" applied to a pointer to

 * a structure means that the pointer is non-NULL and the structure it

 * points to has all fields initialized to consistent values.

 *

 offsets to various blocks */

 Driver Descriptor block */

 First block of partition map */

 Block (w/i partition) of MDB */

 magic numbers for various disk blocks */

 "ER": driver descriptor map */

 "TS": old-type partition map */

 "PM": new-type partition map */

 "BD": HFS MDB (super block) */

 MFS MDB (super block) */

/*

 * The new style Mac partition map

 *

 * For each partition on the media there is a physical block (512-byte

 * block) containing one of these structures.  These blocks are

 * contiguous starting at block 1.

 signature */

 padding */

 partition blocks count */

 physical block start of partition */

 physical block count of partition */

	u8	pmPartName[32];	/* (null terminated?) string

				   giving the name of this

	u8	pmPartType[32];	/* (null terminated?) string

				   giving the type of this

 a bunch more stuff we don't need */

/*

 * The old style Mac partition map

 *

 * The partition map consists for a 2-byte signature followed by an

 * array of these structures.  The map is terminated with an all-zero

 * one of these.

 Signature bytes */

"TFS1"*/ &&

/*

 * Parse the partition map looking for the start and length of a

 * HFS/HFS+ partition.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/bitmap.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

 * Handling of allocation file

 scan the first partial u32 for zero bits */

 scan complete u32s for the first zero bit */

 do any partial u32 at the start */

 do full u32s */

 do any partial u32 at end */

 is there any actual work to be done? */

 are all of the bits in range? */

 do any partial u32 at the start */

 do full u32s */

 do any partial u32 at end */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/hfsplus/xattr_trusted.c

 *

 * Vyacheslav Dubeyko <slava@dubeyko.com>

 *

 * Handler for trusted extended attributes.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/fs/hfsplus/xattr_trusted.c

 *

 * Vyacheslav Dubeyko <slava@dubeyko.com>

 *

 * Handler for storing security labels as extended attributes.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/brec.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

 * Handle individual btree records

 Get the length and offset of the given record in the given node */

 Get the length of the key from a keyed record */

 new record idx and complete record size */

 get last offset */

 write new last offset */

 move all following entries */

 move data away */

	/*

	 * update parent key if we inserted a key

	 * at the start of the node and it is not the new node

 create index data entry */

 get index key */

 fill hole */

 panic? */

		/* new record is in the lower half,

		 * so leave some more space there

 update new bnode header */

 update previous bnode header */

 update next bnode header */

 if there is no next node, this might be the new tail */

 size difference between old and new key */

 move previous cnid too */

 create index key and entry */

 restore search_key */

 insert old root idx into new root */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/bfind.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

 * Search routines for btrees

 used-uninitialized warning */

 Find the record in bnode that best matches key (not greater than...)*/

 Traverse a B*Tree from the root to a leaf finding best fit to key */

 Return allocated copy of node found, set recnum to best record */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/options.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

 * Option parsing

 Initialize an options object to reasonable defaults */

 convert a "four byte character" to a 32 bit int with error checks */

 Parse options from mount. Returns 0 on failure */

 input is the options passed to mount() as a string */

 try utf8 first, as this is the old default behaviour */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/fs/hfsplus/btree.c

 *

 * Copyright (C) 2001

 * Brad Boyer (flar@allandria.com)

 * (C) 2003 Ardis Technologies <roman@ardistech.com>

 *

 * Handle opening/closing btree

/*

 * Initial source code of clump size calculation is gotten

 * from http://opensource.apple.com/tarballs/diskdev_cmds/

/*

 *	    Volume	Attributes	 Catalog	 Extents

 *	     Size	Clump (MB)	Clump (MB)	Clump (MB)

   1GB */	  4,		  4,		 4,

   2GB */	  6,		  6,		 4,

   4GB */	  8,		  8,		 4,

   8GB */	 11,		 11,		 5,

	/*

	 * For volumes 16GB and larger, we want to make sure that a full OS

	 * install won't require fragmentation of the Catalog or Attributes

	 * B-trees.  We do this by making the clump sizes sufficiently large,

	 * and by leaving a gap after the B-trees for them to grow into.

	 *

	 * For SnowLeopard 10A298, a FullNetInstall with all packages selected

	 * results in:

	 * Catalog B-tree Header

	 *	nodeSize:          8192

	 *	totalNodes:       31616

	 *	freeNodes:         1978

	 * (used = 231.55 MB)

	 * Attributes B-tree Header

	 *	nodeSize:          8192

	 *	totalNodes:       63232

	 *	freeNodes:          958

	 * (used = 486.52 MB)

	 *

	 * We also want Time Machine backup volumes to have a sufficiently

	 * large clump size to reduce fragmentation.

	 *

	 * The series of numbers for Catalog and Attribute form a geometric

	 * series. For Catalog (16GB to 512GB), each term is 8**(1/5) times

	 * the previous term.  For Attributes (16GB to 512GB), each term is

	 * 4**(1/5) times the previous term.  For 1TB to 16TB, each term is

	 * 2**(1/5) times the previous term.

  16GB */	 64,		 32,		 5,

  32GB */	 84,		 49,		 6,

  64GB */	111,		 74,		 7,

 128GB */	147,		111,		 8,

 256GB */	194,		169,		 9,

 512GB */	256,		256,		11,

   1TB */	294,		294,		14,

   2TB */	338,		338,		16,

   4TB */	388,		388,		20,

   8TB */	446,		446,		25,

  16TB */	512,		512,		32

 Figure out which column of the above table to use for this file. */

	/*

	 * The default clump size is 0.8% of the volume size. And

	 * it must also be a multiple of the node and block size.

  0.8 %  */

 turn exponent into table index... */

 empty body */

	/*

	 * Round the clump size to a multiple of node and block size.

	 * NOTE: This rounds down.

	/*

	 * Rounding down could have rounded down to 0 if the block size was

	 * greater than the clump size.  If so, just use one block or node.

 Get a reference to a B*Tree and do some initial checks */

 Load the header */

 Verify the tree and set the correct compare function */

 Release resources used by a btree */

 panic? */

 Load the header */

 Make sure @tree has enough space for the @rsvd_nodes */

 panic */;

 panic */;

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.

/*

 * If new entry was created in the parent, it could create the 8.3 alias (the

 * shortname of logname).  So, the parent may have the negative-dentry which

 * matches the created 8.3 alias.

 *

 * If it happened, the negative dentry isn't actually negative anymore.  So,

 * drop it.

	/*

	 * This is not negative dentry. Always valid.

	 *

	 * Note, rename() to existing directory entry will have ->d_inode, and

	 * will use existing name which isn't specified name by user.

	 *

	 * We may be able to drop this positive dentry here. But dropping

	 * positive dentry isn't good idea. So it's unsupported like

	 * rename("filename", "FILENAME") for now.

	/*

	 * Drop the negative dentry, in order to make sure to use the case

	 * sensitive name which is specified by user if this is for creation.

 returns the length of a struct qstr, ignoring trailing dots */

/*

 * Compute the hash for the exfat name corresponding to the dentry.  If the name

 * is invalid, we leave the hash code unchanged so that the existing dentry can

 * be used. The exfat fs routines will return ENOENT or EINVAL as appropriate.

		/*

		 * exfat_toupper() works only for code points up to the U+FFFF.

 used only in search empty_slot() */

 search EMPTY CONTINUOUS "num_entries" entries */

					/* unused empty group means

					 * an empty group which includes

					 * unused dentry

 found and invalidate hint_femp */

		/*

		 * exFAT spec allows a dir to grow up to 8388608(256MB)

		 * dentries

/* find empty directory entry.

 * if there isn't any empty slot, expand cluster chain.

 we trust p_dir->size regardless of FAT type */

		/*

		 * Allocate new cluster to this directory

 allocate a cluster */

 append to the FAT chain */

			/* no-fat-chain bit is disabled,

			 * so fat-chain should be synced with alloc-bitmap

			/* the special case that new dentry

			 * should be allocated from the start of new cluster

 update the directory entry */

 directory inode should be updated in here */

/*

 * Name Resolution Functions :

 * Zero if it was successful; otherwise nonzero.

 strip all trailing periods */

	/*

	 * strip all leading spaces :

	 * "MS windows 7" supports leading spaces.

	 * So we should skip this preprocessing for compatibility.

	/* file name conversion :

	 * If lookup case, we allow bad-name for compatibility.

 return error value */

 exfat_find_empty_entry must be called before alloc_cluster() */

 -EIO or -ENOSPC */

 update the directory entry */

	/* fill the dos name directory entry information of the created file.

	 * the first cluster is not determined yet. (0)

 timestamp is already written, so mark_inode_dirty() is unneeded. */

 lookup a file */

 for optimized dir & entry to prevent long traverse of cluster chain */

 check the validity of directory name in the given pathname */

 check the validation of hint_stat and initialize it if required */

 search the file name for directories */

 -error value */

 adjust cdir to the optimized value */

	/*

	 * Checking "alias->d_parent == dentry->d_parent" to make sure

	 * FS is not corrupted (especially double linked dir).

		/*

		 * Unhashed alias is able to exist because of revalidate()

		 * called by lookup_fast. You can easily make this status

		 * by calling create and lookup concurrently

		 * In such case, we reuse an alias instead of new dentry

			/*

			 * This inode has non anonymous-DCACHE_DISCONNECTED

			 * dentry. This means, the user did ->lookup() by an

			 * another name (longname vs 8.3 alias of it) in past.

			 *

			 * Switch to new one for reason of locality if possible.

 remove an entry, BUT don't truncate */

 update the directory entry */

 This doesn't modify ei */

 timestamp is already written, so mark_inode_dirty() is unneeded. */

 -EIO or -ENOSPC */

 -EIO or -ENOSPC */

	/*

	 * the problem that struct exfat_inode_info caches wrong parent info.

	 *

	 * because of flag-mismatch of ei->dir,

	 * there is abnormal traversing cluster chain.

 rename or move a old file into a new file */

 check the validity of pointer parameters */

 check whether new dir is existing directory and empty */

 if new_inode exists, update ei */

 check the validity of directory name in the given new pathname */

 delete entries of new_dir */

 Free the clusters if new_inode is a dir(as if exfat_rmdir) */

 new_ei, new_clu_to_free */

 just set I/O error only */

		/* Update new_inode ei

		 * Prevent syncing removed new_inode

		 * (new_ei is already initialized above code ("if (new_inode)")

	/*

	 * The VFS already checks for existence, so for local filesystems

	 * the RENAME_NOREPLACE implementation is equivalent to plain rename.

	 * Don't support any other flags

 skip drop_nlink if new_inode already has been dropped */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.

 If there are some dirty buffers in the bdev inode */

 clu 0 & 1 */

 Unicode utf16 255 characters */

 retain persistent-flags */

 flags are not changed */

	/* skip updating volume dirty flag,

	 * if this volume has been mounted with read-only

 Show partition info */

 Deprecated options */

		/*

		 * Make the limit 24 just in case someone invents something

		 * unusual.

 set block size to read super block */

 read boot sector */

 check the validity of BOOT */

 fs_name may unprintable */

	/*

	 * must_be_zero field must be filled with zero to prevent mounting

	 * from FAT volume.

	/*

	 * sect_size_bits could be at least 9 and at most 12.

	/*

	 * sect_per_clus_bits could be at least 0 and at most 25 - sect_size_bits.

 because the cluster index starts with 2 */

 check consistencies */

 exFAT file size is limited by a disk volume size */

 check logical sector size */

 read boot sector sub-regions */

 extended boot sector sub-regions */

 boot checksum sub-regions */

 mount the file system volume */

 set up enough so that it can read an inode */

 volume flag will be updated in exfat_sync_fs */

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.

	/*

	 * If the indode is already unlinked, there is no need for updating it.

 get the directory entry of given file or directory */

 set FILE_INFO structure using the acquired struct exfat_dentry */

 File size should be zero if there is no cluster allocated */

/*

 * Input: inode, (logical) clu_offset, target allocation area

 * Output: errcode, cluster number

 * *clu = (~0), if it's unable to allocate a new cluster

 hint information */

 hint_bmap.clu should be valid */

 allocate a cluster */

 Broken FAT (i_sze > allocated FAT) */

 append to the FAT chain */

				/* no-fat-chain bit is disabled,

				 * so fat-chain should be synced with

				 * alloc-bitmap

 get stream entry */

 update directory entry */

 end of if != DIR_DELETED */

		/*

		 * Move *clu pointer along FAT chains (hole care) because the

		 * caller of this function expect *clu to be the last cluster.

		 * This only works when num_to_be_allocated >= 2,

		 * *clu = (the first cluster of the allocated chain) =>

		 * (the last cluster of ...)

 hint information */

	/*

	 * Adjust i_size_aligned if i_size_ondisk is bigger than it.

 Is this block already allocated? */

 sector offset in cluster */

 Treat newly added block / cluster */

		/*

		 * FIXME: blockdev_direct_IO() doesn't use ->write_begin(),

		 * so we need to update the ->i_size_aligned to block boundary.

		 *

		 * But we must fill the remaining area or hole by nul for

		 * updating ->i_size_aligned

		 *

		 * Return 0, and fallback to normal buffered write.

	/*

	 * Need to use the DIO_LOCKING for avoiding the race

	 * condition of exfat_get_block() and ->truncate().

 exfat_get_cluster() assumes the requested blocknr isn't truncated. */

/*

 * exfat_block_truncate_page() zeroes out a mapping from file offset `from'

 * up to the end of the block which corresponds to `from'.

 * This is required during truncate to physically zeroout the tail end

 * of that block so it doesn't yield old data if the file is later grown.

 * Also, avoid causing failure from fsx for cases of "data past EOF"

 doesn't deal with root inode */

 directory */

 regular file */

 ondisk and aligned size should be aligned with block size */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.

	/*

	 * First entry  : file entry

	 * Second entry : stream-extension entry

	 * Third entry  : first file-name entry

	 * So, the index of first file-name dentry should start from 2.

 end of name entry */

 read a directory entry from the opened directory */

 check if the given file ID is opened */

 hint_information */

 skip iterating emit_dots when dir is empty */

 name buffer should be allocated before use */

		/*

		 * At least we tried to read a sector.  Move cpos to next sector

		 * position (should be aligned).

	/*

	 * Before calling dir_emit(), sb_lock should be released.

	 * Because page fault can occur in dir_emit() when the size

	 * of buffer given from user is larger than one page size.

	/*

	 * To improve performance, free namebuf after unlock sb_lock.

	 * If namebuf is not allocated, this function do nothing

 1 file entry + 1 stream entry + name entries */

	/*

	 * We cannot use exfat_get_dentry_set here because file ep is not

	 * initialized yet.

 byte offset in cluster */

 byte offset in sector    */

 sector offset in cluster */

 Read-ahead is not required */

 Not sector aligned with ra_count, resize ra_count to page size */

/*

 * Returns a set of dentries for a file or dir.

 *

 * Note It provides a direct pointer to bh->data via exfat_get_dentry_cached().

 * User should call exfat_get_dentry_set() after setting 'modified' to apply

 * changes made in this entry set to the real device.

 *

 * in:

 *   sb+p_dir+entry: indicates a file/dir

 *   type:  specifies how many dentries should be included.

 * return:

 *   pointer of entry set on success,

 *   NULL on failure.

 byte offset in cluster */

 byte offset in sector */

 sector offset in cluster */

 get the next sector */

 validiate cached dentries */

/*

 * @ei:         inode info of parent directory

 * @p_dir:      directory structure of parent directory

 * @num_entries:entry size of p_uniname

 * @hint_opt:   If p_uniname is found, filled with optimized dir/entry

 *              for traversing cluster chain.

 * @return:

 *   >= 0:      file directory entry position where the name exists

 *   -ENOENT:   entry with the name does not exist

 *   -EIO:      I/O error

	/*

	 * We started at not 0 index,so we should try to find target

	 * from 0 index to the index we started at.

 reset empty hint */

 initialized hint_stat */

 next dentry we'll find is out of this cluster */

 just initialized hint_stat */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.

  0 ~  19*/

 20 ~  39*/

 40 ~  59*/

 60 ~  79*/

 80 ~  99*/

100 ~ 119*/

120 ~ 139*/

140 ~ 159*/

160 ~ 179*/

180 ~ 199*/

200 ~ 219*/

220 ~ 239*/

240 ~ 254*/

  0 ~  19*/

 20 ~  39*/

 40 ~  59*/

 60 ~  79*/

 80 ~  99*/

100 ~ 119*/

120 ~ 139*/

140 ~ 159*/

160 ~ 179*/

180 ~ 199*/

200 ~ 219*/

220 ~ 239*/

240 ~ 255*/

/*

 *  Allocation Bitmap Management Functions

		/*

		 * Only allowed when bogus allocation

		 * bitmap size is large

 release all buffers and free vol_amap */

/*

 * If the value of "clu" is 0, it means cluster 2 which is the first cluster of

 * the cluster heap.

 extend trim range for continuous free cluster */

 trim current range if it's larger than trim_minlen */

 set next start point of the free hole */

 try to trim remainder */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Written 1992,1993 by Werner Almesberger

 *  22/11/2000 - Fixed fat_date_unix2dos for dates earlier than 01/01/1980

 *		 and date_dos2unix for date==0 by Igor Zhbanov(bsg@uniyar.ac.ru)

 * Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.

/*

 * exfat_fs_error reports a file system problem that might indicate fa data

 * corruption/inconsistency. Depending on 'errors' mount option the

 * panic() is called, or error message is printed FAT and nothing is done,

 * or filesystem is remounted read-only (default behavior).

 * In case the file system is remounted read-only, it can be made writable

 * again by remounting it.

/*

 * exfat_msg() - print preformated EXFAT specific messages.

 * All logs except what uses exfat_fs_error() should be written by exfat_msg()

 level means KERN_ pacility level */

 0x40 <= (tz_off & 0x7F) <=0x7F */

 Convert a EXFAT time/date pair to a UNIX date (seconds since 1 1 70). */

 time_cs field represent 0 ~ 199cs(1990 ms) */

 Adjust timezone to UTC0. */

 Convert from local time to UTC using time_offset. */

 Convert linear UNIX date to a EXFAT time/date pair. */

 time_cs field represent 0 ~ 199cs(1990 ms) */

	/*

	 * Record 00h value for OffsetFromUtc field and 1 value for OffsetValid

	 * to indicate that local time and UTC are the same.

/*

 * The timestamp for access_time has double seconds granularity.

 * (There is no 10msIncrement field for access_time unlike create/modify_time)

 * atime also has only a 2-second resolution.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.

 use a default check */

 Of the r and x bits, all (subject to umask) must be present.*/

		/*

		 * Of the w bits, either all (subject to umask) or none must

		 * be present.

		/*

		 * If exfat_mode_can_hold_ro(inode) is false, can't change

		 * w bits.

 resize the file length */

 check if the given file ID is opened */

		/*

		 * Truncate FAT chain num_clusters after the first cluster

		 * num_clusters = min(new, phys);

		/*

		 * Follow FAT chain

		 * (defensive coding - works fine even with corrupted FAT table

 update the directory entry */

 File size should be zero if there is no cluster allocated */

 Any directory can not be truncated to zero */

 cut off from the FAT chain */

 invalidate cache and free the clusters */

 clear exfat cache */

 hint information */

 hint_stat will be used if this is directory. */

 free the clusters */

		/*

		 * Empty start_clu != ~0 (not allocated)

 Check for setting the inode time. */

	/*

	 * We don't return -EPERM here. Yes, strange, but this is too

	 * old behavior.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  linux/fs/fat/cache.c

 *

 *  Written 1992,1993 by Werner Almesberger

 *

 *  Mar 1999. AV. Changed cache, so that it uses the starting cluster instead

 *	of inode number.

 *  May 1999. AV. Fixed the bogosity with FAT32 (read "FAT28"). Fscking lusers.

 *  Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.

 number of contiguous clusters */

 cluster number in the file. */

 cluster number on disk. */

 Find the cache of "fclus" or nearest cache. */

 Find the same part as "new" in cluster-chain. */

 dummy cache */

 this cache was invalidated */

/*

 * Cache invalidation occurs rarely, thus the LRU chain is not updated. It

 * fixes itself after a while.

 Update. The copy of caches before this id is discarded. */

	/*

	 * Don`t use exfat_cache if zero offset or non-cluster allocation

		/*

		 * dummy, always not contiguous

		 * This is reinitialized by cache_init(), later.

 prevent the infinite loop of cluster chain */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.

 remap reserved clusters to simplify code */

 This function must be called with bitmap_lock held */

 invalid cluster number */

 no cluster to truncate */

 check cluster validation */

 flush bitmap only if index would be changed or for last cluster */

 Zeroing the unused blocks on this cluster */

 find new cluster */

 check cluster validation */

 update allocation bitmap */

 update FAT table */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2012-2013 Samsung Electronics Co., Ltd.

 Upcase table macro */

/*

 * Upcase table in compressed format (7.2.5.1 Recommended Up-case Table

 * in exfat specification, See:

 * https://docs.microsoft.com/en-us/windows/win32/fileio/exfat-specification).

/*

 * Allow full-width illegal characters :

 * "MS windows 7" supports full-width-invalid-name-characters.

 * So we should check half-width-invalid-name-characters(ASCII) only

 * for compatibility.

 *

 * " * / : < > ? \ |

 conversion failed */

 conversion failed */

 always len >= 0 */

 Process UTF-16 surrogate pair as one character */

			/*

			 * UTF-16 surrogate pair encodes code points above

			 * U+FFFF. Code points above U+FFFF are not supported

			 * by kernel NLS framework therefore use replacement

			 * character

 len == 1 */

 uni != index , uni != 0xFFFF */

 FATAL error: default upcase table has error */

 load successfully */

 load default upcase table */

 SPDX-License-Identifier: GPL-2.0

/*

 * fs/sysfs/dir.c - sysfs core and dir operation implementation

 *

 * Copyright (c) 2001-3 Patrick Mochel

 * Copyright (c) 2007 SUSE Linux Products GmbH

 * Copyright (c) 2007 Tejun Heo <teheo@suse.de>

 *

 * Please see Documentation/filesystems/sysfs.rst for more information.

/**

 * sysfs_create_dir_ns - create a directory for an object with a namespace tag

 * @kobj: object we're creating directory for

 * @ns: the namespace tag to use

/**

 *	sysfs_remove_dir - remove an object's directory.

 *	@kobj:	object.

 *

 *	The only thing special about this is that we remove any files in

 *	the directory before we remove the directory, and we've inlined

 *	what used to be sysfs_rmdir() below, instead of calling separately.

	/*

	 * In general, kboject owner is responsible for ensuring removal

	 * doesn't race with other operations and sysfs doesn't provide any

	 * protection; however, when @kobj is used as a symlink target, the

	 * symlinking entity usually doesn't own @kobj and thus has no

	 * control over removal.  @kobj->sd may be removed anytime

	 * and symlink code may end up dereferencing an already freed node.

	 *

	 * sysfs_symlink_target_lock synchronizes @kobj->sd

	 * disassociation against symlink operations so that symlink code

	 * can safely dereference @kobj->sd.

/**

 * sysfs_create_mount_point - create an always empty directory

 * @parent_kobj:  kobject that will contain this always empty directory

 * @name: The name of the always empty directory to add

/**

 *	sysfs_remove_mount_point - remove an always empty directory.

 *	@parent_kobj: kobject that will contain this always empty directory

 *	@name: The name of the always empty directory to remove

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * fs/sysfs/file.c - sysfs regular (text) file implementation

 *

 * Copyright (c) 2001-3 Patrick Mochel

 * Copyright (c) 2007 SUSE Linux Products GmbH

 * Copyright (c) 2007 Tejun Heo <teheo@suse.de>

 *

 * Please see Documentation/filesystems/sysfs.rst for more information.

/*

 * Determine ktype->sysfs_ops for the given kernfs_node.  This function

 * must be called while holding an active reference.

/*

 * Reads on sysfs are handled through seq_file, which takes care of hairy

 * details like buffering and seeking.  The following function pipes

 * sysfs_ops->show() result through seq_file.

 acquire buffer and ensure that it's >= PAGE_SIZE and clear */

	/*

	 * The code works fine with PAGE_SIZE return but it's likely to

	 * indicate truncated result or overflow in normal use cases.

 Try to struggle along */

 kernfs read callback for regular sysfs files with pre-alloc */

	/*

	 * If buf != of->prealloc_buf, we don't know how

	 * large it is, so cannot safely pass it to ->show

 kernfs write callback for regular sysfs files */

 kernfs write callback for bin sysfs files */

 every kobject with an attribute needs a ktype assigned */

/**

 * sysfs_create_file_ns - create an attribute file for an object with custom ns

 * @kobj: object we're creating for

 * @attr: attribute descriptor

 * @ns: namespace the new file should belong to

/**

 * sysfs_add_file_to_group - add an attribute file to a pre-existing group.

 * @kobj: object we're acting for.

 * @attr: attribute descriptor.

 * @group: group name.

/**

 * sysfs_chmod_file - update the modified mode value on an object attribute.

 * @kobj: object we're acting for.

 * @attr: attribute descriptor.

 * @mode: file permissions.

 *

/**

 * sysfs_break_active_protection - break "active" protection

 * @kobj: The kernel object @attr is associated with.

 * @attr: The attribute to break the "active" protection for.

 *

 * With sysfs, just like kernfs, deletion of an attribute is postponed until

 * all active .show() and .store() callbacks have finished unless this function

 * is called. Hence this function is useful in methods that implement self

 * deletion.

/**

 * sysfs_unbreak_active_protection - restore "active" protection

 * @kn: Pointer returned by sysfs_break_active_protection().

 *

 * Undo the effects of sysfs_break_active_protection(). Since this function

 * calls kernfs_put() on the kernfs node that corresponds to the 'attr'

 * argument passed to sysfs_break_active_protection() that attribute may have

 * been removed between the sysfs_break_active_protection() and

 * sysfs_unbreak_active_protection() calls, it is not safe to access @kn after

 * this function has returned.

/**

 * sysfs_remove_file_ns - remove an object attribute with a custom ns tag

 * @kobj: object we're acting for

 * @attr: attribute descriptor

 * @ns: namespace tag of the file to remove

 *

 * Hash the attribute name and namespace tag and kill the victim.

/**

 * sysfs_remove_file_self - remove an object attribute from its own method

 * @kobj: object we're acting for

 * @attr: attribute descriptor

 *

 * See kernfs_remove_self() for details.

/**

 * sysfs_remove_file_from_group - remove an attribute file from a group.

 * @kobj: object we're acting for.

 * @attr: attribute descriptor.

 * @group: group name.

/**

 *	sysfs_create_bin_file - create binary file for object.

 *	@kobj:	object.

 *	@attr:	attribute descriptor.

/**

 *	sysfs_remove_bin_file - remove binary file for object.

 *	@kobj:	object.

 *	@attr:	attribute descriptor.

/**

 *	sysfs_link_change_owner - change owner of a sysfs file.

 *	@kobj:	object of the kernfs_node the symlink is located in.

 *	@targ:	object of the kernfs_node the symlink points to.

 *	@name:	name of the link.

 *	@kuid:	new owner's kuid

 *	@kgid:	new owner's kgid

 *

 * This function looks up the sysfs symlink entry @name under @kobj and changes

 * the ownership to @kuid/@kgid. The symlink is looked up in the namespace of

 * @targ.

 *

 * Returns 0 on success or error code on failure.

/**

 *	sysfs_file_change_owner - change owner of a sysfs file.

 *	@kobj:	object.

 *	@name:	name of the file to change.

 *	@kuid:	new owner's kuid

 *	@kgid:	new owner's kgid

 *

 * This function looks up the sysfs entry @name under @kobj and changes the

 * ownership to @kuid/@kgid.

 *

 * Returns 0 on success or error code on failure.

/**

 *	sysfs_change_owner - change owner of the given object.

 *	@kobj:	object.

 *	@kuid:	new owner's kuid

 *	@kgid:	new owner's kgid

 *

 * Change the owner of the default directory, files, groups, and attributes of

 * @kobj to @kuid/@kgid. Note that sysfs_change_owner mirrors how the sysfs

 * entries for a kobject are added by driver core. In summary,

 * sysfs_change_owner() takes care of the default directory entry for @kobj,

 * the default attributes associated with the ktype of @kobj and the default

 * attributes associated with the ktype of @kobj.

 * Additional properties not added by driver core have to be changed by the

 * driver or subsystem which created them. This is similar to how

 * driver/subsystem specific entries are removed.

 *

 * Returns 0 on success or error code on failure.

 Change the owner of the kobject itself. */

		/*

		 * Change owner of the default attributes associated with the

		 * ktype of @kobj.

		/*

		 * Change owner of the default groups associated with the

		 * ktype of @kobj.

/**

 *	sysfs_emit - scnprintf equivalent, aware of PAGE_SIZE buffer.

 *	@buf:	start of PAGE_SIZE buffer.

 *	@fmt:	format

 *	@...:	optional arguments to @format

 *

 *

 * Returns number of characters written to @buf.

/**

 *	sysfs_emit_at - scnprintf equivalent, aware of PAGE_SIZE buffer.

 *	@buf:	start of PAGE_SIZE buffer.

 *	@at:	offset in @buf to start write in bytes

 *		@at must be >= 0 && < PAGE_SIZE

 *	@fmt:	format

 *	@...:	optional arguments to @fmt

 *

 *

 * Returns number of characters written starting at &@buf[@at].

 SPDX-License-Identifier: GPL-2.0

/*

 * fs/sysfs/symlink.c - sysfs symlink implementation

 *

 * Copyright (c) 2001-3 Patrick Mochel

 * Copyright (c) 2007 SUSE Linux Products GmbH

 * Copyright (c) 2007 Tejun Heo <teheo@suse.de>

 *

 * Please see Documentation/filesystems/sysfs.rst for more information.

	/*

	 * We don't own @target_kobj and it may be removed at any time.

	 * Synchronize using sysfs_symlink_target_lock.  See

	 * sysfs_remove_dir() for details.

/**

 *	sysfs_create_link_sd - create symlink to a given object.

 *	@kn:		directory we're creating the link in.

 *	@target:	object we're pointing to.

 *	@name:		name of the symlink.

/**

 *	sysfs_create_link - create symlink between two objects.

 *	@kobj:	object whose directory we're creating the link in.

 *	@target:	object we're pointing to.

 *	@name:		name of the symlink.

/**

 *	sysfs_create_link_nowarn - create symlink between two objects.

 *	@kobj:	object whose directory we're creating the link in.

 *	@target:	object we're pointing to.

 *	@name:		name of the symlink.

 *

 *	This function does the same as sysfs_create_link(), but it

 *	doesn't warn if the link already exists.

/**

 *	sysfs_delete_link - remove symlink in object's directory.

 *	@kobj:	object we're acting for.

 *	@targ:	object we're pointing to.

 *	@name:	name of the symlink to remove.

 *

 *	Unlike sysfs_remove_link sysfs_delete_link has enough information

 *	to successfully delete symlinks in tagged directories.

	/*

	 * We don't own @target and it may be removed at any time.

	 * Synchronize using sysfs_symlink_target_lock.  See

	 * sysfs_remove_dir() for details.

/**

 *	sysfs_remove_link - remove symlink in object's directory.

 *	@kobj:	object we're acting for.

 *	@name:	name of the symlink to remove.

/**

 *	sysfs_rename_link_ns - rename symlink in object's directory.

 *	@kobj:	object we're acting for.

 *	@targ:	object we're pointing to.

 *	@old:	previous name of the symlink.

 *	@new:	new name of the symlink.

 *	@new_ns: new namespace of the symlink.

 *

 *	A helper function for the common rename symlink idiom.

 SPDX-License-Identifier: GPL-2.0

/*

 * fs/sysfs/group.c - Operations for adding/removing multiple files at once.

 *

 * Copyright (c) 2003 Patrick Mochel

 * Copyright (c) 2003 Open Source Development Lab

 * Copyright (c) 2013 Greg Kroah-Hartman

 * Copyright (c) 2013 The Linux Foundation

			/*

			 * In update mode, we're changing the permissions or

			 * visibility.  Do this by first removing then

			 * re-adding (if required) the file.

 Updates may happen before the object has been instantiated */

/**

 * sysfs_create_group - given a directory kobject, create an attribute group

 * @kobj:	The kobject to create the group on

 * @grp:	The attribute group to create

 *

 * This function creates a group for the first time.  It will explicitly

 * warn and error if any of the attribute files being created already exist.

 *

 * Returns 0 on success or error code on failure.

/**

 * sysfs_create_groups - given a directory kobject, create a bunch of attribute groups

 * @kobj:	The kobject to create the group on

 * @groups:	The attribute groups to create, NULL terminated

 *

 * This function creates a bunch of attribute groups.  If an error occurs when

 * creating a group, all previously created groups will be removed, unwinding

 * everything back to the original state when this function was called.

 * It will explicitly warn and error if any of the attribute files being

 * created already exist.

 *

 * Returns 0 on success or error code from sysfs_create_group on failure.

/**

 * sysfs_update_groups - given a directory kobject, create a bunch of attribute groups

 * @kobj:	The kobject to update the group on

 * @groups:	The attribute groups to update, NULL terminated

 *

 * This function update a bunch of attribute groups.  If an error occurs when

 * updating a group, all previously updated groups will be removed together

 * with already existing (not updated) attributes.

 *

 * Returns 0 on success or error code from sysfs_update_group on failure.

/**

 * sysfs_update_group - given a directory kobject, update an attribute group

 * @kobj:	The kobject to update the group on

 * @grp:	The attribute group to update

 *

 * This function updates an attribute group.  Unlike

 * sysfs_create_group(), it will explicitly not warn or error if any

 * of the attribute files being created already exist.  Furthermore,

 * if the visibility of the files has changed through the is_visible()

 * callback, it will update the permissions and add or remove the

 * relevant files. Changing a group's name (subdirectory name under

 * kobj's directory in sysfs) is not allowed.

 *

 * The primary use for this function is to call it after making a change

 * that affects group visibility.

 *

 * Returns 0 on success or error code on failure.

/**

 * sysfs_remove_group: remove a group from a kobject

 * @kobj:	kobject to remove the group from

 * @grp:	group to remove

 *

 * This function removes a group of attributes from a kobject.  The attributes

 * previously have to have been created for this group, otherwise it will fail.

/**

 * sysfs_remove_groups - remove a list of groups

 *

 * @kobj:	The kobject for the groups to be removed from

 * @groups:	NULL terminated list of groups to be removed

 *

 * If groups is not NULL, remove the specified groups from the kobject.

/**

 * sysfs_merge_group - merge files into a pre-existing attribute group.

 * @kobj:	The kobject containing the group.

 * @grp:	The files to create and the attribute group they belong to.

 *

 * This function returns an error if the group doesn't exist or any of the

 * files already exist in that group, in which case none of the new files

 * are created.

/**

 * sysfs_unmerge_group - remove files from a pre-existing attribute group.

 * @kobj:	The kobject containing the group.

 * @grp:	The files to remove and the attribute group they belong to.

/**

 * sysfs_add_link_to_group - add a symlink to an attribute group.

 * @kobj:	The kobject containing the group.

 * @group_name:	The name of the group.

 * @target:	The target kobject of the symlink to create.

 * @link_name:	The name of the symlink to create.

/**

 * sysfs_remove_link_from_group - remove a symlink from an attribute group.

 * @kobj:	The kobject containing the group.

 * @group_name:	The name of the group.

 * @link_name:	The name of the symlink to remove.

/**

 * compat_only_sysfs_link_entry_to_kobj - add a symlink to a kobject pointing

 * to a group or an attribute

 * @kobj:		The kobject containing the group.

 * @target_kobj:	The target kobject.

 * @target_name:	The name of the target group or attribute.

 * @symlink_name:	The name of the symlink file (target_name will be

 *			considered if symlink_name is NULL).

	/*

	 * We don't own @target_kobj and it may be removed at any time.

	 * Synchronize using sysfs_symlink_target_lock. See sysfs_remove_dir()

	 * for details.

/**

 * sysfs_group_change_owner - change owner of an attribute group.

 * @kobj:	The kobject containing the group.

 * @grp:	The attribute group.

 * @kuid:	new owner's kuid

 * @kgid:	new owner's kgid

 *

 * Returns 0 on success or error code on failure.

/**

 * sysfs_groups_change_owner - change owner of a set of attribute groups.

 * @kobj:	The kobject containing the groups.

 * @groups:	The attribute groups.

 * @kuid:	new owner's kuid

 * @kgid:	new owner's kgid

 *

 * Returns 0 on success or error code on failure.

 SPDX-License-Identifier: GPL-2.0

/*

 * fs/sysfs/symlink.c - operations for initializing and mounting sysfs

 *

 * Copyright (c) 2001-3 Patrick Mochel

 * Copyright (c) 2007 SUSE Linux Products GmbH

 * Copyright (c) 2007 Tejun Heo <teheo@suse.de>

 *

 * Please see Documentation/filesystems/sysfs.rst for more information.

 SPDX-License-Identifier: GPL-2.0

/*

 * Super block/filesystem wide operations

 *

 * Copyright (C) 1996 Peter J. Braam <braam@maths.ox.ac.uk> and 

 * Michael Callahan <callahan@maths.ox.ac.uk> 

 * 

 * Rewritten for Linux 2.1.  Peter Braam <braam@cs.cmu.edu>

 * Copyright (C) Carnegie Mellon University

 VFS super_block ops */

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

 exported operations */

 Ignore errors in data, for backward compatibility */

 XXXXX  what do we put here?? */

 get root fid from Venus: this needs the root inode */

 make root inode */

 cannot set type */

 Venus is responsible for truncating the container-file!!! */

 fake something like AFS does */

 and fill in the rest */

 init_coda: used by filesystems.c to register coda */

 SPDX-License-Identifier: GPL-2.0

/*

 * Inode operations for Coda filesystem

 * Original version: (C) 1996 P. Braam and M. Callahan

 * Rewritten for Linux 2.1. (C) 1997 Carnegie Mellon University

 * 

 * Carnegie Mellon encourages users to contribute improvements to

 * the Coda project. Contact Peter Braam (coda@cs.cmu.edu).

 initialize the debugging variables */

 print a fid */

 recognize special .CONTROL name */

 utility functions below */

	/* inode's i_flags, i_ino are set by iget

	 * XXX: is this all we need ??

/* 

 * BSD sets attributes that need not be modified to -1. 

 * Linux uses the valid field to indicate what should be

 * looked at.  The BSD type field needs to be deduced from linux 

 * mode.

 * So we have to do some translations here.

 clean out */        

 determine the type */

 don't do others */

 set those vattrs that need change */

 SPDX-License-Identifier: GPL-2.0

/*

 * Directory operations for Coda filesystem

 * Original version: (C) 1996 P. Braam and M. Callahan

 * Rewritten for Linux 2.1. (C) 1997 Carnegie Mellon University

 * 

 * Carnegie Mellon encourages users to contribute improvements to

 * the Coda project. Contact Peter Braam (coda@cs.cmu.edu).

 same as fs/bad_inode.c */

 inode operations for directories */

 access routines: lookup, readlink, permission */

 control object, create inode on the fly */

	/* invalidate the directory cnode's attributes so we refetch the

	/* optimistically we can also act as if our nose bleeds. The

	 * granularity of the mtime is coarse anyways so we might actually be

/* we have to wrap inc_nlink/drop_nlink because sometimes userspace uses a

 * trick to fool GNU find's optimizations. If we can't be sure of the link

 * (because of volume mount points) we set i_nlink to 1 which forces find

 * to consider every child as a possible directory. We should also never

 creation routines: create, mknod, mkdir, link, symlink */

 invalidate the directory cnode's attributes */

 invalidate the directory cnode's attributes */

 try to make de an entry in dir_inodde linked to source_de */ 

	/*

	 * This entry is now negative. Since we do not create

	 * an inode for the entry we have to drop it.

 mtime is no good anymore */

 destruction routines: unlink, rmdir */

 VFS may delete the child */

 fix the link count of the parent */

 rename */

 support routines */

 read entries from the directory file */

 end of directory file reached */

 catch truncated reads */

 validate whether the directory file actually makes sense */

 Make sure we skip '.' and '..', we already got those */

 skip null entries */

		/* we'll always have progress because d_reclen is unsigned and

 file operations for directories */

 Venus: we must read Venus dirents from a file */

 called when a cache lookup succeeds */

 propagate for a flush */

 pretend it's valid, but don't change the flags */

 clear the flags. */

/*

 * This is the callback from dput() when d_count is going to 0.

 * We use this to unhash dentries with bad inodes.

/*

 * This is called when we want to check if the inode has

 * changed on the server.  Coda makes this easy since the

 * cache manager Venus issues a downcall to the kernel when this 

 * happens 

		/* this inode may be lost if:

		   - it's ino changed 

		   - type changes must be permitted for repair and

		   missing mount points.

		/* the following can happen when a local fid is replaced 

 SPDX-License-Identifier: GPL-2.0

/* cnode related routines for the coda kernel code

   (C) 1996 Peter Braam

 cnode.c */

 we still need to set i_ino for things like stat(2) */

 inode is locked and unique, no need to grab cii->c_lock */

 Inode has changed type, mark bad and grab a new one */

/* this is effectively coda_iget:

   - get attributes (might be cached)

   - get the inode for the fid using vfs iget

   - link the two up if this is needed

   - fill in the attributes

 We get inode numbers from Venus -- see venus source */

/* Although we treat Coda file identifiers as immutable, there is one

 * special case for files created during a disconnection where they may

 * not be globally unique. When an identifier collision is detected we

 * first try to flush the cached inode from the kernel and finally

 * resort to renaming/rehashing in-place. Userspace remembers both old

 * and new values of the identifier to handle any in-flight upcalls.

 * The real solution is to use globally unique UUIDs as identifiers, but

 replace fid and rehash inode */

 XXX we probably need to hold some lock here! */

 convert a fid to an inode. */

	/* we should never see newly created inodes because we intentionally

 the CONTROL inode is made without asking attributes from Venus */

 SPDX-License-Identifier: GPL-2.0

/*

 * Sysctl operations for Coda filesystem

 * Original version: (C) 1996 P. Braam and M. Callahan

 * Rewritten for Linux 2.1. (C) 1997 Carnegie Mellon University

 * 

 * Carnegie Mellon encourages users to contribute improvements to

 * the Coda project. Contact Peter Braam (coda@cs.cmu.edu).

 SPDX-License-Identifier: GPL-2.0

/*

 * File operations for Coda.

 * Original version: (C) 1996 Peter Braam 

 * Rewritten for Linux 2.1: (C) 1997 Carnegie Mellon University

 *

 * Carnegie Mellon encourages users of this code to contribute improvements

 * to the Coda project. Contact Peter Braam <coda@cs.cmu.edu>.

	/* only allow additional mmaps as long as userspace isn't changing

 keep track of how often the coda_inode/host_file has been mmapped */

		/* if call_mmap fails, our caller will put host_file so we

		 * should drop the reference to the coda_file that we got.

 here we add redirects for the open/close vm_operations */

 assume access intents are supported unless we hear otherwise */

 did we mmap this file? */

	/* VFS fput ignores the return value from file_operations->release, so

 SPDX-License-Identifier: GPL-2.0

/*

 * Pioctl operations for Coda.

 * Original version: (C) 1996 Peter Braam

 * Rewritten for Linux 2.1: (C) 1997 Carnegie Mellon University

 *

 * Carnegie Mellon encourages users of this code to contribute improvements

 * to the Coda project. Contact Peter Braam <coda@cs.cmu.edu>.

 pioctl ops */

 exported from this file */

 the coda pioctl inode ops */

 get the Pioctl data arguments from user space */

	/*

	 * Look up the pathname. Note that the pathname is in

	 * user memory, and namei takes care of this

 return if it is not a Coda inode */

 now proceed to make the upcall */

 SPDX-License-Identifier: GPL-2.0

/*

 * Symlink inode operations for Coda filesystem

 * Original version: (C) 1996 P. Braam and M. Callahan

 * Rewritten for Linux 2.1. (C) 1997 Carnegie Mellon University

 * 

 * Carnegie Mellon encourages users to contribute improvements to

 * the Coda project. Contact Peter Braam (coda@cs.cmu.edu).

 SPDX-License-Identifier: GPL-2.0

/*

 * Cache operations for Coda.

 * For Linux 2.1: (C) 1997 Carnegie Mellon University

 * For Linux 2.3: (C) 2000 Carnegie Mellon University

 *

 * Carnegie Mellon encourages users of this code to contribute improvements

 * to the Coda project http://www.coda.cs.cmu.edu/ <coda@cs.cmu.edu>.

 replace or extend an acl cache hit */

 remove cached acl from an inode */

 remove all acl caches */

 check if the mask has been matched against the acl already */

 Purging dentries and children */

/* The following routines drop dentries which are not

   in use and flag dentries which are in use to be 

   zapped later.



   The flags are detected by:

   - coda_dentry_revalidate (for lookups) if the flag is C_PURGE

   - coda_dentry_delete: to remove dentry from the cache when d_count

     falls to zero

   - an inode method coda_revalidate (for attributes) if the 

     flag is C_VATTR

 this won't do any harm: just flag all children */

 don't know what to do with negative dentries */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *      	An implementation of a loadable kernel mode driver providing

 *		multiple kernel/user space bidirectional communications links.

 *

 * 		Author: 	Alan Cox <alan@lxorguk.ukuu.org.uk>

 * 

 *              Adapted to become the Linux 2.0 Coda pseudo device

 *              Peter  Braam  <braam@maths.ox.ac.uk> 

 *              Michael Callahan <mjc@emmy.smith.edu>           

 *

 *              Changes for Linux 2.1

 *              Copyright (c) 1997 Carnegie-Mellon University

 statistics */

 allows signals during upcalls */

 .. secs, then signals will dequeue */

/*

 * Device operations

/*

 *	Receive a message written by Venus to the psdev

 make sure there is enough to copy out the (opcode, unique) values */

 Peek at the opcode, uniquefier */

 what downcall errors does Venus handle ? */

 Look for the message on the processing queue. */

 move data into response buffer. */

 don't have more space! */

 adjust outsize. is this useful ?? */

 Convert filedescriptor into a file handle */

/*

 *	Read a message from the kernel to Venus

 Move the input args into userspace */

 If request was not a signal, enqueue and don't free */

 Wakeup clients so they can return. */

 Async requests need to be freed here */

 SPDX-License-Identifier: GPL-2.0

/*

 * Mostly platform independent upcall operations to Venus:

 *  -- upcalls

 *  -- upcall routines

 *

 * Linux 2.0 version

 * Copyright (C) 1996 Peter J. Braam <braam@maths.ox.ac.uk>, 

 * Michael Callahan <callahan@maths.ox.ac.uk> 

 * 

 * Redone for Linux 2.1

 * Copyright (C) 1997 Carnegie Mellon University

 *

 * Carnegie Mellon University encourages users of this code to contribute

 * improvements to the Coda project. Contact Peter Braam <coda@cs.cmu.edu>.

 the upcalls */

 send Venus a null terminated string */

 Venus must get null terminated string */

 Venus must receive an null terminated string */

 round up to word boundary */

 another null terminated string for Venus */

 round up to word boundary */

 Venus must get null terminated string */

 make sure strings are null terminated */

        inp->coda_symlink.attr = *tva; XXXXXX */ 

 Round up to word boundary and null terminate */

 Round up to word boundary and null terminate */

 build packet for Venus */

        /* the cmd field was mutated by increasing its size field to

         * reflect the path and follow args. We need to subtract that

 in->coda_ioctl.rwflag = flag; */

 get the data out of user space */

 Copy out the OUT buffer. */

 Copy out the OUT buffer. */

	/*

	 * we have to free the request buffer for synchronous upcalls

	 * or when asynchronous upcalls fail, but not when asynchronous

	 * upcalls succeed

 Chunked access is not supported or an old Coda client */

/*

 * coda_upcall and coda_downcall routines.

/* Don't allow signals to interrupt the following upcalls before venus

 * has seen them,

 * - CODA_CLOSE or CODA_RELEASE upcall  (to avoid reference count problems)

 * - CODA_STORE				(to avoid data loss)

 * - CODA_ACCESS_INTENT                 (to avoid reference count problems)

 got a reply */

/*

 * coda_upcall will return an error in the case of

 * failed communication with Venus _or_ will peek at Venus

 * reply and return Venus' error.

 *

 * As venus has 2 types of errors, normal errors (positive) and internal

 * errors (negative), normal errors are negated, while internal errors

 * are all mapped to -EINTR, while showing a nice warning message. (jh)

 Format the request message. */

 Append msg to pending queue and poke Venus. */

 We can return early on asynchronous requests */

	/* We can be interrupted while we wait for Venus to process

	 * our request.  If the interrupt occurs before Venus has read

	 * the request, we dequeue and return. If it occurs after the

	 * read but before the reply, we dequeue, send a signal

	 * message, and return. If it occurs after the reply we ignore

	 * it. In no case do we want to restart the syscall.  If it

	 * was interrupted by a venus shutdown (psdev_close), return

 Go to sleep.  Wake up on signals only after the timeout. */

 Op went through, interrupt or not... */

 here we map positive Venus errors to kernel errors */

 Interrupted before venus read it. */

 Venus saw the upcall, make sure we can send interrupt signal */

 insert at head of queue! */

/*  

    The statements below are part of the Coda opportunistic

    programming -- taken from the Mach/BSD kernel code for Coda. 

    You don't get correct semantics by stating what needs to be

    done without guaranteeing the invariants needed for it to happen.

    When will be have time to find out what exactly is going on?  (pjb)

/* 

 * There are 7 cases where cache invalidations occur.  The semantics

 *  of each is listed here:

 *

 * CODA_FLUSH     -- flush all entries from the name cache and the cnode cache.

 * CODA_PURGEUSER -- flush all entries from the name cache for a specific user

 *                  This call is a result of token expiration.

 *

 * The next arise as the result of callbacks on a file or directory.

 * CODA_ZAPFILE   -- flush the cached attributes for a file.



 * CODA_ZAPDIR    -- flush the attributes for the dir and

 *                  force a new lookup for all the children

                    of this dir.



 *

 * The next is a result of Venus detecting an inconsistent file.

 * CODA_PURGEFID  -- flush the attribute for the file

 *                  purge it and its children from the dcache

 *

 * The last  allows Venus to replace local fids with global ones

 * during reintegration.

 *

	/*

	 * Make sure we have received enough data from the cache

	 * manager to populate the necessary fields in the buffer

 Handle invalidation requests. */

 catch the dentries later if some are still busy */

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Licensed under the GPL

 *

 * Ported the filesystem routines to 2.5.

 * 2003-02-10 Petr Baudis <pasky@ucw.cz>

 Changed in hostfs_args before the kernel starts running */

	/*

	 * This function relies on the fact that dentry_path_raw() will place

	 * the path name at the end of the provided buffer.

	/*

	 * do_statfs uses struct statfs64 internally, but the linux kernel

	 * struct statfs still has 32-bit versions for most of these fields,

	 * so we convert them here

 somebody else had handled it first? */

	/*

	 * If err > 0, write_file has added err to pos, so we are comparing

	 * i_size against the last byte written.

 Reencode maj and min with the kernel encoding.*/

 NULL is printed as '(null)' by printf(): avoid that. */

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Licensed under the GPL

	/*

	 * Update accessed and/or modified time, in two parts: first set

	 * times according to the changes to perform, and then call futimes()

	 * or utimes() to apply them.

 Note: ctime is not handled */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * namei.c

/*

 * This file implements code to do filename lookup in directories.

 *

 * Like inodes, directories are packed into compressed metadata blocks, stored

 * in a directory table.  Directories are accessed using the start address of

 * the metablock containing the directory and the offset into the

 * decompressed block (<block, offset>).

 *

 * Directories are organised in a slightly complex way, and are not simply

 * a list of file names.  The organisation takes advantage of the

 * fact that (in most cases) the inodes of the files will be in the same

 * compressed metadata block, and therefore, can share the start block.

 * Directories are therefore organised in a two level list, a directory

 * header containing the shared start block value, and a sequence of directory

 * entries, each of which share the shared start block.  A new directory header

 * is written once/if the inode start block changes.  The directory

 * header/directory entry list is repeated as many times as necessary.

 *

 * Directories are sorted, and can contain a directory index to speed up

 * file lookup.  Directory indexes store one entry per metablock, each entry

 * storing the index/filename mapping to the first directory header

 * in each metadata block.  Directories are sorted in alphabetical order,

 * and at lookup the index is scanned linearly looking for the first filename

 * alphabetically larger than the filename being looked up.  At this point the

 * location of the metadata block the filename is in has been found.

 * The general idea of the index is ensure only one metadata block needs to be

 * decompressed to do a lookup irrespective of the length of the directory.

 * This scheme has the advantage that it doesn't require extra memory overhead

 * and doesn't require much extra storage on disk.

/*

 * Lookup name in the directory index, returning the location of the metadata

 * block containing it, and the directory index this represents.

 *

 * If we get an error reading the index then return the part of the index

 * (if any) we have managed to read - the index isn't essential, just

 * quicker.

	/*

	 * Return index (f_pos) of the looked up metadata block.  Translate

	 * from internal f_pos to external f_pos which is offset by 3 because

	 * we invent "." and ".." entries which are not actually stored in the

	 * directory.

		/*

		 * Read directory header.

			/*

			 * Read directory entry.

 size should never be larger than SQUASHFS_NAME_LEN */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * super.c

/*

 * This file implements code to read the superblock, read and initialise

 * in-memory structures at mount time, and all the VFS glue code to register

 * the filesystem.

	/*

	 * msblk->bytes_used is checked in squashfs_read_table to ensure reads

	 * are not beyond filesystem end.  But as we're using

	 * squashfs_read_table here to read the superblock (including the value

	 * of bytes_used) we need to set it to an initial sensible dummy value

 Check it is a SQUASHFS superblock */

 Check the MAJOR & MINOR versions and lookup compression type */

	/* Check the filesystem does not extend beyond the end of the

 Check block size for sanity */

	/*

	 * Check the system page size is not larger than the filesystem

	 * block size (by default 128K).  This is currently not supported.

 Check block log for sanity */

 Check that block_size and block_log match */

 Check the root inode for sanity */

 Allocate read_page block */

 Handle xattrs */

 Allocate and read xattr id lookup table */

 Allocate and read id index table */

 Handle inode lookup table */

 Allocate and read inode lookup table */

 Allocate and read fragment index table */

 Sanity check directory_table */

 Sanity check inode_table */

 allocate root */

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2013

 * Phillip Lougher <phillip@squashfs.org.uk>

 Read separately compressed datablock directly into page cache */

	/*

	 * Create a "page actor" which will kmap and kunmap the

	 * page cache pages appropriately within the decompressor

 Try to grab all the pages covered by the Squashfs block */

		/*

		 * Couldn't get one or more pages, this page has either

		 * been VM reclaimed, but others are still in the page cache

		 * and uptodate, or we're racing with another thread in

		 * squashfs_readpage also trying to grab them.  Fall back to

		 * using an intermediate buffer.

 Decompress directly into the page cache buffers */

 Last page may have trailing bytes not filled */

 Mark pages as uptodate, unlock and release */

	/* Decompression failed, mark pages as errored.  Target_page is

	 * dealt with by the caller

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2013

 * Phillip Lougher <phillip@squashfs.org.uk>

 Read separately compressed datablock and memcopy into page cache */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * xz_wrapper.c

 check compressor options are the expected length */

 the dictionary size should be 2^n or 2^n+2^(n+1) */

 use defaults */

 XZ_STREAM_END must be reached. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2013

 * Phillip Lougher <phillip@squashfs.org.uk>

/*

 * This file contains implementations of page_actor for decompressing into

 * an intermediate buffer, and for decompressing directly into the

 * page cache.

 *

 * Calling code should avoid sleeping between calls to squashfs_first_page()

 * and squashfs_finish_page().

 Implementation of page_actor for decompressing into intermediate buffer */

 empty */

 Implementation of page_actor for decompressing directly into page cache. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2010

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * xattr.c

 check that the file system has xattrs */

 loop reading each xattr name */

 no handler or insuffficient privileges, so skip */

 skip remaining xattr entry */

 loop reading each xattr name */

 found xattr */

 val is a reference to the real location */

 read xattr value */

 no match, skip remaining xattr entry */

/*

 * User namespace support

/*

 * Trusted namespace support

/*

 * Security namespace support

 ignore unrecognised type */

 ignore unrecognised type */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * fragment.c

/*

 * This file implements code to handle compressed fragments (tail-end packed

 * datablocks).

 *

 * Regular files contain a fragment index which is mapped to a fragment

 * location on disk and compressed size using a fragment lookup table.

 * Like everything in Squashfs this fragment lookup table is itself stored

 * compressed into metadata blocks.  A second index table is used to locate

 * these.  This second index table for speed of access (and because it

 * is small) is read at mount time and cached in memory.

/*

 * Look-up fragment using the fragment index table.  Return the on disk

 * location of the fragment and its compressed size

/*

 * Read the uncompressed fragment lookup table indexes off disk into memory

	/*

	 * Sanity check, length bytes should not extend into the next table -

	 * this check also traps instances where fragment_table_start is

	 * incorrectly larger than the next table start

	/*

	 * table[0] points to the first fragment table metadata block, this

	 * should be less than fragment_table_start

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * inode.c

/*

 * This file implements code to create and read inodes from disk.

 *

 * Inodes in Squashfs are identified by a 48-bit inode which encodes the

 * location of the compressed metadata block containing the inode, and the byte

 * offset into that block where the inode is placed (<block, offset>).

 *

 * To maximise compression there are different inodes for each file type

 * (regular file, directory, device, etc.), the inode contents and length

 * varying with the type.

 *

 * To further maximise compression, two types of regular file inode and

 * directory inode are defined: inodes optimised for frequently occurring

 * regular files and directories, and extended types where extra

 * information has to be stored.

/*

 * Initialise VFS inode with the base inode information common to all

 * Squashfs inode types.  Sqsh_ino contains the unswapped base inode

 * off disk.

/*

 * Initialise VFS inode by reading inode from inode table (compressed

 * metadata).  The format and amount of data read depends on type.

	/*

	 * Read inode base common to all inode types.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2013

 * Phillip Lougher <phillip@squashfs.org.uk>

/*

 * This file implements multi-threaded decompression using percpu

 * variables, one thread per cpu core.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * zlib_wrapper.c

 Z_STREAM_END must be reached. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (c) 2013

 *  Minchan Kim <minchan@kernel.org>

/*

 * This file implements multi-threaded decompression in the

 * decompressor framework

/*

 * The reason that multiply two is that a CPU can request new I/O

 * while it is waiting previous request.

	/*

	 * We should have a decompressor at least as default

	 * so if we fail to allocate new decompressor dynamically,

	 * we could always fall back to default decompressor and

	 * file system works.

 There is available decomp_stream */

		/*

		 * If there is no available decomp and already full,

		 * let's wait for releasing decomp from other users.

 Let's allocate new decomp */

		/*

		 * If system memory is tough, let's for other's

		 * releasing instead of hurting VM because it could

		 * make page cache thrashing.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * dir.c

/*

 * This file implements code to read directories from disk.

 *

 * See namei.c for a description of directory organisation on disk.

/*

 * Lookup offset (f_pos) in the directory index, returning the

 * metadata block containing it.

 *

 * If we get an error reading the index then return the part of the index

 * (if any) we have managed to read - the index isn't essential, just

 * quicker.

	/*

	 * Translate from external f_pos to the internal f_pos.  This

	 * is offset by 3 because we invent "." and ".." entries which are

	 * not actually stored in the directory.

			/*

			 * Found the index we're looking for.

 size should never be larger than SQUASHFS_NAME_LEN */

	/*

	 * Translate back from internal f_pos to external f_pos.

	/*

	 * Return "." and  ".." entries as the first two filenames in the

	 * directory.  To maximise compression these two entries are not

	 * stored in the directory, and so we invent them here.

	 *

	 * It also means that the external f_pos is offset by 3 from the

	 * on-disk directory f_pos.

		/*

		 * Read directory header

			/*

			 * Read directory entry.

 size should never be larger than SQUASHFS_NAME_LEN */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * block.c

/*

 * This file implements the low-level routines to read and decompress

 * datablocks and metadata blocks.

/*

 * Returns the amount of bytes copied to the page actor.

/*

 * Read and decompress a metadata block or datablock.  Length is non-zero

 * if a datablock is being read (the size is stored elsewhere in the

 * filesystem), otherwise the length is obtained from the first two bytes of

 * the metadata block.  A bit in the length field indicates if the block

 * is stored uncompressed in the filesystem (usually because compression

 * generated a larger block - this does occasionally happen with compression

 * algorithms).

		/*

		 * Datablock.

		/*

		 * Metadata block.

 Extract the length of the metadata block */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2013

 * Phillip Lougher <phillip@squashfs.org.uk>

/*

 * This file implements single-threaded decompression in the

 * decompressor framework

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * file.c

/*

 * This file contains code for handling regular files.  A regular file

 * consists of a sequence of contiguous compressed blocks, and/or a

 * compressed fragment block (tail-end packed block).   The compressed size

 * of each datablock is stored in a block list contained within the

 * file inode (itself stored in one or more compressed metadata blocks).

 *

 * To speed up access to datablocks when reading 'large' files (256 Mbytes or

 * larger), the code implements an index cache that caches the mapping from

 * block index to datablock location on disk.

 *

 * The index cache allows Squashfs to handle large files (up to 1.75 TiB) while

 * retaining a simple and space-efficient block list on disk.  The cache

 * is split into slots, caching up to eight 224 GiB files (128 KiB blocks).

 * Larger files use multiple slots, with 1.75 TiB files using all 8 slots.

 * The index cache is designed to be memory efficient, and by default uses

 * 16 KiB.

/*

 * Locate cache slot in range [offset, index] for specified inode.  If

 * there's more than one return the slot closest to index.

/*

 * Find and initialise an empty cache slot for index offset.

		/*

		 * First time cache index has been used, allocate and

		 * initialise.  The cache index could be allocated at

		 * mount time but doing it here means it is allocated only

		 * if a 'large' file is read.

/*

 * Read the next n blocks from the block list, starting from

 * metadata block <start_block, offset>.

/*

 * Each cache index slot has SQUASHFS_META_ENTRIES, each of which

 * can cache one index -> datablock/blocklist-block mapping.  We wish

 * to distribute these over the length of the file, entry[0] maps index x,

 * entry[1] maps index x + skip, entry[2] maps index x + 2 * skip, and so on.

 * The larger the file, the greater the skip factor.  The skip factor is

 * limited to the size of the metadata cache (SQUASHFS_CACHED_BLKS) to ensure

 * the number of metadata blocks that need to be read fits into the cache.

 * If the skip factor is limited in this way then the file will use multiple

 * slots.

/*

 * Search and grow the index cache for the specified inode, returning the

 * on-disk locations of the datablock and block list metadata block

 * <index_block, index_offset> for index (scaled to nearest cache index).

	/*

	 * Scale index to cache index (cache slot entry)

		/*

		 * If necessary grow cache slot by reading block list.  Cache

		 * slot is extended up to index or to the end of the slot, in

		 * which case further slots will be used.

					/*

					 * Don't leave an empty slot on read

					 * error allocated to this inode...

	/*

	 * Scale cache index (cache slot entry) to index

/*

 * Get the on-disk location and compressed size of the datablock

 * specified by index.  Fill_meta_index() does most of the work.

	/*

	 * res contains the index of the mapping returned by fill_meta_index(),

	 * this will likely be less than the desired index (because the

	 * meta_index cache works at a higher granularity).  Read any

	 * extra block indexes needed.

	/*

	 * Read length of block specified by index.

 Copy data into page cache  */

	/*

	 * Loop copying datablock into pages.  As the datablock likely covers

	 * many PAGE_SIZE pages (default block size is 128 KiB) explicitly

	 * grab the pages from the page cache, except for the page that we've

	 * been called to fill.

 Read datablock stored packed inside a fragment (tail-end packed block) */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * symlink.c

/*

 * This file implements code to handle symbolic links.

 *

 * The data contents of symbolic links are stored inside the symbolic

 * link inode within the inode table.  This allows the normally small symbolic

 * link to be compressed as part of the inode table, achieving much greater

 * compression than if the symbolic link was compressed individually.

	/*

	 * Skip index bytes into symlink metadata.

	/*

	 * Read length bytes from symlink metadata.  Squashfs_read_metadata

	 * is not used here because it can sleep and we want to use

	 * kmap_atomic to map the page.  Instead call the underlying

	 * squashfs_cache_get routine.  As length bytes may overlap metadata

	 * blocks, we may need to call squashfs_cache_get multiple times.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * cache.c

/*

 * Blocks in Squashfs are compressed.  To avoid repeatedly decompressing

 * recently accessed data Squashfs uses two small metadata and fragment caches.

 *

 * This file implements a generic cache implementation used for both caches,

 * plus functions layered ontop of the generic cache implementation to

 * access the metadata and fragment caches.

 *

 * To avoid out of memory and fragmentation issues with vmalloc the cache

 * uses sequences of kmalloced PAGE_SIZE buffers.

 *

 * It should be noted that the cache is not used for file datablocks, these

 * are decompressed and cached in the page-cache in the normal way.  The

 * cache is only used to temporarily cache fragment and metadata blocks

 * which have been read as as a result of a metadata (i.e. inode or

 * directory) or fragment access.  Because metadata and fragments are packed

 * together into blocks (to gain greater compression) the read of a particular

 * piece of metadata or fragment will retrieve other metadata/fragments which

 * have been packed with it, these because of locality-of-reference may be read

 * in the near future. Temporarily caching them ensures they are available for

 * near future access without requiring an additional read and decompress.

/*

 * Look-up block in cache, and increment usage count.  If not in cache, read

 * and decompress it from disk.

			/*

			 * Block not in cache, if all cache entries are used

			 * go to sleep waiting for one to become available.

			/*

			 * At least one unused cache entry.  A simple

			 * round-robin strategy is used to choose the entry to

			 * be evicted from the cache.

			/*

			 * Initialise chosen cache entry, and fill it in from

			 * disk.

			/*

			 * While filling this entry one or more other processes

			 * have looked it up in the cache, and have slept

			 * waiting for it to become available.

		/*

		 * Block already in cache.  Increment refcount so it doesn't

		 * get reused until we're finished with it, if it was

		 * previously unused there's one less cache entry available

		 * for reuse.

		/*

		 * If the entry is currently being filled in by another process

		 * go to sleep waiting for it to become available.

/*

 * Release cache entry, once usage count is zero it can be reused.

		/*

		 * If there's any processes waiting for a block to become

		 * available, wake one up.

/*

 * Delete cache reclaiming all kmalloced buffers.

/*

 * Initialise cache allocating the specified number of entries, each of

 * size block_size.  To avoid vmalloc fragmentation issues each entry

 * is allocated as a sequence of kmalloced PAGE_SIZE buffers.

/*

 * Copy up to length bytes from cache entry to buffer starting at offset bytes

 * into the cache entry.  If there's not length bytes then copy the number of

 * bytes available.  In all cases return the number of bytes copied.

/*

 * Read length bytes from metadata position <block, offset> (block is the

 * start of the compressed block on disk, and offset is the offset into

 * the block once decompressed).  Data is packed into consecutive blocks,

 * and length bytes may require reading more than one block.

/*

 * Look-up in the fragmment cache the fragment located at <start_block> in the

 * filesystem.  If necessary read and decompress it from disk.

/*

 * Read and decompress the datablock located at <start_block> in the

 * filesystem.  The cache is used here to avoid duplicating locking and

 * read/decompress code.

/*

 * Read a filesystem table (uncompressed sequence of bytes) from disk

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2013, 2014

 * Phillip Lougher <phillip@squashfs.org.uk>

 LZ4 compressed filesystems always have compression options */

		/* LZ4 format currently used by the kernel is the 'legacy'

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2016-present, Facebook, Inc.

 * All rights reserved.

 *

 * zstd_wrapper.c

				/* Shouldn't run out of pages

				 * before stream is done.

 add the additional data produced */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * export.c

/*

 * This file implements code to make Squashfs filesystems exportable (NFS etc.)

 *

 * The export code uses an inode lookup table to map inode numbers passed in

 * filehandles to an inode location on disk.  This table is stored compressed

 * into metadata blocks.  A second index table is used to locate these.  This

 * second index table for speed of access (and because it is small) is read at

 * mount time and cached in memory.

 *

 * The inode lookup table is used only by the export code, inode disk

 * locations are directly encoded in directories, enabling direct access

 * without an intermediate lookup for all operations except the export ops.

/*

 * Look-up inode number (ino) in table, returning the inode location.

/*

 * Read uncompressed inode lookup table indexes off disk into memory

 Sanity check values */

 there should always be at least one inode */

	/*

	 * The computed size of the lookup table (length bytes) should exactly

	 * match the table start and end points

	/*

	 * table0], table[1], ... table[indexes - 1] store the locations

	 * of the compressed inode lookup blocks.  Each entry should be

	 * less than the next (i.e. table[0] < table[1]), and the difference

	 * between them should be SQUASHFS_METADATA_SIZE or less.

	 * table[indexes - 1] should  be less than lookup_table_start, and

	 * again the difference should be SQUASHFS_METADATA_SIZE or less

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * decompressor.c

/*

 * This file (and decompressor.h) implements a decompressor framework for

 * Squashfs, allowing multiple decompressors to be easily supported

	/*

	 * Read decompressor specific options from file system if present

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2010 LG Electronics

 * Chan Jeong <chan.jeong@lge.com>

 *

 * lzo_wrapper.c

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2002, 2003, 2004, 2005, 2006, 2007, 2008

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * id.c

/*

 * This file implements code to handle uids and gids.

 *

 * For space efficiency regular files store uid and gid indexes, which are

 * converted to 32-bit uids/gids using an id look up table.  This table is

 * stored compressed into metadata blocks.  A second index table is used to

 * locate these.  This second index table for speed of access (and because it

 * is small) is read at mount time and cached in memory.

/*

 * Map uid/gid index into real 32-bit uid/gid using the id look up table

/*

 * Read uncompressed id lookup table indexes from disk into memory

 Sanity check values */

 there should always be at least one id */

	/*

	 * The computed size of the index table (length bytes) should exactly

	 * match the table start and end points

	/*

	 * table[0], table[1], ... table[indexes - 1] store the locations

	 * of the compressed id blocks.   Each entry should be less than

	 * the next (i.e. table[0] < table[1]), and the difference between them

	 * should be SQUASHFS_METADATA_SIZE or less.  table[indexes - 1]

	 * should be less than id_table_start, and again the difference

	 * should be SQUASHFS_METADATA_SIZE or less

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Squashfs - a compressed read only filesystem for Linux

 *

 * Copyright (c) 2010

 * Phillip Lougher <phillip@squashfs.org.uk>

 *

 * xattr_id.c

/*

 * This file implements code to map the 32-bit xattr id stored in the inode

 * into the on disk location of the xattr data.

/*

 * Map xattr id using the xattr id look up table

/*

 * Read uncompressed xattr id lookup table indexes from disk into memory

 Sanity check values */

 there is always at least one xattr id */

	/*

	 * The computed size of the index table (len bytes) should exactly

	 * match the table start and end points

	/* table[0], table[1], ... table[indexes - 1] store the locations

	 * of the compressed xattr id blocks.  Each entry should be less than

	 * the next (i.e. table[0] < table[1]), and the difference between them

	 * should be SQUASHFS_METADATA_SIZE or less.  table[indexes - 1]

	 * should be less than table_start, and again the difference

	 * shouls be SQUASHFS_METADATA_SIZE or less.

	 *

	 * Finally xattr_table_start should be less than table[0].

 SPDX-License-Identifier: GPL-2.0

/*

 * namei.c

 *

 * Copyright (c) 1999 Al Smith

 *

 * Portions derived from work (c) 1995,1996 Christian Vogelgsang.

 SPDX-License-Identifier: GPL-2.0

/*

 * super.c

 *

 * Copyright (c) 1999 Al Smith

 *

 * Portions derived from work (c) 1995,1996 Christian Vogelgsang.

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

aeschi.ch.eu.org/efs/\n");

 shuts up gcc */

		/*

		 * assume that we're dealing with a partition and allow

		 * read_super() to try and detect a valid superblock

		 * on the next block.

 read the vh (volume header) block */

	/*

	 * if this returns zero then we didn't find any partition table.

	 * this isn't (yet) an error - just assume for the moment that

	 * the device is valid and go on to search for a superblock.

 efs magic number */

 blocksize */

 total data blocks */

 free data blocks */

 free blocks for non-root */

 total inodes */

 free inodes */

 max filename length */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * inode.c

 *

 * Copyright (c) 1999 Al Smith

 *

 * Portions derived from work (c) 1995,1996 Christian Vogelgsang,

 *              and from work (c) 1998 Mike Shaver.

	/*

	 * this is slightly evil. it doesn't just copy

	 * efs_extent from src to dst, it also mangles

	 * the bits so that dst ends up in cpu byte-order.

	/*

	** EFS layout:

	**

	** |   cylinder group    |   cylinder group    |   cylinder group ..etc

	** |inodes|data          |inodes|data          |inodes|data       ..etc

	**

	** work out the inode block index, (considering initially that the

	** inodes are stored as consecutive blocks). then work out the block

	** number of that inode given the above layout, and finally the

	** offset of the inode within that block.

 this is the number of blocks in the file */

 get the number of extents for this object */

 copy the extents contained within the inode to memory */

	/*

	 * given an extent and a logical block within a file,

	 * can this block be found within this extent ?

 first check the last extent we returned */

 if we only have one extent then nothing can be found */

		/*

		 * check the stored extents in the inode

		 * start with next extent and check forwards

		/*

		 * work out which direct extent contains `cur'.

		 *

		 * also compute ibase: i.e. the number of the first

		 * indirect extent contained within direct extent `cur'.

		 *

 should never happen */

 work out block number and offset of this indirect extent */

 SPDX-License-Identifier: GPL-2.0

/*

 * dir.c

 *

 * Copyright (c) 1999 Al Smith

 work out where this entry can be found */

 each block contains at most 256 slots */

 look at all blocks */

 read the dir block */

 found the next entry */

 sanity check */

 copy filename and data in dirslot */

 SPDX-License-Identifier: GPL-2.0

/*

 * file.c

 *

 * Copyright (c) 1999 Al Smith

 *

 * Portions derived from work (c) 1995,1996 Christian Vogelgsang.

		/*

		 * i have no idea why this happens as often as it does

 are we about to read past the end of a file ? */

		/*

		 * i have no idea why this happens as often as it does

 SPDX-License-Identifier: GPL-2.0

/*

 * symlink.c

 *

 * Copyright (c) 1999 Al Smith

 *

 * Portions derived from work (c) 1995,1996 Christian Vogelgsang.

 read first 512 bytes of link target */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2006  NEC Corporation

 *

 * Created by KaiGai Kohei <kaigai@ak.jp.nec.com>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2006  NEC Corporation

 *

 * Created by KaiGai Kohei <kaigai@ak.jp.nec.com>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

	/* Later, this will provide for lsattr.jffs2 and chattr.jffs2, which

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 Duplicate. Free one */

				/* It may have been a 'placeholder' deletion dirent, 

 We know frag->ofs <= size. That's what lookup does for us */

 Sanity check for truncation to longer than we started with... */

	/* If the last fragment starts at the RAM page boundary, it is

 The node has no valid frags left. It's totally obsoleted */

/*

 * Allocate and initializes a new fragment.

/*

 * Called when there is no overlapping fragment exist. Inserts a hole before the new

 * fragment and inserts the new fragment to the fragtree.

 put a hole in before the new fragment */

			/* By definition, the 'this' node has no right-hand child,

			   because there are no frags with offset greater than it.

		/* By definition, the 'this' node has no right-hand child,

		   because there are no frags with offset greater than it.

 Doesn't set inode->i_size */

 Skip all the nodes which are completed before this one starts */

 See if we ran off the end of the fragtree */

 We did */

		/* Check if 'this' node was on the same page as the new node.

		   If so, both 'this' and the new node get marked REF_NORMAL so

		   the GC can take a look.

	/* OK. 'this' is pointing at the first frag that newfrag->ofs at least partially obsoletes,

	 * - i.e. newfrag->ofs < this->ofs+this->size && newfrag->ofs >= this->ofs

 This node isn't completely obsoleted. The start of it remains valid */

		/* Mark the new node and the partially covered node REF_NORMAL -- let

 The new node splits 'this' frag into two */

 New second frag pointing to this's node */

 Adjust size of original 'this' */

			/* Now, we know there's no node with offset

			   greater than this->ofs but smaller than

			   newfrag2->ofs or newfrag->ofs, for obvious

			   reasons. So we can do a tree insert from

			   'this' to insert newfrag, and a tree insert

 New node just reduces 'this' frag in size, doesn't split it */

 Again, we know it lives down here in the tree */

		/* New frag starts at the same point as 'this' used to. Replace

	/* OK, now we have newfrag added in the correct place in the tree, but

	   frag_next(newfrag) may be a fragment which is overlapped by it

 'this' frag is obsoleted completely. */

	/* Now we're pointing at the first frag which isn't totally obsoleted by

 Still some overlap but we don't need to move it in the tree */

 And mark them REF_NORMAL so the GC takes a look at them */

/*

 * Given an inode, probably with existing tree of fragments, add the new node

 * to the fragment tree.

	/* If we now share a page with other nodes, mark either previous

 If we don't start at zero there's _always_ a previous */

/* During mount, this needs no locking. During normal operation, its

   callers want to do other stuff while still holding the inocache_lock.

   Rather than introducing special case get_ino_cache functions or

	/* Free it now unless it's in READING or CLEARING state, which

	   are the transitions upon read_inode() and clear_inode(). The

	   rest of the time we know nobody else is looking at it, and

	   if it's held by read_inode() or clear_inode() they'll free it

	/* The common case in lookup is that there will be a node

 Remember the closest smaller match on the way down */

	/* Exact match not found. Go back up looking at each parent,

/* Pass 'c' argument to indicate that nodes should be marked obsolete as

			/* Not a hole, and it's the final remaining frag

 Set (and test) __totlen field... for now */

 No locking, no reservation of 'ref'. Do not use on a live file system */

 REF_EMPTY_NODE is !obsolete, so that works OK */

 Calculate totlen from surrounding nodes or eraseblock */

 Last node in block. Use free_space */

 TEST_TOTLEN */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 Linux */

 __ECOS */

 Erase failed immediately. Refile it on the list */

 Be nice */

 Wake the GC thread to mark them clean */

	/* For NAND, if the failure did not occur at the device level for a

		/* We had a device-level failure to erase.  Let's see if we've

 We'd like to give this block another try. */

/* Hmmm. Maybe we should accept the extra space it takes and make

 Walk the inode's list once, removing any nodes from this eraseblock */

			/* We're looking at the jffs2_inode_cache, which is

			   at the end of the linked list. Stash it and continue

 It's in the block we're erasing */

 Not to be deleted. Skip */

 PARANOIA */

 else it was a non-inode node or already removed, so don't bother */

 Don't muck about if it won't let us point to the whole erase sector */

 It's OK. We know it's properly aligned */

 Write the erase complete marker */

 Cleanmarker in oob area or no cleanmarker at all ? */

 Everything else got zeroed before the erase */

 Account for cleanmarker now, if it's in-band */

 Stick it back on the list from whence it came and come back later */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 should never happen; programmer error */

	/* We don't care about i_generation. We'll destroy the flash

	   before we start re-using inode numbers anyway. And even

/*

 * JFFS2 mount options.

 *

 * Opt_source: The source device

 * Opt_override_compr: override default compressor

 * Opt_rp_size: size of reserved pool in KiB

/*

 * fill in the superblock

	/* Initialize JFFS2 superblock locks, the further initialization will

	/* Paranoia checks for on-medium structures. If we ask GCC

	   to pack them with __attribute__((packed)) then it _also_

	   assumes that they're not aligned -- so it emits crappy

	   code on some architectures. Ideally we want an attribute

	   which means just 'no padding', without the alignment

	   thing. But GCC doesn't have that -- we have to just

	/*

	 * Make sure all delayed rcu free inodes are flushed before we

	 * destroy cache.

 Actually dual-licensed, but it doesn't matter for

 the sake of this tag. It's Free Software.

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 Will be overwritten shortly for directories */

/* jffs2_write_dnode - given a raw_inode, allocate a full_dnode for it,

 check number of valid vecs */

 Mark the space as dirtied */

			/* Don't change raw->size to match retlen. We may have

			   written the node header already, and only the data will

			   seem corrupted, in which case the scan would skip over

			   any node we write before the original intended end of

 Try to reallocate space and retry */

 Locking pain */

 Release the full_dnode which is now useless, and return */

 Mark the space used */

	/* If node covers at least a whole page, or if it starts at the

	   beginning of a page and runs to the end of the file, or if

	   it's a hole node, mark it REF_PRISTINE, else REF_NORMAL.

 Release the full_dnode which is now useless, and return */

		/* This should never happen, but seems to have done on at least one

dev.laptop.org/ticket/4184 */

 Mark the space as dirtied */

 Try to reallocate space and retry */

 Locking pain */

 Release the full_dnode which is now useless, and return */

 Mark the space used */

 Release the full_dirent which is now useless, and return */

/* The OS-specific code fills in the metadata in the jffs2_raw_inode for us, so that

   we don't have to go digging in struct inode or its equivalent. It should set:

 Write error to be retried */

 Eep */

	/* Try to reserve enough space for both node and dirent.

	 * Just the node will do for now, though

 Eeek. Wave bye bye */

	/* No data here. Only a metadata node, which will be

	   obsoleted by the first data write

 Eep. */

 Argh. Now we treat it like a normal delete */

		/* dirent failed to write. Delete the inode normally

	/* Link the fd into the inode's list, obsoleting an old

 We can't mark stuff obsolete on the medium. We need to write a deletion dirent */

 Build a deletion node */

 File it. This will mark the old one obsolete. */

		/* We don't actually want to reserve any space, but we do

				/* We don't want to remove it from the list immediately,

				   because that screws up getdents()/seek() semantics even

				   more than they're screwed already. Turn it into a

 dead_f is NULL if this was a rename not a real unlink */

	/* Also catch the !f->inocache case, where there was a dirent

 There can be only deleted ones */

 NB: Caller must set inode nlink if appropriate */

 Build a deletion node */

 File it. This will mark the old one obsolete. */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2006  NEC Corporation

 *

 * Created by KaiGai Kohei <kaigai@ak.jp.nec.com>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

/* -------- xdatum related functions ----------------

 * xattr_datum_hashkey(xprefix, xname, xvalue, xsize)

 *   is used to calcurate xdatum hashkey. The reminder of hashkey into XATTRINDEX_HASHSIZE is

 *   the index of the xattr name/value pair cache (c->xattrindex).

 * is_xattr_datum_unchecked(c, xd)

 *   returns 1, if xdatum contains any unchecked raw nodes. if all raw nodes are not

 *   unchecked, it returns 0.

 * unload_xattr_datum(c, xd)

 *   is used to release xattr name/value pair and detach from c->xattrindex.

 * reclaim_xattr_datum(c)

 *   is used to reclaim xattr name/value pairs on the xattr name/value pair cache when

 *   memory usage by cache is over c->xdatum_mem_threshold. Currently, this threshold

 *   is hard coded as 32KiB.

 * do_verify_xattr_datum(c, xd)

 *   is used to load the xdatum informations without name/value pair from the medium.

 *   It's necessary once, because those informations are not collected during mounting

 *   process when EBS is enabled.

 *   0 will be returned, if success. An negative return value means recoverable error, and

 *   positive return value means unrecoverable error. Thus, caller must remove this xdatum

 *   and xref when it returned positive value.

 * do_load_xattr_datum(c, xd)

 *   is used to load name/value pair from the medium.

 *   The meanings of return value is same as do_verify_xattr_datum().

 * load_xattr_datum(c, xd)

 *   is used to be as a wrapper of do_verify_xattr_datum() and do_load_xattr_datum().

 *   If xd need to call do_verify_xattr_datum() at first, it's called before calling

 *   do_load_xattr_datum(). The meanings of return value is same as do_verify_xattr_datum().

 * save_xattr_datum(c, xd)

 *   is used to write xdatum to medium. xd->version will be incremented.

 * create_xattr_datum(c, xprefix, xname, xvalue, xsize)

 *   is used to create new xdatum and write to medium.

 * unrefer_xattr_datum(c, xd)

 *   is used to delete a xdatum. When nobody refers this xdatum, JFFS2_XFLAGS_DEAD

 *   is set on xd->flags and chained xattr_dead_list or release it immediately.

 *   In the first case, the garbage collector release it later.

 must be called under down_write(xattr_sem) */

 must be called under down_write(xattr_sem) */

 20% reduction */

 must be called under down_write(xattr_sem) */

 unchecked xdatum is chained with c->xattr_unchecked */

 must be called under down_write(xattr_sem) */

	/* must be called under down_write(xattr_sem);

	 * rc < 0 : recoverable error, try again

	 * rc = 0 : success

	 * rc > 0 : Unrecoverable error, this node should be deleted.

 must be called under down_write(xattr_sem) */

 Setup raw-xattr */

 success */

 must be called under down_write(xattr_sem) */

 Search xattr_datum has same xname/xvalue by index */

 Not found, Create NEW XATTR-Cache */

 Insert Hash Index */

 must be called under down_write(xattr_sem) */

/* -------- xref related functions ------------------

 * verify_xattr_ref(c, ref)

 *   is used to load xref information from medium. Because summary data does not

 *   contain xid/ino, it's necessary to verify once while mounting process.

 * save_xattr_ref(c, ref)

 *   is used to write xref to medium. If delete marker is marked, it write

 *   a delete marker of xref into medium.

 * create_xattr_ref(c, ic, xd)

 *   is used to create a new xref and write to medium.

 * delete_xattr_ref(c, ref)

 *   is used to delete jffs2_xattr_ref. It marks xref XREF_DELETE_MARKER,

 *   and allows GC to reclaim those physical nodes.

 * jffs2_xattr_delete_inode(c, ic)

 *   is called to remove xrefs related to obsolete inode when inode is unlinked.

 * jffs2_xattr_free_inode(c, ic)

 *   is called to release xattr related objects when unmounting. 

 * check_xattr_ref_inode(c, ic)

 *   is used to confirm inode does not have duplicate xattr name/value pair.

 * jffs2_xattr_do_crccheck_inode(c, ic)

 *   is used to force xattr data integrity check during the initial gc scan.

 obsolete node */

 must be called under down_write(xattr_sem) */

 success */

 must be called under down_write(xattr_sem) */

 Chain to inode */

 success */

 must be called under down_write(xattr_sem) */

	/* It's called from jffs2_evict_inode() on inode removing.

 It's called from jffs2_free_ino_caches() until unmounting FS. */

	/* success of check_xattr_ref_inode() means that inode (ic) dose not have

	 * duplicate name/value pairs. If duplicate name/value pair would be found,

	 * one will be removed.

/* -------- xattr subsystem functions ---------------

 * jffs2_init_xattr_subsystem(c)

 *   is used to initialize semaphore and list_head, and some variables.

 * jffs2_find_xattr_datum(c, xid)

 *   is used to lookup xdatum while scanning process.

 * jffs2_clear_xattr_subsystem(c)

 *   is used to release any xattr related objects.

 * jffs2_build_xattr_subsystem(c)

 *   is used to associate xdatum and xref while super block building process.

 * jffs2_setup_xattr_datum(c, xid, version)

 *   is used to insert xdatum while scanning process.

 Default 32KB */

 It's only used in scanning/building process. */

 Phase.1 : Merge same xref */

 Phase.2 : Bind xref with inode_cache and xattr_datum */

			/* At this point, ref->xid and ref->ino contain XID and inode number.

 Phase.3 : Link unchecked xdatum to xattr_unchecked list */

 build complete */

/* -------- xattr subsystem functions ---------------

 * xprefix_to_handler(xprefix)

 *   is used to translate xprefix into xattr_handler.

 * jffs2_listxattr(dentry, buffer, size)

 *   is an implementation of listxattr handler on jffs2.

 * do_jffs2_getxattr(inode, xprefix, xname, buffer, size)

 *   is an implementation of getxattr handler on jffs2.

 * do_jffs2_setxattr(inode, xprefix, xname, buffer, size, flags)

 *   is an implementation of setxattr handler on jffs2.

 xdatum is unchached */

 xdatum is unchached */

 Find existing xattr */

 not found */

 create xattr_ref */

/* -------- garbage collector functions -------------

 * jffs2_garbage_collect_xattr_datum(c, xd, raw)

 *   is used to move xdatum into new node.

 * jffs2_garbage_collect_xattr_ref(c, ref, raw)

 *   is used to move xref into new node.

 * jffs2_verify_xattr(c)

 *   is used to call do_verify_xattr_datum() before garbage collecting.

 * jffs2_release_xattr_datum(c, xd)

 *   is used to release an in-memory object of xdatum.

 * jffs2_release_xattr_ref(c, ref)

 *   is used to release an in-memory object of xref.

 must be called under spin_lock(&c->erase_completion_lock) */

 must be called under spin_lock(&c->erase_completion_lock) */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 Called with erase_completion_lock held */

	/* Pick an eraseblock to garbage collect next. This is where we'll

	/* We possibly want to favour the dirtier blocks more when the

		/* Note that most of them will have gone directly to be erased.

 Most of the time, pick one off the very_dirty list */

 There are blocks are wating for the wbuf sync */

 Eep. All were empty */

 Have we accidentally picked a clean block with wasted space ? */

/* jffs2_garbage_collect_pass

 * Make a single attempt to progress GC. Move one node, and possibly

 * start erasing one eraseblock.

		/* We can't start doing GC until we've finished checking

		/* Instead of doing the inodes in numeric order, doing a lookup

		 * in the hash for each possible number, just walk the hash

		 * buckets of *existing* inodes. This means that we process

		 * them out-of-order, but it can be a lot faster if there's

 with inocache_lock held */

 Point c->check_ino past the end of the last bucket. */

		/* For next time round the loop, we want c->checked_ino to indicate

		 * the *next* one we want to check. And since we're walking the

			/* We need to wait for it to finish, lest we move on

			   and trigger the BUG() above while we haven't yet

			/* We need to come back again for the _same_ inode. We've

 If there are any blocks which need erasing, erase them now */

 First, work out which block we're garbage-collecting */

 Couldn't find a free block. But maybe we can just erase one and make 'progress'? */

 Inode-less node. Clean marker, snapshot or something like that */

 It's an unknown node with JFFS2_FEATURE_RWCOMPAT_COPY */

 Just mark it obsolete */

	/* When 'ic' refers xattr_datum/xattr_ref, this node is GCed as xattr.

	/* We need to hold the inocache. Either the erase_completion_lock or

	   the inocache_lock are sufficient; we trade down since the inocache_lock

	/* Three possibilities:

	   1. Inode is already in-core. We must iget it and do proper

	      updating to its fragtree, etc.

	   2. Inode is not in-core, node is REF_PRISTINE. We lock the

	      inocache to prevent a read_inode(), copy the node intact.

	   3. Inode is not in-core, node is not pristine. We must iget()

	      and take the slow path.

		/* It's been checked, but it's not currently in-core.

		   We can just copy any pristine nodes, but have

		   to prevent anyone else from doing read_inode() while

 It's in-core. GC must iget() it. */

		/* Should never happen. We should have finished checking

		   by the time we actually start doing any GC, and since

		   we're holding the alloc_sem, no other garbage collection

		   can happen.

		/* Someone's currently trying to read it. We must wait for

		   them to finish and then go through the full iget() route

		   to do the GC. However, sometimes read_inode() needs to get

		   the alloc_sem() (for marking nodes invalid) so we must

		/* And because we dropped the alloc_sem we must start again from the

		   beginning. Ponder chance of livelock here -- we're returning success

		   without actually making any progress.



		   Q: What are the chances that the inode is back in INO_STATE_READING

		   again by the time we next enter this function? And that this happens

		   enough times to cause a real delay?



		   A: Small enough that I don't care :)

	/* OK. Now if the inode is in state INO_STATE_GC, we are going to copy the

	   node intact, and we don't have to muck about with the fragtree etc.

	   because we know it's not in-core. If it _was_ in-core, we go through

 Fall through if it wanted us to, with inocache_lock held */

	/* Prevent the fairly unlikely race where the gcblock is

	   entirely obsoleted by the final close of a file which had

	   the only valid nodes in the block, followed by erasure,

	   followed by freeing of the ic because the erased block(s)

	   held _all_ the nodes of that inode.... never been seen but

 Eep. This really should never happen. GC is broken */

 If we've finished this block, start it erasing */

 We're GC'ing an empty block? */

	/* Now we have the lock for this inode. Check that it's still the one at the head

 They'll call again */

 OK. Looks safe. And nobody can get us now because we have the semaphore. Move the block */

 FIXME. Read node and do lookup? */

 We've found them all */

 Urgh. Return it sensibly. */

 We found a datanode. Do the GC */

 It crosses a page boundary. Therefore, it must be a hole. */

 It could still be a hole. But we GC the page this way anyway */

 Wasn't a dnode. Try dirent */

	/* Ask for a small amount of space (or the totlen if smaller) because we

	   don't want to force wastage of the end of a block if splitting would

 'rawlen' is not the exact summary size; it is only an upper estimation */

 Doesn't fit untouched. We'll go the old route and split it */

 If it's inode-less, we don't _know_ what it is. Just copy it intact */

 OK, all the CRCs are good; this node can just be copied as-is. */

 Try to reallocate space and retry */

						/* this is not the exact summary size of it,

 For these, we don't actually need to read the old node */

		/* Fetch the inode length from the fragtree rather then

	/* If the times on this inode were set by explicit utime() they can be different,

	/* On a medium where we can't actually mark nodes obsolete

	   pernamently, such as NAND flash, we need to work out

	   whether this deletion dirent is still needed to actively

	   delete a 'real' dirent with the same name that's still

		/* Prevent the erase code from nicking the obsolete node refs while

		   we're looking at them. I really don't like this extra lock but

 We only care about obsolete ones */

 Any dirent with the same name is going to have the same length... */

			/* Doesn't matter if there's one in the same erase block. We're going to

			/* This is an obsolete node belonging to the same directory, and it's of the right

 If we can't read it, we don't need to continue to obsolete it. Continue */

 If the name CRC doesn't match, skip */

 If the name length doesn't match, or it's another deletion dirent, skip */

 OK, check the actual name now */

			/* OK. The name really does match. There really is still an older node on

			   the flash which our deletion dirent obsoletes. So we have to write out

	/* FIXME: If we're deleting a dirent which contains the current mtime and ctime,

 No need for it any more. Just mark it obsolete and remove it from the list */

		/* It's partially obsoleted by a later write. So we have to

 FIXME: We could possibly deal with this by writing new holes for each frag */

		/* Fetch the inode length from the fragtree rather then

	/*

	 * We should only get here in the case where the node we are

	 * replacing had more than one frag, so we kept the same version

	 * number as before. (Except in case of error -- see 'goto fill;'

	 * above.)

 This is a partially-overlapped hole node. Mark it REF_NORMAL not REF_PRISTINE */

		/* Attempt to do some merging. But only expand to cover logically

		   adjacent frags if the block containing them is already considered

		   to be dirty. Otherwise we end up with GC just going round in

		   circles dirtying the nodes it already wrote out, especially

		   on NAND where we have small eraseblocks and hence a much higher

 BUG_ON(!frag) but that'll happen anyway... */

 First grow down... */

			/* If the previous frag doesn't even reach the beginning, there's

 OK. This frag holds the first byte of the page. */

				/* OK, it's a frag which extends to the beginning of the page. Does it live

				   in a block which is still considered clean? If so, don't obsolete it.

 ... then up */

 Find last frag which is actually part of the node we're to GC. */

			/* If the previous frag doesn't even reach the beginning, there's lots

				/* OK, it's a frag which extends to the beginning of the page. Does it live

				   in a block which is still considered clean? If so, don't obsolete it.

	/* The rules state that we must obtain the page lock *before* f->sem, so

	 * drop f->sem temporarily. Since we also hold c->alloc_sem, nothing's

	 * actually going to *change* so we're safe; we only allow reading.

	 *

	 * It is important to note that jffs2_write_begin() will ensure that its

	 * page is marked Uptodate before allocating space. That means that if we

	 * end up here trying to GC the *same* page that jffs2_write_begin() is

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

/* These are initialised to NULL in the kernel startup code.

 If jeb->last_node is really a valid node then skip over it */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004 Thomas Gleixner <tglx@linutronix.de>

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 * Modified debugged and enhanced by Thomas Gleixner <tglx@linutronix.de>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 For testing write failures */

 max. erase failures before we mark a block bad */

 If a malloc failed, consider _everything_ dirty */

 If ino == 0, _any_ non-GC writes mean 'yes' */

 Look to see if the inode in question is pending in the wbuf */

 Schedule delayed write-buffer write-out */

			/* Most of the time, we just erase it immediately. Otherwise we

			/* Sometimes, however, we leave it elsewhere so it doesn't get

 File the existing block on the bad_used_list.... */

 Not sure this should ever happen... need more coffee */

 It has to have had some nodes or we couldn't be here */

 convert to wasted */

 Find a frag which refers to the full_dnode we want to modify */

/* Recover from failure to write wbuf. Recover the nodes up to the

	/* Find the first node to be recovered, by skipping over every

 All nodes were obsolete. Nothing to recover. */

 Count the number of refs which need to be copied */

		/* First affected node was already partially written.

 Do the read... */

 ECC recovered ? */

 If this was the only node to be recovered, give up */

 It wasn't. Go on and try to recover nodes complete in the wbuf */

 Read succeeded. Copy the remaining data from the wbuf */

	/* OK... we're to rewrite (end-start) bytes of data from first_raw onwards.

 ... and get an allocation of space from a shiny new block instead */

 The summary is not recovered, so it must be disabled for this erase block */

		/* Need to do another write immediately, but it's possible

		   that this is just because the wbuf itself is completely

		   full, and there's nothing earlier read back from the

		   flash. Hence 'buf' isn't necessarily what we're writing

 Argh. We tried. Really we did. */

 Don't muck about with c->wbuf_inodes. False positives are harmless. */

 OK, now we're left with the dregs in whichever buffer we're using */

 Now sort out the jffs2_raw_node_refs, moving them from the old to the next block */

 Ick. This XATTR mess should be fixed shortly... */

 Remove the old node from the per-inode list */

				/* If it's an in-core inode, then we have to adjust any

				   full_dirent or full_dnode structure to point to the

 Should never happen; it _must_ be present */

				/* We don't lock f->sem. There's a number of ways we could

				   end up in here with it already being locked, and nobody's

				   going to modify it on us anyway because we hold the

				   alloc_sem. We're only changing one ->raw pointer too,

 Fix up the original jeb now it's on the bad_list */

/* Meaning of pad argument:

   0: Do not pad. Probably pointless - we only ever use this when we can't pad anyway.

   1: Pad, do not adjust nextblock free_size

   2: Pad, adjust nextblock free_size

	/* Nothing to do if not write-buffering the flash. In particular, we shouldn't

 already checked c->wbuf above */

	/* claim remaining space on the page

	   this happens, if we have a change to a new block,

	   or if fsync forces us to flush the writebuffer.

	   if we have a switch to next page, we will not have

	   enough remaining space for this.

		/* Pad with JFFS2_DIRTY_BITMASK initially.  this helps out ECC'd NOR

	/* else jffs2_flash_writev has actually filled in the rest of the

 Adjust free size of the block if we padded. */

		/* wbuf_pagesize - wbuf_len is the amount of space that's to be

		   padded. If there is less free space in the block than that,

 FIXME: that made it count as dirty. Convert to wasted */

 Stick any now-obsoleted blocks on the erase_pending_list */

 adjust write buffer offset, else we get a non contiguous write bug */

/* Trigger garbage collection to flush the write-buffer.

   If ino arg is zero, do it if _any_ real (i.e. not GC) writes are

   outstanding. If ino arg non-zero, do it only if a write for the

 GC won't make any progress for a while */

		/* retry flushing wbuf in case jffs2_wbuf_recover

 GC failed. Flush it with padding instead */

			/* retry flushing wbuf in case jffs2_wbuf_recover

 Pad write-buffer to end and write it, wasting space. */

 retry - maybe wbuf recover left some data in wbuf. */

 If not writebuffered flash, don't bother */

 If wbuf_ofs is not initialized, set it to target address */

	/*

	 * Sanity checks on target address.  It's permitted to write

	 * at PAD(c->wbuf_len+c->wbuf_ofs), and it's permitted to

	 * write at the beginning of a new erase block. Anything else,

	 * and you die.  New block starts at xxx000c (0-b = block

	 * header)

 It's a write to a new block */

 set pointer to new block */

 We're not writing immediately after the writebuffer. Bad. */

 adjust alignment offset */

 take care of alignment to next page */

	/*

	 * If there's a remainder in the wbuf and it's a non-GC write,

	 * remember that the wbuf affects this ino

	/*

	 * At this point we have no problem, c->wbuf is empty. However

	 * refile nextblock to avoid writing again to same address.

/*

 *	This is the entry for flash write.

 *	Check, if we work on NAND FLASH, if so build an kvec and write it via vritev

/*

	Handle readback from writebuffer and ECC failure return

 Read flash */

		/*

		 * We have the raw data without ECC correction in the buffer,

		 * maybe we are lucky and all data or parts are correct. We

		 * check the node.  If data are corrupted node check will sort

		 * it out.  We keep this block, it will fail on write or erase

		 * and the we mark it bad. Or should we do that now? But we

		 * should give him a chance.  Maybe we had a system crash or

		 * power loss before the ecc write or a erase was completed.

		 * So we return success. :)

 if no writebuffer available or write buffer empty, return */

 if we read in a different block, return */

 offset in write buffer */

 is read beyond write buffer ? */

 number of bytes to copy */

 offset in read buffer */

 is write beyond write buffer ? */

 number of bytes to copy */

 For historical reasons we use only 8 bytes for OOB clean marker */

/*

 * Check, if the out of band area is empty. This function knows about the clean

 * marker and if it is present in OOB, treats the OOB as empty anyway.

 Yeah, we know about the cleanmarker */

/*

 * Check for a valid cleanmarker.

 * Returns: 0 if a valid cleanmarker was found

 *	    1 if no cleanmarker was found

 *	    negative error code if an error occurred

/*

 * On NAND we try to mark this block bad. If the block was erased more

 * than MAX_ERASE_FAILURES we mark it finally bad.

 * Don't care about failures. This block remains on the erase-pending

 * or badblock list as long as nobody manipulates the flash with

 * a bootloader or something like that.

 if the count is < max, we try to write the counter to the 2nd page oob area */

 Cleanmarker is out-of-band, so inline size zero */

 Initialise write buffer */

 No cleanmarkers needed */

 Initialize write buffer */

	/* Find a suitable c->sector_size

	 * - Not too much sectors

	 * - Sectors have to be at least 4 K + some bytes

	 * - All known dataflashes have erase sizes of 528 or 1056

	 * - we take at least 8 eraseblocks and want to have at least 8K size

	 * - The concatenation should be a power of 2

 It may be necessary to adjust the flash size */

	/* Cleanmarker currently occupies whole programming regions,

 Initialize write buffer */

 We do not need write-buffer */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

/* These helper functions _must_ increase ofs and also do the dirty/used space accounting.

 * Returning an error will abort the mount - bad checksums etc. should just mark the space

 * as dirty.

	/* Turned wasted size into dirty, since we apparently 

 summary info collected by the scan process */

 Don't muck about if it won't let us point to the whole flash */

		/* For NAND it's quicker to read a whole eraseblock at a time,

 reset summary info for next eraseblock scan */

 Now decide which list to put it on */

			/*

			 * Empty block.   Since we can't be sure it

			 * was entirely erased, we just queue it for erase

			 * again.  It will be marked as such when the erase

			 * is complete.  Meanwhile we still count it as empty

			 * for later checks.

 Only a CLEANMARKER node is valid */

 It's actually free */

 Dirt */

 Full (or almost full) of clean data. Clean list */

 Some data, but not full. Dirty list. */

			/* We want to remember the block with most free space

 Better candidate for the next writes to go to */

 deleting summary information of the old nextblock */

 update collected summary information for the current nextblock */

 Nothing valid - not even a clean marker. Needs erasing. */

 For now we just put it on the erasing list. We'll start the erases later */

 Nextblock dirty is always seen as wasted, because we cannot recycle it now */

		/* If we're going to start writing into a block which already

		   contains data, and the end of the data isn't page-aligned,

 move blocks with max 4 byte dirty space to cleanlist */

	/* BEFORE jffs2_build_xattr_subsystem() called, 

	 * and AFTER xattr_ref is marked as a dead xref,

	 * ref->xid is used to store 32bit xid, xd is not used

	 * ref->ino is used to store 32bit inode-number, ic is not used

	 * Thoes variables are declared as union, thus using those

	 * are exclusive. In a similar way, ref->next is temporarily

	 * used to chain all xattr_ref object. It's re-chained to

	 * jffs2_inode_cache in jffs2_build_xattr_subsystem() correctly.

/* Called with 'buf_size == 0' if buf is in fact a pointer _directly_ into

		/* Even if it's not found, we still scan to see

		   if the block is empty. We use this information

 XIP case. Just look, point at the summary if it's there */

 If NAND flash, read a whole page of it. Else just the end */

 Read as much as we want into the _end_ of the preallocated buffer */

 sm->offset maybe wrong but MAGIC maybe right */

 Now, make sure the summary itself is available */

 Need to kmalloc for this. */

 Need to read more so that the entire summary node is present */

			/* If it returns with a real error, bail. 

			   If it returns positive, that's a block classification

			   (i.e. BLK_STATE_xxx) so return that too.

 This is the XIP case -- we're reading _directly_ from the flash chip */

 We temporarily use 'ofs' as a pointer into the buffer/jeb */

 Scan only EMPTY_SCAN_SIZE of 0xFF before declaring it's empty */

 scan oob, take care of cleanmarker */

 don't bother with re-erase */

 OK to erase if all blocks are like this */

 Now ofs is a complete physical flash offset as it always was... */

 Make sure there are node refs available for use */

 Ran off end. */

			/* If we're only checking the beginning of a block with a cleanmarker,

 XIP/point case */

 See how much more there is to read in this eraseblock... */

				/* No more to read. Break out of main loop without marking

 point never reaches here */

 OK. We're out of possibilities. Whinge and move on */

 We seem to have a node of sorts. Check the CRC */

 Eep. Node goes over the end of the erase block. */

 Wheee. This is an obsoleted node */

 CONFIG_JFFS2_FS_XATTR */

 We can't summarise nodes we don't grok */

 mark_node_obsolete can add to wasted !! */

	/* We do very little here now. Just check the ino# to which we should attribute

	   this node; we can do all the CRC checking etc. later. There's a tradeoff here --

	   we used to scan the flash once only, reading everything we want from it into

	   memory, then building all our in-core data structures and freeing the extra

	   information. Now we allow the first part of the mount to complete a lot quicker,

	   but we have to go _back_ to the flash in order to finish the CRC checking, etc.

	   Which means that the _full_ amount of time to get to proper write mode with GC

 Check the node CRC in any case. */

		/*

		 * We believe totlen because the CRC on the node

		 * _header_ was OK, just the node itself failed.

 Wheee. It worked */

	/* We don't get here unless the node is still valid, so we don't have to

 We believe totlen because the CRC on the node _header_ was OK, just the node itself failed. */

 Should never happen. Did. (OLPC trac #4184)*/

 FIXME: Why do we believe totlen? */

 We believe totlen because the CRC on the node _header_ was OK, just the name failed. */

/* Note: This breaks if list_empty(head). I don't care. You

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

**********************************************************************/

/* We keep the dirent list sorted in increasing order of name hash,

   and we use the same hash function as the dentries. Makes this

   nice and simple

 The 'nhash' on the fd_list is not the same as the dentry hash */

 NB: The 2.2 backport will need to explicitly check for '.' and '..' here */

**********************************************************************/

 First loop: curofs = 2; pos = 2 */

**********************************************************************/

	/* jffs2_do_create() will want to lock it, _after_ reserving

	   space and taking c-alloc_sem. If we keep it locked here,

	   lockdep gets unhappy (although it's a false positive;

	   nothing else will be looking at this inode yet so there's

**********************************************************************/

**********************************************************************/

 Don't let people make hard links to bad inodes. */

 XXX: This is ugly */

**********************************************************************/

	/* FIXME: If you care. We'd need to use frags for the target

	/* Try to reserve enough space for both node and dirent.

	 * Just the node will do for now, though

 Eeek. Wave bye bye */

 We use f->target field to store the target path. */

	/* No data here. Only a metadata node, which will be

	   obsoleted by the first data write

 Argh. Now we treat it like a normal delete */

		/* dirent failed to write. Delete the inode normally

	/* Link the fd into the inode's list, obsoleting an old

	/* Try to reserve enough space for both node and dirent.

	 * Just the node will do for now, though

 Directories get nlink 2 at start */

 but ic->pino_nlink is the parent ino# */

 Eeek. Wave bye bye */

	/* No data here. Only a metadata node, which will be

	   obsoleted by the first data write

 Argh. Now we treat it like a normal delete */

		/* dirent failed to write. Delete the inode normally

	/* Link the fd into the inode's list, obsoleting an old

	/* Try to reserve enough space for both node and dirent.

	 * Just the node will do for now, though

 Eeek. Wave bye bye */

	/* No data here. Only a metadata node, which will be

	   obsoleted by the first data write

 Argh. Now we treat it like a normal delete */

 XXX: This is ugly. */

		/* dirent failed to write. Delete the inode normally

	/* Link the fd into the inode's list, obsoleting an old

	/* The VFS will check for us and prevent trying to rename a

	 * file over a directory and vice versa, but if it's a directory,

	 * the VFS can't check whether the victim is empty. The filesystem

	 * needs to do that for itself.

	/* XXX: We probably ought to alloc enough space for

	   both nodes at the same time. Writing the new link,

	   then getting -ENOSPC, is quite bad :)

 Make a hard link */

 XXX: This is ugly */

 There was a victim. Kill it off nicely */

		/* Don't oops if the victim was a dirent pointing to an

	/* If it was a directory we moved, and there was no victim,

 Unlink the original */

 We don't touch inode->i_nlink */

 Oh shit. We really ought to make a single node which can do both atomically */

		/*

		 * We can't keep the target in dcache after that.

		 * For one thing, we can't afford dentry aliases for directories.

		 * For another, if there was a victim, we _can't_ set new inode

		 * for that sucker and we have to trigger mount eviction - the

		 * caller won't do it on its own since we are returning an error.

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2006  NEC Corporation

 *

 * Created by KaiGai Kohei <kaigai@ak.jp.nec.com>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 ---- Initial Security Label(s) Attachment callback --- */

 ---- Initial Security Label(s) Attachment ----------- */

 ---- XATTR Handler for "security.*" ----------------- */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

	/* Plan: call deflate() with avail_in == *sourcelen,

		avail_out = *dstlen - 12 and flush == Z_FINISH.

		If it doesn't manage to finish,	call it again with

		avail_in == 0 and avail_out set to the remaining 12

		bytes for it to clean up.

	   Q: Is 12 bytes sufficient?

 Linux-only */

 __KERNEL__ */

	/* If it's deflate, and it's got no preset dictionary, then

 Let this remain D1 for now -- it should never happen */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

	/* There was a bug where we wrote hole nodes out with csize/dsize

	/* Cases:

	   Reading whole node and it's uncompressed - read directly to buffer provided, check CRC.

	   Reading whole node and it's compressed - read into comprbuf, check CRC and decompress to buffer provided

	   Reading partial node and it's uncompressed - read into readbuf, check CRC, and copy

	   Reading partial node and it's compressed - read into readbuf, check checksum, decompress to decomprbuf and copy

	/* XXX FIXME: Where a single physical node actually shows up in two

	/* Now we're pointing at the first frag which overlaps our page

	 * (or perhaps is before it, if we've been asked to read off the

 offset within the frag to start reading */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

	/* Special cases - we don't want more than one data node

	   for these types on the medium at any time. So setattr

	   must read the original data associated with the node

	   (i.e. the device numbers or the target name) and write

 For these, we don't actually need to read the old node */

 It's an extension. Make it a hole node */

		/* For truncate-to-zero, treat it as deletion because

 It worked. Update the inode */

	/* We have to do the truncate_setsize() without f->sem held, since

	   some pages may be locked and waiting for it in readpage().

	   We are protected from a simultaneous write() extending i_size

	   back past iattr->ia_size, because do_truncate() holds the

	/* We can forget about this inode for now - drop all

	 *  the nodelists associated with it, etc.

 parent and '.' */

 Root dir gets i_nlink 3 for some reason */

 Read the device numbers from the media */

 Eep */

	/* We stop if it was running, then restart if it needs to.

	   This also catches the case where it was stopped and this

	   is just a remount to restart it.

/* jffs2_new_inode: allocate a new inode and inocache, add it to the hash,

 Set OS-specific defaults for new inodes */

	/* POSIX ACLs have to be processed now, at least partly.

	/*

	 * Pick a inocache hash size based on the size of the medium.

	 * Count how many megabytes we're dealing with, apply a hashsize twice

	 * that size, but rounding down to the usual big powers of 2. And keep

	 * to sensible bounds.

 Do not support the MLC nand */

	/*

	 * Size alignment check

 NAND (or other bizarre) flash... do setup accordingly */

		/* The inode has zero nlink but its nodes weren't yet marked

		   obsolete. This has to be because we're still waiting for

		   the final (close() and) iput() to happen.



		   There's a possibility that the final iput() could have

		   happened while we were contemplating. In order to ensure

		   that we don't cause a new read_inode() (which would fail)

		   for the inode in question, we use ilookup() in this case

		   instead of iget().



		   The nlink can't _become_ zero at this point because we're

		   holding the alloc_sem, and jffs2_do_unlink() would also

		   need that while decrementing nlink on any inode.

 Wait for progress. Don't just loop */

		/* Inode has links to it still; they're not going away because

		   jffs2_do_unlink() would need the alloc_sem and we have it.

		   Just iget() it, and if read_inode() is necessary that's OK.

 NB. This will happen again. We need to do something appropriate here. */

 NAND flash... do setup accordingly */

 and Dataflash */

 and Intel "Sibley" flash */

 and an UBI volume */

 and DataFlash */

 and Intel "Sibley" flash */

 and an UBI volume */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 Trigger GC to flush any pending writes for this inode */

 jffs2_file_inode_operations */

 FIXME: Can kmap fail? */

 Make new hole frag from old EOF to new page */

	/*

	 * Read in the page if it wasn't already present. Cannot optimize away

	 * the whole page write case until jffs2_write_end can handle the

	 * case of a short-copy.

	/* Actually commit the write from the page cache page we're looking at.

	 * For now, we write the full page out each time. It sucks, but it's simple

	/* We need to avoid deadlock with page_cache_read() in

	   jffs2_garbage_collect_pass(). So the page must be

	   up to date to prevent page_cache_read() from trying

		/* When writing out the end of a page, write out the

		   _whole_ page. This helps to reduce the number of

		   nodes in files which have many short writes, like

 Set the fields that the generic jffs2_write_inode_range() code can't find */

	/* In 2.4, it was already kmapped by generic_file_write(). Doesn't

 There was an error writing. */

 Adjust writtenlen for the padding we did, so we don't confuse our caller */

		/* generic_file_write has written more to the page cache than we've

		   actually written to the medium. Mark the page !Uptodate so that

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by Arjan van de Ven <arjanv@redhat.com>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 *

 *

 * Very simple lz77-ish encoder.

 *

 * Theory of operation: Both encoder and decoder have a list of "last

 * occurrences" for every possible source-value; after sending the

 * first source-byte, the second byte indicated the "run" length of

 * matches

 *

 * The algorithm is intended to only send "whole bytes", no bit-messing.

 *

 _compress returns the compressed size, -1 if bigger */

 We failed */

 Tell the caller how much we managed to compress, and how much space it took */

 first the verbatim copied byte */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 This must only ever be called when no GC thread is currently running */

 Wait for it... */

		/* Problem - immediately after bootup, the GCD spends a lot

		 * of time in places like jffs2_kill_fragtree(); so much so

		 * that userspace processes (like gdm and X) are starved

		 * despite plenty of cond_resched()s and renicing.  Yield()

		 * doesn't help, either (presumably because userspace and GCD

		 * are generally competing for a higher latency resource -

		 * disk).

		 * This forces the GCD to slow the hell down.   Pulling an

		 * inode in with read_inode() is much preferable to having

		/* Put_super will send a SIGKILL and then wait on the sem.

 We don't want SIGHUP to interrupt us. STOP and KILL are OK though. */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 kvfree() */

 More in this chain? */

 For each child, increase nlink */

 we can get high latency here with huge directories */

 Clear the ic/raw union so it doesn't cause problems later. */

 From this point, fd->raw is no longer used so we can set fd->ic */

		/* If we appear (at this stage) to have hard-linked directories,

 Can't free scan_dents so far. We might need them in pass 2 */

/* Scan plan:

 - Scan physical nodes. Build map of inodes/dirents. Allocate inocaches as we go

 - Scan directory tree from top down, setting nlink in inocaches

 - Scan inocaches for inodes with nlink==0

	/* First, scan the medium and build all the inode caches with

 Now scan the directory tree, increasing nlink according to every dirent found. */

	/* Next, scan for inodes with nlink == 0 and remove them. If

	   they were directories, then decrement the nlink of their

	   children too, and repeat the scan. As that's going to be

	   a fairly uncommon occurrence, it's not so evil to do it this

		/* If we detected directory hardlinks earlier, *hopefully*

		 * they are gone now because some of the links were from

		 * dead directories which still had some old dirents lying

		 * around and not yet garbage-collected, but which have

		 * been discarded above. So clear the pino_nlink field

		 * in each directory, so that the final scan below can

 Finally, we can scan again and free the dirent structs */

			/* We do use the pino_nlink field to count nlink of

			 * directories during fs build, so set it to the

			 * parent ino# now. Now that there's hopefully only

					/* We'll have complained about it and marked the coresponding

 We *have* to have set this in jffs2_build_inode_pass1() */

				/* We clear ic->pino_nlink ∀ directories' ic *only* if dir_hardlinks

				 * is set. Otherwise, we know this should never trigger anyway, so

				 * we don't do the check. And ic->pino_nlink still contains the nlink

 Should we unlink it from its previous parent? */

 For directories, ic->pino_nlink holds that parent inode # */

 Rotate the lists by some number to ensure wear levelling */

 It's a deletion dirent. Ignore it */

			/* Reduce nlink of the child. If it's now zero, stick it on the

	/*

	   We don't delete the inocache from the hash list and free it yet.

	   The erase code will do that, when all the nodes are completely gone.

	/* Deletion should almost _always_ be allowed. We're fairly

	   buggered once we stop allowing people to delete stuff

	/* Be conservative about how much space we need before we allow writes.

	   On top of that which is required for deletia, require an extra 2%

	   of the medium to be available, for overhead caused by nodes being

 2% of flash size */

 And 100 bytes per eraseblock */

 ... and round up */

 When do we let the GC thread run in the background */

	/* When do we allow garbage collection to merge nodes to make

	/* When do we allow garbage collection to eat from bad blocks rather

c->resv_blocks_deletion + 2;

	/* What number of 'very dirty' eraseblocks do we allow before we

	   trigger the GC thread even if we don't _need_ the space. When we

	   can't mark nodes obsolete on the medium, the old dirty nodes cause

	/* If there's less than this amount of dirty space, don't bother

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

/*

 * Check the data CRC of the node.

 *

 * Returns: 0 if the data CRC is correct;

 * 	    1 - if incorrect;

 *	    error code if an error occurred.

 Calculate how many bytes were already checked */

	/* TODO: instead, incapsulate point() stuff to jffs2_flash_read(),

 succefully pointed to device */

		/* TODO: this is very frequent pattern, make it a separate

 Continue calculating CRC */

	/* If it should be REF_NORMAL, it'll get marked as such when

	   we build the fragtree, shortly. No need to worry about GC

	   moving it while it's marked REF_PRISTINE -- GC won't happen

	/*

	 * Mark the node as having been checked and fix the

	 * accounting accordingly.

/*

 * Helper function for jffs2_add_older_frag_to_fragtree().

 *

 * Checks the node if we are in the checking stage.

 We only check the data CRC of unchecked nodes */

/*

 * This function is used when we read an inode. Data nodes arrive in

 * arbitrary order -- they may be older or newer than the nodes which

 * are already in the tree. Where overlaps occur, the older node can

 * be discarded as long as the newer passes the CRC check. We don't

 * bother to keep track of holes in this rbtree, and neither do we deal

 * with frags -- we can have multiple entries starting at the same

 * offset, and the one with the smallest length will come first in the

 * ordering.

 *

 * Returns 0 if the node was handled (including marking it obsolete)

 *	 < 0 an if error occurred

	/* If a node has zero dsize, we only have to keep it if it might be the

	   node with highest version -- i.e. the one which will end up as f->metadata.

	   Note that such nodes won't be REF_UNCHECKED since there are no data to

 We had a candidate mdata node already */

 Find the earliest node which _may_ be relevant to this one */

		/* If the node is coincident with another at a lower address,

				/*

				 * We killed a node which set the overlapped

				 * flags during the scan. Fix it up.

			/* Version number collision means REF_PRISTINE GC. Accept either of them

 The one we already had was OK. Keep it and throw away the new one */

 Who cares if the new one is good; keep it for now anyway. */

 Same overlapping from in front and behind */

 New node entirely overlaps 'this' */

 ... and is good. Kill 'this' and any subsequent nodes which are also overlapped */

 New node entirely overlapped by 'this' */

 ... but 'this' was bad. Replace it... */

	/* We neither completely obsoleted nor were completely

 If there's anything behind that overlaps us, note it */

				/*

				 * We killed a node which set the overlapped

				 * flags during the scan. Fix it up.

 If the new node overlaps anything ahead, note it */

/* Trivial function to remove the last node in the tree. Which by definition

   has no right-hand child — so can be removed just by making its left-hand

   child (if any) take its place under its parent. Since this is only done

   when we're consuming the whole tree, there's no need to use rb_erase()

   and let it worry about adjusting colours and balancing the tree. That

 LAST! */

/* We put the version tree in reverse order, so we can use the same eat_last()

/* Build final, normal fragtree from tn tree. It doesn't matter which order

   we add nodes to the real fragtree, as long as they don't overlap. And

   having thrown away the majority of overlapped nodes as we went, there

   really shouldn't be many sets of nodes which do overlap. If we start at

   the end, we can use the overlap markers -- we can just eat nodes which

   aren't overlapped, and when we encounter nodes which _do_ overlap we

			/*

			 * We killed a node which set the overlapped

			 * flags during the scan. Fix it up.

		/* Now we have a bunch of nodes in reverse version

		   order, in the tree at ver_root. Most of the time,

		   there'll actually be only one node in the 'tree',

					/* Note that this is different from the other

					   highest_version, because this one is only

					   counting _valid_ nodes which could give the

					/* Free the nodes in vers_root; let the caller

 Returns first valid node after 'ref'. May return 'ref' */

/*

 * Helper function for jffs2_get_inode_nodes().

 * It is called every time an directory entry node is found.

 *

 * Returns: 0 on success;

 * 	    negative error code on failure.

 Obsoleted. This cannot happen, surely? dwmw2 20020308 */

 If we've never checked the CRCs on this node, check them now */

 Sanity check */

 Pick out the mctime of the latest dirent */

	/*

	 * Copy as much of the name as possible from the raw

	 * dirent we've already read from the flash.

 Do we need to copy any more of the name directly from the flash? */

 FIXME: point() */

		/*

		 * we use CONFIG_JFFS2_SUMMARY because without it, we

		 * have checked it while mounting

	/*

	 * Wheee. We now have a complete jffs2_full_dirent structure, with

	 * the name in it and everything. Link it into the list

/*

 * Helper function for jffs2_get_inode_nodes().

 * It is called every time an inode node is found.

 *

 * Returns: 0 on success (possibly after marking a bad node obsolete);

 * 	    negative error code on failure.

 Obsoleted. This cannot happen, surely? dwmw2 20020308 */

 If we've never checked the CRCs on this node, check them now */

 Sanity checks */

			/* At this point we are supposed to check the data CRC

			 * of our unchecked node. But thus far, we do not

			 * know whether the node is valid or obsolete. To

			 * figure this out, we need to walk all the nodes of

			 * the inode and build the inode fragtree. We don't

			 * want to spend time checking data of nodes which may

			 * later be found to be obsolete. So we put off the full

			 * data CRC checking until we have read all the inode

			 * nodes and have started building the fragtree.

			 *

			 * The fragtree is being built starting with nodes

			 * having the highest version number, so we'll be able

			 * to detect whether a node is valid (i.e., it is not

			 * overlapped by a node with higher version) or not.

			 * And we'll be able to check only those nodes, which

			 * are not obsolete.

			 *

			 * Of course, this optimization only makes sense in case

			 * of NAND flashes (or other flashes with

			 * !jffs2_can_mark_obsolete()), since on NOR flashes

			 * nodes are marked obsolete physically.

			 *

			 * Since NAND flashes (or other flashes with

			 * jffs2_is_writebuffered(c)) are anyway read by

			 * fractions of c->wbuf_pagesize, and we have just read

			 * the node header, it is likely that the starting part

			 * of the node data is also read when we read the

			 * header. So we don't mind to check the CRC of the

			 * starting part of the data of the node now, and check

			 * the second part later (in jffs2_check_node_data()).

			 * Of course, we will not need to re-read and re-check

			 * the NAND page which we have just read. This is why we

			 * read the whole NAND page at jffs2_get_inode_nodes(),

			 * while we needed only the node header.

 'buf' will point to the start of data */

 len will be the read data length */

			/* If we actually calculated the whole data CRC

			/*

			 * We checked the header CRC. If the node has no data, adjust

			 * the space accounting now. For other nodes this will be done

			 * later either when the node is marked obsolete or when its

			 * data is checked.

	/* There was a bug where we wrote hole nodes out with

 normal case...

/*

 * Helper function for jffs2_get_inode_nodes().

 * It is called every time an unknown node is found.

 *

 * Returns: 0 on success;

 * 	    negative error code on failure.

 We don't mark unknown nodes as REF_UNCHECKED */

 EEP */

/*

 * Helper function for jffs2_get_inode_nodes().

 * The function detects whether more data should be read and reads it if yes.

 *

 * Returns: 0 on success;

 * 	    negative error code on failure.

 We need to read more data */

/* Get tmp_dnode_info and full_dirent for all non-obsolete nodes associated

   with this ino. Perform a preliminary ordering on data nodes, throwing away

   those which are completely obsoleted by newer ones. The naïve approach we

   use to take of just returning them _all_ in version order will cause us to

	/* FIXME: in case of NOR and available ->point() this

		/* We can hold a pointer to a non-obsolete node without the spinlock,

		   but _obsolete_ nodes may disappear at any time, if the block

		   they're in gets erased. So if we mark 'ref' obsolete while we're

		   not holding the lock, it can go away immediately. For that reason,

		   we find the next valid node first, before processing 'ref'.

		/*

		 * At this point we don't know the type of the node we're going

		 * to read, so we do not know the size of its header. In order

		 * to minimize the amount of flash IO we assume the header is

		 * of size = JFFS2_MIN_NODE_HEADER.

			/*

			 * We are about to read JFFS2_MIN_NODE_HEADER bytes,

			 * but this flash has some minimal I/O unit. It is

			 * possible that we'll need to read more soon, so read

			 * up to the next min. I/O unit, in order not to

			 * re-read the same min. I/O unit twice.

 FIXME: point() */

 No need to mask in the valid bit; it shouldn't be invalid */

 Not a JFFS2 node, whinge and move on */

 Grab all nodes relevant to this ino */

 FIXME: We could at least crc-check them all */

 No data nodes for this inode. */

 FIXME: If this fails, there seems to be a memory leak. Find it. */

			/* The times in the latest_node are actually older than

 If it was a regular file, truncate it to the latest node's isize */

		/* Hack to work around broken isize in old symlink code.

		   Remove this when dwmw2 comes to his senses and stops

		   symlinks from being an entirely gratuitous special

			/* Symlink's inode data is the target path. Read it and

			 * keep in RAM to facilitate quick follow symlink

		/* Certain inode types should have only one data node, and it's

 ASSERT: f->fraglist != NULL */

 FIXME: Deal with it - check crc32, check for duplicate node, check times and discard the older one */

 OK. We're happy */

 Scan the list of all nodes present for this ino, build map of versions, etc. */

 Check its state. We may need to wait before we can use it */

			/* If it's in either of these states, we need

			   to wait for whoever's got it to finish and

			/* Eep. This should never happen. It can

			happen if Linux calls read_inode() again

 Fail. That's probably better than allowing it to succeed */

 Special case - no root inode on medium */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 * Copyright © 2004 Ferenc Havasi <havasi@inf.u-szeged.hu>,

 *		    University of Szeged, Hungary

 *

 * Created by Arjan van de Ven <arjan@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 Available compressors are on this list */

 Actual compression mode */

 Statistics for blocks stored without compression */

/*

 * Return 1 to use this compression

 Shouldn't happen */

/*

 * jffs2_selected_compress:

 * @compr: Explicit compression type to use (ie, JFFS2_COMPR_ZLIB).

 *	If 0, just take the first available compression mode.

 * @data_in: Pointer to uncompressed data

 * @cpage_out: Pointer to returned pointer to buffer for compressed data

 * @datalen: On entry, holds the amount of data available for compression.

 *	On exit, expected to hold the amount of data actually compressed.

 * @cdatalen: On entry, holds the amount of space available for compressed

 *	data. On exit, expected to hold the actual size of the compressed

 *	data.

 *

 * Returns: the compression type used.  Zero is used to show that the data

 * could not be compressed; probably because we couldn't find the requested

 * compression mode.

 Skip decompress-only and disabled modules */

 Skip if not the desired compression type */

		/*

		 * Either compression type was unspecified, or we found our

		 * compressor; either way, we're good to go.

 Success */

/* jffs2_compress:

 * @data_in: Pointer to uncompressed data

 * @cpage_out: Pointer to returned pointer to buffer for compressed data

 * @datalen: On entry, holds the amount of data available for compression.

 *	On exit, expected to hold the amount of data actually compressed.

 * @cdatalen: On entry, holds the amount of space available for compressed

 *	data. On exit, expected to hold the actual size of the compressed

 *	data.

 *

 * Returns: Lower byte to be stored with data indicating compression type used.

 * Zero is used to show that the data could not be compressed - the

 * compressed version was actually larger than the original.

 * Upper byte will be used later. (soon)

 *

 * If the cdata buffer isn't large enough to hold all the uncompressed data,

 * jffs2_compress should compress as much as will fit, and should set

 * *datalen accordingly to show the amount of data which were compressed.

 Skip decompress-only backwards-compatibility and disabled modules */

 Allocating memory for output buffer if necessary */

	/* Older code had a bug where it would write non-zero 'usercompr'

 This should be special-cased elsewhere, but we might as well deal with it */

 Registering compressors */

 Setting default compression mode */

 Unregistering compressors */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

/*

 * Check whether the user is allowed to write.

 Always allow root */

/**

 *	jffs2_reserve_space - request physical space to write nodes to flash

 *	@c: superblock info

 *	@minsize: Minimum acceptable size of allocation

 *	@len: Returned value of allocation length

 *	@prio: Allocation type - ALLOC_{NORMAL,DELETION}

 *

 *	Requests a block of physical space on the flash. Returns zero for success

 *	and puts 'len' into the appropriate place, or returns -ENOSPC or other 

 *	error if appropriate. Doesn't return len since that's 

 *

 *	If it returns zero, jffs2_reserve_space() also downs the per-filesystem

 *	allocation semaphore, to prevent more than one allocation from being

 *	active at any time. The semaphore is later released by jffs2_commit_allocation()

 *

 *	jffs2_reserve_space() may trigger garbage collection in order to make room

 *	for the requested allocation.

 align it */

	/*

	 * Check if the free space is greater then size of the reserved pool.

	 * If not, only allow root to proceed with writing.

 this needs a little more thought (true <tglx> :)) */

			/* calculate real dirty size

			 * dirty_size contains blocks on erase_pending_list

			 * those blocks are counted in c->nr_erasing_blocks.

			 * If one block is actually erased, it is not longer counted as dirty_space

			 * but it is counted in c->nr_erasing_blocks, so we add it and subtract it

			 * with c->nr_erasing_blocks * c->sector_size again.

			 * Blocks on erasable_list are counted as dirty_size, but not in c->nr_erasing_blocks

			 * This helps us to force gc and pick eventually a clean block to spread the load.

			 * We add unchecked_size here, as we hopefully will find some space to use.

			 * This will affect the sum only once, as gc first finishes checking

			 * of nodes.

			/* Calc possibly available space. Possibly available means that we

			 * don't know, if unchecked size contains obsoleted nodes, which could give us some

			 * more usable space. This will affect the sum only once, as gc first finishes checking

			 * of nodes.

			 + Return -ENOSPC, if the maximum possibly available space is less or equal than

			 * blocksneeded * sector_size.

			 * This blocks endless gc looping on a filesystem, which is nearly full, even if

			 * the check above passes.

 Classify nextblock (clean, dirty of verydirty) and force to select an other one */

 Check, if we have a dirty block now, or if it was dirty already */

 Select a new jeb for nextblock */

 Take the next block off the 'free' list */

 c->nextblock is NULL, no update to c->nextblock allowed */

 Have another go. It'll be on the erasable_list now */

			/* Ouch. We're in GC, or we wouldn't have got here.

 Don't wait for it; just erase one right now */

		/* An erase may have failed, decreasing the

		   amount of free space available. So we must

 reset collected summary */

 adjust write buffer offset, else we get a non contiguous write bug */

 Called with alloc sem _and_ erase_completion_lock */

 for summary information at the end of the jeb */

 NOSUM_SIZE means not to generate summary */

		/* Is there enough space for writing out the current node, or we have to

 Has summary been disabled for this jeb? */

 Writing out the collected summary information */

				/* jffs2_write_sumnode() couldn't write out the summary information

				   diabling summary for this jeb and free the collected information

 keep always valid value in reserved_size */

 Skip the end of this block and file it as having some dirty space */

 If there's a pending write to it, flush now */

			/* Just lock it again and continue. Nothing much can change because

			   we hold c->alloc_sem anyway. In fact, it's not entirely clear why

			   we hold c->erase_completion_lock in the majority of this function...

 FIXME: that made it count as dirty. Convert to wasted */

	/* OK, jeb (==c->nextblock) is now pointing at a block which definitely has

		/* Only node in it beforehand was a CLEANMARKER node (we think).

		   So mark it obsolete now that there's going to be another node

		   in the block. This will reduce used_size to zero but We've

		   already set c->nextblock so that jffs2_mark_node_obsolete()

		   won't try to refile it to the dirty_list.

/**

 *	jffs2_add_physical_node_ref - add a physical node reference to the list

 *	@c: superblock info

 *	@new: new node reference to add

 *	@len: length of this physical node

 *

 *	Should only be used to report nodes for which space has been allocated

 *	by jffs2_reserve_space.

 *

 *	Must be called with the alloc_sem held.

	/* Allow non-obsolete nodes only to be added at the end of c->nextblock, 

	   if c->nextblock is set. Note that wbuf.c will file obsolete nodes

 If it lives on the dirty_list, jffs2_reserve_space will put it there */

 Flush the last write in the block if it's outstanding */

		/* Hm. This may confuse static lock analysis. If any of the above

		   three conditions is false, we're going to return from this

		   function without actually obliterating any nodes or freeing

		   any jffs2_raw_node_refs. So we don't need to stop erases from

		   happening, or protect against people holding an obsolete

 Take care, that wasted size is taken into concern

 Convert wasted space to dirty, if not a bad block */

 To fool the refiling code later */

		/* Flash scanning is in progress. Don't muck about with the block

		   lists because they're not ready yet, and don't actually

		   obliterate nodes that look obsolete. If they weren't

		   marked obsolete on the flash at the time they _became_

 We didn't lock the erase_free_sem */

				/* Most of the time, we just erase it immediately. Otherwise we

				/* Sometimes, however, we leave it elsewhere so it doesn't get

 We didn't lock the erase_free_sem */

	/* The erase_free_sem is locked, and has been since before we marked the node obsolete

	   and potentially put its eraseblock onto the erase_pending_list. Thus, we know that

	   the block hasn't _already_ been erased, and that 'ref' itself hasn't been freed yet

 XXX FIXME: This is ugly now */

	/* Nodes which have been marked obsolete no longer need to be

	   associated with any inode. Remove them from the per-inode list.



	   Note we can't do this for NAND at the moment because we need

	   obsolete dirent nodes to stay on the lists, because of the

	   horridness in jffs2_garbage_collect_deletion_dirent(). Also

	   because we delete the inocache, and on NAND we need that to

	   stay around until all the nodes are actually erased, in order

	   to stop us from giving the same inode number to another newly

	/* dirty_size contains blocks on erase_pending_list

	 * those blocks are counted in c->nr_erasing_blocks.

	 * If one block is actually erased, it is not longer counted as dirty_space

	 * but it is counted in c->nr_erasing_blocks, so we add it and subtract it

	 * with c->nr_erasing_blocks * c->sector_size again.

	 * Blocks on erasable_list are counted as dirty_size, but not in c->nr_erasing_blocks

	 * This helps us to force gc and pick eventually a clean block to spread the load.

 In debug mode, actually go through and count them all */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by Arjan van de Ven <arjanv@redhat.com>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 behalve lower */

	/*

	 * First, work out how many bits we need from the input stream.

	 * Note that we have already done the initial check on this

	 * loop prior to calling this function.

	/*

	 * Now get the bits.  We really want this to be "get n bits".

 Failed. Restore old state */

 We failed */

	/* Tell the caller how much we managed to compress,

 We didn't actually compress */

 _compress returns the compressed size, -1 if bigger */

 Add back the 8 bytes we took for the probabilities */

 We compressed */

&jffs2_rubinmips_compress,*/

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2007 Nokia Corporation. All rights reserved.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by Richard Purdie <rpurdie@openedhand.com>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 for lzo_mem and lzo_compress_buf */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2006  NEC Corporation

 *

 * Created by KaiGai Kohei <kaigai@ak.jp.nec.com>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2004  Ferenc Havasi <havasi@inf.u-szeged.hu>,

 *		     Zoltan Sogor <weth@inf.u-szeged.hu>,

 *		     Patrik Kluba <pajko@halom.u-szeged.hu>,

 *		     University of Szeged, Hungary

 *	       2006  KaiGai Kohei <kaigai@ak.jp.nec.com>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 The following 3 functions are called from scan.c to collect summary info for not closed jeb */

 relative offset from the beginning of the jeb */

 relative from the beginning of the jeb */

 Cleanup every collected summary information */

 Move the collected summary information into sb (called from scan.c) */

 Called from wbuf.c to collect writed node info */

 impossible count value */

			/* If you implement a new node type you should also implement

			   summary support for it or disable summary.

 If there was a gap, mark it dirty */

 Ew. Summary doesn't actually tell us explicitly about dirty space */

 Process the stored summary information - helper function for jffs2_sum_scan_sumnode() */

 Make sure there's a spare ref for dirty space */

dev.laptop.org/ticket/4184 */

 node is not the newest one */

 For compatible node types, just fall back to the full scan */

 Process the summary node - called from jffs2_scan_eraseblock() */

 OK, now check for node validity and CRC */

	/* -ENOTRECOVERABLE isn't a fatal error -- it means we should do a full

 real error */

 for PARANOIA_CHECK */

 Write summary data to flash - helper function for jffs2_sum_write_sumnode() */

 It won't fit in the buffer. Abort summary for this jeb */

 Non-fatal */

 Is there enough space for summary? */

 don't try to write out summary for this jeb */

 Non-fatal */

 The above call removes the list, nothing more to do */

 unknown node in summary information */

 Waste remaining space */

 Write out summary information - called from jffs2_do_reserve_space */

/*

 * JFFS2 -- Journalling Flash File System, Version 2.

 *

 * Copyright © 2001-2007 Red Hat, Inc.

 * Copyright © 2004-2010 David Woodhouse <dwmw2@infradead.org>

 *

 * Created by David Woodhouse <dwmw2@infradead.org>

 *

 * For licensing information, see the file 'LICENCE' in this directory.

 *

 JFFS2_DBG_SANITY_CHECKS */

/*

 * Check the fragtree.

			/* A hole node which isn't multi-page should be garbage-collected

			   and merged anyway, so we just check for the frag size here,

			   rather than mucking around with actually reading the node

			   and checking the compression type, which is the real way

/*

 * Check if the flash contains all 0xFF before we start writing.

/*

 * Check the space accounting and node_ref list correctness for the JFFS2 erasable block 'jeb'.

 This should work when we implement ref->__totlen elemination */

 JFFS2_DBG_PARANOIA_CHECKS */

/*

 * Dump the node_refs of the 'jeb' JFFS2 eraseblock.

/*

 * Dump an eraseblock's space accounting.

/*

 * Dump a JFFS2 node.

 JFFS2_DBG_DUMPS || JFFS2_DBG_PARANOIA_CHECKS */

 SPDX-License-Identifier: GPL-2.0

/*

 * Data verification functions, i.e. hooks for ->readpages()

 *

 * Copyright 2019 Google LLC

/**

 * hash_at_level() - compute the location of the block's hash at the given level

 *

 * @params:	(in) the Merkle tree parameters

 * @dindex:	(in) the index of the data block being verified

 * @level:	(in) the level of hash we want (0 is leaf level)

 * @hindex:	(out) the index of the hash block containing the wanted hash

 * @hoffset:	(out) the byte offset to the wanted hash within the hash block

 Offset of the hash within the level's region, in hashes */

 Index of the hash block in the tree overall */

 Offset of the wanted hash (in bytes) within the hash block */

 Extract a hash from a hash page */

/*

 * Verify a single data page against the file's Merkle tree.

 *

 * In principle, we need to verify the entire path to the root node.  However,

 * for efficiency the filesystem may cache the hash pages.  Therefore we need

 * only ascend the tree until an already-verified page is seen, as indicated by

 * the PageChecked bit being set; then verify the path to that page.

 *

 * This code currently only supports the case where the verity block size is

 * equal to PAGE_SIZE.  Doing otherwise would be possible but tricky, since we

 * wouldn't be able to use the PageChecked bit.

 *

 * Note that multiple processes may race to verify a hash page and mark it

 * Checked, but it doesn't matter; the result will be the same either way.

 *

 * Return: true if the page is valid, else false.

	/*

	 * Starting at the leaf level, ascend the tree saving hash pages along

	 * the way until we find a verified hash page, indicated by PageChecked;

	 * or until we reach the root.

 Descend the tree verifying hash pages */

 Finally, verify the data page */

/**

 * fsverity_verify_page() - verify a data page

 * @page: the page to verity

 *

 * Verify a page that has just been read from a verity file.  The page must be a

 * pagecache page that is still locked and not yet uptodate.

 *

 * Return: true if the page is valid, else false.

 This allocation never fails, since it's mempool-backed. */

/**

 * fsverity_verify_bio() - verify a 'read' bio that has just completed

 * @bio: the bio to verify

 *

 * Verify a set of pages that have just been read from a verity file.  The pages

 * must be pagecache pages that are still locked and not yet uptodate.  Pages

 * that fail verification are set to the Error state.  Verification is skipped

 * for pages already in the Error state, e.g. due to fscrypt decryption failure.

 *

 * This is a helper function for use by the ->readpages() method of filesystems

 * that issue bios to read data directly into the page cache.  Filesystems that

 * populate the page cache without issuing bios (e.g. non block-based

 * filesystems) must instead call fsverity_verify_page() directly on each page.

 * All filesystems must also call fsverity_verify_page() on holes.

 This allocation never fails, since it's mempool-backed. */

		/*

		 * If this bio is for data readahead, then we also do readahead

		 * of the first (largest) level of the Merkle tree.  Namely,

		 * when a Merkle tree page is read, we also try to piggy-back on

		 * some additional pages -- up to 1/4 the number of data pages.

		 *

		 * This improves sequential read performance, as it greatly

		 * reduces the number of I/O requests made to the Merkle tree.

 CONFIG_BLOCK */

/**

 * fsverity_enqueue_verify_work() - enqueue work on the fs-verity workqueue

 * @work: the work to enqueue

 *

 * Enqueue verification work for asynchronous processing.

	/*

	 * Use an unbound workqueue to allow bios to be verified in parallel

	 * even when they happen to complete on the same CPU.  This sacrifices

	 * locality, but it's worthwhile since hashing is CPU-intensive.

	 *

	 * Also use a high-priority workqueue to prioritize verification work,

	 * which blocks reads from completing, over regular application tasks.

 SPDX-License-Identifier: GPL-2.0

/*

 * fs-verity module initialization and logging

 *

 * Copyright 2019 Google LLC

 SPDX-License-Identifier: GPL-2.0

/*

 * fs-verity hash algorithms

 *

 * Copyright 2019 Google LLC

 The hash algorithms supported by fs-verity */

/**

 * fsverity_get_hash_alg() - validate and prepare a hash algorithm

 * @inode: optional inode for logging purposes

 * @num: the hash algorithm number

 *

 * Get the struct fsverity_hash_alg for the given hash algorithm number, and

 * ensure it has a hash transform ready to go.  The hash transforms are

 * allocated on-demand so that we don't waste resources unnecessarily, and

 * because the crypto modules may be initialized later than fs/verity/.

 *

 * Return: pointer to the hash alg on success, else an ERR_PTR()

 pairs with smp_store_release() below */

	/*

	 * Using the shash API would make things a bit simpler, but the ahash

	 * API is preferable as it allows the use of crypto accelerators.

 pairs with smp_load_acquire() above */

/**

 * fsverity_alloc_hash_request() - allocate a hash request object

 * @alg: the hash algorithm for which to allocate the request

 * @gfp_flags: memory allocation flags

 *

 * This is mempool-backed, so this never fails if __GFP_DIRECT_RECLAIM is set in

 * @gfp_flags.  However, in that case this might need to wait for all

 * previously-allocated requests to be freed.  So to avoid deadlocks, callers

 * must never need multiple requests at a time to make forward progress.

 *

 * Return: the request object on success; NULL on failure (but see above)

/**

 * fsverity_free_hash_request() - free a hash request object

 * @alg: the hash algorithm

 * @req: the hash request object to free

/**

 * fsverity_prepare_hash_state() - precompute the initial hash state

 * @alg: hash algorithm

 * @salt: a salt which is to be prepended to all data to be hashed

 * @salt_size: salt size in bytes, possibly 0

 *

 * Return: NULL if the salt is empty, otherwise the kmalloc()'ed precomputed

 *	   initial hash state on success or an ERR_PTR() on failure.

 This allocation never fails, since it's mempool-backed. */

	/*

	 * Zero-pad the salt to the next multiple of the input size of the hash

	 * algorithm's compression function, e.g. 64 bytes for SHA-256 or 128

	 * bytes for SHA-512.  This ensures that the hash algorithm won't have

	 * any bytes buffered internally after processing the salt, thus making

	 * salted hashing just as fast as unsalted hashing.

/**

 * fsverity_hash_page() - hash a single data or hash page

 * @params: the Merkle tree's parameters

 * @inode: inode for which the hashing is being done

 * @req: preallocated hash request

 * @page: the page to hash

 * @out: output digest, size 'params->digest_size' bytes

 *

 * Hash a single data or hash block, assuming block_size == PAGE_SIZE.

 * The hash is salted if a salt is specified in the Merkle tree parameters.

 *

 * Return: 0 on success, -errno on failure

/**

 * fsverity_hash_buffer() - hash some data

 * @alg: the hash algorithm to use

 * @data: the data to hash

 * @size: size of data to hash, in bytes

 * @out: output digest, size 'alg->digest_size' bytes

 *

 * Hash some data which is located in physically contiguous memory (i.e. memory

 * allocated by kmalloc(), not by vmalloc()).  No salt is used.

 *

 * Return: 0 on success, -errno on failure

 This allocation never fails, since it's mempool-backed. */

	/*

	 * Sanity check the hash algorithms (could be a build-time check, but

	 * they're in an array)

		/*

		 * For efficiency, the implementation currently assumes the

		 * digest and block sizes are powers of 2.  This limitation can

		 * be lifted if the code is updated to handle other values.

 SPDX-License-Identifier: GPL-2.0

/*

 * Opening fs-verity files

 *

 * Copyright 2019 Google LLC

/**

 * fsverity_init_merkle_tree_params() - initialize Merkle tree parameters

 * @params: the parameters struct to initialize

 * @inode: the inode for which the Merkle tree is being built

 * @hash_algorithm: number of hash algorithm to use

 * @log_blocksize: log base 2 of block size to use

 * @salt: pointer to salt (optional)

 * @salt_size: size of salt, possibly 0

 *

 * Validate the hash algorithm and block size, then compute the tree topology

 * (num levels, num blocks in each level, etc.) and initialize @params.

 *

 * Return: 0 on success, -errno on failure

	/*

	 * Compute the number of levels in the Merkle tree and create a map from

	 * level to the starting block of that level.  Level 'num_levels - 1' is

	 * the root and is stored first.  Level 0 is the level directly "above"

	 * the data blocks and is stored last.

 Compute number of levels and the number of blocks in each level */

 temporarily using level_start[] to store blocks in level */

 Compute the starting block of each level */

/*

 * Compute the file digest by hashing the fsverity_descriptor excluding the

 * signature and with the sig_size field set to 0.

/*

 * Create a new fsverity_info from the given fsverity_descriptor (with optional

 * appended signature), and check the signature if present.  The

 * fsverity_descriptor must have already undergone basic validation.

	/*

	 * Multiple tasks may race to set ->i_verity_info, so use

	 * cmpxchg_release().  This pairs with the smp_load_acquire() in

	 * fsverity_get_info().  I.e., here we publish ->i_verity_info with a

	 * RELEASE barrier so that other tasks can ACQUIRE it.

 Lost the race, so free the fsverity_info we allocated. */

		/*

		 * Afterwards, the caller may access ->i_verity_info directly,

		 * so make sure to ACQUIRE the winning fsverity_info.

/*

 * Read the inode's fsverity_descriptor (with optional appended signature) from

 * the filesystem, and do basic validation of it.

 Ensure the inode has an ->i_verity_info */

/**

 * fsverity_file_open() - prepare to open a verity file

 * @inode: the inode being opened

 * @filp: the struct file being set up

 *

 * When opening a verity file, deny the open if it is for writing.  Otherwise,

 * set up the inode's ->i_verity_info if not already done.

 *

 * When combined with fscrypt, this must be called after fscrypt_file_open().

 * Otherwise, we won't have the key set up to decrypt the verity metadata.

 *

 * Return: 0 on success, -errno on failure

/**

 * fsverity_prepare_setattr() - prepare to change a verity inode's attributes

 * @dentry: dentry through which the inode is being changed

 * @attr: attributes to change

 *

 * Verity files are immutable, so deny truncates.  This isn't covered by the

 * open-time check because sys_truncate() takes a path, not a file descriptor.

 *

 * Return: 0 on success, -errno on failure

/**

 * fsverity_cleanup_inode() - free the inode's verity info, if present

 * @inode: an inode being evicted

 *

 * Filesystems must call this on inode eviction to free ->i_verity_info.

 SPDX-License-Identifier: GPL-2.0

/*

 * Ioctl to get a verity file's digest

 *

 * Copyright 2019 Google LLC

/**

 * fsverity_ioctl_measure() - get a verity file's digest

 * @filp: file to get digest of

 * @_uarg: user pointer to fsverity_digest

 *

 * Retrieve the file digest that the kernel is enforcing for reads from a verity

 * file.  See the "FS_IOC_MEASURE_VERITY" section of

 * Documentation/filesystems/fsverity.rst for the documentation.

 *

 * Return: 0 on success, -errno on failure

 not a verity file */

	/*

	 * The user specifies the digest_size their buffer has space for; we can

	 * return the digest if it fits in the available space.  We write back

	 * the actual size, which may be shorter than the user-specified size.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Ioctl to read verity metadata

 *

 * Copyright 2021 Google LLC

	/*

	 * Iterate through each Merkle tree page in the requested range and copy

	 * the requested portion to userspace.  Note that the Merkle tree block

	 * size isn't important here, as we are returning a byte stream; i.e.,

	 * we can just work with pages even if the tree block size != PAGE_SIZE.

 Copy the requested portion of the buffer to userspace. */

 don't include the signature */

	/*

	 * Include only the signature.  Note that fsverity_get_descriptor()

	 * already verified that sig_size is in-bounds.

/**

 * fsverity_ioctl_read_metadata() - read verity metadata from a file

 * @filp: file to read the metadata from

 * @uarg: user pointer to fsverity_read_metadata_arg

 *

 * Return: length read on success, 0 on EOF, -errno on failure

 not a verity file */

	/*

	 * Note that we don't have to explicitly check that the file is open for

	 * reading, since verity files can only be opened for reading.

 offset + length must not overflow. */

 Ensure that the return value will fit in INT_MAX. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Ioctl to enable verity on a file

 *

 * Copyright 2019 Google LLC

/*

 * Read a file data page for Merkle tree construction.  Do aggressive readahead,

 * since we're sequentially reading the entire file.

 checked earlier too */

 unused */

 Leaf: hashing a data block */

 Non-leaf: hashing hash block from level below */

 Root hash? */

 Flush the pending hash block */

/*

 * Build the Merkle tree for the given file using the given parameters, and

 * return the root hash in @root_hash.

 *

 * The tree is written to a filesystem-specific location as determined by the

 * ->write_merkle_tree_block() method.  However, the blocks that comprise the

 * tree are the same for all filesystems.

 Empty file is a special case; root hash is all 0's */

 This allocation never fails, since it's mempool-backed. */

	/*

	 * Build each level of the Merkle tree, starting at the leaf level

	 * (level 0) and ascending to the root node (level 'num_levels - 1').

	 * Then at the end (level 'num_levels'), calculate the root hash.

 Start initializing the fsverity_descriptor */

 Get the salt if the user provided one */

 Get the signature if the user provided one */

 Prepare the Merkle tree parameters */

	/*

	 * Start enabling verity on this file, serialized by the inode lock.

	 * Fail if verity is already enabled or is already being enabled.

	/*

	 * Build the Merkle tree.  Don't hold the inode lock during this, since

	 * on huge files this may take a very long time and we don't want to

	 * force unrelated syscalls like chown() to block forever.  We don't

	 * need the inode lock here because deny_write_access() already prevents

	 * the file from being written to or truncated, and we still serialize

	 * ->begin_enable_verity() and ->end_enable_verity() using the inode

	 * lock and only allow one process to be here at a time on a given file.

	/*

	 * Create the fsverity_info.  Don't bother trying to save work by

	 * reusing the merkle_tree_params from above.  Instead, just create the

	 * fsverity_info from the fsverity_descriptor as if it were just loaded

	 * from disk.  This is simpler, and it serves as an extra check that the

	 * metadata we're writing is valid before actually enabling verity.

	/*

	 * Tell the filesystem to finish enabling verity on the file.

	 * Serialized with ->begin_enable_verity() by the inode lock.

 Successfully enabled verity */

		/*

		 * Readers can start using ->i_verity_info immediately, so it

		 * can't be rolled back once set.  So don't set it until just

		 * after the filesystem has successfully enabled verity.

/**

 * fsverity_ioctl_enable() - enable verity on a file

 * @filp: file to enable verity on

 * @uarg: user pointer to fsverity_enable_arg

 *

 * Enable fs-verity on a file.  See the "FS_IOC_ENABLE_VERITY" section of

 * Documentation/filesystems/fsverity.rst for the documentation.

 *

 * Return: 0 on success, -errno on failure

	/*

	 * Require a regular file with write access.  But the actual fd must

	 * still be readonly so that we can lock out all writers.  This is

	 * needed to guarantee that no writable fds exist to the file once it

	 * has verity enabled, and to stabilize the data being hashed.

 -EROFS */

 -ETXTBSY */

	/*

	 * Some pages of the file may have been evicted from pagecache after

	 * being used in the Merkle tree construction, then read into pagecache

	 * again by another process reading from the file concurrently.  Since

	 * these pages didn't undergo verification against the file digest which

	 * fs-verity now claims to be enforcing, we have to wipe the pagecache

	 * to ensure that all future reads are verified.

	/*

	 * allow_write_access() is needed to pair with deny_write_access().

	 * Regardless, the filesystem won't allow writing to verity files.

 SPDX-License-Identifier: GPL-2.0

/*

 * Verification of builtin signatures

 *

 * Copyright 2019 Google LLC

/*

 * /proc/sys/fs/verity/require_signatures

 * If 1, all verity files must have a valid builtin signature.

/*

 * Keyring that contains the trusted X.509 certificates.

 *

 * Only root (kuid=0) can modify this.  Also, root may use

 * keyctl_restrict_keyring() to prevent any more additions.

/**

 * fsverity_verify_signature() - check a verity file's signature

 * @vi: the file's fsverity_info

 * @signature: the file's built-in signature

 * @sig_size: size of signature in bytes, or 0 if no signature

 *

 * If the file includes a signature of its fs-verity file digest, verify it

 * against the certificates in the fs-verity keyring.

 *

 * Return: 0 on success (signature valid or not required); -errno on failure

 !CONFIG_SYSCTL */

 !CONFIG_SYSCTL */

 SPDX-License-Identifier: GPL-2.0-only

/******************************************************************************

*******************************************************************************

**

**  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.

**  Copyright (C) 2004-2021 Red Hat, Inc.  All rights reserved.

**

**

*******************************************************************************

/*

 * midcomms.c

 *

 * This is the appallingly named "mid-level" comms layer. It takes care about

 * deliver an on application layer "reliable" communication above the used

 * lowcomms transport layer.

 *

 * How it works:

 *

 * Each nodes keeps track of all send DLM messages in send_queue with a sequence

 * number. The receive will send an DLM_ACK message back for every DLM message

 * received at the other side. If a reconnect happens in lowcomms we will send

 * all unacknowledged dlm messages again. The receiving side might drop any already

 * received message by comparing sequence numbers.

 *

 * How version detection works:

 *

 * Due the fact that dlm has pre-configured node addresses on every side

 * it is in it's nature that every side connects at starts to transmit

 * dlm messages which ends in a race. However DLM_RCOM_NAMES, DLM_RCOM_STATUS

 * and their replies are the first messages which are exchanges. Due backwards

 * compatibility these messages are not covered by the midcomms re-transmission

 * layer. These messages have their own re-transmission handling in the dlm

 * application layer. The version field of every node will be set on these RCOM

 * messages as soon as they arrived and the node isn't yet part of the nodes

 * hash. There exists also logic to detect version mismatched if something weird

 * going on or the first messages isn't an expected one.

 *

 * Termination:

 *

 * The midcomms layer does a 4 way handshake for termination on DLM protocol

 * like TCP supports it with half-closed socket support. SCTP doesn't support

 * half-closed socket, so we do it on DLM layer. Also socket shutdown() can be

 * interrupted by .e.g. tcp reset itself. Additional there exists the othercon

 * paradigm in lowcomms which cannot be easily without breaking backwards

 * compatibility. A node cannot send anything to another node when a DLM_FIN

 * message was send. There exists additional logic to print a warning if

 * DLM wants to do it. There exists a state handling like RFC 793 but reduced

 * to termination only. The event "member removal event" describes the cluster

 * manager removed the node from internal lists, at this point DLM does not

 * send any message to the other node. There exists two cases:

 *

 * 1. The cluster member was removed and we received a FIN

 * OR

 * 2. We received a FIN but the member was not removed yet

 *

 * One of these cases will do the CLOSE_WAIT to LAST_ACK change.

 *

 *

 *                              +---------+

 *                              | CLOSED  |

 *                              +---------+

 *                                   | add member/receive RCOM version

 *                                   |            detection msg

 *                                   V

 *                              +---------+

 *                              |  ESTAB  |

 *                              +---------+

 *                       CLOSE    |     |    rcv FIN

 *                      -------   |     |    -------

 * +---------+          snd FIN  /       \   snd ACK          +---------+

 * |  FIN    |<-----------------           ------------------>|  CLOSE  |

 * | WAIT-1  |------------------                              |   WAIT  |

 * +---------+          rcv FIN  \                            +---------+

 * | rcv ACK of FIN   -------   |                            CLOSE  | member

 * | --------------   snd ACK   |                           ------- | removal

 * V        x                   V                           snd FIN V event

 * +---------+                  +---------+                   +---------+

 * |FINWAIT-2|                  | CLOSING |                   | LAST-ACK|

 * +---------+                  +---------+                   +---------+

 * |                rcv ACK of FIN |                 rcv ACK of FIN |

 * |  rcv FIN       -------------- |                 -------------- |

 * |  -------              x       V                        x       V

 *  \ snd ACK                 +---------+                   +---------+

 *   ------------------------>| CLOSED  |                   | CLOSED  |

 *                            +---------+                   +---------+

 *

 * NOTE: any state can interrupted by midcomms_close() and state will be

 * switched to CLOSED in case of fencing. There exists also some timeout

 * handling when we receive the version detection RCOM messages which is

 * made by observation.

 *

 * Future improvements:

 *

 * There exists some known issues/improvements of the dlm handling. Some

 * of them should be done in a next major dlm version bump which makes

 * it incompatible with previous versions.

 *

 * Unaligned memory access:

 *

 * There exists cases when the dlm message buffer length is not aligned

 * to 8 byte. However seems nobody detected any problem with it. This

 * can be fixed in the next major version bump of dlm.

 *

 * Version detection:

 *

 * The version detection and how it's done is related to backwards

 * compatibility. There exists better ways to make a better handling.

 * However this should be changed in the next major version bump of dlm.

 *

 * Tail Size checking:

 *

 * There exists a message tail payload in e.g. DLM_MSG however we don't

 * check it against the message length yet regarding to the receive buffer

 * length. That need to be validated.

 *

 * Fencing bad nodes:

 *

 * At timeout places or weird sequence number behaviours we should send

 * a fencing request to the cluster manager.

/* Debug switch to enable a 5 seconds sleep waiting of a termination.

 * This can be useful to test fencing while termination is running.

 * This requires a setup with only gfs2 as dlm user, so that the

 * last umount will terminate the connection.

 *

 * However it became useful to test, while the 5 seconds block in umount

 * just press the reset button. In a lot of dropping the termination

 * process can could take several seconds.

 init value for sequence numbers for testing purpose only e.g. overflows */

 3 minutes wait to sync ending of dlm */

	/* These queues are unbound because we cannot drop any message in dlm.

	 * We could send a fence signal for a specific node to the cluster

	 * manager if queues hits some maximum value, however this handling

	 * not supported yet.

 dlm tcp termination state */

	/* counts how many lockspaces are using this node

	 * this refcount is necessary to determine if the

	 * node wants to disconnect.

 not protected by srcu, node_hash lifetime */

 get_mhandle/commit srcu idx exchange */

/* This mutex prevents that midcomms_close() is running while

 * stop() or remove(). As I experienced invalid memory access

 * behaviours when DLM_DEBUG_FENCE_TERMINATION is enabled and

 * resetting machines. I will end in some double deletion in nodes

 * datastructure.

	/* check again if there was somebody else

	 * earlier here to add the node

 send queue should be ordered */

 send queue should be ordered */

 DLM_CLOSED */

 not valid but somehow we got what we want */

 send ack before fin */

				/* passive shutdown DLM_LAST_ACK case 1

				 * additional we check if the node is used by

				 * cluster manager events at all.

 probably remove_member caught it, do nothing */

		/* retry to ack message which we already have by sending back

		 * current node->seq_next number as ack.

					/* some invalid state passive shutdown

					 * was failed, we try to reset and

					 * hope it will go on.

	/* we only trust outer header msglen because

	 * it's checked against receive buffer length.

		/* these rcom message we use to determine version.

		 * they have their own retransmission handling and

		 * are the first messages of dlm.

		 *

		 * length already checked.

 recheck inner msglen just if it's not garbage */

 length already checked */

/*

 * Called from the low-level comms layer to process a buffer of

 * commands.

		/* no message should be more than DLM_MAX_SOCKET_BUFSIZE or

		 * less than dlm_header size.

		 *

		 * Some messages does not have a 8 byte length boundary yet

		 * which can occur in a unaligned memory access of some dlm

		 * messages. However this problem need to be fixed at the

		 * sending side, for now it seems nobody run into architecture

		 * related issues yet but it slows down some processing.

		 * Fixing this issue should be scheduled in future by doing

		 * the next major version bump.

		/* caller will take care that leftover

		 * will be parsed next call with more data

 old protocol, we do nothing */

 do nothing if we didn't delivered stateful to ulp */

 we only ack if state is ESTABLISHED */

 do nothing FIN has it's own ack send */

 old protocol, we don't support to retransmit on failure */

 add possible options here */

 this is a bug, however we going on and hope it will be resolved */

	/* keep in mind that is a must to call

	 * dlm_midcomms_commit_msg() which releases

	 * nodes_srcu using mh->idx which is assumed

	 * here that the application will call it.

 nexthdr chain for fast lookup */

 mh is not part of rcu list in this case */

 not valid but somehow we got what we want */

			/* some invalid state passive shutdown

			 * was failed, we try to reset and

			 * hope it will go on.

	/* hitting users count to zero means the

	 * other side is running dlm_midcomms_stop()

	 * we meet us to have a clean disconnect.

 passive shutdown DLM_LAST_ACK case 2 */

 probably receive fin caught it, do nothing */

 already gone, do nothing */

 old protocol, we don't wait for pending operations */

 we have what we want */

		/* busy to enter DLM_FIN_WAIT1, wait until passive

		 * done in shutdown_wait to enter DLM_CLOSED.

 wait for other side dlm + fin */

 Abort pending close/remove operation */

 let shutdown waiters leave */

