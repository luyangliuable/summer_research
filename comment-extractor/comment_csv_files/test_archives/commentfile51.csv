 SPDX-License-Identifier: GPL-2.0

/*

 * <linux/swait.h> (simple wait queues ) implementation:

/*

 * The thing about the wake_up_state() return value; I think we can ignore it.

 *

 * If for some reason it would return 0, that means the previously waiting

 * task is already running, so it will observe condition true (or has already).

/*

 * Wake up all waiters. This is an interface which is solely exposed for

 * completions and not for general usage.

 *

 * It is intentionally different from swake_up_all() to allow usage from

 * hard interrupt context and interrupt disabled regions.

/*

 * Does not allow usage from IRQ disabled, since we must be able to

 * release IRQs to guarantee bounded hold time.

		/*

		 * See prepare_to_wait_event(). TL;DR, subsequent swake_up_one()

		 * must not see us.

 SPDX-License-Identifier: GPL-2.0

/*

 * Auto-group scheduling implementation:

 We've redirected RT tasks to the root task group... */

	/*

	 * Autogroup RT tasks are redirected to the root task group

	 * so we don't have to move tasks around upon policy change,

	 * or flail around trying to allocate bandwidth on the fly.

	 * A bandwidth exception in __sched_setscheduler() allows

	 * the policy change to proceed.

	/*

	 * If we race with autogroup_move_group() the caller can use the old

	 * value of signal->autogroup but in this case sched_move_task() will

	 * be called again before autogroup_kref_put().

	 *

	 * However, there is no way sched_autogroup_exit_task() could tell us

	 * to avoid autogroup->tg, so we abuse PF_EXITING flag for this case.

	/*

	 * We are going to call exit_notify() and autogroup_move_group() can't

	 * see this thread after that: we can no longer use signal->autogroup.

	 * See the PF_EXITING check in task_wants_autogroup().

	/*

	 * We can't avoid sched_move_task() after we changed signal->autogroup,

	 * this process can already run with task_group() == prev->tg or we can

	 * race with cgroup code which can read autogroup = prev under rq->lock.

	 * In the latter case for_each_thread() can not miss a migrating thread,

	 * cpu_cgroup_attach() must not be possible after cgroup_exit() and it

	 * can't be removed from thread list, we hold ->siglock.

	 *

	 * If an exiting thread was already removed from thread list we rely on

	 * sched_autogroup_exit_task().

 Allocates GFP_KERNEL, cannot be called under any spinlock: */

 Drop extra reference added by autogroup_create(): */

 Cannot be called under siglock. Currently has no users: */

 This is a heavy operation, taking global locks.. */

 CONFIG_PROC_FS */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * A simple wrapper around refcount. An allocated sched_core_cookie's

 * address is used to compute the cookie of the task.

/*

 * sched_core_update_cookie - replace the cookie on a task

 * @p: the task to update

 * @cookie: the new cookie

 *

 * Effectively exchange the task cookie; caller is responsible for lifetimes on

 * both ends.

 *

 * Returns: the old cookie

	/*

	 * Since creating a cookie implies sched_core_get(), and we cannot set

	 * a cookie until after we've created it, similarly, we cannot destroy

	 * a cookie until after we've removed it, we must have core scheduling

	 * enabled here.

	/*

	 * If task is currently running, it may not be compatible anymore after

	 * the cookie change, so enter the scheduler on its CPU to schedule it

	 * away.

 Called from prctl interface: PR_SCHED_CORE */

	/*

	 * Check if this process has the right to modify the specified

	 * process. Use the regular "ptrace_may_access()" checks.

 XXX improve ? */

 SPDX-License-Identifier: GPL-2.0

/*

 * /proc/schedstat implementation

			/*

			 * Preserve migrating task's wait time so wait_start

			 * time stamp can be adjusted to accumulate wait time

			 * prior to migration.

			/*

			 * Blocking time is in units of nanosecs, so shift by

			 * 20 to get a milliseconds-range estimation of the

			 * amount of time that the task spent sleeping:

/*

 * Current schedstat API version.

 *

 * Bump this up when changing the output format or the meaning of an existing

 * format, so that tools can adapt (or abort)

 runqueue-specific stats */

 domain-specific stats */

/*

 * This iterator needs some explanation.

 * It returns 1 for the header position.

 * This means 2 is cpu 0.

 * In a hotplugged system some CPUs, including cpu 0, may be missing so we have

 * to use cpumask_* to iterate over the CPUs.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * The implementation of the wait_bit*() and related waiting APIs:

/*

 * To allow interruptible waiting and asynchronous (i.e. nonblocking)

 * waiting, the actions of __wait_on_bit() and __wait_on_bit_lock() are

 * permitted return codes. Nonzero return codes halt waiting and return.

			/*

			 * See the comment in prepare_to_wait_event().

			 * finish_wait() does not necessarily takes wwq_head->lock,

			 * but test_and_set_bit() implies mb() which pairs with

			 * smp_mb__after_atomic() before wake_up_page().

/**

 * wake_up_bit - wake up a waiter on a bit

 * @word: the word being waited on, a kernel virtual address

 * @bit: the bit of the word being waited on

 *

 * There is a standard hashed waitqueue table for generic use. This

 * is the part of the hashtable's accessor API that wakes up waiters

 * on a bit. For instance, if one were to have waiters on a bitflag,

 * one would call wake_up_bit() after clearing the bit.

 *

 * In order for this to function properly, as it uses waitqueue_active()

 * internally, some kind of memory barrier must be done prior to calling

 * this. Typically, this will be smp_mb__after_atomic(), but in some

 * cases where bitflags are manipulated non-atomically under a lock, one

 * may need to use a less regular barrier, such fs/inode.c's smp_mb(),

 * because spin_unlock() does not guarantee a memory barrier.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  kernel/sched/core.c

 *

 *  Core kernel scheduler code and related syscalls

 *

 *  Copyright (C) 1991-2002  Linus Torvalds

/*

 * Export tracepoints that act as a bare tracehook (ie: have no trace event

 * associated with them) to allow external modules to probe them.

/*

 * Debugging: various feature bits

 *

 * If SCHED_DEBUG is disabled, each compilation unit has its own copy of

 * sysctl_sched_features, defined in sched.h, to allow constants propagation

 * at compile time and compiler optimization based on features default.

/*

 * Print a warning if need_resched is set for the given duration (if

 * LATENCY_WARN is enabled).

 *

 * If sysctl_resched_latency_warn_once is set, only one warning will be shown

 * per boot.

 CONFIG_SCHED_DEBUG */

/*

 * Number of tasks to iterate in a single balance run.

 * Limited because this is done with IRQs disabled.

/*

 * period over which we measure -rt task CPU usage in us.

 * default: 1s

 kernel prio, less is more */

 trumps deadline */

 includes deadline */

 [-1, 99] */

 140 */

 120, squash fair */

/*

 * l(a,b)

 * le(a,b) := !l(b,a)

 * g(a,b)  := l(b,a)

 * ge(a,b) := !l(a,b)

 real prio, less is less */

 dl_prio() doesn't work because of stop_class above */

 fair */

 flip prio, so high prio is leftmost */

/*

 * Find left-most (aka, highest priority) task matching @cookie.

	/*

	 * The idle task always matches any cookie!

/*

 * Magic required such that:

 *

 *	raw_spin_rq_lock(rq);

 *	...

 *	raw_spin_rq_unlock(rq);

 *

 * ends up locking and unlocking the _same_ lock, and all CPUs

 * always agree on what rq has what lock.

 *

 * XXX entirely possible to selectively enable cores, don't bother for now.

	/*

	 * Toggle the online cores, one by one.

	/*

	 * Toggle the offline CPUs.

	/*

	 * Ensure all previous instances of raw_spin_rq_*lock() have finished

	 * and future ones will observe !sched_core_disabled().

	/*

	 * "There can be only one"

	 *

	 * Either this is the last one, or we don't actually need to do any

	 * 'work'. If it is the last *again*, we rely on

	 * WORK_STRUCT_PENDING_BIT.

 !CONFIG_SCHED_CORE */

 CONFIG_SCHED_CORE */

/*

 * part of the period that we allow rt tasks to run in us.

 * default: 0.95s

/*

 * Serialization rules:

 *

 * Lock order:

 *

 *   p->pi_lock

 *     rq->lock

 *       hrtimer_cpu_base->lock (hrtimer_start() for bandwidth controls)

 *

 *  rq1->lock

 *    rq2->lock  where: rq1 < rq2

 *

 * Regular state:

 *

 * Normal scheduling state is serialized by rq->lock. __schedule() takes the

 * local CPU's rq->lock, it optionally removes the task from the runqueue and

 * always looks at the local rq data structures to find the most eligible task

 * to run next.

 *

 * Task enqueue is also under rq->lock, possibly taken from another CPU.

 * Wakeups from another LLC domain might use an IPI to transfer the enqueue to

 * the local CPU to avoid bouncing the runqueue state around [ see

 * ttwu_queue_wakelist() ]

 *

 * Task wakeup, specifically wakeups that involve migration, are horribly

 * complicated to avoid having to take two rq->locks.

 *

 * Special state:

 *

 * System-calls and anything external will use task_rq_lock() which acquires

 * both p->pi_lock and rq->lock. As a consequence the state they change is

 * stable while holding either lock:

 *

 *  - sched_setaffinity()/

 *    set_cpus_allowed_ptr():	p->cpus_ptr, p->nr_cpus_allowed

 *  - set_user_nice():		p->se.load, p->*prio

 *  - __sched_setscheduler():	p->sched_class, p->policy, p->*prio,

 *				p->se.load, p->rt_priority,

 *				p->dl.dl_{runtime, deadline, period, flags, bw, density}

 *  - sched_setnuma():		p->numa_preferred_nid

 *  - sched_move_task()/

 *    cpu_cgroup_fork():	p->sched_task_group

 *  - uclamp_update_active()	p->uclamp*

 *

 * p->state <- TASK_*:

 *

 *   is changed locklessly using set_current_state(), __set_current_state() or

 *   set_special_state(), see their respective comments, or by

 *   try_to_wake_up(). This latter uses p->pi_lock to serialize against

 *   concurrent self.

 *

 * p->on_rq <- { 0, 1 = TASK_ON_RQ_QUEUED, 2 = TASK_ON_RQ_MIGRATING }:

 *

 *   is set by activate_task() and cleared by deactivate_task(), under

 *   rq->lock. Non-zero indicates the task is runnable, the special

 *   ON_RQ_MIGRATING state is used for migration without holding both

 *   rq->locks. It indicates task_cpu() is not stable, see task_rq_lock().

 *

 * p->on_cpu <- { 0, 1 }:

 *

 *   is set by prepare_task() and cleared by finish_task() such that it will be

 *   set before p is scheduled-in and cleared after p is scheduled-out, both

 *   under rq->lock. Non-zero indicates the task is running on its CPU.

 *

 *   [ The astute reader will observe that it is possible for two tasks on one

 *     CPU to have ->on_cpu = 1 at the same time. ]

 *

 * task_cpu(p): is changed by set_task_cpu(), the rules are:

 *

 *  - Don't call set_task_cpu() on a blocked task:

 *

 *    We don't care what CPU we're not running on, this simplifies hotplug,

 *    the CPU assignment of blocked tasks isn't required to be valid.

 *

 *  - for try_to_wake_up(), called under p->pi_lock:

 *

 *    This allows try_to_wake_up() to only take one rq->lock, see its comment.

 *

 *  - for migration called under rq->lock:

 *    [ see task_on_rq_migrating() in task_rq_lock() ]

 *

 *    o move_queued_task()

 *    o detach_task()

 *

 *  - for migration called under double_rq_lock():

 *

 *    o __migrate_swap_task()

 *    o push_rt_task() / pull_rt_task()

 *    o push_dl_task() / pull_dl_task()

 *    o dl_task_offline_migration()

 *

 Matches synchronize_rcu() in __sched_core_enable() */

 preempt_count *MUST* be > 1 */

 preempt_count *MUST* be > 1 */

 Matches synchronize_rcu() in __sched_core_enable() */

/*

 * double_rq_lock - safely lock two runqueues

/*

 * __task_rq_lock - lock the rq @p resides on.

/*

 * task_rq_lock - lock p->pi_lock and lock the rq @p resides on.

		/*

		 *	move_queued_task()		task_rq_lock()

		 *

		 *	ACQUIRE (rq->lock)

		 *	[S] ->on_rq = MIGRATING		[L] rq = task_rq()

		 *	WMB (__set_task_cpu())		ACQUIRE (rq->lock);

		 *	[S] ->cpu = new_cpu		[L] task_rq()

		 *					[L] ->on_rq

		 *	RELEASE (rq->lock)

		 *

		 * If we observe the old CPU in task_rq_lock(), the acquire of

		 * the old rq->lock will fully serialize against the stores.

		 *

		 * If we observe the new CPU in task_rq_lock(), the address

		 * dependency headed by '[L] rq = task_rq()' and the acquire

		 * will pair with the WMB to ensure we then also see migrating.

/*

 * RQ-clock updating methods:

/*

 * In theory, the compile should just see 0 here, and optimize out the call

 * to sched_rt_avg_update. But I don't trust it...

	/*

	 * Since irq_time is only updated on {soft,}irq_exit, we might run into

	 * this case when a previous update_rq_clock() happened inside a

	 * {soft,}irq region.

	 *

	 * When this happens, we stop ->clock_task and only update the

	 * prev_irq_time stamp to account for the part that fit, so that a next

	 * update will consume the rest. This ensures ->clock_task is

	 * monotonic.

	 *

	 * It does however cause some slight miss-attribution of {soft,}irq

	 * time, a more accurate solution would be to update the irq_time using

	 * the current rq->clock timestamp, except that would require using

	 * atomic ops.

/*

 * Use HR-timers to deliver accurate preemption points.

/*

 * High-resolution timer tick.

 * Runs from hardirq context with interrupts disabled.

/*

 * called from hardirq (IPI) context

/*

 * Called to set the hrtick timer state.

 *

 * called with rq->lock held and irqs disabled

	/*

	 * Don't schedule slices shorter than 10000ns, that just

	 * doesn't make sense and can cause timer DoS.

/*

 * Called to set the hrtick timer state.

 *

 * called with rq->lock held and irqs disabled

	/*

	 * Don't schedule slices shorter than 10000ns, that just

	 * doesn't make sense. Rely on vruntime for fairness.

 CONFIG_SMP */

 CONFIG_SCHED_HRTICK */

 CONFIG_SCHED_HRTICK */

/*

 * cmpxchg based fetch_or, macro so it works for different integer types

/*

 * Atomically set TIF_NEED_RESCHED and test for TIF_POLLING_NRFLAG,

 * this avoids any races wrt polling state changes and thereby avoids

 * spurious IPIs.

/*

 * Atomically set TIF_NEED_RESCHED if TIF_POLLING_NRFLAG is set.

 *

 * If this returns true, then the idle task promises to call

 * sched_ttwu_pending() and reschedule soon.

	/*

	 * Atomically grab the task, if ->wake_q is !nil already it means

	 * it's already queued (either by us or someone else) and will get the

	 * wakeup due to that.

	 *

	 * In order to ensure that a pending wakeup will observe our pending

	 * state, even in the failed case, an explicit smp_mb() must be used.

	/*

	 * The head is context local, there can be no concurrency.

/**

 * wake_q_add() - queue a wakeup for 'later' waking.

 * @head: the wake_q_head to add @task to

 * @task: the task to queue for 'later' wakeup

 *

 * Queue a task for later wakeup, most likely by the wake_up_q() call in the

 * same context, _HOWEVER_ this is not guaranteed, the wakeup can come

 * instantly.

 *

 * This function must be used as-if it were wake_up_process(); IOW the task

 * must be ready to be woken at this location.

/**

 * wake_q_add_safe() - safely queue a wakeup for 'later' waking.

 * @head: the wake_q_head to add @task to

 * @task: the task to queue for 'later' wakeup

 *

 * Queue a task for later wakeup, most likely by the wake_up_q() call in the

 * same context, _HOWEVER_ this is not guaranteed, the wakeup can come

 * instantly.

 *

 * This function must be used as-if it were wake_up_process(); IOW the task

 * must be ready to be woken at this location.

 *

 * This function is essentially a task-safe equivalent to wake_q_add(). Callers

 * that already hold reference to @task can call the 'safe' version and trust

 * wake_q to do the right thing depending whether or not the @task is already

 * queued for wakeup.

 Task can safely be re-inserted now: */

		/*

		 * wake_up_process() executes a full barrier, which pairs with

		 * the queueing in wake_q_add() so as not to miss wakeups.

/*

 * resched_curr - mark rq's current task 'to be rescheduled now'.

 *

 * On UP this means the setting of the need_resched flag, on SMP it

 * might also involve a cross-CPU call to trigger the scheduler on

 * the target CPU.

/*

 * In the semi idle case, use the nearest busy CPU for migrating timers

 * from an idle CPU.  This is good for power-savings.

 *

 * We don't do similar optimization for completely idle system, as

 * selecting an idle CPU will add more delays to the timers than intended

 * (as that CPU's timer base may not be uptodate wrt jiffies etc).

/*

 * When add_timer_on() enqueues a timer into the timer wheel of an

 * idle CPU then this timer might expire before the next timer event

 * which is scheduled to wake up that CPU. In case of a completely

 * idle system the next event might even be infinite time into the

 * future. wake_up_idle_cpu() ensures that the CPU is woken up and

 * leaves the inner idle loop so the newly added timer is taken into

 * account when the CPU goes back to idle and evaluates the timer

 * wheel for the next timer event.

	/*

	 * We just need the target to call irq_exit() and re-evaluate

	 * the next tick. The nohz full kick at least implies that.

	 * If needed we can still optimize that later with an

	 * empty IRQ.

 Don't try to wake offline CPUs. */

/*

 * Wake up the specified CPU.  If the CPU is going offline, it is the

 * caller's responsibility to deal with the lost wakeup, for example,

 * by hooking into the CPU_DEAD notifier like timers and hrtimers do.

	/*

	 * Release the rq::nohz_csd.

 CONFIG_NO_HZ_COMMON */

 Deadline tasks, even if single, need the tick */

	/*

	 * If there are more than one RR tasks, we need the tick to affect the

	 * actual RR behaviour.

	/*

	 * If there's no RR tasks, but FIFO tasks, we can skip the tick, no

	 * forced preemption between FIFO tasks.

	/*

	 * If there are no DL,RR/FIFO tasks, there must only be CFS tasks left;

	 * if there's more than one we need the tick for involuntary

	 * preemption.

 CONFIG_NO_HZ_FULL */

 CONFIG_SMP */

/*

 * Iterate task_group tree rooted at *from, calling @down when first entering a

 * node and @up when leaving it for the final time.

 *

 * Caller must hold rcu_lock or sufficient equivalent.

	/*

	 * SCHED_IDLE tasks get minimal weight:

	/*

	 * SCHED_OTHER tasks have to update their load when changing their

	 * weight

/*

 * Serializes updates of utilization clamp values

 *

 * The (slow-path) user-space triggers utilization clamp value updates which

 * can require updates on (fast-path) scheduler's data structures used to

 * support enqueue/dequeue operations.

 * While the per-CPU rq lock protects fast-path update operations, user-space

 * requests are serialized using a mutex to reduce the risk of conflicting

 * updates or API abuses.

 Max allowed minimum utilization */

 Max allowed maximum utilization */

/*

 * By default RT tasks run at the maximum performance point/capacity of the

 * system. Uclamp enforces this by always setting UCLAMP_MIN of RT tasks to

 * SCHED_CAPACITY_SCALE.

 *

 * This knob allows admins to change the default behavior when uclamp is being

 * used. In battery powered devices, particularly, running at the maximum

 * capacity and frequency will increase energy consumption and shorten the

 * battery life.

 *

 * This knob only affects RT tasks that their uclamp_se->user_defined == false.

 *

 * This knob will not override the system default sched_util_clamp_min defined

 * above.

 All clamps are required to be less or equal than these values */

/*

 * This static key is used to reduce the uclamp overhead in the fast path. It

 * primarily disables the call to uclamp_rq_{inc, dec}() in

 * enqueue/dequeue_task().

 *

 * This allows users to continue to enable uclamp in their kernel config with

 * minimum uclamp overhead in the fast path.

 *

 * As soon as userspace modifies any of the uclamp knobs, the static key is

 * enabled, since we have an actual users that make use of uclamp

 * functionality.

 *

 * The knobs that would enable this static key are:

 *

 *   * A task modifying its uclamp value with sched_setattr().

 *   * An admin modifying the sysctl_sched_uclamp_{min, max} via procfs.

 *   * An admin modifying the cgroup cpu.uclamp.{min, max}

 Integer rounded range for each bucket */

	/*

	 * Avoid blocked utilization pushing up the frequency when we go

	 * idle (which drops the max-clamp) by retaining the last known

	 * max-clamp.

 Reset max-clamp retention only on idle exit */

	/*

	 * Since both min and max clamps are max aggregated, find the

	 * top most bucket with tasks in.

 No tasks -- default clamp values */

 Only sync if user didn't override the default */

 Protect updates to p->uclamp_* */

	/*

	 * copy_process()			sysctl_uclamp

	 *					  uclamp_min_rt = X;

	 *   write_lock(&tasklist_lock)		  read_lock(&tasklist_lock)

	 *   // link thread			  smp_mb__after_spinlock()

	 *   write_unlock(&tasklist_lock)	  read_unlock(&tasklist_lock);

	 *   sched_post_fork()			  for_each_process_thread()

	 *     __uclamp_sync_rt()		    __uclamp_sync_rt()

	 *

	 * Ensures that either sched_post_fork() will observe the new

	 * uclamp_min_rt or for_each_process_thread() will observe the new

	 * task.

 Copy by value as we could modify it */

	/*

	 * Tasks in autogroups or root task group will be

	 * restricted by system defaults.

/*

 * The effective clamp bucket index of a task depends on, by increasing

 * priority:

 * - the task specific clamp value, when explicitly requested from userspace

 * - the task group effective clamp value, for tasks not either in the root

 *   group or in an autogroup

 * - the system default clamp value, defined by the sysadmin

 System default restrictions always apply */

 Task currently refcounted: use back-annotated (effective) value */

/*

 * When a task is enqueued on a rq, the clamp bucket currently defined by the

 * task's uclamp::bucket_id is refcounted on that rq. This also immediately

 * updates the rq's clamp value if required.

 *

 * Tasks can have a task-specific value requested from user-space, track

 * within each bucket the maximum value for tasks refcounted in it.

 * This "local max aggregation" allows to track the exact "requested" value

 * for each bucket when all its RUNNABLE tasks require the same clamp.

 Update task effective clamp */

	/*

	 * Local max aggregation: rq buckets always track the max

	 * "requested" clamp value of its RUNNABLE tasks.

/*

 * When a task is dequeued from a rq, the clamp bucket refcounted by the task

 * is released. If this is the last task reference counting the rq's max

 * active clamp value, then the rq's clamp value is updated.

 *

 * Both refcounted tasks and rq's cached clamp values are expected to be

 * always valid. If it's detected they are not, as defensive programming,

 * enforce the expected state and warn.

	/*

	 * If sched_uclamp_used was enabled after task @p was enqueued,

	 * we could end up with unbalanced call to uclamp_rq_dec_id().

	 *

	 * In this case the uc_se->active flag should be false since no uclamp

	 * accounting was performed at enqueue time and we can just return

	 * here.

	 *

	 * Need to be careful of the following enqueue/dequeue ordering

	 * problem too

	 *

	 *	enqueue(taskA)

	 *	// sched_uclamp_used gets enabled

	 *	enqueue(taskB)

	 *	dequeue(taskA)

	 *	// Must not decrement bucket->tasks here

	 *	dequeue(taskB)

	 *

	 * where we could end up with stale data in uc_se and

	 * bucket[uc_se->bucket_id].

	 *

	 * The following check here eliminates the possibility of such race.

	/*

	 * Keep "local max aggregation" simple and accept to (possibly)

	 * overboost some RUNNABLE tasks in the same bucket.

	 * The rq clamp bucket value is reset to its base value whenever

	 * there are no more RUNNABLE tasks refcounting it.

	/*

	 * Defensive programming: this should never happen. If it happens,

	 * e.g. due to future modification, warn and fixup the expected value.

	/*

	 * Avoid any overhead until uclamp is actually used by the userspace.

	 *

	 * The condition is constructed such that a NOP is generated when

	 * sched_uclamp_used is disabled.

 Reset clamp idle holding when there is one RUNNABLE task */

	/*

	 * Avoid any overhead until uclamp is actually used by the userspace.

	 *

	 * The condition is constructed such that a NOP is generated when

	 * sched_uclamp_used is disabled.

	/*

	 * Make sure to clear the idle flag if we've transiently reached 0

	 * active tasks on rq.

	/*

	 * Lock the task and the rq where the task is (or was) queued.

	 *

	 * We might lock the (previous) rq of a !RUNNABLE task, but that's the

	 * price to pay to safely serialize util_{min,max} updates with

	 * enqueues, dequeues and migration operations.

	 * This is the same locking schema used by __set_cpus_allowed_ptr().

	/*

	 * Setting the clamp bucket is serialized by task_rq_lock().

	 * If the task is not yet RUNNABLE and its task_struct is not

	 * affecting a valid clamp bucket, the next time it's enqueued,

	 * it will already see the updated clamp bucket value.

	/*

	 * We update all RUNNABLE tasks only when task groups are in use.

	 * Otherwise, keep it simple and do just a lazy update at each next

	 * task enqueue time.

	/*

	 * We have valid uclamp attributes; make sure uclamp is enabled.

	 *

	 * We need to do that here, because enabling static branches is a

	 * blocking operation which obviously cannot be done while holding

	 * scheduler locks.

 Reset on sched class change for a non user-defined clamp value. */

 Reset on sched_util_{min,max} == -1. */

		/*

		 * RT by default have a 100% boost value that could be modified

		 * at runtime.

	/*

	 * We don't need to hold task_rq_lock() when updating p->uclamp_* here

	 * as the task is still at its early fork stages.

 System defaults allow max clamp values for both indexes */

 CONFIG_UCLAMP_TASK */

 CONFIG_UCLAMP_TASK */

 Only get wchan if task is blocked and we can keep it that way. */

 see try_to_wake_up() */

/*

 * Calculate the expected normal priority: i.e. priority

 * without taking RT-inheritance into account. Might be

 * boosted by interactivity modifiers. Changes upon fork,

 * setprio syscalls, and whenever the interactivity

 * estimator recalculates.

/*

 * Calculate the current priority, i.e. the priority

 * taken into account by the scheduler. This value might

 * be boosted by RT tasks, or might be boosted by

 * interactivity modifiers. Will be RT if the task got

 * RT-boosted. If not then it returns p->normal_prio.

	/*

	 * If we are RT tasks or we were boosted to RT priority,

	 * keep the priority unchanged. Otherwise, update priority

	 * to the normal priority:

/**

 * task_curr - is this task currently executing on a CPU?

 * @p: the task in question.

 *

 * Return: 1 if the task is currently executing. 0 otherwise.

/*

 * switched_from, switched_to and prio_changed must _NOT_ drop rq->lock,

 * use the balance_callback list if you want balancing.

 *

 * this means any call to check_class_changed() must be followed by a call to

 * balance_callback().

	/*

	 * A queue event has occurred, and we're going to schedule.  In

	 * this case, we can save a useless back to back clock update.

	/*

	 * Violates locking rules! see comment in __do_set_cpus_allowed().

	/*

	 * Ensure stop_task runs either before or after this, and that

	 * __set_cpus_allowed_ptr(SCA_MIGRATE_ENABLE) doesn't schedule().

	/*

	 * Mustn't clear migration_disabled() until cpus_ptr points back at the

	 * regular cpus_mask, otherwise things that race (eg.

	 * select_fallback_rq) get confused.

/*

 * Per-CPU kthreads are allowed to run on !active && online CPUs, see

 * __set_cpus_allowed_ptr() and select_fallback_rq().

 When not in the task's cpumask, no point in looking further. */

 migrate_disabled() must be allowed to finish. */

 Non kernel threads are not allowed during either online or offline. */

 KTHREAD_IS_PER_CPU is always allowed. */

 Regular kernel threads don't get to stay during offline. */

 But are allowed during online. */

/*

 * This is how migration works:

 *

 * 1) we invoke migration_cpu_stop() on the target CPU using

 *    stop_one_cpu().

 * 2) stopper starts to run (implicitly forcing the migrated thread

 *    off the CPU)

 * 3) it checks whether the migrated task is still in the wrong runqueue.

 * 4) if it's in the wrong runqueue then the migration thread removes

 *    it and puts it into the right queue.

 * 5) stopper completes and stop_one_cpu() returns and the migration

 *    is done.

/*

 * move_queued_task - move a queued task to new rq.

 *

 * Returns (locked) new rq. Old rq's lock is released.

/*

 * @refs: number of wait_for_completion()

 * @stop_pending: is @stop_work in use

/*

 * Move (not current) task off this CPU, onto the destination CPU. We're doing

 * this because either it can't run here any more (set_cpus_allowed()

 * away from this CPU, or CPU going down), or because we're

 * attempting to rebalance this task on exec (sched_exec).

 *

 * So we race with normal scheduler movements, but that's OK, as long

 * as the task is no longer on this CPU.

 Affinity changed (again). */

/*

 * migration_cpu_stop - this will be executed by a highprio stopper thread

 * and performs thread migration by bumping thread off CPU then

 * 'pushing' onto another runqueue.

	/*

	 * The original target CPU might have gone down and we might

	 * be on another CPU but it doesn't matter.

	/*

	 * We need to explicitly wake pending tasks before running

	 * __migrate_task() such that we will not miss enforcing cpus_ptr

	 * during wakeups, see set_cpus_allowed_ptr()'s TASK_WAKING test.

	/*

	 * If we were passed a pending, then ->stop_pending was set, thus

	 * p->migration_pending must have remained stable.

	/*

	 * If task_rq(p) != rq, it cannot be migrated here, because we're

	 * holding rq->lock, if p->on_rq == 0 it cannot get enqueued because

	 * we're holding p->pi_lock.

		/*

		 * XXX __migrate_task() can fail, at which point we might end

		 * up running on a dodgy CPU, AFAICT this can only happen

		 * during CPU hotplug, at which point we'll get pushed out

		 * anyway, so it's probably not a big deal.

		/*

		 * This happens when we get migrated between migrate_enable()'s

		 * preempt_enable() and scheduling the stopper task. At that

		 * point we're a regular task again and not current anymore.

		 *

		 * A !PREEMPT kernel has a giant hole here, which makes it far

		 * more likely.

		/*

		 * The task moved before the stopper got to run. We're holding

		 * ->pi_lock, so the allowed mask is stable - if it got

		 * somewhere allowed, we're done.

		/*

		 * When migrate_enable() hits a rq mis-match we can't reliably

		 * determine is_migration_disabled() and so have to chase after

		 * it.

 XXX validate p is still the highest prio task

/*

 * sched_class::set_cpus_allowed must do the below, but is not required to

 * actually call this function.

	/*

	 * This here violates the locking rules for affinity, since we're only

	 * supposed to change these variables while holding both rq->lock and

	 * p->pi_lock.

	 *

	 * HOWEVER, it magically works, because ttwu() is the only code that

	 * accesses these variables under p->pi_lock and only does so after

	 * smp_cond_load_acquire(&p->on_cpu, !VAL), and we're in __schedule()

	 * before finish_task().

	 *

	 * XXX do further audits, this smells like something putrid.

		/*

		 * Because __kthread_bind() calls this on blocked tasks without

		 * holding rq->lock.

/*

 * This function is wildly self concurrent; here be dragons.

 *

 *

 * When given a valid mask, __set_cpus_allowed_ptr() must block until the

 * designated task is enqueued on an allowed CPU. If that task is currently

 * running, we have to kick it out using the CPU stopper.

 *

 * Migrate-Disable comes along and tramples all over our nice sandcastle.

 * Consider:

 *

 *     Initial conditions: P0->cpus_mask = [0, 1]

 *

 *     P0@CPU0                  P1

 *

 *     migrate_disable();

 *     <preempted>

 *                              set_cpus_allowed_ptr(P0, [1]);

 *

 * P1 *cannot* return from this set_cpus_allowed_ptr() call until P0 executes

 * its outermost migrate_enable() (i.e. it exits its Migrate-Disable region).

 * This means we need the following scheme:

 *

 *     P0@CPU0                  P1

 *

 *     migrate_disable();

 *     <preempted>

 *                              set_cpus_allowed_ptr(P0, [1]);

 *                                <blocks>

 *     <resumes>

 *     migrate_enable();

 *       __set_cpus_allowed_ptr();

 *       <wakes local stopper>

 *                         `--> <woken on migration completion>

 *

 * Now the fun stuff: there may be several P1-like tasks, i.e. multiple

 * concurrent set_cpus_allowed_ptr(P0, [*]) calls. CPU affinity changes of any

 * task p are serialized by p->pi_lock, which we can leverage: the one that

 * should come into effect at the end of the Migrate-Disable region is the last

 * one. This means we only need to track a single cpumask (i.e. p->cpus_mask),

 * but we still need to properly signal those waiting tasks at the appropriate

 * moment.

 *

 * This is implemented using struct set_affinity_pending. The first

 * __set_cpus_allowed_ptr() caller within a given Migrate-Disable region will

 * setup an instance of that struct and install it on the targeted task_struct.

 * Any and all further callers will reuse that instance. Those then wait for

 * a completion signaled at the tail of the CPU stopper callback (1), triggered

 * on the end of the Migrate-Disable region (i.e. outermost migrate_enable()).

 *

 *

 * (1) In the cases covered above. There is one more where the completion is

 * signaled within affine_move_task() itself: when a subsequent affinity request

 * occurs after the stopper bailed out due to the targeted task still being

 * Migrate-Disable. Consider:

 *

 *     Initial conditions: P0->cpus_mask = [0, 1]

 *

 *     CPU0		  P1				P2

 *     <P0>

 *       migrate_disable();

 *       <preempted>

 *                        set_cpus_allowed_ptr(P0, [1]);

 *                          <blocks>

 *     <migration/0>

 *       migration_cpu_stop()

 *         is_migration_disabled()

 *           <bails>

 *                                                       set_cpus_allowed_ptr(P0, [0, 1]);

 *                                                         <signal completion>

 *                          <awakes>

 *

 * Note that the above is safe vs a concurrent migrate_enable(), as any

 * pending affinity completion is preceded by an uninstallation of

 * p->migration_pending done with p->pi_lock held.

 Can the task run on the task's current CPU? If so, we're done */

		/*

		 * If there are pending waiters, but no pending stop_work,

		 * then complete now.

 serialized by p->pi_lock */

 Install the request */

			/*

			 * Affinity has changed, but we've already installed a

			 * pending. migration_cpu_stop() *must* see this, else

			 * we risk a completion of the pending despite having a

			 * task on a disallowed CPU.

			 *

			 * Serialized by p->pi_lock, so this is safe.

	/*

	 * - !MIGRATE_ENABLE:

	 *   we'll have installed a pending if there wasn't one already.

	 *

	 * - MIGRATE_ENABLE:

	 *   we're here because the current CPU isn't matching anymore,

	 *   the only way that can happen is because of a concurrent

	 *   set_cpus_allowed_ptr() call, which should then still be

	 *   pending completion.

	 *

	 * Either way, we really should have a @pending here.

		/*

		 * MIGRATE_ENABLE gets here because 'p == current', but for

		 * anything else we cannot do is_migration_disabled(), punt

		 * and have the stopper function handle it all race-free.

 No UaF, just an address */

	/*

	 * Block the original owner of &pending until all subsequent callers

	 * have seen the completion and decremented the refcount

 ARGH */

/*

 * Called with both p->pi_lock and rq->lock held; drops both before returning.

		/*

		 * Kernel threads are allowed on online && !active CPUs,

		 * however, during cpu-hot-unplug, even these might get pushed

		 * away if not KTHREAD_IS_PER_CPU.

		 *

		 * Specifically, migration_disabled() tasks must not fail the

		 * cpumask_any_and_distribute() pick below, esp. so on

		 * SCA_MIGRATE_ENABLE, otherwise we'll not call

		 * set_cpus_allowed_common() and actually reset p->cpus_ptr.

	/*

	 * Must re-check here, to close a race against __kthread_bind(),

	 * sched_setaffinity() is not guaranteed to observe the flag.

	/*

	 * Picking a ~random cpu helps in cases where we are changing affinity

	 * for groups of tasks (ie. cpuset), so that load balancing is not

	 * immediately required to distribute the tasks within their new mask.

/*

 * Change a given task's CPU affinity. Migrate the thread to a

 * proper CPU and schedule it away if the CPU it's executing on

 * is removed from the allowed bitmask.

 *

 * NOTE: the caller must have a valid reference to the task, the

 * task must not exit() & deallocate itself prematurely. The

 * call is not atomic; no spinlocks may be held.

/*

 * Change a given task's CPU affinity to the intersection of its current

 * affinity mask and @subset_mask, writing the resulting mask to @new_mask

 * and pointing @p->user_cpus_ptr to a copy of the old mask.

 * If the resulting mask is empty, leave the affinity unchanged and return

 * -EINVAL.

	/*

	 * Forcefully restricting the affinity of a deadline task is

	 * likely to cause problems, so fail and noisily override the

	 * mask entirely.

	/*

	 * We're about to butcher the task affinity, so keep track of what

	 * the user asked for in case we're able to restore it later on.

/*

 * Restrict the CPU affinity of task @p so that it is a subset of

 * task_cpu_possible_mask() and point @p->user_cpu_ptr to a copy of the

 * old affinity mask. If the resulting mask is empty, we warn and walk

 * up the cpuset hierarchy until we find a suitable mask.

	/*

	 * __migrate_task() can fail silently in the face of concurrent

	 * offlining of the chosen destination CPU, so take the hotplug

	 * lock to ensure that the migration succeeds.

	/*

	 * We failed to find a valid subset of the affinity mask for the

	 * task, so override it based on its cpuset hierarchy.

/*

 * Restore the affinity of a task @p which was previously restricted by a

 * call to force_compatible_cpus_allowed_ptr(). This will clear (and free)

 * @p->user_cpus_ptr.

 *

 * It is the caller's responsibility to serialise this with any calls to

 * force_compatible_cpus_allowed_ptr(@p).

	/*

	 * Try to restore the old affinity mask. If this fails, then

	 * we free the mask explicitly to avoid it being inherited across

	 * a subsequent fork().

	/*

	 * We should never call set_task_cpu() on a blocked task,

	 * ttwu() will sort out the placement.

	/*

	 * Migrating fair class task must have p->on_rq = TASK_ON_RQ_MIGRATING,

	 * because schedstat_wait_{start,end} rebase migrating task's wait_start

	 * time relying on p->on_rq.

	/*

	 * The caller should hold either p->pi_lock or rq->lock, when changing

	 * a task's CPU. ->pi_lock for waking tasks, rq->lock for runnable tasks.

	 *

	 * sched_move_task() holds both and thus holding either pins the cgroup,

	 * see task_group().

	 *

	 * Furthermore, all task_rq users should acquire both locks, see

	 * task_rq_lock().

	/*

	 * Clearly, migrating tasks to offline CPUs is a fairly daft thing.

		/*

		 * Task isn't running anymore; make it appear like we migrated

		 * it before it went to sleep. This means on wakeup we make the

		 * previous CPU our target instead of where it really is.

/*

 * Cross migrate two tasks

	/*

	 * These three tests are all lockless; this is OK since all of them

	 * will be re-checked with proper locks held further down the line.

 CONFIG_NUMA_BALANCING */

/*

 * wait_task_inactive - wait for a thread to unschedule.

 *

 * If @match_state is nonzero, it's the @p->state value just checked and

 * not expected to change.  If it changes, i.e. @p might have woken up,

 * then return zero.  When we succeed in waiting for @p to be off its CPU,

 * we return a positive number (its total switch count).  If a second call

 * a short while later returns the same number, the caller can be sure that

 * @p has remained unscheduled the whole time.

 *

 * The caller must ensure that the task *will* unschedule sometime soon,

 * else this function might spin for a *long* time. This function can't

 * be called with interrupts off, or it may introduce deadlock with

 * smp_call_function() if an IPI is sent by the same process we are

 * waiting to become inactive.

		/*

		 * We do the initial early heuristics without holding

		 * any task-queue locks at all. We'll only try to get

		 * the runqueue lock when things look like they will

		 * work out!

		/*

		 * If the task is actively running on another CPU

		 * still, just relax and busy-wait without holding

		 * any locks.

		 *

		 * NOTE! Since we don't hold any locks, it's not

		 * even sure that "rq" stays as the right runqueue!

		 * But we don't care, since "task_running()" will

		 * return false if the runqueue has changed and p

		 * is actually now running somewhere else!

		/*

		 * Ok, time to look more closely! We need the rq

		 * lock now, to be *sure*. If we're wrong, we'll

		 * just go back and repeat.

 sets MSB */

		/*

		 * If it changed from the expected state, bail out now.

		/*

		 * Was it really running after all now that we

		 * checked with the proper locks actually held?

		 *

		 * Oops. Go back and try again..

		/*

		 * It's not enough that it's not actively running,

		 * it must be off the runqueue _entirely_, and not

		 * preempted!

		 *

		 * So if it was still runnable (but just not actively

		 * running right now), it's preempted, and we should

		 * yield - it could be a while.

		/*

		 * Ahh, all good. It wasn't running, and it wasn't

		 * runnable, which means that it will never become

		 * running in the future either. We're all done!

/***

 * kick_process - kick a running thread to enter/exit the kernel

 * @p: the to-be-kicked thread

 *

 * Cause a process which is running on another CPU to enter

 * kernel-mode, without any delay. (to get signals handled.)

 *

 * NOTE: this function doesn't have to take the runqueue lock,

 * because all it wants to ensure is that the remote task enters

 * the kernel. If the IPI races and the task has been migrated

 * to another CPU then no harm is done and the purpose has been

 * achieved as well.

/*

 * ->cpus_ptr is protected by both rq->lock and p->pi_lock

 *

 * A few notes on cpu_active vs cpu_online:

 *

 *  - cpu_active must be a subset of cpu_online

 *

 *  - on CPU-up we allow per-CPU kthreads on the online && !active CPU,

 *    see __set_cpus_allowed_ptr(). At this point the newly online

 *    CPU isn't yet part of the sched domains, and balancing will not

 *    see it.

 *

 *  - on CPU-down we clear cpu_active() to mask the sched domains and

 *    avoid the load balancer to place new tasks on the to be removed

 *    CPU. Existing tasks will remain running there and will be taken

 *    off.

 *

 * This means that fallback selection must not select !active CPUs.

 * And can assume that any active CPU must be online. Conversely

 * select_task_rq() below may allow selection of !active CPUs in order

 * to satisfy the above rules.

	/*

	 * If the node that the CPU is on has been offlined, cpu_to_node()

	 * will return -1. There is no CPU on the node, and we should

	 * select the CPU on the other node.

 Look for allowed, online CPU in same node. */

 Any allowed, online CPU? */

 No more Mr. Nice Guy. */

			/*

			 * XXX When called from select_task_rq() we only

			 * hold p->pi_lock and again violate locking order.

			 *

			 * More yuck to audit.

		/*

		 * Don't tell them about moving exiting tasks or

		 * kernel threads (both mm NULL), since they never

		 * leave kernel.

/*

 * The caller (fork, wakeup) owns p->pi_lock, ->cpus_ptr is stable.

	/*

	 * In order not to call set_task_cpu() on a blocking task we need

	 * to rely on ttwu() to place the task on a valid ->cpus_ptr

	 * CPU.

	 *

	 * Since this is common to all placement strategies, this lives here.

	 *

	 * [ this allows ->select_task() to simply return task_cpu(p) and

	 *   not worry about this generic constraint ]

		/*

		 * Make it appear like a SCHED_FIFO task, its something

		 * userspace knows about and won't get confused about.

		 *

		 * Also, it will make PI more or less work without too

		 * much confusion -- but then, stop work should not

		 * rely on PI working anyway.

		/*

		 * The PI code calls rt_mutex_setprio() with ->pi_lock held to

		 * adjust the effective priority of a task. As a result,

		 * rt_mutex_setprio() can trigger (RT) balancing operations,

		 * which can then trigger wakeups of the stop thread to push

		 * around the current task.

		 *

		 * The stop task itself will never be part of the PI-chain, it

		 * never blocks, therefore that ->pi_lock recursion is safe.

		 * Tell lockdep about this by placing the stop->pi_lock in its

		 * own class.

		/*

		 * Reset it back to a normal scheduling class so that

		 * it can die in pieces.

 CONFIG_SMP */

 !CONFIG_SMP */

 CONFIG_SMP */

/*

 * Mark the task runnable and perform wakeup-preemption.

		/*

		 * Our task @p is fully woken up and running; so it's safe to

		 * drop the rq->lock, hereafter rq is only used for statistics.

/*

 * Consider @p being inside a wait loop:

 *

 *   for (;;) {

 *      set_current_state(TASK_UNINTERRUPTIBLE);

 *

 *      if (CONDITION)

 *         break;

 *

 *      schedule();

 *   }

 *   __set_current_state(TASK_RUNNING);

 *

 * between set_current_state() and schedule(). In this case @p is still

 * runnable, so all that needs doing is change p->state back to TASK_RUNNING in

 * an atomic manner.

 *

 * By taking task_rq(p)->lock we serialize against schedule(), if @p->on_rq

 * then schedule() must still happen and p->state can be changed to

 * TASK_RUNNING. Otherwise we lost the race, schedule() has happened, and we

 * need to do a full wakeup with enqueue.

 *

 * Returns: %true when the wakeup is done,

 *          %false otherwise.

 check_preempt_curr() may use rq clock */

	/*

	 * rq::ttwu_pending racy indication of out-standing wakeups.

	 * Races such that false-negatives are possible, since they

	 * are shorter lived that false-positives would be.

/*

 * Queue a task on the target CPUs wake_list and wake the CPU via IPI if

 * necessary. The wakee CPU on receipt of the IPI will queue the task

 * via sched_ttwu_wakeup() for activation so the wakee incurs the cost

 * of the wakeup instead of the waker.

 Else CPU is not idle, do nothing here: */

	/*

	 * Do not complicate things with the async wake_list while the CPU is

	 * in hotplug state.

	/*

	 * If the CPU does not share cache, then queue the task on the

	 * remote rqs wakelist to avoid accessing remote data.

	/*

	 * If the task is descheduling and the only running task on the

	 * CPU then use the wakelist to offload the task activation to

	 * the soon-to-be-idle CPU as the current CPU is likely busy.

	 * nr_running is checked to avoid unnecessary task stacking.

 Sync clocks across CPUs */

 !CONFIG_SMP */

 CONFIG_SMP */

/*

 * Invoked from try_to_wake_up() to check whether the task can be woken up.

 *

 * The caller holds p::pi_lock if p != current or has preemption

 * disabled when p == current.

 *

 * The rules of PREEMPT_RT saved_state:

 *

 *   The related locking code always holds p::pi_lock when updating

 *   p::saved_state, which means the code is fully serialized in both cases.

 *

 *   The lock wait and lock wakeups happen via TASK_RTLOCK_WAIT. No other

 *   bits set. This allows to distinguish all wakeup scenarios.

	/*

	 * Saved state preserves the task state across blocking on

	 * an RT lock.  If the state matches, set p::saved_state to

	 * TASK_RUNNING, but do not wake the task because it waits

	 * for a lock wakeup. Also indicate success because from

	 * the regular waker's point of view this has succeeded.

	 *

	 * After acquiring the lock the task will restore p::__state

	 * from p::saved_state which ensures that the regular

	 * wakeup is not lost. The restore will also set

	 * p::saved_state to TASK_RUNNING so any further tests will

	 * not result in false positives vs. @success

/*

 * Notes on Program-Order guarantees on SMP systems.

 *

 *  MIGRATION

 *

 * The basic program-order guarantee on SMP systems is that when a task [t]

 * migrates, all its activity on its old CPU [c0] happens-before any subsequent

 * execution on its new CPU [c1].

 *

 * For migration (of runnable tasks) this is provided by the following means:

 *

 *  A) UNLOCK of the rq(c0)->lock scheduling out task t

 *  B) migration for t is required to synchronize *both* rq(c0)->lock and

 *     rq(c1)->lock (if not at the same time, then in that order).

 *  C) LOCK of the rq(c1)->lock scheduling in task

 *

 * Release/acquire chaining guarantees that B happens after A and C after B.

 * Note: the CPU doing B need not be c0 or c1

 *

 * Example:

 *

 *   CPU0            CPU1            CPU2

 *

 *   LOCK rq(0)->lock

 *   sched-out X

 *   sched-in Y

 *   UNLOCK rq(0)->lock

 *

 *                                   LOCK rq(0)->lock // orders against CPU0

 *                                   dequeue X

 *                                   UNLOCK rq(0)->lock

 *

 *                                   LOCK rq(1)->lock

 *                                   enqueue X

 *                                   UNLOCK rq(1)->lock

 *

 *                   LOCK rq(1)->lock // orders against CPU2

 *                   sched-out Z

 *                   sched-in X

 *                   UNLOCK rq(1)->lock

 *

 *

 *  BLOCKING -- aka. SLEEP + WAKEUP

 *

 * For blocking we (obviously) need to provide the same guarantee as for

 * migration. However the means are completely different as there is no lock

 * chain to provide order. Instead we do:

 *

 *   1) smp_store_release(X->on_cpu, 0)   -- finish_task()

 *   2) smp_cond_load_acquire(!X->on_cpu) -- try_to_wake_up()

 *

 * Example:

 *

 *   CPU0 (schedule)  CPU1 (try_to_wake_up) CPU2 (schedule)

 *

 *   LOCK rq(0)->lock LOCK X->pi_lock

 *   dequeue X

 *   sched-out X

 *   smp_store_release(X->on_cpu, 0);

 *

 *                    smp_cond_load_acquire(&X->on_cpu, !VAL);

 *                    X->state = WAKING

 *                    set_task_cpu(X,2)

 *

 *                    LOCK rq(2)->lock

 *                    enqueue X

 *                    X->state = RUNNING

 *                    UNLOCK rq(2)->lock

 *

 *                                          LOCK rq(2)->lock // orders against CPU1

 *                                          sched-out Z

 *                                          sched-in X

 *                                          UNLOCK rq(2)->lock

 *

 *                    UNLOCK X->pi_lock

 *   UNLOCK rq(0)->lock

 *

 *

 * However, for wakeups there is a second guarantee we must provide, namely we

 * must ensure that CONDITION=1 done by the caller can not be reordered with

 * accesses to the task state; see try_to_wake_up() and set_current_state().

/**

 * try_to_wake_up - wake up a thread

 * @p: the thread to be awakened

 * @state: the mask of task states that can be woken

 * @wake_flags: wake modifier flags (WF_*)

 *

 * Conceptually does:

 *

 *   If (@state & @p->state) @p->state = TASK_RUNNING.

 *

 * If the task was not queued/runnable, also place it back on a runqueue.

 *

 * This function is atomic against schedule() which would dequeue the task.

 *

 * It issues a full memory barrier before accessing @p->state, see the comment

 * with set_current_state().

 *

 * Uses p->pi_lock to serialize against concurrent wake-ups.

 *

 * Relies on p->pi_lock stabilizing:

 *  - p->sched_class

 *  - p->cpus_ptr

 *  - p->sched_task_group

 * in order to do migration, see its use of select_task_rq()/set_task_cpu().

 *

 * Tries really hard to only take one task_rq(p)->lock for performance.

 * Takes rq->lock in:

 *  - ttwu_runnable()    -- old rq, unavoidable, see comment there;

 *  - ttwu_queue()       -- new rq, for enqueue of the task;

 *  - psi_ttwu_dequeue() -- much sadness :-( accounting will kill us.

 *

 * As a consequence we race really badly with just about everything. See the

 * many memory barriers and their comments for details.

 *

 * Return: %true if @p->state changes (an actual wakeup was done),

 *	   %false otherwise.

		/*

		 * We're waking current, this means 'p->on_rq' and 'task_cpu(p)

		 * == smp_processor_id()'. Together this means we can special

		 * case the whole 'p->on_rq && ttwu_runnable()' case below

		 * without taking any locks.

		 *

		 * In particular:

		 *  - we rely on Program-Order guarantees for all the ordering,

		 *  - we're serialized against set_special_state() by virtue of

		 *    it disabling IRQs (this allows not taking ->pi_lock).

	/*

	 * If we are going to wake up a thread waiting for CONDITION we

	 * need to ensure that CONDITION=1 done by the caller can not be

	 * reordered with p->state check below. This pairs with smp_store_mb()

	 * in set_current_state() that the waiting thread does.

	/*

	 * Ensure we load p->on_rq _after_ p->state, otherwise it would

	 * be possible to, falsely, observe p->on_rq == 0 and get stuck

	 * in smp_cond_load_acquire() below.

	 *

	 * sched_ttwu_pending()			try_to_wake_up()

	 *   STORE p->on_rq = 1			  LOAD p->state

	 *   UNLOCK rq->lock

	 *

	 * __schedule() (switch to task 'p')

	 *   LOCK rq->lock			  smp_rmb();

	 *   smp_mb__after_spinlock();

	 *   UNLOCK rq->lock

	 *

	 * [task p]

	 *   STORE p->state = UNINTERRUPTIBLE	  LOAD p->on_rq

	 *

	 * Pairs with the LOCK+smp_mb__after_spinlock() on rq->lock in

	 * __schedule().  See the comment for smp_mb__after_spinlock().

	 *

	 * A similar smb_rmb() lives in try_invoke_on_locked_down_task().

	/*

	 * Ensure we load p->on_cpu _after_ p->on_rq, otherwise it would be

	 * possible to, falsely, observe p->on_cpu == 0.

	 *

	 * One must be running (->on_cpu == 1) in order to remove oneself

	 * from the runqueue.

	 *

	 * __schedule() (switch to task 'p')	try_to_wake_up()

	 *   STORE p->on_cpu = 1		  LOAD p->on_rq

	 *   UNLOCK rq->lock

	 *

	 * __schedule() (put 'p' to sleep)

	 *   LOCK rq->lock			  smp_rmb();

	 *   smp_mb__after_spinlock();

	 *   STORE p->on_rq = 0			  LOAD p->on_cpu

	 *

	 * Pairs with the LOCK+smp_mb__after_spinlock() on rq->lock in

	 * __schedule().  See the comment for smp_mb__after_spinlock().

	 *

	 * Form a control-dep-acquire with p->on_rq == 0 above, to ensure

	 * schedule()'s deactivate_task() has 'happened' and p will no longer

	 * care about it's own p->state. See the comment in __schedule().

	/*

	 * We're doing the wakeup (@success == 1), they did a dequeue (p->on_rq

	 * == 0), which means we need to do an enqueue, change p->state to

	 * TASK_WAKING such that we can unlock p->pi_lock before doing the

	 * enqueue, such as ttwu_queue_wakelist().

	/*

	 * If the owning (remote) CPU is still in the middle of schedule() with

	 * this task as prev, considering queueing p on the remote CPUs wake_list

	 * which potentially sends an IPI instead of spinning on p->on_cpu to

	 * let the waker make forward progress. This is safe because IRQs are

	 * disabled and the IPI will deliver after on_cpu is cleared.

	 *

	 * Ensure we load task_cpu(p) after p->on_cpu:

	 *

	 * set_task_cpu(p, cpu);

	 *   STORE p->cpu = @cpu

	 * __schedule() (switch to task 'p')

	 *   LOCK rq->lock

	 *   smp_mb__after_spin_lock()		smp_cond_load_acquire(&p->on_cpu)

	 *   STORE p->on_cpu = 1		LOAD p->cpu

	 *

	 * to ensure we observe the correct CPU on which the task is currently

	 * scheduling.

	/*

	 * If the owning (remote) CPU is still in the middle of schedule() with

	 * this task as prev, wait until it's done referencing the task.

	 *

	 * Pairs with the smp_store_release() in finish_task().

	 *

	 * This ensures that tasks getting woken will be fully ordered against

	 * their previous state and preserve Program Order.

 CONFIG_SMP */

/**

 * task_call_func - Invoke a function on task in fixed state

 * @p: Process for which the function is to be invoked, can be @current.

 * @func: Function to invoke.

 * @arg: Argument to function.

 *

 * Fix the task in it's current state by avoiding wakeups and or rq operations

 * and call @func(@arg) on it.  This function can use ->on_rq and task_curr()

 * to work out what the state is, if required.  Given that @func can be invoked

 * with a runqueue lock held, it had better be quite lightweight.

 *

 * Returns:

 *   Whatever @func returns

	/*

	 * Ensure we load p->on_rq after p->__state, otherwise it would be

	 * possible to, falsely, observe p->on_rq == 0.

	 *

	 * See try_to_wake_up() for a longer comment.

	/*

	 * Since pi->lock blocks try_to_wake_up(), we don't need rq->lock when

	 * the task is blocked. Make sure to check @state since ttwu() can drop

	 * locks at the end, see ttwu_queue_wakelist().

	/*

	 * At this point the task is pinned; either:

	 *  - blocked and we're holding off wakeups	 (pi->lock)

	 *  - woken, and we're holding off enqueue	 (rq->lock)

	 *  - queued, and we're holding off schedule	 (rq->lock)

	 *  - running, and we're holding off de-schedule (rq->lock)

	 *

	 * The called function (@func) can use: task_curr(), p->on_rq and

	 * p->__state to differentiate between these states.

/**

 * wake_up_process - Wake up a specific process

 * @p: The process to be woken up.

 *

 * Attempt to wake up the nominated process and move it to the set of runnable

 * processes.

 *

 * Return: 1 if the process was woken up, 0 if it was already running.

 *

 * This function executes a full memory barrier before accessing the task state.

/*

 * Perform scheduler related setup for a newly forked process p.

 * p is forked by current.

 *

 * __sched_fork() is basic setup used by init_idle() too:

 Even if schedstat is disabled, there should not be garbage */

 CONFIG_PROC_SYSCTL */

 CONFIG_SCHEDSTATS */

/*

 * fork()/clone()-time setup:

	/*

	 * We mark the process as NEW here. This guarantees that

	 * nobody will actually run it, and a signal or other external

	 * event cannot wake it up and insert it on the runqueue either.

	/*

	 * Make sure we do not leak PI boosting priority to the child.

	/*

	 * Revert to default priority/policy on fork if requested.

		/*

		 * We don't need the reset flag anymore after the fork. It has

		 * fulfilled its duty:

	/*

	 * We're setting the CPU for the first time, we don't migrate,

	 * so use __set_task_cpu().

	/*

	 * Doing this here saves a lot of checks in all

	 * the calling paths, and returning zero seems

	 * safe for them anyway.

/*

 * wake_up_new_task - wake up a newly created task for the first time.

 *

 * This function will do some initial scheduler statistics housekeeping

 * that must be done for every newly created context, then puts the task

 * on the runqueue and wakes it.

	/*

	 * Fork balancing, do it here and not earlier because:

	 *  - cpus_ptr can change in the fork path

	 *  - any previously selected CPU might disappear through hotplug

	 *

	 * Use __set_task_cpu() to avoid calling sched_class::migrate_task_rq,

	 * as we're not fully set-up yet.

		/*

		 * Nothing relies on rq->lock after this, so it's fine to

		 * drop it.

/**

 * preempt_notifier_register - tell me when current is being preempted & rescheduled

 * @notifier: notifier struct to register

/**

 * preempt_notifier_unregister - no longer interested in preemption notifications

 * @notifier: notifier struct to unregister

 *

 * This is *not* safe to call from within a preemption notifier.

 !CONFIG_PREEMPT_NOTIFIERS */

 CONFIG_PREEMPT_NOTIFIERS */

	/*

	 * Claim the task as running, we do this before switching to it

	 * such that any running task will have this set.

	 *

	 * See the ttwu() WF_ON_CPU case and its ordering comment.

	/*

	 * This must be the very last reference to @prev from this CPU. After

	 * p->on_cpu is cleared, the task can be moved to a different CPU. We

	 * must ensure this doesn't happen until the switch is completely

	 * finished.

	 *

	 * In particular, the load of prev->state in finish_task_switch() must

	 * happen before this.

	 *

	 * Pairs with the smp_cond_load_acquire() in try_to_wake_up().

	/*

	 * Since the runqueue lock will be released by the next

	 * task (which is an invalid locking op but in the case

	 * of the scheduler it's an obvious special-case), so we

	 * do an early lockdep release here:

 this is a valid case when another task releases the spinlock */

	/*

	 * If we are tracking spinlock dependencies then we have to

	 * fix up the runqueue lock - which gets 'carried over' from

	 * prev into current:

/*

 * NOP if the arch has not defined these:

/**

 * prepare_task_switch - prepare to switch tasks

 * @rq: the runqueue preparing to switch

 * @prev: the current task that is being switched out

 * @next: the task we are going to switch to.

 *

 * This is called with the rq lock held and interrupts off. It must

 * be paired with a subsequent finish_task_switch after the context

 * switch.

 *

 * prepare_task_switch sets up locking and calls architecture specific

 * hooks.

/**

 * finish_task_switch - clean up after a task-switch

 * @prev: the thread we just switched away from.

 *

 * finish_task_switch must be called after the context switch, paired

 * with a prepare_task_switch call before the context switch.

 * finish_task_switch will reconcile locking set up by prepare_task_switch,

 * and do any other architecture-specific cleanup actions.

 *

 * Note that we may have delayed dropping an mm in context_switch(). If

 * so, we finish that here outside of the runqueue lock. (Doing it

 * with the lock held can cause deadlocks; see schedule() for

 * details.)

 *

 * The context switch have flipped the stack from under us and restored the

 * local variables which were saved when this task called schedule() in the

 * past. prev == current is still correct but we need to recalculate this_rq

 * because prev may have moved to another CPU.

	/*

	 * The previous task will have left us with a preempt_count of 2

	 * because it left us after:

	 *

	 *	schedule()

	 *	  preempt_disable();			// 1

	 *	  __schedule()

	 *	    raw_spin_lock_irq(&rq->lock)	// 2

	 *

	 * Also, see FORK_PREEMPT_COUNT.

	/*

	 * A task struct has one reference for the use as "current".

	 * If a task dies, then it sets TASK_DEAD in tsk->state and calls

	 * schedule one last time. The schedule call will never return, and

	 * the scheduled task must drop that reference.

	 *

	 * We must observe prev->state before clearing prev->on_cpu (in

	 * finish_task), otherwise a concurrent wakeup can get prev

	 * running on another CPU and we could rave with its RUNNING -> DEAD

	 * transition, resulting in a double drop.

	/*

	 * kmap_local_sched_out() is invoked with rq::lock held and

	 * interrupts disabled. There is no requirement for that, but the

	 * sched out code does not have an interrupt enabled section.

	 * Restoring the maps on sched in does not require interrupts being

	 * disabled either.

	/*

	 * When switching through a kernel thread, the loop in

	 * membarrier_{private,global}_expedited() may have observed that

	 * kernel thread and not issued an IPI. It is therefore possible to

	 * schedule between user->kernel->user threads without passing though

	 * switch_mm(). Membarrier requires a barrier after storing to

	 * rq->curr, before returning to userspace, so provide them here:

	 *

	 * - a full memory barrier for {PRIVATE,GLOBAL}_EXPEDITED, implicitly

	 *   provided by mmdrop(),

	 * - a sync_core for SYNC_CORE.

 Task is done with its stack. */

/**

 * schedule_tail - first thing a freshly forked thread must call.

 * @prev: the thread we just switched away from.

	/*

	 * New tasks start with FORK_PREEMPT_COUNT, see there and

	 * finish_task_switch() for details.

	 *

	 * finish_task_switch() will drop rq->lock() and lower preempt_count

	 * and the preempt_enable() will end up enabling preemption (on

	 * PREEMPT_COUNT kernels).

/*

 * context_switch - switch to the new MM and the new thread's register state.

	/*

	 * For paravirt, this is coupled with an exit in switch_to to

	 * combine the page table reload and the switch backend into

	 * one hypercall.

	/*

	 * kernel -> kernel   lazy + transfer active

	 *   user -> kernel   lazy + mmgrab() active

	 *

	 * kernel ->   user   switch + mmdrop() active

	 *   user ->   user   switch

 to kernel

 from user

 to user

		/*

		 * sys_membarrier() requires an smp_mb() between setting

		 * rq->curr / membarrier_switch_mm() and returning to userspace.

		 *

		 * The below provides this either through switch_mm(), or in

		 * case 'prev->active_mm == next->mm' through

		 * finish_task_switch()'s mmdrop().

 from kernel

 will mmdrop() in finish_task_switch(). */

 Here we just switch the register state and the stack. */

/*

 * nr_running and nr_context_switches:

 *

 * externally visible scheduler statistics: current number of runnable

 * threads, total number of context switches performed since bootup.

/*

 * Check if only the current task is running on the CPU.

 *

 * Caution: this function does not check that the caller has disabled

 * preemption, thus the result might have a time-of-check-to-time-of-use

 * race.  The caller is responsible to use it correctly, for example:

 *

 * - from a non-preemptible section (of course)

 *

 * - from a thread that is bound to a single CPU

 *

 * - in a loop with very short iterations (e.g. a polling loop)

/*

 * Consumers of these two interfaces, like for example the cpuidle menu

 * governor, are using nonsensical data. Preferring shallow idle state selection

 * for a CPU that has IO-wait which might not even end up running the task when

 * it does become runnable.

/*

 * IO-wait accounting, and how it's mostly bollocks (on SMP).

 *

 * The idea behind IO-wait account is to account the idle time that we could

 * have spend running if it were not for IO. That is, if we were to improve the

 * storage performance, we'd have a proportional reduction in IO-wait time.

 *

 * This all works nicely on UP, where, when a task blocks on IO, we account

 * idle time as IO-wait, because if the storage were faster, it could've been

 * running and we'd not be idle.

 *

 * This has been extended to SMP, by doing the same for each CPU. This however

 * is broken.

 *

 * Imagine for instance the case where two tasks block on one CPU, only the one

 * CPU will have IO-wait accounted, while the other has regular idle. Even

 * though, if the storage were faster, both could've ran at the same time,

 * utilising both CPUs.

 *

 * This means, that when looking globally, the current IO-wait accounting on

 * SMP is a lower bound, by reason of under accounting.

 *

 * Worse, since the numbers are provided per CPU, they are sometimes

 * interpreted per CPU, and that is nonsensical. A blocked task isn't strictly

 * associated with any one particular CPU, it can wake to another CPU than it

 * blocked on. This means the per CPU IO-wait number is meaningless.

 *

 * Task CPU affinities can make all that even more 'interesting'.

/*

 * sched_exec - execve() is a valuable balancing opportunity, because at

 * this point the task has the smallest effective memory and cache footprint.

/*

 * The function fair_sched_class.update_curr accesses the struct curr

 * and its field curr->exec_start; when called from task_sched_runtime(),

 * we observe a high rate of cache misses in practice.

 * Prefetching this data results in improved performance.

/*

 * Return accounted runtime for the task.

 * In case the task is currently running, return the runtime plus current's

 * pending runtime that have not been accounted yet.

	/*

	 * 64-bit doesn't need locks to atomically read a 64-bit value.

	 * So we have a optimization chance when the task's delta_exec is 0.

	 * Reading ->on_cpu is racy, but this is ok.

	 *

	 * If we race with it leaving CPU, we'll take a lock. So we're correct.

	 * If we race with it entering CPU, unaccounted time is 0. This is

	 * indistinguishable from the read occurring a few cycles earlier.

	 * If we see ->on_cpu without ->on_rq, the task is leaving, and has

	 * been accounted, so we're correct here as well.

	/*

	 * Must be ->curr _and_ ->on_rq.  If dequeued, we would

	 * project cycles that may never be accounted to this

	 * thread, breaking clock_gettime().

 CONFIG_SCHED_DEBUG */

/*

 * This function gets called by the timer code, with HZ frequency.

 * We call it with interrupts disabled.

 Values for ->state, see diagram below. */

/*

 * State diagram for ->state:

 *

 *

 *          TICK_SCHED_REMOTE_OFFLINE

 *                    |   ^

 *                    |   |

 *                    |   | sched_tick_remote()

 *                    |   |

 *                    |   |

 *                    +--TICK_SCHED_REMOTE_OFFLINING

 *                    |   ^

 *                    |   |

 * sched_tick_start() |   | sched_tick_stop()

 *                    |   |

 *                    V   |

 *          TICK_SCHED_REMOTE_RUNNING

 *

 *

 * Other transitions get WARN_ON_ONCE(), except that sched_tick_remote()

 * and sched_tick_start() are happy to leave the state in RUNNING.

	/*

	 * Handle the tick only if it appears the remote CPU is running in full

	 * dynticks mode. The check is racy by nature, but missing a tick or

	 * having one too much is no big deal because the scheduler tick updates

	 * statistics and checks timeslices in a time-independent way, regardless

	 * of when exactly it is running.

		/*

		 * Make sure the next tick runs within a reasonable

		 * amount of time.

	/*

	 * Run the remote tick once per second (1Hz). This arbitrary

	 * frequency is large enough to avoid overload but short enough

	 * to keep scheduler internal stats reasonably up to date.  But

	 * first update state to reflect hotplug activity if required.

 There cannot be competing actions, but don't rely on stop-machine. */

 Don't cancel, as this would mess up the state machine. */

 CONFIG_HOTPLUG_CPU */

 !CONFIG_NO_HZ_FULL */

/*

 * If the value passed in is equal to the current preempt count

 * then we just disabled preemption. Start timing the latency.

	/*

	 * Underflow?

	/*

	 * Spinlock count overflowing soon?

/*

 * If the value passed in equals to the current preempt count

 * then we just enabled preemption. Stop timing the latency.

	/*

	 * Underflow?

	/*

	 * Is the spinlock portion underflowing?

/*

 * Print scheduling while atomic bug:

 Save this before calling printk(), since that will clobber it */

/*

 * Various schedule()-time debugging checks and statistics:

	/*

	 * We must do the balancing pass before put_prev_task(), such

	 * that when we release the rq->lock the task is in the same

	 * state as before we took rq->lock.

	 *

	 * We can terminate the balance pass as soon as we know there is

	 * a runnable task of @class priority or higher.

/*

 * Pick up the highest-prio task:

	/*

	 * Optimization: we know that if all tasks are in the fair class we can

	 * call that function directly, but only if the @prev task wasn't of a

	 * higher scheduling class, because otherwise those lose the

	 * opportunity to pull in more work from other CPUs.

 Assume the next prioritized class is idle_sched_class */

 The idle class should always have a runnable task. */

 The idle class should always have a runnable task. */

 Stopper task is switching into idle, no need core-wide selection. */

		/*

		 * Reset core_pick so that we don't enter the fastpath when

		 * coming online. core_pick would already be migrated to

		 * another cpu during offline.

	/*

	 * If there were no {en,de}queues since we picked (IOW, the task

	 * pointers are all still valid), and we haven't scheduled the last

	 * pick yet, do so now.

	 *

	 * rq->core_pick can be NULL if no selection was made for a CPU because

	 * it was either offline or went offline during a sibling's core-wide

	 * selection. In this case, do a core-wide selection.

 reset state */

	/*

	 * core->core_task_seq, core->core_pick_seq, rq->core_sched_seq

	 *

	 * @task_seq guards the task state ({en,de}queues)

	 * @pick_seq is the @task_seq we did a selection on

	 * @sched_seq is the @pick_seq we scheduled

	 *

	 * However, preemptions can cause multiple picks on the same task set.

	 * 'Fix' this by also increasing @task_seq for every pick.

	/*

	 * Optimize for common case where this CPU has no cookies

	 * and there are no cookied tasks running on siblings.

			/*

			 * For robustness, update the min_vruntime_fi for

			 * unconstrained picks as well.

	/*

	 * For each thread: do the regular task pick and find the max prio task

	 * amongst them.

	 *

	 * Tie-break prio towards the current CPU

	/*

	 * For each thread: try and find a runnable task that matches @max or

	 * force idle.

 Something should have been selected for current CPU */

	/*

	 * Reschedule siblings

	 *

	 * NOTE: L1TF -- at this point we're no longer running the old task and

	 * sending an IPI (below) ensures the sibling will no longer be running

	 * their task. This ensures there is no inter-sibling overlap between

	 * non-matching user state.

		/*

		 * An online sibling might have gone offline before a task

		 * could be picked for it, or it might be offline but later

		 * happen to come online, but its too late and nothing was

		 * picked for it.  That's Ok - it will pick tasks for itself,

		 * so ignore it.

		/*

		 * Update for new !FI->FI transitions, or if continuing to be in !FI:

		 * fi_before     fi      update?

		 *  0            0       1

		 *  0            1       1

		 *  1            0       1

		 *  1            1       0

 Did we break L1TF mitigation requirements? */

 not forced idle */

 if we're the first, we'll be our own leader */

 find the leader */

 whoopsie */

 install and validate core_rq */

 if we're the last man standing, nothing to do */

 if we're not the leader, nothing to do */

 find a new leader */

 impossible */

 copy the shared state to the new leader */

 install new leader */

 !CONFIG_SCHED_CORE */

 CONFIG_SCHED_CORE */

/*

 * Constants for the sched_mode argument of __schedule().

 *

 * The mode argument allows RT enabled kernels to differentiate a

 * preemption from blocking on an 'sleeping' spin/rwlock. Note that

 * SM_MASK_PREEMPT for !RT has all bits set, which allows the compiler to

 * optimize the AND operation out and just check for zero.

/*

 * __schedule() is the main scheduler function.

 *

 * The main means of driving the scheduler and thus entering this function are:

 *

 *   1. Explicit blocking: mutex, semaphore, waitqueue, etc.

 *

 *   2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return

 *      paths. For example, see arch/x86/entry_64.S.

 *

 *      To drive preemption between tasks, the scheduler sets the flag in timer

 *      interrupt handler scheduler_tick().

 *

 *   3. Wakeups don't really cause entry into schedule(). They add a

 *      task to the run-queue and that's it.

 *

 *      Now, if the new task added to the run-queue preempts the current

 *      task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets

 *      called on the nearest possible occasion:

 *

 *       - If the kernel is preemptible (CONFIG_PREEMPTION=y):

 *

 *         - in syscall or exception context, at the next outmost

 *           preempt_enable(). (this might be as soon as the wake_up()'s

 *           spin_unlock()!)

 *

 *         - in IRQ context, return from interrupt-handler to

 *           preemptible context

 *

 *       - If the kernel is not preemptible (CONFIG_PREEMPTION is not set)

 *         then at the next:

 *

 *          - cond_resched() call

 *          - explicit schedule() call

 *          - return from syscall or exception to user-space

 *          - return from interrupt-handler to user-space

 *

 * WARNING: must be called with preemption disabled!

	/*

	 * Make sure that signal_pending_state()->signal_pending() below

	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)

	 * done by the caller to avoid the race with signal_wake_up():

	 *

	 * __set_current_state(@state)		signal_wake_up()

	 * schedule()				  set_tsk_thread_flag(p, TIF_SIGPENDING)

	 *					  wake_up_state(p, state)

	 *   LOCK rq->lock			    LOCK p->pi_state

	 *   smp_mb__after_spinlock()		    smp_mb__after_spinlock()

	 *     if (signal_pending_state())	    if (p->state & @state)

	 *

	 * Also, the membarrier system call requires a full memory barrier

	 * after coming from user-space, before storing to rq->curr.

 Promote REQ to ACT */

	/*

	 * We must load prev->state once (task_struct::state is volatile), such

	 * that:

	 *

	 *  - we form a control dependency vs deactivate_task() below.

	 *  - ptrace_{,un}freeze_traced() can change ->state underneath us.

			/*

			 * __schedule()			ttwu()

			 *   prev_state = prev->state;    if (p->on_rq && ...)

			 *   if (prev_state)		    goto out;

			 *     p->on_rq = 0;		  smp_acquire__after_ctrl_dep();

			 *				  p->state = TASK_WAKING

			 *

			 * Where __schedule() and ttwu() have matching control dependencies.

			 *

			 * After this, schedule() must not care about p->state any more.

		/*

		 * RCU users of rcu_dereference(rq->curr) may not see

		 * changes to task_struct made by pick_next_task().

		/*

		 * The membarrier system call requires each architecture

		 * to have a full memory barrier after updating

		 * rq->curr, before returning to user-space.

		 *

		 * Here are the schemes providing that barrier on the

		 * various architectures:

		 * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC.

		 *   switch_mm() rely on membarrier_arch_switch_mm() on PowerPC.

		 * - finish_lock_switch() for weakly-ordered

		 *   architectures where spin_unlock is a full barrier,

		 * - switch_to() for arm64 (weakly-ordered, spin_unlock

		 *   is a RELEASE barrier),

 Also unlocks the rq: */

 Causes final put_task_struct in finish_task_switch(): */

 Tell freezer to ignore us: */

 Avoid "noreturn function does return" - but don't continue if BUG() is a NOP: */

	/*

	 * If a worker goes to sleep, notify and ask workqueue whether it

	 * wants to wake up a task to maintain concurrency.

	/*

	 * If we are going to sleep and we have plugged IO queued,

	 * make sure to submit it to avoid deadlocks.

/*

 * synchronize_rcu_tasks() makes sure that no task is stuck in preempted

 * state (have scheduled out non-voluntarily) by making sure that all

 * tasks have either left the run queue or have gone into user space.

 * As idle tasks do not do either, they must not ever be preempted

 * (schedule out non-voluntarily).

 *

 * schedule_idle() is similar to schedule_preempt_disable() except that it

 * never enables preemption because it does not call sched_submit_work().

	/*

	 * As this skips calling sched_submit_work(), which the idle task does

	 * regardless because that function is a nop when the task is in a

	 * TASK_RUNNING state, make sure this isn't used someplace that the

	 * current task can be in any other state. Note, idle is always in the

	 * TASK_RUNNING state.

	/*

	 * If we come here after a random call to set_need_resched(),

	 * or we have been woken up remotely but the IPI has not yet arrived,

	 * we haven't yet exited the RCU idle mode. Do it here manually until

	 * we find a better solution.

	 *

	 * NB: There are buggy callers of this function.  Ideally we

	 * should warn if prev_state != CONTEXT_USER, but that will trigger

	 * too frequently to make sense yet.

/**

 * schedule_preempt_disabled - called with preemption disabled

 *

 * Returns with preemption disabled. Note: preempt_count must be 1

		/*

		 * Because the function tracer can trace preempt_count_sub()

		 * and it also uses preempt_enable/disable_notrace(), if

		 * NEED_RESCHED is set, the preempt_enable_notrace() called

		 * by the function tracer will call this function again and

		 * cause infinite recursion.

		 *

		 * Preemption must be disabled here before the function

		 * tracer can trace. Break up preempt_disable() into two

		 * calls. One to disable preemption without fear of being

		 * traced. The other to still record the preemption latency,

		 * which can also be traced by the function tracer.

		/*

		 * Check again in case we missed a preemption opportunity

		 * between schedule and now.

/*

 * This is the entry point to schedule() from in-kernel preemption

 * off of preempt_enable.

	/*

	 * If there is a non-zero preempt_count or interrupts are disabled,

	 * we do not want to preempt the current task. Just return..

/**

 * preempt_schedule_notrace - preempt_schedule called by tracing

 *

 * The tracing infrastructure uses preempt_enable_notrace to prevent

 * recursion and tracing preempt enabling caused by the tracing

 * infrastructure itself. But as tracing can happen in areas coming

 * from userspace or just about to enter userspace, a preempt enable

 * can occur before user_exit() is called. This will cause the scheduler

 * to be called when the system is still in usermode.

 *

 * To prevent this, the preempt_enable_notrace will use this function

 * instead of preempt_schedule() to exit user context if needed before

 * calling the scheduler.

		/*

		 * Because the function tracer can trace preempt_count_sub()

		 * and it also uses preempt_enable/disable_notrace(), if

		 * NEED_RESCHED is set, the preempt_enable_notrace() called

		 * by the function tracer will call this function again and

		 * cause infinite recursion.

		 *

		 * Preemption must be disabled here before the function

		 * tracer can trace. Break up preempt_disable() into two

		 * calls. One to disable preemption without fear of being

		 * traced. The other to still record the preemption latency,

		 * which can also be traced by the function tracer.

		/*

		 * Needs preempt disabled in case user_exit() is traced

		 * and the tracer calls preempt_enable_notrace() causing

		 * an infinite recursion.

 CONFIG_PREEMPTION */

/*

 * SC:cond_resched

 * SC:might_resched

 * SC:preempt_schedule

 * SC:preempt_schedule_notrace

 * SC:irqentry_exit_cond_resched

 *

 *

 * NONE:

 *   cond_resched               <- __cond_resched

 *   might_resched              <- RET0

 *   preempt_schedule           <- NOP

 *   preempt_schedule_notrace   <- NOP

 *   irqentry_exit_cond_resched <- NOP

 *

 * VOLUNTARY:

 *   cond_resched               <- __cond_resched

 *   might_resched              <- __cond_resched

 *   preempt_schedule           <- NOP

 *   preempt_schedule_notrace   <- NOP

 *   irqentry_exit_cond_resched <- NOP

 *

 * FULL:

 *   cond_resched               <- RET0

 *   might_resched              <- RET0

 *   preempt_schedule           <- preempt_schedule

 *   preempt_schedule_notrace   <- preempt_schedule_notrace

 *   irqentry_exit_cond_resched <- irqentry_exit_cond_resched

	/*

	 * Avoid {NONE,VOLUNTARY} -> FULL transitions from ever ending up in

	 * the ZERO state, which is invalid.

 Default static call setting, nothing to do */

 !CONFIG_PREEMPT_DYNAMIC */

 #ifdef CONFIG_PREEMPT_DYNAMIC */

/*

 * This is the entry point to schedule() from kernel preemption

 * off of irq context.

 * Note, that this is called and return with irqs disabled. This will

 * protect us against recursive calling from irq.

 Catch callers which need to be fixed */

/*

 * rt_mutex_setprio - set the current priority of a task

 * @p: task to boost

 * @pi_task: donor task

 *

 * This function changes the 'effective' priority of a task. It does

 * not touch ->normal_prio like __setscheduler().

 *

 * Used by the rt_mutex code to implement priority inheritance

 * logic. Call site only calls if the priority of the task changed.

 XXX used to be waiter->prio, not waiter->task->prio */

	/*

	 * If nothing changed; bail early.

	/*

	 * Set under pi_lock && rq->lock, such that the value can be used under

	 * either lock.

	 *

	 * Note that there is loads of tricky to make this pointer cache work

	 * right. rt_mutex_slowunlock()+rt_mutex_postunlock() work together to

	 * ensure a task is de-boosted (pi_task is set to NULL) before the

	 * task is allowed to run again (and can exit). This ensures the pointer

	 * points to a blocked task -- which guarantees the task is present.

	/*

	 * For FIFO/RR we only need to set prio, if that matches we're done.

	/*

	 * Idle task boosting is a nono in general. There is one

	 * exception, when PREEMPT_RT and NOHZ is active:

	 *

	 * The idle task calls get_next_timer_interrupt() and holds

	 * the timer wheel base->lock on the CPU and another CPU wants

	 * to access the timer (probably to cancel it). We can safely

	 * ignore the boosting request, as the idle CPU runs this code

	 * with interrupts disabled and will complete the lock

	 * protected section without being interrupted. So there is no

	 * real need to boost.

	/*

	 * Boosting condition are:

	 * 1. -rt task is running and holds mutex A

	 *      --> -dl task blocks on mutex A

	 *

	 * 2. -dl task is running and holds mutex A

	 *      --> -dl task blocks on mutex A and could preempt the

	 *          running task

 Avoid rq from going away on us: */

	/*

	 * We have to be careful, if called from sys_setpriority(),

	 * the task might be in the middle of scheduling on another CPU.

	/*

	 * The RT priorities are set via sched_setscheduler(), but we still

	 * allow the 'normal' nice value to be set - but as expected

	 * it won't have any effect on scheduling until the task is

	 * SCHED_DEADLINE, SCHED_FIFO or SCHED_RR:

	/*

	 * If the task increased its priority or is running and

	 * lowered its priority, then reschedule its CPU:

/*

 * can_nice - check if a task can reduce its nice value

 * @p: task

 * @nice: nice value

 Convert nice value [19,-20] to rlimit style value [1,40]: */

/*

 * sys_nice - change the priority of the current process.

 * @increment: priority increment

 *

 * sys_setpriority is a more generic, but much slower function that

 * does similar things.

	/*

	 * Setpriority might change our priority at the same moment.

	 * We don't have to worry. Conceptually one call occurs first

	 * and we have a single winner.

/**

 * task_prio - return the priority value of a given task.

 * @p: the task in question.

 *

 * Return: The priority value as seen by users in /proc.

 *

 * sched policy         return value   kernel prio    user prio/nice

 *

 * normal, batch, idle     [0 ... 39]  [100 ... 139]          0/[-20 ... 19]

 * fifo, rr             [-2 ... -100]     [98 ... 0]  [1 ... 99]

 * deadline                     -101             -1           0

/**

 * idle_cpu - is a given CPU idle currently?

 * @cpu: the processor in question.

 *

 * Return: 1 if the CPU is currently idle. 0 otherwise.

/**

 * available_idle_cpu - is a given CPU idle for enqueuing work.

 * @cpu: the CPU in question.

 *

 * Return: 1 if the CPU is currently idle. 0 otherwise.

/**

 * idle_task - return the idle task for a given CPU.

 * @cpu: the processor in question.

 *

 * Return: The idle task for the CPU @cpu.

/*

 * This function computes an effective utilization for the given CPU, to be

 * used for frequency selection given the linear relation: f = u * f_max.

 *

 * The scheduler tracks the following metrics:

 *

 *   cpu_util_{cfs,rt,dl,irq}()

 *   cpu_bw_dl()

 *

 * Where the cfs,rt and dl util numbers are tracked with the same metric and

 * synchronized windows and are thus directly comparable.

 *

 * The cfs,rt,dl utilization are the running times measured with rq->clock_task

 * which excludes things like IRQ and steal-time. These latter are then accrued

 * in the irq utilization.

 *

 * The DL bandwidth number otoh is not a measured metric but a value computed

 * based on the task model parameters and gives the minimal utilization

 * required to meet deadlines.

	/*

	 * Early check to see if IRQ/steal time saturates the CPU, can be

	 * because of inaccuracies in how we track these -- see

	 * update_irq_load_avg().

	/*

	 * Because the time spend on RT/DL tasks is visible as 'lost' time to

	 * CFS tasks and we use the same metric to track the effective

	 * utilization (PELT windows are synchronized) we can directly add them

	 * to obtain the CPU's actual utilization.

	 *

	 * CFS and RT utilization can be boosted or capped, depending on

	 * utilization clamp constraints requested by currently RUNNABLE

	 * tasks.

	 * When there are no CFS RUNNABLE tasks, clamps are released and

	 * frequency will be gracefully reduced with the utilization decay.

	/*

	 * For frequency selection we do not make cpu_util_dl() a permanent part

	 * of this sum because we want to use cpu_bw_dl() later on, but we need

	 * to check if the CFS+RT+DL sum is saturated (ie. no idle time) such

	 * that we select f_max when there is no idle time.

	 *

	 * NOTE: numerical errors or stop class might cause us to not quite hit

	 * saturation when we should -- something for later.

	/*

	 * OTOH, for energy computation we need the estimated running time, so

	 * include util_dl and ignore dl_bw.

	/*

	 * There is still idle time; further improve the number by using the

	 * irq metric. Because IRQ/steal time is hidden from the task clock we

	 * need to scale the task numbers:

	 *

	 *              max - irq

	 *   U' = irq + --------- * U

	 *                 max

	/*

	 * Bandwidth required by DEADLINE must always be granted while, for

	 * FAIR and RT, we use blocked utilization of IDLE CPUs as a mechanism

	 * to gracefully reduce the frequency when no tasks show up for longer

	 * periods of time.

	 *

	 * Ideally we would like to set bw_dl as min/guaranteed freq and util +

	 * bw_dl as requested freq. However, cpufreq is not yet ready for such

	 * an interface. So, we only do the latter for now.

 CONFIG_SMP */

/**

 * find_process_by_pid - find a process with a matching PID value.

 * @pid: the pid in question.

 *

 * The task of @pid, if found. %NULL otherwise.

/*

 * sched_setparam() passes in -1 for its policy, to let the functions

 * it calls know not to change it.

	/*

	 * __sched_setscheduler() ensures attr->sched_priority == 0 when

	 * !rt_policy. Always setting this ensures that things like

	 * getparam()/getattr() don't report silly values for !rt tasks.

/*

 * Check the target process has a UID that matches the current process's:

 The pi code expects interrupts enabled */

 Double check policy once rq lock held: */

	/*

	 * Valid priorities for SCHED_FIFO and SCHED_RR are

	 * 1..MAX_RT_PRIO-1, valid priority for SCHED_NORMAL,

	 * SCHED_BATCH and SCHED_IDLE is 0.

	/*

	 * Allow unprivileged RT tasks to decrease priority:

 Can't set/change the rt policy: */

 Can't increase priority: */

		 /*

		  * Can't set/change SCHED_DEADLINE policy at all for now

		  * (safest behavior); in the future we would like to allow

		  * unprivileged DL tasks to increase their relative deadline

		  * or reduce their runtime (both ways reducing utilization)

		/*

		 * Treat SCHED_IDLE as nice 20. Only allow a switch to

		 * SCHED_NORMAL if the RLIMIT_NICE would normally permit it.

 Can't change other user's priorities: */

 Normal users shall not reset the sched_reset_on_fork flag: */

 Update task specific "requested" clamps */

	/*

	 * Make sure no PI-waiters arrive (or leave) while we are

	 * changing the priority of the task:

	 *

	 * To be able to change p->policy safely, the appropriate

	 * runqueue lock must be held.

	/*

	 * Changing the policy of the stop threads its a very bad idea:

	/*

	 * If not changing anything there's no need to proceed further,

	 * but store a possible modification of reset_on_fork.

		/*

		 * Do not allow realtime tasks into groups that have no runtime

		 * assigned.

			/*

			 * Don't allow tasks with an affinity mask smaller than

			 * the entire root_domain to become SCHED_DEADLINE. We

			 * will also fail if there's no bandwidth available.

 Re-check policy now with rq lock held: */

	/*

	 * If setscheduling to SCHED_DEADLINE (or changing the parameters

	 * of a SCHED_DEADLINE task) we need to check if enough bandwidth

	 * is available.

		/*

		 * Take priority boosted tasks into account. If the new

		 * effective priority is unchanged, we just store the new

		 * normal parameters and do not touch the scheduler class and

		 * the runqueue. This will be done when the task deboost

		 * itself.

		/*

		 * We enqueue to tail when the priority of a task is

		 * increased (user space view).

 Avoid rq from going away on us: */

 Run balance callbacks after we've adjusted the PI chain: */

 Fixup the legacy SCHED_RESET_ON_FORK hack. */

/**

 * sched_setscheduler - change the scheduling policy and/or RT priority of a thread.

 * @p: the task in question.

 * @policy: new policy.

 * @param: structure containing the new RT priority.

 *

 * Use sched_set_fifo(), read its comment.

 *

 * Return: 0 on success. An error code otherwise.

 *

 * NOTE that the task may be already dead.

/**

 * sched_setscheduler_nocheck - change the scheduling policy and/or RT priority of a thread from kernelspace.

 * @p: the task in question.

 * @policy: new policy.

 * @param: structure containing the new RT priority.

 *

 * Just like sched_setscheduler, only don't bother checking if the

 * current context has permission.  For example, this is needed in

 * stop_machine(): we create temporary high priority worker threads,

 * but our caller might not have that capability.

 *

 * Return: 0 on success. An error code otherwise.

/*

 * SCHED_FIFO is a broken scheduler model; that is, it is fundamentally

 * incapable of resource management, which is the one thing an OS really should

 * be doing.

 *

 * This is of course the reason it is limited to privileged users only.

 *

 * Worse still; it is fundamentally impossible to compose static priority

 * workloads. You cannot take two correctly working static prio workloads

 * and smash them together and still expect them to work.

 *

 * For this reason 'all' FIFO tasks the kernel creates are basically at:

 *

 *   MAX_RT_PRIO / 2

 *

 * The administrator _MUST_ configure the system, the kernel simply doesn't

 * know enough information to make a sensible choice.

/*

 * For when you don't much care about FIFO, but want to be above SCHED_NORMAL.

/*

 * Mimics kernel/events/core.c perf_copy_attr().

 Zero the full structure, so that a short copy will be nice: */

 ABI compatibility quirk: */

	/*

	 * XXX: Do we want to be lenient like existing syscalls; or do we want

	 * to be strict and return an error on out-of-bounds values?

/**

 * sys_sched_setscheduler - set/change the scheduler policy and RT priority

 * @pid: the pid in question.

 * @policy: new policy.

 * @param: structure containing the new RT priority.

 *

 * Return: 0 on success. An error code otherwise.

/**

 * sys_sched_setparam - set/change the RT priority of a thread

 * @pid: the pid in question.

 * @param: structure containing the new RT priority.

 *

 * Return: 0 on success. An error code otherwise.

/**

 * sys_sched_setattr - same as above, but with extended sched_attr

 * @pid: the pid in question.

 * @uattr: structure containing the extended parameters.

 * @flags: for future extension.

/**

 * sys_sched_getscheduler - get the policy (scheduling class) of a thread

 * @pid: the pid in question.

 *

 * Return: On success, the policy of the thread. Otherwise, a negative error

 * code.

/**

 * sys_sched_getparam - get the RT priority of a thread

 * @pid: the pid in question.

 * @param: structure containing the RT priority.

 *

 * Return: On success, 0 and the RT priority is in @param. Otherwise, an error

 * code.

	/*

	 * This one might sleep, we cannot do it with a spinlock held ...

/*

 * Copy the kernel size attribute structure (which might be larger

 * than what user-space knows about) to user-space.

 *

 * Note that all cases are valid: user-space buffer can be larger or

 * smaller than the kernel-space buffer. The usual case is that both

 * have the same size.

	/*

	 * sched_getattr() ABI forwards and backwards compatibility:

	 *

	 * If usize == ksize then we just copy everything to user-space and all is good.

	 *

	 * If usize < ksize then we only copy as much as user-space has space for,

	 * this keeps ABI compatibility as well. We skip the rest.

	 *

	 * If usize > ksize then user-space is using a newer version of the ABI,

	 * which part the kernel doesn't know about. Just ignore it - tooling can

	 * detect the kernel's knowledge of attributes from the attr->size value

	 * which is set to ksize in this case.

/**

 * sys_sched_getattr - similar to sched_getparam, but with sched_attr

 * @pid: the pid in question.

 * @uattr: structure containing the extended parameters.

 * @usize: sizeof(attr) for fwd/bwd comp.

 * @flags: for future extension.

	/*

	 * This could race with another potential updater, but this is fine

	 * because it'll correctly read the old or the new value. We don't need

	 * to guarantee who wins the race as long as it doesn't return garbage.

	/*

	 * If the task isn't a deadline task or admission control is

	 * disabled then we don't care about affinity changes.

	/*

	 * Since bandwidth control happens on root_domain basis,

	 * if admission test is enabled, we only admit -deadline

	 * tasks allowed to run on all the CPUs in the task's

	 * root_domain.

		/*

		 * We must have raced with a concurrent cpuset update.

		 * Just reset the cpumask to the cpuset's cpus_allowed.

 Prevent p going away */

/**

 * sys_sched_setaffinity - set the CPU affinity of a process

 * @pid: pid of the process

 * @len: length in bytes of the bitmask pointed to by user_mask_ptr

 * @user_mask_ptr: user-space pointer to the new CPU mask

 *

 * Return: 0 on success. An error code otherwise.

/**

 * sys_sched_getaffinity - get the CPU affinity of a process

 * @pid: pid of the process

 * @len: length in bytes of the bitmask pointed to by user_mask_ptr

 * @user_mask_ptr: user-space pointer to hold the current CPU mask

 *

 * Return: size of CPU mask copied to user_mask_ptr on success. An

 * error code otherwise.

/**

 * sys_sched_yield - yield the current processor to other threads.

 *

 * This function yields the current CPU to other tasks. If there are no

 * other threads running on this CPU then this function will return.

 *

 * Return: 0.

	/*

	 * In preemptible kernels, ->rcu_read_lock_nesting tells the tick

	 * whether the current CPU is in an RCU read-side critical section,

	 * so the tick can report quiescent states even for CPUs looping

	 * in kernel context.  In contrast, in non-preemptible kernels,

	 * RCU readers leave no in-memory hints, which means that CPU-bound

	 * processes executing in kernel context might never report an

	 * RCU quiescent state.  Therefore, the following code causes

	 * cond_resched() to report a quiescent state, but only when RCU

	 * is in urgent need of one.

/*

 * __cond_resched_lock() - if a reschedule is pending, drop the given lock,

 * call schedule, and on return reacquire the lock.

 *

 * This works OK both with and without CONFIG_PREEMPTION. We do strange low-level

 * operations here to prevent schedule() from being called twice (once via

 * spin_unlock(), once by hand).

/**

 * yield - yield the current processor to other threads.

 *

 * Do not ever use this function, there's a 99% chance you're doing it wrong.

 *

 * The scheduler is at all times free to pick the calling task as the most

 * eligible task to run, if removing the yield() call from your code breaks

 * it, it's already broken.

 *

 * Typical broken usage is:

 *

 * while (!event)

 *	yield();

 *

 * where one assumes that yield() will let 'the other' process run that will

 * make event true. If the current task is a SCHED_FIFO task that will never

 * happen. Never use yield() as a progress guarantee!!

 *

 * If you want to use yield() to wait for something, use wait_event().

 * If you want to use yield() to be 'nice' for others, use cond_resched().

 * If you still want to use yield(), do not!

/**

 * yield_to - yield the current processor to another thread in

 * your thread group, or accelerate that thread toward the

 * processor it's on.

 * @p: target task

 * @preempt: whether task preemption is allowed or not

 *

 * It's the caller's job to ensure that the target task struct

 * can't go away on us before we can do any checks.

 *

 * Return:

 *	true (>0) if we indeed boosted the target task.

 *	false (0) if we failed to boost the target.

 *	-ESRCH if there's no task to yield to.

	/*

	 * If we're the only runnable task on the rq and target rq also

	 * has only one task, there's absolutely no point in yielding.

		/*

		 * Make p's CPU reschedule; pick_next_entity takes care of

		 * fairness.

/*

 * This task is about to go to sleep on IO. Increment rq->nr_iowait so

 * that process accounting knows that this is a task in IO wait state.

/**

 * sys_sched_get_priority_max - return maximum RT priority.

 * @policy: scheduling class.

 *

 * Return: On success, this syscall returns the maximum

 * rt_priority that can be used by a given scheduling class.

 * On failure, a negative error code is returned.

/**

 * sys_sched_get_priority_min - return minimum RT priority.

 * @policy: scheduling class.

 *

 * Return: On success, this syscall returns the minimum

 * rt_priority that can be used by a given scheduling class.

 * On failure, a negative error code is returned.

/**

 * sys_sched_rr_get_interval - return the default timeslice of a process.

 * @pid: pid of the process.

 * @interval: userspace pointer to the timeslice value.

 *

 * this syscall writes the default timeslice value of a given process

 * into the user-space timespec buffer. A value of '0' means infinity.

 *

 * Return: On success, 0 and the timeslice is in @interval. Otherwise,

 * an error code.

 no filter, everything matches */

 filter, but doesn't match */

	/*

	 * When looking for TASK_UNINTERRUPTIBLE skip TASK_IDLE (allows

	 * TASK_KILLABLE).

		/*

		 * reset the NMI-timeout, listing all files on a slow

		 * console might take a lot of time:

		 * Also, reset softlockup watchdogs on all CPUs, because

		 * another CPU might be blocked waiting for us to process

		 * an IPI.

	/*

	 * Only show locks if all tasks are dumped:

/**

 * init_idle - set up an idle thread for a given CPU

 * @idle: task in question

 * @cpu: CPU the idle task belongs to

 *

 * NOTE: this function does not set the idle thread's NEED_RESCHED

 * flag, to make booting more robust.

	/*

	 * The idle task doesn't need the kthread struct to function, but it

	 * is dressed up as a per-CPU kthread and thus needs to play the part

	 * if we want to avoid special-casing it in code that deals with per-CPU

	 * kthreads.

	/*

	 * PF_KTHREAD should already be set at this point; regardless, make it

	 * look like a proper per-CPU kthread.

	/*

	 * It's possible that init_idle() gets called multiple times on a task,

	 * in that case do_set_cpus_allowed() will not do the right thing.

	 *

	 * And since this is boot we can forgo the serialization.

	/*

	 * We're having a chicken and egg problem, even though we are

	 * holding rq->lock, the CPU isn't yet set to this CPU so the

	 * lockdep check in task_group() will fail.

	 *

	 * Similar case to sched_fork(). / Alternatively we could

	 * use task_rq_lock() here and obtain the other rq->lock.

	 *

	 * Silence PROVE_RCU

 Set the preempt count _outside_ the spinlocks! */

	/*

	 * The idle tasks have their own, simple scheduling class:

	/*

	 * Kthreads which disallow setaffinity shouldn't be moved

	 * to a new cpuset; we don't want to change their CPU

	 * affinity and isolating such threads by their set of

	 * allowed nodes is unnecessary.  Thus, cpusets are not

	 * applicable for such threads.  This prevents checking for

	 * success of set_cpus_allowed_ptr() on all attached tasks

	 * before cpus_mask may be changed.

 Migrate current task p to target_cpu */

 TODO: This is not properly updating schedstats */

/*

 * Requeue a task on a given node and accurately track the number of NUMA

 * tasks on the runqueues

 CONFIG_NUMA_BALANCING */

/*

 * Ensure that the idle task is using init_mm right before its CPU goes

 * offline.

 finish_cpu(), as ran on the BP, will clean up the active_mm state */

/*

 * Ensure we only run per-cpu kthreads once the CPU goes !active.

 *

 * This is enabled below SCHED_AP_ACTIVE; when !cpu_active(), but only

 * effective when the hotplug motion is down.

	/*

	 * Ensure the thing is persistent until balance_push_set(.on = false);

	/*

	 * Only active while going offline and when invoked on the outgoing

	 * CPU.

	/*

	 * Both the cpu-hotplug and stop task are in this case and are

	 * required to complete the hotplug process.

		/*

		 * If this is the idle task on the outgoing CPU try to wake

		 * up the hotplug control thread which might wait for the

		 * last task to vanish. The rcuwait_active() check is

		 * accurate here because the waiter is pinned on this CPU

		 * and can't obviously be running in parallel.

		 *

		 * On RT kernels this also has to check whether there are

		 * pinned and scheduled out tasks on the runqueue. They

		 * need to leave the migrate disabled section first.

	/*

	 * Temporarily drop rq->lock such that we can wake-up the stop task.

	 * Both preemption and IRQs are still disabled.

	/*

	 * At this point need_resched() is true and we'll take the loop in

	 * schedule(). The next pick is obviously going to be the stop task

	 * which kthread_is_per_cpu() and will push this task away.

/*

 * Invoked from a CPUs hotplug control thread after the CPU has been marked

 * inactive. All tasks which are not per CPU kernel threads are either

 * pushed off this CPU now via balance_push() or placed on a different CPU

 * during wakeup. Wait until the CPU is quiescent.

 CONFIG_HOTPLUG_CPU */

/*

 * used to mark begin/end of suspend/resume:

/*

 * Update cpusets according to cpu_active mask.  If cpusets are

 * disabled, cpuset_update_active_cpus() becomes a simple wrapper

 * around partition_sched_domains().

 *

 * If we come here as part of a suspend/resume, don't touch cpusets because we

 * want to restore it back to its original state upon resume anyway.

		/*

		 * num_cpus_frozen tracks how many CPUs are involved in suspend

		 * resume sequence. As long as this is not the last online

		 * operation in the resume sequence, just build a single sched

		 * domain, ignoring cpusets.

		/*

		 * This is the last CPU online operation. So fall through and

		 * restore the original sched domains by considering the

		 * cpuset configurations.

	/*

	 * Clear the balance_push callback and prepare to schedule

	 * regular tasks.

	/*

	 * When going up, increment the number of cores with SMT present.

	/*

	 * Put the rq online, if not already. This happens:

	 *

	 * 1) In the early boot process, because we build the real domains

	 *    after all CPUs have been brought up.

	 *

	 * 2) At runtime, if cpuset_cpu_active() fails to rebuild the

	 *    domains.

	/*

	 * Remove CPU from nohz.idle_cpus_mask to prevent participating in

	 * load balancing when not active

	/*

	 * From this point forward, this CPU will refuse to run any task that

	 * is not: migrate_disable() or KTHREAD_IS_PER_CPU, and will actively

	 * push those tasks away until this gets cleared, see

	 * sched_cpu_dying().

	/*

	 * We've cleared cpu_active_mask / set balance_push, wait for all

	 * preempt-disabled and RCU users of this state to go away such that

	 * all new such users will observe it.

	 *

	 * Specifically, we rely on ttwu to no longer target this CPU, see

	 * ttwu_queue_cond() and is_cpu_allowed().

	 *

	 * Do sync before park smpboot threads to take care the rcu boost case.

	/*

	 * When going down, decrement the number of cores with SMT present.

/*

 * Invoked immediately before the stopper thread is invoked to bring the

 * CPU down completely. At this point all per CPU kthreads except the

 * hotplug thread (current) and the stopper thread (inactive) have been

 * either parked or have been unbound from the outgoing CPU. Ensure that

 * any of those which might be on the way out are gone.

 *

 * If after this point a bound task is being woken on this CPU then the

 * responsible hotplug callback has failed to do it's job.

 * sched_cpu_dying() will catch it with the appropriate fireworks.

/*

 * Since this CPU is going 'away' for a while, fold any nr_active delta we

 * might have. Called from the CPU stopper task after ensuring that the

 * stopper is the last running task on the CPU, so nr_active count is

 * stable. We need to take the teardown thread which is calling this into

 * account, so we hand in adjust = 1 to the load calculation.

 *

 * Also see the comment "Global load-average calculations".

 Handle pending wakeups and then migrate everything off */

	/*

	 * There's no userspace yet to cause hotplug operations; hence all the

	 * CPU masks are stable and all blatant races in the below code cannot

	 * happen.

 Move init over to a non-isolated CPU */

 CONFIG_SMP */

/*

 * Default task group.

 * Every task in system belongs to this group at bootup.

 Cacheline aligned slab cache for task_group */

 Make sure the linker didn't screw up */

 CONFIG_FAIR_GROUP_SCHED */

 CONFIG_RT_GROUP_SCHED */

 CONFIG_CPUMASK_OFFSTACK */

 CONFIG_RT_GROUP_SCHED */

 CONFIG_CGROUP_SCHED */

		/*

		 * How much CPU bandwidth does root_task_group get?

		 *

		 * In case of task-groups formed thr' the cgroup filesystem, it

		 * gets 100% of the CPU resources in the system. This overall

		 * system CPU resource is divided among the tasks of

		 * root_task_group and its child task-groups in a fair manner,

		 * based on each entity's (task or task-group's) weight

		 * (se->load.weight).

		 *

		 * In other words, if root_task_group has 10 tasks of weight

		 * 1024) and two child groups A0 and A1 (of weight 1024 each),

		 * then A0's share of the CPU resource is:

		 *

		 *	A0's bandwidth = 1024 / (10*1024 + 1024 + 1024) = 8.33%

		 *

		 * We achieve this by letting root_task_group's tasks sit

		 * directly in rq->cfs (i.e root_task_group->se[] = NULL).

 CONFIG_FAIR_GROUP_SCHED */

 CONFIG_SMP */

	/*

	 * The boot idle thread does lazy MMU switching as well:

	/*

	 * Make us the idle thread. Technically, schedule() should not be

	 * called from this thread, however somewhere below it might be,

	 * but because we are the idle thread, we just pick up running again

	 * when this runqueue becomes "idle".

	/*

	 * Blocking primitives will set (and therefore destroy) current->state,

	 * since we will exit with TASK_RUNNING make sure we enter with it,

	 * otherwise we will destroy state.

 Ratelimiting timestamp: */

 WARN_ON_ONCE() by default, no rate limit required: */

 Save this before calling printk(), since that will clobber it: */

		/*

		 * Only normalize user tasks:

			/*

			 * Renice negative nice level userspace

			 * tasks back to 0:

 CONFIG_MAGIC_SYSRQ */

/*

 * These functions are only useful for the IA64 MCA handling, or kdb.

 *

 * They can only be called when the whole system has been

 * stopped - every CPU needs to be quiescent, and no scheduling

 * activity can take place. Using them for anything else would

 * be a serious bug, and as a result, they aren't even visible

 * under any other configuration.

/**

 * curr_task - return the current task for a given CPU.

 * @cpu: the processor in question.

 *

 * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!

 *

 * Return: The current task for @cpu.

 defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB) */

/**

 * ia64_set_curr_task - set the current task for a given CPU.

 * @cpu: the processor in question.

 * @p: the task pointer to set.

 *

 * Description: This function must only be used when non-maskable interrupts

 * are serviced on a separate stack. It allows the architecture to switch the

 * notion of the current task on a CPU in a non-blocking manner. This function

 * must be called with all CPU's synchronized, and interrupts disabled, the

 * and caller must save the original value of the current task (see

 * curr_task() above) and restore that value before reenabling interrupts and

 * re-starting the system.

 *

 * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!

 task_group_lock serializes the addition/removal of task groups */

	/*

	 * We have to wait for yet another RCU grace period to expire, as

	 * print_cfs_stats() might run concurrently.

 allocate runqueue etc for a new task group */

 Root should already exist: */

 rcu callback to free various structures associated with a task group */

 Now it should be safe to free those cfs_rqs: */

 Wait for possible concurrent references to cfs_rqs complete: */

	/*

	 * Unlink first, to avoid walk_tg_tree_from() from finding us (via

	 * sched_cfs_period_timer()).

	 *

	 * For this to be effective, we have to wait for all pending users of

	 * this task group to leave their RCU critical section to ensure no new

	 * user will see our dying task group any more. Specifically ensure

	 * that tg_unthrottle_up() won't add decayed cfs_rq's to it.

	 *

	 * We therefore defer calling unregister_fair_sched_group() to

	 * sched_unregister_group() which is guarantied to get called only after the

	 * current RCU grace period has expired.

	/*

	 * All callers are synchronized by task_rq_lock(); we do not use RCU

	 * which is pointless here. Thus, we pass "true" to task_css_check()

	 * to prevent lockdep warnings.

/*

 * Change task's runqueue when it moves between groups.

 *

 * The caller of this function should have put the task in its new group by

 * now. This function just updates tsk->se.cfs_rq and tsk->se.parent to reflect

 * its new group.

		/*

		 * After changing group, the running task may have joined a

		 * throttled one but it's still the running task. Trigger a

		 * resched to make sure that task can still run.

 This is early initialization for the top cgroup */

 Expose task group only after completing cgroup initialization */

 Propagate the effective uclamp value for the new group */

	/*

	 * Relies on the RCU grace period between css_released() and this.

/*

 * This is called before wake_up_new_task(), therefore we really only

 * have to set its group bits, all the other stuff does not apply.

		/*

		 * Serialize against wake_up_new_task() such that if it's

		 * running, we're sure to observe its full state.

		/*

		 * Avoid calling sched_move_task() before wake_up_new_task()

		 * has happened. This would lead to problems with PELT, due to

		 * move wanting to detach+attach while we're not attached yet.

 Assume effective clamps matches requested clamps */

 Cap effective clamps with parent's effective clamps */

 Ensure protection is always capped by limit */

 Propagate most restrictive effective clamps */

 Immediately update descendants RUNNABLE tasks */

/*

 * Integer 10^N with a given N exponent by casting to integer the literal "1eN"

 * C expression. Since there is no way to convert a macro argument (N) into a

 * character constant, use two levels of macros.

	/*

	 * Because of not recoverable conversion rounding we keep track of the

	 * exact requested value

 Update effective clamps to track the most restrictive value */

 CONFIG_UCLAMP_TASK_GROUP */

 1s */

 1ms */

 More than 203 days if BW_SHIFT equals 20. */

	/*

	 * Ensure we have at some amount of bandwidth every period.  This is

	 * to prevent reaching a state of large arrears when throttled via

	 * entity_tick() resulting in prolonged exit starvation.

	/*

	 * Likewise, bound things on the other side by preventing insane quota

	 * periods.  This also allows us to normalize in computing quota

	 * feasibility.

	/*

	 * Bound quota to defend quota against overflow during bandwidth shift.

	/*

	 * Prevent race between setting of cfs_rq->runtime_enabled and

	 * unthrottle_offline_cfs_rqs().

	/*

	 * If we need to toggle cfs_bandwidth_used, off->on must occur

	 * before making related changes, and on->off must occur afterwards

 Restart the period timer (if active) to handle new period expiry: */

/*

 * normalize group quota/period to be quota/max_period

 * note: units are usecs

 note: these should typically be equivalent */

		/*

		 * Ensure max(child_quota) <= parent_quota.  On cgroup2,

		 * always take the min.  On cgroup1, only inherit when no

		 * limit is set:

 CONFIG_CFS_BANDWIDTH */

 CONFIG_FAIR_GROUP_SCHED */

 CONFIG_RT_GROUP_SCHED */

 Terminate */

	/*

	 * cgroup weight knobs should use the common MIN, DFL and MAX

	 * values which are 1, 100 and 10000 respectively.  While it loses

	 * a bit of range on both ends, it maps pretty well onto the shares

	 * value used by scheduler and the round-trip conversions preserve

	 * the original value over the entire range.

 find the closest nice value to the current weight */

 caller should put the current value in *@periodp before calling */

 U64_MAX */

 terminate */

 CONFIG_CGROUP_SCHED */

/*

 * Nice levels are multiplicative, with a gentle 10% change for every

 * nice level changed. I.e. when a CPU-bound task goes from nice 0 to

 * nice 1, it will get ~10% less CPU time than another CPU-bound task

 * that remained on nice 0.

 *

 * The "10% effect" is relative and cumulative: from _any_ nice level,

 * if you go up 1 level, it's -10% CPU usage, if you go down 1 level

 * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.

 * If a task goes up by ~10% and another task goes down by ~10% then

 * the relative distance between them is ~25%.)

 -20 */     88761,     71755,     56483,     46273,     36291,

 -15 */     29154,     23254,     18705,     14949,     11916,

 -10 */      9548,      7620,      6100,      4904,      3906,

  -5 */      3121,      2501,      1991,      1586,      1277,

   0 */      1024,       820,       655,       526,       423,

   5 */       335,       272,       215,       172,       137,

  10 */       110,        87,        70,        56,        45,

  15 */        36,        29,        23,        18,        15,

/*

 * Inverse (2^32/x) values of the sched_prio_to_weight[] array, precalculated.

 *

 * In cases where the weight does not change often, we can use the

 * precalculated inverse to speed up arithmetics by turning divisions

 * into multiplications:

 -20 */     48388,     59856,     76040,     92818,    118348,

 -15 */    147320,    184698,    229616,    287308,    360437,

 -10 */    449829,    563644,    704093,    875809,   1099582,

  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,

   0 */   4194304,   5237765,   6557202,   8165337,  10153587,

   5 */  12820798,  15790321,  19976592,  24970740,  31350126,

  10 */  39045157,  49367440,  61356676,  76695844,  95443717,

  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kernel/sched/debug.c

 *

 * Print the CFS rbtree and other debugging details

 *

 * Copyright(C) 2007, Red Hat, Inc., Ingo Molnar

/*

 * This allows printing both to /proc/sched_debug and

 * to the console

/*

 * Ease the printing of nsec fields:

 CONFIG_JUMP_LABEL */

 Ensure the static_key remains in a consistent state */

 SMP */

 CONFIG_PREEMPT_DYNAMIC */

	/*

	 * This can unfortunately be invoked before sched_debug_init() creates

	 * the debug directory. Don't touch sd_sysctl_cpus until then.

 CONFIG_SMP */

/*

 * Only 1 SEQ_printf_task_group_path() caller can use the full length

 * group_path[] for cgroup path. Other simultaneous callers will have

 * to use a shorter stack buffer. A "..." suffix is appended at the end

 * of the stack buffer so that it will show up in case the output length

 * matches the given buffer size to indicate possible path name truncation.

		/*

		 * Need to reset softlockup watchdogs on all CPUs, because

		 * another CPU might be blocked waiting for us to process

		 * an IPI or stop_machine.

/*

 * This iterator needs some explanation.

 * It returns 1 for the header position.

 * This means 2 is CPU 0.

 * In a hotplugged system some CPUs, including CPU 0, may be missing so we have

 * to use cpumask_* to iterate over the CPUs.

 SPDX-License-Identifier: GPL-2.0

/*

 * CPU accounting code for task groups.

 *

 * Based on the work by Paul Menage (menage@google.com) and Balbir Singh

 * (balbir@in.ibm.com).

 Time spent by the tasks of the CPU accounting group executing in ... */

 ... user mode */

 ... kernel mode */

 track CPU usage of a group of tasks and its child groups */

 cpuusage holds pointer to a u64-type object on every CPU */

 Return CPU accounting group to which this task belongs */

 Create a new CPU accounting group */

 Destroy an existing CPU accounting group */

	/*

	 * We allow index == CPUACCT_STAT_NSTATS here to read

	 * the sum of usages.

	/*

	 * Take rq->lock to make 64-bit read safe on 32-bit platforms.

	/*

	 * Take rq->lock to make 64-bit write safe on 32-bit platforms.

 Return total CPU usage (in nanoseconds) of a group */

	/*

	 * Only allow '0' here to do a reset.

			/*

			 * Take rq->lock to make 64-bit read safe on 32-bit

			 * platforms.

 terminate */

/*

 * charge this task's execution time to its accounting group.

 *

 * called with rq->lock held.

/*

 * Add user/system time to cpuacct.

 *

 * Note: it's the caller that updates the account of the root cgroup.

 SPDX-License-Identifier: GPL-2.0

/*

 * Scheduler code and data structures related to cpufreq.

 *

 * Copyright (C) 2016, Intel Corporation

 * Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

/**

 * cpufreq_add_update_util_hook - Populate the CPU's update_util_data pointer.

 * @cpu: The CPU to set the pointer for.

 * @data: New pointer value.

 * @func: Callback function to set for the CPU.

 *

 * Set and publish the update_util_data pointer for the given CPU.

 *

 * The update_util_data pointer of @cpu is set to @data and the callback

 * function pointer in the target struct update_util_data is set to @func.

 * That function will be called by cpufreq_update_util() from RCU-sched

 * read-side critical sections, so it must not sleep.  @data will always be

 * passed to it as the first argument which allows the function to get to the

 * target update_util_data structure and its container.

 *

 * The update_util_data pointer of @cpu must be NULL when this function is

 * called or it will WARN() and return with no effect.

/**

 * cpufreq_remove_update_util_hook - Clear the CPU's update_util_data pointer.

 * @cpu: The CPU to clear the pointer for.

 *

 * Clear the update_util_data pointer for the given CPU.

 *

 * Callers must use RCU callbacks to free any memory that might be

 * accessed via the old update_util_data pointer or invoke synchronize_rcu()

 * right after this function to avoid use-after-free.

/**

 * cpufreq_this_cpu_can_update - Check if cpufreq policy can be updated.

 * @policy: cpufreq policy to check.

 *

 * Return 'true' if:

 * - the local and remote CPUs share @policy,

 * - dvfs_possible_from_any_cpu is set in @policy and the local CPU is not going

 *   offline (in which case it is not expected to run cpufreq updates any more).

 SPDX-License-Identifier: GPL-2.0

/*

 * kernel/sched/loadavg.c

 *

 * This file contains the magic bits required to compute the global loadavg

 * figure. Its a silly number but people think its important. We go through

 * great pains to make it work on big machines and tickless kernels.

/*

 * Global load-average calculations

 *

 * We take a distributed and async approach to calculating the global load-avg

 * in order to minimize overhead.

 *

 * The global load average is an exponentially decaying average of nr_running +

 * nr_uninterruptible.

 *

 * Once every LOAD_FREQ:

 *

 *   nr_active = 0;

 *   for_each_possible_cpu(cpu)

 *	nr_active += cpu_of(cpu)->nr_running + cpu_of(cpu)->nr_uninterruptible;

 *

 *   avenrun[n] = avenrun[0] * exp_n + nr_active * (1 - exp_n)

 *

 * Due to a number of reasons the above turns in the mess below:

 *

 *  - for_each_possible_cpu() is prohibitively expensive on machines with

 *    serious number of CPUs, therefore we need to take a distributed approach

 *    to calculating nr_active.

 *

 *        \Sum_i x_i(t) = \Sum_i x_i(t) - x_i(t_0) | x_i(t_0) := 0

 *                      = \Sum_i { \Sum_j=1 x_i(t_j) - x_i(t_j-1) }

 *

 *    So assuming nr_active := 0 when we start out -- true per definition, we

 *    can simply take per-CPU deltas and fold those into a global accumulate

 *    to obtain the same result. See calc_load_fold_active().

 *

 *    Furthermore, in order to avoid synchronizing all per-CPU delta folding

 *    across the machine, we assume 10 ticks is sufficient time for every

 *    CPU to have completed this task.

 *

 *    This places an upper-bound on the IRQ-off latency of the machine. Then

 *    again, being late doesn't loose the delta, just wrecks the sample.

 *

 *  - cpu_rq()->nr_uninterruptible isn't accurately tracked per-CPU because

 *    this would add another cross-CPU cacheline miss and atomic operation

 *    to the wakeup path. Instead we increment on whatever CPU the task ran

 *    when it went into uninterruptible state and decrement on whatever CPU

 *    did the wakeup. This means that only the sum of nr_uninterruptible over

 *    all CPUs yields the correct result.

 *

 *  This covers the NO_HZ=n code, for extra head-aches, see the comment below.

 Variables and functions for calc_load */

 should be removed */

/**

 * get_avenrun - get the load average array

 * @loads:	pointer to dest load array

 * @offset:	offset to add

 * @shift:	shift count to shift the result left

 *

 * These values are estimates at best, so no need for locking.

/**

 * fixed_power_int - compute: x^n, in O(log n) time

 *

 * @x:         base of the power

 * @frac_bits: fractional bits of @x

 * @n:         power to raise @x to.

 *

 * By exploiting the relation between the definition of the natural power

 * function: x^n := x*x*...*x (x multiplied by itself for n times), and

 * the binary encoding of numbers used by computers: n := \Sum n_i * 2^i,

 * (where: n_i \elem {0, 1}, the binary vector representing n),

 * we find: x^n := x^(\Sum n_i * 2^i) := \Prod x^(n_i * 2^i), which is

 * of course trivially computable in O(log_2 n), the length of our binary

 * vector.

/*

 * a1 = a0 * e + a * (1 - e)

 *

 * a2 = a1 * e + a * (1 - e)

 *    = (a0 * e + a * (1 - e)) * e + a * (1 - e)

 *    = a0 * e^2 + a * (1 - e) * (1 + e)

 *

 * a3 = a2 * e + a * (1 - e)

 *    = (a0 * e^2 + a * (1 - e) * (1 + e)) * e + a * (1 - e)

 *    = a0 * e^3 + a * (1 - e) * (1 + e + e^2)

 *

 *  ...

 *

 * an = a0 * e^n + a * (1 - e) * (1 + e + ... + e^n-1) [1]

 *    = a0 * e^n + a * (1 - e) * (1 - e^n)/(1 - e)

 *    = a0 * e^n + a * (1 - e^n)

 *

 * [1] application of the geometric series:

 *

 *              n         1 - x^(n+1)

 *     S_n := \Sum x^i = -------------

 *             i=0          1 - x

/*

 * Handle NO_HZ for the global load-average.

 *

 * Since the above described distributed algorithm to compute the global

 * load-average relies on per-CPU sampling from the tick, it is affected by

 * NO_HZ.

 *

 * The basic idea is to fold the nr_active delta into a global NO_HZ-delta upon

 * entering NO_HZ state such that we can include this as an 'extra' CPU delta

 * when we read the global state.

 *

 * Obviously reality has to ruin such a delightfully simple scheme:

 *

 *  - When we go NO_HZ idle during the window, we can negate our sample

 *    contribution, causing under-accounting.

 *

 *    We avoid this by keeping two NO_HZ-delta counters and flipping them

 *    when the window starts, thus separating old and new NO_HZ load.

 *

 *    The only trick is the slight shift in index flip for read vs write.

 *

 *        0s            5s            10s           15s

 *          +10           +10           +10           +10

 *        |-|-----------|-|-----------|-|-----------|-|

 *    r:0 0 1           1 0           0 1           1 0

 *    w:0 1 1           0 0           1 1           0 0

 *

 *    This ensures we'll fold the old NO_HZ contribution in this window while

 *    accumulating the new one.

 *

 *  - When we wake up from NO_HZ during the window, we push up our

 *    contribution, since we effectively move our sample point to a known

 *    busy state.

 *

 *    This is solved by pushing the window forward, and thus skipping the

 *    sample, for this CPU (effectively using the NO_HZ-delta for this CPU which

 *    was in effect at the time the window opened). This also solves the issue

 *    of having to deal with a CPU having been in NO_HZ for multiple LOAD_FREQ

 *    intervals.

 *

 * When making the ILB scale, we should try to pull this in as well.

	/*

	 * See calc_global_nohz(), if we observe the new index, we also

	 * need to observe the new update time.

	/*

	 * If the folding window started, make sure we start writing in the

	 * next NO_HZ-delta.

	/*

	 * We're going into NO_HZ mode, if there's any pending delta, fold it

	 * into the pending NO_HZ delta.

/*

 * Keep track of the load for NOHZ_FULL, must be called between

 * calc_load_nohz_{start,stop}().

	/*

	 * If we're still before the pending sample window, we're done.

	/*

	 * We woke inside or after the sample window, this means we're already

	 * accounted through the nohz accounting, so skip the entire deal and

	 * sync up for the next window.

/*

 * NO_HZ can leave us missing all per-CPU ticks calling

 * calc_load_fold_active(), but since a NO_HZ CPU folds its delta into

 * calc_load_nohz per calc_load_nohz_start(), all we need to do is fold

 * in the pending NO_HZ delta if our NO_HZ period crossed a load cycle boundary.

 *

 * Once we've updated the global active value, we need to apply the exponential

 * weights adjusted to the number of cycles missed.

		/*

		 * Catch-up, fold however many we are behind still

	/*

	 * Flip the NO_HZ index...

	 *

	 * Make sure we first write the new time then flip the index, so that

	 * calc_load_write_idx() will see the new time when it reads the new

	 * index, this avoids a double flip messing things up.

 !CONFIG_NO_HZ_COMMON */

 CONFIG_NO_HZ_COMMON */

/*

 * calc_load - update the avenrun load estimates 10 ticks after the

 * CPUs have updated calc_load_tasks.

 *

 * Called from the global timer code.

	/*

	 * Fold the 'old' NO_HZ-delta to include all NO_HZ CPUs.

	/*

	 * In case we went to NO_HZ for multiple LOAD_FREQ intervals

	 * catch up in bulk.

/*

 * Called from scheduler_tick() to periodically update this CPU's

 * active count.

 SPDX-License-Identifier: GPL-2.0

/*

 * CPUFreq governor based on scheduler-provided CPU utilization data.

 *

 * Copyright (C) 2016, Intel Corporation

 * Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

 The next fields are only needed if fast switch cannot be used: */

 The field below is for single-CPU policies only: */

*********************** Governor internals ***********************/

	/*

	 * Since cpufreq_update_util() is called with rq->lock held for

	 * the @target_cpu, our per-CPU data is fully serialized.

	 *

	 * However, drivers cannot in general deal with cross-CPU

	 * requests, so while get_next_freq() will work, our

	 * sugov_update_commit() call may not for the fast switching platforms.

	 *

	 * Hence stop here for remote requests if they aren't supported

	 * by the hardware, as calculating the frequency is pointless if

	 * we cannot in fact act on it.

	 *

	 * This is needed on the slow switching platforms too to prevent CPUs

	 * going offline from leaving stale IRQ work items behind.

/**

 * get_next_freq - Compute a new frequency for a given cpufreq policy.

 * @sg_policy: schedutil policy object to compute the new frequency for.

 * @util: Current CPU utilization.

 * @max: CPU capacity.

 *

 * If the utilization is frequency-invariant, choose the new frequency to be

 * proportional to it, that is

 *

 * next_freq = C * max_freq * util / max

 *

 * Otherwise, approximate the would-be frequency-invariant utilization by

 * util_raw * (curr_freq / max_freq) which leads to

 *

 * next_freq = C * curr_freq * util_raw / max

 *

 * Take C = 1.25 for the frequency tipping point at (util / max) = 0.8.

 *

 * The lowest driver-supported frequency which is equal or greater than the raw

 * next_freq (as calculated above) is returned, subject to policy min/max and

 * cpufreq driver limitations.

/**

 * sugov_iowait_reset() - Reset the IO boost status of a CPU.

 * @sg_cpu: the sugov data for the CPU to boost

 * @time: the update time from the caller

 * @set_iowait_boost: true if an IO boost has been requested

 *

 * The IO wait boost of a task is disabled after a tick since the last update

 * of a CPU. If a new IO wait boost is requested after more then a tick, then

 * we enable the boost starting from IOWAIT_BOOST_MIN, which improves energy

 * efficiency by ignoring sporadic wakeups from IO.

 Reset boost only if a tick has elapsed since last request */

/**

 * sugov_iowait_boost() - Updates the IO boost status of a CPU.

 * @sg_cpu: the sugov data for the CPU to boost

 * @time: the update time from the caller

 * @flags: SCHED_CPUFREQ_IOWAIT if the task is waking up after an IO wait

 *

 * Each time a task wakes up after an IO operation, the CPU utilization can be

 * boosted to a certain utilization which doubles at each "frequent and

 * successive" wakeup from IO, ranging from IOWAIT_BOOST_MIN to the utilization

 * of the maximum OPP.

 *

 * To keep doubling, an IO boost has to be requested at least once per tick,

 * otherwise we restart from the utilization of the minimum OPP.

 Reset boost if the CPU appears to have been idle enough */

 Boost only tasks waking up after IO */

 Ensure boost doubles only one time at each request */

 Double the boost at each request */

 First wakeup after IO: start with minimum boost */

/**

 * sugov_iowait_apply() - Apply the IO boost to a CPU.

 * @sg_cpu: the sugov data for the cpu to boost

 * @time: the update time from the caller

 *

 * A CPU running a task which woken up after an IO operation can have its

 * utilization boosted to speed up the completion of those IO operations.

 * The IO boost value is increased each time a task wakes up from IO, in

 * sugov_iowait_apply(), and it's instead decreased by this function,

 * each time an increase has not been requested (!iowait_boost_pending).

 *

 * A CPU which also appears to have been idle for at least one tick has also

 * its IO boost utilization reset.

 *

 * This mechanism is designed to boost high frequently IO waiting tasks, while

 * being more conservative on tasks which does sporadic IO operations.

 No boost currently required */

 Reset boost if the CPU appears to have been idle enough */

		/*

		 * No boost pending; reduce the boost value.

	/*

	 * sg_cpu->util is already in capacity scale; convert iowait_boost

	 * into the same scale so we can compare.

 CONFIG_NO_HZ_COMMON */

/*

 * Make sugov_should_update_freq() ignore the rate limit when DL

 * has increased the utilization.

	/*

	 * Do not reduce the frequency if the CPU has not been idle

	 * recently, as the reduction is likely to be premature then.

 Restore cached freq as next_freq has changed */

	/*

	 * This code runs under rq->lock for the target CPU, so it won't run

	 * concurrently on two different CPUs for the same target and it is not

	 * necessary to acquire the lock in the fast switch case.

	/*

	 * Fall back to the "frequency" path if frequency invariance is not

	 * supported, because the direct mapping between the utilization and

	 * the performance levels depends on the frequency invariance.

	/*

	 * Do not reduce the target performance level if the CPU has not been

	 * idle recently, as the reduction is likely to be premature then.

	/*

	 * Hold sg_policy->update_lock shortly to handle the case where:

	 * in case sg_policy->next_freq is read here, and then updated by

	 * sugov_deferred_update() just before work_in_progress is set to false

	 * here, we may miss queueing the new update.

	 *

	 * Note: If a work was queued after the update_lock is released,

	 * sugov_work() will just be called again by kthread_work code; and the

	 * request will be proceed before the sugov thread sleeps.

************************* sysfs interface ************************/

********************* cpufreq governor interface *********************/

		/*

		 * Fake (unused) bandwidth; workaround to "fix"

		 * priority inheritance.

 kthread only required for slow path */

 kthread only required for slow path */

 State should be equivalent to EXIT */

/*

 * EAS shouldn't be attempted without sugov, so rebuild the sched_domains

 * on governor changes to make sure the scheduler knows about it.

		/*

		 * When called from the cpufreq_register_driver() path, the

		 * cpu_hotplug_lock is already held, so use a work item to

		 * avoid nested locking in rebuild_sched_domains().

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Simple CPU accounting cgroup controller

/*

 * There are no locks covering percpu hardirq/softirq time.

 * They are only modified in vtime_account, on corresponding CPU

 * with interrupts disabled. So, writes are safe.

 * They are read and saved off onto struct rq in update_rq_clock().

 * This may result in other CPU reading this CPU's irq time and can

 * race with irq/vtime_account on this CPU. We would either get old

 * or new value with a side effect of accounting a slice of irq time to wrong

 * task when irq is in progress while we read rq->clock. That is a worthy

 * compromise in place of having locks on each irq in account_system_time.

/*

 * Called after incrementing preempt_count on {soft,}irq_enter

 * and before decrementing preempt_count on {soft,}irq_exit.

	/*

	 * We do not account for softirq time from ksoftirqd here.

	 * We want to continue accounting softirq time to ksoftirqd thread

	 * in that case, so as not to confuse scheduler with a special task

	 * that do not consume any time, but still wants to run.

 CONFIG_IRQ_TIME_ACCOUNTING */

 !CONFIG_IRQ_TIME_ACCOUNTING */

	/*

	 * Since all updates are sure to touch the root cgroup, we

	 * get ourselves ahead and touch it first. If the root cgroup

	 * is the only cgroup, then nothing else should be necessary.

	 *

/*

 * Account user CPU time to a process.

 * @p: the process that the CPU time gets accounted to

 * @cputime: the CPU time spent in user space since the last update

 Add user time to process. */

 Add user time to cpustat. */

 Account for user time used */

/*

 * Account guest CPU time to a process.

 * @p: the process that the CPU time gets accounted to

 * @cputime: the CPU time spent in virtual machine since the last update

 Add guest time to process. */

 Add guest time to cpustat. */

/*

 * Account system CPU time to a process and desired cpustat field

 * @p: the process that the CPU time gets accounted to

 * @cputime: the CPU time spent in kernel space since the last update

 * @index: pointer to cpustat field that has to be updated

 Add system time to process. */

 Add system time to cpustat. */

 Account for system time used */

/*

 * Account system CPU time to a process.

 * @p: the process that the CPU time gets accounted to

 * @hardirq_offset: the offset to subtract from hardirq_count()

 * @cputime: the CPU time spent in kernel space since the last update

/*

 * Account for involuntary wait time.

 * @cputime: the CPU time spent in involuntary wait

/*

 * Account for idle time.

 * @cputime: the CPU time spent in idle wait

/*

 * When a guest is interrupted for a longer amount of time, missed clock

 * ticks are not redelivered later. Due to that, this function may on

 * occasion account more time than the calling functions think elapsed.

/*

 * Account how much elapsed time was spent in steal, irq, or softirq time.

/*

 * Accumulate raw cputime values of dead tasks (sig->[us]time) and live

 * tasks (sum on group iteration) belonging to @tsk's group.

	/*

	 * Update current task runtime to account pending time since last

	 * scheduler action or thread_group_cputime() call. This thread group

	 * might have other running tasks on different CPUs, but updating

	 * their runtime can affect syscall performance, so we skip account

	 * those pending times and rely only on values updated on tick or

	 * other scheduler action.

 Attempt a lockless read on the first round. */

 If lockless access failed, take the lock. */

/*

 * Account a tick to a process and cpustat

 * @p: the process that the CPU time gets accounted to

 * @user_tick: is the tick from userspace

 * @rq: the pointer to rq

 *

 * Tick demultiplexing follows the order

 * - pending hardirq update

 * - pending softirq update

 * - user_time

 * - idle_time

 * - system time

 *   - check for guest_time

 *   - else account as system_time

 *

 * Check for hardirq is done both for system and user time as there is

 * no timer going off while we are on hardirq and hence we may never get an

 * opportunity to update it solely in system time.

 * p->stime and friends are only updated on system time and not on irq

 * softirq as those do not count in task exec_runtime any more.

	/*

	 * When returning from idle, many ticks can get accounted at

	 * once, including some ticks of steal, irq, and softirq time.

	 * Subtract those ticks from the amount of time accounted to

	 * idle, or potentially user or system time. Due to rounding,

	 * other time can exceed ticks occasionally.

		/*

		 * ksoftirqd time do not get accounted in cpu_softirq_time.

		 * So, we have to handle it separately here.

		 * Also, p->stime needs to be updated for ksoftirqd.

 System time or guest time */

 CONFIG_IRQ_TIME_ACCOUNTING */

 CONFIG_IRQ_TIME_ACCOUNTING */

/*

 * Use precise platform statistics if available:

 !CONFIG_VIRT_CPU_ACCOUNTING_NATIVE: */

/*

 * Account a single tick of CPU time.

 * @p: the process that the CPU time gets accounted to

 * @user_tick: indicates if the tick is a user or a system tick

/*

 * Account multiple ticks of idle time.

 * @ticks: number of stolen ticks

/*

 * Adjust tick based cputime random precision against scheduler runtime

 * accounting.

 *

 * Tick based cputime accounting depend on random scheduling timeslices of a

 * task to be interrupted or not by the timer.  Depending on these

 * circumstances, the number of these interrupts may be over or

 * under-optimistic, matching the real user and system cputime with a variable

 * precision.

 *

 * Fix this by scaling these tick based values against the total runtime

 * accounted by the CFS scheduler.

 *

 * This code provides the following guarantees:

 *

 *   stime + utime == rtime

 *   stime_i+1 >= stime_i, utime_i+1 >= utime_i

 *

 * Assuming that rtime_i+1 >= rtime_i.

 Serialize concurrent callers such that we can honour our guarantees */

	/*

	 * This is possible under two circumstances:

	 *  - rtime isn't monotonic after all (a bug);

	 *  - we got reordered by the lock.

	 *

	 * In both cases this acts as a filter such that the rest of the code

	 * can assume it is monotonic regardless of anything else.

	/*

	 * If either stime or utime are 0, assume all runtime is userspace.

	 * Once a task gets some ticks, the monotonicity code at 'update:'

	 * will ensure things converge to the observed ratio.

	/*

	 * Make sure stime doesn't go backwards; this preserves monotonicity

	 * for utime because rtime is monotonic.

	 *

	 *  utime_i+1 = rtime_i+1 - stime_i

	 *            = rtime_i+1 - (rtime_i - utime_i)

	 *            = (rtime_i+1 - rtime_i) + utime_i

	 *            >= utime_i

	/*

	 * Make sure utime doesn't go backwards; this still preserves

	 * monotonicity for stime, analogous argument to above.

 !CONFIG_VIRT_CPU_ACCOUNTING_NATIVE */

	/*

	 * Unlike tick based timing, vtime based timing never has lost

	 * ticks, and no need for steal time accounting to make up for

	 * lost ticks. Vtime accounts a rounded version of actual

	 * elapsed time. Limit account_other_time to prevent rounding

	 * errors from causing elapsed vtime to go negative.

 We might have scheduled out from guest path */

	/*

	 * The flags must be updated under the lock with

	 * the vtime_starttime flush and update.

	 * That enforces a right ordering and update sequence

	 * synchronization against the reader (task_gtime())

	 * that can thus safely catch up with a tickless delta.

/*

 * Fetch cputime raw values from fields of task_struct and

 * add up the pending nohz execution time since the last

 * cputime snapshot.

 Task is sleeping or idle, nothing to add */

		/*

		 * Task runs either in user (including guest) or kernel space,

		 * add pending nohz time to the right place.

	/*

	 * We raced against a context switch, fetch the

	 * kcpustat task again.

	/*

	 * Two possible things here:

	 * 1) We are seeing the scheduling out task (prev) or any past one.

	 * 2) We are seeing the scheduling in task (next) but it hasn't

	 *    passed though vtime_task_switch() yet so the pending

	 *    cputime of the prev task may not be flushed yet.

	 *

	 * Case 1) is ok but 2) is not. So wait for a safe VTIME state.

		/*

		 * Nice VS unnice cputime accounting may be inaccurate if

		 * the nice value has changed since the last vtime update.

		 * But proper fix would involve interrupting target on nice

		 * updates which is a no go on nohz_full (although the scheduler

		 * may still interrupt the target if rescheduling is needed...)

 Task is sleeping, dead or idle, nothing to add */

		/*

		 * Task runs either in user (including guest) or kernel space,

		 * add pending nohz time to the right place.

 CONFIG_VIRT_CPU_ACCOUNTING_GEN */

 SPDX-License-Identifier: GPL-2.0

/*

 * Performance events ring-buffer code:

 *

 *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>

 *  Copyright (C) 2008-2011 Red Hat, Inc., Ingo Molnar

 *  Copyright (C) 2008-2011 Red Hat, Inc., Peter Zijlstra

 *  Copyright  ©  2009 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

/*

 * We need to ensure a later event_id doesn't publish a head when a former

 * event isn't done writing. However since we need to deal with NMIs we

 * cannot fully serialize things.

 *

 * We only publish the head (and generate a wakeup) when the outer-most

 * event completes.

	/*

	 * Avoid an explicit LOAD/STORE such that architectures with memops

	 * can use them.

	/*

	 * If this isn't the outermost nesting, we don't have to update

	 * @rb->user_page->data_head.

	/*

	 * In order to avoid publishing a head value that goes backwards,

	 * we must ensure the load of @rb->head happens after we've

	 * incremented @rb->nest.

	 *

	 * Otherwise we can observe a @rb->head value before one published

	 * by an IRQ/NMI happening between the load and the increment.

	/*

	 * IRQ/NMI can happen here and advance @rb->head, causing our

	 * load above to be stale.

	/*

	 * Since the mmap() consumer (userspace) can run on a different CPU:

	 *

	 *   kernel				user

	 *

	 *   if (LOAD ->data_tail) {		LOAD ->data_head

	 *			(A)		smp_rmb()	(C)

	 *	STORE $data			LOAD $data

	 *	smp_wmb()	(B)		smp_mb()	(D)

	 *	STORE ->data_head		STORE ->data_tail

	 *   }

	 *

	 * Where A pairs with D, and B pairs with C.

	 *

	 * In our case (A) is a control dependency that separates the load of

	 * the ->data_tail and the stores of $data. In case ->data_tail

	 * indicates there is no room in the buffer to store $data we do not.

	 *

	 * D needs to be a full barrier since it separates the data READ

	 * from the tail WRITE.

	 *

	 * For B a WMB is sufficient since it separates two WRITEs, and for C

	 * an RMB is sufficient since it separates two READs.

	 *

	 * See perf_output_begin().

 B, matches C */

	/*

	 * We must publish the head before decrementing the nest count,

	 * otherwise an IRQ/NMI can publish a more recent head value and our

	 * write will (temporarily) publish a stale value.

	/*

	 * Ensure we decrement @rb->nest before we validate the @rb->head.

	 * Otherwise we cannot be sure we caught the 'last' nested update.

	/*

	 * For inherited events we send all the output towards the parent.

		/*

		 * The above forms a control dependency barrier separating the

		 * @tail load above from the data stores below. Since the @tail

		 * load is required to compute the branch to fail below.

		 *

		 * A, matches D; the full memory barrier userspace SHOULD issue

		 * after reading the data and before storing the new tail

		 * position.

		 *

		 * See perf_output_put_handle().

	/*

	 * We rely on the implied barrier() by local_cmpxchg() to ensure

	 * none of the data stores below can be lifted up by the compiler.

 XXX mostly redundant; @data is already fully initializes */

	/*

	 * perf_output_begin() only checks rb->paused, therefore

	 * rb->paused must be true if we have no pages for output.

	/*

	 * OVERWRITE is determined by perf_aux_output_end() and can't

	 * be passed in directly.

/*

 * This is called before hardware starts writing to the AUX area to

 * obtain an output handle and make sure there's room in the buffer.

 * When the capture completes, call perf_aux_output_end() to commit

 * the recorded data to the buffer.

 *

 * The ordering is similar to that of perf_output_{begin,end}, with

 * the exception of (B), which should be taken care of by the pmu

 * driver, since ordering rules will differ depending on hardware.

 *

 * Call this from pmu::start(); see the comment in perf_aux_output_end()

 * about its use in pmu callbacks. Both can also be called from the PMI

 * handler if needed.

	/*

	 * Since this will typically be open across pmu::add/pmu::del, we

	 * grab ring_buffer's refcount instead of holding rcu read lock

	 * to make sure it doesn't disappear under us.

	/*

	 * If aux_mmap_count is zero, the aux buffer is in perf_mmap_close(),

	 * about to get freed, so we leave immediately.

	 *

	 * Checking rb::aux_mmap_count and rb::refcount has to be done in

	 * the same order, see perf_mmap_close. Otherwise we end up freeing

	 * aux pages in this path, which is a bug, because in_atomic().

	/*

	 * Nesting is not supported for AUX area, make sure nested

	 * writers are caught early

	/*

	 * In overwrite mode, AUX data stores do not depend on aux_tail,

	 * therefore (A) control dependency barrier does not exist. The

	 * (B) <-> (C) ordering is still observed by the pmu driver.

		/*

		 * handle->size computation depends on aux_tail load; this forms a

		 * control dependency barrier separating aux_tail load from aux data

		 * store that will be enabled on successful return

 A, matches D */

 can't be last */

/*

 * Commit the data written by hardware into the ring buffer by adjusting

 * aux_head and posting a PERF_RECORD_AUX into the perf buffer. It is the

 * pmu driver's responsibility to observe ordering rules of the hardware,

 * so that all the data is externally visible before this is called.

 *

 * Note: this has to be called from pmu::stop() callback, as the assumption

 * of the AUX buffer management code is that after pmu::stop(), the AUX

 * transaction must be stopped and therefore drop the AUX reference count.

 in overwrite mode, driver provides aux_head via handle */

	/*

	 * Only send RECORD_AUX if we have something useful to communicate

	 *

	 * Note: the OVERWRITE records by themselves are not considered

	 * useful, as they don't communicate any *new* information,

	 * aside from the short-lived offset, that becomes history at

	 * the next event sched-in and therefore isn't useful.

	 * The userspace that needs to copy out AUX data in overwrite

	 * mode should know to use user_page::aux_head for the actual

	 * offset. So, from now on we don't output AUX records that

	 * have *only* OVERWRITE flag set.

 can't be last */

/*

 * Skip over a given number of bytes in the AUX buffer, due to, for example,

 * hardware's alignment constraints.

 this is only valid between perf_aux_output_begin and *_end */

/*

 * Copy out AUX data from an AUX handle.

		/*

		 * Communicate the allocation size to the driver:

		 * if we managed to secure a high-order allocation,

		 * set its first page's private to this order;

		 * !PagePrivate(page) means it's just a normal page.

	/*

	 * Should never happen, the last reference should be dropped from

	 * perf_mmap_close() path, which first stops aux transactions (which

	 * in turn are the atomic holders of aux_refcount) and then does the

	 * last rb_free_aux().

		/*

		 * Watermark defaults to half the buffer, and so does the

		 * max_order, to aid PMU drivers in double buffering.

		/*

		 * Use aux_watermark as the basis for chunking to

		 * help PMU drivers honor the watermark.

		/*

		 * We need to start with the max_order that fits in nr_pages,

		 * not the other way around, hence ilog2() and not get_order.

	/*

	 * In overwrite mode, PMUs that don't support SG may not handle more

	 * than one contiguous allocation, since they rely on PMI to do double

	 * buffering. In this case, the entire buffer has to be one contiguous

	 * chunk.

	/*

	 * aux_pages (and pmu driver's private data, aux_priv) will be

	 * referenced in both producer's and consumer's contexts, thus

	 * we keep a refcount here to make sure either of the two can

	 * reference them safely.

/*

 * Back perf_mmap() with regular GFP_KERNEL-0 pages.

 The '>' counts in the user page. */

 The '<=' counts in the user page. */

 above AUX space */

 AUX space */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2007 Alan Stern

 * Copyright (C) IBM Corporation, 2009

 * Copyright (C) 2009, Frederic Weisbecker <fweisbec@gmail.com>

 *

 * Thanks to Ingo Molnar for his many suggestions.

 *

 * Authors: Alan Stern <stern@rowland.harvard.edu>

 *          K.Prasad <prasad@linux.vnet.ibm.com>

 *          Frederic Weisbecker <fweisbec@gmail.com>

/*

 * HW_breakpoint: a unified kernel/user-space hardware breakpoint facility,

 * using the CPU's debug registers.

 * This file contains the arch-independent routines.

/*

 * Constraints data

 Number of pinned cpu breakpoints in a cpu */

 tsk_pinned[n] is the number of tasks having n+1 breakpoints */

 Number of non-pinned cpu/task breakpoints in a cpu */

 XXX: placeholder, see fetch_this_slot() */

 Keep track of the breakpoints attached to tasks */

 Gather the number of total pinned and un-pinned bp in a cpuset */

 Serialize accesses to the above constraints */

/*

 * Report the maximum number of pinned breakpoints a task

 * have in this cpu

/*

 * Count the number of breakpoints of the same type and same task.

 * The given event must be not on the list.

/*

 * Report the number of pinned/un-pinned breakpoints we have in

 * a given cpu (cpu > -1) or in all of them (cpu = -1).

/*

 * For now, continue to consider flexible as pinned, until we can

 * ensure no flexible event can ever be scheduled before a pinned event

 * in a same cpu.

/*

 * Add a pinned breakpoint for the given task in our constraint table

/*

 * Add/remove the given breakpoint in our constraint table

 Pinned counter cpu profiling */

 Pinned counter task profiling */

/*

 * Function to perform processor-specific cleanup during unregistration

	/*

	 * A weak stub function here for those archs that don't define

	 * it inside arch/.../kernel/hw_breakpoint.c

/*

 * Constraints to check before allowing this new breakpoint counter:

 *

 *  == Non-pinned counter == (Considered as pinned for now)

 *

 *   - If attached to a single cpu, check:

 *

 *       (per_cpu(info->flexible, cpu) || (per_cpu(info->cpu_pinned, cpu)

 *           + max(per_cpu(info->tsk_pinned, cpu)))) < HBP_NUM

 *

 *       -> If there are already non-pinned counters in this cpu, it means

 *          there is already a free slot for them.

 *          Otherwise, we check that the maximum number of per task

 *          breakpoints (for this cpu) plus the number of per cpu breakpoint

 *          (for this cpu) doesn't cover every registers.

 *

 *   - If attached to every cpus, check:

 *

 *       (per_cpu(info->flexible, *) || (max(per_cpu(info->cpu_pinned, *))

 *           + max(per_cpu(info->tsk_pinned, *)))) < HBP_NUM

 *

 *       -> This is roughly the same, except we check the number of per cpu

 *          bp for every cpu and we keep the max one. Same for the per tasks

 *          breakpoints.

 *

 *

 * == Pinned counter ==

 *

 *   - If attached to a single cpu, check:

 *

 *       ((per_cpu(info->flexible, cpu) > 1) + per_cpu(info->cpu_pinned, cpu)

 *            + max(per_cpu(info->tsk_pinned, cpu))) < HBP_NUM

 *

 *       -> Same checks as before. But now the info->flexible, if any, must keep

 *          one register at least (or they will never be fed).

 *

 *   - If attached to every cpus, check:

 *

 *       ((per_cpu(info->flexible, *) > 1) + max(per_cpu(info->cpu_pinned, *))

 *            + max(per_cpu(info->tsk_pinned, *))) < HBP_NUM

 We couldn't initialize breakpoint constraints on boot */

 Basic checks */

	/*

	 * Simulate the addition of this breakpoint to the constraints

	 * and see the result.

 Flexible counters need to keep at least one slot */

		/*

		 * Reserve the old_type slot back in case

		 * there's no space for the new type.

		 *

		 * This must succeed, because we just released

		 * the old_type slot in the __release_bp_slot

		 * call above. If not, something is broken.

/*

 * Allow the kernel debugger to reserve breakpoint slots without

 * taking a lock using the dbg_* variant of for the reserve and

 * release breakpoint slots.

		/*

		 * Don't let unprivileged users set a breakpoint in the trap

		 * path to avoid trap recursion attacks.

/**

 * register_user_hw_breakpoint - register a hardware breakpoint for user space

 * @attr: breakpoint attributes

 * @triggered: callback to trigger when we hit the breakpoint

 * @context: context data could be used in the triggered callback

 * @tsk: pointer to 'task_struct' of the process to which the address belongs

/**

 * modify_user_hw_breakpoint - modify a user-space hardware breakpoint

 * @bp: the breakpoint structure to modify

 * @attr: new breakpoint attributes

	/*

	 * modify_user_hw_breakpoint can be invoked with IRQs disabled and hence it

	 * will not be possible to raise IPIs that invoke __perf_event_disable.

	 * So call the function directly after making sure we are targeting the

	 * current task.

/**

 * unregister_hw_breakpoint - unregister a user-space hardware breakpoint

 * @bp: the breakpoint structure to unregister

/**

 * register_wide_hw_breakpoint - register a wide breakpoint in the kernel

 * @attr: breakpoint attributes

 * @triggered: callback to trigger when we hit the breakpoint

 * @context: context data could be used in the triggered callback

 *

 * @return a set of per_cpu pointers to perf events

/**

 * unregister_wide_hw_breakpoint - unregister a wide breakpoint in the kernel

 * @cpu_events: the per cpu set of events to unregister

 we need to be notified first */

	/*

	 * no branch sampling for breakpoint events

 could eventually get its own */

 SPDX-License-Identifier: GPL-2.0

/*

 * Performance events core code:

 *

 *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>

 *  Copyright (C) 2008-2011 Red Hat, Inc., Ingo Molnar

 *  Copyright (C) 2008-2011 Red Hat, Inc., Peter Zijlstra

 *  Copyright  ©  2009 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

 -EAGAIN */

		/*

		 * Now that we're on right CPU with IRQs disabled, we can test

		 * if we hit the right task without races.

 No such (running) process */

/**

 * task_function_call - call a function on the cpu on which a task runs

 * @p:		the task to evaluate

 * @func:	the function to be called

 * @info:	the function call argument

 *

 * Calls the function @func when the task is currently running. This might

 * be on the current CPU, which just calls the function directly.  This will

 * retry due to any failures in smp_call_function_single(), such as if the

 * task_cpu() goes offline concurrently.

 *

 * returns @func return value or -ESRCH or -ENXIO when the process isn't running

/**

 * cpu_function_call - call a function on the cpu

 * @cpu:	target cpu to queue this function

 * @func:	the function to be called

 * @info:	the function call argument

 *

 * Calls the function @func on the remote cpu.

 *

 * returns: @func return value or -ENXIO when the cpu is offline

 No such CPU */

/*

 * On task ctx scheduling...

 *

 * When !ctx->nr_events a task context will not be scheduled. This means

 * we can disable the scheduler hooks (for performance) without leaving

 * pending task ctx state.

 *

 * This however results in two special cases:

 *

 *  - removing the last event from a task ctx; this is relatively straight

 *    forward and is done in __perf_remove_from_context.

 *

 *  - adding the first event to a task ctx; this is tricky because we cannot

 *    rely on ctx->is_active and therefore cannot use event_function_call().

 *    See perf_install_in_context().

 *

 * If ctx->nr_events, then ctx->is_active and cpuctx->task_ctx are set.

	/*

	 * Since we do the IPI call without holding ctx->lock things can have

	 * changed, double check we hit the task we set out to hit.

		/*

		 * We only use event_function_call() on established contexts,

		 * and event_function() is only ever called when active (or

		 * rather, we'll have bailed in task_function_call() or the

		 * above ctx->task != current test), therefore we must have

		 * ctx->is_active here.

		/*

		 * And since we have ctx->is_active, cpuctx->task_ctx must

		 * match.

 verified in event_function */

		/*

		 * If this is a !child event, we must hold ctx::mutex to

		 * stabilize the event->ctx relation. See

		 * perf_event_ctx_lock().

	/*

	 * Reload the task pointer, it might have been changed by

	 * a concurrent perf_event_context_sched_out().

/*

 * Similar to event_function_call() + event_function(), but hard assumes IRQs

 * are already disabled and we're on the right CPU.

		/*

		 * We must be either inactive or active and the right task,

		 * otherwise we're screwed, since we cannot IPI to somewhere

		 * else.

/*

 * branch priv levels that need permission checks

 see ctx_resched() for details */

/*

 * perf_sched_events : >0 events exist

 * perf_cgroup_events: >0 per-cpu cgroup events exist on this cpu

/*

 * perf event paranoia level:

 *  -1 - not paranoid at all

 *   0 - disallow raw tracepoint access for unpriv

 *   1 - disallow cpu events for unpriv

 *   2 - disallow kernel profiling for unpriv

 Minimum for 512 kiB + 1 user control page */

 'free' kiB per user */

/*

 * max perf event sample rate

	/*

	 * If throttling is disabled don't allow the write:

/*

 * perf samples are done in some very critical code paths (NMIs).

 * If they take too much CPU time, the system can lock up and not

 * get any real work done.  This will drop the sample rate when

 * we detect that events are taking too long.

 Decay the counter by 1 average sample. */

	/*

	 * Note: this will be biased artifically low until we have

	 * seen NR_ACCUMULATED_SAMPLES. Doing it this way keeps us

	 * from having to maintain a count.

	/*

	 * Compute a throttle threshold 25% below the current duration.

/*

 * State based event timekeeping...

 *

 * The basic idea is to use event->state to determine which (if any) time

 * fields to increment with the current delta. This means we only need to

 * update timestamps when we change state or when they are explicitly requested

 * (read).

 *

 * Event groups make things a little more complicated, but not terribly so. The

 * rules for a group are that if the group leader is OFF the entire group is

 * OFF, irrespecive of what the group member states are. This results in

 * __perf_effective_state().

 *

 * A futher ramification is that when a group leader flips between OFF and

 * !OFF, we need to update all group member times.

 *

 *

 * NOTE: perf_event_time() is based on the (cgroup) context time, and thus we

 * need to make sure the relevant context time is updated before we try and

 * update our timestamps.

	/*

	 * If a group leader gets enabled/disabled all its siblings

	 * are affected too.

 @event doesn't care about cgroup */

 wants specific cgroup scope but @cpuctx isn't associated with any */

	/*

	 * Cgroup scoping is recursive.  An event enabled for a cgroup is

	 * also enabled for all its descendant cgroups.  If @cpuctx's

	 * cgroup is a descendant of @event's (the test covers identity

	 * case), it's a match.

	/*

	 * ensure we access cgroup data only when needed and

	 * when we know the cgroup is pinned (css_get)

	/*

	 * Do not update time when cgroup is not active

	/*

	 * ctx->lock held by caller

	 * ensure we do not access cgroup data

	 * unless we have the cgroup pinned (css_get)

 cgroup switch out every event */

 cgroup switch in events based on task */

/*

 * reschedule events based on the cgroup constraint of task.

 *

 * mode SWOUT : schedule out everything

 * mode SWIN : schedule in based on cgroup for next

	/*

	 * Disable interrupts and preemption to avoid this CPU's

	 * cgrp_cpuctx_entry to change under us.

			/*

			 * must not be done before ctxswout due

			 * to event_filter_match() in event_sched_out()

			/*

			 * set cgrp before ctxsw in to allow

			 * event_filter_match() to not have to pass

			 * task around

			 * we pass the cpuctx->ctx to perf_cgroup_from_task()

			 * because cgorup events are only per-cpu

	/*

	 * we come here when we know perf_cgroup_events > 0

	 * we do not need to pass the ctx here because we know

	 * we are holding the rcu lock

	/*

	 * only schedule out current cgroup events if we know

	 * that we are switching to a different cgroup. Otherwise,

	 * do no touch the cgroup events.

	/*

	 * we come here when we know perf_cgroup_events > 0

	 * we do not need to pass the ctx here because we know

	 * we are holding the rcu lock

	/*

	 * only need to schedule in cgroup events if we are changing

	 * cgroup during ctxsw. Cgroup events were not scheduled

	 * out of ctxsw out if that was not the case.

	/*

	 * Allow storage to have sufficent space for an iterator for each

	 * possibly nested cgroup plus an iterator for events with no cgroup.

	/*

	 * all events in a group must monitor

	 * the same cgroup because a task belongs

	 * to only one perf cgroup at a time

	/*

	 * Because cgroup events are always per-cpu events,

	 * @ctx == &cpuctx->ctx.

	/*

	 * Since setting cpuctx->cgrp is conditional on the current @cgrp

	 * matching the event's cgroup, we must do this for every new event,

	 * because if the first would mismatch, the second would not try again

	 * and we would leave cpuctx->cgrp unset.

	/*

	 * Because cgroup events are always per-cpu events,

	 * @ctx == &cpuctx->ctx.

 !CONFIG_CGROUP_PERF */

/*

 * set default to be dependent on timer tick just

 * like original code

/*

 * function must be called with interrupts disabled

 no multiplexing needed for SW PMU */

	/*

	 * check default is sane, if not set then force to

	 * default interval (1/tick)

 not for SW PMU */

/*

 * perf_event_ctx_activate(), perf_event_ctx_deactivate(), and

 * perf_event_task_tick() are fully serialized because they're strictly cpu

 * affine and perf_event_ctx{activate,deactivate} are called with IRQs

 * disabled, while perf_event_task_tick is called from IRQ context.

/*

 * Because of perf_event::ctx migration in sys_perf_event_open::move_group and

 * perf_pmu_migrate_context() we need some magic.

 *

 * Those places that change perf_event::ctx will hold both

 * perf_event_ctx::mutex of the 'old' and 'new' ctx value.

 *

 * Lock ordering is by mutex address. There are two other sites where

 * perf_event_context::mutex nests and those are:

 *

 *  - perf_event_exit_task_context()	[ child , 0 ]

 *      perf_event_exit_event()

 *        put_event()			[ parent, 1 ]

 *

 *  - perf_event_init_context()		[ parent, 0 ]

 *      inherit_task_group()

 *        inherit_group()

 *          inherit_event()

 *            perf_event_alloc()

 *              perf_init_event()

 *                perf_try_init_event()	[ child , 1 ]

 *

 * While it appears there is an obvious deadlock here -- the parent and child

 * nesting levels are inverted between the two. This is in fact safe because

 * life-time rules separate them. That is an exiting task cannot fork, and a

 * spawning task cannot (yet) exit.

 *

 * But remember that these are parent<->child context relations, and

 * migration does not affect children, therefore these two orderings should not

 * interact.

 *

 * The change in perf_event::ctx does not affect children (as claimed above)

 * because the sys_perf_event_open() case will install a new event and break

 * the ctx parent<->child relation, and perf_pmu_migrate_context() is only

 * concerned with cpuctx and that doesn't have children.

 *

 * The places that change perf_event::ctx will issue:

 *

 *   perf_remove_from_context();

 *   synchronize_rcu();

 *   perf_install_in_context();

 *

 * to affect the change. The remove_from_context() + synchronize_rcu() should

 * quiesce the event, after which we can install it in the new location. This

 * means that only external vectors (perf_fops, prctl) can perturb the event

 * while in transit. Therefore all such accessors should also acquire

 * perf_event_context::mutex to serialize against this.

 *

 * However; because event->ctx can change while we're waiting to acquire

 * ctx->mutex we must be careful and use the below perf_event_ctx_lock()

 * function.

 *

 * Lock order:

 *    exec_update_lock

 *	task_struct::perf_event_mutex

 *	  perf_event_context::mutex

 *	    perf_event::child_mutex;

 *	      perf_event_context::lock

 *	    perf_event::mmap_mutex

 *	    mmap_lock

 *	      perf_addr_filters_head::lock

 *

 *    cpu_hotplug_lock

 *      pmus_lock

 *	  cpuctx->mutex / perf_event_context::mutex

/*

 * This must be done under the ctx->lock, such as to serialize against

 * context_equiv(), therefore we cannot call put_ctx() since that might end up

 * calling scheduler related locks and ctx->lock nests inside those.

	/*

	 * only top level events have the pid namespace they were created in

 avoid -1 if it is idle thread or runs in another ns */

/*

 * If we inherit events we want to return the parent event id

 * to userspace.

/*

 * Get the perf_event_context for a task and lock it.

 *

 * This has to cope with the fact that until it is locked,

 * the context could get moved to another task.

	/*

	 * One of the few rules of preemptible RCU is that one cannot do

	 * rcu_read_unlock() while holding a scheduler (or nested) lock when

	 * part of the read side critical section was irqs-enabled -- see

	 * rcu_read_unlock_special().

	 *

	 * Since ctx->lock nests under rq->lock we must ensure the entire read

	 * side critical section has interrupts disabled.

		/*

		 * If this context is a clone of another, it might

		 * get swapped for another underneath us by

		 * perf_event_task_sched_out, though the

		 * rcu_read_lock() protects us from any context

		 * getting freed.  Lock the context and check if it

		 * got swapped before we could get the lock, and retry

		 * if so.  If we locked the right context, then it

		 * can't get swapped on us any more.

/*

 * Get the context for a task and increment its pin_count so it

 * can't get swapped to another task.  This also increments its

 * reference count so that the context can't get freed.

/*

 * Update the record of the current time in a context.

	/*

	 * It's 'group type', really, because if our group leader is

	 * pinned, so are we.

/*

 * Helper function to initialize event group nodes.

/*

 * Extract pinned or flexible groups from the context

 * based on event attrs bits.

/*

 * Helper function to initializes perf_event_group trees.

/*

 * Compare function for event groups;

 *

 * Implements complex key that first sorts by CPU and then by virtual index

 * which provides ordering when rotating groups for the same CPU.

				/*

				 * Left has no cgroup but right does, no

				 * cgroups come first.

				/*

				 * Right has no cgroup but left does, no

				 * cgroups come first.

 Two dissimilar cgroups, order by id. */

 partial/subtree match: @cpu, @cgroup; ignore: @group_index */

/*

 * Insert @event into @groups' tree; using {@event->cpu, ++@groups->index} for

 * key (see perf_event_groups_less). This places it last inside the CPU

 * subtree.

/*

 * Helper function to insert event into the pinned or flexible groups.

/*

 * Delete a group from a tree.

/*

 * Helper function to delete event from its groups.

/*

 * Get the leftmost event in the cpu/cgroup subtree.

/*

 * Like rb_entry_next_safe() for the @cpu subtree.

/*

 * Iterate through the whole groups tree.

/*

 * Add an event from the lists for its context.

 * Must be called with ctx->mutex and ctx->lock held.

	/*

	 * If we're a stand alone event or group leader, we go to the context

	 * list, group events are kept attached to the group so that

	 * perf_group_detach can, at all times, locate all siblings.

/*

 * Initialize event state based on the perf_event_attr::disabled.

 value */

/*

 * Called at perf_event creation and when events are attached/detached from a

 * group.

	/*

	 * The values computed here will be over-written when we actually

	 * attach the event.

	/*

	 * Sum the lot; should not exceed the 64k limit we have on records.

	 * Conservative limit to allow for callchains and other variable fields.

	/*

	 * We can have double attach due to group movement in perf_event_open.

/*

 * Remove an event from the lists for its context.

 * Must be called with ctx->mutex and ctx->lock held.

	/*

	 * We can have double detach due to exit/hot-unplug + close.

	/*

	 * If event was in error state, then keep it

	 * that way, otherwise bogus counts will be

	 * returned on read(). The only way to get out

	 * of error state is by explicit re-enabling

	 * of the event

	/*

	 * If event uses aux_event tear down the link

	/*

	 * If the event is an aux_event, tear down all links to

	 * it from other events.

		/*

		 * If it's ACTIVE, schedule it out and put it into ERROR

		 * state so that we don't try to schedule it again. Note

		 * that perf_event_enable() will clear the ERROR status.

	/*

	 * Our group leader must be an aux event if we want to be

	 * an aux_output. This way, the aux event will precede its

	 * aux_output events in the group, and therefore will always

	 * schedule first.

	/*

	 * aux_output and aux_sample_size are mutually exclusive.

	/*

	 * Link aux_outputs to their aux event; this is undone in

	 * perf_group_detach() by perf_put_aux_event(). When the

	 * group in torn down, the aux_output events loose their

	 * link to the aux_event and can't schedule any more.

/*

 * Events that have PERF_EV_CAP_SIBLING require being part of a group and

 * cannot exist on their own, schedule them out and move them into the ERROR

 * state. Also see _perf_event_enable(), it will not be able to recover

 * this ERROR state.

	/*

	 * We can have double detach due to exit/hot-unplug + close.

	/*

	 * If this is a sibling, remove it from its group.

	/*

	 * If this was a group event with sibling events then

	 * upgrade the siblings to singleton events by adding them

	 * to whatever list we are on.

 Inherit group flags from the previous leader */

/*

 * Check whether we should attempt to schedule an event group based on

 * PMU-specific filtering. An event group can consist of HW and SW events,

 * potentially with a SW leader, so we must check all the filters, to

 * determine whether a group is schedulable:

	/*

	 * Asymmetry; we only schedule events _IN_ through ctx_sched_in(), but

	 * we can schedule events _OUT_ individually through things like

	 * __perf_remove_from_context().

	/*

	 * Schedule out siblings (if any):

/*

 * Cross CPU call to remove a performance event

 *

 * We disable the event on the hardware level first. After that we

 * remove it from the context list.

/*

 * Remove the event from a task's (or a CPU's) list of events.

 *

 * If event->ctx is a cloned context, callers must make sure that

 * every task struct that event->ctx->task could possibly point to

 * remains valid.  This is OK when called from perf_release since

 * that only calls us on the top-level context, which can't be a clone.

 * When called from perf_event_exit_task, it's OK because the

 * context has been detached from its task.

	/*

	 * Because of perf_event_exit_task(), perf_remove_from_context() ought

	 * to work in the face of TASK_TOMBSTONE, unlike every other

	 * event_function_call() user.

/*

 * Cross CPU call to disable a performance event

/*

 * Disable an event.

 *

 * If event->ctx is a cloned context, callers must make sure that

 * every task struct that event->ctx->task could possibly point to

 * remains valid.  This condition is satisfied when called through

 * perf_event_for_each_child or perf_event_for_each because they

 * hold the top-level event's child_mutex, so any descendant that

 * goes to exit will block in perf_event_exit_event().

 *

 * When called from perf_pending_event it's OK because event->ctx

 * is the current context on this CPU and preemption is disabled,

 * hence we can't get into perf_event_task_sched_out for this context.

/*

 * Strictly speaking kernel users cannot create groups and therefore this

 * interface does not need the perf_event_ctx_lock() magic.

 can fail, see perf_pending_event_disable() */

	/*

	 * use the correct time source for the time snapshot

	 *

	 * We could get by without this by leveraging the

	 * fact that to get to this function, the caller

	 * has most likely already called update_context_time()

	 * and update_cgrp_time_xx() and thus both timestamp

	 * are identical (or very close). Given that tstamp is,

	 * already adjusted for cgroup, we could say that:

	 *    tstamp - ctx->timestamp

	 * is equivalent to

	 *    tstamp - cgrp->timestamp.

	 *

	 * Then, in perf_output_read(), the calculation would

	 * work with no changes because:

	 * - event is guaranteed scheduled in

	 * - no scheduled out in between

	 * - thus the timestamp would be the same

	 *

	 * But this is a bit hairy.

	 *

	 * So instead, we have an explicit cgroup call to remain

	 * within the time source all along. We believe it

	 * is cleaner and simpler to understand.

	/*

	 * Order event::oncpu write to happen before the ACTIVE state is

	 * visible. This allows perf_event_{stop,read}() to observe the correct

	 * ->oncpu if it sees ACTIVE.

	/*

	 * Unthrottle events, since we scheduled we might have missed several

	 * ticks already, also for a heavily scheduling task there is little

	 * guarantee it'll get a tick in a timely manner.

	/*

	 * Schedule in siblings as one group (if any):

	/*

	 * Groups can be scheduled in as one unit only, so undo any

	 * partial group before returning:

	 * The events up to the failed event are scheduled out normally.

/*

 * Work out whether we can put this event group on the CPU now.

	/*

	 * Groups consisting entirely of software events can always go on.

	/*

	 * If an exclusive group is already on, no other hardware

	 * events can go on.

	/*

	 * If this group is exclusive and there are already

	 * events on the CPU, it can't go on.

	/*

	 * Otherwise, try to add it if all previous groups were able

	 * to go on.

/*

 * We want to maintain the following priority of scheduling:

 *  - CPU pinned (EVENT_CPU | EVENT_PINNED)

 *  - task pinned (EVENT_PINNED)

 *  - CPU flexible (EVENT_CPU | EVENT_FLEXIBLE)

 *  - task flexible (EVENT_FLEXIBLE).

 *

 * In order to avoid unscheduling and scheduling back in everything every

 * time an event is added, only do it for the groups of equal priority and

 * below.

 *

 * This can be called after a batch operation on task events, in which case

 * event_type is a bit mask of the types of events involved. For CPU events,

 * event_type is only either EVENT_PINNED or EVENT_FLEXIBLE.

	/*

	 * If pinned groups are involved, flexible groups also need to be

	 * scheduled out.

	/*

	 * Decide which cpu ctx groups to schedule out based on the types

	 * of events that caused rescheduling:

	 *  - EVENT_CPU: schedule out corresponding groups;

	 *  - EVENT_PINNED task events: schedule out EVENT_FLEXIBLE groups;

	 *  - otherwise, do nothing more.

/*

 * Cross CPU call to install and enable a performance event

 *

 * Very similar to remote_function() + event_function() but cannot assume that

 * things like ctx->is_active and cpuctx->task_ctx are set.

		/*

		 * If the task is running, it must be running on this CPU,

		 * otherwise we cannot reprogram things.

		 *

		 * If its not running, we don't care, ctx->lock will

		 * serialize against it becoming runnable.

		/*

		 * If the current cgroup doesn't match the event's

		 * cgroup, we should not try to schedule it.

/*

 * Attach a performance event to a context.

 *

 * Very similar to event_function_call, see comment there.

	/*

	 * Ensures that if we can observe event->ctx, both the event and ctx

	 * will be 'complete'. See perf_iterate_sb_cpu().

	/*

	 * perf_event_attr::disabled events will not run and can be initialized

	 * without IPI. Except when this is the first event for the context, in

	 * that case we need the magic of the IPI to set ctx->is_active.

	 *

	 * The IOC_ENABLE that is sure to follow the creation of a disabled

	 * event will issue the IPI and reprogram the hardware.

	/*

	 * Should not happen, we validate the ctx is still alive before calling.

	/*

	 * Installing events is tricky because we cannot rely on ctx->is_active

	 * to be set in case this is the nr_events 0 -> 1 transition.

	 *

	 * Instead we use task_curr(), which tells us if the task is running.

	 * However, since we use task_curr() outside of rq::lock, we can race

	 * against the actual state. This means the result can be wrong.

	 *

	 * If we get a false positive, we retry, this is harmless.

	 *

	 * If we get a false negative, things are complicated. If we are after

	 * perf_event_context_sched_in() ctx::lock will serialize us, and the

	 * value must be correct. If we're before, it doesn't matter since

	 * perf_event_context_sched_in() will program the counter.

	 *

	 * However, this hinges on the remote context switch having observed

	 * our task->perf_event_ctxp[] store, such that it will in fact take

	 * ctx::lock in perf_event_context_sched_in().

	 *

	 * We do this by task_function_call(), if the IPI fails to hit the task

	 * we know any future context switch of task must see the

	 * perf_event_ctpx[] store.

	/*

	 * This smp_mb() orders the task->perf_event_ctxp[] store with the

	 * task_cpu() load, such that if the IPI then does not find the task

	 * running, a future context switch of that task must observe the

	 * store.

		/*

		 * Cannot happen because we already checked above (which also

		 * cannot happen), and we hold ctx->mutex, which serializes us

		 * against perf_event_exit_task_context().

	/*

	 * If the task is not running, ctx->lock will avoid it becoming so,

	 * thus we can safely install the event.

/*

 * Cross CPU call to enable a performance event

	/*

	 * If the event is in a group and isn't the group leader,

	 * then don't put it on unless the group is on.

/*

 * Enable an event.

 *

 * If event->ctx is a cloned context, callers must make sure that

 * every task struct that event->ctx->task could possibly point to

 * remains valid.  This condition is satisfied when called through

 * perf_event_for_each_child or perf_event_for_each as described

 * for perf_event_disable.

	/*

	 * If the event is in error state, clear that first.

	 *

	 * That way, if we see the event in error state below, we know that it

	 * has gone back into error state, as distinct from the task having

	 * been scheduled away before the cross-call arrived.

		/*

		 * Detached SIBLING events cannot leave ERROR state.

/*

 * See perf_event_disable();

 if it's already INACTIVE, do nothing */

 matches smp_wmb() in event_sched_in() */

	/*

	 * There is a window with interrupts enabled before we get here,

	 * so we need to check again lest we try to stop another CPU's event.

	/*

	 * May race with the actual stop (through perf_pmu_output_stop()),

	 * but it is only used for events with AUX ring buffer, and such

	 * events will refuse to restart because of rb::aux_mmap_count==0,

	 * see comments in perf_aux_output_begin().

	 *

	 * Since this is happening on an event-local CPU, no trace is lost

	 * while restarting.

 matches smp_wmb() in event_sched_in() */

		/*

		 * We only want to restart ACTIVE events, so if the event goes

		 * inactive here (event->oncpu==-1), there's nothing more to do;

		 * fall through with ret==-ENXIO.

/*

 * In order to contain the amount of racy and tricky in the address filter

 * configuration management, it is a two part process:

 *

 * (p1) when userspace mappings change as a result of (1) or (2) or (3) below,

 *      we update the addresses of corresponding vmas in

 *	event::addr_filter_ranges array and bump the event::addr_filters_gen;

 * (p2) when an event is scheduled in (pmu::add), it calls

 *      perf_event_addr_filters_sync() which calls pmu::addr_filters_sync()

 *      if the generation has changed since the previous call.

 *

 * If (p1) happens while the event is active, we restart it to force (p2).

 *

 * (1) perf_addr_filters_apply(): adjusting filters' offsets based on

 *     pre-existing mappings, called once when new filters arrive via SET_FILTER

 *     ioctl;

 * (2) perf_addr_filters_adjust(): adjusting filters' offsets based on newly

 *     registered mapping, called for every new mmap(), with mm::mmap_lock down

 *     for reading;

 * (3) perf_event_addr_filters_exec(): clearing filters' offsets in the process

 *     of exec.

	/*

	 * not supported on inherited events

/*

 * See perf_event_disable()

 Place holder for future additions. */

		/*

		 * See __perf_remove_from_context().

	/*

	 * Always update time if it was set; not only when it changes.

	 * Otherwise we can 'forget' to update time for any but the last

	 * context we sched out. For example:

	 *

	 *   ctx_sched_out(.event_type = EVENT_FLEXIBLE)

	 *   ctx_sched_out(.event_type = EVENT_PINNED)

	 *

	 * would only update time for the pinned events.

 update (and stop) ctx time */

 changed bits */

		/*

		 * Since we cleared EVENT_FLEXIBLE, also clear

		 * rotate_necessary, is will be reset by

		 * ctx_flexible_sched_in() when needed.

/*

 * Test whether two contexts are equivalent, i.e. whether they have both been

 * cloned from the same version of the same context.

 *

 * Equivalence is measured using a generation number in the context that is

 * incremented on each modification to it; see unclone_ctx(), list_add_event()

 * and list_del_event().

 Pinning disables the swap optimization */

 If ctx1 is the parent of ctx2 */

 If ctx2 is the parent of ctx1 */

	/*

	 * If ctx1 and ctx2 have the same parent; we flatten the parent

	 * hierarchy, see perf_event_init_context().

 Unmatched */

	/*

	 * Update the event value, we cannot use perf_event_read()

	 * because we're in the middle of a context switch and have IRQs

	 * disabled, which upsets smp_call_function_single(), however

	 * we know the event must be on the current CPU, therefore we

	 * don't need to use it.

	/*

	 * In order to keep per-task stats reliable we need to flip the event

	 * values when we flip the contexts.

	/*

	 * Since we swizzled the values, update the user visible data too.

 If neither context have a parent context; they cannot be clones. */

		/*

		 * Looks like the two contexts are clones, so we might be

		 * able to optimize the context switch.  We lock both

		 * contexts and check that they are clones under the

		 * lock (including re-checking that neither has been

		 * uncloned in the meantime).  It doesn't matter which

		 * order we take the locks because no other cpu could

		 * be trying to lock both of these tasks.

			/*

			 * PMU specific parts of task perf context can require

			 * additional synchronization. As an example of such

			 * synchronization see implementation details of Intel

			 * LBR call stack data profiling;

			/*

			 * RCU_INIT_POINTER here is safe because we've not

			 * modified the ctx and the above modification of

			 * ctx->task and ctx->task_ctx_data are immaterial

			 * since those values are always verified under

			 * ctx->lock which we're now holding.

/*

 * This function provides the context switch callback to the lower code

 * layer. It is invoked ONLY when the context switch callback is enabled.

 *

 * This callback is relevant even to per-cpu events; for example multi event

 * PEBS requires this to provide PID/TID information. This requires we flush

 * all queued PEBS records before we context switch to a new task.

 software PMUs will not have sched_task */

 will be handled in perf_event_context_sched_in/out */

/*

 * Called from scheduler to remove the events of the current task,

 * with interrupts disabled.

 *

 * We stop each event and update the event value in event->count.

 *

 * This does not protect us against NMI, but disable()

 * sets the disabled bit in the control field of event _before_

 * accessing the event control register. If a NMI hits, then it will

 * not restart the event.

	/*

	 * if cgroup events exist on this CPU, then we need

	 * to check if we have to switch out PMU state.

	 * cgroup event are system-wide mode only

/*

 * Called with IRQs disabled

 Space for per CPU and/or any CPU event iterators. */

 Events not within a CPU context may be on any CPU. */

 changed bits */

 start ctx time */

	/*

	 * First go through the list and put on any pinned groups

	 * in order to give them the best chance of going on.

 Then walk through the lower prio flexible groups */

	/*

	 * HACK: for HETEROGENEOUS the task context might have switched to a

	 * different PMU, force (re)set the context,

	/*

	 * We must check ctx->nr_events while holding ctx->lock, such

	 * that we serialize against perf_install_in_context().

	/*

	 * We want to keep the following priority order:

	 * cpu pinned (that don't need to move), task pinned,

	 * cpu flexible, task flexible.

	 *

	 * However, if task's ctx is not carrying any pinned

	 * events, no need to flip the cpuctx's events around.

/*

 * Called from scheduler to add the events of the current task

 * with interrupts disabled.

 *

 * We restore the event value and then enable it.

 *

 * This does not protect us against NMI, but enable()

 * sets the enabled bit in the control field of event _before_

 * accessing the event control register. If a NMI hits, then it will

 * keep the event running.

	/*

	 * If cgroup events exist on this CPU, then we need to check if we have

	 * to switch in PMU state; cgroup event are system-wide mode only.

	 *

	 * Since cgroup events are CPU events, we must schedule these in before

	 * we schedule in the task events.

	/*

	 * We got @count in @nsec, with a target of sample_freq HZ

	 * the target period becomes:

	 *

	 *             @count * 10^9

	 * period = -------------------

	 *          @nsec * sample_freq

	 *

	/*

	 * Reduce accuracy by one bit such that @a and @b converge

	 * to a similar magnitude.

	/*

	 * Reduce accuracy until either term fits in a u64, then proceed with

	 * the other, so that finally we can do a u64/u64 division.

 low pass filter */

/*

 * combine freq adjustment with unthrottling to avoid two passes over the

 * events. At the same time, make sure, having freq events does not change

 * the rate of unthrottling as that would introduce bias.

	/*

	 * only need to iterate over all events iff:

	 * - context have events in frequency mode (needs freq adjust)

	 * - there are events to unthrottle on this cpu

		/*

		 * stop the event and update event->count

		/*

		 * restart the event

		 * reload only if value has changed

		 * we have stopped the event so tell that

		 * to perf_adjust_period() to avoid stopping it

		 * twice.

/*

 * Move @event to the tail of the @ctx's elegible events.

	/*

	 * Rotate the first entry last of non-pinned groups. Rotation might be

	 * disabled by the inheritance code.

 pick an event from the flexible_groups to rotate */

 pick the first active flexible event */

 if no active flexible event, pick the first event */

	/*

	 * Unconditionally clear rotate_necessary; if ctx_flexible_sched_in()

	 * finds there are unschedulable events, it will set it again.

	/*

	 * Since we run this from IRQ context, nobody can install new

	 * events, thus the event count values are stable.

	/*

	 * As per the order given at ctx_resched() first 'pop' task flexible

	 * and then, if needed CPU flexible.

/*

 * Enable all of a task's events that have been marked enable-on-exec.

 * This expects task == current.

	/*

	 * Unclone and reschedule this context if we enabled any event.

/*

 * Removes all events from the current task that have been marked

 * remove-on-exec, and feeds their values back to parent events.

/*

 * Cross CPU call to read the hardware event

	/*

	 * If this is a task context, we need to check whether it is

	 * the current task context of this cpu.  If not it has been

	 * scheduled out before the smp call arrived.  In that case

	 * event->count would have been updated to a recent sample

	 * when the event was scheduled out.

			/*

			 * Use sibling's PMU rather than @event's since

			 * sibling could be on different (eg: software) PMU.

/*

 * NMI-safe method to read a local event, that is an event that

 * is:

 *   - either for the current task, or for this CPU

 *   - does not have inherit set, for inherited task events

 *     will not be local and we cannot read them atomically

 *   - must not have a pmu::count method

	/*

	 * Disabling interrupts avoids all counter scheduling (context

	 * switches, timer based rotation and IPIs).

	/*

	 * It must not be an event with inherit set, we cannot read

	 * all child counters from atomic context.

 If this is a per-task event, it must be for current */

 If this is a per-CPU event, it must be for this CPU */

 If this is a pinned event it must be running on this CPU */

	/*

	 * If the event is currently on this CPU, its either a per-task event,

	 * or local to this CPU. Furthermore it means its ACTIVE (otherwise

	 * oncpu == -1).

	/*

	 * If event is enabled and currently active on a CPU, update the

	 * value in the event structure:

		/*

		 * Orders the ->state and ->oncpu loads such that if we see

		 * ACTIVE we must also see the right ->oncpu.

		 *

		 * Matches the smp_wmb() from event_sched_in().

		/*

		 * Purposely ignore the smp_call_function_single() return

		 * value.

		 *

		 * If event_cpu isn't a valid CPU it means the event got

		 * scheduled out and that will have updated the event count.

		 *

		 * Therefore, either way, we'll have an up-to-date event count

		 * after this.

		/*

		 * May read while context is not active (e.g., thread is

		 * blocked), in that case we cannot update context time

/*

 * Initialize the perf_event context in a task_struct:

/*

 * Returns a matching context with refcount and pincount.

 Must be root to operate on a CPU event: */

		/*

		 * If it has already passed perf_event_exit_task().

		 * we must see PF_EXITING, it takes this mutex too.

/*

 * The following implement mutual exclusion of events on "exclusive" pmus

 * (PERF_PMU_CAP_EXCLUSIVE). Such pmus can only have one event scheduled

 * at a time, so we disallow creating events that might conflict, namely:

 *

 *  1) cpu-wide events in the presence of per-task events,

 *  2) per-task events in the presence of cpu-wide events,

 *  3) two matching events on the same context.

 *

 * The former two cases are handled in the allocation path (perf_event_alloc(),

 * _free_event()), the latter -- before the first perf_install_in_context().

	/*

	 * Prevent co-existence of per-task and cpu-wide events on the

	 * same exclusive pmu.

	 *

	 * Negative pmu::exclusive_cnt means there are cpu-wide

	 * events on this "exclusive" pmu, positive means there are

	 * per-task events.

	 *

	 * Since this is called in perf_event_alloc() path, event::ctx

	 * doesn't exist yet; it is, however, safe to use PERF_ATTACH_TASK

	 * to mean "per-task event", because unlike other attach states it

	 * never gets cleared.

 see comment in exclusive_event_init() */

		/*

		 * Can happen when we close an event with re-directed output.

		 *

		 * Since we have a 0 refcount, perf_mmap_close() will skip

		 * over us; possibly making our ring_buffer_put() the last.

	/*

	 * Must be after ->destroy(), due to uprobe_perf_close() using

	 * hw.target.

	/*

	 * perf_event_free_task() relies on put_ctx() being 'last', in particular

	 * all task references must be cleaned up.

/*

 * Used to free events which have a known refcount of 1, such as in error paths

 * where the event isn't exposed yet and inherited events.

 leak to avoid use-after-free */

/*

 * Remove user event from the owner task.

	/*

	 * Matches the smp_store_release() in perf_event_exit_task(). If we

	 * observe !owner it means the list deletion is complete and we can

	 * indeed free this event, otherwise we need to serialize on

	 * owner->perf_event_mutex.

		/*

		 * Since delayed_put_task_struct() also drops the last

		 * task reference we can safely take a new reference

		 * while holding the rcu_read_lock().

		/*

		 * If we're here through perf_event_exit_task() we're already

		 * holding ctx->mutex which would be an inversion wrt. the

		 * normal lock order.

		 *

		 * However we can safely take this lock because its the child

		 * ctx->mutex.

		/*

		 * We have to re-check the event->owner field, if it is cleared

		 * we raced with perf_event_exit_task(), acquiring the mutex

		 * ensured they're done, and we can proceed with freeing the

		 * event.

/*

 * Kill an event dead; while event:refcount will preserve the event

 * object, it will not preserve its functionality. Once the last 'user'

 * gives up the object, we'll destroy the thing.

	/*

	 * If we got here through err_file: fput(event_file); we will not have

	 * attached to a context yet.

	/*

	 * Mark this event as STATE_DEAD, there is no external reference to it

	 * anymore.

	 *

	 * Anybody acquiring event->child_mutex after the below loop _must_

	 * also see this, most importantly inherit_event() which will avoid

	 * placing more children on the list.

	 *

	 * Thus this guarantees that we will in fact observe and kill _ALL_

	 * child events.

		/*

		 * Cannot change, child events are not migrated, see the

		 * comment with perf_event_ctx_lock_nested().

		/*

		 * Since child_mutex nests inside ctx::mutex, we must jump

		 * through hoops. We start by grabbing a reference on the ctx.

		 *

		 * Since the event cannot get freed while we hold the

		 * child_mutex, the context must also exist and have a !0

		 * reference count.

		/*

		 * Now that we have a ctx ref, we can drop child_mutex, and

		 * acquire ctx::mutex without fear of it going away. Then we

		 * can re-acquire child_mutex.

		/*

		 * Now that we hold ctx::mutex and child_mutex, revalidate our

		 * state, if child is still the first entry, it didn't get freed

		 * and we can continue doing so.

			/*

			 * This matches the refcount bump in inherit_event();

			 * this can't be the last reference.

		/*

		 * Wake any perf_event_free_task() waiting for this event to be

		 * freed.

 pairs with wait_var_event() */

 Must be the 'last' reference */

/*

 * Called when the last reference to the file is gone.

 skip @nr */

	/*

	 * Since we co-schedule groups, {enabled,running} times of siblings

	 * will be identical to those of the leader, so we only publish one

	 * set.

	/*

	 * Write {count,id} tuples for every sibling.

	/*

	 * By locking the child_mutex of the leader we effectively

	 * lock the child list of all siblings.. XXX explain how.

/*

 * Read the performance event - simple non blocking version for now

	/*

	 * Return end-of-file for a read on an event that is in

	 * error state (i.e. because it was pinned but it couldn't be

	 * scheduled on to the CPU at some point).

	/*

	 * Pin the event->rb by taking event->mmap_mutex; otherwise

	 * perf_event_set_output() can swizzle our rb and make us miss wakeups.

 Assume it's not an event with inherit set. */

/*

 * Holding the top-level event's child_mutex means that any

 * descendant process that has inherited this event will block

 * in perf_event_exit_event() if it goes to exit, thus satisfying the

 * task existence requirements of perf_event_enable/disable.

		/*

		 * We could be throttled; unthrottle now to avoid the tick

		 * trying to unthrottle while we already re-started the event.

 Treat ioctl like writes as it is likely a mutating operation. */

 Fix up pointer size (usually 4 -> 8 in 32-on-64-bit case */

 Allow new userspace to detect that bit 0 is deprecated */

/*

 * Callers need to ensure there can be no nesting of this function, otherwise

 * the seqlock logic goes bad. We can not serialize this because the arch

 * code calls this from NMI context.

	/*

	 * compute total_time_enabled, total_time_running

	 * based on snapshot values taken when the event

	 * was last scheduled in.

	 *

	 * we cannot simply called update_context_time()

	 * because of locking issue as we can be called in

	 * NMI context

	/*

	 * Disable preemption to guarantee consistent time stamps are stored to

	 * the user page.

		/*

		 * Should be impossible, we set this when removing

		 * event->rb_entry and wait/clear when adding event->rb_entry.

	/*

	 * Avoid racing with perf_mmap_close(AUX): stop the event

	 * before swizzling the event::rb pointer; if it's getting

	 * unmapped, its aux_mmap_count will be 0 and it won't

	 * restart. See the comment in __perf_pmu_output_stop().

	 *

	 * Data will inevitably be lost when set_output is done in

	 * mid-air, but then again, whoever does it like this is

	 * not in for the data anyway.

		/*

		 * Since we detached before setting the new rb, so that we

		 * could attach the new rb, we could have missed a wakeup.

		 * Provide it now.

/*

 * A buffer can be mmap()ed multiple times; either directly through the same

 * event, or through other events by use of perf_event_set_output().

 *

 * In order to undo the VM accounting done by perf_mmap() we need to destroy

 * the buffer here, where we still have a VM context. This means we need

 * to detach all events redirecting to us.

	/*

	 * rb->aux_mmap_count will always drop before rb->mmap_count and

	 * event->mmap_count, so it is ok to use event->mmap_mutex to

	 * serialize with perf_mmap here.

		/*

		 * Stop all AUX events that are writing to this buffer,

		 * so that we can free its AUX pages and corresponding PMU

		 * data. Note that after rb::aux_mmap_count dropped to zero,

		 * they won't start any more (see perf_aux_output_begin()).

 now it's safe to free the pages */

 this has to be the last one */

 If there's still other mmap()s of this buffer, we're done. */

	/*

	 * No other mmap()s, detach from all other events that might redirect

	 * into the now unreachable buffer. Somewhat complicated by the

	 * fact that rb::event_lock otherwise nests inside mmap_mutex.

			/*

			 * This event is en-route to free_event() which will

			 * detach it and remove it from the list.

		/*

		 * Check we didn't race with perf_event_set_output() which can

		 * swizzle the rb from under us while we were waiting to

		 * acquire mmap_mutex.

		 *

		 * If we find a different rb; ignore this event, a next

		 * iteration will no longer find it on the list. We have to

		 * still restart the iteration to make sure we're not now

		 * iterating the wrong list.

		/*

		 * Restart the iteration; either we're on the wrong list or

		 * destroyed its integrity by doing a deletion.

	/*

	 * It could be there's still a few 0-ref events on the list; they'll

	 * get cleaned up by free_event() -- they'll also still have their

	 * ref on the rb and will free it whenever they are done with it.

	 *

	 * Aside from that, this buffer is 'fully' detached and unmapped,

	 * undo the VM accounting.

 could be last */

 non mergeable */

	/*

	 * Don't allow mmap() of inherited per-task counters. This would

	 * create a performance issue due to all children writing to the

	 * same rb.

		/*

		 * AUX area mapping: if rb->aux_nr_pages != 0, it's already

		 * mapped, all subsequent mappings should have the same size

		 * and offset. Must be above the normal perf buffer.

 already mapped with a different offset */

 already mapped with a different size */

	/*

	 * If we have rb pages ensure they're a power-of-two number, so we

	 * can do bitmasks instead of modulo.

			/*

			 * Raced against perf_mmap_close() through

			 * perf_event_set_output(). Try again, hope for better

			 * luck.

	/*

	 * Increase the limit linearly with more CPUs:

	/*

	 * sysctl_perf_event_mlock may have changed, so that

	 *     user->locked_vm > user_lock_limit

		/*

		 * charge locked_vm until it hits user_lock_limit;

		 * charge the rest from pinned_vm

	/*

	 * Since pinned accounting is per vm we cannot allow fork() to copy our

	 * vma.

/*

 * Perf event wakeup

 *

 * If there's data, ensure we set the poll() state and publish everything

 * to user-space before waking everybody up.

 only the parent has fasync state */

	/*

	 * We'd expect this to only occur if the irq_work is delayed and either

	 * ctx->task or current has changed in the meantime. This can be the

	 * case on architectures that do not implement arch_irq_work_raise().

	/*

	 * perf_pending_event() can race with the task exiting.

 rearm event */

	/*

	 *  CPU-A			CPU-B

	 *

	 *  perf_event_disable_inatomic()

	 *    @pending_disable = CPU-A;

	 *    irq_work_queue();

	 *

	 *  sched-out

	 *    @pending_disable = -1;

	 *

	 *				sched-in

	 *				perf_event_disable_inatomic()

	 *				  @pending_disable = CPU-B;

	 *				  irq_work_queue(); // FAILS

	 *

	 *  irq_work_run()

	 *    perf_pending_event()

	 *

	 * But the event runs on CPU-B and wants disabling there.

	/*

	 * If we 'fail' here, that's OK, it means recursion is already disabled

	 * and we won't recurse 'further'.

/*

 * We assume there is only KVM supporting the callbacks.

 * Later on, we might change it to a list if there is

 * another virtualization implementation supporting the callbacks.

/*

 * Get remaining task size from user stack pointer.

 *

 * It'd be better to take stack vma map and limit this more

 * precisely, but there's no way to get it safely under interrupt,

 * so using TASK_SIZE as limit.

 No regs, no stack pointer, no dump. */

	/*

	 * Check if we fit in with the requested stack size into the:

	 * - TASK_SIZE

	 *   If we don't, we limit the size to the TASK_SIZE.

	 *

	 * - remaining sample size

	 *   If we don't, we customize the stack size to

	 *   fit in to the remaining sample size.

 Current header size plus static size and dynamic size. */

 Do we fit in with the current stack dump size? */

		/*

		 * If we overflow the maximum size for the sample,

		 * we customize the stack dump size to fit in.

 Case of a kernel thread, nothing to dump */

		/*

		 * We dump:

		 * static size

		 *   - the size requested by user or the best one we can fit

		 *     in to the sample max size

		 * data

		 *   - user stack dump data

		 * dynamic size

		 *   - the actual dumped size

 Static size. */

 Data. */

 Dynamic size. */

	/*

	 * If this is an NMI hit inside sampling code, don't take

	 * the sample. See also perf_aux_sample_output().

	/*

	 * Normal ->start()/->stop() callbacks run in IRQ mode in scheduler

	 * paths. If we start calling them in NMI context, they may race with

	 * the IRQ ones, that is, for example, re-starting an event that's just

	 * been stopped, which is why we're using a separate callback that

	 * doesn't change the event state.

	 *

	 * IRQs need to be disabled to prevent IPIs from racing with us.

	/*

	 * Guard against NMI hits inside the critical section;

	 * see also perf_prepare_sample_aux().

	/*

	 * An error here means that perf_output_copy() failed (returned a

	 * non-zero surplus that it didn't copy), which in its current

	 * enlightened implementation is not possible. If that changes, we'd

	 * like to know.

	/*

	 * The pad comes from ALIGN()ing data->aux_size up to u64 in

	 * perf_prepare_sample_aux(), so should not be more than that.

 namespace issues */

/*

 * XXX PERF_SAMPLE_READ vs inherited events seems difficult.

 *

 * The problem is that its both hard and excessively expensive to iterate the

 * child list, not to mention that its impossible to IPI the children running

 * on another CPU, from interrupt/NMI context.

	/*

	 * compute total_time_enabled, total_time_running

	 * based on snapshot values taken when the event

	 * was last scheduled in.

	 *

	 * we cannot simply called update_context_time()

	 * because of locking issue as we are called in

	 * NMI context

			/*

			 * we always store at least the value of nr

		/*

		 * If there are no regs to dump, notice it through

		 * first u64 being zero (PERF_SAMPLE_REGS_ABI_NONE).

		/*

		 * If there are no regs to dump, notice it through

		 * first u64 being zero (PERF_SAMPLE_REGS_ABI_NONE).

 If it's vmalloc()d memory, leave phys_addr as 0 */

		/*

		 * Walking the pages tables for user address.

		 * Interrupts are disabled, so it prevents any tear down

		 * of the page tables.

		 * Try IRQ-safe get_user_page_fast_only first.

		 * If failed, leave phys_addr as 0.

/*

 * Return the pagetable size of a given virtual address.

 CONFIG_HAVE_FAST_GUP */

	/*

	 * Software page-table walkers must disable IRQs,

	 * which prevents any tear down of the page tables.

		/*

		 * For kernel threads and the like, use init_mm so that

		 * we can find kernel memory.

 Disallow cross-task user callchains. */

 nr */

 regs dump ABI info */

		/*

		 * Either we need PERF_SAMPLE_STACK_USER bit to be always

		 * processed as the last one or have additional check added

		 * in case new sample type is added, because we could eat

		 * up the rest of the sample size.

		/*

		 * If there is something to dump, add space for the dump

		 * itself and for the field that tells the dynamic size,

		 * which is how many have been actually dumped.

 regs dump ABI info */

 protected by RCU */

	/*

	 * PERF_DATA_PAGE_SIZE requires PERF_SAMPLE_ADDR. If the user doesn't

	 * require PERF_SAMPLE_ADDR, kernel implicitly retrieve the data->addr,

	 * but the value will not dump to the userspace.

 size */

		/*

		 * Given the 16bit nature of header::size, an AUX sample can

		 * easily overflow it, what with all the preceding sample bits.

		 * Make sure this doesn't happen by using up to U16_MAX bytes

		 * per sample in total (rounded down to 8 byte boundary).

	/*

	 * If you're adding more sample types here, you likely need to do

	 * something about the overflowing header::size, like repurpose the

	 * lowest 3 bits of size, which should be always zero at the moment.

	 * This raises a more important question, do we really need 512k sized

	 * samples and why, so good argumentation is in order for whatever you

	 * do here next.

 protect the callchain buffers */

/*

 * read event_id

		/*

		 * Skip events that are not fully formed yet; ensure that

		 * if we observe event->ctx, both event and ctx will be

		 * complete enough. See perf_install_in_context().

/*

 * Iterate all events that need to receive side-band events.

 *

 * For new callers; ensure that account_pmu_sb_event() includes

 * your event, otherwise it might not get delivered.

	/*

	 * If we have task_ctx != NULL we only notify the task context itself.

	 * The task_ctx is set only for EXIT events before releasing task

	 * context.

/*

 * Clear all file-based filters at exec, they'll have to be

 * re-instated when/if these objects are mmapped again.

	/*

	 * In case of inheritance, it will be the parent that links to the

	 * ring-buffer, but it will be the child that's actually using it.

	 *

	 * We are using event::rb to determine if the event should be stopped,

	 * however this may race with ring_buffer_attach() (through set_output),

	 * which will make us skip the event that actually needs to be stopped.

	 * So ring_buffer_attach() has to stop an aux event before re-assigning

	 * its rb pointer.

		/*

		 * For per-CPU events, we need to make sure that neither they

		 * nor their children are running; for cpu==-1 events it's

		 * sufficient to stop the event itself if it's active, since

		 * it can't have children.

/*

 * task tracking -- fork/exit

 *

 * enabled by: attr.comm | attr.mmap | attr.mmap2 | attr.mmap_data | attr.task

 PERF_RECORD_FORK */

 .pid  */

 .ppid */

 .tid  */

 .ptid */

 .time */

/*

 * comm tracking

 .comm      */

 .comm_size */

 .size */

 .pid */

 .tid */

/*

 * namespaces tracking

 .pid */

 .tid */

 .link_info[NR_NAMESPACES] */

/*

 * cgroup tracking

enomem";

 just to be sure to have enough space for alignment */

	/*

	 * Since our buffer works in 8 byte units we need to align our string

	 * size to a multiple of 8. However, we must guarantee the tail end is

	 * zero'd out to avoid leaking random bits to userspace.

/*

 * mmap tracking

enomem";

		/*

		 * d_path() works from the end of the rb backwards, so we

		 * need to add enough zero bytes after the string to handle

		 * the 64bit alignment we do later.

toolong";

anon";

	/*

	 * Since our buffer works in 8 byte units we need to align our string

	 * size to a multiple of 8. However, we must guarantee the tail end is

	 * zero'd out to avoid leaking random bits to userspace.

/*

 * Check whether inode and address range match filter criteria.

 d_inode(NULL) won't be equal to any mapped user-space file */

/*

 * Adjust all task's events' filters to the new vma

	/*

	 * Data tracing isn't supported yet and as such there is no need

	 * to keep track of anything that isn't related to executable code:

 .file_name */

 .file_size */

 .size */

 .pid */

 .tid */

 .maj (attr_mmap2 only) */

 .min (attr_mmap2 only) */

 .ino (attr_mmap2 only) */

 .ino_generation (attr_mmap2 only) */

 .prot (attr_mmap2 only) */

 .flags (attr_mmap2 only) */

/*

 * Lost/dropped samples logging

/*

 * context_switch tracking

 Only CPU-wide events are allowed to see next/prev pid/tid */

 N.B. caller checks nr_switch_events != 0 */

 .type */

 .size */

 .next_prev_pid */

 .next_prev_tid */

/*

 * IRQ throttle logging

/*

 * ksymbol register/unregister tracking

/*

 * bpf program load/unload tracking

/*

 * Generic event overflow handling, sampling.

	/*

	 * Non-sampling counters might still use the PMI to fold short

	 * hardware counters, ignore those.

	/*

	 * XXX event_limit might not quite work as expected on inherited

	 * events

/*

 * Generic software event infrastructure

 Recursion avoidance in each contexts */

/*

 * We directly increment event->count and keep a second value in

 * event->hw.period_left to count intervals. This period event

 * is kept in the range [-sample_period, 0] so that we can use the

 * sign as trigger.

			/*

			 * We inhibit the overflow from happening when

			 * hwc->interrupts == MAX_INTERRUPTS.

 For the read side: events when they trigger */

 For the event head insertion and removal in the hlist */

	/*

	 * Event scheduling is always serialized against hlist allocation

	 * and release. Which makes the protected version suitable here.

	 * The context lock guarantees that.

 Deref the hlist from the update side */

	/*

	 * no branch sampling for software events

 only top level events have filters set */

	/*

	 * If exclude_kernel, only trace user-space tracepoints (uprobes)

	/*

	 * If we got specified a target task, also iterate its context and

	 * deliver this event there too.

	/*

	 * no branch sampling for tracepoint events

/*

 * Flags in config, used by dynamic PMU kprobe and uprobe

 * The flags should match following PMU_FORMAT_ATTR().

 *

 * PERF_PROBE_CONFIG_IS_RETPROBE if set, create kretprobe/uretprobe

 *                               if not set, create kprobe/uprobe

 *

 * The following values specify a reference counter (or semaphore in the

 * terminology of tools like dtrace, systemtap, etc.) Userspace Statically

 * Defined Tracepoints (USDT). Currently, we use 40 bit for the offset.

 *

 * PERF_UPROBE_REF_CTR_OFFSET_BITS	# of bits in config as th offset

 * PERF_UPROBE_REF_CTR_OFFSET_SHIFT	# of bits to shift left

 [k,u]retprobe */

	/*

	 * no branch sampling for probe events

 CONFIG_KPROBE_EVENTS */

	/*

	 * no branch sampling for probe events

 CONFIG_UPROBE_EVENTS */

 hw breakpoint or kernel counter */

		/*

		 * On perf_event with precise_ip, calling bpf_get_stack()

		 * may trigger unwinder warnings and occasional crashes.

		 * bpf_get_[stack|stackid] works around this issue by using

		 * callchain attached to perf_sample_data. If the

		 * perf_event does not full (kernel and user) callchain

		 * attached to perf_sample_data, do not allow attaching BPF

		 * program that calls bpf_get_[stack|stackid].

/*

 * returns true if the event is a tracepoint, or a kprobe/upprobe created

 * with perf_event_open()

 bpf programs can only be attached to u/kprobe or tracepoint */

 Kprobe override only works for kprobes, not uprobes. */

 CONFIG_EVENT_TRACING */

/*

 * Allocate a new address filter

/*

 * Free existing address filters and optionally install new ones

 don't bother with children, they don't have their own filters */

/*

 * Scan through mm's vmas and see if one of them matches the

 * @filter; if so, adjust filter's address range.

 * Called with mm::mmap_lock down for reading.

/*

 * Update event's address range filters based on the

 * task's existing mappings, if any.

	/*

	 * We may observe TASK_TOMBSTONE, which means that the event tear-down

	 * will stop on the parent's child_mutex that our caller is also holding

			/*

			 * Adjust base offset if the filter is associated to a

			 * binary that needs to be mapped:

/*

 * Address range filtering: limiting the data to certain

 * instruction address ranges. Filters are ioctl()ed to us from

 * userspace as ascii strings.

 *

 * Filter string format:

 *

 * ACTION RANGE_SPEC

 * where ACTION is one of the

 *  * "filter": limit the trace to this region

 *  * "start": start tracing from this address

 *  * "stop": stop tracing at this address/region;

 * RANGE_SPEC is

 *  * for kernel addresses: <start address>[/<size>]

 *  * for object files:     <start address>[/<size>]@</path/to/object/file>

 *

 * if <size> is not specified or is zero, the range is treated as a single

 * address; not valid for ACTION=="filter".

/*

 * Address filter string parser

 filter definition begins */

		/*

		 * Filter definition is fully parsed, validate and install it.

		 * Make sure that it doesn't contradict itself or the event's

		 * attribute.

			/*

			 * ACTION "filter" must have a non-zero length region

			 * specified.

				/*

				 * For now, we only support file-based filters

				 * in per-task events; doing so for CPU-wide

				 * events requires additional context switching

				 * trickery, since same object code will be

				 * mapped at different virtual addresses in

				 * different processes.

 look up the path and grab its inode */

 ready to consume more filters */

	/*

	 * Since this is called in perf_ioctl() path, we're already holding

	 * ctx::mutex.

 remove existing filters, if any */

 install new filters */

		/*

		 * Beware, here be dragons!!

		 *

		 * the tracepoint muck will deadlock against ctx->mutex, but

		 * the tracepoint stuff does not actually need it. So

		 * temporarily drop ctx->mutex. As per perf_event_ctx_lock() we

		 * already have a reference on ctx.

		 *

		 * This can result in event getting moved to a different ctx,

		 * but that does not affect the tracepoint state.

/*

 * hrtimer based swevent callback

	/*

	 * Since hrtimers have a fixed rate, we can do a static freq->period

	 * mapping and avoid the whole period adjust feedback stuff.

/*

 * Software event: cpu wall time clock

	/*

	 * no branch sampling for software events

/*

 * Software event: task time clock

	/*

	 * no branch sampling for software events

/*

 * Ensures all contexts with the same task_ctx_nr have the same

 * pmu_cpu_context too.

	/*

	 * Static contexts such as perf_sw_context have a global lifetime

	 * and may be shared between different PMUs. Avoid freeing them

	 * when a single PMU is going away.

/*

 * Let userspace know that this PMU supports address range filtering:

 same value, noting to do */

 update all cpuctx for this PMU */

 For PMUs with address filters, throw in an extra attribute: */

		/*

		 * Other than systems with heterogeneous CPUs, it never makes

		 * sense for two PMUs to share perf_hw_context. PMUs which are

		 * uncore must use perf_invalid_context.

			/*

			 * If we have pmu_enable/pmu_disable calls, install

			 * transaction stubs that use that to try and batch

			 * hardware accesses.

	/*

	 * Ensure the TYPE_SOFTWARE PMUs are at the head of the list,

	 * since these cannot be in the IDR. This way the linear search

	 * is fast, provided a valid software event is provided.

	/*

	 * We dereference the pmu list under both SRCU and regular RCU, so

	 * synchronize against both of those.

	/*

	 * A number of pmu->event_init() methods iterate the sibling_list to,

	 * for example, validate if the group fits on the PMU. Therefore,

	 * if this is a sibling event, acquire the ctx->mutex to protect

	 * the sibling_list.

		/*

		 * This ctx->mutex can nest when we're called through

		 * inheritance. See the perf_event_ctx_lock_nested() comment.

 Try parent's PMU first: */

	/*

	 * PERF_TYPE_HARDWARE and PERF_TYPE_HW_CACHE

	 * are often aliases for PERF_TYPE_RAW.

/*

 * We keep a list of all !task (and therefore per-cpu) events

 * that need to receive side-band records.

 *

 * This avoids having to scan all the various PMU per-cpu contexts

 * looking for them.

 Freq events need the tick to stay alive (see perf_event_task_tick). */

 Lock so we don't race with concurrent unaccount */

		/*

		 * We need the mutex here because static_branch_enable()

		 * must complete *before* the perf_sched_count increment

		 * becomes visible.

			/*

			 * Guarantee that all CPUs observe they key change and

			 * call the perf scheduling hooks before proceeding to

			 * install events that need them.

		/*

		 * Now that we have waited for the sync_sched(), allow further

		 * increments to by-pass the mutex.

/*

 * Allocate and initialize an event structure

 Requires a task: avoid signalling random tasks. */

	/*

	 * Single events are their own group leaders, with an

	 * empty sibling list:

		/*

		 * XXX pmu::event_init needs to know what task to account to

		 * and we cannot use the ctx information because we need the

		 * pmu before we get a ctx.

	/*

	 * We currently do not support PERF_SAMPLE_READ on inherited events.

	 * See perf_output_read().

	/*

	 * Disallow uncore-cgroup events, they don't make sense as the cgroup will

	 * be different on other CPUs in the uncore mask.

		/*

		 * Clone the parent's vma offsets: they are valid until exec()

		 * even if the mm is not shared with the parent.

 force hw sync on the address filters */

 symmetric to unaccount_event() in _free_event() */

 Zero the full structure, so that a short copy will be nice. */

 ABI compatibility quirk: */

 only using defined bits */

 at least one branch bit must be set */

 propagate priv level, when not set for branch */

 exclude_kernel checked on syscall entry */

			/*

			 * adjust user setting (for HW filter setup)

 privileged levels capture (kernel, hv): check permissions */

		/*

		 * We have __u32 type for the size, but so far

		 * we can only use __u16 as maximum due to the

		 * __u16 sample size limit.

 don't allow circular references */

	/*

	 * Don't allow cross-cpu buffers

	/*

	 * If its not a per-cpu rb, it must be the same task.

	/*

	 * Mixing clocks in the same buffer is trouble you don't need.

	/*

	 * Either writing ring buffer from beginning or from end.

	 * Mixing is not allowed.

	/*

	 * If both events generate aux data, they must be on the same PMU

 Can't redirect output if we've got an active mmap() */

 get the rb we want to redirect to */

/*

 * Variation on perf_event_ctx_lock_nested(), except we take two context

 * mutexes.

		/*

		 * perf_event_attr::sigtrap sends signals to the other task.

		 * Require the current task to also have CAP_KILL.

		/*

		 * If the required capabilities aren't available, checks for

		 * ptrace permissions: upgrade to ATTACH, since sending signals

		 * can effectively change the target task.

	/*

	 * Preserve ptrace permission check for backwards compatibility. The

	 * ptrace check also includes checks that the current task and other

	 * task have matching uids, and is therefore not done here explicitly.

/**

 * sys_perf_event_open - open a performance event, associate it to a task/cpu

 *

 * @attr_uptr:	event_id type attributes for monitoring/sampling

 * @pid:		target pid

 * @cpu:		target cpu

 * @group_fd:		group leader event fd

 * @flags:		perf event open flags

 for future expandability... */

 Do we allow access to perf_event_open(2) ? */

 Only privileged users can get physical addresses */

 REGS_INTR can leak data, lockdown must prevent this */

	/*

	 * In cgroup mode, the pid argument is used to pass the fd

	 * opened to the cgroup directory in cgroupfs. The cpu argument

	 * designates the cpu on which to monitor threads from that

	 * cgroup.

	/*

	 * Special case software events and allow them to be part of

	 * any hardware group.

			/*

			 * If the event is a sw event, but the group_leader

			 * is on hw context.

			 *

			 * Allow the addition of software events to hw

			 * groups, this is safe because software events

			 * never fail to schedule.

			/*

			 * In case the group is a pure software group, and we

			 * try to add a hardware event, move the whole group to

			 * the hardware context.

	/*

	 * Get the target context (task or percpu):

	/*

	 * Look up the group leader (we will attach this event to it):

		/*

		 * Do not allow a recursive hierarchy (this new sibling

		 * becoming part of another group-sibling):

 All events in a group should have the same clock */

		/*

		 * Make sure we're both events for the same CPU;

		 * grouping events for different CPUs is broken; since

		 * you can never concurrently schedule them anyhow.

		/*

		 * Make sure we're both on the same task, or both

		 * per-CPU events.

		/*

		 * Do not allow to attach to a group in a different task

		 * or CPU context. If we're moving SW events, we'll fix

		 * this up later, so allow that.

		/*

		 * Only a group leader can be exclusive or pinned

		/*

		 * We must hold exec_update_lock across this and any potential

		 * perf_install_in_context() call for this new event to

		 * serialize against exec() altering our credentials (and the

		 * perf_event_exit_task() that could imply).

		/*

		 * Check if we raced against another sys_perf_event_open() call

		 * moving the software group underneath us.

			/*

			 * If someone moved the group out from under us, check

			 * if this new event wound up on the same ctx, if so

			 * its the regular !move_group case, otherwise fail.

		/*

		 * Failure to create exclusive events returns -EBUSY.

		/*

		 * Check if the @cpu we're creating an event for is online.

		 *

		 * We use the perf_cpu_context::ctx::mutex to serialize against

		 * the hotplug notifiers. See perf_event_{init,exit}_cpu().

	/*

	 * Must be under the same ctx::mutex as perf_install_in_context(),

	 * because we need to serialize with concurrent event creation.

	/*

	 * This is the point on no return; we cannot fail hereafter. This is

	 * where we start modifying current state.

		/*

		 * See perf_event_ctx_lock() for comments on the details

		 * of swizzling perf_event::ctx.

		/*

		 * Wait for everybody to stop referencing the events through

		 * the old lists, before installing it on new lists.

		/*

		 * Install the group siblings before the group leader.

		 *

		 * Because a group leader will try and install the entire group

		 * (through the sibling list, which is still in-tact), we can

		 * end up with siblings installed in the wrong context.

		 *

		 * By installing siblings first we NO-OP because they're not

		 * reachable through the group lists.

		/*

		 * Removing from the context ends up with disabled

		 * event. What we want here is event in the initial

		 * startup state, ready to be add into new context.

	/*

	 * Precalculate sample_data sizes; do while holding ctx::mutex such

	 * that we're serialized against further additions and before

	 * perf_install_in_context() which is the point the event is active and

	 * can use these values.

	/*

	 * Drop the reference on the group_event after placing the

	 * new event on the sibling_list. This ensures destruction

	 * of the group leader will find the pointer to itself in

	 * perf_group_detach().

	/*

	 * If event_file is set, the fput() above will have called ->release()

	 * and that will take care of freeing the event.

/**

 * perf_event_create_kernel_counter

 *

 * @attr: attributes of the counter to create

 * @cpu: cpu in which the counter is bound

 * @task: task to profile (NULL for percpu)

 * @overflow_handler: callback to trigger when we hit the event

 * @context: context data could be used in overflow_handler callback

	/*

	 * Grouping is not supported for kernel events, neither is 'AUX',

	 * make sure the caller's intentions are adjusted.

 Mark owner so we could distinguish it from user events. */

	/*

	 * Get the target context (task or percpu):

		/*

		 * Check if the @cpu we're creating an event for is online.

		 *

		 * We use the perf_cpu_context::ctx::mutex to serialize against

		 * the hotplug notifiers. See perf_event_{init,exit}_cpu().

	/*

	 * See perf_event_ctx_lock() for comments on the details

	 * of swizzling perf_event::ctx.

	/*

	 * Wait for the events to quiesce before re-instating them.

	/*

	 * Re-instate events in 2 passes.

	 *

	 * Skip over group leaders and only install siblings on this first

	 * pass, siblings will not get enabled without a leader, however a

	 * leader will enable its siblings, even if those are still on the old

	 * context.

	/*

	 * Once all the siblings are setup properly, install the group leaders

	 * to make it go.

	/*

	 * Add back the child's count to the parent's count:

		/*

		 * Do not destroy the 'original' grouping; because of the

		 * context switch optimization the original events could've

		 * ended up in a random child task.

		 *

		 * If we were to destroy the original group, all group related

		 * operations would cease to function properly after this

		 * random child dies.

		 *

		 * Do destroy all inherited groups, we don't care about those

		 * and being thorough is better.

	/*

	 * Child events can be freed.

		/*

		 * Kick perf_poll() for is_event_hup();

	/*

	 * Parent events are governed by their filedesc, retain them.

	/*

	 * In order to reduce the amount of tricky in ctx tear-down, we hold

	 * ctx::mutex over the entire thing. This serializes against almost

	 * everything that wants to access the ctx.

	 *

	 * The exception is sys_perf_event_open() /

	 * perf_event_create_kernel_count() which does find_get_context()

	 * without ctx::mutex (it cannot because of the move_group double mutex

	 * lock thing). See the comments in perf_install_in_context().

	/*

	 * In a single ctx::lock section, de-schedule the events and detach the

	 * context from the task such that we cannot ever get it scheduled back

	 * in.

	/*

	 * Now that the context is inactive, destroy the task <-> ctx relation

	 * and mark the context dead.

 cannot be last */

 cannot be last */

	/*

	 * Report the task dead after unscheduling the events so that we

	 * won't get any samples after PERF_RECORD_EXIT. We can however still

	 * get a few PERF_RECORD_READ events.

/*

 * When a child task exits, feed back event values to parent events.

 *

 * Can be called with exec_update_lock held when called from

 * setup_new_exec().

		/*

		 * Ensure the list deletion is visible before we clear

		 * the owner, closes a race against perf_release() where

		 * we need to serialize on the owner->perf_event_mutex.

	/*

	 * The perf_event_exit_task_context calls perf_event_task

	 * with child's task_ctx, which generates EXIT events for

	 * child contexts and sets child->perf_event_ctxp[] to NULL.

	 * At this point we need to send EXIT events to cpu contexts.

/*

 * Free a context as created by inheritance by perf_event_init_task() below,

 * used by fork() in case of fail.

 *

 * Even though the task has never lived, the context and events have been

 * exposed through the child_list, so we must take care tearing it all down.

		/*

		 * Destroy the task <-> ctx relation and mark the context dead.

		 *

		 * This is important because even though the task hasn't been

		 * exposed yet the context has been (through child_list).

 cannot be last */

		/*

		 * perf_event_release_kernel() could've stolen some of our

		 * child events and still have them on its free_list. In that

		 * case we must wait for these events to have been freed (in

		 * particular all their references to this task must've been

		 * dropped).

		 *

		 * Without this copy_process() will unconditionally free this

		 * task (irrespective of its reference count) and

		 * _free_event()'s put_task_struct(event->hw.target) will be a

		 * use-after-free.

		 *

		 * Wait for all events to drop their context reference.

 must be last */

/*

 * Inherit an event from parent task to child task.

 *

 * Returns:

 *  - valid pointer on success

 *  - NULL for orphaned events

 *  - IS_ERR() on error

	/*

	 * Instead of creating recursive hierarchies of events,

	 * we link inherited events back to the original parent,

	 * which has a filp for sure, which we use as the reference

	 * count:

	/*

	 * is_orphaned_event() and list_add_tail(&parent_event->child_list)

	 * must be under the same lock in order to serialize against

	 * perf_event_release_kernel(), such that either we must observe

	 * is_orphaned_event() or they will observe us on the child_list.

 task_ctx_data is freed with child_ctx */

	/*

	 * Make the child state follow the state of the parent event,

	 * not its attr.disabled bit.  We hold the parent's mutex,

	 * so we won't race with perf_event_{en, dis}able_family.

	/*

	 * Precalculate sample_data sizes

	/*

	 * Link it up in the child's context:

	/*

	 * Link this into the parent event's child list

/*

 * Inherits an event group.

 *

 * This will quietly suppress orphaned events; !inherit_event() is not an error.

 * This matches with perf_event_release_kernel() removing all child events.

 *

 * Returns:

 *  - 0 on success

 *  - <0 on error

	/*

	 * @leader can be NULL here because of is_orphaned_event(). In this

	 * case inherit_event() will create individual events, similar to what

	 * perf_group_detach() would do anyway.

/*

 * Creates the child task context and tries to inherit the event-group.

 *

 * Clears @inherited_all on !attr.inherited or error. Note that we'll leave

 * inherited_all set when we 'fail' to inherit an orphaned event; this is

 * consistent with perf_event_release_kernel() removing all child events.

 *

 * Returns:

 *  - 0 on success

 *  - <0 on error

 Do not inherit if sigtrap and signal handlers were cleared. */

		/*

		 * This is executed from the parent task context, so

		 * inherit events that have been marked for cloning.

		 * First allocate and initialize a context for the

		 * child.

/*

 * Initialize the perf_event context in task_struct

	/*

	 * If the parent's context is a clone, pin it so it won't get

	 * swapped under us.

	/*

	 * No need to check if parent_ctx != NULL here; since we saw

	 * it non-NULL earlier, the only reason for it to become NULL

	 * is if we exit, and since we're currently in the middle of

	 * a fork we can't be exiting at the same time.

	/*

	 * Lock the parent list. No need to lock the child - not PID

	 * hashed yet and not running, so nobody can access it.

	/*

	 * We dont have to disable NMIs - we are only looking at

	 * the list, not manipulating it:

	/*

	 * We can't hold ctx->lock when iterating the ->flexible_group list due

	 * to allocations, but we need to prevent rotation because

	 * rotate_ctx() will change the list from interrupt context.

		/*

		 * Mark the child context as a clone of the parent

		 * context, or of whatever the parent is a clone of.

		 *

		 * Note that if the parent is a clone, the holding of

		 * parent_ctx->lock avoids it from being uncloned.

/*

 * Initialize the perf_event context in task_struct

/*

 * Run the perf reboot notifier at the very last possible moment so that

 * the generic watchdog code runs as long as possible.

	/*

	 * Build time assertion that we keep the data_head at the intended

	 * location.  IOW, validation we got the __reserved[] size right.

	/*

	 * Implicitly enable on dfl hierarchy so that perf events can

	 * always be filtered by cgroup2 path as long as perf_event

	 * controller is not mounted on a legacy hierarchy.

 CONFIG_CGROUP_PERF */

 SPDX-License-Identifier: GPL-2.0+

/*

 * User-space Probes (UProbes)

 *

 * Copyright (C) IBM Corporation, 2008-2012

 * Authors:

 *	Srikar Dronamraju

 *	Jim Keniston

 * Copyright (C) 2011-2012 Red Hat, Inc., Peter Zijlstra

 read_mapping_page */

 anon_vma_prepare */

 set_pte_at_notify */

 try_to_free_swap */

 user_enable_single_step */

 notifier mechanism */

 munlock_vma_page */

/*

 * allows us to skip the uprobe_mmap if there are no uprobe events active

 * at this time.  Probably a fine grained per inode count is better?

 serialize rbtree access */

 serialize uprobe->pending_list */

 Have a copy of original instruction */

 node in the rb tree */

 Also hold a ref to inode */

	/*

	 * The generic code assumes that it has two members of unknown type

	 * owned by the arch-specific code:

	 *

	 * 	insn -	copy_insn() saves the original instruction here for

	 *		arch_uprobe_analyze_insn().

	 *

	 *	ixol -	potentially modified instruction to execute out of

	 *		line, copied to xol_area by xol_get_insn_slot().

/*

 * Execute out of line area: anonymous executable mapping installed

 * by the probed task to execute the copy of the original instruction

 * mangled by set_swbp().

 *

 * On a breakpoint hit, thread contests for a slot.  It frees the

 * slot after singlestep. Currently a fixed number of slots are

 * allocated.

 if all slots are busy */

 number of in-use slots */

 0 = free slot */

	/*

	 * We keep the vma's vm_start rather than a pointer to the vma

	 * itself.  The probed process or a naughty kernel module could make

	 * the vma go away, and we must handle that reasonably gracefully.

 Page(s) of instruction slots */

/*

 * valid_vma: Verify if the specified vma is an executable vma

 * Relax restrictions while unregistering: vm_flags might have

 * changed after breakpoint was inserted.

 *	- is_register: indicates if we are in register context.

 *	- Return 1 if the specified virtual address is in an

 *	  executable vma.

/**

 * __replace_page - replace page in vma by new page.

 * based on replace_page in mm/ksm.c

 *

 * @vma:      vma that holds the pte pointing to page

 * @addr:     address the old @page is mapped at

 * @old_page: the page we are replacing by new_page

 * @new_page: the modified page we replace page by

 *

 * If @new_page is NULL, only unmap @old_page.

 *

 * Returns 0 on success, negative error code otherwise.

 For try_to_free_swap() and munlock_vma_page() below */

 no new page, just dec_mm_counter for old_page */

/**

 * is_swbp_insn - check if instruction is breakpoint instruction.

 * @insn: instruction to be checked.

 * Default implementation of is_swbp_insn

 * Returns true if @insn is a breakpoint instruction.

/**

 * is_trap_insn - check if instruction is breakpoint instruction.

 * @insn: instruction to be checked.

 * Default implementation of is_trap_insn

 * Returns true if @insn is a breakpoint instruction.

 *

 * This function is needed for the case where an architecture has multiple

 * trap instructions (like powerpc).

	/*

	 * Note: We only check if the old_opcode is UPROBE_SWBP_INSN here.

	 * We do not check if it is any other 'trap variant' which could

	 * be conditional trap instruction such as the one powerpc supports.

	 *

	 * The logic is that we do not care if the underlying instruction

	 * is a trap variant; uprobes always wins over any other (gdb)

	 * breakpoint.

 register: already installed? */

 unregister: was it changed by us? */

		/*

		 * We are asking for 1 page. If get_user_pages_remote() fails,

		 * it may return 0, in that case we have to return error.

/*

 * NOTE:

 * Expect the breakpoint instruction to be the smallest size instruction for

 * the architecture. If an arch has variable length instruction and the

 * breakpoint instruction is not of the smallest length instruction

 * supported by that architecture then we need to modify is_trap_at_addr and

 * uprobe_write_opcode accordingly. This would never be a problem for archs

 * that have fixed length instructions.

 *

 * uprobe_write_opcode - write the opcode at a given virtual address.

 * @auprobe: arch specific probepoint information.

 * @mm: the probed process address space.

 * @vaddr: the virtual address to store the opcode.

 * @opcode: opcode to be written at @vaddr.

 *

 * Called with mm->mmap_lock held for write.

 * Return 0 (success) or a negative errno.

 Read the page with vaddr into memory */

 We are going to replace instruction, update ref_ctr. */

 let go new_page */

 Revert back reference counter if instruction update failed. */

 try collapse pmd for compound page */

/**

 * set_swbp - store breakpoint at a given address.

 * @auprobe: arch specific probepoint information.

 * @mm: the probed process address space.

 * @vaddr: the virtual address to insert the opcode.

 *

 * For mm @mm, store the breakpoint instruction at @vaddr.

 * Return 0 (success) or a negative errno.

/**

 * set_orig_insn - Restore the original instruction.

 * @mm: the probed process address space.

 * @auprobe: arch specific probepoint information.

 * @vaddr: the virtual address to insert the opcode.

 *

 * For mm @mm, restore the original opcode (opcode) at @vaddr.

 * Return 0 (success) or a negative errno.

		/*

		 * If application munmap(exec_vma) before uprobe_unregister()

		 * gets called, we don't get a chance to remove uprobe from

		 * delayed_uprobe_list from remove_breakpoint(). Do it here.

/*

 * Find a uprobe corresponding to a given inode:offset

 * Acquires uprobes_treelock

 get access + creation ref */

/*

 * Acquire uprobes_treelock.

 * Matching uprobe already exists in rbtree;

 *	increment (access refcount) and return the matching uprobe.

 *

 * No matching uprobe; insert the uprobe in rb_tree;

 *	get a double refcount (access + creation) and return NULL.

 add to uprobes_tree, sorted on inode:offset */

 a uprobe exists for this inode:offset combination */

/*

 * For uprobe @uprobe, delete the consumer @uc.

 * Return true if the @uc is deleted successfully

 * or return false.

	/*

	 * Ensure that the page that has the original instruction is populated

	 * and in page-cache. If ->readpage == NULL it must be shmem_mapping(),

	 * see uprobe_register().

 Copy only available bytes, -EIO if nothing was read */

 TODO: move this into _register, until then we abuse this sem. */

 pairs with the smp_rmb() in handle_swbp() */

	/*

	 * set MMF_HAS_UPROBES in advance for uprobe_pre_sstep_notifier(),

	 * the task can hit this breakpoint right after __replace_page().

/*

 * There could be threads that have already hit the breakpoint. They

 * will recheck the current insn and restart if find_uprobe() fails.

 * See find_active_uprobe().

 for uprobe_is_active() */

			/*

			 * Needs GFP_NOWAIT to avoid i_mmap_rwsem recursion through

			 * reclaim. This is optimistic, no harm done if it fails.

 consult only the "caller", new consumer. */

 TODO : cant unregister? schedule a worker thread */

/*

 * uprobe_unregister - unregister an already registered probe.

 * @inode: the file in which the probe has to be removed.

 * @offset: offset from the start of the file.

 * @uc: identify which probe if multiple probes are colocated.

/*

 * __uprobe_register - register a probe

 * @inode: the file in which the probe has to be placed.

 * @offset: offset from the start of the file.

 * @uc: information on howto handle the probe..

 *

 * Apart from the access refcount, __uprobe_register() takes a creation

 * refcount (thro alloc_uprobe) if and only if this @uprobe is getting

 * inserted into the rbtree (i.e first consumer for a @inode:@offset

 * tuple).  Creation refcount stops uprobe_unregister from freeing the

 * @uprobe even before the register operation is complete. Creation

 * refcount is released when the last @uc for the @uprobe

 * unregisters. Caller of __uprobe_register() is required to keep @inode

 * (and the containing mount) referenced.

 *

 * Return errno if it cannot successully install probes

 * else return 0 (success)

 Uprobe must have at least one set consumer */

 copy_insn() uses read_mapping_page() or shmem_read_mapping_page() */

 Racy, just to catch the obvious mistakes */

	/*

	 * This ensures that copy_from_page(), copy_to_page() and

	 * __update_ref_ctr() can't cross page boundary.

	/*

	 * We can race with uprobe_unregister()->delete_uprobe().

	 * Check uprobe_is_active() and retry if it is false.

/*

 * uprobe_apply - unregister an already registered probe.

 * @inode: the file in which the probe has to be removed.

 * @offset: offset from the start of the file.

 * @uc: consumer which wants to add more or remove some breakpoints

 * @add: add or remove the breakpoints

/*

 * For a given range in vma, build a list of probes that need to be inserted.

 @vma contains reference counter, not the probed instruction. */

/*

 * Called from mmap_region/vma_adjust with mm->mmap_lock acquired.

 *

 * Currently we ignore all errors and always return 0, the callers

 * can't handle the failure anyway.

	/*

	 * We can race with uprobe_unregister(), this uprobe can be already

	 * removed. But in this case filter_chain() must return false, all

	 * consumers have gone away.

/*

 * Called in context of a munmap of a vma.

 called by mmput() ? */

 Slot allocation for XOL */

 Try to map as high as possible, this is only a hint. */

 pairs with get_xol_area() */

 ^^^ */

 Reserve the 1st slot for get_trampoline_vaddr() */

/*

 * get_xol_area - Allocate process's xol_area if necessary.

 * This area will be used for storing instructions for execution out of line.

 *

 * Returns the allocated area or NULL.

 Pairs with xol_add_vma() smp_store_release() */

 ^^^ */

/*

 * uprobe_clear_state - Free the area allocated for slots.

 unconditionally, dup_mmap() skips VM_DONTCOPY vmas */

/*

 *  - search for a free slot.

/*

 * xol_get_insn_slot - allocate a slot for xol.

 * Returns the allocated slot address or 0.

/*

 * xol_free_insn_slot - If slot was earlier allocated by

 * @xol_get_insn_slot(), make the slot available for

 * subsequent requests.

 pairs with prepare_to_wait() */

 Initialize the slot */

	/*

	 * We probably need flush_icache_user_page() but it needs vma.

	 * This should work on most of architectures by default. If

	 * architecture needs to do something different it can define

	 * its own version of the function.

/**

 * uprobe_get_swbp_addr - compute address of swbp given post-swbp regs

 * @regs: Reflects the saved state of the task after it has hit a breakpoint

 * instruction.

 * Return the address of the breakpoint instruction.

/*

 * Called with no locks held.

 * Called in context of an exiting or an exec-ing thread.

/*

 * Allocate a uprobe_task object for the task if necessary.

 * Called when the thread hits a breakpoint.

 *

 * Returns:

 * - pointer to new uprobe_task on success

 * - NULL otherwise

/*

 * Called in context of a new clone/fork from copy_process.

 The task can fork() after dup_xol_work() fails */

/*

 * Current area->vaddr notion assume the trampoline address is always

 * equal area->vaddr.

 *

 * Returns -1 in case the xol_area is not allocated.

 Pairs with xol_add_vma() smp_store_release() */

 ^^^ */

 drop the entries invalidated by longjmp() */

	/*

	 * We don't want to keep trampoline address in stack, rather keep the

	 * original return address of first caller thru all the consequent

	 * instances. This also makes breakpoint unwrapping easier.

			/*

			 * This situation is not possible. Likely we have an

			 * attack from user-space.

 Prepare to single-step probed instruction out of line. */

/*

 * If we are singlestepping, then ensure this thread is not connected to

 * non-fatal signals until completion of singlestep.  When xol insn itself

 * triggers the signal,  restart the original insn even if the task is

 * already SIGKILL'ed (since coredump should report the correct ip).  This

 * is even more important if the task has a handler for SIGSEGV/etc, The

 * _same_ instruction should be repeated again after return from the signal

 * handler, and SSTEP can never finish in this case.

		/*

		 * This is not strictly accurate, we can race with

		 * uprobe_unregister() and see the already removed

		 * uprobe if delete_uprobe() was not yet called.

		 * Or this uprobe can be filtered out.

	/*

	 * The NULL 'tsk' here ensures that any faults that occur here

	 * will not be accounted to the task.  'mm' *is* current->mm,

	 * but we treat this as a 'remote' access since it is

	 * essentially a kernel access to the memory.

 This needs to return true for any variant of the trap insn */

 prepare return uprobe, when needed */

 put bp at return */

 can't be NULL if chained */

		/*

		 * We should throw out the frames invalidated by longjmp().

		 * If this chain is valid, then the next one should be alive

		 * or NULL; the latter case means that nobody but ri->func

		 * could hit this trampoline on return. TODO: sigaltstack().

/*

 * Run handler and ask thread to singlestep.

 * Ensure all non-fatal signals cannot interrupt thread while it singlesteps.

 No matching uprobe; signal SIGTRAP. */

			/*

			 * Either we raced with uprobe_unregister() or we can't

			 * access this memory. The latter is only possible if

			 * another thread plays with our ->mm. In both cases

			 * we can simply restart. If this vma was unmapped we

			 * can pretend this insn was not executed yet and get

			 * the (correct) SIGSEGV after restart.

 change it in advance for ->handler() and restart */

	/*

	 * TODO: move copy_insn/etc into _register and remove this hack.

	 * After we hit the bp, _unregister + _register can install the

	 * new and not-yet-analyzed uprobe at the same address, restart.

	/*

	 * Pairs with the smp_wmb() in prepare_uprobe().

	 *

	 * Guarantees that if we see the UPROBE_COPY_INSN bit set, then

	 * we must also see the stores to &uprobe->arch performed by the

	 * prepare_uprobe() call.

 Tracing handlers use ->utask to communicate with fetch methods */

 arch_uprobe_skip_sstep() succeeded, or restart if can't singlestep */

/*

 * Perform required fix-ups and disable singlestep.

 * Allow pending signals to take effect.

 see uprobe_deny_signal() */

/*

 * On breakpoint hit, breakpoint notifier sets the TIF_UPROBE flag and

 * allows the thread to return from interrupt. After that handle_swbp()

 * sets utask->active_uprobe.

 *

 * On singlestep exception, singlestep notifier sets the TIF_UPROBE flag

 * and allows the thread to return from interrupt.

 *

 * While returning to userspace, thread notices the TIF_UPROBE flag and calls

 * uprobe_notify_resume().

/*

 * uprobe_pre_sstep_notifier gets called from interrupt context as part of

 * notifier mechanism. Set TIF_UPROBE flag and indicate breakpoint hit.

/*

 * uprobe_post_sstep_notifier gets called in interrupt context as part of notifier

 * mechanism. Set TIF_UPROBE flag and indicate completion of singlestep.

 task is currently not uprobed */

 notified after kprobes, kgdb */

 SPDX-License-Identifier: GPL-2.0

/*

 * Performance events callchain code, extracted from core.c:

 *

 *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>

 *  Copyright (C) 2008-2011 Red Hat, Inc., Ingo Molnar

 *  Copyright (C) 2008-2011 Red Hat, Inc., Peter Zijlstra

 *  Copyright  ©  2009 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

	/*

	 * We can't use the percpu allocation API for data that can be

	 * accessed from NMI. Use a temporary manual per cpu allocation

	 * until that gets sorted out.

	/*

	 * If requesting per event more than the global cap,

	 * return a different error to help userspace figure

	 * this out.

	 *

	 * And also do it here so that we have &callchain_mutex held.

/*

 * Used for sysctl_perf_event_max_stack and

 * sysctl_perf_event_max_contexts_per_stack.

 SPDX-License-Identifier: GPL-2.0

/*

 * __gcov_init is called by gcc-generated constructor code for each object

 * file compiled with -fprofile-arcs.

		/*

		 * Printing gcc's version magic may prove useful for debugging

		 * incompatibility reports.

	/*

	 * Add new profiling data structure to list and inform event

	 * listener.

/*

 * These functions may be referenced by gcc-generated profiling code but serve

 * no function for kernel profiling.

 Unused. */

 Unused. */

 Unused. */

 Unused. */

 Unused. */

 Unused. */

 Unused. */

 Unused. */

 SPDX-License-Identifier: GPL-2.0

/*

 *  This code provides functions to handle gcc's profiling data format

 *  introduced with gcc 4.7.

 *

 *  This file is based heavily on gcc_3_4.c file.

 *

 *  For a better understanding, refer to gcc source:

 *  gcc/gcov-io.h

 *  libgcc/libgcov.c

 *

 *  Uses gcc-internal data definitions.

/**

 * struct gcov_ctr_info - information about counters for a single function

 * @num: number of counter values for this type

 * @values: array of counter values for this type

 *

 * This data is generated by gcc during compilation and doesn't change

 * at run-time with the exception of the values array.

/**

 * struct gcov_fn_info - profiling meta data per function

 * @key: comdat key

 * @ident: unique ident of function

 * @lineno_checksum: function lineo_checksum

 * @cfg_checksum: function cfg checksum

 * @ctrs: instrumented counters

 *

 * This data is generated by gcc during compilation and doesn't change

 * at run-time.

 *

 * Information about a single function.  This uses the trailing array

 * idiom. The number of counters is determined from the merge pointer

 * array in gcov_info.  The key is used to detect which of a set of

 * comdat functions was selected -- it points to the gcov_info object

 * of the object file containing the selected comdat function.

/**

 * struct gcov_info - profiling data per object file

 * @version: gcov version magic indicating the gcc version used for compilation

 * @next: list head for a singly-linked list

 * @stamp: uniquifying time stamp

 * @filename: name of the associated gcov data file

 * @merge: merge functions (null for unused counter type)

 * @n_functions: number of instrumented functions

 * @functions: pointer to pointers to function information

 *

 * This data is generated by gcc during compilation and doesn't change

 * at run-time with the exception of the next pointer.

/**

 * gcov_info_filename - return info filename

 * @info: profiling data set

/**

 * gcov_info_version - return info version

 * @info: profiling data set

/**

 * gcov_info_next - return next profiling data set

 * @info: profiling data set

 *

 * Returns next gcov_info following @info or first gcov_info in the chain if

 * @info is %NULL.

/**

 * gcov_info_link - link/add profiling data set to the list

 * @info: profiling data set

/**

 * gcov_info_unlink - unlink/remove profiling data set from the list

 * @prev: previous profiling data set

 * @info: profiling data set

/**

 * gcov_info_within_module - check if a profiling data set belongs to a module

 * @info: profiling data set

 * @mod: module

 *

 * Returns true if profiling data belongs module, false otherwise.

 Symbolic links to be created for each profiling data file. */

 Link to .gcno file in $(objtree). */

/*

 * Determine whether a counter is active. Doesn't change at run-time.

 Determine number of active counters. Based on gcc magic. */

/**

 * gcov_info_reset - reset profiling data to zero

 * @info: profiling data set

/**

 * gcov_info_is_compatible - check if profiling data can be added

 * @info1: first profiling data set

 * @info2: second profiling data set

 *

 * Returns non-zero if profiling data can be added, zero otherwise.

/**

 * gcov_info_add - add up profiling data

 * @dst: profiling data set to which data is added

 * @src: profiling data set which is added

 *

 * Adds profiling counts of @src to @dst.

/**

 * gcov_info_dup - duplicate profiling data set

 * @info: profiling data set to duplicate

 *

 * Return newly allocated duplicate on success, %NULL on error.

 dst counter info */

 src counter info */

 function info idx */

 counter type idx */

 function info size */

 counter values size */

/**

 * gcov_info_free - release memory for profiling data set duplicate

 * @info: profiling data set duplicate to free

/**

 * convert_to_gcda - convert profiling data set to gcda file format

 * @buffer: the buffer to store file data or %NULL if no data should be stored

 * @info: profiling data set to be converted

 *

 * Returns the number of bytes that were/would have been stored into the buffer.

 File header. */

 Function record. */

 Counter record. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 Google, Inc.

 * modified from kernel/gcov/gcc_4_7.c

 *

 * This software is licensed under the terms of the GNU General Public

 * License version 2, as published by the Free Software Foundation, and

 * may be copied, distributed, and modified under those terms.

 *

 * This program is distributed in the hope that it will be useful,

 * but WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

 *

 *

 * LLVM uses profiling data that's deliberately similar to GCC, but has a

 * very different way of exporting that data.  LLVM calls llvm_gcov_init() once

 * per module, and provides a couple of callbacks that we can use to ask for

 * more data.

 *

 * We care about the "writeout" callback, which in turn calls back into

 * compiler-rt/this module to dump all the gathered coverage data to disk:

 *

 *    llvm_gcda_start_file()

 *      llvm_gcda_emit_function()

 *      llvm_gcda_emit_arcs()

 *      llvm_gcda_emit_function()

 *      llvm_gcda_emit_arcs()

 *      [... repeats for each function ...]

 *    llvm_gcda_summary_info()

 *    llvm_gcda_end_file()

 *

 * This design is much more stateless and unstructured than gcc's, and is

 * intended to run at process exit.  This forces us to keep some local state

 * about which module we're dealing with at the moment.  On the other hand, it

 * also means we don't depend as much on how LLVM represents profiling data

 * internally.

 *

 * See LLVM's lib/Transforms/Instrumentation/GCOVProfiling.cpp for more

 * details on how this works, particularly GCOVProfiler::emitProfileArcs(),

 * GCOVProfiler::insertCounterWriteout(), and

 * GCOVProfiler::insertFlush().

/**

 * gcov_info_filename - return info filename

 * @info: profiling data set

/**

 * gcov_info_version - return info version

 * @info: profiling data set

/**

 * gcov_info_next - return next profiling data set

 * @info: profiling data set

 *

 * Returns next gcov_info following @info or first gcov_info in the chain if

 * @info is %NULL.

/**

 * gcov_info_link - link/add profiling data set to the list

 * @info: profiling data set

/**

 * gcov_info_unlink - unlink/remove profiling data set from the list

 * @prev: previous profiling data set

 * @info: profiling data set

 Generic code unlinks while iterating. */

/**

 * gcov_info_within_module - check if a profiling data set belongs to a module

 * @info: profiling data set

 * @mod: module

 *

 * Returns true if profiling data belongs module, false otherwise.

 Symbolic links to be created for each profiling data file. */

 Link to .gcno file in $(objtree). */

/**

 * gcov_info_reset - reset profiling data to zero

 * @info: profiling data set

/**

 * gcov_info_is_compatible - check if profiling data can be added

 * @info1: first profiling data set

 * @info2: second profiling data set

 *

 * Returns non-zero if profiling data can be added, zero otherwise.

/**

 * gcov_info_add - add up profiling data

 * @dest: profiling data set to which data is added

 * @source: profiling data set which is added

 *

 * Adds profiling counts of @source to @dest.

 counter values size */

/**

 * gcov_info_dup - duplicate profiling data set

 * @info: profiling data set to duplicate

 *

 * Return newly allocated duplicate on success, %NULL on error.

/**

 * gcov_info_free - release memory for profiling data set duplicate

 * @info: profiling data set duplicate to free

/**

 * convert_to_gcda - convert profiling data set to gcda file format

 * @buffer: the buffer to store file data or %NULL if no data should be stored

 * @info: profiling data set to be converted

 *

 * Returns the number of bytes that were/would have been stored into the buffer.

 File header. */

 SPDX-License-Identifier: GPL-2.0

/*

 *  This code maintains a list of active profiling data structures.

 *

 *    Copyright IBM Corp. 2009

 *    Author(s): Peter Oberparleiter <oberpar@linux.vnet.ibm.com>

 *

 *    Uses gcc-internal data definitions.

 *    Based on the gcov-kernel patch by:

 *		 Hubertus Franke <frankeh@us.ibm.com>

 *		 Nigel Hinds <nhinds@us.ibm.com>

 *		 Rajan Ravindran <rajancr@us.ibm.com>

 *		 Peter Oberparleiter <oberpar@linux.vnet.ibm.com>

 *		 Paul Larson

/**

 * gcov_enable_events - enable event reporting through gcov_event()

 *

 * Turn on reporting of profiling data load/unload-events through the

 * gcov_event() callback. Also replay all previous events once. This function

 * is needed because some events are potentially generated too early for the

 * callback implementation to handle them initially.

 Perform event callback for previously registered entries. */

/**

 * store_gcov_u32 - store 32 bit number in gcov format to buffer

 * @buffer: target buffer or NULL

 * @off: offset into the buffer

 * @v: value to be stored

 *

 * Number format defined by gcc: numbers are recorded in the 32 bit

 * unsigned binary form of the endianness of the machine generating the

 * file. Returns the number of bytes stored. If @buffer is %NULL, doesn't

 * store anything.

/**

 * store_gcov_u64 - store 64 bit number in gcov format to buffer

 * @buffer: target buffer or NULL

 * @off: offset into the buffer

 * @v: value to be stored

 *

 * Number format defined by gcc: numbers are recorded in the 32 bit

 * unsigned binary form of the endianness of the machine generating the

 * file. 64 bit numbers are stored as two 32 bit numbers, the low part

 * first. Returns the number of bytes stored. If @buffer is %NULL, doesn't store

 * anything.

 Update list and generate events when modules are unloaded. */

 Remove entries located in module from linked list. */

 CONFIG_MODULES */

 SPDX-License-Identifier: GPL-2.0

/*

 *  This code exports profiling data as debugfs files to userspace.

 *

 *    Copyright IBM Corp. 2009

 *    Author(s): Peter Oberparleiter <oberpar@linux.vnet.ibm.com>

 *

 *    Uses gcc-internal data definitions.

 *    Based on the gcov-kernel patch by:

 *		 Hubertus Franke <frankeh@us.ibm.com>

 *		 Nigel Hinds <nhinds@us.ibm.com>

 *		 Rajan Ravindran <rajancr@us.ibm.com>

 *		 Peter Oberparleiter <oberpar@linux.vnet.ibm.com>

 *		 Paul Larson

 *		 Yi CDL Yang

/**

 * struct gcov_node - represents a debugfs entry

 * @list: list head for child node list

 * @children: child nodes

 * @all: list head for list of all nodes

 * @parent: parent node

 * @loaded_info: array of pointers to profiling data sets for loaded object

 *   files.

 * @num_loaded: number of profiling data sets for loaded object files.

 * @unloaded_info: accumulated copy of profiling data sets for unloaded

 *   object files. Used only when gcov_persist=1.

 * @dentry: main debugfs entry, either a directory or data file

 * @links: associated symbolic links

 * @name: data file basename

 *

 * struct gcov_node represents an entity within the gcov/ subdirectory

 * of debugfs. There are directory and data file nodes. The latter represent

 * the actual synthesized data file plus any associated symbolic links which

 * are needed by the gcov tool to work correctly.

 If non-zero, keep copies of profiling data for unloaded modules. */

/**

 * struct gcov_iterator - specifies current file position in logical records

 * @info: associated profiling data

 * @buffer: buffer containing file data

 * @size: size of buffer

 * @pos: current position in file

/**

 * gcov_iter_new - allocate and initialize profiling data iterator

 * @info: profiling data set to be iterated

 *

 * Return file iterator on success, %NULL otherwise.

 Dry-run to get the actual buffer size. */

/**

 * gcov_iter_free - free iterator data

 * @iter: file iterator

/**

 * gcov_iter_get_info - return profiling data set for given file iterator

 * @iter: file iterator

/**

 * gcov_iter_start - reset file iterator to starting position

 * @iter: file iterator

/**

 * gcov_iter_next - advance file iterator to next logical record

 * @iter: file iterator

 *

 * Return zero if new position is valid, non-zero if iterator has reached end.

/**

 * gcov_iter_write - write data for current pos to seq_file

 * @iter: file iterator

 * @seq: seq_file handle

 *

 * Return zero on success, non-zero otherwise.

/*

 * seq_file.start() implementation for gcov data files. Note that the

 * gcov_iterator interface is designed to be more restrictive than seq_file

 * (no start from arbitrary position, etc.), to simplify the iterator

 * implementation.

 seq_file.next() implementation for gcov data files. */

 seq_file.show() implementation for gcov data files. */

 Unused. */

/*

 * Return a profiling data set associated with the given node. This is

 * either a data set for a loaded object file or a data set copy in case

 * all associated object files have been unloaded.

/*

 * Return a newly allocated profiling data set which contains the sum of

 * all profiling data associated with the given node.

/*

 * open() implementation for gcov data files. Create a copy of the profiling

 * data set and initialize the iterator and seq_file interface.

	/*

	 * Read from a profiling data copy to minimize reference tracking

	 * complexity and concurrent access and to keep accumulating multiple

	 * profiling data sets associated with one node simple.

/*

 * release() implementation for gcov data files. Release resources allocated

 * by open().

/*

 * Find a node by the associated data file name. Needs to be called with

 * node_lock held.

/*

 * Reset all profiling data associated with the specified node.

/*

 * write() implementation for gcov data files. Reset profiling data for the

 * corresponding file. If all associated object files have been unloaded,

 * remove the debug fs node as well.

 Reset counts or remove node for unloaded modules. */

 Reset counts for open file. */

/*

 * Given a string <path> representing a file path of format:

 *   path/to/file.gcda

 * construct and return a new string:

 *   <dir/>path/to/file.<ext>

/*

 * Construct a string representing the symbolic link target for the given

 * gcov data file name and link type. Depending on the link type and the

 * location of the data file, the link target can either point to a

 * subdirectory of srctree, objtree or in an external location.

 External compilation. */

/*

 * For a filename .tmp_filename.ext return filename.ext. Needed to compensate

 * for filename skewing caused by the mod-versioning mechanism.

/*

 * Create links to additional files (usually .c and .gcno files) which the

 * gcov tool expects to find in the same directory as the gcov data file.

 Nothing. */;

 Basic initialization of a new node. */

/*

 * Create a new node and associated debugfs entry. Needs to be called with

 * node_lock held.

 Differentiate between gcov data file nodes and directory nodes. */

 Remove symbolic links associated with node. */

/*

 * Remove node from all lists and debugfs and release associated resources.

 * Needs to be called with node_lock held.

 Release node and empty parents. Needs to be called with node_lock held. */

/*

 * Find child node with given basename. Needs to be called with node_lock

 * held.

/*

 * write() implementation for reset file. Reset all profiling data to zero

 * and remove nodes for which all associated object files are unloaded.

 Several nodes may have gone - restart loop. */

 read() implementation for reset file. Unused. */

 Allow read operation so that a recursive copy won't fail. */

/*

 * Create a node for a given profiling data set and add it to all lists and

 * debugfs. Needs to be called with node_lock held.

 Create directory nodes along the path. */

 Create file node. */

/*

 * Associate a profiling data set with an existing node. Needs to be called

 * with node_lock held.

	/*

	 * Prepare new array. This is done first to simplify cleanup in

	 * case the new data set is incompatible, the node only contains

	 * unloaded data sets and there's not enough memory for the array.

 Check if the new data set is compatible. */

		/*

		 * A module was unloaded, modified and reloaded. The new

		 * data set replaces the copy of the last one.

		/*

		 * Two different versions of the same object file are loaded.

		 * The initial one takes precedence.

 Overwrite previous array. */

/*

 * Return the index of a profiling data set associated with a node.

/*

 * Save the data of a profiling data set which is being unloaded.

/*

 * Disassociate a profiling data set from a node. Needs to be called with

 * node_lock held.

 Shrink array. */

 Last loaded data set was removed. */

/*

 * Callback to create/remove profiling files when code compiled with

 * -fprofile-arcs is loaded/unloaded.

 Create debugfs entries. */

	/*

	 * /sys/kernel/debug/gcov will be parent for the reset control file

	 * and all profiling files.

	/*

	 * Create reset file which resets all profiling counts when written

	 * to.

 Replay previous events to get our fs hierarchy up-to-date. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2009 Rafael J. Wysocki <rjw@sisk.pl>, Novell Inc.

 *

 * This file contains power management functions related to interrupts.

/*

 * Called from __setup_irq() with desc->lock held after @action has

 * been installed in the action chain.

/*

 * Called from __free_irq() with desc->lock held after @action has

 * been removed from the action chain.

			/*

			 * Interrupt marked for wakeup is in disabled state.

			 * Enable interrupt here to unmask/enable in irqchip

			 * to be able to resume with such interrupts.

		/*

		 * We return true here to force the caller to issue

		 * synchronize_irq(). We need to make sure that the

		 * IRQD_WAKEUP_ARMED is visible before we return from

		 * suspend_device_irqs().

	/*

	 * Hardware which has no wakeup source configuration facility

	 * requires that the non wakeup interrupts are masked at the

	 * chip level. The chip implementation indicates that with

	 * IRQCHIP_MASK_ON_SUSPEND.

/**

 * suspend_device_irqs - disable all currently enabled interrupt lines

 *

 * During system-wide suspend or hibernation device drivers need to be

 * prevented from receiving interrupts and this function is provided

 * for this purpose.

 *

 * So we disable all interrupts and mark them IRQS_SUSPENDED except

 * for those which are unused, those which are marked as not

 * suspendable via an interrupt request with the flag IRQF_NO_SUSPEND

 * set and those which are marked as active wakeup sources.

 *

 * The active wakeup sources are handled by the flow handler entry

 * code which checks for the IRQD_WAKEUP_ARMED flag, suspends the

 * interrupt and notifies the pm core about the wakeup.

		/*

		 * Interrupt marked for wakeup was enabled during suspend

		 * entry. Disable such interrupts to restore them back to

		 * original state.

 Force resume the interrupt? */

 Pretend that it got disabled ! */

/**

 * rearm_wake_irq - rearm a wakeup interrupt line after signaling wakeup

 * @irq: Interrupt to rearm

/**

 * irq_pm_syscore_resume - enable interrupt lines early

 *

 * Enable all interrupt lines with %IRQF_EARLY_RESUME set.

/**

 * resume_device_irqs - enable interrupt lines disabled by suspend_device_irqs()

 *

 * Enable all non-%IRQF_EARLY_RESUME interrupt lines previously

 * disabled by suspend_device_irqs() that have the IRQS_SUSPENDED flag

 * set as well as those with %IRQF_FORCE_RESUME.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1992, 1998-2004 Linus Torvalds, Ingo Molnar

 *

 * This file contains spurious interrupt handling.

/*

 * We wait here for a poller to finish.

 *

 * If the poll runs on this CPU, then we yell loudly and return

 * false. That will leave the interrupt line disabled in the worst

 * case, but it should never happen.

 *

 * We wait until the poller is done and then recheck disabled and

 * action (about to be disabled). Only if it's still active, we return

 * true and let the handler run.

 Might have been disabled in meantime */

/*

 * Recovery handler for misrouted interrupts.

	/*

	 * PER_CPU, nested thread interrupts and interrupts explicitly

	 * marked polled are excluded from polling.

	/*

	 * Do not poll disabled interrupts unless the spurious

	 * disabled poller asks explicitly.

	/*

	 * All handlers must agree on IRQF_SHARED, so we test just the

	 * first.

 Already running on another processor */

		/*

		 * Already running: If it is shared get the other

		 * CPU to go looking for our mystery interrupt too

 Mark it poll in progress */

 Make sure that there is still a valid action */

 Already tried */

 So the caller can adjust the irq error counts */

 Racy but it doesn't matter */

/*

 * If 99,900 of the previous 100,000 interrupts have not been handled

 * then assume that the IRQ is stuck in some manner. Drop a diagnostic

 * and try to turn the IRQ off.

 *

 * (The other 100-of-100,000 interrupts may have been a correctly

 *  functioning device sharing an IRQ with the failing one)

	/*

	 * We need to take desc->lock here. note_interrupt() is called

	 * w/o desc->lock held, but IRQ_PROGRESS set. We might race

	 * with something else removing an action. It's ok to take

	 * desc->lock here. See synchronize_irq().

 We didn't actually handle the IRQ - see if it was misrouted? */

	/*

	 * But for 'irqfixup == 2' we also do it for handled interrupts if

	 * they are marked as IRQF_IRQPOLL (or for irq zero, which is the

	 * traditional PC timer interrupt.. Legacy)

	/*

	 * Since we don't get the descriptor lock, "action" can

	 * change under us.  We don't really care, but we don't

	 * want to follow a NULL pointer. So tell the compiler to

	 * just load it once by using a barrier.

	/*

	 * We cannot call note_interrupt from the threaded handler

	 * because we need to look at the compound of all handlers

	 * (primary and threaded). Aside of that in the threaded

	 * shared case we have no serialization against an incoming

	 * hardware interrupt while we are dealing with a threaded

	 * result.

	 *

	 * So in case a thread is woken, we just note the fact and

	 * defer the analysis to the next hardware interrupt.

	 *

	 * The threaded handlers store whether they successfully

	 * handled an interrupt and we check whether that number

	 * changed versus the last invocation.

	 *

	 * We could handle all interrupts with the delayed by one

	 * mechanism, but for the non forced threaded case we'd just

	 * add pointless overhead to the straight hardirq interrupts

	 * for the sake of a few lines less code.

		/*

		 * There is a thread woken. Check whether one of the

		 * shared primary handlers returned IRQ_HANDLED. If

		 * not we defer the spurious detection to the next

		 * interrupt.

			/*

			 * We use bit 31 of thread_handled_last to

			 * denote the deferred spurious detection

			 * active. No locking necessary as

			 * thread_handled_last is only accessed here

			 * and we have the guarantee that hard

			 * interrupts are not reentrant.

			/*

			 * Check whether one of the threaded handlers

			 * returned IRQ_HANDLED since the last

			 * interrupt happened.

			 *

			 * For simplicity we just set bit 31, as it is

			 * set in threads_handled_last as well. So we

			 * avoid extra masking. And we really do not

			 * care about the high bits of the handled

			 * count. We just care about the count being

			 * different than the one we saw before.

				/*

				 * Note: We keep the SPURIOUS_DEFERRED

				 * bit set. We are handling the

				 * previous invocation right now.

				 * Keep it for the current one, so the

				 * next hardware interrupt will

				 * account for it.

				/*

				 * None of the threaded handlers felt

				 * responsible for the last interrupt

				 *

				 * We keep the SPURIOUS_DEFERRED bit

				 * set in threads_handled_last as we

				 * need to account for the current

				 * interrupt as well.

			/*

			 * One of the primary handlers returned

			 * IRQ_HANDLED. So we don't care about the

			 * threaded handlers on the same line. Clear

			 * the deferred detection bit.

			 *

			 * In theory we could/should check whether the

			 * deferred bit is set and take the result of

			 * the previous run into account here as

			 * well. But it's really not worth the

			 * trouble. If every other interrupt is

			 * handled we never trigger the spurious

			 * detector. And if this is just the one out

			 * of 100k unhandled ones which is handled

			 * then we merily delay the spurious detection

			 * by one hard interrupt. Not a real problem.

		/*

		 * If we are seeing only the odd spurious IRQ caused by

		 * bus asynchronicity then don't eventually trigger an error,

		 * otherwise the counter becomes a doomsday timer for otherwise

		 * working systems

 Now getting into unhandled irq detection */

		/*

		 * The interrupt is stuck

		/*

		 * Now kill the IRQ

 SPDX-License-Identifier: GPL-2.0

/**

 * __irq_domain_alloc_fwnode - Allocate a fwnode_handle suitable for

 *                           identifying an irq domain

 * @type:	Type of irqchip_fwnode. See linux/irqdomain.h

 * @id:		Optional user provided id if name != NULL

 * @name:	Optional user provided domain name

 * @pa:		Optional user-provided physical address

 *

 * Allocate a struct irqchip_fwid, and return a pointer to the embedded

 * fwnode_handle (or NULL on failure).

 *

 * Note: The types IRQCHIP_FWNODE_NAMED and IRQCHIP_FWNODE_NAMED_ID are

 * solely to transport name information to irqdomain creation code. The

 * node is not stored. For other types the pointer is kept in the irq

 * domain struct.

/**

 * irq_domain_free_fwnode - Free a non-OF-backed fwnode_handle

 *

 * Free a fwnode_handle allocated with irq_domain_alloc_fwnode.

/**

 * __irq_domain_add() - Allocate a new irq_domain data structure

 * @fwnode: firmware node for the interrupt controller

 * @size: Size of linear map; 0 for radix mapping only

 * @hwirq_max: Maximum number of interrupts supported by controller

 * @direct_max: Maximum value of direct maps; Use ~0 for no limit; 0 for no

 *              direct mapping

 * @ops: domain callbacks

 * @host_data: Controller private data pointer

 *

 * Allocates and initializes an irq_domain structure.

 * Returns pointer to IRQ domain, or NULL on failure.

		/*

		 * fwnode paths contain '/', which debugfs is legitimately

		 * unhappy about. Replace them with ':', which does

		 * the trick and is not as offensive as '\'...

 Fill structure */

/**

 * irq_domain_remove() - Remove an irq domain.

 * @domain: domain to remove

 *

 * This routine is used to remove an irq domain. The caller must ensure

 * that all mappings within the domain have been disposed of prior to

 * use, depending on the revmap type.

	/*

	 * If the going away domain is the default one, reset it.

/**

 * irq_domain_create_simple() - Register an irq_domain and optionally map a range of irqs

 * @fwnode: firmware node for the interrupt controller

 * @size: total number of irqs in mapping

 * @first_irq: first number of irq block assigned to the domain,

 *	pass zero to assign irqs on-the-fly. If first_irq is non-zero, then

 *	pre-map all of the irqs in the domain to virqs starting at first_irq.

 * @ops: domain callbacks

 * @host_data: Controller private data pointer

 *

 * Allocates an irq_domain, and optionally if first_irq is positive then also

 * allocate irq_descs and map all of the hwirqs to virqs starting at first_irq.

 *

 * This is intended to implement the expected behaviour for most

 * interrupt controllers. If device tree is used, then first_irq will be 0 and

 * irqs get mapped dynamically on the fly. However, if the controller requires

 * static virq assignments (non-DT boot) then it will set that up correctly.

 attempt to allocated irq_descs */

/**

 * irq_domain_add_legacy() - Allocate and register a legacy revmap irq_domain.

 * @of_node: pointer to interrupt controller's device tree node.

 * @size: total number of irqs in legacy mapping

 * @first_irq: first number of irq block assigned to the domain

 * @first_hwirq: first hwirq number to use for the translation. Should normally

 *               be '0', but a positive integer can be used if the effective

 *               hwirqs numbering does not begin at zero.

 * @ops: map/unmap domain callbacks

 * @host_data: Controller private data pointer

 *

 * Note: the map() callback will be called before this function returns

 * for all legacy interrupts except 0 (which is always the invalid irq for

 * a legacy controller).

/**

 * irq_find_matching_fwspec() - Locates a domain for a given fwspec

 * @fwspec: FW specifier for an interrupt

 * @bus_token: domain-specific data

	/* We might want to match the legacy controller last since

	 * it might potentially be set to match all interrupts in

	 * the absence of a device node. This isn't a problem so far

	 * yet though...

	 *

	 * bus_token == DOMAIN_BUS_ANY matches any domain, any other

	 * values must generate an exact match for the domain to be

	 * selected.

/**

 * irq_domain_check_msi_remap - Check whether all MSI irq domains implement

 * IRQ remapping

 *

 * Return: false if any MSI irq domain does not support IRQ remapping,

 * true otherwise (including if there is no MSI irq domain)

/**

 * irq_set_default_host() - Set a "default" irq domain

 * @domain: default domain pointer

 *

 * For convenience, it's possible to set a "default" domain that will be used

 * whenever NULL is passed to irq_create_mapping(). It makes life easier for

 * platforms that want to manipulate a few hard coded interrupt numbers that

 * aren't properly represented in the device-tree.

/**

 * irq_get_default_host() - Retrieve the "default" irq domain

 *

 * Returns: the default domain, if any.

 *

 * Modern code should never use this. This should only be used on

 * systems that cannot implement a firmware->fwnode mapping (which

 * both DT and ACPI provide).

 remove chip and handler */

 Make sure it's completed */

 Tell the PIC about it */

 Clear reverse map for this hwirq */

			/*

			 * If map() returns -EPERM, this interrupt is protected

			 * by the firmware or some other service and shall not

			 * be mapped. Don't bother telling the user about it.

 If not already assigned, give the domain the chip's name */

/**

 * irq_create_direct_mapping() - Allocate an irq for direct mapping

 * @domain: domain to allocate the irq for or NULL for default domain

 *

 * This routine is used for irq controllers which can choose the hardware

 * interrupt numbers they generate. In such a case it's simplest to use

 * the linux irq as the hardware interrupt number. It still uses the linear

 * or radix tree to store the mapping, but the irq controller can optimize

 * the revmap path by using the hwirq directly.

/**

 * irq_create_mapping_affinity() - Map a hardware interrupt into linux irq space

 * @domain: domain owning this hardware interrupt or NULL for default domain

 * @hwirq: hardware irq number in that domain space

 * @affinity: irq affinity

 *

 * Only one mapping per hardware interrupt is permitted. Returns a linux

 * irq number.

 * If the sense/trigger is to be specified, set_irq_type() should be called

 * on the number returned from that call.

 Look for default domain if necessary */

 Check if mapping already exists */

 Allocate a virtual interrupt number */

 If domain has no translation, then we assume interrupt line */

	/*

	 * WARN if the irqchip returns a type with bits

	 * outside the sense mask set and clear these bits.

	/*

	 * If we've already configured this interrupt,

	 * don't do it again, or hell will break loose.

		/*

		 * If the trigger type is not specified or matches the

		 * current trigger type then we are done so return the

		 * interrupt number.

		/*

		 * If the trigger type has not been set yet, then set

		 * it now and return the interrupt number.

 Create mapping */

 Store trigger type */

/**

 * irq_dispose_mapping() - Unmap an interrupt

 * @virq: linux irq number of the interrupt to unmap

/**

 * __irq_resolve_mapping() - Find a linux irq from a hw irq number.

 * @domain: domain owning this hardware interrupt

 * @hwirq: hardware irq number in that domain space

 * @irq: optional pointer to return the Linux irq if required

 *

 * Returns the interrupt descriptor.

 Look for default domain if necessary */

 Check if the hwirq is in the linear revmap. */

/**

 * irq_domain_xlate_onecell() - Generic xlate for direct one cell bindings

 *

 * Device Tree IRQ specifier translation function which works with one cell

 * bindings where the cell value maps directly to the hwirq number.

/**

 * irq_domain_xlate_twocell() - Generic xlate for direct two cell bindings

 *

 * Device Tree IRQ specifier translation function which works with two cell

 * bindings where the cell values map directly to the hwirq number

 * and linux irq flags.

/**

 * irq_domain_xlate_onetwocell() - Generic xlate for one or two cell bindings

 *

 * Device Tree IRQ specifier translation function which works with either one

 * or two cell bindings where the cell values map directly to the hwirq number

 * and linux irq flags.

 *

 * Note: don't use this function unless your interrupt controller explicitly

 * supports both one and two cell bindings.  For the majority of controllers

 * the _onecell() or _twocell() variants above should be used.

/**

 * irq_domain_translate_onecell() - Generic translate for direct one cell

 * bindings

/**

 * irq_domain_translate_twocell() - Generic translate for direct two cell

 * bindings

 *

 * Device Tree IRQ specifier translation function which works with two cell

 * bindings where the cell values map directly to the hwirq number

 * and linux irq flags.

/**

 * irq_domain_reset_irq_data - Clear hwirq, chip and chip_data in @irq_data

 * @irq_data:	The pointer to irq_data

/**

 * irq_domain_create_hierarchy - Add a irqdomain into the hierarchy

 * @parent:	Parent irq domain to associate with the new domain

 * @flags:	Irq domain flags associated to the domain

 * @size:	Size of the domain. See below

 * @fwnode:	Optional fwnode of the interrupt controller

 * @ops:	Pointer to the interrupt domain callbacks

 * @host_data:	Controller private data pointer

 *

 * If @size is 0 a tree domain is created, otherwise a linear domain.

 *

 * If successful the parent is associated to the new domain and the

 * domain flags are set.

 * Returns pointer to IRQ domain, or NULL on failure.

 If not already assigned, give the domain the chip's name */

/**

 * irq_domain_disconnect_hierarchy - Mark the first unused level of a hierarchy

 * @domain:	IRQ domain from which the hierarchy is to be disconnected

 * @virq:	IRQ number where the hierarchy is to be trimmed

 *

 * Marks the @virq level belonging to @domain as disconnected.

 * Returns -EINVAL if @virq doesn't have a valid irq_data pointing

 * to @domain.

 *

 * Its only use is to be able to trim levels of hierarchy that do not

 * have any real meaning for this interrupt, and that the driver marks

 * as such from its .alloc() callback.

 The first entry must have a valid irqchip */

	/*

	 * Validate that the irq_data chain is sane in the presence of

	 * a hierarchy trimming marker.

 Can't have a valid irqchip after a trim marker */

 Can't have an empty irqchip before a trim marker */

 Only -ENOTCONN is a valid trim marker */

 No trim marker, nothing to do */

 Sever the inner part of the hierarchy...  */

 The outermost irq_data is embedded in struct irq_desc */

/**

 * irq_domain_get_irq_data - Get irq_data associated with @virq and @domain

 * @domain:	domain to match

 * @virq:	IRQ number to get irq_data

/**

 * irq_domain_set_hwirq_and_chip - Set hwirq and irqchip of @virq at @domain

 * @domain:	Interrupt domain to match

 * @virq:	IRQ number

 * @hwirq:	The hwirq number

 * @chip:	The associated interrupt chip

 * @chip_data:	The associated chip data

/**

 * irq_domain_set_info - Set the complete data for a @virq in @domain

 * @domain:		Interrupt domain to match

 * @virq:		IRQ number

 * @hwirq:		The hardware interrupt number

 * @chip:		The associated interrupt chip

 * @chip_data:		The associated interrupt chip data

 * @handler:		The interrupt flow handler

 * @handler_data:	The interrupt flow handler data

 * @handler_name:	The interrupt handler name

/**

 * irq_domain_free_irqs_common - Clear irq_data and free the parent

 * @domain:	Interrupt domain to match

 * @virq:	IRQ number to start with

 * @nr_irqs:	The number of irqs to free

/**

 * irq_domain_free_irqs_top - Clear handler and handler data, clear irqdata and free parent

 * @domain:	Interrupt domain to match

 * @virq:	IRQ number to start with

 * @nr_irqs:	The number of irqs to free

/**

 * __irq_domain_alloc_irqs - Allocate IRQs from domain

 * @domain:	domain to allocate from

 * @irq_base:	allocate specified IRQ number if irq_base >= 0

 * @nr_irqs:	number of IRQs to allocate

 * @node:	NUMA node id for memory allocation

 * @arg:	domain specific argument

 * @realloc:	IRQ descriptors have already been allocated if true

 * @affinity:	Optional irq affinity mask for multiqueue devices

 *

 * Allocate IRQ numbers and initialized all data structures to support

 * hierarchy IRQ domains.

 * Parameter @realloc is mainly to support legacy IRQs.

 * Returns error code or allocated IRQ number

 *

 * The whole process to setup an IRQ has been split into two steps.

 * The first step, __irq_domain_alloc_irqs(), is to allocate IRQ

 * descriptor and required hardware resources. The second step,

 * irq_domain_activate_irq(), is to program the hardware with preallocated

 * resources. In this way, it's easier to rollback when failing to

 * allocate resources.

 The irq_data was moved, fix the revmap to refer to the new location */

 Fix up the revmap. */

 Not using radix tree */

/**

 * irq_domain_push_irq() - Push a domain in to the top of a hierarchy.

 * @domain:	Domain to push.

 * @virq:	Irq to push the domain in to.

 * @arg:	Passed to the irq_domain_ops alloc() function.

 *

 * For an already existing irqdomain hierarchy, as might be obtained

 * via a call to pci_enable_msix(), add an additional domain to the

 * head of the processing chain.  Must be called before request_irq()

 * has been called.

	/*

	 * Check that no action has been set, which indicates the virq

	 * is in a state where this function doesn't have to deal with

	 * races between interrupt handling and maintaining the

	 * hierarchy.  This will catch gross misuse.  Attempting to

	 * make the check race free would require holding locks across

	 * calls to struct irq_domain_ops->alloc(), which could lead

	 * to deadlock, so we just do a simple check before starting.

 Copy the original irq_data. */

	/*

	 * Overwrite the root_irq_data, which is embedded in struct

	 * irq_desc, with values for this domain.

 May (probably does) set hwirq, chip, etc. */

 Restore the original irq_data. */

/**

 * irq_domain_pop_irq() - Remove a domain from the top of a hierarchy.

 * @domain:	Domain to remove.

 * @virq:	Irq to remove the domain from.

 *

 * Undo the effects of a call to irq_domain_push_irq().  Must be

 * called either before request_irq() or after free_irq().

	/*

	 * Check that no action is set, which indicates the virq is in

	 * a state where this function doesn't have to deal with races

	 * between interrupt handling and maintaining the hierarchy.

	 * This will catch gross misuse.  Attempting to make the check

	 * race free would require holding locks across calls to

	 * struct irq_domain_ops->free(), which could lead to

	 * deadlock, so we just do a simple check before starting.

 We can only "pop" if this domain is at the top of the list */

 Restore the original irq_data. */

/**

 * irq_domain_free_irqs - Free IRQ number and associated data structures

 * @virq:	base IRQ number

 * @nr_irqs:	number of IRQs to free

/**

 * irq_domain_alloc_irqs_parent - Allocate interrupts from parent domain

 * @domain:	Domain below which interrupts must be allocated

 * @irq_base:	Base IRQ number

 * @nr_irqs:	Number of IRQs to allocate

 * @arg:	Allocation data (arch/domain specific)

/**

 * irq_domain_free_irqs_parent - Free interrupts from parent domain

 * @domain:	Domain below which interrupts must be freed

 * @irq_base:	Base IRQ number

 * @nr_irqs:	Number of IRQs to free

 Rollback in case of error */

/**

 * irq_domain_activate_irq - Call domain_ops->activate recursively to activate

 *			     interrupt

 * @irq_data:	Outermost irq_data associated with interrupt

 * @reserve:	If set only reserve an interrupt vector instead of assigning one

 *

 * This is the second step to call domain_ops->activate to program interrupt

 * controllers, so the interrupt could actually get delivered.

/**

 * irq_domain_deactivate_irq - Call domain_ops->deactivate recursively to

 *			       deactivate interrupt

 * @irq_data: outermost irq_data associated with interrupt

 *

 * It calls domain_ops->deactivate to program interrupt controllers to disable

 * interrupt delivery.

 Hierarchy irq_domains must implement callback alloc() */

/**

 * irq_domain_hierarchical_is_msi_remap - Check if the domain or any

 * parent has MSI remapping support

 * @domain: domain pointer

 CONFIG_IRQ_DOMAIN_HIERARCHY */

/**

 * irq_domain_get_irq_data - Get irq_data associated with @virq and @domain

 * @domain:	domain to match

 * @virq:	IRQ number to get irq_data

/**

 * irq_domain_set_info - Set the complete data for a @virq in @domain

 * @domain:		Interrupt domain to match

 * @virq:		IRQ number

 * @hwirq:		The hardware interrupt number

 * @chip:		The associated interrupt chip

 * @chip_data:		The associated interrupt chip data

 * @handler:		The interrupt flow handler

 * @handler_data:	The interrupt flow handler data

 * @handler_name:	The interrupt handler name

 CONFIG_IRQ_DOMAIN_HIERARCHY */

 Default domain? Might be NULL */

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic cpu hotunplug interrupt migration code copied from the

 * arch/arm implementation

 *

 * Copyright (C) Russell King

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 For !GENERIC_IRQ_EFFECTIVE_AFF_MASK this looks at general affinity mask */

	/*

	 * The cpumask_empty() check is a workaround for interrupt chips,

	 * which do not implement effective affinity, but the architecture has

	 * enabled the config switch. Use the general affinity mask instead.

	/*

	 * Sanity check. If the mask is not empty when excluding the outgoing

	 * CPU then it must contain at least one online CPU. The outgoing CPU

	 * has been removed from the online mask already.

		/*

		 * If this happens then there was a missed IRQ fixup at some

		 * point. Warn about it and enforce fixup.

	/*

	 * IRQ chip might be already torn down, but the irq descriptor is

	 * still in the radix tree. Also if the chip has no affinity setter,

	 * nothing can be done here.

	/*

	 * No move required, if:

	 * - Interrupt is per cpu

	 * - Interrupt is not started

	 * - Affinity mask does not include this CPU.

	 *

	 * Note: Do not check desc->action as this might be a chained

	 * interrupt.

		/*

		 * If an irq move is pending, abort it if the dying CPU is

		 * the sole target.

	/*

	 * Complete an eventually pending irq move cleanup. If this

	 * interrupt was moved in hard irq context, then the vectors need

	 * to be cleaned up. It can't wait until this interrupt actually

	 * happens and this CPU was involved.

	/*

	 * If there is a setaffinity pending, then try to reuse the pending

	 * mask, so the last change of the affinity does not get lost. If

	 * there is no move pending or the pending mask does not contain

	 * any online CPU, use the current affinity mask.

 Mask the chip for interrupts which cannot move in process context */

		/*

		 * If the interrupt is managed, then shut it down and leave

		 * the affinity untouched.

	/*

	 * Do not set the force argument of irq_do_set_affinity() as this

	 * disables the masking of offline CPUs from the supplied affinity

	 * mask and therefore might keep/reassign the irq to the outgoing

	 * CPU.

/**

 * irq_migrate_all_off_this_cpu - Migrate irqs away from offline cpu

 *

 * The current CPU has been marked offline.  Migrate IRQs off this CPU.

 * If the affinity settings do not allow other CPUs, force them onto any

 * available CPU.

 *

 * Note: we must iterate over all IRQs, whether they have an attached

 * action structure or not, as we need to get chained interrupts too.

	/*

	 * If the interrupt can only be directed to a single target

	 * CPU then it is already assigned to a CPU in the affinity

	 * mask. No point in trying to move it around unless the

	 * isolation mechanism requests to move it to an upcoming

	 * housekeeping CPU.

/**

 * irq_affinity_online_cpu - Restore affinity for managed interrupts

 * @cpu:	Upcoming CPU for which interrupts should be restored

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2017 Thomas Gleixner <tglx@linutronix.de>

/**

 * irq_alloc_matrix - Allocate a irq_matrix structure and initialize it

 * @matrix_bits:	Number of matrix bits must be <= IRQ_MATRIX_BITS

 * @alloc_start:	From which bit the allocation search starts

 * @alloc_end:		At which bit the allocation search ends, i.e first

 *			invalid bit

/**

 * irq_matrix_online - Bring the local CPU matrix online

 * @m:		Matrix pointer

/**

 * irq_matrix_offline - Bring the local CPU matrix offline

 * @m:		Matrix pointer

 Update the global available size */

 Find the best CPU which has the lowest vector allocation count */

 Find the best CPU which has the lowest number of managed IRQs allocated */

/**

 * irq_matrix_assign_system - Assign system wide entry in the matrix

 * @m:		Matrix pointer

 * @bit:	Which bit to reserve

 * @replace:	Replace an already allocated vector with a system

 *		vector at the same bit position.

 *

 * The BUG_ON()s below are on purpose. If this goes wrong in the

 * early boot process, then the chance to survive is about zero.

 * If this happens when the system is life, it's not much better.

/**

 * irq_matrix_reserve_managed - Reserve a managed interrupt in a CPU map

 * @m:		Matrix pointer

 * @msk:	On which CPUs the bits should be reserved.

 *

 * Can be called for offline CPUs. Note, this will only reserve one bit

 * on all CPUs in @msk, but it's not guaranteed that the bits are at the

 * same offset on all CPUs

/**

 * irq_matrix_remove_managed - Remove managed interrupts in a CPU map

 * @m:		Matrix pointer

 * @msk:	On which CPUs the bits should be removed

 *

 * Can be called for offline CPUs

 *

 * This removes not allocated managed interrupts from the map. It does

 * not matter which one because the managed interrupts free their

 * allocation when they shut down. If not, the accounting is screwed,

 * but all what can be done at this point is warn about it.

 Get managed bit which are not allocated */

/**

 * irq_matrix_alloc_managed - Allocate a managed interrupt in a CPU map

 * @m:		Matrix pointer

 * @msk:	Which CPUs to search in

 * @mapped_cpu:	Pointer to store the CPU for which the irq was allocated

 Get managed bit which are not allocated */

/**

 * irq_matrix_assign - Assign a preallocated interrupt in the local CPU map

 * @m:		Matrix pointer

 * @bit:	Which bit to mark

 *

 * This should only be used to mark preallocated vectors

/**

 * irq_matrix_reserve - Reserve interrupts

 * @m:		Matrix pointer

 *

 * This is merely a book keeping call. It increments the number of globally

 * reserved interrupt bits w/o actually allocating them. This allows to

 * setup interrupt descriptors w/o assigning low level resources to it.

 * The actual allocation happens when the interrupt gets activated.

/**

 * irq_matrix_remove_reserved - Remove interrupt reservation

 * @m:		Matrix pointer

 *

 * This is merely a book keeping call. It decrements the number of globally

 * reserved interrupt bits. This is used to undo irq_matrix_reserve() when the

 * interrupt was never in use and a real vector allocated, which undid the

 * reservation.

/**

 * irq_matrix_alloc - Allocate a regular interrupt in a CPU map

 * @m:		Matrix pointer

 * @msk:	Which CPUs to search in

 * @reserved:	Allocate previously reserved interrupts

 * @mapped_cpu: Pointer to store the CPU for which the irq was allocated

	/*

	 * Not required in theory, but matrix_find_best_cpu() uses

	 * for_each_cpu() which ignores the cpumask on UP .

/**

 * irq_matrix_free - Free allocated interrupt in the matrix

 * @m:		Matrix pointer

 * @cpu:	Which CPU map needs be updated

 * @bit:	The bit to remove

 * @managed:	If true, the interrupt is managed and not accounted

 *		as available.

/**

 * irq_matrix_available - Get the number of globally available irqs

 * @m:		Pointer to the matrix to query

 * @cpudown:	If true, the local CPU is about to go down, adjust

 *		the number of available irqs accordingly

/**

 * irq_matrix_reserved - Get the number of globally reserved irqs

 * @m:		Pointer to the matrix to query

/**

 * irq_matrix_allocated - Get the number of allocated irqs on the local cpu

 * @m:		Pointer to the matrix to search

 *

 * This returns number of allocated irqs

/**

 * irq_matrix_debug_show - Show detailed allocation information

 * @sf:		Pointer to the seq_file to print to

 * @m:		Pointer to the matrix allocator

 * @ind:	Indentation for the print format

 *

 * Note, this is a lockless snapshot.

 SPDX-License-Identifier: GPL-2.0

/**

 * irq_fixup_move_pending - Cleanup irq move pending from a dying CPU

 * @desc:		Interrupt descriptor to clean up

 * @force_clear:	If set clear the move pending bit unconditionally.

 *			If not set, clear it only when the dying CPU is the

 *			last one in the pending mask.

 *

 * Returns true if the pending bit was set and the pending mask contains an

 * online CPU other than the dying CPU.

	/*

	 * The outgoing CPU might be the last online target in a pending

	 * interrupt move. If that's the case clear the pending move bit.

	/*

	 * Paranoia: cpu-local interrupts shouldn't be calling in here anyway.

	/*

	 * If there was a valid mask to work with, please

	 * do the disable, re-program, enable sequence.

	 * This is *not* particularly important for level triggered

	 * but in a edge trigger case, we might be setting rte

	 * when an active trigger is coming in. This could

	 * cause some ioapics to mal-function.

	 * Being paranoid i guess!

	 *

	 * For correct operation this depends on the caller

	 * masking the irqs.

		/*

		 * If the there is a cleanup pending in the underlying

		 * vector management, reschedule the move for the next

		 * interrupt. Leave desc->pending_mask intact.

	/*

	 * Get top level irq_data when CONFIG_IRQ_DOMAIN_HIERARCHY is enabled,

	 * and it should be optimized away when CONFIG_IRQ_DOMAIN_HIERARCHY is

	 * disabled. So we avoid an "#ifdef CONFIG_IRQ_DOMAIN_HIERARCHY" here.

	/*

	 * Be careful vs. already masked interrupts. If this is a

	 * threaded interrupt with ONESHOT set, we can end up with an

	 * interrupt storm.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1992, 1998-2006 Linus Torvalds, Ingo Molnar

 * Copyright (C) 2005-2006, Thomas Gleixner, Russell King

 *

 * This file contains the core interrupt handling code, for irq-chip based

 * architectures. Detailed information is available in

 * Documentation/core-api/genericirq.rst

/*

 * Chained handlers should never call action on their IRQ. This default

 * action will emit warning if such thing happens.

/**

 *	irq_set_chip - set the irq chip for an irq

 *	@irq:	irq number

 *	@chip:	pointer to irq chip description structure

	/*

	 * For !CONFIG_SPARSE_IRQ make the irq show up in

	 * allocated_irqs.

/**

 *	irq_set_irq_type - set the irq trigger type for an irq

 *	@irq:	irq number

 *	@type:	IRQ_TYPE_{LEVEL,EDGE}_* value - see include/linux/irq.h

/**

 *	irq_set_handler_data - set irq handler data for an irq

 *	@irq:	Interrupt number

 *	@data:	Pointer to interrupt specific data

 *

 *	Set the hardware irq controller data for an irq

/**

 *	irq_set_msi_desc_off - set MSI descriptor data for an irq at offset

 *	@irq_base:	Interrupt number base

 *	@irq_offset:	Interrupt number offset

 *	@entry:		Pointer to MSI descriptor data

 *

 *	Set the MSI descriptor entry for an irq at offset

/**

 *	irq_set_msi_desc - set MSI descriptor data for an irq

 *	@irq:	Interrupt number

 *	@entry:	Pointer to MSI descriptor data

 *

 *	Set the MSI descriptor entry for an irq

/**

 *	irq_set_chip_data - set irq chip data for an irq

 *	@irq:	Interrupt number

 *	@data:	Pointer to chip specific data

 *

 *	Set the hardware irq chip data for an irq

		/*

		 * Catch code which fiddles with enable_irq() on a managed

		 * and potentially shutdown IRQ. Chained interrupt

		 * installment or irq auto probing should not happen on

		 * managed irqs either.

		/*

		 * The interrupt was requested, but there is no online CPU

		 * in it's affinity mask. Put it into managed shutdown

		 * state and let the cpu hotplug mechanism start it up once

		 * a CPU in the mask becomes available.

	/*

	 * Managed interrupts have reserved resources, so this should not

	 * happen.

 Warn if this interrupt is not activated but try nevertheless */

	/*

	 * This must be called even if the interrupt was never started up,

	 * because the activation can happen before the interrupt is

	 * available for request/startup. It has it's own state tracking so

	 * it's safe to call it unconditionally.

/**

 * irq_disable - Mark interrupt disabled

 * @desc:	irq descriptor which should be disabled

 *

 * If the chip does not implement the irq_disable callback, we

 * use a lazy disable approach. That means we mark the interrupt

 * disabled, but leave the hardware unmasked. That's an

 * optimization because we avoid the hardware access for the

 * common case where no interrupt happens after we marked it

 * disabled. If an interrupt happens, then the interrupt flow

 * handler masks the line at the hardware level and marks it

 * pending.

 *

 * If the interrupt chip does not implement the irq_disable callback,

 * a driver can disable the lazy approach for a particular irq line by

 * calling 'irq_set_status_flags(irq, IRQ_DISABLE_UNLAZY)'. This can

 * be used for devices which cannot disable the interrupt at the

 * device level under certain circumstances and have to use

 * disable_irq[_nosync] instead.

/*

 *	handle_nested_irq - Handle a nested irq from a irq thread

 *	@irq:	the interrupt number

 *

 *	Handle interrupts which are nested into a threaded interrupt

 *	handler. The handler function is called inside the calling

 *	threads context.

	/*

	 * If the interrupt is not in progress and is not an armed

	 * wakeup interrupt, proceed.

	/*

	 * If the interrupt is an armed wakeup source, mark it pending

	 * and suspended, disable it and notify the pm core about the

	 * event.

	/*

	 * Handle a potential concurrent poll on a different core.

/**

 *	handle_simple_irq - Simple and software-decoded IRQs.

 *	@desc:	the interrupt description structure for this irq

 *

 *	Simple interrupts are either sent from a demultiplexing interrupt

 *	handler or come from hardware, where no interrupt hardware control

 *	is necessary.

 *

 *	Note: The caller is expected to handle the ack, clear, mask and

 *	unmask issues if necessary.

/**

 *	handle_untracked_irq - Simple and software-decoded IRQs.

 *	@desc:	the interrupt description structure for this irq

 *

 *	Untracked interrupts are sent from a demultiplexing interrupt

 *	handler when the demultiplexer does not know which device it its

 *	multiplexed irq domain generated the interrupt. IRQ's handled

 *	through here are not subjected to stats tracking, randomness, or

 *	spurious interrupt detection.

 *

 *	Note: Like handle_simple_irq, the caller is expected to handle

 *	the ack, clear, mask and unmask issues if necessary.

/*

 * Called unconditionally from handle_level_irq() and only for oneshot

 * interrupts from handle_fasteoi_irq()

	/*

	 * We need to unmask in the following cases:

	 * - Standard level irq (IRQF_ONESHOT is not set)

	 * - Oneshot irq which did not wake the thread (caused by a

	 *   spurious interrupt or a primary handler handling it

	 *   completely).

/**

 *	handle_level_irq - Level type irq handler

 *	@desc:	the interrupt description structure for this irq

 *

 *	Level type interrupts are active as long as the hardware line has

 *	the active level. This may require to mask the interrupt and unmask

 *	it after the associated handler has acknowledged the device, so the

 *	interrupt line is back to inactive.

	/*

	 * If its disabled or no action available

	 * keep it masked and get out of here

	/*

	 * We need to unmask in the following cases:

	 * - Oneshot irq which did not wake the thread (caused by a

	 *   spurious interrupt or a primary handler handling it

	 *   completely).

/**

 *	handle_fasteoi_irq - irq handler for transparent controllers

 *	@desc:	the interrupt description structure for this irq

 *

 *	Only a single callback will be issued to the chip: an ->eoi()

 *	call when the interrupt has been serviced. This enables support

 *	for modern forms of interrupt handlers, which handle the flow

 *	details in hardware, transparently.

	/*

	 * If its disabled or no action available

	 * then mask it and get out of here:

/**

 *	handle_fasteoi_nmi - irq handler for NMI interrupt lines

 *	@desc:	the interrupt description structure for this irq

 *

 *	A simple NMI-safe handler, considering the restrictions

 *	from request_nmi.

 *

 *	Only a single callback will be issued to the chip: an ->eoi()

 *	call when the interrupt has been serviced. This enables support

 *	for modern forms of interrupt handlers, which handle the flow

 *	details in hardware, transparently.

	/*

	 * NMIs cannot be shared, there is only one action.

/**

 *	handle_edge_irq - edge type IRQ handler

 *	@desc:	the interrupt description structure for this irq

 *

 *	Interrupt occurs on the falling and/or rising edge of a hardware

 *	signal. The occurrence is latched into the irq controller hardware

 *	and must be acked in order to be reenabled. After the ack another

 *	interrupt can happen on the same source even before the first one

 *	is handled by the associated event handler. If this happens it

 *	might be necessary to disable (mask) the interrupt depending on the

 *	controller hardware. This requires to reenable the interrupt inside

 *	of the loop which handles the interrupts which have arrived while

 *	the handler was running. If all pending interrupts are handled, the

 *	loop is left.

	/*

	 * If its disabled or no action available then mask it and get

	 * out of here.

 Start handling the irq */

		/*

		 * When another irq arrived while we were handling

		 * one, we could have masked the irq.

		 * Reenable it, if it was not disabled in meantime.

/**

 *	handle_edge_eoi_irq - edge eoi type IRQ handler

 *	@desc:	the interrupt description structure for this irq

 *

 * Similar as the above handle_edge_irq, but using eoi and w/o the

 * mask/unmask logic.

	/*

	 * If its disabled or no action available then mask it and get

	 * out of here.

/**

 *	handle_percpu_irq - Per CPU local irq handler

 *	@desc:	the interrupt description structure for this irq

 *

 *	Per CPU interrupts on SMP machines without locking requirements

	/*

	 * PER CPU interrupts are not serialized. Do not touch

	 * desc->tot_count.

/**

 * handle_percpu_devid_irq - Per CPU local irq handler with per cpu dev ids

 * @desc:	the interrupt description structure for this irq

 *

 * Per CPU interrupts on SMP machines without locking requirements. Same as

 * handle_percpu_irq() above but with the following extras:

 *

 * action->percpu_dev_id is a pointer to percpu variables which

 * contain the real device id for the cpu on which this handler is

 * called

	/*

	 * PER CPU interrupts are not serialized. Do not touch

	 * desc->tot_count.

/**

 * handle_percpu_devid_fasteoi_nmi - Per CPU local NMI handler with per cpu

 *				     dev ids

 * @desc:	the interrupt description structure for this irq

 *

 * Similar to handle_fasteoi_nmi, but handling the dev_id cookie

 * as a percpu pointer.

		/*

		 * With hierarchical domains we might run into a

		 * situation where the outermost chip is not yet set

		 * up, but the inner chips are there.  Instead of

		 * bailing we install the handler, but obviously we

		 * cannot enable/startup the interrupt at this point.

			/*

			 * Bail out if the outer chip is not set up

			 * and the interrupt supposed to be started

			 * right away.

 Try the parent */

 Uninstall? */

		/*

		 * We're about to start this interrupt immediately,

		 * hence the need to set the trigger configuration.

		 * But the .set_type callback may have overridden the

		 * flow handler, ignoring that we're dealing with a

		 * chained interrupt. Reset it immediately because we

		 * do know better.

	/*

	 * Warn when a driver sets the no autoenable flag on an already

	 * active interrupt.

/**

 *	irq_cpu_online - Invoke all irq_cpu_online functions.

 *

 *	Iterate through all irqs and invoke the chip.irq_cpu_online()

 *	for each.

/**

 *	irq_cpu_offline - Invoke all irq_cpu_offline functions.

 *

 *	Iterate through all irqs and invoke the chip.irq_cpu_offline()

 *	for each.

/**

 *	handle_fasteoi_ack_irq - irq handler for edge hierarchy

 *	stacked on transparent controllers

 *

 *	@desc:	the interrupt description structure for this irq

 *

 *	Like handle_fasteoi_irq(), but for use with hierarchy where

 *	the irq_chip also needs to have its ->irq_ack() function

 *	called.

	/*

	 * If its disabled or no action available

	 * then mask it and get out of here:

 Start handling the irq */

/**

 *	handle_fasteoi_mask_irq - irq handler for level hierarchy

 *	stacked on transparent controllers

 *

 *	@desc:	the interrupt description structure for this irq

 *

 *	Like handle_fasteoi_irq(), but for use with hierarchy where

 *	the irq_chip also needs to have its ->irq_mask_ack() function

 *	called.

	/*

	 * If its disabled or no action available

	 * then mask it and get out of here:

 CONFIG_IRQ_FASTEOI_HIERARCHY_HANDLERS */

/**

 * irq_chip_set_parent_state - set the state of a parent interrupt.

 *

 * @data: Pointer to interrupt specific data

 * @which: State to be restored (one of IRQCHIP_STATE_*)

 * @val: Value corresponding to @which

 *

 * Conditional success, if the underlying irqchip does not implement it.

/**

 * irq_chip_get_parent_state - get the state of a parent interrupt.

 *

 * @data: Pointer to interrupt specific data

 * @which: one of IRQCHIP_STATE_* the caller wants to know

 * @state: a pointer to a boolean where the state is to be stored

 *

 * Conditional success, if the underlying irqchip does not implement it.

/**

 * irq_chip_enable_parent - Enable the parent interrupt (defaults to unmask if

 * NULL)

 * @data:	Pointer to interrupt specific data

/**

 * irq_chip_disable_parent - Disable the parent interrupt (defaults to mask if

 * NULL)

 * @data:	Pointer to interrupt specific data

/**

 * irq_chip_ack_parent - Acknowledge the parent interrupt

 * @data:	Pointer to interrupt specific data

/**

 * irq_chip_mask_parent - Mask the parent interrupt

 * @data:	Pointer to interrupt specific data

/**

 * irq_chip_mask_ack_parent - Mask and acknowledge the parent interrupt

 * @data:	Pointer to interrupt specific data

/**

 * irq_chip_unmask_parent - Unmask the parent interrupt

 * @data:	Pointer to interrupt specific data

/**

 * irq_chip_eoi_parent - Invoke EOI on the parent interrupt

 * @data:	Pointer to interrupt specific data

/**

 * irq_chip_set_affinity_parent - Set affinity on the parent interrupt

 * @data:	Pointer to interrupt specific data

 * @dest:	The affinity mask to set

 * @force:	Flag to enforce setting (disable online checks)

 *

 * Conditional, as the underlying parent chip might not implement it.

/**

 * irq_chip_set_type_parent - Set IRQ type on the parent interrupt

 * @data:	Pointer to interrupt specific data

 * @type:	IRQ_TYPE_{LEVEL,EDGE}_* value - see include/linux/irq.h

 *

 * Conditional, as the underlying parent chip might not implement it.

/**

 * irq_chip_retrigger_hierarchy - Retrigger an interrupt in hardware

 * @data:	Pointer to interrupt specific data

 *

 * Iterate through the domain hierarchy of the interrupt and check

 * whether a hw retrigger function exists. If yes, invoke it.

/**

 * irq_chip_set_vcpu_affinity_parent - Set vcpu affinity on the parent interrupt

 * @data:	Pointer to interrupt specific data

 * @vcpu_info:	The vcpu affinity information

/**

 * irq_chip_set_wake_parent - Set/reset wake-up on the parent interrupt

 * @data:	Pointer to interrupt specific data

 * @on:		Whether to set or reset the wake-up capability of this irq

 *

 * Conditional, as the underlying parent chip might not implement it.

/**

 * irq_chip_request_resources_parent - Request resources on the parent interrupt

 * @data:	Pointer to interrupt specific data

/**

 * irq_chip_release_resources_parent - Release resources on the parent interrupt

 * @data:	Pointer to interrupt specific data

/**

 * irq_chip_compose_msi_msg - Compose msi message for a irq chip

 * @data:	Pointer to interrupt specific data

 * @msg:	Pointer to the MSI message

 *

 * For hierarchical domains we find the first chip in the hierarchy

 * which implements the irq_compose_msi_msg callback. For non

 * hierarchical we use the top level chip.

/**

 * irq_chip_pm_get - Enable power for an IRQ chip

 * @data:	Pointer to interrupt specific data

 *

 * Enable the power to the IRQ chip referenced by the interrupt data

 * structure.

/**

 * irq_chip_pm_put - Disable power for an IRQ chip

 * @data:	Pointer to interrupt specific data

 *

 * Disable the power to the IRQ chip referenced by the interrupt data

 * structure, belongs. Note that power will only be disabled, once this

 * function has been called for all IRQs that have called irq_chip_pm_get().

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2014 Intel Corp.

 * Author: Jiang Liu <jiang.liu@linux.intel.com>

 *

 * This file is licensed under GPLv2.

 *

 * This file contains common code to support Message Signaled Interrupts for

 * PCI compatible and non PCI compatible devices.

/**

 * alloc_msi_entry - Allocate an initialized msi_desc

 * @dev:	Pointer to the device for which this is allocated

 * @nvec:	The number of vectors used in this entry

 * @affinity:	Optional pointer to an affinity mask array size of @nvec

 *

 * If @affinity is not %NULL then an affinity array[@nvec] is allocated

 * and the affinity masks and flags from @affinity are copied.

 *

 * Return: pointer to allocated &msi_desc on success or %NULL on failure

/**

 * msi_populate_sysfs - Populate msi_irqs sysfs entries for devices

 * @dev:	The device(PCI, platform etc) who will get sysfs entries

 *

 * Return attribute_group ** so that specific bus MSI can save it to

 * somewhere during initilizing msi irqs. If devices has no MSI irq,

 * return NULL; if it fails to populate sysfs, return ERR_PTR

 Determine how many msi entries we have */

 Dynamically create the MSI attributes for the device */

/**

 * msi_destroy_sysfs - Destroy msi_irqs sysfs entries for devices

 * @dev:		The device(PCI, platform etc) who will remove sysfs entries

 * @msi_irq_groups:	attribute_group for device msi_irqs entries

	/*

	 * If the MSI provider has messed with the second message and

	 * not advertized that it is level-capable, signal the breakage.

/**

 * msi_domain_set_affinity - Generic affinity setter function for MSI domains

 * @irq_data:	The irq data associated to the interrupt

 * @mask:	The affinity mask to set

 * @force:	Flag to enforce setting (disable online checks)

 *

 * Intended to be used by MSI interrupt controllers which are

 * implemented with hierarchical domains.

 *

 * Return: IRQ_SET_MASK_* result code

/**

 * msi_create_irq_domain - Create an MSI interrupt domain

 * @fwnode:	Optional fwnode of the interrupt controller

 * @info:	MSI domain info

 * @parent:	Parent irq domain

 *

 * Return: pointer to the created &struct irq_domain or %NULL on failure

 Don't even try the multi-MSI brain damage. */

 Assumes the domain mutex is held! */

 Mop up the damage */

/*

 * Carefully check whether the device can use reservation mode. If

 * reservation mode is enabled then the early activation will assign a

 * dummy vector to the device. If the PCI/MSI device does not support

 * masking of the entry then this can result in spurious interrupts when

 * the device driver is not absolutely careful. But even then a malfunction

 * of the hardware could result in a spurious interrupt on the dummy vector

 * and render the device unusable. If the entry can be masked then the core

 * logic will prevent the spurious interrupt and reservation mode can be

 * used. For now reservation mode is restricted to PCI/MSI.

	/*

	 * Checking the first MSI descriptor is sufficient. MSIX supports

	 * masking and MSI does so when the can_mask attribute is set.

	/*

	 * This flag is set by the PCI layer as we need to activate

	 * the MSI entries before the PCI layer enables MSI in the

	 * card. Otherwise the card latches a random msi message.

	/*

	 * If these interrupts use reservation mode, clear the activated bit

	 * so request_irq() will assign the final vector.

/**

 * msi_domain_alloc_irqs - Allocate interrupts from a MSI interrupt domain

 * @domain:	The domain to allocate from

 * @dev:	Pointer to device struct of the device for which the interrupts

 *		are allocated

 * @nvec:	The number of interrupts to allocate

 *

 * Return: %0 on success or an error code.

		/*

		 * We might have failed to allocate an MSI early

		 * enough that there is no IRQ associated to this

		 * entry. If that's the case, don't do anything.

/**

 * msi_domain_free_irqs - Free interrupts from a MSI interrupt @domain associated to @dev

 * @domain:	The domain to managing the interrupts

 * @dev:	Pointer to device struct of the device for which the interrupts

 *		are free

/**

 * msi_get_domain_info - Get the MSI interrupt domain info for @domain

 * @domain:	The interrupt domain to retrieve data from

 *

 * Return: the pointer to the msi_domain_info stored in @domain->host_data.

 CONFIG_GENERIC_MSI_IRQ_DOMAIN */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1992, 1998-2006 Linus Torvalds, Ingo Molnar

 * Copyright (C) 2005-2006, Thomas Gleixner, Russell King

 *

 * This file contains the interrupt descriptor management code. Detailed

 * information is available in Documentation/core-api/genericirq.rst

 *

/*

 * lockdep: we want to handle all irq_desc locks as a single lock-class:

	/*

	 * Set at least the boot cpu. We don't want to end up with

	 * bugreports caused by random commandline masks

		/*

		 * Continue even in case of failure as this is nothing

		 * crucial.

	/*

	 * If irq_sysfs_init() has not yet been invoked (early boot), then

	 * irq_kobj_base is NULL and the descriptor was never added.

	 * kobject_del() complains about a object with no parent, so make

	 * it conditional.

 Prevent concurrent irq alloc/free */

 Add the already allocated interrupts */

 !CONFIG_SYSFS */

 CONFIG_SYSFS */

 allocate based on nr_cpu_ids */

	/*

	 * sparse_irq_lock protects also show_interrupts() and

	 * kstat_irq_usr(). Once we deleted the descriptor from the

	 * sparse tree we can free it. Access in proc will fail to

	 * lookup the descriptor.

	 *

	 * The sysfs entry must be serialized against a concurrent

	 * irq_sysfs_init() as well.

	/*

	 * We free the descriptor, masks and stat fields via RCU. That

	 * allows demultiplex interrupts to do rcu based management of

	 * the child interrupts.

	 * This also allows us to use rcu in kstat_irqs_usr().

 Validate affinity mask(s) */

 Let arch update nr_irqs and return the nr of preallocated irqs */

 !CONFIG_SPARSE_IRQ */

 !CONFIG_SPARSE_IRQ */

/**

 * generic_handle_irq - Invoke the handler for a particular irq

 * @irq:	The irq number to handle

 *

 * Returns:	0 on success, or -EINVAL if conversion has failed

 *

 * 		This function must be called from an IRQ context with irq regs

 * 		initialized.

/**

 * generic_handle_domain_irq - Invoke the handler for a HW irq belonging

 *                             to a domain.

 * @domain:	The domain where to perform the lookup

 * @hwirq:	The HW irq number to convert to a logical one

 *

 * Returns:	0 on success, or -EINVAL if conversion has failed

 *

 * 		This function must be called from an IRQ context with irq regs

 * 		initialized.

/**

 * generic_handle_domain_nmi - Invoke the handler for a HW nmi belonging

 *                             to a domain.

 * @domain:	The domain where to perform the lookup

 * @hwirq:	The HW irq number to convert to a logical one

 *

 * Returns:	0 on success, or -EINVAL if conversion has failed

 *

 * 		This function must be called from an NMI context with irq regs

 * 		initialized.

 Dynamic interrupt handling */

/**

 * irq_free_descs - free irq descriptors

 * @from:	Start of descriptor range

 * @cnt:	Number of consecutive irqs to free

/**

 * __irq_alloc_descs - allocate and initialize a range of irq descriptors

 * @irq:	Allocate for specific irq number if irq >= 0

 * @from:	Start the search from this irq number

 * @cnt:	Number of consecutive irqs to allocate.

 * @node:	Preferred node on which the irq descriptor should be allocated

 * @owner:	Owning module (can be NULL)

 * @affinity:	Optional pointer to an affinity mask array of size @cnt which

 *		hints where the irq descriptors should be allocated and which

 *		default affinities to use

 *

 * Returns the first irq number or error code

		/*

		 * For interrupts which are freely allocated the

		 * architecture can force a lower bound to the @from

		 * argument. x86 uses this to exclude the GSI space.

/**

 * irq_get_next_irq - get next allocated irq number

 * @offset:	where to start the search

 *

 * Returns next irq number after offset or nr_irqs if none is found.

/**

 * kstat_irqs_cpu - Get the statistics for an interrupt on a cpu

 * @irq:	The interrupt number

 * @cpu:	The cpu number

 *

 * Returns the sum of interrupt counts on @cpu since boot for

 * @irq. The caller must ensure that the interrupt is not removed

 * concurrently.

/**

 * kstat_irqs_usr - Get the statistics for an interrupt from thread context

 * @irq:	The interrupt number

 *

 * Returns the sum of interrupt counts on all cpus since boot for @irq.

 *

 * It uses rcu to protect the access since a concurrent removal of an

 * interrupt descriptor is observing an rcu grace period before

 * delayed_free_desc()/irq_kobj_release().

 SPDX-License-Identifier: GPL-2.0

 Copyright 2017 Thomas Gleixner <tglx@linutronix.de>

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2015 Imagination Technologies Ltd

 * Author: Qais Yousef <qais.yousef@imgtec.com>

 *

 * This file contains driver APIs to the IPI subsystem.

/**

 * irq_reserve_ipi() - Setup an IPI to destination cpumask

 * @domain:	IPI domain

 * @dest:	cpumask of CPUs which can receive the IPI

 *

 * Allocate a virq that can be used to send IPI to any CPU in dest mask.

 *

 * Return: Linux IRQ number on success or error code on failure

		/*

		 * If the underlying implementation uses a single HW irq on

		 * all cpus then we only need a single Linux irq number for

		 * it. We have no restrictions vs. the destination mask. The

		 * underlying implementation can deal with holes nicely.

		/*

		 * The IPI requires a separate HW irq on each CPU. We require

		 * that the destination mask is consecutive. If an

		 * implementation needs to support holes, it can reserve

		 * several IPI ranges.

		/*

		 * Find a hole and if found look for another set bit after the

		 * hole. For now we don't support this scenario.

/**

 * irq_destroy_ipi() - unreserve an IPI that was previously allocated

 * @irq:	Linux IRQ number to be destroyed

 * @dest:	cpumask of CPUs which should have the IPI removed

 *

 * The IPIs allocated with irq_reserve_ipi() are returned to the system

 * destroying all virqs associated with them.

 *

 * Return: %0 on success or error code on failure.

		/*

		 * Must be destroying a subset of CPUs to which this IPI

		 * was set up to target

/**

 * ipi_get_hwirq - Get the hwirq associated with an IPI to a CPU

 * @irq:	Linux IRQ number

 * @cpu:	the target CPU

 *

 * When dealing with coprocessors IPI, we need to inform the coprocessor of

 * the hwirq it needs to use to receive and send IPIs.

 *

 * Return: hwirq value on success or INVALID_HWIRQ on failure.

	/*

	 * Get the real hardware irq number if the underlying implementation

	 * uses a separate irq per cpu. If the underlying implementation uses

	 * a single hardware irq for all cpus then the IPI send mechanism

	 * needs to take care of the cpu destinations.

/**

 * __ipi_send_single - send an IPI to a target Linux SMP CPU

 * @desc:	pointer to irq_desc of the IRQ

 * @cpu:	destination CPU, must in the destination mask passed to

 *		irq_reserve_ipi()

 *

 * This function is for architecture or core code to speed up IPI sending. Not

 * usable from driver code.

 *

 * Return: %0 on success or negative error number on failure.

	/*

	 * Minimise the overhead by omitting the checks for Linux SMP IPIs.

	 * Since the callers should be arch or core code which is generally

	 * trusted, only check for errors when debugging.

 FIXME: Store this information in irqdata flags */

 use the correct data for that cpu */

/**

 * __ipi_send_mask - send an IPI to target Linux SMP CPU(s)

 * @desc:	pointer to irq_desc of the IRQ

 * @dest:	dest CPU(s), must be a subset of the mask passed to

 *		irq_reserve_ipi()

 *

 * This function is for architecture or core code to speed up IPI sending. Not

 * usable from driver code.

 *

 * Return: %0 on success or negative error number on failure.

	/*

	 * Minimise the overhead by omitting the checks for Linux SMP IPIs.

	 * Since the callers should be arch or core code which is generally

	 * trusted, only check for errors when debugging.

/**

 * ipi_send_single - Send an IPI to a single CPU

 * @virq:	Linux IRQ number from irq_reserve_ipi()

 * @cpu:	destination CPU, must in the destination mask passed to

 *		irq_reserve_ipi()

 *

 * Return: %0 on success or negative error number on failure.

/**

 * ipi_send_mask - Send an IPI to target CPU(s)

 * @virq:	Linux IRQ number from irq_reserve_ipi()

 * @dest:	dest CPU(s), must be a subset of the mask passed to

 *		irq_reserve_ipi()

 *

 * Return: %0 on success or negative error number on failure.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2016 Thomas Gleixner.

 * Copyright (C) 2016-2017 Christoph Hellwig.

 Should not happen, but I'm too lazy to think about it */

 If the cpu has siblings, use them first */

 Calculate the number of nodes in the supplied affinity mask */

/*

 * Allocate vector number for each node, so that for each node:

 *

 * 1) the allocated number is >= 1

 *

 * 2) the allocated numbver is <= active CPU number of this node

 *

 * The actual allocated total vectors may be less than @numvecs when

 * active total CPU number is less than @numvecs.

 *

 * Active CPUs means the CPUs in '@cpu_mask AND @node_to_cpumask[]'

 * for each node.

	/*

	 * Allocate vectors for each node according to the ratio of this

	 * node's nr_cpus to remaining un-assigned ncpus. 'numvecs' is

	 * bigger than number of active numa nodes. Always start the

	 * allocation from the node with minimized nr_cpus.

	 *

	 * This way guarantees that each active node gets allocated at

	 * least one vector, and the theory is simple: over-allocation

	 * is only done when this node is assigned by one vector, so

	 * other nodes will be allocated >= 1 vector, since 'numvecs' is

	 * bigger than number of numa nodes.

	 *

	 * One perfect invariant is that number of allocated vectors for

	 * each node is <= CPU count of this node:

	 *

	 * 1) suppose there are two nodes: A and B

	 * 	ncpu(X) is CPU count of node X

	 * 	vecs(X) is the vector count allocated to node X via this

	 * 	algorithm

	 *

	 * 	ncpu(A) <= ncpu(B)

	 * 	ncpu(A) + ncpu(B) = N

	 * 	vecs(A) + vecs(B) = V

	 *

	 * 	vecs(A) = max(1, round_down(V * ncpu(A) / N))

	 * 	vecs(B) = V - vecs(A)

	 *

	 * 	both N and V are integer, and 2 <= V <= N, suppose

	 * 	V = N - delta, and 0 <= delta <= N - 2

	 *

	 * 2) obviously vecs(A) <= ncpu(A) because:

	 *

	 * 	if vecs(A) is 1, then vecs(A) <= ncpu(A) given

	 * 	ncpu(A) >= 1

	 *

	 * 	otherwise,

	 * 		vecs(A) <= V * ncpu(A) / N <= ncpu(A), given V <= N

	 *

	 * 3) prove how vecs(B) <= ncpu(B):

	 *

	 * 	if round_down(V * ncpu(A) / N) == 0, vecs(B) won't be

	 * 	over-allocated, so vecs(B) <= ncpu(B),

	 *

	 * 	otherwise:

	 *

	 * 	vecs(A) =

	 * 		round_down(V * ncpu(A) / N) =

	 * 		round_down((N - delta) * ncpu(A) / N) =

	 * 		round_down((N * ncpu(A) - delta * ncpu(A)) / N)	 >=

	 * 		round_down((N * ncpu(A) - delta * N) / N)	 =

	 * 		cpu(A) - delta

	 *

	 * 	then:

	 *

	 * 	vecs(A) - V >= ncpu(A) - delta - V

	 * 	=>

	 * 	V - vecs(A) <= V + delta - ncpu(A)

	 * 	=>

	 * 	vecs(B) <= N - ncpu(A)

	 * 	=>

	 * 	vecs(B) <= cpu(B)

	 *

	 * For nodes >= 3, it can be thought as one node and another big

	 * node given that is exactly what this algorithm is implemented,

	 * and we always re-calculate 'remaining_ncpus' & 'numvecs', and

	 * finally for each node X: vecs(X) <= ncpu(X).

	 *

	/*

	 * If the number of nodes in the mask is greater than or equal the

	 * number of vectors we just spread the vectors across the nodes.

 allocate vector number for each node */

 Get the cpus on this node which are in the mask */

 Account for rounding errors */

 Spread allocated vectors on CPUs of the current node */

 Account for extra vectors to compensate rounding errors */

			/*

			 * wrapping has to be considered given 'startvec'

			 * may start anywhere

/*

 * build affinity in two stages:

 *	1) spread present CPU on these vectors

 *	2) spread other possible CPUs on these vectors

 Stabilize the cpumasks */

 Spread on present CPUs starting from affd->pre_vectors */

	/*

	 * Spread on non present CPUs starting from the next vector to be

	 * handled. If the spreading of present CPUs already exhausted the

	 * vector space, assign the non present CPUs to the already spread

	 * out vectors.

/**

 * irq_create_affinity_masks - Create affinity masks for multiqueue spreading

 * @nvecs:	The total number of vectors

 * @affd:	Description of the affinity requirements

 *

 * Returns the irq_affinity_desc pointer or NULL if allocation failed.

	/*

	 * Determine the number of vectors which need interrupt affinities

	 * assigned. If the pre/post request exhausts the available vectors

	 * then nothing to do here except for invoking the calc_sets()

	 * callback so the device driver can adjust to the situation.

	/*

	 * Simple invocations do not provide a calc_sets() callback. Install

	 * the generic one.

 Recalculate the sets */

 Nothing to assign? */

 Fill out vectors at the beginning that don't need affinity */

	/*

	 * Spread on present CPUs starting from affd->pre_vectors. If we

	 * have multiple sets, build each sets affinity mask separately.

 Fill out vectors at the end that don't need affinity */

 Mark the managed interrupts */

/**

 * irq_calc_affinity_vectors - Calculate the optimal number of vectors

 * @minvec:	The minimum number of vectors available

 * @maxvec:	The maximum number of vectors available

 * @affd:	Description of the affinity requirements

 SPDX-License-Identifier: GPL-2.0

/*

 * Library implementing the most common irq chip callback functions

 *

 * Copyright (C) 2011, Thomas Gleixner

/**

 * irq_gc_noop - NOOP function

 * @d: irq_data

/**

 * irq_gc_mask_disable_reg - Mask chip via disable register

 * @d: irq_data

 *

 * Chip has separate enable/disable registers instead of a single mask

 * register.

/**

 * irq_gc_mask_set_bit - Mask chip via setting bit in mask register

 * @d: irq_data

 *

 * Chip has a single mask register. Values of this register are cached

 * and protected by gc->lock

/**

 * irq_gc_mask_clr_bit - Mask chip via clearing bit in mask register

 * @d: irq_data

 *

 * Chip has a single mask register. Values of this register are cached

 * and protected by gc->lock

/**

 * irq_gc_unmask_enable_reg - Unmask chip via enable register

 * @d: irq_data

 *

 * Chip has separate enable/disable registers instead of a single mask

 * register.

/**

 * irq_gc_ack_set_bit - Ack pending interrupt via setting bit

 * @d: irq_data

/**

 * irq_gc_ack_clr_bit - Ack pending interrupt via clearing bit

 * @d: irq_data

/**

 * irq_gc_mask_disable_and_ack_set - Mask and ack pending interrupt

 * @d: irq_data

 *

 * This generic implementation of the irq_mask_ack method is for chips

 * with separate enable/disable registers instead of a single mask

 * register and where a pending interrupt is acknowledged by setting a

 * bit.

 *

 * Note: This is the only permutation currently used.  Similar generic

 * functions should be added here if other permutations are required.

/**

 * irq_gc_eoi - EOI interrupt

 * @d: irq_data

/**

 * irq_gc_set_wake - Set/clr wake bit for an interrupt

 * @d:  irq_data

 * @on: Indicates whether the wake bit should be set or cleared

 *

 * For chips where the wake from suspend functionality is not

 * configured in a separate register and the wakeup active state is

 * just stored in a bitmask.

/**

 * irq_alloc_generic_chip - Allocate a generic chip and initialize it

 * @name:	Name of the irq chip

 * @num_ct:	Number of irq_chip_type instances associated with this

 * @irq_base:	Interrupt base nr for this chip

 * @reg_base:	Register base address (virtual)

 * @handler:	Default flow handler associated with this chip

 *

 * Returns an initialized irq_chip_generic structure. The chip defaults

 * to the primary (index 0) irq_chip_type and @handler

/**

 * __irq_alloc_domain_generic_chips - Allocate generic chips for an irq domain

 * @d:			irq domain for which to allocate chips

 * @irqs_per_chip:	Number of interrupts each chip handles (max 32)

 * @num_ct:		Number of irq_chip_type instances associated with this

 * @name:		Name of the irq chip

 * @handler:		Default flow handler associated with these chips

 * @clr:		IRQ_* bits to clear in the mapping function

 * @set:		IRQ_* bits to set in the mapping function

 * @gcflags:		Generic chip specific setup flags

 Allocate a pointer, generic chip and chiptypes for each chip */

 Calc pointer to the first generic chip */

 Store the pointer to the generic chip */

 Calc pointer to the next generic chip */

/**

 * irq_get_domain_generic_chip - Get a pointer to the generic chip of a hw_irq

 * @d:			irq domain pointer

 * @hw_irq:		Hardware interrupt number

/*

 * Separate lockdep classes for interrupt chip which can nest irq_desc

 * lock and request mutex.

/*

 * irq_map_generic_chip - Map a generic chip for an irq domain

 We only init the cache for the first mapping of a generic chip */

 Mark the interrupt as installed */

/**

 * irq_setup_generic_chip - Setup a range of interrupts with a generic chip

 * @gc:		Generic irq chip holding all data

 * @msk:	Bitmask holding the irqs to initialize relative to gc->irq_base

 * @flags:	Flags for initialization

 * @clr:	IRQ_* bits to clear

 * @set:	IRQ_* bits to set

 *

 * Set up max. 32 interrupts starting from gc->irq_base. Note, this

 * initializes all interrupts to the primary irq_chip_type and its

 * associated handler.

/**

 * irq_setup_alt_chip - Switch to alternative chip

 * @d:		irq_data for this interrupt

 * @type:	Flow type to be initialized

 *

 * Only to be called from chip->irq_set_type() callbacks.

/**

 * irq_remove_generic_chip - Remove a chip

 * @gc:		Generic irq chip holding all data

 * @msk:	Bitmask holding the irqs to initialize relative to gc->irq_base

 * @clr:	IRQ_* bits to clear

 * @set:	IRQ_* bits to set

 *

 * Remove up to 32 interrupts starting from gc->irq_base.

 Remove handler first. That will mask the irq line */

	/*

	 * We don't know which of the irqs has been actually

	 * installed. Use the first one.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1992, 1998-2004 Linus Torvalds, Ingo Molnar

 *

 * This file contains the /proc/irq/ handling code.

/*

 * Access rules:

 *

 * procfs protects read/write of /proc/irq/N/ files against a

 * concurrent free of the interrupt descriptor. remove_proc_entry()

 * immediately prevents new read/writes to happen and waits for

 * already running read/write functions to complete.

 *

 * We remove the proc entries first and then delete the interrupt

 * descriptor from the radix tree and free it. So it is guaranteed

 * that irq_to_desc(N) is valid as long as the read/writes are

 * permitted by procfs.

 *

 * The read from /proc/interrupts is a different problem because there

 * is no protection. So the lookup and the access to irqdesc

 * information must be protected by sparse_irq_lock.

	/*

	 * If the interrupt is started up already then this fails. The

	 * interrupt is assigned to an online CPU already. There is no

	 * point to move it around randomly. Tell user space that the

	 * selected mask is bogus.

	 *

	 * If not then any change to the affinity is pointless because the

	 * startup code invokes irq_setup_affinity() which will select

	 * a online CPU anyway.

 ALPHA magic affinity auto selector. Keep it for historical reasons. */

	/*

	 * Do not allow disabling IRQs completely - it's a too easy

	 * way to make the system unusable accidentally :-) At least

	 * one online CPU still has to be targeted.

		/*

		 * Special case for empty set - allow the architecture code

		 * to set default SMP affinity.

	/*

	 * Do not allow disabling IRQs completely - it's a too easy

	 * way to make the system unusable accidentally :-) At least

	 * one online CPU still has to be targeted.

 create /proc/irq/1234/handler/ */

	/*

	 * irq directories are registered only when a handler is

	 * added, not when the descriptor is created, so multiple

	 * tasks might try to register at the same time.

 create /proc/irq/1234 */

 create /proc/irq/<irq>/smp_affinity */

 create /proc/irq/<irq>/affinity_hint */

 create /proc/irq/<irq>/smp_affinity_list */

 create /proc/irq */

	/*

	 * Create entries for all existing IRQs.

 print header and calculate the width of the first column */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1992, 1998-2006 Linus Torvalds, Ingo Molnar

 * Copyright (C) 2005-2006 Thomas Gleixner

 *

 * This file contains driver APIs to the irq subsystem.

		/*

		 * Wait until we're out of the critical section.  This might

		 * give the wrong answer due to the lack of memory barriers.

 Ok, that indicated we're done: double-check carefully. */

		/*

		 * If requested and supported, check at the chip whether it

		 * is in flight at the hardware level, i.e. already pending

		 * in a CPU and waiting for service and acknowledge.

			/*

			 * Ignore the return code. inprogress is only updated

			 * when the chip supports it.

 Oops, that failed? */

/**

 *	synchronize_hardirq - wait for pending hard IRQ handlers (on other CPUs)

 *	@irq: interrupt number to wait for

 *

 *	This function waits for any pending hard IRQ handlers for this

 *	interrupt to complete before returning. If you use this

 *	function while holding a resource the IRQ handler may need you

 *	will deadlock. It does not take associated threaded handlers

 *	into account.

 *

 *	Do not use this for shutdown scenarios where you must be sure

 *	that all parts (hardirq and threaded handler) have completed.

 *

 *	Returns: false if a threaded handler is active.

 *

 *	This function may be called - with care - from IRQ context.

 *

 *	It does not check whether there is an interrupt in flight at the

 *	hardware level, but not serviced yet, as this might deadlock when

 *	called with interrupts disabled and the target CPU of the interrupt

 *	is the current CPU.

/**

 *	synchronize_irq - wait for pending IRQ handlers (on other CPUs)

 *	@irq: interrupt number to wait for

 *

 *	This function waits for any pending IRQ handlers for this interrupt

 *	to complete before returning. If you use this function while

 *	holding a resource the IRQ handler may need you will deadlock.

 *

 *	Can only be called from preemptible code as it might sleep when

 *	an interrupt thread is associated to @irq.

 *

 *	It optionally makes sure (when the irq chip supports that method)

 *	that the interrupt is not pending in any CPU and waiting for

 *	service.

		/*

		 * We made sure that no hardirq handler is

		 * running. Now verify that no threaded handlers are

		 * active.

/**

 *	irq_can_set_affinity - Check if the affinity of a given irq can be set

 *	@irq:		Interrupt to check

 *

/**

 * irq_can_set_affinity_usr - Check if affinity of a irq can be set from user space

 * @irq:	Interrupt to check

 *

 * Like irq_can_set_affinity() above, but additionally checks for the

 * AFFINITY_MANAGED flag.

/**

 *	irq_set_thread_affinity - Notify irq threads to adjust affinity

 *	@desc:		irq descriptor which has affinity changed

 *

 *	We just set IRQTF_AFFINITY and delegate the affinity setting

 *	to the interrupt thread itself. We can not call

 *	set_cpus_allowed_ptr() here as we hold desc->lock and this

 *	code can be called from hard interrupt context.

	/*

	 * If this is a managed interrupt and housekeeping is enabled on

	 * it check whether the requested affinity mask intersects with

	 * a housekeeping CPU. If so, then remove the isolated CPUs from

	 * the mask and just keep the housekeeping CPU(s). This prevents

	 * the affinity setter from routing the interrupt to an isolated

	 * CPU to avoid that I/O submitted from a housekeeping CPU causes

	 * interrupts on an isolated one.

	 *

	 * If the masks do not intersect or include online CPU(s) then

	 * keep the requested mask. The isolated target CPUs are only

	 * receiving interrupts when the I/O operation was submitted

	 * directly from them.

	 *

	 * If all housekeeping CPUs in the affinity mask are offline, the

	 * interrupt will be migrated by the CPU hotplug code once a

	 * housekeeping CPU which belongs to the affinity mask comes

	 * online.

	/*

	 * In case that the underlying vector management is busy and the

	 * architecture supports the generic pending mechanism then utilize

	 * this to avoid returning an error to user space.

	/*

	 * Handle irq chips which can handle affinity only in activated

	 * state correctly

	 *

	 * If the interrupt is not yet activated, just store the affinity

	 * mask and do not call the chip driver at all. On activation the

	 * driver has to make sure anyway that the interrupt is in a

	 * usable state so startup works.

 Work was already scheduled, drop our extra ref */

/**

 * irq_update_affinity_desc - Update affinity management for an interrupt

 * @irq:	The interrupt number to update

 * @affinity:	Pointer to the affinity descriptor

 *

 * This interface can be used to configure the affinity management of

 * interrupts which have been allocated already.

 *

 * There are certain limitations on when it may be used - attempts to use it

 * for when the kernel is configured for generic IRQ reservation mode (in

 * config GENERIC_IRQ_RESERVATION_MODE) will fail, as it may conflict with

 * managed/non-managed interrupt accounting. In addition, attempts to use it on

 * an interrupt which is already started or which has already been configured

 * as managed will also fail, as these mean invalid init state or double init.

	/*

	 * Supporting this with the reservation scheme used by x86 needs

	 * some more thought. Fail it for now.

 Requires the interrupt to be shut down */

 Interrupts which are already managed cannot be modified */

	/*

	 * Deactivate the interrupt. That's required to undo

	 * anything an earlier activation has established.

 Restore the activation state */

/**

 * irq_set_affinity - Set the irq affinity of a given irq

 * @irq:	Interrupt to set affinity

 * @cpumask:	cpumask

 *

 * Fails if cpumask does not contain an online CPU

/**

 * irq_force_affinity - Force the irq affinity of a given irq

 * @irq:	Interrupt to set affinity

 * @cpumask:	cpumask

 *

 * Same as irq_set_affinity, but without checking the mask against

 * online cpus.

 *

 * Solely for low level cpu hotplug code, where we need to make per

 * cpu interrupts affine before the cpu becomes online.

 set the initial affinity to prevent every interrupt being on CPU0 */

/**

 *	irq_set_affinity_notifier - control notification of IRQ affinity changes

 *	@irq:		Interrupt for which to enable/disable notification

 *	@notify:	Context for notification, or %NULL to disable

 *			notification.  Function pointers must be initialised;

 *			the other fields will be initialised by this function.

 *

 *	Must be called in process context.  Notification may only be enabled

 *	after the IRQ is allocated and must be disabled before the IRQ is

 *	freed using free_irq().

 The release function is promised process context */

 Complete initialisation of *notify */

 Pending work had a ref, put that one too */

/*

 * Generic version of the affinity autoselector.

 Excludes PER_CPU and NO_BALANCE interrupts */

	/*

	 * Preserve the managed affinity setting and a userspace affinity

	 * setup, but make sure that one of the targets is online.

 make sure at least one of the cpus in nodemask is online */

 Wrapper for ALPHA specific affinity selector magic */

 CONFIG_AUTO_IRQ_AFFINITY */

 CONFIG_SMP */

/**

 *	irq_set_vcpu_affinity - Set vcpu affinity for the interrupt

 *	@irq: interrupt number to set affinity

 *	@vcpu_info: vCPU specific data or pointer to a percpu array of vCPU

 *	            specific data for percpu_devid interrupts

 *

 *	This function uses the vCPU specific data to set the vCPU

 *	affinity for an irq. The vCPU specific data is passed from

 *	outside, such as KVM. One example code path is as below:

 *	KVM -> IOMMU -> irq_set_vcpu_affinity().

/**

 *	disable_irq_nosync - disable an irq without waiting

 *	@irq: Interrupt to disable

 *

 *	Disable the selected interrupt line.  Disables and Enables are

 *	nested.

 *	Unlike disable_irq(), this function does not ensure existing

 *	instances of the IRQ handler have completed before returning.

 *

 *	This function may be called from IRQ context.

/**

 *	disable_irq - disable an irq and wait for completion

 *	@irq: Interrupt to disable

 *

 *	Disable the selected interrupt line.  Enables and Disables are

 *	nested.

 *	This function waits for any pending IRQ handlers for this interrupt

 *	to complete before returning. If you use this function while

 *	holding a resource the IRQ handler may need you will deadlock.

 *

 *	This function may be called - with care - from IRQ context.

/**

 *	disable_hardirq - disables an irq and waits for hardirq completion

 *	@irq: Interrupt to disable

 *

 *	Disable the selected interrupt line.  Enables and Disables are

 *	nested.

 *	This function waits for any pending hard IRQ handlers for this

 *	interrupt to complete before returning. If you use this function while

 *	holding a resource the hard IRQ handler may need you will deadlock.

 *

 *	When used to optimistically disable an interrupt from atomic context

 *	the return value must be checked.

 *

 *	Returns: false if a threaded handler is active.

 *

 *	This function may be called - with care - from IRQ context.

/**

 *	disable_nmi_nosync - disable an nmi without waiting

 *	@irq: Interrupt to disable

 *

 *	Disable the selected interrupt line. Disables and enables are

 *	nested.

 *	The interrupt to disable must have been requested through request_nmi.

 *	Unlike disable_nmi(), this function does not ensure existing

 *	instances of the IRQ handler have completed before returning.

 Prevent probing on this irq: */

		/*

		 * Call irq_startup() not irq_enable() here because the

		 * interrupt might be marked NOAUTOEN. So irq_startup()

		 * needs to be invoked when it gets enabled the first

		 * time. If it was already started up, then irq_startup()

		 * will invoke irq_enable() under the hood.

/**

 *	enable_irq - enable handling of an irq

 *	@irq: Interrupt to enable

 *

 *	Undoes the effect of one call to disable_irq().  If this

 *	matches the last disable, processing of interrupts on this

 *	IRQ line is re-enabled.

 *

 *	This function may be called from IRQ context only when

 *	desc->irq_data.chip->bus_lock and desc->chip->bus_sync_unlock are NULL !

/**

 *	enable_nmi - enable handling of an nmi

 *	@irq: Interrupt to enable

 *

 *	The interrupt to enable must have been requested through request_nmi.

 *	Undoes the effect of one call to disable_nmi(). If this

 *	matches the last disable, processing of interrupts on this

 *	IRQ line is re-enabled.

/**

 *	irq_set_irq_wake - control irq power management wakeup

 *	@irq:	interrupt to control

 *	@on:	enable/disable power management wakeup

 *

 *	Enable/disable power management wakeup mode, which is

 *	disabled by default.  Enables and disables must match,

 *	just as they match for non-wakeup mode support.

 *

 *	Wakeup mode lets this IRQ wake the system from sleep

 *	states like "suspend to RAM".

 *

 *	Note: irq enable/disable state is completely orthogonal

 *	to the enable/disable state of irq wake. An irq can be

 *	disabled with disable_irq() and still wake the system as

 *	long as the irq has wake enabled. If this does not hold,

 *	then the underlying irq chip and the related driver need

 *	to be investigated.

 Don't use NMIs as wake up interrupts please */

	/* wakeup-capable irqs can be shared between drivers that

	 * don't need to have the same sleep mode behaviors.

/*

 * Internal function that tells the architecture code whether a

 * particular irq has been exclusively allocated or is available

 * for driver use.

		/*

		 * IRQF_TRIGGER_* but the PIC does not support multiple

		 * flow-types?

 Mask all flags except trigger mode */

/*

 * Default primary interrupt handler for threaded interrupts. Is

 * assigned as primary handler when request_threaded_irq is called

 * with handler == NULL. Useful for oneshot interrupts.

/*

 * Primary handler for nested threaded interrupts. Should never be

 * called.

 may need to run one last time */

/*

 * Oneshot interrupts keep the irq line masked until the threaded

 * handler finished. unmask if the interrupt has not been disabled and

 * is marked MASKED.

	/*

	 * Implausible though it may be we need to protect us against

	 * the following scenario:

	 *

	 * The thread is faster done than the hard interrupt handler

	 * on the other CPU. If we unmask the irq line then the

	 * interrupt can come in again and masks the line, leaves due

	 * to IRQS_INPROGRESS and the irq line is masked forever.

	 *

	 * This also serializes the state of shared oneshot handlers

	 * versus "desc->threads_oneshot |= action->thread_mask;" in

	 * irq_wake_thread(). See the comment there which explains the

	 * serialization.

	/*

	 * Now check again, whether the thread should run. Otherwise

	 * we would clear the threads_oneshot bit of this thread which

	 * was just set.

/*

 * Check whether we need to change the affinity of the interrupt thread.

	/*

	 * In case we are out of memory we set IRQTF_AFFINITY again and

	 * try again next time

	/*

	 * This code is triggered unconditionally. Check the affinity

	 * mask pointer. For CPU_MASK_OFFSTACK=n this is optimized out.

/*

 * Interrupts which are not explicitly requested as threaded

 * interrupts rely on the implicit bh/preempt disable of the hard irq

 * context. So we need to disable bh here to avoid deadlocks and other

 * side effects.

/*

 * Interrupts explicitly requested as threaded interrupts want to be

 * preemptible - many of them need to sleep and wait for slow busses to

 * complete.

	/*

	 * If IRQTF_RUNTHREAD is set, we need to decrement

	 * desc->threads_active and wake possible waiters.

 Prevent a stale desc->threads_oneshot */

/*

 * Interrupt handler thread

	/*

	 * This is the regular exit path. __free_irq() is stopping the

	 * thread via kthread_stop() after calling

	 * synchronize_hardirq(). So neither IRQTF_RUNTHREAD nor the

	 * oneshot mask bit can be set.

/**

 *	irq_wake_thread - wake the irq thread for the action identified by dev_id

 *	@irq:		Interrupt line

 *	@dev_id:	Device identity for which the thread should be woken

 *

	/*

	 * No further action required for interrupts which are requested as

	 * threaded interrupts already

	/*

	 * Handle the case where we have a real primary handler and a

	 * thread handler. We force thread them as well by creating a

	 * secondary action.

 Allocate the secondary action */

 Deal with the primary handler */

 Only IRQs directly managed by the root irqchip can be set as NMI */

 Don't support NMIs for chips behind a slow bus */

	/*

	 * We keep the reference to the task struct even if

	 * the thread dies to avoid that the interrupt code

	 * references an already freed task_struct.

	/*

	 * Tell the thread to set its affinity. This is

	 * important for shared interrupt handlers as we do

	 * not invoke setup_affinity() for the secondary

	 * handlers as everything is already set up. Even for

	 * interrupts marked with IRQF_NO_BALANCE this is

	 * correct as we want the thread to move to the cpu(s)

	 * on which the requesting code placed the interrupt.

/*

 * Internal function to register an irqaction - typically used to

 * allocate special interrupts that are part of the architecture.

 *

 * Locking rules:

 *

 * desc->request_mutex	Provides serialization against a concurrent free_irq()

 *   chip_bus_lock	Provides serialization for slow bus operations

 *     desc->lock	Provides serialization against hard interrupts

 *

 * chip_bus_lock and desc->lock are sufficient for all other management and

 * interrupt related functions. desc->request_mutex solely serializes

 * request/free_irq().

	/*

	 * If the trigger type is not specified by the caller,

	 * then use the default for this interrupt.

	/*

	 * Check whether the interrupt nests into another interrupt

	 * thread.

		/*

		 * Replace the primary handler which was provided from

		 * the driver for non nested interrupt handling by the

		 * dummy function which warns when called.

	/*

	 * Create a handler thread when a thread function is supplied

	 * and the interrupt does not nest into another interrupt

	 * thread.

	/*

	 * Drivers are often written to work w/o knowledge about the

	 * underlying irq chip implementation, so a request for a

	 * threaded irq without a primary hard irq context handler

	 * requires the ONESHOT flag to be set. Some irq chips like

	 * MSI based interrupts are per se one shot safe. Check the

	 * chip flags, so we can avoid the unmask dance at the end of

	 * the threaded handler for those.

	/*

	 * Protects against a concurrent __free_irq() call which might wait

	 * for synchronize_hardirq() to complete without holding the optional

	 * chip bus lock and desc->lock. Also protects against handing out

	 * a recycled oneshot thread_mask bit while it's still in use by

	 * its previous owner.

	/*

	 * Acquire bus lock as the irq_request_resources() callback below

	 * might rely on the serialization or the magic power management

	 * functions which are abusing the irq_bus_lock() callback,

 First installed action requests resources. */

	/*

	 * The following block of code has to be executed atomically

	 * protected against a concurrent interrupt and any of the other

	 * management calls which are not serialized via

	 * desc->request_mutex or the optional bus lock.

		/*

		 * Can't share interrupts unless both agree to and are

		 * the same type (level, edge, polarity). So both flag

		 * fields must have IRQF_SHARED set and the bits which

		 * set the trigger type must match. Also all must

		 * agree on ONESHOT.

		 * Interrupt lines used for NMIs cannot be shared.

		/*

		 * If nobody did set the configuration before, inherit

		 * the one provided by the requester.

 All handlers must agree on per-cpuness */

 add new interrupt at end of irq queue */

			/*

			 * Or all existing action->thread_mask bits,

			 * so we can find the next zero bit for this

			 * new action.

	/*

	 * Setup the thread mask for this irqaction for ONESHOT. For

	 * !ONESHOT irqs the thread mask is 0 so we can avoid a

	 * conditional in irq_wake_thread().

		/*

		 * Unlikely to have 32 resp 64 irqs sharing one line,

		 * but who knows.

		/*

		 * The thread_mask for the action is or'ed to

		 * desc->thread_active to indicate that the

		 * IRQF_ONESHOT thread handler has been woken, but not

		 * yet finished. The bit is cleared when a thread

		 * completes. When all threads of a shared interrupt

		 * line have completed desc->threads_active becomes

		 * zero and the interrupt line is unmasked. See

		 * handle.c:irq_wake_thread() for further information.

		 *

		 * If no thread is woken by primary (hard irq context)

		 * interrupt handlers, then desc->threads_active is

		 * also checked for zero to unmask the irq line in the

		 * affected hard irq flow handlers

		 * (handle_[fasteoi|level]_irq).

		 *

		 * The new action gets the first zero bit of

		 * thread_mask assigned. See the loop above which or's

		 * all existing action->thread_mask bits.

		/*

		 * The interrupt was requested with handler = NULL, so

		 * we use the default primary handler for it. But it

		 * does not have the oneshot flag set. In combination

		 * with level interrupts this is deadly, because the

		 * default primary handler just wakes the thread, then

		 * the irq lines is reenabled, but the device still

		 * has the level irq asserted. Rinse and repeat....

		 *

		 * While this works for edge type interrupts, we play

		 * it safe and reject unconditionally because we can't

		 * say for sure which type this interrupt really

		 * has. The type flags are unreliable as the

		 * underlying chip implementation can override them.

 Setup the type (level, edge polarity) if configured: */

		/*

		 * Activate the interrupt. That activation must happen

		 * independently of IRQ_NOAUTOEN. request_irq() can fail

		 * and the callers are supposed to handle

		 * that. enable_irq() of an interrupt requested with

		 * IRQ_NOAUTOEN is not supposed to fail. The activation

		 * keeps it in shutdown mode, it merily associates

		 * resources if necessary and if that's not possible it

		 * fails. Interrupts which are in managed shutdown mode

		 * will simply ignore that activation request.

 Exclude IRQ from balancing if requested */

			/*

			 * Shared interrupts do not go well with disabling

			 * auto enable. The sharing interrupt might request

			 * it while it's still disabled and then wait for

			 * interrupts forever.

 Undo nested disables: */

 hope the handler works with current  trigger mode */

 Reset broken irq detection when installing new handler */

	/*

	 * Check whether we disabled the irq via the spurious handler

	 * before. Reenable it and give it another chance.

	/*

	 * Strictly no need to wake it up, but hung_task complains

	 * when no hard interrupt wakes the thread up.

/*

 * Internal function to unregister an irqaction - used to free

 * regular and special interrupts that are part of the architecture.

	/*

	 * There can be multiple actions per IRQ descriptor, find the right

	 * one based on the dev_id:

 Found it - now remove it from the list of entries: */

 If this was the last handler, shut down the IRQ line: */

 Only shutdown. Deactivate after synchronize_hardirq() */

 make sure affinity_hint is cleaned up */

	/*

	 * Drop bus_lock here so the changes which were done in the chip

	 * callbacks above are synced out to the irq chips which hang

	 * behind a slow bus (I2C, SPI) before calling synchronize_hardirq().

	 *

	 * Aside of that the bus_lock can also be taken from the threaded

	 * handler in irq_finalize_oneshot() which results in a deadlock

	 * because kthread_stop() would wait forever for the thread to

	 * complete, which is blocked on the bus lock.

	 *

	 * The still held desc->request_mutex() protects against a

	 * concurrent request_irq() of this irq so the release of resources

	 * and timing data is properly serialized.

	/*

	 * Make sure it's not being used on another CPU and if the chip

	 * supports it also make sure that there is no (not yet serviced)

	 * interrupt in flight at the hardware level.

	/*

	 * It's a shared IRQ -- the driver ought to be prepared for an IRQ

	 * event to happen even now it's being freed, so let's make sure that

	 * is so by doing an extra call to the handler ....

	 *

	 * ( We do this after actually deregistering it, to make sure that a

	 *   'real' IRQ doesn't run in parallel with our fake. )

	/*

	 * The action has already been removed above, but the thread writes

	 * its oneshot mask bit when it completes. Though request_mutex is

	 * held across this which prevents __setup_irq() from handing out

	 * the same bit to a newly requested action.

 Last action releases resources */

		/*

		 * Reacquire bus lock as irq_release_resources() might

		 * require it to deallocate resources over the slow bus.

		/*

		 * There is no interrupt on the fly anymore. Deactivate it

		 * completely.

/**

 *	free_irq - free an interrupt allocated with request_irq

 *	@irq: Interrupt line to free

 *	@dev_id: Device identity to free

 *

 *	Remove an interrupt handler. The handler is removed and if the

 *	interrupt line is no longer in use by any driver it is disabled.

 *	On a shared IRQ the caller must ensure the interrupt is disabled

 *	on the card it drives before calling this function. The function

 *	does not return until any executing interrupts for this IRQ

 *	have completed.

 *

 *	This function must not be called from interrupt context.

 *

 *	Returns the devname argument passed to request_irq.

 This function must be called with desc->lock held */

 NMI still enabled */

/**

 *	request_threaded_irq - allocate an interrupt line

 *	@irq: Interrupt line to allocate

 *	@handler: Function to be called when the IRQ occurs.

 *		  Primary handler for threaded interrupts.

 *		  If handler is NULL and thread_fn != NULL

 *		  the default primary handler is installed.

 *	@thread_fn: Function called from the irq handler thread

 *		    If NULL, no irq thread is created

 *	@irqflags: Interrupt type flags

 *	@devname: An ascii name for the claiming device

 *	@dev_id: A cookie passed back to the handler function

 *

 *	This call allocates interrupt resources and enables the

 *	interrupt line and IRQ handling. From the point this

 *	call is made your handler function may be invoked. Since

 *	your handler function must clear any interrupt the board

 *	raises, you must take care both to initialise your hardware

 *	and to set up the interrupt handler in the right order.

 *

 *	If you want to set up a threaded irq handler for your device

 *	then you need to supply @handler and @thread_fn. @handler is

 *	still called in hard interrupt context and has to check

 *	whether the interrupt originates from the device. If yes it

 *	needs to disable the interrupt on the device and return

 *	IRQ_WAKE_THREAD which will wake up the handler thread and run

 *	@thread_fn. This split handler design is necessary to support

 *	shared interrupts.

 *

 *	Dev_id must be globally unique. Normally the address of the

 *	device data structure is used as the cookie. Since the handler

 *	receives this value it makes sense to use it.

 *

 *	If your interrupt is shared you must pass a non NULL dev_id

 *	as this is required when freeing the interrupt.

 *

 *	Flags:

 *

 *	IRQF_SHARED		Interrupt is shared

 *	IRQF_TRIGGER_*		Specify active edge(s) or level

 *	IRQF_ONESHOT		Run thread_fn with interrupt line masked

	/*

	 * Sanity-check: shared interrupts must pass in a real dev-ID,

	 * otherwise we'll have trouble later trying to figure out

	 * which interrupt is which (messes up the interrupt freeing

	 * logic etc).

	 *

	 * Also shared interrupts do not go well with disabling auto enable.

	 * The sharing interrupt might request it while it's still disabled

	 * and then wait for interrupts forever.

	 *

	 * Also IRQF_COND_SUSPEND only makes sense for shared interrupts and

	 * it cannot be set along with IRQF_NO_SUSPEND.

		/*

		 * It's a shared IRQ -- the driver ought to be prepared for it

		 * to happen immediately, so let's make sure....

		 * We disable the irq to make sure that a 'real' IRQ doesn't

		 * run in parallel with our fake.

/**

 *	request_any_context_irq - allocate an interrupt line

 *	@irq: Interrupt line to allocate

 *	@handler: Function to be called when the IRQ occurs.

 *		  Threaded handler for threaded interrupts.

 *	@flags: Interrupt type flags

 *	@name: An ascii name for the claiming device

 *	@dev_id: A cookie passed back to the handler function

 *

 *	This call allocates interrupt resources and enables the

 *	interrupt line and IRQ handling. It selects either a

 *	hardirq or threaded handling method depending on the

 *	context.

 *

 *	On failure, it returns a negative value. On success,

 *	it returns either IRQC_IS_HARDIRQ or IRQC_IS_NESTED.

/**

 *	request_nmi - allocate an interrupt line for NMI delivery

 *	@irq: Interrupt line to allocate

 *	@handler: Function to be called when the IRQ occurs.

 *		  Threaded handler for threaded interrupts.

 *	@irqflags: Interrupt type flags

 *	@name: An ascii name for the claiming device

 *	@dev_id: A cookie passed back to the handler function

 *

 *	This call allocates interrupt resources and enables the

 *	interrupt line and IRQ handling. It sets up the IRQ line

 *	to be handled as an NMI.

 *

 *	An interrupt line delivering NMIs cannot be shared and IRQ handling

 *	cannot be threaded.

 *

 *	Interrupt lines requested for NMI delivering must produce per cpu

 *	interrupts and have auto enabling setting disabled.

 *

 *	Dev_id must be globally unique. Normally the address of the

 *	device data structure is used as the cookie. Since the handler

 *	receives this value it makes sense to use it.

 *

 *	If the interrupt line cannot be used to deliver NMIs, function

 *	will fail and return a negative value.

 NMI cannot be shared, used for Polling */

 Setup NMI state */

	/*

	 * If the trigger type is not specified by the caller, then

	 * use the default for this interrupt.

/**

 * irq_percpu_is_enabled - Check whether the per cpu irq is enabled

 * @irq:	Linux irq number to check for

 *

 * Must be called from a non migratable context. Returns the enable

 * state of a per cpu interrupt on the current cpu.

/*

 * Internal function to unregister a percpu irqaction.

 Found it - now remove it from the list of entries: */

/**

 *	remove_percpu_irq - free a per-cpu interrupt

 *	@irq: Interrupt line to free

 *	@act: irqaction for the interrupt

 *

 * Used to remove interrupts statically setup by the early boot process.

/**

 *	free_percpu_irq - free an interrupt allocated with request_percpu_irq

 *	@irq: Interrupt line to free

 *	@dev_id: Device identity to free

 *

 *	Remove a percpu interrupt handler. The handler is removed, but

 *	the interrupt line is not disabled. This must be done on each

 *	CPU before calling this function. The function does not return

 *	until any executing interrupts for this IRQ have completed.

 *

 *	This function must not be called from interrupt context.

/**

 *	setup_percpu_irq - setup a per-cpu interrupt

 *	@irq: Interrupt line to setup

 *	@act: irqaction for the interrupt

 *

 * Used to statically setup per-cpu interrupts in the early boot process.

/**

 *	__request_percpu_irq - allocate a percpu interrupt line

 *	@irq: Interrupt line to allocate

 *	@handler: Function to be called when the IRQ occurs.

 *	@flags: Interrupt type flags (IRQF_TIMER only)

 *	@devname: An ascii name for the claiming device

 *	@dev_id: A percpu cookie passed back to the handler function

 *

 *	This call allocates interrupt resources and enables the

 *	interrupt on the local CPU. If the interrupt is supposed to be

 *	enabled on other CPUs, it has to be done on each CPU using

 *	enable_percpu_irq().

 *

 *	Dev_id must be globally unique. It is a per-cpu variable, and

 *	the handler gets called with the interrupted CPU's instance of

 *	that variable.

/**

 *	request_percpu_nmi - allocate a percpu interrupt line for NMI delivery

 *	@irq: Interrupt line to allocate

 *	@handler: Function to be called when the IRQ occurs.

 *	@name: An ascii name for the claiming device

 *	@dev_id: A percpu cookie passed back to the handler function

 *

 *	This call allocates interrupt resources for a per CPU NMI. Per CPU NMIs

 *	have to be setup on each CPU by calling prepare_percpu_nmi() before

 *	being enabled on the same CPU by using enable_percpu_nmi().

 *

 *	Dev_id must be globally unique. It is a per-cpu variable, and

 *	the handler gets called with the interrupted CPU's instance of

 *	that variable.

 *

 *	Interrupt lines requested for NMI delivering should have auto enabling

 *	setting disabled.

 *

 *	If the interrupt line cannot be used to deliver NMIs, function

 *	will fail returning a negative value.

 The line cannot already be NMI */

/**

 *	prepare_percpu_nmi - performs CPU local setup for NMI delivery

 *	@irq: Interrupt line to prepare for NMI delivery

 *

 *	This call prepares an interrupt line to deliver NMI on the current CPU,

 *	before that interrupt line gets enabled with enable_percpu_nmi().

 *

 *	As a CPU local operation, this should be called from non-preemptible

 *	context.

 *

 *	If the interrupt line cannot be used to deliver NMIs, function

 *	will fail returning a negative value.

/**

 *	teardown_percpu_nmi - undoes NMI setup of IRQ line

 *	@irq: Interrupt line from which CPU local NMI configuration should be

 *	      removed

 *

 *	This call undoes the setup done by prepare_percpu_nmi().

 *

 *	IRQ line should not be enabled for the current CPU.

 *

 *	As a CPU local operation, this should be called from non-preemptible

 *	context.

/**

 *	irq_get_irqchip_state - returns the irqchip state of a interrupt.

 *	@irq: Interrupt line that is forwarded to a VM

 *	@which: One of IRQCHIP_STATE_* the caller wants to know about

 *	@state: a pointer to a boolean where the state is to be stored

 *

 *	This call snapshots the internal irqchip state of an

 *	interrupt, returning into @state the bit corresponding to

 *	stage @which

 *

 *	This function should be called with preemption disabled if the

 *	interrupt controller has per-cpu registers.

/**

 *	irq_set_irqchip_state - set the state of a forwarded interrupt.

 *	@irq: Interrupt line that is forwarded to a VM

 *	@which: State to be restored (one of IRQCHIP_STATE_*)

 *	@val: Value corresponding to @which

 *

 *	This call sets the internal irqchip state of an interrupt,

 *	depending on the value of @which.

 *

 *	This function should be called with migration disabled if the

 *	interrupt controller has per-cpu registers.

/**

 * irq_has_action - Check whether an interrupt is requested

 * @irq:	The linux irq number

 *

 * Returns: A snapshot of the current state

/**

 * irq_check_status_bit - Check whether bits in the irq descriptor status are set

 * @irq:	The linux irq number

 * @bitmask:	The bitmask to evaluate

 *

 * Returns: True if one of the bits in @bitmask is set

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1992, 1998-2006 Linus Torvalds, Ingo Molnar

 * Copyright (C) 2005-2006, Thomas Gleixner, Russell King

 *

 * This file contains the dummy interrupt chip implementation

/*

 * What should we do if we get a hw irq event on an illegal vector?

 * Each architecture has to answer this themselves.

/*

 * NOP functions

/*

 * Generic no controller implementation

/*

 * Generic dummy implementation which can be used for

 * real dumb interrupt sources

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1992, 1998-2004 Linus Torvalds, Ingo Molnar

 *

 * This file contains the interrupt probing code and driver APIs.

/*

 * Autodetection depends on the fact that any interrupt that

 * comes in on to an unassigned handler will get stuck with

 * "IRQS_WAITING" cleared and the interrupt disabled.

/**

 *	probe_irq_on	- begin an interrupt autodetect

 *

 *	Commence probing for an interrupt. The interrupts are scanned

 *	and a mask of potential interrupt lines is returned.

 *

	/*

	 * quiesce the kernel, or at least the asynchronous portion

	/*

	 * something may have generated an irq long ago and we want to

	 * flush such a longstanding irq before considering it as spurious.

			/*

			 * Some chips need to know about probing in

			 * progress:

 Wait for longstanding interrupts to trigger. */

	/*

	 * enable any unassigned irqs

	 * (we must startup again here because if a longstanding irq

	 * happened in the previous stage, it may have masked itself)

	/*

	 * Wait for spurious interrupts to trigger

	/*

	 * Now filter out any obviously spurious interrupts

 It triggered already - consider it spurious. */

/**

 *	probe_irq_mask - scan a bitmap of interrupt lines

 *	@val:	mask of interrupts to consider

 *

 *	Scan the interrupt lines and return a bitmap of active

 *	autodetect interrupts. The interrupt probe logic state

 *	is then returned to its previous value.

 *

 *	Note: we need to scan all the irq's even though we will

 *	only return autodetect irq numbers - just so that we reset

 *	them all to a known state.

/**

 *	probe_irq_off	- end an interrupt autodetect

 *	@val: mask of potential interrupts (unused)

 *

 *	Scans the unused interrupt lines and returns the line which

 *	appears to have triggered the interrupt. If no interrupt was

 *	found then zero is returned. If more than one interrupt is

 *	found then minus the first candidate is returned to indicate

 *	their is doubt.

 *

 *	The interrupt probe logic state is returned to its previous

 *	value.

 *

 *	BUGS: When used in a module (which arguably shouldn't happen)

 *	nothing prevents two IRQ probe callers from overlapping. The

 *	results of this are non-optimal.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1992, 1998-2006 Linus Torvalds, Ingo Molnar

 * Copyright (C) 2005-2006, Thomas Gleixner

 *

 * This file contains the IRQ-resend code

 *

 * If the interrupt is waiting to be processed, we try to re-run it.

 * We can't directly run it from here since the caller might be in an

 * interrupt-protected region. Not all irq controller chips can

 * retrigger interrupts at the hardware level, so in those cases

 * we allow the resending of IRQs via a tasklet.

 Bitmap to handle software resend of interrupts: */

/*

 * Run software resends of IRQ's

 Tasklet to handle resend: */

	/*

	 * Validate whether this interrupt can be safely injected from

	 * non interrupt context

	/*

	 * If the interrupt is running in the thread context of the parent

	 * irq we need to be careful, because we cannot trigger it

	 * directly.

		/*

		 * If the parent_irq is valid, we retrigger the parent,

		 * otherwise we do nothing.

 Set it pending and activate the softirq: */

/*

 * IRQ resend

 *

 * Is called with interrupts disabled and desc->lock held.

	/*

	 * We do not resend level type interrupts. Level type interrupts

	 * are resent by hardware when they are still active. Clear the

	 * pending bit so suspend/resume does not get confused.

 If the retrigger was successful, mark it with the REPLAY bit */

/**

 * irq_inject_interrupt - Inject an interrupt for testing/error injection

 * @irq:	The interrupt number

 *

 * This function must only be used for debug and testing purposes!

 *

 * Especially on x86 this can cause a premature completion of an interrupt

 * affinity change causing the interrupt line to become stale. Very

 * unlikely, but possible.

 *

 * The injection can fail for various reasons:

 * - Interrupt is not activated

 * - Interrupt is NMI type or currently replaying

 * - Interrupt is level type

 * - Interrupt does not support hardware retrigger and software resend is

 *   either not enabled or not possible for the interrupt.

 Try the state injection hardware interface first */

 That failed, try via the resend mechanism */

	/*

	 * Only try to inject when the interrupt is:

	 *  - not NMI type

	 *  - activated

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2017-2018 Bartosz Golaszewski <brgl@bgdev.pl>

 * Copyright (C) 2020 Bartosz Golaszewski <bgolaszewski@baylibre.com>

 We only support rising and falling edge trigger types. */

/**

 * irq_domain_create_sim - Create a new interrupt simulator irq_domain and

 *                         allocate a range of dummy interrupts.

 *

 * @fwnode:     struct fwnode_handle to be associated with this domain.

 * @num_irqs:   Number of interrupts to allocate.

 *

 * On success: return a new irq_domain object.

 * On failure: a negative errno wrapped with ERR_PTR().

/**

 * irq_domain_remove_sim - Deinitialize the interrupt simulator domain: free

 *                         the interrupt descriptors and allocated memory.

 *

 * @domain:     The interrupt simulator domain to tear down.

/**

 * devm_irq_domain_create_sim - Create a new interrupt simulator for

 *                              a managed device.

 *

 * @dev:        Device to initialize the simulator object for.

 * @fwnode:     struct fwnode_handle to be associated with this domain.

 * @num_irqs:   Number of interrupts to allocate

 *

 * On success: return a new irq_domain object.

 * On failure: a negative errno wrapped with ERR_PTR().

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1992, 1998-2006 Linus Torvalds, Ingo Molnar

 * Copyright (C) 2005-2006, Thomas Gleixner, Russell King

 *

 * This file contains the core interrupt handling code. Detailed

 * information is available in Documentation/core-api/genericirq.rst

 *

/**

 * handle_bad_irq - handle spurious and unhandled irqs

 * @desc:      description of the interrupt

 *

 * Handles spurious and unhandled IRQ's. It also prints a debugmessage.

/*

 * Special, empty irq handler:

	/*

	 * In case the thread crashed and was killed we just pretend that

	 * we handled the interrupt. The hardirq handler has disabled the

	 * device interrupt, so no irq storm is lurking.

	/*

	 * Wake up the handler thread for this action. If the

	 * RUNTHREAD bit is already set, nothing to do.

	/*

	 * It's safe to OR the mask lockless here. We have only two

	 * places which write to threads_oneshot: This code and the

	 * irq thread.

	 *

	 * This code is the hard irq context and can never run on two

	 * cpus in parallel. If it ever does we have more serious

	 * problems than this bitmask.

	 *

	 * The irq threads of this irq which clear their "running" bit

	 * in threads_oneshot are serialized via desc->lock against

	 * each other and they are serialized against this code by

	 * IRQS_INPROGRESS.

	 *

	 * Hard irq handler:

	 *

	 *	spin_lock(desc->lock);

	 *	desc->state |= IRQS_INPROGRESS;

	 *	spin_unlock(desc->lock);

	 *	set_bit(IRQTF_RUNTHREAD, &action->thread_flags);

	 *	desc->threads_oneshot |= mask;

	 *	spin_lock(desc->lock);

	 *	desc->state &= ~IRQS_INPROGRESS;

	 *	spin_unlock(desc->lock);

	 *

	 * irq thread:

	 *

	 * again:

	 *	spin_lock(desc->lock);

	 *	if (desc->state & IRQS_INPROGRESS) {

	 *		spin_unlock(desc->lock);

	 *		while(desc->state & IRQS_INPROGRESS)

	 *			cpu_relax();

	 *		goto again;

	 *	}

	 *	if (!test_bit(IRQTF_RUNTHREAD, &action->thread_flags))

	 *		desc->threads_oneshot &= ~mask;

	 *	spin_unlock(desc->lock);

	 *

	 * So either the thread waits for us to clear IRQS_INPROGRESS

	 * or we are waiting in the flow handler for desc->lock to be

	 * released before we reach this point. The thread also checks

	 * IRQTF_RUNTHREAD under desc->lock. If set it leaves

	 * threads_oneshot untouched and runs the thread another time.

	/*

	 * We increment the threads_active counter in case we wake up

	 * the irq thread. The irq thread decrements the counter when

	 * it returns from the handler or in the exit path and wakes

	 * up waiters which are stuck in synchronize_irq() when the

	 * active count becomes zero. synchronize_irq() is serialized

	 * against this code (hard irq handler) via IRQS_INPROGRESS

	 * like the finalize_oneshot() code. See comment above.

		/*

		 * If this IRQ would be threaded under force_irqthreads, mark it so.

			/*

			 * Catch drivers which return WAKE_THREAD but

			 * did not set up a thread function

 to add to randomness */

/**

 * generic_handle_arch_irq - root irq handler for architectures which do no

 *                           entry accounting themselves

 * @regs:	Register file coming from the low-level handling code

 SPDX-License-Identifier: GPL-2.0

/*

 * Device resource management aware IRQ request/free implementation.

/**

 *	devm_request_threaded_irq - allocate an interrupt line for a managed device

 *	@dev: device to request interrupt for

 *	@irq: Interrupt line to allocate

 *	@handler: Function to be called when the IRQ occurs

 *	@thread_fn: function to be called in a threaded interrupt context. NULL

 *		    for devices which handle everything in @handler

 *	@irqflags: Interrupt type flags

 *	@devname: An ascii name for the claiming device, dev_name(dev) if NULL

 *	@dev_id: A cookie passed back to the handler function

 *

 *	Except for the extra @dev argument, this function takes the

 *	same arguments and performs the same function as

 *	request_threaded_irq().  IRQs requested with this function will be

 *	automatically freed on driver detach.

 *

 *	If an IRQ allocated with this function needs to be freed

 *	separately, devm_free_irq() must be used.

/**

 *	devm_request_any_context_irq - allocate an interrupt line for a managed device

 *	@dev: device to request interrupt for

 *	@irq: Interrupt line to allocate

 *	@handler: Function to be called when the IRQ occurs

 *	@irqflags: Interrupt type flags

 *	@devname: An ascii name for the claiming device, dev_name(dev) if NULL

 *	@dev_id: A cookie passed back to the handler function

 *

 *	Except for the extra @dev argument, this function takes the

 *	same arguments and performs the same function as

 *	request_any_context_irq().  IRQs requested with this function will be

 *	automatically freed on driver detach.

 *

 *	If an IRQ allocated with this function needs to be freed

 *	separately, devm_free_irq() must be used.

/**

 *	devm_free_irq - free an interrupt

 *	@dev: device to free interrupt for

 *	@irq: Interrupt line to free

 *	@dev_id: Device identity to free

 *

 *	Except for the extra @dev argument, this function takes the

 *	same arguments and performs the same function as free_irq().

 *	This function instead of free_irq() should be used to manually

 *	free IRQs allocated with devm_request_irq().

/**

 * __devm_irq_alloc_descs - Allocate and initialize a range of irq descriptors

 *			    for a managed device

 * @dev:	Device to allocate the descriptors for

 * @irq:	Allocate for specific irq number if irq >= 0

 * @from:	Start the search from this irq number

 * @cnt:	Number of consecutive irqs to allocate

 * @node:	Preferred node on which the irq descriptor should be allocated

 * @owner:	Owning module (can be NULL)

 * @affinity:	Optional pointer to an irq_affinity_desc array of size @cnt

 *		which hints where the irq descriptors should be allocated

 *		and which default affinities to use

 *

 * Returns the first irq number or error code.

 *

 * Note: Use the provided wrappers (devm_irq_alloc_desc*) for simplicity.

/**

 * devm_irq_alloc_generic_chip - Allocate and initialize a generic chip

 *                               for a managed device

 * @dev:	Device to allocate the generic chip for

 * @name:	Name of the irq chip

 * @num_ct:	Number of irq_chip_type instances associated with this

 * @irq_base:	Interrupt base nr for this chip

 * @reg_base:	Register base address (virtual)

 * @handler:	Default flow handler associated with this chip

 *

 * Returns an initialized irq_chip_generic structure. The chip defaults

 * to the primary (index 0) irq_chip_type and @handler

/**

 * devm_irq_setup_generic_chip - Setup a range of interrupts with a generic

 *                               chip for a managed device

 *

 * @dev:	Device to setup the generic chip for

 * @gc:		Generic irq chip holding all data

 * @msk:	Bitmask holding the irqs to initialize relative to gc->irq_base

 * @flags:	Flags for initialization

 * @clr:	IRQ_* bits to clear

 * @set:	IRQ_* bits to set

 *

 * Set up max. 32 interrupts starting from gc->irq_base. Note, this

 * initializes all interrupts to the primary irq_chip_type and its

 * associated handler.

 CONFIG_GENERIC_IRQ_CHIP */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2016, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>

/*

 * The main goal of this algorithm is to predict the next interrupt

 * occurrence on the current CPU.

 *

 * Currently, the interrupt timings are stored in a circular array

 * buffer every time there is an interrupt, as a tuple: the interrupt

 * number and the associated timestamp when the event occurred <irq,

 * timestamp>.

 *

 * For every interrupt occurring in a short period of time, we can

 * measure the elapsed time between the occurrences for the same

 * interrupt and we end up with a suite of intervals. The experience

 * showed the interrupts are often coming following a periodic

 * pattern.

 *

 * The objective of the algorithm is to find out this periodic pattern

 * in a fastest way and use its period to predict the next irq event.

 *

 * When the next interrupt event is requested, we are in the situation

 * where the interrupts are disabled and the circular buffer

 * containing the timings is filled with the events which happened

 * after the previous next-interrupt-event request.

 *

 * At this point, we read the circular buffer and we fill the irq

 * related statistics structure. After this step, the circular array

 * containing the timings is empty because all the values are

 * dispatched in their corresponding buffers.

 *

 * Now for each interrupt, we can predict the next event by using the

 * suffix array, log interval and exponential moving average

 *

 * 1. Suffix array

 *

 * Suffix array is an array of all the suffixes of a string. It is

 * widely used as a data structure for compression, text search, ...

 * For instance for the word 'banana', the suffixes will be: 'banana'

 * 'anana' 'nana' 'ana' 'na' 'a'

 *

 * Usually, the suffix array is sorted but for our purpose it is

 * not necessary and won't provide any improvement in the context of

 * the solved problem where we clearly define the boundaries of the

 * search by a max period and min period.

 *

 * The suffix array will build a suite of intervals of different

 * length and will look for the repetition of each suite. If the suite

 * is repeating then we have the period because it is the length of

 * the suite whatever its position in the buffer.

 *

 * 2. Log interval

 *

 * We saw the irq timings allow to compute the interval of the

 * occurrences for a specific interrupt. We can reasonably assume the

 * longer is the interval, the higher is the error for the next event

 * and we can consider storing those interval values into an array

 * where each slot in the array correspond to an interval at the power

 * of 2 of the index. For example, index 12 will contain values

 * between 2^11 and 2^12.

 *

 * At the end we have an array of values where at each index defines a

 * [2^index - 1, 2 ^ index] interval values allowing to store a large

 * number of values inside a small array.

 *

 * For example, if we have the value 1123, then we store it at

 * ilog2(1123) = 10 index value.

 *

 * Storing those value at the specific index is done by computing an

 * exponential moving average for this specific slot. For instance,

 * for values 1800, 1123, 1453, ... fall under the same slot (10) and

 * the exponential moving average is computed every time a new value

 * is stored at this slot.

 *

 * 3. Exponential Moving Average

 *

 * The EMA is largely used to track a signal for stocks or as a low

 * pass filter. The magic of the formula, is it is very simple and the

 * reactivity of the average can be tuned with the factors called

 * alpha.

 *

 * The higher the alphas are, the faster the average respond to the

 * signal change. In our case, if a slot in the array is a big

 * interval, we can have numbers with a big difference between

 * them. The impact of those differences in the average computation

 * can be tuned by changing the alpha value.

 *

 *

 *  -- The algorithm --

 *

 * We saw the different processing above, now let's see how they are

 * used together.

 *

 * For each interrupt:

 *	For each interval:

 *		Compute the index = ilog2(interval)

 *		Compute a new_ema(buffer[index], interval)

 *		Store the index in a circular buffer

 *

 *	Compute the suffix array of the indexes

 *

 *	For each suffix:

 *		If the suffix is reverse-found 3 times

 *			Return suffix

 *

 *	Return Not found

 *

 * However we can not have endless suffix array to be build, it won't

 * make sense and it will add an extra overhead, so we can restrict

 * this to a maximum suffix length of 5 and a minimum suffix length of

 * 2. The experience showed 5 is the majority of the maximum pattern

 * period found for different devices.

 *

 * The result is a pattern finding less than 1us for an interrupt.

 *

 * Example based on real values:

 *

 * Example 1 : MMC write/read interrupt interval:

 *

 *	223947, 1240, 1384, 1386, 1386,

 *	217416, 1236, 1384, 1386, 1387,

 *	214719, 1241, 1386, 1387, 1384,

 *	213696, 1234, 1384, 1386, 1388,

 *	219904, 1240, 1385, 1389, 1385,

 *	212240, 1240, 1386, 1386, 1386,

 *	214415, 1236, 1384, 1386, 1387,

 *	214276, 1234, 1384, 1388, ?

 *

 * For each element, apply ilog2(value)

 *

 *	15, 8, 8, 8, 8,

 *	15, 8, 8, 8, 8,

 *	15, 8, 8, 8, 8,

 *	15, 8, 8, 8, 8,

 *	15, 8, 8, 8, 8,

 *	15, 8, 8, 8, 8,

 *	15, 8, 8, 8, 8,

 *	15, 8, 8, 8, ?

 *

 * Max period of 5, we take the last (max_period * 3) 15 elements as

 * we can be confident if the pattern repeats itself three times it is

 * a repeating pattern.

 *

 *	             8,

 *	15, 8, 8, 8, 8,

 *	15, 8, 8, 8, 8,

 *	15, 8, 8, 8, ?

 *

 * Suffixes are:

 *

 *  1) 8, 15, 8, 8, 8  <- max period

 *  2) 8, 15, 8, 8

 *  3) 8, 15, 8

 *  4) 8, 15           <- min period

 *

 * From there we search the repeating pattern for each suffix.

 *

 * buffer: 8, 15, 8, 8, 8, 8, 15, 8, 8, 8, 8, 15, 8, 8, 8

 *         |   |  |  |  |  |   |  |  |  |  |   |  |  |  |

 *         8, 15, 8, 8, 8  |   |  |  |  |  |   |  |  |  |

 *                         8, 15, 8, 8, 8  |   |  |  |  |

 *                                         8, 15, 8, 8, 8

 *

 * When moving the suffix, we found exactly 3 matches.

 *

 * The first suffix with period 5 is repeating.

 *

 * The next event is (3 * max_period) % suffix_period

 *

 * In this example, the result 0, so the next event is suffix[0] => 8

 *

 * However, 8 is the index in the array of exponential moving average

 * which was calculated on the fly when storing the values, so the

 * interval is ema[8] = 1366

 *

 *

 * Example 2:

 *

 *	4, 3, 5, 100,

 *	3, 3, 5, 117,

 *	4, 4, 5, 112,

 *	4, 3, 4, 110,

 *	3, 5, 3, 117,

 *	4, 4, 5, 112,

 *	4, 3, 4, 110,

 *	3, 4, 5, 112,

 *	4, 3, 4, 110

 *

 * ilog2

 *

 *	0, 0, 0, 4,

 *	0, 0, 0, 4,

 *	0, 0, 0, 4,

 *	0, 0, 0, 4,

 *	0, 0, 0, 4,

 *	0, 0, 0, 4,

 *	0, 0, 0, 4,

 *	0, 0, 0, 4,

 *	0, 0, 0, 4

 *

 * Max period 5:

 *	   0, 0, 4,

 *	0, 0, 0, 4,

 *	0, 0, 0, 4,

 *	0, 0, 0, 4

 *

 * Suffixes:

 *

 *  1) 0, 0, 4, 0, 0

 *  2) 0, 0, 4, 0

 *  3) 0, 0, 4

 *  4) 0, 0

 *

 * buffer: 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 4

 *         |  |  |  |  |  |  X

 *         0, 0, 4, 0, 0, |  X

 *                        0, 0

 *

 * buffer: 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 4

 *         |  |  |  |  |  |  |  |  |  |  |  |  |  |  |

 *         0, 0, 4, 0, |  |  |  |  |  |  |  |  |  |  |

 *                     0, 0, 4, 0, |  |  |  |  |  |  |

 *                                 0, 0, 4, 0, |  |  |

 *                                             0  0  4

 *

 * Pattern is found 3 times, the remaining is 1 which results from

 * (max_period * 3) % suffix_period. This value is the index in the

 * suffix arrays. The suffix array for a period 4 has the value 4

 * at index 1.

 2 ^ PREDICTION_MAX useconds */

 slots for EMAs, hardly more than 16 */

/*

 * Number of elements in the circular buffer: If it happens it was

 * flushed before, then the number of elements could be smaller than

 * IRQ_TIMINGS_SIZE, so the count is used, otherwise the array size is

 * used as we wrapped. The index begins from zero when we did not

 * wrap. That could be done in a nicer way with the proper circular

 * array structure type but with the cost of extra computation in the

 * interrupt handler hot path. We choose efficiency.

/*

 * Exponential moving average computation

	/*

	 * We can use a s64 type variable to be added with the u64

	 * ema_old variable as this one will never have its topmost

	 * bit set, it will be always smaller than 2^63 nanosec

	 * interrupt interval (292 years).

	/*

	 * Move the beginning pointer to the end minus the max period x 3.

	 * We are at the point we can begin searching the pattern

 Adjust the length to the maximum allowed period x 3 */

	/*

	 * The buffer contains the suite of intervals, in a ilog2

	 * basis, we are looking for a repetition. We point the

	 * beginning of the search three times the length of the

	 * period beginning at the end of the buffer. We do that for

	 * each suffix.

		/*

		 * The first comparison always succeed because the

		 * suffix is deduced from the first n-period bytes of

		 * the buffer and we compare the initial suffix with

		 * itself, so we can skip the first iteration.

		/*

		 * We look if the suite with period 'i' repeat

		 * itself. If it is truncated at the end, as it

		 * repeats we can use the period to find out the next

		 * element with the modulo.

			/*

			 * Move the index in a period basis

			/*

			 * If this condition is reached, all previous

			 * memcmp were successful, so the period is

			 * found.

			/*

			 * If the remaining elements to compare are

			 * smaller than the period, readjust the size

			 * of the comparison for the last iteration.

	/*

	 * As we want to find three times the repetition, we need a

	 * number of intervals greater or equal to three times the

	 * maximum period, otherwise we truncate the max period.

	/*

	 * If we don't have enough irq timings for this prediction,

	 * just bail out.

	/*

	 * 'count' will depends if the circular buffer wrapped or not

	/*

	 * Copy the content of the circular buffer into another buffer

	 * in order to linearize the buffer instead of dealing with

	 * wrapping indexes and shifted array which will be prone to

	 * error and extremely difficult to debug.

	/*

	 * The PREDICTION_FACTOR increase the interval size for the

	 * array of exponential average.

	/*

	 * Get the index in the ema table for this interrupt.

	/*

	 * Store the index as an element of the pattern in another

	 * circular array.

	/*

	 * The timestamps are absolute time values, we need to compute

	 * the timing interval between two interrupts.

	/*

	 * The interval type is u64 in order to deal with the same

	 * type in our computation, that prevent mindfuck issues with

	 * overflow, sign and division.

	/*

	 * The interrupt triggered more than one second apart, that

	 * ends the sequence as predictable for our purpose. In this

	 * case, assume we have the beginning of a sequence and the

	 * timestamp is the first value. As it is impossible to

	 * predict anything at this point, return.

	 *

	 * Note the first timestamp of the sequence will always fall

	 * in this test because the old_ts is zero. That is what we

	 * want as we need another timestamp to compute an interval.

/**

 * irq_timings_next_event - Return when the next event is supposed to arrive

 *

 * During the last busy cycle, the number of interrupts is incremented

 * and stored in the irq_timings structure. This information is

 * necessary to:

 *

 * - know if the index in the table wrapped up:

 *

 *      If more than the array size interrupts happened during the

 *      last busy/idle cycle, the index wrapped up and we have to

 *      begin with the next element in the array which is the last one

 *      in the sequence, otherwise it is at the index 0.

 *

 * - have an indication of the interrupts activity on this CPU

 *   (eg. irq/sec)

 *

 * The values are 'consumed' after inserting in the statistical model,

 * thus the count is reinitialized.

 *

 * The array of values **must** be browsed in the time direction, the

 * timestamp must increase between an element and the next one.

 *

 * Returns a nanosec time based estimation of the earliest interrupt,

 * U64_MAX otherwise.

	/*

	 * This function must be called with the local irq disabled in

	 * order to prevent the timings circular buffer to be updated

	 * while we are reading it.

	/*

	 * Number of elements in the circular buffer: If it happens it

	 * was flushed before, then the number of elements could be

	 * smaller than IRQ_TIMINGS_SIZE, so the count is used,

	 * otherwise the array size is used as we wrapped. The index

	 * begins from zero when we did not wrap. That could be done

	 * in a nicer way with the proper circular array structure

	 * type but with the cost of extra computation in the

	 * interrupt handler hot path. We choose efficiency.

	 *

	 * Inject measured irq/timestamp to the pattern prediction

	 * model while decrementing the counter because we consume the

	 * data from our circular buffer.

	/*

	 * Look in the list of interrupts' statistics, the earliest

	 * next event.

	/*

	 * Some platforms can have the same private interrupt per cpu,

	 * so this function may be called several times with the

	 * same interrupt number. Just bail out in case the per cpu

	 * stat structure is already allocated.

/*

 * Intervals are given in nanosecond base

	/*

	 * Inject all values except the last one which will be used

	 * to compare with the next index result.

	/*

	 * Fill the circular buffer by using the dedicated function.

	/*

	 * Compute the first elements values after the index wrapped

	 * up or not.

	/*

	 * Test the circular buffer count is correct.

	/*

	 * Test the macro allowing to browse all the irqts.

	/*

	 * The circular buffer should have be flushed when browsed

	 * with for_each_irqts

	/*

	 * Test the circular buffer with different number of

	 * elements. The purpose is to test at the limits (empty, half

	 * full, full, wrapped with the cursor at the boundaries,

	 * wrapped several times, etc ...

	/*

	 * At this point, we don't except any subsystem to use the irq

	 * timings but us, so it should not be enabled.

 SPDX-License-Identifier: GPL-2.0

/*

 * drivers/power/process.c - Functions for starting/stopping processes on

 *                           suspend transitions.

 *

 * Originally from swsusp.

/*

 * Timeout for stopping processes

		/*

		 * We need to retry, but first give the freezing tasks some

		 * time to enter the refrigerator.  Start with an initial

		 * 1 ms sleep followed by exponential backoff until 8 ms.

/**

 * freeze_processes - Signal user space processes to enter the refrigerator.

 * The current thread will not be frozen.  The same process that calls

 * freeze_processes must later call thaw_processes.

 *

 * On success, returns 0.  On failure, -errno and system is fully thawed.

 Make sure this task doesn't get frozen */

	/*

	 * Now that the whole userspace is frozen we need to disable

	 * the OOM killer to disallow any further interference with

	 * killable tasks. There is no guarantee oom victims will

	 * ever reach a point they go away we have to wait with a timeout.

/**

 * freeze_kernel_threads - Make freezable kernel threads go to the refrigerator.

 *

 * On success, returns 0.  On failure, -errno and only the kernel threads are

 * thawed, so as to give a chance to the caller to do additional cleanups

 * (if any) before thawing the userspace tasks. So, it is the responsibility

 * of the caller to thaw the userspace tasks, when the time is right.

 No other threads should have PF_SUSPEND_TASK set */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kernel/power/hibernate.c - Hibernation (a.k.a suspend-to-disk) support.

 *

 * Copyright (c) 2003 Patrick Mochel

 * Copyright (c) 2003 Open Source Development Lab

 * Copyright (c) 2004 Pavel Machek <pavel@ucw.cz>

 * Copyright (c) 2009 Rafael J. Wysocki, Novell Inc.

 * Copyright (C) 2012 Bojan Smojver <bojan@rexursive.com>

 keep last */

/**

 * hibernation_set_ops - Set the global hibernate operations.

 * @ops: Hibernation operations to use in subsequent hibernation transitions.

 !CONFIG_PM_DEBUG */

 !CONFIG_PM_DEBUG */

/**

 * platform_begin - Call platform to start hibernation.

 * @platform_mode: Whether or not to use the platform driver.

/**

 * platform_end - Call platform to finish transition to the working state.

 * @platform_mode: Whether or not to use the platform driver.

/**

 * platform_pre_snapshot - Call platform to prepare the machine for hibernation.

 * @platform_mode: Whether or not to use the platform driver.

 *

 * Use the platform driver to prepare the system for creating a hibernate image,

 * if so configured, and return an error code if that fails.

/**

 * platform_leave - Call platform to prepare a transition to the working state.

 * @platform_mode: Whether or not to use the platform driver.

 *

 * Use the platform driver prepare to prepare the machine for switching to the

 * normal mode of operation.

 *

 * This routine is called on one CPU with interrupts disabled.

/**

 * platform_finish - Call platform to switch the system to the working state.

 * @platform_mode: Whether or not to use the platform driver.

 *

 * Use the platform driver to switch the machine to the normal mode of

 * operation.

 *

 * This routine must be called after platform_prepare().

/**

 * platform_pre_restore - Prepare for hibernate image restoration.

 * @platform_mode: Whether or not to use the platform driver.

 *

 * Use the platform driver to prepare the system for resume from a hibernation

 * image.

 *

 * If the restore fails after this function has been called,

 * platform_restore_cleanup() must be called.

/**

 * platform_restore_cleanup - Switch to the working state after failing restore.

 * @platform_mode: Whether or not to use the platform driver.

 *

 * Use the platform driver to switch the system to the normal mode of operation

 * after a failing restore.

 *

 * If platform_pre_restore() has been called before the failing restore, this

 * function must be called too, regardless of the result of

 * platform_pre_restore().

/**

 * platform_recover - Recover from a failure to suspend devices.

 * @platform_mode: Whether or not to use the platform driver.

/**

 * swsusp_show_speed - Print time elapsed between two events during hibernation.

 * @start: Starting event.

 * @stop: Final event.

 * @nr_pages: Number of memory pages processed between @start and @stop.

 * @msg: Additional diagnostic message to print.

 avoid div-by-zero */

/**

 * create_image - Create a hibernation image.

 * @platform_mode: Whether or not to use the platform driver.

 *

 * Execute device drivers' "late" and "noirq" freeze callbacks, create a

 * hibernation image and run the drivers' "noirq" and "early" thaw callbacks.

 *

 * Control reappears in this routine after the subsequent restore.

 Restore control flow magically appears here */

 Allow architectures to do nosmt-specific post-resume dances */

/**

 * hibernation_snapshot - Quiesce devices and create a hibernation image.

 * @platform_mode: If set, use platform driver to prepare for the transition.

 *

 * This routine must be called with system_transition_mutex held.

 Preallocate image memory before shutting down devices. */

		/*

		 * Indicate to the caller that we are returning due to a

		 * successful freezer test.

	/*

	 * In the case that we call create_image() above, the control

	 * returns here (1) after the image has been created or the

	 * image creation has failed and (2) after a successful restore.

 We may need to release the preallocated image pages here. */

/**

 * resume_target_kernel - Restore system state from a hibernation image.

 * @platform_mode: Whether or not to use the platform driver.

 *

 * Execute device drivers' "noirq" and "late" freeze callbacks, restore the

 * contents of highmem that have not been restored yet from the image and run

 * the low-level code that will restore the remaining contents of memory and

 * switch to the just restored target kernel.

		/*

		 * The code below is only ever reached in case of a failure.

		 * Otherwise, execution continues at the place where

		 * swsusp_arch_suspend() was called.

		/*

		 * This call to restore_highmem() reverts the changes made by

		 * the previous one.

	/*

	 * The only reason why swsusp_arch_resume() can fail is memory being

	 * very tight, so we have to free it as soon as we can to avoid

	 * subsequent failures.

/**

 * hibernation_restore - Quiesce devices and restore from a hibernation image.

 * @platform_mode: If set, use platform driver to prepare for the transition.

 *

 * This routine must be called with system_transition_mutex held.  If it is

 * successful, control reappears in the restored target kernel in

 * hibernation_snapshot().

		/*

		 * The above should either succeed and jump to the new kernel,

		 * or return with an error. Otherwise things are just

		 * undefined, so let's be paranoid.

/**

 * hibernation_platform_enter - Power off the system using the platform driver.

	/*

	 * We have cancelled the power transition by running

	 * hibernation_ops->finish() before saving the image, so we should let

	 * the firmware know that we're going to enter the sleep state after all

 We should never get here */

/**

 * power_down - Shut the machine down for hibernation.

 *

 * Use the platform driver, if configured, to put the system into the sleep

 * state corresponding to hibernation, or try to power it off or reboot,

 * depending on the value of hibernation_mode.

 Restore swap signature. */

	/*

	 * Valid image is on the disk, if we continue we risk serious data

	 * corruption after resume.

/**

 * hibernate - Carry out system hibernation, including saving the image.

 The snapshot device should not be opened while we're running */

 Allocate memory management structures */

 Don't bother checking whether freezer_test_done is true */

/**

 * hibernate_quiet_exec - Execute a function with all devices frozen.

 * @func: Function to execute.

 * @data: Data pointer to pass to @func.

 *

 * Return the @func return value or an error code if it cannot be executed.

/**

 * software_resume - Resume from a saved hibernation image.

 *

 * This routine is called as a late initcall, when all devices have been

 * discovered and initialized already.

 *

 * The image reading code is called to see if there is a hibernation image

 * available for reading.  If that is the case, devices are quiesced and the

 * contents of memory is restored from the saved image.

 *

 * If this is successful, control reappears in the restored target kernel in

 * hibernation_snapshot() which returns to hibernate().  Otherwise, the routine

 * attempts to recover gracefully and make the kernel return to the normal mode

 * of operation.

	/*

	 * If the user said "noresume".. bail out early.

	/*

	 * name_to_dev_t() below takes a sysfs buffer mutex when sysfs

	 * is configured into the kernel. Since the regular hibernate

	 * trigger path is via sysfs which takes a buffer mutex before

	 * calling hibernate functions (which take system_transition_mutex)

	 * this can cause lockdep to complain about a possible ABBA deadlock

	 * which cannot happen since we're in the boot code here and

	 * sysfs can't be invoked yet. Therefore, we use a subclass

	 * here to avoid lockdep complaining.

 Check if the device is there */

		/*

		 * Some device discovery might still be in progress; we need

		 * to wait for this to finish.

 The snapshot device should not be opened while we're running */

 For success case, the suspend path will release the lock */

/*

 * /sys/power/disk - Control hibernation mode.

 *

 * Hibernation can be handled in several ways.  There are a few different ways

 * to put the system into the sleep state: using the platform driver (e.g. ACPI

 * or other hibernation_ops), powering it off or rebooting it (for testing

 * mostly).

 *

 * The sysfs file /sys/power/disk provides an interface for selecting the

 * hibernation mode to use.  Reading from this file causes the available modes

 * to be printed.  There are 3 modes that can be supported:

 *

 *	'platform'

 *	'shutdown'

 *	'reboot'

 *

 * If a platform hibernation driver is in use, 'platform' will be supported

 * and will be used by default.  Otherwise, 'shutdown' will be used by default.

 * The selected option (i.e. the one corresponding to the current value of

 * hibernation_mode) is enclosed by a square bracket.

 *

 * To select a given hibernation mode it is necessary to write the mode's

 * string representation (as returned by reading from /sys/power/disk) back

 * into /sys/power/disk.

 not a valid mode, continue with loop */

 SPDX-License-Identifier: GPL-2.0

/*

 * Functions for saving/restoring console.

 *

 * Originally from swsusp.

/**

 * pm_vt_switch_required - indicate VT switch at suspend requirements

 * @dev: device

 * @required: if true, caller needs VT switch at suspend/resume time

 *

 * The different console drivers may or may not require VT switches across

 * suspend/resume, depending on how they handle restoring video state and

 * what may be running.

 *

 * Drivers can indicate support for switchless suspend/resume, which can

 * save time and flicker, by using this routine and passing 'false' as

 * the argument.  If any loaded driver needs VT switching, or the

 * no_console_suspend argument has been passed on the command line, VT

 * switches will occur.

 already registered, update requirement */

/**

 * pm_vt_switch_unregister - stop tracking a device's VT switching needs

 * @dev: device

 *

 * Remove @dev from the vt switch list.

/*

 * There are three cases when a VT switch on suspend/resume are required:

 *   1) no driver has indicated a requirement one way or another, so preserve

 *      the old behavior

 *   2) console suspend is disabled, we want to see debug messages across

 *      suspend/resume

 *   3) any registered driver indicates it needs a VT switch

 *

 * If none of these conditions is present, meaning we have at least one driver

 * that doesn't need the switch, and none that do, we can avoid it to make

 * resume look a little prettier (and suspend too, but that's usually hidden,

 * e.g. when closing the lid on a laptop).

 SPDX-License-Identifier: GPL-2.0-only

/*

 * poweroff.c - sysrq handler to gracefully power down machine.

/*

 * When the user hits Sys-Rq o to power down the machine this is the

 * callback we use.

 run sysrq poweroff on boot cpu */

 SPDX-License-Identifier: GPL-2.0

/*

 * Energy Model of devices

 *

 * Copyright (c) 2018-2021, Arm ltd.

 * Written by: Quentin Perret, Arm ltd.

 * Improvements provided by: Lukasz Luba, Arm ltd.

/*

 * Mutex serializing the registrations of performance domains and letting

 * callbacks defined by drivers sleep.

 Create per-ps directory */

 Create the directory of the performance domain */

 Create a sub-directory for each performance state */

 Create /sys/kernel/debug/energy_model directory */

 CONFIG_DEBUG_FS */

 Build the list of performance states for this performance domain */

		/*

		 * active_power() is a driver callback which ceils 'freq' to

		 * lowest performance state of 'dev' above 'freq' and updates

		 * 'power' and 'freq' accordingly.

		/*

		 * We expect the driver callback to increase the frequency for

		 * higher performance states.

		/*

		 * The power returned by active_state() is expected to be

		 * positive and to fit into 16 bits.

 Compute the cost of each performance state. */

	/*

	 * Efficiencies have been installed in CPUFreq, inefficient frequencies

	 * will be skipped. The EM can do the same.

/**

 * em_pd_get() - Return the performance domain for a device

 * @dev : Device to find the performance domain for

 *

 * Returns the performance domain to which @dev belongs, or NULL if it doesn't

 * exist.

/**

 * em_cpu_get() - Return the performance domain for a CPU

 * @cpu : CPU to find the performance domain for

 *

 * Returns the performance domain to which @cpu belongs, or NULL if it doesn't

 * exist.

/**

 * em_dev_register_perf_domain() - Register the Energy Model (EM) for a device

 * @dev		: Device for which the EM is to register

 * @nr_states	: Number of performance states to register

 * @cb		: Callback functions providing the data of the Energy Model

 * @cpus	: Pointer to cpumask_t, which in case of a CPU device is

 *		obligatory. It can be taken from i.e. 'policy->cpus'. For other

 *		type of devices this should be set to NULL.

 * @milliwatts	: Flag indicating that the power values are in milliWatts or

 *		in some other scale. It must be set properly.

 *

 * Create Energy Model tables for a performance domain using the callbacks

 * defined in cb.

 *

 * The @milliwatts is important to set with correct value. Some kernel

 * sub-systems might rely on this flag and check if all devices in the EM are

 * using the same scale.

 *

 * If multiple clients register the same performance domain, all but the first

 * registration will be ignored.

 *

 * Return 0 on success

	/*

	 * Use a mutex to serialize the registration of performance domains and

	 * let the driver-defined callback functions sleep.

			/*

			 * All CPUs of a domain must have the same

			 * micro-architecture since they all share the same

			 * table.

/**

 * em_dev_unregister_perf_domain() - Unregister Energy Model (EM) for a device

 * @dev		: Device for which the EM is registered

 *

 * Unregister the EM for the specified @dev (but not a CPU device).

	/*

	 * The mutex separates all register/unregister requests and protects

	 * from potential clean-up/setup issues in the debugfs directories.

	 * The debugfs directory name is the same as device's name.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/kernel/power/swap.c

 *

 * This file provides functions for reading the suspend image from

 * and writing it to a swap partition.

 *

 * Copyright (C) 1998,2001-2005 Pavel Machek <pavel@ucw.cz>

 * Copyright (C) 2006 Rafael J. Wysocki <rjw@sisk.pl>

 * Copyright (C) 2010-2012 Bojan Smojver <bojan@rexursive.com>

/*

 * When reading an {un,}compressed image, we may restore pages in place,

 * in which case some architectures need these pages cleaning before they

 * can be executed. We don't know which pages these may be, so clean the lot.

/*

 *	The swap map is a data structure used for keeping track of each page

 *	written to a swap partition.  It consists of many swap_map_page

 *	structures that contain each an array of MAP_PAGE_ENTRIES swap entries.

 *	These structures are stored on the swap and linked together with the

 *	help of the .next_swap member.

 *

 *	The swap map is created during suspend.  The swap map pages are

 *	allocated and populated one at a time, so we only need one memory

 *	page to set up the entire structure.

 *

 *	During resume we pick up all swap_map_page structures into a list.

/*

 * Number of free pages that are not high.

/*

 * Number of pages required to be kept free while writing the image. Always

 * half of all available low pages before the writing starts.

/**

 *	The swap_map_handle structure is used for handling swap in

 *	a file-alike way

 Flags to pass to the "boot" kernel */

/**

 *	The following functions are used for tracing the allocated

 *	swap pages, so that they can be freed in case of an error.

 Figure out where to put the new node */

 Try to merge */

 Try to merge */

 It already is in the tree */

 Add the new node and rebalance the tree. */

/**

 *	alloc_swapdev_block - allocate a swap page and register that it has

 *	been allocated, so that it can be freed in case of an error.

/**

 *	free_all_swap_pages - free swap pages allocated for saving image data.

 *	It also frees the extents used to register which swap entries had been

 *	allocated.

/*

 * General things

	/*

	 * We are relying on the behavior of blk_plug that a thread with

	 * a plug will flush the plug list before sleeping.

/*

 * Saving part

/**

 *	swsusp_swap_check - check if the resume device is a swap device

 *	and get its index (if so)

 *

 *	This is called before saving image

/**

 *	write_page - Write one page to given swap location.

 *	@buf:		Address we're writing.

 *	@offset:	Offset of the swap page we're writing to.

 *	@hb:		bio completion batch

 Free pages */

 Go synchronous */

			/*

			 * Recalculate the number of required free pages, to

			 * make sure we never take more than half.

 We need to remember how much compressed data we need to read. */

 Number of pages/bytes we'll compress at one time. */

 Number of pages/bytes we need for compressed data (worst case). */

 Maximum number of threads for compression/decompression. */

 Minimum/maximum number of pages for read buffering. */

/**

 *	save_image - save the suspend image data

/**

 * Structure used for CRC32.

 thread */

 ready to start flag */

 ready to stop flag */

 nr current threads */

 start crc update */

 crc update done */

 points to handle's crc32 */

 uncompressed lengths */

 uncompressed data */

/**

 * CRC32 update function that runs in its own thread.

/**

 * Structure used for LZO data compression.

 thread */

 ready to start flag */

 ready to stop flag */

 return code */

 start compression */

 compression done */

 uncompressed length */

 compressed length */

 uncompressed buffer */

 compressed buffer */

 compression workspace */

/**

 * Compression function that runs in its own thread.

/**

 * save_image_lzo - Save the suspend image data compressed with LZO.

 * @handle: Swap map handle to use for saving the image.

 * @snapshot: Image to read data from.

 * @nr_to_write: Number of pages to save.

	/*

	 * We'll limit the number of threads for compression to limit memory

	 * footprint.

	/*

	 * Start the compression threads.

	/*

	 * Start the CRC32 thread.

	/*

	 * Adjust the number of required free pages after all allocations have

	 * been done. We don't want to run out of pages when writing.

			/*

			 * Given we are writing one page at a time to disk, we

			 * copy that much from the buffer, although the last

			 * bit will likely be smaller than full page. This is

			 * OK - we saved the length of the compressed data, so

			 * any garbage at the end will be discarded when we

			 * read it.

/**

 *	enough_swap - Make sure we have enough swap to save the image.

 *

 *	Returns TRUE or FALSE after checking the total amount of swap

 *	space available from the resume partition.

/**

 *	swsusp_write - Write entire image and metadata.

 *	@flags: flags to pass to the "boot" kernel in the image header

 *

 *	It is important _NOT_ to umount filesystems at this point. We want

 *	them synced (in case something goes wrong) but we DO not want to mark

 *	filesystem clean: it is not. (And it does not matter, if we resume

 *	correctly, we'll mark system clean, anyway.)

/**

 *	The following functions allow us to read data using a swap map

 *	in a file-alike way

 how can this happen? */

/**

 *	load_image - load the image using the swap map handle

 *	@handle and the snapshot handle @snapshot

 *	(assume there are @nr_pages pages to load)

/**

 * Structure used for LZO data decompression.

 thread */

 ready to start flag */

 ready to stop flag */

 return code */

 start decompression */

 decompression done */

 uncompressed length */

 compressed length */

 uncompressed buffer */

 compressed buffer */

/**

 * Decompression function that runs in its own thread.

/**

 * load_image_lzo - Load compressed image data and decompress them with LZO.

 * @handle: Swap map handle to use for loading data.

 * @snapshot: Image to copy uncompressed data into.

 * @nr_to_read: Number of pages to load.

	/*

	 * We'll limit the number of threads for decompression to limit memory

	 * footprint.

	/*

	 * Start the decompression threads.

	/*

	 * Start the CRC32 thread.

	/*

	 * Set the number of pages for read buffering.

	 * This is complete guesswork, because we'll only know the real

	 * picture once prepare_image() is called, which is much later on

	 * during the image load phase. We'll assume the worst case and

	 * say that none of the image pages are from high memory.

				/*

				 * On real read error, finish. On end of data,

				 * set EOF flag and just exit the read loop.

		/*

		 * We are out of data, wait for some more.

		/*

		 * Wait for more data while we are decompressing.

/**

 *	swsusp_read - read the hibernation image.

 *	@flags_p: flags passed by the "frozen" kernel in the image header should

 *		  be written into this memory location

/**

 *      swsusp_check - Check for swsusp signature in the resume device

 Reset swap signature now */

/**

 *	swsusp_close - close swap device.

/**

 *      swsusp_unmark - Unmark swsusp signature in the resume device

	/*

	 * We just returned from suspend, we don't need the image any more.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kernel/power/suspend_test.c - Suspend to RAM and standby test facility.

 *

 * Copyright (c) 2009 Pavel Machek <pavel@ucw.cz>

/*

 * We test the system suspend code by setting an RTC wakealarm a short

 * time in the future, then suspending.  Suspending the devices won't

 * normally take long ... some systems only need a few milliseconds.

 *

 * The time it takes is system-specific though, so when we test this

 * during system bootup we allow a LOT of time.

	/* FIXME Use better timebase than "jiffies", ideally a clocksource.

	 * What we want is a hardware counter that will work correctly even

	 * during the irqs-are-off stages of the suspend/resume cycle...

	/* Warning on suspend means the RTC alarm period needs to be

	 * larger -- the system was sooo slooowwww to suspend that the

	 * alarm (should have) fired before the system went to sleep!

	 *

	 * Warning on either suspend or resume also means the system

	 * has some performance issues.  The stack dump of a WARN_ON

	 * is more likely to get the right attention than a printk...

/*

 * To test system suspend, we need a hands-off mechanism to resume the

 * system.  RTCs wake alarms are a common self-contained mechanism.

 this may fail if the RTC hasn't been initialized */

	/* Some platforms can't detect that the alarm triggered the

	 * wakeup, or (accordingly) disable it after it afterwards.

	 * It's supposed to give oneshot behavior; cope.

/*

 * Kernel options like "test_suspend=mem" force suspend/resume sanity tests

 * at startup time.  They're normally disabled, for faster boot and because

 * we can't know which states really work on this particular system.

 example : "=mem[,N]" ==> "mem[,N]" */

 PM is initialized by now; is that state testable? */

 RTCs have initialized by now too ... can we use one? */

 go for it */

 SPDX-License-Identifier: GPL-2.0

/*

 * kernel/power/wakelock.c

 *

 * User space wakeup sources support.

 *

 * Copyright (C) 2012 Rafael J. Wysocki <rjw@sisk.pl>

 *

 * This code is based on the analogous interface allowing user space to

 * manipulate wakelocks on Android.

 CONFIG_PM_WAKELOCKS_LIMIT = 0 */

 CONFIG_PM_WAKELOCKS_LIMIT */

 !CONFIG_PM_WAKELOCKS_GC */

 !CONFIG_PM_WAKELOCKS_GC */

 Not found, we have to add a new one. */

 Find out if there's a valid timeout string appended. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Power Management Quality of Service (PM QoS) support base.

 *

 * Copyright (C) 2020 Intel Corporation

 *

 * Authors:

 *	Mark Gross <mgross@linux.intel.com>

 *	Rafael J. Wysocki <rafael.j.wysocki@intel.com>

 *

 * Provided here is an interface for specifying PM QoS dependencies.  It allows

 * entities depending on QoS constraints to register their requests which are

 * aggregated as appropriate to produce effective constraints (target values)

 * that can be monitored by entities needing to respect them, either by polling

 * or through a built-in notification mechanism.

 *

 * In addition to the basic functionality, more specific interfaces for managing

 * global CPU latency QoS requests and frequency QoS requests are provided.

#define DEBUG*/

/*

 * locking rule: all changes to constraints or notifiers lists

 * or pm_qos_object list and pm_qos_objects need to happen with pm_qos_lock

 * held, taken with _irqsave.  One lock to rule them all

/**

 * pm_qos_read_value - Return the current effective constraint value.

 * @c: List of PM QoS constraint requests.

/**

 * pm_qos_update_target - Update a list of PM QoS constraint requests.

 * @c: List of PM QoS requests.

 * @node: Target list entry.

 * @action: Action to carry out (add, update or remove).

 * @value: New request value for the target list entry.

 *

 * Update the given list of PM QoS constraint requests, @c, by carrying an

 * @action involving the @node list entry and @value on it.

 *

 * The recognized values of @action are PM_QOS_ADD_REQ (store @value in @node

 * and add it to the list), PM_QOS_UPDATE_REQ (remove @node from the list, store

 * @value in it and add it to the list again), and PM_QOS_REMOVE_REQ (remove

 * @node from the list, ignore @value).

 *

 * Return: 1 if the aggregate constraint value has changed, 0  otherwise.

		/*

		 * To change the list, atomically remove, reinit with new value

		 * and add, then see if the aggregate has changed.

 no action */

/**

 * pm_qos_flags_remove_req - Remove device PM QoS flags request.

 * @pqf: Device PM QoS flags set to remove the request from.

 * @req: Request to remove from the set.

/**

 * pm_qos_update_flags - Update a set of PM QoS flags.

 * @pqf: Set of PM QoS flags to update.

 * @req: Request to add to the set, to modify, or to remove from the set.

 * @action: Action to take on the set.

 * @val: Value of the request to add or modify.

 *

 * Return: 1 if the aggregate constraint value has changed, 0 otherwise.

 no action */

 Definitions related to the CPU latency QoS. */

/**

 * cpu_latency_qos_limit - Return current system-wide CPU latency QoS limit.

/**

 * cpu_latency_qos_request_active - Check the given PM QoS request.

 * @req: PM QoS request to check.

 *

 * Return: 'true' if @req has been added to the CPU latency QoS list, 'false'

 * otherwise.

/**

 * cpu_latency_qos_add_request - Add new CPU latency QoS request.

 * @req: Pointer to a preallocated handle.

 * @value: Requested constraint value.

 *

 * Use @value to initialize the request handle pointed to by @req, insert it as

 * a new entry to the CPU latency QoS list and recompute the effective QoS

 * constraint for that list.

 *

 * Callers need to save the handle for later use in updates and removal of the

 * QoS request represented by it.

/**

 * cpu_latency_qos_update_request - Modify existing CPU latency QoS request.

 * @req : QoS request to update.

 * @new_value: New requested constraint value.

 *

 * Use @new_value to update the QoS request represented by @req in the CPU

 * latency QoS list along with updating the effective constraint value for that

 * list.

/**

 * cpu_latency_qos_remove_request - Remove existing CPU latency QoS request.

 * @req: QoS request to remove.

 *

 * Remove the CPU latency QoS request represented by @req from the CPU latency

 * QoS list along with updating the effective constraint value for that list.

 User space interface to the CPU latency QoS via misc device. */

 CONFIG_CPU_IDLE */

 Definitions related to the frequency QoS below. */

/**

 * freq_constraints_init - Initialize frequency QoS constraints.

 * @qos: Frequency QoS constraints to initialize.

/**

 * freq_qos_read_value - Get frequency QoS constraint for a given list.

 * @qos: Constraints to evaluate.

 * @type: QoS request type.

/**

 * freq_qos_apply - Add/modify/remove frequency QoS request.

 * @req: Constraint request to apply.

 * @action: Action to perform (add/update/remove).

 * @value: Value to assign to the QoS request.

 *

 * This is only meant to be called from inside pm_qos, not drivers.

/**

 * freq_qos_add_request - Insert new frequency QoS request into a given list.

 * @qos: Constraints to update.

 * @req: Preallocated request object.

 * @type: Request type.

 * @value: Request value.

 *

 * Insert a new entry into the @qos list of requests, recompute the effective

 * QoS constraint value for that list and initialize the @req object.  The

 * caller needs to save that object for later use in updates and removal.

 *

 * Return 1 if the effective constraint value has changed, 0 if the effective

 * constraint value has not changed, or a negative error code on failures.

/**

 * freq_qos_update_request - Modify existing frequency QoS request.

 * @req: Request to modify.

 * @new_value: New request value.

 *

 * Update an existing frequency QoS request along with the effective constraint

 * value for the list of requests it belongs to.

 *

 * Return 1 if the effective constraint value has changed, 0 if the effective

 * constraint value has not changed, or a negative error code on failures.

/**

 * freq_qos_remove_request - Remove frequency QoS request from its list.

 * @req: Request to remove.

 *

 * Remove the given frequency QoS request from the list of constraints it

 * belongs to and recompute the effective constraint value for that list.

 *

 * Return 1 if the effective constraint value has changed, 0 if the effective

 * constraint value has not changed, or a negative error code on failures.

/**

 * freq_qos_add_notifier - Add frequency QoS change notifier.

 * @qos: List of requests to add the notifier to.

 * @type: Request type.

 * @notifier: Notifier block to add.

/**

 * freq_qos_remove_notifier - Remove frequency QoS change notifier.

 * @qos: List of requests to remove the notifier from.

 * @type: Request type.

 * @notifier: Notifier block to remove.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/kernel/power/user.c

 *

 * This file provides the user space interface for software suspend/resume.

 *

 * Copyright (C) 2006 Rafael J. Wysocki <rjw@sisk.pl>

 Hibernating.  The image device should be accessible. */

		/*

		 * Resuming.  We may need to wait for the image device to

		 * appear.

 on page boundary? */

	/*

	 * User space encodes device types as two-byte values,

	 * so we need to recode them

		/*

		 * It is necessary to thaw kernel threads here, because

		 * SNAPSHOT_CREATE_IMAGE may be invoked directly after

		 * SNAPSHOT_FREE.  In that case, if kernel threads were not

		 * thawed, the preallocation of memory carried out by

		 * hibernation_snapshot() might run into problems (i.e. it

		 * might fail or even deadlock).

		/*

		 * Tasks are frozen and the notifiers have been called with

		 * PM_HIBERNATION_PREPARE

 CONFIG_COMPAT */

 SPDX-License-Identifier: GPL-2.0

/*

 * kernel/power/autosleep.c

 *

 * Opportunistic sleep support.

 *

 * Copyright (C) 2012 Rafael J. Wysocki <rjw@sisk.pl>

/*

 * Note: it is only safe to mutex_lock(&autosleep_lock) if a wakeup_source

 * is active, otherwise a deadlock with try_to_suspend() is possible.

 * Alternatively mutex_lock_interruptible() can be used.  This will then fail

 * if an auto_sleep cycle tries to freeze processes.

	/*

	 * If the wakeup occurred for an unknown reason, wait to prevent the

	 * system from trying to suspend and waking up in a tight loop.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/kernel/power/snapshot.c

 *

 * This file provides system snapshot/restore functionality for swsusp.

 *

 * Copyright (C) 1998-2005 Pavel Machek <pavel@ucw.cz>

 * Copyright (C) 2006 Rafael J. Wysocki <rjw@sisk.pl>

 CONFIG_STRICT_KERNEL_RWX  && CONFIG_ARCH_HAS_SET_MEMORY */

/*

 * The calls to set_direct_map_*() should not fail because remapping a page

 * here means that we only update protection bits in an existing PTE.

 * It is still worth to have a warning here if something changes and this

 * will no longer be the case.

/*

 * Number of bytes to reserve for memory allocations made by device drivers

 * from their ->freeze() and ->freeze_noirq() callbacks so that they don't

 * cause image creation to fail (tunable via /sys/power/reserved_size).

/*

 * Preferred image size in bytes (tunable via /sys/power/image_size).

 * When it is set to N, swsusp will do its best to ensure the image

 * size will not exceed N bytes, but if that is impossible, it will

 * try to create the smallest image possible.

/*

 * List of PBEs needed for restoring the pages that were allocated before

 * the suspend and included in the suspend image, but have also been

 * allocated by the "resume" kernel, so their contents cannot be written

 * directly to their "original" page frames.

 struct linked_page is used to build chains of pages */

/*

 * List of "safe" pages (ie. pages that were not used by the image kernel

 * before hibernation) that may be used as temporary storage for image kernel

 * memory contents.

 Pointer to an auxiliary buffer (1 page) */

/**

 * get_image_page - Allocate a page for a hibernation image.

 * @gfp_mask: GFP mask for the allocation.

 * @safe_needed: Get pages that were not used before hibernation (restore only)

 *

 * During image restoration, for storing the PBE list and the image data, we can

 * only use memory pages that do not conflict with the pages used before

 * hibernation.  The "unsafe" pages have PageNosaveFree set and we count them

 * using allocated_unsafe_pages.

 *

 * Each allocated image page is marked as PageNosave and PageNosaveFree so that

 * swsusp_free() can release it.

 The page is unsafe, mark it for swsusp_free() */

/**

 * free_image_page - Free a page allocated for hibernation image.

 * @addr: Address of the page to free.

 * @clear_nosave_free: If set, clear the PageNosaveFree bit for the page.

 *

 * The page to free should have been allocated by get_image_page() (page flags

 * set by it are affected).

/*

 * struct chain_allocator is used for allocating small objects out of

 * a linked list of pages called 'the chain'.

 *

 * The chain grows each time when there is no room for a new object in

 * the current page.  The allocated objects cannot be freed individually.

 * It is only possible to free them all at once, by freeing the entire

 * chain.

 *

 * NOTE: The chain allocator may be inefficient if the allocated objects

 * are not much smaller than PAGE_SIZE.

 the chain */

	unsigned int used_space;	/* total size of objects allocated out

 mask for allocating pages */

 if set, only "safe" pages are allocated */

/**

 * Data types related to memory bitmaps.

 *

 * Memory bitmap is a structure consisting of many linked lists of

 * objects.  The main list's elements are of type struct zone_bitmap

 * and each of them corresponds to one zone.  For each zone bitmap

 * object there is a list of objects of type struct bm_block that

 * represent each blocks of bitmap in which information is stored.

 *

 * struct memory_bitmap contains a pointer to the main list of zone

 * bitmap objects, a struct bm_position used for browsing the bitmap,

 * and a pointer to the list of pages used for allocating all of the

 * zone bitmap objects and bitmap block objects.

 *

 * NOTE: It has to be possible to lay out the bitmap in memory

 * using only allocations of order 0.  Additionally, the bitmap is

 * designed to work with arbitrary number of zones (this is over the

 * top for now, but let's avoid making unnecessary assumptions ;-).

 *

 * struct zone_bitmap contains a pointer to a list of bitmap block

 * objects and a pointer to the bitmap block object that has been

 * most recently used for setting bits.  Additionally, it contains the

 * PFNs that correspond to the start and end of the represented zone.

 *

 * struct bm_block contains a pointer to the memory page in which

 * information is stored (in the form of a block of bitmap)

 * It also contains the pfns that correspond to the start and end of

 * the represented memory area.

 *

 * The memory bitmap is organized as a radix tree to guarantee fast random

 * access to the bits. There is one radix tree for each zone (as returned

 * from create_mem_extents).

 *

 * One radix tree is represented by one struct mem_zone_bm_rtree. There are

 * two linked lists for the nodes of the tree, one for the inner nodes and

 * one for the leave nodes. The linked leave nodes are used for fast linear

 * access of the memory bitmap.

 *

 * The struct rtree_node represents one node of the radix tree.

/*

 * struct rtree_node is a wrapper struct to link the nodes

 * of the rtree together for easy linear iteration over

 * bits and easy freeing

/*

 * struct mem_zone_bm_rtree represents a bitmap used for one

 * populated memory zone.

 Link Zones together         */

 Radix Tree inner nodes      */

 Radix Tree leaves           */

 Zone start page frame       */

 Zone end page frame + 1     */

 Radix Tree Root             */

 Number of Radix Tree Levels */

 Number of Bitmap Blocks     */

 strcut bm_position is used for browsing memory bitmaps */

	struct linked_page *p_list;	/* list of pages used to store zone

					   bitmap objects and bitmap block

 most recently used bit position */

 Functions that operate on memory bitmaps */

/**

 * alloc_rtree_node - Allocate a new node and add it to the radix tree.

 *

 * This function is used to allocate inner nodes as well as the

 * leave nodes of the radix tree. It also adds the node to the

 * corresponding linked list passed in by the *list parameter.

/**

 * add_rtree_block - Add a new leave node to the radix tree.

 *

 * The leave nodes need to be allocated in order to keep the leaves

 * linked list in order. This is guaranteed by the zone->blocks

 * counter.

 How many levels do we need for this block nr? */

 Make sure the rtree has enough levels */

 Allocate new block */

 Now walk the rtree to insert the block */

/**

 * create_zone_bm_rtree - Create a radix tree for one zone.

 *

 * Allocated the mem_zone_bm_rtree structure and initializes it.

 * This function also allocated and builds the radix tree for the

 * zone.

/**

 * free_zone_bm_rtree - Free the memory of the radix tree.

 *

 * Free all node pages of the radix tree. The mem_zone_bm_rtree

 * structure itself is not freed here nor are the rtree_node

 * structs.

/**

 * free_mem_extents - Free a list of memory extents.

 * @list: List of extents to free.

/**

 * create_mem_extents - Create a list of memory extents.

 * @list: List to put the extents into.

 * @gfp_mask: Mask to use for memory allocations.

 *

 * The extents represent contiguous ranges of PFNs.

 New extent is necessary */

 Merge this zone's range of PFNs with the existing one */

 More merging may be possible */

/**

 * memory_bm_create - Allocate memory for a memory bitmap.

/**

 * memory_bm_free - Free memory occupied by the memory bitmap.

 * @bm: Memory bitmap.

/**

 * memory_bm_find_bit - Find the bit for a given PFN in a memory bitmap.

 *

 * Find the bit in memory bitmap @bm that corresponds to the given PFN.

 * The cur.zone, cur.block and cur.node_pfn members of @bm are updated.

 *

 * Walk the radix tree to find the page containing the bit that represents @pfn

 * and return the position of the bit in @addr and @bit_nr.

 Find the right zone */

	/*

	 * We have found the zone. Now walk the radix tree to find the leaf node

	 * for our PFN.

	/*

	 * If the zone we wish to scan is the current zone and the

	 * pfn falls into the current node then we do not need to walk

	 * the tree.

 Update last position */

 Set return values */

/*

 * rtree_next_node - Jump to the next leaf node.

 *

 * Set the position to the beginning of the next node in the

 * memory bitmap. This is either the next node in the current

 * zone's radix tree or the first node in the radix tree of the

 * next zone.

 *

 * Return true if there is a next node, false otherwise.

 No more nodes, goto next zone */

 No more zones */

/**

 * memory_bm_rtree_next_pfn - Find the next set bit in a memory bitmap.

 * @bm: Memory bitmap.

 *

 * Starting from the last returned position this function searches for the next

 * set bit in @bm and returns the PFN represented by it.  If no more bits are

 * set, BM_END_OF_MAP is returned.

 *

 * It is required to run memory_bm_position_reset() before the first call to

 * this function for the given memory bitmap.

/*

 * This structure represents a range of page frames the contents of which

 * should not be saved during hibernation.

/**

 * register_nosave_region - Register a region of unsaveable memory.

 *

 * Register a range of page frames the contents of which should not be saved

 * during hibernation (to be used in the early initialization code).

 Try to extend the previous region (they should be sorted) */

 During init, this shouldn't fail */

 This allocation cannot fail */

/*

 * Set bits in this map correspond to the page frames the contents of which

 * should not be saved during the suspend.

 Set bits in this map correspond to free page frames. */

/*

 * Each page frame allocated for creating the image is marked by setting the

 * corresponding bits in forbidden_pages_map and free_pages_map simultaneously

/**

 * mark_nosave_pages - Mark pages that should not be saved.

 * @bm: Memory bitmap.

 *

 * Set the bits in @bm that correspond to the page frames the contents of which

 * should not be saved.

				/*

				 * It is safe to ignore the result of

				 * mem_bm_set_bit_check() here, since we won't

				 * touch the PFNs for which the error is

				 * returned anyway.

/**

 * create_basic_memory_bitmaps - Create bitmaps to hold basic page information.

 *

 * Create bitmaps needed for marking page frames that should not be saved and

 * free page frames.  The forbidden_pages_map and free_pages_map pointers are

 * only modified if everything goes well, because we don't want the bits to be

 * touched before both bitmaps are set up.

/**

 * free_basic_memory_bitmaps - Free memory bitmaps holding basic information.

 *

 * Free memory bitmaps allocated by create_basic_memory_bitmaps().  The

 * auxiliary pointers are necessary so that the bitmaps themselves are not

 * referred to while they are being freed.

/**

 * snapshot_additional_pages - Estimate the number of extra pages needed.

 * @zone: Memory zone to carry out the computation for.

 *

 * Estimate the number of additional pages needed for setting up a hibernation

 * image data structures for @zone (usually, the returned value is greater than

 * the exact number).

/**

 * count_free_highmem_pages - Compute the total number of free highmem pages.

 *

 * The returned number is system-wide.

/**

 * saveable_highmem_page - Check if a highmem page is saveable.

 *

 * Determine whether a highmem page should be included in a hibernation image.

 *

 * We should save the page if it isn't Nosave or NosaveFree, or Reserved,

 * and it isn't part of a free chunk of pages.

/**

 * count_highmem_pages - Compute the total number of saveable highmem pages.

 CONFIG_HIGHMEM */

/**

 * saveable_page - Check if the given page is saveable.

 *

 * Determine whether a non-highmem page should be included in a hibernation

 * image.

 *

 * We should save the page if it isn't Nosave, and is not in the range

 * of pages statically defined as 'unsaveable', and it isn't part of

 * a free chunk of pages.

/**

 * count_data_pages - Compute the total number of saveable non-highmem pages.

/*

 * This is needed, because copy_page and memcpy are not usable for copying

 * task structs.

/**

 * safe_copy_page - Copy a page in a safe way.

 *

 * Check if the page we are going to copy is marked as present in the kernel

 * page tables. This always is the case if CONFIG_DEBUG_PAGEALLOC or

 * CONFIG_ARCH_HAS_SET_DIRECT_MAP is not set. In that case kernel_page_present()

 * always returns 'true'.

			/*

			 * The page pointed to by src may contain some kernel

			 * data modified by kmap_atomic()

 CONFIG_HIGHMEM */

 Total number of image pages */

 Number of pages needed for saving the original pfns of the image pages */

/*

 * Numbers of normal and highmem page frames allocated for hibernation image

 * before suspending devices.

/*

 * Memory bitmap used for marking saveable pages (during hibernation) or

 * hibernation image pages (during restore)

/*

 * Memory bitmap used during hibernation for marking allocated page frames that

 * will contain copies of saveable pages.  During restore it is initially used

 * for marking hibernation image pages, but then the set bits from it are

 * duplicated in @orig_bm and it is released.  On highmem systems it is next

 * used for marking "safe" highmem pages, but it has to be reinitialized for

 * this purpose.

/**

 * swsusp_free - Free pages allocated for hibernation image.

 *

 * Image pages are allocated before snapshot creation, so they need to be

 * released after resume.

	/*

	 * Find the next bit set in both bitmaps. This is guaranteed to

	 * terminate when fb_pfn == fr_pfn == BM_END_OF_MAP.

 Helper functions used for the shrinking of memory. */

/**

 * preallocate_image_pages - Allocate a number of pages for hibernation image.

 * @nr_pages: Number of page frames to allocate.

 * @mask: GFP flags to use for the allocation.

 *

 * Return value: Number of page frames actually allocated

/**

 *  __fraction - Compute (an approximation of) x * (multiplier / base).

 CONFIG_HIGHMEM */

 CONFIG_HIGHMEM */

/**

 * free_unnecessary_pages - Release preallocated pages not needed for the image.

/**

 * minimum_image_size - Estimate the minimum acceptable size of an image.

 * @saveable: Number of saveable pages in the system.

 *

 * We want to avoid attempting to free too much memory too hard, so estimate the

 * minimum acceptable size of a hibernation image to use as the lower limit for

 * preallocating memory.

 *

 * We assume that the minimum image size should be proportional to

 *

 * [number of saveable pages] - [number of pages that can be freed in theory]

 *

 * where the second term is the sum of (1) reclaimable slab pages, (2) active

 * and (3) inactive anonymous pages, (4) active and (5) inactive file pages.

/**

 * hibernate_preallocate_memory - Preallocate memory for hibernation image.

 *

 * To create a hibernation image it is necessary to make a copy of every page

 * frame in use.  We also need a number of page frames to be free during

 * hibernation for allocations made while saving the image and for device

 * drivers, in case they need to allocate memory from their hibernation

 * callbacks (these two numbers are given by PAGES_FOR_IO (which is a rough

 * estimate) and reserved_size divided by PAGE_SIZE (which is tunable through

 * /sys/power/reserved_size, respectively).  To make this happen, we compute the

 * total number of available page frames and allocate at least

 *

 * ([page frames total] + PAGES_FOR_IO + [metadata pages]) / 2

 *  + 2 * DIV_ROUND_UP(reserved_size, PAGE_SIZE)

 *

 * of them, which corresponds to the maximum size of a hibernation image.

 *

 * If image_size is set below the number following from the above formula,

 * the preallocation of memory is continued until the total number of saveable

 * pages in the system is below the requested image size or the minimum

 * acceptable image size returned by minimum_image_size(), whichever is greater.

 Count the number of saveable data pages. */

	/*

	 * Compute the total number of page frames we can use (count) and the

	 * number of pages needed for image metadata (size).

 Compute the maximum number of saveable pages to leave in memory. */

 Compute the desired number of image pages specified by image_size. */

	/*

	 * If the desired number of image pages is at least as large as the

	 * current number of saveable pages in memory, allocate page frames for

	 * the image and we're done.

 Estimate the minimum size of the image. */

	/*

	 * To avoid excessive pressure on the normal zone, leave room in it to

	 * accommodate an image of the minimum size (unless it's already too

	 * small, in which case don't preallocate pages from it at all).

	/*

	 * Let the memory management subsystem know that we're going to need a

	 * large number of page frames to allocate and make it free some memory.

	 * NOTE: If this is not done, performance will be hurt badly in some

	 * test cases.

	/*

	 * The number of saveable pages in memory was too high, so apply some

	 * pressure to decrease it.  First, make room for the largest possible

	 * image and fail if that doesn't work.  Next, try to decrease the size

	 * of the image as much as indicated by 'size' using allocations from

	 * highmem and non-highmem zones separately.

 We have exhausted non-highmem pages, try highmem. */

		/*

		 * size is the desired number of saveable pages to leave in

		 * memory, so try to preallocate (all memory - size) pages.

		/*

		 * There are approximately max_size saveable pages at this point

		 * and we want to reduce this number down to size.

	/*

	 * We only need as many page frames for the image as there are saveable

	 * pages in memory, but we have allocated more.  Release the excessive

	 * ones now.

/**

 * count_pages_for_highmem - Count non-highmem pages needed for copying highmem.

 *

 * Compute the number of non-highmem pages that will be necessary for creating

 * copies of highmem pages.

 CONFIG_HIGHMEM */

/**

 * enough_free_mem - Check if there is enough free memory for the image.

/**

 * get_highmem_buffer - Allocate a buffer for highmem pages.

 *

 * If there are some highmem pages in the hibernation image, we may need a

 * buffer to copy them and/or load their data.

/**

 * alloc_highmem_image_pages - Allocate some highmem pages for the image.

 *

 * Try to allocate as many pages as needed, but if the number of free highmem

 * pages is less than that, allocate them all.

 CONFIG_HIGHMEM */

/**

 * swsusp_alloc - Allocate memory for hibernation image.

 *

 * We first try to allocate as many highmem pages as there are

 * saveable highmem pages in the system.  If that fails, we allocate

 * non-highmem pages for the copies of the remaining highmem ones.

 *

 * In this approach it is likely that the copies of highmem pages will

 * also be located in the high memory, because of the way in which

 * copy_data_pages() works.

	/*

	 * During allocating of suspend pagedir, new cold pages may appear.

	 * Kill them.

	/*

	 * End of critical section. From now on, we can write to memory,

	 * but we should not touch disk. This specially means we must _not_

	 * touch swap space! Except we must write out our image of course.

 CONFIG_ARCH_HIBERNATION_HEADER */

/**

 * pack_pfns - Prepare PFNs for saving.

 * @bm: Memory bitmap.

 * @buf: Memory buffer to store the PFNs in.

 *

 * PFNs corresponding to set bits in @bm are stored in the area of memory

 * pointed to by @buf (1 page at a time).

/**

 * snapshot_read_next - Get the address to read the next image page from.

 * @handle: Snapshot handle to be used for the reading.

 *

 * On the first call, @handle should point to a zeroed snapshot_handle

 * structure.  The structure gets populated then and a pointer to it should be

 * passed to this function every next time.

 *

 * On success, the function returns a positive number.  Then, the caller

 * is allowed to read up to the returned number of bytes from the memory

 * location computed by the data_of() macro.

 *

 * The function returns 0 to indicate the end of the data stream condition,

 * and negative numbers are returned on errors.  If that happens, the structure

 * pointed to by @handle is not updated and should not be used any more.

 This makes the buffer be freed by swsusp_free() */

			/*

			 * Highmem pages are copied to the buffer,

			 * because we can't return with a kmapped

			 * highmem page (we may not be called again).

/**

 * mark_unsafe_pages - Mark pages that were used before hibernation.

 *

 * Mark the pages that cannot be used for storing the image during restoration,

 * because they conflict with the pages that had been used before hibernation.

 Clear the "free"/"unsafe" bit for all PFNs */

 Mark pages that correspond to the "original" PFNs as "unsafe" */

/**

 * load header - Check the image header and copy the data from it.

/**

 * unpack_orig_pfns - Set bits corresponding to given PFNs in a memory bitmap.

 * @bm: Memory bitmap.

 * @buf: Area of memory containing the PFNs.

 *

 * For each element of the array pointed to by @buf (1 page at a time), set the

 * corresponding bit in @bm.

/*

 * struct highmem_pbe is used for creating the list of highmem pages that

 * should be restored atomically during the resume from disk, because the page

 * frames they have occupied before the suspend are in use.

 data is here now */

 data was here before the suspend */

/*

 * List of highmem PBEs needed for restoring the highmem pages that were

 * allocated before the suspend and included in the suspend image, but have

 * also been allocated by the "resume" kernel, so their contents cannot be

 * written directly to their "original" page frames.

/**

 * count_highmem_image_pages - Compute the number of highmem pages in the image.

 * @bm: Memory bitmap.

 *

 * The bits in @bm that correspond to image pages are assumed to be set.

/**

 * prepare_highmem_image - Allocate memory for loading highmem data from image.

 * @bm: Pointer to an uninitialized memory bitmap structure.

 * @nr_highmem_p: Pointer to the number of highmem image pages.

 *

 * Try to allocate as many highmem pages as there are highmem image pages

 * (@nr_highmem_p points to the variable containing the number of highmem image

 * pages).  The pages that are "safe" (ie. will not be overwritten when the

 * hibernation image is restored entirely) have the corresponding bits set in

 * @bm (it must be uninitialized).

 *

 * NOTE: This function should not be called if there are no highmem image pages.

 The page is "safe", set its bit the bitmap */

 Mark the page as allocated */

/**

 * get_highmem_page_buffer - Prepare a buffer to store a highmem image page.

 *

 * For a given highmem image page get a buffer that suspend_write_next() should

 * return to its caller to write to.

 *

 * If the page is to be saved to its "original" page frame or a copy of

 * the page is to be made in the highmem, @buffer is returned.  Otherwise,

 * the copy of the page is to be made in normal memory, so the address of

 * the copy is returned.

 *

 * If @buffer is returned, the caller of suspend_write_next() will write

 * the page's contents to @buffer, so they will have to be copied to the

 * right location on the next call to suspend_write_next() and it is done

 * with the help of copy_last_highmem_page().  For this purpose, if

 * @buffer is returned, @last_highmem_page is set to the page to which

 * the data will have to be copied from @buffer.

		/*

		 * We have allocated the "original" page frame and we can

		 * use it directly to store the loaded page.

	/*

	 * The "original" page frame has not been allocated and we have to

	 * use a "safe" page frame to store the loaded page.

 Copy of the page will be stored in high memory */

 Copy of the page will be stored in normal memory */

/**

 * copy_last_highmem_page - Copy most the most recent highmem image page.

 *

 * Copy the contents of a highmem image from @buffer, where the caller of

 * snapshot_write_next() has stored them, to the right location represented by

 * @last_highmem_page .

 CONFIG_HIGHMEM */

/**

 * prepare_image - Make room for loading hibernation image.

 * @new_bm: Uninitialized memory bitmap structure.

 * @bm: Memory bitmap with unsafe pages marked.

 *

 * Use @bm to mark the pages that will be overwritten in the process of

 * restoring the system memory state from the suspend image ("unsafe" pages)

 * and allocate memory for the image.

 *

 * The idea is to allocate a new memory bitmap first and then allocate

 * as many pages as needed for image data, but without specifying what those

 * pages will be used for just yet.  Instead, we mark them all as allocated and

 * create a lists of "safe" pages to be used later.  On systems with high

 * memory a list of "safe" highmem pages is created too.

 If there is no highmem, the buffer will not be necessary */

	/*

	 * Reserve some safe pages for potential later use.

	 *

	 * NOTE: This way we make sure there will be enough safe pages for the

	 * chain_alloc() in get_buffer().  It is a bit wasteful, but

	 * nr_copy_pages cannot be greater than 50% of the memory anyway.

	 *

	 * nr_copy_pages cannot be less than allocated_unsafe_pages too.

 Preallocate memory for the image */

 The page is "safe", add it to the list */

 Mark the page as allocated */

/**

 * get_buffer - Get the address to store the next image data page.

 *

 * Get the address that snapshot_write_next() should return to its caller to

 * write to.

		/*

		 * We have allocated the "original" page frame and we can

		 * use it directly to store the loaded page.

	/*

	 * The "original" page frame has not been allocated and we have to

	 * use a "safe" page frame to store the loaded page.

/**

 * snapshot_write_next - Get the address to store the next image page.

 * @handle: Snapshot handle structure to guide the writing.

 *

 * On the first call, @handle should point to a zeroed snapshot_handle

 * structure.  The structure gets populated then and a pointer to it should be

 * passed to this function every next time.

 *

 * On success, the function returns a positive number.  Then, the caller

 * is allowed to write up to the returned number of bytes to the memory

 * location computed by the data_of() macro.

 *

 * The function returns 0 to indicate the "end of file" condition.  Negative

 * numbers are returned on errors, in which cases the structure pointed to by

 * @handle is not updated and should not be used any more.

 Check if we have already loaded the entire image */

 This makes the buffer be freed by swsusp_free() */

/**

 * snapshot_write_finalize - Complete the loading of a hibernation image.

 *

 * Must be called after the last call to snapshot_write_next() in case the last

 * page in the image happens to be a highmem page and its contents should be

 * stored in highmem.  Additionally, it recycles bitmap memory that's not

 * necessary any more.

 Do that only if we have loaded the image entirely */

 Assumes that @buf is ready and points to a "safe" page */

/**

 * restore_highmem - Put highmem image pages into their original locations.

 *

 * For each highmem page that was in use before hibernation and is included in

 * the image, and also has been allocated by the "restore" kernel, swap its

 * current contents with the previous (ie. "before hibernation") ones.

 *

 * If the restore eventually fails, we can call this function once again and

 * restore the highmem state as seen by the restore kernel.

 CONFIG_HIGHMEM */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kernel/power/suspend.c - Suspend to RAM and standby functionality.

 *

 * Copyright (c) 2003 Patrick Mochel

 * Copyright (c) 2003 Open Source Development Lab

 * Copyright (c) 2009 Rafael J. Wysocki <rjw@sisk.pl>, Novell Inc.

/**

 * pm_suspend_default_s2idle - Check if suspend-to-idle is the default suspend.

 *

 * Return 'true' if suspend-to-idle has been selected as the default system

 * suspend method.

 Push all the CPUs into the idle loop. */

 Make the current CPU wait so it can enter the idle loop too. */

	/*

	 * Suspend-to-idle equals:

	 * frozen processes + suspended devices + idle processors.

	 * Thus s2idle_enter() should be called right after all devices have

	 * been suspended.

	 *

	 * Wakeups during the noirq suspend of devices may be spurious, so try

	 * to avoid them upfront.

	/*

	 * The PM_SUSPEND_STANDBY and PM_SUSPEND_MEM states require low-level

	 * support and need to be valid to the low-level implementation.

	 *

	 * No ->valid() or ->enter() callback implies that none are valid.

 "mem" and "freeze" are always present in /sys/power/state. */

	/*

	 * Suspend-to-idle should be supported even without any suspend_ops,

	 * initialize mem_sleep_states[] accordingly here.

/**

 * suspend_set_ops - Set the global suspend method table.

 * @ops: Suspend operations to use.

/**

 * suspend_valid_only_mem - Generic memory-only valid callback.

 * @state: Target system sleep state.

 *

 * Platform drivers that implement mem suspend only and only need to check for

 * that in their .valid() callback can use this instead of rolling their own

 * .valid() callback.

 !CONFIG_PM_DEBUG */

/**

 * suspend_prepare - Prepare for entering system sleep state.

 * @state: Target system sleep state.

 *

 * Common code run for every system sleep state that can be entered (except for

 * hibernation).  Run suspend notifiers, allocate the "suspend" console and

 * freeze processes.

 default implementation */

 default implementation */

/**

 * suspend_enter - Make the system enter the given sleep state.

 * @state: System sleep state to enter.

 * @wakeup: Returns information that the sleep state should not be re-entered.

 *

 * This function should be called after devices have been suspended.

/**

 * suspend_devices_and_enter - Suspend devices and enter system sleep state.

 * @state: System sleep state to enter.

/**

 * suspend_finish - Clean up before finishing the suspend sequence.

 *

 * Call platform code to clean up, restart processes, and free the console that

 * we've allocated. This routine is not called for hibernation.

/**

 * enter_state - Do common work needed to enter system sleep state.

 * @state: System sleep state to enter.

 *

 * Make sure that no one else is trying to put the system into a sleep state.

 * Fail if that's not the case.  Otherwise, prepare for system suspend, make the

 * system enter the given sleep state and clean up after wakeup.

/**

 * pm_suspend - Externally visible function for suspending the system.

 * @state: System sleep state to enter.

 *

 * Check if the value of @state represents one of the supported states,

 * execute enter_state() and update system suspend statistics.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kernel/power/main.c - PM subsystem core functionality.

 *

 * Copyright (c) 2003 Patrick Mochel

 * Copyright (c) 2003 Open Source Development Lab

	/*

	 * Don't use freezer_count() because we don't want the call to

	 * try_to_freeze() here.

	 *

	 * Reason:

	 * Fundamentally, we just don't need it, because freezing condition

	 * doesn't come into effect until we release the

	 * system_transition_mutex lock, since the freezer always works with

	 * system_transition_mutex held.

	 *

	 * More importantly, in the case of hibernation,

	 * unlock_system_sleep() gets called in snapshot_read() and

	 * snapshot_write() when the freezing condition is still in effect.

	 * Which means, if we use try_to_freeze() here, it would make them

	 * enter the refrigerator, thus causing hibernation to lockup.

 Routines for PM-transition notifications */

 If set, devices may be suspended and resumed asynchronously. */

 Convert the last space to a newline if needed. */

/*

 * sync_on_suspend: invoke ksys_sync_helper() before suspend.

 *

 * show() returns whether ksys_sync_helper() is invoked before suspend.

 * store() accepts 0 or 1.  0 disables ksys_sync_helper() and 1 enables it.

 CONFIG_SUSPEND */

 convert the last space to a newline */

 CONFIG_PM_SLEEP_DEBUG */

 CONFIG_DEBUG_FS */

 CONFIG_PM_SLEEP */

/*

 * pm_print_times: print time taken by devices to suspend and resume.

 *

 * show() returns whether printing of suspend and resume times is enabled.

 * store() accepts 0 or 1.  0 disables printing and 1 enables it.

/**

 * __pm_pr_dbg - Print a suspend debug message to the kernel log.

 * @defer: Whether or not to use printk_deferred() to print the message.

 * @fmt: Message format.

 *

 * The message will be emitted if enabled through the pm_debug_messages

 * sysfs attribute.

 !CONFIG_PM_SLEEP_DEBUG */

 CONFIG_PM_SLEEP_DEBUG */

/*

 * state - control system sleep states.

 *

 * show() returns available sleep state labels, which may be "mem", "standby",

 * "freeze" and "disk" (hibernation).

 * See Documentation/admin-guide/pm/sleep-states.rst for a description of

 * what they mean.

 *

 * store() accepts one of those strings, translates it into the proper

 * enumerated value, and initiates a suspend transition.

 convert the last space to a newline */

 Check hibernation first. */

/*

 * The 'wakeup_count' attribute, along with the functions defined in

 * drivers/base/power/wakeup.c, provides a means by which wakeup events can be

 * handled in a non-racy way.

 *

 * If a wakeup event occurs when the system is in a sleep state, it simply is

 * woken up.  In turn, if an event that would wake the system up from a sleep

 * state occurs when it is undergoing a transition to that sleep state, the

 * transition should be aborted.  Moreover, if such an event occurs when the

 * system is in the working state, an attempt to start a transition to the

 * given sleep state should fail during certain period after the detection of

 * the event.  Using the 'state' attribute alone is not sufficient to satisfy

 * these requirements, because a wakeup event may occur exactly when 'state'

 * is being written to and may be delivered to user space right before it is

 * frozen, so the event will remain only partially processed until the system is

 * woken up by another event.  In particular, it won't cause the transition to

 * a sleep state to be aborted.

 *

 * This difficulty may be overcome if user space uses 'wakeup_count' before

 * writing to 'state'.  It first should read from 'wakeup_count' and store

 * the read value.  Then, after carrying out its own preparations for the system

 * transition to a sleep state, it should write the stored value to

 * 'wakeup_count'.  If that fails, at least one wakeup event has occurred since

 * 'wakeup_count' was read and 'state' should not be written to.  Otherwise, it

 * is allowed to write to 'state', but the transition will be aborted if there

 * are any wakeup events detected after 'wakeup_count' was written to.

 CONFIG_PM_AUTOSLEEP */

 CONFIG_PM_WAKELOCKS */

 CONFIG_PM_SLEEP */

 CONFIG_PM_TRACE */

 CONFIG_FREEZER*/

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel Debug Core

 *

 * Maintainer: Jason Wessel <jason.wessel@windriver.com>

 *

 * Copyright (C) 2000-2001 VERITAS Software Corporation.

 * Copyright (C) 2002-2004 Timesys Corporation

 * Copyright (C) 2003-2004 Amit S. Kale <amitkale@linsyssoft.com>

 * Copyright (C) 2004 Pavel Machek <pavel@ucw.cz>

 * Copyright (C) 2004-2006 Tom Rini <trini@kernel.crashing.org>

 * Copyright (C) 2004-2006 LinSysSoft Technologies Pvt. Ltd.

 * Copyright (C) 2005-2009 Wind River Systems, Inc.

 * Copyright (C) 2007 MontaVista Software, Inc.

 * Copyright (C) 2008 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 *

 * Contributors at various stages not listed above:

 *  Jason Wessel ( jason.wessel@windriver.com )

 *  George Anzinger <george@mvista.com>

 *  Anurekh Saxena (anurekh.saxena@timesys.com)

 *  Lake Stevens Instrument Division (Glenn Engel)

 *  Jim Kingdon, Cygnus Support.

 *

 * Original KGDB stub: David Grothe <dave@gcom.com>,

 * Tigran Aivazian <tigran@sco.com>

 kgdb_connected - Is a host GDB connected to us? */

 All the KGDB handlers are installed */

 Guard for recursive entry */

 Action for the reboot notifier, a global allow kdb to change it */

 kgdb console driver is loaded */

 determine if kgdb console output should be used */

 Flag for alternate operations for early debugging */

 Next cpu to become the master debug core */

 Use kdb or gdbserver mode */

/*

 * Holds information about breakpoints in a kernel. These breakpoints are

 * added and removed by gdb.

/*

 * The CPU# of the active CPU, or -1 if none:

/*

 * We use NR_CPUs not PERCPU, in case kgdb is used to debug early

 * bootup code (which might not have percpu set up yet):

 to keep track of the CPU which is doing the single stepping*/

/*

 * If you are debugging a problem where roundup (the collection of

 * all other CPUs) is a problem [this should be extremely rare],

 * then use the nokgdbroundup option to avoid roundup. In that case

 * the other CPUs might interfere with your debugging context, so

 * use this with care:

/*

 * Finally, some KGDB code :-)

/*

 * Weak aliases for breakpoint management,

 * can be overridden by architectures when needed:

	/* Validate setting the breakpoint and then removing it.  If the

	 * remove fails, the kernel needs to emit a bad message because we

	 * are deep trouble not being able to put things back the way we

	 * found them.

/*

 * Default (weak) implementation for kgdb_roundup_cpus

	/*

	 * NOTE: get_irq_regs() is supposed to get the registers from

	 * before the IPI interrupt happened and so is supposed to

	 * show where the processor was.  In some situations it's

	 * possible we might be called without an IPI, so it might be

	 * safer to figure out how to make kgdb_breakpoint() work

	 * properly here.

 No need to roundup ourselves */

		/*

		 * If it didn't round up last time, don't try again

		 * since smp_call_function_single_async() will block.

		 *

		 * If rounding_up is false then we know that the

		 * previous call must have at least started and that

		 * means smp_call_function_single_async() won't block.

/*

 * Some architectures need cache flushes when we set/clear a

 * breakpoint:

 Force flush instruction cache if it was outside the mm */

/*

 * SW breakpoint management:

 Clear memory breakpoints. */

 Clear hardware breakpoints. */

 Clear init memory breakpoints. */

	/*

	 * In general, architectures don't support dumping the stack of a

	 * "running" process that's not the current one.  From the point of

	 * view of the Linux, kernel processes that are looping in the kgdb

	 * slave loop are still "running".  There's also no API (that actually

	 * works across all architectures) that can do a stack crawl based

	 * on registers passed as a parameter.

	 *

	 * Solve this conundrum by asking slave CPUs to do the backtrace

	 * themselves.

/*

 * Return true if there is a valid kgdb I/O module.  Also if no

 * debugger is attached a message can be printed to the console about

 * waiting for the debugger to attach.

 *

 * The print_wait argument is only to be true when called from inside

 * the core kgdb_handle_exception, because it will wait for the

 * debugger to attach.

 Panic on recursive debugger calls: */

	/*

	 * If the break point removed ok at the place exception

	 * occurred, try to recover and print a warning to the end

	 * user because the user planted a breakpoint in a place that

	 * KGDB needs in order to function.

 Allow kdb to debug itself one level */

	/*

	 * Interrupts will be restored by the 'trap return' code, except when

	 * single stepping.

 Make sure the above info reaches the primary CPU */

	/*

	 * CPU will loop if it is a slave or request to become a kgdb

	 * master cpu and acquire the kgdb_active lock:

			/* Return to normal operation by executing any

			 * hw breakpoint fixup.

	/*

	 * For single stepping, try to only enter on the processor

	 * that was single stepping.  To guard against a deadlock, the

	 * kernel will only try for the value of sstep_tries before

	 * giving up and continuing on.

 No I/O connection, resume the system */

	/*

	 * Don't enter if we have hit a removed breakpoint.

 Call the I/O driver's pre_exception routine */

	/*

	 * Get the passive CPU lock which will hold all the non-primary

	 * CPU in a spin state while the debugger is active

 If send_ready set, slaves are already waiting */

 Signal the other CPUs to enter kgdb_wait() */

	/*

	 * Wait for the other CPUs to be notified and be waiting for us:

	/*

	 * At this point the primary processor is completely

	 * in the debugger and all secondary CPUs are quiescent

 Call the I/O driver's post_exception routine */

 Wait till all the CPUs have quit from the debugger. */

 Free kgdb_active */

/*

 * kgdb_handle_exception() - main entry point from a kernel exception

 *

 * Locking hierarchy:

 *	interface locks, if any (begin_session)

 *	kgdb lock (kgdb_active)

	/*

	 * Avoid entering the debugger if we were triggered due to an oops

	 * but panic_timeout indicates the system should automatically

	 * reboot on panic. We don't want to get stuck waiting for input

	 * on such systems, especially if its "just" an oops.

 Ouch, double exception ! */

/*

 * GDB places a breakpoint at this function to know dynamically loaded objects.

	/* If we're debugging, or KGDB has not connected, don't try

	/*

	 * We don't want to get stuck waiting for input from user if

	 * "panic_timeout" indicates the system should automatically

	 * reboot on panic.

	/*

	 * Take the following action on reboot notify depending on value:

	 *    1 == Enter debugger

	 *    0 == [the default] detach debug client

	 *   -1 == Do nothing... and use this until the board resets

	/*

	 * When this routine is called KGDB should unregister from

	 * handlers and clean up, making sure it is not handling any

	 * break exceptions at the time.

/**

 *	kgdb_register_io_module - register KGDB IO module

 *	@new_dbg_io_ops: the io ops vector

 *

 *	Register it with the KGDB core.

 Arm KGDB now. */

/**

 *	kgdb_unregister_io_module - unregister KGDB IO module

 *	@old_dbg_io_ops: the io ops vector

 *

 *	Unregister it with the KGDB core.

	/*

	 * KGDB is no longer able to communicate out, so

	 * unregister our callbacks and reset state.

/**

 * kgdb_breakpoint - generate breakpoint exception

 *

 * This function will generate a breakpoint exception.  It is used at the

 * beginning of a program to sync up with a debugger and can be used

 * otherwise as a quick means to stop program execution and "break" into

 * the debugger.

 Sync point before breakpoint */

 Sync point after breakpoint */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel Debug Core

 *

 * Maintainer: Jason Wessel <jason.wessel@windriver.com>

 *

 * Copyright (C) 2000-2001 VERITAS Software Corporation.

 * Copyright (C) 2002-2004 Timesys Corporation

 * Copyright (C) 2003-2004 Amit S. Kale <amitkale@linsyssoft.com>

 * Copyright (C) 2004 Pavel Machek <pavel@ucw.cz>

 * Copyright (C) 2004-2006 Tom Rini <trini@kernel.crashing.org>

 * Copyright (C) 2004-2006 LinSysSoft Technologies Pvt. Ltd.

 * Copyright (C) 2005-2009 Wind River Systems, Inc.

 * Copyright (C) 2007 MontaVista Software, Inc.

 * Copyright (C) 2008 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 *

 * Contributors at various stages not listed above:

 *  Jason Wessel ( jason.wessel@windriver.com )

 *  George Anzinger <george@mvista.com>

 *  Anurekh Saxena (anurekh.saxena@timesys.com)

 *  Lake Stevens Instrument Division (Glenn Engel)

 *  Jim Kingdon, Cygnus Support.

 *

 * Original KGDB stub: David Grothe <dave@gcom.com>,

 * Tigran Aivazian <tigran@sco.com>

 Our I/O buffers. */

 Storage for the registers, in GDB format. */

/*

 * GDB remote protocol parser:

 poll any additional I/O interfaces that are defined */

 scan for the sequence $<data>#<checksum> */

		/*

		 * Spin and wait around for the start character, ignore all

		 * other characters:

 nothing */;

		/*

		 * now, read until a # or end of buffer is found:

 failed checksum */

 successful transfer */

/*

 * Send the packet in buffer.

 * Check for gdb connection if asked for.

	/*

	 * $<packet info>#<checksum>.

 Now see what we get in reply. */

 If we get an ACK, we are done. */

		/*

		 * If we get the start of another packet, this means

		 * that GDB is attempting to reconnect.  We will NAK

		 * the packet being sent, and stop trying to send this

		 * packet.

 'O'utput */

 Fill and send buffers... */

 Calculate how many this time */

 Pack in hex chars */

 Move up */

 Write packet */

/*

 * Convert the memory pointed to by mem into hex, placing result in

 * buf.  Return a pointer to the last char put in buf (null). May

 * return an error.

	/*

	 * We use the upper half of buf as an intermediate buffer for the

	 * raw memory copy.  Hex conversion will work against this one.

/*

 * Convert the hex array pointed to by buf into binary to be placed in

 * mem.  Return a pointer to the character AFTER the last byte

 * written.  May return an error.

	/*

	 * We use the upper half of buf as an intermediate buffer for the

	 * raw memory that is converted from hex.

/*

 * While we find nice hex chars, build a long_val.

 * Return number of chars processed.

/*

 * Copy the binary array pointed to by buf into mem.  Fix $, #, and

 * 0x7d escaped with 0x7d. Return -EFAULT on failure or 0 on success.

 * The input buf is overwritten with the result to write to mem.

 DBG_MAX_REG_NUM > 0 */

 Write memory due to an 'M' or 'X' packet. */

/*

 * Thread ID accessors. We represent a flat TID space to GDB, where

 * the per CPU idle threads (which under Linux all have PID 0) are

 * remapped to negative TIDs.

	/*

	 * Non-positive TIDs are remapped to the cpu shadow information

	/*

	 * find_task_by_pid_ns() does not take the tasklist lock anymore

	 * but is nicely RCU locked - hence is a pretty resilient

	 * thing to use:

/*

 * Remap normal tasks to their real PID,

 * CPU shadow threads are mapped to -CPU - 2

/*

 * All the functions that start with gdb_cmd are the various

 * operations to implement the handlers for the gdbserial protocol

 * where KGDB is communicating with an external debugger

 Handle the '?' status packets */

	/*

	 * We know that this packet is only sent

	 * during initial connect.  So to be safe,

	 * we clear out our breakpoints now in case

	 * GDB is reconnecting.

			/*

			 * Try to find the task on some other

			 * or possibly this node if we do not

			 * find the matching task then we try

			 * to approximate the results.

	/*

	 * All threads that don't have debuggerinfo should be

	 * in schedule() sleeping, since all other CPUs

	 * are in kgdb_wait, and thus have debuggerinfo.

		/*

		 * Pull stuff saved during switch_to; nothing

		 * else is accessible (or even particularly

		 * relevant).

		 *

		 * This should be enough for a stack trace.

 Handle the 'g' get registers request */

 Handle the 'G' set registers request */

 Handle the 'm' memory read bytes */

 Handle the 'M' memory write bytes */

 Handle the 'p' individual register get */

 Handle the 'P' individual register set */

 DBG_MAX_REG_NUM > 0 */

 Handle the 'X' memory binary write bytes */

 Handle the 'D' or 'k', detach or kill packets */

 The detach case */

		/*

		 * Assume the kill case, with no exit code checking,

		 * trying to force detach the debugger:

 Handle the 'R' reboot packets */

 For now, only honor R0 */

		/*

		 * Execution should not return from

		 * machine_emergency_restart()

 Handle the 'q' query packets */

 Each cpu is a shadow thread */

 Current thread id */

 Handle the 'H' task query packets */

 Handle the 'T' thread query packets */

 Handle the 'z' or 'Z' breakpoint remove or set packets */

	/*

	 * Since GDB-5.3, it's been drafted that '0' is a software

	 * breakpoint, '1' is a hardware breakpoint, so let's do that.

 Unsupported */

 Unsupported. */

	/*

	 * Test if this is a hardware breakpoint, and

	 * if we support it:

 Unsupported. */

 Handle the 'C' signal / exception passing packets */

	/* C09 == pass exception

	 * C15 == detach kgdb, pass exception

 Indicate fall through */

/*

 * This function performs all gdbserial command processing

 Initialize comm buffer and globals. */

 Reply to host that an exception has occurred */

 Clear the out buffer. */

 gdbserial status */

 return the value of the CPU registers */

 set the value of the CPU registers - return OK */

 mAA..AA,LLLL  Read LLLL bytes at address AA..AA */

 MAA..AA,LLLL: Write LLLL bytes at address AA..AA */

 pXX Return gdb register XX (in hex) */

 PXX=aaaa Set gdb register XX to aaaa (in hex) */

 DBG_MAX_REG_NUM > 0 */

 XAA..AA,LLLL: Write LLLL bytes at address AA..AA */

			/* kill or detach. KGDB should treat this like a

			 * continue.

 Debugger detach */

 Debugger detach via kill */

 Reboot */

 query command */

 task related */

 Query thread status */

 Break point remove */

 Break point set */

 Escape into back into kdb */

 Exception passing */

 on tmp < 0 */

 Continue packet */

 Single step packet */

 Can't switch threads in kgdb */

 to default processing */

			/*

			 * Leave cmd processing on error, detach,

			 * kill, continue, or single step.

 reply to the request */

/**

 * gdbstub_exit - Send an exit message to GDB

 * @status: The exit code to report.

 make sure the output is flushed, lest the bootloader clobber it */

/*

 * Kernel Debugger Architecture Independent Support Functions

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (c) 1999-2004 Silicon Graphics, Inc.  All Rights Reserved.

 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved.

 * 03/02/13    added new 2.5 kallsyms <xavier.bru@bull.net>

/*

 * kdbgetsymval - Return the address of the given symbol.

 *

 * Parameters:

 *	symname	Character string containing symbol name

 *      symtab  Structure to receive results

 * Returns:

 *	0	Symbol not found, symtab zero filled

 *	1	Symbol mapped to module/symbol/section, data in symtab

/**

 * kdbnearsym() - Return the name of the symbol with the nearest address

 *                less than @addr.

 * @addr: Address to check for near symbol

 * @symtab: Structure to receive results

 *

 * WARNING: This function may return a pointer to a single statically

 * allocated buffer (namebuf). kdb's unusual calling context (single

 * threaded, all other CPUs halted) provides us sufficient locking for

 * this to be safe. The only constraint imposed by the static buffer is

 * that the caller must consume any previous reply prior to another call

 * to lookup a new symbol.

 *

 * Note that, strictly speaking, some architectures may re-enter the kdb

 * trap if the system turns out to be very badly damaged and this breaks

 * the single-threaded assumption above. In these circumstances successful

 * continuation and exit from the inner trap is unlikely to work and any

 * user attempting this receives a prominent warning before being allowed

 * to progress. In these circumstances we remain memory safe because

 * namebuf[KSYM_NAME_LEN-1] will never change from '\0' although we do

 * tolerate the possibility of garbled symbol display from the outer kdb

 * trap.

 *

 * Return:

 * * 0 - No sections contain this address, symtab zero filled

 * * 1 - Address mapped to module/symbol/section, data in symtab

/*

 * kallsyms_symbol_complete

 *

 * Parameters:

 *	prefix_name	prefix of a symbol name to lookup

 *	max_len		maximum length that can be returned

 * Returns:

 *	Number of symbols which match the given prefix.

 * Notes:

 *	prefix_name is changed to contain the longest unique prefix that

 *	starts with this prefix (tab completion).

 Work out the longest name that matches the prefix */

/*

 * kallsyms_symbol_next

 *

 * Parameters:

 *	prefix_name	prefix of a symbol name to lookup

 *	flag	0 means search from the head, 1 means continue search.

 *	buf_size	maximum length that can be written to prefix_name

 *			buffer

 * Returns:

 *	1 if a symbol matches the given prefix.

 *	0 if no string found

/*

 * kdb_symbol_print - Standard method for printing a symbol name and offset.

 * Inputs:

 *	addr	Address to be printed.

 *	symtab	Address of symbol data, if NULL this routine does its

 *		own lookup.

 *	punc	Punctuation for string, bit field.

 * Remarks:

 *	The string and its punctuation is only printed if the address

 *	is inside the kernel, except that the value is always printed

 *	when requested.

/*

 * kdb_strdup - kdb equivalent of strdup, for disasm code.

 * Inputs:

 *	str	The string to duplicate.

 *	type	Flags to kmalloc for the new string.

 * Returns:

 *	Address of the new string, NULL if storage could not be allocated.

 * Remarks:

 *	This is not in lib/string.c because it uses kmalloc which is not

 *	available when string.o is used in boot loaders.

/*

 * kdb_getarea_size - Read an area of data.  The kdb equivalent of

 *	copy_from_user, with kdb messages for invalid addresses.

 * Inputs:

 *	res	Pointer to the area to receive the result.

 *	addr	Address of the area to copy.

 *	size	Size of the area.

 * Returns:

 *	0 for success, < 0 for error.

/*

 * kdb_putarea_size - Write an area of data.  The kdb equivalent of

 *	copy_to_user, with kdb messages for invalid addresses.

 * Inputs:

 *	addr	Address of the area to write to.

 *	res	Pointer to the area holding the data.

 *	size	Size of the area.

 * Returns:

 *	0 for success, < 0 for error.

/*

 * kdb_getphys - Read data from a physical address. Validate the

 * 	address is in range, use kmap_atomic() to get data

 * 	similar to kdb_getarea() - but for phys addresses

 * Inputs:

 * 	res	Pointer to the word to receive the result

 * 	addr	Physical address of the area to copy

 * 	size	Size of the area

 * Returns:

 *	0 for success, < 0 for error.

/*

 * kdb_getphysword

 * Inputs:

 *	word	Pointer to the word to receive the result.

 *	addr	Address of the area to copy.

 *	size	Size of the area.

 * Returns:

 *	0 for success, < 0 for error.

 Default value if addr or size is invalid */

/*

 * kdb_getword - Read a binary value.  Unlike kdb_getarea, this treats

 *	data as numbers.

 * Inputs:

 *	word	Pointer to the word to receive the result.

 *	addr	Address of the area to copy.

 *	size	Size of the area.

 * Returns:

 *	0 for success, < 0 for error.

 Default value if addr or size is invalid */

/*

 * kdb_putword - Write a binary value.  Unlike kdb_putarea, this

 *	treats data as numbers.

 * Inputs:

 *	addr	Address of the area to write to..

 *	word	The value to set.

 *	size	Size of the area.

 * Returns:

 *	0 for success, < 0 for error.

/*

 * kdb_task_state_char - Return the character that represents the task state.

 * Inputs:

 *	p	struct task for the process

 * Returns:

 *	One character to represent the task state.

		/* Idle task.  Is it really idle, apart from the kdb

 idle task */

 sleeping system daemon */

/*

 * kdb_task_state - Return true if a process has the desired state

 *	given by the mask.

 * Inputs:

 *	p	struct task for the process

 *	mask	set of characters used to select processes; both NULL

 *	        and the empty string mean adopt a default filter, which

 *	        is to suppress sleeping system daemons and the idle tasks

 * Returns:

 *	True if the process matches at least one criteria defined by the mask.

	/* If there is no mask, then we will filter code that runs when the

	 * scheduler is idling and any system daemons that are currently

	 * sleeping.

 A is a special case that matches all states */

/* Maintain a small stack of kdb_flags to allow recursion without disturbing

 * the global kdb state.

/*

 * Kernel Debugger Architecture Independent Stack Traceback

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (c) 1999-2004 Silicon Graphics, Inc.  All Rights Reserved.

 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved.

/*

 * kdb_bt

 *

 *	This function implements the 'bt' command.  Print a stack

 *	traceback.

 *

 *	bt [<address-expression>]	(addr-exp is for alternate stacks)

 *	btp <pid>			Kernel stack for <pid>

 *	btt <address-expression>	Kernel stack for task structure at

 *					<address-expression>

 *	bta [state_chars>|A]		All useful processes, optionally

 *					filtered by state

 *	btc [<cpu>]			The current process on one cpu,

 *					default is all cpus

 *

 *	bt <address-expression> refers to a address on the stack, that location

 *	is assumed to contain a return address.

 *

 *	btt <address-expression> refers to the address of a struct task.

 *

 * Inputs:

 *	argc	argument count

 *	argv	argument vector

 * Outputs:

 *	None.

 * Returns:

 *	zero for success, a kdb diagnostic if error

 * Locking:

 *	none.

 * Remarks:

 *	Backtrack works best when the code uses frame pointers.  But even

 *	without frame pointers we should get a reasonable trace.

 *

 *	mds comes in handy when examining the stack to do a manual traceback or

 *	to get a starting point for bt <address-expression>.

 reset the pager */

 If a CPU failed to round up we could be here */

 Prompt after each proc in bta */

 Run the active tasks first */

 Now the inactive tasks */

			/*

			 * Recursive use of kdb_parse, do not use argv after

			 * this point.

 NOTREACHED */

/*

 * Kernel Debugger Architecture Dependent Console I/O handler

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.

 *

 * Copyright (c) 1999-2006 Silicon Graphics, Inc.  All Rights Reserved.

 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved.

 Keyboard Controller Registers on normal PCs. */

 Status register (R) */

 Keyboard data register (R/W) */

 Status Register Bits */

 Keyboard output buffer full */

 Mouse output buffer full */

/*

 * Check if the keyboard controller has a keypress for us.

 * Some parts (Enter Release, LED change) are still blocking polled here,

 * but hopefully they are all short.

 CAPS LOCK state (0-off, 1-on) */

 Shift next keypress */

	/*

	 * Fetch the scancode

	/*

	 * Ignore mouse events.

	/*

	 * Ignore release, trigger on make

	 * (except for shift keys, where we want to

	 *  keep the shift state so long as the key is

	 *  held down).

		/*

		 * Next key may use shift table

		/*

		 * Left ctrl key

	/*

	 * Translate scancode

		/*

		 * Toggle caps lock

		/*

		 * Backspace

 Special Key */

 Tab */

 Del */

 Home */

 End */

 Left */

 Up */

 Down */

 Right */

	/*

	 * For Japanese 86/106 keyboards

	 * 	See comment in drivers/char/pc_keyb.c.

	 * 	- Masahiro Adegawa

 printable characters */

 ignore unprintables */

/*

 * Best effort cleanup of ENTER break codes on leaving KDB. Called on

 * exiting KDB, when we know we processed an ENTER or KP ENTER scan

 * code.

	/*

	 * Nothing to clean up, since either

	 * ENTER was never pressed, or has already

	 * gotten cleaned up.

	/*

	 * Enter key. Need to absorb the break code here, lest it gets

	 * leaked out if we exit KDB as the result of processing 'g'.

	 *

	 * This has several interesting implications:

	 * + Need to handle KP ENTER, which has break code 0xe0 0x9c.

	 * + Need to handle repeat ENTER and repeat KP ENTER. Repeats

	 *   only get a break code at the end of the repeated

	 *   sequence. This means we can't propagate the repeated key

	 *   press, and must swallow it away.

	 * + Need to handle possible PS/2 mouse input.

	 * + Need to handle mashed keys.

		/*

		 * Fetch the scancode.

		/*

		 * Skip mouse input.

		/*

		 * If we see 0xe0, this is either a break code for KP

		 * ENTER, or a repeat make for KP ENTER. Either way,

		 * since the second byte is equivalent to an ENTER,

		 * skip the 0xe0 and try again.

		 *

		 * If we see 0x1c, this must be a repeat ENTER or KP

		 * ENTER (and we swallowed 0xe0 before). Try again.

		 *

		 * We can also see make and break codes for other keys

		 * mashed before or after pressing ENTER. Thus, if we

		 * see anything other than 0x9c, we have to try again.

		 *

		 * Note, if you held some key as ENTER was depressed,

		 * that break code would get leaked out.

/*

 * Kernel Debugger Architecture Independent Main Code

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1999-2004 Silicon Graphics, Inc.  All Rights Reserved.

 * Copyright (C) 2000 Stephane Eranian <eranian@hpl.hp.com>

 * Xscale (R) modifications copyright (C) 2003 Intel Corporation.

 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved.

/*

 * Kernel debugger state flags

/*

 * kdb_lock protects updates to kdb_initial_cpu.  Used to

 * single thread processors through the kernel debugger.

 cpu number that owns kdb */

 General KDB state */

 kdb_cmds_head describes the available commands. */

 kdb diagnostic */

 Corresponding message text */

/*

 * Initial environment.   This is all kept static and local to

 * this file.   We don't want to rely on the memory allocation

 * mechanisms in the kernel, so we use a very limited allocate-only

 * heap for new and altered environment variables.  The entire

 * environment is limited to a fixed number of entries (add more

 * to __env[] if required) and a fixed amount of heap (add more to

 * KDB_ENVBUFSIZE if required).

 lines of md output */

/*

 * Check whether the flags of the current command and the permissions

 * of the kdb console has allow a command to be run.

 permissions comes from userspace so needs massaging slightly */

 some commands change group when launched with no arguments */

/*

 * kdbgetenv - This function will return the character string value of

 *	an environment variable.

 * Parameters:

 *	match	A character string representing an environment variable.

 * Returns:

 *	NULL	No environment variable matches 'match'

 *	char*	Pointer to string value of environment variable.

/*

 * kdballocenv - This function is used to allocate bytes for

 *	environment entries.

 * Parameters:

 *	match	A character string representing a numeric value

 * Outputs:

 *	*value  the unsigned long representation of the env variable 'match'

 * Returns:

 *	Zero on success, a kdb diagnostic on failure.

 * Remarks:

 *	We use a static environment buffer (envbuffer) to hold the values

 *	of dynamically generated environment variables (see kdb_set).  Buffer

 *	space once allocated is never free'd, so over time, the amount of space

 *	(currently 512 bytes) will be exhausted if env variables are changed

 *	frequently.

/*

 * kdbgetulenv - This function will return the value of an unsigned

 *	long-valued environment variable.

 * Parameters:

 *	match	A character string representing a numeric value

 * Outputs:

 *	*value  the unsigned long representation of the env variable 'match'

 * Returns:

 *	Zero on success, a kdb diagnostic on failure.

/*

 * kdbgetintenv - This function will return the value of an

 *	integer-valued environment variable.

 * Parameters:

 *	match	A character string representing an integer-valued env variable

 * Outputs:

 *	*value  the integer representation of the environment variable 'match'

 * Returns:

 *	Zero on success, a kdb diagnostic on failure.

/*

 * kdb_setenv() - Alter an existing environment variable or create a new one.

 * @var: Name of the variable

 * @val: Value of the variable

 *

 * Return: Zero on success, a kdb diagnostic on failure.

	/*

	 * Wasn't existing variable.  Fit into slot.

/*

 * kdb_printenv() - Display the current environment variables.

/*

 * kdbgetularg - This function will convert a numeric string into an

 *	unsigned long value.

 * Parameters:

 *	arg	A character string representing a numeric value

 * Outputs:

 *	*value  the unsigned long representation of arg.

 * Returns:

 *	Zero on success, a kdb diagnostic on failure.

		/*

		 * Also try base 16, for us folks too lazy to type the

		 * leading 0x...

/*

 * kdb_set - This function implements the 'set' command.  Alter an

 *	existing environment variable or create a new one.

	/*

	 * we can be invoked two ways:

	 *   set var=value    argv[1]="var", argv[2]="value"

	 *   set var = value  argv[1]="var", argv[2]="=", argv[3]="value"

	 * - if the latter, shift 'em down.

	/*

	 * Censor sensitive variables

	/*

	 * Check for internal variables

	/*

	 * Tokenizer squashed the '=' sign.  argv[1] is variable

	 * name, argv[2] = value.

/*

 * kdbgetaddrarg - This function is responsible for parsing an

 *	address-expression and returning the value of the expression,

 *	symbol name, and offset to the caller.

 *

 *	The argument may consist of a numeric value (decimal or

 *	hexadecimal), a symbol name, a register name (preceded by the

 *	percent sign), an environment variable with a numeric value

 *	(preceded by a dollar sign) or a simple arithmetic expression

 *	consisting of a symbol name, +/-, and a numeric constant value

 *	(offset).

 * Parameters:

 *	argc	- count of arguments in argv

 *	argv	- argument vector

 *	*nextarg - index to next unparsed argument in argv[]

 *	regs	- Register state at time of KDB entry

 * Outputs:

 *	*value	- receives the value of the address-expression

 *	*offset - receives the offset specified, if any

 *	*name   - receives the symbol name, if any

 *	*nextarg - index to next unparsed argument in argv[]

 * Returns:

 *	zero is returned on success, a kdb diagnostic code is

 *      returned on error.

	/*

	 * If the enable flags prohibit both arbitrary memory access

	 * and flow control then there are no reasonable grounds to

	 * provide symbol lookup.

	/*

	 * Process arguments which follow the following syntax:

	 *

	 *  symbol | numeric-address [+/- numeric-offset]

	 *  %register

	 *  $environment-variable

	/*

	 * If there is no whitespace between the symbol

	 * or address and the '+' or '-' symbols, we

	 * remember the character and replace it with a

	 * null so the symbol/value can be properly parsed

		/* Implement register values with % at a later time as it is

		 * arch optional.

	/*

	 * check for +/- and offset

			/*

			 * Not our argument.  Return.

	/*

	 * Now there must be an offset!

/*

 * kdb_defcmd, kdb_defcmd2 - This function implements the 'defcmd'

 *	command which defines one command as a set of other commands,

 *	terminated by endefcmd.  kdb_defcmd processes the initial

 *	'defcmd' command, kdb_defcmd2 is invoked from kdb_parse for

 *	the following commands until 'endefcmd'.

 * Inputs:

 *	argc	argument count

 *	argv	argument vector

 * Returns:

 *	zero for success, a kdb diagnostic if error

 Macro command */

 Associated statement list */

 Statement text */

 Statement list node */

 Forward references */

/*

 * kdb_exec_defcmd - Execute the set of commands associated with this

 *	defcmd name.

 * Inputs:

 *	argc	argument count

 *	argv	argument vector

 * Returns:

 *	zero for success, a kdb diagnostic if error

		/*

		 * Recursive use of kdb_parse, do not use argv after this point.

 Command history */

#define CMD_BUFLEN		200	/* kdb_printf: max printline

/*

 * The "str" argument may point to something like  | grep xyz

 sanity check: we should have been called with the \ first */

 remove the trailing newline */

 now cp points to a nonzero length search string */

		/* allow it be "x y z" by removing the "'s - there must

 end the string where the 2nd " was */

/*

 * kdb_parse - Parse the command line, search the command table for a

 *	matching command and invoke the command function.  This

 *	function may be called recursively, if it is, the second call

 *	will overwrite argv and cbuf.  It is the caller's

 *	responsibility to save their argv if they recursively call

 *	kdb_parse().

 * Parameters:

 *      cmdstr	The input command line to be parsed.

 *	regs	The registers at the time kdb was entered.

 * Returns:

 *	Zero for success, a kdb diagnostic if failure.

 * Remarks:

 *	Limited to 20 tokens.

 *

 *	Real rudimentary tokenization. Basically only whitespace

 *	is considered a token delimiter (but special consideration

 *	is taken of the '=' sign as used by the 'set' command).

 *

 *	The algorithm used to tokenize the input string relies on

 *	there being at least one whitespace (or otherwise useless)

 *	character between tokens as the character immediately following

 *	the token is altered in-place to a null-byte to terminate the

 *	token string.

	/*

	 * First tokenize the command string.

		/* Previous command was interrupted, newline must not

 no repeat */

 skip whitespace */

 special case: check for | grep pattern */

			/* Copy to next unquoted and unescaped

 Squash a ws or '=' character */

 avoid repeat on endefcmd */

		/*

		 * If this command is allowed to be abbreviated,

		 * check to see if this is it.

	/*

	 * If we don't find a command by this name, see if the first

	 * few characters of this match any of the known commands.

	 * e.g., md1c20 should match md.

	/*

	 * If the input with which we were presented does not

	 * map to an existing command, attempt to parse it as an

	 * address argument and display the result.   Useful for

	 * obtaining the address of a variable, or the nearest symbol

	 * to an address contained in a register.

 initial situation */

/*

 * kdb_reboot - This function implements the 'reboot' command.  Reboot

 *	the system immediately, or loop for ever on failure.

 NOTREACHED */

/*

 * kdb_local - The main code for kdb.  This routine is invoked on a

 *	specific processor, it is not global.  The main kdb() routine

 *	ensures that only one processor at a time is in this routine.

 *	This code is called with the real reason code on the first

 *	entry to a kdb session, thereafter it is called with reason

 *	SWITCH, even if the user goes back to the original cpu.

 * Inputs:

 *	reason		The reason KDB was invoked

 *	error		The hardware-defined error code

 *	regs		The exception frame at time of fault/breakpoint.

 *	db_result	Result code from the break or debug point.

 * Returns:

 *	0	KDB was invoked for an event which it wasn't responsible

 *	1	KDB handled the event for which it was invoked.

 *	KDB_CMD_GO	User typed 'go'.

 *	KDB_CMD_CPU	User switched to another cpu.

 *	KDB_CMD_SS	Single step.

 special case below */

		/*

		 * If re-entering kdb after a single step

		 * command, don't print the message.

 kdba_db_trap did the work */

 drop through, slaves only get released via cpu switch */

		/*

		 * Determine if this breakpoint is one that we

		 * are interested in.

 Not for us, dismiss it */

 Not for us, dismiss it */

		/*

		 * Initialize pager context.

 ensure the old search does not leak into '/' commands */

 PROMPT can only be set if we have MEM_READ permission. */

		/*

		 * Fetch command from keyboard

/*

 * kdb_print_state - Print the state data for the current processor

 *	for debugging.

 * Inputs:

 *	text		Identifies the debug point

 *	value		Any integer value to be printed, e.g. reason code.

/*

 * kdb_main_loop - After initial setup and assignment of the

 *	controlling cpu, all cpus are in this loop.  One cpu is in

 *	control and will issue the kdb prompt, the others will spin

 *	until 'go' or cpu switch.

 *

 *	To get a consistent view of the kernel stacks for all

 *	processes, this routine is invoked from the main kdb code via

 *	an architecture specific routine.  kdba_main_loop is

 *	responsible for making the kernel stacks consistent for all

 *	processes, there should be no difference between a blocked

 *	process and a running process as far as kdb is concerned.

 * Inputs:

 *	reason		The reason KDB was invoked

 *	error		The hardware-defined error code

 *	reason2		kdb's current reason code.

 *			Initially error but can change

 *			according to kdb state.

 *	db_result	Result code from break or debug point.

 *	regs		The exception frame at time of fault/breakpoint.

 *			should always be valid.

 * Returns:

 *	0	KDB was invoked for an event which it wasn't responsible

 *	1	KDB handled the event for which it was invoked.

 Stay in kdb() until 'go', 'ss[b]' or an error */

		/*

		 * All processors except the one that is in control

		 * will spin here.

			/* state KDB is turned off by kdb_cpu to see if the

			 * other cpus are still live, each cpu in this loop

			 * turns it back on.

 Another cpu said 'go' */

 Still using kdb, this processor is in control */

 Clean up any keyboard devices before leaving */

/*

 * kdb_mdr - This function implements the guts of the 'mdr', memory

 * read command.

 *	mdr  <addr arg>,<byte count>

 * Inputs:

 *	addr	Start address

 *	count	Number of bytes

 * Returns:

 *	Always 0.  Any errors are detected and printed by kdb_getarea.

/*

 * kdb_md - This function implements the 'md', 'md1', 'md2', 'md4',

 *	'md8' 'mdr' and 'mds' commands.

 *

 *	md|mds  [<addr arg> [<line count> [<radix>]]]

 *	mdWcN	[<addr arg> [<line count> [<radix>]]]

 *		where W = is the width (1, 2, 4 or 8) and N is the count.

 *		for eg., md1c20 reads 20 bytes, 1 at a time.

 *	mdr  <addr arg>,<byte count>

 print just one line of data */

 Assume 'md <addr>' and start with environment values */

 to make REPEAT happy

		/* Do not save these changes as last_*, they are temporary mds

		 * overrides.

 Round address down modulo BYTESPERWORD */

/*

 * kdb_mm - This function implements the 'mm' command.

 *	mm address-expression new-value

 * Remarks:

 *	mm works on machine words, mmW works on bytes.

/*

 * kdb_go - This function implements the 'go' command.

 *	go [address-expression]

/*

 * kdb_rd - This function implements the 'rd' command.

/*

 * kdb_rm - This function implements the 'rm' (register modify)  command.

 *	rm register-name new-contents

 * Remarks:

 *	Allows register modification with the same restrictions as gdb

	/*

	 * Allow presence or absence of leading '%' symbol.

/*

 * kdb_sr - This function implements the 'sr' (SYSRQ key) command

 *	which interfaces to the soi-disant MAGIC SYSRQ functionality.

 *		sr <magic-sysrq-code>

 CONFIG_MAGIC_SYSRQ */

/*

 * kdb_ef - This function implements the 'regs' (display exception

 *	frame) command.  This command takes an address and expects to

 *	find an exception frame at that address, formats and prints

 *	it.

 *		regs address-expression

 * Remarks:

 *	Not done yet.

/*

 * kdb_lsmod - This function implements the 'lsmod' command.  Lists

 *	currently loaded kernel modules.

 *	Mostly taken from userland lsmod.

 CONFIG_MODULES */

/*

 * kdb_env - This function implements the 'env' command.  Display the

 *	current environment variables.

/*

 * kdb_dmesg - This function implements the 'dmesg' command to display

 *	the contents of the syslog buffer.

 *		dmesg [lines] [adjust]

 disable LOGGING if set */

 CONFIG_PRINTK */

 Make sure we balance enable/disable calls, must disable first. */

/*

 * kdb_cpu - This function implements the 'cpu' command.

 *	cpu	[<cpunum>]

 * Returns:

 *	KDB_CMD_CPU for success, a kdb diagnostic if error

 cpu is offline */

 cpu is online but unresponsive */

 cpu is responding to kdb */

 idle task */

 print the trailing cpus, ignoring them if they are all offline */

	/*

	 * Validate cpunum

	/*

	 * Switch to other cpu

/* The user may not realize that ps/bta with no parameters does not print idle

 * or sleeping system daemon processes, so tell them how many were suppressed.

/*

 * kdb_ps - This function implements the 'ps' command which shows a

 *	    list of the active processes.

 *

 * ps [<state_chars>]   Show processes, optionally selecting only those whose

 *                      state character is found in <state_chars>.

 Run the active tasks first */

 Now the real tasks */

/*

 * kdb_pid - This function implements the 'pid' command which switches

 *	the currently active process.

 *		pid [<pid> | R]

/*

 * kdb_help - This function implements the 'help' and '?' commands.

/*

 * kdb_kill - This function implements the 'kill' commands.

 Find the process. */

/*

 * Most of this code has been lifted from kernel/timer.c::sys_sysinfo().

 * I cannot call that code directly from kdb, it has an unconditional

 * cli()/sti() and calls routines that take locks which can stop the debugger.

/*

 * kdb_summary - This function implements the 'summary' command.

 Display in kilobytes */

/*

 * kdb_per_cpu - This function implements the 'per_cpu' command.

	/* Most architectures use __per_cpu_offset[cpu], some use

	 * __per_cpu_offset(cpu), smp has no __per_cpu_offset.

/*

 * display help for the use of cmd | grep pattern

/**

 * kdb_register() - This function is used to register a kernel debugger

 *                  command.

 * @cmd: pointer to kdb command

 *

 * Note that it's the job of the caller to keep the memory for the cmd

 * allocated until unregister is called.

/**

 * kdb_register_table() - This function is used to register a kdb command

 *                        table.

 * @kp: pointer to kdb command table

 * @len: length of kdb command table

/**

 * kdb_unregister() - This function is used to unregister a kernel debugger

 *                    command. It is generally called when a module which

 *                    implements kdb command is unloaded.

 * @cmd: pointer to kdb command

		/*

		 * Macros are always safe because when executed each

		 * internal command re-enters kdb_parse() and is safety

		 * checked individually.

 Initialize the kdb command table. */

 Execute any commands defined in kdb_cmds.  */

 Initialize kdb_printf, breakpoint tables and kdb state */

 Initialize Command Table */

 Initialize Breakpoints */

 Build kdb_cmds tables */

/*

 * Created by: Jason Wessel <jason.wessel@windriver.com>

 *

 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved.

 *

 * This file is licensed under the terms of the GNU General Public

 * License version 2. This program is licensed "as is" without any

 * warranty of any kind, whether express or implied.

/*

 * KDB interface to KGDB internals

	/*

	 * SSBPT is set when the kernel debugger must single step a

	 * task in order to re-establish an instruction breakpoint

	 * which uses the instruction replacement mechanism.  It is

	 * cleared by any action that removes the need to single-step

	 * the breakpoint.

 Set initial kdb state variables */

 Remove any breakpoints as needed by kdb and clear single step */

 set CATASTROPHIC if the system contains unresponsive processors */

 Start kdb main loop */

	/*

	 * Upon exit from the kdb main loop setup break points and restart

	 * the system based on the requested continue state

 Set the exit state to a single step or a continue */

 Invoke arch specific exception handling prior to system resume */

		/*

		 * Force clear the single step bit because kdb emulates this

		 * differently vs the gdbstub

/*

 * Kernel Debugger Architecture Independent Console I/O handler

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (c) 1999-2006 Silicon Graphics, Inc.  All Rights Reserved.

 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved.

/**

 * kdb_handle_escape() - validity check on an accumulated escape sequence.

 * @buf:	Accumulated escape characters to be examined. Note that buf

 *		is not a string, it is an array of characters and need not be

 *		nil terminated.

 * @sz:		Number of accumulated escape characters.

 *

 * Return: -1 if the escape sequence is unwanted, 0 if it is incomplete,

 * otherwise it returns a mapped key value to pass to the upper layers.

 \e<something> */

 \e[A, up arrow */

 \e[B, down arrow */

 \e[C, right arrow */

 \e[D, left arrow */

 \e[<1,3,4>], may be home, del, end */

 \e[1~, home */

 \e[3~, del */

 \e[4~, end */

/**

 * kdb_getchar() - Read a single character from a kdb console (or consoles).

 *

 * Other than polling the various consoles that are currently enabled,

 * most of the work done in this function is dealing with escape sequences.

 *

 * An escape key could be the start of a vt100 control sequence such as \e[D

 * (left arrow) or it could be a character in its own right.  The standard

 * method for detecting the difference is to wait for 2 seconds to see if there

 * are any other characters.  kdb is complicated by the lack of a timer service

 * (interrupts are off), by multiple input sources. Escape sequence processing

 * has to be done as states in the polling loop.

 *

 * Return: The key pressed or a control code derived from an escape sequence.

 2 seconds worth of udelays */

 longest vt100 escape sequence is 4 bytes */

 Reset NMI watchdog once per poll loop */

		/*

		 * When the first character is received (or we get a change

		 * input source) we set ourselves up to handle an escape

		 * sequences (just in case).

 no escape sequence; return best character */

/*

 * kdb_read

 *

 *	This function reads a string of characters, terminated by

 *	a newline, or by reaching the end of the supplied buffer,

 *	from the current kernel debugger console device.

 * Parameters:

 *	buffer	- Address of character buffer to receive input characters.

 *	bufsize - size, in bytes, of the character buffer

 * Returns:

 *	Returns a pointer to the buffer containing the received

 *	character string.  This string will be terminated by a

 *	newline character.

 * Locking:

 *	No locks are required to be held upon entry to this

 *	function.  It is not reentrant - it relies on the fact

 *	that while kdb is running on only one "master debug" cpu.

 * Remarks:

 *	The buffer size must be >= 2.

	char *bufend = buffer+bufsize-2;	/* Reserve space for newline

 backspace */

 enter */

 Del */

 Home */

 End */

 Left */

 Down */

 Right */

 Up */

 Tab */

 reset output line number */

				/* The kgdb transition check will hide

				 * printed characters if we think that

				 * kgdb is connecting, until the check

 Special escape to kgdb */

/*

 * kdb_getstr

 *

 *	Print the prompt string and read a command from the

 *	input device.

 *

 * Parameters:

 *	buffer	Address of buffer to receive command

 *	bufsize Size of buffer in bytes

 *	prompt	Pointer to string to use as prompt string

 * Returns:

 *	Pointer to command buffer.

 * Locking:

 *	None.

 * Remarks:

 *	For SMP kernels, the processor number will be

 *	substituted for %d, %x or %o in the prompt.

 Prompt and input resets line number */

/*

 * kdb_input_flush

 *

 *	Get rid of any buffered console input.

 *

 * Parameters:

 *	none

 * Returns:

 *	nothing

 * Locking:

 *	none

 * Remarks:

 *	Call this function whenever you want to flush input.  If there is any

 *	outstanding input, it ignores all characters until there has been no

 *	data for approximately 1ms.

/*

 * kdb_printf

 *

 *	Print a string to the output device(s).

 *

 * Parameters:

 *	printf-like format and optional args.

 * Returns:

 *	0

 * Locking:

 *	None.

 * Remarks:

 *	use 'kdbcons->write()' to avoid polluting 'log_buf' with

 *	kdb output.

 *

 *  If the user is doing a cmd args | grep srch

 *  then kdb_grepping_flag is set.

 *  In that case we need to accumulate full lines (ending in \n) before

 *  searching for the pattern.

 A bit too big to go on stack */

/*

 * search arg1 to see if it contains arg2

 * (kdmain.c provides flags for ^pat and pat$)

 *

 * return 1 for found, 0 for not found

 not counting the newline at the end of "searched" */

		/*

		 * Set oops_in_progress to encourage the console drivers to

		 * disregard their internal spin locks: in the current calling

		 * context the risk of deadlock is a bigger problem than risks

		 * due to re-entering the console driver. We operate directly on

		 * oops_in_progress rather than using bust_spinlocks() because

		 * the calls bust_spinlocks() makes on exit are not appropriate

		 * for this calling context.

	/* Serialize kdb_printf if multiple cpus try to write at once.

	 * But if any cpu goes recursive in kdb, just print the output,

	 * even if it is interleaved with any other text.

 normally, every vsnprintf starts a new buffer */

	/*

	 * If kdb_parse() found that the command was cmd xxx | grep yyy

	 * then kdb_grepping_flag is set, and kdb_grep_string contains yyy

	 *

	 * Accumulate the print data up to a newline before searching it.

	 * (vsnprintf does null-terminate the string that it generates)

 skip the search if prints are temporarily unconditional */

			/*

			 * Special cases that don't end with newlines

			 * but should be written without one:

			 *   The "[nn]kdb> " prompt should

			 *   appear at the front of the buffer.

			 *

			 *   The "[nn]more " prompt should also be

			 *     (MOREPROMPT -> moreprompt)

			 *   written *   but we print that ourselves,

			 *   we set the suspend_grep flag to make

			 *   it unconditional.

			 *

				/*

				 * these should occur after a newline,

				 * so they will be at the front of the

				 * buffer

					/*

					 * We're about to start a new

					 * command, so we can go back

					 * to normal mode.

			/* no newline; don't search/write the buffer

		/*

		 * The newline is present; print through it or discard

		 * it, depending on the results of the search.

 to byte after the newline */

 remember what/where it was */

 end the string for our search */

		/*

		 * We now have a newline at the end of the string

		 * Only continue with this output if it contains the

		 * search string.

			/*

			 * At this point the complete line at the start

			 * of kdb_buffer can be discarded, as it does

			 * not contain what the user is looking for.

			 * Shift the buffer left.

			/*

			 * This was a interactive search (using '/' at more

			 * prompt) and it has completed. Replace the \0 with

			 * its original value to ensure multi-line strings

			 * are handled properly, and return to normal mode.

		/*

		 * at this point the string is a full line and

		 * should be printed, up to the null.

	/*

	 * Write to all consoles.

		/*

		 * Check printed string to decide how to bump the

		 * kdb_nextline to control when the more prompt should

		 * show up.

 check for having reached the LINES number of printed lines */

		/* Watch out for recursion here.  Any routine that calls

		 * kdb_printf will come back through here.  And kdb_read

		 * uses kdb_printf to echo on serial consoles ...

 In case of recursion */

		/*

		 * Pause until cr.

 Really set output line 1 */

 empty and reset the buffer: */

 user hit q or Q */

 command interrupted */

 end of command output; back to normal mode */

 for this recursion */

 for this recursion */

 for this recursion */

 user hit something unexpected */

 for this recursion */

 user hit enter */

 for this recursion */

	/*

	 * For grep searches, shift the printed string left.

	 *  replaced_byte contains the character that was overwritten with

	 *  the terminating null, and cphold points to the null.

	 * Then adjust the notion of available space in the buffer.

 end of what may have been a recursive call */

 kdb_printf_cpu locked the code above. */

/*

 * Kernel Debugger Architecture Independent Breakpoint Handler

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (c) 1999-2004 Silicon Graphics, Inc.  All Rights Reserved.

 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved.

/*

 * Table of kdb_breakpoints

	/*

	 * Setup single step

	/*

	 * Reset delay attribute

	/*

	 * Install the breakpoint, if it is not already installed.

/*

 * kdb_bp_install

 *

 *	Install kdb_breakpoints prior to returning from the

 *	kernel debugger.  This allows the kdb_breakpoints to be set

 *	upon functions that are used internally by kdb, such as

 *	printk().  This function is only called once per kdb session.

/*

 * kdb_bp_remove

 *

 *	Remove kdb_breakpoints upon entry to the kernel debugger.

 *

 * Parameters:

 *	None.

 * Outputs:

 *	None.

 * Returns:

 *	None.

 * Locking:

 *	None.

 * Remarks:

/*

 * kdb_printbp

 *

 *	Internal function to format and print a breakpoint entry.

 *

 * Parameters:

 *	None.

 * Outputs:

 *	None.

 * Returns:

 *	None.

 * Locking:

 *	None.

 * Remarks:

/*

 * kdb_bp

 *

 *	Handle the bp commands.

 *

 *	[bp|bph] <addr-expression> [DATAR|DATAW]

 *

 * Parameters:

 *	argc	Count of arguments in argv

 *	argv	Space delimited command line arguments

 * Outputs:

 *	None.

 * Returns:

 *	Zero for success, a kdb diagnostic if failure.

 * Locking:

 *	None.

 * Remarks:

 *

 *	bp	Set breakpoint on all cpus.  Only use hardware assist if need.

 *	bph	Set breakpoint on all cpus.  Force hardware register

		/*

		 * Display breakpoint table

	/*

	 * This check is redundant (since the breakpoint machinery should

	 * be doing the same check during kdb_bp_install) but gives the

	 * user immediate feedback.

	/*

	 * Find an empty bp structure to allocate

	/*

	 * Check for clashing breakpoints.

	 *

	 * Note, in this design we can't have hardware breakpoints

	 * enabled for both read and write on the same address.

	/*

	 * Actually allocate the breakpoint found earlier

/*

 * kdb_bc

 *

 *	Handles the 'bc', 'be', and 'bd' commands

 *

 *	[bd|bc|be] <breakpoint-number>

 *	[bd|bc|be] *

 *

 * Parameters:

 *	argc	Count of arguments in argv

 *	argv	Space delimited command line arguments

 * Outputs:

 *	None.

 * Returns:

 *	Zero for success, a kdb diagnostic for failure

 * Locking:

 *	None.

 * Remarks:

 KDBCMD_B? */

		/*

		 * For addresses less than the maximum breakpoint number,

		 * assume that the breakpoint number is desired.

	/*

	 * Now operate on the set of breakpoints matching the input

	 * criteria (either '*' for all, or an individual breakpoint).

/*

 * kdb_ss

 *

 *	Process the 'ss' (Single Step) command.

 *

 *	ss

 *

 * Parameters:

 *	argc	Argument count

 *	argv	Argument vector

 * Outputs:

 *	None.

 * Returns:

 *	KDB_CMD_SS for success, a kdb error if failure.

 * Locking:

 *	None.

 * Remarks:

 *

 *	Set the arch specific option to trigger a debug trap after the next

 *	instruction.

	/*

	 * Set trace flag and go.

 Initialize the breakpoint table and register	breakpoint commands. */

	/*

	 * First time initialization.

 SPDX-License-Identifier: GPL-2.0

/*

 * Test module for in-kernel kprobe event creation and generation.

 *

 * Copyright (C) 2019 Tom Zanussi <zanussi@kernel.org>

/*

 * This module is a simple test of basic functionality for in-kernel

 * kprobe/kretprobe event creation.  The first test uses

 * kprobe_event_gen_cmd_start(), kprobe_event_add_fields() and

 * kprobe_event_gen_cmd_end() to create a kprobe event, which is then

 * enabled in order to generate trace output.  The second creates a

 * kretprobe event using kretprobe_event_gen_cmd_start() and

 * kretprobe_event_gen_cmd_end(), and is also then enabled.

 *

 * To test, select CONFIG_KPROBE_EVENT_GEN_TEST and build the module.

 * Then:

 *

 * # insmod kernel/trace/kprobe_event_gen_test.ko

 * # cat /sys/kernel/debug/tracing/trace

 *

 * You should see many instances of the "gen_kprobe_test" and

 * "gen_kretprobe_test" events in the trace buffer.

 *

 * To remove the events, remove the module:

 *

 * # rmmod kprobe_event_gen_test

 *

/*

 * Test to make sure we can create a kprobe event, then add more

 * fields.

 Create a buffer to hold the generated command */

 Before generating the command, initialize the cmd object */

	/*

	 * Define the gen_kprobe_test event with the first 2 kprobe

	 * fields.

 Use kprobe_event_add_fields to add the rest of the fields */

	/*

	 * This actually creates the event.

	/*

	 * Now get the gen_kprobe_test event file.  We need to prevent

	 * the instance and event from disappearing from underneath

	 * us, which trace_get_event_file() does (though in this case

	 * we're using the top-level instance which never goes away).

 Enable the event or you won't see anything */

 We got an error after creating the event, delete it */

/*

 * Test to make sure we can create a kretprobe event.

 Create a buffer to hold the generated command */

 Before generating the command, initialize the cmd object */

	/*

	 * Define the kretprobe event.

	/*

	 * This actually creates the event.

	/*

	 * Now get the gen_kretprobe_test event file.  We need to

	 * prevent the instance and event from disappearing from

	 * underneath us, which trace_get_event_file() does (though in

	 * this case we're using the top-level instance which never

	 * goes away).

 Enable the event or you won't see anything */

 We got an error after creating the event, delete it */

 Disable the event or you can't remove it */

 Now give the file and instance back */

 Now unregister and free the event */

 Disable the event or you can't remove it */

 Now give the file and instance back */

 Now unregister and free the event */

 SPDX-License-Identifier: GPL-2.0

/*

 * Function used during the kprobe self test. This function is in a separate

 * compile unit so it can be compile with CC_FLAGS_FTRACE to ensure that it

 * can be probed by the selftests.

 SPDX-License-Identifier: GPL-2.0

/*

 *

 * Function graph tracer.

 * Copyright (c) 2008-2009 Frederic Weisbecker <fweisbec@gmail.com>

 * Mostly borrowed from function tracer which

 * is Copyright (c) Steven Rostedt <srostedt@redhat.com>

 *

 When set, irq functions will be ignored */

 Place to preserve last processed entry. */

 Display overruns? (for self-debug purpose) */

 Display CPU ? */

 Display Overhead ? */

 Display proc name/pid */

 Display duration of execution */

 Display absolute time of an entry */

 Display interrupts */

 Display function name after trailing } */

 Include sleep time (scheduled out) between entry and return */

 Include time within nested functions */

 Empty entry */

 Don't display overruns, proc, or tail by default */

/*

 * DURATION column is being also used to display IRQ signs,

 * following values are used by print_graph_irq and others

 * to fill in space into DURATION column.

	/*

	 * Do not trace a function if it's filtered by set_graph_notrace.

	 * Make the index of ret stack negative to indicate that it should

	 * ignore further functions.  But it needs its own ret stack entry

	 * to recover the original index in order to continue tracing after

	 * returning from the function.

		/*

		 * Need to return 1 to have the return called

		 * that will clear the NOTRACE bit.

	/*

	 * Stop here if tracing_threshold is set. We only write function return

	 * events to the ring buffer.

 Make graph_array visible before we start tracing */

	/*

	 * Start with a space character - to make it stand out

	 * to the right a bit when trace output is pasted into

	 * email:

 sign + log10(MAX_INT) + '\0' */

 1 stands for the "-" character */

 First spaces to align center */

 Last spaces to align center */

 If the pid changed since the last trace, output this event */

/*

 * Context-switch trace line:



 ------------------------------------------

 | 1)  migration/0--1  =>  sshd-1755

 ------------------------------------------



	/*

	 * If the previous output failed to write to the seq buffer,

	 * then we just reuse the data from before.

 First peek to compare current entry and the next one */

			/*

			 * We need to consume the current entry to see

			 * the next one.

			/*

			 * Save current and next entries for later reference

			 * if the output fails.

			/*

			 * If the next event is not a return type, then

			 * we only care about what type it is. Otherwise we can

			 * safely copy the entire event.

 this is a leaf, now advance the iterator */

 Absolute time */

 Relative time */

 Cpu */

 Proc */

 Latency format */

 No overhead */

 log10(ULONG_MAX) + '\0' */

 Print msecs */

 Print nsecs (we don't want to exceed 7 numbers) */

 Print remaining spaces to fit the row's width */

 No real adata, just filling the column with spaces */

 Signal a overhead of time execution to the output */

 Case of a leaf function on its call entry */

		/*

		 * Comments display at + 1 to depth. Since

		 * this is a leaf function, keep the comments

		 * equal to this depth.

 No need to keep this function around for this depth */

 Overhead and duration */

 Function */

 Save this function pointer to see if the exit matches */

 No time */

 Function */

	/*

	 * we already consumed the current entry to check the next one

	 * and see if this is a leaf.

 Pid */

 Interrupt */

 Absolute time */

 Relative time */

 Cpu */

 Proc */

 Latency format */

/*

 * Entry check for irq code

 *

 * returns 1 if

 *  - we are inside irq code

 *  - we just entered irq code

 *

 * returns 0 if

 *  - funcgraph-interrupts option is set

 *  - we are not inside irq code

	/*

	 * If we are either displaying irqs, or we got called as

	 * a graph event and private data does not exist,

	 * then we bypass the irq check.

	/*

	 * We are inside the irq code

	/*

	 * We are entering irq code.

/*

 * Return check for irq code

 *

 * returns 1 if

 *  - we are inside irq code

 *  - we just left irq code

 *

 * returns 0 if

 *  - funcgraph-interrupts option is set

 *  - we are not inside irq code

	/*

	 * If we are either displaying irqs, or we got called as

	 * a graph event and private data does not exist,

	 * then we bypass the irq check.

	/*

	 * We are not inside the irq code.

	/*

	 * We are inside the irq code, and this is returning entry.

	 * Let's not trace it and clear the entry depth, since

	 * we are out of irq code.

	 *

	 * This condition ensures that we 'leave the irq code' once

	 * we are out of the entry depth. Thus protecting us from

	 * the RETURN entry loss.

	/*

	 * We are inside the irq code, and this is not the entry.

		/*

		 * If we failed to write our output, then we need to make

		 * note of it. Because we already consumed our entry.

		/*

		 * Comments display at + 1 to depth. This is the

		 * return from a function, we now want the comments

		 * to display at the same level of the bracket.

 Overhead and duration */

 Closing brace */

	/*

	 * If the return function does not have a matching entry,

	 * then the entry was lost. Instead of just printing

	 * the '}' and letting the user guess what function this

	 * belongs to, write out the function name. Always do

	 * that if the funcgraph-tail option is enabled.

 %ps */\n", (void *)trace->func);

 Overrun */

 No time */

 Indentation */

 The comment */

	trace_seq_puts(s, "/* ");



	switch (iter->ent->type) {

	case TRACE_BPUTS:

		ret = trace_print_bputs_msg_only(iter);

		if (ret != TRACE_TYPE_HANDLED)

			return ret;

		break;

	case TRACE_BPRINT:

		ret = trace_print_bprintk_msg_only(iter);

		if (ret != TRACE_TYPE_HANDLED)

			return ret;

		break;

	case TRACE_PRINT:

		ret = trace_print_printk_msg_only(iter);

		if (ret != TRACE_TYPE_HANDLED)

			return ret;

		break;

	default:

		event = ftrace_find_event(ent->type);

		if (!event)

			return TRACE_TYPE_UNHANDLED;



		ret = event->funcs->trace(iter, sym_flags, event);

		if (ret != TRACE_TYPE_HANDLED)

			return ret;

	}



	if (trace_seq_has_overflowed(s))

		goto out;



	/* Strip ending newline */

	if (s->buffer[s->seq.len - 1] == '\n') {

		s->buffer[s->seq.len - 1] = '\0';

		s->seq.len--;

	}



	/*

	 * If the last output failed, there's a possibility we need

	 * to print out the missing entry which would never go out.

		/*

		 * print_graph_entry() may consume the current event,

		 * thus @field may become invalid, so we need to save it.

		 * sizeof(struct ftrace_graph_ent_entry) is very small,

		 * it can be safely saved at the stack.

 dont trace stack and functions as comments */

 16 spaces */

 4 spaces */

 17 spaces */

 1st line */

 2nd line */

 print nothing if the buffers are empty */

 pid and depth on the last trace processed */

 We can be called in atomic context via ftrace_dump() */

 More than enough to hold UINT_MAX + "\n"*/

 SPDX-License-Identifier: GPL-2.0

/*

 * trace_hwlat.c - A simple Hardware Latency detector.

 *

 * Use this tracer to detect large system latencies induced by the behavior of

 * certain underlying system hardware or firmware, independent of Linux itself.

 * The code was developed originally to detect the presence of SMIs on Intel

 * and AMD systems, although there is no dependency upon x86 herein.

 *

 * The classical example usage of this tracer is in detecting the presence of

 * SMIs or System Management Interrupts on Intel and AMD systems. An SMI is a

 * somewhat special form of hardware interrupt spawned from earlier CPU debug

 * modes in which the (BIOS/EFI/etc.) firmware arranges for the South Bridge

 * LPC (or other device) to generate a special interrupt under certain

 * circumstances, for example, upon expiration of a special SMI timer device,

 * due to certain external thermal readings, on certain I/O address accesses,

 * and other situations. An SMI hits a special CPU pin, triggers a special

 * SMI mode (complete with special memory map), and the OS is unaware.

 *

 * Although certain hardware-inducing latencies are necessary (for example,

 * a modern system often requires an SMI handler for correct thermal control

 * and remote management) they can wreak havoc upon any OS-level performance

 * guarantees toward low-latency, especially when the OS is not even made

 * aware of the presence of these interrupts. For this reason, we need a

 * somewhat brute force mechanism to detect these interrupts. In this case,

 * we do it by hogging all of the CPU(s) for configurable timer intervals,

 * sampling the built-in CPU timer, looking for discontiguous readings.

 *

 * WARNING: This implementation necessarily introduces latencies. Therefore,

 *          you should NEVER use this tracer while running in a production

 *          environment requiring any kind of low-latency performance

 *          guarantee(s).

 *

 * Copyright (C) 2008-2009 Jon Masters, Red Hat, Inc. <jcm@redhat.com>

 * Copyright (C) 2013-2016 Steven Rostedt, Red Hat, Inc. <srostedt@redhat.com>

 *

 * Includes useful feedback from Clark Williams <williams@redhat.com>

 *

 20 digits max */

 1s */

 0.5s */

 10us */

 sample width us */

 sample window us */

 hwlat thread mode */

 Save the previous tracing_thresh value */

 runtime kthread data */

 NMI timestamp counters */

 Tells NMIs to call back to the hwlat tracer to record timestamps */

 If the user changed threshold, remember it */

 Individual latency samples are stored here when detected. */

 unique sequence */

 delta */

 delta (outer loop) */

 Total time spent in NMIs */

 wall time */

 # NMIs during this sample */

 # of iterations over thresh */

 keep the global state somewhere. */

 protect changes */

 total since reset */

 total sampling window (on+off) */

 active sampling portion of window */

 thread mode */

 Macros to encapsulate the time capturing infrastructure */

	/*

	 * Currently trace_clock_local() calls sched_clock() and the

	 * generic version is not NMI safe.

/*

 * hwlat_err - report a hwlat error.

/**

 * get_sample - sample the CPU TSC and look for likely hardware latencies

 *

 * Used to repeatedly capture the CPU TSC (or similar), looking for potential

 * hardware-induced latency. Called with interrupts disabled and with

 * hwlat_data.lock held.

 modifies interval value */

 Make sure NMIs see this first */

 start timestamp */

 we'll look for a discontinuity */

 Check the delta from outer loop (t2 to next t1) */

 This shouldn't happen */

 sample width */

 Check for possible overflows */

 This checks the inner loop (t1 to t2) */

 current diff */

 This shouldn't happen */

 only want highest value */

 finish the above in the view for NMIs */

 Make sure nmi_total_ts is no longer updated */

 If we exceed the threshold value, we have found a hardware latency */

 We read in microseconds */

 Keep a running maximum ever recorded hardware latency */

	/*

	 * If for some reason the user modifies the CPU affinity

	 * of this thread, then stop migrating for the duration

	 * of the current test.

 Shouldn't happen! */

/*

 * kthread_fn - The CPU time sampling/hardware latency detection kernel thread

 *

 * Used to periodically sample the CPU TSC via a call to get_sample. We

 * disable interrupts, which does (intentionally) introduce latency since we

 * need to ensure nothing else might be running (and thus preempting).

 * Obviously this should never be used in production environments.

 *

 * Executes one loop interaction on each CPU in tracing_cpumask sysfs file.

 modifies interval value */

 Always sleep for at least 1ms */

/*

 * stop_stop_kthread - Inform the hardware latency sampling/detector kthread to stop

 *

 * This kicks the running hardware latency sampling/detector kernel thread and

 * tells it to stop sampling now. Use this on unload and at system shutdown.

/*

 * start_single_kthread - Kick off the hardware latency sampling/detector kthread

 *

 * This starts the kernel thread that will sit and sample the CPU timestamp

 * counter (TSC or similar) and look for potential hardware latencies.

 Just pick the first CPU on first iteration */

/*

 * stop_cpu_kthread - Stop a hwlat cpu kthread

/*

 * stop_per_cpu_kthreads - Inform the hardware latency sampling/detector kthread to stop

 *

 * This kicks the running hardware latency sampling/detector kernel threads and

 * tells it to stop sampling now. Use this on unload and at system shutdown.

/*

 * start_cpu_kthread - Start a hwlat cpu kthread

/*

 * hwlat_cpu_init - CPU hotplug online callback function

/*

 * hwlat_cpu_die - CPU hotplug offline callback function

 CONFIG_HOTPLUG_CPU */

 CONFIG_HOTPLUG_CPU */

/*

 * start_per_cpu_kthreads - Kick off the hardware latency sampling/detector kthreads

 *

 * This starts the kernel threads that will sit on potentially all cpus and

 * sample the CPU timestamp counter (TSC or similar) and look for potential

 * hardware latencies.

	/*

	 * Run only on CPUs in which hwlat is allowed to run.

/**

 * hwlat_mode_write - Write function for "mode" entry

 * @filp: The active open file structure

 * @ubuf: The user buffer that contains the value to write

 * @cnt: The maximum number of bytes to write to "file"

 * @ppos: The current position in @file

 *

 * This function provides a write implementation for the "mode" interface

 * to the hardware latency detector. hwlatd has different operation modes.

 * The "none" sets the allowed cpumask for a single hwlatd thread at the

 * startup and lets the scheduler handle the migration. The default mode is

 * the "round-robin" one, in which a single hwlatd thread runs, migrating

 * among the allowed CPUs in a round-robin fashion. The "per-cpu" mode

 * creates one hwlatd thread per allowed CPU.

	/*

	 * trace_types_lock is taken to avoid concurrency on start/stop

	 * and hwlat_busy.

/*

 * The width parameter is read/write using the generic trace_min_max_param

 * method. The *val is protected by the hwlat_data lock and is upper

 * bounded by the window parameter.

/*

 * The window parameter is read/write using the generic trace_min_max_param

 * method. The *val is protected by the hwlat_data lock and is lower

 * bounded by the width parameter.

/**

 * init_tracefs - A function to initialize the tracefs interface files

 *

 * This function creates entries in tracefs for "hwlat_detector".

 * It creates the hwlat_detector directory in the tracing directory,

 * and within that directory is the count, width and window files to

 * change and view those values.

 Only allow one instance to enable this */

 tracing_thresh is in nsecs, we speak in usecs */

 the tracing threshold is static between runs */

 SPDX-License-Identifier: GPL-2.0

/*

 * trace irqs off critical timings

 *

 * Copyright (C) 2007-2008 Steven Rostedt <srostedt@redhat.com>

 * Copyright (C) 2008 Ingo Molnar <mingo@redhat.com>

 *

 * From code in the latency_tracer, that is:

 *

 *  Copyright (C) 2004-2006 Ingo Molnar

 *  Copyright (C) 2004 Nadia Yvette Chambers

/*

 * Sequence count - we record it when starting a measurement and

 * skip the latency if the sequence has changed - some other section

 * did a maximum and could disturb our measurement with serial console

 * printouts, etc. Truly coinciding maximum latencies should be rare

 * and what happens together happens separately as well, so this doesn't

 * decrease the validity of the maximum found:

/*

 * Prologue for the preempt and irqs off function tracers.

 *

 * Returns 1 if it is OK to continue, and data->disabled is

 *            incremented.

 *         0 if the trace is to be ignored, and data->disabled

 *            is kept the same.

 *

 * Note, this function is also used outside this ifdef but

 *  inside the #ifdef of the function graph tracer below.

 *  This is OK, since the function graph tracer is

 *  dependent on the function tracer.

	/*

	 * Does not matter if we preempt. We test the flags

	 * afterward, to see if irqs are disabled or not.

	 * If we preempt and get a false positive, the flags

	 * test will fail.

	/*

	 * Slight chance to get a false positive on tracing_cpu,

	 * although I'm starting to think there isn't a chance.

	 * Leave this for now just to be paranoid.

/*

 * irqsoff uses its own tracer function to keep the overhead down:

 CONFIG_FUNCTION_TRACER */

	/*

	 * Do not trace a function if it's filtered by set_graph_notrace.

	 * Make the index of ret stack negative to indicate that it should

	 * ignore further functions.  But it needs its own ret stack entry

	 * to recover the original index in order to continue tracing after

	 * returning from the function.

	/*

	 * In graph mode call the graph tracer output function,

	 * otherwise go with the TRACE_FN event handler

 CONFIG_FUNCTION_TRACER */

 CONFIG_FUNCTION_GRAPH_TRACER */

/*

 * Should this new latency be reported/recorded?

 check if we are still the max latency */

 Skip 5 functions to get to the irq/preempt enable function */

 Always clear the tracing cpu on stopping the trace */

 start and stop critical timings used to for stoppage (in idle) */

 'set' is set if TRACE_ITER_FUNCTION is about to be set */

 CONFIG_FUNCTION_TRACER */

 non overwrite screws up the latency tracers */

 without pause, we will produce garbage if another latency occurs */

 make sure that the tracer is visible */

 Only toplevel instance supports graph tracing */

/*

 * We are only interested in hardirq on/off events:

  CONFIG_IRQSOFF_TRACER */

 CONFIG_PREEMPT_TRACER */

 IRQSOFF_TRACER || PREEMPTOFF_TRACER */

 SPDX-License-Identifier: GPL-2.0

/*

 * uprobes-based tracing events

 *

 * Copyright (C) IBM Corporation, 2010-2012

 * Author:	Srikar Dronamraju <srikar@linux.vnet.ibm.com>

/*

 * uprobe event core functions

/**

 * for_each_trace_uprobe - iterate over the trace_uprobe list

 * @pos:	the struct trace_uprobe * for each entry

 * @dpos:	the struct dyn_event * to use as a loop cursor

/*

 * Uprobes-specific fetch functions

/*

 * Fetch a null-terminated string. Caller MUST set *(u32 *)dest with max

 * length and relative data location.

			/*

			 * Include the terminating null byte. In this case it

			 * was copied by strncpy_from_user but not accounted

			 * for in ret.

 Return the length of string -- including null terminal byte */

 Note that we don't verify it, since the code does not come from user space */

 1st stage: get value from context */

/*

 * Allocate new trace_uprobe and initialize it (including uprobes).

 Unregister a trace_uprobe and probe_event */

 If there's a reference to the dynamic event */

		/*

		 * trace_probe_compare_arg_type() ensured that nr_args and

		 * each argument name and type are same. Let's compare comm.

 Note that argument starts index = 2 */

 Append to existing event */

/*

 * Uprobe with multiple reference counter is not allowed. i.e.

 * If inode and offset matches, reference counter offset *must*

 * match as well. Though, there is one exception: If user is

 * replacing old trace_uprobe with new one(same group/event),

 * then we allow same uprobe with new reference counter as far

 * as the new one does not conflict with any other existing

 * ones.

 Register a trace_uprobe and probe_event */

 register as an event */

/*

 * Argument syntax:

 *  - Add uprobe: p|r[:[GRP/]EVENT] PATH:OFFSET[%return][(REF)] [FETCHARGS]

 Find the last occurrence, in case the path contains ':' too. */

 filename is the 2nd argument */

 Parse reference counter offset if specified. */

 Check if there is %return suffix */

 Parse uprobe offset. */

 setup a probe */

 This must return -ENOMEM otherwise there is a bug */

 parse arguments */

 Probes listing interfaces */

 Probes profiling interfaces */

	/*

	 * Use per-cpu buffers for fastest access, but we might migrate

	 * so the mutex makes sure we have sole access to it.

 uprobe handler */

 Event entry printers */

 This may also change "enabled" state */

 This returns true if the filter always covers target mm */

		/*

		 * event->parent != NULL means copy_process(), we can avoid

		 * uprobe_apply(). current->mm must be probed and we can rely

		 * on dup_mmap() which preserves the already installed bp's.

		 *

		 * attr.enable_on_exec means that exec/mmap will install the

		 * breakpoints we need.

 uprobe profile handler */

 CONFIG_PERF_EVENTS */

	/*

	 * local trace_kprobes are not added to dyn_event, so they are never

	 * searched in find_trace_kprobe(). Therefore, there is no concern of

	 * duplicated name "DUMMY_EVENT" here.

 CONFIG_PERF_EVENTS */

 Make a trace interface for controlling probe points */

 Profile interface */

 SPDX-License-Identifier: GPL-2.0

/*

 * trace_events_synth - synthetic trace events

 *

 * Copyright (C) 2015, 2020 Tom Zanussi <tom.zanussi@linux.intel.com>

 for gfp flag names */

 variable-length string */

 parameter types */

 parameter values */

	/*

	 * Avoid ring buffer recursion detection, as this event

	 * is being performed within another event.

 only dynamic string increments */

 When len=0, we just calculate the needed length */

 return the length of print_fmt */

 First: called with 0 length to calculate the needed length */

 Second: actually write the @print_fmt */

	/*

	 * For backward compatibility, the old synthetic event command

	 * format did not require semicolons, and in order to not

	 * break user space, that old format must still work. If a new

	 * feature is added, then the format that uses the new feature

	 * will be required to have semicolons, as nothing that uses

	 * the old format would be using the new, yet to be created,

	 * feature. When a new feature is added, this will detect it,

	 * and return a number greater than 1, and require the format

	 * to use semicolons.

/**

 * synth_event_add_field - Add a new field to a synthetic event cmd

 * @cmd: A pointer to the dynevent_cmd struct representing the new event

 * @type: The type of the new field to add

 * @name: The name of the new field to add

 *

 * Add a new field to a synthetic event cmd object.  Field ordering is in

 * the same order the fields are added.

 *

 * See synth_field_size() for available types. If field_name contains

 * [n] the field is considered to be an array.

 *

 * Return: 0 if successful, error otherwise.

/**

 * synth_event_add_field_str - Add a new field to a synthetic event cmd

 * @cmd: A pointer to the dynevent_cmd struct representing the new event

 * @type_name: The type and name of the new field to add, as a single string

 *

 * Add a new field to a synthetic event cmd object, as a single

 * string.  The @type_name string is expected to be of the form 'type

 * name', which will be appended by ';'.  No sanity checking is done -

 * what's passed in is assumed to already be well-formed.  Field

 * ordering is in the same order the fields are added.

 *

 * See synth_field_size() for available types. If field_name contains

 * [n] the field is considered to be an array.

 *

 * Return: 0 if successful, error otherwise.

/**

 * synth_event_add_fields - Add multiple fields to a synthetic event cmd

 * @cmd: A pointer to the dynevent_cmd struct representing the new event

 * @fields: An array of type/name field descriptions

 * @n_fields: The number of field descriptions contained in the fields array

 *

 * Add a new set of fields to a synthetic event cmd object.  The event

 * fields that will be defined for the event should be passed in as an

 * array of struct synth_field_desc, and the number of elements in the

 * array passed in as n_fields.  Field ordering will retain the

 * ordering given in the fields array.

 *

 * See synth_field_size() for available types. If field_name contains

 * [n] the field is considered to be an array.

 *

 * Return: 0 if successful, error otherwise.

/**

 * __synth_event_gen_cmd_start - Start a synthetic event command from arg list

 * @cmd: A pointer to the dynevent_cmd struct representing the new event

 * @name: The name of the synthetic event

 * @mod: The module creating the event, NULL if not created from a module

 * @args: Variable number of arg (pairs), one pair for each field

 *

 * NOTE: Users normally won't want to call this function directly, but

 * rather use the synth_event_gen_cmd_start() wrapper, which

 * automatically adds a NULL to the end of the arg list.  If this

 * function is used directly, make sure the last arg in the variable

 * arg list is NULL.

 *

 * Generate a synthetic event command to be executed by

 * synth_event_gen_cmd_end().  This function can be used to generate

 * the complete command or only the first part of it; in the latter

 * case, synth_event_add_field(), synth_event_add_field_str(), or

 * synth_event_add_fields() can be used to add more fields following

 * this.

 *

 * There should be an even number variable args, each pair consisting

 * of a type followed by a field name.

 *

 * See synth_field_size() for available types. If field_name contains

 * [n] the field is considered to be an array.

 *

 * Return: 0 if successful, error otherwise.

/**

 * synth_event_gen_cmd_array_start - Start synthetic event command from an array

 * @cmd: A pointer to the dynevent_cmd struct representing the new event

 * @name: The name of the synthetic event

 * @fields: An array of type/name field descriptions

 * @n_fields: The number of field descriptions contained in the fields array

 *

 * Generate a synthetic event command to be executed by

 * synth_event_gen_cmd_end().  This function can be used to generate

 * the complete command or only the first part of it; in the latter

 * case, synth_event_add_field(), synth_event_add_field_str(), or

 * synth_event_add_fields() can be used to add more fields following

 * this.

 *

 * The event fields that will be defined for the event should be

 * passed in as an array of struct synth_field_desc, and the number of

 * elements in the array passed in as n_fields.  Field ordering will

 * retain the ordering given in the fields array.

 *

 * See synth_field_size() for available types. If field_name contains

 * [n] the field is considered to be an array.

 *

 * Return: 0 if successful, error otherwise.

	/*

	 * Argument syntax:

	 *  - Add synthetic event: <event_name> field[;field] ...

	 *  - Remove synthetic event: !<event_name> field[;field] ...

	 *      where 'field' = type field_name

			/*

			 * Track the highest version of any field we

			 * found in the command.

			/*

			 * Now sort out what is and isn't valid for

			 * each supported version.

			 *

			 * If we see more than 1 field per loop, it

			 * means we have multiple fields between

			 * semicolons, and that's something we no

			 * longer support in a version 2 or greater

			 * command.

/**

 * synth_event_create - Create a new synthetic event

 * @name: The name of the new synthetic event

 * @fields: An array of type/name field descriptions

 * @n_fields: The number of field descriptions contained in the fields array

 * @mod: The module creating the event, NULL if not created from a module

 *

 * Create a new synthetic event with the given name under the

 * trace/events/synthetic/ directory.  The event fields that will be

 * defined for the event should be passed in as an array of struct

 * synth_field_desc, and the number elements in the array passed in as

 * n_fields. Field ordering will retain the ordering given in the

 * fields array.

 *

 * If the new synthetic event is being created from a module, the mod

 * param must be non-NULL.  This will ensure that the trace buffer

 * won't contain unreadable events.

 *

 * The new synth event should be deleted using synth_event_delete()

 * function.  The new synthetic event can be generated from modules or

 * other kernel code using trace_synth_event() and related functions.

 *

 * Return: 0 if successful, error otherwise.

/**

 * synth_event_delete - Delete a synthetic event

 * @event_name: The name of the new synthetic event

 *

 * Delete a synthetic event that was created with synth_event_create().

 *

 * Return: 0 if successful, error otherwise.

		/*

		 * It is safest to reset the ring buffer if the module

		 * being unloaded registered any events that were

		 * used. The only worry is if a new module gets

		 * loaded, and takes on the same id as the events of

		 * this module. When printing out the buffer, traced

		 * events left over from this module may be passed to

		 * the new module events and unexpected results may

		 * occur.

/**

 * synth_event_cmd_init - Initialize a synthetic event command object

 * @cmd: A pointer to the dynevent_cmd struct representing the new event

 * @buf: A pointer to the buffer used to build the command

 * @maxlen: The length of the buffer passed in @buf

 *

 * Initialize a synthetic event command object.  Use this before

 * calling any of the other dyenvent_cmd functions.

	/*

	 * Normal event tracing doesn't get called at all unless the

	 * ENABLED bit is set (which attaches the probe thus allowing

	 * this code to be called, etc).  Because this is called

	 * directly by the user, we don't have that but we still need

	 * to honor not logging when disabled.  For the iterated

	 * trace case, we save the enabled state upon start and just

	 * ignore the following data calls.

	/*

	 * Avoid ring buffer recursion detection, as this event

	 * is being performed within another event.

/**

 * synth_event_trace - Trace a synthetic event

 * @file: The trace_event_file representing the synthetic event

 * @n_vals: The number of values in vals

 * @args: Variable number of args containing the event values

 *

 * Trace a synthetic event using the values passed in the variable

 * argument list.

 *

 * The argument list should be a list 'n_vals' u64 values.  The number

 * of vals must match the number of field in the synthetic event, and

 * must be in the same order as the synthetic event fields.

 *

 * All vals should be cast to u64, and string vals are just pointers

 * to strings, cast to u64.  Strings will be copied into space

 * reserved in the event for the string, using these pointers.

 *

 * Return: 0 on success, err otherwise.

 just disabled, not really an error */

 only dynamic string increments */

/**

 * synth_event_trace_array - Trace a synthetic event from an array

 * @file: The trace_event_file representing the synthetic event

 * @vals: Array of values

 * @n_vals: The number of values in vals

 *

 * Trace a synthetic event using the values passed in as 'vals'.

 *

 * The 'vals' array is just an array of 'n_vals' u64.  The number of

 * vals must match the number of field in the synthetic event, and

 * must be in the same order as the synthetic event fields.

 *

 * All vals should be cast to u64, and string vals are just pointers

 * to strings, cast to u64.  Strings will be copied into space

 * reserved in the event for the string, using these pointers.

 *

 * Return: 0 on success, err otherwise.

 just disabled, not really an error */

 only dynamic string increments */

/**

 * synth_event_trace_start - Start piecewise synthetic event trace

 * @file: The trace_event_file representing the synthetic event

 * @trace_state: A pointer to object tracking the piecewise trace state

 *

 * Start the trace of a synthetic event field-by-field rather than all

 * at once.

 *

 * This function 'opens' an event trace, which means space is reserved

 * for the event in the trace buffer, after which the event's

 * individual field values can be set through either

 * synth_event_add_next_val() or synth_event_add_val().

 *

 * A pointer to a trace_state object is passed in, which will keep

 * track of the current event trace state until the event trace is

 * closed (and the event finally traced) using

 * synth_event_trace_end().

 *

 * Note that synth_event_trace_end() must be called after all values

 * have been added for each event trace, regardless of whether adding

 * all field values succeeded or not.

 *

 * Note also that for a given event trace, all fields must be added

 * using either synth_event_add_next_val() or synth_event_add_val()

 * but not both together or interleaved.

 *

 * Return: 0 on success, err otherwise.

 just disabled, not really an error */

 can't mix add_next_synth_val() with add_synth_val() */

 add_val can't do dynamic strings */

/**

 * synth_event_add_next_val - Add the next field's value to an open synth trace

 * @val: The value to set the next field to

 * @trace_state: A pointer to object tracking the piecewise trace state

 *

 * Set the value of the next field in an event that's been opened by

 * synth_event_trace_start().

 *

 * The val param should be the value cast to u64.  If the value points

 * to a string, the val param should be a char * cast to u64.

 *

 * This function assumes all the fields in an event are to be set one

 * after another - successive calls to this function are made, one for

 * each field, in the order of the fields in the event, until all

 * fields have been set.  If you'd rather set each field individually

 * without regard to ordering, synth_event_add_val() can be used

 * instead.

 *

 * Note however that synth_event_add_next_val() and

 * synth_event_add_val() can't be intermixed for a given event trace -

 * one or the other but not both can be used at the same time.

 *

 * Note also that synth_event_trace_end() must be called after all

 * values have been added for each event trace, regardless of whether

 * adding all field values succeeded or not.

 *

 * Return: 0 on success, err otherwise.

/**

 * synth_event_add_val - Add a named field's value to an open synth trace

 * @field_name: The name of the synthetic event field value to set

 * @val: The value to set the next field to

 * @trace_state: A pointer to object tracking the piecewise trace state

 *

 * Set the value of the named field in an event that's been opened by

 * synth_event_trace_start().

 *

 * The val param should be the value cast to u64.  If the value points

 * to a string, the val param should be a char * cast to u64.

 *

 * This function looks up the field name, and if found, sets the field

 * to the specified value.  This lookup makes this function more

 * expensive than synth_event_add_next_val(), so use that or the

 * none-piecewise synth_event_trace() instead if efficiency is more

 * important.

 *

 * Note however that synth_event_add_next_val() and

 * synth_event_add_val() can't be intermixed for a given event trace -

 * one or the other but not both can be used at the same time.

 *

 * Note also that synth_event_trace_end() must be called after all

 * values have been added for each event trace, regardless of whether

 * adding all field values succeeded or not.

 *

 * Return: 0 on success, err otherwise.

/**

 * synth_event_trace_end - End piecewise synthetic event trace

 * @trace_state: A pointer to object tracking the piecewise trace state

 *

 * End the trace of a synthetic event opened by

 * synth_event_trace__start().

 *

 * This function 'closes' an event trace, which basically means that

 * it commits the reserved event and cleans up other loose ends.

 *

 * A pointer to a trace_state object is passed in, which will keep

 * track of the current event trace state opened with

 * synth_event_trace_start().

 *

 * Note that this function must be called after all values have been

 * added for each event trace, regardless of whether adding all field

 * values succeeded or not.

 *

 * Return: 0 on success, err otherwise.

 This interface accepts group name prefix */

 __data_loc belongs in format but not event desc */

 parameter values */

/*

 * Register dynevent at core_initcall. This allows kernel to setup kprobe

 * events in postcore_initcall without tracefs.

 SPDX-License-Identifier: GPL-2.0

/*

 * unlikely profiler

 *

 * Copyright (C) 2008 Steven Rostedt <srostedt@redhat.com>

	/*

	 * I would love to save just the ftrace_likely_data pointer, but

	 * this code can also be used by modules. Ugly things can happen

	 * if the module is unloaded, and then we go and read the

	 * pointer.  This is slower, but much safer.

 Strip off the path, only save the file */

	/*

	 * Must be seen before enabling. The reader is a condition

	 * where we do not need a matching rmb()

 CONFIG_FTRACE_SELFTEST */

 CONFIG_BRANCH_TRACER */

 A constant is always correct */

	/*

	 * I would love to have a trace point here instead, but the

	 * trace point code is so inundated with unlikely and likely

	 * conditions that the recursive nightmare that exists is too

	 * much to try to get working. At least for now.

 FIXME: Make this atomic! */

 Only print the file, not the path */

	/*

	 * The miss is overlayed on correct, and hit on incorrect.

	/*

	 * Since the above shows worse (incorrect) cases

	 * first, we continue that by showing best (correct)

	 * cases last.

 CONFIG_PROFILE_ALL_BRANCHES */

 SPDX-License-Identifier: GPL-2.0

/*

 * kdb helper for dumping the ftrace buffer

 *

 * Copyright (C) 2010 Jason Wessel <jason.wessel@windriver.com>

 *

 * ftrace_dump_buf based on ftrace_dump:

 * Copyright (C) 2007-2008 Steven Rostedt <srostedt@redhat.com>

 * Copyright (C) 2008 Ingo Molnar <mingo@redhat.com>

 *

 don't look at user memory in panic mode */

/*

 * kdb_ftdump - Dump the ftrace log buffer

 A negative skip_entries means skip all but the last entries */

 SPDX-License-Identifier: GPL-2.0

/*

 * tracing clocks

 *

 *  Copyright (C) 2009 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 *

 * Implements 3 trace clock variants, with differing scalability/precision

 * tradeoffs:

 *

 *  -   local: CPU-local trace clock

 *  -  medium: scalable global clock with some jitter

 *  -  global: globally monotonic, serialized clock

 *

 * Tracer plugins will chose a default from these clocks.

/*

 * trace_clock_local(): the simplest and least coherent tracing clock.

 *

 * Useful for tracing that does not cross to other CPUs nor

 * does it go through idle events.

	/*

	 * sched_clock() is an architecture implemented, fast, scalable,

	 * lockless clock. It is not guaranteed to be coherent across

	 * CPUs, nor across CPU idle events.

/*

 * trace_clock(): 'between' trace clock. Not completely serialized,

 * but not completely incorrect when crossing CPUs either.

 *

 * This is based on cpu_clock(), which will allow at most ~1 jiffy of

 * jitter between CPUs. So it's a pretty scalable clock, but there

 * can be offsets in the trace data.

/*

 * trace_jiffy_clock(): Simply use jiffies as a clock counter.

 * Note that this use of jiffies_64 is not completely safe on

 * 32-bit systems. But the window is tiny, and the effect if

 * we are affected is that we will have an obviously bogus

 * timestamp on a trace event - i.e. not life threatening.

/*

 * trace_clock_global(): special globally coherent trace clock

 *

 * It has higher overhead than the other trace clocks but is still

 * an order of magnitude faster than GTOD derived hardware clocks.

 *

 * Used by plugins that need globally coherent timestamps.

 keep prev_time and lock in the same cacheline. */

	/*

	 * The global clock "guarantees" that the events are ordered

	 * between CPUs. But if two events on two different CPUS call

	 * trace_clock_global at roughly the same time, it really does

	 * not matter which one gets the earlier time. Just make sure

	 * that the same CPU will always show a monotonic clock.

	 *

	 * Use a read memory barrier to get the latest written

	 * time that was recorded.

 Make sure that now is always greater than or equal to prev_time */

	/*

	 * If in an NMI context then dont risk lockups and simply return

	 * the current time.

 Tracing can cause strange recursion, always use a try lock */

 Reread prev_time in case it was already updated */

 The unlock acts as the wmb for the above rmb */

/*

 * trace_clock_counter(): simply an atomic counter.

 * Use the trace_counter "counter" for cases where you do not care

 * about timings, but are interested in strict ordering.

 SPDX-License-Identifier: GPL-2.0

/*

 * trace_output.c

 *

 * Copyright (C) 2008 Red Hat Inc, Steven Rostedt <srostedt@redhat.com>

 *

 must be a power of 2 */

 check for left over flags */

 check for left over flags */

/**

 * trace_print_hex_seq - print buffer as hex sequence

 * @p: trace seq struct to write to

 * @buf: The buffer to print

 * @buf_len: Length of @buf in bytes

 * @concatenate: Print @buf as single hex string or with spacing

 *

 * Prints the passed buffer as a hex sequence either as a whole,

 * single hex string if @concatenate is true or with spacing after

 * each byte in case @concatenate is false.

/**

 * trace_print_lat_fmt - print the irq, preempt and lockdep fields

 * @s: trace seq struct to write to

 * @entry: The trace entry field from the ring buffer

 *

 * Prints the generic fields of irqs off, in hard or softirq, preempt

 * count.

 trace overhead mark */

 unit: nsec */

 1 sec */

 100 msec */

 10 msec */

 1000 usecs */

 100 usecs */

 10 usecs */

 !verbose && !in_ns */

 trace_find_next_entry() may change iter->ent */

/**

 * ftrace_find_event - find a registered event

 * @type: the type of event to look for

 *

 * Returns an event of type @type otherwise NULL

 * Called with trace_event_read_lock() held.

	/*

	 * We used up all possible max events,

	 * lets see if somebody freed one.

 Did we used up all 65 thousand events??? */

/**

 * register_trace_event - register output for an event type

 * @event: the event type to register

 *

 * Event types are stored in a hash and this hash is used to

 * find a way to print an event. If the @event->type is set

 * then it will use that type, otherwise it will assign a

 * type to use.

 *

 * If you assign your own type, please make sure it is added

 * to the trace_type enum in trace.h, to avoid collisions

 * with the dynamic types.

 *

 * Returns the event type number or zero on error.

 Is this event already used */

/*

 * Used by module code with the trace_event_sem held for write.

/**

 * unregister_trace_event - remove a no longer used event

 * @event: the event to remove

/*

 * Standard events

 TRACE_FN */

 TRACE_CTX an TRACE_WAKE */

 TRACE_STACK */

 TRACE_USER_STACK */

		/*

		 * we do the lookup on the thread group leader,

		 * since individual threads might have already quit!

 TRACE_HWLAT */

		/*

		 * The generic sched_clock() is not NMI safe, thus

		 * we only record the count and not the time.

 TRACE_OSNOISE */

	/*

	 * compute the available % of cpu time.

 TRACE_TIMERLAT */

 TRACE_BPUTS */

 TRACE_BPRINT */

 TRACE_PRINT */

 SPDX-License-Identifier: GPL-2.0

/*

 * trace_events_inject - trace event injection

 *

 * Copyright (C) 2019 Cong Wang <cwang@twitter.com>

 First find the field to associate to */

 Make sure the field is not a string */

 We allow 0xDEADBEEF */

 Make sure it is a value */

 Make sure the field is OK for strings */

 Skip quotes */

 go past the last quote */

 We need an extra '\0' at the end. */

 string length is 0. */

 Caller is responsible to free the *pentry. */

 SPDX-License-Identifier: GPL-2.0

/*

 * tracing_map - lock-free map for tracing

 *

 * Copyright (C) 2015 Tom Zanussi <tom.zanussi@linux.intel.com>

 *

 * tracing_map implementation inspired by lock-free map algorithms

 * originated by Dr. Cliff Click:

 *

 * http://www.azulsystems.com/blog/cliff/2007-03-26-non-blocking-hashtable

 * http://www.azulsystems.com/events/javaone_2007/2007_LockFreeHash.pdf

/*

 * NOTE: For a detailed description of the data structures used by

 * these functions (such as tracing_map_elt) please see the overview

 * of tracing_map data structures at the beginning of tracing_map.h.

/**

 * tracing_map_update_sum - Add a value to a tracing_map_elt's sum field

 * @elt: The tracing_map_elt

 * @i: The index of the given sum associated with the tracing_map_elt

 * @n: The value to add to the sum

 *

 * Add n to sum i associated with the specified tracing_map_elt

 * instance.  The index i is the index returned by the call to

 * tracing_map_add_sum_field() when the tracing map was set up.

/**

 * tracing_map_read_sum - Return the value of a tracing_map_elt's sum field

 * @elt: The tracing_map_elt

 * @i: The index of the given sum associated with the tracing_map_elt

 *

 * Retrieve the value of the sum i associated with the specified

 * tracing_map_elt instance.  The index i is the index returned by the

 * call to tracing_map_add_sum_field() when the tracing map was set

 * up.

 *

 * Return: The sum associated with field i for elt.

/**

 * tracing_map_set_var - Assign a tracing_map_elt's variable field

 * @elt: The tracing_map_elt

 * @i: The index of the given variable associated with the tracing_map_elt

 * @n: The value to assign

 *

 * Assign n to variable i associated with the specified tracing_map_elt

 * instance.  The index i is the index returned by the call to

 * tracing_map_add_var() when the tracing map was set up.

/**

 * tracing_map_var_set - Return whether or not a variable has been set

 * @elt: The tracing_map_elt

 * @i: The index of the given variable associated with the tracing_map_elt

 *

 * Return true if the variable has been set, false otherwise.  The

 * index i is the index returned by the call to tracing_map_add_var()

 * when the tracing map was set up.

/**

 * tracing_map_read_var - Return the value of a tracing_map_elt's variable field

 * @elt: The tracing_map_elt

 * @i: The index of the given variable associated with the tracing_map_elt

 *

 * Retrieve the value of the variable i associated with the specified

 * tracing_map_elt instance.  The index i is the index returned by the

 * call to tracing_map_add_var() when the tracing map was set

 * up.

 *

 * Return: The variable value associated with field i for elt.

/**

 * tracing_map_read_var_once - Return and reset a tracing_map_elt's variable field

 * @elt: The tracing_map_elt

 * @i: The index of the given variable associated with the tracing_map_elt

 *

 * Retrieve the value of the variable i associated with the specified

 * tracing_map_elt instance, and reset the variable to the 'not set'

 * state.  The index i is the index returned by the call to

 * tracing_map_add_var() when the tracing map was set up.  The reset

 * essentially makes the variable a read-once variable if it's only

 * accessed using this function.

 *

 * Return: The variable value associated with field i for elt.

/**

 * tracing_map_add_sum_field - Add a field describing a tracing_map sum

 * @map: The tracing_map

 *

 * Add a sum field to the key and return the index identifying it in

 * the map and associated tracing_map_elts.  This is the index used

 * for instance to update a sum for a particular tracing_map_elt using

 * tracing_map_update_sum() or reading it via tracing_map_read_sum().

 *

 * Return: The index identifying the field in the map and associated

 * tracing_map_elts, or -EINVAL on error.

/**

 * tracing_map_add_var - Add a field describing a tracing_map var

 * @map: The tracing_map

 *

 * Add a var to the map and return the index identifying it in the map

 * and associated tracing_map_elts.  This is the index used for

 * instance to update a var for a particular tracing_map_elt using

 * tracing_map_update_var() or reading it via tracing_map_read_var().

 *

 * Return: The index identifying the var in the map and associated

 * tracing_map_elts, or -EINVAL on error.

/**

 * tracing_map_add_key_field - Add a field describing a tracing_map key

 * @map: The tracing_map

 * @offset: The offset within the key

 * @cmp_fn: The comparison function that will be used to sort on the key

 *

 * Let the map know there is a key and that if it's used as a sort key

 * to use cmp_fn.

 *

 * A key can be a subset of a compound key; for that purpose, the

 * offset param is used to describe where within the compound key

 * the key referenced by this key field resides.

 *

 * Return: The index identifying the field in the map and associated

 * tracing_map_elts, or -EINVAL on error.

				/*

				 * The key is present. But, val (pointer to elt

				 * struct) is still NULL. which means some other

				 * thread is in the process of inserting an

				 * element.

				 *

				 * On top of that, it's key_hash is same as the

				 * one being inserted right now. So, it's

				 * possible that the element has the same

				 * key as well.

				/*

				 * cmpxchg() failed. Loop around once

				 * more to check what key was inserted.

/**

 * tracing_map_insert - Insert key and/or retrieve val from a tracing_map

 * @map: The tracing_map to insert into

 * @key: The key to insert

 *

 * Inserts a key into a tracing_map and creates and returns a new

 * tracing_map_elt for it, or if the key has already been inserted by

 * a previous call, returns the tracing_map_elt already associated

 * with it.  When the map was created, the number of elements to be

 * allocated for the map was specified (internally maintained as

 * 'max_elts' in struct tracing_map), and that number of

 * tracing_map_elts was created by tracing_map_init().  This is the

 * pre-allocated pool of tracing_map_elts that tracing_map_insert()

 * will allocate from when adding new keys.  Once that pool is

 * exhausted, tracing_map_insert() is useless and will return NULL to

 * signal that state.  There are two user-visible tracing_map

 * variables, 'hits' and 'drops', which are updated by this function.

 * Every time an element is either successfully inserted or retrieved,

 * the 'hits' value is incremented.  Every time an element insertion

 * fails, the 'drops' value is incremented.

 *

 * This is a lock-free tracing map insertion function implementing a

 * modified form of Cliff Click's basic insertion algorithm.  It

 * requires the table size be a power of two.  To prevent any

 * possibility of an infinite loop we always make the internal table

 * size double the size of the requested table size (max_elts * 2).

 * Likewise, we never reuse a slot or resize or delete elements - when

 * we've reached max_elts entries, we simply return NULL once we've

 * run out of entries.  Readers can at any point in time traverse the

 * tracing map and safely access the key/val pairs.

 *

 * Return: the tracing_map_elt pointer val associated with the key.

 * If this was a newly inserted key, the val will be a newly allocated

 * and associated tracing_map_elt pointer val.  If the key wasn't

 * found and the pool of tracing_map_elts has been exhausted, NULL is

 * returned and no further insertions will succeed.

/**

 * tracing_map_lookup - Retrieve val from a tracing_map

 * @map: The tracing_map to perform the lookup on

 * @key: The key to look up

 *

 * Looks up key in tracing_map and if found returns the matching

 * tracing_map_elt.  This is a lock-free lookup; see

 * tracing_map_insert() for details on tracing_map and how it works.

 * Every time an element is retrieved, the 'hits' value is

 * incremented.  There is one user-visible tracing_map variable,

 * 'hits', which is updated by this function.  Every time an element

 * is successfully retrieved, the 'hits' value is incremented.  The

 * 'drops' value is never updated by this function.

 *

 * Return: the tracing_map_elt pointer val associated with the key.

 * If the key wasn't found, NULL is returned.

/**

 * tracing_map_destroy - Destroy a tracing_map

 * @map: The tracing_map to destroy

 *

 * Frees a tracing_map along with its associated array of

 * tracing_map_elts.

 *

 * Callers should make sure there are no readers or writers actively

 * reading or inserting into the map before calling this.

/**

 * tracing_map_clear - Clear a tracing_map

 * @map: The tracing_map to clear

 *

 * Resets the tracing map to a cleared or initial state.  The

 * tracing_map_elts are all cleared, and the array of struct

 * tracing_map_entry is reset to an initialized state.

 *

 * Callers should make sure there are no writers actively inserting

 * into the map before calling this.

/**

 * tracing_map_create - Create a lock-free map and element pool

 * @map_bits: The size of the map (2 ** map_bits)

 * @key_size: The size of the key for the map in bytes

 * @ops: Optional client-defined tracing_map_ops instance

 * @private_data: Client data associated with the map

 *

 * Creates and sets up a map to contain 2 ** map_bits number of

 * elements (internally maintained as 'max_elts' in struct

 * tracing_map).  Before using, map fields should be added to the map

 * with tracing_map_add_sum_field() and tracing_map_add_key_field().

 * tracing_map_init() should then be called to allocate the array of

 * tracing_map_elts, in order to avoid allocating anything in the map

 * insertion path.  The user-specified map size reflects the maximum

 * number of elements that can be contained in the table requested by

 * the user - internally we double that in order to keep the table

 * sparse and keep collisions manageable.

 *

 * A tracing_map is a special-purpose map designed to aggregate or

 * 'sum' one or more values associated with a specific object of type

 * tracing_map_elt, which is attached by the map to a given key.

 *

 * tracing_map_create() sets up the map itself, and provides

 * operations for inserting tracing_map_elts, but doesn't allocate the

 * tracing_map_elts themselves, or provide a means for describing the

 * keys or sums associated with the tracing_map_elts.  All

 * tracing_map_elts for a given map have the same set of sums and

 * keys, which are defined by the client using the functions

 * tracing_map_add_key_field() and tracing_map_add_sum_field().  Once

 * the fields are defined, the pool of elements allocated for the map

 * can be created, which occurs when the client code calls

 * tracing_map_init().

 *

 * When tracing_map_init() returns, tracing_map_elt elements can be

 * inserted into the map using tracing_map_insert().  When called,

 * tracing_map_insert() grabs a free tracing_map_elt from the pool, or

 * finds an existing match in the map and in either case returns it.

 * The client can then use tracing_map_update_sum() and

 * tracing_map_read_sum() to update or read a given sum field for the

 * tracing_map_elt.

 *

 * The client can at any point retrieve and traverse the current set

 * of inserted tracing_map_elts in a tracing_map, via

 * tracing_map_sort_entries().  Sorting can be done on any field,

 * including keys.

 *

 * See tracing_map.h for a description of tracing_map_ops.

 *

 * Return: the tracing_map pointer if successful, ERR_PTR if not.

/**

 * tracing_map_init - Allocate and clear a map's tracing_map_elts

 * @map: The tracing_map to initialize

 *

 * Allocates a clears a pool of tracing_map_elts equal to the

 * user-specified size of 2 ** map_bits (internally maintained as

 * 'max_elts' in struct tracing_map).  Before using, the map fields

 * should be added to the map with tracing_map_add_sum_field() and

 * tracing_map_add_key_field().  tracing_map_init() should then be

 * called to allocate the array of tracing_map_elts, in order to avoid

 * allocating anything in the map insertion path.  The user-specified

 * map size reflects the max number of elements requested by the user

 * - internally we double that in order to keep the table sparse and

 * keep collisions manageable.

 *

 * See tracing_map.h for a description of tracing_map_ops.

 *

 * Return: the tracing_map pointer if successful, ERR_PTR if not.

 need at least 1 key and 1 val */

/**

 * tracing_map_destroy_sort_entries - Destroy an array of sort entries

 * @entries: The entries to destroy

 * @n_entries: The number of entries in the array

 *

 * Destroy the elements returned by a tracing_map_sort_entries() call.

/**

 * tracing_map_sort_entries - Sort the current set of tracing_map_elts in a map

 * @map: The tracing_map

 * @sort_key: The sort key to use for sorting

 * @sort_entries: outval: pointer to allocated and sorted array of entries

 *

 * tracing_map_sort_entries() sorts the current set of entries in the

 * map and returns the list of tracing_map_sort_entries containing

 * them to the client in the sort_entries param.  The client can

 * access the struct tracing_map_elt element of interest directly as

 * the 'elt' field of a returned struct tracing_map_sort_entry object.

 *

 * The sort_key has only two fields: idx and descending.  'idx' refers

 * to the index of the field added via tracing_map_add_sum_field() or

 * tracing_map_add_key_field() when the tracing_map was initialized.

 * 'descending' is a flag that if set reverses the sort order, which

 * by default is ascending.

 *

 * The client should not hold on to the returned array but should use

 * it and call tracing_map_destroy_sort_entries() when done.

 *

 * Return: the number of sort_entries in the struct tracing_map_sort_entry

 * array, negative on error

 SPDX-License-Identifier: GPL-2.0

/*

 * OS Noise Tracer: computes the OS Noise suffered by a running thread.

 * Timerlat Tracer: measures the wakeup latency of a timer triggered IRQ and thread.

 *

 * Based on "hwlat_detector" tracer by:

 *   Copyright (C) 2008-2009 Jon Masters, Red Hat, Inc. <jcm@redhat.com>

 *   Copyright (C) 2013-2016 Steven Rostedt, Red Hat, Inc. <srostedt@redhat.com>

 *   With feedback from Clark Williams <williams@redhat.com>

 *

 * And also based on the rtsl tracer presented on:

 *  DE OLIVEIRA, Daniel Bristot, et al. Demystifying the real-time linux

 *  scheduling latency. In: 32nd Euromicro Conference on Real-Time Systems

 *  (ECRTS 2020). Schloss Dagstuhl-Leibniz-Zentrum fur Informatik, 2020.

 *

 * Copyright (C) 2021 Daniel Bristot de Oliveira, Red Hat, Inc. <bristot@redhat.com>

 CONFIG_X86_LOCAL_APIC */

/*

 * Default values.

 1s */

 1s */

 1ms */

 FIFO 95 */

/*

 * trace_array of the enabled osnoise/timerlat instances.

/*

 * osnoise_instance_registered - check if a tr is already registered

/*

 * osnoise_register_instance - register a new trace instance

 *

 * Register a trace_array *tr in the list of instances running

 * osnoise/timerlat tracers.

	/*

	 * register/unregister serialization is provided by trace's

	 * trace_types_lock.

/*

 *  osnoise_unregister_instance - unregister a registered trace instance

 *

 * Remove the trace_array *tr from the list of instances running

 * osnoise/timerlat tracers.

	/*

	 * register/unregister serialization is provided by trace's

	 * trace_types_lock.

/*

 * NMI runtime info.

/*

 * IRQ runtime info.

/*

 * sofirq runtime info.

/*

 * thread runtime info.

/*

 * Runtime information: this structure saves the runtime information used by

 * one sampling thread.

/*

 * Per-cpu runtime information.

/*

 * this_cpu_osn_var - Return the per-cpu osnoise_variables on its relative CPU

/*

 * Runtime information for the timer mode.

/*

 * this_cpu_tmr_var - Return the per-cpu timerlat_variables on its relative CPU

/*

 * tlat_var_reset - Reset the values of the given timerlat_variables

	/*

	 * So far, all the values are initialized as 0, so

	 * zeroing the structure is perfect.

 CONFIG_TIMERLAT_TRACER */

 CONFIG_TIMERLAT_TRACER */

/*

 * osn_var_reset - Reset the values of the given osnoise_variables

	/*

	 * So far, all the values are initialized as 0, so

	 * zeroing the structure is perfect.

/*

 * osn_var_reset_all - Reset the value of all per-cpu osnoise_variables

/*

 * Tells NMIs to call back to the osnoise tracer to record timestamps.

/*

 * osnoise sample structure definition. Used to store the statistics of a

 * sample run.

 runtime */

 noise */

 max single noise sample */

 # HW (incl. hypervisor) interference */

 # NMIs during this sample */

 # IRQs during this sample */

 # softirqs during this sample */

 # threads during this sample */

/*

 * timerlat sample structure definition. Used to store the statistics of

 * a sample run.

 timer_latency */

 unique sequence */

 timer context */

/*

 * Protect the interface.

/*

 * Tracer data.

 total sampling period */

 active sampling portion of period */

 stop trace in the internal operation (loop/irq) */

 stop trace in the final operation (report/thread) */

 timerlat period */

 print IRQ stack if total > */

 timerlat tracer */

 infor users and developers about a problem */

	/*

	 * If the timerlat is enabled, but the irq handler did

	 * not run yet enabling timerlat_tracer, do not trace.

	/*

	 * If the timerlat is enabled, but the irq handler did

	 * not run yet enabling timerlat_tracer, do not trace.

 CONFIG_TIMERLAT_TRACER */

/*

 * Print the osnoise header info.

 CONFIG_PREEMPT_RT */

 CONFIG_PREEMPT_RT */

/*

 * osnoise_taint - report an osnoise error.

/*

 * Record an osnoise_sample into the tracer buffer.

/*

 * Record an osnoise_sample on all osnoise instances.

/*

 * Print the timerlat header info.

 CONFIG_PREEMPT_RT */

 CONFIG_PREEMPT_RT */

/*

 * Record an timerlat_sample into the tracer buffer.

/*

 * Stack trace will take place only at IRQ level, so, no need

 * to control nesting here.

/*

 * timerlat_save_stack - save a stack trace without printing

 *

 * Save the current stack trace without printing. The

 * stack will be printed later, after the end of the measurement.

/*

 * timerlat_dump_stack - dump a stack trace previously saved

	/*

	 * trace only if latency > print_stack config, if enabled.

 CONFIG_STACKTRACE */

 CONFIG_STACKTRACE */

 CONFIG_TIMERLAT_TRACER */

/*

 * Macros to encapsulate the time capturing infrastructure.

/*

 * cond_move_irq_delta_start - Forward the delta_start of a running IRQ

 *

 * If an IRQ is preempted by an NMI, its delta_start is pushed forward

 * to discount the NMI interference.

 *

 * See get_int_safe_duration().

/*

 * cond_move_softirq_delta_start - Forward the delta_start of a running softirq.

 *

 * If a softirq is preempted by an IRQ or NMI, its delta_start is pushed

 * forward to discount the interference.

 *

 * See get_int_safe_duration().

 CONFIG_PREEMPT_RT */

/*

 * cond_move_thread_delta_start - Forward the delta_start of a running thread

 *

 * If a noisy thread is preempted by an softirq, IRQ or NMI, its delta_start

 * is pushed forward to discount the interference.

 *

 * See get_int_safe_duration().

/*

 * get_int_safe_duration - Get the duration of a window

 *

 * The irq, softirq and thread varaibles need to have its duration without

 * the interference from higher priority interrupts. Instead of keeping a

 * variable to discount the interrupt interference from these variables, the

 * starting time of these variables are pushed forward with the interrupt's

 * duration. In this way, a single variable is used to:

 *

 *   - Know if a given window is being measured.

 *   - Account its duration.

 *   - Discount the interference.

 *

 * To avoid getting inconsistent values, e.g.,:

 *

 *	now = time_get()

 *		--->	interrupt!

 *			delta_start -= int duration;

 *		<---

 *	duration = now - delta_start;

 *

 *	result: negative duration if the variable duration before the

 *	interrupt was smaller than the interrupt execution.

 *

 * A counter of interrupts is used. If the counter increased, try

 * to capture an interference safe duration.

 synchronize with interrupts */

 synchronize with interrupts */

	/*

	 * This is an evidence of race conditions that cause

	 * a value to be "discounted" too much.

/*

 *

 * set_int_safe_time - Save the current time on *time, aware of interference

 *

 * Get the time, taking into consideration a possible interference from

 * higher priority interrupts.

 *

 * See get_int_safe_duration() for an explanation.

 synchronize with interrupts */

 synchronize with interrupts */

/*

 * copy_int_safe_time - Copy *src into *desc aware of interference

 synchronize with interrupts */

 synchronize with interrupts */

 CONFIG_TIMERLAT_TRACER */

/*

 * trace_osnoise_callback - NMI entry/exit callback

 *

 * This function is called at the entry and exit NMI code. The bool enter

 * distinguishes between either case. This function is used to note a NMI

 * occurrence, compute the noise caused by the NMI, and to remove the noise

 * it is potentially causing on other interference variables.

	/*

	 * Currently trace_clock_local() calls sched_clock() and the

	 * generic version is not NMI safe.

/*

 * osnoise_trace_irq_entry - Note the starting of an IRQ

 *

 * Save the starting time of an IRQ. As IRQs are non-preemptive to other IRQs,

 * it is safe to use a single variable (ons_var->irq) to save the statistics.

 * The arrival_time is used to report... the arrival time. The delta_start

 * is used to compute the duration at the IRQ exit handler. See

 * cond_move_irq_delta_start().

	/*

	 * This value will be used in the report, but not to compute

	 * the execution time, so it is safe to get it unsafe.

/*

 * osnoise_irq_exit - Note the end of an IRQ, sava data and trace

 *

 * Computes the duration of the IRQ noise, and trace it. Also discounts the

 * interference from other sources of noise could be currently being accounted.

/*

 * trace_irqentry_callback - Callback to the irq:irq_entry traceevent

 *

 * Used to note the starting of an IRQ occurece.

/*

 * trace_irqexit_callback - Callback to the irq:irq_exit traceevent

 *

 * Used to note the end of an IRQ occurece.

/*

 * arch specific register function.

/*

 * arch specific unregister function.

/*

 * hook_irq_events - Hook IRQ handling events

 *

 * This function hooks the IRQ related callbacks to the respective trace

 * events.

/*

 * unhook_irq_events - Unhook IRQ handling events

 *

 * This function unhooks the IRQ related callbacks to the respective trace

 * events.

/*

 * trace_softirq_entry_callback - Note the starting of a softirq

 *

 * Save the starting time of a softirq. As softirqs are non-preemptive to

 * other softirqs, it is safe to use a single variable (ons_var->softirq)

 * to save the statistics. The arrival_time is used to report... the

 * arrival time. The delta_start is used to compute the duration at the

 * softirq exit handler. See cond_move_softirq_delta_start().

	/*

	 * This value will be used in the report, but not to compute

	 * the execution time, so it is safe to get it unsafe.

/*

 * trace_softirq_exit_callback - Note the end of an softirq

 *

 * Computes the duration of the softirq noise, and trace it. Also discounts the

 * interference from other sources of noise could be currently being accounted.

/*

 * hook_softirq_events - Hook softirq handling events

 *

 * This function hooks the softirq related callbacks to the respective trace

 * events.

/*

 * unhook_softirq_events - Unhook softirq handling events

 *

 * This function hooks the softirq related callbacks to the respective trace

 * events.

 CONFIG_PREEMPT_RT */

/*

 * softirq are threads on the PREEMPT_RT mode.

/*

 * thread_entry - Record the starting of a thread noise window

 *

 * It saves the context switch time for a noisy thread, and increments

 * the interference counters.

	/*

	 * The arrival time will be used in the report, but not to compute

	 * the execution time, so it is safe to get it unsafe.

/*

 * thread_exit - Report the end of a thread noise window

 *

 * It computes the total noise from a thread, tracing if needed.

/*

 * trace_sched_switch - sched:sched_switch trace event handler

 *

 * This function is hooked to the sched:sched_switch trace event, and it is

 * used to record the beginning and to report the end of a thread noise window.

/*

 * hook_thread_events - Hook the insturmentation for thread noise

 *

 * Hook the osnoise tracer callbacks to handle the noise from other

 * threads on the necessary kernel events.

/*

 * unhook_thread_events - *nhook the insturmentation for thread noise

 *

 * Unook the osnoise tracer callbacks to handle the noise from other

 * threads on the necessary kernel events.

/*

 * save_osn_sample_stats - Save the osnoise_sample statistics

 *

 * Save the osnoise_sample statistics before the sampling phase. These

 * values will be used later to compute the diff betwneen the statistics

 * before and after the osnoise sampling.

/*

 * diff_osn_sample_stats - Compute the osnoise_sample statistics

 *

 * After a sample period, compute the difference on the osnoise_sample

 * statistics. The struct osnoise_sample *s contains the statistics saved via

 * save_osn_sample_stats() before the osnoise sampling.

/*

 * osnoise_stop_tracing - Stop tracing and the tracer.

/*

 * notify_new_max_latency - Notify a new max latency via fsnotify interface.

/*

 * run_osnoise - Sample the time and look for osnoise

 *

 * Used to capture the time, looking for potential osnoise latency repeatedly.

 * Different from hwlat_detector, it is called with preemption and interrupts

 * enabled. This allows irqs, softirqs and threads to run, interfering on the

 * osnoise sampling thread, as they would do with a regular thread.

	/*

	 * Considers the current thread as the workload.

	/*

	 * Save the current stats for the diff

	/*

	 * if threshold is 0, use the default value of 5 us.

	/*

	 * Make sure NMIs see sampling first

	/*

	 * Transform the *_us config to nanoseconds to avoid the

	 * division on the main loop.

	/*

	 * Start timestemp

	/*

	 * "previous" loop.

		/*

		 * Get sample!

		/*

		 * This shouldn't happen.

		/*

		 * Sample runtime.

		/*

		 * Check for possible overflows.

		/*

		 * For the non-preemptive kernel config: let threads runs, if

		 * they so wish.

	/*

	 * Finish the above in the view for interrupts.

	/*

	 * Make sure sampling data is no longer updated.

	/*

	 * Save noise info.

 Save interference stats info */

/*

 * osnoise_main - The osnoise detection kernel thread

 *

 * Calls run_osnoise() function to measure the osnoise for the configured runtime,

 * every period.

		/*

		 * differently from hwlat_detector, the osnoise tracer can run

		 * without a pause because preemption is on.

 Let synchronize_rcu_tasks() make progress */

/*

 * timerlat_irq - hrtimer handler for timerlat.

	/*

	 * I am not sure if the timer was armed for this CPU. So, get

	 * the timerlat struct from the timer itself, not from this

	 * CPU.

	/*

	 * Enable the osnoise: events for thread an softirq.

	/*

	 * A hardirq is running: the timer IRQ. It is for sure preempting

	 * a thread, and potentially preempting a softirq.

	 *

	 * At this point, it is not interesting to know the duration of the

	 * preempted thread (and maybe softirq), but how much time they will

	 * delay the beginning of the execution of the timer thread.

	 *

	 * To get the correct (net) delay added by the softirq, its delta_start

	 * is set as the IRQ one. In this way, at the return of the IRQ, the delta

	 * start of the sofitrq will be zeroed, accounting then only the time

	 * after that.

	 *

	 * The thread follows the same principle. However, if a softirq is

	 * running, the thread needs to receive the softirq delta_start. The

	 * reason being is that the softirq will be the last to be unfolded,

	 * resseting the thread delay to zero.

	 *

	 * The PREEMPT_RT is a special case, though. As softirqs run as threads

	 * on RT, moving the thread is enough.

	/*

	 * Compute the current time with the expected time.

/*

 * wait_next_period - Wait for the next period for timerlat

	/*

	 * Save the next abs_period.

	/*

	 * If the new abs_period is in the past, skip the activation.

/*

 * timerlat_main- Timerlat main

	/*

	 * Make the thread RT, that is how cyclictest is usually used.

	/*

	 * Anotate the arrival time.

 CONFIG_TIMERLAT_TRACER */

 CONFIG_TIMERLAT_TRACER */

/*

 * stop_kthread - stop a workload thread

/*

 * stop_per_cpu_kthread - Stop per-cpu threads

 *

 * Stop the osnoise sampling htread. Use this on unload and at system

 * shutdown.

/*

 * start_kthread - Start a workload tread

/*

 * start_per_cpu_kthread - Kick off per-cpu osnoise sampling kthreads

 *

 * This starts the kernel thread that will look for osnoise on many

 * cpus.

	/*

	 * Run only on online CPUs in which osnoise is allowed to run.

/*

 * osnoise_cpu_init - CPU hotplug online callback function

/*

 * osnoise_cpu_die - CPU hotplug offline callback function

 CONFIG_HOTPLUG_CPU */

 CONFIG_HOTPLUG_CPU */

/*

 * osnoise_cpus_read - Read function for reading the "cpus" file

 * @filp: The active open file structure

 * @ubuf: The userspace provided buffer to read value into

 * @cnt: The maximum number of bytes to read

 * @ppos: The current "file" position

 *

 * Prints the "cpus" output into the user-provided buffer.

/*

 * osnoise_cpus_write - Write function for "cpus" entry

 * @filp: The active open file structure

 * @ubuf: The user buffer that contains the value to write

 * @cnt: The maximum number of bytes to write to "file"

 * @ppos: The current position in @file

 *

 * This function provides a write implementation for the "cpus"

 * interface to the osnoise trace. By default, it lists all  CPUs,

 * in this way, allowing osnoise threads to run on any online CPU

 * of the system. It serves to restrict the execution of osnoise to the

 * set of CPUs writing via this interface. Why not use "tracing_cpumask"?

 * Because the user might be interested in tracing what is running on

 * other CPUs. For instance, one might run osnoise in one HT CPU

 * while observing what is running on the sibling HT CPU.

	/*

	 * trace_types_lock is taken to avoid concurrency on start/stop.

	/*

	 * osnoise_cpumask is read by CPU hotplug operations.

/*

 * osnoise/runtime_us: cannot be greater than the period.

/*

 * osnoise/period_us: cannot be smaller than the runtime.

/*

 * osnoise/stop_tracing_us: no limit.

/*

 * osnoise/stop_tracing_total_us: no limit.

/*

 * osnoise/print_stack: print the stacktrace of the IRQ handler if the total

 * latency is higher than val.

/*

 * osnoise/timerlat_period: min 100 us, max 1 s

 CONFIG_STACKTRACE */

 CONFIG_STACKTRACE */

/*

 * init_timerlat_tracefs - A function to initialize the timerlat interface files

 CONFIG_TIMERLAT_TRACER */

 CONFIG_TIMERLAT_TRACER */

/*

 * init_tracefs - A function to initialize the tracefs interface files

 *

 * This function creates entries in tracefs for "osnoise" and "timerlat".

 * It creates these directories in the tracing directory, and within that

 * directory the use can change and view the configs.

	/*

	 * Trace is already hooked, we are re-enabling from

	 * a stop_tracing_*.

	/*

	 * All fine!

/*

 * osnoise_workload_start - start the workload and hook to events

	/*

	 * Instances need to be registered after calling workload

	 * start. Hence, if there is already an instance, the

	 * workload was already registered. Otherwise, this

	 * code is on the way to register the first instance,

	 * and the workload will start.

	/*

	 * Make sure that ftrace_nmi_enter/exit() see reset values

	 * before enabling trace_osnoise_callback_enabled.

/*

 * osnoise_workload_stop - stop the workload and unhook the events

	/*

	 * Instances need to be unregistered before calling

	 * stop. Hence, if there is a registered instance, more

	 * than one instance is running, and the workload will not

	 * yet stop. Otherwise, this code is on the way to disable

	 * the last instance, and the workload can stop.

	/*

	 * Make sure that ftrace_nmi_enter/exit() see

	 * trace_osnoise_callback_enabled as false before continuing.

	/*

	 * If the instance is already registered, there is no need to

	 * register it again.

	/*

	 * Only allow osnoise tracer if timerlat tracer is not running

	 * already.

	/*

	 * If the instance is already registered, there is no need to

	 * register it again.

	/*

	 * Instruct the threads to stop only if this is the last instance.

	/*

	 * Only allow timerlat tracer if osnoise tracer is not running already.

	/*

	 * If this is the first instance, set timerlat_tracer to block

	 * osnoise tracer start.

	/*

	 * If this is the last instance, reset timerlat_tracer allowing

	 * osnoise to be started.

 CONFIG_TIMERLAT_TRACER */

 CONFIG_TIMERLAT_TRACER */

 SPDX-License-Identifier: GPL-2.0

/*

 * trace_events_filter - generic event filtering

 *

 * Copyright (C) 2009 Tom Zanussi <tzanussi@gmail.com>

 Due to token parsing '<=' must be before '<' and '>=' must be before '>' */

/*

 * pred functions are OP_LE, OP_LT, OP_GE, OP_GT, and OP_BAND

 * pred_funcs_##type below must match the order of them above.

 Called after a '!' character but "!=" and "!~" are not "not"s */

/**

 * prog_entry - a singe entry in the filter program

 * @target:	     Index to jump to on a branch (actually one minus the index)

 * @when_to_branch:  The value of the result of the predicate to do a branch

 * @pred:	     The predicate to execute.

/**

 * update_preds- assign a program entry a label target

 * @prog: The program array

 * @N: The index of the current entry in @prog

 * @when_to_branch: What to assign a program entry for its branch condition

 *

 * The program entry at @N has a target that points to the index of a program

 * entry that can have its target and when_to_branch fields updated.

 * Update the current program entry denoted by index @N target field to be

 * that of the updated entry. This will denote the entry to update if

 * we are processing an "||" after an "&&"

/*

 * Without going into a formal proof, this explains the method that is used in

 * parsing the logical expressions.

 *

 * For example, if we have: "a && !(!b || (c && g)) || d || e && !f"

 * The first pass will convert it into the following program:

 *

 * n1: r=a;       l1: if (!r) goto l4;

 * n2: r=b;       l2: if (!r) goto l4;

 * n3: r=c; r=!r; l3: if (r) goto l4;

 * n4: r=g; r=!r; l4: if (r) goto l5;

 * n5: r=d;       l5: if (r) goto T

 * n6: r=e;       l6: if (!r) goto l7;

 * n7: r=f; r=!r; l7: if (!r) goto F

 * T: return TRUE

 * F: return FALSE

 *

 * To do this, we use a data structure to represent each of the above

 * predicate and conditions that has:

 *

 *  predicate, when_to_branch, invert, target

 *

 * The "predicate" will hold the function to determine the result "r".

 * The "when_to_branch" denotes what "r" should be if a branch is to be taken

 * "&&" would contain "!r" or (0) and "||" would contain "r" or (1).

 * The "invert" holds whether the value should be reversed before testing.

 * The "target" contains the label "l#" to jump to.

 *

 * A stack is created to hold values when parentheses are used.

 *

 * To simplify the logic, the labels will start at 0 and not 1.

 *

 * The possible invert values are 1 and 0. The number of "!"s that are in scope

 * before the predicate determines the invert value, if the number is odd then

 * the invert value is 1 and 0 otherwise. This means the invert value only

 * needs to be toggled when a new "!" is introduced compared to what is stored

 * on the stack, where parentheses were used.

 *

 * The top of the stack and "invert" are initialized to zero.

 *

 * ** FIRST PASS **

 *

 * #1 A loop through all the tokens is done:

 *

 * #2 If the token is an "(", the stack is push, and the current stack value

 *    gets the current invert value, and the loop continues to the next token.

 *    The top of the stack saves the "invert" value to keep track of what

 *    the current inversion is. As "!(a && !b || c)" would require all

 *    predicates being affected separately by the "!" before the parentheses.

 *    And that would end up being equivalent to "(!a || b) && !c"

 *

 * #3 If the token is an "!", the current "invert" value gets inverted, and

 *    the loop continues. Note, if the next token is a predicate, then

 *    this "invert" value is only valid for the current program entry,

 *    and does not affect other predicates later on.

 *

 * The only other acceptable token is the predicate string.

 *

 * #4 A new entry into the program is added saving: the predicate and the

 *    current value of "invert". The target is currently assigned to the

 *    previous program index (this will not be its final value).

 *

 * #5 We now enter another loop and look at the next token. The only valid

 *    tokens are ")", "&&", "||" or end of the input string "\0".

 *

 * #6 The invert variable is reset to the current value saved on the top of

 *    the stack.

 *

 * #7 The top of the stack holds not only the current invert value, but also

 *    if a "&&" or "||" needs to be processed. Note, the "&&" takes higher

 *    precedence than "||". That is "a && b || c && d" is equivalent to

 *    "(a && b) || (c && d)". Thus the first thing to do is to see if "&&" needs

 *    to be processed. This is the case if an "&&" was the last token. If it was

 *    then we call update_preds(). This takes the program, the current index in

 *    the program, and the current value of "invert".  More will be described

 *    below about this function.

 *

 * #8 If the next token is "&&" then we set a flag in the top of the stack

 *    that denotes that "&&" needs to be processed, break out of this loop

 *    and continue with the outer loop.

 *

 * #9 Otherwise, if a "||" needs to be processed then update_preds() is called.

 *    This is called with the program, the current index in the program, but

 *    this time with an inverted value of "invert" (that is !invert). This is

 *    because the value taken will become the "when_to_branch" value of the

 *    program.

 *    Note, this is called when the next token is not an "&&". As stated before,

 *    "&&" takes higher precedence, and "||" should not be processed yet if the

 *    next logical operation is "&&".

 *

 * #10 If the next token is "||" then we set a flag in the top of the stack

 *     that denotes that "||" needs to be processed, break out of this loop

 *     and continue with the outer loop.

 *

 * #11 If this is the end of the input string "\0" then we break out of both

 *     loops.

 *

 * #12 Otherwise, the next token is ")", where we pop the stack and continue

 *     this inner loop.

 *

 * Now to discuss the update_pred() function, as that is key to the setting up

 * of the program. Remember the "target" of the program is initialized to the

 * previous index and not the "l" label. The target holds the index into the

 * program that gets affected by the operand. Thus if we have something like

 *  "a || b && c", when we process "a" the target will be "-1" (undefined).

 * When we process "b", its target is "0", which is the index of "a", as that's

 * the predicate that is affected by "||". But because the next token after "b"

 * is "&&" we don't call update_preds(). Instead continue to "c". As the

 * next token after "c" is not "&&" but the end of input, we first process the

 * "&&" by calling update_preds() for the "&&" then we process the "||" by

 * calling updates_preds() with the values for processing "||".

 *

 * What does that mean? What update_preds() does is to first save the "target"

 * of the program entry indexed by the current program entry's "target"

 * (remember the "target" is initialized to previous program entry), and then

 * sets that "target" to the current index which represents the label "l#".

 * That entry's "when_to_branch" is set to the value passed in (the "invert"

 * or "!invert"). Then it sets the current program entry's target to the saved

 * "target" value (the old value of the program that had its "target" updated

 * to the label).

 *

 * Looking back at "a || b && c", we have the following steps:

 *  "a"  - prog[0] = { "a", X, -1 } // pred, when_to_branch, target

 *  "||" - flag that we need to process "||"; continue outer loop

 *  "b"  - prog[1] = { "b", X, 0 }

 *  "&&" - flag that we need to process "&&"; continue outer loop

 * (Notice we did not process "||")

 *  "c"  - prog[2] = { "c", X, 1 }

 *  update_preds(prog, 2, 0); // invert = 0 as we are processing "&&"

 *    t = prog[2].target; // t = 1

 *    s = prog[t].target; // s = 0

 *    prog[t].target = 2; // Set target to "l2"

 *    prog[t].when_to_branch = 0;

 *    prog[2].target = s;

 * update_preds(prog, 2, 1); // invert = 1 as we are now processing "||"

 *    t = prog[2].target; // t = 0

 *    s = prog[t].target; // s = -1

 *    prog[t].target = 2; // Set target to "l2"

 *    prog[t].when_to_branch = 1;

 *    prog[2].target = s;

 *

 * #13 Which brings us to the final step of the first pass, which is to set

 *     the last program entry's when_to_branch and target, which will be

 *     when_to_branch = 0; target = N; ( the label after the program entry after

 *     the last program entry processed above).

 *

 * If we denote "TRUE" to be the entry after the last program entry processed,

 * and "FALSE" the program entry after that, we are now done with the first

 * pass.

 *

 * Making the above "a || b && c" have a program of:

 *  prog[0] = { "a", 1, 2 }

 *  prog[1] = { "b", 0, 2 }

 *  prog[2] = { "c", 0, 3 }

 *

 * Which translates into:

 * n0: r = a; l0: if (r) goto l2;

 * n1: r = b; l1: if (!r) goto l2;

 * n2: r = c; l2: if (!r) goto l3;  // Which is the same as "goto F;"

 * T: return TRUE; l3:

 * F: return FALSE

 *

 * Although, after the first pass, the program is correct, it is

 * inefficient. The simple sample of "a || b && c" could be easily been

 * converted into:

 * n0: r = a; if (r) goto T

 * n1: r = b; if (!r) goto F

 * n2: r = c; if (!r) goto F

 * T: return TRUE;

 * F: return FALSE;

 *

 * The First Pass is over the input string. The next too passes are over

 * the program itself.

 *

 * ** SECOND PASS **

 *

 * Which brings us to the second pass. If a jump to a label has the

 * same condition as that label, it can instead jump to its target.

 * The original example of "a && !(!b || (c && g)) || d || e && !f"

 * where the first pass gives us:

 *

 * n1: r=a;       l1: if (!r) goto l4;

 * n2: r=b;       l2: if (!r) goto l4;

 * n3: r=c; r=!r; l3: if (r) goto l4;

 * n4: r=g; r=!r; l4: if (r) goto l5;

 * n5: r=d;       l5: if (r) goto T

 * n6: r=e;       l6: if (!r) goto l7;

 * n7: r=f; r=!r; l7: if (!r) goto F:

 * T: return TRUE;

 * F: return FALSE

 *

 * We can see that "l3: if (r) goto l4;" and at l4, we have "if (r) goto l5;".

 * And "l5: if (r) goto T", we could optimize this by converting l3 and l4

 * to go directly to T. To accomplish this, we start from the last

 * entry in the program and work our way back. If the target of the entry

 * has the same "when_to_branch" then we could use that entry's target.

 * Doing this, the above would end up as:

 *

 * n1: r=a;       l1: if (!r) goto l4;

 * n2: r=b;       l2: if (!r) goto l4;

 * n3: r=c; r=!r; l3: if (r) goto T;

 * n4: r=g; r=!r; l4: if (r) goto T;

 * n5: r=d;       l5: if (r) goto T;

 * n6: r=e;       l6: if (!r) goto F;

 * n7: r=f; r=!r; l7: if (!r) goto F;

 * T: return TRUE

 * F: return FALSE

 *

 * In that same pass, if the "when_to_branch" doesn't match, we can simply

 * go to the program entry after the label. That is, "l2: if (!r) goto l4;"

 * where "l4: if (r) goto T;", then we can convert l2 to be:

 * "l2: if (!r) goto n5;".

 *

 * This will have the second pass give us:

 * n1: r=a;       l1: if (!r) goto n5;

 * n2: r=b;       l2: if (!r) goto n5;

 * n3: r=c; r=!r; l3: if (r) goto T;

 * n4: r=g; r=!r; l4: if (r) goto T;

 * n5: r=d;       l5: if (r) goto T

 * n6: r=e;       l6: if (!r) goto F;

 * n7: r=f; r=!r; l7: if (!r) goto F

 * T: return TRUE

 * F: return FALSE

 *

 * Notice, all the "l#" labels are no longer used, and they can now

 * be discarded.

 *

 * ** THIRD PASS **

 *

 * For the third pass we deal with the inverts. As they simply just

 * make the "when_to_branch" get inverted, a simple loop over the

 * program to that does: "when_to_branch ^= invert;" will do the

 * job, leaving us with:

 * n1: r=a; if (!r) goto n5;

 * n2: r=b; if (!r) goto n5;

 * n3: r=c: if (!r) goto T;

 * n4: r=g; if (!r) goto T;

 * n5: r=d; if (r) goto T

 * n6: r=e; if (!r) goto F;

 * n7: r=f; if (r) goto F

 * T: return TRUE

 * F: return FALSE

 *

 * As "r = a; if (!r) goto n5;" is obviously the same as

 * "if (!a) goto n5;" without doing anything we can interpret the

 * program as:

 * n1: if (!a) goto n5;

 * n2: if (!b) goto n5;

 * n3: if (!c) goto T;

 * n4: if (!g) goto T;

 * n5: if (d) goto T

 * n6: if (!e) goto F;

 * n7: if (f) goto F

 * T: return TRUE

 * F: return FALSE

 *

 * Since the inverts are discarded at the end, there's no reason to store

 * them in the program array (and waste memory). A separate array to hold

 * the inverts is used and freed at the end.

 For TRUE and FALSE */

 First pass */

 #1 */

 #2 */

 #3 */

 #4 */

 #5 */

 accepting only "&&" or "||" */

 #7 */

 #8 */

 #9 */

 #10 */

 #11 */

 Too few '(' */

 #12 */

 Too many '(' */

 No program? */

 #13 */

 TRUE */

 FALSE */

 Second Pass */

 Third Pass */

 Make sure the program always moves forward */

 Filter predicate for fixed sized arrays of characters */

 Filter predicate for char * pointers */

 including tailing '\0' */

/*

 * Filter predicate for dynamic sized arrays of characters.

 * These are implemented through a list of strings at the end

 * of the entry.

 * Also each of these strings have a field in the entry which

 * contains its offset from the beginning of the entry.

 * We have then first to get this field, dereference it

 * and add it to the address of the entry, and at last we have

 * the address of the string.

 Filter predicate for CPUs. */

 Filter predicate for COMM. */

/*

 * regex_match_foo - Basic regex callbacks

 *

 * @str: the string to be searched

 * @r:   the regex structure containing the pattern string

 * @len: the length of the string to be searched (including '\0')

 *

 * Note:

 * - @str might not be NULL-terminated if it's of type DYN_STRING

 *   or STATIC_STRING, unless @len is zero.

 len of zero means str is dynamic and ends with '\0' */

/**

 * filter_parse_regex - parse a basic regex

 * @buff:   the raw regex

 * @len:    length of the regex

 * @search: will point to the beginning of the string to compare

 * @not:    tell whether the match will have to be inverted

 *

 * This passes in a buffer containing a regex and this function will

 * set search to point to the search part of the buffer and

 * return the type of search it is (see enum above).

 * This does modify buff.

 *

 * Returns enum type.

 *  search returns the pointer to use for comparison.

 *  not returns 1 if buff started with a '!'

 *     0 otherwise.

 pattern continues, use full glob */

 MATCH_INDEX should not happen, but if it does, match full */

 return 1 if event matches, 0 otherwise (discard) */

 no filter is considered a match */

 Protected by either SRCU(tracepoint_srcu) or preempt_disable */

 indexing is off by one */

 caller must hold event_mutex */

 Called when a predicate is encountered by predicate_parse() */

 Big enough to hold an address */

 First find the field to associate to */

 Make sure that the field exists */

 Make sure this op is supported */

 This is why '<=' must come before '<' in ops[] */

		/*

		 * Perf does things different with function events.

		 * It only allows an "ip" field, and expects a string.

		 * But the string does not need to be surrounded by quotes.

		 * If it is a string, the assigned function as a nop,

		 * (perf doesn't use it) and grab everything.

		/*

		 * Quotes are not required, but if they exist then we need

		 * to read them till we hit a matching one.

 Skip quotes */

 This is either a string, or an integer */

 Make sure the op is OK for strings */

 Make sure the field is OK for strings */

 Skip quotes */

 go past the last quote */

 Make sure the field is not a string */

 We allow 0xDEADBEEF */

 0xfeedfacedeadbeef is 18 chars max */

 Make sure it is a value */

/*

 * Read the filter string once to calculate the number of predicates

 * as well as how deep the parentheses go.

 *

 * Returns:

 *   0 - everything is fine (err is undefined)

 *  -1 - too many ')'

 *  -2 - too many '('

 *  -3 - No matching quote

 Count the expression as "(E)" */

 find the bad open */

 First character is the '(' with missing ')' */

 Set the size of the required stacks */

		/*

		 * Regardless of if this returned an error, we still

		 * replace the filter for the call.

	/*

	 * The calls can still be using the old filters.

	 * Do a synchronize_rcu() and to ensure all calls are

	 * done with them before we free them.

 No call succeeded */

 If any call succeeded, we still need to sync */

 we're committed to creating a new filter */

/**

 * create_filter - create a filter for a trace_event_call

 * @tr: the trace array associated with these events

 * @call: trace_event_call to create a filter for

 * @filter_str: filter string

 * @set_str: remember @filter_str and enable detailed error in filter

 * @filterp: out param for created filter (always updated on return)

 *           Must be a pointer that references a NULL pointer.

 *

 * Creates a filter for @call with @filter_str.  If @set_str is %true,

 * @filter_str is copied and recorded in the new filter.

 *

 * On success, returns 0 and *@filterp points to the new filter.  On

 * failure, returns -errno and *@filterp may point to %NULL or to a new

 * filter.  In the latter case, the returned filter contains error

 * information if @set_str is %true and the caller is responsible for

 * freeing it.

 filterp must point to NULL */

/**

 * create_system_filter - create a filter for an event subsystem

 * @dir: the descriptor for the subsystem directory

 * @filter_str: filter string

 * @filterp: out param for created filter (always updated on return)

 *

 * Identical to create_filter() except that it creates a subsystem filter

 * and always remembers @filter_str.

 System filters just show a default message */

 caller must hold event_mutex */

 Make sure the filter is not being used */

	/*

	 * Always swap the call filter with the new filter

	 * even if there was an error. If there was an error

	 * in the filter, we disable the filter and show the error

	 * string

 Make sure the call is done with the filter */

 Make sure the system still has events */

 Ensure all filters are no longer used */

		/*

		 * No event actually uses the system filter

		 * we can free it without synchronize_rcu().

	/*

	 * The argv_split function takes white space

	 * as a separator, so convert ',' into spaces.

	/*

	 * The 'ip' field could have multiple filters set, separated

	 * either by space or comma. We first cut the filter and apply

	 * all pieces separately.

	/*

	 * Check the predicate for function trace, verify:

	 *  - only '==' and '!=' is used

	 *  - the 'ip' field is used

 Checking the node is valid for function trace. */

	/*

	 * Only "||" is allowed for function events, thus,

	 * all true branches should jump to true, and any

	 * false branch should jump to false.

 True and false have NULL preds (all prog entries should jump to one */

 prog[target].target is 1 for TRUE, 0 for FALSE */

 CONFIG_FUNCTION_TRACER */

 CONFIG_PERF_EVENTS */

 Needed to dereference filter->prog */

		/*

		 * The preemption disabling is not really needed for self

		 * tests, but the rcu dereference will complain without it.

 CONFIG_FTRACE_STARTUP_TEST */

 SPDX-License-Identifier: GPL-2.0

/*

 * nop tracer

 *

 * Copyright (C) 2008 Steven Noonan <steven@uplinklabs.net>

 *

 Our two options */

 Options for the tracer (see trace_options file) */

 Option that will be accepted by set_flag callback */

 Option that will be refused by set_flag callback */

 Always set a last empty entry */

 You can check your flags value here when you want. */

 By default: all flags disabled */

 Nothing to do! */

 Nothing to do! */

/* It only serves as a signal handler and a callback to

 * accept or refuse the setting of a flag.

 * If you don't implement it, then the flag setting will be

 * automatically accepted.

	/*

	 * Note that you don't need to update nop_flags.val yourself.

	 * The tracing Api will do it automatically if you return 0

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic ring buffer

 *

 * Copyright (C) 2008 Steven Rostedt <srostedt@redhat.com>

 for self test */

/*

 * The ring buffer header is special. We must manually up keep it.

/*

 * The ring buffer is made up of a list of pages. A separate list of pages is

 * allocated for each CPU. A writer may only write to a buffer that is

 * associated with the CPU it is currently executing on.  A reader may read

 * from any per cpu buffer.

 *

 * The reader is special. For each per cpu buffer, the reader has its own

 * reader page. When a reader has read the entire reader page, this reader

 * page is swapped with another page in the ring buffer.

 *

 * Now, as long as the writer is off the reader page, the reader can do what

 * ever it wants with that page. The writer will never write to that page

 * again (as long as it is out of the ring buffer).

 *

 * Here's some silly ASCII art.

 *

 *   +------+

 *   |reader|          RING BUFFER

 *   |page  |

 *   +------+        +---+   +---+   +---+

 *                   |   |-->|   |-->|   |

 *                   +---+   +---+   +---+

 *                     ^               |

 *                     |               |

 *                     +---------------+

 *

 *

 *   +------+

 *   |reader|          RING BUFFER

 *   |page  |------------------v

 *   +------+        +---+   +---+   +---+

 *                   |   |-->|   |-->|   |

 *                   +---+   +---+   +---+

 *                     ^               |

 *                     |               |

 *                     +---------------+

 *

 *

 *   +------+

 *   |reader|          RING BUFFER

 *   |page  |------------------v

 *   +------+        +---+   +---+   +---+

 *      ^            |   |-->|   |-->|   |

 *      |            +---+   +---+   +---+

 *      |                              |

 *      |                              |

 *      +------------------------------+

 *

 *

 *   +------+

 *   |buffer|          RING BUFFER

 *   |page  |------------------v

 *   +------+        +---+   +---+   +---+

 *      ^            |   |   |   |-->|   |

 *      |   New      +---+   +---+   +---+

 *      |  Reader------^               |

 *      |   page                       |

 *      +------------------------------+

 *

 *

 * After we make this swap, the reader can hand this page off to the splice

 * code and be done with it. It can even allocate a new page if it needs to

 * and swap that into the ring buffer.

 *

 * We will be using cmpxchg soon to make all this lockless.

 *

 Used for individual buffers (after the counter) */

 two 32bit words */

 define RINGBUF_TYPE_DATA for 'case RINGBUF_TYPE_DATA:' */

 padding has a NULL time_delta */

/*

 * Return the length of the given event. Will return

 * the length of the time extend if the event is a

 * time extend.

 undefined */

 not hit */

/*

 * Return total length of time extend and data,

 *   or just the event length for all other events.

 time extends include the data event after it */

/**

 * ring_buffer_event_length - return the length of the event

 * @event: the event to get the length of

 *

 * Returns the size of the data load of a data event.

 * If the event is something other than a data event, it

 * returns the size of the event itself. With the exception

 * of a TIME EXTEND, where it still returns the size of the

 * data load of the data event after it.

 inline for ring buffer fast paths */

 If length is in len field, then array[0] has the data */

 Otherwise length is in array[0] and array[1] has the data */

/**

 * ring_buffer_event_data - return the data of the event

 * @event: the event to get the data from

 Flag when events were overwritten */

 Missed count stored at end */

 page time stamp */

 write committed index */

 data of buffer page */

/*

 * Note, the buffer_page list must be first. The buffer pages

 * are allocated in cache lines, which means that each buffer

 * page will be at the beginning of a cache line, and thus

 * the least significant bits will be zero. We use this to

 * add flags in the list struct pointers, to make the ring buffer

 * lockless.

 list of buffer pages */

 index for next write */

 index for next read */

 entries on this page */

 real end of data */

 Actual data page */

/*

 * The buffer page counters, write and entries, must be reset

 * atomically when crossing page boundaries. To synchronize this

 * update, two counters are inserted into the number. One is

 * the actual counter for the write position or count on the page.

 *

 * The other is a counter of updaters. Before an update happens

 * the update partition of the counter is incremented. This will

 * allow the updater to update the counter atomically.

 *

 * The counter is 20 bits, and the state data is 12.

/*

 * Also stolen from mm/slob.c. Thanks to Mathieu Desnoyers for pointing

 * this issue out.

/*

 * We need to fit the time_stamp delta into 27 bits.

 Max payload is BUF_PAGE_SIZE - header (8bytes) */

/*

 * Structure to hold event state and handle nested events.

/*

 * Used for the add_timestamp

 *  NONE

 *  EXTEND - wants a time extend

 *  ABSOLUTE - the buffer requests all events to have absolute time stamps

 *  FORCE - force a full time stamp.

/*

 * Used for which event context the event is in.

 *  TRANSITION = 0

 *  NMI     = 1

 *  IRQ     = 2

 *  SOFTIRQ = 3

 *  NORMAL  = 4

 *

 * See trace_recursive_lock() comment below for more details.

 To test on 64 bit machines */

#define RB_TIME_32

/*

 * head_page == tail_page && head == tail then buffer is empty.

 serialize readers */

 read from head */

 write to tail */

 committed pages */

 ring buffer pages to update, > 0 to add, < 0 to remove */

 new pages to add */

/*

 * On 32 bit machines, local64_t is very expensive. As the ring

 * buffer doesn't need all the features of a true 64 bit atomic,

 * on 32 bit, it uses these functions (64 still uses local64_t).

 *

 * For the ring buffer, 64 bit required operations for the time is

 * the following:

 *

 *  - Only need 59 bits (uses 60 to make it even).

 *  - Reads may fail if it interrupted a modification of the time stamp.

 *      It will succeed if it did not interrupt another write even if

 *      the read itself is interrupted by a write.

 *      It returns whether it was successful or not.

 *

 *  - Writes always succeed and will overwrite other writes and writes

 *      that were done by events interrupting the current write.

 *

 *  - A write followed by a read of the same time stamp will always succeed,

 *      but may not contain the same value.

 *

 *  - A cmpxchg will fail if it interrupted another write or cmpxchg.

 *      Other than that, it acts like a normal cmpxchg.

 *

 * The 60 bit time stamp is broken up by 30 bits in a top and bottom half

 *  (bottom being the least significant 30 bits of the 60 bit time stamp).

 *

 * The two most significant bits of each half holds a 2 bit counter (0-3).

 * Each update will increment this counter by one.

 * When reading the top and bottom, if the two counter bits match then the

 *  top and bottom together make a valid 60 bit number.

	/*

	 * If the read is interrupted by a write, then the cnt will

	 * be different. Loop until both top and bottom have been read

	 * without interruption.

 If top and bottom counts don't match, this interrupted a write */

 Writes always succeed with a valid number even if it gets interrupted. */

 The cmpxchg always fails if it interrupted an update */

 64 bits */

 local64_t always succeeds */

/*

 * Enable this to make sure that the event passed to

 * ring_buffer_event_time_stamp() is not committed and also

 * is on the buffer that it passed in.

#define RB_VERIFY_EVENT

 Make sure the event exists and is not committed yet */

/**

 * ring_buffer_event_time_stamp - return the event's current time stamp

 * @buffer: The buffer that the event is on

 * @event: the event to get the time stamp of

 *

 * Note, this must be called after @event is reserved, and before it is

 * committed to the ring buffer. And must be called from the same

 * context where the event was reserved (normal, softirq, irq, etc).

 *

 * Returns the time stamp associated with the current event.

 * If the event has an extended time stamp, then that is used as

 * the time stamp to return.

 * In the highly unlikely case that the event was nested more than

 * the max nesting, then the write_stamp of the buffer is returned,

 * otherwise  current time is returned, but that really neither of

 * the last two cases should ever happen.

 If the event includes an absolute time, then just use that */

 Read the current saved nesting level time stamp */

 Shouldn't happen, warn if it does */

 Can only fail on 32 bit */

 Screw it, just read the current time */

/**

 * ring_buffer_nr_pages - get the number of buffer pages in the ring buffer

 * @buffer: The ring_buffer to get the number of pages from

 * @cpu: The cpu of the ring_buffer to get the number of pages from

 *

 * Returns the number of pages used by a per_cpu buffer of the ring buffer.

/**

 * ring_buffer_nr_pages_dirty - get the number of used pages in the ring buffer

 * @buffer: The ring_buffer to get the number of pages from

 * @cpu: The cpu of the ring_buffer to get the number of pages from

 *

 * Returns the number of pages that have content in the ring buffer.

 The reader can read an empty page, but not more than that */

/*

 * rb_wake_up_waiters - wake up tasks waiting for ring buffer input

 *

 * Schedules a delayed work to wake up any task that is blocked on the

 * ring buffer waiters queue.

/**

 * ring_buffer_wait - wait for input to the ring buffer

 * @buffer: buffer to wait on

 * @cpu: the cpu buffer to wait on

 * @full: wait until the percentage of pages are available, if @cpu != RING_BUFFER_ALL_CPUS

 *

 * If @cpu == RING_BUFFER_ALL_CPUS then the task will wake up as soon

 * as data is added to any of the @buffer's cpu buffers. Otherwise

 * it will wait for data to be added to a specific cpu buffer.

	/*

	 * Depending on what the caller is waiting for, either any

	 * data in any cpu buffer, or a specific buffer, put the

	 * caller on the appropriate wait queue.

 Full only makes sense on per cpu reads */

		/*

		 * The events can happen in critical sections where

		 * checking a work queue can cause deadlocks.

		 * After adding a task to the queue, this flag is set

		 * only to notify events to try to wake up the queue

		 * using irq_work.

		 *

		 * We don't clear it even if the buffer is no longer

		 * empty. The flag only causes the next event to run

		 * irq_work to do the work queue wake up. The worse

		 * that can happen if we race with !trace_empty() is that

		 * an event will cause an irq_work to try to wake up

		 * an empty queue.

		 *

		 * There's no reason to protect this flag either, as

		 * the work queue and irq_work logic will do the necessary

		 * synchronization for the wake ups. The only thing

		 * that is necessary is that the wake up happens after

		 * a task has been queued. It's OK for spurious wake ups.

/**

 * ring_buffer_poll_wait - poll on buffer input

 * @buffer: buffer to wait on

 * @cpu: the cpu buffer to wait on

 * @filp: the file descriptor

 * @poll_table: The poll descriptor

 *

 * If @cpu == RING_BUFFER_ALL_CPUS then the task will wake up as soon

 * as data is added to any of the @buffer's cpu buffers. Otherwise

 * it will wait for data to be added to a specific cpu buffer.

 *

 * Returns EPOLLIN | EPOLLRDNORM if data exists in the buffers,

 * zero otherwise.

	/*

	 * There's a tight race between setting the waiters_pending and

	 * checking if the ring buffer is empty.  Once the waiters_pending bit

	 * is set, the next event will wake the task up, but we can get stuck

	 * if there's only a single event in.

	 *

	 * FIXME: Ideally, we need a memory barrier on the writer side as well,

	 * but adding a memory barrier to all events will cause too much of a

	 * performance hit in the fast path.  We only need a memory barrier when

	 * the buffer goes from empty to having content.  But as this race is

	 * extremely small, and it's not a problem if another event comes in, we

	 * will fix it later.

 buffer may be either ring_buffer or ring_buffer_per_cpu */

 Up this if you want to test the TIME_EXTENTS and normalization */

 Skip retpolines :-( */

 shift to debug/test normalization and TIME_EXTENTS */

 Just stupid testing the normalize function and deltas */

/*

 * Making the ring buffer lockless makes things tricky.

 * Although writes only happen on the CPU that they are on,

 * and they only need to worry about interrupts. Reads can

 * happen on any CPU.

 *

 * The reader page is always off the ring buffer, but when the

 * reader finishes with a page, it needs to swap its page with

 * a new one from the buffer. The reader needs to take from

 * the head (writes go to the tail). But if a writer is in overwrite

 * mode and wraps, it must push the head page forward.

 *

 * Here lies the problem.

 *

 * The reader must be careful to replace only the head page, and

 * not another one. As described at the top of the file in the

 * ASCII art, the reader sets its old page to point to the next

 * page after head. It then sets the page after head to point to

 * the old reader page. But if the writer moves the head page

 * during this operation, the reader could end up with the tail.

 *

 * We use cmpxchg to help prevent this race. We also do something

 * special with the page before head. We set the LSB to 1.

 *

 * When the writer must push the page forward, it will clear the

 * bit that points to the head page, move the head, and then set

 * the bit that points to the new head page.

 *

 * We also don't want an interrupt coming in and moving the head

 * page on another writer. Thus we use the second LSB to catch

 * that too. Thus:

 *

 * head->list->prev->next        bit 1          bit 0

 *                              -------        -------

 * Normal page                     0              0

 * Points to head page             0              1

 * New head page                   1              0

 *

 * Note we can not trust the prev pointer of the head page, because:

 *

 * +----+       +-----+        +-----+

 * |    |------>|  T  |---X--->|  N  |

 * |    |<------|     |        |     |

 * +----+       +-----+        +-----+

 *   ^                           ^ |

 *   |          +-----+          | |

 *   +----------|  R  |----------+ |

 *              |     |<-----------+

 *              +-----+

 *

 * Key:  ---X-->  HEAD flag set in pointer

 *         T      Tail page

 *         R      Reader page

 *         N      Next page

 *

 * (see __rb_reserve_next() to see where this happens)

 *

 *  What the above shows is that the reader just swapped out

 *  the reader page with a page in the buffer, but before it

 *  could make the new header point back to the new page added

 *  it was preempted by a writer. The writer moved forward onto

 *  the new page added by the reader and is about to move forward

 *  again.

 *

 *  You can see, it is legitimate for the previous pointer of

 *  the head (or any page) not to point back to itself. But only

 *  temporarily.

 PAGE_MOVED is not part of the mask */

/*

 * rb_list_head - remove any bit

/*

 * rb_is_head_page - test if the given page is the head page

 *

 * Because the reader may move the head_page pointer, we can

 * not trust what the head page is (it may be pointing to

 * the reader page). But if the next page is a header page,

 * its flags will be non zero.

/*

 * rb_is_reader_page

 *

 * The unique thing about the reader page, is that, if the

 * writer is ever on it, the previous pointer never points

 * back to the reader page.

/*

 * rb_set_list_to_head - set a list_head to be pointing to head.

/*

 * rb_head_page_activate - sets up head page

	/*

	 * Set the previous list pointer to have the HEAD flag.

/*

 * rb_head_page_deactivate - clears head page ptr (for free list)

 Go through the whole list and clear any pointers found. */

 check if the reader took the page */

 sanity check */

	/*

	 * It is possible that the writer moves the header behind

	 * where we started, and we miss in one loop.

	 * A second loop should grab the header, but we'll do

	 * three loops just because I'm paranoid.

/*

 * rb_tail_page_update - move the tail page forward

	/*

	 * The tail page now needs to be moved forward.

	 *

	 * We need to reset the tail page, but without messing

	 * with possible erasing of data brought in by interrupts

	 * that have moved the tail page and are currently on it.

	 *

	 * We add a counter to the write field to denote this.

	/*

	 * Just make sure we have seen our old_write and synchronize

	 * with any interrupts that come in.

	/*

	 * If the tail page is still the same as what we think

	 * it is, then it is up to us to update the tail

	 * pointer.

 Zero the write counter */

		/*

		 * This will only succeed if an interrupt did

		 * not come in and change it. In which case, we

		 * do not want to modify it.

		 *

		 * We add (void) to let the compiler know that we do not care

		 * about the return value of these functions. We use the

		 * cmpxchg to only update if an interrupt did not already

		 * do it for us. If the cmpxchg fails, we don't care.

		/*

		 * No need to worry about races with clearing out the commit.

		 * it only can increment when a commit takes place. But that

		 * only happens in the outer most nested commit.

 Again, either we update tail_page or an interrupt does */

/**

 * rb_check_list - make sure a pointer to a list has the last bits zero

/**

 * rb_check_pages - integrity check of buffer pages

 * @cpu_buffer: CPU buffer with pages to test

 *

 * As a safety measure we check to make sure the data pages have not

 * been corrupted.

 Reset the head page if it exists */

	/*

	 * Check if the available memory is there first.

	 * Note, si_mem_available() only gives us a rough estimate of available

	 * memory. It may not be accurate. But we don't care, we just want

	 * to prevent doing any allocation when it is obvious that it is

	 * not going to succeed.

	/*

	 * __GFP_RETRY_MAYFAIL flag makes sure that the allocation fails

	 * gracefully without invoking oom-killer and the system is not

	 * destabilized.

	/*

	 * If a user thread allocates too much, and si_mem_available()

	 * reports there's enough memory, even though there is not.

	 * Make sure the OOM killer kills this thread. This can happen

	 * even with RETRY_MAYFAIL because another task may be doing

	 * an allocation after this task has taken all memory.

	 * This is the task the OOM killer needs to take out during this

	 * loop, even if it was triggered by an allocation somewhere else.

	/*

	 * The ring buffer page list is a circular list that does not

	 * start and end with a list head. All page list items point to

	 * other pages.

/**

 * __ring_buffer_alloc - allocate a new ring_buffer

 * @size: the size in bytes per cpu that is needed.

 * @flags: attributes to set for the ring buffer.

 * @key: ring buffer reader_lock_key.

 *

 * Currently the only flag that is available is the RB_FL_OVERWRITE

 * flag. This flag means that the buffer will overwrite old data

 * when the buffer wraps. If this flag is not set, the buffer will

 * drop data when the tail hits the head.

 keep it in its own cache line */

 need at least two pages */

/**

 * ring_buffer_free - free a ring buffer.

 * @buffer: the buffer to free.

	/*

	 * We don't race with the readers since we have acquired the reader

	 * lock. We also don't race with writers after disabling recording.

	 * This makes it easy to figure out the first and the last page to be

	 * removed from the list. We unlink all the pages in between including

	 * the first and last pages. This is done in a busy loop so that we

	 * lose the least number of traces.

	 * The pages are freed after we restart recording and unlock readers.

	/*

	 * tail page might be on reader page, we remove the next page

	 * from the ring buffer

 start of pages to remove */

	/*

	 * Now we remove all pages between tail_page and next_page.

	 * Make sure that we have head_bit value preserved for the

	 * next page

 make sure pages points to a valid page in the ring buffer */

 update head page */

	/*

	 * change read pointer to make sure any read iterators reset

	 * themselves

 pages are removed, resume tracing and then free the pages */

 last buffer page to remove */

 update the counters */

			/*

			 * If something was added to this page, it was full

			 * since it is not the tail page. So we deduct the

			 * bytes consumed in ring buffer from here.

			 * Increment overrun to account for the lost events.

		/*

		 * We have already removed references to this list item, just

		 * free up the buffer_page and its page

	/*

	 * We are holding the reader lock, so the reader page won't be swapped

	 * in the ring buffer. Now we are racing with the writer trying to

	 * move head page and the tail page.

	 * We are going to adapt the reader page update process where:

	 * 1. We first splice the start and end of list of new pages between

	 *    the head page and its previous page.

	 * 2. We cmpxchg the prev_page->next to point from head page to the

	 *    start of new pages list.

	 * 3. Finally, we update the head->prev to the end of new list.

	 *

	 * We will try this process 10 times, to make sure that we don't keep

	 * spinning.

			/*

			 * yay, we replaced the page pointer to our new list,

			 * now, we just have to update to head page's prev

			 * pointer to point to end of list

	/*

	 * If we weren't successful in adding in new pages, warn and stop

	 * tracing

 free pages if they weren't inserted */

/**

 * ring_buffer_resize - resize the ring buffer

 * @buffer: the buffer to resize.

 * @size: the new size.

 * @cpu_id: the cpu buffer to resize

 *

 * Minimum size is 2 * BUF_PAGE_SIZE.

 *

 * Returns 0 on success and < 0 on failure.

	/*

	 * Always succeed at resizing a non-existent buffer:

 Make sure the requested buffer exists */

 we need a minimum of two pages */

 prevent another thread from changing buffer sizes */

		/*

		 * Don't succeed if resizing is disabled, as a reader might be

		 * manipulating the ring buffer and is expecting a sane state while

		 * this is true.

 calculate the pages to update */

			/*

			 * nothing more to do for removing pages or no update

			/*

			 * to add pages, make sure all new pages can be

			 * allocated without receiving ENOMEM

 not enough memory for new pages */

		/*

		 * Fire off all the required work handlers

		 * We can't schedule on offline CPUs, but it's not necessary

		 * since we can change their buffer sizes without any race.

 Can't run something on an offline CPU. */

 wait for all the updates to complete */

		/*

		 * Don't succeed if resizing is disabled, as a reader might be

		 * manipulating the ring buffer and is expecting a sane state while

		 * this is true.

 Can't run something on an offline CPU. */

	/*

	 * The ring buffer resize can happen with the ring buffer

	 * enabled, so that the update disturbs the tracing as little

	 * as possible. But if the buffer is disabled, we do not need

	 * to worry about that, and we can take the time to verify

	 * that the buffer is not corrupt.

		/*

		 * Even though the buffer was disabled, we must make sure

		 * that it is truly disabled before calling rb_check_pages.

		 * There could have been a race between checking

		 * record_disable and incrementing it.

	/*

	 * When the writer goes across pages, it issues a cmpxchg which

	 * is a mb(), which will synchronize with the rmb here.

	 * (see rb_tail_page_update() and __rb_reserve_next())

	/*

	 * READ_ONCE() doesn't work on functions and we don't want the

	 * compiler doing any crazy optimizations with length.

 Writer corrupted the read? */

	/*

	 * If the page stamp is still the same after this rmb() then the

	 * event was safely copied without the writer entering the page.

 Make sure the page didn't change since we read this */

 Reset to the beginning */

 Size is determined by what has been committed */

	/*

	 * The iterator could be on the reader page (it starts there).

	 * But the head could have moved, since the reader was

	 * found. Check for this case and assign the iterator

	 * to the head page instead of next.

/*

 * rb_handle_head_page - writer hit the head page

 *

 * Returns: +1 to retry page

 *           0 to continue

 *          -1 on error

	/*

	 * The hard part is here. We need to move the head

	 * forward, and protect against both readers on

	 * other CPUs and writers coming in via interrupts.

	/*

	 * type can be one of four:

	 *  NORMAL - an interrupt already moved it for us

	 *  HEAD   - we are the first to get here.

	 *  UPDATE - we are the interrupt interrupting

	 *           a current move.

	 *  MOVED  - a reader on another CPU moved the next

	 *           pointer to its reader page. Give up

	 *           and try again.

		/*

		 * We changed the head to UPDATE, thus

		 * it is our responsibility to update

		 * the counters.

		/*

		 * The entries will be zeroed out when we move the

		 * tail page.

 still more to do */

		/*

		 * This is an interrupt that interrupt the

		 * previous update. Still more to do.

		/*

		 * An interrupt came in before the update

		 * and processed this for us.

		 * Nothing left to do.

		/*

		 * The reader is on another CPU and just did

		 * a swap with our next_page.

		 * Try again.

 WTF??? */

	/*

	 * Now that we are here, the old head pointer is

	 * set to UPDATE. This will keep the reader from

	 * swapping the head page with the reader page.

	 * The reader (on another CPU) will spin till

	 * we are finished.

	 *

	 * We just need to protect against interrupts

	 * doing the job. We will set the next pointer

	 * to HEAD. After that, we set the old pointer

	 * to NORMAL, but only if it was HEAD before.

	 * otherwise we are an interrupt, and only

	 * want the outer most commit to reset it.

	/*

	 * Valid returns are:

	 *  HEAD   - an interrupt came in and already set it.

	 *  NORMAL - One of two things:

	 *            1) We really set it.

	 *            2) A bunch of interrupts came in and moved

	 *               the page forward again.

 OK */

	/*

	 * It is possible that an interrupt came in,

	 * set the head up, then more interrupts came in

	 * and moved it again. When we get back here,

	 * the page would have been set to NORMAL but we

	 * just set it back to HEAD.

	 *

	 * How do you detect this? Well, if that happened

	 * the tail page would have moved.

		/*

		 * If the tail had moved passed next, then we need

		 * to reset the pointer.

	/*

	 * If this was the outer most commit (the one that

	 * changed the original pointer from HEAD to UPDATE),

	 * then it is up to us to reset it to NORMAL.

	/*

	 * Only the event that crossed the page boundary

	 * must fill the old tail_page with padding.

		/*

		 * If the page was filled, then we still need

		 * to update the real_end. Reset it to zero

		 * and the reader will ignore it.

 account for padding bytes */

	/*

	 * Save the original length to the meta data.

	 * This will be used by the reader to add lost event

	 * counter.

	/*

	 * If this event is bigger than the minimum size, then

	 * we need to be careful that we don't subtract the

	 * write counter enough to allow another writer to slip

	 * in on this page.

	 * We put in a discarded commit instead, to make sure

	 * that this space is not used again.

	 *

	 * If we are less than the minimum size, we don't need to

	 * worry about it.

 No room for any events */

 Mark the rest of the page with padding */

 Set the write back to the previous setting */

 Put in a discarded event */

 time delta must be non zero */

 Set write to end of buffer */

/*

 * This is the slow path, force gcc not to inline it.

	/*

	 * If for some reason, we had an interrupt storm that made

	 * it all the way around the buffer, bail, and warn

	 * about it.

	/*

	 * This is where the fun begins!

	 *

	 * We are fighting against races between a reader that

	 * could be on another CPU trying to swap its reader

	 * page with the buffer head.

	 *

	 * We are also fighting against interrupts coming in and

	 * moving the head or tail on us as well.

	 *

	 * If the next page is the head page then we have filled

	 * the buffer, unless the commit page is still on the

	 * reader page.

		/*

		 * If the commit is not on the reader page, then

		 * move the header page.

			/*

			 * If we are not in overwrite mode,

			 * this is easy, just stop here.

			/*

			 * We need to be careful here too. The

			 * commit page could still be on the reader

			 * page. We could have a small buffer, and

			 * have filled up the buffer with events

			 * from interrupts and such, and wrapped.

			 *

			 * Note, if the tail page is also on the

			 * reader_page, we let it move out.

 Commit what we have for now. */

 rb_end_commit() decs committing */

 fail and let the caller try again */

 reset write */

 Slow path */

 Not the first event on the page, or not delta? */

 nope, just zero it */

 did the clock go backwards */

 not interrupted */

			/*

			 * This is possible with a recalibrating of the TSC.

			 * Do not produce a call stack, but just report it.

/**

 * rb_update_event - update event type and data

 * @cpu_buffer: The per cpu buffer of the @event

 * @event: the event to update

 * @info: The info to update the @event with (contains length and delta)

 *

 * Update the type and data fields of the @event. The length

 * is the actual size that is written to the ring buffer,

 * and with this, we can determine what to place into the

 * data field.

	/*

	 * If we need to add a timestamp, then we

	 * add it to the start of the reserved space.

 Used only for sizeof array */

 zero length can cause confusions */

	/*

	 * In case the time delta is larger than the 27 bits for it

	 * in the header, we need to add a timestamp. If another

	 * event comes in when trying to discard this one to increase

	 * the length, then the timestamp will be added in the allocated

	 * space of this event. If length is bigger than the size needed

	 * for the TIME_EXTEND, then padding has to be used. The events

	 * length must be either RB_LEN_TIME_EXTEND, or greater than or equal

	 * to RB_LEN_TIME_EXTEND + 8, as 8 is the minimum size for padding.

	 * As length is a multiple of 4, we only need to worry if it

	 * is 12 (RB_LEN_TIME_EXTEND + 4).

 Make sure the write stamp is read before testing the location */

 Something came in, can't discard */

		/*

		 * It's possible that the event time delta is zero

		 * (has the same time stamp as the previous event)

		 * in which case write_stamp and before_stamp could

		 * be the same. In such a case, force before_stamp

		 * to be different than write_stamp. It doesn't

		 * matter what it is, as long as its different.

		/*

		 * If an event were to come in now, it would see that the

		 * write_stamp and the before_stamp are different, and assume

		 * that this event just added itself before updating

		 * the write stamp. The interrupting event will fix the

		 * write stamp for us, and use the before stamp as its delta.

		/*

		 * This is on the tail page. It is possible that

		 * a write could come in and move the tail page

		 * and write to the next page. That is fine

		 * because we just shorten what is on this page.

 update counters */

 could not discard */

	/*

	 * We only race with interrupts and NMIs on this CPU.

	 * If we own the commit event, then we can commit

	 * all others that interrupted us, since the interruptions

	 * are in stack format (they finish before they come

	 * back to us). This allows us to do a simple loop to

	 * assign the commit to the tail.

 add barrier to keep gcc from optimizing too much */

 again, keep gcc from optimizing */

	/*

	 * If an interrupt came in just after the first while loop

	 * and pushed the tail page forward, we will be left with

	 * a dangling commit that will never go forward.

 synchronize with interrupts */

 synchronize with interrupts */

	/*

	 * Need to account for interrupts coming in between the

	 * updating of the commit page and the clearing of the

	 * committing counter.

 array[0] holds the actual length for the discarded event */

 time delta must be non zero */

 irq_work_queue() supplies it's own memory barriers */

 irq_work_queue() supplies it's own memory barriers */

 irq_work_queue() supplies it's own memory barriers */

/*

 * The lock and unlock are done within a preempt disable section.

 * The current_context per_cpu variable can only be modified

 * by the current task between lock and unlock. But it can

 * be modified more than once via an interrupt. To pass this

 * information from the lock to the unlock without having to

 * access the 'in_interrupt()' functions again (which do show

 * a bit of overhead in something as critical as function tracing,

 * we use a bitmask trick.

 *

 *  bit 1 =  NMI context

 *  bit 2 =  IRQ context

 *  bit 3 =  SoftIRQ context

 *  bit 4 =  normal context.

 *

 * This works because this is the order of contexts that can

 * preempt other contexts. A SoftIRQ never preempts an IRQ

 * context.

 *

 * When the context is determined, the corresponding bit is

 * checked and set (if it was set, then a recursion of that context

 * happened).

 *

 * On unlock, we need to clear this bit. To do so, just subtract

 * 1 from the current_context and AND it to itself.

 *

 * (binary)

 *  101 - 1 = 100

 *  101 & 100 = 100 (clearing bit zero)

 *

 *  1010 - 1 = 1001

 *  1010 & 1001 = 1000 (clearing bit 1)

 *

 * The least significant bit can be cleared this way, and it

 * just so happens that it is the same bit corresponding to

 * the current context.

 *

 * Now the TRANSITION bit breaks the above slightly. The TRANSITION bit

 * is set when a recursion is detected at the current context, and if

 * the TRANSITION bit is already set, it will fail the recursion.

 * This is needed because there's a lag between the changing of

 * interrupt context and updating the preempt count. In this case,

 * a false positive will be found. To handle this, one extra recursion

 * is allowed, and this is done by the TRANSITION bit. If the TRANSITION

 * bit is already set, then it is considered a recursion and the function

 * ends. Otherwise, the TRANSITION bit is set, and that bit is returned.

 *

 * On the trace_recursive_unlock(), the TRANSITION bit will be the first

 * to be cleared. Even if it wasn't the context that set it. That is,

 * if an interrupt comes in while NORMAL bit is set and the ring buffer

 * is called before preempt_count() is updated, since the check will

 * be on the NORMAL bit, the TRANSITION bit will then be set. If an

 * NMI then comes in, it will set the NMI bit, but when the NMI code

 * does the trace_recursive_unlock() it will clear the TRANSITION bit

 * and leave the NMI bit set. But this is fine, because the interrupt

 * code that set the TRANSITION bit will then clear the NMI bit when it

 * calls trace_recursive_unlock(). If another NMI comes in, it will

 * set the TRANSITION bit and continue.

 *

 * Note: The TRANSITION bit only handles a single transition between context.

		/*

		 * It is possible that this was called by transitioning

		 * between interrupt context, and preempt_count() has not

		 * been updated yet. In this case, use the TRANSITION bit.

 The recursive locking above uses 5 bits */

/**

 * ring_buffer_nest_start - Allow to trace while nested

 * @buffer: The ring buffer to modify

 *

 * The ring buffer has a safety mechanism to prevent recursion.

 * But there may be a case where a trace needs to be done while

 * tracing something else. In this case, calling this function

 * will allow this function to nest within a currently active

 * ring_buffer_lock_reserve().

 *

 * Call this function before calling another ring_buffer_lock_reserve() and

 * call ring_buffer_nest_end() after the nested ring_buffer_unlock_commit().

 Enabled by ring_buffer_nest_end() */

 This is the shift value for the above recursive locking */

/**

 * ring_buffer_nest_end - Allow to trace while nested

 * @buffer: The ring buffer to modify

 *

 * Must be called after ring_buffer_nest_start() and after the

 * ring_buffer_unlock_commit().

 disabled by ring_buffer_nest_start() */

 This is the shift value for the above recursive locking */

/**

 * ring_buffer_unlock_commit - commit a reserved

 * @buffer: The buffer to commit to

 * @event: The event pointer to commit.

 *

 * This commits the data to the ring buffer, and releases any locks held.

 *

 * Must be paired with ring_buffer_lock_reserve.

 Special value to validate all deltas on a page. */

/*

 * Check if the current event time stamp matches the deltas on

 * the buffer page.

 Ignore events with absolute time stamps */

	/*

	 * Do not check the first event (skip possible extends too).

	 * Also do not check if previous events have not been committed.

	/*

	 * If this interrupted another event, 

 If another report is happening, ignore this one */

 There's some cases in boot up that this can happen */

 Do not re-enable checking */

 CONFIG_RING_BUFFER_VALIDATE_TIME_DELTAS */

 Don't let the compiler play games with cpu_buffer->tail_page */

A*/	w = local_read(&tail_page->write) & RB_WRITE_MASK;

		/*

		 * If interrupting an event time update, we may need an

		 * absolute timestamp.

		 * Don't bother if this is the start of a new page (w == 0).

B*/	rb_time_set(&cpu_buffer->before_stamp, info->ts);

C*/	write = local_add_return(info->length, &tail_page->write);

 set write to only the index of the write */

 See if we shot pass the end of this buffer page */

 before and after may now different, fix it up*/

 Nothing interrupted us between A and C */

D*/		rb_time_set(&cpu_buffer->write_stamp, info->ts);

E*/		s_ok = rb_time_read(&cpu_buffer->before_stamp, &save_before);

 This did not interrupt any time update */

 Just use full timestamp for interrupting event */

 SLOW PATH - Interrupted between C and E */

 Write stamp must only go forward */

				/*

				 * We do not care about the result, only that

				 * it gets updated atomically.

 SLOW PATH - Interrupted between A and C */

 Was interrupted before here, write_stamp must be valid */

E*/		if (write == (local_read(&tail_page->write) & RB_WRITE_MASK) &&

 Nothing came after this event between C and E */

			/*

			 * Interrupted between C and E:

			 * Lost the previous events time stamp. Just set the

			 * delta to zero, and this will be the same time as

			 * the event this event interrupted. And the events that

			 * came after this will still be correct (as they would

			 * have built their delta on the previous event.

	/*

	 * If this is the first commit on the page, then it has the same

	 * timestamp as the page itself.

 We reserved something on the buffer */

	/*

	 * If this is the first commit on the page, then update

	 * its timestamp.

 account for these added bytes */

 The commit page can not change after this */

	/*

	 * Due to the ability to swap a cpu buffer from a buffer

	 * it is possible it was swapped before we committed.

	 * (committing stops a swap). We check for it here and

	 * if it happened, we have to fail the write.

	/*

	 * We allow for interrupts to reenter here and do a trace.

	 * If one does, it will cause this original code to loop

	 * back here. Even with heavy interrupts happening, this

	 * should only happen a few times in a row. If this happens

	 * 1000 times in a row, there must be either an interrupt

	 * storm or we have something buggy.

	 * Bail!

/**

 * ring_buffer_lock_reserve - reserve a part of the buffer

 * @buffer: the ring buffer to reserve from

 * @length: the length of the data to reserve (excluding event header)

 *

 * Returns a reserved event on the ring buffer to copy directly to.

 * The user of this interface will need to get the body to write into

 * and can use the ring_buffer_event_data() interface.

 *

 * The length is the length of the data needed, not the event length

 * which also includes the event header.

 *

 * Must be paired with ring_buffer_unlock_commit, unless NULL is returned.

 * If NULL is returned, then nothing has been allocated or locked.

 If we are tracing schedule, we don't want to recurse */

/*

 * Decrement the entries to the page that an event is on.

 * The event does not even need to exist, only the pointer

 * to the page it is on. This may only be called before the commit

 * takes place.

 Do the likely case first */

	/*

	 * Because the commit page may be on the reader page we

	 * start with the next page and check the end loop there.

 commit not part of this buffer?? */

/**

 * ring_buffer_discard_commit - discard an event that has not been committed

 * @buffer: the ring buffer

 * @event: non committed event to discard

 *

 * Sometimes an event that is in the ring buffer needs to be ignored.

 * This function lets the user discard an event in the ring buffer

 * and then that event will not be read later.

 *

 * This function only works if it is called before the item has been

 * committed. It will try to free the event from the ring buffer

 * if another event has not been added behind it.

 *

 * If another event has been added behind it, it will set the event

 * up as discarded, and perform the commit.

 *

 * If this function is called, do not call ring_buffer_unlock_commit on

 * the event.

 The event is discarded regardless */

	/*

	 * This must only be called if the event has not been

	 * committed yet. Thus we can assume that preemption

	 * is still disabled.

/**

 * ring_buffer_write - write data to the buffer without reserving

 * @buffer: The ring buffer to write to.

 * @length: The length of the data being written (excluding the event header)

 * @data: The data to write to the buffer.

 *

 * This is like ring_buffer_lock_reserve and ring_buffer_unlock_commit as

 * one function. If you already have the data to write to the buffer, it

 * may be easier to simply call this function.

 *

 * Note, like ring_buffer_lock_reserve, the length is the length of the data

 * and not the length of the event which would hold the header.

 In case of error, head will be NULL */

 Reader should exhaust content in reader page */

	/*

	 * If writers are committing on the reader page, knowing all

	 * committed content has been read, the ring buffer is empty.

	/*

	 * If writers are committing on a page other than reader page

	 * and head page, there should always be content to read.

	/*

	 * Writers are committing on the head page, we just need

	 * to care about there're committed data, and the reader will

	 * swap reader page with head page when it is to read data.

/**

 * ring_buffer_record_disable - stop all writes into the buffer

 * @buffer: The ring buffer to stop writes to.

 *

 * This prevents all writes to the buffer. Any attempt to write

 * to the buffer after this will fail and return NULL.

 *

 * The caller should call synchronize_rcu() after this.

/**

 * ring_buffer_record_enable - enable writes to the buffer

 * @buffer: The ring buffer to enable writes

 *

 * Note, multiple disables will need the same number of enables

 * to truly enable the writing (much like preempt_disable).

/**

 * ring_buffer_record_off - stop all writes into the buffer

 * @buffer: The ring buffer to stop writes to.

 *

 * This prevents all writes to the buffer. Any attempt to write

 * to the buffer after this will fail and return NULL.

 *

 * This is different than ring_buffer_record_disable() as

 * it works like an on/off switch, where as the disable() version

 * must be paired with a enable().

/**

 * ring_buffer_record_on - restart writes into the buffer

 * @buffer: The ring buffer to start writes to.

 *

 * This enables all writes to the buffer that was disabled by

 * ring_buffer_record_off().

 *

 * This is different than ring_buffer_record_enable() as

 * it works like an on/off switch, where as the enable() version

 * must be paired with a disable().

/**

 * ring_buffer_record_is_on - return true if the ring buffer can write

 * @buffer: The ring buffer to see if write is enabled

 *

 * Returns true if the ring buffer is in a state that it accepts writes.

/**

 * ring_buffer_record_is_set_on - return true if the ring buffer is set writable

 * @buffer: The ring buffer to see if write is set enabled

 *

 * Returns true if the ring buffer is set writable by ring_buffer_record_on().

 * Note that this does NOT mean it is in a writable state.

 *

 * It may return true when the ring buffer has been disabled by

 * ring_buffer_record_disable(), as that is a temporary disabling of

 * the ring buffer.

/**

 * ring_buffer_record_disable_cpu - stop all writes into the cpu_buffer

 * @buffer: The ring buffer to stop writes to.

 * @cpu: The CPU buffer to stop

 *

 * This prevents all writes to the buffer. Any attempt to write

 * to the buffer after this will fail and return NULL.

 *

 * The caller should call synchronize_rcu() after this.

/**

 * ring_buffer_record_enable_cpu - enable writes to the buffer

 * @buffer: The ring buffer to enable writes

 * @cpu: The CPU to enable.

 *

 * Note, multiple disables will need the same number of enables

 * to truly enable the writing (much like preempt_disable).

/*

 * The total entries in the ring buffer is the running counter

 * of entries entered into the ring buffer, minus the sum of

 * the entries read from the ring buffer and the number of

 * entries that were overwritten.

/**

 * ring_buffer_oldest_event_ts - get the oldest event timestamp from the buffer

 * @buffer: The ring buffer

 * @cpu: The per CPU buffer to read from.

	/*

	 * if the tail is on reader_page, oldest time stamp is on the reader

	 * page

/**

 * ring_buffer_bytes_cpu - get the number of bytes consumed in a cpu buffer

 * @buffer: The ring buffer

 * @cpu: The per CPU buffer to read from.

/**

 * ring_buffer_entries_cpu - get the number of entries in a cpu buffer

 * @buffer: The ring buffer

 * @cpu: The per CPU buffer to get the entries from.

/**

 * ring_buffer_overrun_cpu - get the number of overruns caused by the ring

 * buffer wrapping around (only if RB_FL_OVERWRITE is on).

 * @buffer: The ring buffer

 * @cpu: The per CPU buffer to get the number of overruns from

/**

 * ring_buffer_commit_overrun_cpu - get the number of overruns caused by

 * commits failing due to the buffer wrapping around while there are uncommitted

 * events, such as during an interrupt storm.

 * @buffer: The ring buffer

 * @cpu: The per CPU buffer to get the number of overruns from

/**

 * ring_buffer_dropped_events_cpu - get the number of dropped events caused by

 * the ring buffer filling up (only if RB_FL_OVERWRITE is off).

 * @buffer: The ring buffer

 * @cpu: The per CPU buffer to get the number of overruns from

/**

 * ring_buffer_read_events_cpu - get the number of events successfully read

 * @buffer: The ring buffer

 * @cpu: The per CPU buffer to get the number of events read

/**

 * ring_buffer_entries - get the number of entries in a buffer

 * @buffer: The ring buffer

 *

 * Returns the total number of entries in the ring buffer

 * (all CPU entries)

 if you care about this being correct, lock the buffer */

/**

 * ring_buffer_overruns - get the number of overruns in buffer

 * @buffer: The ring buffer

 *

 * Returns the total number of overruns in the ring buffer

 * (all CPU entries)

 if you care about this being correct, lock the buffer */

 Iterator usage is expected to have record disabled */

/**

 * ring_buffer_iter_reset - reset an iterator

 * @iter: The iterator to reset

 *

 * Resets the iterator, so that it will start from the beginning

 * again.

/**

 * ring_buffer_iter_empty - check if an iterator has no more to read

 * @iter: The iterator to check

	/*

	 * When the writer goes across pages, it issues a cmpxchg which

	 * is a mb(), which will synchronize with the rmb here.

	 * (see rb_tail_page_update())

 We want to make sure that the commit page doesn't change */

 Make sure commit page didn't change */

 If the commit page changed, then there's more data */

 Still racy, as it may return a false positive, but that's OK */

	/*

	 * This should normally only loop twice. But because the

	 * start of the reader inserts an empty page, it causes

	 * a case where we will loop three times. There should be no

	 * reason to loop four times (that I know of).

 If there's more to read, return this page */

 Never should we have an index greater than the size */

 check if we caught up to the tail */

 Don't bother swapping if the ring buffer is empty */

	/*

	 * Reset the reader page to size zero.

	/*

	 * Splice the empty reader page into the list around the head.

	/*

	 * cpu_buffer->pages just needs to point to the buffer, it

	 *  has no specific buffer page to point to. Lets move it out

	 *  of our way so we don't accidentally swap it.

 The reader page will be pointing to the new head */

	/*

	 * We want to make sure we read the overruns after we set up our

	 * pointers to the next object. The writer side does a

	 * cmpxchg to cross pages which acts as the mb on the writer

	 * side. Note, the reader will constantly fail the swap

	 * while the writer is updating the pointers, so this

	 * guarantees that the overwrite recorded here is the one we

	 * want to compare with the last_overrun.

	/*

	 * Here's the tricky part.

	 *

	 * We need to move the pointer past the header page.

	 * But we can only do that if a writer is not currently

	 * moving it. The page before the header page has the

	 * flag bit '1' set if it is pointing to the page we want.

	 * but if the writer is in the process of moving it

	 * than it will be '2' or already moved '0'.

	/*

	 * If we did not convert it, then we must try again.

	/*

	 * Yay! We succeeded in replacing the page.

	 *

	 * Now make the new head point back to the reader page.

 Finally update the reader page to the new head */

 Update the read_stamp on the first event */

 This function should not be called when buffer is empty */

 If head == next_event then we need to jump to the next event */

 If the event gets overwritten again, there's nothing to do */

	/*

	 * Check if we are at the end of the buffer.

 discarded commits can make the page empty */

	/*

	 * We repeat when a time extend is encountered.

	 * Since the time extend is always attached to a data event,

	 * we should never loop more than once.

	 * (We never hit the following condition more than twice).

		/*

		 * Because the writer could be discarding every

		 * event it creates (which would probably be bad)

		 * if we were to go back to "again" then we may never

		 * catch up, and will trigger the warn on, or lock

		 * the box. Return the padding, and we will release

		 * the current locks, and try again.

 Internal data, OK to advance */

 Internal data, OK to advance */

	/*

	 * Check if someone performed a consuming read to

	 * the buffer. A consuming read invalidates the iterator

	 * and we need to reset the iterator in this case.

	/*

	 * As the writer can mess with what the iterator is trying

	 * to read, just give up if we fail to get an event after

	 * three tries. The iterator is not as reliable when reading

	 * the ring buffer with an active write as the consumer is.

	 * Do not warn if the three failures is reached.

 Internal data, OK to advance */

 Internal data, OK to advance */

	/*

	 * If an NMI die dumps out the content of the ring buffer

	 * trylock must be used to prevent a deadlock if the NMI

	 * preempted a task that holds the ring buffer locks. If

	 * we get the lock then all is fine, if not, then continue

	 * to do the read, but this can corrupt the ring buffer,

	 * so it must be permanently disabled from future writes.

	 * Reading from NMI is a oneshot deal.

 Continue without locking, but disable the ring buffer */

/**

 * ring_buffer_peek - peek at the next event to be read

 * @buffer: The ring buffer to read

 * @cpu: The cpu to peak at

 * @ts: The timestamp counter of this event.

 * @lost_events: a variable to store if events were lost (may be NULL)

 *

 * This will return the event that will be read next, but does

 * not consume the data.

/** ring_buffer_iter_dropped - report if there are dropped events

 * @iter: The ring buffer iterator

 *

 * Returns true if there was dropped events since the last peek.

/**

 * ring_buffer_iter_peek - peek at the next event to be read

 * @iter: The ring buffer iterator

 * @ts: The timestamp counter of this event.

 *

 * This will return the event that will be read next, but does

 * not increment the iterator.

/**

 * ring_buffer_consume - return an event and consume it

 * @buffer: The ring buffer to get the next event from

 * @cpu: the cpu to read the buffer from

 * @ts: a variable to store the timestamp (may be NULL)

 * @lost_events: a variable to store if events were lost (may be NULL)

 *

 * Returns the next event in the ring buffer, and that event is consumed.

 * Meaning, that sequential reads will keep returning a different event,

 * and eventually empty the ring buffer if the producer is slower.

 might be called in atomic */

/**

 * ring_buffer_read_prepare - Prepare for a non consuming read of the buffer

 * @buffer: The ring buffer to read from

 * @cpu: The cpu buffer to iterate over

 * @flags: gfp flags to use for memory allocation

 *

 * This performs the initial preparations necessary to iterate

 * through the buffer.  Memory is allocated, buffer recording

 * is disabled, and the iterator pointer is returned to the caller.

 *

 * Disabling buffer recording prevents the reading from being

 * corrupted. This is not a consuming read, so a producer is not

 * expected.

 *

 * After a sequence of ring_buffer_read_prepare calls, the user is

 * expected to make at least one call to ring_buffer_read_prepare_sync.

 * Afterwards, ring_buffer_read_start is invoked to get things going

 * for real.

 *

 * This overall must be paired with ring_buffer_read_finish.

/**

 * ring_buffer_read_prepare_sync - Synchronize a set of prepare calls

 *

 * All previously invoked ring_buffer_read_prepare calls to prepare

 * iterators will be synchronized.  Afterwards, read_buffer_read_start

 * calls on those iterators are allowed.

/**

 * ring_buffer_read_start - start a non consuming read of the buffer

 * @iter: The iterator returned by ring_buffer_read_prepare

 *

 * This finalizes the startup of an iteration through the buffer.

 * The iterator comes from a call to ring_buffer_read_prepare and

 * an intervening ring_buffer_read_prepare_sync must have been

 * performed.

 *

 * Must be paired with ring_buffer_read_finish.

/**

 * ring_buffer_read_finish - finish reading the iterator of the buffer

 * @iter: The iterator retrieved by ring_buffer_start

 *

 * This re-enables the recording to the buffer, and frees the

 * iterator.

	/*

	 * Ring buffer is disabled from recording, here's a good place

	 * to check the integrity of the ring buffer.

	 * Must prevent readers from trying to read, as the check

	 * clears the HEAD page and readers require it.

/**

 * ring_buffer_iter_advance - advance the iterator to the next location

 * @iter: The ring buffer iterator

 *

 * Move the location of the iterator such that the next read will

 * be the next location of the iterator.

/**

 * ring_buffer_size - return the size of the ring buffer (in bytes)

 * @buffer: The ring buffer.

 * @cpu: The CPU to get ring buffer size from.

	/*

	 * Earlier, this method returned

	 *	BUF_PAGE_SIZE * buffer->nr_pages

	 * Since the nr_pages field is now removed, we have converted this to

	 * return the per cpu buffer value.

 Must have disabled the cpu buffer then done a synchronize_rcu */

/**

 * ring_buffer_reset_cpu - reset a ring buffer per CPU buffer

 * @buffer: The ring buffer to reset a per cpu buffer of

 * @cpu: The CPU buffer to be reset

 prevent another thread from changing buffer sizes */

 Make sure all commits have finished */

/**

 * ring_buffer_reset_cpu - reset a ring buffer per CPU buffer

 * @buffer: The ring buffer to reset a per cpu buffer of

 * @cpu: The CPU buffer to be reset

 prevent another thread from changing buffer sizes */

 Make sure all commits have finished */

/**

 * ring_buffer_reset - reset a ring buffer

 * @buffer: The ring buffer to reset all cpu buffers

 prevent another thread from changing buffer sizes */

 Make sure all commits have finished */

/**

 * rind_buffer_empty - is the ring buffer empty?

 * @buffer: The ring buffer to test

 yes this is racy, but if you don't like the race, lock the buffer */

/**

 * ring_buffer_empty_cpu - is a cpu buffer of a ring buffer empty?

 * @buffer: The ring buffer

 * @cpu: The CPU buffer to test

/**

 * ring_buffer_swap_cpu - swap a CPU buffer between two ring buffers

 * @buffer_a: One buffer to swap with

 * @buffer_b: The other buffer to swap with

 * @cpu: the CPU of the buffers to swap

 *

 * This function is useful for tracers that want to take a "snapshot"

 * of a CPU buffer and has another back up buffer lying around.

 * it is expected that the tracer handles the cpu buffer not being

 * used at the moment.

 At least make sure the two buffers are somewhat the same */

	/*

	 * We can't do a synchronize_rcu here because this

	 * function can be called in atomic context.

	 * Normally this will be called from the same CPU as cpu.

	 * If not it's up to the caller to protect this.

 CONFIG_RING_BUFFER_ALLOW_SWAP */

/**

 * ring_buffer_alloc_read_page - allocate a page to read from buffer

 * @buffer: the buffer to allocate for.

 * @cpu: the cpu buffer to allocate.

 *

 * This function is used in conjunction with ring_buffer_read_page.

 * When reading a full page from the ring buffer, these functions

 * can be used to speed up the process. The calling function should

 * allocate a few pages first with this function. Then when it

 * needs to get pages from the ring buffer, it passes the result

 * of this function into ring_buffer_read_page, which will swap

 * the page that was allocated, with the read page of the buffer.

 *

 * Returns:

 *  The page allocated, or ERR_PTR

/**

 * ring_buffer_free_read_page - free an allocated read page

 * @buffer: the buffer the page was allocate for

 * @cpu: the cpu buffer the page came from

 * @data: the page to free

 *

 * Free a page allocated from ring_buffer_alloc_read_page.

 If the page is still in use someplace else, we can't reuse it */

/**

 * ring_buffer_read_page - extract a page from the ring buffer

 * @buffer: buffer to extract from

 * @data_page: the page to use allocated from ring_buffer_alloc_read_page

 * @len: amount to extract

 * @cpu: the cpu of the buffer to extract

 * @full: should the extraction only happen when the page is full.

 *

 * This function will pull out a page from the ring buffer and consume it.

 * @data_page must be the address of the variable that was returned

 * from ring_buffer_alloc_read_page. This is because the page might be used

 * to swap with a page in the ring buffer.

 *

 * for example:

 *	rpage = ring_buffer_alloc_read_page(buffer, cpu);

 *	if (IS_ERR(rpage))

 *		return PTR_ERR(rpage);

 *	ret = ring_buffer_read_page(buffer, &rpage, len, cpu, 0);

 *	if (ret >= 0)

 *		process_page(rpage, ret);

 *

 * When @full is set, the function will not return true unless

 * the writer is off the reader page.

 *

 * Note: it is up to the calling functions to handle sleeps and wakeups.

 *  The ring buffer can be used anywhere in the kernel and can not

 *  blindly call wake_up. The layer that uses the ring buffer must be

 *  responsible for that.

 *

 * Returns:

 *  >=0 if data has been transferred, returns the offset of consumed data.

 *  <0 if no data has been transferred.

	/*

	 * If len is not big enough to hold the page header, then

	 * we can not copy anything.

 Check if any events were dropped */

	/*

	 * If this page has been partially read or

	 * if len is not big enough to read the rest of the page or

	 * a writer is still on the page, then

	 * we must copy the data from the page to the buffer.

	 * Otherwise, we can simply swap the page with the one passed in.

 Always keep the time extend and data together */

 save the current timestamp, since the user will need it */

 Need to copy one event at a time */

			/* We need the size of one event, because

			 * rb_advance_reader only advances by one event,

			 * whereas rb_event_ts_length may include the size of

			 * one or two events.

			 * We have already ensured there's enough space if this

 Always keep the time extend and data together */

 update bpage */

 we copied everything to the beginning */

 update the entry counter */

 swap the pages */

		/*

		 * Use the real_end for the data size,

		 * This gives us a chance to store the lost events

		 * on the page.

	/*

	 * Set a flag in the commit field if we lost events

		/* If there is room at the end of the page to save the

		 * missed events, then record it there.

	/*

	 * This page may be off to user land. Zero it out here.

/*

 * We only allocate new buffers, never free them if the CPU goes down.

 * If we were to free the buffer, then the user would lose any trace that was in

 * the buffer.

 check if all cpu sizes are same */

 fill in the size from first enabled cpu */

 allocate minimum pages, user can later expand it */

/*

 * This is a basic integrity check of the ring buffer.

 * Late in the boot cycle this test will run when configured in.

 * It will kick off a thread per CPU that will go into a loop

 * writing to the per cpu ring buffer various sizes of data.

 * Some of the data will be large items, some small.

 *

 * Another thread is created that goes into a spin, sending out

 * IPIs to the other CPUs to also write into the ring buffer.

 * this is to test the nesting ability of the buffer.

 *

 * Basic stats are recorded and reported. If something in the

 * ring buffer should happen that's not expected, a big warning

 * is displayed and all ring buffers are disabled.

 1 meg per cpu */

 Have nested writes different that what is written */

 Multiply cnt by ~e, to make some unique increment */

 read rb_test_started before checking buffer enabled */

 Ignore dropped events before test starts. */

 Now sleep between a min of 100-300us and a max of 1ms */

 Send an IPI to all cpus to write data! */

 No sleep, but for non preempt, let others run */

 Disable buffer so that threads can't write to it yet */

 Now create the rb hammer! */

	/*

	 * Show buffer is enabled before setting rb_test_started.

	 * Yes there's a small race window where events could be

	 * dropped and the thread wont catch it. But when a ring

	 * buffer gets enabled, there will always be some kind of

	 * delay before other CPUs see it. Thus, we don't care about

	 * those dropped events. We care about events dropped after

	 * the threads see that the buffer is active.

 Just run for 10 seconds */;

 Report! */

 CONFIG_RING_BUFFER_STARTUP_TEST */

 SPDX-License-Identifier: GPL-2.0

/*

 * trace task wakeup timings

 *

 * Copyright (C) 2007-2008 Steven Rostedt <srostedt@redhat.com>

 * Copyright (C) 2008 Ingo Molnar <mingo@redhat.com>

 *

 * Based on code from the latency_tracer, that is:

 *

 *  Copyright (C) 2004-2006 Ingo Molnar

 *  Copyright (C) 2004 Nadia Yvette Chambers

/*

 * Prologue for the wakeup function tracers.

 *

 * Returns 1 if it is OK to continue, and preemption

 *            is disabled and data->disabled is incremented.

 *         0 if the trace is to be ignored, and preemption

 *            is not disabled and data->disabled is

 *            kept the same.

 *

 * Note, this function is also used outside this ifdef but

 *  inside the #ifdef of the function graph tracer below.

 *  This is OK, since the function graph tracer is

 *  dependent on the function tracer.

	/*

	 * Do not trace a function if it's filtered by set_graph_notrace.

	 * Make the index of ret stack negative to indicate that it should

	 * ignore further functions.  But it needs its own ret stack entry

	 * to recover the original index in order to continue tracing after

	 * returning from the function.

	/*

	 * In graph mode call the graph tracer output function,

	 * otherwise go with the TRACE_FN event handler

 else CONFIG_FUNCTION_GRAPH_TRACER */

/*

 * wakeup uses its own tracer function to keep the overhead down:

 'set' is set if TRACE_ITER_FUNCTION is about to be set */

 CONFIG_FUNCTION_TRACER */

 else CONFIG_FUNCTION_TRACER */

 !CONFIG_FUNCTION_GRAPH_TRACER */

/*

 * Should this new latency be reported/recorded?

	/*

	 * When we start a new trace, we set wakeup_task to NULL

	 * and then set tracer_enabled = 1. We want to make sure

	 * that another CPU does not see the tracer_enabled = 1

	 * and the wakeup_task with an older task, that might

	 * actually be the same as next.

 disable local data, not wakeup_cpu data */

 We could race with grabbing wakeup_lock */

 The task we are waiting for is waking up */

	/*

	 * Semantic is like this:

	 *  - wakeup tracer handles all tasks in the system, independently

	 *    from their scheduling class;

	 *  - wakeup_rt tracer handles tasks belonging to sched_dl and

	 *    sched_rt class;

	 *  - wakeup_dl handles tasks belonging to sched_dl class only.

 interrupts should be off from try_to_wake_up */

 check for races. */

 reset the trace */

	/*

	 * Once you start tracing a -deadline task, don't bother tracing

	 * another task until the first one wakes up.

	/*

	 * We must be careful in using CALLER_ADDR2. But since wake_up

	 * is not called by an assembly function  (where as schedule is)

	 * it should be safe to use it here.

	/*

	 * Don't let the tracer_enabled = 1 show up before

	 * the wakeup_task is reset. This may be overkill since

	 * wakeup_reset does a spin_unlock after setting the

	 * wakeup_task to NULL, but I want to be safe.

	 * This is a slow path anyway.

 non overwrite screws up the latency tracers */

 make sure we put back any tasks we are tracing */

 SPDX-License-Identifier: GPL-2.0

/*

 * event tracer

 *

 * Copyright (C) 2008 Red Hat Inc, Steven Rostedt <srostedt@redhat.com>

 *

 *  - Added format output of fields of the trace point.

 *    This was based off of work by Tom Zanussi <tzanussi@gmail.com>.

 *

 Double loops, do not use break, only goto's work */

 Holds both preempt_count and migrate_disable */

/*

 * run-time version of trace_event_get_offsets_<call>() that returns the last

 * accessible offset of trace fields excluding __dynamic_array bytes

	/*

	 * head->next points to the last field with the largest offset,

	 * since it was added last by trace_define_field()

/*

 * Check if the referenced field is an array and return true,

 * as arrays are OK to dereference.

 This is an array and is OK to dereference. */

/*

 * Examine the print fmt of the event looking for unsafe dereference

 * pointers using %p* that could be recorded in the trace event and

 * much later referenced after the pointer was freed. Dereferencing

 * pointers are OK, if it is dereferenced into the event itself.

			/*

			 * The print fmt starts with a string that

			 * is processed first to find %p* usage,

			 * then after the first string, the print fmt

			 * contains arguments that are used to check

			 * if the dereferenced %p* usage is safe.

					/*

					 * If there was no %p* uses

					 * the fmt is OK.

 Find dereferencing fields */

 Increment arg if %*s exists. */

 default */

 switch */

 Find the REC-> in the argument */

				/*

				 * Addresses of events on the buffer,

				 * or an array on the buffer is

				 * OK to dereference.

				 * There's ways to fool this, but

				 * this is to catch common mistakes,

				 * not malicious code.

	/*

	 * If you triggered the below warning, the trace event reported

	 * uses an unsafe dereference pointer %p*. As the data stored

	 * at the trace event time may no longer exist when the trace

	 * event is printed, dereferencing to the original source is

	 * unsafe. The source of the dereference must be copied into the

	 * event itself, and the dereference must access the copy instead.

	/*

	 * If CONFIG_PREEMPTION is enabled, then the tracepoint itself disables

	 * preemption (adding one to the preempt_count). Since we are

	 * interested in the preempt_count at the time the tracepoint was

	 * hit, we need to subtract one to offset the increment.

		/*

		 * When soft_disable is set and enable is cleared, the sm_ref

		 * reference counter is decremented. If it reaches 0, we want

		 * to clear the SOFT_DISABLED flag but leave the event in the

		 * state that it was. That is, if the event was enabled and

		 * SOFT_DISABLED isn't set, then do nothing. But if SOFT_DISABLED

		 * is set we do not want the event to be enabled before we

		 * clear the bit.

		 *

		 * When soft_disable is not set but the SOFT_MODE flag is,

		 * we do nothing. Do not disable the tracepoint, otherwise

		 * "soft enable"s (clearing the SOFT_DISABLED bit) wont work.

 If in SOFT_MODE, just set the SOFT_DISABLE_BIT, else clear it */

		/*

		 * When soft_disable is set and enable is set, we want to

		 * register the tracepoint for the event, but leave the event

		 * as is. That means, if the event was already enabled, we do

		 * nothing (but set SOFT_MODE). If the event is disabled, we

		 * set SOFT_DISABLED before enabling the event tracepoint, so

		 * it still seems to be disabled.

 Keep the event disabled, when going to SOFT_MODE. */

 WAS_ENABLED gets set but never cleared. */

 Enable or disable use of trace_buffered_event */

	/*

	 * Sched switch is funny, as we only want to ignore it

	 * in the notrace case if both prev and next should be ignored.

 Nothing to do if we are already tracing */

 Nothing to do if we are not tracing */

 Set tracing if current is enabled */

 Make sure there's something to do */

 Wait till all users are no longer using pid filtering */

 If the subsystem is about to be freed, the dir must be too */

 probably unneeded */

 probably unneeded */

/*

 * __ftrace_set_clr_event(NULL, NULL, NULL, set) will set/unset all events.

		/*

		 * Save the first error and return that. Some events

		 * may still have been enabled, but let the user

		 * know that something went wrong.

	/*

	 * The buf format can be <subsystem>:<event-name>

	 *  *:<event-name> means any event by that name.

	 *  :<event-name> is the same.

	 *

	 *  <subsystem>:* means all events in that subsystem

	 *  <subsystem>: means the same.

	 *

	 *  <name> (no ':') means all events in a subsystem with

	 *  the name <name> or any event that matches <name>

 Put back the colon to allow this to be called again */

/**

 * trace_set_clr_event - enable or disable an event

 * @system: system name to match (NULL for any system)

 * @event: event name to match (NULL for all events, within system)

 * @set: 1 to enable, 0 to disable

 *

 * This is a way for other parts of the kernel to enable or disable

 * event recording.

 *

 * Returns 0 on success, -EINVAL if the parameters do not match any

 * registered events.

/**

 * trace_array_set_clr_event - enable or disable an event for a trace array.

 * @tr: concerned trace array.

 * @system: system name to match (NULL for any system)

 * @event: event name to match (NULL for all events, within system)

 * @enable: true to enable, false to disable

 *

 * This is a way for other parts of the kernel to enable or disable

 * event recording.

 *

 * Returns 0 on success, -EINVAL if the parameters do not match any

 * registered events.

 128 should be much more than enough */

		/*

		 * The ftrace subsystem is for showing formats only.

		 * They can not be enabled or disabled via the event files.

	/*

	 * Grab the mutex, to keep calls to p_next() having the same

	 * tr->filtered_pids as p_start() has.

	 * If we just passed the tr->filtered_pids around, then RCU would

	 * have been enough, but doing that makes things more complex.

		/*

		 * We need to find out if all the events are set

		 * or if all events or cleared, or if we have

		 * a mixture.

		/*

		 * If we have a mixture, no need to look further.

	/*

	 * Opening of "enable" adds a ref count to system,

	 * so the name is safe to use.

 all done */

	/*

	 * Smartly shows the array type(except dynamic array).

	 * Normal:

	 *	field:TYPE VAR

	 * If TYPE := TYPE[LEN], it is shown:

	 *	field:TYPE VAR[LEN]

 ->stop() is called even if ->start() fails */

 Do we want to hide event format files on tracefs lockdown? */

 Initialize for gcc */

 Make sure the system still exists */

 Don't open systems with no events */

 Some versions of gcc think dir can be uninitialized here */

 Still need to increment the ref count of the system */

 Make a temporary dir that has no system but points to tr */

	/*

	 * If dir->subsystem is NULL, then this is a temporary

	 * descriptor that was made for a trace_array to enable

	 * all subsystems.

	/*

	 * This function is called by on_each_cpu() while the

	 * event_mutex is held.

	/*

	 * Register a probe that is called before all other probes

	 * to set ignore_pid if next or prev do not match.

	 * Register a probe this is called after all other probes

	 * to only keep ignore_pid set if next pid matches.

	/*

	 * Ignoring of pids is done at task switch. But we have to

	 * check for those tasks that are currently running.

	 * Always do this in case a pid was appended or removed.

 copy tr over to seq ops */

 Checks for tracefs lockdown */

 need to create new entry */

 Only allocate if dynamic (kprobes and modules) */

 First see if we did not already create this dir */

 Now see if the system itself exists. */

 Reset system variable when not found */

 the ftrace system is special, do not create enable or filter files */

 Only print this message if failed on memory allocation */

	/*

	 * Other events may have the same class. Only update

	 * the fields if they are not already defined.

	/*

	 * If the trace point header did not define TRACE_SYSTEM

	 * then the system would be called "TRACE_SYSTEM".

	/*

	 * Only event directories that can be enabled should have

	 * triggers or filters.

		/*

		 * The do_for_each_event_file_safe() is

		 * a double loop. After finding the call for this

		 * trace_array, we use break to jump to the next

		 * trace_array.

		/*

		 * The do_for_each_event_file() is

		 * a double loop. After finding the call for this

		 * trace_array, we use break to jump to the next

		 * trace_array.

 Find the length of the eval value as a string */

 Make sure there's enough room to replace the string with the value */

 Get the rest of the string of ptr */

 Make sure we end the new string */

 paranoid */

 skip numbers */

 Check for alpha chars like ULL */

			/*

			 * A number must have some kind of delimiter after

			 * it, and we can ignore that too.

 enum/sizeof string smaller than value */

				/*

				 * No need to decrement here, as eval_replace()

				 * returns the pointer to the character passed

				 * the eval, and two evals can not be placed

				 * back to back without something in between.

				 * We can skip that something in between.

			/*

			 * If what comes after this variable is a '.' or

			 * '->' then we can continue to ignore that string.

			/*

			 * Once again, we can skip the delimiter that came

			 * after the string.

 events are usually grouped together with systems */

		/*

		 * Since calls are grouped by systems, the likelihood that the

		 * next call in the iteration belongs to the same system as the

		 * previous call is high. As an optimization, we skip searching

		 * for a map[] that matches the call's system if the last call

		 * was from the same system. That's what last_i is for. If the

		 * call has the same system as the previous call, then last_i

		 * will be the index of the first map[] that has a matching

		 * system.

 Save the first system if need be */

 Add an event to a trace directory */

/*

 * Just create a descriptor for early init. A descriptor is required

 * for enabling events at boot. We want to enable events before

 * the filesystem is initialized.

 Add an additional event_call dynamically */

/*

 * Must be called under locking of trace_types_lock, event_mutex and

 * trace_event_sem.

		/*

		 * We can't rely on ftrace_event_enable_disable(enable => 0)

		 * we are going to do, EVENT_FILE_FL_SOFT_MODE can suppress

		 * TRACE_REG_UNREGISTER.

		/*

		 * The do_for_each_event_file_safe() is

		 * a double loop. After finding the call for this

		 * trace_array, we use break to jump to the next

		 * trace_array.

 Remove an event_call */

 Don't add infrastructure for mods without tracepoints */

	/*

	 * It is safest to reset the ring buffer if the module being unloaded

	 * registered any events that were used. The only worry is if

	 * a new module gets loaded, and takes on the same id as the events

	 * of this module. When printing out the buffer, traced events left

	 * over from this module may be passed to the new module events and

	 * unexpected results may occur.

 higher than trace.c module notify */

 CONFIG_MODULES */

 Create a new event directory structure for a trace directory. */

 Returns any file that matches the system and event */

 Returns valid trace event files that match system and event */

/**

 * trace_get_event_file - Find and return a trace event file

 * @instance: The name of the trace instance containing the event

 * @system: The name of the system containing the event

 * @event: The name of the event

 *

 * Return a trace event file given the trace instance name, trace

 * system, and trace event name.  If the instance name is NULL, it

 * refers to the top-level trace array.

 *

 * This function will look it up and return it if found, after calling

 * trace_array_get() to prevent the instance from going away, and

 * increment the event's module refcount to prevent it from being

 * removed.

 *

 * To release the file, call trace_put_event_file(), which will call

 * trace_array_put() and decrement the event's module refcount.

 *

 * Return: The trace event on success, ERR_PTR otherwise.

 Don't let event modules unload while in use */

/**

 * trace_put_event_file - Release a file from trace_get_event_file()

 * @file: The trace event file

 *

 * If a file was retrieved using trace_get_event_file(), this should

 * be called when it's no longer needed.  It will cancel the previous

 * trace_array_get() called by that function, and decrement the

 * event's module refcount.

 Avoid typos */

 Skip if the event is in a state we want to switch to */

 Remove the SOFT_MODE flag */

 hash funcs only work with set_ftrace_filter */

	/*

	 * We use the callback data field (which is a pointer)

	 * as our counter.

 Don't let event modules unload while probe registered */

	/*

	 * The above returns on success the # of functions enabled,

	 * but if it didn't find any functions it returns zero.

	 * Consider no functions a failure too.

 Just return zero, not the number of enabled functions */

 CONFIG_DYNAMIC_FTRACE */

/*

 * The top level array and trace arrays created by boot-time tracing

 * have already had its trace_event_file descriptors created in order

 * to allow for early events to be recorded.

 * This function is called after the tracefs has been initialized,

 * and we now have to create the files associated to the events.

/*

 * For early boot up, the top trace array and the trace arrays created

 * by boot-time tracing require to have a list of events that can be

 * enabled. This must be done before the filesystem is set up in order

 * to allow events to be traced early.

 Early boot up should not have any modules loaded */

 Remove the event directory structure for a trace directory. */

 Expects to have event_mutex held when called */

 There are not as crucial, just warn if they are not created */

 ring buffer internal formats */

/**

 * event_trace_add_tracer - add a instance of a trace_array to events

 * @parent: The parent dentry to place the files/directories for events in

 * @tr: The trace array associated with these events

 *

 * When a new instance is created, it needs to set up its events

 * directory, as well as other files associated with events. It also

 * creates the event hierarchy in the @parent/events directory.

 *

 * Returns 0 on success.

 *

 * Must be called with event_mutex held.

 If tr already has the event list, it is initialized in early boot. */

/*

 * The top trace array already had its file descriptors created.

 * Now the files themselves need to be created.

 Must be called with event_mutex held */

 Disable any event triggers and associated soft-disabled events */

 Clear the pid list */

 Disable any running events */

 Make sure no more events are being executed */

 Restarting syscalls requires that we stop them first */

 Put back the comma to allow this to be called again */

	/*

	 * We need the top trace array to have a working set of trace

	 * points at early init, before the debug files and directories

	 * are created. Create the file entries now, and attach them

	 * to the actual file dentries later.

/*

 * event_trace_enable() is called from trace_event_init() first to

 * initialize events and perhaps start any events that are on the

 * command line. Unfortunately, there are some events that will not

 * start this early, like the system call tracepoints that need

 * to set the %SYSCALL_WORK_SYSCALL_TRACEPOINT flag of pid 1. But

 * event_trace_enable() is called before pid 1 starts, and this flag

 * is never set, making the syscall tracepoint never get reached, but

 * the event is enabled regardless (and not doing anything).

 Init fields which doesn't related to the tracefs */

/*

 * Do various things that may trigger events.

/*

 * For every trace event defined, we will test each trace point separately,

 * and then by groups, and finally all trace points.

 Only test those that have a probe */

/*

 * Testing syscall events here is pretty useless, but

 * we still do it if configured. But this is time consuming.

 * What we really need is a user thread to perform the

 * syscalls as we test.

		/*

		 * If an event is already enabled, someone is using

		 * it and the self test should not be on.

 Now test at the sub system level */

 the ftrace system is special, skip it */

 Test with all events enabled */

 reset sysname */

 SPDX-License-Identifier: GPL-2.0

/* Copyright (c) 2011-2015 PLUMgrid, http://plumgrid.com

 * Copyright (c) 2016 Facebook

 CONFIG_MODULES */

/**

 * trace_call_bpf - invoke BPF program

 * @call: tracepoint event

 * @ctx: opaque context pointer

 *

 * kprobe handlers execute BPF programs via this helper.

 * Can be used from static tracepoints in the future.

 *

 * Return: BPF programs always return an integer which is interpreted by

 * kprobe handler as:

 * 0 - return from kprobe (event is filtered out)

 * 1 - store kprobe event into ring buffer

 * Other values are reserved and currently alias to 1

		/*

		 * since some bpf program is already running on this cpu,

		 * don't call into another bpf program (same or different)

		 * and don't send kprobe event into ring-buffer,

		 * so return zero here

	/*

	 * Instead of moving rcu_read_lock/rcu_dereference/rcu_read_unlock

	 * to all call sites, we did a bpf_prog_array_valid() there to check

	 * whether call->prog_array is empty or not, which is

	 * a heuristic to speed up execution.

	 *

	 * If bpf_prog_array_valid() fetched prog_array was

	 * non-NULL, we go into trace_call_bpf() and do the actual

	 * proper rcu_dereference() under RCU lock.

	 * If it turns out that prog_array is NULL then, we bail out.

	 * For the opposite, if the bpf_prog_array_valid() fetched pointer

	 * was NULL, you'll skip the prog_array with the risk of missing

	 * out of events when it was updated in between this and the

	 * rcu_dereference() which is accepted risk.

	/*

	 * NB: We rely on strncpy_from_user() not copying junk past the NUL

	 * terminator into `dst`.

	 *

	 * strncpy_from_user() does long-sized strides in the fast path. If the

	 * strncpy does not mask out the bytes after the NUL in `unsafe_ptr`,

	 * then there could be junk after the NUL in `dst`. If user takes `dst`

	 * and keys a hash map with it, then semantically identical strings can

	 * occupy multiple entries in the map.

	/*

	 * The strncpy_from_kernel_nofault() call will likely not fill the

	 * entire buffer, but that's okay in this circumstance as we're probing

	 * arbitrary memory anyway similar to bpf_probe_read_*() and might

	 * as well probe the stack. Thus, memory is explicitly cleared

	 * only in error case, so that improper users ignoring return

	 * code altogether don't copy garbage; otherwise length of string

	 * is returned that can be used for bpf_perf_event_output() et al.

 CONFIG_ARCH_HAS_NON_OVERLAPPING_ADDRESS_SPACE */

	/*

	 * Ensure we're in user context which is safe for the helper to

	 * run. This helper has no business in a kthread.

	 *

	 * access_ok() should prevent writing to non-user memory, but in

	 * some situations (nommu, temporary switch, etc) access_ok() does

	 * not provide enough validation, hence the check on KERNEL_DS.

	 *

	 * nmi_uaccess_okay() ensures the probe is not run in an interim

	 * state, when the task or mm are switched. This is specifically

	 * required to prevent the use of temporary mm.

	/*

	 * This program might be calling bpf_trace_printk,

	 * so enable the associated bpf_trace/bpf_trace_printk event.

	 * Repeat this each time as it is possible a user has

	 * disabled bpf_trace_printk events.  By loading a program

	 * calling bpf_trace_printk() however the user has expressed

	 * the intent to see such events.

	/*

	 * this api is ugly since we miss [-22..-2] range of valid

	 * counter values, but that's uapi

/*

 * Support executing tracepoints in normal, irq, and nmi context that each call

 * bpf_perf_event_output

	/* Similar to bpf_probe_write_user, task needs to be

	 * in a sound condition and kernel memory access be

	 * permitted in order to send signal to the current

	 * task.

		/* Do an early check on signal validity. Otherwise,

		 * the error is lost in deferred irq_work.

		/* Add the current task, which is the target of sending signal,

		 * to the irq_work. The current task may change when queued

		 * irq works get executed.

 This helper call is inlined by verifier. */

 bpf+kprobe programs can access fields of 'struct pt_regs' */

	/*

	 * Assertion for 32 bit to make sure last 8 byte access

	 * (BPF_DW) to the last 4 byte member is disallowed.

	/*

	 * r1 points to perf tracepoint buffer where first 8 bytes are hidden

	 * from bpf program and contain a pointer to 'struct pt_regs'. Fetch it

	 * from there and call the same bpf_perf_event_output() helper inline.

	/*

	 * Same comment as in bpf_perf_event_output_tp(), only that this time

	 * the other helper's function body cannot be inlined due to being

	 * external, thus we need to call raw helper function.

/*

 * bpf_raw_tp_regs are separate from bpf_pt_regs used from skb/xdp

 * to avoid potential recursive reuse issue when/if tracepoints are added

 * inside bpf_*_event_output, bpf_get_stackid and/or bpf_get_stack.

 *

 * Since raw tracepoints run despite bpf_prog_active, support concurrent usage

 * in normal, irq, and nmi context.

 similar to bpf_perf_event_output_tp, but pt_regs fetched differently */

	/*

	 * Kprobe override only works if they are on the function entry,

	 * and only if they are on the opt-in list.

 set the new array to event->tp_event and set event->prog */

	/*

	 * The above kcalloc returns ZERO_SIZE_PTR when ids_len = 0, which

	 * is required when user only wants to check for uquery->prog_cnt.

	 * There is no need to check for it since the case is handled

	 * gracefully in bpf_prog_array_copy_info.

	/*

	 * check that program doesn't access arguments beyond what's

	 * available in this tracepoint

 not supporting BPF_PROG_TYPE_PERF_EVENT yet */

 kprobe/uprobe */

 CONFIG_MODULES */

 SPDX-License-Identifier: GPL-2.0

 used to call mcount */

 used to call mcount */

 SPDX-License-Identifier: GPL-2.0

/*

 * Infrastructure to took into function calls and returns.

 * Copyright (c) 2008-2009 Frederic Weisbecker <fweisbec@gmail.com>

 * Mostly borrowed from function tracer which

 * is Copyright (c) Steven Rostedt <srostedt@redhat.com>

 *

 * Highly modified by Steven Rostedt (VMware).

 Both enabled by default (can be cleared by function_graph tracer flags */

/**

 * ftrace_graph_is_dead - returns true if ftrace_graph_stop() was called

 *

 * ftrace_graph_stop() is called when a severe error is detected in

 * the function graph tracing. This function is called by the critical

 * paths of function graph to keep those paths from doing any more harm.

/**

 * ftrace_graph_stop - set to permanently disable function graph tracing

 *

 * In case of an error int function graph tracing, this is called

 * to try to keep function graph tracing from causing any more harm.

 * Usually this is pretty severe and this is called to try to at least

 * get a warning out to the user.

 Add a function return address to the trace stack on thread info.*/

	/*

	 * We must make sure the ret_stack is tested before we read

	 * anything else.

 The return trace stack is full */

/*

 * Not all archs define MCOUNT_INSN_SIZE which is used to look for direct

 * functions. But those archs currently don't support direct functions

 * anyway, and ftrace_find_rec_direct() is just a stub for them.

 * Define MCOUNT_INSN_SIZE to keep those archs compiling.

 Make sure this only works without direct calls */

	/*

	 * Skip graph tracing if the return location is served by direct trampoline,

	 * since call sequence and return addresses are unpredictable anyway.

	 * Ex: BPF trampoline may call original function and may skip frame

	 * depending on type of BPF programs attached.

 Only trace if the calling function expects to */

 Retrieve a function return address to the trace stack on thread info.*/

 Might as well panic, otherwise we have no where to go */

	/*

	 * The arch may choose to record the frame pointer used

	 * and check it here to make sure that it is what we expect it

	 * to be. If gcc does not set the place holder of the return

	 * address in the frame pointer, and does a copy instead, then

	 * the function graph trace will fail. This test detects this

	 * case.

	 *

	 * Currently, x86_32 with optimize for size (-Os) makes the latest

	 * gcc do the above.

	 *

	 * Note, -mfentry does not use frame pointers, and this test

	 *  is not needed if CC_USING_FENTRY is set.

	/*

	 * We still want to trace interrupts coming in if

	 * max_depth is set to 1. Make sure the decrement is

	 * seen before ftrace_graph_return.

/*

 * Hibernation protection.

 * The state of the current task is too much unstable during

 * suspend/restore to disk. We want to protect against that.

/*

 * Send the trace to the ring-buffer.

 * @return the original return address.

	/*

	 * The ftrace_graph_return() may still access the current

	 * ret_stack structure, we need to make sure the update of

	 * curr_ret_stack is after that.

 Might as well panic. What else to do? */

/**

 * ftrace_graph_get_ret_stack - return the entry of the shadow stack

 * @task: The task to read the shadow stack from

 * @idx: Index down the shadow stack

 *

 * Return the ret_struct on the shadow stack of the @task at the

 * call graph at @idx starting with zero. If @idx is zero, it

 * will return the last saved ret_stack entry. If it is greater than

 * zero, it will return the corresponding ret_stack for the depth

 * of saved return addresses.

/**

 * ftrace_graph_ret_addr - convert a potentially modified stack return address

 *			   to its original value

 *

 * This function can be called by stack unwinding code to convert a found stack

 * return address ('ret') to its original value, in case the function graph

 * tracer has modified it to be 'return_to_handler'.  If the address hasn't

 * been modified, the unchanged value of 'ret' is returned.

 *

 * 'idx' is a state variable which should be initialized by the caller to zero

 * before the first call.

 *

 * 'retp' is a pointer to the return address on the stack.  It's ignored if

 * the arch doesn't have HAVE_FUNCTION_GRAPH_RET_ADDR_PTR defined.

 !HAVE_FUNCTION_GRAPH_RET_ADDR_PTR */

 HAVE_FUNCTION_GRAPH_RET_ADDR_PTR */

 trampoline_size is only needed for dynamically allocated tramps */

/*

 * Simply points to ftrace_stub, but with the proper protocol.

 * Defined by the linker script in linux/vmlinux.lds.h

 The callbacks that hook a function */

 Try to assign a return stack array on FTRACE_RETSTACK_ALLOC_SIZE tasks. */

 Make sure the tasks see the -1 first: */

	/*

	 * Does the user want to count the time a function was asleep.

	 * If so, do not update the time stamps.

 only process tasks that we timestamped */

	/*

	 * Update all the counters in next to make up for the

	 * time next was sleeping.

/*

 * The function graph tracer should only trace the functions defined

 * by set_ftrace_filter and set_ftrace_notrace. If another function

 * tracer ops is registered, the graph tracer requires testing the

 * function against the global ops, and not just trace any function

 * that any ftrace_ops registered.

	/*

	 * The graph and global ops share the same set of functions

	 * to test. If any other ops is on the list, then

	 * the graph tracing needs to test if its the function

	 * it should call.

 in double loop, break out with goto */

 make curr_ret_stack visible before we add the ret_stack */

/*

 * Allocate a return stack for the idle task. May be the first

 * time through, or it may be done by CPU hotplug online.

	/*

	 * The idle task has no parent, it either has its own

	 * stack or no stack at all.

 Allocate a return stack for newly created task */

 Make sure we do not use the parent ret_stack */

 NULL must become visible to IRQs before we free it: */

 Allocate a return stack for each task */

 The cpu_boot init_task->ret_stack will never be freed */

 we currently allow only one tracer registered at a time */

	/*

	 * Update the indirect function to the entryfunc, and the

	 * function that gets called to the entry_test first. Then

	 * call the update fgraph entry function to determine if

	 * the entryfunc should be called directly or not.

 SPDX-License-Identifier: GPL-2.0

 Include in trace.c */

		/*

		 * The ring buffer is a size of trace_buf_size, if

		 * we loop more than the size, there's something wrong

		 * with the ring buffer.

 disable tracing */

/*

 * Test the trace buffer to see if all the elements

 * are still sane.

 Don't allow flipping of max traces now */

	/*

	 * The trace_test_buffer_cpu runs a while loop to consume all data.

	 * If the calling tracer is broken, and is constantly filling

	 * the buffer, this will run forever, and hard lock the box.

	 * We disable the ring buffer while we do this test to prevent

	 * a hard lock up.

 Handle PPC64 '.' name */

	/*

	 * Probe 1 will trace function 1.

	 * Probe 2 will trace function 2.

	 * Probe 3 will trace functions 1 and 2.

 First time we are running with main function */

 Add a dynamic probe */

 Remove trace function from probe 3 */

 Purposely unregister in the same order */

 Make sure everything is off */

 Test dynamic code modification and ftrace filters */

 The ftrace test PASSED */

 enable tracing, and record the filter function */

 passed in by parameter to fool gcc from optimizing */

	/*

	 * Some archs *cough*PowerPC*cough* add characters to the

	 * start of the function names. We simply put a '*' to

	 * accommodate them.

 filter only on our function */

 enable tracing */

 Sleep for a 1/10 of a second */

 we should have nothing in the buffer */

 call our function again */

 sleep again */

 stop the tracing. */

 check the trace buffer */

 we should only have one item */

 Test the ops with global tracing running */

 Enable tracing on all functions again */

 Test the ops with global tracing off */

	/*

	 * This function is registered without the recursion safe flag.

	 * The ftrace infrastructure should provide the recursion

	 * protection. If not, this will crash the kernel!

	/*

	 * We said we would provide our own recursion. By calling

	 * this function again, we should recurse back into this function

	 * and count again. But this only happens if the arch supports

	 * all of ftrace features and nothing else is using the function

	 * tracing utility.

 The previous test PASSED */

 enable tracing, and record the filter function */

 Handle PPC64 '.' name */

	/*

	 * Recursion allows for transitions between context,

	 * and may call the callback twice.

 CONFIG_DYNAMIC_FTRACE */

 The previous test PASSED */

 enable tracing, and record the filter function */

 Handle PPC64 '.' name */

	/*

	 * If DYNAMIC_FTRACE is not set, then we just trace all functions.

	 * This test really doesn't care.

	/*

	 * Now if the arch does not support passing regs, then this should

	 * have failed.

/*

 * Simple verification test of ftrace function tracer.

 * Enable ftrace, sleep 1/10 second, and then read the trace

 * buffer to see if all is in order.

 make sure msleep has been recorded */

 start the tracing */

 Sleep for a 1/10 of a second */

 stop the tracing. */

 check the trace buffer */

 kill ftrace totally if we failed */

 CONFIG_FUNCTION_TRACER */

 Maximum number of functions to trace before diagnosing a hang */

 Wrap the real function entry probe to avoid possible hanging */

 This is harmlessly racy, we want to approximately detect a hang */

 ftrace_dump() disables tracing */

/*

 * Pretty much the same than for the function tracer from which the selftest

 * has been borrowed.

	/*

	 * Simulate the init() callback but we attach a watchdog callback

	 * to detect and recover from possible hangs

 Sleep for a 1/10 of a second */

 Have we just recovered from a hang? */

 check the trace buffer */

 Need to also simulate the tr->reset to remove this fgraph_ops */

	/*

	 * Some archs *cough*PowerPC*cough* add characters to the

	 * start of the function names. We simply put a '*' to

	 * accommodate them.

	/*

	 * Register direct function together with graph tracer

	 * and make sure we get graph trace.

 check the trace buffer */

 Don't test dynamic tracing, the function tracer already did */

 Stop it if we failed */

 CONFIG_FUNCTION_GRAPH_TRACER */

 start the tracing */

 reset the max latency */

 disable interrupts for a bit */

	/*

	 * Stop the tracer to avoid a warning subsequent

	 * to buffer flipping failure because tracing_stop()

	 * disables the tr and max buffers, making flipping impossible

	 * in case of parallels max irqs off latencies.

 stop the tracing. */

 check both trace buffers */

 CONFIG_IRQSOFF_TRACER */

	/*

	 * Now that the big kernel lock is no longer preemptible,

	 * and this is called with the BKL held, it will always

	 * fail. If preemption is already disabled, simply

	 * pass the test. When the BKL is removed, or becomes

	 * preemptible again, we will once again test this,

	 * so keep it in.

 start the tracing */

 reset the max latency */

 disable preemption for a bit */

	/*

	 * Stop the tracer to avoid a warning subsequent

	 * to buffer flipping failure because tracing_stop()

	 * disables the tr and max buffers, making flipping impossible

	 * in case of parallels max preempt off latencies.

 stop the tracing. */

 check both trace buffers */

 CONFIG_PREEMPT_TRACER */

	/*

	 * Now that the big kernel lock is no longer preemptible,

	 * and this is called with the BKL held, it will always

	 * fail. If preemption is already disabled, simply

	 * pass the test. When the BKL is removed, or becomes

	 * preemptible again, we will once again test this,

	 * so keep it in.

 start the tracing */

 reset the max latency */

 disable preemption and interrupts for a bit */

 reverse the order of preempt vs irqs */

	/*

	 * Stop the tracer to avoid a warning subsequent

	 * to buffer flipping failure because tracing_stop()

	 * disables the tr and max buffers, making flipping impossible

	 * in case of parallels max irqs/preempt off latencies.

 stop the tracing. */

 check both trace buffers */

 do the test by disabling interrupts first this time */

 reverse the order of preempt vs irqs */

 stop the tracing. */

 check both trace buffers */

 CONFIG_IRQSOFF_TRACER && CONFIG_PREEMPT_TRACER */

 What could possibly go wrong? */

 Make this a -deadline thread */

 Make it know we have a new prio */

 now go to sleep and let the test wake us up */

 we are awake, now wait to disappear */

 create a -deadline thread */

 make sure the thread is running at -deadline policy */

 start the tracing */

 reset the max latency */

		/*

		 * Sleep to make sure the -deadline thread is asleep too.

		 * On virtual machines we can't rely on timings,

		 * but we want to make sure this test still works.

 memory barrier is in the wake_up_process() */

 Wait for the task to wake up */

 stop the tracing. */

 check both trace buffers */

 kill the thread */

 CONFIG_SCHED_TRACER */

 start the tracing */

 Sleep for a 1/10 of a second */

 stop the tracing. */

 check the trace buffer */

 CONFIG_BRANCH_TRACER */

 SPDX-License-Identifier: GPL-2.0

/*

 * Cache the last found function. Yes, updates to this is racey, but

 * so is memory cache ;-)

 First check the last one recorded */

 nr_records is -1 when clearing records */

	/*

	 * If there's two writers and this writer comes in second,

	 * the cmpxchg() below to update the ip will fail. Then this

	 * writer will try again. It is possible that index will now

	 * be greater than nr_records. This is because the writer

	 * that succeeded has not updated the nr_records yet.

	 * This writer could keep trying again until the other writer

	 * updates nr_records. But if the other writer takes an

	 * interrupt, and that interrupt locks up that CPU, we do

	 * not want this CPU to lock up due to the recursion protection,

	 * and have a bug report showing this CPU as the cause of

	 * locking up the computer. To not lose this record, this

	 * writer will simply use the next position to update the

	 * recursed_functions, and it will update the nr_records

	 * accordingly.

	/*

	 * We only want to add a function if it hasn't been added before.

	 * Add to the current location before incrementing the count.

	 * If it fails to add, then increment the index (save in i)

	 * and try again.

 Did something else already added this for us? */

 Try the next location (use i for the next index) */

	/*

	 * It's still possible that we could race with the clearing

	 *    CPU0                                    CPU1

	 *    ----                                    ----

	 *                                       ip = func

	 *  nr_records = -1;

	 *  recursed_functions[0] = 0;

	 *                                       i = -1

	 *                                       if (i < 0)

	 *  nr_records = 0;

	 *  (new recursion detected)

	 *      recursed_functions[0] = func

	 *                                            cmpxchg(recursed_functions[0],

	 *                                                    func, 0)

	 *

	 * But the worse that could happen is that we get a zero in

	 * the recursed_functions array, and it's likely that "func" will

	 * be recorded again.

 If this file was opened for write, then erase contents */

 disable updating records */

 enable them again */

 SPDX-License-Identifier: GPL-2.0

/*

 * Infrastructure for profiling code inserted by 'gcc -pg'.

 *

 * Copyright (C) 2007-2008 Steven Rostedt <srostedt@redhat.com>

 * Copyright (C) 2004-2008 Ingo Molnar <mingo@redhat.com>

 *

 * Originally ported from the -rt patch by:

 *   Copyright (C) 2007 Arnaldo Carvalho de Melo <acme@redhat.com>

 *

 * Based on code in the latency_tracer, that is:

 *

 *  Copyright (C) 2004-2006 Ingo Molnar

 *  Copyright (C) 2004 Nadia Yvette Chambers

 hash bits for specific function selection */

 ftrace_enabled is a method to turn ftrace on or off */

 Current function tracing op */

 What to set function_trace_op to */

/*

 * ftrace_disabled is set when an anomaly is discovered.

 * ftrace_disabled is much stronger than ftrace_enabled.

 Defined by vmlinux.lds.h see the commment above arch_ftrace_ops_list_func for details */

 Probably not needed, but do it anyway */

	/*

	 * If this is a dynamic, RCU, or per CPU ops, or we force list func,

	 * then it needs to call the list anyway.

	/*

	 * Prepare the ftrace_ops that the arch callback will use.

	 * If there's only one ftrace_ops registered, the ftrace_ops_list

	 * will point to the ops we want.

 If there's no ftrace_ops registered, just call the stub function */

	/*

	 * If we are at the end of the list and this ops is

	 * recursion safe and not dynamic and the arch supports passing ops,

	 * then have the mcount trampoline call the function directly.

 Just use the default ftrace_ops */

 If there's no change, then do nothing more here */

	/*

	 * If we are using the list function, it doesn't care

	 * about the function_trace_ops.

		/*

		 * Don't even bother setting function_trace_ops,

		 * it would be racy to do so anyway.

	/*

	 * For static tracing, we need to be a bit more careful.

	 * The function change takes affect immediately. Thus,

	 * we need to coordinate the setting of the function_trace_ops

	 * with the setting of the ftrace_trace_function.

	 *

	 * Set the function to the list ops, which will call the

	 * function we want, albeit indirectly, but it handles the

	 * ftrace_ops and doesn't depend on function_trace_op.

	/*

	 * Make sure all CPUs see this. Yes this is slow, but static

	 * tracing is slow and nasty to have enabled.

 Now all cpus are using the list ops. */

 Make sure the function_trace_op is visible on all CPUs */

 Nasty way to force a rmb on all cpus */

 OK, we are all set to update the ftrace_trace_function now! */

 !CONFIG_DYNAMIC_FTRACE */

	/*

	 * We are entering ops into the list but another

	 * CPU might be walking that list. We need to make sure

	 * the ops->next pointer is valid before another CPU sees

	 * the ops pointer included into the list.

	/*

	 * If we are removing the last function, then simply point

	 * to the ftrace_stub.

	/*

	 * If the ftrace_ops specifies SAVE_REGS, then it only can be used

	 * if the arch supports it, or SAVE_REGS_IF_SUPPORTED is also set.

	 * Setting SAVE_REGS_IF_SUPPORTED makes SAVE_REGS irrelevant.

 Always save the function, and reset at unregistering */

 Only do something if we are tracing something */

 ftrace_profile_lock - synchronize the enable and disable of the profiler */

 function graph compares on total time */

 not function graph compares against hits */

 we raced with function_profile_reset() */

 Sample standard deviation (s^2) */

		/*

		 * Apply Welford's method:

		 * s^2 = 1 / (n * (n-1)) * (n * \Sum (x_i)^2 - (\Sum x_i)^2)

		/*

		 * Divide only 1000 for ns^2 -> us^2 conversion.

		 * trace_print_graph_duration will divide 1000 again.

 If we already allocated, do nothing */

	/*

	 * We do not know the number of functions that exist because

	 * dynamic tracing is what counts them. With past experience

	 * we have around 20K functions. That should be more than enough.

	 * It is highly unlikely we will execute every function in

	 * the kernel.

 If the profile is already created, simply reset it */

	/*

	 * We are profiling all functions, but usually only a few thousand

	 * functions are hit. We'll make a hash of 1024 items.

 Preallocate the function profiling pages */

 interrupts must be disabled */

/*

 * The memory is already allocated, this simply finds a new record to use.

 prevent recursion (from NMIs) */

	/*

	 * Try to find the function again since an NMI

	 * could have added it

 If function graph is shutting down, ret_stack can be NULL */

 If the calltime was zero'd ignore it */

 Append this call time to the parent time to subtract */

 CONFIG_FUNCTION_GRAPH_TRACER */

			/*

			 * unregister_ftrace_profiler calls stop_machine

			 * so this acts like an synchronize_rcu.

 big enough to hold a number */

 used to initialize the real stat files */

			/*

			 * The files created are permanent, if something happens

			 * we still do not free memory.

 CONFIG_FUNCTION_PROFILER */

 CONFIG_FUNCTION_PROFILER */

/*

 * Set when doing a global update, like enabling all recs or disabling them.

 * It is not set when just updating a single ftrace_ops.

/*

 * We make these constant because no one should touch them,

 * but they are used as the default "empty hash", to avoid allocating

 * it all the time. These are in a read only section such that if

 * anyone does try to modify it, it will cause an exception.

/*

 * Used by the stack unwinder to know about dynamic ftrace trampolines.

	/*

	 * Some of the ops may be dynamically allocated,

	 * they are freed after a synchronize_rcu().

		/*

		 * This is to check for dynamically allocated trampolines.

		 * Trampolines that are in kernel text will have

		 * core_kernel_text() return true.

/*

 * This is used by __kernel_text_address() to return true if the

 * address is on a dynamically allocated trampoline that would

 * not return true for either core_kernel_text() or

 * is_module_text_address().

 Only use this function if ftrace_hash_empty() has already been tested */

/**

 * ftrace_lookup_ip - Test to see if an ip exists in an ftrace_hash

 * @hash: The hash to look at

 * @ip: The instruction pointer to test

 *

 * Search a given @hash to see if a given instruction pointer (@ip)

 * exists in it.

 *

 * Returns the entry that holds the @ip if found. NULL otherwise.

 stack tracer isn't supported yet */

 Empty hash? */

	/*

	 * Use around half the size (max bit of it), but

	 * a minimum of 2 is fine (as size of 0 or 1 both give 1 for bits).

 Don't allocate too much */

	/*

	 * If the new source is empty, just return the empty_hash.

 Reject setting notrace hash on IPMODIFY ftrace_ops */

 Make sure this can be applied if it is IPMODIFY ftrace_ops */

 IPMODIFY should be updated only when filter_hash updating */

	/*

	 * Remove the current set, update the hash and add

	 * them back.

	/*

	 * The function record is a match if it exists in the filter

	 * hash and not in the notrace hash. Note, an empty hash is

	 * considered a match for the filter hash, but an empty

	 * notrace hash is considered not in the notrace hash.

/*

 * Test the hashes for this ops to see if we want to call

 * the ops->func or not.

 *

 * It's a match if the ip is in the ops->filter_hash or

 * the filter_hash does not exist or is empty,

 *  AND

 * the ip is not in the ops->notrace_hash.

 *

 * This needs to be called with preemption disabled as

 * the hashes are freed with call_rcu().

	/*

	 * There's a small race when adding ops that the ftrace handler

	 * that wants regs, may be called without them. We can not

	 * allow that handler to be called if regs is NULL.

/*

 * This is a double for. Do not use 'break' to break out of the loop,

 * you must use a goto.

 overload flags, as it is unsigned long */

/**

 * ftrace_location_range - return the first address of a traced location

 *	if it touches the given ip range

 * @start: start of range to search.

 * @end: end of range to search (inclusive). @end points to the last byte

 *	to check.

 *

 * Returns rec->ip if the related ftrace location is a least partly within

 * the given address range. That is, the first address of the instruction

 * that is either a NOP or call to the function tracer. It checks the ftrace

 * internal tables to determine if the address belongs or not.

/**

 * ftrace_location - return true if the ip giving is a traced location

 * @ip: the instruction pointer to check

 *

 * Returns rec->ip if @ip given is a pointer to a ftrace location.

 * That is, the instruction that is either a NOP or call to

 * the function tracer. It checks the ftrace internal tables to

 * determine if the address belongs or not.

/**

 * ftrace_text_reserved - return true if range contains an ftrace location

 * @start: start of range to search

 * @end: end of range to search (inclusive). @end points to the last byte to check.

 *

 * Returns 1 if @start and @end contains a ftrace location.

 * That is, the instruction that is either a NOP or call to

 * the function tracer. It checks the ftrace internal tables to

 * determine if the address belongs or not.

 Test if ops registered to this rec needs regs */

 pass rec in as regs to have non-NULL val */

 Only update if the ops has been registered */

	/*

	 * In the filter_hash case:

	 *   If the count is zero, we update all records.

	 *   Otherwise we just update the items in the hash.

	 *

	 * In the notrace_hash case:

	 *   We enable the update in the hash.

	 *   As disabling notrace means enabling the tracing,

	 *   and enabling notrace means disabling, the inc variable

	 *   gets inversed.

		/*

		 * If the notrace hash has no items,

		 * then there's nothing to do.

			/*

			 * Only the filter_hash affects all records.

			 * Update if the record is not in the notrace hash.

			/*

			 * If filter_hash is set, we want to match all functions

			 * that are in the hash but not in the other hash.

			 *

			 * If filter_hash is not set, then we are decrementing.

			 * That means we match anything that is in the hash

			 * and also in the other_hash. That is, we need to turn

			 * off functions in the other hash because they are disabled

			 * by this hash.

			/*

			 * If there's only a single callback registered to a

			 * function, and the ops has a trampoline registered

			 * for it, then we can call it directly.

				/*

				 * If we are adding another function callback

				 * to this function, and the previous had a

				 * custom trampoline in use, then we need to go

				 * back to the default trampoline.

			/*

			 * If any ops wants regs saved for this function

			 * then all ops will get saved regs.

			/*

			 * Only the internal direct_ops should have the

			 * DIRECT flag set. Thus, if it is removing a

			 * function, then that function should no longer

			 * be direct.

			/*

			 * If the rec had REGS enabled and the ops that is

			 * being removed had REGS set, then see if there is

			 * still any ops for this record that wants regs.

			 * If not, we can stop recording them.

			/*

			 * The TRAMP needs to be set only if rec count

			 * is decremented to one, and the ops that is

			 * left has a trampoline. As TRAMP can only be

			 * enabled if there is only a single ops attached

			 * to it.

			/*

			 * flags will be cleared in ftrace_check_record()

			 * if rec count is zero.

 Must match FTRACE_UPDATE_CALLS in ftrace_modify_all_code() */

 Shortcut, if we handled all records, we are done. */

	/*

	 * If the ops shares the global_ops hash, then we need to update

	 * all ops that are enabled and use this hash.

 Already done */

/*

 * Try to update IPMODIFY flag on each ftrace_rec. Return 0 if it is OK

 * or no-needed to update, -EBUSY if it detects a conflict of the flag

 * on a ftrace_rec, and -EINVAL if the new_hash tries to trace all recs.

 * Note that old_hash and new_hash has below meanings

 *  - If the hash is NULL, it hits all recs (if IPMODIFY is set, this is rejected)

 *  - If the hash is EMPTY_HASH, it hits nothing

 *  - Anything else hits the recs which match the hash entries.

 Only update if the ops has been registered */

	/*

	 * Since the IPMODIFY is a very address sensitive action, we do not

	 * allow ftrace_ops to set all functions to new hash.

 Update rec->flags */

 We need to update only differences of filter_hash */

 New entries must ensure no others are using it */

 Removed entry */

 Roll back what we did above */

 Disabling always succeeds */

/**

 * ftrace_bug - report and shutdown function tracer

 * @failed: The failed type (EFAULT, EINVAL, EPERM)

 * @rec: The record that failed

 *

 * The arch code that enables or disables the function tracing

 * can call ftrace_bug() when it has detected a problem in

 * modifying the code. @failed should be one of either:

 * EFAULT - if the problem happens on reading the @ip address

 * EINVAL - if what is read at @ip is not what was expected

 * EPERM - if the problem happens on writing to the @ip address

	/*

	 * If we are updating calls:

	 *

	 *   If the record has a ref count, then we need to enable it

	 *   because someone is using it.

	 *

	 *   Otherwise we make sure its disabled.

	 *

	 * If we are disabling calls, then disable all records that

	 * are enabled.

	/*

	 * If enabling and the REGS flag does not match the REGS_EN, or

	 * the TRAMP flag doesn't match the TRAMP_EN, then do not ignore

	 * this record. Set flags to fail the compare against ENABLED.

	 * Same for direct calls.

		/*

		 * Direct calls are special, as count matters.

		 * We must test the record for direct, if the

		 * DIRECT and DIRECT_EN do not match, but only

		 * if the count is 1. That's because, if the

		 * count is something other than one, we do not

		 * want the direct enabled (it will be done via the

		 * direct helper). But if DIRECT_EN is set, and

		 * the count is not one, we need to clear it.

 If the state of this record hasn't changed, then do nothing */

 Save off if rec is being enabled (for return value) */

				/*

				 * If there's only one user (direct_ops helper)

				 * then we can call the direct function

				 * directly (no ftrace trampoline).

					/*

					 * Can only call directly if there's

					 * only one callback to the function.

		/*

		 * If this record is being updated from a nop, then

		 *   return UPDATE_MAKE_CALL.

		 * Otherwise,

		 *   return UPDATE_MODIFY_CALL to tell the caller to convert

		 *   from the save regs, to a non-save regs function or

		 *   vice versa, or from a trampoline call.

 If there's no more users, clear all flags */

			/*

			 * Just disable the record, but keep the ops TRAMP

			 * and REGS states. The _EN flags must be disabled though.

/**

 * ftrace_update_record - set a record that now is tracing or not

 * @rec: the record to update

 * @enable: set to true if the record is tracing, false to force disable

 *

 * The records that represent all functions that can be traced need

 * to be updated when tracing has been enabled.

/**

 * ftrace_test_record - check if the record has been enabled or not

 * @rec: the record to test

 * @enable: set to true to check if enabled, false if it is disabled

 *

 * The arch code may need to test if a record is already set to

 * tracing to determine how to modify the function code that it

 * represents.

	/*

	 * Need to check removed ops first.

	 * If they are being removed, and this rec has a tramp,

	 * and this rec is in the ops list, then it would be the

	 * one with the tramp.

	/*

	 * Need to find the current trampoline for a rec.

	 * Now, a trampoline is only attached to a rec if there

	 * was a single 'ops' attached to it. But this can be called

	 * when we are adding another op to the rec or removing the

	 * current one. Thus, if the op is being added, we can

	 * ignore it because it hasn't attached itself to the rec

	 * yet.

	 *

	 * If an ops is being modified (hooking to different functions)

	 * then we don't care about the new functions that are being

	 * added, just the old ones (that are probably being removed).

	 *

	 * If we are adding an ops to a function that already is using

	 * a trampoline, it needs to be removed (trampolines are only

	 * for single ops connected), then an ops that is not being

	 * modified also needs to be checked.

		/*

		 * If the ops is being added, it hasn't gotten to

		 * the point to be removed from this tree yet.

		/*

		 * If the ops is being modified and is in the old

		 * hash, then it is probably being removed from this

		 * function.

		/*

		 * If the ops is not being added or modified, and it's

		 * in its normal filter hash, then this must be the one

		 * we want!

 pass rec in as regs to have non-NULL val */

 Protected by rcu_tasks for reading, and direct_mutex for writing */

/*

 * Search the direct_functions hash to see if the given instruction pointer

 * has a direct caller attached to it.

	/*

	 * By declaring the main trampoline as this trampoline

	 * it will never have one allocated for it. Allocated

	 * trampolines should not call direct functions.

	 * The direct_ops should only be called by the builtin

	 * ftrace_regs_caller trampoline.

 CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS */

/**

 * ftrace_get_addr_new - Get the call address to set to

 * @rec:  The ftrace record descriptor

 *

 * If the record has the FTRACE_FL_REGS set, that means that it

 * wants to convert to a callback that saves all regs. If FTRACE_FL_REGS

 * is not set, then it wants to convert to the normal callback.

 *

 * Returns the address of the trampoline to set to

 Trampolines take precedence over regs */

 Ftrace is shutting down, return anything */

/**

 * ftrace_get_addr_curr - Get the call address that is already there

 * @rec:  The ftrace record descriptor

 *

 * The FTRACE_FL_REGS_EN is set when the record already points to

 * a function that saves all the regs. Basically the '_EN' version

 * represents the current state of the function.

 *

 * Returns the address of the trampoline that is currently being called

 Direct calls take precedence over trampolines */

 Trampolines take precedence over regs */

 Ftrace is shutting down, return anything */

 This needs to be done before we call ftrace_update_record */

 unknown ftrace bug */

 Stop processing */

/**

 * ftrace_rec_iter_start - start up iterating over traced functions

 *

 * Returns an iterator handle that is used to iterate over all

 * the records that represent address locations where functions

 * are traced.

 *

 * May return NULL if no records are available.

	/*

	 * We only use a single iterator.

	 * Protected by the ftrace_lock mutex.

 Could have empty pages */

/**

 * ftrace_rec_iter_next - get the next record to process.

 * @iter: The handle to the iterator.

 *

 * Returns the next iterator after the given iterator @iter.

 Could have empty pages */

/**

 * ftrace_rec_iter_record - get the record at the iterator location

 * @iter: The current iterator location

 *

 * Returns the record that the current @iter is at.

/*

 * archs can override this function if they must do something

 * before the modifying code is performed.

/*

 * archs can override this function if they must do something

 * after the modifying code is performed.

	/*

	 * If the ftrace_caller calls a ftrace_ops func directly,

	 * we need to make sure that it only traces functions it

	 * expects to trace. When doing the switch of functions,

	 * we need to update to the ftrace_ops_list_func first

	 * before the transition between old and new calls are set,

	 * as the ftrace_ops_list_func will check the ops hashes

	 * to make sure the ops are having the right functions

	 * traced.

 If irqs are disabled, we are in stop machine */

/**

 * ftrace_run_stop_machine - go back to the stop machine method

 * @command: The command to tell ftrace what to do

 *

 * If an arch needs to fall back to the stop machine method, the

 * it can call this function.

/**

 * arch_ftrace_update_code - modify the code to trace or not trace

 * @command: The command that needs to be done

 *

 * Archs can override this function if it does not need to

 * run stop_machine() to modify code.

	/*

	 * By default we use stop_machine() to modify the code.

	 * But archs can do what ever they want as long as it

	 * is safe. The stop_machine() is the safest, but also

	 * produces the most overhead.

 List of trace_ops that have allocated trampolines */

/*

 * "__builtin__ftrace" is used as a module name in /proc/kallsyms for symbols

 * for pages allocated for ftrace purposes, even though "__builtin__ftrace" is

 * not a module.

		/*

		 * Record the text poke event before the ksymbol unregister

		 * event.

 Remove from kallsyms after the perf events */

	/*

	 * Note that ftrace probes uses this to start up

	 * and modify functions it will probe. But we still

	 * set the ADDING flag for modification, as probes

	 * do not have trampolines. If they add them in the

	 * future, then the probes will need to distinguish

	 * between adding and updating probes.

 Rollback registration process */

	/*

	 * Just warn in case of unbalance, no need to kill ftrace, it's not

	 * critical but the ftrace_call callers may be never nopped again after

	 * further ftrace uses.

 Disabling ipmodify never fails */

		/*

		 * If these are dynamic or per_cpu ops, they still

		 * need their data freed. Since, function tracing is

		 * not currently active, we can just free them

		 * without synchronizing all CPUs.

	/*

	 * If the ops uses a trampoline, then it needs to be

	 * tested first on update.

 The trampoline logic checks the old hashes */

	/*

	 * If there's no more ops registered with ftrace, run a

	 * sanity check to make sure all rec flags are cleared.

	/*

	 * Dynamic ops may be freed, we must make sure that all

	 * callers are done before leaving this function.

	 * The same goes for freeing the per_cpu data of the per_cpu

	 * ops.

		/*

		 * We need to do a hard force of sched synchronization.

		 * This is because we use preempt_disable() to do RCU, but

		 * the function tracers can be called where RCU is not watching

		 * (like before user_exit()). We can not rely on the RCU

		 * infrastructure to do the synchronization, thus we must do it

		 * ourselves.

		/*

		 * When the kernel is preemptive, tasks can be preempted

		 * while on a ftrace trampoline. Just scheduling a task on

		 * a CPU is not good enough to flush them. Calling

		 * synchronize_rcu_tasks() will wait for those tasks to

		 * execute and either schedule voluntarily or enter user space.

 Force update next time */

 ftrace_start_up is true if we want ftrace running */

 ftrace_start_up is true if ftrace is running */

	/*

	 * Filter_hash being empty will default to trace module.

	 * But notrace hash requires a test of individual module functions.

/*

 * Check if the current ops references the record.

 *

 * If the ops traces all functions, then it was already accounted for.

 * If the ops does not trace the current record function, skip it.

 * If the ops ignores the function via notrace filter, skip it.

 If ops isn't enabled, ignore it */

 If ops traces all then it includes this function */

 The function must be in the filter */

 If in notrace hash, we ignore it too */

	/*

	 * When a module is loaded, this function is called to convert

	 * the calls to mcount in its text to nops, and also to create

	 * an entry in the ftrace data. Now, if ftrace is activated

	 * after this call, but before the module sets its text to

	 * read-only, the modification of enabling ftrace can fail if

	 * the read-only is done while ftrace is converting the calls.

	 * To prevent this, the module's records are set as disabled

	 * and will be enabled after the call to set the module's text

	 * to read-only.

 If something went wrong, bail without enabling anything */

			/*

			 * Do the initial record conversion from mcount jump

			 * to the NOP instructions.

 We want to fill as much as possible, with no empty pages */

 if we can't allocate this size, try something smaller */

	/*

	 * Try to allocate as much as possible in one continues

	 * location that fills in all of the space. We want to

	 * waste as little space as possible.

 room for wildcards */

	/*

	 * A probe being registered may temporarily have an empty hash

	 * and it's at the end of the func_probes list.

 Only set this if we have an item */

 probes are only available if tr is set */

 Only set this if we have an item */

 t_probe_start() must use original pos */

 next must increment pos, and t_probe_start does not */

	/*

	 * If an lseek was done, then reset and start from beginning.

	/*

	 * For set_ftrace_filter reading, if we have the filter

	 * off, we can short cut and just print out that all

	 * functions are enabled.

 Account for the message */

 reset in case of seek/pread */

	/*

	 * Unfortunately, we need to restart at ftrace_pages_start

	 * every time we let go of the ftrace_mutex. This is because

	 * those pointers can change without the lock.

	/*

	 * This shows us what functions are currently being

	 * traced and by what. Not sure if we want lockdown

	 * to hide such critical information for an admin.

	 * Although, perhaps it can show information we don't

	 * want people to see, but if something is tracing

	 * something, we probably want to know about it.

/**

 * ftrace_regex_open - initialize function tracer filter files

 * @ops: The ftrace_ops that hold the hash filters

 * @flag: The type of filter to process

 * @inode: The inode, usually passed in to your open routine

 * @file: The file, usually passed in to your open routine

 *

 * ftrace_regex_open() initializes the filter files for the

 * @ops. Depending on @flag it may process the filter hash or

 * the notrace hash of @ops. With this called from the open

 * routine, you can use ftrace_filter_write() for the write

 * routine if @flag has FTRACE_ITER_FILTER set, or

 * ftrace_notrace_write() if @flag has FTRACE_ITER_NOTRACE set.

 * tracing_lseek() should be used as the lseek routine, and

 * release must call ftrace_regex_release().

 Failed */

 Checks for tracefs lockdown */

 Checks for tracefs lockdown */

 Type for quick search ftrace basic regexes (globs) from filter_parse_regex */

/*

 * If symbols in an architecture don't correspond exactly to the user-visible

 * name of what they represent, it is possible to define this function to

 * perform the necessary adjustments.

 Do nothing if it doesn't exist */

 Do nothing if it exists */

 The index starts at 1 */

 this is a double loop, break goes to the next page */

 blank module name to match all modules */

 blank module globbing: modname xor exclude_mod */

		/*

		 * exclude_mod is set to trace everything but the given

		 * module. If it is set and the module matches, then

		 * return 0. If it is not set, and the module doesn't match

		 * also return 0. Otherwise, check the function to see if

		 * that matches.

 blank search means to match all funcs in the mod */

	/*

	 * If this is the shared global_ops filter, then we need to

	 * check if there is another ops that shares it, is enabled.

	 * If so, we still need to run the modify code.

 Only need to do this once */

 All modules have the symbol __this_module */

 We do not cache inverse filters */

 Look to remove this hash */

 no func matches all */

 We only care about modules that have not been loaded yet */

 Save this string off, and execute it when the module is loaded */

 warn? */

 warn? */

 Use the newly allocated func, as it may be "*" */

 Grabs ftrace_lock, which is why we have this extra step */

/*

 * We register the module command as a template to show others how

 * to register the a command as well.

 match_records() modifies func, and we need the original */

	/*

	 * cmd == 'mod' because we only registered this func

	 * for the 'mod' ftrace_func_command.

	 * But if you register one func with multiple commands,

	 * you can tell which command was used by the cmd

	 * parameter.

	/*

	 * Disable preemption for these calls to prevent a RCU grace

	 * period. This syncs the hash iteration and freeing of items

	 * on the hash. rcu_read_lock is too dangerous here.

/**

 * allocate_ftrace_func_mapper - allocate a new ftrace_func_mapper

 *

 * Returns a ftrace_func_mapper descriptor that can be used to map ips to data.

	/*

	 * The mapper is simply a ftrace_hash, but since the entries

	 * in the hash are not ftrace_func_entry type, we define it

	 * as a separate structure.

/**

 * ftrace_func_mapper_find_ip - Find some data mapped to an ip

 * @mapper: The mapper that has the ip maps

 * @ip: the instruction pointer to find the data for

 *

 * Returns the data mapped to @ip if found otherwise NULL. The return

 * is actually the address of the mapper data pointer. The address is

 * returned for use cases where the data is no bigger than a long, and

 * the user can use the data pointer as its data instead of having to

 * allocate more memory for the reference.

/**

 * ftrace_func_mapper_add_ip - Map some data to an ip

 * @mapper: The mapper that has the ip maps

 * @ip: The instruction pointer address to map @data to

 * @data: The data to map to @ip

 *

 * Returns 0 on success otherwise an error.

/**

 * ftrace_func_mapper_remove_ip - Remove an ip from the mapping

 * @mapper: The mapper that has the ip maps

 * @ip: The instruction pointer address to remove the data from

 *

 * Returns the data if it is found, otherwise NULL.

 * Note, if the data pointer is used as the data itself, (see 

 * ftrace_func_mapper_find_ip(), then the return value may be meaningless,

 * if the data pointer was set to zero.

/**

 * free_ftrace_func_mapper - free a mapping of ips and data

 * @mapper: The mapper that has the ip maps

 * @free_func: A function to be called on each data item.

 *

 * This is used to free the function mapper. The @free_func is optional

 * and can be used if the data needs to be freed as well.

 Subtract the ref that was used to protect this instance */

		/*

		 * Sending zero as ip tells probe_ops to free

		 * the probe->data itself

	/*

	 * Add one ref to keep it from being freed when releasing the

	 * ftrace_lock mutex.

 We do not support '!' for function probes */

 Check if the probe_ops is already registered */

	/*

	 * Note, there's a small window here that the func_hash->filter_hash

	 * may be NULL or empty. Need to be careful when reading the loop.

 Nothing found? */

			/*

			 * The caller might want to do something special

			 * for each function we find. We call the callback

			 * to give the caller an opportunity to do so.

 Nothing was added? */

 One ref for each new function traced */

 Failed to do the move, need to call the free functions */

 we do not support '!' for function probes */

 Check if the probe_ops is already registered */

 Probes only have filters */

 Nothing found? */

 still need to update the function call sites */

/*

 * Currently we only register ftrace commands from __init, so mark this

 * __init too.

/*

 * Currently we only unregister ftrace commands from __init, so mark

 * this __init too.

 command found */

 iter->hash is a local copy, so we don't need regex_lock */

/**

 * ftrace_find_direct_func - test an address if it is a registered direct caller

 * @addr: The address of a registered direct caller

 *

 * This searches to see if a ftrace direct caller has been registered

 * at a specific address, and if so, it returns a descriptor for it.

 *

 * This can be used by architecture code to see if an address is

 * a direct caller (trampoline) attached to a fentry/mcount location.

 * This is useful for the function_graph tracer, as it may need to

 * do adjustments if it traced a location that also has a direct

 * trampoline attached to it.

 May be called by fgraph trampoline (protected by rcu tasks) */

/**

 * register_ftrace_direct - Call a custom trampoline directly

 * @ip: The address of the nop at the beginning of a function

 * @addr: The address of the trampoline to call at @ip

 *

 * This is used to connect a direct call from the nop location (@ip)

 * at the start of ftrace traced functions. The location that it calls

 * (@addr) must be able to handle a direct call, and save the parameters

 * of the function being traced, and restore them (or inject new ones

 * if needed), before returning.

 *

 * Returns:

 *  0 on success

 *  -EBUSY - Another direct function is already attached (there can be only one)

 *  -ENODEV - @ip does not point to a ftrace nop location (or not supported)

 *  -ENOMEM - There was an allocation failure.

 See if there's a direct function at @ip already */

	/*

	 * Check if the rec says it has a direct call but we didn't

	 * find one earlier?

 Make sure the ip points to the exact record */

 Need to check this ip for a direct. */

 Passed in ip just needs to be on the call site */

 This is the good path (see the ! before WARN) */

/**

 * ftrace_modify_direct_caller - modify ftrace nop directly

 * @entry: The ftrace hash entry of the direct helper for @rec

 * @rec: The record representing the function site to patch

 * @old_addr: The location that the site at @rec->ip currently calls

 * @new_addr: The location that the site at @rec->ip should call

 *

 * An architecture may overwrite this function to optimize the

 * changing of the direct callback on an ftrace nop location.

 * This is called with the ftrace_lock mutex held, and no other

 * ftrace callbacks are on the associated record (@rec). Thus,

 * it is safe to modify the ftrace record, where it should be

 * currently calling @old_addr directly, to call @new_addr.

 *

 * Safety checks should be made to make sure that the code at

 * @rec->ip is currently calling @old_addr. And this must

 * also update entry->direct to @new_addr.

	/*

	 * The ftrace_lock was used to determine if the record

	 * had more than one registered user to it. If it did,

	 * we needed to prevent that from changing to do the quick

	 * switch. But if it did not (only a direct caller was attached)

	 * then this function is called. But this function can deal

	 * with attached callers to the rec that we care about, and

	 * since this function uses standard ftrace calls that take

	 * the ftrace_lock mutex, we need to release it.

	/*

	 * By setting a stub function at the same address, we force

	 * the code to call the iterator and the direct_ops helper.

	 * This means that @ip does not call the direct call, and

	 * we can simply modify it.

	/*

	 * By removing the stub, we put back the direct call, calling

	 * the @new_addr.

/**

 * modify_ftrace_direct - Modify an existing direct call to call something else

 * @ip: The instruction pointer to modify

 * @old_addr: The address that the current @ip calls directly

 * @new_addr: The address that the @ip should call

 *

 * This modifies a ftrace direct caller at an instruction pointer without

 * having to disable it first. The direct call will switch over to the

 * @new_addr without missing anything.

 *

 * Returns: zero on success. Non zero on error, which includes:

 *  -ENODEV : the @ip given has no direct caller attached

 *  -EINVAL : the @old_addr does not match the current direct caller

	/*

	 * If there's no other ftrace callback on the rec->ip location,

	 * then it can be changed directly by the architecture.

	 * If there is another caller, then we just need to change the

	 * direct caller helper to point to @new_addr.

/**

 * register_ftrace_direct_multi - Call a custom trampoline directly

 * for multiple functions registered in @ops

 * @ops: The address of the struct ftrace_ops object

 * @addr: The address of the trampoline to call at @ops functions

 *

 * This is used to connect a direct calls to @addr from the nop locations

 * of the functions registered in @ops (with by ftrace_set_filter_ip

 * function).

 *

 * The location that it calls (@addr) must be able to handle a direct call,

 * and save the parameters of the function being traced, and restore them

 * (or inject new ones if needed), before returning.

 *

 * Returns:

 *  0 on success

 *  -EINVAL  - The @ops object was already registered with this call or

 *             when there are no functions in @ops object.

 *  -EBUSY   - Another direct function is already attached (there can be only one)

 *  -ENODEV  - @ip does not point to a ftrace nop location (or not supported)

 *  -ENOMEM  - There was an allocation failure.

 Make sure requested entries are not already registered.. */

 ... and insert them to direct_functions hash. */

/**

 * unregister_ftrace_direct_multi - Remove calls to custom trampoline

 * previously registered by register_ftrace_direct_multi for @ops object.

 * @ops: The address of the struct ftrace_ops object

 *

 * This is used to remove a direct calls to @addr from the nop locations

 * of the functions registered in @ops (with by ftrace_set_filter_ip

 * function).

 *

 * Returns:

 *  0 on success

 *  -EINVAL - The @ops object was not properly registered.

/**

 * modify_ftrace_direct_multi - Modify an existing direct 'multi' call

 * to call something else

 * @ops: The address of the struct ftrace_ops object

 * @addr: The address of the new trampoline to call at @ops functions

 *

 * This is used to unregister currently registered direct caller and

 * register new one @addr on functions registered in @ops object.

 *

 * Note there's window between ftrace_shutdown and ftrace_startup calls

 * where there will be no callbacks called.

 *

 * Returns: zero on success. Non zero on error, which includes:

 *  -EINVAL - The @ops object was not properly registered.

 Enable the tmp_ops to have the same functions as the direct ops */

	/*

	 * Now the ftrace_ops_list_func() is called to do the direct callers.

	 * We can safely change the direct functions attached to each entry.

 Removing the tmp_ops will add the updated direct callers to the functions */

 CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS */

/**

 * ftrace_set_filter_ip - set a function to filter on in ftrace by address

 * @ops - the ops to set the filter with

 * @ip - the address to add to or remove from the filter.

 * @remove - non zero to remove the ip from the filter

 * @reset - non zero to reset all filters before applying this filter.

 *

 * Filters denote which functions should be enabled when tracing is enabled

 * If @ip is NULL, it fails to update filter.

/**

 * ftrace_ops_set_global_filter - setup ops to use global filters

 * @ops - the ops which will use the global filters

 *

 * ftrace users who need global function trace filtering should call this.

 * It can set the global filter only if ops were not initialized before.

/**

 * ftrace_set_filter - set a function to filter on in ftrace

 * @ops - the ops to set the filter with

 * @buf - the string that holds the function filter text.

 * @len - the length of the string.

 * @reset - non zero to reset all filters before applying this filter.

 *

 * Filters denote which functions should be enabled when tracing is enabled.

 * If @buf is NULL and reset is set, all functions will be enabled for tracing.

/**

 * ftrace_set_notrace - set a function to not trace in ftrace

 * @ops - the ops to set the notrace filter with

 * @buf - the string that holds the function notrace text.

 * @len - the length of the string.

 * @reset - non zero to reset all filters before applying this filter.

 *

 * Notrace Filters denote which functions should not be enabled when tracing

 * is enabled. If @buf is NULL and reset is set, all functions will be enabled

 * for tracing.

/**

 * ftrace_set_global_filter - set a function to filter on with global tracers

 * @buf - the string that holds the function filter text.

 * @len - the length of the string.

 * @reset - non zero to reset all filters before applying this filter.

 *

 * Filters denote which functions should be enabled when tracing is enabled.

 * If @buf is NULL and reset is set, all functions will be enabled for tracing.

/**

 * ftrace_set_global_notrace - set a function to not trace with global tracers

 * @buf - the string that holds the function notrace text.

 * @len - the length of the string.

 * @reset - non zero to reset all filters before applying this filter.

 *

 * Notrace Filters denote which functions should not be enabled when tracing

 * is enabled. If @buf is NULL and reset is set, all functions will be enabled

 * for tracing.

/*

 * command line interface to allow users to set filters on boot up.

 Used by function selftest to not test if filter is set */

 we allow only one expression at a time */

 CONFIG_FUNCTION_GRAPH_TRACER */

 CONFIG_FUNCTION_GRAPH_TRACER */

 For read only, the hash is the ops hash */

 for hash table iteration */

 Nothing, tell g_show to print all functions are enabled */

 Failed */

	/*

	 * All uses of fgd->hash must be taken with the graph_lock

	 * held. The graph_lock is going to be released, so force

	 * fgd->hash to be reinitialized when it is taken again.

		/*

		 * We need to do a hard force of sched synchronization.

		 * This is because we use preempt_disable() to do RCU, but

		 * the function tracers can be called where RCU is not watching

		 * (like before user_exit()). We can not rely on the RCU

		 * infrastructure to do the synchronization, thus we must do it

		 * ourselves.

 decode regex */

 Read mode uses seq functions */

 CONFIG_FUNCTION_GRAPH_TRACER */

/*

 * The name "destroy_filter_files" is really a misnomer. Although

 * in the future, it may actually delete the files, but this is

 * really intended to make sure the ops passed in are disabled

 * and that when this function returns, the caller is free to

 * free the ops.

 *

 * The "destroy" name is only to match the "create" name that this

 * should be paired with.

 CONFIG_FUNCTION_GRAPH_TRACER */

 Shut up gcc */

	/*

	 * Core and each module needs their own pages, as

	 * modules will free them when they are removed.

	 * Force a new page to be allocated for modules.

 First initialization */

 Hmm, we have free pages? */

		/*

		 * Some architecture linkers will pad between

		 * the different mcount_loc sections of different

		 * object files to satisfy alignments.

		 * Skip any NULL pointers.

 We should have allocated enough */

 We should have used all pages */

 Assign the last page to ftrace_pages */

	/*

	 * We only need to disable interrupts on start up

	 * because we are modifying code that an interrupt

	 * may execute, and the modification is not atomic.

	 * But for modules, nothing runs the code we modify

	 * until we are finished with it, and there's no

	 * reason to cause large interrupt latencies while we do it.

		/*

		 * Do not allow this rec to match again.

		 * Yeah, it may waste some memory, but will be removed

		 * if/when the hash is modified again.

 Clear any records from hashes */

 All the contents of mod_map are now not visible to readers */

	/*

	 * Each module has its own ftrace_pages, remove

	 * them from the list.

			/*

			 * As core pages are first, the first

			 * page should never be a module page.

 Check if we are deleting the last page */

 Needs to be called outside of ftrace_lock */

	/*

	 * If the tracing is enabled, go ahead and enable the record.

	 *

	 * The reason not to enable the record immediately is the

	 * inherent check of ftrace_make_nop/ftrace_make_call for

	 * correct previous instructions.  Making first the NOP

	 * conversion puts the module to the correct state, thus

	 * passing the ftrace_make_call check.

	 *

	 * We also delay this to after the module code already set the

	 * text to read-only, as we now need to set it back to read-write

	 * so that we can modify the text.

		/*

		 * do_for_each_ftrace_rec() is a double loop.

		 * module text shares the pg. If a record is

		 * not part of this module, then skip this pg,

		 * which the "break" will do.

		/*

		 * When adding a module, we need to check if tracers are

		 * currently enabled and if they are, and can trace this record,

		 * we need to enable the module functions as well as update the

		 * reference counts for those function records.

 mod_map is freed via call_rcu() */

 CONFIG_MODULES */

 Clear any init ips from hashes */

	/*

	 * Do not allow this rec to match again.

	 * Yeah, it may waste some memory, but will be removed

	 * if/when the hash is modified again.

 overload flags, as it is unsigned long */

	/*

	 * If we are freeing module init memory, then check if

	 * any tracer is active. If so, we need to save a mapping of

	 * the module functions being freed with the address.

 rec will be cleared from hashes after ftrace_lock unlock */

 More than one function may be in this block */

 Do nothing if arch does not support this */

 Add to kallsyms before the perf events */

		/*

		 * Record the perf text poke event after the ksymbol register

		 * event.

 CONFIG_DYNAMIC_FTRACE */

 If we filter on pids, update to use the pid function */

	/*

	 * The ftrace_test_and_set_recursion() will disable preemption,

	 * which is required since some of the ops may be dynamically

	 * allocated, they must be freed after a synchronize_rcu().

 Stub functions don't need to be called nor tested */

		/*

		 * Check the following for each ops before calling their func:

		 *  if RCU flag is set, then rcu_is_watching() must be true

		 *  if PER_CPU is set, then ftrace_function_local_disable()

		 *                          must be false

		 *  Otherwise test if the ip matches the ops filter

		 *

		 * If any of the above fails then the op->func() is not executed.

/*

 * Some archs only support passing ip and parent_ip. Even though

 * the list function ignores the op parameter, we do not want any

 * C side effects, where a function is called without the caller

 * sending a third parameter.

 * Archs are to support both the regs and ftrace_ops at the same time.

 * If they support ftrace_ops, it is assumed they support regs.

 * If call backs want to use regs, they must either check for regs

 * being NULL, or CONFIG_DYNAMIC_FTRACE_WITH_REGS.

 * Note, CONFIG_DYNAMIC_FTRACE_WITH_REGS expects a full regs to be saved.

 * An architecture can pass partial regs with ftrace_ops and still

 * set the ARCH_SUPPORTS_FTRACE_OPS.

 *

 * In vmlinux.lds.h, ftrace_ops_list_func() is defined to be

 * arch_ftrace_ops_list_func.

/*

 * If there's only one function registered but it does not support

 * recursion, needs RCU protection and/or requires per cpu handling, then

 * this function will be called by the mcount trampoline.

/**

 * ftrace_ops_get_func - get the function a trampoline should call

 * @ops: the ops to get the function for

 *

 * Normally the mcount trampoline will call the ops->func, but there

 * are times that it should not. For example, if the ops does not

 * have its own recursion protection, then it should call the

 * ftrace_ops_assist_func() instead.

 *

 * Returns the function that the trampoline should call for @ops.

	/*

	 * If the function does not handle recursion or needs to be RCU safe,

	 * then we need to call the assist handler.

 Make sure there's something to do */

 See if the pids still need to be checked after this */

 Wait till all users are no longer using pid filtering */

 Greater than any max PID */

 copy tr over to seq ops */

	/*

	 * This function is called by on_each_cpu() while the

	 * event_mutex is held.

 Register a probe to set whether to ignore the tracing of a task */

	/*

	 * Ignoring of pids is done at task switch. But we have to

	 * check for those tasks that are currently running.

	 * Always do this in case a pid was appended or removed.

 Only the top level directory has the dyn_tracefs and profile */

/**

 * ftrace_kill - kill ftrace

 *

 * This function should be used by panic code. It stops ftrace

 * but in a not so nice way. If you need to simply kill ftrace

 * from a non-atomic section, use ftrace_kill.

/**

 * ftrace_is_dead - Test if ftrace is dead or not.

 *

 * Returns 1 if ftrace is "dead", zero otherwise.

/**

 * register_ftrace_function - register a function for profiling

 * @ops - ops structure that holds the function for profiling.

 *

 * Register a function to be called by all functions in the

 * kernel.

 *

 * Note: @ops->func and all the functions it calls must be labeled

 *       with "notrace", otherwise it will go into a

 *       recursive loop.

/**

 * unregister_ftrace_function - unregister a function for profiling.

 * @ops - ops structure that holds the function to unregister

 *

 * Unregister a function that was added to be called by ftrace profiling.

 we are starting ftrace again */

 stopping ftrace calls (just send to ftrace_stub) */

 SPDX-License-Identifier: GPL-2.0

/*

 * trace_seq.c

 *

 * Copyright (C) 2008-2014 Red Hat Inc, Steven Rostedt <srostedt@redhat.com>

 *

 * The trace_seq is a handy tool that allows you to pass a descriptor around

 * to a buffer that other functions can write to. It is similar to the

 * seq_file functionality but has some differences.

 *

 * To use it, the trace_seq must be initialized with trace_seq_init().

 * This will set up the counters within the descriptor. You can call

 * trace_seq_init() more than once to reset the trace_seq to start

 * from scratch.

 * 

 * The buffer size is currently PAGE_SIZE, although it may become dynamic

 * in the future.

 *

 * A write to the buffer will either succeed or fail. That is, unlike

 * sprintf() there will not be a partial write (well it may write into

 * the buffer but it wont update the pointers). This allows users to

 * try to write something into the trace_seq buffer and if it fails

 * they can flush it and try again.

 *

 How much buffer is left on the trace_seq? */

/*

 * trace_seq should work with being initialized with 0s.

/**

 * trace_print_seq - move the contents of trace_seq into a seq_file

 * @m: the seq_file descriptor that is the destination

 * @s: the trace_seq descriptor that is the source.

 *

 * Returns 0 on success and non zero on error. If it succeeds to

 * write to the seq_file it will reset the trace_seq, otherwise

 * it does not modify the trace_seq to let the caller try again.

	/*

	 * Only reset this buffer if we successfully wrote to the

	 * seq_file buffer. This lets the caller try again or

	 * do something else with the contents.

/**

 * trace_seq_printf - sequence printing of trace information

 * @s: trace sequence descriptor

 * @fmt: printf format string

 *

 * The tracer may use either sequence operations or its own

 * copy to user routines. To simplify formatting of a trace

 * trace_seq_printf() is used to store strings into a special

 * buffer (@s). Then the output may be either used by

 * the sequencer or pulled into another buffer.

 If we can't write it all, don't bother writing anything */

/**

 * trace_seq_bitmask - write a bitmask array in its ASCII representation

 * @s:		trace sequence descriptor

 * @maskp:	points to an array of unsigned longs that represent a bitmask

 * @nmaskbits:	The number of bits that are valid in @maskp

 *

 * Writes a ASCII representation of a bitmask string into @s.

/**

 * trace_seq_vprintf - sequence printing of trace information

 * @s: trace sequence descriptor

 * @fmt: printf format string

 *

 * The tracer may use either sequence operations or its own

 * copy to user routines. To simplify formatting of a trace

 * trace_seq_printf is used to store strings into a special

 * buffer (@s). Then the output may be either used by

 * the sequencer or pulled into another buffer.

 If we can't write it all, don't bother writing anything */

/**

 * trace_seq_bprintf - Write the printf string from binary arguments

 * @s: trace sequence descriptor

 * @fmt: The format string for the @binary arguments

 * @binary: The binary arguments for @fmt.

 *

 * When recording in a fast path, a printf may be recorded with just

 * saving the format and the arguments as they were passed to the

 * function, instead of wasting cycles converting the arguments into

 * ASCII characters. Instead, the arguments are saved in a 32 bit

 * word array that is defined by the format string constraints.

 *

 * This function will take the format and the binary array and finish

 * the conversion into the ASCII string within the buffer.

 If we can't write it all, don't bother writing anything */

/**

 * trace_seq_puts - trace sequence printing of simple string

 * @s: trace sequence descriptor

 * @str: simple string to record

 *

 * The tracer may use either the sequence operations or its own

 * copy to user routines. This function records a simple string

 * into a special buffer (@s) for later retrieval by a sequencer

 * or other mechanism.

/**

 * trace_seq_putc - trace sequence printing of simple character

 * @s: trace sequence descriptor

 * @c: simple character to record

 *

 * The tracer may use either the sequence operations or its own

 * copy to user routines. This function records a simple character

 * into a special buffer (@s) for later retrieval by a sequencer

 * or other mechanism.

/**

 * trace_seq_putmem - write raw data into the trace_seq buffer

 * @s: trace sequence descriptor

 * @mem: The raw memory to copy into the buffer

 * @len: The length of the raw memory to copy (in bytes)

 *

 * There may be cases where raw memory needs to be written into the

 * buffer and a strcpy() would not work. Using this function allows

 * for such cases.

/**

 * trace_seq_putmem_hex - write raw memory into the buffer in ASCII hex

 * @s: trace sequence descriptor

 * @mem: The raw memory to write its hex ASCII representation of

 * @len: The length of the raw memory to copy (in bytes)

 *

 * This is similar to trace_seq_putmem() except instead of just copying the

 * raw memory into the buffer it writes its ASCII representation of it

 * in hex characters.

 Each byte is represented by two chars */

 The added spaces can still cause an overflow */

/**

 * trace_seq_path - copy a path into the sequence buffer

 * @s: trace sequence descriptor

 * @path: path to write into the sequence buffer.

 *

 * Write a path name into the sequence buffer.

 *

 * Returns 1 if we successfully written all the contents to

 *   the buffer.

 * Returns 0 if we the length to write is bigger than the

 *   reserved buffer space. In this case, nothing gets written.

/**

 * trace_seq_to_user - copy the sequence buffer to user space

 * @s: trace sequence descriptor

 * @ubuf: The userspace memory location to copy to

 * @cnt: The amount to copy

 *

 * Copies the sequence buffer into the userspace memory pointed to

 * by @ubuf. It starts from the last read position (@s->readpos)

 * and writes up to @cnt characters or till it reaches the end of

 * the content in the buffer (@s->len), which ever comes first.

 *

 * On success, it returns a positive number of the number of bytes

 * it copied.

 *

 * On failure it returns -EBUSY if all of the content in the

 * sequence has been already read, which includes nothing in the

 * sequence (@s->len == @s->readpos).

 *

 * Returns -EFAULT if the copy to userspace fails.

 SPDX-License-Identifier: GPL-2.0

/*

 * Common code for probe-based Dynamic events.

 *

 * This code was copied from kernel/trace/trace_kprobe.c written by

 * Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>

 *

 * Updates to make this generic:

 * Copyright (C) IBM Corporation, 2010-2011

 * Author:     Srikar Dronamraju

 Printing  in basic type function template */

 Print type function for string type */

 Fetch type information table */

 Special types */

 Basic types */

 Special case: bitfield */

 Recalculate the length and allocate buffer */

		/**

		 * Set the error position is next to the last arg + space.

		 * Note that len includes the terminal null and the cursor

		 * appears at pos + 1.

 And make a command string from argv array */

 Split symbol and offset. */

 @buf must has MAX_EVENT_NAME_LEN size */

 Recursive argument parser */

 named register */

 memory, file-offset or symbol */

 load address */

 kprobes don't support file offsets */

 imm64?

 uprobes don't support symbols */

 Preserve symbol for updating */

 These are fetching from memory */

 deref memory */

 Skip '+', because kstrtol() rejects it. */

 Immediate value */

 Immediate string */

 Parsed, but do not find fetch method */

 Bitfield type needs to be parsed into a fetch function */

 Use simple one */

 String length checking wrapper */

	/*

	 * Since $comm and immediate string can not be dereferenced,

	 * we can find those by strcmp.

 The type of $comm must be "string", and not an array. */

 Store operation */

			/*

			 * IMM, DATA and COMM is pointing actual address, those

			 * must be kept, and if parg->count != 0, this is an

			 * array of string pointers instead of string address

			 * itself.

 If op == DEREF, replace it with STRING */

 Modify operation */

 Loop(Array) operation */

 Shrink down the code buffer */

 Return 1 if name is reserved or already used by another argument */

 Increment count for freeing args in error case */

 If argument name is omitted, set "argN" */

 Parse fetch argument */

 When len=0, we just calculate the needed length */

 return the length of print_fmt */

 First: called with 0 length to calculate the needed length */

 Second: actually write the @print_fmt */

 Set argument names as fields */

/*

 * Return the smallest index of different type argument (start from 1).

 * If all argument types and name are same, return 0.

 In case of more arguments */

 SPDX-License-Identifier: GPL-2.0

/*

 * ring buffer tester and benchmark

 *

 * Copyright (C) 2009 Steven Rostedt <srostedt@redhat.com>

 run time and sleep time in seconds */

 number of events for writer to wake up the reader */

 The commit may have missed event flags set, clear them */

 failed writes may be discarded events */

 toggle between reading pages and events */

	/*

	 * Continue running until the producer specifically asks to stop

	 * and is ready for the completion.

		/* Wait till the producer wakes us up when there is more data

		 * available or when the producer wants us to finish reading.

	/*

	 * Hammer the buffer for 10 secs (this may

	 * make the system stall)

		/*

		 * If we are a non preempt kernel, the 10 seconds run will

		 * stop everything while it runs. Instead, we will call

		 * cond_resched and also add any time that was lost by a

		 * reschedule.

		 *

		 * Do a cond resched at the same frequency we would wake up

		 * the reader.

 Init both completions here to avoid races */

 the completions must be visible before the finish var */

 Let the user know that the test is running at low priority */

 Convert time from usecs to millisecs */

 Calculate the average time in nanosecs */

 it is possible that hit + missed will overflow and be zero */

 make it non zero */

 Calculate the average time in nanosecs */

 make a one meg buffer in overwite mode */

	/*

	 * Run them as low-prio background tasks by default:

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2008 Steven Rostedt <srostedt@redhat.com>

 *

/*

 * The stack tracer looks for a maximum stack at each call from a function. It

 * registers a callback from ftrace, and in that callback it examines the stack

 * size. It determines the stack size from the variable passed in, which is the

 * address of a local variable in the stack_trace_call() callback function.

 * The stack size is calculated by the address of the local variable to the top

 * of the current stack. If that size is smaller than the currently saved max

 * stack size, nothing more is done.

 *

 * If the size of the stack is greater than the maximum recorded size, then the

 * following algorithm takes place.

 *

 * For architectures (like x86) that store the function's return address before

 * saving the function's local variables, the stack will look something like

 * this:

 *

 *   [ top of stack ]

 *    0: sys call entry frame

 *   10: return addr to entry code

 *   11: start of sys_foo frame

 *   20: return addr to sys_foo

 *   21: start of kernel_func_bar frame

 *   30: return addr to kernel_func_bar

 *   31: [ do trace stack here ]

 *

 * The save_stack_trace() is called returning all the functions it finds in the

 * current stack. Which would be (from the bottom of the stack to the top):

 *

 *   return addr to kernel_func_bar

 *   return addr to sys_foo

 *   return addr to entry code

 *

 * Now to figure out how much each of these functions' local variable size is,

 * a search of the stack is made to find these values. When a match is made, it

 * is added to the stack_dump_trace[] array. The offset into the stack is saved

 * in the stack_trace_index[] array. The above example would show:

 *

 *        stack_dump_trace[]        |   stack_trace_index[]

 *        ------------------        +   -------------------

 *  return addr to kernel_func_bar  |          30

 *  return addr to sys_foo          |          20

 *  return addr to entry            |          10

 *

 * The print_max_stack() function above, uses these values to print the size of

 * each function's portion of the stack.

 *

 *  for (i = 0; i < nr_entries; i++) {

 *     size = i == nr_entries - 1 ? stack_trace_index[i] :

 *                    stack_trace_index[i] - stack_trace_index[i+1]

 *     print "%d %d %d %s\n", i, stack_trace_index[i], size, stack_dump_trace[i]);

 *  }

 *

 * The above shows

 *

 *     depth size  location

 *     ----- ----  --------

 *  0    30   10   kernel_func_bar

 *  1    20   10   sys_foo

 *  2    10   10   entry code

 *

 * Now for architectures that might save the return address after the functions

 * local variables (saving the link register before calling nested functions),

 * this will cause the stack to look a little different:

 *

 * [ top of stack ]

 *  0: sys call entry frame

 * 10: start of sys_foo_frame

 * 19: return addr to entry code << lr saved before calling kernel_func_bar

 * 20: start of kernel_func_bar frame

 * 29: return addr to sys_foo_frame << lr saved before calling next function

 * 30: [ do trace stack here ]

 *

 * Although the functions returned by save_stack_trace() may be the same, the

 * placement in the stack will be different. Using the same algorithm as above

 * would yield:

 *

 *        stack_dump_trace[]        |   stack_trace_index[]

 *        ------------------        +   -------------------

 *  return addr to kernel_func_bar  |          30

 *  return addr to sys_foo          |          29

 *  return addr to entry            |          19

 *

 * Where the mapping is off by one:

 *

 *   kernel_func_bar stack frame size is 29 - 19 not 30 - 29!

 *

 * To fix this, if the architecture sets ARCH_RET_ADDR_AFTER_LOCAL_VARS the

 * values in stack_trace_index[] are shifted by one to and the number of

 * stack trace entries is decremented by one.

 *

 *        stack_dump_trace[]        |   stack_trace_index[]

 *        ------------------        +   -------------------

 *  return addr to kernel_func_bar  |          29

 *  return addr to sys_foo          |          19

 *

 * Although the entry function is not displayed, the first function (sys_foo)

 * will still include the stack size of it.

 Remove the frame of the tracer */

 we do not handle interrupt stacks yet */

 Can't do this from NMI context (can cause deadlocks) */

 In case another CPU set the tracer_frame on us */

 a race could have already updated it */

 Skip over the overhead of the stack tracer itself */

	/*

	 * Some archs may not have the passed in ip in the dump.

	 * If that happens, we need to show everything.

	/*

	 * Now find where in the stack these are.

	/*

	 * Loop through all the entries. One of the entries may

	 * for some reason be missed on the stack, so we may

	 * have to account for them. If they are all there, this

	 * loop will only happen once. This code only takes place

	 * on a new max, so it is far from a fast path.

			/*

			 * The READ_ONCE_NOCHECK is used to let KASAN know that

			 * this is not a stack-out-of-bounds error.

 Start the search from here */

				/*

				 * We do not want to show the overhead

				 * of the stack tracer stack in the

				 * max stack. If we haven't figured

				 * out what that is, then figure it out

				 * now.

	/*

	 * Some archs will store the link register before calling

	 * nested functions. This means the saved return address

	 * comes after the local storage, and we need to shift

	 * for that.

 Some archs may not define MCOUNT_INSN_SIZE */

 no atomic needed, we only modify this variable by this cpu */

 If rcu is not watching, then save stack trace can fail */

 prevent recursion in schedule */

	/*

	 * In case we trace inside arch_spin_lock() or after (NMI),

	 * we will cause circular lock, so we also need to increase

	 * the percpu disable_stack_tracer here.

 Checks for tracefs lockdown */

 CONFIG_DYNAMIC_FTRACE */

 SPDX-License-Identifier: GPL-2.0

/*

 * trace binary printk

 *

 * Copyright (C) 2008 Lai Jiangshan <laijs@cn.fujitsu.com>

 *

/*

 * modules trace_printk()'s formats are autosaved in struct trace_bprintk_fmt

 * which are queued on trace_bprintk_fmt_list.

 serialize accesses to trace_bprintk_fmt_list */

 allocate the trace_printk per cpu buffers */

/*

 * The debugfs/tracing/printk_formats file maps the addresses with

 * the ASCII formats that are used in the bprintk events in the

 * buffer. For userspace tools to be able to decode the events from

 * the buffer, they need to be able to map the address with the format.

 *

 * The addresses of the bprintk formats are in their own section

 * __trace_printk_fmt. But for modules we copy them into a link list.

 * The code to print the formats and their addresses passes around the

 * address of the fmt string. If the fmt address passed into the seq

 * functions is within the kernel core __trace_printk_fmt section, then

 * it simply uses the next pointer in the list.

 *

 * When the fmt pointer is outside the kernel core __trace_printk_fmt

 * section, then we need to read the link list pointers. The trick is

 * we pass the address of the string to the seq function just like

 * we do for the kernel core formats. To get back the structure that

 * holds the format, we simply use container_of() and then go to the

 * next format in the list.

	/*

	 * v will point to the address of the fmt record from t_next

	 * v will be NULL from t_start.

	 * If this is the first pointer or called from start

	 * then we need to walk the list.

 search the module list */

 pos > index */

	/*

	 * v points to the address of the fmt field in the mod list

	 * structure that holds the module print format.

 !CONFIG_MODULES */

 CONFIG_MODULES */

	/*

	 * The __tracepoint_str section is treated the same as the

	 * __trace_printk_fmt section. The difference is that the

	 * __trace_printk_fmt section should only be used by trace_printk()

	 * in a debugging environment, as if anything exists in that section

	 * the trace_prink() helper buffers are allocated, which would just

	 * waste space in a production environment.

	 *

	 * The __tracepoint_str sections on the other hand are used by

	 * tracepoints which need to map pointers to their strings to

	 * the ASCII text for userspace.

	/*

	 * Tabs and new lines need to be converted.

 SPDX-License-Identifier: GPL-2.0

/*

 * Power trace points

 *

 * Copyright (C) 2009 Ming Lei <ming.lei@canonical.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic dynamic event control interface

 *

 * Copyright (C) 2018 Masami Hiramatsu <mhiramat@kernel.org>

 for trace_event_sem */

 Protected by event_mutex */

/*

 * dyn_events_release_all - Release all specific events

 * @type:	the dyn_event_operations * which filters releasing events

 *

 * This releases all events which ->ops matches @type. If @type is NULL,

 * all events are released.

 * Return -EBUSY if any of them are in use, and return other errors when

 * it failed to free the given event. Except for -EBUSY, event releasing

 * process will be aborted at that point and there may be some other

 * releasable events on the list.

 Make a tracefs interface for controlling dynamic events */

 Event list interface */

/**

 * dynevent_arg_add - Add an arg to a dynevent_cmd

 * @cmd: A pointer to the dynevent_cmd struct representing the new event cmd

 * @arg: The argument to append to the current cmd

 * @check_arg: An (optional) pointer to a function checking arg sanity

 *

 * Append an argument to a dynevent_cmd.  The argument string will be

 * appended to the current cmd string, followed by a separator, if

 * applicable.  Before the argument is added, the @check_arg function,

 * if present, will be used to check the sanity of the current arg

 * string.

 *

 * The cmd string and separator should be set using the

 * dynevent_arg_init() before any arguments are added using this

 * function.

 *

 * Return: 0 if successful, error otherwise.

/**

 * dynevent_arg_pair_add - Add an arg pair to a dynevent_cmd

 * @cmd: A pointer to the dynevent_cmd struct representing the new event cmd

 * @arg_pair: The argument pair to append to the current cmd

 * @check_arg: An (optional) pointer to a function checking arg sanity

 *

 * Append an argument pair to a dynevent_cmd.  An argument pair

 * consists of a left-hand-side argument and a right-hand-side

 * argument separated by an operator, which can be whitespace, all

 * followed by a separator, if applicable.  This can be used to add

 * arguments of the form 'type variable_name;' or 'x+y'.

 *

 * The lhs argument string will be appended to the current cmd string,

 * followed by an operator, if applicable, followed by the rhs string,

 * followed finally by a separator, if applicable.  Before the

 * argument is added, the @check_arg function, if present, will be

 * used to check the sanity of the current arg strings.

 *

 * The cmd strings, operator, and separator should be set using the

 * dynevent_arg_pair_init() before any arguments are added using this

 * function.

 *

 * Return: 0 if successful, error otherwise.

/**

 * dynevent_str_add - Add a string to a dynevent_cmd

 * @cmd: A pointer to the dynevent_cmd struct representing the new event cmd

 * @str: The string to append to the current cmd

 *

 * Append a string to a dynevent_cmd.  The string will be appended to

 * the current cmd string as-is, with nothing prepended or appended.

 *

 * Return: 0 if successful, error otherwise.

/**

 * dynevent_cmd_init - Initialize a dynevent_cmd object

 * @cmd: A pointer to the dynevent_cmd struct representing the cmd

 * @buf: A pointer to the buffer to generate the command into

 * @maxlen: The length of the buffer the command will be generated into

 * @type: The type of the cmd, checked against further operations

 * @run_command: The type-specific function that will actually run the command

 *

 * Initialize a dynevent_cmd.  A dynevent_cmd is used to build up and

 * run dynamic event creation commands, such as commands for creating

 * synthetic and kprobe events.  Before calling any of the functions

 * used to build the command, a dynevent_cmd object should be

 * instantiated and initialized using this function.

 *

 * The initialization sets things up by saving a pointer to the

 * user-supplied buffer and its length via the @buf and @maxlen

 * params, and by saving the cmd-specific @type and @run_command

 * params which are used to check subsequent dynevent_cmd operations

 * and actually run the command when complete.

/**

 * dynevent_arg_init - Initialize a dynevent_arg object

 * @arg: A pointer to the dynevent_arg struct representing the arg

 * @separator: An (optional) separator, appended after adding the arg

 *

 * Initialize a dynevent_arg object.  A dynevent_arg represents an

 * object used to append single arguments to the current command

 * string.  After the arg string is successfully appended to the

 * command string, the optional @separator is appended.  If no

 * separator was specified when initializing the arg, a space will be

 * appended.

/**

 * dynevent_arg_pair_init - Initialize a dynevent_arg_pair object

 * @arg_pair: A pointer to the dynevent_arg_pair struct representing the arg

 * @operator: An (optional) operator, appended after adding the first arg

 * @separator: An (optional) separator, appended after adding the second arg

 *

 * Initialize a dynevent_arg_pair object.  A dynevent_arg_pair

 * represents an object used to append argument pairs such as 'type

 * variable_name;' or 'x+y' to the current command string.  An

 * argument pair consists of a left-hand-side argument and a

 * right-hand-side argument separated by an operator, which can be

 * whitespace, all followed by a separator, if applicable.  After the

 * first arg string is successfully appended to the command string,

 * the optional @operator is appended, followed by the second arg and

 * optional @separator.  If no separator was specified when

 * initializing the arg, a space will be appended.

/**

 * dynevent_create - Create the dynamic event contained in dynevent_cmd

 * @cmd: The dynevent_cmd object containing the dynamic event creation command

 *

 * Once a dynevent_cmd object has been successfully built up via the

 * dynevent_cmd_init(), dynevent_arg_add() and dynevent_arg_pair_add()

 * functions, this function runs the final command to actually create

 * the event.

 *

 * Return: 0 if the event was successfully created, error otherwise.

 SPDX-License-Identifier: GPL-2.0

/*

 * Test module for in-kernel synthetic event creation and generation.

 *

 * Copyright (C) 2019 Tom Zanussi <zanussi@kernel.org>

/*

 * This module is a simple test of basic functionality for in-kernel

 * synthetic event creation and generation, the first and second tests

 * using synth_event_gen_cmd_start() and synth_event_add_field(), the

 * third uses synth_event_create() to do it all at once with a static

 * field array.

 *

 * Following that are a few examples using the created events to test

 * various ways of tracing a synthetic event.

 *

 * To test, select CONFIG_SYNTH_EVENT_GEN_TEST and build the module.

 * Then:

 *

 * # insmod kernel/trace/synth_event_gen_test.ko

 * # cat /sys/kernel/debug/tracing/trace

 *

 * You should see several events in the trace buffer -

 * "create_synth_test", "empty_synth_test", and several instances of

 * "gen_synth_test".

 *

 * To remove the events, remove the module:

 *

 * # rmmod synth_event_gen_test

 *

/*

 * Test to make sure we can create a synthetic event, then add more

 * fields.

 Create a buffer to hold the generated command */

 Before generating the command, initialize the cmd object */

	/*

	 * Create the empty gen_synth_test synthetic event with the

	 * first 4 fields.

 Use synth_event_add_field to add the rest of the fields */

	/*

	 * Now get the gen_synth_test event file.  We need to prevent

	 * the instance and event from disappearing from underneath

	 * us, which trace_get_event_file() does (though in this case

	 * we're using the top-level instance which never goes away).

 Enable the event or you won't see anything */

 Create some bogus values just for testing */

 next_pid_field */

 next_comm_field */

 ts_ns */

 ts_ms */

 cpu */

 my_string_field */

 my_int_field */

 Now generate a gen_synth_test event */

 We got an error after creating the event, delete it */

/*

 * Test to make sure we can create an initially empty synthetic event,

 * then add all the fields.

 Create a buffer to hold the generated command */

 Before generating the command, initialize the cmd object */

	/*

	 * Create the empty_synth_test synthetic event with no fields.

 Use synth_event_add_field to add all of the fields */

 All fields have been added, close and register the synth event */

	/*

	 * Now get the empty_synth_test event file.  We need to

	 * prevent the instance and event from disappearing from

	 * underneath us, which trace_get_event_file() does (though in

	 * this case we're using the top-level instance which never

	 * goes away).

 Enable the event or you won't see anything */

 Create some bogus values just for testing */

 next_pid_field */

 next_comm_field */

 ts_ns */

 ts_ms */

 cpu */

 my_string_field */

 my_int_field */

 Now trace an empty_synth_test event */

 We got an error after creating the event, delete it */

/*

 * Test synthetic event creation all at once from array of field

 * descriptors.

 Create the create_synth_test event with the fields above */

	/*

	 * Now get the create_synth_test event file.  We need to

	 * prevent the instance and event from disappearing from

	 * underneath us, which trace_get_event_file() does (though in

	 * this case we're using the top-level instance which never

	 * goes away).

 Enable the event or you won't see anything */

 Create some bogus values just for testing */

 next_pid_field */

 next_comm_field */

 ts_ns */

 dynstring_field_1 */

 ts_ms */

 cpu */

 my_string_field */

 dynstring_field_2 */

 my_int_field */

 Now generate a create_synth_test event */

 We got an error after creating the event, delete it */

/*

 * Test tracing a synthetic event by reserving trace buffer space,

 * then filling in fields one after another.

 Start by reserving space in the trace buffer */

 Write some bogus values into the trace buffer, one after another */

 next_pid_field */

 next_comm_field */

 ts_ns */

 ts_ms */

 cpu */

 my_string_field */

 my_int_field */

 Finally, commit the event */

/*

 * Test tracing a synthetic event by reserving trace buffer space,

 * then filling in fields using field names, which can be done in any

 * order.

 Start by reserving space in the trace buffer */

 Write some bogus values into the trace buffer, using field names */

 Finally, commit the event */

/*

 * Test tracing a synthetic event all at once from array of values.

 Trace some bogus values just for testing */

 number of values */

 next_pid_field */

 next_comm_field */

 ts_ns */

 dynstring_field_1 */

 ts_ms */

 cpu */

 my_string_field */

 dynstring_field_2 */

 my_int_field */

 Disable the event or you can't remove it */

 Now give the file and instance back */

 Now unregister and free the synthetic event */

 Disable the event or you can't remove it */

 Now give the file and instance back */

 Now unregister and free the synthetic event */

 Disable the event or you can't remove it */

 Now give the file and instance back */

 Now unregister and free the synthetic event */

 SPDX-License-Identifier: GPL-2.0

/*

 * Error reporting trace points.

 *

 * Copyright (C) 2021, Google LLC.

 SPDX-License-Identifier: GPL-2.0

/*

 * This gets called in a loop recording the time it took to write

 * the tracepoint. What it writes is the time statistics of the last

 * tracepoint write. As there is nothing to write the first time

 * it simply writes "START". As the first write is cold cache and

 * the rest is hot, we save off that time in bm_first and it is

 * reported as "first", which is shown in the second write to the

 * tracepoint. The "first" field is written within the statics from

 * then on but never changes.

 Only run if the tracepoint is actually active */

	/*

	 * The first read is cold cached, keep it separate from the

	 * other calculations.

	/*

	 * When bm_cnt is greater than UINT_MAX, it breaks the statistics

	 * accounting. Freeze the statistics when that happens.

	 * We should have enough data for the avg and stddev anyway.

		/*

		 * Apply Welford's method to calculate standard deviation:

		 * s^2 = 1 / (n * (n-1)) * (n * \Sum (x_i)^2 - (\Sum x_i)^2)

		/*

		 * stddev is the square of standard deviation but

		 * we want the actually number. Use the average

		 * as our seed to find the std.

		 *

		 * The next try is:

		 *  x = (x + N/x) / 2

		 *

		 * Where N is the squared number to find the square

		 * root of.

 sleep a bit to make sure the tracepoint gets activated */

		/*

		 * We don't go to sleep, but let others run as well.

		 * This is basically a "yield()" to let any task that

		 * wants to run, schedule in, but if the CPU is idle,

		 * we'll keep burning cycles.

		 *

		 * Note the tasks_rcu_qs() version of cond_resched() will

		 * notify synchronize_rcu_tasks() that this thread has

		 * passed a quiescent state for rcu_tasks. Otherwise

		 * this thread will never voluntarily schedule which would

		 * block synchronize_rcu_tasks() indefinitely.

/*

 * When the benchmark tracepoint is enabled, it calls this

 * function and the thread that calls the tracepoint is created.

/*

 * When the benchmark tracepoint is disabled, it calls this

 * function and the thread that calls the tracepoint is deleted

 * and all the numbers are reset.

 These don't need to be reset but reset them anyway */

 SPDX-License-Identifier: GPL-2.0

/*

 * event probes

 *

 * Part of this code was copied from kernel/trace/trace_kprobe.c written by

 * Masami Hiramatsu <mhiramat@kernel.org>

 *

 * Copyright (C) 2021, VMware Inc, Steven Rostedt <rostedt@goodmis.org>

 * Copyright (C) 2021, VMware Inc, Tzvetomir Stoyanov tz.stoyanov@gmail.com>

 *

 tracepoint system */

 tracepoint event */

 If other probes are on the event, just unregister eprobe */

 Enabled event can not be unregistered */

 Will fail if probe is being used by ftrace or perf */

	/*

	 * We match the following:

	 *  event only			- match all eprobes with event name

	 *  system and event only	- match all system/event probes

	 *

	 * The below has the above satisfied with more arguments:

	 *

	 *  attached system/event	- If the arg has the system and event

	 *				  the probe is attached to, match

	 *				  probes with the attachment.

	 *

	 *  If any more args are given, then it requires a full match.

	/*

	 * If system exists, but this probe is not part of that system

	 * do not match.

 Must match the event name */

 No arguments match all */

 First argument is the system/event the probe is attached to */

 If there are no other args, then match */

 Event entry printers */

 Kprobe specific fetch functions */

 Note that we don't verify it, since the code does not come from user space */

 Return the length of string -- including null terminal byte */

 Return the length of string -- including null terminal byte */

/*

 * Fetch a null-terminated string from user. Caller MUST set *(u32 *)buf

 * with max length and relative data location.

/*

 * Fetch a null-terminated string. Caller MUST set *(u32 *)buf with max

 * length and relative data location.

	/*

	 * Try to get string again, since the string can be changed while

	 * probing.

 eprobe handler */

/*

 * The event probe implementation uses event triggers to get access to

 * the event it is attached to, but is not an actual trigger. The below

 * functions are just stubs to fulfill what is needed to use the trigger

 * infrastructure.

 Do not print eprobe event triggers */

	/*

	 * EVENT PROBE triggers are not registered as commands with

	 * register_event_command(), as they are not controlled by the user

	 * from the trigger file

 Make sure nothing is using the edata or trigger */

 This also changes "enabled" state */

 Failed to enable one of them. Roll back all */

		/*

		 * Synchronization is done in below function. For perf event,

		 * file == NULL and perf_trace_event_unreg() calls

		 * tracepoint_synchronize_unregister() to ensure synchronize

		 * event. We don't need to care about it.

 Skip other probes and ftrace events */

	/*

	 * Argument syntax:

	 *      e[:[GRP/]ENAME] SYSTEM.EVENT [FETCHARGS]

	 * Fetch args:

	 *  <name>=$<field>[:TYPE]

 This must return -ENOMEM or missing event, else there is a bug */

 parse arguments */

/*

 * Register dynevent at core_initcall. This allows kernel to setup eprobe

 * events in postcore_initcall without tracefs.

 SPDX-License-Identifier: GPL-2.0

/*

 * trace context switch

 *

 * Copyright (C) 2007 Steven Rostedt <srostedt@redhat.com>

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * Power trace points

 *

 * Copyright (C) 2009 Arjan van de Ven <arjan@linux.intel.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * trace_events_hist - trace event hist triggers

 *

 * Copyright (C) 2015 Tom Zanussi <tom.zanussi@linux.intel.com>

 for gfp flag names */

 For optimizing division by constants */

/*

 * A hist_var (histogram variable) contains variable information for

 * hist_fields having the HIST_FIELD_FL_VAR or HIST_FIELD_FL_VAR_REF

 * flag set.  A hist_var has a variable name e.g. ts0, and is

 * associated with a given histogram trigger, as specified by

 * hist_data.  The hist_var idx is the unique index assigned to the

 * variable by the hist trigger's tracing_map.  The idx is what is

 * used to set a variable's value and, by a variable reference, to

 * retrieve it.

	/*

	 * Variable fields contain variable-specific info in var.

	/*

	 * The name field is used for EXPR and VAR_REF fields.  VAR

	 * fields contain the variable name in var.name.

	/*

	 * When a histogram trigger is hit, if it has any references

	 * to variables, the values of those variables are collected

	 * into a var_ref_vals array by resolve_var_refs().  The

	 * current value of each variable is read from the tracing_map

	 * using the hist field's hist_var.idx and entered into the

	 * var_ref_idx entry i.e. var_ref_vals[var_ref_idx].

 Numeric literals are represented as u64 */

 Used to optimize division by constants */

 Return -1 for the undefined case */

 Use shift if the divisor is a power of 2 */

	/*

	 * If the divisor is a constant, do a multiplication and shift instead.

	 *

	 * Choose Z = some power of 2. If Y <= Z, then:

	 *     X / Y = (X * (Z / Y)) / Z

	 *

	 * (Z / Y) is a constant (mult) which is calculated at parse time, so:

	 *     X / Y = (X * mult) / Z

	 *

	 * The division by Z can be replaced by a shift since Z is a power of 2:

	 *     X / Y = (X * mult) >> HIST_DIV_SHIFT

	 *

	 * As long, as X < Z the results will not be off by more than 1.

	/*

	 * When a histogram trigger is hit, the values of any

	 * references to variables, including variables being passed

	 * as parameters to synthetic events, are collected into a

	 * var_ref_vals array.  This var_ref_idx array is an array of

	 * indices into the var_ref_vals array, one for each synthetic

	 * event param, and is passed to the synthetic event

	 * invocation.

			/*

			 * var_str contains the $-unstripped variable

			 * name referenced by var_ref, and used when

			 * printing the action.  Because var_ref

			 * creation is deferred to create_actions(),

			 * we need a per-action way to save it until

			 * then, thus var_str.

			/*

			 * var_ref refers to the variable being

			 * tracked e.g onmax($var).

			/*

			 * track_var contains the 'invisible' tracking

			 * variable created to keep the current

			 * e.g. max value.

/*

 * Returns the specific division function to use if the divisor

 * is constant. This avoids extra branches when the trigger is hit.

 If the divisor is too large, do a regular division */

/**

 * check_field_for_var_ref - Check if a VAR_REF field references a variable

 * @hist_field: The VAR_REF field to check

 * @var_data: The hist trigger that owns the variable

 * @var_idx: The trigger variable identifier

 *

 * Check the given VAR_REF field to see whether or not it references

 * the given variable associated with the given trigger.

 *

 * Return: The VAR_REF field if it does reference the variable, NULL if not

/**

 * find_var_ref - Check if a trigger has a reference to a trigger variable

 * @hist_data: The hist trigger that might have a reference to the variable

 * @var_data: The hist trigger that owns the variable

 * @var_idx: The trigger variable identifier

 *

 * Check the list of var_refs[] on the first hist trigger to see

 * whether any of them are references to the variable on the second

 * trigger.

 *

 * Return: The VAR_REF field referencing the variable if so, NULL if not

/**

 * find_any_var_ref - Check if there is a reference to a given trigger variable

 * @hist_data: The hist trigger

 * @var_idx: The trigger variable identifier

 *

 * Check to see whether the given variable is currently referenced by

 * any other trigger.

 *

 * The trigger the variable is defined on is explicitly excluded - the

 * assumption being that a self-reference doesn't prevent a trigger

 * from being removed.

 *

 * Return: The VAR_REF field referencing the variable if so, NULL if not

/**

 * check_var_refs - Check if there is a reference to any of trigger's variables

 * @hist_data: The hist trigger

 *

 * A trigger can define one or more variables.  If any one of them is

 * currently referenced by any other trigger, this function will

 * determine that.



 * Typically used to determine whether or not a trigger can be removed

 * - if there are any references to a trigger's variables, it cannot.

 *

 * Return: True if there is a reference to any of trigger's variables

/*

 * If field_op != FIELD_OP_NONE, *sep points to the root operator

 * of the expression tree to be evaluated.

	/*

	 * Report the last occurrence of the operators first, so that the

	 * expression is evaluated left to right. This is important since

	 * subtraction and division are not associative.

	 *

	 *	e.g

	 *		64/8/4/2 is 1, i.e 64/8/4/2 = ((64/8)/4)/2

	 *		14-7-5-2 is 0, i.e 14-7-5-2 = ((14-7)-5)-2

	/*

	 * First, find lower precedence addition and subtraction

	 * since the expression will be evaluated recursively.

		/*

		 * Unary minus is not supported in sub-expressions. If

		 * present, it is always the next root operator.

		/*

		 * For operators of the same precedence use to rightmost as the

		 * root, so that the expression is evaluated left to right.

	/*

	 * Multiplication and division have higher precedence than addition and

	 * subtraction.

	/*

	 * For operators of the same precedence use to rightmost as the

	 * root, so that the expression is evaluated left to right.

 Can likely be a const */

 var refs will be destroyed separately */

 caller will populate */

 Pointers to strings are just pointers and dangerous to dereference */

/**

 * create_var_ref - Create a variable reference and attach it to trigger

 * @hist_data: The trigger that will be referencing the variable

 * @var_field: The VAR field to create a reference to

 * @system: The optional system string

 * @event_name: The optional event_name string

 *

 * Given a variable hist_field, create a VAR_REF hist_field that

 * represents a reference to it.

 *

 * This function also adds the reference to the trigger that

 * now references the variable.

 *

 * Return: The VAR_REF field if successful, NULL if not

 Check if the variable already exists */

		/*

		 * 'sym-offset' occurrences in the trigger string are modified

		 * to 'symXoffset' to simplify arithmetic expression parsing.

			/*

			 * For backward compatibility, if field_name

			 * was "cpu", then we treat this the same as

			 * common_cpu.

 Unary minus operator, increment n_subexprs */

 we support only -(xxx) i.e. explicit parens required */

 skip leading '-' */

 unary minus not supported in sub-expressions */

 no closing ')' */

 String type can not be the operand of unary operator. */

/*

 * If the operands are var refs, return pointers the

 * variable(s) referenced in var1 and var2, else NULL.

 Binary operator found, increment n_subexprs */

 Split the expression string at the root operator */

 Binary operator requires both operands */

 LHS of string is an expression e.g. a+b in a+b+c */

 RHS of string is another expression e.g. c in a+b+c */

	/*

	 * If both operands are constant, the expression can be

	 * collapsed to a single constant.

 The operands are now owned and free'd by 'expr' */

		/*

		 * Copy the divisor here so we don't have to look it up

		 * later if this is a var ref

		/*

		 * var refs won't be destroyed immediately

		 * See: destroy_hist_field()

 The operand sizes should be the same, so just pick one */

/**

 * create_field_var_hist - Automatically create a histogram and var for a field

 * @target_hist_data: The target hist trigger

 * @subsys_name: Optional subsystem name

 * @event_name: Optional event name

 * @field_name: The name of the field (and the resulting variable)

 *

 * Hist trigger actions fetch data from variables, not directly from

 * events.  However, for convenience, users are allowed to directly

 * specify an event field in an action, which will be automatically

 * converted into a variable on their behalf.

 *

 * If a user specifies a field on an event that isn't the event the

 * histogram currently being defined (the target event histogram), the

 * only way that can be accomplished is if a new hist trigger is

 * created and the field variable defined on that.

 *

 * This function creates a new histogram compatible with the target

 * event (meaning a histogram with the same key as the target

 * histogram), and creates a variable for the specified field, but

 * with 'synthetic_' prepended to the variable name in order to avoid

 * collision with normal field variables.

 *

 * Return: The variable created for the field.

	/*

	 * Look for a histogram compatible with target.  We'll use the

	 * found histogram specification to create a new matching

	 * histogram with our variable on it.  target_hist_data is not

	 * yet a registered histogram so we can't use that.

 See if a synthetic field variable has already been created */

 Use the same keys as the compatible histogram */

 Create the synthetic field variable specification */

 Use the same filter as the compatible histogram */

 Save the compatible histogram information */

 Create the new histogram with our variable */

 If we can't find the variable, something went wrong */

/**

 * create_target_field_var - Automatically create a variable for a field

 * @target_hist_data: The target hist trigger

 * @subsys_name: Optional subsystem name

 * @event_name: Optional event name

 * @var_name: The name of the field (and the resulting variable)

 *

 * Hist trigger actions fetch data from variables, not directly from

 * events.  However, for convenience, users are allowed to directly

 * specify an event field in an action, which will be automatically

 * converted into a variable on their behalf.



 * This function creates a field variable with the name var_name on

 * the hist trigger currently being defined on the target event.  If

 * subsys_name and event_name are specified, this function simply

 * verifies that they do in fact match the target event subsystem and

 * event name.

 *

 * Return: The variable created for the field.

 called with tr->max_lock held */

 CONFIG_TRACER_SNAPSHOT */

	/*

	 * A dynamic string synth field can accept static or

	 * dynamic. A static string synth field can only accept a

	 * same-sized static string, which is checked for later.

 skip '$' */

	/*

	 * First try to create a field var on the target event (the

	 * currently being defined).  This will create a variable for

	 * unqualified fields on the target event, or if qualified,

	 * target fields that have qualified names matching the target.

		/*

		 * If no explicit system.event is specified, default to

		 * looking for fields on the onmatch(system.event.xxx)

		 * event.

		/*

		 * At this point, we're looking at a field on another

		 * event.  Because we can't modify a hist trigger on

		 * another event to add a variable for a field, we need

		 * to create a new trigger on that event and create the

		 * variable at the same time.

 Convert a var that points to common_pid.execname to a string */

 we always have at least one, hitcount */

 ensure NULL-termination */

 skip VAR vals */

	/*

	 * separate the trigger from the filter (k:v [if filter])

	 * allowing for whitespace in the trigger

	/*

	 * To simplify arithmetic expression parsing, replace occurrences of

	 * '.sym-offset' modifier with '.symXoffset'

 if param is non-empty, it's supposed to be a filter */

	/*

	 * The above returns on success the # of triggers registered,

	 * but if it didn't register any it returns zero.  Consider no

	 * triggers registered a failure too.

 Just return zero, not the number of registered triggers */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2006 Jens Axboe <axboe@kernel.dk>

 *

 Select an alternative, minimalistic output than the original one */

 Default disable the minimalistic output */

 Global reference count of probes */

/*

 * Send out a notify message.

/*

 * Send out a notify for this process, if we haven't done so since a trace

 * started

 need to check user space to see if this breaks in y2038 or y2106 */

	/*

	 * If the BLK_TC_NOTIFY action mask isn't set, don't send any note

	 * message to the trace.

/*

 * Data direction bit lookup

 The ilog2() calls fall out because they're constant */

/*

 * The worker for the various blk_add_trace*() types. Fills out a

 * blk_io_trace structure and places it in a per-cpu subbuffer.

	/*

	 * A word about the locking here - we disable interrupts to reserve

	 * some space in the relay per-cpu buffer, to prevent an irq

	 * from coming in and stepping on our toes.

		/*

		 * These two are not needed in ftrace as they are in the

		 * generic trace_entry, filled by tracing_generic_entry_update,

		 * but for the trace_event->bin() synthesizer benefit we do it

		 * here too.

/*

 * Keep track of how many times we encountered a full subbuffer, to aid

 * the user space app in telling how many lost events there were.

/*

 * Setup everything required to start tracing

	/*

	 * some device names have larger paths - convert the slashes

	 * to underscores for this to work as expected

	/*

	 * bdev can be NULL, as with scsi-generic, this is a helpful as

	 * we can be.

	/*

	 * When tracing the whole disk reuse the existing debugfs directory

	 * created by the block layer on init. For partitions block devices,

	 * and scsi-generic block devices we create a temporary new debugfs

	 * directory that will be removed once the trace ends.

	/*

	 * As blktrace relies on debugfs for its interface the debugfs directory

	 * is required, contrary to the usual mantra of not checking for debugfs

	 * files or directories.

 overwrite with user settings */

	/*

	 * For starting a trace, we can transition from a setup or stopped

	 * trace. For stopping a trace, the state must be running

/*

 * When reading or writing the blktrace sysfs files, the references to the

 * opened sysfs or device files should prevent the underlying block device

 * from being removed. So no further delete protection is really needed.

/**

 * blk_trace_ioctl: - handle the ioctls associated with tracing

 * @bdev:	the block device

 * @cmd:	the ioctl cmd

 * @arg:	the argument data, if any

 *

/**

 * blk_trace_shutdown: - stop and cleanup trace structures

 * @q:    the request queue associated with the device

 *

 We don't use the 'bt' value here except as an optimization... */

 Use the first bio */

/*

 * blktrace probes

/**

 * blk_add_trace_rq - Add a trace for a request oriented action

 * @rq:		the source request

 * @error:	return status to log

 * @nr_bytes:	number of completed bytes

 * @what:	the action

 * @cgid:	the cgroup info

 *

 * Description:

 *     Records an action against a request. Will log the bio offset + size.

 *

/**

 * blk_add_trace_bio - Add a trace for a bio oriented action

 * @q:		queue the io is for

 * @bio:	the source bio

 * @what:	the action

 * @error:	error, if any

 *

 * Description:

 *     Records an action against a bio. Will log the bio offset + size.

 *

/**

 * blk_add_trace_bio_remap - Add a trace for a bio-remap operation

 * @ignore:	trace callback data parameter (not used)

 * @bio:	the source bio

 * @dev:	source device

 * @from:	source sector

 *

 * Called after a bio is remapped to a different device and/or sector.

/**

 * blk_add_trace_rq_remap - Add a trace for a request-remap operation

 * @ignore:	trace callback data parameter (not used)

 * @rq:		the source request

 * @dev:	target device

 * @from:	source sector

 *

 * Description:

 *     Device mapper remaps request to other devices.

 *     Add a trace for that action.

 *

/**

 * blk_add_driver_data - Add binary message with driver-specific data

 * @rq:		io request

 * @data:	driver-specific data

 * @len:	length of driver-specific data

 *

 * Description:

 *     Some drivers might want to write driver-specific data per request.

 *

/*

 * struct blk_io_tracer formatting routines

			/*

			 * The cgid portion used to be "INO,GEN".  Userland

			 * builds a FILEID_INO32_GEN fid out of them and

			 * opens the cgroup using open_by_handle_at(2).

			 * While 32bit ino setups are still the same, 64bit

			 * ones now use the 64bit ino as the whole ID and

			 * no longer use generation.

			 *

			 * Regardless of the content, always output

			 * "LOW32,HIGH32" so that FILEID_INO32_GEN fid can

			 * be mapped back to @id on both 64 and 32bit ino

			 * setups.  See __kernfs_fh_to_dentry().

 find the last zero that needs to be printed */

		/*

		 * stop when the rest is just zeros and indicate so

		 * with a ".." appended

/*

 * struct tracer operations

 don't output context-info for blk_classic output */

/*

 * Setup everything required to start tracing

/*

 * sysfs interface to enable and configure tracing

 Assume it is a list of trace category names */

 CONFIG_BLK_DEV_IO_TRACE */

/**

 * blk_fill_rwbs - Fill the buffer rwbs by mapping op to character string.

 * @rwbs:	buffer to be filled

 * @op:		REQ_OP_XXX for the tracepoint

 *

 * Description:

 *     Maps the REQ_OP_XXX to character and fills the buffer provided by the

 *     caller with resulting string.

 *

 CONFIG_EVENT_TRACING */

 SPDX-License-Identifier: GPL-2.0

/*

 * Kprobes-based tracing events

 *

 * Created by Masami Hiramatsu <mhiramat@redhat.com>

 *

 for COMMAND_LINE_SIZE */

 Kprobe early definition from command line */

/*

 * Kprobe event core functions

 Use rp.kp for kprobe use */

 symbol name */

/**

 * for_each_trace_kprobe - iterate over the trace_kprobe list

 * @pos:	the struct trace_kprobe * for each entry

 * @dpos:	the struct dyn_event * to use as a loop cursor

 Return 0 if it fails to find the symbol address */

/*

 * Allocate new trace_probe and initialize it (including kprobes).

/*

 * Enable trace_probe

 * if the file is NULL, enable "perf" handler, or enable "trace" handler.

 This also changes "enabled" state */

 Failed to enable one of them. Roll back all */

/*

 * Disable trace_probe

 * if the file is NULL, disable "perf" handler, or disable "trace" handler.

		/*

		 * Synchronization is done in below function. For perf event,

		 * file == NULL and perf_trace_event_unreg() calls

		 * tracepoint_synchronize_unregister() to ensure synchronize

		 * event. We don't need to care about it.

 Get the entry address of the target function */

	/*

	 * Since ftrace_location_range() does inclusive range check, we need

	 * to subtract 1 byte from the end address.

 Check if the address is on a suffixed-symbol */

 Internal register function - just handle k*probes and flags */

 Set/clear disabled flag according to tp->flag */

 Internal unregister function - just handle k*probes and flags */

 Cleanup kprobe for reuse and mark it unregistered */

 Unregister a trace_probe and probe_event */

 If other probes are on the event, just unregister kprobe */

 Enabled event can not be unregistered */

 If there's a reference to the dynamic event */

 Will fail if probe is being used by ftrace or perf */

		/*

		 * trace_probe_compare_arg_type() ensured that nr_args and

		 * each argument name and type are same. Let's compare comm.

 Note that argument starts index = 2 */

 Append to existing event */

 Register k*probe */

 Register a trace_probe and probe_event */

 Register new event */

 Register k*probe */

 Module notifier call back, checking event on the module */

 Update probes on coming module */

 Don't need to check busy - this should have gone. */

 Invoked after kprobe module callback */

	/*

	 * Argument syntax:

	 *  - Add kprobe:

	 *      p[:[GRP/]EVENT] [MOD:]KSYM[+OFFS]|KADDR [FETCHARGS]

	 *  - Add kretprobe:

	 *      r[MAXACTIVE][:[GRP/]EVENT] [MOD:]KSYM[+0] [FETCHARGS]

	 *    Or

	 *      p:[GRP/]EVENT] [MOD:]KSYM[+0]%return [FETCHARGS]

	 *

	 * Fetch args:

	 *  $retval	: fetch return value

	 *  $stack	: fetch stack address

	 *  $stackN	: fetch Nth of stack (N:0-)

	 *  $comm       : fetch current task comm

	 *  @ADDR	: fetch memory at ADDR (ADDR should be in kernel)

	 *  @SYM[+|-offs] : fetch memory at SYM +|- offs (SYM is a data symbol)

	 *  %REG	: fetch register REG

	 * Dereferencing memory fetch:

	 *  +|-offs(ARG) : fetch memory at ARG +|- offs address.

	 * Alias name of args:

	 *  NAME=FETCHARG : set NAME as alias of FETCHARG.

	 * Type of args:

	 *  FETCHARG:TYPE : use TYPE instead of unsigned long.

		/* kretprobes instances are iterated over via a list. The

		 * maximum should stay reasonable.

	/* try to parse an address. if that fails, try to read the

 Check whether uprobe event specified */

 a symbol specified */

 TODO: support .init module functions */

 Defer the ENOENT case until register kprobe */

 Make a new event name */

 setup a probe */

 This must return -ENOMEM, else there is a bug */

 We know tk is not allocated */

 parse arguments */

 This can be -ENOMEM */

/**

 * kprobe_event_cmd_init - Initialize a kprobe event command object

 * @cmd: A pointer to the dynevent_cmd struct representing the new event

 * @buf: A pointer to the buffer used to build the command

 * @maxlen: The length of the buffer passed in @buf

 *

 * Initialize a synthetic event command object.  Use this before

 * calling any of the other kprobe_event functions.

/**

 * __kprobe_event_gen_cmd_start - Generate a kprobe event command from arg list

 * @cmd: A pointer to the dynevent_cmd struct representing the new event

 * @name: The name of the kprobe event

 * @loc: The location of the kprobe event

 * @kretprobe: Is this a return probe?

 * @args: Variable number of arg (pairs), one pair for each field

 *

 * NOTE: Users normally won't want to call this function directly, but

 * rather use the kprobe_event_gen_cmd_start() wrapper, which automatically

 * adds a NULL to the end of the arg list.  If this function is used

 * directly, make sure the last arg in the variable arg list is NULL.

 *

 * Generate a kprobe event command to be executed by

 * kprobe_event_gen_cmd_end().  This function can be used to generate the

 * complete command or only the first part of it; in the latter case,

 * kprobe_event_add_fields() can be used to add more fields following this.

 *

 * Unlikely the synth_event_gen_cmd_start(), @loc must be specified. This

 * returns -EINVAL if @loc == NULL.

 *

 * Return: 0 if successful, error otherwise.

/**

 * __kprobe_event_add_fields - Add probe fields to a kprobe command from arg list

 * @cmd: A pointer to the dynevent_cmd struct representing the new event

 * @args: Variable number of arg (pairs), one pair for each field

 *

 * NOTE: Users normally won't want to call this function directly, but

 * rather use the kprobe_event_add_fields() wrapper, which

 * automatically adds a NULL to the end of the arg list.  If this

 * function is used directly, make sure the last arg in the variable

 * arg list is NULL.

 *

 * Add probe fields to an existing kprobe command using a variable

 * list of args.  Fields are added in the same order they're listed.

 *

 * Return: 0 if successful, error otherwise.

/**

 * kprobe_event_delete - Delete a kprobe event

 * @name: The name of the kprobe event to delete

 *

 * Delete a kprobe event with the give @name from kernel code rather

 * than directly from the command line.

 *

 * Return: 0 if successful, error otherwise.

 Probes profiling interfaces */

 Kprobe specific fetch functions */

 Return the length of string -- including null terminal byte */

 Return the length of string -- including null terminal byte */

/*

 * Fetch a null-terminated string from user. Caller MUST set *(u32 *)buf

 * with max length and relative data location.

/*

 * Fetch a null-terminated string. Caller MUST set *(u32 *)buf with max

 * length and relative data location.

	/*

	 * Try to get string again, since the string can be changed while

	 * probing.

 Note that we don't verify it, since the code does not come from user space */

 1st stage: get value from context */

 Ignore a place holder */

 Kprobe handler */

 Kretprobe handler */

 Event entry printers */

 Kprobe profile handler */

		/*

		 * We need to check and see if we modified the pc of the

		 * pt_regs, and if so return 1 so that we don't do the

		 * single stepping.

 Kretprobe profile handler */

 CONFIG_PERF_EVENTS */

/*

 * called by perf_trace_init() or __ftrace_set_clr_event() under event_mutex.

 *

 * kprobe_trace_self_tests_init() does enable_trace_probe/disable_trace_probe

 * lockless, but we can't race with this __init function.

 We don't tweak kernel, so just return 0 */

 create a trace_kprobe, but don't add it to global lists */

	/*

	 * local trace_kprobes are not added to dyn_event, so they are never

	 * searched in find_trace_kprobe(). Therefore, there is no concern of

	 * duplicated name here.

 maxactive */, 0 
 CONFIG_PERF_EVENTS */

/*

 * Register dynevent at core_initcall. This allows kernel to setup kprobe

 * events in postcore_initcall without tracefs.

 Make a tracefs interface for controlling probe points */

 Event list interface */

 Profile interface */

/*

 * Nobody but us can call enable_trace_kprobe/disable_trace_kprobe at this

 * stage, we can do this lockless.

 Enable trace point */

 Enable trace point */

	/*

	 * Not expecting an error here, the check is only to prevent the

	 * optimizer from removing the call to target() as otherwise there

	 * are no side-effects and the call is never performed.

 Disable trace points before removing it */

	/*

	 * Wait for the optimizer work to finish. Otherwise it might fiddle

	 * with probes in already freed __init text.

 SPDX-License-Identifier: GPL-2.0

/*

 * Infrastructure for statistic tracing (histogram output).

 *

 * Copyright (C) 2008-2009 Frederic Weisbecker <fweisbec@gmail.com>

 *

 * Based on the code from trace_branch.c which is

 * Copyright (C) 2008 Steven Rostedt <srostedt@redhat.com>

 *

/*

 * List of stat red-black nodes from a tracer

 * We use a such tree to sort quickly the stat

 * entries from the tracer.

 A stat session is the stats output in one file */

 All of the sessions currently in use. Each stat file embed one session */

 The root directory for all stat files */

	/*

	 * Figure out where to put new node

	 * This is a descendent sorting

/*

 * For tracers that don't provide a stat_cmp callback.

 * This one will force an insertion as right-most node

 * in the rbtree.

/*

 * Initialize the stat rbtree at each trace_stat file opening.

 * All of these copies and sorting are required on all opening

 * since the stats could have changed between two file sessions.

	/*

	 * Iterate over the tracer stat entries and store them in an rbtree.

 End of insertion */

 Prevent from tracer switch or rbtree modification */

 If we are in the beginning of the file, print the headers */

 The session stat is refilled and resorted at each stat file opening */

/*

 * Avoid consuming memory with our now useless rbtree.

 Already registered? */

 Init the session */

 Register */

 SPDX-License-Identifier: GPL-2.0

/*

 * ring buffer based function tracer

 *

 * Copyright (C) 2007-2008 Steven Rostedt <srostedt@redhat.com>

 * Copyright (C) 2008 Ingo Molnar <mingo@redhat.com>

 *

 * Based on code from the latency_tracer, that is:

 *

 *  Copyright (C) 2004-2006 Ingo Molnar

 *  Copyright (C) 2004 Nadia Yvette Chambers

 Our option */

 No flags set. */

 Update this to next highest bit. */

 The top level array uses the "global_ops" */

 Currently only the non stack version is supported */

	/*

	 * The top level array uses the "global_ops", and the files are

	 * created on boot up.

	/*

	 * Instance trace_arrays get their ops allocated

	 * at instance creation. Unless it failed

	 * the allocation.

/*

 * Skip 2:

 *

 *   function_stack_trace_call()

 *   ftrace_call()

/*

 * Skip 3:

 *   __trace_stack()

 *   function_stack_trace_call()

 *   ftrace_call()

	/*

	 * Need to use raw, since this must be called before the

	 * recursive protection is performed.

	/*

	 * An interrupt may happen at any place here. But as far as I can see,

	 * the only damage that this can cause is to mess up the repetition

	 * counter without valuable data being lost.

	 * TODO: think about a solution that is better than just hoping to be

	 * lucky.

	/*

	 * Need to use raw, since this must be called before the

	 * recursive protection is performed.

 Always set a last empty entry */

 By default: all flags disabled */

 Do nothing if already set. */

 We can change this flag only when not running. */

 Check if there's anything to change. */

	/*

	 * Tracing gets disabled (or enabled) once per count.

	 * This function can be called at the same time on multiple CPUs.

	 * It is fine if both disable (or enable) tracing, as disabling

	 * (or enabling) the second time doesn't do anything as the

	 * state of the tracer is already disabled (or enabled).

	 * What needs to be synchronized in this case is that the count

	 * only gets decremented once, even if the tracer is disabled

	 * (or enabled) twice, as the second one is really a nop.

	 *

	 * The memory barriers guarantee that we only decrement the

	 * counter once. First the count is read to a local variable

	 * and a read barrier is used to make sure that it is loaded

	 * before checking if the tracer is in the state we want.

	 * If the tracer is not in the state we want, then the count

	 * is guaranteed to be the old count.

	 *

	 * Next the tracer is set to the state we want (disabled or enabled)

	 * then a write memory barrier is used to make sure that

	 * the new state is visible before changing the counter by

	 * one minus the old counter. This guarantees that another CPU

	 * executing this code will see the new state before seeing

	 * the new counter value, and would not do anything if the new

	 * counter is seen.

	 *

	 * Note, there is no synchronization between this and a user

	 * setting the tracing_on file. But we currently don't care

	 * about that.

 Make sure we see count before checking tracing state */

 Make sure tracing state is visible before updating count */

/*

 * Skip 3:

 *

 *   function_trace_probe_call()

 *   ftrace_ops_assist_func()

 *   ftrace_call()

/*

 * Skip 5:

 *

 *   __trace_stack()

 *   ftrace_stacktrace()

 *   function_trace_probe_call()

 *   ftrace_ops_assist_func()

 *   ftrace_call()

 unlimited? */

	/*

	 * Stack traces should only execute the number of times the

	 * user specified in the counter.

 Only dump the current CPU buffer. */

 hash funcs only work with set_ftrace_filter */

	/*

	 * We use the callback data field (which is a pointer)

	 * as our counter.

 we register both traceon and traceoff to this callback */

 Only dump once. */

 Only dump once. */

 CONFIG_DYNAMIC_FTRACE */

 SPDX-License-Identifier: GPL-2.0

 for MODULE_NAME_LEN via KSYM_SYMBOL_LEN */

	/*

	 * Only compare after the "sys" prefix. Archs that use

	 * syscall wrappers may have syscalls symbols aliases prefixed

	 * with ".SyS" or ".sys" instead of "sys", leading to an unwanted

	 * mismatch.

/*

 * Some architectures that allow for 32bit applications

 * to run on a 64bit kernel, do not map the syscalls for

 * the 32bit tasks the same as they do for 64bit tasks.

 *

 *     *cough*x86*cough*

 *

 * In such a case, instead of reporting the wrong syscalls,

 * simply ignore them.

 *

 * For an arch to ignore the compat syscalls it needs to

 * define ARCH_TRACE_IGNORE_COMPAT_SYSCALLS as well as

 * define the function arch_trace_is_compat_syscall() to let

 * the tracing system know that it should ignore it.

 ARCH_TRACE_IGNORE_COMPAT_SYSCALLS */

 parameter types */

 parameter values */

 When len=0, we just calculate the needed length */

 return the length of print_fmt */

 First: called with 0 length to calculate the needed length */

 Second: actually write the @print_fmt */

 Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE) */

 Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE()) */

 get the size after alignment with the u32 buffer size field */

 We can probably do that at build time */

 CONFIG_PERF_EVENTS */

 SPDX-License-Identifier: GPL-2.0

/*

 * trace_events_trigger - trace event triggers

 *

 * Copyright (C) 2013 Tom Zanussi <tom.zanussi@linux.intel.com>

 make sure current triggers exit before free */

/**

 * event_triggers_call - Call triggers associated with a trace event

 * @file: The trace_event_file associated with the event

 * @rec: The trace entry for the event, NULL for unconditional invocation

 *

 * For each trigger associated with an event, invoke the trigger

 * function registered with the associated trigger command.  If rec is

 * non-NULL, it means that the trigger requires further processing and

 * shouldn't be unconditionally invoked.  If rec is non-NULL and the

 * trigger has a filter associated with it, rec will checked against

 * the filter and if the record matches the trigger will be invoked.

 * If the trigger is a 'post_trigger', meaning it shouldn't be invoked

 * in any case until the current event is written, the trigger

 * function isn't invoked but the bit associated with the deferred

 * trigger is set in the return value.

 *

 * Returns an enum event_trigger_type value containing a set bit for

 * any trigger that should be deferred, ETT_NONE if nothing to defer.

 *

 * Called from tracepoint handlers (with rcu_read_lock_sched() held).

 *

 * Return: an enum event_trigger_type value containing a set bit for

 * any trigger that should be deferred, ETT_NONE if nothing to defer.

/**

 * event_triggers_post_call - Call 'post_triggers' for a trace event

 * @file: The trace_event_file associated with the event

 * @tt: enum event_trigger_type containing a set bit for each trigger to invoke

 *

 * For each trigger associated with an event, invoke the trigger

 * function registered with the associated trigger command, if the

 * corresponding bit is set in the tt enum passed into this function.

 * See @event_triggers_call for details on how those bits are set.

 *

 * Called from tracepoint handlers (with rcu_read_lock_sched() held).

 ->stop() is called even if ->start() fails */

 Checks for tracefs lockdown */

/*

 * Currently we only register event commands from __init, so mark this

 * __init too.

/*

 * Currently we only unregister event commands from __init, so mark

 * this __init too.

/**

 * event_trigger_print - Generic event_trigger_ops @print implementation

 * @name: The name of the event trigger

 * @m: The seq_file being printed to

 * @data: Trigger-specific data

 * @filter_str: filter_str to print, if present

 *

 * Common implementation for event triggers to print themselves.

 *

 * Usually wrapped by a function that simply sets the @name of the

 * trigger command and then invokes this.

 *

 * Return: 0 on success, errno otherwise

/**

 * event_trigger_init - Generic event_trigger_ops @init implementation

 * @ops: The trigger ops associated with the trigger

 * @data: Trigger-specific data

 *

 * Common implementation of event trigger initialization.

 *

 * Usually used directly as the @init method in event trigger

 * implementations.

 *

 * Return: 0 on success, errno otherwise

/**

 * event_trigger_free - Generic event_trigger_ops @free implementation

 * @ops: The trigger ops associated with the trigger

 * @data: Trigger-specific data

 *

 * Common implementation of event trigger de-initialization.

 *

 * Usually used directly as the @free method in event trigger

 * implementations.

/**

 * clear_event_triggers - Clear all triggers associated with a trace array

 * @tr: The trace array to clear

 *

 * For each trigger, the triggering event has its tm_ref decremented

 * via trace_event_trigger_enable_disable(), and any associated event

 * (in the case of enable/disable_event triggers) will have its sm_ref

 * decremented via free()->trace_event_enable_disable().  That

 * combination effectively reverses the soft-mode/trigger state added

 * by trigger registration.

 *

 * Must be called with event_mutex held.

/**

 * update_cond_flag - Set or reset the TRIGGER_COND bit

 * @file: The trace_event_file associated with the event

 *

 * If an event has triggers and any of those triggers has a filter or

 * a post_trigger, trigger invocation needs to be deferred until after

 * the current event has logged its data, and the event should have

 * its TRIGGER_COND bit set, otherwise the TRIGGER_COND bit should be

 * cleared.

/**

 * register_trigger - Generic event_command @reg implementation

 * @glob: The raw string used to register the trigger

 * @ops: The trigger ops associated with the trigger

 * @data: Trigger-specific data to associate with the trigger

 * @file: The trace_event_file associated with the event

 *

 * Common implementation for event trigger registration.

 *

 * Usually used directly as the @reg method in event command

 * implementations.

 *

 * Return: 0 on success, errno otherwise

/**

 * unregister_trigger - Generic event_command @unreg implementation

 * @glob: The raw string used to register the trigger

 * @ops: The trigger ops associated with the trigger

 * @test: Trigger-specific data used to find the trigger to remove

 * @file: The trace_event_file associated with the event

 *

 * Common implementation for event trigger unregistration.

 *

 * Usually used directly as the @unreg method in event command

 * implementations.

/**

 * event_trigger_callback - Generic event_command @func implementation

 * @cmd_ops: The command ops, used for trigger registration

 * @file: The trace_event_file associated with the event

 * @glob: The raw string used to register the trigger

 * @cmd: The cmd portion of the string used to register the trigger

 * @param: The params portion of the string used to register the trigger

 *

 * Common implementation for event command parsing and trigger

 * instantiation.

 *

 * Usually used directly as the @func method in event command

 * implementations.

 *

 * Return: 0 on success, errno otherwise

 separate the trigger from the filter (t:n [if filter]) */

		/*

		 * We use the callback data field (which is a pointer)

		 * as our counter.

 if param is non-empty, it's supposed to be a filter */

 Up the trigger_data count to make sure reg doesn't free it on failure */

	/*

	 * The above returns on success the # of functions enabled,

	 * but if it didn't find any functions it returns zero.

	 * Consider no functions a failure too.

 Down the counter of trigger_data or free it if not used anymore */

/**

 * set_trigger_filter - Generic event_command @set_filter implementation

 * @filter_str: The filter string for the trigger, NULL to remove filter

 * @trigger_data: Trigger-specific data

 * @file: The trace_event_file associated with the event

 *

 * Common implementation for event command filter parsing and filter

 * instantiation.

 *

 * Usually used directly as the @set_filter method in event command

 * implementations.

 *

 * Also used to remove a filter (if filter_str = NULL).

 *

 * Return: 0 on success, errno otherwise

 clear the current filter */

 The filter is for the 'trigger' event, not the triggered event */

	/*

	 * If create_event_filter() fails, filter still needs to be freed.

	 * Which the calling code will do with data->filter.

 Make sure the call is done with the filter */

/**

 * find_named_trigger - Find the common named trigger associated with @name

 * @name: The name of the set of named triggers to find the common data for

 *

 * Named triggers are sets of triggers that share a common set of

 * trigger data.  The first named trigger registered with a given name

 * owns the common trigger data that the others subsequently

 * registered with the same name will reference.  This function

 * returns the common trigger data associated with that first

 * registered instance.

 *

 * Return: the common trigger data for the given named trigger on

 * success, NULL otherwise.

/**

 * is_named_trigger - determine if a given trigger is a named trigger

 * @test: The trigger data to test

 *

 * Return: true if 'test' is a named trigger, false otherwise.

/**

 * save_named_trigger - save the trigger in the named trigger list

 * @name: The name of the named trigger set

 * @data: The trigger data to save

 *

 * Return: 0 if successful, negative error otherwise.

/**

 * del_named_trigger - delete a trigger from the named trigger list

 * @data: The trigger data to delete

/**

 * pause_named_trigger - Pause all named triggers with the same name

 * @data: The trigger data of a named trigger to pause

 *

 * Pauses a named trigger along with all other triggers having the

 * same name.  Because named triggers share a common set of data,

 * pausing only one is meaningless, so pausing one named trigger needs

 * to pause all triggers with the same name.

/**

 * unpause_named_trigger - Un-pause all named triggers with the same name

 * @data: The trigger data of a named trigger to unpause

 *

 * Un-pauses a named trigger along with all other triggers having the

 * same name.  Because named triggers share a common set of data,

 * unpausing only one is meaningless, so unpausing one named trigger

 * needs to unpause all triggers with the same name.

/**

 * set_named_trigger_data - Associate common named trigger data

 * @data: The trigger data to associate

 * @named_data: The common named trigger to be associated

 *

 * Named triggers are sets of triggers that share a common set of

 * trigger data.  The first named trigger registered with a given name

 * owns the common trigger data that the others subsequently

 * registered with the same name will reference.  This function

 * associates the common trigger data from the first trigger with the

 * given trigger.

 we register both traceon and traceoff to this callback */

 CONFIG_TRACER_SNAPSHOT */

/* Skip 2:

 *   event_triggers_post_call()

 *   trace_event_raw_event_xxx()

/*

 * Skip 4:

 *   stacktrace_trigger()

 *   event_triggers_post_call()

 *   trace_event_buffer_commit()

 *   trace_event_raw_event_xxx()

 CONFIG_STACKTRACE */

 Skip if the event is in a state we want to switch to */

 Remove the SOFT_MODE flag */

 separate the trigger from the filter (s:e:n [if filter]) */

 Up the trigger_data count to make sure nothing frees it on failure */

		/*

		 * We use the callback data field (which is a pointer)

		 * as our counter.

 if param is non-empty, it's supposed to be a filter */

 Don't let event modules unload while probe registered */

	/*

	 * The above returns on success the # of functions enabled,

	 * but if it didn't find any functions it returns zero.

	 * Consider no functions a failure too.

 Just return zero, not the number of enabled functions */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2021 VMware Inc, Steven Rostedt <rostedt@goodmis.org>

 See pid_list.h for details */

	/*

	 * If a refill needs to happen, it can not happen here

	 * as the scheduler run queue locks are held.

	/*

	 * If a refill needs to happen, it can not happen here

	 * as the scheduler run queue locks are held.

	/*

	 * If chunk->data has no lower chunks, it will be the same

	 * as a zeroed bitmask. Use find_first_bit() to test it

	 * and if it doesn't find any bits set, then the array

	 * is empty.

 MAX_PID should cover all pids */

 In case a bad pid is passed in, then fail */

/**

 * trace_pid_list_is_set - test if the pid is set in the list

 * @pid_list: The pid list to test

 * @pid: The pid to to see if set in the list.

 *

 * Tests if @pid is is set in the @pid_list. This is usually called

 * from the scheduler when a task is scheduled. Its pid is checked

 * if it should be traced or not.

 *

 * Return true if the pid is in the list, false otherwise.

/**

 * trace_pid_list_set - add a pid to the list

 * @pid_list: The pid list to add the @pid to.

 * @pid: The pid to add.

 *

 * Adds @pid to @pid_list. This is usually done explicitly by a user

 * adding a task to be traced, or indirectly by the fork function

 * when children should be traced and a task's pid is in the list.

 *

 * Return 0 on success, negative otherwise.

/**

 * trace_pid_list_clear - remove a pid from the list

 * @pid_list: The pid list to remove the @pid from.

 * @pid: The pid to remove.

 *

 * Removes @pid from @pid_list. This is usually done explicitly by a user

 * removing tasks from tracing, or indirectly by the exit function

 * when a task that is set to be traced exits.

 *

 * Return 0 on success, negative otherwise.

 if there's no more bits set, add it to the free list */

/**

 * trace_pid_list_next - return the next pid in the list

 * @pid_list: The pid list to examine.

 * @pid: The pid to start from

 * @next: The pointer to place the pid that is set starting from @pid.

 *

 * Looks for the next consecutive pid that is in @pid_list starting

 * at the pid specified by @pid. If one is set (including @pid), then

 * that pid is placed into @next.

 *

 * Return 0 when a pid is found, -1 if there are no more pids included.

/**

 * trace_pid_list_first - return the first pid in the list

 * @pid_list: The pid list to examine.

 * @pid: The pointer to place the pid first found pid that is set.

 *

 * Looks for the first pid that is set in @pid_list, and places it

 * into @pid if found.

 *

 * Return 0 when a pid is found, -1 if there are no pids set.

	/*

	 * On success of allocating all the chunks, both counters

	 * will be less than zero. If they are not, then an allocation

	 * failed, and we should not try again.

	/*

	 * When the locks were released, free chunks could have

	 * been used and allocation needs to be done again. Might as

	 * well allocate it now.

/**

 * trace_pid_list_alloc - create a new pid_list

 *

 * Allocates a new pid_list to store pids into.

 *

 * Returns the pid_list on success, NULL otherwise.

 According to linux/thread.h, pids can be no bigger that 30 bits */

/**

 * trace_pid_list_free - Frees an allocated pid_list.

 *

 * Frees the memory for a pid_list that was allocated.

 SPDX-License-Identifier: GPL-2.0

/*

 * Memory mapped I/O tracing

 *

 * Copyright (C) 2008 Pekka Paalanen <pq@iki.fi>

 XXX: This is not called when the pipe is closed! */

 XXX: This is later than where events were lost. */

 The trailing newline must be in the message. */

 ignore unknown entries */

 SPDX-License-Identifier: GPL-2.0

/*

 * trace_boot.c

 * Tracing kernel boot-time

 Common ftrace options */

 Compose 'handler' parameter */

 Compose 'action' parameter */

 All digit started node should be instances. */

/*

 * Histogram boottime tracing syntax.

 *

 * ftrace.[instance.INSTANCE.]event.GROUP.EVENT.hist[.N] {

 *	keys = <KEY>[,...]

 *	values = <VAL>[,...]

 *	sort = <SORT-KEY>[,...]

 *	size = <ENTRIES>

 *	name = <HISTNAME>

 *	var { <VAR> = <EXPR> ... }

 *	pause|continue|clear

 *	onmax|onchange[.N] { var = <VAR>; <ACTION> [= <PARAM>] }

 *	onmatch[.N] { event = <EVENT>; <ACTION> [= <PARAM>] }

 *	filter = <FILTER>

 * }

 *

 * Where <ACTION> are;

 *

 *	trace = <EVENT>, <ARG1>[, ...]

 *	save = <ARG1>[, ...]

 *	snapshot

 Expression must not include spaces. */

 Histogram control attributes (mutual exclusive) */

 Histogram handler and actions */

 All digit started node should be instances. */

 do nothing */

 per-event key starts with "event.GROUP.EVENT" */

 Event enablement must be done after event settings */

 Ditto */

 Since tracer can free snapshot buffer, allocate snapshot here.*/

 Global trace array is also one instance */

/*

 * Start tracing at the end of core-initcall, so that it starts tracing

 * from the beginning of postcore_initcall.

 SPDX-License-Identifier: GPL-2.0

/*

 * trace_export.c - export basic ftrace utilities to user space

 *

 * Copyright (C) 2009 Steven Rostedt <srostedt@redhat.com>

 Stub function for events with triggers */

/*

 * The FTRACE_ENTRY_REG macro allows ftrace entry to define register

 * function and thus become accessible via perf.

 not needed for this file */

 force compile-time check on F_printk() */			\

 SPDX-License-Identifier: GPL-2.0

/*

 * trace event based perf event profiling/tracing

 *

 * Copyright (C) 2009 Red Hat Inc, Peter Zijlstra

 * Copyright (C) 2009-2010 Frederic Weisbecker <fweisbec@gmail.com>

/*

 * Force it to be aligned to unsigned long to avoid misaligned accesses

 * surprises

 Count the events in use (per event id, not per instance) */

	/*

	 * We checked and allowed to create parent,

	 * allow children without checking.

	/*

	 * It's ok to check current process (owner) permissions in here,

	 * because code below is called only via perf_event_open syscall.

 The ftrace function trace is allowed only for root. */

		/*

		 * We don't allow user space callchains for  function trace

		 * event, due to issues with page faults while tracing page

		 * fault handler and its overall trickiness nature.

		/*

		 * Same reason to disable user stack dump as for user space

		 * callchains above.

 No tracing, just counting, so no obvious leak */

 Some events are ok to be traced by non-root users... */

	/*

	 * ...otherwise raw tracepoint data can be a severe data leak,

	 * only allow root to have these.

	/*

	 * Ensure our callback won't be called anymore. The buffers

	 * will be freed after that.

 CONFIG_KPROBE_EVENTS */

	/*

	 * local trace_uprobe need to hold event_mutex to call

	 * uprobe_buffer_enable() and uprobe_buffer_disable().

	 * event_mutex is not required for local trace_kprobes.

 CONFIG_UPROBE_EVENTS */

	/*

	 * If TRACE_REG_PERF_ADD returns false; no custom action was performed

	 * and we need to take the default action of enqueueing our event on

	 * the right per-cpu hlist.

	/*

	 * If TRACE_REG_PERF_DEL returns false; no custom action was performed

	 * and we need to take the default action of dequeueing our event from

	 * the right per-cpu hlist.

 zero the dead bytes from align to not leak stack to user */

	/*

	 * @event->hlist entry is NULL (per INIT_HLIST_NODE), and all

	 * the perf code does is hlist_for_each_entry_rcu(), so we can

	 * get away with simply setting the @head.first pointer in order

	 * to create a singular list.

 CONFIG_FUNCTION_TRACER */

 SPDX-License-Identifier: GPL-2.0

/*

 * ring buffer based function tracer

 *

 * Copyright (C) 2007-2012 Steven Rostedt <srostedt@redhat.com>

 * Copyright (C) 2008 Ingo Molnar <mingo@redhat.com>

 *

 * Originally taken from the RT patch by:

 *    Arnaldo Carvalho de Melo <acme@redhat.com>

 *

 * Based on code from the latency_tracer, that is:

 *  Copyright (C) 2004-2006 Ingo Molnar

 *  Copyright (C) 2004 Nadia Yvette Chambers

/*

 * On boot up, the ring buffer is set to the minimum size, so that

 * we do not waste memory on systems that are not using tracing.

/*

 * We need to change this state when a selftest is running.

 * A selftest will lurk into the ring-buffer to count the

 * entries inserted during the selftest although some concurrent

 * insertions into the ring-buffer such as trace_printk could occurred

 * at the same time, giving false positive or negative results.

/*

 * If boot-time tracing including tracers/events via kernel cmdline

 * is running, we do not want to run SELFTEST.

 Pipe tracepoints to printk */

 For tracers that don't implement custom flags */

/*

 * To prevent the comm cache from being overwritten when no

 * tracing is active, only save the comm when a trace event

 * occurred.

/*

 * Kill all tracing for good (never come back).

 * It is initialized to 1 but will turn to zero if the initialization

 * of the tracer is successful. But that is the only place that sets

 * this back to zero.

/*

 * ftrace_dump_on_oops - variable to dump ftrace buffer on oops

 *

 * If there is an oops (or kernel panic) and the ftrace_dump_on_oops

 * is set, then ftrace_dump is called. This will output the contents

 * of the ftrace buffers to the console.  This is very useful for

 * capturing traces that lead to crashes and outputing it to a

 * serial console.

 *

 * It is default off, but you can enable it with either specifying

 * "ftrace_dump_on_oops" in the kernel command line, or setting

 * /proc/sys/kernel/ftrace_dump_on_oops

 * Set 1 if you want to dump buffers of all CPUs

 * Set 2 if you want to dump the buffer of the CPU that triggered oops

 When set, tracing will stop when a WARN*() is hit */

 Map of enums to their values, for "eval_map" file */

	/*

	 * "end" is first and points to NULL as it must be different

	 * than "mod" or "eval_string"

 points to NULL */

/*

 * The trace_eval_maps are saved in an array with two extra elements,

 * one at the beginning, and one at the end. The beginning item contains

 * the count of the saved maps (head.length), and the module they

 * belong to if not built in (head.mod). The ending item contains a

 * pointer to the next array of saved eval_map items.

 CONFIG_TRACE_EVAL_MAP_FILE */

 We are using ftrace early, expand it */

 We also need the main ring buffer expanded */

	/*

	 * We are entering export into the list but another

	 * CPU might be walking that list. We need to make sure

	 * the export->next pointer is valid before another CPU sees

	 * the export pointer included into the list.

 trace_flags holds trace_options default values */

 trace_options that are only supported by global_trace */

 trace_flags that are default zero for instances */

/*

 * The global_trace is the descriptor that holds the top-level tracing

 * buffers for the live tracing.

/**

 * trace_array_put - Decrement the reference counter for this trace array.

 * @this_tr : pointer to the trace array

 *

 * NOTE: Use this when we no longer need the trace array returned by

 * trace_array_get_by_name(). This ensures the trace array can be later

 * destroyed.

 *

/**

 * trace_find_filtered_pid - check if a pid exists in a filtered_pid list

 * @filtered_pids: The list of pids to check

 * @search_pid: The PID to find in @filtered_pids

 *

 * Returns true if @search_pid is found in @filtered_pids, and false otherwise.

/**

 * trace_ignore_this_task - should a task be ignored for tracing

 * @filtered_pids: The list of pids to check

 * @filtered_no_pids: The list of pids not to be traced

 * @task: The task that should be ignored if not filtered

 *

 * Checks if @task should be traced or not from @filtered_pids.

 * Returns true if @task should *NOT* be traced.

 * Returns false if @task should be traced.

	/*

	 * If filtered_no_pids is not empty, and the task's pid is listed

	 * in filtered_no_pids, then return true.

	 * Otherwise, if filtered_pids is empty, that means we can

	 * trace all tasks. If it has content, then only trace pids

	 * within filtered_pids.

/**

 * trace_filter_add_remove_task - Add or remove a task from a pid_list

 * @pid_list: The list to modify

 * @self: The current task for fork or NULL for exit

 * @task: The task to add or remove

 *

 * If adding a task, if @self is defined, the task is only added if @self

 * is also included in @pid_list. This happens on fork and tasks should

 * only be added when the parent is listed. If @self is NULL, then the

 * @task pid will be removed from the list, which would happen on exit

 * of a task.

 For forks, we only add if the forking task is listed */

 "self" is set for forks, and NULL for exits */

/**

 * trace_pid_next - Used for seq_file to get to the next pid of a pid_list

 * @pid_list: The pid list to show

 * @v: The last pid that was shown (+1 the actual pid to let zero be displayed)

 * @pos: The position of the file

 *

 * This is used by the seq_file "next" operation to iterate the pids

 * listed in a trace_pid_list structure.

 *

 * Returns the pid+1 as we want to display pid of zero, but NULL would

 * stop the iteration.

 pid already is +1 of the actual previous bit */

 Return pid + 1 to allow zero to be represented */

/**

 * trace_pid_start - Used for seq_file to start reading pid lists

 * @pid_list: The pid list to show

 * @pos: The position of the file

 *

 * This is used by seq_file "start" operation to start the iteration

 * of listing pids.

 *

 * Returns the pid+1 as we want to display pid of zero, but NULL would

 * stop the iteration.

 Return pid + 1 so that zero can be the exit value */

/**

 * trace_pid_show - show the current pid in seq_file processing

 * @m: The seq_file structure to write into

 * @v: A void pointer of the pid (+1) value to display

 *

 * Can be directly used by seq_file operations to display the current

 * pid value.

 128 should be much more than enough */

	/*

	 * Always recreate a new array. The write is an all or nothing

	 * operation. Always create a new array when adding new pids by

	 * the user. If the operation fails, then the current list is

	 * not modified.

 copy the current bits to the new max */

 Cleared the list of pids */

 Early boot up does not have a buffer yet */

/**

 * tracing_is_enabled - Show if global_trace has been enabled

 *

 * Shows if the global trace has been enabled or not. It uses the

 * mirror flag "buffer_disabled" to be used in fast paths such as for

 * the irqsoff tracer. But it may be inaccurate due to races. If you

 * need to know the accurate state, use tracing_is_on() which is a little

 * slower, but accurate.

	/*

	 * For quick access (irqsoff uses this in fast path), just

	 * return the mirror variable of the state of the ring buffer.

	 * It's a little racy, but we don't really care.

/*

 * trace_buf_size is the size in bytes that is allocated

 * for a buffer. Note, the number of bytes is always rounded

 * to page size.

 *

 * This number is purposely set to a low number of 16384.

 * If the dump on oops happens, it will be much appreciated

 * to not have to wait for all that output. Anyway this can be

 * boot time and run time configurable.

 16384 * 88 (sizeof(entry)) */

 trace_types holds a link list of available tracers. */

/*

 * trace_types_lock is used to protect the trace_types list.

/*

 * serialize the access of the ring buffer

 *

 * ring buffer serializes readers, but it is low level protection.

 * The validity of the events (which returns by ring_buffer_peek() ..etc)

 * are not protected by ring buffer.

 *

 * The content of events may become garbage if we allow other process consumes

 * these events concurrently:

 *   A) the page of the consumed events may become a normal page

 *      (not reader page) in ring buffer, and this page will be rewritten

 *      by events producer.

 *   B) The page of the consumed events may become a page for splice_read,

 *      and this page will be returned to system.

 *

 * These primitives allow multi process access to different cpu ring buffer

 * concurrently.

 *

 * These primitives don't distinguish read-only and read-consume access.

 * Multi read-only access are also serialized.

 gain it for accessing the whole ring buffer. */

 gain it for accessing a cpu ring buffer. */

 Firstly block other trace_access_lock(RING_BUFFER_ALL_CPUS). */

 Secondly block other access to this @cpu ring buffer. */

	/*

	 * This flag is looked at when buffers haven't been allocated

	 * yet, or by some tracers (like irqsoff), that just want to

	 * know if the ring buffer has been disabled, but it can handle

	 * races of where it gets disabled but we still do a record.

	 * As the check is in the fast path of the tracers, it is more

	 * important to be fast than accurate.

 Make the flag seen by readers */

/**

 * tracing_on - enable tracing buffers

 *

 * This function enables tracing buffers that may have been

 * disabled with tracing_off.

 If this is the temp buffer, we need to commit fully */

 Length is in event->array[0] */

 Release the temp buffer */

/**

 * __trace_puts - write a constant string into the trace buffer.

 * @ip:	   The address of the caller

 * @str:   The constant string to write

 * @size:  The size of the string.

 possible \n added */

 Add a newline if necessary */

/**

 * __trace_bputs - write the pointer to a constant string into trace buffer

 * @ip:	   The address of the caller

 * @str:   The constant string to write to the buffer to

 Note, snapshot can not be used when the tracer uses it */

/**

 * tracing_snapshot - take a snapshot of the current buffer.

 *

 * This causes a swap between the snapshot buffer and the current live

 * tracing buffer. You can use this to take snapshots of the live

 * trace when some condition is triggered, but continue to trace.

 *

 * Note, make sure to allocate the snapshot with either

 * a tracing_snapshot_alloc(), or by doing it manually

 * with: echo 1 > /sys/kernel/debug/tracing/snapshot

 *

 * If the snapshot buffer is not allocated, it will stop tracing.

 * Basically making a permanent snapshot.

/**

 * tracing_snapshot_cond - conditionally take a snapshot of the current buffer.

 * @tr:		The tracing instance to snapshot

 * @cond_data:	The data to be tested conditionally, and possibly saved

 *

 * This is the same as tracing_snapshot() except that the snapshot is

 * conditional - the snapshot will only happen if the

 * cond_snapshot.update() implementation receiving the cond_data

 * returns true, which means that the trace array's cond_snapshot

 * update() operation used the cond_data to determine whether the

 * snapshot should be taken, and if it was, presumably saved it along

 * with the snapshot.

/**

 * tracing_snapshot_cond_data - get the user data associated with a snapshot

 * @tr:		The tracing instance

 *

 * When the user enables a conditional snapshot using

 * tracing_snapshot_cond_enable(), the user-defined cond_data is saved

 * with the snapshot.  This accessor is used to retrieve it.

 *

 * Should not be called from cond_snapshot.update(), since it takes

 * the tr->max_lock lock, which the code calling

 * cond_snapshot.update() has already done.

 *

 * Returns the cond_data associated with the trace array's snapshot.

 allocate spare buffer */

	/*

	 * We don't free the ring buffer. instead, resize it because

	 * The max_tr ring buffer has some state (e.g. ring->clock) and

	 * we want preserve it.

/**

 * tracing_alloc_snapshot - allocate snapshot buffer.

 *

 * This only allocates the snapshot buffer if it isn't already

 * allocated - it doesn't also take a snapshot.

 *

 * This is meant to be used in cases where the snapshot buffer needs

 * to be set up for events that can't sleep but need to be able to

 * trigger a snapshot.

/**

 * tracing_snapshot_alloc - allocate and take a snapshot of the current buffer.

 *

 * This is similar to tracing_snapshot(), but it will allocate the

 * snapshot buffer if it isn't already allocated. Use this only

 * where it is safe to sleep, as the allocation may sleep.

 *

 * This causes a swap between the snapshot buffer and the current live

 * tracing buffer. You can use this to take snapshots of the live

 * trace when some condition is triggered, but continue to trace.

/**

 * tracing_snapshot_cond_enable - enable conditional snapshot for an instance

 * @tr:		The tracing instance

 * @cond_data:	User data to associate with the snapshot

 * @update:	Implementation of the cond_snapshot update function

 *

 * Check whether the conditional snapshot for the given instance has

 * already been enabled, or if the current tracer is already using a

 * snapshot; if so, return -EBUSY, else create a cond_snapshot and

 * save the cond_data and update function inside.

 *

 * Returns 0 if successful, error otherwise.

	/*

	 * The cond_snapshot can only change to NULL without the

	 * trace_types_lock. We don't care if we race with it going

	 * to NULL, but we want to make sure that it's not set to

	 * something other than NULL when we get here, which we can

	 * do safely with only holding the trace_types_lock and not

	 * having to take the max_lock.

/**

 * tracing_snapshot_cond_disable - disable conditional snapshot for an instance

 * @tr:		The tracing instance

 *

 * Check whether the conditional snapshot for the given instance is

 * enabled; if so, free the cond_snapshot associated with it,

 * otherwise return -EINVAL.

 *

 * Returns 0 if successful, error otherwise.

 Give warning */

 CONFIG_TRACER_SNAPSHOT */

	/*

	 * This flag is looked at when buffers haven't been allocated

	 * yet, or by some tracers (like irqsoff), that just want to

	 * know if the ring buffer has been disabled, but it can handle

	 * races of where it gets disabled but we still do a record.

	 * As the check is in the fast path of the tracers, it is more

	 * important to be fast than accurate.

 Make the flag seen by readers */

/**

 * tracing_off - turn off tracing buffers

 *

 * This function stops the tracing buffers from recording data.

 * It does not disable any overhead the tracers themselves may

 * be causing. This function simply causes all recording to

 * the ring buffers to fail.

/**

 * tracer_tracing_is_on - show real state of ring buffer enabled

 * @tr : the trace array to know if ring buffer is enabled

 *

 * Shows real state of the ring buffer if it is enabled or not.

/**

 * tracing_is_on - show state of ring buffers enabled

 nr_entries can not be zero */

/*

 * TRACE_FLAGS is defined as a tuple matching bit masks with strings.

 * It uses C(a, b) where 'a' is the eval (enum) name and 'b' is the string that

 * matches it. By defining "C(a, b) b", TRACE_FLAGS becomes a list

 * of strings in the order that the evals (enum) were defined.

 These must match the bit positions in trace_iterator_flags */

 is this clock in nanoseconds? */

/*

 * trace_parser_get_init - gets the buffer for trace parser

/*

 * trace_parser_put - frees the buffer for trace parser

/*

 * trace_get_user - reads the user input string separated by  space

 * (matched by isspace(ch))

 *

 * For each string found the 'struct trace_parser' is updated,

 * and the function returns.

 *

 * Returns number of bytes read.

 *

 * See kernel/trace/trace.h for 'struct trace_parser' details.

	/*

	 * The parser is not finished with the last write,

	 * continue reading the user input without skipping spaces.

 skip white space */

 only spaces were written */

 read the non-space input */

 We either got finished input or we have to wait for another call. */

 Make sure the parsed string always terminates with '\0'. */

 TODO add a seq_buf_to_buffer() */

	/*

	 * We cannot call queue_work(&tr->fsnotify_work) from here because it's

	 * possible that we are called from __schedule() or do_idle(), which

	 * could cause a deadlock.

/*

 * Copy the new maximum trace into the separate maximum-trace

 * structure. (this way the maximum trace is permanently saved,

 * for later retrieval via /sys/kernel/tracing/tracing_max_latency)

	/*

	 * If tsk == current, then use current_uid(), as that does not use

	 * RCU. The irq tracer can be called out of RCU scope.

 record this tasks comm */

/**

 * update_max_tr - snapshot all trace buffers from global_trace to max_tr

 * @tr: tracer

 * @tsk: the task with the latency

 * @cpu: The cpu that initiated the trace.

 * @cond_data: User data associated with a conditional snapshot

 *

 * Flip the buffers between the @tr and the max_tr and record information

 * about which task was the cause of this latency.

 Only the nop tracer should hit this when disabling */

 Inherit the recordable setting from array_buffer */

/**

 * update_max_tr_single - only copy one trace over, and reset the rest

 * @tr: tracer

 * @tsk: task with the latency

 * @cpu: the cpu of the buffer to copy.

 *

 * Flip the trace of a single CPU buffer between the @tr and the max_tr.

 Only the nop tracer should hit this when disabling */

		/*

		 * We failed to swap the buffer due to a commit taking

		 * place on this CPU. We fail to record, but we reset

		 * the max trace buffer (no one writes directly to it)

		 * and flag that it failed.

 CONFIG_TRACER_MAX_TRACE */

 Iterators are static, they should be filled or empty */

	/*

	 * If a tracer registers early in boot up (before scheduling is

	 * initialized and such), then do not run its selftests yet.

	 * Instead, run it a little later in the boot process.

	/*

	 * Run a selftest on this tracer.

	 * Here we reset the trace buffer, and set the current

	 * tracer to be this tracer. The tracer can then run some

	 * internal tracing to verify that everything is in order.

	 * If we fail, we do not register this tracer.

 If we expanded the buffers, make sure the max is expanded too */

 the test is responsible for initializing and enabling */

 the test is responsible for resetting too */

 Add the warning after printing 'FAILED' */

 Only reset on passing, to avoid touching corrupted buffers */

 Shrink the max buffer again */

		/* This loop can take minutes when sanitizers are enabled, so

		 * lets make sure we allow RCU processing.

 If the test fails, then warn and remove from available_tracers */

 CONFIG_FTRACE_STARTUP_TEST */

/**

 * register_tracer - register a tracer with the ftrace system.

 * @type: the plugin for the tracer

 *

 * Register a new plugin tracer.

 already found */

allocate a dummy tracer_flags*/

 store the tracer for __set_tracer_option */

 Do we want this tracer to start on bootup? */

 disable other selftests, since this will break it. */

 Make sure all commits have finished */

 Make sure all commits have finished */

 Must have trace_types_lock held */

/*

 * The tgid_map array maps from pid to tgid; i.e. the value stored at index i

 * is the tgid last observed corresponding to pid=i.

 The maximum valid index into tgid_map. */

/**

 * tracing_start - quick start of the tracer

 *

 * If tracing is enabled but was stopped by tracing_stop,

 * this will start the tracer back up.

 Someone screwed up their debugging */

 Prevent the buffers from switching */

 If global, we need to also start the max tracer */

 Someone screwed up their debugging */

/**

 * tracing_stop - quick stop of the tracer

 *

 * Light weight way to stop tracing. Use in conjunction with

 * tracing_start.

 Prevent the buffers from switching */

 If global, we need to also stop the max tracer */

 treat recording of idle task as a success */

	/*

	 * It's not the end of the world if we don't get

	 * the lock, but we also don't want to spin

	 * nor do we want to disable interrupts,

	 * so if we miss here, then better luck next time.

	/*

	 * Pairs with the smp_store_release in set_tracer_flag() to ensure that

	 * if we observe a non-NULL tgid_map then we also observe the correct

	 * tgid_map_max.

 treat recording of idle task as a success */

/**

 * tracing_record_taskinfo - record the task info of a task

 *

 * @task:  task to record

 * @flags: TRACE_RECORD_CMDLINE for recording comm

 *         TRACE_RECORD_TGID for recording tgid

	/*

	 * Record as much task information as possible. If some fail, continue

	 * to try to record the others.

 If recording any information failed, retry again soon. */

/**

 * tracing_record_taskinfo_sched_switch - record task info for sched_switch

 *

 * @prev: previous task during sched_switch

 * @next: next task during sched_switch

 * @flags: TRACE_RECORD_CMDLINE for recording comm

 *         TRACE_RECORD_TGID for recording tgid

	/*

	 * Record as much task information as possible. If some fail, continue

	 * to try to record the others.

 If recording any information failed, retry again soon. */

 Helpers to record a specific task information */

/*

 * Several functions return TRACE_TYPE_PARTIAL_LINE if the trace_seq

 * overflowed, and TRACE_TYPE_HANDLED otherwise. This helper function

 * simplifies those functions and keeps them in sync.

/**

 * trace_buffered_event_enable - enable buffering events

 *

 * When events are being filtered, it is quicker to use a temporary

 * buffer to write the event data into if there's a likely chance

 * that it will not be committed. The discard of the ring buffer

 * is not as fast as committing, and is much slower than copying

 * a commit.

 *

 * When an event is to be filtered, allocate per cpu buffers to

 * write the event data into, and if the event is filtered and discarded

 * it is simply dropped, otherwise, the entire data is to be committed

 * in one shot.

 Probably not needed, but do it anyway */

/**

 * trace_buffered_event_disable - disable buffering events

 *

 * When a filter is removed, it is faster to not use the buffered

 * events, and to commit directly into the ring buffer. Free up

 * the temp buffers when there are no more users. This requires

 * special synchronization with current events.

 For each CPU, set the buffer as used. */

 Wait for all current users to finish */

	/*

	 * Make sure trace_buffered_event is NULL before clearing

	 * trace_buffered_event_cnt.

 Do the work on each cpu */

		/*

		 * Filtering is on, so try to use the per cpu buffer first.

		 * This buffer will simulate a ring_buffer_event,

		 * where the type_len is zero and the array[0] will

		 * hold the full length.

		 * (see include/linux/ring-buffer.h for details on

		 *  how the ring_buffer_event is structured).

		 *

		 * Using a temp buffer during filtering and copying it

		 * on a matched filter is quicker than writing directly

		 * into the ring buffer and then discarding it when

		 * it doesn't match. That is because the discard

		 * requires several atomic operations to get right.

		 * Copying on match and doing nothing on a failed match

		 * is still quicker than no copy on match, but having

		 * to discard out of the ring buffer on a failed match.

		/*

		 * Preemption is disabled, but interrupts and NMIs

		 * can still come in now. If that happens after

		 * the above increment, then it will have to go

		 * back to the old method of allocating the event

		 * on the ring buffer, and if the filter fails, it

		 * will have to call ring_buffer_discard_commit()

		 * to remove it.

		 *

		 * Need to also check the unlikely case that the

		 * length is bigger than the temp buffer size.

		 * If that happens, then the reserve is pretty much

		 * guaranteed to fail, as the ring buffer currently

		 * only allows events less than a page. But that may

		 * change in the future, so let the ring buffer reserve

		 * handle the failure in that case.

	/*

	 * If tracing is off, but we have triggers enabled

	 * we still need to look at the event data. Use the temp_buffer

	 * to store the trace event for the trigger to use. It's recursive

	 * safe and will not be recorded anywhere.

 We should never get here if iter is NULL */

	/*

	 * This will force exiting early, as tracepoint_printk

	 * is always zero when tracepoint_printk_iter is not allocated

/*

 * Skip 3:

 *

 *   trace_buffer_unlock_commit_regs()

 *   trace_event_buffer_commit()

 *   trace_event_raw_event_xxx()

	/*

	 * If regs is not set, then skip the necessary functions.

	 * Note, we can still get here via blktrace, wakeup tracer

	 * and mmiotrace, but that's ok if they lose a function or

	 * two. They are not that meaningful.

/*

 * Similar to trace_buffer_unlock_commit_regs() but do not dump stack.

 Allow 4 levels of nesting: normal, softirq, irq, NMI */

	/*

	 * Add one, for this function and the call to save_stack_trace()

	 * If regs is set, then these functions will not be in the way.

 This should never happen. If it does, yell once and skip */

	/*

	 * The above __this_cpu_inc_return() is 'atomic' cpu local. An

	 * interrupt will either see the value pre increment or post

	 * increment. If the interrupt happens pre increment it will have

	 * restored the counter when it returns.  We just need a barrier to

	 * keep gcc from moving things around.

 Again, don't let gcc optimize things here */

	/*

	 * When an NMI triggers, RCU is enabled via rcu_nmi_enter(),

	 * but if the above rcu_is_watching() failed, then the NMI

	 * triggered someplace critical, and rcu_irq_enter() should

	 * not be called from NMI.

/**

 * trace_dump_stack - record a stack back trace in the trace buffer

 * @skip: Number of functions to skip (helper handlers)

 Skip 1 to skip this function. */

	/*

	 * NMIs can not handle page faults, even with fix ups.

	 * The save user stack can (and often does) fault.

	/*

	 * prevent recursion, since the user stack tracing may

	 * trigger other kernel events.

 CONFIG_USER_STACKTRACE_SUPPORT */

 !CONFIG_USER_STACKTRACE_SUPPORT */

 CONFIG_STACKTRACE */

 created for use with alloc_percpu */

/*

 * This allows for lockless recording.  If we're nested too deeply, then

 * this returns NULL.

 Interrupts must see nesting incremented before we use the buffer */

 Don't let the decrement of nesting leak before this */

 trace_printk() is for debug use only. Don't use it in production. */

 Expand the buffers to set size */

	/*

	 * trace_printk_init_buffers() can be called by modules.

	 * If that happens, then we need to start cmdline recording

	 * directly here. If the global_trace.buffer is already

	 * allocated here, then this was called by module code.

 Start tracing comms if trace printk is set */

/**

 * trace_vbprintk - write binary msg to tracing buffer

 * @ip:    The address of the caller

 * @fmt:   The string format to write to the buffer

 * @args:  Arguments for @fmt

 Don't pollute graph traces with trace_vprintk internals */

 Don't pollute graph traces with trace_vprintk internals */

/**

 * trace_array_printk - Print a message to a specific instance

 * @tr: The instance trace_array descriptor

 * @ip: The instruction pointer that this is called from.

 * @fmt: The format to print (printf format)

 *

 * If a subsystem sets up its own instance, they have the right to

 * printk strings into their tracing instance buffer using this

 * function. Note, this function will not write into the top level

 * buffer (use trace_printk() for that), as writing into the top level

 * buffer should only have events that can be individually disabled.

 * trace_printk() is only used for debugging a kernel, and should not

 * be ever incorporated in normal use.

 *

 * trace_array_printk() can be used, as it will not add noise to the

 * top level tracing buffer.

 *

 * Note, trace_array_init_printk() must be called on @tr before this

 * can be used.

 This is only allowed for created instances */

/**

 * trace_array_init_printk - Initialize buffers for trace_array_printk()

 * @tr: The trace array to initialize the buffers for

 *

 * As trace_array_printk() only writes into instances, they are OK to

 * have in the kernel (unlike trace_printk()). This needs to be called

 * before trace_array_printk() can be used on a trace_array.

 This is only allowed for created instances */

	/*

	 * If we are in a per_cpu trace file, don't bother by iterating over

	 * all cpu and peek directly.

		/*

		 * Pick the entry with the smallest timestamp:

	/*

	 * iter->tr is NULL when used with tp_printk, which makes

	 * this get called where it is not safe to call krealloc().

 Returns true if the string is safe to dereference from an event */

 OK if part of the event data */

 OK if part of the temp seq buffer */

 Core rodata can not be freed */

	/*

	 * Now this could be a module event, referencing core module

	 * data, which is OK.

 Would rather have rodata, but this will suffice */

	/*

	 * The verifier is dependent on vsnprintf() modifies the va_list

	 * passed to it, where it is sent as a reference. Some architectures

	 * (like x86_32) passes it by value, which means that vsnprintf()

	 * does not modify the va_list passed to it, and the verifier

	 * would then need to be able to understand all the values that

	 * vsnprintf can use. If it is passed by value, then the verifier

	 * is disabled.

/**

 * trace_check_vprintf - Check dereferenced strings while writing to the seq buffer

 * @iter: The iterator that holds the seq buffer and the event being printed

 * @fmt: The format used to print the event

 * @ap: The va_list holding the data to print from @fmt.

 *

 * This writes the data into the @iter->seq buffer using the data from

 * @fmt and @ap. If the format has a %s, then the source of the string

 * is examined to make sure it is safe to print, otherwise it will

 * warn and print "[UNSAFE MEMORY]" in place of the dereferenced string

 * pointer.

 Don't bother checking when doing a ftrace_dump() */

 We only care about %s and variants */

				/*

				 * If we can't expand the copy buffer,

				 * just print it.

 Need to test cases like %08.*s */

 If no %s found then just print normally */

 Copy up to the %s, and print that */

		/*

		 * If iter->seq is full, the above call no longer guarantees

		 * that ap is in sync with fmt processing, and further calls

		 * to va_arg() can return wrong positional arguments.

		 *

		 * Ensure that ap is no longer used in this case.

 The ap now points to the string data of the %s */

		/*

		 * If you hit this warning, it is likely that the

		 * trace event in question used %s on a string that

		 * was saved at the time of the event, but may not be

		 * around when the trace is read. Use __string(),

		 * __assign_str() and __get_str() helpers in the TRACE_EVENT()

		 * instead. See samples/trace_events/trace-events-sample.h

		 * for reference.

 Try to safely read the string */

 Replace %p with %px */

 Find the next real entry, without updating the iterator itself */

 __find_next_entry will reset ent_size */

	/*

	 * If called from ftrace_dump(), then the iter->temp buffer

	 * will be the static_temp_buf and not created from kmalloc.

	 * If the entry size is greater than the buffer, we can

	 * not save it. Just return NULL in that case. This is only

	 * used to add markers when two consecutive events' time

	 * stamps have a large delta. See trace_print_lat_context()

	/*

	 * The __find_next_entry() may call peek_next_entry(), which may

	 * call ring_buffer_peek() that may make the contents of iter->ent

	 * undefined. Need to copy iter->ent now.

 Put back the original ent_size */

 Find the next real entry, and increment the iterator to the next entry */

 can't go backwards */

	/*

	 * We could have the case with the max latency tracers

	 * that a reset never took place on a cpu. This is evident

	 * by the timestamp being before the start of the buffer.

/*

 * The current tracer is copied to avoid a global locking

 * all around.

	/*

	 * copy the tracer to avoid using a global lock all around.

	 * iter->trace is a copy of current_trace, the pointer to the

	 * name may be used instead of a strcmp(), as iter->trace->name

	 * will point to the same string as current_trace->name.

		/*

		 * If we overflowed the seq_file before, then we want

		 * to just reuse the trace_seq buffer again.

	/*

	 * If this buffer has skipped entries, then we hold all

	 * entries for the trace and we need to ignore the

	 * ones before the time stamp.

 total is the same as the entries */

 These are reserved for later use */

 Don't print started cpu buffer for the first entry of the trace */

 If we are looking at one CPU buffer, only check that one */

  Called with trace_event_read_lock() held. */

 print nothing if the buffers are empty */

 print nothing if the buffers are empty */

 Should never be called */

		/*

		 * If we filled the seq_file buffer earlier, we

		 * want to just show it now.

 ret should this time be zero, but you never know */

		/*

		 * If we overflow the seq_file buffer, then it will

		 * ask us for this data again at start up.

		 * Use that instead.

		 *  ret is 0 if seq_file write succeeded.

		 *        -1 otherwise.

/*

 * Should be used after trace_array_get(), trace_types_lock

 * ensures that i_cdev was already initialized.

 See trace_create_cpu_file() */

	/*

	 * trace_find_next_entry() may need to save off iter->ent.

	 * It will place it into the iter->temp buffer. As most

	 * events are less than 128, allocate a buffer of that size.

	 * If one is greater, then trace_find_next_entry() will

	 * allocate a new buffer to adjust for the bigger iter->ent.

	 * It's not critical if it fails to get allocated here.

	/*

	 * trace_event_printf() may need to modify given format

	 * string to replace %p with %px so that it shows real address

	 * instead of hash value. However, that is only for the event

	 * tracing, other tracer may not need. Defer the allocation

	 * until it is needed.

	/*

	 * We make a copy of the current tracer to avoid concurrent

	 * changes on it while we are reading.

 Currently only the top directory has a snapshot */

 Notify the tracer early; before we stop tracing. */

 Annotate start of buffers if we had overruns */

 Output in nanoseconds only if we are using a clock in nanoseconds. */

	/*

	 * If pause-on-trace is enabled, then stop the trace while

	 * dumping, unless this is the "snapshot" file

/*

 * Open and update trace_array ref count.

 * Must have the current trace_array passed to it.

 Writes do not use seq_file */

 reenable tracing if it was previously enabled */

 If this file was open for write, then erase contents */

/*

 * Some tracers are not suitable for instance buffers.

 * A tracer is always available for the global array (toplevel)

 * or if it explicitly states that it is.

 Find the next tracer that this trace array may use */

		/*

		 * Increase/decrease the disabled counter if we are

		 * about to flip a bit in the cpumask:

 Try to assign a tracer specific option */

 Some tracers require overwrite to stay enabled */

 do nothing if flag is already set */

 Give the tracer a chance to approve the change */

			/*

			 * Pairs with smp_load_acquire() in

			 * trace_find_tgid_ptr() to ensure that if it observes

			 * the tgid_map we just allocated then it also observes

			 * the corresponding tgid_map_max value.

 If no option could be set, test the specific tracer options */

	/*

	 * If the first trailing whitespace is replaced with '\0' by strstrip,

	 * turn it back into a space.

 Put back the comma to allow this to be called again */

 CONFIG_DYNAMIC_FTRACE */

 CONFIG_STACK_TRACER */

 must have at least 1 entry or less than PID_MAX_DEFAULT */

 Set ptr to the next real item (skip head) */

	/*

	 * Paranoid! If ptr points to end, we don't want to increment past it.

	 * This really should never happen.

 Return tail of array given the head */

	/*

	 * The trace_eval_maps contains the map plus a head and tail item,

	 * where the head holds the module and length of array, and the

	 * tail holds a pointer to the next list.

 CONFIG_TRACE_EVAL_MAP_FILE */

 !CONFIG_TRACE_EVAL_MAP_FILE */

 resize @tr's buffer to the size of @size_tr's entries */

 CONFIG_TRACER_MAX_TRACE */

	/*

	 * If kernel or user changes the size of the ring buffer

	 * we use the size that was given, and we can forget about

	 * expanding it later.

 May be called before buffers are initialized */

			/*

			 * AARGH! We are left with different

			 * size max buffer!!!!

			 * The max buffer is our "snapshot" buffer.

			 * When a tracer needs a snapshot (one of the

			 * latency tracers), it swaps the max buffer

			 * with the saved snap shot. We succeeded to

			 * update the size of the main buffer, but failed to

			 * update the size of the max buffer. But when we tried

			 * to reset the main buffer to the original size, we

			 * failed there too. This is very unlikely to

			 * happen, but if it does, warn and kill all

			 * tracing.

 CONFIG_TRACER_MAX_TRACE */

 make sure, this cpu is enabled in the mask */

/**

 * tracing_update_buffers - used by tracing facility to expand ring buffers

 *

 * To save on memory when the tracing is never used on a system with it

 * configured in. The ring buffers are set to a minimum size. But once

 * a user starts to use the tracing facility, then they need to grow

 * to their default size.

 *

 * This function is to be called when a tracer is about to be used.

/*

 * Used to clear out the tracer before deletion of an instance.

 * Must have trace_types_lock held.

 Only enable if the directory has been created already. */

 Some tracers won't work on kernel command line */

 Some tracers are only allowed for the top level buffer */

 If trace pipe files are being read, we can't change the tracer */

 Current trace needs to be nop_trace before synchronize_rcu */

		/*

		 * We need to make sure that the update_max_tr sees that

		 * current_trace changed to nop_trace to keep it from

		 * swapping the buffers after we resize it.

		 * The update_max_tr is called from interrupts disabled

		 * so a synchronized_sched() is sufficient.

 strip ending whitespace. */

 create a buffer to store the information to pass to userspace */

 trace pipe does not show start of buffer */

 Output in nanoseconds only if we are using a clock in nanoseconds. */

 Iterators are static, they should be filled or empty */

		/*

		 * Always select as readable when in blocking mode

 Must be called with iter->mutex held. */

		/*

		 * We block until we read something and tracing is disabled.

		 * We still block if tracing is disabled, but we have never

		 * read anything. This allows a user to cat this file, and

		 * then enable tracing. But after we have read something,

		 * we give an EOF when tracing is again disabled.

		 *

		 * iter->pos will be 0 if we haven't read anything.

/*

 * Consumer reader.

	/*

	 * Avoid more than one consumer on a single file descriptor

	 * This is just a matter of traces coherency, the ring buffer itself

	 * is protected.

 return any leftover data */

 stop when tracing is finished */

 reset all but tr, trace, and overruns */

 don't print partial lines */

		/*

		 * Setting the full flag means we reached the trace_seq buffer

		 * size and we should leave by partial output condition above.

		 * One of the trace_seq_* functions is not used properly.

 Now copy what we have to the user */

	/*

	 * If there was nothing to send to user, in spite of consuming trace

	 * entries, go back to wait for more entries.

 Seq buffer is page-sized, exactly what we need. */

		/*

		 * This should not be hit, because it should only

		 * be set if the iter->seq overflowed. But check it

		 * anyway to be safe.

 This gets updated below. */

 Fill as many pages as possible. */

 Copy the data into the page, so we can start over. */

 check if all cpu sizes are same */

 fill in the size from first enabled cpu */

 must have at least 1 entry */

 value is in KB */

	/*

	 * There is no need to read what the user has written, this function

	 * is just to make sure that there is no error when "echo" is used

 disable tracing ? */

 resize the ring buffer to 0 */

 Used in tracing_mark_raw_write() as well */

 '\0' is already accounted for */

 add '\0' and possible '\n' */

 If less than "<faulted>", then make sure we can still add that */

 Ring buffer disabled, return as if not open for write */

 do not add \n before testing triggers, but add \0 */

 Limit it for now to 3K (including tag) */

 The marker must at least have a tag id */

 Ring buffer disabled, return as if not open for write */

	/*

	 * New clock may not be consistent with the previous clock.

	 * Reset the buffer so that it doesn't have incomparable timestamps.

/*

 * Set or disable using the per CPU trace_buffer_event when possible.

 Writes still need the seq_file to hold the private data */

 Only allow per-cpu swap if the ring buffer supports it */

 Now, we're going to swap */

 If write only, the seq_file is just a stub */

 The following checks for tracefs lockdown */

 CONFIG_TRACER_SNAPSHOT */

 CONFIG_TRACER_SNAPSHOT */

/*

 * trace_min_max_write - Write a u64 value to a trace_min_max_param struct

 * @filp: The active open file structure

 * @ubuf: The userspace provided buffer to read value into

 * @cnt: The maximum number of bytes to read

 * @ppos: The current "file" position

 *

 * This function implements the write interface for a struct trace_min_max_param.

 * The filp->private_data must point to a trace_min_max_param structure that

 * defines where to write the value, the min and the max acceptable values,

 * and a lock to protect the write.

/*

 * trace_min_max_read - Read a u64 value from a trace_min_max_param struct

 * @filp: The active open file structure

 * @ubuf: The userspace provided buffer to read value into

 * @cnt: The maximum number of bytes to read

 * @ppos: The current "file" position

 *

 * This function implements the read interface for a struct trace_min_max_param.

 * The filp->private_data must point to a trace_min_max_param struct with valid

 * data.

 ptr to loc-specific array of err strings */

 index into errs -> specific err string */

 MAX_FILTER_STR_VAL = 256 */

 err location */

 what caused err */

/**

 * err_pos - find the position of a string within a command for error careting

 * @cmd: The tracing command that caused the error

 * @str: The string to position the caret at within @cmd

 *

 * Finds the position of the first occurrence of @str within @cmd.  The

 * return value can be passed to tracing_log_err() for caret placement

 * within @cmd.

 *

 * Returns the index within @cmd of the first occurrence of @str or 0

 * if @str was not found.

/**

 * tracing_log_err - write an error to the tracing error log

 * @tr: The associated trace array for the error (NULL for top level array)

 * @loc: A string describing where the error occurred

 * @cmd: The tracing command that caused the error

 * @errs: The array of loc-specific static error strings

 * @type: The index into errs[], which produces the specific static err string

 * @pos: The position the caret should be placed in the cmd

 *

 * Writes an error into tracing/error_log of the form:

 *

 * <loc>: error: <text>

 *   Command: <cmd>

 *              ^

 *

 * tracing/error_log is a small log file containing the last

 * TRACING_LOG_ERRS_MAX errors (8).  Memory for errors isn't allocated

 * unless there has been a tracing error, and the error log can be

 * cleared and have its memory freed by writing the empty string in

 * truncation mode to it i.e. echo > tracing/error_log.

 *

 * NOTE: the @errs array along with the @type param are used to

 * produce a static error string - this string is not copied and saved

 * when the error is logged - only a pointer to it is saved.  See

 * existing callers for examples of how static strings are typically

 * defined for use with tracing_log_err().

 If this file was opened for write, then erase contents */

 Force reading ring buffer for first read */

 Do we have previous read data to read? */

 Pipe buffer operations for a buffer. */

/*

 * Callback from splice_to_pipe(), if we need to release some pages

 * at the end of the spd in case we error'ed out in filling the pipe.

 did we read anything? */

 local or global for trace_clock */

 counter or tsc mode for trace_clock */

 256 should be plenty to hold the amount needed */

 CONFIG_DYNAMIC_FTRACE */

 hash funcs only work with set_ftrace_filter */

	/*

	 * We use the callback data field (which is a pointer)

	 * as our counter.

 defined(CONFIG_TRACER_SNAPSHOT) && defined(CONFIG_DYNAMIC_FTRACE) */

 Top directory uses NULL as the parent */

 All sub buffers have a descriptor */

 See tracing_get_cpu() */

 30 characters should be more than enough */

 per cpu trace_pipe */

 per cpu trace */

 Let selftest have access to static functions in this file */

/*

 * In order to pass in both the trace_array descriptor as well as the index

 * to the flag that the trace option file represents, the trace_array

 * has a character array of trace_flags_index[], which holds the index

 * of the bit for the flag it represents. index[0] == 0, index[1] == 1, etc.

 * The address of this character array is passed to the flag option file

 * read/write callbacks.

 *

 * In order to extract both the index and the trace_array descriptor,

 * get_tr_index() uses the following algorithm.

 *

 *   idx = *ptr;

 *

 * As the pointer itself contains the address of the index (remember

 * index[1] == 1).

 *

 * Then to get the trace_array descriptor, by subtracting that index

 * from the ptr, we get to the start of the index itself.

 *

 *   ptr - idx == &index[0]

 *

 * Then a simple container_of() from that pointer gets us to the

 * trace_array descriptor.

	/*

	 * If this is an instance, only create flags for tracers

	 * the instance may have.

 Make sure there's no duplicate flags. */

 do nothing */

 Allocate the first page for all buffers */

	/*

	 * Only the top level trace array gets its snapshot allocated

	 * from the kernel command line.

 Used by the trace options files */

 Must have trace_types_lock held */

/**

 * trace_array_get_by_name - Create/Lookup a trace array, given its name.

 * @name: The name of the trace array to be looked up/created.

 *

 * Returns pointer to trace array with given name.

 * NULL, if it cannot be created.

 *

 * NOTE: This function increments the reference counter associated with the

 * trace array returned. This makes sure it cannot be freed while in use.

 * Use trace_array_put() once the trace array is no longer needed.

 * If the trace_array is to be freed, trace_array_destroy() needs to

 * be called after the trace_array_put(), or simply let user space delete

 * it from the tracefs instances directory. But until the

 * trace_array_put() is called, user space can not delete it.

 *

 Reference counter for a newly created trace array = 1. */

 Disable all the flags that were enabled coming in */

 Making sure trace array exists before destroying it. */

	/*

	 * To maintain backward compatibility for tools that mount

	 * debugfs to get to the tracing facility, tracefs is automatically

	 * mounted to the debugfs/tracing directory.

/**

 * tracing_init_dentry - initialize top level trace array

 *

 * This is called when creating files or directories in the tracing

 * directory. It is called via fs_initcall() by any of the boot up code

 * and expects to return the dentry of the top level tracing directory.

 The top level trace array uses  NULL as parent */

	/*

	 * As there may still be users that expect the tracing

	 * files to exist in debugfs/tracing, we must automount

	 * the tracefs file system there, so older tools still

	 * work with the newer kernel.

 Do work here */

 Make sure the eval map updates are finished */

	/*

	 * Modules with bad taint do not have events created, do

	 * not bother with enums either.

 CONFIG_TRACE_EVAL_MAP_FILE */

 CONFIG_MODULES */

 priority: INT_MAX >= x >= 0 */

/*

 * printk is set to max of 1024, we really don't need it that big.

 * Nothing should be printing 1000 characters anyway.

/*

 * Define here KERN_TRACE so that we have one place to modify

 * it if we decide to change what log level the ftrace dump

 * should be at.

 Probably should print a warning here. */

	/*

	 * More paranoid code. Although the buffer size is set to

	 * PAGE_SIZE, and TRACE_MAX_PRINT is 1000, this is just

	 * an extra layer of protection.

 should be zero ended, but we are paranoid. */

 Annotate start of buffers if we had overruns */

 Output in nanoseconds only if we are using a clock in nanoseconds. */

 use static because iter can be a bit big for the stack */

 Only allow one dump user at a time. */

	/*

	 * Always turn off tracing when we dump.

	 * We don't need to show trace output of what happens

	 * between multiple crashes.

	 *

	 * If the user does a sysrq-z, then they can re-enable

	 * tracing with echo 1 > tracing_on.

 Simulate the iterator */

 Can not use kmalloc for iter.temp and iter.fmt */

 don't look at user memory in panic mode */

 Did function tracer already get disabled? */

	/*

	 * We need to stop all tracing on all CPUS to read

	 * the next buffer. This is a bit expensive, but is

	 * not done often. We fill all what we can read,

	 * and then release the locks again.

 This can accept WRITE_BUFSIZE - 2 ('\n' + '\0') */

 Remove comments */

	/*

	 * Make sure we don't accidentally add more trace options

	 * than we have bits for.

 Only allocate trace_printk buffers if a trace_printk exists */

 Must be called before global_trace.buffer is allocated */

 To save memory, keep the ring buffer size to its minimum */

	/*

	 * The prepare callbacks allocates some memory for the ring buffer. We

	 * don't free the buffer if the CPU goes down. If we were to free

	 * the buffer, then the user would lose any trace that was in the

	 * buffer. The memory will be removed once the "instance" is removed.

 Used for event triggers */

 TODO: make the number of buffers hot pluggable with CPUS */

	/*

	 * register_tracer() might reference current_trace, so it

	 * needs to be set before we register anything. This is

	 * just a bootstrap of current_trace anyway.

 Function tracing may start here (via kernel command line) */

 All seems OK, enable tracing */

	/*

	 * The default tracer at boot buffer is an init section.

	 * This function is called in lateinit. If we did not

	 * find the boot tracer, then clear it out, to prevent

	 * later registration from accessing the buffer that is

	 * about to be freed.

 sched_clock_stable() is determined in late_initcall */

 SPDX-License-Identifier: GPL-2.0

/*

 * preemptoff and irqoff tracepoints

 *

 * Copyright (C) Joel Fernandes (Google) <joel@joelfernandes.org>

 Per-cpu variable to prevent redundant calls when IRQs already off */

/*

 * Like trace_hardirqs_on() but without the lockdep invocation. This is

 * used in the low level entry code where the ordering vs. RCU is important

 * and lockdep uses a staged approach which splits the lockdep hardirq

 * tracking into a RCU on and a RCU off section.

/*

 * Like trace_hardirqs_off() but without the lockdep invocation. This is

 * used in the low level entry code where the ordering vs. RCU is important

 * and lockdep uses a staged approach which splits the lockdep hardirq

 * tracking into a RCU on and a RCU off section.

 CONFIG_TRACE_IRQFLAGS */

 SPDX-License-Identifier: GPL-2.0

/*

 * Preempt / IRQ disable delay thread to test latency tracers

 *

 * Copyright (C) 2018 Joel Fernandes (Google) <joel@joelfernandes.org>

/*

 * We create 10 different functions, so that we can get 10 different

 * backtraces.

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2017 Covalent IO, Inc. http://covalent.io

/* Devmaps primary use is as a backend map for XDP BPF helper call

 * bpf_redirect_map(). Because XDP is mostly concerned with performance we

 * spent some effort to ensure the datapath with redirect maps does not use

 * any locking. This is a quick note on the details.

 *

 * We have three possible paths to get into the devmap control plane bpf

 * syscalls, bpf programs, and driver side xmit/flush operations. A bpf syscall

 * will invoke an update, delete, or lookup operation. To ensure updates and

 * deletes appear atomic from the datapath side xchg() is used to modify the

 * netdev_map array. Then because the datapath does a lookup into the netdev_map

 * array (read-only) from an RCU critical section we use call_rcu() to wait for

 * an rcu grace period before free'ing the old data structures. This ensures the

 * datapath always has a valid copy. However, the datapath does a "flush"

 * operation that pushes any pending packets in the driver outside the RCU

 * critical section. Each bpf_dtab_netdev tracks these pending operations using

 * a per-cpu flush list. The bpf_dtab_netdev object will not be destroyed  until

 * this list is empty, indicating outstanding flush operations have completed.

 *

 * BPF syscalls may race with BPF program calls on any of the update, delete

 * or lookup operations. As noted above the xchg() operation also keep the

 * netdev_map consistent in this case. From the devmap side BPF programs

 * calling into these operations are the same as multiple user space threads

 * making system calls.

 *

 * Finally, any of the above may race with a netdev_unregister notifier. The

 * unregister notifier must search for net devices in the map structure that

 * contain a reference to the net device and remove them. This is a two step

 * process (a) dereference the bpf_dtab_netdev object in netdev_map and (b)

 * check to see if the ifindex is the same as the net_device being removed.

 * When removing the dev a cmpxchg() is used to ensure the correct dev is

 * removed, in the case of a concurrent update or delete operation it is

 * possible that the initially referenced dev is no longer in the map. As the

 * notifier hook walks the map we know that new dev references can not be

 * added by the user because core infrastructure ensures dev_get_by_index()

 * calls will fail at this point.

 *

 * The devmap_hash type is a map type which interprets keys as ifindexes and

 * indexes these using a hashmap. This allows maps that use ifindex as key to be

 * densely packed instead of having holes in the lookup array for unused

 * ifindexes. The setup and packet enqueue/send code is shared between the two

 * types of devmap; only the lookup and insertion is different.

 must be first member, due to tracepoint */

 DEVMAP type only */

 these are only used for DEVMAP_HASH type maps */

	/* check sanity of attributes. 2 value sizes supported:

	 * 4 bytes: ifindex

	 * 8 bytes: ifindex + prog fd

	/* Lookup returns a pointer straight to dev->ifindex, so make sure the

	 * verifier prevents writes from the BPF side

 Overflow check */

	/* At this point bpf_prog->aux->refcnt == 0 and this map->refcnt == 0,

	 * so the programs (can be more than one that used this map) were

	 * disconnected from events. The following synchronize_rcu() guarantees

	 * both rcu read critical sections complete and waits for

	 * preempt-disable regions (NAPI being the relevant context here) so we

	 * are certain there will be no further reads against the netdev_map and

	 * all flush operations are complete. Flush operations can only be done

	 * from NAPI context for this reason.

 Make sure prior __dev_map_entry_free() have completed. */

/* Elements are kept alive by RCU; either by rcu_read_lock() (from syscall) or

 * by local_bh_disable() (from XDP calls inside NAPI). The

 * rcu_read_lock_bh_held() below makes lockdep accept both.

 sent frames count */

		/* If ndo_xdp_xmit fails with an errno, no frames have

		 * been xmit'ed.

	/* If not all frames have been transmitted, it is our

	 * responsibility to free them

/* __dev_flush is called from xdp_do_flush() which _must_ be signalled from the

 * driver before returning from its napi->poll() routine. See the comment above

 * xdp_do_flush() in filter.c.

/* Elements are kept alive by RCU; either by rcu_read_lock() (from syscall) or

 * by local_bh_disable() (from XDP calls inside NAPI). The

 * rcu_read_lock_bh_held() below makes lockdep accept both.

/* Runs in NAPI, i.e., softirq under local_bh_disable(). Thus, safe percpu

 * variable access, and map elements stick around. See comment above

 * xdp_do_flush() in filter.c.

	/* Ingress dev_rx will be the same for all xdp_frame's in

	 * bulk_queue, because bq stored per-CPU and must be flushed

	 * from net_device drivers NAPI func end.

	 *

	 * Do the same with xdp_prog and flush_list since these fields

	 * are only ever modified together.

/* Get ifindex of each upper device. 'indexes' must be able to hold at

 * least MAX_NEST_DEV elements.

 * Returns the number of ifindexes added.

 we only need n-1 clones; last_dst enqueued below */

 BPF_MAP_TYPE_DEVMAP_HASH */

 we only need n-1 clones; last_dst enqueued below */

 consume the last copy of the frame */

 dtab is empty */

	/* Redirect has already succeeded semantically at this point, so we just

	 * return 0 even if packet is dropped. Helper below takes care of

	 * freeing skb.

 we only need n-1 clones; last_dst enqueued below */

 BPF_MAP_TYPE_DEVMAP_HASH */

 we only need n-1 clones; last_dst enqueued below */

 consume the first skb and return */

 dtab is empty */

 already verified value_size <= sizeof val */

 can not specify fd if ifindex is 0 */

	/* Use call_rcu() here to ensure rcu critical sections have completed

	 * Remembering the driver side flush operation will happen before the

	 * net device is removed.

 already verified value_size <= sizeof val */

 will be freed in free_netdev() */

		/* This rcu_read_lock/unlock pair is needed because

		 * dev_map_list is an RCU list AND to ensure a delete

		 * operation does not free a netdev_map entry while we

		 * are comparing it against the netdev being unregistered.

 Assure tracepoint shadow struct _bpf_dtab_netdev is in sync */

 SPDX-License-Identifier: GPL-2.0-only

 Copyright (c) 2019 Facebook */

 dummy _ops. The verifier will operate on target program's ops. */

 btf_vmlinux has ~22k attachable functions. 1k htab is enough. */

 serializes access to trampoline_table */

	/* Keep image as writeable. The alternative is to keep flipping ro/rw

	 * everytime new program is attached or detached.

 first time registering */

 callback, fexit step 3 or fentry step 2 */

 callback, fexit step 2. Called after percpu_ref_kill confirms. */

 callback, fexit or fentry step 1 */

 the case of fmod_ret/fexit trampoline and CONFIG_PREEMPTION=y */

 the case of fentry trampoline */

	/* The trampoline image that calls original function is using:

	 * rcu_read_lock_trace to protect sleepable bpf progs

	 * rcu_read_lock to protect normal bpf progs

	 * percpu_ref to protect trampoline itself

	 * rcu tasks to protect trampoline asm not covered by percpu_ref

	 * (which are few asm insns before __bpf_tramp_enter and

	 *  after __bpf_tramp_exit)

	 *

	 * The trampoline is unreachable before bpf_tramp_image_put().

	 *

	 * First, patch the trampoline to avoid calling into fexit progs.

	 * The progs will be freed even if the original function is still

	 * executing or sleeping.

	 * In case of CONFIG_PREEMPT=y use call_rcu_tasks() to wait on

	 * first few asm instructions to execute and call into

	 * __bpf_tramp_enter->percpu_ref_get.

	 * Then use percpu_ref_kill to wait for the trampoline and the original

	 * function to finish.

	 * Then use call_rcu_tasks() to make sure few asm insns in

	 * the trampoline epilogue are done as well.

	 *

	 * In !PREEMPT case the task that got interrupted in the first asm

	 * insns won't go through an RCU quiescent state which the

	 * percpu_ref_kill will be waiting for. Hence the first

	 * call_rcu_tasks() is not necessary.

	/* The trampoline without fexit and fmod_ret progs doesn't call original

	 * function and doesn't use percpu_ref.

	 * Use call_rcu_tasks_trace() to wait for sleepable progs to finish.

	 * Then use call_rcu_tasks() to wait for the rest of trampoline asm

	 * and normal progs.

 progs already running at this address */

 first time registering */

			/* The function returns void, we cannot modify its

			 * return value.

		/* cannot attach fentry/fexit if extension prog is attached.

		 * cannot overwrite extension prog either.

 Cannot attach extension if fentry/fexit are in use. */

 prog already linked */

 bpf_trampoline_unlink_prog() should never fail. */

	/* This code will be executed even when the last bpf_tramp_image

	 * is alive. All progs are detached from the trampoline and the

	 * trampoline image is patched with jmp into epilogue to skip

	 * fexit progs. The fentry-only trampoline will be freed via

	 * multiple rcu callbacks.

/* The logic is similar to bpf_prog_run(), but with an explicit

 * rcu_read_lock() and migrate_disable() which are required

 * for the trampoline. The macro is split into

 * call __bpf_prog_enter

 * call prog->bpf_func

 * call __bpf_prog_exit

 *

 * __bpf_prog_enter returns:

 * 0 - skip execution of the bpf prog

 * 1 - execute bpf prog

 * [2..MAX_U64] - execute bpf prog and record execution time.

 *     This is start time.

	    /* static_key could be enabled in __bpf_prog_enter*

	     * and disabled in __bpf_prog_exit*.

	     * And vice versa.

	     * Hence check that 'start' is valid.

/*

 * Copyright (C) 2017-2018 Netronome Systems, Inc.

 *

 * This software is licensed under the GNU General License Version 2,

 * June 1991 as shown in the file COPYING in the top-level directory of this

 * source tree.

 *

 * THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS"

 * WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING,

 * BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS

 * FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE

 * OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME

 * THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

/* Protects offdevs, members of bpf_offload_netdev and offload members

 * of all progs.

 * RTNL lock cannot be taken when holding this lock.

 Make sure BPF_PROG_GET_NEXT_ID can't find this dead program */

 Caller must make sure netdev is valid */

 Make sure BPF_MAP_GET_NEXT_ID can't find this dead map */

 Try to move the objects to another netdev of the device */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com

/*

 * If we're handed a bigger struct than we know of, ensure all the unknown bits

 * are 0 - i.e. new user-space does not rely on any kernel feature extensions

 * we don't know about yet.

 *

 * There is a ToCToU between this function call and the following

 * copy_from_user() call. However, this is not a concern since this function is

 * meant to be a future-proofing of bits.

 silly large */

	/* Wait for any running BPF programs to complete so that

	 * userspace, when we return to it, knows that all programs

	 * that could be running use the new map value.

 Need to create a kthread, thus must support schedule */

 rcu_read_lock() is not needed */

 struct_ops map requires directly updating "value" */

 lock 'ptr' and copy everything but lock */

 mask lock and timer, since value wasn't zero inited */

/* Please, do not use this function outside from the map creation path

 * (e.g. in map update path) without taking care of setting the active

 * memory cgroup (see at bpf_map_kmalloc_node() for example).

	/* We really just want to fail instead of triggering OOM killer

	 * under memory pressure, therefore we set __GFP_NORETRY to kmalloc,

	 * which is used for lower order allocation requests.

	 *

	 * It has been observed that higher order allocation requests done by

	 * vmalloc with __GFP_NORETRY being set might fail due to not trying

	 * to reclaim memory from the page cache, thus we set

	 * __GFP_RETRY_MAYFAIL to avoid such situations.

 kmalloc()'ed memory can't be mmap()'ed */

	/* Some map creation flags are not tied to the map object but

	 * rather to the map fd instead, so they have no meaning upon

	 * map object inspection since multiple file descriptors with

	 * different (access) properties can exist here. Thus, given

	 * this has zero meaning for the map itself, lets clear these

	 * from here.

	/* Offloaded maps are removed from the IDR store when their device

	 * disappears - even if someone holds an fd to them they are unusable,

	 * the memory is gone, all ops will fail; they are simply waiting for

	 * refcnt to drop to be freed.

 called from workqueue */

 implementation dependent freeing */

/* decrement map refcnt and schedule it for freeing via workqueue

 * (unrelying map implementation ops->map_free() might sleep)

 bpf_map_free_id() must be called first */

	/* Our file permissions may have been overridden by global

	 * map permissions facing syscall side.

/* Provides an approximation of the map's memory footprint.

 * Used only to provide a backward compatibility and display

 * a reasonable "memlock" info.

	/* We need this handler such that alloc_file() enables

	 * f_mode with FMODE_CAN_READ.

	/* We need this handler such that alloc_file() enables

	 * f_mode with FMODE_CAN_WRITE.

 called for any extra memory-mapped regions (except initial) */

 called for all unmapped memory region (including initial) */

		/* map is meant to be read-only, so do not allow mapping as

		 * writable, because it's possible to leak a writable page

		 * reference and allows user-space to still modify it after

		 * freezing, while verifier will assume contents do not change

 set default open/close callbacks */

 disallow re-mapping with PROT_WRITE */

 helper macro to check that unused fields 'union bpf_attr' are zero */

/* dst and src must have at least "size" number of bytes.

 * Return strlen on success and < 0 on error.

 Copy all isalnum(), '_' and '.' chars. */

 No '\0' found in "size" number of bytes */

 Some maps allow key to be unspecified. */

 called via syscall */

 find map type and init map: hashtable vs rbtree vs bloom vs ... */

	    /* Even the map's value is a kernel's struct,

	     * the bpf_prog.o must have BTF to begin with

	     * to figure out the corresponding kernel's

	     * counter part.  Thus, attr->btf_fd has

	     * to be valid also.

		/* failed to allocate fd.

		 * bpf_map_put_with_uref() is needed because the above

		 * bpf_map_alloc_id() has published the map

		 * to the userspace and the userspace may

		 * have refcnt-ed it through BPF_MAP_GET_FD_BY_ID.

/* if error is returned, fd is released.

 * On success caller should complete fd access with matching fdput()

 map_idr_lock should have been held */

 last field in 'union bpf_attr' used by this command */

 These maps require sleepable context */

 last field in 'union bpf_attr' used by this command */

 bpf_map_do_batch() guarantees ufd is valid */

 id is in [1, INT_MAX) */

	/* cBPF to eBPF migrations are currently not in the idr store.

	 * Offloaded programs are removed from the store when their device

	 * disappears - even if someone grabs an fd to them they are unusable,

	 * simply waiting for refcnt to drop to be freed.

 bpf_prog_free_id() must be called first */

	/* Only to be used for undoing previous bpf_prog_add() in some

	 * error path. We still know that another entity in our call

	 * path holds a reference to the program, thus atomic_sub() can

	 * be safely used in such cases!

 prog_idr_lock should have been held */

 not an attachment, just a refcount inc, always allow */

/* Initially all BPF programs could be loaded w/o specifying

 * expected_attach_type. Later for some of them specifying expected_attach_type

 * at load time became required so that program could be validated properly.

 * Programs of types that are allowed to be loaded both w/ and w/o (for

 * backward compatibility) expected_attach_type, should have the default attach

 * type assigned to expected_attach_type for the latter case, so that it can be

 * validated later at attach time.

 *

 * bpf_prog_load_fixup_attach_type() sets expected_attach_type in @attr if

 * prog type requires it but has some attach types that have to be backward

 * compatible.

		/* Unfortunately BPF_ATTACH_TYPE_UNSPEC enumeration doesn't

		 * exist so checking for non-zero is the way to go here.

 extends any prog */

 always unpriv */

 equivalent to SOCKET_FILTER. need CAP_BPF only */

 has access to struct sock */

 extends any prog */

 last field in 'union bpf_attr' used by this command */

 copy eBPF program license from user space */

 eBPF programs must be GPL compatible to use GPL-ed functions */

	/* attach_prog_fd/attach_btf_obj_fd can specify fd of either bpf_prog

	 * or btf, we need to check which one it is

				/* attaching through specifying bpf_prog's BTF

				 * objects directly might be supported eventually

 fall back to vmlinux BTF, if BTF type ID is specified */

 plain bpf_prog allocation */

 find program type: socket_filter vs tracing_filter */

 run eBPF verifier */

	/* Upon success of bpf_prog_alloc_id(), the BPF prog is

	 * effectively publicly exposed. However, retrieving via

	 * bpf_prog_get_fd_by_id() will take another reference,

	 * therefore it cannot be gone underneath us.

	 *

	 * Only for the time /after/ successful bpf_prog_new_fd()

	 * and before returning to userspace, we might just hold

	 * one reference and any parallel close on that fd could

	 * rip everything out. Hence, below notifications must

	 * happen before bpf_prog_new_fd().

	 *

	 * Also, any failure handling from this point onwards must

	 * be using bpf_prog_put() given the program is exposed.

	/* In case we have subprogs, we need to wait for a grace

	 * period before we can tear down JIT memory since symbols

	 * are already exposed under kallsyms.

/* Clean up bpf_link and corresponding anon_inode file and FD. After

 * anon_inode is created, bpf_link can't be just kfree()'d due to deferred

 * anon_inode's release() call. This helper marksbpf_link as

 * defunct, releases anon_inode file and puts reserved FD. bpf_prog's refcnt

 * is not decremented, it's the responsibility of a calling code that failed

 * to complete bpf_link initialization.

 bpf_link_free is guaranteed to be called from process context */

 detach BPF program, clean up used resources */

 free bpf_link and its containing memory */

/* bpf_link_put can be called from atomic context, but ensures that resources

 * are freed from process context

/* Prepare bpf_link to be exposed to user-space by allocating anon_inode file,

 * reserving unused FD and allocating ID from link_idr. This is to be paired

 * with bpf_link_settle() to install FD and ID and expose bpf_link to

 * user-space, if bpf_link is successfully attached. If not, bpf_link and

 * pre-allocated resources are to be freed with bpf_cleanup() call. All the

 * transient state is passed around in struct bpf_link_primer.

 * This is preferred way to create and initialize bpf_link, especially when

 * there are complicated and expensive operations inbetween creating bpf_link

 * itself and attaching it to BPF hook. By using bpf_link_prime() and

 * bpf_link_settle() kernel code using bpf_link doesn't have to perform

 * expensive (and potentially failing) roll back operations in a rare case

 * that file, FD, or ID can't be allocated.

 make bpf_link fetchable by ID */

 make bpf_link fetchable by FD */

 pass through installed FD */

 tgt_prog is NULL if target is a kernel function */

 For now we only allow new targets for BPF_PROG_TYPE_EXT */

	/* There are a few possible cases here:

	 *

	 * - if prog->aux->dst_trampoline is set, the program was just loaded

	 *   and not yet attached to anything, so we can use the values stored

	 *   in prog->aux

	 *

	 * - if prog->aux->dst_trampoline is NULL, the program has already been

         *   attached to a target and its initial target was cleared (below)

	 *

	 * - if tgt_prog != NULL, the caller specified tgt_prog_fd +

	 *   target_btf_id using the link_create API.

	 *

	 * - if tgt_prog == NULL when this function was called using the old

	 *   raw_tracepoint_open API, and we need a target from prog->aux

	 *

	 * - if prog->aux->dst_trampoline and tgt_prog is NULL, the program

	 *   was detached and is going for re-attachment.

		/*

		 * Allow re-attach for TRACING and LSM programs. If it's

		 * currently linked, bpf_trampoline_link_prog will fail.

		 * EXT programs need to specify tgt_prog_fd, so they

		 * re-attach in separate code path.

		/* If there is no saved target, or the specified target is

		 * different from the destination specified at load time, we

		 * need a new trampoline and a check for compatibility

		/* The caller didn't specify a target, or the target was the

		 * same as the destination supplied during program load. This

		 * means we can reuse the trampoline and reference from program

		 * load time, and there is no need to allocate a new one. This

		 * can only happen once for any program, as the saved values in

		 * prog->aux are cleared below.

	/* Always clear the trampoline and target prog from prog->aux to make

	 * sure the original attach destination is not kept alive after a

	 * program is (re-)attached to another target.

 got extra prog ref from syscall, or attaching to different prog */

 we allocated a new trampoline, so free the old one */

 perf_event_set_bpf_prog() doesn't take its own refcnt on prog */

 CONFIG_PERF_EVENTS */

			/* The attach point for this category of programs

			 * should be specified via btf_id during program load.

			/* cg-skb progs can be loaded by unpriv user.

			 * check permissions at attach time.

 fall-through */

	/*

	 * Ensure info.*_rec_size is the same as kernel expected size

	 *

	 * or

	 *

	 * Only allow zero *_rec_size if both _rec_size and _cnt are

	 * zero.  In this case, the kernel will set the expected

	 * _rec_size back to the info.

	/* NOTE: the following code is supposed to be skipped for offload.

	 * bpf_prog_offload_info_fill() is the place to fill similar fields

	 * for offload.

			/* for multi-function programs, copy the JITed

			 * instructions for all the functions

			/* copy the address of the kernel symbol

			 * corresponding to each function

 copy the JITed image lengths for each function */

 nothing to copy, just make ubuf NULL terminated */

 ubuf can hold the string with NULL terminator */

			/* ubuf cannot hold the string with NULL terminator,

			 * do a partial copy with NULL terminator.

 before link is "settled", ID is 0, pretend it doesn't exist yet */

 Set a very high limit to avoid overflow */

 copy attributes from user space, may be less than sizeof(bpf_attr) */

	/* case BPF_PROG_TEST_RUN:

	 * is not part of this list to prevent recursive test_run

	/* When bpf program calls this helper there should not be

	 * an fdget() without matching completed fdput().

	 * This helper is allowed in the following callchain only:

	 * sys_bpf->prog_test_run->bpf_prog->bpf_sys_close

 SPDX-License-Identifier: GPL-2.0-only

 Copyright (c) 2020 Facebook */

 cached value */

 protect bpf_iter_link changes */

 incremented on every opened seq_file */

 maximum visited objects before bailing out */

/* bpf_seq_read, a customized and simpler version for bpf iterator.

 * no_llseek is assumed for this file.

 * The following are differences from seq_read():

 *  . fixed buffer size (PAGE_SIZE)

 *  . assuming no_llseek

 *  . stop() may call bpf program, handling potential overflow there

		/* object is skipped, decrease seq_num, so next

		 * valid object can reuse the same seq_num.

 got a valid next object, increase seq_num */

 bpf program called if !p */

/* The argument reg_info will be cached in bpf_iter_target_info.

 * The common practice is to declare target reg_info as

 * a const static variable and passed as an argument to

 * bpf_iter_reg_target().

	/* bpf program can only return 0 or 1:

	 *  0 : okay

	 *  1 : retry the same object

	 * The bpf_iter_run_prog() return value

	 * will be seq_ops->show() return value.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020 Facebook

 * Copyright 2020 Google LLC.

	/* Neither the bpf_prog nor the bpf-map's syscall

	 * could be modifying the local_storage->list now.

	 * Thus, no elem can be added-to or deleted-from the

	 * local_storage->list by the bpf_prog or by the bpf-map's syscall.

	 *

	 * It is racing with bpf_local_storage_map_free() alone

	 * when unlinking elem from the local_storage->list and

	 * the map's bucket->list.

		/* Always unlink from map before unlinking from

		 * local_storage.

	/* free_task_storage should always be true as long as

	 * local_storage->list was non-empty.

	/* We should be in an RCU read side critical section, it should be safe

	 * to call pid_task.

	/* We should be in an RCU read side critical section, it should be safe

	 * to call pid_task.

	/* We should be in an RCU read side critical section, it should be safe

	 * to call pid_task.

 only allocate new storage, when the task is refcounted */

	/* This helper must only be called from places where the lifetime of the task

	 * is guaranteed. Either by being refcounted or by being protected

	 * by an RCU read-side critical section.

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 Facebook

 cannot lock any per cpu lock, try extralist */

 No locking required as this is not visible yet. */

 per cpu lists are all empty, try extralist */

 cannot pop from per cpu lists, try extralist */

 SPDX-License-Identifier: (GPL-2.0-only OR BSD-2-Clause)

/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com

 * Copyright (c) 2016 Facebook

 BPF_NOSPEC, no UAPI */) {

			/* At this point, we already made sure that the second

			 * part of the ldimm64 insn is accessible.

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com

 * Copyright (c) 2016,2017 Facebook

 Called from syscall */

 check sanity of attributes */

		/* if value_size is bigger, the user space won't be able to

		 * access the elements.

	/* On 32 bit archs roundup_pow_of_two() with max_entries that has

	 * upper most bit set in u32 space is undefined behavior due to

	 * resulting 1U << 32, so do it manually here in u64 space.

		/* round up array size to nearest power of 2,

		 * since cpu will speculate within index_mask limits

 Check for overflows. */

		/* rely on vmalloc() to return page-aligned memory and

		 * ensure array->value is exactly page-aligned

 allocate all map elements and zero-initialize them */

 kmalloc'ed memory can't be mmap'ed, use explicit vmalloc */

 copy mandatory map attributes */

 Called from syscall or from eBPF program */

 emit BPF instructions equivalent to C code of array_map_lookup_elem() */

 Called from eBPF program */

	/* per_cpu areas are zero-filled and bpf programs can only

	 * access 'value_size' of them, so copying rounded areas

	 * will not leak any kernel data

 Called from syscall */

 Called from syscall or from eBPF program */

 unknown flags */

 all elements were pre-allocated, cannot insert a new one */

 all elements already exist */

 unknown flags */

 all elements were pre-allocated, cannot insert a new one */

 all elements already exist */

	/* the user space will provide round_up(value_size, 8) bytes that

	 * will be copied into per-cpu area. bpf programs can only access

	 * value_size of it. During lookup the same extra bytes will be

	 * returned or zeros which were zero-filled by percpu_alloc,

	 * so no kernel data leaks possible

 Called from syscall or from eBPF program */

 Called when map->refcnt goes to zero, either from workqueue or from syscall */

 One exception for keyless BTF: .bss/.data/.rodata map */

	/* bpf array can only take a u32 key. This check makes sure

	 * that the btf matches the attr used during map_create.

 return value: 0 - continue, 1 - stop and return */

 only file descriptors can be stored in this type of map */

 Program read-only/write-only not supported for special maps yet. */

 make sure it's empty */

 only called from syscall */

 only called from syscall */

 decrement refcnt of all bpf_progs that are stored in this map */

	/* We must track the program's aux info at this point in time

	 * since the program pointer itself may not be stable yet, see

	 * also comment in prog_array_map_poke_run().

			/* Few things to be aware of:

			 *

			 * 1) We can only ever access aux in this context, but

			 *    not aux->prog since it might not be stable yet and

			 *    there could be danger of use after free otherwise.

			 * 2) Initially when we start tracking aux, the program

			 *    is not JITed yet and also does not have a kallsyms

			 *    entry. We skip these as poke->tailcall_target_stable

			 *    is not active yet. The JIT will do the final fixup

			 *    before setting it stable. The various

			 *    poke->tailcall_target_stable are successively

			 *    activated, so tail call updates can arrive from here

			 *    while JIT is still finishing its final fixup for

			 *    non-activated poke entries.

			 * 3) On program teardown, the program's kallsym entry gets

			 *    removed out of RCU callback, but we can only untrack

			 *    from sleepable context, therefore bpf_arch_text_poke()

			 *    might not see that this is in BPF text section and

			 *    bails out with -EINVAL. As these are unreachable since

			 *    RCU grace period already passed, we simply skip them.

			 * 4) Also programs reaching refcount of zero while patching

			 *    is in progress is okay since we're protected under

			 *    poke_mutex and untrack the programs before the JIT

			 *    buffer is freed. When we're still in the middle of

			 *    patching and suddenly kallsyms entry of the program

			 *    gets evicted, we just skip the rest which is fine due

			 *    to point 3).

			 * 5) Any other error happening below from bpf_arch_text_poke()

			 *    is a unexpected bug.

				/* let other CPUs finish the execution of program

				 * so that it will not possible to expose them

				 * to invalid nop, stack unwind, nop state

/* prog_array->aux->{type,jited} is a runtime binding.

 * Doing static check alone in the verifier is not enough.

 * Thus, prog_array_map cannot be used as an inner_map

 * and map_meta_equal is not implemented.

 not used */,

 cgroup_put free cgrp after a rcu grace period */

	/* map->inner_map_meta is only accessed by syscall which

	 * is protected by fdget/fdput.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Minimal file system backend for holding eBPF maps and programs,

 * used by bpf(2) object pinning.

 *

 * Authors:

 *

 *	Daniel Borkmann <daniel@iogearbox.net>

/* bpffs_map_fops should only implement the basic

 * read operation for a BPF map.  The purpose is to

 * provide a simple user intuitive way to do

 * "cat bpffs/pathto/a-pinned-map".

 *

 * Other operations (e.g. write, lookup...) should be realized by

 * the userspace tools (e.g. bpftool) through the

 * BPF_OBJ_GET_INFO_BY_FD and the map's lookup/update

 * interface.

	/* Dots in names (e.g. "/sys/fs/bpf/foo.bar") are reserved for future

	 * extensions. That allows popoulate_bpffs() create special files.

 pin iterator link into bpffs */

/*

 * Display the mount options in /proc/mounts.

		/* We might like to report bad mount options here, but

		 * traditionally we've ignored all mount options, so we'd

		 * better continue to ignore non-existing options for bpf.

	/* If bpf_preload.ko wasn't loaded earlier then load it now.

	 * When bpf_preload is built into vmlinux the module's __init

	 * function will populate it.

	/* And grab the reference, so the module doesn't disappear while the

	 * kernel is interacting with the kernel module and its UMD.

 now user can "rmmod bpf_preload" if necessary */

	/* grab the mutex to make sure the kernel interactions with bpf_preload

	 * UMD are serialized

 if bpf_preload.ko wasn't built into vmlinux then load it */

 preload() will start UMD that will load BPF iterator programs */

			/* do not unlink successfully pinned links even

			 * if later link fails to pin

 finish() will tell UMD process to exit */

/*

 * Set up the filesystem mount context.

 SPDX-License-Identifier: GPL-2.0-only

 Copyright (c) 2020 Facebook */

 non-mmap()'able part of bpf_ringbuf (everything up to consumer page) */

 consumer page and producer page */

/* Maximum size of ring buffer area is limited by 32-bit page offset within

 * record header, counted in pages. Reserve 8 bits for extensibility, and take

 * into account few extra pages for consumer/producer pages and

 * non-mmap()'able parts. This gives 64GB limit, which seems plenty for single

 * ring buffer.

	/* Consumer and producer counters are put into separate pages to allow

	 * mapping consumer page as r/w, but restrict producer page to r/o.

	 * This protects producer position from being modified by user-space

	 * application and ruining in-kernel position tracking.

 8-byte ring buffer record header structure */

	/* Each data page is mapped twice to allow "virtual"

	 * continuous read of samples wrapping around the end of ring

	 * buffer area:

	 * ------------------------------------------------------

	 * | meta pages |  real data pages  |  same data pages  |

	 * ------------------------------------------------------

	 * |            | 1 2 3 4 5 6 7 8 9 | 1 2 3 4 5 6 7 8 9 |

	 * ------------------------------------------------------

	 * |            | TA             DA | TA             DA |

	 * ------------------------------------------------------

	 *                               ^^^^^^^

	 *                                  |

	 * Here, no need to worry about special handling of wrapped-around

	 * data due to double-mapped data pages. This works both in kernel and

	 * when mmap()'ed in user-space, simplifying both kernel and

	 * user-space implementations significantly.

 on 32-bit arch, it's impossible to overflow record's hdr->pgoff */

	/* copy pages pointer and nr_pages to local variable, as we are going

	 * to unmap rb itself with vunmap() below

 allow writable mapping for the consumer_pos only */

 remap_vmalloc_range() checks size and offset constraints */

/* Given pointer to ring buffer record metadata and struct bpf_ringbuf itself,

 * calculate offset from record metadata to ring buffer in pages, rounded

 * down. This page offset is stored as part of record metadata and allows to

 * restore struct bpf_ringbuf * from record pointer. This page offset is

 * stored at offset 4 of record metadata header.

/* Given pointer to ring buffer record header, restore pointer to struct

 * bpf_ringbuf itself by using page offset stored at offset 4

	/* check for out of ringbuf space by ensuring producer position

	 * doesn't advance more than (ringbuf_size - 1) ahead

 pairs with consumer's smp_load_acquire() */

 update record header with correct final size prefix */

	/* if consumer caught up and is waiting for our record, notify about

	 * new data availability

 discard */);

 discard */);

 discard */);

SPDX-License-Identifier: GPL-2.0

	/* per_cpu areas are zero-filled and bpf programs can only

	 * access 'value_size' of them, so copying rounded areas

	 * will not leak any kernel data

	/* the user space will provide round_up(value_size, 8) bytes that

	 * will be copied into per-cpu area. bpf programs can only access

	 * value_size of it. During lookup the same extra bytes will be

	 * returned or zeros which were zero-filled by percpu_alloc,

	 * so no kernel data leaks possible

	/* percpu is bound by PCPU_MIN_UNIT_SIZE, non-percu

	 * is the same as other local storages.

 max_entries is not used and enforced to be 0 */

 copy mandatory map attributes */

		/* Key is expected to be of struct bpf_cgroup_storage_key type,

		 * which is:

		 * struct bpf_cgroup_storage_key {

		 *	__u64	cgroup_inode_id;

		 *	__u32	attach_type;

		 * };

		/*

		 * Key_type must be a structure with two fields.

		/*

		 * The first field must be a 64 bit integer at 0 offset.

		/*

		 * The second field must be a 32 bit integer at 64 bit offset.

		/*

		 * Key is expected to be u64, which stores the cgroup_inode_id

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 Google LLC.

/* For every LSM hook that allows attachment of BPF programs, declare a nop

 * function where a BPF program can be attached.

 Mask for all the currently supported BPRM option flags */

 CONFIG_NET */

/* The set of hooks which are called without pagefaults disabled and are allowed

 * to "sleep" and thus can be used for sleepable BPF programs.

 CONFIG_SECURITY_NETWORK */

 CONFIG_KEYS */

 CONFIG_SECURITY_NETWORK */

 SPDX-License-Identifier: GPL-2.0

/*

 * queue_stack_maps.c: BPF queue and stack maps

 *

 * Copyright (c) 2018 Politecnico di Torino

 max_entries + 1 */

 Called from syscall */

 check sanity of attributes */

		/* if value_size is bigger, the user space won't be able to

		 * access the elements.

 Called when map->refcnt goes to zero, either from workqueue or from syscall */

 Called from syscall or from eBPF program */

 Called from syscall or from eBPF program */

 Called from syscall or from eBPF program */

 Called from syscall or from eBPF program */

 Called from syscall or from eBPF program */

	/* BPF_EXIST is used to force making room for a new element in case the

	 * map is full

 Check supported flags for queue and stack maps */

 advance tail pointer to overwrite oldest element */

 Called from syscall or from eBPF program */

 Called from syscall or from eBPF program */

 Called from syscall or from eBPF program */

 Called from syscall */

 SPDX-License-Identifier: GPL-2.0

/*

 * Functions to manage BPF programs attached to netns

	/* We don't hold a ref to net in order to auto-detach the link

	 * when netns is going away. Instead we rely on pernet

	 * pre_exit callback to clear this pointer. Must be accessed

	 * with netns_bpf_mutex held.

 node in list of links attached to net */

 Protects updates to netns_bpf */

 Must be called with netns_bpf_mutex held. */

	/* We can race with cleanup_net, but if we see a non-NULL

	 * struct net pointer, pre_exit has not run yet and wait for

	 * netns_bpf_mutex.

 Mark attach point as unused */

 Remember link position in case of safe delete */

 Link auto-detached or netns dying */

 Must be called with netns_bpf_mutex held. */

 Attaching prog directly is not compatible with links */

 The same program cannot be attached twice */

 Must be called with netns_bpf_mutex held. */

 Progs attached via links cannot be detached */

 Links are not compatible with attaching prog directly */

 nothing to check */

 Mark attach point as used */

 auto-detach link */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 Facebook

 irq_work to run up_read() for build_id lookup in nmi context */

 Called from syscall */

 check sanity of attributes */

 hash table size must be power of 2 */

 cannot queue more up_read, fallback */

			/*

			 * PREEMPT_RT does not allow to trylock mmap sem in

			 * interrupt disabled context. Force the fallback code.

	/*

	 * We cannot do up_read() when the irq is disabled, because of

	 * risk to deadlock with rq_lock. To do build_id lookup when the

	 * irqs are disabled, we need to run up_read() in irq_work. We use

	 * a percpu variable to do the irq_work. If the irq_work is

	 * already used by another lookup, we fall back to report ips.

	 *

	 * Same fallback is used for kernel stack (!user) on a stackmap

	 * with build_id.

 cannot access current->mm, fall back to ips */

 per entry fall back to ips */

		/* The lock will be released once we're out of interrupt

		 * context. Tell lockdep that we've released it now so

		 * it doesn't complain that we forgot to release it.

	/* stack_trace_save_tsk() works on unsigned long array, while

	 * perf_callchain_entry uses u64 array. For 32-bit systems, it is

	 * necessary to fix this mismatch.

 copy data from the end to avoid using extra buffer */

 CONFIG_STACKTRACE */

 stack_map_alloc() checks that max_depth <= sysctl_perf_event_max_stack */

	/* get_perf_callchain() guarantees that trace->nr >= init_nr

	 * and trace-nr <= sysctl_perf_event_max_stack, so trace_nr <= max_depth

 skipping more than usable stack trace */

 fast cmp */

 for build_id+offset, pop a bucket before slow cmp */

 stack_map_alloc() checks that max_depth <= sysctl_perf_event_max_stack */

 couldn't fetch the stack trace */

 perf_sample_data doesn't have callchain, use bpf_get_stackid */

 restore nr */

 user */

 cannot get valid user stack for task without user_mode regs */

 restore nr */

 user */

 Called from eBPF program */

 Called from syscall */

 Called from syscall or from eBPF program */

 Called when map->refcnt goes to zero, either from workqueue or from syscall */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com

 * Copyright (c) 2016 Facebook

 * Copyright (c) 2018 Covalent IO, Inc. http://covalent.io

/* bpf_check() is a static code analyzer that walks eBPF program

 * instruction by instruction and updates register/stack state.

 * All paths of conditional branches are analyzed until 'bpf_exit' insn.

 *

 * The first pass is depth-first-search to check that the program is a DAG.

 * It rejects the following programs:

 * - larger than BPF_MAXINSNS insns

 * - if loop is present (detected via back-edge)

 * - unreachable insns exist (shouldn't be a forest. program = one function)

 * - out of bounds or malformed jumps

 * The second pass is all possible path descent from the 1st insn.

 * Since it's analyzing all paths through the program, the length of the

 * analysis is limited to 64k insn, which may be hit even if total number of

 * insn is less then 4K, but there are too many branches that change stack/regs.

 * Number of 'branches to be analyzed' is limited to 1k

 *

 * On entry to each instruction, each register has a type, and the instruction

 * changes the types of the registers depending on instruction semantics.

 * If instruction is BPF_MOV64_REG(BPF_REG_1, BPF_REG_5), then type of R5 is

 * copied to R1.

 *

 * All registers are 64-bit.

 * R0 - return register

 * R1-R5 argument passing registers

 * R6-R9 callee saved registers

 * R10 - frame pointer read-only

 *

 * At the start of BPF program the register R1 contains a pointer to bpf_context

 * and has type PTR_TO_CTX.

 *

 * Verifier tracks arithmetic operations on pointers in case:

 *    BPF_MOV64_REG(BPF_REG_1, BPF_REG_10),

 *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, -20),

 * 1st insn copies R10 (which has FRAME_PTR) type into R1

 * and 2nd arithmetic instruction is pattern matched to recognize

 * that it wants to construct a pointer to some element within stack.

 * So after 2nd insn, the register R1 has type PTR_TO_STACK

 * (and -20 constant is saved for further stack bounds checking).

 * Meaning that this reg is a pointer to stack plus known immediate constant.

 *

 * Most of the time the registers have SCALAR_VALUE type, which

 * means the register has some value, but it's not a valid pointer.

 * (like pointer plus pointer becomes SCALAR_VALUE type)

 *

 * When verifier sees load or store instructions the type of base register

 * can be: PTR_TO_MAP_VALUE, PTR_TO_CTX, PTR_TO_STACK, PTR_TO_SOCKET. These are

 * four pointer types recognized by check_mem_access() function.

 *

 * PTR_TO_MAP_VALUE means that this register is pointing to 'map element value'

 * and the range of [ptr, ptr + map's value_size) is accessible.

 *

 * registers used to pass values to function calls are checked against

 * function argument constraints.

 *

 * ARG_PTR_TO_MAP_KEY is one of such argument constraints.

 * It means that the register type passed to this function must be

 * PTR_TO_STACK and it will be used inside the function as

 * 'pointer to map element key'

 *

 * For example the argument constraints for bpf_map_lookup_elem():

 *   .ret_type = RET_PTR_TO_MAP_VALUE_OR_NULL,

 *   .arg1_type = ARG_CONST_MAP_PTR,

 *   .arg2_type = ARG_PTR_TO_MAP_KEY,

 *

 * ret_type says that this function returns 'pointer to map elem value or null'

 * function expects 1st argument to be a const pointer to 'struct bpf_map' and

 * 2nd argument should be a pointer to stack, which will be used inside

 * the helper function as a pointer to map element key.

 *

 * On the kernel side the helper function looks like:

 * u64 bpf_map_lookup_elem(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)

 * {

 *    struct bpf_map *map = (struct bpf_map *) (unsigned long) r1;

 *    void *key = (void *) (unsigned long) r2;

 *    void *value;

 *

 *    here kernel can access 'key' and 'map' pointers safely, knowing that

 *    [key, key + map->key_size) bytes are valid and were initialized on

 *    the stack of eBPF program.

 * }

 *

 * Corresponding eBPF program may look like:

 *    BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),  // after this insn R2 type is FRAME_PTR

 *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4), // after this insn R2 type is PTR_TO_STACK

 *    BPF_LD_MAP_FD(BPF_REG_1, map_fd),      // after this insn R1 type is CONST_PTR_TO_MAP

 *    BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),

 * here verifier looks at prototype of map_lookup_elem() and sees:

 * .arg1_type == ARG_CONST_MAP_PTR and R1->type == CONST_PTR_TO_MAP, which is ok,

 * Now verifier knows that this map has key of R1->map_ptr->key_size bytes

 *

 * Then .arg2_type == ARG_PTR_TO_MAP_KEY and R2->type == PTR_TO_STACK, ok so far,

 * Now verifier checks that [R2, R2 + map's key_size) are within stack limits

 * and were initialized prior to this call.

 * If it's ok, then verifier allows this BPF_CALL insn and looks at

 * .ret_type which is RET_PTR_TO_MAP_VALUE_OR_NULL, so it sets

 * R0->type = PTR_TO_MAP_VALUE_OR_NULL which means bpf_map_lookup_elem() function

 * returns either pointer to map value or NULL.

 *

 * When type PTR_TO_MAP_VALUE_OR_NULL passes through 'if (reg != 0) goto +off'

 * insn, the register holding that pointer in the true branch changes state to

 * PTR_TO_MAP_VALUE and the same register changes state to CONST_IMM in the false

 * branch. See check_cond_jmp_op().

 *

 * After the call R0 is set to return type of the function and registers R1-R5

 * are set to NOT_INIT to indicate that they are no longer readable.

 *

 * The following reference types represent a potential reference to a kernel

 * resource which, after first being allocated, must be checked and freed by

 * the BPF program:

 * - PTR_TO_SOCKET_OR_NULL, PTR_TO_SOCKET

 *

 * When the verifier sees a helper call return a reference type, it allocates a

 * pointer id for the reference and stores it in the current function state.

 * Similar to the way that PTR_TO_MAP_VALUE_OR_NULL is converted into

 * PTR_TO_MAP_VALUE, PTR_TO_SOCKET_OR_NULL becomes PTR_TO_SOCKET when the type

 * passes through a NULL-check conditional. For the branch wherein the state is

 * changed to CONST_IMM, the verifier releases the reference.

 *

 * For each helper function that allocates a reference, such as

 * bpf_sk_lookup_tcp(), there is a corresponding release function, such as

 * bpf_sk_release(). When a reference type passes into the release function,

 * the verifier also releases the reference. If any unchecked or unreleased

 * reference remains at the end of the program, the verifier rejects it.

 verifier_state + insn_idx are pushed to stack when branch is encountered */

	/* verifer state is 'st'

	 * before processing instruction 'insn_idx'

	 * and after processing instruction 'prev_insn_idx'

 length of verifier log at the time this state was pushed on stack */

/* log_level controls verbosity level of eBPF verifier.

 * bpf_verifier_log_write() is used to dump the verification trace to the log,

 * so the user can figure out what's wrong with the program

/* Determine whether the function releases some resources allocated by another

 * function call. The first reference type argument will be assumed to be

 * released by release_reference().

 string representation of 'enum bpf_reg_type' */

/* The reg state of a pointer or a bounded scalar was saved when

 * it was spilled to the stack.

 reg->off should be 0 for SCALAR_VALUE */

				/* Typically an immediate SCALAR_VALUE, but

				 * could be a pointer whose offset is too big

				 * for reg->off

/* copy array src of length n * size bytes to dst. dst is reallocated if it's too

 * small to hold src. This is different from krealloc since we don't want to preserve

 * the contents of dst.

 *

 * Leaves dst untouched if src is NULL or length is zero. Returns NULL if memory could

 * not be allocated.

/* resize an array from old_n items to new_n items. the array is reallocated if it's too

 * small to hold new_n items. new items are zeroed out if the array grows.

 *

 * Contrary to krealloc_array, does not free arr if new_n is zero.

/* Acquire a pointer id from the env and update the state->refs to include

 * this new pointer reference.

 * On success, returns a valid pointer id to associate with the register

 * On failure, returns a negative errno.

 release function corresponding to acquire_reference_state(). Idempotent. */

/* copy verifier state from src to dst growing dst stack space

 * when necessary to accommodate larger src stack

 if dst has more stack frames then src frame, free them */

		/* WARN_ON(br > 1) technically makes sense here,

		 * but see comment in push_stack(), hence:

		/* WARN_ON(branches > 2) technically makes sense here,

		 * but

		 * 1. speculative states will bump 'branches' for non-branch

		 * instructions

		 * 2. is_state_visited() heuristics may decide not to create

		 * a new state for a sequence of branches and all such current

		 * and cloned states will be pointing to a single parent state

		 * which might have large 'branches' count.

 pop all elements and return */

 This helper doesn't clear reg->id */

/* Mark the unknown part of a register (variable offset or scalar value) as

 * known to have the value @imm.

 Clear id, off, and union(map_ptr, range) */

/* Mark the 'variable offset' part of a register as zero.  This should be

 * used only on registers holding a pointer type.

 Something bad happened, let's kill all regs */

			/* transfer reg's id which is unique for every map_lookup_elem

			 * as UID of the inner map.

 Unmodified PTR_TO_PACKET[_META,_END] register from ctx access. */

	/* The register can already have a range from prior markings.

	 * This is fine as long as it hasn't been advanced from its

	 * origin.

 Reset the min/max bounds of a register */

 min signed is max(sign bit) | min(other bits) */

 max signed is min(sign bit) | max(other bits) */

 min signed is max(sign bit) | min(other bits) */

 max signed is min(sign bit) | max(other bits) */

 Uses signed min/max values to inform unsigned, and vice-versa */

	/* Learn sign from signed bounds.

	 * If we cannot cross the sign boundary, then signed and unsigned bounds

	 * are the same, so combine.  This works even in the negative case, e.g.

	 * -3 s<= x s<= -1 implies 0xf...fd u<= x u<= 0xf...ff.

	/* Learn sign from unsigned bounds.  Signed bounds cross the sign

	 * boundary, so we must be careful.

		/* Positive.  We can't learn anything from the smin, but smax

		 * is positive, hence safe.

		/* Negative.  We can't learn anything from the smax, but smin

		 * is negative, hence safe.

	/* Learn sign from signed bounds.

	 * If we cannot cross the sign boundary, then signed and unsigned bounds

	 * are the same, so combine.  This works even in the negative case, e.g.

	 * -3 s<= x s<= -1 implies 0xf...fd u<= x u<= 0xf...ff.

	/* Learn sign from unsigned bounds.  Signed bounds cross the sign

	 * boundary, so we must be careful.

		/* Positive.  We can't learn anything from the smin, but smax

		 * is positive, hence safe.

		/* Negative.  We can't learn anything from the smax, but smin

		 * is negative, hence safe.

 Attempts to improve var_off based on unsigned min/max information */

	/* Attempt to pull 32-bit signed bounds into 64-bit bounds

	 * but must be positive otherwise set to worse case bounds

	 * and refine later from tnum.

	/* special case when 64-bit register has upper 32-bit register

	 * zeroed. Typically happens after zext or <<32, >>32 sequence

	 * allowing us to use 32-bit bounds directly,

		/* Otherwise the best we can do is push lower 32bit known and

		 * unknown bits into register (var_off set from jmp logic)

		 * then learn as much as possible from the 64-bit tnum

		 * known and unknown bits. The previous smin/smax bounds are

		 * invalid here because of jmp32 compare so mark them unknown

		 * so they do not impact tnum bounds calculation.

	/* Intersecting with the old var_off might have improved our bounds

	 * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),

	 * then new var_off is (0; 0x7f...fc) which improves our umax.

	/* Intersecting with the old var_off might have improved our bounds

	 * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),

	 * then new var_off is (0; 0x7f...fc) which improves our umax.

 Mark a register as having a completely unknown (scalar) value. */

	/*

	 * Clear type, id, off, and union(map_ptr, range) and

	 * padding between 'type' and union

 Something bad happened, let's kill all regs except FP */

 Something bad happened, let's kill all regs except FP */

 frame pointer */

 Similar to push_stack(), but for async callbacks */

	/* Unlike push_stack() do not copy_verifier_state().

	 * The caller state doesn't matter.

	 * This is async callback. It starts in a fresh stack.

	 * Initialize it similar to do_check_common().

 callsite */,

 frameno within this callchain */,

 subprog number within this prog */);

 pop all elements and return */

 register is used as source operand */

 register is used as destination operand */

 same as above, check only, don't mark */

 determine subprog starts. The end is one before the next starts */

 func_id is not greater than BTF_MAX_TYPE */

			/* In the future, this can be allowed to increase limit

			 * of fd index into fd_array, interpreted as u16.

	/* func_id == 0 is always invalid, but instead of returning an error, be

	 * conservative and wait until the code elimination pass before returning

	 * error, so that invalid calls that get pruned out can be in BPF programs

	 * loaded from userspace.  It is also required that offset be untouched

	 * for such calls.

 Add entry function. */

	/* Add a fake 'exit' subprog which could simplify subprog iteration

	 * logic. 'subprog_cnt' should not be increased.

 now check that all jumps are within the same subprog */

			/* to avoid fall-through from one subprog into another

			 * the last insn of the subprog should be either exit

			 * or unconditional jump back

/* Parentage chain of this register (or stack slot) should take care of all

 * issues like callee-saved registers, stack slot allocation time, etc.

 Observe write marks */

 if read wasn't screened by an earlier write ... */

		/* The first condition is more likely to be true than the

		 * second, checked it first.

			/* The parentage chain never changes and

			 * this parent was already marked as LIVE_READ.

			 * There is no need to keep walking the chain again and

			 * keep re-marking all parents as LIVE_READ.

			 * This case happens when the same register is read

			 * multiple times without writes into it in-between.

			 * Also, if parent has the stronger REG_LIVE_READ64 set,

			 * then no need to set the weak REG_LIVE_READ32.

 ... then we depend on parent's value */

 REG_LIVE_READ64 overrides REG_LIVE_READ32. */

/* This function is supposed to be used by the following 32-bit optimization

 * code only. It returns TRUE if the source or destination register operates

 * on 64-bit, otherwise return FALSE.

		/* BPF_EXIT for "main" will reach here. Return TRUE

		 * conservatively.

			/* BPF to BPF call will reach here because of marking

			 * caller saved clobber with DST_OP_NO_MARK for which we

			 * don't care the register def because they are anyway

			 * marked as NOT_INIT already.

			/* Helper call will reach here because of arg type

			 * check, conservatively return TRUE.

 BPF_END always use BPF_ALU class. */

 LDX source must be ptr. */

		/* BPF_STX (including atomic variants) has multiple source

		 * operands, one of which is a ptr. Check whether the caller is

		 * asking about it.

 LD_IMM64 */

 Both LD_IND and LD_ABS return 32-bit data. */

 Implicit ctx ptr. */

 Explicit source could be any width. */

 The only source register for BPF_ST is a ptr. */

 Conservatively return true at default. */

 Return the regno defined by the insn, or -1. */

 Return TRUE if INSN has defined any 32-bit value explicitly. */

 The dst will be zero extended, so won't be sub-register anymore. */

 check whether register used as source operand can be read */

 We don't need to worry about FP liveness because it's read-only */

 check whether register used as dest operand can be written to */

 for any branch, call, exit record the history of jmps in the given state */

/* Backtrack one insn at a time. If idx is not at the top of recorded

 * history then previous instruction came from straight line execution.

/* For given verifier state backtrack_insn() is called from the last insn to

 * the first insn. Its purpose is to compute a bitmask of registers and

 * stack slots that needs precision in the parent verifier state.

				/* dreg = sreg

				 * dreg needs precision after this insn

				 * sreg needs precision before this insn

				/* dreg = K

				 * dreg needs precision after this insn.

				 * Corresponding register is already marked

				 * as precise=true in this verifier state.

				 * No further markings in parent are necessary

				/* dreg += sreg

				 * both dreg and sreg need precision

				 * before this insn

			} /* else dreg += K

			   * dreg still needs precision before this insn

		/* scalars can only be spilled into stack w/o losing precision.

		 * Load from any other memory can be zero extended.

		 * The desire to keep that precision is already indicated

		 * by 'precise' mark in corresponding register of this state.

		 * No further tracking necessary.

		/* dreg = *(u64 *)[fp - off] was a fill from the stack.

		 * that [fp - off] slot contains scalar that needs to be

		 * tracked with precision

			/* stx & st shouldn't be using _scalar_ dst_reg

			 * to access memory. It means backtracking

			 * encountered a case of pointer subtraction.

 scalars can only be spilled into stack */

 regular helper call sets R0 */

				/* if backtracing was looking for registers R1-R5

				 * they should have been found already.

		/* It's ld_imm64 or ld_abs or ld_ind.

		 * For ld_imm64 no further tracking of precision

		 * into parent is necessary

 to be analyzed */

/* the scalar precision tracking algorithm:

 * . at the start all registers have precise=false.

 * . scalar ranges are tracked as normal through alu and jmp insns.

 * . once precise value of the scalar register is used in:

 *   .  ptr + scalar alu

 *   . if (scalar cond K|scalar)

 *   .  helper_call(.., scalar, ...) where ARG_CONST is expected

 *   backtrack through the verifier states and mark all registers and

 *   stack slots with spilled constants that these scalar regisers

 *   should be precise.

 * . during state pruning two registers (or spilled stack slots)

 *   are equivalent if both are not precise.

 *

 * Note the verifier cannot simply walk register parentage chain,

 * since many different registers and stack slots could have been

 * used to compute single precise scalar.

 *

 * The approach of starting with precise=true for all registers and then

 * backtrack to mark a register as not precise when the verifier detects

 * that program doesn't care about specific value (e.g., when helper

 * takes register as ARG_ANYTHING parameter) is not safe.

 *

 * It's ok to walk single parentage chain of the verifier states.

 * It's possible that this backtracking will go all the way till 1st insn.

 * All other branches will be explored for needing precision later.

 *

 * The backtracking needs to deal with cases like:

 *   R8=map_value(id=0,off=0,ks=4,vs=1952,imm=0) R9_w=map_value(id=0,off=40,ks=4,vs=1952,imm=0)

 * r9 -= r8

 * r5 = r9

 * if r5 > 0x79f goto pc+7

 *    R5_w=inv(id=0,umax_value=1951,var_off=(0x0; 0x7ff))

 * r5 += 1

 * ...

 * call bpf_perf_event_output#25

 *   where .arg5_type = ARG_CONST_SIZE_OR_ZERO

 *

 * and this case:

 * r6 = 1

 * call foo // uses callee's r6 inside to compute r0

 * r0 += r6

 * if r0 == 0 goto

 *

 * to track above reg_mask/stack_mask needs to be independent for each frame.

 *

 * Also if parent's curframe > frame where backtracking started,

 * the verifier need to mark registers in both frames, otherwise callees

 * may incorrectly prune callers. This is similar to

 * commit 7640ead93924 ("bpf: verifier: make sure callees don't prune with caller differences")

 *

 * For now backtracking falls back into conservative marking.

	/* big hammer: mark all scalars precise in this path.

	 * pop_stack may still get !precise scalars.

				/* Found assignment(s) into tracked register in this state.

				 * Since this state is already marked, just return.

				 * Nothing to be tracked further in the parent state.

				/* This can happen if backtracking reached insn 0

				 * and there are still reg_mask or stack_mask

				 * to backtrack.

				 * It means the backtracking missed the spot where

				 * particular register was initialized with a constant.

				/* the sequence of instructions:

				 * 2: (bf) r3 = r10

				 * 3: (7b) *(u64 *)(r3 -8) = r0

				 * 4: (79) r4 = *(u64 *)(r10 -8)

				 * doesn't contain jmps. It's backtracked

				 * as a single block.

				 * During backtracking insn 3 is not recognized as

				 * stack access, so at the end of backtracking

				 * stack slot fp-8 is still marked in stack_mask.

				 * However the parent state may not have accessed

				 * fp-8 and it's "unallocated" stack space.

				 * In such case fallback to conservative.

 Does this register contain a constant zero? */

 size < 8 bytes spill */

/* check_stack_{read,write}_fixed_off functions track spill/fill of registers,

 * stack boundary and alignment are checked in check_mem_access()

 stack frame we're writing to */

 state of the current function */

	/* caller checked that off % size == 0 and -MAX_BPF_STACK <= off < 0,

	 * so it's aligned access and [off, off + size) are within stack limits

			/* The backtracking logic can only recognize explicit

			 * stack slot address like [fp - 8]. Other spill of

			 * scalar via different register has to be conservative.

			 * Backtrack from here and mark all registers as precise

			 * that contributed into 'reg' being a constant.

 register containing pointer is being spilled into stack */

 regular write of data into stack destroys any spilled ptr */

 Mark slots as STACK_MISC if they belonged to spilled ptr. */

		/* only mark the slot as written if all 8 bytes were written

		 * otherwise read propagation may incorrectly stop too soon

		 * when stack slots are partially written.

		 * This heuristic means that read propagation will be

		 * conservative, since it will add reg_live_read marks

		 * to stack slots all the way to first state when programs

		 * writes+reads less than 8 bytes

 when we zero initialize stack slots mark them as such */

 backtracking doesn't work for STACK_ZERO yet. */

 Mark slots affected by this stack write. */

/* Write the stack: 'stack[ptr_regno + off] = value_regno'. 'ptr_regno' is

 * known to contain a variable offset.

 * This function checks whether the write is permitted and conservatively

 * tracks the effects of the write, considering that each stack slot in the

 * dynamic range is potentially written to.

 *

 * 'off' includes 'regno->off'.

 * 'value_regno' can be -1, meaning that an unknown value is being written to

 * the stack.

 *

 * Spilled pointers in range are not marked as written because we don't know

 * what's going to be actually written. This means that read propagation for

 * future reads cannot be terminated by this write.

 *

 * For privileged programs, uninitialized stack slots are considered

 * initialized by this write (even though we don't know exactly what offsets

 * are going to be written to). The idea is that we don't want the verifier to

 * reject future reads that access slots written to through variable offsets.

 func where register points to */

 state of the current function */

	/* set if the fact that we're writing a zero is used to let any

	 * stack slots remain STACK_ZERO

 Variable offset writes destroy any spilled pointers in range. */

			/* Reject the write if there's are spilled pointers in

			 * range. If we didn't reject here, the ptr status

			 * would be erased below (even though not all slots are

			 * actually overwritten), possibly opening the door to

			 * leaks.

 Erase all spilled pointers. */

 Update the slot type. */

		/* If the slot is STACK_INVALID, we check whether it's OK to

		 * pretend that it will be initialized by this write. The slot

		 * might not actually be written to, and so if we mark it as

		 * initialized future reads might leak uninitialized memory.

		 * For privileged programs, we will accept such reads to slots

		 * that may or may not be written because, if we're reject

		 * them, the error would be too confusing.

 backtracking doesn't work for STACK_ZERO yet. */

/* When register 'dst_regno' is assigned some values from stack[min_off,

 * max_off), we set the register's type according to the types of the

 * respective stack slots. If all the stack values are known to be zeros, then

 * so is the destination reg. Otherwise, the register is considered to be

 * SCALAR. This function does not deal with register filling; the caller must

 * ensure that all spilled registers in the stack range have been marked as

 * read.

 func where src register points to */

		/* any access_size read into register is zero extended,

		 * so the whole register == const_zero

		/* backtracking doesn't support STACK_ZERO yet,

		 * so mark it precise here, so that later

		 * backtracking can stop here.

		 * Backtracking may not need this if this register

		 * doesn't participate in pointer adjustment.

		 * Forward propagation of precise flag is not

		 * necessary either. This mark is only to stop

		 * backtracking. Any register that contributed

		 * to const 0 was marked precise before spill.

 have read misc data from the stack */

/* Read the stack at 'off' and put the results into the register indicated by

 * 'dst_regno'. It handles reg filling if the addressed stack slot is a

 * spilled reg.

 *

 * 'dst_regno' can be -1, meaning that the read value is not going to a

 * register.

 *

 * The access is assumed to be within the current stack bounds.

 func where src register points to */

				/* The earlier check_reg_arg() has decided the

				 * subreg_def for this insn.  Save it first.

 restore register state from stack */

			/* mark reg as written since spilled pointer state likely

			 * has its liveness marks cleared by is_state_visited()

			 * which resets stack/reg liveness for state transitions

			/* If dst_regno==-1, the caller is asking us whether

			 * it is acceptable to use this value as a SCALAR_VALUE

			 * (e.g. for XADD).

			 * We must not allow unprivileged callers to do that

			 * with spilled pointers.

 the access is performed by an instruction */

 the access is performed by a helper */

/* Read the stack at 'ptr_regno + off' and put the result into the register

 * 'dst_regno'.

 * 'off' includes the pointer register's fixed offset(i.e. 'ptr_regno.off'),

 * but not its variable offset.

 * 'size' is assumed to be <= reg size and the access is assumed to be aligned.

 *

 * As opposed to check_stack_read_fixed_off, this function doesn't deal with

 * filling registers (i.e. reads of spilled register cannot be detected when

 * the offset is not fixed). We conservatively mark 'dst_regno' as containing

 * SCALAR_VALUE. That's why we assert that the 'ptr_regno' has a variable

 * offset; for a fixed offset check_stack_read_fixed_off should be used

 * instead.

 The state of the source register. */

	/* Note that we pass a NULL meta, so raw access will not be permitted.

/* check_stack_read dispatches to check_stack_read_fixed_off or

 * check_stack_read_var_off.

 *

 * The caller must ensure that the offset falls within the allocated stack

 * bounds.

 *

 * 'dst_regno' is a register which will receive the value from the stack. It

 * can be -1, meaning that the read value is not going to a register.

 Some accesses are only permitted with a static offset. */

	/* The offset is required to be static when reads don't go to a

	 * register, in order to not leak pointers (see

	 * check_stack_read_fixed_off).

	/* Variable offset is prohibited for unprivileged mode for simplicity

	 * since it requires corresponding support in Spectre masking for stack

	 * ALU. See also retrieve_ptr_limit().

		/* Variable offset stack reads need more conservative handling

		 * than fixed offset ones. Note that dst_regno >= 0 on this

		 * branch.

/* check_stack_write dispatches to check_stack_write_fixed_off or

 * check_stack_write_var_off.

 *

 * 'ptr_regno' is the register used as a pointer into the stack.

 * 'off' includes 'ptr_regno->off', but not its variable offset (if any).

 * 'value_regno' is the register whose value we're writing to the stack. It can

 * be -1, meaning that we're not writing from a register.

 *

 * The caller must ensure that the offset falls within the maximum stack size.

		/* Variable offset stack reads need more conservative handling

		 * than fixed offset ones.

 check read/write into memory region (e.g., map value, ringbuf sample, etc) */

 check read/write into a memory region with possible variable offset */

	/* We may have adjusted the register pointing to memory region, so we

	 * need to try adding each of min_value and max_value to off

	 * to make sure our theoretical access will be safe.

	/* The minimum value is only important with signed

	 * comparisons where we can't assume the floor of a

	 * value is 0.  If we are using signed variables for our

	 * index'es we need to make sure that whatever we use

	 * will have a set floor within our range.

	/* If we haven't set a max value then we need to bail since we can't be

	 * sure we won't do bad things.

	 * If reg->umax_value + off could overflow, treat that as unbounded too.

 check read/write into a map element with possible variable offset */

		/* if any part of struct bpf_spin_lock can be touched by

		 * load/store reject this program.

		 * To check that [x1, x2) overlaps with [y1, y2)

		 * it is sufficient to check x1 < y2 && y1 < x2.

 Program types only with direct read access go here! */

 Program types with direct read + write access go here! */

	/* We may have added a variable offset to the packet pointer; but any

	 * reg->range we have comes after that.  We are only checking the fixed

	 * offset.

	/* We don't allow negative numbers, because we aren't tracking enough

	 * detail to prove they're safe.

	/* __check_mem_access has made sure "off + size - 1" is within u16.

	 * reg->umax_value can't be bigger than MAX_PACKET_OFF which is 0xffff,

	 * otherwise find_good_pkt_pointers would have refused to set range info

	 * that __check_mem_access would have rejected this pkt access.

	 * Therefore, "off + reg->umax_value + size - 1" won't overflow u32.

 check access to 'struct bpf_context' fields.  Supports fixed offsets only */

		/* A non zero info.ctx_field_size indicates that this field is a

		 * candidate for later verifier transformation to load the whole

		 * field and then apply a mask when accessed with a narrower

		 * access than actual ctx access size. A zero info.ctx_field_size

		 * will only allow for whole field access and rejects any other

		 * type of narrower access.

 remember the offset of last byte accessed in ctx */

 Separate to is_ctx_reg() since we still want to allow BPF_ST here. */

 Byte size accesses are always allowed. */

	/* For platforms that do not have a Kconfig enabling

	 * CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS the value of

	 * NET_IP_ALIGN is universally set to '2'.  And on platforms

	 * that do set CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS, we get

	 * to this code only in strict mode where we want to emulate

	 * the NET_IP_ALIGN==2 checking.  Therefore use an

	 * unconditional IP align value of '2'.

 Byte size accesses are always allowed. */

		/* Special case, because of NET_IP_ALIGN. Given metadata sits

		 * right in front, treat it the very same way.

		/* The stack spill tracking logic in check_stack_write_fixed_off()

		 * and check_stack_read_fixed_off() relies on stack accesses being

		 * aligned.

 update known max for given subprogram */

/* starting from main bpf function walk all instructions of the function

 * and recursively walk all callees that given function can call.

 * Ignore jump and exit insns.

 * Since recursion is prevented by check_cfg() this algorithm

 * only needs a local stack of MAX_CALL_FRAMES to remember callsites

	/* protect against potential stack overflow that might happen when

	 * bpf2bpf calls get combined with tailcalls. Limit the caller's stack

	 * depth for such case down to 256 so that the worst case scenario

	 * would result in 8k stack size (32 which is tailcall limit * 256 =

	 * 8k).

	 *

	 * To get the idea what might happen, see an example:

	 * func1 -> sub rsp, 128

	 *  subfunc1 -> sub rsp, 256

	 *  tailcall1 -> add rsp, 256

	 *   func2 -> sub rsp, 192 (total stack size = 128 + 192 = 320)

	 *   subfunc2 -> sub rsp, 64

	 *   subfunc22 -> sub rsp, 128

	 *   tailcall2 -> add rsp, 128

	 *    func3 -> sub rsp, 32 (total stack size 128 + 192 + 64 + 32 = 416)

	 *

	 * tailcall will unwind the current stack frame but it will not get rid

	 * of caller's stack as shown on the example above.

	/* round up to 32-bytes, since this is granularity

	 * of interpreter stack size

 remember insn and function to return to */

 find the callee */

 async callbacks don't increase bpf prog stack size */

	/* if tail call got detected across bpf2bpf calls then mark each of the

	 * currently present subprog frames as tail call reachable subprogs;

	 * this info will be utilized by JIT so that we will be preserving the

	 * tail call counter throughout bpf2bpf calls combined with tailcalls

	/* end of for() loop means the last insn of the 'subprog'

	 * was reached. Doesn't matter whether it was JA or EXIT

	/* Access to ctx or passing it to a helper is only allowed in

	 * its original, unmodified form.

 BPF architecture zero extends alu32 ops into 64-bit registesr */

/* truncate register to smaller size (in bytes)

 * must be called with size < BPF_REG_SIZE

 clear high bits in bit representation */

 fix arithmetic bounds */

	/* If size is smaller than 32bit register the 32bit register

	 * values are also truncated so we push 64-bit bounds into

	 * 32-bit bounds. Above were truncated < 32-bits already.

	/* A map is considered read-only if the following condition are true:

	 *

	 * 1) BPF program side cannot change any of the map content. The

	 *    BPF_F_RDONLY_PROG flag is throughout the lifetime of a map

	 *    and was set at map creation time.

	 * 2) The map value(s) have been initialized from user space by a

	 *    loader and then "frozen", such that no new map update/delete

	 *    operations from syscall side are possible for the rest of

	 *    the map's lifetime from that point onwards.

	 * 3) Any parallel/pending map update/delete operations from syscall

	 *    side have been completed. Only after that point, it's safe to

	 *    assume that map value(s) are immutable.

/* Check that the stack access at the given offset is within bounds. The

 * maximum valid offset is -1.

 *

 * The minimum valid offset is -MAX_BPF_STACK for writes, and

 * -state->allocated_stack for reads.

/* Check that the stack access at 'regno + off' falls within the maximum stack

 * bounds.

 *

 * 'off' includes `regno->offset`, but not its dynamic part (if any).

 We don't know if helpers are reading or writing (or both). */

/* check whether memory at (regno + off) is accessible for t = (read | write)

 * if t==write, value_regno is a register which value is stored into memory

 * if t==read, value_regno is a register which will receive the value from memory

 * if t==write && value_regno==-1, some unknown value is stored into memory

 * if t==read && value_regno==-1, don't care what we read from memory

 alignment checks will add in reg->off themselves */

 for access checks, reg->off is just part of off */

 if map is read-only, track its contents as scalars */

			/* ctx access returns either a scalar, or a

			 * PTR_TO_PACKET[_META,_END]. In the latter

			 * case, we know the offset is zero.

				/* A load of ctx field could have different

				 * actual load size with the one encoded in the

				 * insn. When the dst is PTR, it is for sure not

				 * a sub-register.

 Basic bounds checks. */

 b/h/w load zero-extends, mark upper bits as known 0 */

 check src1 operand */

 check src2 operand */

 Check comparison of R0 with memory location */

 check and record load of old value */

		/* This instruction accesses a memory location but doesn't

		 * actually load it into a register.

 check whether we can read the memory */

 check whether we can write into the same memory */

/* When register 'regno' is used to read the stack (either directly or through

 * a helper function) make sure that it's within stack boundary and, depending

 * on the access type, that all elements of the stack are initialized.

 *

 * 'off' includes 'regno->off', but not its dynamic part (if any).

 *

 * All registers that have been spilled on the stack in the slots within the

 * read offsets are marked as read.

	/* Some accesses can write anything into the stack, others are

	 * read-only.

		/* The bounds checks for writes are more permissive than for

		 * reads. However, if raw_mode is not set, we'll do extra

		 * checks below.

		/* Variable offset is prohibited for unprivileged mode for

		 * simplicity since it requires corresponding support in

		 * Spectre masking for stack ALU.

		 * See also retrieve_ptr_limit().

		/* Only initialized buffer on stack is allowed to be accessed

		 * with variable offset. With uninitialized buffer it's hard to

		 * guarantee that whole memory is marked as initialized on

		 * helper return since specific bounds are unknown what may

		 * cause uninitialized stack leaking.

 helper can write anything into the stack */

		/* reading any byte out of 8-byte 'spill_slot' will cause

		 * the whole slot to be marked as 'read'

 scalar_value or invalid ptr */

 Allow zero-byte read from NULL, regardless of pointer type */

		/* Assuming that the register contains a value check if the memory

		 * access is safe. Temporarily save and restore the register's state as

		 * the conversion shouldn't be visible to a caller.

/* Implementation details:

 * bpf_map_lookup returns PTR_TO_MAP_VALUE_OR_NULL

 * Two bpf_map_lookups (even with the same key) will have different reg->id.

 * For traditional PTR_TO_MAP_VALUE the verifier clears reg->id after

 * value_or_null->value transition, since the verifier only cares about

 * the range of access to valid map value pointer and doesn't care about actual

 * address of the map element.

 * For maps with 'struct bpf_spin_lock' inside map value the verifier keeps

 * reg->id > 0 after value_or_null->value transition. By doing so

 * two bpf_map_lookups will be considered two different pointers that

 * point to different bpf_spin_locks.

 * The verifier allows taking only one bpf_spin_lock at a time to avoid

 * dead-locks.

 * Since only one bpf_spin_lock is allowed the checks are simpler than

 * reg_is_refcounted() logic. The verifier needs to remember only

 * one spin_lock instead of array of acquired_refs.

 * cur_state->active_spin_lock remembers which map value element got locked

 * and clears it after bpf_spin_unlock.

 kernel subsystem misconfigured verifier */

		/* A NULL register has a SCALAR_VALUE type, so skip

		 * type checking.

 bpf_map_xxx(map_ptr) call: remember that map_ptr */

			/* Use map_uid (which is unique id of inner map) to reject:

			 * inner_map1 = bpf_map_lookup_elem(outer_map, key1)

			 * inner_map2 = bpf_map_lookup_elem(outer_map, key2)

			 * if (inner_map1 && inner_map2) {

			 *     timer = bpf_map_lookup_elem(inner_map1);

			 *     if (timer)

			 *         // mismatch would have been allowed

			 *         bpf_timer_init(timer, inner_map2);

			 * }

			 *

			 * Comparing map_ptr is enough to distinguish normal and outer maps.

		/* bpf_map_xxx(..., map_ptr, ..., key) call:

		 * check that [key, key + map->key_size) are within

		 * stack limits and initialized

			/* in function declaration map_ptr must come before

			 * map_key, so that it's verified and known before

			 * we have to check map_key here. Otherwise it means

			 * that kernel subsystem misconfigured verifier

		/* bpf_map_xxx(..., map_ptr, ..., value) call:

		 * check [value, value + map->value_size) validity

 kernel subsystem misconfigured verifier */

		/* The access to this pointer is only checked when we hit the

		 * next is_mem_size argument below.

		/* This is used to refine r0 return value bounds for helpers

		 * that enforce this value as an upper bound on return values.

		 * See do_refine_retval_range() for helpers that can refine

		 * the return value. C type of helper is u32 so we pull register

		 * bound from umax_value however, if negative verifier errors

		 * out. Only upper bounds can be learned because retval is an

		 * int type and negative retvals are allowed.

		/* The register is SCALAR_VALUE; the access check

		 * happens using its boundaries.

			/* For unprivileged variable accesses, disable raw

			 * mode so that the program is required to

			 * initialize all the memory that the helper could

			 * just partially fill up.

	/* It's not possible to get access to a locked struct sock in these

	 * contexts, so updating is safe.

 We need a two way check, first is from map perspective ... */

	/* Restrict bpf side of cpumap and xskmap, open when use-cases

	 * appear.

 ... and second from the function itself. */

	/* We only support one arg being in raw mode at the moment,

	 * which is sufficient for the helper functions we have

	 * right now.

	/* bpf_xxx(..., buf, len) call will access 'len'

	 * bytes from memory 'buf'. Both arg types need

	 * to be paired, so make sure there's no buggy

	 * helper function specification.

	/* A reference acquiring function cannot acquire

	 * another refcounted ptr.

	/* We only support one arg being unreferenced at the moment,

	 * which is sufficient for the helper functions we have right now.

/* Packet data might have moved, any old PTR_TO_PACKET[_META,_END]

 * are now invalid, so turn them into unknown SCALAR_VALUE.

 PTR_TO_PACKET_META is not supported yet */

	/* The 'reg' is pkt > pkt_end or pkt >= pkt_end.

	 * How far beyond pkt_end it goes is unknown.

	 * if (!range_open) it's the case of pkt >= pkt_end

	 * if (range_open) it's the case of pkt > pkt_end

	 * hence this pointer is at least 1 byte bigger than pkt_end

/* The pointer with the specified id has released its reference to kernel

 * resources. Identify all copies of the same pointer and clear the reference.

 after the call registers r0 - r5 were scratched */

 All global functions return a 64-bit SCALAR_VALUE */

 continue with next insn after call */

 there is no real recursion here. timer callbacks are async */

 Convert bpf_timer_set_callback() args into timer callback args */

 continue with next insn after call */

	/* callee cannot access r0, r6 - r9 for reading and has to write

	 * into its own stack before reading from it.

	 * callee can read/write into caller's stack

 remember the callsite, it will be used by bpf_exit */

 callsite */,

 frameno within this callchain */,

 subprog number within this prog */);

 Transfer references to the callee */

 only increment it after check_reg_arg() finished */

 and go analyze first insn of the callee */

	/* bpf_for_each_map_elem(struct bpf_map *map, void *callback_fn,

	 *      void *callback_ctx, u64 flags);

	 * callback_fn(struct bpf_map *map, void *key, void *value,

	 *      void *callback_ctx);

 pointer to stack or null */

 unused */

	/* copy r1 - r5 args that callee can access.  The copy includes parent

	 * pointers, which connects us up to the liveness chain

	/* bpf_timer_set_callback(struct bpf_timer *timer, void *callback_fn);

	 * callback_fn(struct bpf_map *map, void *key, void *value);

 unused */

		/* technically it's ok to return caller's stack pointer

		 * (or caller's caller's pointer) back to the caller,

		 * since these pointers are valid. Only current stack

		 * pointer will be invalid as soon as function exits,

		 * but let's be conservative

 enforce R0 return value range [0, 1]. */

 return to the caller whatever r0 had in the callee */

 Transfer references to the caller */

 clear everything in the callee */

	/* In case of read-only, some additional restrictions

	 * need to be applied in order to prevent altering the

	 * state of the map from program side.

 data must be an array of u64 */

	/* fmt being ARG_PTR_TO_CONST_STR guarantees that var_off is const

	 * and map_direct_value_addr is set.

	/* We are also guaranteed that fmt+fmt_map_off is NULL terminated, we

	 * can focus on validating the format specifiers.

 find function prototype */

 eBPF programs must be GPL compatible to use GPL-ed functions */

 With LD_ABS/IND some JITs save/restore skb from r1. */

 check args */

	/* Mark slots with STACK_MISC in case of raw mode, stack offset

	 * is inferred from register state.

	/* check that flags argument in get_local_storage(map, flags) is 0,

	 * this is required because get_local_storage() can't return an error.

 reset caller saved regs */

 helper call returns 64-bit value. */

 update return register (already marked as written above) */

 sets type to SCALAR_VALUE */

 There is no offset yet applied, variable or fixed */

		/* remember map_ptr, so that check_map_access()

		 * can check 'value_size' boundary of memory access

		 * to map element returned from bpf_map_lookup_elem()

 resolve the type size of ksym. */

		/* current BPF helper definitions are only coming from

		 * built-in code with type IDs from  vmlinux BTF

 For release_reference() */

 For mark_ptr_or_null_reg() */

 For release_reference() */

/* mark_btf_func_reg_size() is used when the reg size is determined by

 * the BTF func_proto's return value size and argument.

 Function return value */

 Function argument */

 skip for now, but return error when we find this in fixup_kfunc_call */

 Check the arguments */

 Check return type */

 else { add_kfunc_call() ensures it is btf_type_is_void(t) } */

 scalar. ensured by btf_check_kfunc_arg_match() */

 Do the add in u64, where overflow is well-defined */

 Do the add in u32, where overflow is well-defined */

 Do the sub in u64, where overflow is well-defined */

 Do the sub in u32, where overflow is well-defined */

		/* Offset 0 is out-of-bounds, but acceptable start for the

		 * left direction, see BPF_REG_FP. Also, unknown scalar

		 * offset where we would need to deal with min/max bounds is

		 * currently prohibited for unprivileged.

	/* If we arrived here from different branches with different

	 * state or limits to sanitize, then this won't work.

 Corresponding fixup done in do_misc_fixups(). */

	/* We already marked aux for masking from non-speculative

	 * paths, thus we got here in the first place. We only care

	 * to explore bad access from here.

		/* In commit phase we narrow the masking window based on

		 * the observed pointer move after the simulated operation.

		/* Limit pruning on unknown scalars to enable deep search for

		 * potential masking differences from other program paths.

	/* If we're in commit phase, we're done here given we already

	 * pushed the truncated dst_reg into the speculative verification

	 * stack.

	 *

	 * Also, when register is a known constant, we rewrite register-based

	 * operation to immediate-based, and thus do not need masking (and as

	 * a consequence, do not need to simulate the zero-truncation either).

	/* Simulate and find potential out-of-bounds access under

	 * speculative execution from truncation as a result of

	 * masking when off was not within expected range. If off

	 * sits in dst, then we temporarily need to move ptr there

	 * to simulate dst (== 0) +/-= ptr. Needed, for example,

	 * for cases where we use K-based arithmetic in one direction

	 * and truncated reg-based in the other in order to explore

	 * bad access.

	/* If we simulate paths under speculation, we don't update the

	 * insn as 'seen' such that when we verify unreachable paths in

	 * the non-speculative domain, sanitize_dead_code() can still

	 * rewrite/sanitize them.

/* check that stack access falls within stack limits and that 'reg' doesn't

 * have a variable offset.

 *

 * Variable offset is prohibited for unprivileged mode for simplicity since it

 * requires corresponding support in Spectre masking for stack ALU.  See also

 * retrieve_ptr_limit().

 *

 *

 * 'off' includes 'reg->off'.

	/* For unprivileged we require that resulting offset must be in bounds

	 * in order to be able to sanitize access later on.

/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.

 * Caller should also handle BPF_MOV case separately.

 * If we return -EACCES, caller may want to try again treating pointer as a

 * scalar.  So we only emit a diagnostic if !env->allow_ptr_leaks.

		/* Taint dst register if offset had invalid bounds derived from

		 * e.g. dead branches.

 32-bit ALU ops on pointers produce (meaningless) scalars */

 smin_val represents the known value */

	/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.

	 * The id may be overwritten later if we create a new variable offset.

 pointer types do not carry 32-bit bounds at the moment. */

		/* We can take a fixed offset as long as it doesn't overflow

		 * the s32 'off' field

 pointer += K.  Accumulate it into fixed offset */

		/* A new variable offset is created.  Note that off_reg->off

		 * == 0, since it's a scalar.

		 * dst_reg gets the pointer type and since some positive

		 * integer value was added to the pointer, give it a new 'id'

		 * if it's a PTR_TO_PACKET.

		 * this creates a new 'base' pointer, off_reg (variable) gets

		 * added into the variable offset, and we copy the fixed offset

		 * from ptr_reg.

 something was added to pkt_ptr, set range to zero */

 scalar -= pointer.  Creates an unknown scalar */

		/* We don't allow subtraction from FP, because (according to

		 * test_verifier.c test "invalid fp arithmetic", JITs might not

		 * be able to deal with it.

 pointer -= K.  Subtract it from fixed offset */

		/* A new variable offset is created.  If the subtrahend is known

		 * nonnegative, then any reg->range we had before is still good.

 Overflow possible, we know nothing */

 Overflow possible, we know nothing */

 Cannot overflow (as long as bounds are consistent) */

 something was added to pkt_ptr, set range to zero */

 bitwise ops on pointers are troublesome, prohibit. */

 other operators (e.g. MUL,LSH) produce non-pointer results */

 Overflow possible, we know nothing */

 Overflow possible, we know nothing */

 Cannot overflow (as long as bounds are consistent) */

 Overflow possible, we know nothing */

 Overflow possible, we know nothing */

 Cannot overflow (as long as bounds are consistent) */

 Ain't nobody got time to multiply that sign */

	/* Both values are positive, so we can work with unsigned and

	 * copy the result to signed (unless it exceeds S32_MAX).

 Potential overflow, we know nothing */

 Overflow possible, we know nothing */

 Ain't nobody got time to multiply that sign */

	/* Both values are positive, so we can work with unsigned and

	 * copy the result to signed (unless it exceeds S64_MAX).

 Potential overflow, we know nothing */

 Overflow possible, we know nothing */

	/* We get our minimum from the var_off, since that's inherently

	 * bitwise.  Our maximum is the minimum of the operands' maxima.

		/* Lose signed bounds when ANDing negative numbers,

		 * ain't nobody got time for that.

		/* ANDing two positives gives a positive, so safe to

		 * cast result into s64.

	/* We get our minimum from the var_off, since that's inherently

	 * bitwise.  Our maximum is the minimum of the operands' maxima.

		/* Lose signed bounds when ANDing negative numbers,

		 * ain't nobody got time for that.

		/* ANDing two positives gives a positive, so safe to

		 * cast result into s64.

 We may learn something more from the var_off */

	/* We get our maximum from the var_off, and our minimum is the

	 * maximum of the operands' minima

		/* Lose signed bounds when ORing negative numbers,

		 * ain't nobody got time for that.

		/* ORing two positives gives a positive, so safe to

		 * cast result into s64.

	/* We get our maximum from the var_off, and our minimum is the

	 * maximum of the operands' minima

		/* Lose signed bounds when ORing negative numbers,

		 * ain't nobody got time for that.

		/* ORing two positives gives a positive, so safe to

		 * cast result into s64.

 We may learn something more from the var_off */

 We get both minimum and maximum from the var32_off. */

		/* XORing two positive sign numbers gives a positive,

		 * so safe to cast u32 result into s32.

 dst_reg->var_off.value has been updated earlier */

 We get both minimum and maximum from the var_off. */

		/* XORing two positive sign numbers gives a positive,

		 * so safe to cast u64 result into s64.

	/* We lose all sign bit information (except what we can pick

	 * up from var_off)

 If we might shift our top bit out, then we know nothing */

 u32 alu operation will zext upper bits */

	/* Not required but being careful mark reg64 bounds as unknown so

	 * that we are forced to pick them up from tnum and zext later and

	 * if some path skips this step we are still safe.

	/* Special case <<32 because it is a common compiler pattern to sign

	 * extend subreg by doing <<32 s>>32. In this case if 32bit bounds are

	 * positive we know this shift will also be positive so we can track

	 * bounds correctly. Otherwise we lose all sign bit information except

	 * what we can pick up from var_off. Perhaps we can generalize this

	 * later to shifts of any length.

 If we might shift our top bit out, then we know nothing */

 scalar64 calc uses 32bit unshifted bounds so must be called first */

 We may learn something more from the var_off */

	/* BPF_RSH is an unsigned shift.  If the value in dst_reg might

	 * be negative, then either:

	 * 1) src_reg might be zero, so the sign bit of the result is

	 *    unknown, so we lose our signed bounds

	 * 2) it's known negative, thus the unsigned bounds capture the

	 *    signed bounds

	 * 3) the signed bounds cross zero, so they tell us nothing

	 *    about the result

	 * If the value in dst_reg is known nonnegative, then again the

	 * unsigned bounds capture the signed bounds.

	 * Thus, in all cases it suffices to blow away our signed bounds

	 * and rely on inferring new ones from the unsigned bounds and

	 * var_off of the result.

	/* BPF_RSH is an unsigned shift.  If the value in dst_reg might

	 * be negative, then either:

	 * 1) src_reg might be zero, so the sign bit of the result is

	 *    unknown, so we lose our signed bounds

	 * 2) it's known negative, thus the unsigned bounds capture the

	 *    signed bounds

	 * 3) the signed bounds cross zero, so they tell us nothing

	 *    about the result

	 * If the value in dst_reg is known nonnegative, then again the

	 * unsigned bounds capture the signed bounds.

	 * Thus, in all cases it suffices to blow away our signed bounds

	 * and rely on inferring new ones from the unsigned bounds and

	 * var_off of the result.

	/* Its not easy to operate on alu32 bounds here because it depends

	 * on bits being shifted in. Take easy way out and mark unbounded

	 * so we can recalculate later from tnum.

	/* Upon reaching here, src_known is true and

	 * umax_val is equal to umin_val.

	/* blow away the dst_reg umin_value/umax_value and rely on

	 * dst_reg var_off to refine the result.

	/* Upon reaching here, src_known is true and umax_val is equal

	 * to umin_val.

	/* blow away the dst_reg umin_value/umax_value and rely on

	 * dst_reg var_off to refine the result.

	/* Its not easy to operate on alu32 bounds here because it depends

	 * on bits being shifted in from upper 32-bits. Take easy way out

	 * and mark unbounded so we can recalculate later from tnum.

/* WARNING: This function does calculations on 64-bit values, but the actual

 * execution may occur on 32-bit values. Therefore, things like bitshifts

 * need extra checks in the 32-bit case.

			/* Taint dst register if offset had invalid bounds

			 * derived from e.g. dead branches.

			/* Taint dst register if offset had invalid bounds

			 * derived from e.g. dead branches.

	/* Calculate sign/unsigned bounds and tnum for alu32 and alu64 bit ops.

	 * There are two classes of instructions: The first class we track both

	 * alu32 and alu64 sign/unsigned bounds independently this provides the

	 * greatest amount of precision when alu operations are mixed with jmp32

	 * operations. These operations are BPF_ADD, BPF_SUB, BPF_MUL, BPF_ADD,

	 * and BPF_OR. This is possible because these ops have fairly easy to

	 * understand and calculate behavior in both 32-bit and 64-bit alu ops.

	 * See alu32 verifier tests for examples. The second class of

	 * operations, BPF_LSH, BPF_RSH, and BPF_ARSH, however are not so easy

	 * with regards to tracking sign/unsigned bounds because the bits may

	 * cross subreg boundaries in the alu64 case. When this happens we mark

	 * the reg unbounded in the subreg bound space and use the resulting

	 * tnum to calculate an approximation of the sign/unsigned bounds.

			/* Shifts greater than 31 or 63 are undefined.

			 * This includes shifts by a negative number.

			/* Shifts greater than 31 or 63 are undefined.

			 * This includes shifts by a negative number.

			/* Shifts greater than 31 or 63 are undefined.

			 * This includes shifts by a negative number.

 ALU32 ops are zero extended into 64bit register */

/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max

 * and var_off.

		/* Make sure ID is cleared otherwise dst_reg min/max could be

		 * incorrectly propagated into other registers by find_equal_scalars()

				/* Combining two pointers by any ALU op yields

				 * an arbitrary scalar. Disallow all math except

				 * pointer subtraction

				/* scalar += pointer

				 * This is legal, but we have to reverse our

				 * src/dest handling in computing the range

 pointer += scalar */

		/* Pretend the src is a reg with a known value, since we only

		 * need to be able to read from this state.

 pointer += K */

 Got here implies adding two SCALAR_VALUEs */

 check validity of 32-bit and 64-bit arithmetic operations */

 check src operand */

 check dest operand */

 check src operand */

 check dest operand, mark as required later */

				/* case: R1 = R2

				 * copy register state to dest reg

					/* Assign src and dst registers the same ID

					 * that will be used by find_equal_scalars()

					 * to propagate min/max range.

 R1 = (u32) R2 */

					/* Make sure ID is cleared otherwise

					 * dst_reg min/max could be incorrectly

					 * propagated into src_reg by find_equal_scalars()

			/* case: R = imm

			 * remember the value we stored into this reg

 clear any state __mark_reg_known doesn't set */

 all other ALU ops: and, sub, xor, add, ... */

 check src1 operand */

 check src2 operand */

 check dest operand */

 keep the maximum range already checked */

 This doesn't give us any range */

		/* Risk of overflow.  For instance, ptr + (1<<63) may be less

		 * than pkt_end, but that's because it's also less than pkt.

	/* Examples for register markings:

	 *

	 * pkt_data in dst register:

	 *

	 *   r2 = r3;

	 *   r2 += 8;

	 *   if (r2 > pkt_end) goto <handle exception>

	 *   <access okay>

	 *

	 *   r2 = r3;

	 *   r2 += 8;

	 *   if (r2 < pkt_end) goto <access okay>

	 *   <handle exception>

	 *

	 *   Where:

	 *     r2 == dst_reg, pkt_end == src_reg

	 *     r2=pkt(id=n,off=8,r=0)

	 *     r3=pkt(id=n,off=0,r=0)

	 *

	 * pkt_data in src register:

	 *

	 *   r2 = r3;

	 *   r2 += 8;

	 *   if (pkt_end >= r2) goto <access okay>

	 *   <handle exception>

	 *

	 *   r2 = r3;

	 *   r2 += 8;

	 *   if (pkt_end <= r2) goto <handle exception>

	 *   <access okay>

	 *

	 *   Where:

	 *     pkt_end == dst_reg, r2 == src_reg

	 *     r2=pkt(id=n,off=8,r=0)

	 *     r3=pkt(id=n,off=0,r=0)

	 *

	 * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)

	 * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)

	 * and [r3, r3 + 8-1) respectively is safe to access depending on

	 * the check.

	/* If our ids match, then we must have the same max_value.  And we

	 * don't care about the other reg's fixed offset, since if it's too big

	 * the range won't allow anything.

	 * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.

/* compute branch direction of the expression "if (reg opcode val) goto target;"

 * and return:

 *  1 - branch will be taken and "goto target" will be executed

 *  0 - branch will not be taken and fall-through to next insn

 * -1 - unknown. Example: "if (reg < 5)" is unknown when register value

 *      range [0,10]

		/* If pointer is valid tests against zero will fail so we can

		 * use this to direct branch taken.

 How can we transform "a <op> b" into "b <op> a"? */

 these stay the same */

 these swap "lesser" and "greater" (L and G in the opcodes) */

 pkt <= pkt_end */

 pkt > pkt_end */

 pkt has at last one extra byte beyond pkt_end */

 pkt < pkt_end */

 pkt >= pkt_end */

/* Adjusts the register min/max values in the case that the dst_reg is the

 * variable register that we are working on, and src_reg is a constant or we're

 * simply doing a BPF_K check.

 * In JEQ/JNE cases we also adjust the var_off values.

	/* If the dst_reg is a pointer, we can't learn anything about its

	 * variable offset from the compare (unless src_reg were a pointer into

	 * the same object, but we don't bother with that.

	 * Since false_reg and true_reg have the same type by construction, we

	 * only need to check one of them for pointerness.

		/* JEQ/JNE comparison doesn't change the register equivalence.

		 * r1 = r2;

		 * if (r1 == 42) goto label;

		 * ...

		 * label: // here both r1 and r2 are known to be 42.

		 *

		 * Hence when marking register as known preserve it's ID.

/* Same as above, but for the case that dst_reg holds a constant and src_reg is

 * the variable reg.

	/* This uses zero as "not present in table"; luckily the zero opcode,

	 * BPF_JA, can't get here.

 Regs are known to be equal, so intersect their min/max/var_off */

 We might have learned new bounds from the var_off. */

 We might have learned something about the sign bit. */

 We might have learned some bits from the bounds. */

	/* Intersecting with the old var_off might have improved our bounds

	 * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),

	 * then new var_off is (0; 0x7f...fc) which improves our umax.

		/* Old offset (both fixed and variable parts) should

		 * have been known-zero, because we don't allow pointer

		 * arithmetic on pointers that might be NULL.

			/* We don't need id and ref_obj_id from this point

			 * onwards anymore, thus we should better reset it,

			 * so that state pruning has chances to take effect.

			/* For not-NULL ptr, reg->ref_obj_id will be reset

			 * in release_reg_references().

			 *

			 * reg->id is still used by spin_lock ptr. Other

			 * than spin_lock ptr type, reg->id can be reset.

/* The logic is similar to find_good_pkt_pointers(), both could eventually

 * be folded together at some point.

		/* regs[regno] is in the " == NULL" branch.

		 * No one could have freed the reference state before

		 * doing the NULL check.

 Pointers are always 64-bit. */

 pkt_data' > pkt_end, pkt_meta' > pkt_data */

 pkt_end > pkt_data', pkt_data > pkt_meta' */

 pkt_data' < pkt_end, pkt_meta' < pkt_data */

 pkt_end < pkt_data', pkt_data > pkt_meta' */

 pkt_data' >= pkt_end, pkt_meta' >= pkt_data */

 pkt_end >= pkt_data', pkt_data >= pkt_meta' */

 pkt_data' <= pkt_end, pkt_meta' <= pkt_data */

 pkt_end <= pkt_data', pkt_data <= pkt_meta' */

 Only conditional jumps are expected to reach here. */

 check src1 operand */

 check src2 operand */

		/* If we get here with a dst_reg pointer type it is because

		 * above is_branch_taken() special cased the 0 comparison.

		/* Only follow the goto, ignore fall-through. If needed, push

		 * the fall-through branch for simulation under speculative

		 * execution.

		/* Only follow the fall-through branch, since that's where the

		 * program will go. If needed, push the goto branch for

		 * simulation under speculative execution.

	/* detect if we are comparing against a constant value so we can adjust

	 * our min/max values for our dst register.

	 * this is only legit if both are scalars (or pointers to the same

	 * object, I suppose, but we don't support that right now), because

	 * otherwise the different base pointers mean the offsets aren't

	 * comparable.

 Comparing for equality, we can combine knowledge */

	/* detect if R == 0 where R is returned from bpf_map_lookup_elem().

	 * NOTE: these optimizations below are related with pointer comparison

	 *       which will never be JMP32.

		/* Mark all identical registers in each branch as either

		 * safe or unknown depending R == 0 or R != 0 conditional.

 verify BPF_LD_IMM64 instruction */

/* verify safety of LD_ABS|LD_IND instructions:

 * - they can only appear in the programs where ctx == skb

 * - since they are wrappers of function calls, they scratch R1-R5 registers,

 *   preserve R6-R9, and store return value into R0

 *

 * Implicit input:

 *   ctx == skb == R6 == CTX

 *

 * Explicit input:

 *   SRC == any register

 *   IMM == 32-bit immediate

 *

 * Output:

 *   R0 - 8/16/32-bit skb data converted to cpu endianness

 check whether implicit source operand (register R6) is readable */

	/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as

	 * gen_ld_abs() may terminate the program at runtime, leading to

	 * reference leak.

 check explicit source operand */

 reset caller saved regs to unreadable */

	/* mark destination R0 register as readable, since it contains

	 * the value fetched from the packet.

	 * Already marked as written above.

 ld_abs load up to 32-bit skb data. */

 LSM and struct_ops func-ptr's return type could be "void" */

	/* eBPF calling convention is such that R0 is used

	 * to return the value from eBPF program.

	 * Make sure that it's readable at this time

	 * of bpf_exit, which means that program wrote

	 * something into it earlier

 enforce return zero from async callbacks like timer */

		/* freplace program can return anything as its return value

		 * depends on the to-be-replaced kernel func or bpf program.

/* non-recursive DFS pseudo code

 * 1  procedure DFS-iterative(G,v):

 * 2      label v as discovered

 * 3      let S be a stack

 * 4      S.push(v)

 * 5      while S is not empty

 * 6            t <- S.pop()

 * 7            if t is what we're looking for:

 * 8                return t

 * 9            for all edges e in G.adjacentEdges(t) do

 * 10               if edge e is already labelled

 * 11                   continue with the next edge

 * 12               w <- G.adjacentVertex(t,e)

 * 13               if vertex w is not discovered and not explored

 * 14                   label e as tree-edge

 * 15                   label w as discovered

 * 16                   S.push(w)

 * 17                   continue at 5

 * 18               else if vertex w is discovered

 * 19                   label e as back-edge

 * 20               else

 * 21                   // vertex w is explored

 * 22                   label e as forward- or cross-edge

 * 23           label t as explored

 * 24           S.pop()

 *

 * convention:

 * 0x10 - discovered

 * 0x11 - discovered and fall-through edge labelled

 * 0x12 - discovered and fall-through and branch edges labelled

 * 0x20 - explored

/* t, w, e - match pseudo-code above:

 * t - index of current instruction

 * w - next instruction

 * e - edge

 mark branch target for state pruning */

 tree-edge */

 forward- or cross-edge */

				/* It's ok to allow recursion from CFG point of

				 * view. __check_func_call() will do the actual

				 * check.

/* Visits the instruction at index t and returns one of the following:

 *  < 0 - an error occurred

 *  DONE_EXPLORING - the instruction was fully explored

 *  KEEP_EXPLORING - there is still work to be done before it is fully explored

 All non-branch instructions have a single fall-through edge. */

			/* Mark this call insn to trigger is_state_visited() check

			 * before call itself is processed by __check_func_call().

			 * Otherwise new async state will be pushed for further

			 * exploration.

 unconditional jump with single edge */

		/* unconditional jmp is not a good pruning point,

		 * but it's marked, since backtracking needs

		 * to record jmp history in is_state_visited().

		/* tell verifier to check for equivalent states

		 * after every call and jump

 conditional jump with two edges */

/* non-recursive depth-first-search to detect loops in BPF program

 * loop == back-edge in directed graph

 mark 1st insn as discovered */

 0 is the first instruction */

 cfg looks good */

 The minimum supported BTF func info size */

				/* set the size kernel expects so loader can zero

				 * out the rest of the record.

 check insn_off */

 check type_id */

 btf_func_check() already verified it during BTF load */

	/* Need to zero it in case the userspace may

	 * pass in a smaller bpf_line_info object.

		/*

		 * Check insn_off to ensure

		 * 1) strictly increasing AND

		 * 2) bounded by prog->len

		 *

		 * The linfo[0].insn_off == 0 check logically falls into

		 * the later "missing bpf_line_info for func..." case

		 * because the first linfo[0].insn_off must be the

		 * first sub also and the first sub must have

		 * subprog_info[0].start == 0.

 check %cur's range satisfies %old's */

/* If in the old state two registers had the same id, then they need to have

 * the same id in the new state as well.  But that id could be different from

 * the old state, so we need to track the mapping from old to new ids.

 * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent

 * regs with old id 5 must also have new id 9 for the new state to be safe.  But

 * regs with a different old id could still have new id 9, we don't care about

 * that.

 * So we look through our idmap to see if this old id has been seen before.  If

 * so, we require the new id to match; otherwise, we add the id pair to the map.

 Reached an empty slot; haven't seen this id before */

 We ran out of idmap slots, which should be impossible */

 liveness must not touch this register anymore */

			/* since the register is unused, clear its state

			 * to make further comparison simpler

 liveness must not touch this stack slot anymore */

 all regs in this state in all frames were already marked */

/* the parentage chains form a tree.

 * the verifier states are added to state lists at given insn and

 * pushed into state stack for future exploration.

 * when the verifier reaches bpf_exit insn some of the verifer states

 * stored in the state lists have their final liveness state already,

 * but a lot of states will get revised from liveness point of view when

 * the verifier explores other branches.

 * Example:

 * 1: r0 = 1

 * 2: if r1 == 100 goto pc+1

 * 3: r0 = 2

 * 4: exit

 * when the verifier reaches exit insn the register r0 in the state list of

 * insn 2 will be seen as !REG_LIVE_READ. Then the verifier pops the other_branch

 * of insn 2 and goes exploring further. At the insn 4 it will walk the

 * parentage chain from insn 4 into insn 2 and will mark r0 as REG_LIVE_READ.

 *

 * Since the verifier pushes the branch states as it sees them while exploring

 * the program the condition of walking the branch instruction for the second

 * time means that all states below this branch were already explored and

 * their final liveness marks are already propagated.

 * Hence when the verifier completes the search of state list in is_state_visited()

 * we can call this clean_live_states() function to mark all liveness states

 * as REG_LIVE_DONE to indicate that 'parent' pointers of 'struct bpf_reg_state'

 * will not be used.

 * This function also clears the registers and stack for states that !READ

 * to simplify state merging.

 *

 * Important note here that walking the same branch instruction in the callee

 * doesn't meant that the states are DONE. The verifier has to compare

 * the callsites

 Returns true if (rold safe implies rcur safe) */

 explored state didn't use this */

		/* two stack pointers are equal only if they're pointing to

		 * the same stack frame, since fp-8 in foo != fp-8 in bar

 explored state can't have used this */

 new val must satisfy old val knowledge */

			/* We're trying to use a pointer in place of a scalar.

			 * Even if the scalar was unbounded, this could lead to

			 * pointer leaks because scalars are allowed to leak

			 * while pointers are not. We could make this safe in

			 * special cases if root is calling us, but it's

			 * probably not worth the hassle.

		/* If the new min/max/var_off satisfy the old ones and

		 * everything else matches, we are OK.

		 * 'id' is not compared, since it's only used for maps with

		 * bpf_spin_lock inside map element and in such cases if

		 * the rest of the prog is valid for one map element then

		 * it's valid for all map elements regardless of the key

		 * used in bpf_map_lookup()

		/* a PTR_TO_MAP_VALUE could be safe to use as a

		 * PTR_TO_MAP_VALUE_OR_NULL into the same map.

		 * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-

		 * checked, doing so could have affected others with the same

		 * id, and we can't check for that because we lost the id when

		 * we converted to a PTR_TO_MAP_VALUE.

 Check our ids match any regs they're supposed to */

		/* We must have at least as much range as the old ptr

		 * did, so that any accesses which were safe before are

		 * still safe.  This is true even if old range < old off,

		 * since someone could have accessed through (ptr - k), or

		 * even done ptr -= k in a register, to get a safe access.

		/* If the offsets don't match, we can't trust our alignment;

		 * nor can we be sure that we won't fall out of range.

 id relations must be preserved */

 new val must satisfy old val knowledge */

		/* Only valid matches are exact, which memcmp() above

		 * would have accepted

 Don't know what's going on, just say it's not safe */

 Shouldn't get here; if we do, say it's not safe */

	/* walk slots of the explored stack and ignore any additional

	 * slots in the current stack, since explored(safe) state

	 * didn't use them

 explored state didn't use this */

		/* explored stack has more populated slots than current stack

		 * and these slots were used

		/* if old state was safe with misc data in the stack

		 * it will be safe with zero-initialized stack.

		 * The opposite is not true

			/* Ex: old explored (safe) state has STACK_SPILL in

			 * this stack slot, but current has STACK_MISC ->

			 * this verifier states are not equivalent,

			 * return false to continue verification of this path

			/* when explored and current stack slot are both storing

			 * spilled registers, check that stored pointers types

			 * are the same as well.

			 * Ex: explored safe path could have stored

			 * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -8}

			 * but current path has stored:

			 * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -16}

			 * such verifier states are not equivalent.

			 * return false to continue verification of this path

/* compare two verifier states

 *

 * all states stored in state_list are known to be valid, since

 * verifier reached 'bpf_exit' instruction through them

 *

 * this function is called when verifier exploring different branches of

 * execution popped from the state stack. If it sees an old state that has

 * more strict register state and more strict stack state then this execution

 * branch doesn't need to be explored further, since verifier already

 * concluded that more strict state leads to valid finish.

 *

 * Therefore two states are equivalent if register state is more conservative

 * and explored stack state is more conservative than the current one.

 * Example:

 *       explored                   current

 * (slot1=INV slot2=MISC) == (slot1=MISC slot2=MISC)

 * (slot1=MISC slot2=MISC) != (slot1=INV slot2=MISC)

 *

 * In other words if current stack state (one being explored) has more

 * valid slots than old one that already passed validation, it means

 * the verifier can stop exploring and conclude that current state is valid too

 *

 * Similarly with registers. If explored state has register type as invalid

 * whereas register type in current state is meaningful, it means that

 * the current state will reach 'bpf_exit' instruction safely

	/* Verification state from speculative execution simulation

	 * must never prune a non-speculative execution one.

	/* for states to be equal callsites have to be the same

	 * and all frame states need to be equivalent

/* Return 0 if no propagation happened. Return negative error code if error

 * happened. Otherwise, return the propagated bit.

	/* When comes here, read flags of PARENT_REG or REG could be any of

	 * REG_LIVE_READ64, REG_LIVE_READ32, REG_LIVE_NONE. There is no need

	 * of propagation if PARENT_REG has strongest REG_LIVE_READ64.

 Or if there is no read flag from REG. */

 Or if the read flag from REG is the same as PARENT_REG. */

/* A write screens off any subsequent reads; but write marks come from the

 * straight-line code between a state and its parent.  When we arrive at an

 * equivalent state (jump target or such) we didn't arrive by the straight-line

 * code, so read marks in the state must propagate to the parent regardless

 * of the state's write marks. That's what 'parent == state->parent' comparison

 * in mark_reg_read() is for.

 Propagate read liveness of registers... */

 We don't need to worry about FP liveness, it's read-only */

 Propagate stack slots. */

/* find precise scalars in the previous equivalent state and

 * propagate them into the current state

		/* this 'insn_idx' instruction wasn't marked, so we will not

		 * be doing state search here

	/* bpf progs typically have pruning point every 4 instructions

	 * http://vger.kernel.org/bpfconf2019.html#session-1

	 * Do not add new state for future pruning if the verifier hasn't seen

	 * at least 2 jumps and at least 8 instructions.

	 * This heuristics helps decrease 'total_states' and 'peak_states' metric.

	 * In tests that amounts to up to 50% reduction into total verifier

	 * memory consumption and 20% verifier time speedup.

				/* Different async_entry_cnt means that the verifier is

				 * processing another entry into async callback.

				 * Seeing the same state is not an indication of infinite

				 * loop or infinite recursion.

				 * But finding the same state doesn't mean that it's safe

				 * to stop processing the current state. The previous state

				 * hasn't yet reached bpf_exit, since state.branches > 0.

				 * Checking in_async_callback_fn alone is not enough either.

				 * Since the verifier still needs to catch infinite loops

				 * inside async callbacks.

			/* if the verifier is processing a loop, avoid adding new state

			 * too often, since different loop iterations have distinct

			 * states and may not help future pruning.

			 * This threshold shouldn't be too low to make sure that

			 * a loop with large bound will be rejected quickly.

			 * The most abusive loop will be:

			 * r1 += 1

			 * if r1 < 1000000 goto pc-2

			 * 1M insn_procssed limit / 100 == 10k peak states.

			 * This threshold shouldn't be too high either, since states

			 * at the end of the loop are likely to be useful in pruning.

			/* reached equivalent register/stack state,

			 * prune the search.

			 * Registers read by the continuation are read by us.

			 * If we have any write marks in env->cur_state, they

			 * will prevent corresponding reads in the continuation

			 * from reaching our parent (an explored_state).  Our

			 * own state will get the read marks recorded, but

			 * they'll be immediately forgotten as we're pruning

			 * this state and will pop a new one.

			/* if previous state reached the exit with precision and

			 * current state is equivalent to it (except precsion marks)

			 * the precision needs to be propagated back in

			 * the current state.

		/* when new state is not going to be added do not increase miss count.

		 * Otherwise several loop iterations will remove the state

		 * recorded earlier. The goal of these heuristics is to have

		 * states from some iterations of the loop (some in the beginning

		 * and some at the end) to help pruning.

		/* heuristic to determine whether this state is beneficial

		 * to keep checking from state equivalence point of view.

		 * Higher numbers increase max_states_per_insn and verification time,

		 * but do not meaningfully decrease insn_processed.

			/* the state is unlikely to be useful. Remove it to

			 * speed up verification

				/* cannot free this state, since parentage chain may

				 * walk it later. Add it for free_list instead to

				 * be freed at the end of verification

	/* There were no equivalent states, remember the current one.

	 * Technically the current state is not proven to be safe yet,

	 * but it will either reach outer most bpf_exit (which means it's safe)

	 * or it will be rejected. When there are no loops the verifier won't be

	 * seeing this tuple (frame[0].callsite, frame[1].callsite, .. insn_idx)

	 * again on the way to bpf_exit.

	 * When looping the sl->state.branches will be > 0 and this state

	 * will not be considered for equivalence until branches == 0.

 add new state to the head of linked list */

	/* connect new state to parentage chain. Current frame needs all

	 * registers connected. Only r6 - r9 of the callers are alive (pushed

	 * to the stack implicitly by JITs) so in callers' frames connect just

	 * r6 - r9 as an optimization. Callers will have r1 - r5 connected to

	 * the state of the call instruction (with WRITTEN set), and r0 comes

	 * from callee with its full parentage chain, anyway.

	/* clear write marks in current state: the writes we did are not writes

	 * our child did, so they don't screen off its reads from us.

	 * (There are no read marks in current state, because reads always mark

	 * their parent and current state never has children yet.  Only

	 * explored_states can get read marks.)

 all stack frames are accessible from callee, clear them all */

 Return true if it's OK to have the same insn return a different type. */

/* If an instruction was previously used with particular pointer types, then we

 * need to be careful to avoid cases such as the below, where it may be ok

 * for one branch accessing the pointer, but not ok for the other branch:

 *

 * R1 = sock_ptr

 * goto X;

 * ...

 * R1 = some_other_valid_ptr;

 * goto X;

 * ...

 * R2 = *(u32 *)(R1 + 0);

 found equivalent state, can prune the search */

 check for reserved fields is already done */

 check src operand */

			/* check that memory (src_reg + off) is readable,

			 * the state of dst_reg will be updated by this func

				/* saw a valid insn

				 * dst_reg = *(u32 *)(src_reg + off)

				 * save type to validate intersecting paths

				/* ABuser program is trying to use the same insn

				 * dst_reg = *(u32*) (src_reg + off)

				 * with different pointer types:

				 * src_reg == ctx in one branch and

				 * src_reg == stack|map in some other branch.

				 * Reject it.

 check src1 operand */

 check src2 operand */

 check that memory (dst_reg + off) is writeable */

 check src operand */

 check that memory (dst_reg + off) is writeable */

 exit from nested function */

	/*

	 * Both vmlinux and module each have their own ".data..percpu"

	 * DATASECs in BTF. So for module's case, we need to skip vmlinux BTF

	 * types to look at only module's own BTF types.

 replace pseudo btf_id with kernel symbol address */

 resolve the type size of ksym. */

 check whether we recorded this BTF (and maybe module) already */

 if we reference variables from kernel module, bump its refcount */

	/*

	 * Validate that trace type programs use preallocated hash maps.

	 *

	 * For programs attached to PERF events this is mandatory as the

	 * perf NMI can hit any arbitrary code sequence.

	 *

	 * All other trace types using preallocated hash maps are unsafe as

	 * well because tracepoint or kprobes can be inside locked regions

	 * of the memory allocator or at a place where a recursion into the

	 * memory allocator would see inconsistent state.

	 *

	 * On RT enabled kernels run-time allocation of all trace type

	 * programs is strictly prohibited due to lock type constraints. On

	 * !RT kernels it is allowed for backwards compatibility reasons for

	 * now, but warnings are emitted so developers are made aware of

	 * the unsafety and can fix their programs before this is enforced.

/* find and rewrite pseudo imm in ld_imm64 instructions:

 *

 * 1. if it accesses map FD, replace it with actual map pointer.

 * 2. if it accesses btf_id of a VAR, replace it with pointer to the var.

 *

 * NOTE: btf_vmlinux is required for converting pseudo btf_id.

 valid generic load 64-bit imm */

			/* In final convert_pseudo_ld_imm64() step, this is

			 * converted into regular 64-bit imm load insn.

 check whether we recorded this map already */

			/* hold the map. If the program is rejected by verifier,

			 * the map will be released by release_maps() or it

			 * will be used by the valid program until it's unloaded

			 * and all maps are released in free_used_maps()

 Basic sanity check before we invest more work here. */

	/* now all pseudo BPF_LD_IMM64 instructions load valid

	 * 'struct bpf_map *' into a register instead of user map_fd.

	 * These pointers will be used later by verifier to validate map access.

 drop refcnt of maps used by the rejected program */

 drop refcnt of maps used by the rejected program */

 convert pseudo BPF_LD_IMM64 into generic BPF_LD_IMM64 */

/* single env->prog->insni[off] instruction was replaced with the range

 * insni[off, off + cnt).  Adjust corresponding insn_aux_data by copying

 * [0, off) and [off, end) to new locations, so the patched range stays zero

	/* aux info at OFF always needs adjustment, no matter fast path

	 * (cnt == 1) is taken or not. There is no guarantee INSN at OFF is the

	 * original insn at old prog.

 Expand insni[off]'s seen count to the patched range. */

 NOTE: fake 'exit' subprog should be updated as well. */

 find first prog starting at or after off (first to remove) */

 find first prog starting at or after off + cnt (first to stay) */

	/* if j doesn't start exactly at off + cnt, we are just removing

	 * the front of previous prog

 move fake 'exit' subprog as well */

 remove func_info */

			/* func_info->insn_off is set after all code rewrites,

			 * in adjust_btf_func() - no need to adjust

 convert i from "first prog to remove" to "first to adjust" */

 update fake 'exit' subprog as well */

 find first line info to remove, count lines to be removed */

	/* First live insn doesn't match first live linfo, it needs to "inherit"

	 * last removed linfo.  prog is already modified, so prog->len == off

	 * means no live instructions after (tail of the program was removed).

 remove the line info which refer to the removed instructions */

 pull all linfo[i].insn_off >= off + cnt in by cnt */

 fix up all subprogs (incl. 'exit') which start >= off */

			/* program may have started in the removed region but

			 * may not be fully removed

/* The verifier does more data flow analysis than llvm and will not

 * explore branches that are dead at run time. Malicious programs can

 * have dead code too. Therefore replace all dead at-run-time code

 * with 'ja -1'.

 *

 * Just nops are not optimal, e.g. if they would sit at the end of the

 * program and through another bug we would manage to jump there, then

 * we'd execute beyond program memory otherwise. Returning exception

 * code also wouldn't work since we can have subprogs where the dead

 * code could be located.

			/* NOTE: arg "reg" (the fourth one) is only used for

			 *       BPF_STX + SRC_OP, so it is safe to pass NULL

			 *       here.

 ctx load could be transformed into wider load. */

		/* Add in an zero-extend instruction if a) the JIT has requested

		 * it or b) it's a CMPXCHG.

		 *

		 * The latter is because: BPF_CMPXCHG always loads a value into

		 * R0, therefore always zero-extends. However some archs'

		 * equivalent instruction only does this load when the

		 * comparison is successful. This detail of CMPXCHG is

		 * orthogonal to the general zero-extension behaviour of the

		 * CPU, so it's treated independently of bpf_jit_needs_zext.

/* convert load instructions that access fields of a context type into a

 * sequence of instructions that access fields of the underlying structure:

 *     struct __sk_buff    -> struct sk_buff

 *     struct bpf_sock_ops -> struct sock

		/* If the read access is a narrower load of the field,

		 * convert to a 4/8-byte load, to minimum program type specific

		 * convert_ctx_access changes. If conversion is successful,

		 * we will apply proper mask to the result.

 keep walking new program and skip insns we just inserted */

		/* Upon error here we cannot fall back to interpreter but

		 * need a hard reject of the program. Thus -EFAULT is

		 * propagated in any case.

		/* temporarily remember subprog id inside insn instead of

		 * aux_data, since next loop will split up all insns into funcs

		/* remember original imm in case JIT fails and fallback

		 * to interpreter will be needed

 point imm to __bpf_call_base+1 from JITs point of view */

			/* jit (e.g. x86_64) may emit fewer instructions

			 * if it learns a u32 imm is the same as a u64 imm.

			 * Force a non zero here.

		/* bpf_prog_run() doesn't call subprogs directly,

		 * hence main prog stats include the runtime of subprogs.

		 * subprogs don't have IDs and not reachable via prog_get_next_id

		 * func[i]->stats will never be accessed and stays NULL

 Below members will be freed only at prog->aux */

		/* Use bpf_prog_F_tag to indicate functions in stack traces.

		 * Long term would need debug info to populate names

	/* at this point all bpf functions were successfully JITed

	 * now populate all bpf_calls with correct addresses and

	 * run last pass of JIT

		/* we use the aux data to keep a list of the start addresses

		 * of the JITed images for each function in the program

		 *

		 * for some architectures, such as powerpc64, the imm field

		 * might not be large enough to hold the offset of the start

		 * address of the callee's JITed image from __bpf_call_base

		 *

		 * in such cases, we can lookup the start address of a callee

		 * by using its subprog id, available from the off field of

		 * the call instruction, as an index for this list

	/* finally lock prog and jit images for all functions and

	 * populate kallsysm

	/* Last step: make now unused interpreter insns from main

	 * prog consistent for later dump requests, so they can

	 * later look the same as if they were interpreted only.

	/* We failed JIT'ing, so at this point we need to unregister poke

	 * descriptors from subprogs, so that kernel is not attempting to

	 * patch it anymore as we're freeing the subprog JIT memory.

	/* At this point we're guaranteed that poke descriptors are not

	 * live anymore. We can just unlink its descriptor table as it's

	 * released with the main prog.

 cleanup main prog to be interpreted */

		/* When JIT fails the progs with bpf2bpf calls and tail_calls

		 * have to be rejected, since interpreter doesn't support them yet.

			/* When JIT fails the progs with callback calls

			 * have to be rejected, since interpreter doesn't support them yet.

	/* insn->imm has the btf func_id. Replace it with

	 * an address (relative to __bpf_base_call).

/* Do various post-verification rewrites in a single program pass.

 * These rewrites simplify JIT and interpreter implementations.

 Make divide-by-zero exceptions impossible. */

 [R,W]x div 0 -> 0 */

 [R,W]x mod 0 -> [R,W]x */

 Implement LD_ABS and LD_IND with a rewrite, if supported by the program type. */

 Rewrite pointer arithmetic to mitigate speculation attacks. */

			/* If we tail call into other programs, we

			 * cannot make any assumptions since they can

			 * be replaced dynamically during runtime in

			 * the program array.

			/* mark bpf_tail_call as different opcode to avoid

			 * conditional branch in the interpreter for every normal

			 * call and to prevent accidental JITing by JIT compiler

			 * that doesn't support bpf_tail_call yet

			/* instead of changing every JIT dealing with tail_call

			 * emit two extra insns:

			 * if (index >= max_entries) goto out;

			 * index &= array->index_mask;

			 * to avoid out-of-bounds cpu speculation

			/* The verifier will process callback_fn as many times as necessary

			 * with different maps and the register states prepared by

			 * set_timer_callback_state will be accurate.

			 *

			 * The following use case is valid:

			 *   map1 is shared by prog1, prog2, prog3.

			 *   prog1 calls bpf_timer_init for some map1 elements

			 *   prog2 calls bpf_timer_set_callback for some map1 elements.

			 *     Those that were not bpf_timer_init-ed will return -EINVAL.

			 *   prog3 calls bpf_timer_start for some map1 elements.

			 *     Those that were not both bpf_timer_init-ed and

			 *     bpf_timer_set_callback-ed will return -EINVAL.

		/* BPF_EMIT_CALL() assumptions in some of the map_gen_lookup

		 * and other inlining handlers are currently limited to 64 bit

		 * only.

 Implement bpf_jiffies64 inline. */

 Implement bpf_get_func_ip inline. */

 Load IP address from ctx - 8 */

		/* all functions that have prototype and verifier allowed

		 * programs to call them, must be real in-kernel functions

 Since poke tab is now finalized, publish aux to tracker. */

 callsite */,

 frameno */,

 1st arg to a function */

			/* unlikely verifier bug. abort.

			 * ret == 0 and ret < 0 are sadly acceptable for

			 * main() function due to backward compatibility.

			 * Like socket filter program may be written as:

			 * int bpf_prog(struct pt_regs *ctx)

			 * and never dereference that ctx in the program.

			 * 'struct pt_regs' is a type mismatch for socket

			 * filter that should be using 'struct __sk_buff'.

	/* check for NULL is necessary, since cur_state can be freed inside

	 * do_check() under memory pressure.

/* Verify all global functions in a BPF program one by one based on their BTF.

 * All global functions must pass verification. Otherwise the whole program is rejected.

 * Consider:

 * int bar(int);

 * int foo(int f)

 * {

 *    return bar(f);

 * }

 * int bar(int b)

 * {

 *    ...

 * }

 * foo() will be verified first for R1=any_scalar_value. During verification it

 * will be assumed that bar() already verified successfully and call to bar()

 * from foo() will be checked for type match only. Later bar() will be verified

 * independently to check that it's safe for R1=any_scalar_value.

/* list of non-sleepable functions that are otherwise on

 * ALLOW_ERROR_INJECTION list

/* Three functions below can be called from sleepable and non-sleepable context.

 * Assume non-sleepable from bpf safety point of view.

			/* Cannot fentry/fexit another fentry/fexit program.

			 * Cannot attach program extension to another extension.

			 * It's ok to attach fentry/fexit to extension program.

			/* Program extensions can extend all program types

			 * except fentry/fexit. The reason is the following.

			 * The fentry/fexit programs are used for performance

			 * analysis, stats and can be attached to any program

			 * type except themselves. When extension program is

			 * replacing XDP function it is necessary to allow

			 * performance analysis of all functions. Both original

			 * XDP program and its program extension. Hence

			 * attaching fentry/fexit to BPF_PROG_TYPE_EXT is

			 * allowed. If extending of fentry/fexit was allowed it

			 * would be possible to create long call chain

			 * fentry->extension->fentry->extension beyond

			 * reasonable stack size. Hence extending fentry is not

			 * allowed.

 should never happen in valid vmlinux build */

 should never happen in valid vmlinux build */

				/* fentry/fexit/fmod_ret progs can be sleepable only if they are

				 * attached to ALLOW_ERROR_INJECTION and are not in denylist.

				/* LSM progs check that they are attached to bpf_lsm_*() funcs.

				 * Only some of them are sleepable.

 attach_btf_id checked to be zero already */

		/* to make freplace equivalent to their targets, they need to

		 * inherit env->ops and expected_attach_type for the rest of the

		 * verification

 store info about the attachment target that will be used later */

 no program is valid */

	/* 'struct bpf_verifier_env' can be global, but since it's not small,

	 * allocate/free it every time bpf_check() is called

 grab the mutex to protect few globals used by verifier */

		/* user requested verbose verifier output

		 * and supplied buffer to store the verification trace

 log attributes have to be sane */

 Either gcc or pahole or kernel are broken. */

 instruction rewrites happen after this point */

 program is valid, convert *(u32*)(ctx + off) accesses */

	/* do 32-bit optimization after insn patching has done so those patched

	 * insns could be handled correctly.

 if program passed verifier, update used_maps in bpf_prog_info */

 if program passed verifier, update used_btfs in bpf_prog_aux */

		/* program is valid. Convert pseudo bpf_ld_imm64 into generic

		 * bpf_ld_imm64 instructions

		/* if we didn't copy map pointers into bpf_prog_info, release

		 * them now. Otherwise free_used_maps() will release them.

	/* extension progs temporarily inherit the attach_type of their targets

	   for verification purposes, so set it back to zero before returning

 SPDX-License-Identifier: GPL-2.0 */

 Copyright (c) 2018 Facebook */

/* BTF (BPF Type Format) is the meta data format which describes

 * the data types of BPF program/map.  Hence, it basically focus

 * on the C programming language which the modern BPF is primary

 * using.

 *

 * ELF Section:

 * ~~~~~~~~~~~

 * The BTF data is stored under the ".BTF" ELF section

 *

 * struct btf_type:

 * ~~~~~~~~~~~~~~~

 * Each 'struct btf_type' object describes a C data type.

 * Depending on the type it is describing, a 'struct btf_type'

 * object may be followed by more data.  F.e.

 * To describe an array, 'struct btf_type' is followed by

 * 'struct btf_array'.

 *

 * 'struct btf_type' and any extra data following it are

 * 4 bytes aligned.

 *

 * Type section:

 * ~~~~~~~~~~~~~

 * The BTF type section contains a list of 'struct btf_type' objects.

 * Each one describes a C type.  Recall from the above section

 * that a 'struct btf_type' object could be immediately followed by extra

 * data in order to describe some particular C types.

 *

 * type_id:

 * ~~~~~~~

 * Each btf_type object is identified by a type_id.  The type_id

 * is implicitly implied by the location of the btf_type object in

 * the BTF type section.  The first one has type_id 1.  The second

 * one has type_id 2...etc.  Hence, an earlier btf_type has

 * a smaller type_id.

 *

 * A btf_type object may refer to another btf_type object by using

 * type_id (i.e. the "type" in the "struct btf_type").

 *

 * NOTE that we cannot assume any reference-order.

 * A btf_type object can refer to an earlier btf_type object

 * but it can also refer to a later btf_type object.

 *

 * For example, to describe "const void *".  A btf_type

 * object describing "const" may refer to another btf_type

 * object describing "void *".  This type-reference is done

 * by specifying type_id:

 *

 * [1] CONST (anon) type_id=2

 * [2] PTR (anon) type_id=0

 *

 * The above is the btf_verifier debug log:

 *   - Each line started with "[?]" is a btf_type object

 *   - [?] is the type_id of the btf_type object.

 *   - CONST/PTR is the BTF_KIND_XXX

 *   - "(anon)" is the name of the type.  It just

 *     happens that CONST and PTR has no name.

 *   - type_id=XXX is the 'u32 type' in btf_type

 *

 * NOTE: "void" has type_id 0

 *

 * String section:

 * ~~~~~~~~~~~~~~

 * The BTF string section contains the names used by the type section.

 * Each string is referred by an "offset" from the beginning of the

 * string section.

 *

 * Each string is '\0' terminated.

 *

 * The first character in the string section must be '\0'

 * which is used to mean 'anonymous'. Some btf_type may not

 * have a name.

/* BTF verification:

 *

 * To verify BTF data, two passes are needed.

 *

 * Pass #1

 * ~~~~~~~

 * The first pass is to collect all btf_type objects to

 * an array: "btf->types".

 *

 * Depending on the C type that a btf_type is describing,

 * a btf_type may be followed by extra data.  We don't know

 * how many btf_type is there, and more importantly we don't

 * know where each btf_type is located in the type section.

 *

 * Without knowing the location of each type_id, most verifications

 * cannot be done.  e.g. an earlier btf_type may refer to a later

 * btf_type (recall the "const void *" above), so we cannot

 * check this type-reference in the first pass.

 *

 * In the first pass, it still does some verifications (e.g.

 * checking the name is a valid offset to the string section).

 *

 * Pass #2

 * ~~~~~~~

 * The main focus is to resolve a btf_type that is referring

 * to another type.

 *

 * We have to ensure the referring type:

 * 1) does exist in the BTF (i.e. in btf->types[])

 * 2) does not cause a loop:

 *	struct A {

 *		struct B b;

 *	};

 *

 *	struct B {

 *		struct A a;

 *	};

 *

 * btf_type_needs_resolve() decides if a btf_type needs

 * to be resolved.

 *

 * The needs_resolve type implements the "resolve()" ops which

 * essentially does a DFS and detects backedge.

 *

 * During resolve (or DFS), different C types have different

 * "RESOLVED" conditions.

 *

 * When resolving a BTF_KIND_STRUCT, we need to resolve all its

 * members because a member is always referring to another

 * type.  A struct's member can be treated as "RESOLVED" if

 * it is referring to a BTF_KIND_PTR.  Otherwise, the

 * following valid C struct would be rejected:

 *

 *	struct A {

 *		int m;

 *		struct A *a;

 *	};

 *

 * When resolving a BTF_KIND_PTR, it needs to keep resolving if

 * it is referring to another BTF_KIND_PTR.  Otherwise, we cannot

 * detect a pointer loop, e.g.:

 * BTF_KIND_CONST -> BTF_KIND_PTR -> BTF_KIND_CONST -> BTF_KIND_PTR +

 *                        ^                                         |

 *                        +-----------------------------------------+

 *

/* 16MB for 64k structs and each has 16 members and

 * a few MB spaces for the string section.

 * The hard limit is S32_MAX.

 includes VOID for base BTF */

 split BTF support */

 first type ID in this BTF (0 for base BTF) */

 first string offset (0 for base BTF) */

 To Be Determined */

 Resolving for Pointer */

	RESOLVE_STRUCT_OR_ARRAY,	/* Resolving for struct/union

					 * or array

 Chunk size we use in safe copy of data to be shown. */

/*

 * This is the maximum size of a base type value (equivalent to a

 * 128-bit int); if we are at the end of our safe buffer and have

 * less than 16 bytes space we can't be assured of being able

 * to copy the next type safely, so in such cases we will initiate

 * a new copy.

 Type name size */

/*

 * Common data to all BTF show operations. Private show functions can add

 * their own data to a structure containing a struct btf_show and consult it

 * in the show callback.  See btf_type_show() below.

 *

 * One challenge with showing nested data is we want to skip 0-valued

 * data, but in order to figure out whether a nested object is all zeros

 * we need to walk through it.  As a result, we need to make two passes

 * when handling structs, unions and arrays; the first path simply looks

 * for nonzero data, while the second actually does the display.  The first

 * pass is signalled by show->state.depth_check being set, and if we

 * encounter a non-zero value we set show->state.depth_to_show to

 * the depth at which we encountered it.  When we have completed the

 * first pass, we will know if anything needs to be displayed if

 * depth_to_show > depth.  See btf_[struct,array]_show() for the

 * implementation of this.

 *

 * Another problem is we want to ensure the data for display is safe to

 * access.  To support this, the anonymous "struct {} obj" tracks the data

 * object and our safe copy of it.  We copy portions of the data needed

 * to the object "copy" buffer, but because its size is limited to

 * BTF_SHOW_OBJ_COPY_LEN bytes, multiple copies may be required as we

 * traverse larger objects for display.

 *

 * The various data type show functions all start with a call to

 * btf_show_start_type() which returns a pointer to the safe copy

 * of the data needed (or if BTF_SHOW_UNSAFE is specified, to the

 * raw data itself).  btf_show_obj_safe() is responsible for

 * using copy_from_kernel_nofault() to update the safe data if necessary

 * as we traverse the object's data.  skbuff-like semantics are

 * used:

 *

 * - obj.head points to the start of the toplevel object for display

 * - obj.size is the size of the toplevel object

 * - obj.data points to the current point in the original data at

 *   which our safe data starts.  obj.data will advance as we copy

 *   portions of the data.

 *

 * In most cases a single copy will suffice, but larger data structures

 * such as "struct task_struct" will require many copies.  The logic in

 * btf_show_obj_safe() handles the logic that determines if a new

 * copy_from_kernel_nofault() is needed.

 target of show operation (seq file, buffer) */

 below are used during iteration */

 non-zero for error */

 space for member name/type */

	/* Some of them is not strictly a C modifier

	 * but they are grouped into the same bucket

	 * for BTF concern:

	 *   A type (t) that refers to another

	 *   type through t->type AND its size cannot

	 *   be determined without following the t->type.

	 *

	 * ptr does not fall into this bucket

	 * because its size is always sizeof(void *).

/* Types that act only as a source, not sink or intermediate

 * type when resolving.

/* What types need to be resolved?

 *

 * btf_type_is_modifier() is an obvious one.

 *

 * btf_type_is_struct() because its member refers to

 * another type (through member->type).

 *

 * btf_type_is_var() because the variable refers to

 * another type. btf_type_is_datasec() holds multiple

 * btf_type_is_var() types that need resolving.

 *

 * btf_type_is_array() because its element (array->type)

 * refers to another type.  Array can be thought of a

 * special case of struct while array just has the same

 * member-type repeated by array->nelems of times.

 t->size can be used */

 offset must be valid */

 set a limit on identifier length */

/* Only C-style identifier is permitted. This can be relaxed if

 * necessary.

/*

 * Regular int is not a bit field and it must be either

 * u8/u16/u32/u64 or __int128.

/*

 * Check that given struct member is a regular int with expected

 * offset and size.

		/* if kflag set, int should be a regular int and

		 * bit offset should be at byte boundary.

 Similar to btf_type_skip_modifiers() but does not skip typedefs. */

/*

 * Populate show->state.name with type name information.

 * Format of type name is

 *

 * [.member_name = ] (type_name)

 BTF_MAX_ITER array suffixes "[]" */

 BTF_MAX_ITER pointer suffixes "*" */

	/*

	 * Don't show type name if we're showing an array member;

	 * in that case we show the array type so don't need to repeat

	 * ourselves for each member.

 Retrieve member name, if any. */

	/*

	 * Start with type_id, as we have resolved the struct btf_type *

	 * via btf_modifier_show() past the parent typedef to the child

	 * struct, int etc it is defined as.  In such cases, the type_id

	 * still represents the starting type while the struct btf_type *

	 * in our show->state points at the resolved type of the typedef.

	/*

	 * The goal here is to build up the right number of pointer and

	 * array suffixes while ensuring the type name for a typedef

	 * is represented.  Along the way we accumulate a list of

	 * BTF kinds we have encountered, since these will inform later

	 * display; for example, pointer types will not require an

	 * opening "{" for struct, we will just display the pointer value.

	 *

	 * We also want to accumulate the right number of pointer or array

	 * indices in the format string while iterating until we get to

	 * the typedef/pointee/array member target type.

	 *

	 * We start by pointing at the end of pointer and array suffix

	 * strings; as we accumulate pointers and arrays we move the pointer

	 * or array string backwards so it will show the expected number of

	 * '*' or '[]' for the type.  BTF_SHOW_MAX_ITER of nesting of pointers

	 * and/or arrays and typedefs are supported as a precaution.

	 *

	 * We also want to get typedef name while proceeding to resolve

	 * type it points to so that we can add parentheses if it is a

	 * "typedef struct" etc.

 We may not be able to represent this type; bail to be safe */

 if it's an array of struct/union, parens is already set */

 pointer does not require parens */

 typedef does not require struct/union/enum prefix */

 Even if we don't want type name info, we want parentheses etc */

 first 3 strings comprise ".member = " */

 ...next is our prefix (struct, enum, etc) */

 ...this is the type name itself */

 ...suffixed by the appropriate '*', '[]' suffixes */

/* Macros are used here as btf_show_type_value[s]() prepends and appends

 * format specifiers to the format specifier passed in; these do the work of

 * adding indentation, delimiters etc while the caller simply has to specify

 * the type value(s) in the format specifier + value(s).

 How much is left to copy to safe buffer after @data? */

 Is object pointed to by @data of @size already copied to our safe buffer? */

/*

 * If object pointed to by @data of @size falls within our safe buffer, return

 * the equivalent pointer to the same safe data.  Assumes

 * copy_from_kernel_nofault() has already happened and our safe buffer is

 * populated.

/*

 * Return a safe-to-access version of data pointed to by @data.

 * We do this by copying the relevant amount of information

 * to the struct btf_show obj.safe buffer using copy_from_kernel_nofault().

 *

 * If BTF_SHOW_UNSAFE is specified, just return data as-is; no

 * safe copy is needed.

 *

 * Otherwise we need to determine if we have the required amount

 * of data (determined by the @data pointer and the size of the

 * largest base type we can encounter (represented by

 * BTF_SHOW_OBJ_BASE_TYPE_SIZE). Having that much data ensures

 * that we will be able to print some of the current object,

 * and if more is needed a copy will be triggered.

 * Some objects such as structs will not fit into the buffer;

 * in such cases additional copies when we iterate over their

 * members may be needed.

 *

 * btf_show_obj_safe() is used to return a safe buffer for

 * btf_show_start_type(); this ensures that as we recurse into

 * nested types we always have safe data for the given type.

 * This approach is somewhat wasteful; it's possible for example

 * that when iterating over a large union we'll end up copying the

 * same data repeatedly, but the goal is safety not performance.

 * We use stack data as opposed to per-CPU buffers because the

 * iteration over a type can take some time, and preemption handling

 * would greatly complicate use of the safe buffer.

	/*

	 * Is this toplevel object? If so, set total object size and

	 * initialize pointers.  Otherwise check if we still fall within

	 * our safe object data.

		/*

		 * If the size of the current object is > our remaining

		 * safe buffer we _may_ need to do a new copy.  However

		 * consider the case of a nested struct; it's size pushes

		 * us over the safe buffer limit, but showing any individual

		 * struct members does not.  In such cases, we don't need

		 * to initiate a fresh copy yet; however we definitely need

		 * at least BTF_SHOW_OBJ_BASE_TYPE_SIZE bytes left

		 * in our buffer, regardless of the current object size.

		 * The logic here is that as we resolve types we will

		 * hit a base type at some point, and we need to be sure

		 * the next chunk of data is safely available to display

		 * that type info safely.  We cannot rely on the size of

		 * the current object here because it may be much larger

		 * than our current buffer (e.g. task_struct is 8k).

		 * All we want to do here is ensure that we can print the

		 * next basic type, which we can if either

		 * - the current type size is within the safe buffer; or

		 * - at least BTF_SHOW_OBJ_BASE_TYPE_SIZE bytes are left in

		 *   the safe buffer.

	/*

	 * We need a new copy to our safe object, either because we haven't

	 * yet copied and are initializing safe data, or because the data

	 * we want falls outside the boundaries of the safe object.

/*

 * Set the type we are starting to show and return a safe data pointer

 * to be used for showing the associated data.

	/* btf verifier prints all types it is processing via

	 * btf_verifier_log_type(..., fmt = NULL).

	 * Skip those prints for in-kernel BTF verification.

	/* The CHECK_META phase already did a btf dump.

	 *

	 * If member is logged again, it must hit an error in

	 * parsing this member.  It is useful to print out which

	 * struct this member belongs to.

 Expand 'types' array */

 lazily init VOID type */

	/*

	 * In map-in-map, calling map_delete_elem() on outer

	 * map will call bpf_map_put on the inner map.

	 * It will then eventually call btf_free_id()

	 * on the inner map.  Some of the map_delete_elem()

	 * implementation may have irq disabled, so

	 * we need to use the _irqsave() version instead

	 * of the _bh() version.

 int, enum or void is a sink */

		/* int, enum, void, struct, array, func or func_proto is a sink

		 * for ptr

		/* int, enum, void, ptr, func or func_proto is a sink

		 * for struct and array

 base BTF types should be resolved by now */

 adjust to local type id */

/* Resolve the size of a passed-in "type"

 *

 * type: is an array (e.g. u32 array[x][y])

 * return type: type "u32[x][y]", i.e. BTF_KIND_ARRAY,

 * *type_size: (x * y * sizeof(u32)).  Hence, *type_size always

 *             corresponds to the return type.

 * *elem_type: u32

 * *elem_id: id of u32

 * *total_nelems: (x * y).  Hence, individual elem size is

 *                (*type_size / *total_nelems)

 * *type_id: id of type if it's changed within the function, 0 if not

 *

 * type: is not an array (e.g. const struct X)

 * return type: type "struct X"

 * *type_size: sizeof(struct X)

 * *elem_type: same as return type ("struct X")

 * *elem_id: 0

 * *total_nelems: 1

 * *type_id: id of type if it's changed within the function, 0 if not

 type->size can be used */

 Modifiers */

 type without size */

 The input param "type_id" must point to a needs_resolve type */

/* Used for ptr, array struct/union and float type members.

 * int, enum and modifier types have their specific callback functions.

	/* bitfield size is 0, so member->offset represents bit offset only.

	 * It is safe to call non kflag check_member variants.

 a regular int type is required for the kflag int member */

 check sanity of bitfield size */

		/* Not a bitfield member, member offset must be at byte

		 * boundary.

	/*

	 * Only one of the encoding bits is allowed and it

	 * should be sufficient for the pretty print purpose (i.e. decoding).

	 * Multiple bits can be allowed later if it is found

	 * to be insufficient.

	/* data points to a __int128 number.

	 * Suppose

	 *     int128_num = *(__int128 *)data;

	 * The below formulas shows what upper_num and lower_num represents:

	 *     upper_num = int128_num >> 64;

	 *     lower_num = int128_num & 0xffffffffFFFFFFFFULL;

 shake out un-needed bits by shift/or operations */

	/*

	 * bits_offset is at most 7.

	 * BTF_INT_OFFSET() cannot exceed 128 bits.

 check for null terminator */

	/* typedef type must have a valid name, and other ref types,

	 * volatile, const, restrict, should have a null name.

	/* Figure out the resolved next_type_id with size.

	 * They will be stored in the current modifier's

	 * resolved_ids and resolved_sizes such that it can

	 * save us a few type-following when we use it later (e.g. in

	 * pretty print).

 "typedef void new_void", "const void"...etc */

	/* We must resolve to something concrete at this point, no

	 * forward types or similar that would resolve to size of

	 * zero is allowed.

	/* If the modifier was RESOLVED during RESOLVE_STRUCT_OR_ARRAY,

	 * the modifier may have stopped resolving when it was resolved

	 * to a ptr (last-resolved-ptr).

	 *

	 * We now need to continue from the last-resolved-ptr to

	 * ensure the last-resolved-ptr will not referring back to

	 * the currenct ptr (t).

 It is a hashed value unless BTF_SHOW_PTR_RAW is specified */

 fwd type must have a valid name */

 array type should not have a name */

	/* Array elem type and index type cannot be in type void,

	 * so !array->type and !array->index_type are not allowed.

 Check array->index_type */

 Check array->type */

		/*

		 * BTF_INT_CHAR encoding never seems to be set for

		 * char arrays, so if size is 1 and element is

		 * printable as a char, we'll do that.

	/*

	 * First check if any members would be shown (are non-zero).

	 * See comments above "struct btf_show" definition for more

	 * details on how this works at a high-level.

		/*

		 * Reaching here indicates we have recursed and found

		 * non-zero array member(s).

 struct type either no name or a valid one */

 struct member either no name or a valid one */

 A member cannot be in type void */

		/*

		 * ">" instead of ">=" because the last member could be

		 * "char a[0];"

	/* Before continue resolving the next_member,

	 * ensure the last member is indeed resolved to a

	 * type with size info.

 only one such field is allowed */

 valid C code cannot generate such BTF */

 only one such field is allowed */

/* find 'struct bpf_spin_lock' in map value.

 * return >= 0 offset if found

 * and < 0 in case of error

	/*

	 * First check if any members would be shown (are non-zero).

	 * See comments above "struct btf_show" definition for more

	 * details on how this works at a high-level.

 Restore saved member data here */

		/*

		 * Reaching here indicates we have recursed and found

		 * non-zero child values.

 enum type either no name or a valid one */

 enum member must have a valid name */

 Only one vararg */

	/*

	 * BTF_KIND_FUNC_PROTO cannot be directly referred by

	 * a struct's member.

	 *

	 * It should be a function pointer instead.

	 * (i.e. struct's member -> BTF_KIND_PTR -> BTF_KIND_FUNC_PROTO)

	 *

	 * Hence, there is no btf_func_check_member().

 A var cannot be in type void */

 A var cannot be in type void */

	/* Different architectures have different alignment requirements, so

	 * here we check only for the reasonable minimum. This way we ensure

	 * that types after CO-RE can pass the kernel BTF verifier.

 next_type should be a function */

 Check func return type which could be "void" (t->type == 0) */

 Ensure the return type is a type that has a size */

 Last func arg type_id could be 0 if it is a vararg */

 Final sanity check */

 Type section must align to 4 bytes */

 Populate the secs from hdr */

 Check for gaps and overlap among sections */

 gap */

 There is data other than hdr and known sections */

 Ensure the unsupported header fields are zero */

		/* user requested verbose verifier output

		 * and supplied buffer to store the verification trace

 log attributes have to be sane */

 't' is written once under lock. Read many times. */

 to avoid empty enum in extreme .config */

 avoid empty array */

		/* Only pointer to struct is supported for now.

		 * That means that BPF_PROG_TYPE_TRACEPOINT with BTF

		 * is not supported yet.

		 * BPF_PROG_TYPE_RAW_TRACEPOINT is fine.

 prog_type is valid bpf program type. No need for bounds check. */

	/* ctx_struct is a pointer to prog_ctx_type in vmlinux.

	 * Like 'struct __sk_buff'

 should not happen */

 should not happen */

	/* only compare that prog's ctx type name is the same as

	 * kernel expects. No need to compare field by field.

	 * It's ok for bpf prog to do:

	 * struct __sk_buff {};

	 * int socket_filter_bpf_prog(struct __sk_buff *skb)

	 * { // no fields of skb are ever used }

 btf_parse_vmlinux() runs under bpf_verifier_lock */

 find bpf map structs for map_ptr access checking */

 CONFIG_DEBUG_INFO_BTF_MODULES */

 t comes in already as a pointer */

 allow const */

 char, signed char, unsigned char */

	/* if (t == NULL) Fall back to default BPF prog with

	 * MAX_BPF_FUNC_REG_ARGS u64 arguments.

 skip first 'void *__data' argument in btf_trace_##name typedef */

			/* When LSM programs are attached to void LSM hooks

			 * they use FEXIT trampolines and when attached to

			 * int LSM hooks, they use MODIFY_RETURN trampolines.

			 *

			 * While the LSM programs are BPF_MODIFY_RETURN-like

			 * the check:

			 *

			 *	if (ret_type != 'int')

			 *		return -EINVAL;

			 *

			 * is _not_ done here. This is still safe as LSM hooks

			 * have only void and int return types.

			/* For now the BPF_MODIFY_RETURN can only be attached to

			 * functions that return an int.

 Default prog with MAX_BPF_FUNC_REG_ARGS args */

 skip modifiers */

 accessing a scalar */

 check for PTR_TO_RDONLY_BUF_OR_NULL or PTR_TO_RDWR_BUF_OR_NULL */

		/* This is a pointer to void.

		 * It is the same as scalar from the verifier safety pov.

		 * No further pointer walking is allowed.

 this is a pointer to another type */

 skip modifiers */

 < 0 error */

		/* If the last element is a variable size array, we may

		 * need to relax the rule.

		/* Only allow structure for now, can be relaxed for

		 * other types later.

 offset of the field in bytes */

 won't find anything, field is already too far */

			/* off <= moff instead of off == moff because clang

			 * does not generate a BTF member for anonymous

			 * bitfield like the ":16" here:

			 * struct {

			 *	int :16;

			 *	int x:8;

			 * };

			/* off may be accessing a following member

			 *

			 * or

			 *

			 * Doing partial access at either end of this

			 * bitfield.  Continue on this case also to

			 * treat it as not accessing this bitfield

			 * and eventually error out as field not

			 * found to keep it simple.

			 * It could be relaxed if there was a legit

			 * partial access case later.

 In case of "off" is pointing to holes of a struct */

 type of the field */

 no overlap with member, keep iterating */

			/* __btf_resolve_size() above helps to

			 * linearize a multi-dimensional array.

			 *

			 * The logic here is treating an array

			 * in a struct as the following way:

			 *

			 * struct outer {

			 *	struct inner array[2][2];

			 * };

			 *

			 * looks like:

			 *

			 * struct outer {

			 *	struct inner array_elem0;

			 *	struct inner array_elem1;

			 *	struct inner array_elem2;

			 *	struct inner array_elem3;

			 * };

			 *

			 * When accessing outer->array[1][0], it moves

			 * moff to "array_elem2", set mtype to

			 * "struct inner", and msize also becomes

			 * sizeof(struct inner).  Then most of the

			 * remaining logic will fall through without

			 * caring the current member is an array or

			 * not.

			 *

			 * Unlike mtype/msize/moff, mtrue_end does not

			 * change.  The naming difference ("_true") tells

			 * that it is not always corresponding to

			 * the current mtype/msize/moff.

			 * It is the true end of the current

			 * member (i.e. array in this case).  That

			 * will allow an int array to be accessed like

			 * a scratch space,

			 * i.e. allow access beyond the size of

			 *      the array's element as long as it is

			 *      within the mtrue_end boundary.

 skip empty array */

		/* the 'off' we're looking for is either equal to start

		 * of this field or inside of this struct

 our field must be inside that union or struct */

 return if the offset matches the member offset */

 adjust offset we're looking for */

		/* Allow more flexible access within an int as long as

		 * it is within mtrue_end.

		 * Since mtrue_end could be the end of an array,

		 * that also allows using an array of int as a scratch

		 * space. e.g. skb->cb[].

			/* If we found the pointer or scalar on t+off,

			 * we're done.

			/* We found nested struct, so continue the search

			 * by diving in it. At this point the offset is

			 * aligned with the new type, so set it to 0.

			/* It's either error or unknown return value..

			 * scream and leave.

/* Check that two BTF types, each specified as an BTF object + id, are exactly

 * the same. Trivial ID check is not enough due to module BTFs, because we can

 * end up with two different module BTFs, but IDs point to the common type in

 * vmlinux BTF.

 Are we already done? */

	/* We found nested struct object. If it matches

	 * the requested ID, we're done. Otherwise let's

	 * continue the search with offset 0 in the new

	 * type.

 void */

 kernel size of pointer. Not BPF's size of pointer*/

		/* BTF function prototype doesn't match the verifier types.

		 * Fall back to MAX_BPF_FUNC_REG_ARGS u64 args.

/* Compare BTFs of two functions assuming only scalars and pointers to context.

 * t1 points to BTF_KIND_FUNC in btf1

 * t2 points to BTF_KIND_FUNC in btf2

 * Returns:

 * EINVAL - function prototype mismatch

 * EFAULT - verifier bug

 * 0 - 99% match. The last 1% is validated by the verifier.

		/* global functions are validated with scalars and pointers

		 * to context only. And only global functions can be replaced.

		 * Hence type check only those types.

		/* This is an optional check to make program writing easier.

		 * Compare names of structs and report an error to the user.

		 * btf_prepare_func_args() already checked that t2 struct

		 * is a context type. btf_prepare_func_args() will check

		 * later that t1 struct is a context type as well.

 Compare BTFs of given program with BTF of target program */

		/* These checks were already done by the verifier while loading

		 * struct bpf_func_info or in add_kfunc_call().

	/* check that BTF function arguments match actual types that the

	 * verifier sees.

			/* If function expects ctx type in BTF check that caller

			 * is passing PTR_TO_CTX.

/* Compare BTF of a function with given bpf_reg_state.

 * Returns:

 * EFAULT - there is a verifier bug. Abort verification.

 * EINVAL - there is a type mismatch or BTF is not available.

 * 0 - BTF matches with what bpf_reg_state expects.

 * Only PTR_TO_CTX and SCALAR_VALUE states are recognized.

	/* Compiler optimizations can remove arguments from static functions

	 * or mismatched type can be passed into a global function.

	 * In such cases mark the function as unreliable from BTF point of view.

/* Convert BTF of a function into bpf_reg_state if possible

 * Returns:

 * EFAULT - there is a verifier bug. Abort verification.

 * EINVAL - cannot convert BTF.

 * 0 - Successfully converted BTF into bpf_reg_state

 * (either PTR_TO_CTX or SCALAR_VALUE).

		/* These checks were already done by the verifier while loading

		 * struct bpf_func_info

 check that function returns int */

	/* Convert BTF function arguments into verifier types.

	 * Only PTR_TO_CTX and SCALAR are supported atm.

 space left in string */

 length we would have written */

 no space, drive on to get length we would have written */

 If we encontered an error, return it. */

 Otherwise return length we would have written */

	/*

	 * The BTF ID is published to the userspace.

	 * All BTF free must go through call_rcu() from

	 * now on (i.e. free by calling btf_put()).

 let user-space know about too short buffer */

 CONFIG_DEBUG_INFO_BTF_MODULES */

	/* ret is never zero, since btf_find_by_name_kind returns

	 * positive btf_id or negative error.

 If name is not found in vmlinux's BTF then search in module's BTFs */

			/* linear search could be slow hence unlock/lock

			 * the IDR to avoiding holding it for too long

 BTF ID set registration API for modules */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com

 * Copyright (c) 2016 Facebook

/*

 * The bucket lock has two protection scopes:

 *

 * 1) Serializing concurrent operations from BPF programs on different

 *    CPUs

 *

 * 2) Serializing concurrent operations from BPF programs and sys_bpf()

 *

 * BPF programs can execute in any context including perf, kprobes and

 * tracing. As there are almost no limits where perf, kprobes and tracing

 * can be invoked from the lock operations need to be protected against

 * deadlocks. Deadlocks can be caused by recursion and by an invocation in

 * the lock held section when functions which acquire this lock are invoked

 * from sys_bpf(). BPF recursion is prevented by incrementing the per CPU

 * variable bpf_prog_active, which prevents BPF programs attached to perf

 * events, kprobes and tracing to be invoked before the prior invocation

 * from one of these contexts completed. sys_bpf() uses the same mechanism

 * by pinning the task to the current CPU and incrementing the recursion

 * protection across the map operation.

 *

 * This has subtle implications on PREEMPT_RT. PREEMPT_RT forbids certain

 * operations like memory allocations (even with GFP_ATOMIC) from atomic

 * contexts. This is required because even with GFP_ATOMIC the memory

 * allocator calls into code paths which acquire locks with long held lock

 * sections. To ensure the deterministic behaviour these locks are regular

 * spinlocks, which are converted to 'sleepable' spinlocks on RT. The only

 * true atomic contexts on an RT kernel are the low level hardware

 * handling, scheduling, low level interrupt handling, NMIs etc. None of

 * these contexts should ever do memory allocations.

 *

 * As regular device interrupt handlers and soft interrupts are forced into

 * thread context, the existing code which does

 *   spin_lock*(); alloc(GPF_ATOMIC); spin_unlock*();

 * just works.

 *

 * In theory the BPF locks could be converted to regular spinlocks as well,

 * but the bucket locks and percpu_freelist locks can be taken from

 * arbitrary contexts (perf, kprobes, tracepoints) which are required to be

 * atomic contexts even on RT. These mechanisms require preallocated maps,

 * so there is no need to invoke memory allocations within the lock held

 * sections.

 *

 * BPF maps which need dynamic allocation are only used from (forced)

 * thread context on RT and can therefore use regular spinlocks which in

 * turn allows to invoke memory allocations from the lock held section.

 *

 * On a non RT kernel this distinction is neither possible nor required.

 * spinlock maps to raw_spinlock and the extra code is optimized out by the

 * compiler.

 number of elements in this hashtable */

 number of hash buckets */

 size of each element in bytes */

 each htab element is struct htab_elem + key + value */

/* The LRU list has a lock (lru_lock). Each htab bucket has a lock

 * (bucket_lock). If both locks need to be acquired together, the lock

 * order is always lru_lock -> bucket_lock and this only happens in

 * bpf_lru_list.c logic. For example, certain code path of

 * bpf_lru_pop_free(), which is called by function prealloc_lru_pop(),

 * will acquire lru_lock first followed by acquiring bucket_lock.

 *

 * In hashtab.c, to avoid deadlock, lock acquisition of

 * bucket_lock followed by lru_lock is not allowed. In such cases,

 * bucket_lock needs to be released first before acquiring lru_lock.

		/* pop will succeed, since prealloc_init()

		 * preallocated extra num_possible_cpus elements

 Called from syscall */

	/* percpu_lru means each cpu has its own LRU list.

	 * it is different from BPF_MAP_TYPE_PERCPU_HASH where

	 * the map's value itself is percpu.  percpu_lru has

	 * nothing to do with the map's value.

		/* LRU implementation is much complicated than other

		 * maps.  Hence, limit to CAP_BPF.

 Guard against local DoS, and discourage production use. */

	/* check sanity of attributes.

	 * value_size == 0 may be allowed in the future to use map as a set

		/* if key_size + value_size is bigger, the user space won't be

		 * able to access the elements via bpf syscall. This check

		 * also makes sure that the elem_size doesn't overflow and it's

		 * kmalloc-able later in htab_map_update_elem()

	/* percpu_lru means each cpu has its own LRU list.

	 * it is different from BPF_MAP_TYPE_PERCPU_HASH where

	 * the map's value itself is percpu.  percpu_lru has

	 * nothing to do with the map's value.

		/* ensure each CPU's lru list has >=1 elements.

		 * since we are at it, make each lru list has the same

		 * number of elements.

 hash table size must be power of 2 */

 prevent zero size kmalloc and check for u32 overflow */

			/* lru itself can remove the least used element, so

			 * there is no need for an extra elem during map_update.

 this lookup function can only be called with bucket lock taken */

/* can be called without bucket lock. it will repeat the loop in

 * the unlikely event when elements moved from one bucket into another

 * while link list is being walked

/* Called from syscall or from eBPF program directly, so

 * arguments have to match bpf_map_lookup_elem() exactly.

 * The return value is adjusted by BPF instructions

 * in htab_map_gen_lookup().

/* inline bpf_map_lookup_elem() call.

 * Instead of:

 * bpf_prog

 *   bpf_map_lookup_elem

 *     map->ops->map_lookup_elem

 *       htab_map_lookup_elem

 *         __htab_map_lookup_elem

 * do:

 * bpf_prog

 *   __htab_map_lookup_elem

/* It is called from the bpf_lru_list when the LRU needs to delete

 * older elements from the htab.

 Called from syscall */

 lookup the key */

 key was found, get next key in the same bucket */

 if next elem in this hash list is non-zero, just return it */

 no more elements in this hash list, go to the next bucket */

 iterate over buckets */

 pick first element in the bucket */

 if it's not empty, just return it */

 iterated over all buckets and all elements */

 copy true value_size bytes */

	/* When using prealloc and not setting the initial value on all cpus,

	 * zero-fill element values for other cpus (just as what happens when

	 * not using prealloc). Otherwise, bpf program has no way to ensure

	 * known initial values for cpus other than current one

	 * (onallcpus=false always when coming from bpf prog).

			/* if we're updating the existing element,

			 * use per-cpu extra elems to avoid freelist_pop/push

				/* when map is full and update() is replacing

				 * old element, it's ok to allocate, since

				 * old element will be freed immediately.

				 * Otherwise return an error

 alloc_percpu zero-fills */

 elem already exists */

 elem doesn't exist, cannot update it */

 Called from syscall or from eBPF program */

 unknown flags */

 find an element without taking the bucket lock */

 grab the element lock and update value in place */

		/* fall through, grab the bucket lock and lookup again.

		 * 99.9% chance that the element won't be found,

		 * but second lookup under lock has to be done.

		/* first lookup without the bucket lock didn't find the element,

		 * but second lookup with the bucket lock found it.

		 * This case is highly unlikely, but has to be dealt with:

		 * grab the element lock in addition to the bucket lock

		 * and update element in place

 all pre-allocated elements are in use or memory exhausted */

	/* add new element to the head of the list, so that

	 * concurrent search will find it before old elem

 unknown flags */

	/* For LRU, we need to alloc before taking bucket's

	 * spinlock because getting free nodes from LRU may need

	 * to remove older elements from htab and this removal

	 * operation will need a bucket lock.

	/* add new element to the head of the list, so that

	 * concurrent search will find it before old elem

 unknown flags */

 per-cpu hash map can update value in-place */

 unknown flags */

	/* For LRU, we need to alloc before taking bucket's

	 * spinlock because LRU's elem alloc may need

	 * to remove older elem from htab and this removal

	 * operation will need a bucket lock.

 per-cpu hash map can update value in-place */

 Called from syscall or from eBPF program */

 Called when map->refcnt goes to zero, either from workqueue or from syscall */

	/* bpf_free_used_maps() or close(map_fd) will trigger this map_free callback.

	 * bpf_free_used_maps() is called after bpf prog is no longer executing.

	 * There is no need to synchronize_rcu() here to protect map elements.

	/* some of free_htab_elem() callbacks for elements of this map may

	 * not have executed. Wait for them.

	/* while experimenting with hash tables with sizes ranging from 10 to

	 * 1000, it was observed that a bucket can have upto 5 entries.

	/* We cannot do copy_from_user or copy_to_user inside

	 * the rcu_read_lock. Allocate enough space here.

 do not grab the lock unless need it (bucket_cnt > 0). */

		/* Note that since bucket_cnt > 0 here, it is implicit

		 * that the locked was grabbed, so release it.

		/* Note that since bucket_cnt > 0 here, it is implicit

		 * that the locked was grabbed, so release it.

 Next block is only safe to run if you have grabbed the lock */

			/* bpf_lru_push_free() will acquire lru_lock, which

			 * may cause deadlock. See comments in function

			 * prealloc_lru_pop(). Let us do bpf_lru_push_free()

			 * after releasing the bucket lock.

	/* If we are not copying data, we can go to next bucket and avoid

	 * unlocking the rcu.

 copy # of entries and next batch */

 non-zero means percpu hash

 try to find next elem in the same bucket */

		/* no update/deletion on this bucket, prev_elem should be still valid

		 * and we won't skip elements.

 not found, unlock and go to the next bucket */

	/* disable migration so percpu value prepared here will be the

	 * same as the one seen by the bpf program with bpf_map_lookup_elem().

 current cpu value for percpu map */

 return value: 0 - continue, 1 - stop and return */

 Called from eBPF program */

	/* per_cpu areas are zero-filled and bpf programs can only

	 * access 'value_size' of them, so copying rounded areas

	 * will not leak any kernel data

	/* We do not mark LRU map element here in order to not mess up

	 * eviction heuristics when user space does a map walk.

 only called from syscall */

 only called from syscall */

 SPDX-License-Identifier: GPL-2.0-only

 Copyright(c) 2019 Intel Corporation. */

/* The BPF dispatcher is a multiway branch code generator. The

 * dispatcher is a mechanism to avoid the performance penalty of an

 * indirect call, which is expensive when retpolines are enabled. A

 * dispatch client registers a BPF program into the dispatcher, and if

 * there is available room in the dispatcher a direct call to the BPF

 * program will be generated. All calls to the BPF programs called via

 * the dispatcher will then be a direct call, instead of an

 * indirect. The dispatcher hijacks a trampoline function it via the

 * __fentry__ of the trampoline. The trampoline function has the

 * following signature:

 *

 * unsigned int trampoline(const void *ctx, const struct bpf_insn *insnsi,

 *                         unsigned int (*bpf_func)(const void *,

 *                                                  const struct bpf_insn *));

 SPDX-License-Identifier: GPL-2.0

 Copyright (c) 2021 Facebook */

	/* If the size of the values in the bloom filter is u32 aligned,

	 * then it is more performant to use jhash2 as the underlying hash

	 * function, else we use jhash. This tracks the number of u32s

	 * in an u32-aligned value size. If the value size is not u32 aligned,

	 * this will be 0.

	    /* The lower 4 bits of map_extra (0xF) specify the number

	     * of hash functions

 Default to using 5 hash functions if unspecified */

	/* For the bloom filter, the optimal bit array size that minimizes the

	 * false positive probability is n * k / ln(2) where n is the number of

	 * expected entries in the bloom filter and k is the number of hash

	 * functions. We use 7 / 5 to approximate 1 / ln(2).

	 *

	 * We round this up to the nearest power of two to enable more efficient

	 * hashing using bitmasks. The bitmask will be the bit array size - 1.

	 *

	 * If this overflows a u32, the bit array size will have 2^32 (4

	 * GB) bits.

		/* The bit array size is 2^32 bits but to avoid overflowing the

		 * u32, we use U32_MAX, which will round up to the equivalent

		 * number of bytes

 Check whether the value size is u32-aligned */

 The eBPF program should use map_peek_elem instead */

 The eBPF program should use map_push_elem instead */

 Bloom filter maps are keyless */

 SPDX-License-Identifier: GPL-2.0-only

 Copyright (c) 2020 Facebook */

	/* The first field must be struct bpf_iter_seq_task_common.

	 * this is assumed by {init, fini}_seq_pidns() callback functions.

	/* The first field must be struct bpf_iter_seq_task_common.

	 * this is assumed by {init, fini}_seq_pidns() callback functions.

	/* If this function returns a non-NULL file object,

	 * it held a reference to the task/file.

	 * Otherwise, it does not hold any reference.

 set info->task and info->tid */

 set info->fd */

 the current task is done, go to the next task */

	/* The first field must be struct bpf_iter_seq_task_common.

	 * this is assumed by {init, fini}_seq_pidns() callback functions.

 use mm->mmap */

 use curr_vma->vm_next */

 use find_vma() to find next vma */

	/* If this function returns a non-NULL vma, it holds a reference to

	 * the task_struct, and holds read lock on vma->mm->mmap_lock.

	 * If this function returns NULL, it does not hold any reference or

	 * lock.

		/* In case of lock contention, drop mmap_lock to unblock

		 * the writer.

		 *

		 * After relock, call find(mm, prev_vm_end - 1) to find

		 * new vma to process.

		 *

		 *   +------+------+-----------+

		 *   | VMA1 | VMA2 | VMA3      |

		 *   +------+------+-----------+

		 *   |      |      |           |

		 *  4k     8k     16k         400k

		 *

		 * For example, curr_vma == VMA2. Before unlock, we set

		 *

		 *    prev_vm_start = 8k

		 *    prev_vm_end   = 16k

		 *

		 * There are a few cases:

		 *

		 * 1) VMA2 is freed, but VMA3 exists.

		 *

		 *    find_vma() will return VMA3, just process VMA3.

		 *

		 * 2) VMA2 still exists.

		 *

		 *    find_vma() will return VMA2, process VMA2->next.

		 *

		 * 3) no more vma in this mm.

		 *

		 *    Process the next task.

		 *

		 * 4) find_vma() returns a different vma, VMA2'.

		 *

		 *    4.1) If VMA2 covers same range as VMA2', skip VMA2',

		 *         because we already covered the range;

		 *    4.2) VMA2 and VMA2' covers different ranges, process

		 *         VMA2'.

 new task, process the first vma */

			/* Found the same tid, which means the user space

			 * finished data in previous buffer and read more.

			 * We dropped mmap_lock before returning to user

			 * space, so it is necessary to use find_vma() to

			 * find the next vma to process.

		/* We dropped mmap_lock so it is necessary to use find_vma

		 * to find the next vma. This is similar to the  mechanism

		 * in show_smaps_rollup().

 case 1) and 4.2) above just use curr_vma */

 check for case 2) or case 4.1) above */

 case 3) above, or case 2) 4.1) with vma->next == NULL */

		/* info->vma has not been seen by the BPF program. If the

		 * user space reads more, task_vma_seq_get_next should

		 * return this vma again. Set prev_vm_start to ~0UL,

		 * so that we don't skip the vma returned by the next

		 * find_vma() (case task_vma_iter_find_vma in

		 * task_vma_seq_get_next()).

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Functions to manage eBPF programs attached to cgroups

 *

 * Copyright (c) 2016 Daniel Mack

/* Called when bpf_cgroup_link is auto-detached from dying cgroup.

 * It drops cgroup and bpf_prog refcounts, and marks bpf_link as defunct. It

 * doesn't free link memory, which will eventually be done by bpf_link's

 * release() callback, when its last FD is closed.

/**

 * cgroup_bpf_release() - put references of all bpf programs and

 *                        release all cgroup bpf data

 * @work: work structure embedded into the cgroup to modify

/**

 * cgroup_bpf_release_fn() - callback used to schedule releasing

 *                           of bpf cgroup data

 * @ref: percpu ref counter structure

/* Get underlying bpf_prog of bpf_prog_list entry, regardless if it's through

 * link or direct prog.

/* count number of elements in the list.

 * it's slow but the list cannot be long

/* if parent has non-overridable prog attached,

 * disallow attaching new programs to the descendent cgroup.

 * if parent has overridable or multi-prog, allow attaching

/* compute a chain of effective programs for a given cgroup:

 * start from the list of programs in this cgroup and add

 * all parent programs.

 * Note that parent's F_ALLOW_OVERRIDE-type program is yielding

 * to programs in this cgroup

 count number of effective programs by walking parents */

 populate the array with effective progs */

	/* free prog array after grace period, since __cgroup_bpf_run_*()

	 * might be still walking the array

/**

 * cgroup_bpf_inherit() - inherit effective programs from parent

 * @cgrp: the cgroup to modify

/* has to use marco instead of const int, since compiler thinks

 * that array below is variable length

 allocate and recompute effective prog arrays */

 all allocations were successful. Activate all prog arrays */

	/* oom while computing effective. Free all computed effective arrays

	 * since they were not activated

 single-attach case */

 disallow attaching the same prog twice */

 disallow attaching the same link twice */

 direct prog multi-attach w/ replacement case */

 a match found */

 prog to replace not found for cgroup */

/**

 * __cgroup_bpf_attach() - Attach the program or the link to a cgroup, and

 *                         propagate the change to descendants

 * @cgrp: The cgroup which descendants to traverse

 * @prog: A program to attach

 * @link: A link to attach

 * @replace_prog: Previously attached program to replace if BPF_F_REPLACE is set

 * @type: Type of attach operation

 * @flags: Option flags

 *

 * Exactly one of @prog or @link can be non-null.

 * Must be called with cgroup_mutex held.

 invalid combination */

 only either link or prog/replace_prog can be specified */

 replace_prog implies BPF_F_REPLACE, and vice versa */

		/* Disallow attaching non-overridable on top

		 * of existing overridable in this cgroup.

		 * Disallow attaching multi-prog if overridable or none

/* Swap updated BPF program for given link in effective program arrays across

 * all descendant cgroups. This function is guaranteed to succeed.

 find position of link in effective progs array */

/**

 * __cgroup_bpf_replace() - Replace link's program and propagate the change

 *                          to descendants

 * @cgrp: The cgroup which descendants to traverse

 * @link: A link for which to replace BPF program

 * @type: Type of attach operation

 *

 * Must be called with cgroup_mutex held.

 link might have been auto-released by dying cgroup, so fail */

 report error when trying to detach and nothing is attached */

		/* to maintain backward compatibility NONE and OVERRIDE cgroups

		 * allow detaching with invalid FD (prog==NULL) in legacy mode

		/* to detach MULTI prog the user has to specify valid FD

		 * of the program or link to be detached

 find the prog or link and detach it */

/**

 * __cgroup_bpf_detach() - Detach the program or link from a cgroup, and

 *                         propagate the change to descendants

 * @cgrp: The cgroup which descendants to traverse

 * @prog: A program to detach or NULL

 * @link: A link to detach or NULL

 * @type: Type of detach operation

 *

 * At most one of @prog or @link can be non-NULL.

 * Must be called with cgroup_mutex held.

 only one of prog or link can be specified */

 mark it deleted, so it's ignored while recomputing effective */

 now can actually delete it from this cgroup list */

 last program was detached, reset flags to zero */

 restore back prog or link */

 Must be called with cgroup_mutex held to avoid races. */

 return early if user requested only program count + flags */

	/* link might have been auto-detached by dying cgroup already,

	 * in that case our work is done here

 re-check cgroup under lock again */

/**

 * __cgroup_bpf_run_filter_skb() - Run a program for packet filtering

 * @sk: The socket sending or receiving traffic

 * @skb: The skb that is being sent or received

 * @type: The type of program to be exectuted

 *

 * If no socket is passed, or the socket is not of type INET or INET6,

 * this function does nothing and returns 0.

 *

 * The program type passed in via @type must be suitable for network

 * filtering. No further check is performed to assert that.

 *

 * For egress packets, this function can return:

 *   NET_XMIT_SUCCESS    (0)	- continue with packet output

 *   NET_XMIT_DROP       (1)	- drop packet and notify TCP to call cwr

 *   NET_XMIT_CN         (2)	- continue with packet output and notify TCP

 *				  to call cwr

 *   -EPERM			- drop packet

 *

 * For ingress packets, this function will return -EPERM if any

 * attached program was found and if it returned != 1 during execution.

 * Otherwise 0 is returned.

 compute pointers for the bpf prog */

/**

 * __cgroup_bpf_run_filter_sk() - Run a program on a sock

 * @sk: sock structure to manipulate

 * @type: The type of program to be exectuted

 *

 * socket is passed is expected to be of type INET or INET6.

 *

 * The program type passed in via @type must be suitable for sock

 * filtering. No further check is performed to assert that.

 *

 * This function will return %-EPERM if any if an attached program was found

 * and if it returned != 1 during execution. In all other cases, 0 is returned.

/**

 * __cgroup_bpf_run_filter_sock_addr() - Run a program on a sock and

 *                                       provided by user sockaddr

 * @sk: sock struct that will use sockaddr

 * @uaddr: sockaddr struct provided by user

 * @type: The type of program to be exectuted

 * @t_ctx: Pointer to attach type specific context

 * @flags: Pointer to u32 which contains higher bits of BPF program

 *         return value (OR'ed together).

 *

 * socket is expected to be of type INET or INET6.

 *

 * This function will return %-EPERM if an attached program is found and

 * returned value != 1 during execution. In all other cases, 0 is returned.

	/* Check socket family since not all sockets represent network

	 * endpoint (e.g. AF_UNIX).

/**

 * __cgroup_bpf_run_filter_sock_ops() - Run a program on a sock

 * @sk: socket to get cgroup from

 * @sock_ops: bpf_sock_ops_kern struct to pass to program. Contains

 * sk with connection information (IP addresses, etc.) May not contain

 * cgroup info if it is a req sock.

 * @type: The type of program to be exectuted

 *

 * socket passed is expected to be of type INET or INET6.

 *

 * The program type passed in via @type must be suitable for sock_ops

 * filtering. No further check is performed to assert that.

 *

 * This function will return %-EPERM if any if an attached program was found

 * and if it returned != 1 during execution. In all other cases, 0 is returned.

 The verifier guarantees that size > 0. */

/**

 * __cgroup_bpf_run_filter_sysctl - Run a program on sysctl

 *

 * @head: sysctl table header

 * @table: sysctl table

 * @write: sysctl is being read (= 0) or written (= 1)

 * @buf: pointer to buffer (in and out)

 * @pcount: value-result argument: value is size of buffer pointed to by @buf,

 *	result is size of @new_buf if program set new value, initial value

 *	otherwise

 * @ppos: value-result argument: value is position at which read from or write

 *	to sysctl is happening, result is new position if program overrode it,

 *	initial value otherwise

 * @type: type of program to be executed

 *

 * Program is run when sysctl is being accessed, either read or written, and

 * can allow or deny such access.

 *

 * This function will return %-EPERM if an attached program is found and

 * returned value != 1 during execution. In all other cases 0 is returned.

 Let BPF program decide how to proceed. */

		/* BPF program should be able to override new value with a

		 * buffer bigger than provided by user.

 Let BPF program decide how to proceed. */

		/* We don't expose optvals that are greater than PAGE_SIZE

		 * to the BPF program.

		/* When the optval fits into BPF_SOCKOPT_KERN_BUF_SIZE

		 * bytes avoid the cost of kzalloc.

	/* Opportunistic check to see whether we have any BPF program

	 * attached to the hook so we don't waste time allocating

	 * memory and locking the socket.

	/* Allocate a bit more than the initial user buffer for

	 * BPF program. The canonical use case is overriding

	 * TCP_CONGESTION(nv) to TCP_CONGESTION(cubic).

 optlen set to -1, bypass kernel */

 optlen is out of bounds */

 optlen within bounds, run kernel handler */

 export any potential modifications */

		/* optlen == 0 from BPF indicates that we should

		 * use original userspace data.

			/* We've used bpf_sockopt_kern->buf as an intermediary

			 * storage, but the BPF program indicates that we need

			 * to pass this data to the kernel setsockopt handler.

			 * No way to export on-stack buf, have to allocate a

			 * new buffer.

 export and don't free sockopt buf */

	/* Opportunistic check to see whether we have any BPF program

	 * attached to the hook so we don't waste time allocating

	 * memory and locking the socket.

		/* If kernel getsockopt finished successfully,

		 * copy whatever was returned to the user back

		 * into our temporary buffer. Set optlen to the

		 * one that kernel returned as well to let

		 * BPF programs inspect the value.

	/* BPF programs only allowed to set retval to 0, not some

	 * arbitrary value.

	/* Note that __cgroup_bpf_run_filter_getsockopt doesn't copy

	 * user data back into BPF buffer when reval != 0. This is

	 * done as an optimization to avoid extra copy, assuming

	 * kernel won't populate the data in case of an error.

	 * Here we always pass the data and memset() should

	 * be called if that data shouldn't be "exported".

	/* BPF programs only allowed to set retval to 0, not some

	 * arbitrary value.

	/* BPF programs can shrink the buffer, export the modifications.

 Avoid leading slash. */

		/* ppos is a pointer so it should be accessed via indirect

		 * loads and stores. Also for stores additional temporary

		 * register is used since neither src_reg nor dst_reg can be

		 * overridden.

	/* Nothing to do for sockopt argument. The data is kzalloc'ated.

 SPDX-License-Identifier: GPL-2.0

 Copyright (c) 2019 Facebook  */

/* local_storage->lock must be held and selem->local_storage == local_storage.

 * The caller must ensure selem->smap is still valid to be

 * dereferenced for its smap->elem_size and smap->cache_idx.

	/* All uncharging on the owner must be done first.

	 * The owner may be freed once the last selem is unlinked

	 * from local_storage.

 After this RCU_INIT, owner may be freed and cannot be used */

		/* local_storage is not freed now.  local_storage->lock is

		 * still held and raw_spin_unlock_bh(&local_storage->lock)

		 * will be done by the caller.

		 *

		 * Although the unlock will be done under

		 * rcu_read_lock(),  it is more intutivie to

		 * read if kfree_rcu(local_storage, rcu) is done

		 * after the raw_spin_unlock_bh(&local_storage->lock).

		 *

		 * Hence, a "bool free_local_storage" is returned

		 * to the caller which then calls the kfree_rcu()

		 * after unlock.

 selem has already been unlinked from sk */

 selem has already be unlinked from smap */

	/* Always unlink from map before unlinking from local_storage

	 * because selem will be freed after successfully unlinked from

	 * the local_storage.

 Fast path (cache hit) */

 Slow path (cache miss) */

		/* spinlock is needed to avoid racing with the

		 * parallel delete.  Otherwise, publishing an already

		 * deleted sdata to the cache will become a use-after-free

		 * problem in the next bpf_local_storage_lookup().

 elem already exists */

 elem doesn't exist, cannot update it */

	/* Publish storage to the owner.

	 * Instead of using any lock of the kernel object (i.e. owner),

	 * cmpxchg will work with any kernel object regardless what

	 * the running context is, bh, irq...etc.

	 *

	 * From now on, the owner->storage pointer (e.g. sk->sk_bpf_storage)

	 * is protected by the storage->lock.  Hence, when freeing

	 * the owner->storage, the storage->lock must be held before

	 * setting owner->storage ptr to NULL.

		/* Note that even first_selem was linked to smap's

		 * bucket->list, first_selem can be freed immediately

		 * (instead of kfree_rcu) because

		 * bpf_local_storage_map_free() does a

		 * synchronize_rcu() before walking the bucket->list.

		 * Hence, no one is accessing selem from the

		 * bucket->list under rcu_read_lock().

/* sk cannot be going away because it is linking new elem

 * to sk->sk_bpf_storage. (i.e. sk->sk_refcnt cannot be 0).

 * Otherwise, it will become a leak (and other memory issues

 * during map destruction).

 BPF_EXIST and BPF_NOEXIST cannot be both set */

 BPF_F_LOCK can only be used in a value with spin_lock */

 Very first elem for the owner */

		/* Hoping to find an old_sdata to do inline update

		 * such that it can avoid taking the local_storage->lock

		 * and changing the lists.

 Recheck local_storage->list under local_storage->lock */

		/* A parallel del is happening and local_storage is going

		 * away.  It has just been checked before, so very

		 * unlikely.  Return instead of retry to keep things

		 * simple.

	/* local_storage->lock is held.  Hence, we are sure

	 * we can unlink and uncharge the old_sdata successfully

	 * later.  Hence, instead of charging the new selem now

	 * and then uncharge the old selem later (which may cause

	 * a potential but unnecessary charge failure),  avoid taking

	 * a charge at all here (the "!old_sdata" check) and the

	 * old_sdata will not be uncharged later during

	 * bpf_selem_unlink_storage_nolock().

 First, link the new selem to the map */

 Second, link (and publish) the new selem to local_storage */

 Third, remove old selem, SELEM(old_sdata) */

 Found a free cache_idx */

	/* Note that this map might be concurrently cloned from

	 * bpf_sk_storage_clone. Wait for any existing bpf_sk_storage_clone

	 * RCU read section to finish before proceeding. New RCU

	 * read sections should be prevented via bpf_map_inc_not_zero.

	/* bpf prog and the userspace can no longer access this map

	 * now.  No new selem (of this map) can be added

	 * to the owner->storage or to the map bucket's list.

	 *

	 * The elem of this map can be cleaned up here

	 * or when the storage is freed e.g.

	 * by bpf_sk_storage_free() during __sk_destruct().

 No one is adding to b->list now */

	/* While freeing the storage we may still need to access the map.

	 *

	 * e.g. when bpf_sk_storage_free() has unlinked selem from the map

	 * which then made the above while((selem = ...)) loop

	 * exit immediately.

	 *

	 * However, while freeing the storage one still needs to access the

	 * smap->elem_size to do the uncharging in

	 * bpf_selem_unlink_storage_nolock().

	 *

	 * Hence, wait another rcu grace period for the storage to be freed.

 Enforce BTF for userspace sk dumping */

 Use at least 2 buckets, select_bucket() is undefined behavior with 1 bucket */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com

/* If kernel subsystem is allowing eBPF programs to call this function,

 * inside its own verifier_ops->get_func_proto() callback it should return

 * bpf_map_lookup_elem_proto, so that verifier can properly check the arguments

 *

 * Different map implementations will rely on rcu in map methods

 * lookup/update/delete, therefore eBPF programs must run under rcu lock

 * if program is allowed to access maps, so check rcu_read_lock_held in

 * all three functions.

 NMI safe access to clock monotonic */

 NMI safe access to clock boottime */

	/* Verifier guarantees that size > 0. For task->comm exceeding

	 * size, guarantee that buf is %NUL-terminated. Unconditionally

	 * done here to save the size test.

	/* flags argument is not used now,

	 * but provides an ability to extend the API.

	 * verifier checks that its value is correct.

 get current cgroup storage from BPF run context */

/* Per-cpu temp buffers used by printf-like helpers to store the bprintf binary

 * arguments representation.

 Support executing three nested bprintf helper calls on a given CPU */

/*

 * bpf_bprintf_prepare - Generic pass on format strings for bprintf-like helpers

 *

 * Returns a negative value if fmt is an invalid format string or 0 otherwise.

 *

 * This can be used in two ways:

 * - Format string verification only: when bin_args is NULL

 * - Arguments preparation: in addition to the above verification, it writes in

 *   bin_args a binary representation of arguments usable by bstr_printf where

 *   pointers from BPF have been sanitized.

 *

 * In argument preparation mode, if 0 is returned, safe temporary buffers are

 * allocated and bpf_bprintf_cleanup should be called to free them after use.

		/* The string is zero-terminated so if fmt[i] != 0, we can

		 * always access fmt[i + 1], in the worst case it will be a 0

 skip optional "[0 +-][num]" width formatting field */

 just kernel pointers */

 only support "%pI4", "%pi4", "%pI6" and "%pi6". */

			/* hack: bstr_printf expects IP addresses to be

			 * pre-formatted as strings, ironically, the easiest way

			 * to do that is to call snprintf.

	/* ARG_PTR_TO_CONST_STR guarantees that fmt is zero-terminated so we

	 * can safely give an unbounded size.

/* BPF map elements can contain 'struct bpf_timer'.

 * Such map owns all of its BPF timers.

 * 'struct bpf_timer' is allocated as part of map element allocation

 * and it's zero initialized.

 * That space is used to keep 'struct bpf_timer_kern'.

 * bpf_timer_init() allocates 'struct bpf_hrtimer', inits hrtimer, and

 * remembers 'struct bpf_map *' pointer it's part of.

 * bpf_timer_set_callback() increments prog refcnt and assign bpf callback_fn.

 * bpf_timer_start() arms the timer.

 * If user space reference to a map goes to zero at this point

 * ops->map_release_uref callback is responsible for cancelling the timers,

 * freeing their memory, and decrementing prog's refcnts.

 * bpf_timer_cancel() cancels the timer and decrements prog's refcnt.

 * Inner maps can contain bpf timers as well. ops->map_release_uref is

 * freeing the timers when inner map is replaced or deleted by user space.

 the actual struct hidden inside uapi struct bpf_timer */

	/* bpf_spin_lock is used here instead of spinlock_t to make

	 * sure that it always fits into space resereved by struct bpf_timer

	 * regardless of LOCKDEP and spinlock debug flags.

	/* bpf_timer_cb() runs in hrtimer_run_softirq. It doesn't migrate and

	 * cannot be preempted by another bpf_timer_cb() on the same cpu.

	 * Remember the timer this callback is servicing to prevent

	 * deadlock if callback_fn() calls bpf_timer_cancel() or

	 * bpf_map_delete_elem() on the same timer.

 compute the key */

 hash or lru */

 The verifier checked that return value is zero. */

 similar to timerfd except _ALARM variants are not supported */

		/* maps with timers must be either held by user space

		 * or pinned in bpffs.

 allocate hrtimer via map_kmalloc to use memcg accounting */

		/* maps with timers must be either held by user space

		 * or pinned in bpffs. Otherwise timer might still be

		 * running even when bpf prog is detached and user space

		 * is gone, since map_release_uref won't ever be called.

		/* Bump prog refcnt once. Every bpf_timer_set_callback()

		 * can pick different callback_fn-s within the same prog.

 Drop prev prog refcnt when swapping with new prog */

		/* If bpf callback_fn is trying to bpf_timer_cancel()

		 * its own timer the hrtimer_cancel() will deadlock

		 * since it waits for callback_fn to finish

	/* Cancel the timer and wait for associated callback to finish

	 * if it was running.

/* This function is called by map_delete/update_elem for individual element and

 * by ops->map_release_uref when the user space reference to a map reaches zero.

 Performance optimization: read timer->timer without lock first. */

 re-read it under lock */

	/* The subsequent bpf_timer_start/cancel() helpers won't be able to use

	 * this timer, since it won't be initialized.

	/* Cancel the timer and wait for callback to complete if it was running.

	 * If hrtimer_cancel() can be safely called it's safe to call kfree(t)

	 * right after for both preallocated and non-preallocated maps.

	 * The timer->timer = NULL was already done and no code path can

	 * see address 't' anymore.

	 *

	 * Check that bpf_map_delete/update_elem() wasn't called from timer

	 * callback_fn. In such case don't call hrtimer_cancel() (since it will

	 * deadlock) and don't call hrtimer_try_to_cancel() (since it will just

	 * return -1). Though callback_fn is still running on this cpu it's

	 * safe to do kfree(t) because bpf_timer_cb() read everything it needed

	 * from 't'. The bpf subprog callback_fn won't be able to access 't',

	 * since timer->timer = NULL was already done. The timer will be

	 * effectively cancelled because bpf_timer_cb() will return

	 * HRTIMER_NORESTART.

 SPDX-License-Identifier: GPL-2.0-only

/* bpf/cpumap.c

 *

 * Copyright (c) 2017 Jesper Dangaard Brouer, Red Hat Inc.

/* The 'cpumap' is primarily used as a backend map for XDP BPF helper

 * call bpf_redirect_map() and XDP_REDIRECT action, like 'devmap'.

 *

 * Unlike devmap which redirects XDP frames out another NIC device,

 * this map type redirects raw XDP frames to another CPU.  The remote

 * CPU will do SKB-allocation and call the normal network stack.

 *

 * This is a scalability and isolation mechanism, that allow

 * separating the early driver network XDP layer, from the rest of the

 * netstack, and assigning dedicated CPUs for this stage.  This

 * basically allows for 10G wirespeed pre-filtering via bpf.

 netif_receive_skb_list */

 eth_type_trans */

/* General idea: XDP packets getting XDP redirected to another CPU,

 * will maximum be stored/queued for one driver ->poll() call.  It is

 * guaranteed that queueing the frame and the flush operation happen on

 * same CPU.  Thus, cpu_map_flush operation can deduct via this_cpu_ptr()

 * which queue in bpf_cpu_map_entry contains packets.

 8 == one cacheline on 64-bit archs */

 Struct for every remote "destination" CPU in map */

 kthread CPU and map index */

 Back reference to map */

 XDP can run multiple RX-ring queues, need __percpu enqueue store */

 Queue with potential multi-producers, and single-consumer kthread */

 Control when this struct can be free'ed */

 Below members specific for map type */

 check sanity of attributes */

 Pre-limit array size based on NR_CPUS, not final CPU check */

 Alloc array for possible remote "destination" CPUs */

 called from workqueue, to workaround syscall using preempt_disable */

	/* Wait for flush in __cpu_map_entry_free(), via full RCU barrier,

	 * as it waits until all in-flight call_rcu() callbacks complete.

 kthread_stop will wake_up_process and wait for it to complete */

	/* The tear-down procedure should have made sure that queue is

	 * empty.  See __cpu_map_entry_replace() and work-queue

	 * invoked cpu_map_kthread_stop(). Catch any broken behaviour

	 * gracefully and warn once.

 The queue should be empty at this point */

 TODO: report queue_index to xdp_rxq_info */

 resched point, may call do_softirq() */

	/* When kthread gives stop order, then rcpu have been disconnected

	 * from map, thus no new packets can enter. Remaining in-flight

	 * per CPU stored packets are flushed to this queue.  Wait honoring

	 * kthread_stop signal until queue is empty.

 zero stats */

 Release CPU reschedule checks */

 Recheck to avoid lost wake-up */

		/*

		 * The bpf_cpu_map_entry is single consumer, with this

		 * kthread CPU pinned. Lockless access to ptr_ring

		 * consume side valid as no-resize allowed of queue.

			/* Bring struct page memory area to curr CPU. Read by

			 * build_skb_around via page_is_pfmemalloc(), and when

			 * freed written by page_frag_free call.

 Support running another XDP prog on this CPU */

 effect: xdp_return_frame */

 Feedback loop via tracepoint */

 resched point, may call do_softirq() */

 Have map->numa_node, but choose node of redirect target CPU */

 Alloc percpu bulkq */

 Alloc queue */

 Setup kthread */

 1-refcnt for being in cmap->cpu_map[] */

 1-refcnt for kthread */

 Make sure kthread runs on a single CPU */

	/* This cpu_map_entry have been disconnected from map and one

	 * RCU grace-period have elapsed.  Thus, XDP cannot queue any

	 * new packets and cannot change/set flush_needed that can

	 * find this entry.

 Cannot kthread_stop() here, last put free rcpu resources */

/* After xchg pointer to bpf_cpu_map_entry, use the call_rcu() to

 * ensure any driver rcu critical sections have completed, but this

 * does not guarantee a flush has happened yet. Because driver side

 * rcu_read_lock/unlock only protects the running XDP program.  The

 * atomic xchg and NULL-ptr check in __cpu_map_flush() makes sure a

 * pending flush op doesn't fail.

 *

 * The bpf_cpu_map_entry is still used by the kthread, and there can

 * still be pending packets (in queue and percpu bulkq).  A refcnt

 * makes sure to last user (kthread_stop vs. call_rcu) free memory

 * resources.

 *

 * The rcu callback __cpu_map_entry_free flush remaining packets in

 * percpu bulkq to queue.  Due to caller map_delete_elem() disable

 * preemption, cannot call kthread_stop() to make sure queue is empty.

 * Instead a work_queue is started for stopping kthread,

 * cpu_map_kthread_stop, which waits for an RCU grace period before

 * stopping kthread, emptying the queue.

 notice caller map_delete_elem() use preempt_disable() */

 Array index key correspond to CPU number */

 sanity limit on qsize */

 Make sure CPU is a valid possible cpu */

 Same as deleting */

 Updating qsize cause re-allocation of bpf_cpu_map_entry */

	/* At this point bpf_prog->aux->refcnt == 0 and this map->refcnt == 0,

	 * so the bpf programs (can be more than one that used this map) were

	 * disconnected from events. Wait for outstanding critical sections in

	 * these programs to complete. The rcu critical section only guarantees

	 * no further "XDP/bpf-side" reads against bpf_cpu_map->cpu_map.

	 * It does __not__ ensure pending flush operations (if any) are

	 * complete.

	/* For cpu_map the remote CPUs can still be using the entries

	 * (struct bpf_cpu_map_entry).

 bq flush and cleanup happens after RCU grace-period */

 call_rcu */

/* Elements are kept alive by RCU; either by rcu_read_lock() (from syscall) or

 * by local_bh_disable() (from XDP calls inside NAPI). The

 * rcu_read_lock_bh_held() below makes lockdep accept both.

 Feedback loop via tracepoints */

/* Runs under RCU-read-side, plus in softirq under NAPI protection.

 * Thus, safe percpu variable access.

	/* Notice, xdp_buff/page MUST be queued here, long enough for

	 * driver to code invoking us to finished, due to driver

	 * (e.g. ixgbe) recycle tricks based on page-refcnt.

	 *

	 * Thus, incoming xdp_frame is always queued here (else we race

	 * with another CPU on page-refcnt and remaining driver code).

	 * Queue time is very short, as driver will invoke flush

	 * operation, when completing napi->poll call.

 Info needed when constructing SKB on remote CPU */

 If already running, costs spin_lock_irqsave + smb_mb */

 SPDX-License-Identifier: GPL-2.0-only

/* tnum: tracked (or tristate) numbers

 *

 * A tnum tracks knowledge about the bits of a value.  Each bit can be either

 * known (0 or 1), or unknown (x).  Arithmetic operations on tnums will

 * propagate the unknown bits such that the tnum result represents all the

 * possible results for possible values of the operands.

 A completely unknown value */

 special case, needed because 1ULL << 64 is undefined */

	/* e.g. if chi = 4, bits = 3, delta = (1<<3) - 1 = 7.

	 * if chi = 0, bits = 0, delta = (1<<0) - 1 = 0, so we return

	 *  constant min (since min == max).

	/* if a.value is negative, arithmetic shifting by minimum shift

	 * will have larger negative offset compared to more shifting.

	 * If a.value is nonnegative, arithmetic shifting by minimum shift

	 * will have larger positive offset compare to more shifting.

/* Generate partial products by multiplying each bit in the multiplier (tnum a)

 * with the multiplicand (tnum b), and add the partial products after

 * appropriately bit-shifting them. Instead of directly performing tnum addition

 * on the generated partial products, equivalenty, decompose each partial

 * product into two tnums, consisting of the value-sum (acc_v) and the

 * mask-sum (acc_m) and then perform tnum addition on them. The following paper

 * explains the algorithm in more detail: https://arxiv.org/abs/2105.05398.

 LSB of tnum a is a certain 1 */

 LSB of tnum a is uncertain */

 Note: no case for LSB is certain 0 */

/* Note that if a and b disagree - i.e. one has a 'known 1' where the other has

 * a 'known 0' - this will return a 'known 1' for that bit.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Longest prefix match list implementation

 *

 * Copyright (c) 2016,2017 Daniel Mack

 * Copyright (c) 2016 David Herrmann

 Intermediate node */

/* This trie implements a longest prefix match algorithm that can be used to

 * match IP addresses to a stored set of ranges.

 *

 * Data stored in @data of struct bpf_lpm_key and struct lpm_trie_node is

 * interpreted as big endian, so data[0] stores the most significant byte.

 *

 * Match ranges are internally stored in instances of struct lpm_trie_node

 * which each contain their prefix length as well as two pointers that may

 * lead to more nodes containing more specific matches. Each node also stores

 * a value that is defined by and returned to userspace via the update_elem

 * and lookup functions.

 *

 * For instance, let's start with a trie that was created with a prefix length

 * of 32, so it can be used for IPv4 addresses, and one single element that

 * matches 192.168.0.0/16. The data array would hence contain

 * [0xc0, 0xa8, 0x00, 0x00] in big-endian notation. This documentation will

 * stick to IP-address notation for readability though.

 *

 * As the trie is empty initially, the new node (1) will be places as root

 * node, denoted as (R) in the example below. As there are no other node, both

 * child pointers are %NULL.

 *

 *              +----------------+

 *              |       (1)  (R) |

 *              | 192.168.0.0/16 |

 *              |    value: 1    |

 *              |   [0]    [1]   |

 *              +----------------+

 *

 * Next, let's add a new node (2) matching 192.168.0.0/24. As there is already

 * a node with the same data and a smaller prefix (ie, a less specific one),

 * node (2) will become a child of (1). In child index depends on the next bit

 * that is outside of what (1) matches, and that bit is 0, so (2) will be

 * child[0] of (1):

 *

 *              +----------------+

 *              |       (1)  (R) |

 *              | 192.168.0.0/16 |

 *              |    value: 1    |

 *              |   [0]    [1]   |

 *              +----------------+

 *                   |

 *    +----------------+

 *    |       (2)      |

 *    | 192.168.0.0/24 |

 *    |    value: 2    |

 *    |   [0]    [1]   |

 *    +----------------+

 *

 * The child[1] slot of (1) could be filled with another node which has bit #17

 * (the next bit after the ones that (1) matches on) set to 1. For instance,

 * 192.168.128.0/24:

 *

 *              +----------------+

 *              |       (1)  (R) |

 *              | 192.168.0.0/16 |

 *              |    value: 1    |

 *              |   [0]    [1]   |

 *              +----------------+

 *                   |      |

 *    +----------------+  +------------------+

 *    |       (2)      |  |        (3)       |

 *    | 192.168.0.0/24 |  | 192.168.128.0/24 |

 *    |    value: 2    |  |     value: 3     |

 *    |   [0]    [1]   |  |    [0]    [1]    |

 *    +----------------+  +------------------+

 *

 * Let's add another node (4) to the game for 192.168.1.0/24. In order to place

 * it, node (1) is looked at first, and because (4) of the semantics laid out

 * above (bit #17 is 0), it would normally be attached to (1) as child[0].

 * However, that slot is already allocated, so a new node is needed in between.

 * That node does not have a value attached to it and it will never be

 * returned to users as result of a lookup. It is only there to differentiate

 * the traversal further. It will get a prefix as wide as necessary to

 * distinguish its two children:

 *

 *                      +----------------+

 *                      |       (1)  (R) |

 *                      | 192.168.0.0/16 |

 *                      |    value: 1    |

 *                      |   [0]    [1]   |

 *                      +----------------+

 *                           |      |

 *            +----------------+  +------------------+

 *            |       (4)  (I) |  |        (3)       |

 *            | 192.168.0.0/23 |  | 192.168.128.0/24 |

 *            |    value: ---  |  |     value: 3     |

 *            |   [0]    [1]   |  |    [0]    [1]    |

 *            +----------------+  +------------------+

 *                 |      |

 *  +----------------+  +----------------+

 *  |       (2)      |  |       (5)      |

 *  | 192.168.0.0/24 |  | 192.168.1.0/24 |

 *  |    value: 2    |  |     value: 5   |

 *  |   [0]    [1]   |  |   [0]    [1]   |

 *  +----------------+  +----------------+

 *

 * 192.168.1.1/32 would be a child of (5) etc.

 *

 * An intermediate node will be turned into a 'real' node on demand. In the

 * example above, (4) would be re-used if 192.168.0.0/23 is added to the trie.

 *

 * A fully populated trie would have a height of 32 nodes, as the trie was

 * created with a prefix length of 32.

 *

 * The lookup starts at the root node. If the current node matches and if there

 * is a child that can be used to become more specific, the trie is traversed

 * downwards. The last node in the traversal that is a non-intermediate one is

 * returned.

/**

 * longest_prefix_match() - determine the longest prefix

 * @trie:	The trie to get internal sizes from

 * @node:	The node to operate on

 * @key:	The key to compare to @node

 *

 * Determine the longest prefix of @node that matches the bits in @key.

	/* data_size >= 16 has very small probability.

	 * We do not use a loop for optimal code generation.

 Called from syscall or from eBPF program */

 Start walking the trie from the root node ... */

		/* Determine the longest prefix of @node that matches @key.

		 * If it's the maximum possible prefix for this trie, we have

		 * an exact match and can return it directly.

		/* If the number of bits that match is smaller than the prefix

		 * length of @node, bail out and return the node we have seen

		 * last in the traversal (ie, the parent).

		/* Consider this node as return candidate unless it is an

		 * artificially added intermediate one.

		/* If the node match is fully satisfied, let's see if we can

		 * become more specific. Determine the next bit in the key and

		 * traverse down.

 Called from syscall or from eBPF program */

 Allocate and fill a new node */

	/* Now find a slot to attach the new node. To do that, walk the tree

	 * from the root and match as many bits as possible for each node until

	 * we either find an empty slot or a slot that needs to be replaced by

	 * an intermediate node.

	/* If the slot is empty (a free child pointer or an empty root),

	 * simply assign the @new_node to that slot and be done.

	/* If the slot we picked already exists, replace it with @new_node

	 * which already has the correct data array set.

	/* If the new node matches the prefix completely, it must be inserted

	 * as an ancestor. Simply insert it between @node and *@slot.

 Now determine which child to install in which slot */

 Finally, assign the intermediate node to the determined spot */

 Called from syscall or from eBPF program */

	/* Walk the tree looking for an exact key/length match and keeping

	 * track of the path we traverse.  We will need to know the node

	 * we wish to delete, and the slot that points to the node we want

	 * to delete.  We may also need to know the nodes parent and the

	 * slot that contains it.

	/* If the node we are removing has two children, simply mark it

	 * as intermediate and we are done.

	/* If the parent of the node we are about to delete is an intermediate

	 * node, and the deleted node doesn't have any children, we can delete

	 * the intermediate parent as well and promote its other child

	 * up the tree.  Doing this maintains the invariant that all

	 * intermediate nodes have exactly 2 children and that there are no

	 * unnecessary intermediate nodes in the tree.

	/* The node we are removing has either zero or one child. If there

	 * is a child, move it into the removed node's slot then delete

	 * the node.  Otherwise just clear the slot and delete the node.

 check sanity of attributes */

 copy mandatory map attributes */

	/* Always start at the root and walk down to a node that has no

	 * children. Then free that node, nullify its reference in the parent

	 * and start over.

	/* The get_next_key follows postorder. For the 4 node example in

	 * the top of this file, the trie_get_next_key() returns the following

	 * one after another:

	 *   192.168.0.0/24

	 *   192.168.1.0/24

	 *   192.168.128.0/24

	 *   192.168.0.0/16

	 *

	 * The idea is to return more specific keys before less specific ones.

 Empty trie */

 For invalid key, find the leftmost node in the trie */

 Try to find the exact node for the given key */

	/* The node with the exactly-matching key has been found,

	 * find the first node in postorder after the matched node.

 did not find anything */

	/* Find the leftmost non-intermediate node, all intermediate nodes

	 * have exact two children, so this function will never return NULL.

 Keys must have struct bpf_lpm_trie_key embedded. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux Socket Filter - Kernel level socket filtering

 *

 * Based on the design of the Berkeley Packet Filter. The new

 * internal format has been designed by PLUMgrid:

 *

 *	Copyright (c) 2011 - 2014 PLUMgrid, http://plumgrid.com

 *

 * Authors:

 *

 *	Jay Schulist <jschlst@samba.org>

 *	Alexei Starovoitov <ast@plumgrid.com>

 *	Daniel Borkmann <dborkman@redhat.com>

 *

 * Andi Kleen - Fix a few bad bugs and races.

 * Kris Katterjohn - Added many additional checks in bpf_check_classic()

 Registers */

 Named registers */

/* No hurry in this branch

 *

 * Exported for the bpf jit load helper.

/* The jit engine is responsible to provide an array

 * for insn_off to the jited_off mapping (insn_to_jit_off).

 *

 * The idx to this array is the insn_off.  Hence, the insn_off

 * here is relative to the prog itself instead of the main prog.

 * This array has one entry for each xlated bpf insn.

 *

 * jited_off is the byte off to the last byte of the jited insn.

 *

 * Hence, with

 * insn_start:

 *      The first bpf insn off of the prog.  The insn off

 *      here is relative to the main prog.

 *      e.g. if prog is a subprog, insn_start > 0

 * linfo_idx:

 *      The prog's idx to prog->aux->linfo and jited_linfo

 *

 * jited_linfo[linfo_idx] = prog->bpf_func

 *

 * For i > linfo_idx,

 *

 * jited_linfo[i] = prog->bpf_func +

 *	insn_to_jit_off[linfo[i].insn_off - insn_start - 1]

 Userspace did not provide linfo */

		/* The verifier ensures that linfo[i].insn_off is

		 * strictly increasing

		/* We keep fp->aux from fp_old around in the new

		 * reallocated structure.

	/* We need to take out the map fd for the digest calculation

	 * since they are unstable from user space side.

		/* In the probing pass we still operate on the original,

		 * unpatched image in order to check overflows before we

		 * do any other adjustments. Therefore skip the patchlet.

 Adjust offset of jmps if we cross patch boundaries. */

 Push all off < linfo[i].insn_off by delta */

 Since our patchlet doesn't expand the image, we're done. */

	/* Reject anything that would potentially let the insn->off

	 * target overflow when we have excessive program expansions.

	 * We need to probe here before we do any reallocation where

	 * we afterwards may not fail anymore.

	/* Several new instructions need to be inserted. Make room

	 * for them. Likely, there's no need for a new allocation as

	 * last page could have large enough tailroom.

	/* Patching happens in 3 steps:

	 *

	 * 1) Move over tail of insnsi from next instruction onwards,

	 *    so we can patch the single target insn with one or more

	 *    new ones (patching is always from 1 to n insns, n > 0).

	 * 2) Inject new instructions at the target location.

	 * 3) Adjust branch offsets if necessary.

	/* We are guaranteed to not fail at this point, otherwise

	 * the ship has sailed to reverse to the original state. An

	 * overflow cannot happen at this point.

	/* Branch offsets can't overflow when program is shrinking, no need

	 * to call bpf_adj_branches(..., true) here

 All BPF JIT sysctl knobs here. */

		     /* name has been null terminated.

		      * We should need +1 for the '_' preceding

		      * the name.  However, the null character

		      * is double counted between the name and the

		      * sizeof("bpf_prog_") above, so we omit

		      * the +1 here.

 prog->aux->name will be ignored if full btf name is available */

/* Can be overridden by an arch's JIT compiler if it has a custom,

 * dedicated BPF backend memory area, or if neither of the two

 * below apply.

 Only used as heuristic here to derive limit. */

	/* Most of BPF filters are really small, but if some of them

	 * fill a page, allow at least 128 extra bytes to insert a

	 * random section of illegal instructions.

 Fill space with illegal/arch-dep instructions. */

 Leave a random number of instructions before BPF code. */

/* This symbol is only overridden by archs that have different

 * requirements than the usual eBPF JITs, f.e. when they only

 * implement cBPF JIT, do not set images read-only, etc.

		/* Place-holder address till the last pass has collected

		 * all addresses for JITed subprograms in which case we

		 * can pick them up from prog->aux.

		/* Address of a BPF helper call. Since part of the core

		 * kernel, it's always at a fixed location. __bpf_call_base

		 * and the helper with imm relative to it are both in core

		 * kernel.

	/* Constraints on AX register:

	 *

	 * AX register is inaccessible from user space. It is mapped in

	 * all JITs, and used here for constant blinding rewrites. It is

	 * typically "stateless" meaning its contents are only valid within

	 * the executed instruction, but not across several instructions.

	 * There are a few exceptions however which are further detailed

	 * below.

	 *

	 * Constant blinding is only used by JITs, not in the interpreter.

	 * The interpreter uses AX in some occasions as a local temporary

	 * register e.g. in DIV or MOD instructions.

	 *

	 * In restricted circumstances, the verifier can also use the AX

	 * register for rewrites as long as they do not interfere with

	 * the above cases!

 Accommodate for extra offset in case of a backjump. */

 Accommodate for extra offset in case of a backjump. */

 Part 2 of BPF_LD | BPF_IMM | BPF_DW. */

		/* aux->prog still points to the fp_other one, so

		 * when promoting the clone to the real program,

		 * this still needs to be adapted.

	/* aux was stolen by the other clone, so we cannot free

	 * it from this path! It will be freed eventually by the

	 * other program on release.

	 *

	 * At this point, we don't need a deferred release since

	 * clone is guaranteed to not be locked.

	/* We have to repoint aux->prog to self, as we don't

	 * know whether fp here is the clone or the original.

		/* We temporarily need to hold the original ld64 insn

		 * so that we can still access the first part in the

		 * second blinding run.

			/* Patching may have repointed aux->prog during

			 * realloc from the original one, so we need to

			 * fix it up here on error.

 Walk new program and skip insns we just inserted. */

 CONFIG_BPF_JIT */

/* Base function for offset calculation. Needs to go into .text section,

 * therefore keeping it non-static as well; will also be used by JITs

 * anyway later on, so do not let the compiler omit it. This also needs

 * to go into kallsyms for correlation from e.g. bpftool, so naming

 * must not change.

 All UAPI available opcodes. */

 32 bit ALU operations. */		\

   Register based. */			\

   Immediate based. */		\

 64 bit ALU operations. */		\

   Register based. */			\

   Immediate based. */		\

 Call instruction. */			\

 Exit instruction. */			\

 32-bit Jump instructions. */		\

   Register based. */			\

   Immediate based. */		\

 Jump instructions. */		\

   Register based. */			\

   Immediate based. */		\

 Store instructions. */		\

   Register based. */			\

   Immediate based. */		\

 Load instructions. */		\

   Register based. */			\

   Immediate based. */		\

 Now overwrite non-defaults ... */

 UAPI exposed, but rewritten opcodes. cBPF carry-over. */

/**

 *	___bpf_prog_run - run eBPF program on a given context

 *	@regs: is the array of MAX_BPF_EXT_REG eBPF pseudo-registers

 *	@insn: is the array of eBPF instructions

 *

 * Decode and execute eBPF instructions.

 *

 * Return: whatever value is in %BPF_R0 at program exit

 Now overwrite non-defaults ... */

 Non-UAPI available opcodes. */

	/* Explicitly mask the register-based shift amounts with 63 or 31

	 * to avoid undefined behavior. Normally this won't affect the

	 * generated code, for example, in case of native 64 bit archs such

	 * as x86-64 or arm64, the compiler is optimizing the AND away for

	 * the interpreter. In case of JITs, each of the JIT backends compiles

	 * the BPF shift operations to machine instructions which produce

	 * implementation-defined results in such a case; the resulting

	 * contents of the register may be arbitrary, but program behaviour

	 * as a whole remains defined. In other words, in case of JIT backends,

	 * the AND must /not/ be added to the emitted LSH/RSH/ARSH translation.

 ALU (shifts) */

 ALU (rest) */

 CALL */

		/* Function call scratches BPF_R1-BPF_R5 registers,

		 * preserves BPF_R6-BPF_R9, and stores return value

		 * into BPF_R0.

		/* ARG1 at this point is guaranteed to point to CTX from

		 * the verifier side due to the fact that the tail call is

		 * handled like a helper, that is, bpf_tail_call_proto,

		 * where arg1_type is ARG_PTR_TO_CTX.

 JMP */

 ST, STX and LDX*/

		/* Speculation barrier for mitigating Speculative Store Bypass.

		 * In case of arm64, we rely on the firmware mitigation as

		 * controlled via the ssbd kernel parameter. Whenever the

		 * mitigation is enabled, it works for all of the kernel code

		 * with no need to provide any additional instructions here.

		 * In case of x86, we use 'lfence' insn for mitigation. We

		 * reuse preexisting logic from Spectre v1 mitigation that

		 * happens to produce the required code on x86 for v4 as well.

		/* If we ever reach this, we have a bug somewhere. Die hard here

		 * instead of just returning 0; we could be somewhere in a subprog,

		 * so execution could continue otherwise which we do /not/ want.

		 *

		 * Note, verifier whitelists all opcodes in bpf_opcode_in_insntable().

	/* If this handler ever gets executed, then BPF_JIT_ALWAYS_ON

	 * is not working properly, so warn about it!

		/* There's no owner yet where we could check for

		 * compatibility.

/**

 *	bpf_prog_select_runtime - select exec runtime for BPF program

 *	@fp: bpf_prog populated with internal BPF program

 *	@err: pointer to error variable

 *

 * Try to JIT eBPF program, if JIT is not available, use interpreter.

 * The BPF program will be executed via bpf_prog_run() function.

 *

 * Return: the &fp argument along with &err set to 0 for success or

 * a negative errno code on failure

	/* In case of BPF to BPF calls, verifier did all the prep

	 * work with regards to JITing, etc.

	/* eBPF JITs can rewrite the program in case constant

	 * blinding is active. However, in case of error during

	 * blinding, bpf_int_jit_compile() must always return a

	 * valid program, which in this case would simply not

	 * be JITed, but falls back to the interpreter.

	/* The tail call compatibility check can only be done at

	 * this late stage as we need to determine, if we deal

	 * with JITed or non JITed program concatenations and not

	 * all eBPF JITs might immediately support all features.

/* to avoid allocating empty bpf_prog_array for cgroups that

 * don't have bpf program attached use one global 'empty_prog_array'

 * It will not be modified the caller of bpf_prog_array_alloc()

 * (since caller requested prog_cnt == 0)

 * that pointer should be 'freed' by bpf_prog_array_free()

	/* users of this function are doing:

	 * cnt = bpf_prog_array_length();

	 * if (cnt > 0)

	 *     bpf_prog_array_copy_to_user(..., cnt);

	 * so below kcalloc doesn't need extra cnt > 0 check.

/**

 * bpf_prog_array_delete_safe_at() - Replaces the program at the given

 *                                   index into the program array with

 *                                   a dummy no-op program.

 * @array: a bpf_prog_array

 * @index: the index of the program to replace

 *

 * Skips over dummy programs, by not counting them, when calculating

 * the position of the program to replace.

 *

 * Return:

 * * 0		- Success

 * * -EINVAL	- Invalid index value. Must be a non-negative integer.

 * * -ENOENT	- Index out of range

/**

 * bpf_prog_array_update_at() - Updates the program at the given index

 *                              into the program array.

 * @array: a bpf_prog_array

 * @index: the index of the program to update

 * @prog: the program to insert into the array

 *

 * Skips over dummy programs, by not counting them, when calculating

 * the position of the program to update.

 *

 * Return:

 * * 0		- Success

 * * -EINVAL	- Invalid index value. Must be a non-negative integer.

 * * -ENOENT	- Index out of range

	/* Figure out how many existing progs we need to carry over to

	 * the new array.

 How many progs (not NULL) will be in the new array? */

 Do we have any prog (not NULL) in the new array? */

 +1 as the end of prog_array is marked with NULL */

 Fill in the new prog array */

 return early if user requested only program count or nothing to copy */

 this function is called under trace/bpf_trace.c: bpf_event_mutex */

		/* We can just unlink the subprog poke descriptor table as

		 * it was originally linked to the main program and is also

		 * released along with it.

 Free internal BPF program */

 RNG for unpriviledged user space with separated state from prandom_u32(). */

	/* Should someone ever have the rather unwise idea to use some

	 * of the registers passed into this function, then note that

	 * this function is called from native eBPF and classic-to-eBPF

	 * transformations. Register assignments from both sides are

	 * different, f.e. classic always sets fn(ctx, A, X) here.

 Weak definitions of helper functions in case we don't have bpf syscall. */

 Always built-in helper functions. */

/* Stub for JITs that only support cBPF. eBPF programs are interpreted.

 * It is encouraged to implement bpf_int_jit_compile() instead, so that

 * eBPF and implicitly also cBPF can get JITed!

/* Stub for JITs that support eBPF. All cBPF code gets transformed into

 * eBPF by the kernel and is later compiled by bpf_int_jit_compile().

/* Return TRUE if the JIT backend wants verifier to enable sub-register usage

 * analysis code and wants explicit zero extension inserted by verifier.

 * Otherwise, return FALSE.

 *

 * The verifier inserts an explicit zero extension after BPF_CMPXCHGs even if

 * you don't override this. JITs that don't want these extra insns can detect

 * them using insn_is_zext.

/* To execute LD_ABS/LD_IND instructions __bpf_prog_run() may call

 * skb_copy_bits(), so provide a weak definition of it for NET-less config.

 All definitions of tracepoints related to BPF. */

 SPDX-License-Identifier: GPL-2.0-only

 Copyright (c) 2020 Facebook */

 SPDX-License-Identifier: GPL-2.0

/*

 * Provide kernel BTF information for introspection and use by eBPF tools.

 See scripts/link-vmlinux.sh, gen_btf() func for details */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2018 Facebook

 The caller must hold the reuseport_lock */

		/*

		 * Do not move this NULL assignment outside of

		 * sk->sk_callback_lock because there is

		 * a race with reuseport_array_free()

		 * which does not hold the reuseport_lock.

 Called from syscall only */

	/*

	 * ops->map_*_elem() will not be able to access this

	 * array now. Hence, this function only races with

	 * bpf_sk_reuseport_detach() which was triggered by

	 * close() or disconnect().

	 *

	 * This function and bpf_sk_reuseport_detach() are

	 * both removing sk from "array".  Who removes it

	 * first does not matter.

	 *

	 * The only concern here is bpf_sk_reuseport_detach()

	 * may access "array" which is being freed here.

	 * bpf_sk_reuseport_detach() access this "array"

	 * through sk->sk_user_data _and_ with sk->sk_callback_lock

	 * held which is enough because this "array" is not freed

	 * until all sk->sk_user_data has stopped referencing this "array".

	 *

	 * Hence, due to the above, taking "reuseport_lock" is not

	 * needed here.

	/*

	 * Since reuseport_lock is not taken, sk is accessed under

	 * rcu_read_lock()

			/*

			 * No need for WRITE_ONCE(). At this point,

			 * no one is reading it without taking the

			 * sk->sk_callback_lock.

	/*

	 * Once reaching here, all sk->sk_user_data is not

	 * referenceing this "array".  "array" can be freed now.

 allocate all map elements and zero-initialize them */

 copy mandatory map attributes */

	/*

	 * sk must be hashed (i.e. listening in the TCP case or binded

	 * in the UDP case) and

	 * it must also be a SO_REUSEPORT sk (i.e. reuse cannot be NULL).

	 *

	 * Also, sk will be used in bpf helper that is protected by

	 * rcu_read_lock().

 READ_ONCE because the sk->sk_callback_lock may not be held here */

/*

 * Called from syscall only.

 * The "nsk" in the fd refcnt.

 * The "osk" and "reuse" are protected by reuseport_lock.

 Quick checks before taking reuseport_lock */

	/*

	 * Some of the checks only need reuseport_lock

	 * but it is done under sk_callback_lock also

	 * for simplicity reason.

 Called from syscall */

 SPDX-License-Identifier: GPL-2.0-only

 Copyright (c) 2019 Facebook */

 protect map_update */

	/* progs has all the bpf_prog that is populated

	 * to the func ptr of the kernel's struct

	 * (in kvalue.data).

	/* image is a page that has all the trampolines

	 * that stores the func args before calling the bpf_prog.

	 * A PAGE_SIZE "image" is enough to store all trampoline for

	 * "progs[]".

	/* uvalue->data stores the kernel struct

	 * (e.g. tcp_congestion_ops) that is more useful

	 * to userspace than the kvalue.  For example,

	 * the bpf_prog's id is stored instead of the kernel

	 * address of a func ptr.

	/* kvalue.data stores the actual kernel's struct

	 * (e.g. tcp_congestion_ops) that will be

	 * registered to the kernel subsystem.

/* bpf_struct_ops_##_name (e.g. bpf_struct_ops_tcp_congestion_ops) is

 * the map's value exposed to the userspace and its btf-type-id is

 * stored at the map->btf_vmlinux_value_type_id.

 *

 Ensure BTF type is emitted for "struct bpf_struct_ops_##_name" */

 Pair with smp_store_release() during map_update */

	/* No lock is needed.  state and refcnt do not need

	 * to be updated together under atomic context.

 The ->init_member() has handled this member */

		/* If st_ops->init_member does not handle it,

		 * we will only handle func ptrs and zero-ed members

		 * here.  Reject everything else.

 All non func ptr member must be 0 */

 Similar check as the attr->attach_prog_fd */

 put prog_id to udata */

		/* Pair with smp_load_acquire() during lookup_elem().

		 * It ensures the above udata updates (e.g. prog->aux->id)

		 * can be seen once BPF_STRUCT_OPS_STATE_INUSE is set.

	/* Error during st_ops->reg().  It is very unlikely since

	 * the above init_member() should have caught it earlier

	 * before reg().  The only possibility is if there was a race

	 * in registering the struct_ops (under the same name) to

	 * a sub-system through different struct_ops's maps.

 Should never happen.  Treat it as not found. */

		/* kvalue stores the

		 * struct bpf_struct_ops_tcp_congestions_ops

/* "const void *" because some subsystem is

 * passing a const (e.g. const struct tcp_congestion_ops *)

		/* The struct_ops's function may switch to another struct_ops.

		 *

		 * For example, bpf_tcp_cc_x->init() may switch to

		 * another tcp_cc_y by calling

		 * setsockopt(TCP_CONGESTION, "tcp_cc_y").

		 * During the switch,  bpf_struct_ops_put(tcp_cc_x) is called

		 * and its map->refcnt may reach 0 which then free its

		 * trampoline image while tcp_cc_x is still running.

		 *

		 * Thus, a rcu grace period is needed here.

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 Facebook

 Helpers to get the local list index */

 Local list helpers */

 bpf_lru_node helpers */

	/* If the removing node is the next_inactive_rotation candidate,

	 * move the next_inactive_rotation pointer also.

 Move nodes from local list to the LRU list */

/* Move nodes between or within active and inactive list (like

 * active to inactive, inactive to active or tail of active back to

 * the head of active).

	/* If the moving node is the next_inactive_rotation candidate,

	 * move the next_inactive_rotation pointer also.

/* Rotate the active list:

 * 1. Start from tail

 * 2. If the node has the ref bit set, it will be rotated

 *    back to the head of active list with the ref bit cleared.

 *    Give this node one more chance to survive in the active list.

 * 3. If the ref bit is not set, move it to the head of the

 *    inactive list.

 * 4. It will at most scan nr_scans nodes

/* Rotate the inactive list.  It starts from the next_inactive_rotation

 * 1. If the node has ref bit set, it will be moved to the head

 *    of active list with the ref bit cleared.

 * 2. If the node does not have ref bit set, it will leave it

 *    at its current location (i.e. do nothing) so that it can

 *    be considered during the next inactive_shrink.

 * 3. It will at most scan nr_scans nodes

/* Shrink the inactive list.  It starts from the tail of the

 * inactive list and only move the nodes without the ref bit

 * set to the designated free list.

/* 1. Rotate the active list (if needed)

 * 2. Always rotate the inactive list

/* Calls __bpf_lru_list_shrink_inactive() to shrink some

 * ref-bit-cleared nodes and move them to the designated

 * free list.

 *

 * If it cannot get a free node after calling

 * __bpf_lru_list_shrink_inactive().  It will just remove

 * one node from either inactive or active list without

 * honoring the ref-bit.  It prefers inactive list to active

 * list in this situation.

 Do a force shrink by ignoring the reference bit */

 Flush the nodes from the local pending list to the LRU list */

 Get from the tail (i.e. older element) of the pending list. */

	/* No free nodes found from the local free list and

	 * the global LRU list.

	 *

	 * Steal from the local free/pending list of the

	 * current CPU and remote CPU in RR.  It starts

	 * with the loc_l->next_steal CPU.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2019 Facebook

 * Copyright 2020 Google LLC.

	/* Neither the bpf_prog nor the bpf-map's syscall

	 * could be modifying the local_storage->list now.

	 * Thus, no elem can be added-to or deleted-from the

	 * local_storage->list by the bpf_prog or by the bpf-map's syscall.

	 *

	 * It is racing with bpf_local_storage_map_free() alone

	 * when unlinking elem from the local_storage->list and

	 * the map's bucket->list.

		/* Always unlink from map before unlinking from

		 * local_storage.

	/* free_inoode_storage should always be true as long as

	 * local_storage->list was non-empty.

	/* explicitly check that the inode_storage_ptr is not

	 * NULL as inode_storage_lookup returns NULL in this case and

	 * bpf_local_storage_update expects the owner to have a

	 * valid storage pointer.

	/* This helper must only called from where the inode is guaranteed

	 * to have a refcount and cannot be freed.

	/* This helper must only called from where the inode is guaranteed

	 * to have a refcount and cannot be freed.

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2017 Facebook

 Does not support >1 level map-in-map */

 In some cases verifier needs to access beyond just base map. */

 Misc members not needed in bpf_map_meta_equal() check. */

 No need to compare ops because it is covered by map_type */

 not used */,

	/* ptr->ops->map_free() has to go through one

	 * rcu grace period by itself.

 SPDX-License-Identifier: GPL-2.0

 send the start magic to let UMD proceed with loading BPF progs */

 receive bpf_link IDs and names from UMD */

 send the last magic to UMD. It will do a normal exit. */

 kill UMD in case it's still there due to earlier error */

 SPDX-License-Identifier: GPL-2.0

 Copyright (c) 2020 Facebook */

 SPDX-License-Identifier: GPL-2.0

 Copyright (c) 2020 Facebook */

 now stdin and stderr point to /dev/console */

 libbpf opens BPF object and loads it into the kernel */

		/* iterators.skel.h is little endian.

		 * libbpf doesn't support automatic little->big conversion

		 * of BPF bytecode yet.

		 * The program load will fail in such case.

 send two bpf_link IDs with names to the kernel */

	/* The kernel will proceed with pinnging the links in bpffs.

	 * UMD will wait on read from pipe.

 SPDX-License-Identifier: GPL-2.0

/*

 * This is a module to test the HMM (Heterogeneous Memory Management)

 * mirror and zone device private memory migration APIs of the kernel.

 * Userspace programs can register with the driver to mirror their own address

 * space and can use the device to read/write any valid virtual address.

/*

 * Data structure to track address ranges and register for mmu interval

 * notifier updates.

/*

 * Data attached to the open device file.

 * Note that it might be shared after a fork().

/*

 * ZONE_DEVICE pages for migration and simulating device memory.

/*

 * Per device data.

 protects the above */

 protects the above */

 Mirror this process address space */

		/*

		 * Since we asked for hmm_range_fault() to populate pages,

		 * it shouldn't return an error entry on success.

	/*

	 * The XArray doesn't hold references to pages since it relies on

	 * the mmu notifier to clear page pointers when they become stale.

	 * Therefore, it is OK to just clear the entry.

	/*

	 * Ignore invalidation callbacks for device private pages since

	 * the invalidation is handled as part of the migration process.

 Since the mm is for the mirrored process, get a reference first. */

	/*

	 * This is a fake device so we alloc real system memory to store

	 * our device memory.

		/*

		 * Note that spage might be NULL which is OK since it is an

		 * unallocated pte_none() or read-only zero page.

		/*

		 * Normally, a device would use the page->zone_device_data to

		 * point to the mirror but here we use it to hold the page for

		 * the simulated device memory and that page holds the pointer

		 * to the mirror.

 Map the migrated pages into the device's page tables. */

 Map the migrated pages into the device's page tables. */

		/*

		 * Store the page that holds the data so the page table

		 * doesn't have to deal with ZONE_DEVICE private pages.

 Since the mm is for the mirrored process, get a reference first. */

 Return the migrated data for verification. */

 Since the mm is for the mirrored process, get a reference first. */

 Return the migrated data for verification. */

 Is the page migrated to this device or some other? */

	/*

	 * Snapshots only need to set the sequence number since any

	 * invalidation in the interval invalidates the whole snapshot.

 Since the mm is for the mirrored process, get a reference first. */

	/*

	 * Register a temporary notifier to detect invalidations even if it

	 * overlaps with other mmu_interval_notifiers.

	/*

	 * Normally, a device would use the page->zone_device_data to point to

	 * the mirror but here we use it to hold the page for the simulated

	 * device memory and that page holds the pointer to the mirror.

 FIXME demonstrate how we can adjust migrate range */

	/*

	 * No device finalize step is needed since

	 * dmirror_devmem_fault_alloc_and_copy() will have already

	 * invalidated the device page table.

 Build a list of free ZONE_DEVICE private struct pages */

 SPDX-License-Identifier: GPL-2.0

/*

 * Dynamic byte queue limits.  See include/linux/dynamic_queue_limits.h

 *

 * Copyright (c) 2011, Tom Herbert <therbert@google.com>

 Records completed count and recalculates the queue limit */

 Can't complete more than what's in queue */

		/*

		 * Queue considered starved if:

		 *   - The queue was over-limit in the last interval,

		 *     and there is no more data in the queue.

		 *  OR

		 *   - The queue was over-limit in the previous interval and

		 *     when enqueuing it was possible that all queued data

		 *     had been consumed.  This covers the case when queue

		 *     may have becomes starved between completion processing

		 *     running and next time enqueue was scheduled.

		 *

		 *     When queue is starved increase the limit by the amount

		 *     of bytes both sent and completed in the last interval,

		 *     plus any previous over-limit.

		/*

		 * Queue was not starved, check if the limit can be decreased.

		 * A decrease is only considered if the queue has been busy in

		 * the whole interval (the check above).

		 *

		 * If there is slack, the amount of excess data queued above

		 * the amount needed to prevent starvation, the queue limit

		 * can be decreased.  To avoid hysteresis we consider the

		 * minimum amount of slack found over several iterations of the

		 * completion routine.

		/*

		 * Slack is the maximum of

		 *   - The queue limit plus previous over-limit minus twice

		 *     the number of objects completed.  Note that two times

		 *     number of completed bytes is a basis for an upper bound

		 *     of the limit.

		 *   - Portion of objects in the last queuing operation that

		 *     was not part of non-zero previous over-limit.  That is

		 *     "round down" by non-overlimit portion of the last

		 *     queueing operation.

 Enforce bounds on limit */

 Reset all dynamic values */

 SPDX-License-Identifier: GPL-2.0

 out-of-line parts */

/**

 * check_zeroed_user: check if a userspace buffer only contains zero bytes

 * @from: Source address, in userspace.

 * @size: Size of buffer.

 *

 * This is effectively shorthand for "memchr_inv(from, 0, size) == NULL" for

 * userspace addresses (and is more efficient because we don't care where the

 * first non-zero byte is).

 *

 * Returns:

 *  * 0: There were non-zero bytes present in the buffer.

 *  * 1: The buffer was full of zero bytes.

 *  * -EFAULT: access to userspace failed.

 SPDX-License-Identifier: GPL-2.0-or-later

/* bit search implementation

 *

 * Copyright (C) 2004 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

 *

 * Copyright (C) 2008 IBM Corporation

 * 'find_last_bit' is written by Rusty Russell <rusty@rustcorp.com.au>

 * (Inspired by David Howell's find_next_bit implementation)

 *

 * Rewritten by Yury Norov <yury.norov@gmail.com> to decrease

 * size and improve performance, 2015.

/*

 * This is a common helper function for find_next_bit, find_next_zero_bit, and

 * find_next_and_bit. The differences are:

 *  - The "invert" argument, which is XORed with each fetched word before

 *    searching it for one bits.

 *  - The optional "addr2", which is anded with "addr1" if present.

 Handle 1st word. */

/*

 * Find the first set bit in a memory region.

/*

 * Find the first cleared bit in a memory region.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2021 Intel Corporation

 * Author: Johannes Berg <johannes@sipsolutions.net>

 CONFIG_LOGIC_IOMEM_FALLBACK */

 CONFIG_LOGIC_IOMEM_FALLBACK */

 SPDX-License-Identifier: GPL-2.0

/*

 * Merge two NULL-terminated pointer arrays into a newly allocated

 * array, which is also NULL-terminated. Nomenclature is inspired by

 * memset_p() and memcat() found elsewhere in the kernel source tree.

 count the elements in both arrays */

 one for the NULL-terminator */

 nr -> last index; p points to NULL in b[] */

 SPDX-License-Identifier: GPL-2.0

 Task has no stack, so the task isn't in a syscall. */

/**

 * task_current_syscall - Discover what a blocked task is doing.

 * @target:		thread to examine

 * @info:		structure with the following fields:

 *			 .sp        - filled with user stack pointer

 *			 .data.nr   - filled with system call number or -1

 *			 .data.args - filled with @maxargs system call arguments

 *			 .data.instruction_pointer - filled with user PC

 *

 * If @target is blocked in a system call, returns zero with @info.data.nr

 * set to the call's number and @info.data.args filled in with its

 * arguments. Registers not used for system call arguments may not be available

 * and it is not kosher to use &struct user_regset calls while the system

 * call is still in progress.  Note we may get this result if @target

 * has finished its system call but not yet returned to user mode, such

 * as when it's stopped for signal handling or syscall exit tracing.

 *

 * If @target is blocked in the kernel during a fault or exception,

 * returns zero with *@info.data.nr set to -1 and does not fill in

 * @info.data.args. If so, it's now safe to examine @target using

 * &struct user_regset get() calls as long as we're sure @target won't return

 * to user mode.

 *

 * Returns -%EAGAIN if @target does not remain blocked.

/*

 * Generic infrastructure for lifetime debugging of objects.

 *

 * Started by Thomas Gleixner

 *

 * Copyright (C) 2008, Thomas Gleixner <tglx@linutronix.de>

 *

 * For licencing details see kernel-base/COPYING

/*

 * We limit the freeing of debug objects via workqueue at a maximum

 * frequency of 10Hz and about 1024 objects for each freeing operation.

 * So it is freeing at most 10k debug objects per second.

/*

 * Debug object percpu free list

 * Access is protected by disabling irq

/*

 * Because of the presence of percpu free pools, obj_pool_free will

 * under-count those in the percpu free pools. Similarly, obj_pool_used

 * will over-count those in the percpu free pools. Adjustments will be

 * made at debug_stats_show(). Both obj_pool_min_free and obj_pool_max_used

 * can be off.

 The number of objs on the global free list */

/*

 * Track numbers of kmem_cache_alloc()/free() calls done.

	/*

	 * Reuse objs from the global free list; they will be reinitialized

	 * when allocating.

	 *

	 * Both obj_nr_tofree and obj_pool_free are checked locklessly; the

	 * READ_ONCE()s pair with the WRITE_ONCE()s in pool_lock critical

	 * sections.

		/*

		 * Recheck with the lock held as the worker thread might have

		 * won the race and freed the global free list already.

/*

 * Lookup an object in the hash bucket.

/*

 * Allocate a new object from the hlist

/*

 * Allocate a new object. If the pool is empty, switch off the debugger.

 * Must be called with interrupts disabled.

		/*

		 * Looking ahead, allocate one batch of debug objects and

		 * put them into the percpu free pool.

/*

 * workqueue function to free objects.

 *

 * To reduce contention on the global pool_lock, the actual freeing of

 * debug objects will be delayed if the pool_lock is busy.

	/*

	 * The objs on the pool list might be allocated before the work is

	 * run, so recheck if pool list it full or not, if not fill pool

	 * list from the global free list. As it is likely that a workload

	 * may be gearing up to use more and more objects, don't free any

	 * of them until the next round.

	/*

	 * Pool list is already full and there are still objs on the free

	 * list. Move remaining free objs to a temporary list to free the

	 * memory outside the pool_lock held region.

	/*

	 * Try to free it into the percpu pool first.

	/*

	 * As the percpu pool is full, look ahead and pull out a batch

	 * of objects from the percpu pool and free them as well.

			/*

			 * Free one more batch of objects from obj_pool.

/*

 * Put the object back into the pool and schedule work to free objects

 * if necessary.

 Remote access is safe as the CPU is dead already */

/*

 * We run out of memory. That means we probably have tons of objects

 * allocated.

 Now free them */

/*

 * We use the pfn of the address for the hash. That way we can check

 * for freed objects simply by checking the affected bucket.

/*

 * Try to repair the damage, so we have a better chance to get useful

 * debug output.

	/*

	 * On RT enabled kernels the pool refill must happen in preemptible

	 * context:

/**

 * debug_object_init - debug checks when an object is initialized

 * @addr:	address of the object

 * @descr:	pointer to an object specific debug description structure

/**

 * debug_object_init_on_stack - debug checks when an object on stack is

 *				initialized

 * @addr:	address of the object

 * @descr:	pointer to an object specific debug description structure

/**

 * debug_object_activate - debug checks when an object is activated

 * @addr:	address of the object

 * @descr:	pointer to an object specific debug description structure

 * Returns 0 for success, -EINVAL for check failed.

	/*

	 * We are here when a static object is activated. We

	 * let the type specific code confirm whether this is

	 * true or not. if true, we just make sure that the

	 * static object is tracked in the object tracker. If

	 * not, this must be a bug, so we try to fix it up.

 track this static object */

/**

 * debug_object_deactivate - debug checks when an object is deactivated

 * @addr:	address of the object

 * @descr:	pointer to an object specific debug description structure

/**

 * debug_object_destroy - debug checks when an object is destroyed

 * @addr:	address of the object

 * @descr:	pointer to an object specific debug description structure

/**

 * debug_object_free - debug checks when an object is freed

 * @addr:	address of the object

 * @descr:	pointer to an object specific debug description structure

/**

 * debug_object_assert_init - debug checks when object should be init-ed

 * @addr:	address of the object

 * @descr:	pointer to an object specific debug description structure

		/*

		 * Maybe the object is static, and we let the type specific

		 * code confirm. Track this static object if true, else invoke

		 * fixup.

 Track this static object */

/**

 * debug_object_active_state - debug checks object usage state machine

 * @addr:	address of the object

 * @descr:	pointer to an object specific debug description structure

 * @expect:	expected state

 * @next:	state to move to if expected state is found

 Schedule work to actually kmem_cache_free() objects */

 Random data structure for the self test */

/*

 * fixup_init is called when:

 * - an active object is initialized

/*

 * fixup_activate is called when:

 * - an active object is activated

 * - an unknown non-static object is activated

/*

 * fixup_destroy is called when:

 * - an active object is destroyed

/*

 * fixup_free is called when:

 * - an active object is freed

/*

 * Called during early boot to initialize the hash buckets and link

 * the static object pool objects into the poll list. After this call

 * the object tracker is fully operational.

/*

 * Convert the statically allocated objects to dynamic ones:

	/*

	 * debug_objects_mem_init() is now called early that only one CPU is up

	 * and interrupts have been disabled, so it is safe to replace the

	 * active object references.

 Remove the statically allocated objects from the pool */

 Move the allocated objects to the pool */

 Replace the active object references */

 copy object data */

/*

 * Called after the kmem_caches are functional to setup a dedicated

 * cache pool, which has the SLAB_DEBUG_OBJECTS flag set. This flag

 * prevents that the debug code is called on kmem_cache_free() for the

 * debug tracker objects to avoid recursive calls.

	/*

	 * Initialize the percpu object pools

	 *

	 * Initialization is not strictly necessary, but was done for

	 * completeness.

	/*

	 * Increase the thresholds for allocating and freeing objects

	 * according to the number of possible CPUs available in the system.

 SPDX-License-Identifier: GPL-2.0-only

 CONFIG_FAULT_INJECTION_DEBUG_FS */

 SPDX-License-Identifier: GPL-2.0

/*

 * kobject.c - library routines for handling generic kernel objects

 *

 * Copyright (c) 2002-2003 Patrick Mochel <mochel@osdl.org>

 * Copyright (c) 2006-2007 Greg Kroah-Hartman <greg@kroah.com>

 * Copyright (c) 2006-2007 Novell Inc.

 *

 * Please see the file Documentation/core-api/kobject.rst for critical information

 * about using the kobject interface.

/**

 * kobject_namespace() - Return @kobj's namespace tag.

 * @kobj: kobject in question

 *

 * Returns namespace tag of @kobj if its parent has namespace ops enabled

 * and thus @kobj should have a namespace tag associated with it.  Returns

 * %NULL otherwise.

/**

 * kobject_get_ownership() - Get sysfs ownership data for @kobj.

 * @kobj: kobject in question

 * @uid: kernel user ID for sysfs objects

 * @gid: kernel group ID for sysfs objects

 *

 * Returns initial uid/gid pair that should be used when creating sysfs

 * representation of given kobject. Normally used to adjust ownership of

 * objects in a container.

/*

 * populate_dir - populate directory with attributes.

 * @kobj: object we're working on.

 *

 * Most subsystems have a set of default attributes that are associated

 * with an object that registers with them.  This is a helper called during

 * object registration that loops through the default attributes of the

 * subsystem and creates attributes files for them in sysfs.

	/*

	 * @kobj->sd may be deleted by an ancestor going away.  Hold an

	 * extra reference so that it stays until @kobj is gone.

	/*

	 * If @kobj has ns_ops, its children need to be filtered based on

	 * their namespace tags.  Enable namespace support on @kobj->sd.

	/* walk up the ancestors until we hit the one pointing to the

	 * root.

	 * Add 1 to strlen for leading '/' of each level.

 back up enough to print this name with '/' */

/**

 * kobject_get_path() - Allocate memory and fill in the path for @kobj.

 * @kobj:	kobject in question, with which to build the path

 * @gfp_mask:	the allocation type used to allocate the path

 *

 * Return: The newly allocated memory, caller must free with kfree().

 add the kobject to its kset's list */

 remove the kobject from its kset's list */

 join kset if set, use it as parent if we do not already have one */

 be noisy on error issues */

/**

 * kobject_set_name_vargs() - Set the name of a kobject.

 * @kobj: struct kobject to set the name of

 * @fmt: format string used to build the name

 * @vargs: vargs to format the string.

	/*

	 * ewww... some of these buggers have '/' in the name ... If

	 * that's the case, we need to make sure we have an actual

	 * allocated copy to modify, since kvasprintf_const may have

	 * returned something from .rodata.

/**

 * kobject_set_name() - Set the name of a kobject.

 * @kobj: struct kobject to set the name of

 * @fmt: format string used to build the name

 *

 * This sets the name of the kobject.  If you have already added the

 * kobject to the system, you must call kobject_rename() in order to

 * change the name of the kobject.

/**

 * kobject_init() - Initialize a kobject structure.

 * @kobj: pointer to the kobject to initialize

 * @ktype: pointer to the ktype for this kobject.

 *

 * This function will properly initialize a kobject such that it can then

 * be passed to the kobject_add() call.

 *

 * After this function is called, the kobject MUST be cleaned up by a call

 * to kobject_put(), not by a call to kfree directly to ensure that all of

 * the memory is cleaned up properly.

 do not error out as sometimes we can recover */

/**

 * kobject_add() - The main kobject add function.

 * @kobj: the kobject to add

 * @parent: pointer to the parent of the kobject.

 * @fmt: format to name the kobject with.

 *

 * The kobject name is set and added to the kobject hierarchy in this

 * function.

 *

 * If @parent is set, then the parent of the @kobj will be set to it.

 * If @parent is NULL, then the parent of the @kobj will be set to the

 * kobject associated with the kset assigned to this kobject.  If no kset

 * is assigned to the kobject, then the kobject will be located in the

 * root of the sysfs tree.

 *

 * Note, no "add" uevent will be created with this call, the caller should set

 * up all of the necessary sysfs files for the object and then call

 * kobject_uevent() with the UEVENT_ADD parameter to ensure that

 * userspace is properly notified of this kobject's creation.

 *

 * Return: If this function returns an error, kobject_put() must be

 *         called to properly clean up the memory associated with the

 *         object.  Under no instance should the kobject that is passed

 *         to this function be directly freed with a call to kfree(),

 *         that can leak memory.

 *

 *         If this function returns success, kobject_put() must also be called

 *         in order to properly clean up the memory associated with the object.

 *

 *         In short, once this function is called, kobject_put() MUST be called

 *         when the use of the object is finished in order to properly free

 *         everything.

/**

 * kobject_init_and_add() - Initialize a kobject structure and add it to

 *                          the kobject hierarchy.

 * @kobj: pointer to the kobject to initialize

 * @ktype: pointer to the ktype for this kobject.

 * @parent: pointer to the parent of this kobject.

 * @fmt: the name of the kobject.

 *

 * This function combines the call to kobject_init() and kobject_add().

 *

 * If this function returns an error, kobject_put() must be called to

 * properly clean up the memory associated with the object.  This is the

 * same type of error handling after a call to kobject_add() and kobject

 * lifetime rules are the same here.

/**

 * kobject_rename() - Change the name of an object.

 * @kobj: object in question.

 * @new_name: object's new name

 *

 * It is the responsibility of the caller to provide mutual

 * exclusion between two different calls of kobject_rename

 * on the same kobject and to ensure that new_name is valid and

 * won't conflict with other kobjects.

 Install the new kobject name */

	/* This function is mostly/only used for network interface.

	 * Some hotplug package track interfaces by their name and

/**

 * kobject_move() - Move object to another parent.

 * @kobj: object in question.

 * @new_parent: object's new parent (can be NULL)

 old object path */

 send "remove" if the caller did not do it but sent "add" */

/**

 * kobject_del() - Unlink kobject from hierarchy.

 * @kobj: object.

 *

 * This is the function that should be called to delete an object

 * successfully added via kobject_add().

/**

 * kobject_get() - Increment refcount for object.

 * @kobj: object.

/*

 * kobject_cleanup - free kobject resources.

 * @kobj: object to cleanup

 remove from sysfs if the caller did not do it */

 avoid dropping the parent reference unnecessarily */

 free name if we allocated it */

/**

 * kobject_put() - Decrement refcount for object.

 * @kobj: object.

 *

 * Decrement the refcount, and if 0, call kobject_cleanup().

/**

 * kobject_create() - Create a struct kobject dynamically.

 *

 * This function creates a kobject structure dynamically and sets it up

 * to be a "dynamic" kobject with a default release function set up.

 *

 * If the kobject was not able to be created, NULL will be returned.

 * The kobject structure returned from here must be cleaned up with a

 * call to kobject_put() and not kfree(), as kobject_init() has

 * already been called on this structure.

/**

 * kobject_create_and_add() - Create a struct kobject dynamically and

 *                            register it with sysfs.

 * @name: the name for the kobject

 * @parent: the parent kobject of this kobject, if any.

 *

 * This function creates a kobject structure dynamically and registers it

 * with sysfs.  When you are finished with this structure, call

 * kobject_put() and the structure will be dynamically freed when

 * it is no longer being used.

 *

 * If the kobject was not able to be created, NULL will be returned.

/**

 * kset_init() - Initialize a kset for use.

 * @k: kset

 default kobject attribute operations */

/**

 * kset_register() - Initialize and add a kset.

 * @k: kset.

/**

 * kset_unregister() - Remove a kset.

 * @k: kset.

/**

 * kset_find_obj() - Search for object in kset.

 * @kset: kset we're looking in.

 * @name: object's name.

 *

 * Lock kset via @kset->subsys, and iterate over @kset->list,

 * looking for a matching kobject. If matching object is found

 * take a reference and return the object.

/**

 * kset_create() - Create a struct kset dynamically.

 *

 * @name: the name for the kset

 * @uevent_ops: a struct kset_uevent_ops for the kset

 * @parent_kobj: the parent kobject of this kset, if any.

 *

 * This function creates a kset structure dynamically.  This structure can

 * then be registered with the system and show up in sysfs with a call to

 * kset_register().  When you are finished with this structure, if

 * kset_register() has been called, call kset_unregister() and the

 * structure will be dynamically freed when it is no longer being used.

 *

 * If the kset was not able to be created, NULL will be returned.

	/*

	 * The kobject of this kset will have a type of kset_ktype and belong to

	 * no kset itself.  That way we can properly free it when it is

	 * finished being used.

/**

 * kset_create_and_add() - Create a struct kset dynamically and add it to sysfs.

 *

 * @name: the name for the kset

 * @uevent_ops: a struct kset_uevent_ops for the kset

 * @parent_kobj: the parent kobject of this kset, if any.

 *

 * This function creates a kset structure dynamically and registers it

 * with sysfs.  When you are finished with this structure, call

 * kset_unregister() and the structure will be dynamically freed when it

 * is no longer being used.

 *

 * If the kset was not able to be created, NULL will be returned.

 SPDX-License-Identifier: GPL-2.0

/* inflate.c -- Not copyrighted 1992 by Mark Adler

/* 

 * Adapted for booting Linux by Hannu Savolainen 1993

 * based on gzip-1.0.3 

 *

 * Nicolas Pitre <nico@fluxnic.net>, 1999/04/14 :

 *   Little mods for all variable to reside either into rodata or bss segments

 *   by marking constant variables with 'const' and initializing all the others

 *   at run-time only.  This allows for the kernel uncompressor to run

 *   directly from Flash or ROM memory on embedded systems.

/*

   Inflate deflated (PKZIP's method 8 compressed) data.  The compression

   method searches for as much of the current string of bytes (up to a

   length of 258) in the previous 32 K bytes.  If it doesn't find any

   matches (of at least length 3), it codes the next byte.  Otherwise, it

   codes the length of the matched string and its distance backwards from

   the current position.  There is a single Huffman code that codes both

   single bytes (called "literals") and match lengths.  A second Huffman

   code codes the distance information, which follows a length code.  Each

   length or distance code actually represents a base value and a number

   of "extra" (sometimes zero) bits to get to add to the base value.  At

   the end of each deflated block is a special end-of-block (EOB) literal/

   length code.  The decoding process is basically: get a literal/length

   code; if EOB then done; if a literal, emit the decoded byte; if a

   length then get the distance and emit the referred-to bytes from the

   sliding window of previously emitted data.



   There are (currently) three kinds of inflate blocks: stored, fixed, and

   dynamic.  The compressor deals with some chunk of data at a time, and

   decides which method to use on a chunk-by-chunk basis.  A chunk might

   typically be 32 K or 64 K.  If the chunk is incompressible, then the

   "stored" method is used.  In this case, the bytes are simply stored as

   is, eight bits per byte, with none of the above coding.  The bytes are

   preceded by a count, since there is no longer an EOB code.



   If the data is compressible, then either the fixed or dynamic methods

   are used.  In the dynamic method, the compressed data is preceded by

   an encoding of the literal/length and distance Huffman codes that are

   to be used to decode this block.  The representation is itself Huffman

   coded, and so is preceded by a description of that code.  These code

   descriptions take up a little space, and so for small blocks, there is

   a predefined set of codes, called the fixed codes.  The fixed method is

   used if the block codes up smaller that way (usually for quite small

   chunks), otherwise the dynamic method is used.  In the latter case, the

   codes are customized to the probabilities in the current block, and so

   can code it much better than the pre-determined fixed codes.

 

   The Huffman codes themselves are decoded using a multi-level table

   lookup, in order to maximize the speed of decoding plus the speed of

   building the decoding tables.  See the comments below that precede the

   lbits and dbits tuning parameters.

/*

   Notes beyond the 1.93a appnote.txt:



   1. Distance pointers never point before the beginning of the output

      stream.

   2. Distance pointers can point back across blocks, up to 32k away.

   3. There is an implied maximum of 7 bits for the bit length table and

      15 bits for the actual data.

   4. If only one code exists, then it is encoded using one bit.  (Zero

      would be more efficient, but perhaps a little confusing.)  If two

      codes exist, they are coded using one bit each (0 and 1).

   5. There is no way of sending zero distance codes--a dummy must be

      sent if there are none.  (History: a pre 2.0 version of PKZIP would

      store blocks with no distance codes, but this was discovered to be

      too harsh a criterion.)  Valid only for 1.93a.  2.04c does allow

      zero distance codes, which is sent as one code of zero bits in

      length.

   6. There are up to 286 literal/length codes.  Code 256 represents the

      end-of-block.  Note however that the static length tree defines

      288 codes just to fill out the Huffman codes.  Codes 286 and 287

      cannot be used though, since there is no length base or extra bits

      defined for them.  Similarly, there are up to 30 distance codes.

      However, static trees define 32 codes (all 5 bits) to fill out the

      Huffman codes, but the last two had better not show up in the data.

   7. Unzip can check dynamic Huffman blocks for complete code sets.

      The exception is that a single code would not be complete (see #4).

   8. The five bits following the block type is really the number of

      literal codes sent minus 257.

   9. Length codes 8,16,16 are interpreted as 13 length codes of 8 bits

      (1+6+6).  Therefore, to output three times the length, you output

      three codes (1+1+1), whereas to output four times the same length,

      you only need two codes (1+3).  Hmm.

  10. In the tree reconstruction algorithm, Code = Code + Increment

      only if BitLength(i) is not zero.  (Pretty obvious.)

  11. Correction: 4 Bits: # of Bit Length codes - 4     (4 - 19)

  12. Note: length code 284 can represent 227-258, but length code 285

      really is 258.  The last length deserves its own, short code

      since it gets used a lot in very redundant files.  The length

      258 is special since 258 - 3 (the min match length) is 255.

  13. The literal/length and distance code bit lengths are read as a

      single stream of lengths.  It is possible (and advantageous) for

      a repeat code (16, 17, or 18) to go across the boundary between

      the two sets of lengths.

 !STATIC */

/* Huffman code lookup table entry--this entry is four bytes for machines

   that have 16-bit pointers (e.g. PC's in the small or medium model).

   Valid extra bits are 0..13.  e == 15 is EOB (end of block), e == 16

   means that v is a literal, 16 < e < 32 means that v is a pointer to

   the next table, which codes e - 16 bits, and lastly e == 99 indicates

   an unused code.  If a code with e == 99 is looked up, this implies an

 number of extra bits or operation */

 number of bits in this code or subcode */

 literal, length base, or distance base */

 pointer to next level of table */

 Function prototypes */

/* The inflate algorithm uses a sliding 32 K byte window on the uncompressed

   stream to find repeated byte strings.  This is implemented here as a

   circular buffer.  The index is updated simply by incrementing and then

/* It is left to other modules to supply the 32 K area.  It is assumed

   to be usable as if it were declared "uch slide[32768];" or as just

   "uch *slide;" and then malloc'ed in the latter case.  The definition

 unsigned wp;             current position in slide */

 Tables for deflate from PKZIP's appnote.txt. */

 Order of the bit length code lengths */

 Copy lengths for literal codes 257..285 */

 note: see note #13 above about the 258 in this list. */

 Extra bits for literal codes 257..285 */

 99==invalid */

 Copy offsets for distance codes 0..29 */

 Extra bits for distance codes */

/* Macros for inflate() bit peeking and grabbing.

   The usage is:

   

        NEEDBITS(j)

        x = b & mask_bits[j];

        DUMPBITS(j)



   where NEEDBITS makes sure that b has at least j bits in it, and

   DUMPBITS removes the bits from b.  The macros use the variable k

   for the number of bits in b.  Normally, b and k are register

   variables for speed, and are initialized at the beginning of a

   routine that uses these macros from a global bit buffer and count.



   If we assume that EOB will be the longest code, then we will never

   ask for bits with NEEDBITS that are beyond the end of the stream.

   So, NEEDBITS should not read any more bytes than are needed to

   meet the request.  Then no bytes need to be "returned" to the buffer

   at the end of the last block.



   However, this assumption is not true for fixed blocks--the EOB code

   is 7 bits, but the other literal/length codes can be 8 or 9 bits.

   (The EOB code is shorter than other codes because fixed blocks are

   generally short.  So, while a block always has an EOB, many other

   literal/length codes have a significantly lower probability of

   showing up at all.)  However, by making the first table have a

   lookup of seven bits, the EOB code will be found in that first

   lookup, and so will not require that too many bits be pulled from

   the stream.

 bit buffer */

 bits in bit buffer */

/* A trivial malloc implementation, adapted from

 *  malloc by Hannu Savolainen 1993 and Matthias Urlichs 1994

 Align */

/*

   Huffman code decoding is performed using a multi-level table lookup.

   The fastest way to decode is to simply build a lookup table whose

   size is determined by the longest code.  However, the time it takes

   to build this table can also be a factor if the data being decoded

   is not very long.  The most common codes are necessarily the

   shortest codes, so those codes dominate the decoding time, and hence

   the speed.  The idea is you can have a shorter table that decodes the

   shorter, more probable codes, and then point to subsidiary tables for

   the longer codes.  The time it costs to decode the longer codes is

   then traded against the time it takes to make longer tables.



   This results of this trade are in the variables lbits and dbits

   below.  lbits is the number of bits the first level table for literal/

   length codes can decode in one step, and dbits is the same thing for

   the distance codes.  Subsequent tables are also less than or equal to

   those sizes.  These values may be adjusted either when all of the

   codes are shorter than that, in which case the longest code length in

   bits is used, or when the shortest code is *longer* than the requested

   table size, in which case the length of the shortest code in bits is

   used.



   There are two different values for the two tables, since they code a

   different number of possibilities each.  The literal/length table

   codes 286 possible values, or in a flat code, a little over eight

   bits.  The distance table codes 30 possible values, or a little less

   than five bits, flat.  The optimum values for speed end up being

   about one bit more than those, so lbits is 8+1 and dbits is 5+1.

   The optimum values may differ though from machine to machine, and

   possibly even between compilers.  Your mileage may vary.

 bits in base literal/length lookup table */

 bits in base distance lookup table */

 If BMAX needs to be larger than 16, then h and x[] should be ulg. */

 maximum bit length of any code (16 for explode) */

 maximum number of codes in any set */

 track memory usage */

 code lengths in bits (all assumed <= BMAX) */

 number of codes (assumed <= N_MAX) */

 number of simple-valued codes (0..s-1) */

 list of base values for non-simple codes */

 list of extra bits for non-simple codes */

 result: starting table */

 maximum lookup bits, returns actual */

/* Given a list of code lengths and a maximum table size, make a set of

   tables to decode that set of codes.  Return zero on success, one if

   the given code set is incomplete (the tables are still built in this

   case), two if the input is invalid (all zero length codes or an

 counter for codes of length k */

 i repeats in table every f entries */

 maximum code length */

 table level */

 counter, current code */

 counter */

 number of bits in current code */

 bits per table (returned in m) */

 pointer into c[], b[], or v[] */

 points to current table */

 table entry for structure assignment */

 bits before this table == (l * h) */

 pointer into x */

 number of dummy codes added */

 number of entries in current table */

 bit length count table */

 table stack */

 values in order of bit length */

 bit offsets, then code stack */

 out of memory */

 Generate counts for each bit length */

 assume all entries <= BMAX */

 Can't combine with above line (Solaris bug) */

 null input--all zero length codes */

 Find minimum and maximum length, bound *m by those */

 minimum code length */

 maximum code length */

 Adjust last length count to fill out codes, if needed */

 bad input: more codes than bits */

 Generate starting offsets into the value table for each length */

 note that i == g from above */

 Make a table of values in order of bit lengths */

 set n to length of v */

 Generate the Huffman codes and for each, make the table entries */

 first Huffman code is zero */

 grab values in bit order */

 no tables yet--level -1 */

 bits decoded == (l * h) */

 just to keep compilers happy */

 ditto */

 ditto */

 go through the bit lengths (k already is bits in shortest code) */

 here i is the Huffman code of length k bits for value *p */

 make tables up to required level */

 previous table always l bits */

 compute minimum size table less than or equal to l bits */

 upper limit on table size */

 try a k-w bit table */

 too few codes for k-w bit table */

 deduct codes from patterns left */

 try smaller tables up to z bits */

 enough codes to use up j bits */

 else deduct codes from patterns */

 table entries for j-bit table */

 allocate and link in new table */

 not enough memory */

 track memory usage */

 link to list for huft_free() */

 table starts after link */

 connect to last table, if there is one */

 save pattern for backing up */

 bits to dump before this table */

 bits in this table */

 pointer to this table */

 (get around Turbo C bug) */

 connect to last table */

 set up table entry in r */

 out of values--invalid code */

 256 is end-of-block code */

 simple code is just the value */

 one compiler does not like *p++ */

 non-simple--look up in lists */

 fill code-like entries with r */

 backwards increment the k-bit code i */

 backup over finished tables */

 don't need to update q */

 Return true (1) if we were given an incomplete table */

 table to free */

/* Free the malloc'ed tables built by huft_build(), which makes a linked

   list of the tables it made, with the links in a dummy first entry of

 Go through linked list, freeing from the malloced (t[-1]) address. */

 literal/length decoder tables */

 distance decoder tables */

 number of bits decoded by tl[] */

 number of bits decoded by td[] */

/* inflate (decompress) the codes in a deflated (compressed) block.

 table entry flag/number of extra bits */

 length and index for copy */

 current window position */

 pointer to table entry */

 masks for bl and bd bits */

 bit buffer */

 number of bits in bit buffer */

 make local copies of globals */

 initialize bit buffer */

 initialize window position */

 inflate the coded data */

 precompute masks for speed */

 do until end of block */

 then it's a literal */

 it's an EOB or a length */

 exit if end of block */

 get length of block to copy */

 decode distance of block to copy */

 do the copy */

 (this test assumes unsigned comparison) */

 do it slow to avoid memcpy() overlap */

 !NOMEMCPY */

 restore the globals from the locals */

 restore global window pointer */

 restore global bit buffer */

 done */

 Input underrun */

 "decompress" an inflated type 0 (stored) block. */

 number of bytes in block */

 current window position */

 bit buffer */

 number of bits in bit buffer */

 make local copies of globals */

 initialize bit buffer */

 initialize window position */

 go to byte boundary */

 get the length and its complement */

 error in compressed data */

 read and output the compressed data */

 restore the globals from the locals */

 restore global window pointer */

 restore global bit buffer */

 Input underrun */

/*

 * We use `noinline' here to prevent gcc-3.5 from using too much stack space

/* decompress an inflated type 1 (fixed Huffman codes) block.  We should

   either replace this with a custom decoder, or at least precompute the

 temporary variable */

 literal/length code table */

 distance code table */

 lookup bits for tl */

 lookup bits for td */

 length list for huft_build */

 out of memory */

 set up literal table */

 make a complete, but wrong code set */

 set up distance table */

 make an incomplete code set */

 decompress until an end-of-block code */

 free the decoding tables, return */

/*

 * We use `noinline' here to prevent gcc-3.5 from using too much stack space

 decompress an inflated type 2 (dynamic Huffman codes) block. */

 temporary variables */

 last length */

 mask for bit lengths table */

 number of lengths to get */

 literal/length code table */

 distance code table */

 lookup bits for tl */

 lookup bits for td */

 number of bit length codes */

 number of literal/length codes */

 number of distance codes */

 literal/length and distance code lengths */

 bit buffer */

 number of bits in bit buffer */

 literal/length and distance code lengths */

 literal/length and distance code lengths */

 make local bit buffer */

 read in table lengths */

 number of literal/length codes */

 number of distance codes */

 number of bit length codes */

 bad lengths */

 read in bit-length-code lengths */

 build decoding table for trees--single level, 7 bit lookup */

 incomplete code set */

 read in literal and distance code lengths */

 length of code in bits (0..15) */

 save last length in l */

 repeat last length 3 to 6 times */

 3 to 10 zero length codes */

 j == 18: 11 to 138 zero length codes */

 free decoding table for trees */

 restore the global bit buffer */

 build the decoding tables for literal/length and distance codes */

 incomplete code set */

 incomplete code set */

 decompress until an end-of-block code */

 free the decoding tables, return */

 Input underrun */

 last block flag */

 decompress an inflated block */

 block type */

 bit buffer */

 number of bits in bit buffer */

 make local bit buffer */

 read in last block bit */

 read in block type */

 restore the global bit buffer */

 inflate that block type */

 bad block type */

 Input underrun */

 decompress an inflated entry */

 last block flag */

 result code */

 maximum struct huft's malloc'ed */

 initialize window, bit buffer */

 decompress until the last block */

  /* Undo too much lookahead. The next read will be byte aligned so we

   * can discard unused bits in the last meaningful byte.

 flush out slide */

 return success */

 DEBUG */

/**********************************************************************

 *

 * The following are support routines for inflate.c

 *

 initialized in makecrc() so it'll reside in bss */

/*

 * Code to compute the CRC-32 table. Borrowed from 

 * gzip-1.0.3/makecrc.c.

 Not copyrighted 1990 Mark Adler	*/

 crc shift register */

 polynomial exclusive-or pattern */

 counter for all possible eight bit values */

 byte being shifted into crc apparatus */

 terms of polynomial defining this crc (except x^32): */

 Make exclusive-or pattern from polynomial */

 this is initialized here so this code could reside in ROM */

 shift register contents */

 gzip flag byte */

 bit 0 set: file probably ASCII text */

 bit 1 set: continuation of multi-part gzip file */

 bit 2 set: extra field present */

 bit 3 set: original file name present */

 bit 4 set: file comment present */

 bit 5 set: file is encrypted */

 bit 6,7:   reserved */

/*

 * Do the uncompression!

 magic header */

 original crc */

 original uncompressed length */

 We only support method #8, DEFLATED */

 Get timestamp */

 Ignore extra flags for the moment */

 Ignore OS type for the moment */

 Get original file name if it was truncated */

 Discard the old name */

 null */ ;

 Discard file comment if any */

 null */ ;

 Decompress */

 Get the crc and original length */

    /* crc32  (see algorithm.doc)

     * uncompressed input size modulo 2^32

 Validate decompression */

 NEXTBYTE() goto's here if needed */

/*

 * lib/parman.c - Manager for linear priority array areas

 * Copyright (c) 2017 Mellanox Technologies. All rights reserved.

 * Copyright (c) 2017 Jiri Pirko <jiri@mellanox.com>

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions are met:

 *

 * 1. Redistributions of source code must retain the above copyright

 *    notice, this list of conditions and the following disclaimer.

 * 2. Redistributions in binary form must reproduce the above copyright

 *    notice, this list of conditions and the following disclaimer in the

 *    documentation and/or other materials provided with the distribution.

 * 3. Neither the names of the copyright holders nor the names of its

 *    contributors may be used to endorse or promote products derived from

 *    this software without specific prior written permission.

 *

 * Alternatively, this software may be distributed under the terms of the

 * GNU General Public License ("GPL") version 2 as published by the Free

 * Software Foundation.

 *

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"

 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE

 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE

 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE

 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR

 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF

 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS

 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN

 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)

 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE

 * POSSIBILITY OF SUCH DAMAGE.

/**

 * parman_create - creates a new parman instance

 * @ops:	caller-specific callbacks

 * @priv:	pointer to a private data passed to the ops

 *

 * Note: all locking must be provided by the caller.

 *

 * Each parman instance manages an array area with chunks of entries

 * with the same priority. Consider following example:

 *

 * item 1 with prio 10

 * item 2 with prio 10

 * item 3 with prio 10

 * item 4 with prio 20

 * item 5 with prio 20

 * item 6 with prio 30

 * item 7 with prio 30

 * item 8 with prio 30

 *

 * In this example, there are 3 priority chunks. The order of the priorities

 * matters, however the order of items within a single priority chunk does not

 * matter. So the same array could be ordered as follows:

 *

 * item 2 with prio 10

 * item 3 with prio 10

 * item 1 with prio 10

 * item 5 with prio 20

 * item 4 with prio 20

 * item 7 with prio 30

 * item 8 with prio 30

 * item 6 with prio 30

 *

 * The goal of parman is to maintain the priority ordering. The caller

 * provides @ops with callbacks parman uses to move the items

 * and resize the array area.

 *

 * Returns a pointer to newly created parman instance in case of success,

 * otherwise it returns NULL.

/**

 * parman_destroy - destroys existing parman instance

 * @parman:	parman instance

 *

 * Note: all locking must be provided by the caller.

/**

 * parman_prio_init - initializes a parman priority chunk

 * @parman:	parman instance

 * @prio:	parman prio structure to be initialized

 * @priority:	desired priority of the chunk

 *

 * Note: all locking must be provided by the caller.

 *

 * Before caller could add an item with certain priority, he has to

 * initialize a priority chunk for it using this function.

 Position inside the list according to priority */

/**

 * parman_prio_fini - finalizes use of parman priority chunk

 * @prio:	parman prio structure

 *

 * Note: all locking must be provided by the caller.

/**

 * parman_item_add - adds a parman item under defined priority

 * @parman:	parman instance

 * @prio:	parman prio instance to add the item to

 * @item:	parman item instance

 *

 * Note: all locking must be provided by the caller.

 *

 * Adds item to a array managed by parman instance under the specified priority.

 *

 * Returns 0 in case of success, negative number to indicate an error.

/**

 * parman_item_remove - deletes parman item

 * @parman:	parman instance

 * @prio:	parman prio instance to delete the item from

 * @item:	parman item instance

 *

 * Note: all locking must be provided by the caller.

 SPDX-License-Identifier: GPL-2.0+

/*

 * test_xarray.c: Test the XArray API

 * Copyright (c) 2017-2018 Microsoft Corporation

 * Copyright (c) 2019-2020 Oracle

 * Author: Matthew Wilcox <willy@infradead.org>

/*

 * If anyone needs this, please move it to xarray.c.  We have no current

 * users outside the test suite because all current multislot users want

 * to use the advanced API.

 The kernel does not fail GFP_NOWAIT allocations */

 kills the test-suite :-(

	XA_BUG_ON(xa, xa_err(xa_store(xa, 0, xa_mk_internal(0), 0)) != -EINVAL);

 Make sure we can iterate through retry entries */

 NULL elements have no marks set */

 Storing a pointer will not make a mark appear */

 Setting one mark will not set another mark */

 Storing NULL clears marks, and they can't be set again */

	/*

	 * Storing a multi-index entry over entries with marks gives the

	 * entire entry the union of the marks

 We should see two elements in the array */

 One of which is marked */

	/*

	 * Check that erasing the entry at 1 shrinks the tree and properly

	 * marks the node as being deleted.

 An array with a reserved entry is not empty */

 Releasing a used entry does nothing */

 cmpxchg sees a reserved entry as ZERO */

 xa_insert treats it as busy */

 Can iterate through a reserved entry */

 If we free a reserved entry, we should be able to allocate it */

 Loading from any position returns the same value */

 Storing adjacent to the value does not alter the value */

 Overwriting multiple indexes works */

 We can erase multiple values with a single store */

 Even when the first slot is empty but the others aren't */

 An empty array should assign %base to the first alloc */

 Erasing it should make the array empty again */

 And it should assign %base again */

 Allocating and then erasing a lot should not lose base */

 Destroying the array should do the same as erasing */

 And it should assign %base again */

 The next assigned ID should be base+1 */

 Storing a value should mark it used */

 If we then erase base, it should be free */

 Check that we fail properly at the limit of allocation */

 Allocate and free a NULL and check xa_empty() behaves */

 Ditto, but check destroy instead of erase */

 Check wrap-around is handled correctly */

	/*

	 * Check xa_find with all pairs between 0 and 99 inclusive,

	 * starting at every index between 0 and 99

 See find_swap_entry() in mm/shmem.c */

/*

 * We should always be able to store without allocating memory after

 * reserving a slot.

/*

 * Check that the pointer / value / sibling entries are accounted the

 * way we expect them to be.

 Destroying an empty array is a no-op */

 Destroying an array with a single entry */

 Destroying an array with a single entry at ULONG_MAX */

 Destroying an array with a multi-index entry */

 SPDX-License-Identifier: GPL-2.0

/*

 * seq_buf.c

 *

 * Copyright (C) 2014 Red Hat Inc, Steven Rostedt <srostedt@redhat.com>

 *

 * The seq_buf is a handy tool that allows you to pass a descriptor around

 * to a buffer that other functions can write to. It is similar to the

 * seq_file functionality but has some differences.

 *

 * To use it, the seq_buf must be initialized with seq_buf_init().

 * This will set up the counters within the descriptor. You can call

 * seq_buf_init() more than once to reset the seq_buf to start

 * from scratch.

/**

 * seq_buf_can_fit - can the new data fit in the current buffer?

 * @s: the seq_buf descriptor

 * @len: The length to see if it can fit in the current buffer

 *

 * Returns true if there's enough unused space in the seq_buf buffer

 * to fit the amount of new data according to @len.

/**

 * seq_buf_print_seq - move the contents of seq_buf into a seq_file

 * @m: the seq_file descriptor that is the destination

 * @s: the seq_buf descriptor that is the source.

 *

 * Returns zero on success, non zero otherwise

/**

 * seq_buf_vprintf - sequence printing of information.

 * @s: seq_buf descriptor

 * @fmt: printf format string

 * @args: va_list of arguments from a printf() type function

 *

 * Writes a vnprintf() format into the sequencce buffer.

 *

 * Returns zero on success, -1 on overflow.

/**

 * seq_buf_printf - sequence printing of information

 * @s: seq_buf descriptor

 * @fmt: printf format string

 *

 * Writes a printf() format into the sequence buffer.

 *

 * Returns zero on success, -1 on overflow.

/**

 * seq_buf_bprintf - Write the printf string from binary arguments

 * @s: seq_buf descriptor

 * @fmt: The format string for the @binary arguments

 * @binary: The binary arguments for @fmt.

 *

 * When recording in a fast path, a printf may be recorded with just

 * saving the format and the arguments as they were passed to the

 * function, instead of wasting cycles converting the arguments into

 * ASCII characters. Instead, the arguments are saved in a 32 bit

 * word array that is defined by the format string constraints.

 *

 * This function will take the format and the binary array and finish

 * the conversion into the ASCII string within the buffer.

 *

 * Returns zero on success, -1 on overflow.

 CONFIG_BINARY_PRINTF */

/**

 * seq_buf_puts - sequence printing of simple string

 * @s: seq_buf descriptor

 * @str: simple string to record

 *

 * Copy a simple string into the sequence buffer.

 *

 * Returns zero on success, -1 on overflow

 Add 1 to len for the trailing null byte which must be there */

 Don't count the trailing null byte against the capacity */

/**

 * seq_buf_putc - sequence printing of simple character

 * @s: seq_buf descriptor

 * @c: simple character to record

 *

 * Copy a single character into the sequence buffer.

 *

 * Returns zero on success, -1 on overflow

/**

 * seq_buf_putmem - write raw data into the sequenc buffer

 * @s: seq_buf descriptor

 * @mem: The raw memory to copy into the buffer

 * @len: The length of the raw memory to copy (in bytes)

 *

 * There may be cases where raw memory needs to be written into the

 * buffer and a strcpy() would not work. Using this function allows

 * for such cases.

 *

 * Returns zero on success, -1 on overflow

/**

 * seq_buf_putmem_hex - write raw memory into the buffer in ASCII hex

 * @s: seq_buf descriptor

 * @mem: The raw memory to write its hex ASCII representation of

 * @len: The length of the raw memory to copy (in bytes)

 *

 * This is similar to seq_buf_putmem() except instead of just copying the

 * raw memory into the buffer it writes its ASCII representation of it

 * in hex characters.

 *

 * Returns zero on success, -1 on overflow

 j increments twice per loop */

/**

 * seq_buf_path - copy a path into the sequence buffer

 * @s: seq_buf descriptor

 * @path: path to write into the sequence buffer.

 * @esc: set of characters to escape in the output

 *

 * Write a path name into the sequence buffer.

 *

 * Returns the number of written bytes on success, -1 on overflow

/**

 * seq_buf_to_user - copy the sequence buffer to user space

 * @s: seq_buf descriptor

 * @ubuf: The userspace memory location to copy to

 * @cnt: The amount to copy

 *

 * Copies the sequence buffer into the userspace memory pointed to

 * by @ubuf. It starts from the last read position (@s->readpos)

 * and writes up to @cnt characters or till it reaches the end of

 * the content in the buffer (@s->len), which ever comes first.

 *

 * On success, it returns a positive number of the number of bytes

 * it copied.

 *

 * On failure it returns -EBUSY if all of the content in the

 * sequence has been already read, which includes nothing in the

 * sequence (@s->len == @s->readpos).

 *

 * Returns -EFAULT if the copy to userspace fails.

/**

 * seq_buf_hex_dump - print formatted hex dump into the sequence buffer

 * @s: seq_buf descriptor

 * @prefix_str: string to prefix each line with;

 *  caller supplies trailing spaces for alignment if desired

 * @prefix_type: controls whether prefix of an offset, address, or none

 *  is printed (%DUMP_PREFIX_OFFSET, %DUMP_PREFIX_ADDRESS, %DUMP_PREFIX_NONE)

 * @rowsize: number of bytes to print per line; must be 16 or 32

 * @groupsize: number of bytes to print at a time (1, 2, 4, 8; default = 1)

 * @buf: data blob to dump

 * @len: number of bytes in the @buf

 * @ascii: include ASCII after the hex output

 *

 * Function is an analogue of print_hex_dump() and thus has similar interface.

 *

 * linebuf size is maximal length for one line.

 * 32 * 3 - maximum bytes per line, each printed into 2 chars + 1 for

 *	separating space

 * 2 - spaces separating hex dump and ascii representation

 * 32 - ascii representation

 * 1 - terminating '\0'

 *

 * Returns zero on success, -1 on overflow

 SPDX-License-Identifier: GPL-2.0

 Return the number of unicode characters in data */

/*

 * Return the number of bytes is the length of this string

 * Note: this is NOT the same as the number of unicode characters

 implies *b == 0 */

/*

 * copy at most maxlength bytes of whole utf8 characters to dest from the

 * ucs2 string src.

 *

 * The return value is the number of characters copied, not including the

 * final NUL character.

 SPDX-License-Identifier: GPL-2.0+

/*

 * XArray implementation

 * Copyright (c) 2017-2018 Microsoft Corporation

 * Copyright (c) 2018-2020 Oracle

 * Author: Matthew Wilcox <willy@infradead.org>

/*

 * Coding conventions in this file:

 *

 * @xa is used to refer to the entire xarray.

 * @xas is the 'xarray operation state'.  It may be either a pointer to

 * an xa_state, or an xa_state stored on the stack.  This is an unfortunate

 * ambiguity.

 * @index is the index of the entry being operated on

 * @mark is an xa_mark_t; a small number indicating one of the mark bits.

 * @node refers to an xa_node; usually the primary one being operated on by

 * this function.

 * @offset is the index into the slots array inside an xa_node.

 * @parent refers to the @xa_node closer to the head than @node.

 * @entry refers to something stored in a slot in the xarray

 returns true if the bit was set */

 returns true if the bit was set */

/*

 * xas_squash_marks() - Merge all marks to the first entry

 * @xas: Array operation state.

 *

 * Set a mark on the first entry if any entry has it set.  Clear marks on

 * all sibling entries.

 extracts the offset within this node from the index */

 move the index either forwards (find) or backwards (sibling slot) */

/*

 * Starts a walk.  If the @xas is already valid, we assume that it's on

 * the right path and just return where we've got to.  If we're in an

 * error state, return NULL.  If the index is outside the current scope

 * of the xarray, return NULL without changing @xas->xa_node.  Otherwise

 * set @xas->xa_node to NULL and return the current head of the array.

/**

 * xas_load() - Load an entry from the XArray (advanced).

 * @xas: XArray operation state.

 *

 * Usually walks the @xas to the appropriate state to load the entry

 * stored at xa_index.  However, it will do nothing and return %NULL if

 * @xas is in an error state.  xas_load() will never expand the tree.

 *

 * If the xa_state is set up to operate on a multi-index entry, xas_load()

 * may return %NULL or an internal entry, even if there are entries

 * present within the range specified by @xas.

 *

 * Context: Any context.  The caller should hold the xa_lock or the RCU lock.

 * Return: Usually an entry in the XArray, but see description for exceptions.

 Move the radix tree node cache here */

/*

 * xas_destroy() - Free any resources allocated during the XArray operation.

 * @xas: XArray operation state.

 *

 * This function is now internal-only.

/**

 * xas_nomem() - Allocate memory if needed.

 * @xas: XArray operation state.

 * @gfp: Memory allocation flags.

 *

 * If we need to add new nodes to the XArray, we try to allocate memory

 * with GFP_NOWAIT while holding the lock, which will usually succeed.

 * If it fails, @xas is flagged as needing memory to continue.  The caller

 * should drop the lock and call xas_nomem().  If xas_nomem() succeeds,

 * the caller should retry the operation.

 *

 * Forward progress is guaranteed as one node is allocated here and

 * stored in the xa_state where it will be found by xas_alloc().  More

 * nodes will likely be found in the slab allocator, but we do not tie

 * them up here.

 *

 * Return: true if memory was needed, and was successfully allocated.

/*

 * __xas_nomem() - Drop locks and allocate memory if needed.

 * @xas: XArray operation state.

 * @gfp: Memory allocation flags.

 *

 * Internal variant of xas_nomem().

 *

 * Return: true if memory was needed, and was successfully allocated.

 Returns the number of indices covered by a given xa_state */

/*

 * Use this to calculate the maximum index that will need to be created

 * in order to add the entry described by @xas.  Because we cannot store a

 * multi-index entry at index 0, the calculation is a little more complex

 * than you might expect.

 The maximum index that can be contained in the array without expanding it */

/*

 * xas_delete_node() - Attempt to delete an xa_node

 * @xas: Array operation state.

 *

 * Attempts to delete the @xas->xa_node.  This will fail if xa->node has

 * a non-zero reference count.

/**

 * xas_free_nodes() - Free this node and all nodes that it references

 * @xas: Array operation state.

 * @top: Node to free

 *

 * This node has been removed from the tree.  We must now free it and all

 * of its subnodes.  There may be RCU walkers with references into the tree,

 * so we must replace all entries with retry markers.

/*

 * xas_expand adds nodes to the head of the tree until it has reached

 * sufficient height to be able to contain @xas->xa_index

 Propagate the aggregated mark info to the new child */

		/*

		 * Now that the new node is fully initialised, we can add

		 * it to the tree

/*

 * xas_create() - Create a slot to store an entry in.

 * @xas: XArray operation state.

 * @allow_root: %true if we can store the entry in the root directly

 *

 * Most users will not need to call this function directly, as it is called

 * by xas_store().  It is useful for doing conditional store operations

 * (see the xa_cmpxchg() implementation for an example).

 *

 * Return: If the slot already existed, returns the contents of this slot.

 * If the slot was newly created, returns %NULL.  If it failed to create the

 * slot, returns %NULL and indicates the error in @xas.

/**

 * xas_create_range() - Ensure that stores to this range will succeed

 * @xas: XArray operation state.

 *

 * Creates all of the slots in the range covered by @xas.  Sets @xas to

 * create single-index entries and positions it at the beginning of the

 * range.  This is for the benefit of users which have not yet been

 * converted to use multi-index entries.

/**

 * xas_store() - Store this entry in the XArray.

 * @xas: XArray operation state.

 * @entry: New entry.

 *

 * If @xas is operating on a multi-index entry, the entry returned by this

 * function is essentially meaningless (it may be an internal entry or it

 * may be %NULL, even if there are non-NULL entries at some of the indices

 * covered by the range).  This is not a problem for any current users,

 * and can be changed if needed.

 *

 * Return: The old entry at this index.

		/*

		 * Must clear the marks before setting the entry to NULL,

		 * otherwise xas_for_each_marked may find a NULL entry and

		 * stop early.  rcu_assign_pointer contains a release barrier

		 * so the mark clearing will appear to happen before the

		 * entry is set to NULL.

/**

 * xas_get_mark() - Returns the state of this mark.

 * @xas: XArray operation state.

 * @mark: Mark number.

 *

 * Return: true if the mark is set, false if the mark is clear or @xas

 * is in an error state.

/**

 * xas_set_mark() - Sets the mark on this entry and its parents.

 * @xas: XArray operation state.

 * @mark: Mark number.

 *

 * Sets the specified mark on this entry, and walks up the tree setting it

 * on all the ancestor entries.  Does nothing if @xas has not been walked to

 * an entry, or is in an error state.

/**

 * xas_clear_mark() - Clears the mark on this entry and its parents.

 * @xas: XArray operation state.

 * @mark: Mark number.

 *

 * Clears the specified mark on this entry, and walks back to the head

 * attempting to clear it on all the ancestor entries.  Does nothing if

 * @xas has not been walked to an entry, or is in an error state.

/**

 * xas_init_marks() - Initialise all marks for the entry

 * @xas: Array operations state.

 *

 * Initialise all marks for the entry specified by @xas.  If we're tracking

 * free entries with a mark, we need to set it on all entries.  All other

 * marks are cleared.

 *

 * This implementation is not as efficient as it could be; we may walk

 * up the tree multiple times.

/**

 * xas_split_alloc() - Allocate memory for splitting an entry.

 * @xas: XArray operation state.

 * @entry: New entry which will be stored in the array.

 * @order: Current entry order.

 * @gfp: Memory allocation flags.

 *

 * This function should be called before calling xas_split().

 * If necessary, it will allocate new nodes (and fill them with @entry)

 * to prepare for the upcoming split of an entry of @order size into

 * entries of the order stored in the @xas.

 *

 * Context: May sleep if @gfp flags permit.

 XXX: no support for splitting really large entries yet */

/**

 * xas_split() - Split a multi-index entry into smaller entries.

 * @xas: XArray operation state.

 * @entry: New entry to store in the array.

 * @order: Current entry order.

 *

 * The size of the new entries is set in @xas.  The value in @entry is

 * copied to all the replacement entries.

 *

 * Context: Any context.  The caller should hold the xa_lock.

/**

 * xas_pause() - Pause a walk to drop a lock.

 * @xas: XArray operation state.

 *

 * Some users need to pause a walk and drop the lock they're holding in

 * order to yield to a higher priority thread or carry out an operation

 * on an entry.  Those users should call this function before they drop

 * the lock.  It resets the @xas to be suitable for the next iteration

 * of the loop after the user has reacquired the lock.  If most entries

 * found during a walk require you to call xas_pause(), the xa_for_each()

 * iterator may be more appropriate.

 *

 * Note that xas_pause() only works for forward iteration.  If a user needs

 * to pause a reverse iteration, we will need a xas_pause_rev().

/*

 * __xas_prev() - Find the previous entry in the XArray.

 * @xas: XArray operation state.

 *

 * Helper function for xas_prev() which handles all the complex cases

 * out of line.

/*

 * __xas_next() - Find the next entry in the XArray.

 * @xas: XArray operation state.

 *

 * Helper function for xas_next() which handles all the complex cases

 * out of line.

/**

 * xas_find() - Find the next present entry in the XArray.

 * @xas: XArray operation state.

 * @max: Highest index to return.

 *

 * If the @xas has not yet been walked to an entry, return the entry

 * which has an index >= xas.xa_index.  If it has been walked, the entry

 * currently being pointed at has been processed, and so we move to the

 * next entry.

 *

 * If no entry is found and the array is smaller than @max, the iterator

 * is set to the smallest index not yet in the array.  This allows @xas

 * to be immediately passed to xas_store().

 *

 * Return: The entry, if found, otherwise %NULL.

/**

 * xas_find_marked() - Find the next marked entry in the XArray.

 * @xas: XArray operation state.

 * @max: Highest index to return.

 * @mark: Mark number to search for.

 *

 * If the @xas has not yet been walked to an entry, return the marked entry

 * which has an index >= xas.xa_index.  If it has been walked, the entry

 * currently being pointed at has been processed, and so we return the

 * first marked entry with an index > xas.xa_index.

 *

 * If no marked entry is found and the array is smaller than @max, @xas is

 * set to the bounds state and xas->xa_index is set to the smallest index

 * not yet in the array.  This allows @xas to be immediately passed to

 * xas_store().

 *

 * If no entry is found before @max is reached, @xas is set to the restart

 * state.

 *

 * Return: The entry, if found, otherwise %NULL.

 Mind the wrap */

/**

 * xas_find_conflict() - Find the next present entry in a range.

 * @xas: XArray operation state.

 *

 * The @xas describes both a range and a position within that range.

 *

 * Context: Any context.  Expects xa_lock to be held.

 * Return: The next entry in the range covered by @xas or %NULL.

/**

 * xa_load() - Load an entry from an XArray.

 * @xa: XArray.

 * @index: index into array.

 *

 * Context: Any context.  Takes and releases the RCU lock.

 * Return: The entry at @index in @xa.

/**

 * __xa_erase() - Erase this entry from the XArray while locked.

 * @xa: XArray.

 * @index: Index into array.

 *

 * After this function returns, loading from @index will return %NULL.

 * If the index is part of a multi-index entry, all indices will be erased

 * and none of the entries will be part of a multi-index entry.

 *

 * Context: Any context.  Expects xa_lock to be held on entry.

 * Return: The entry which used to be at this index.

/**

 * xa_erase() - Erase this entry from the XArray.

 * @xa: XArray.

 * @index: Index of entry.

 *

 * After this function returns, loading from @index will return %NULL.

 * If the index is part of a multi-index entry, all indices will be erased

 * and none of the entries will be part of a multi-index entry.

 *

 * Context: Any context.  Takes and releases the xa_lock.

 * Return: The entry which used to be at this index.

/**

 * __xa_store() - Store this entry in the XArray.

 * @xa: XArray.

 * @index: Index into array.

 * @entry: New entry.

 * @gfp: Memory allocation flags.

 *

 * You must already be holding the xa_lock when calling this function.

 * It will drop the lock if needed to allocate memory, and then reacquire

 * it afterwards.

 *

 * Context: Any context.  Expects xa_lock to be held on entry.  May

 * release and reacquire xa_lock if @gfp flags permit.

 * Return: The old entry at this index or xa_err() if an error happened.

/**

 * xa_store() - Store this entry in the XArray.

 * @xa: XArray.

 * @index: Index into array.

 * @entry: New entry.

 * @gfp: Memory allocation flags.

 *

 * After this function returns, loads from this index will return @entry.

 * Storing into an existing multi-index entry updates the entry of every index.

 * The marks associated with @index are unaffected unless @entry is %NULL.

 *

 * Context: Any context.  Takes and releases the xa_lock.

 * May sleep if the @gfp flags permit.

 * Return: The old entry at this index on success, xa_err(-EINVAL) if @entry

 * cannot be stored in an XArray, or xa_err(-ENOMEM) if memory allocation

 * failed.

/**

 * __xa_cmpxchg() - Store this entry in the XArray.

 * @xa: XArray.

 * @index: Index into array.

 * @old: Old value to test against.

 * @entry: New entry.

 * @gfp: Memory allocation flags.

 *

 * You must already be holding the xa_lock when calling this function.

 * It will drop the lock if needed to allocate memory, and then reacquire

 * it afterwards.

 *

 * Context: Any context.  Expects xa_lock to be held on entry.  May

 * release and reacquire xa_lock if @gfp flags permit.

 * Return: The old entry at this index or xa_err() if an error happened.

/**

 * __xa_insert() - Store this entry in the XArray if no entry is present.

 * @xa: XArray.

 * @index: Index into array.

 * @entry: New entry.

 * @gfp: Memory allocation flags.

 *

 * Inserting a NULL entry will store a reserved entry (like xa_reserve())

 * if no entry is present.  Inserting will fail if a reserved entry is

 * present, even though loading from this index will return NULL.

 *

 * Context: Any context.  Expects xa_lock to be held on entry.  May

 * release and reacquire xa_lock if @gfp flags permit.

 * Return: 0 if the store succeeded.  -EBUSY if another entry was present.

 * -ENOMEM if memory could not be allocated.

/**

 * xa_store_range() - Store this entry at a range of indices in the XArray.

 * @xa: XArray.

 * @first: First index to affect.

 * @last: Last index to affect.

 * @entry: New entry.

 * @gfp: Memory allocation flags.

 *

 * After this function returns, loads from any index between @first and @last,

 * inclusive will return @entry.

 * Storing into an existing multi-index entry updates the entry of every index.

 * The marks associated with @index are unaffected unless @entry is %NULL.

 *

 * Context: Process context.  Takes and releases the xa_lock.  May sleep

 * if the @gfp flags permit.

 * Return: %NULL on success, xa_err(-EINVAL) if @entry cannot be stored in

 * an XArray, or xa_err(-ENOMEM) if memory allocation failed.

/**

 * xa_get_order() - Get the order of an entry.

 * @xa: XArray.

 * @index: Index of the entry.

 *

 * Return: A number between 0 and 63 indicating the order of the entry.

 CONFIG_XARRAY_MULTI */

/**

 * __xa_alloc() - Find somewhere to store this entry in the XArray.

 * @xa: XArray.

 * @id: Pointer to ID.

 * @limit: Range for allocated ID.

 * @entry: New entry.

 * @gfp: Memory allocation flags.

 *

 * Finds an empty entry in @xa between @limit.min and @limit.max,

 * stores the index into the @id pointer, then stores the entry at

 * that index.  A concurrent lookup will not see an uninitialised @id.

 *

 * Context: Any context.  Expects xa_lock to be held on entry.  May

 * release and reacquire xa_lock if @gfp flags permit.

 * Return: 0 on success, -ENOMEM if memory could not be allocated or

 * -EBUSY if there are no free entries in @limit.

/**

 * __xa_alloc_cyclic() - Find somewhere to store this entry in the XArray.

 * @xa: XArray.

 * @id: Pointer to ID.

 * @entry: New entry.

 * @limit: Range of allocated ID.

 * @next: Pointer to next ID to allocate.

 * @gfp: Memory allocation flags.

 *

 * Finds an empty entry in @xa between @limit.min and @limit.max,

 * stores the index into the @id pointer, then stores the entry at

 * that index.  A concurrent lookup will not see an uninitialised @id.

 * The search for an empty entry will start at @next and will wrap

 * around if necessary.

 *

 * Context: Any context.  Expects xa_lock to be held on entry.  May

 * release and reacquire xa_lock if @gfp flags permit.

 * Return: 0 if the allocation succeeded without wrapping.  1 if the

 * allocation succeeded after wrapping, -ENOMEM if memory could not be

 * allocated or -EBUSY if there are no free entries in @limit.

/**

 * __xa_set_mark() - Set this mark on this entry while locked.

 * @xa: XArray.

 * @index: Index of entry.

 * @mark: Mark number.

 *

 * Attempting to set a mark on a %NULL entry does not succeed.

 *

 * Context: Any context.  Expects xa_lock to be held on entry.

/**

 * __xa_clear_mark() - Clear this mark on this entry while locked.

 * @xa: XArray.

 * @index: Index of entry.

 * @mark: Mark number.

 *

 * Context: Any context.  Expects xa_lock to be held on entry.

/**

 * xa_get_mark() - Inquire whether this mark is set on this entry.

 * @xa: XArray.

 * @index: Index of entry.

 * @mark: Mark number.

 *

 * This function uses the RCU read lock, so the result may be out of date

 * by the time it returns.  If you need the result to be stable, use a lock.

 *

 * Context: Any context.  Takes and releases the RCU lock.

 * Return: True if the entry at @index has this mark set, false if it doesn't.

/**

 * xa_set_mark() - Set this mark on this entry.

 * @xa: XArray.

 * @index: Index of entry.

 * @mark: Mark number.

 *

 * Attempting to set a mark on a %NULL entry does not succeed.

 *

 * Context: Process context.  Takes and releases the xa_lock.

/**

 * xa_clear_mark() - Clear this mark on this entry.

 * @xa: XArray.

 * @index: Index of entry.

 * @mark: Mark number.

 *

 * Clearing a mark always succeeds.

 *

 * Context: Process context.  Takes and releases the xa_lock.

/**

 * xa_find() - Search the XArray for an entry.

 * @xa: XArray.

 * @indexp: Pointer to an index.

 * @max: Maximum index to search to.

 * @filter: Selection criterion.

 *

 * Finds the entry in @xa which matches the @filter, and has the lowest

 * index that is at least @indexp and no more than @max.

 * If an entry is found, @indexp is updated to be the index of the entry.

 * This function is protected by the RCU read lock, so it may not find

 * entries which are being simultaneously added.  It will not return an

 * %XA_RETRY_ENTRY; if you need to see retry entries, use xas_find().

 *

 * Context: Any context.  Takes and releases the RCU lock.

 * Return: The entry, if found, otherwise %NULL.

/**

 * xa_find_after() - Search the XArray for a present entry.

 * @xa: XArray.

 * @indexp: Pointer to an index.

 * @max: Maximum index to search to.

 * @filter: Selection criterion.

 *

 * Finds the entry in @xa which matches the @filter and has the lowest

 * index that is above @indexp and no more than @max.

 * If an entry is found, @indexp is updated to be the index of the entry.

 * This function is protected by the RCU read lock, so it may miss entries

 * which are being simultaneously added.  It will not return an

 * %XA_RETRY_ENTRY; if you need to see retry entries, use xas_find().

 *

 * Context: Any context.  Takes and releases the RCU lock.

 * Return: The pointer, if found, otherwise %NULL.

/**

 * xa_extract() - Copy selected entries from the XArray into a normal array.

 * @xa: The source XArray to copy from.

 * @dst: The buffer to copy entries into.

 * @start: The first index in the XArray eligible to be selected.

 * @max: The last index in the XArray eligible to be selected.

 * @n: The maximum number of entries to copy.

 * @filter: Selection criterion.

 *

 * Copies up to @n entries that match @filter from the XArray.  The

 * copied entries will have indices between @start and @max, inclusive.

 *

 * The @filter may be an XArray mark value, in which case entries which are

 * marked with that mark will be copied.  It may also be %XA_PRESENT, in

 * which case all entries which are not %NULL will be copied.

 *

 * The entries returned may not represent a snapshot of the XArray at a

 * moment in time.  For example, if another thread stores to index 5, then

 * index 10, calling xa_extract() may return the old contents of index 5

 * and the new contents of index 10.  Indices not modified while this

 * function is running will not be skipped.

 *

 * If you need stronger guarantees, holding the xa_lock across calls to this

 * function will prevent concurrent modification.

 *

 * Context: Any context.  Takes and releases the RCU lock.

 * Return: The number of entries copied.

/**

 * xa_delete_node() - Private interface for workingset code.

 * @node: Node to be removed from the tree.

 * @update: Function to call to update ancestor nodes.

 *

 * Context: xa_lock must be held on entry and will not be released.

 For the benefit of the test suite */

/**

 * xa_destroy() - Free all internal data structures.

 * @xa: XArray.

 *

 * After calling this function, the XArray is empty and has freed all memory

 * allocated for its internal data structures.  You are responsible for

 * freeing the objects referenced by the XArray.

 *

 * Context: Any context.  Takes and releases the xa_lock, interrupt-safe.

 lockdep checks we're still holding the lock in xas_free_nodes() */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Testsuite for atomic64_t functions

 *

 * Copyright © 2010  Luca Barbieri

 for boot_cpu_has below */

/*

 * Test for a atomic operation family,

 * @test should be a macro accepting parameters (bit, op, ...)

 Confirm the return value fits in an int, even if the value doesn't */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ratelimit.c - Do something with rate limit.

 *

 * Isolated from kernel/printk.c by Dave Young <hidave.darkstar@gmail.com>

 *

 * 2008-05-01 rewrite the function and use a ratelimit_state data struct as

 * parameter. Now every user can use their own standalone ratelimit_state.

/*

 * __ratelimit - rate limiting

 * @rs: ratelimit_state data

 * @func: name of calling function

 *

 * This enforces a rate limit: not more than @rs->burst callbacks

 * in every @rs->interval

 *

 * RETURNS:

 * 0 means callbacks will be suppressed.

 * 1 means go ahead and do it.

	/*

	 * If we contend on this state's lock then almost

	 * by definition we are too busy to print a message,

	 * in addition to the one that will be printed by

	 * the entity that is holding the lock already:

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/lib/ctype.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 0-7 */

 8-15 */

 16-23 */

 24-31 */

 32-39 */

 40-47 */

 48-55 */

 56-63 */

 64-71 */

 72-79 */

 80-87 */

 88-95 */

 96-103 */

 104-111 */

 112-119 */

 120-127 */

 128-143 */

 144-159 */

 160-175 */

 176-191 */

 192-207 */

 208-223 */

 224-239 */

 240-255 */

 SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0

 Copyright (c) 2018 Mellanox Technologies. All rights reserved */

 member of objagg_hints->node_ht */

 member of objagg_hints->node_list */

 member of objagg->obj_ht */

 member of objagg->obj_list */

	struct objagg_obj *parent; /* if the object is nested, this

				    * holds pointer to parent, otherwise NULL

 user delta private */

 user root private */

	unsigned int refcount; /* counts number of users of this object

				* including nested objects

	/* Nesting is not supported, so we can use ->parent

	 * to figure out if the object is root.

/**

 * objagg_obj_root_priv - obtains root private for an object

 * @objagg_obj:	objagg object instance

 *

 * Note: all locking must be provided by the caller.

 *

 * Either the object is root itself when the private is returned

 * directly, or the parent is root and its private is returned

 * instead.

 *

 * Returns a user private root pointer.

/**

 * objagg_obj_delta_priv - obtains delta private for an object

 * @objagg_obj:	objagg object instance

 *

 * Note: all locking must be provided by the caller.

 *

 * Returns user private delta pointer or NULL in case the passed

 * object is root.

/**

 * objagg_obj_raw - obtains object user private pointer

 * @objagg_obj:	objagg object instance

 *

 * Note: all locking must be provided by the caller.

 *

 * Returns user private pointer as was passed to objagg_obj_get() by "obj" arg.

	/* User returned a delta private, that means that

	 * our object can be aggregated into the parent.

		/* Nesting is not supported. In case the object

		 * is not root, it cannot be assigned as parent.

 In case there are no hints available, the root id is invalid. */

		/* For objects with no hint, start after the last

		 * hinted root_id.

	/* First, try to use hints if they are available and

	 * if they provide result.

 Try to find if the object can be aggregated under an existing one. */

 If aggregation is not possible, make the object a root. */

	/* First, try to find the object exactly as user passed it,

	 * perhaps it is already in use.

/**

 * objagg_obj_get - gets an object within objagg instance

 * @objagg:	objagg instance

 * @obj:	user-specific private object pointer

 *

 * Note: all locking must be provided by the caller.

 *

 * Size of the "obj" memory is specified in "objagg->ops".

 *

 * There are 3 main options this function wraps:

 * 1) The object according to "obj" already exist. In that case

 *    the reference counter is incrementes and the object is returned.

 * 2) The object does not exist, but it can be aggregated within

 *    another object. In that case, user ops->delta_create() is called

 *    to obtain delta data and a new object is created with returned

 *    user-delta private pointer.

 * 3) The object does not exist and cannot be aggregated into

 *    any of the existing objects. In that case, user ops->root_create()

 *    is called to create the root and a new object is created with

 *    returned user-root private pointer.

 *

 * Returns a pointer to objagg object instance in case of success,

 * otherwise it returns pointer error using ERR_PTR macro.

/**

 * objagg_obj_put - puts an object within objagg instance

 * @objagg:	objagg instance

 * @objagg_obj:	objagg object instance

 *

 * Note: all locking must be provided by the caller.

 *

 * Symmetric to objagg_obj_get().

/**

 * objagg_create - creates a new objagg instance

 * @ops:		user-specific callbacks

 * @objagg_hints:	hints, can be NULL

 * @priv:		pointer to a private data passed to the ops

 *

 * Note: all locking must be provided by the caller.

 *

 * The purpose of the library is to provide an infrastructure to

 * aggregate user-specified objects. Library does not care about the type

 * of the object. User fills-up ops which take care of the specific

 * user object manipulation.

 *

 * As a very stupid example, consider integer numbers. For example

 * number 8 as a root object. That can aggregate number 9 with delta 1,

 * number 10 with delta 2, etc. This example is implemented as

 * a part of a testing module in test_objagg.c file.

 *

 * Each objagg instance contains multiple trees. Each tree node is

 * represented by "an object". In the current implementation there can be

 * only roots and leafs nodes. Leaf nodes are called deltas.

 * But in general, this can be easily extended for intermediate nodes.

 * In that extension, a delta would be associated with all non-root

 * nodes.

 *

 * Returns a pointer to newly created objagg instance in case of success,

 * otherwise it returns pointer error using ERR_PTR macro.

/**

 * objagg_destroy - destroys a new objagg instance

 * @objagg:	objagg instance

 *

 * Note: all locking must be provided by the caller.

/**

 * objagg_stats_get - obtains stats of the objagg instance

 * @objagg:	objagg instance

 *

 * Note: all locking must be provided by the caller.

 *

 * The returned structure contains statistics of all object

 * currently in use, ordered by following rules:

 * 1) Root objects are always on lower indexes than the rest.

 * 2) Objects with higher delta user count are always on lower

 *    indexes.

 * 3) In case more objects have the same delta user count,

 *    the objects are ordered by user count.

 *

 * Returns a pointer to stats instance in case of success,

 * otherwise it returns pointer error using ERR_PTR macro.

/**

 * objagg_stats_put - puts stats of the objagg instance

 * @objagg_stats:	objagg instance stats

 *

 * Note: all locking must be provided by the caller.

	/* Node weight is sum of node users and all other nodes users

	 * that this node can represent with delta.

	/* Assemble a temporary graph. Insert edge X->Y in case Y can be

	 * in delta of X.

	/* Find the nodes from the ones that can accommodate most users

	 * and cross them out of the graph. Save them to the hint list.

/**

 * objagg_hints_get - obtains hints instance

 * @objagg:		objagg instance

 * @opt_algo_type:	type of hints finding algorithm

 *

 * Note: all locking must be provided by the caller.

 *

 * According to the algo type, the existing objects of objagg instance

 * are going to be went-through to assemble an optimal tree. We call this

 * tree hints. These hints can be later on used for creation of

 * a new objagg instance. There, the future object creations are going

 * to be consulted with these hints in order to find out, where exactly

 * the new object should be put as a root or delta.

 *

 * Returns a pointer to hints instance in case of success,

 * otherwise it returns pointer error using ERR_PTR macro.

/**

 * objagg_hints_put - puts hints instance

 * @objagg_hints:	objagg hints instance

 *

 * Note: all locking must be provided by the caller.

/**

 * objagg_hints_stats_get - obtains stats of the hints instance

 * @objagg_hints:	hints instance

 *

 * Note: all locking must be provided by the caller.

 *

 * The returned structure contains statistics of all objects

 * currently in use, ordered by following rules:

 * 1) Root objects are always on lower indexes than the rest.

 * 2) Objects with higher delta user count are always on lower

 *    indexes.

 * 3) In case multiple objects have the same delta user count,

 *    the objects are ordered by user count.

 *

 * Returns a pointer to stats instance in case of success,

 * otherwise it returns pointer error using ERR_PTR macro.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * setup_fault_attr() is a helper function for various __setup handlers, so it

 * returns 0 on error, because that is what __setup handlers do.

 "<interval>,<probability>,<space>,<times>" */

 CONFIG_FAULT_INJECTION_STACKTRACE_FILTER */

/*

 * This code is stolen from failmalloc-1.0

 * http://www.nongnu.org/failmalloc/

 No need to check any other properties if the probability is 0 */

 CONFIG_FAULT_INJECTION_STACKTRACE_FILTER */

 CONFIG_FAULT_INJECTION_STACKTRACE_FILTER */

 CONFIG_FAULT_INJECTION_DEBUG_FS */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2017 HiSilicon Limited, All Rights Reserved.

 * Author: Gabriele Paoloni <gabriele.paoloni@huawei.com>

 * Author: Zhichang Yuan <yuanzhichang@hisilicon.com>

 * Author: John Garry <john.garry@huawei.com>

 The unique hardware address list */

 Consider a kernel general helper for this */

/**

 * logic_pio_register_range - register logical PIO range for a host

 * @new_range: pointer to the IO range to be registered.

 *

 * Returns 0 on success, the error code in case of failure.

 * If the range already exists, -EEXIST will be returned, which should be

 * considered a success.

 *

 * Register a new IO range node in the IO range list.

 range already there */

 for MMIO ranges we need to check for overlap */

 range not registered yet, check for available space */

 if it's too big check if 64K space can be reserved */

 invalid flag */

/**

 * logic_pio_unregister_range - unregister a logical PIO range for a host

 * @range: pointer to the IO range which has been already registered.

 *

 * Unregister a previously-registered IO range node.

/**

 * find_io_range_by_fwnode - find logical PIO range for given FW node

 * @fwnode: FW node handle associated with logical PIO range

 *

 * Returns pointer to node on success, NULL otherwise.

 *

 * Traverse the io_range_list to find the registered node for @fwnode.

 Return a registered range given an input PIO token */

/**

 * logic_pio_to_hwaddr - translate logical PIO to HW address

 * @pio: logical PIO value

 *

 * Returns HW address if valid, ~0 otherwise.

 *

 * Translate the input logical PIO to the corresponding hardware address.

 * The input PIO should be unique in the whole logical PIO space.

/**

 * logic_pio_trans_hwaddr - translate HW address to logical PIO

 * @fwnode: FW node reference for the host

 * @addr: Host-relative HW address

 * @size: size to translate

 *

 * Returns Logical PIO value if successful, ~0UL otherwise

 CONFIG_INDIRECT_PIO && PCI_IOBASE */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel module for testing static keys.

 *

 * Copyright 2015 Akamai Technologies Inc. All Rights Reserved

 *

 * Authors:

 *      Jason Baron       <jbaron@akamai.com>

 old keys */

 new api */

 external */

 new api */

 internal keys - old keys */

 internal keys - new keys */

 external keys - old keys */

 external keys - new keys */

/*

 * Wrapper for decompressing XZ-compressed kernel, initramfs, and initrd

 *

 * Author: Lasse Collin <lasse.collin@tukaani.org>

 *

 * This file has been put into the public domain.

 * You can do whatever you want with this file.

/*

 * Important notes about in-place decompression

 *

 * At least on x86, the kernel is decompressed in place: the compressed data

 * is placed to the end of the output buffer, and the decompressor overwrites

 * most of the compressed data. There must be enough safety margin to

 * guarantee that the write position is always behind the read position.

 *

 * The safety margin for XZ with LZMA2 or BCJ+LZMA2 is calculated below.

 * Note that the margin with XZ is bigger than with Deflate (gzip)!

 *

 * The worst case for in-place decompression is that the beginning of

 * the file is compressed extremely well, and the rest of the file is

 * incompressible. Thus, we must look for worst-case expansion when the

 * compressor is encoding incompressible data.

 *

 * The structure of the .xz file in case of a compressed kernel is as follows.

 * Sizes (as bytes) of the fields are in parenthesis.

 *

 *    Stream Header (12)

 *    Block Header:

 *      Block Header (8-12)

 *      Compressed Data (N)

 *      Block Padding (0-3)

 *      CRC32 (4)

 *    Index (8-20)

 *    Stream Footer (12)

 *

 * Normally there is exactly one Block, but let's assume that there are

 * 2-4 Blocks just in case. Because Stream Header and also Block Header

 * of the first Block don't make the decompressor produce any uncompressed

 * data, we can ignore them from our calculations. Block Headers of possible

 * additional Blocks have to be taken into account still. With these

 * assumptions, it is safe to assume that the total header overhead is

 * less than 128 bytes.

 *

 * Compressed Data contains LZMA2 or BCJ+LZMA2 encoded data. Since BCJ

 * doesn't change the size of the data, it is enough to calculate the

 * safety margin for LZMA2.

 *

 * LZMA2 stores the data in chunks. Each chunk has a header whose size is

 * a maximum of 6 bytes, but to get round 2^n numbers, let's assume that

 * the maximum chunk header size is 8 bytes. After the chunk header, there

 * may be up to 64 KiB of actual payload in the chunk. Often the payload is

 * quite a bit smaller though; to be safe, let's assume that an average

 * chunk has only 32 KiB of payload.

 *

 * The maximum uncompressed size of the payload is 2 MiB. The minimum

 * uncompressed size of the payload is in practice never less than the

 * payload size itself. The LZMA2 format would allow uncompressed size

 * to be less than the payload size, but no sane compressor creates such

 * files. LZMA2 supports storing incompressible data in uncompressed form,

 * so there's never a need to create payloads whose uncompressed size is

 * smaller than the compressed size.

 *

 * The assumption, that the uncompressed size of the payload is never

 * smaller than the payload itself, is valid only when talking about

 * the payload as a whole. It is possible that the payload has parts where

 * the decompressor consumes more input than it produces output. Calculating

 * the worst case for this would be tricky. Instead of trying to do that,

 * let's simply make sure that the decompressor never overwrites any bytes

 * of the payload which it is currently reading.

 *

 * Now we have enough information to calculate the safety margin. We need

 *   - 128 bytes for the .xz file format headers;

 *   - 8 bytes per every 32 KiB of uncompressed size (one LZMA2 chunk header

 *     per chunk, each chunk having average payload size of 32 KiB); and

 *   - 64 KiB (biggest possible LZMA2 chunk payload size) to make sure that

 *     the decompressor never overwrites anything from the LZMA2 chunk

 *     payload it is currently reading.

 *

 * We get the following formula:

 *

 *    safety_margin = 128 + uncompressed_size * 8 / 32768 + 65536

 *                  = 128 + (uncompressed_size >> 12) + 65536

 *

 * For comparison, according to arch/x86/boot/compressed/misc.c, the

 * equivalent formula for Deflate is this:

 *

 *    safety_margin = 18 + (uncompressed_size >> 12) + 32768

 *

 * Thus, when updating Deflate-only in-place kernel decompressor to

 * support XZ, the fixed overhead has to be increased from 18+32768 bytes

 * to 128+65536 bytes.

/*

 * STATIC is defined to "static" if we are being built for kernel

 * decompression (pre-boot code). <linux/decompress/mm.h> will define

 * STATIC to empty if it wasn't already defined. Since we will need to

 * know later if we are being used for kernel decompression, we define

 * XZ_PREBOOT here.

/*

 * Use the internal CRC32 code instead of kernel's CRC32 module, which

 * is not available in early phase of booting.

/*

 * For boot time use, we enable only the BCJ filter of the current

 * architecture or none if no BCJ filter is available for the architecture.

/*

 * This will get the basic headers so that memeq() and others

 * can be defined.

/*

 * Replace the normal allocation functions with the versions from

 * <linux/decompress/mm.h>. vfree() needs to support vfree(NULL)

 * when XZ_DYNALLOC is used, but the pre-boot free() doesn't support it.

 * Workaround it here because the other decompressors don't need it.

/*

 * FIXME: Not all basic memory functions are provided in architecture-specific

 * files (yet). We define our own versions here for now, but this should be

 * only a temporary solution.

 *

 * memeq and memzero are not used much and any remotely sane implementation

 * is fast enough. memcpy/memmove speed matters in multi-call mode, but

 * the kernel image is decompressed in single-call mode, in which only

 * memmove speed can matter and only if there is a lot of incompressible data

 * (LZMA2 stores incompressible chunks in uncompressed form). Thus, the

 * functions below should just be kept small; it's probably not worth

 * optimizing for speed.

 Not static to avoid a conflict with the prototype in the Linux headers. */

/*

 * Since we need memmove anyway, would use it as memcpy too.

 * Commented out for now to avoid breaking things.

/*

#ifndef memcpy

#	define memcpy memmove

#endif

 XZ_PREBOOT */

 Size of the input and output buffers in multi-call mode */

/*

 * This function implements the API defined in <linux/decompress/generic.h>.

 *

 * This wrapper will automatically choose single-call or multi-call mode

 * of the native XZ decoder API. The single-call mode can be used only when

 * both input and output buffers are available as a single chunk, i.e. when

 * fill() and flush() won't be used.

					/*

					 * This isn't an optimal error code

					 * but it probably isn't worth making

					 * a new one either.

				/*

				 * Setting ret here may hide an error

				 * returned by xz_dec_run(), but probably

				 * it's not too bad.

 This can occur only in multi-call mode. */

/*

 * This macro is used by architecture-specific files to decompress

 * the kernel image.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd.

 * Author: Andrey Ryabinin <a.ryabinin@samsung.com>

/*

 * Some tests use these global variables to store return values from function

 * calls that could otherwise be eliminated by the compiler as dead code.

/*

 * Temporarily enable multi-shot mode. Otherwise, KASAN would only report the

 * first detected bug and panic the kernel if panic_on_warn is enabled. For

 * hardware tag-based KASAN also allow tag checking to be reenabled for each

 * test, see the comment for KUNIT_EXPECT_KASAN_FAIL().

/**

 * KUNIT_EXPECT_KASAN_FAIL() - check that the executed expression produces a

 * KASAN report; causes a test failure otherwise. This relies on a KUnit

 * resource named "kasan_data". Do not use this name for KUnit resources

 * outside of KASAN tests.

 *

 * For hardware tag-based KASAN in sync mode, when a tag fault happens, tag

 * checking is auto-disabled. When this happens, this test handler reenables

 * tag checking. As tag checking can be only disabled or enabled per CPU,

 * this handler disables migration (preemption).

 *

 * Since the compiler doesn't see that the expression can change the fail_data

 * fields, it can reorder or optimize away the accesses to those fields.

 * Use READ/WRITE_ONCE() for the accesses and compiler barriers around the

 * expression to prevent that.

 *

 * In between KUNIT_EXPECT_KASAN_FAIL checks, fail_data.report_found is kept as

 * false. This allows detecting KASAN reports that happen outside of the checks

 * by asserting !fail_data.report_found at the start of KUNIT_EXPECT_KASAN_FAIL

 * and in kasan_test_exit.

	/*

	 * An unaligned access past the requested kmalloc size.

	 * Only generic KASAN can precisely detect these.

	/*

	 * An aligned access into the first out-of-bounds granule that falls

	 * within the aligned kmalloc object.

 Out-of-bounds access past the aligned kmalloc object. */

/*

 * These kmalloc_pagealloc_* tests try allocating a memory chunk that doesn't

 * fit into a slab cache and therefore is allocated via the page allocator

 * fallback. Since this kind of fallback is only implemented for SLUB, these

 * tests are limited to that allocator.

	/*

	 * With generic KASAN page allocations have no redzones, thus

	 * out-of-bounds detection is not guaranteed.

	 * See https://bugzilla.kernel.org/show_bug.cgi?id=210503.

	/*

	 * Allocate a chunk that is large enough, but still fits into a slab

	 * and does not trigger the page allocator fallback in SLUB.

 All offsets up to size2 must be accessible. */

 Generic mode is precise, so unaligned size2 must be inaccessible. */

 For all modes first aligned offset after size2 must be inaccessible. */

 Must be accessible for all modes. */

 Generic mode is precise, so unaligned size2 must be inaccessible. */

 For all modes first aligned offset after size2 must be inaccessible. */

	/*

	 * For all modes all size2, middle, and size1 should land in separate

	 * granules and thus the latter two offsets should be inaccessible.

 page_alloc fallback in only implemented for SLUB. */

 page_alloc fallback in only implemented for SLUB. */

/*

 * Check that krealloc() detects a use-after-free, returns NULL,

 * and doesn't unpoison the freed object.

 This test is specifically crafted for the generic mode. */

/*

 * Note: in the memset tests below, the written range touches both valid and

 * invalid memory. This makes sure that the instrumentation does not only check

 * the starting address but the whole range.

	/*

	 * Hardware tag-based mode doesn't check memmove for negative size.

	 * As a result, this test introduces a side-effect memory corruption,

	 * which can result in a crash.

	/*

	 * Only generic KASAN uses quarantine, which is required to avoid a

	 * kernel memory corruption this test causes.

	/*

	 * For tag-based KASAN ptr1 and ptr2 tags might happen to be the same.

	 * Allow up to 16 attempts at generating different tags.

	/*

	 * Several allocations with a delay to allow for lazy per memcg kmem

	 * cache creation.

	/*

	 * Deliberate out-of-bounds access. To prevent CONFIG_UBSAN_LOCAL_BOUNDS

	 * from failing here and panicking the kernel, access the array via a

	 * volatile pointer, which will prevent the compiler from being able to

	 * determine the array bounds.

	 *

	 * This access uses a volatile pointer to char (char *volatile) rather

	 * than the more conventional pointer to volatile char (volatile char *)

	 * because we want to prevent the compiler from making inferences about

	 * the pointer itself (i.e. its array bounds), not the data that it

	 * refers to.

 Only generic mode instruments globals. */

 Check that ksize() makes the whole object accessible. */

 This access shouldn't trigger a KASAN report. */

 This one must. */

/*

 * Check that a use-after-free is detected by ksize() and via normal accesses

 * after it.

 See comment in kasan_global_oob. */

 See comment in kasan_global_oob. */

 Only generic mode instruments dynamic allocas. */

 See comment in kasan_global_oob. */

 Only generic mode instruments dynamic allocas. */

 Trigger invalid free, the object doesn't get freed. */

	/*

	 * Properly free the object to prevent the "Objects remaining in

	 * test_cache on __kmem_cache_shutdown" BUG failure.

	/*

	 * str* functions are not instrumented with CONFIG_AMD_MEM_ENCRYPT.

	 * See https://bugzilla.kernel.org/show_bug.cgi?id=206337 for details.

	/*

	 * str* functions are not instrumented with CONFIG_AMD_MEM_ENCRYPT.

	 * See https://bugzilla.kernel.org/show_bug.cgi?id=206337 for details.

	/*

	 * str* functions are not instrumented with CONFIG_AMD_MEM_ENCRYPT.

	 * See https://bugzilla.kernel.org/show_bug.cgi?id=206337 for details.

	/*

	 * Try to cause only 1 invalid access (less spam in dmesg).

	 * For that we need ptr to point to zeroed byte.

	 * Skip metadata that could be stored in freed object so ptr

	 * will likely point to zeroed byte.

 This test is specifically crafted for the generic mode. */

	/*

	 * Allocate 1 more byte, which causes kzalloc to round up to 16 bytes;

	 * this way we do not actually corrupt other memory.

	/*

	 * Below calls try to access bit within allocated memory; however, the

	 * below accesses are still out-of-bounds, since bitops are defined to

	 * operate on the whole long the bit is in.

	/*

	 * Below calls try to access bit beyond allocated memory.

 This test is specifically crafted for tag-based modes. */

 kmalloc-64 cache will be used and the last 16 bytes will be the redzone. */

 Do the accesses past the 48 allocated bytes, but within the redone. */

	/*

	 * We have to be careful not to hit the guard page.

	 * The MMU will catch that and crash us.

/*

 * Check that the assigned pointer tag falls within the [KASAN_TAG_MIN,

 * KASAN_TAG_KERNEL) range (note: excluding the match-all tag) for tag-based

 * modes.

 Check that 0xff works as a match-all pointer tag for tag-based modes. */

 Backup the assigned tag. */

 Reset the tag to 0xff.*/

 This access shouldn't trigger a KASAN report. */

 Recover the pointer tag and free. */

 Check that there are no match-all memory tags for tag-based modes. */

 For each possible tag value not matching the pointer tag. */

 Mark the first memory granule with the chosen memory tag. */

 This access must cause a KASAN report. */

 Recover the memory tag and free. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Test module to generate lockups

 should be at least readable kernel address */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Test cases for functions and macros in bits.h

 these should fail compilation */

 these should fail compilation */

 Unknown input */

 Valid input */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *      crc16.c

* CRC table for the CRC-16. The poly is 0x8005 (x^16 + x^15 + x^2 + 1) */

/**

 * crc16 - compute the CRC-16 for the data buffer

 * @crc:	previous CRC value

 * @buffer:	data pointer

 * @len:	number of bytes in the buffer

 *

 * Returns the updated CRC value.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * LZO decompressor for the Linux kernel. Code borrowed from the lzo

 * implementation by Markus Franz Xaver Johannes Oberhumer.

 *

 * Linux kernel adaptation:

 * Copyright (C) 2009

 * Albin Tonnerre, Free Electrons <albin.tonnerre@free-electrons.com>

 *

 * Original code:

 * Copyright (C) 1996-2005 Markus Franz Xaver Johannes Oberhumer

 * All Rights Reserved.

 *

 * Markus F.X.J. Oberhumer

 * <markus@oberhumer.com>

 * http://www.oberhumer.com/opensource/lzop/

	/*

	 * Check that there's enough input to possibly have a valid header.

	 * Then it is possible to parse several fields until the minimum

	 * size may have been used.

 read magic: 9 first bits */

	/* get version (2bytes), skip library version (2),

	 * 'need to be extracted' version (2) and

 flags + filter info */

 flags */

	/*

	 * At least mode, mtime_low, filename length, and checksum must

	 * be left to be parsed. If also mtime_high is present, it's OK

	 * because the next input buffer check is after reading the

	 * filename length.

 skip mode and mtime_low */

 skip mtime_high */

 don't care about the file name, and skip checksum */

		/*

		 * Start from in_buf + HEADER_SIZE_MAX to make it possible

		 * to use memcpy() to copy the unused data to the beginning

		 * of the buffer. This way memmove() isn't needed which

		 * is missing from pre-boot environments of most archs.

 Move the unused data to the beginning of the buffer. */

 read uncompressed block size */

 exit if last block */

 read compressed block size, and skip block checksum info */

 decompress */

		/* When the input data is not compressed at all,

		 * lzo1x_decompress_safe will fail, so call memcpy()

			/*

			 * If there happens to still be unused data left in

			 * in_buf, move it to the beginning of the buffer.

			 * Use a loop to avoid memmove() dependency.

 SPDX-License-Identifier: GPL-2.0

/*

  Generic support for BUG()



  This respects the following config options:



  CONFIG_BUG - emit BUG traps.  Nothing happens without this.

  CONFIG_GENERIC_BUG - enable this code.

  CONFIG_GENERIC_BUG_RELATIVE_POINTERS - use 32-bit pointers relative to

	the containing struct bug_entry for bug_addr and file.

  CONFIG_DEBUG_BUGVERBOSE - emit full file+line information for each BUG



  CONFIG_BUG and CONFIG_DEBUG_BUGVERBOSE are potentially user-settable

  (though they're generally always on).



  CONFIG_GENERIC_BUG is set by each architecture using this code.



  To use this, your architecture must:



  1. Set up the config options:

     - Enable CONFIG_GENERIC_BUG if CONFIG_BUG



  2. Implement BUG (and optionally BUG_ON, WARN, WARN_ON)

     - Define HAVE_ARCH_BUG

     - Implement BUG() to generate a faulting instruction

     - NOTE: struct bug_entry does not have "file" or "line" entries

       when CONFIG_DEBUG_BUGVERBOSE is not enabled, so you must generate

       the values accordingly.



  3. Implement the trap

     - In the illegal instruction trap handler (typically), verify

       that the fault was in kernel mode, and call report_bug()

     - report_bug() will return whether it was a false alarm, a warning,

       or an actual bug.

     - You must implement the is_valid_bugaddr(bugaddr) callback which

       returns true if the eip is a real kernel address, and it points

       to the expected BUG trap instruction.



    Jeremy Fitzhardinge <jeremy@goop.org> 2006

 Updates are protected by module mutex */

 Find the __bug_table section, if present */

	/*

	 * Strictly speaking this should have a spinlock to protect against

	 * traversals, but since we only traverse on BUG()s, a spinlock

	 * could potentially lead to deadlock and thus be counter-productive.

	 * Thus, this uses RCU to safely manipulate the bug list, since BUG

	 * must run in non-interruptive state.

		/*

		 * Since this is the only store, concurrency is not an issue.

	/*

	 * BUG() and WARN_ON() families don't print a custom debug message

	 * before triggering the exception handler, so we must add the

	 * "cut here" line now. WARN() issues its own "cut here" before the

	 * extra debugging message it writes before triggering the handler.

 this is a WARN_ON rather than BUG/BUG_ON */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Test cases for API provided by cmdline.c

 Only first and leading '-' will advance the pointer */

 Only first '-' after the number will advance the pointer */

 When "" or "-" the result will be valid integer */

		/*

		 * Only first and leading '-' not followed by integer

		 * will advance the pointer.

 SPDX-License-Identifier: GPL-2.0

 error-inject.c: Function-level error injection table

 Whitelist of symbols that can be overridden for error injection. */

/*

 * Lookup and populate the error_injection_list.

 *

 * For safety reasons we only allow certain functions to be overridden with

 * bpf_error_injection, so we need to populate the list of the symbols that have

 * been marked as safe for overriding.

 Markers of the _error_inject_whitelist section */

 Module notifier call back, checking error injection table on the module */

 !CONFIG_MODULES */

/*

 * error_injection/whitelist -- shows which functions can be overridden for

 * error injection.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Test cases for using floating point operations inside a kernel module.

 *

 * This tests kernel_fpu_begin() and kernel_fpu_end() functions, especially

 * when userland has modified the floating point control registers. The kernel

 * state might depend on the state set by the userland thread that was active

 * before a syscall.

 *

 * To facilitate the test, this module registers file

 * /sys/kernel/debug/selftest_helpers/test_fpu, which when read causes a

 * sequence of floating point operations. If the operations fail, either the

 * read returns error status or the kernel crashes.

 * If the operations succeed, the read returns "1\n".

	/*

	 * This sequence of operations tests that rounding mode is

	 * to nearest and that denormal numbers are supported.

	 * Volatile variables are used to avoid compiler optimizing

	 * the calculations away.

 Sets precision flag */

 Result depends on rounding mode */

 Denormal and very large values */

 Depends on denormal support */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kmod stress test driver

 *

 * Copyright (C) 2017 Luis R. Rodriguez <mcgrof@kernel.org>

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of the GNU General Public License as published by the Free

 * Software Foundation; either version 2 of the License, or at your option any

 * later version; or, when distributed separately from the Linux kernel or

 * when incorporated into other software packages, subject to the following

 * license:

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of copyleft-next (version 0.3.1 or later) as published

 * at http://copyleft-next.org/.

/*

 * This driver provides an interface to trigger and test the kernel's

 * module loader through a series of configurations and a few triggers.

 * To test this driver use the following script as root:

 *

 * tools/testing/selftests/kmod/kmod.sh --help

/*

 * For device allocation / registration

/*

 * num_test_devs actually represents the *next* ID of the next

 * device we will allow to create.

/**

 * enum kmod_test_case - linker table test case

 *

 * If you add a  test case, please be sure to review if you need to se

 * @need_mod_put for your tests case.

 *

 * @TEST_KMOD_DRIVER: stress tests request_module()

 * @TEST_KMOD_FS_TYPE: stress tests get_fs_type()

/**

 * kmod_test_device_info - thread info

 *

 * @ret_sync: return value if request_module() is used, sync request for

 * 	@TEST_KMOD_DRIVER

 * @fs_sync: return value of get_fs_type() for @TEST_KMOD_FS_TYPE

 * @thread_idx: thread ID

 * @test_dev: test device test is being performed under

 * @need_mod_put: Some tests (get_fs_type() is one) requires putting the module

 *	(module_put(fs_sync->owner)) when done, otherwise you will not be able

 *	to unload the respective modules and re-test. We use this to keep

 *	accounting of when we need this and to help out in case we need to

 *	error out and deal with module_put() on error.

/**

 * kmod_test_device - test device to help test kmod

 *

 * @dev_idx: unique ID for test device

 * @config: configuration for the test

 * @misc_dev: we use a misc device under the hood

 * @dev: pointer to misc_dev's own struct device

 * @config_mutex: protects configuration of test

 * @trigger_mutex: the test trigger can only be fired once at a time

 * @thread_lock: protects @done count, and the @info per each thread

 * @done: number of threads which have completed or failed

 * @test_is_oom: when we run out of memory, use this to halt moving forward

 * @kthreads_done: completion used to signal when all work is done

 * @list: needed to be part of the reg_test_devs

 * @info: array of info for each thread

 Must run with thread_mutex held */

 __trigger_config_run() already checked for test sanity */

		/*

		 * Only capture errors, if one is found that's

		 * enough, for now.

 For now we make this simple */

/*

 * XXX: add result option to display if all errors did not match.

 * For now we just keep any error code if one was found.

 *

 * If this ran it means *all* tasks were created fine and we

 * are now just collecting results.

 *

 * Only propagate errors, do not override with a subsequent success case.

	/*

	 * Note: request_module() returns 256 for a module not found even

	 * though modprobe itself returns 1.

		/*

		 * info->task_sync is well protected, it can only be

		 * NULL or a pointer to a struct. If its NULL we either

		 * never ran, or we did and we completed the work. Completed

		 * tasks *always* put the module for us. This is a sanity

		 * check -- just in case.

/*

 * Only wait *iff* we did not run into any errors during all of our thread

 * set up. If run into any issues we stop threads and just bail out with

 * an error to the trigger. This also means we don't need any tally work

 * for any threads which fail.

/*

 * This ensures we don't allow kicking threads through if our configuration

 * is faulty.

	/*

	 * We must return 0 after a trigger even unless something went

	 * wrong with the setup of the test. If the test setup went fine

	 * then userspace must just check the result of config->test_result.

	 * One issue with relying on the return from a call in the kernel

	 * is if the kernel returns a positive value using this trigger

	 * will not return the value to userspace, it would be lost.

	 *

	 * By not relying on capturing the return value of tests we are using

	 * through the trigger it also us to run tests with set -e and only

	 * fail when something went wrong with the driver upon trigger

	 * requests.

	/* For all intents and purposes we don't care what userspace

	 * sent this trigger, we care only that we were triggered.

	 * We treat the return value only for caputuring issues with

	 * the test setup. At this point all the test variables should

	 * have been allocated so typically this should never fail.

	/*

	 * Note: any return > 0 will be treated as success

	 * and the error value will not be available to userspace.

	 * Do not rely on trying to send to userspace a test value

	 * return value as positive return errors will be lost.

/*

 * XXX: move to kstrncpy() once merged.

 *

 * Users should use kfree_const() when freeing these.

/*

 * As per sysfs_kf_seq_show() the buf is max PAGE_SIZE.

/*

 * Old kernels may not have this, if you want to port this code to

 * test it on older kernels.

 Always return full write size even if we didn't consume all */

 Always return full write size even if we didn't consume all */

 Always return full write size even if we didn't consume all */

 int should suffice for number of devices, test for wrap */

	/*

	 * With some work we might be able to gracefully enable

	 * testing with this driver built-in, for now this seems

	 * rather risky. For those willing to try have at it,

	 * and enable the below. Good luck! If that works, try

	 * lowering the init level for more fun.

 SPDX-License-Identifier: GPL-2.0

/*

 * SHA1 routine optimized to do word accesses rather than byte accesses,

 * and to avoid unnecessary copies into the context array.

 *

 * This was based on the git SHA1 implementation.

/*

 * If you have 32 registers or more, the compiler can (and should)

 * try to change the array[] accesses into registers. However, on

 * machines with less than ~25 registers, that won't really work,

 * and at least gcc will make an unholy mess of it.

 *

 * So to avoid that mess which just slows things down, we force

 * the stores to memory to actually happen (we might be better off

 * with a 'W(t)=(val);asm("":"+m" (W(t))' there instead, as

 * suggested by Artur Skawina - that will also make gcc unable to

 * try to do the silly "optimize away loads" part because it won't

 * see what the value will be).

 *

 * Ben Herrenschmidt reports that on PPC, the C version comes close

 * to the optimized asm with this (ie on PPC you don't want that

 * 'volatile', since there are lots of registers).

 *

 * On ARM we get the best code generation by forcing a full memory barrier

 * between each SHA_ROUND, otherwise gcc happily get wild with spilling and

 * the stack frame size simply explode and performance goes down the drain.

 This "rolls" over the 512-bit array */

/*

 * Where do we get the source from? The first 16 iterations get it from

 * the input data, the next mix it from the 512-bit array.

/**

 * sha1_transform - single block SHA1 transform (deprecated)

 *

 * @digest: 160 bit digest to update

 * @data:   512 bits of data to hash

 * @array:  16 words of workspace (see note)

 *

 * This function executes SHA-1's internal compression function.  It updates the

 * 160-bit internal state (@digest) with a single 512-bit data block (@data).

 *

 * Don't use this function.  SHA-1 is no longer considered secure.  And even if

 * you do have to use SHA-1, this isn't the correct way to hash something with

 * SHA-1 as this doesn't handle padding and finalization.

 *

 * Note: If the hash is security sensitive, the caller should be sure

 * to clear the workspace. This is left to the caller to avoid

 * unnecessary clears between chained hashing operations.

 Round 1 - iterations 0-16 take their input from 'data' */

 Round 1 - tail. Input from 512-bit mixing array */

 Round 2 */

 Round 3 */

 Round 4 */

/**

 * sha1_init - initialize the vectors for a SHA1 digest

 * @buf: vector to initialize

 SPDX-License-Identifier: GPL-2.0-only

/*

 * cpu_rmap.c: CPU affinity reverse-map support

 * Copyright 2011 Solarflare Communications Inc.

/*

 * These functions maintain a mapping from CPUs to some ordered set of

 * objects with CPU affinities.  This can be seen as a reverse-map of

 * CPU affinity.  However, we do not assume that the object affinities

 * cover all CPUs in the system.  For those CPUs not directly covered

 * by object affinities, we attempt to find a nearest object based on

 * CPU topology.

/**

 * alloc_cpu_rmap - allocate CPU affinity reverse-map

 * @size: Number of objects to be mapped

 * @flags: Allocation flags e.g. %GFP_KERNEL

 This is a silly number of objects, and we use u16 indices. */

 Offset of object pointer array from base structure */

	/* Initially assign CPUs to objects on a rota, since we have

	 * no idea where the objects are.  Use infinite distance, so

	 * any object with known distance is preferable.  Include the

	 * CPUs that are not present/online, since we definitely want

	 * any newly-hotplugged CPUs to have some object assigned.

/**

 * cpu_rmap_release - internal reclaiming helper called from kref_put

 * @ref: kref to struct cpu_rmap

/**

 * cpu_rmap_get - internal helper to get new ref on a cpu_rmap

 * @rmap: reverse-map allocated with alloc_cpu_rmap()

/**

 * cpu_rmap_put - release ref on a cpu_rmap

 * @rmap: reverse-map allocated with alloc_cpu_rmap()

/* Reevaluate nearest object for given CPU, comparing with the given

 * neighbours at the given distance.

/**

 * cpu_rmap_add - add object to a rmap

 * @rmap: CPU rmap allocated with alloc_cpu_rmap()

 * @obj: Object to add to rmap

 *

 * Return index of object.

/**

 * cpu_rmap_update - update CPU rmap following a change of object affinity

 * @rmap: CPU rmap to update

 * @index: Index of object whose affinity changed

 * @affinity: New CPU affinity of object

	/* Invalidate distance for all CPUs for which this used to be

	 * the nearest object.  Mark those CPUs for update.

	/* Set distance to 0 for all CPUs in the new affinity mask.

	 * Mark all CPUs within their NUMA nodes for update.

 Update distances based on topology */

		/* We could continue into NUMA node distances, but for now

		 * we give up.

 Glue between IRQ affinity notifiers and CPU rmaps */

/**

 * free_irq_cpu_rmap - free a CPU affinity reverse-map used for IRQs

 * @rmap: Reverse-map allocated with alloc_irq_cpu_map(), or %NULL

 *

 * Must be called in process context, before freeing the IRQs.

/**

 * irq_cpu_rmap_notify - callback for IRQ subsystem when IRQ affinity updated

 * @notify: struct irq_affinity_notify passed by irq/manage.c

 * @mask: cpu mask for new SMP affinity

 *

 * This is executed in workqueue context.

/**

 * irq_cpu_rmap_release - reclaiming callback for IRQ subsystem

 * @ref: kref to struct irq_affinity_notify passed by irq/manage.c

/**

 * irq_cpu_rmap_add - add an IRQ to a CPU affinity reverse-map

 * @rmap: The reverse-map

 * @irq: The IRQ number

 *

 * This adds an IRQ affinity notifier that will update the reverse-map

 * automatically.

 *

 * Must be called in process context, after the IRQ is allocated but

 * before it is bound with request_irq().

 SPDX-License-Identifier: GPL-2.0

/*

 * Fast batching percpu counters.

 CONFIG_DEBUG_OBJECTS_PERCPU_COUNTER */

 CONFIG_DEBUG_OBJECTS_PERCPU_COUNTER */

/*

 * This function is both preempt and irq safe. The former is due to explicit

 * preemption disable. The latter is guaranteed by the fact that the slow path

 * is explicitly protected by an irq-safe spinlock whereas the fast patch uses

 * this_cpu_add which is irq-safe by definition. Hence there is no need muck

 * with irq state before calling this one

/*

 * For percpu_counter with a big batch, the devication of its count could

 * be big, and there is requirement to reduce the deviation, like when the

 * counter's batch could be runtime decreased to get a better accuracy,

 * which can be achieved by running this sync function on each CPU.

/*

 * Add up all the per-cpu counts, return the result.  This is a more accurate

 * but much slower version of percpu_counter_read_positive()

/*

 * Compare counter against given value.

 * Return 1 if greater, 0 if equal and -1 if less

 Check to see if rough count will be sufficient for comparison */

 Need to use precise count */

 SPDX-License-Identifier: GPL-2.0-only

 base autodetection */

 digit >= base */

 overflow */

 negative */

 sign is first character if any */

 nothing after \n */

 sign is first character if any */

 SPDX-License-Identifier: GPL-2.0

/**

 * hweightN - returns the hamming weight of a N-bit word

 * @x: the word to weigh

 *

 * The Hamming Weight of a number is the total number of bits set in it.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Test cases for compiler-based stack variable zeroing via

 * -ftrivial-auto-var-init={zero,pattern} or CONFIG_GCC_PLUGIN_STRUCTLEAK*.

 *

 * External build example:

 *	clang -O2 -Wall -ftrivial-auto-var-init=pattern \

 *		-o test_stackinit test_stackinit.c

 Userspace headers. */

 Linux kernel-ism stubs for stand-alone userspace build. */

*/

*/

*/

*/

 License: str */	\

 __KERNEL__ */

 Exfiltration buffer. */

 Character array to trigger stack protector in all functions. */

 Volatile mask to convince compiler to copy memory with 0xff. */

 Location and size tracking to validate fill and test are colocated. */

 Whether the test is expected to fail. */

*/

*/

*/

*/

/*

 * For the struct, intentionally poison padding to see if it gets

 * copied out in direct assignments.

*/

*/

*/

/*

 * @name: unique string name for the test

 * @var_type: type to be tested for zeroing initialization

 * @which: is this a SCALAR, STRING, or STRUCT type?

 * @init_level: what kind of initialization is performed

 * @xfail: is this test expected to fail?

 Returns 0 on success, 1 on failure. */			\

 Notice when a new test is larger than expected. */	\

 Fill clone type with zero for per-field init. */	\

 Clear entire check buffer for 0xFF overlap test. */	\

 Fill stack with 0xFF. */				\

 Verify all bytes overwritten with 0xFF. */		\

 Clear entire check buffer for later bit tests. */	\

 Extract stack-defined variable contents. */		\

 Validate that compiler lined up fill and target. */	\

 Look for any bytes still 0xFF in check region. */	\

 no-op to force compiler into ignoring "uninitialized" vars */\

 Will always be true, but compiler doesn't know. */	\

	/*							\

	 * Keep this buffer around to make sure we've got a	\

	 * stack frame of SOME kind...				\

 Fill variable with 0xFF. */				\

 Silence "never initialized" warnings. */		\

 Exfiltrate "var". */					\

 Structure with no padding. */

 Simple structure with padding likely to be covered by compiler. */

 3 byte padding hole here. */

 Trigger unhandled padding in a structure. */

 61 byte padding hole here. */

 "sizeof(unsigned long) - 1" byte padding hole here. */

 Test if STRUCTLEAK is clearing structs with __user fields. */

 These should be fully initialized all the time! */

 Struct initializers: padding may be left uninitialized. */

 No initialization without compiler instrumentation. */

 Initialization of members with __user attribute. */

/*

 * Check two uses through a variable declaration outside either path,

 * which was noticed as a special case in porting earlier stack init

 * compiler logic.

		/*

		 * This is intentionally unreachable. To silence the

		 * warning, build with -Wno-switch-unreachable

/*

 * These are expected to fail for most configurations because neither

 * GCC nor Clang have a way to perform initialization of variables in

 * non-code areas (i.e. in a switch statement before the first "case").

 * https://bugs.llvm.org/show_bug.cgi?id=44916

 These are explicitly initialized and should always pass. */

 Padding here appears to be accidentally always initialized? */

 Padding initialization depends on compiler behaviors. */

 Everything fails this since it effectively performs a memcpy(). */

 STRUCTLEAK_BYREF_ALL should cover everything from here down. */

 STRUCTLEAK_BYREF should cover from here down. */

 STRUCTLEAK will only cover this. */

 SPDX-License-Identifier: GPL-2.0

/**

 * crc32init_le() - allocate and initialize LE table data

 *

 * crc is the crc of the byte i; other entries are filled in based on the

 * fact that crctable[i^j] = crctable[i] ^ crctable[j].

 *

/**

 * crc32init_be() - allocate and initialize BE table data

 this file is generated - do not edit */\n\n");

 SPDX-License-Identifier: GPL-2.0

 Pre-boot environment: included */

/* prevent inclusion of _LINUX_KERNEL_H in pre-boot environment: lots

 STATIC */

 initramfs et al: linked */

 STATIC */

 Included from initramfs et al code */

 32 K */

 no limit */

 Always allocate the full workspace for DFLTCC */

 verify the gzip header */

	/* skip over gzip header (1f,8b,08... 10 bytes total +

	 * possible asciz filename)

 skip over asciz filename */

			/*

			 * If the filename doesn't fit into the buffer,

			 * the file is very probably corrupt. Don't try

			 * to read more data.

 Always keep the window for DFLTCC */

 TODO: handle case where both pos and fill are set */

 Write any data generated */

 after Z_FINISH, only Z_STREAM_END is "we unpacked it all" */

 add + 8 to skip over trailer */

 returns Z_OK (0) if successful */

 SPDX-License-Identifier: GPL-2.0-or-later

/* Function to determine if a thread group is single threaded or not

 *

 * Copyright (C) 2008 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

 * - Derived from security/selinux/hooks.c

/*

 * Returns true if the task does not share ->mm with another thread/process.

			/*

			 * t->mm == NULL. Make sure next_thread/next_task

			 * will see other CLONE_VM tasks which might be

			 * forked before exiting.

 SPDX-License-Identifier: GPL-2.0

/*

 * Test module for stress and analyze performance of vmalloc allocator.

 * (C) 2018 Uladzislau Rezki (Sony) <urezki@gmail.com>

 Add a new test case description here. */

/*

 * Read write semaphore for synchronization of setup

 * phase that is done in main thread and workers.

/*

 * Completion tracking for worker threads.

		/*

		 * Maximum 1024 pages, if PAGE_SIZE is 4096.

		/*

		 * Maximum 10 pages.

/*

 * This test case is supposed to be failed.

  Success */

 Success */

		/*

		 * Maximum PAGE_SIZE

 Add a new test case here. */

 Cut the range. */

 Swap indexes. */

	/*

	 * Block until initialization is done.

		/*

		 * Skip tests if run_test_mask has been specified.

		/*

		 * Take an average time that test took.

	/*

	 * Wait for the kthread_stop() call.

	/*

	 * A maximum number of workers is defined as hard-coded

	 * value and set to USHRT_MAX. We add such gap just in

	 * case and for potential heavy stressing.

 Allocate the space for test instances. */

	/*

	 * Set some basic configurations plus sanity check.

	/*

	 * Put on hold all workers.

 Success. */

	/*

	 * Now let the workers do their job.

	/*

	 * Sleep quiet until all workers are done with 1 second

	 * interval. Since the test can take a lot of time we

	 * can run into a stack trace of the hung task. That is

	 * why we go with completion_timeout and HZ value.

 Fail will directly unload the module */

 SPDX-License-Identifier: GPL-2.0-only

/* ----------------------------------------------------------------------- *

 *

 *   Copyright 2012 Intel Corporation; author H. Peter Anvin

 *

/*

 * earlycpio.c

 *

 * Find a specific cpio member; must precede any compressed content.

 * This is used to locate data items in the initramfs used by the

 * kernel itself during early boot (before the main initramfs is

 * decompressed.)  It is the responsibility of the initramfs creator

 * to ensure that these items are uncompressed at the head of the

 * blob.  Depending on the boot loader or package tool that may be a

 * separate file or part of the same file.

/**

 * find_cpio_data - Search for files in an uncompressed cpio

 * @path:       The directory to search for, including a slash at the end

 * @data:       Pointer to the cpio archive or a header inside

 * @len:        Remaining length of the cpio based on data pointer

 * @nextoff:    When a matching file is found, this is the offset from the

 *              beginning of the cpio to the beginning of the next file, not the

 *              matching file itself. It can be used to iterate through the cpio

 *              to find all files inside of a directory path.

 *

 * Return:      &struct cpio_data containing the address, length and

 *              filename (with the directory path cut off) of the found file.

 *              If you search for a filename and not for files in a directory,

 *              pass the absolute path of the filename in the cpio and make sure

 *              the match returned an empty filename string.

 All cpio headers need to be 4-byte aligned */

 The magic field is only 6 characters */

 Invalid hexadecimal */

 All other fields are 8 characters */

 Invalid magic */

 Buffer overrun */

 Found it! */

 SPDX-License-Identifier: GPL-2.0-only

/**

 * idr_alloc_u32() - Allocate an ID.

 * @idr: IDR handle.

 * @ptr: Pointer to be associated with the new ID.

 * @nextid: Pointer to an ID.

 * @max: The maximum ID to allocate (inclusive).

 * @gfp: Memory allocation flags.

 *

 * Allocates an unused ID in the range specified by @nextid and @max.

 * Note that @max is inclusive whereas the @end parameter to idr_alloc()

 * is exclusive.  The new ID is assigned to @nextid before the pointer

 * is inserted into the IDR, so if @nextid points into the object pointed

 * to by @ptr, a concurrent lookup will not find an uninitialised ID.

 *

 * The caller should provide their own locking to ensure that two

 * concurrent modifications to the IDR are not possible.  Read-only

 * accesses to the IDR may be done under the RCU read lock or may

 * exclude simultaneous writers.

 *

 * Return: 0 if an ID was allocated, -ENOMEM if memory allocation failed,

 * or -ENOSPC if no free IDs could be found.  If an error occurred,

 * @nextid is unchanged.

 there is a memory barrier inside radix_tree_iter_replace() */

/**

 * idr_alloc() - Allocate an ID.

 * @idr: IDR handle.

 * @ptr: Pointer to be associated with the new ID.

 * @start: The minimum ID (inclusive).

 * @end: The maximum ID (exclusive).

 * @gfp: Memory allocation flags.

 *

 * Allocates an unused ID in the range specified by @start and @end.  If

 * @end is <= 0, it is treated as one larger than %INT_MAX.  This allows

 * callers to use @start + N as @end as long as N is within integer range.

 *

 * The caller should provide their own locking to ensure that two

 * concurrent modifications to the IDR are not possible.  Read-only

 * accesses to the IDR may be done under the RCU read lock or may

 * exclude simultaneous writers.

 *

 * Return: The newly allocated ID, -ENOMEM if memory allocation failed,

 * or -ENOSPC if no free IDs could be found.

/**

 * idr_alloc_cyclic() - Allocate an ID cyclically.

 * @idr: IDR handle.

 * @ptr: Pointer to be associated with the new ID.

 * @start: The minimum ID (inclusive).

 * @end: The maximum ID (exclusive).

 * @gfp: Memory allocation flags.

 *

 * Allocates an unused ID in the range specified by @nextid and @end.  If

 * @end is <= 0, it is treated as one larger than %INT_MAX.  This allows

 * callers to use @start + N as @end as long as N is within integer range.

 * The search for an unused ID will start at the last ID allocated and will

 * wrap around to @start if no free IDs are found before reaching @end.

 *

 * The caller should provide their own locking to ensure that two

 * concurrent modifications to the IDR are not possible.  Read-only

 * accesses to the IDR may be done under the RCU read lock or may

 * exclude simultaneous writers.

 *

 * Return: The newly allocated ID, -ENOMEM if memory allocation failed,

 * or -ENOSPC if no free IDs could be found.

/**

 * idr_remove() - Remove an ID from the IDR.

 * @idr: IDR handle.

 * @id: Pointer ID.

 *

 * Removes this ID from the IDR.  If the ID was not previously in the IDR,

 * this function returns %NULL.

 *

 * Since this function modifies the IDR, the caller should provide their

 * own locking to ensure that concurrent modification of the same IDR is

 * not possible.

 *

 * Return: The pointer formerly associated with this ID.

/**

 * idr_find() - Return pointer for given ID.

 * @idr: IDR handle.

 * @id: Pointer ID.

 *

 * Looks up the pointer associated with this ID.  A %NULL pointer may

 * indicate that @id is not allocated or that the %NULL pointer was

 * associated with this ID.

 *

 * This function can be called under rcu_read_lock(), given that the leaf

 * pointers lifetimes are correctly managed.

 *

 * Return: The pointer associated with this ID.

/**

 * idr_for_each() - Iterate through all stored pointers.

 * @idr: IDR handle.

 * @fn: Function to be called for each pointer.

 * @data: Data passed to callback function.

 *

 * The callback function will be called for each entry in @idr, passing

 * the ID, the entry and @data.

 *

 * If @fn returns anything other than %0, the iteration stops and that

 * value is returned from this function.

 *

 * idr_for_each() can be called concurrently with idr_alloc() and

 * idr_remove() if protected by RCU.  Newly added entries may not be

 * seen and deleted entries may be seen, but adding and removing entries

 * will not cause other entries to be skipped, nor spurious ones to be seen.

/**

 * idr_get_next_ul() - Find next populated entry.

 * @idr: IDR handle.

 * @nextid: Pointer to an ID.

 *

 * Returns the next populated entry in the tree with an ID greater than

 * or equal to the value pointed to by @nextid.  On exit, @nextid is updated

 * to the ID of the found value.  To use in a loop, the value pointed to by

 * nextid must be incremented by the user.

/**

 * idr_get_next() - Find next populated entry.

 * @idr: IDR handle.

 * @nextid: Pointer to an ID.

 *

 * Returns the next populated entry in the tree with an ID greater than

 * or equal to the value pointed to by @nextid.  On exit, @nextid is updated

 * to the ID of the found value.  To use in a loop, the value pointed to by

 * nextid must be incremented by the user.

/**

 * idr_replace() - replace pointer for given ID.

 * @idr: IDR handle.

 * @ptr: New pointer to associate with the ID.

 * @id: ID to change.

 *

 * Replace the pointer registered with an ID and return the old value.

 * This function can be called under the RCU read lock concurrently with

 * idr_alloc() and idr_remove() (as long as the ID being removed is not

 * the one being replaced!).

 *

 * Returns: the old value on success.  %-ENOENT indicates that @id was not

 * found.  %-EINVAL indicates that @ptr was not valid.

/**

 * DOC: IDA description

 *

 * The IDA is an ID allocator which does not provide the ability to

 * associate an ID with a pointer.  As such, it only needs to store one

 * bit per ID, and so is more space efficient than an IDR.  To use an IDA,

 * define it using DEFINE_IDA() (or embed a &struct ida in a data structure,

 * then initialise it using ida_init()).  To allocate a new ID, call

 * ida_alloc(), ida_alloc_min(), ida_alloc_max() or ida_alloc_range().

 * To free an ID, call ida_free().

 *

 * ida_destroy() can be used to dispose of an IDA without needing to

 * free the individual IDs in it.  You can use ida_is_empty() to find

 * out whether the IDA has any IDs currently allocated.

 *

 * The IDA handles its own locking.  It is safe to call any of the IDA

 * functions without synchronisation in your code.

 *

 * IDs are currently limited to the range [0-INT_MAX].  If this is an awkward

 * limitation, it should be quite straightforward to raise the maximum.

/*

 * Developer's notes:

 *

 * The IDA uses the functionality provided by the XArray to store bitmaps in

 * each entry.  The XA_FREE_MARK is only cleared when all bits in the bitmap

 * have been set.

 *

 * I considered telling the XArray that each slot is an order-10 node

 * and indexing by bit number, but the XArray can't allow a single multi-index

 * entry in the head, which would significantly increase memory consumption

 * for the IDA.  So instead we divide the index by the number of bits in the

 * leaf bitmap before doing a radix tree lookup.

 *

 * As an optimisation, if there are only a few low bits set in any given

 * leaf, instead of allocating a 128-byte bitmap, we store the bits

 * as a value entry.  Value entries never have the XA_FREE_MARK cleared

 * because we can always convert them into a bitmap entry.

 *

 * It would be possible to optimise further; once we've run out of a

 * single 128-byte bitmap, we currently switch to a 576-byte node, put

 * the 128-byte bitmap in the first entry and then start allocating extra

 * 128-byte entries.  We could instead use the 512 bytes of the node's

 * data as a bitmap before moving to that scheme.  I do not believe this

 * is a worthwhile optimisation; Rasmus Villemoes surveyed the current

 * users of the IDA and almost none of them use more than 1024 entries.

 * Those that do use more than the 8192 IDs that the 512 bytes would

 * provide.

 *

 * The IDA always uses a lock to alloc/free.  If we add a 'test_bit'

 * equivalent, it will still need locking.  Going to RCU lookup would require

 * using RCU to free bitmaps, and that's not trivial without embedding an

 * RCU head in the bitmap, which adds a 2-pointer overhead to each 128-byte

 * bitmap, which is excessive.

/**

 * ida_alloc_range() - Allocate an unused ID.

 * @ida: IDA handle.

 * @min: Lowest ID to allocate.

 * @max: Highest ID to allocate.

 * @gfp: Memory allocation flags.

 *

 * Allocate an ID between @min and @max, inclusive.  The allocated ID will

 * not exceed %INT_MAX, even if @max is larger.

 *

 * Context: Any context. It is safe to call this function without

 * locking in your code.

 * Return: The allocated ID, or %-ENOMEM if memory could not be allocated,

 * or %-ENOSPC if there are no free IDs.

/**

 * ida_free() - Release an allocated ID.

 * @ida: IDA handle.

 * @id: Previously allocated ID.

 *

 * Context: Any context. It is safe to call this function without

 * locking in your code.

/**

 * ida_destroy() - Free all IDs.

 * @ida: IDA handle.

 *

 * Calling this function frees all IDs and releases all resources used

 * by an IDA.  When this call returns, the IDA is empty and can be reused

 * or freed.  If the IDA is already empty, there is no need to call this

 * function.

 *

 * Context: Any context. It is safe to call this function without

 * locking in your code.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Test cases for printf facility.

	/*

	 * Every fmt+args is subjected to four tests: Three where we

	 * tell vsnprintf varying buffer sizes (plenty, not quite

	 * enough and 0), and then we also test that kvasprintf would

	 * be able to print it as expected.

 Since elen < BUF_SIZE, we have 1 <= rand <= BUF_SIZE. */

 Work around annoying "warning: zero-length gnu_printf format string". */

	/*

	 * POSIX/C99: »The result of converting zero with an explicit

	 * precision of zero shall be no characters.« Hence the output

	 * from the below test should really be "00|0||| ". However,

	 * the kernel's printf also produces a single 0 in that

	 * case. This test case simply documents the current

	 * behaviour.

		/*

		 * Passing a 'char' to a %02x specifier doesn't do

		 * what was presumably the intention when char is

		 * signed and the value is negative. One must either &

		 * with 0xff or cast to u8.

	/*

	 * POSIX and C99 say that a negative precision (which is only

	 * possible to pass via a * argument) should be treated as if

	 * the precision wasn't present, and that if the precision is

	 * omitted (as in %.s), the precision should be taken to be

	 * 0. However, the kernel's printf behave exactly opposite,

	 * treating a negative precision as 0 and treating an omitted

	 * precision specifier as if no precision was given.

	 *

	 * These test cases document the current behaviour; should

	 * anyone ever feel the need to follow the standards more

	 * closely, this can be revisited.

 leave some space so we don't oops */

 hex 32 zero bits */

 hex 32 one bits */

 Format is implicitly tested for 32 bit machines by plain_hash() */

 BITS_PER_LONG == 64 */

/*

 * We can't use test() to test %p because we don't know what output to expect

 * after an address is hashed.

	/*

	 * No need to increase failed test counter since this is assumed

	 * to be called after plain().

/*

 * NULL pointers aren't hashed.

/*

 * Error pointers aren't hashed.

 We can't test this without access to kptr_restrict. */

 1543210543 */

 2019-01-04T15:32:23 */

 Any flags not translated by the table should remain numeric */

 Guardian */ }

 Check that %pe with a non-ERR_PTR gets treated as ordinary %p. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * INET		An implementation of the TCP/IP protocol suite for the LINUX

 *		operating system.  INET is implemented using the  BSD Socket

 *		interface as the means of communication with the user level.

 *

 *		IP/TCP/UDP checksumming routines

 *

 * Authors:	Jorge Cwik, <jorge@laser.satlink.net>

 *		Arnt Gulbrandsen, <agulbra@nvg.unit.no>

 *		Tom May, <ftom@netcom.com>

 *		Andreas Schwab, <schwab@issan.informatik.uni-dortmund.de>

 *		Lots of code moved from tcp.c and ip.c; see those files

 *		for more names.

 *

 * 03/02/96	Jes Sorensen, Andreas Schwab, Roman Hodek:

 *		Fixed some nasty bugs, causing some horrible crashes.

 *		A: At some points, the sum (%0) was used as

 *		length-counter instead of the length counter

 *		(%1). Thanks to Roman Hodek for pointing this out.

 *		B: GCC seems to mess up if one uses too many

 *		data-registers to hold input values and one tries to

 *		specify d0 and d1 as scratch registers. Letting gcc

 *		choose these registers itself solves the problem.

/* Revised by Kenneth Albanowski for m68knommu. Basic problem: unaligned access

 add up 16-bit and 16-bit for 16+c bit */

 add up carry.. */

/*

 *	This is a version of ip_compute_csum() optimized for IP headers,

 *	which always checksum on 4 octet boundaries.

/*

 * computes the checksum of a memory block at buff, length len,

 * and adds in "sum" (32-bit)

 *

 * returns a 32-bit number suitable for feeding into itself

 * or csum_tcpudp_magic

 *

 * this function must be called with even lengths, except

 * for the last fragment, which may be odd

 *

 * it's best to have buff aligned on a 32-bit boundary

 add in old sum, and carry.. */

/*

 * this routine is used for miscellaneous IP-like checksums, mainly

 * in icmp.c

 add up 32-bit and 32-bit for 32+c bit */

 add up carry.. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Do a strnlen, return length of string *with* final '\0'.

 * 'count' is the user-supplied count, while 'max' is the

 * address space maximum.

 *

 * Return 0 for exceptions (which includes hitting the address

 * space maximum), or 'count+1' if hitting the user-supplied

 * maximum count.

 *

 * NOTE! We can sometimes overshoot the user-supplied maximum

 * if it fits in a aligned 'long'. The caller needs to check

 * the return value against "> max".

	/*

	 * Do everything aligned. But that means that we

	 * need to also expand the maximum..

 We already handled 'unsigned long' bytes. Did we do it all ? */

	/*

	 * Uhhuh. We hit 'max'. But was that the user-specified maximum

	 * too? If so, return the marker for "too long".

	/*

	 * Nope: we hit the address space limit, and we still had more

	 * characters the caller would have wanted. That's 0.

/**

 * strnlen_user: - Get the size of a user string INCLUDING final NUL.

 * @str: The string to measure.

 * @count: Maximum count (including NUL character)

 *

 * Context: User context only. This function may sleep if pagefaults are

 *          enabled.

 *

 * Get the size of a NUL-terminated string in user space.

 *

 * Returns the size of the string INCLUDING the terminating NUL.

 * If the string is too long, returns a number larger than @count. User

 * has to check the return value against "> count".

 * On exception (or invalid count), returns 0.

 *

 * NOTE! You should basically never use this function. There is

 * almost never any valid case for using the length of a user space

 * string, since the string can be changed at any time by other

 * threads. Use "strncpy_from_user()" instead to get a stable copy

 * of the string.

		/*

		 * Truncate 'max' to the user-specified limit, so that

		 * we only have one limit we need to check in the loop

 SPDX-License-Identifier: GPL-2.0

/*

 * Functions related to interrupt-poll handling in the block layer. This

 * is similar to NAPI for network devices.

/**

 * irq_poll_sched - Schedule a run of the iopoll handler

 * @iop:      The parent iopoll structure

 *

 * Description:

 *     Add this irq_poll structure to the pending poll list and trigger the

 *     raise of the blk iopoll softirq.

/**

 * __irq_poll_complete - Mark this @iop as un-polled again

 * @iop:      The parent iopoll structure

 *

 * Description:

 *     See irq_poll_complete(). This function must be called with interrupts

 *     disabled.

/**

 * irq_poll_complete - Mark this @iop as un-polled again

 * @iop:      The parent iopoll structure

 *

 * Description:

 *     If a driver consumes less than the assigned budget in its run of the

 *     iopoll handler, it'll end the polled mode by calling this function. The

 *     iopoll handler will not be invoked again before irq_poll_sched()

 *     is called.

		/*

		 * If softirq window is exhausted then punt.

		/* Even though interrupts have been re-enabled, this

		 * access is safe because interrupts can only add new

		 * entries to the tail of this list, and only ->poll()

		 * calls can remove this head entry from the list.

		/*

		 * Drivers must not modify the iopoll state, if they

		 * consume their assigned weight (or more, some drivers can't

		 * easily just stop processing, they have to complete an

		 * entire mask of commands).In such cases this code

		 * still "owns" the iopoll instance and therefore can

		 * move the instance around on the list at-will.

/**

 * irq_poll_disable - Disable iopoll on this @iop

 * @iop:      The parent iopoll structure

 *

 * Description:

 *     Disable io polling and wait for any pending callbacks to have completed.

/**

 * irq_poll_enable - Enable iopoll on this @iop

 * @iop:      The parent iopoll structure

 *

 * Description:

 *     Enable iopoll on this @iop. Note that the handler run will not be

 *     scheduled, it will only mark it as active.

/**

 * irq_poll_init - Initialize this @iop

 * @iop:      The parent iopoll structure

 * @weight:   The default weight (or command completion budget)

 * @poll_fn:  The handler to invoke

 *

 * Description:

 *     Initialize and enable this irq_poll structure.

	/*

	 * If a CPU goes away, splice its entries to the current CPU

	 * and trigger a run of the softirq

 SPDX-License-Identifier: GPL-2.0

/*

 * Do a strncpy, return length of string without final '\0'.

 * 'count' is the user-supplied count (return 'count' if we

 * hit it), 'max' is the address space maximum (and we return

 * -EFAULT if we hit it).

 Fall back to byte-at-a-time if we get a page fault */

		/*

		 * Note that we mask out the bytes following the NUL. This is

		 * important to do because string oblivious code may read past

		 * the NUL. For those routines, we don't want to give them

		 * potentially random bytes after the NUL in `src`.

		 *

		 * One example of such code is BPF map keys. BPF treats map keys

		 * as an opaque set of bytes. Without the post-NUL mask, any BPF

		 * maps keyed by strings returned from strncpy_from_user() may

		 * have multiple entries for semantically identical strings.

	/*

	 * Uhhuh. We hit 'max'. But was that the user-specified maximum

	 * too? If so, that's ok - we got as much as the user asked for.

	/*

	 * Nope: we hit the address space limit, and we still had more

	 * characters the caller would have wanted. That's an EFAULT.

/**

 * strncpy_from_user: - Copy a NUL terminated string from userspace.

 * @dst:   Destination address, in kernel space.  This buffer must be at

 *         least @count bytes long.

 * @src:   Source address, in user space.

 * @count: Maximum number of bytes to copy, including the trailing NUL.

 *

 * Copies a NUL-terminated string from userspace to kernel space.

 *

 * On success, returns the length of the string (not including the trailing

 * NUL).

 *

 * If access to userspace fails, returns -EFAULT (some data may have been

 * copied).

 *

 * If @count is smaller than the length of the string, copies @count bytes

 * and returns @count.

		/*

		 * Truncate 'max' to the user-specified limit, so that

		 * we only have one limit we need to check in the loop

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * lib/plist.c

 *

 * Descending-priority-sorted double-linked list

 *

 * (C) 2002-2003 Intel Corp

 * Inaky Perez-Gonzalez <inaky.perez-gonzalez@intel.com>.

 *

 * 2001-2005 (c) MontaVista Software, Inc.

 * Daniel Walker <dwalker@mvista.com>

 *

 * (C) 2005 Thomas Gleixner <tglx@linutronix.de>

 *

 * Simplifications of the original code by

 * Oleg Nesterov <oleg@tv-sign.ru>

 *

 * Based on simple lists (include/linux/list.h).

 *

 * This file contains the add / del functions which are considered to

 * be too large to inline. See include/linux/plist.h for further

 * information.

/**

 * plist_add - add @node to @head

 *

 * @node:	&struct plist_node pointer

 * @head:	&struct plist_head pointer

/**

 * plist_del - Remove a @node from plist.

 *

 * @node:	&struct plist_node pointer - entry to be removed

 * @head:	&struct plist_head pointer - list head

 add the next plist_node into prio_list */

/**

 * plist_requeue - Requeue @node at end of same-prio entries.

 *

 * This is essentially an optimized plist_del() followed by

 * plist_add().  It moves an entry already in the plist to

 * after any other same-priority entries.

 *

 * @node:	&struct plist_node pointer - entry to be moved

 * @head:	&struct plist_head pointer - list head

 SPDX-License-Identifier: GPL-2.0

/*

 * Implement the default iomap interfaces

 *

 * (C) Copyright 2004 Linus Torvalds

/**

 * pci_iomap_range - create a virtual mapping cookie for a PCI BAR

 * @dev: PCI device that owns the BAR

 * @bar: BAR number

 * @offset: map memory at the given offset in BAR

 * @maxlen: max length of the memory to map

 *

 * Using this function you will get a __iomem address to your device BAR.

 * You can access it using ioread*() and iowrite*(). These functions hide

 * the details if this is a MMIO or PIO address space and will just do what

 * you expect from them in the correct way.

 *

 * @maxlen specifies the maximum length to map. If you want to get access to

 * the complete BAR from offset to the end, pass %0 here.

 What? */

/**

 * pci_iomap_wc_range - create a virtual WC mapping cookie for a PCI BAR

 * @dev: PCI device that owns the BAR

 * @bar: BAR number

 * @offset: map memory at the given offset in BAR

 * @maxlen: max length of the memory to map

 *

 * Using this function you will get a __iomem address to your device BAR.

 * You can access it using ioread*() and iowrite*(). These functions hide

 * the details if this is a MMIO or PIO address space and will just do what

 * you expect from them in the correct way. When possible write combining

 * is used.

 *

 * @maxlen specifies the maximum length to map. If you want to get access to

 * the complete BAR from offset to the end, pass %0 here.

 What? */

/**

 * pci_iomap - create a virtual mapping cookie for a PCI BAR

 * @dev: PCI device that owns the BAR

 * @bar: BAR number

 * @maxlen: length of the memory to map

 *

 * Using this function you will get a __iomem address to your device BAR.

 * You can access it using ioread*() and iowrite*(). These functions hide

 * the details if this is a MMIO or PIO address space and will just do what

 * you expect from them in the correct way.

 *

 * @maxlen specifies the maximum length to map. If you want to get access to

 * the complete BAR without checking for its length first, pass %0 here.

/**

 * pci_iomap_wc - create a virtual WC mapping cookie for a PCI BAR

 * @dev: PCI device that owns the BAR

 * @bar: BAR number

 * @maxlen: length of the memory to map

 *

 * Using this function you will get a __iomem address to your device BAR.

 * You can access it using ioread*() and iowrite*(). These functions hide

 * the details if this is a MMIO or PIO address space and will just do what

 * you expect from them in the correct way. When possible write combining

 * is used.

 *

 * @maxlen specifies the maximum length to map. If you want to get access to

 * the complete BAR without checking for its length first, pass %0 here.

/*

 * pci_iounmap() somewhat illogically comes from lib/iomap.c for the

 * CONFIG_GENERIC_IOMAP case, because that's the code that knows about

 * the different IOMAP ranges.

 *

 * But if the architecture does not use the generic iomap code, and if

 * it has _not_ defined it's own private pci_iounmap function, we define

 * it here.

 *

 * NOTE! This default implementation assumes that if the architecture

 * support ioport mapping (HAS_IOPORT_MAP), the ioport mapping will

 * be fixed to the range [ PCI_IOBASE, PCI_IOBASE+IO_SPACE_LIMIT [,

 * and does not need unmapping with 'ioport_unmap()'.

 *

 * If you have different rules for your architecture, you need to

 * implement your own pci_iounmap() that knows the rules for where

 * and how IO vs MEM get mapped.

 *

 * This code is odd, and the ARCH_HAS/ARCH_WANTS #define logic comes

 * from legacy <asm-generic/io.h> header file behavior. In particular,

 * it would seem to make sense to do the iounmap(p) for the non-IO-space

 * case here regardless, but that's not what the old header file code

 * did. Probably incorrectly, but this is meant to be bug-for-bug

 * compatible.

 ARCH_WANTS_GENERIC_PCI_IOUNMAP */

 CONFIG_PCI */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Floating proportions with flexible aging period

 *

 *   Copyright (C) 2011, SUSE, Jan Kara <jack@suse.cz>

 *

 * The goal of this code is: Given different types of event, measure proportion

 * of each type of event over time. The proportions are measured with

 * exponentially decaying history to give smooth transitions. A formula

 * expressing proportion of event of type 'j' is:

 *

 *   p_{j} = (\Sum_{i>=0} x_{i,j}/2^{i+1})/(\Sum_{i>=0} x_i/2^{i+1})

 *

 * Where x_{i,j} is j's number of events in i-th last time period and x_i is

 * total number of events in i-th last time period.

 *

 * Note that p_{j}'s are normalised, i.e.

 *

 *   \Sum_{j} p_{j} = 1,

 *

 * This formula can be straightforwardly computed by maintaining denominator

 * (let's call it 'd') and for each event type its numerator (let's call it

 * 'n_j'). When an event of type 'j' happens, we simply need to do:

 *   n_j++; d++;

 *

 * When a new period is declared, we could do:

 *   d /= 2

 *   for each j

 *     n_j /= 2

 *

 * To avoid iteration over all event types, we instead shift numerator of event

 * j lazily when someone asks for a proportion of event j or when event j

 * occurs. This can bit trivially implemented by remembering last period in

 * which something happened with proportion of type j.

 Use 1 to avoid dealing with periods with 0 events... */

/*

 * Declare @periods new periods. It is upto the caller to make sure period

 * transitions cannot happen in parallel.

 *

 * The function returns true if the proportions are still defined and false

 * if aging zeroed out all events. This can be used to detect whether declaring

 * further periods has any effect.

	/*

	 * Don't do anything if there are no events.

 Use addition to avoid losing events happening between sum and set */

/*

 * ---- SINGLE ----

 Fast path - period didn't change */

 Someone updated pl->period while we were spinning? */

 Aging zeroed our fraction? */

 Event of type pl happened */

 Return fraction of events of type pl */

	/*

	 * Make fraction <= 1 and denominator > 0 even in presence of percpu

	 * counter errors

/*

 * ---- PERCPU ----

 Fast path - period didn't change */

 Someone updated pl->period while we were spinning? */

 Aging zeroed our fraction? */

 Event of type pl happened */

	/*

	 * Make fraction <= 1 and denominator > 0 even in presence of percpu

	 * counter errors

/*

 * Like __fprop_add_percpu() except that event is counted only if the given

 * type has fraction smaller than @max_frac/FPROP_FRAC_BASE

 Adding 'nr' to fraction exceeds max_frac/FPROP_FRAC_BASE? */

 Maximum fraction already exceeded? */

 Add just enough for the fraction to saturate */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2006 PathScale, Inc.  All Rights Reserved.

/**

 * __iowrite32_copy - copy data to MMIO space, in 32-bit units

 * @to: destination, in MMIO space (must be 32-bit aligned)

 * @from: source (must be 32-bit aligned)

 * @count: number of 32-bit quantities to copy

 *

 * Copy data from kernel space to MMIO space, in units of 32 bits at a

 * time.  Order of access is not guaranteed, nor is a memory barrier

 * performed afterwards.

/**

 * __ioread32_copy - copy data from MMIO space, in 32-bit units

 * @to: destination (must be 32-bit aligned)

 * @from: source, in MMIO space (must be 32-bit aligned)

 * @count: number of 32-bit quantities to copy

 *

 * Copy data from MMIO space to kernel space, in units of 32 bits at a

 * time.  Order of access is not guaranteed, nor is a memory barrier

 * performed afterwards.

/**

 * __iowrite64_copy - copy data to MMIO space, in 64-bit or 32-bit units

 * @to: destination, in MMIO space (must be 64-bit aligned)

 * @from: source (must be 64-bit aligned)

 * @count: number of 64-bit quantities to copy

 *

 * Copy data from kernel space to MMIO space, in units of 32 or 64 bits at a

 * time.  Order of access is not guaranteed, nor is a memory barrier

 * performed afterwards.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 If we still don't have umul_ppmm, define it using plain C.  */

 this can't give carry */\

 but this indeed can */			\

 did we get it? */			\

 yes, add it in the proper pos */	\

 SPDX-License-Identifier: GPL-2.0

/*

 * Important notes about in-place decompression

 *

 * At least on x86, the kernel is decompressed in place: the compressed data

 * is placed to the end of the output buffer, and the decompressor overwrites

 * most of the compressed data. There must be enough safety margin to

 * guarantee that the write position is always behind the read position.

 *

 * The safety margin for ZSTD with a 128 KB block size is calculated below.

 * Note that the margin with ZSTD is bigger than with GZIP or XZ!

 *

 * The worst case for in-place decompression is that the beginning of

 * the file is compressed extremely well, and the rest of the file is

 * uncompressible. Thus, we must look for worst-case expansion when the

 * compressor is encoding uncompressible data.

 *

 * The structure of the .zst file in case of a compressed kernel is as follows.

 * Maximum sizes (as bytes) of the fields are in parenthesis.

 *

 *    Frame Header: (18)

 *    Blocks: (N)

 *    Checksum: (4)

 *

 * The frame header and checksum overhead is at most 22 bytes.

 *

 * ZSTD stores the data in blocks. Each block has a header whose size is

 * a 3 bytes. After the block header, there is up to 128 KB of payload.

 * The maximum uncompressed size of the payload is 128 KB. The minimum

 * uncompressed size of the payload is never less than the payload size

 * (excluding the block header).

 *

 * The assumption, that the uncompressed size of the payload is never

 * smaller than the payload itself, is valid only when talking about

 * the payload as a whole. It is possible that the payload has parts where

 * the decompressor consumes more input than it produces output. Calculating

 * the worst case for this would be tricky. Instead of trying to do that,

 * let's simply make sure that the decompressor never overwrites any bytes

 * of the payload which it is currently reading.

 *

 * Now we have enough information to calculate the safety margin. We need

 *   - 22 bytes for the .zst file format headers;

 *   - 3 bytes per every 128 KiB of uncompressed size (one block header per

 *     block); and

 *   - 128 KiB (biggest possible zstd block size) to make sure that the

 *     decompressor never overwrites anything from the block it is currently

 *     reading.

 *

 * We get the following formula:

 *

 *    safety_margin = 22 + uncompressed_size * 3 / 131072 + 131072

 *                 <= 22 + (uncompressed_size >> 15) + 131072

/*

 * Preboot environments #include "path/to/decompress_unzstd.c".

 * All of the source files we depend on must be #included.

 * zstd's only source dependency is xxhash, which has no source

 * dependencies.

 *

 * When UNZSTD_PREBOOT is defined we declare __decompress(), which is

 * used for kernel decompression, instead of unzstd().

 *

 * Define __DISABLE_EXPORTS in preboot environments to prevent symbols

 * from xxhash and zstd from being exported by the EXPORT_SYMBOL macro.

 128MB is the maximum window size supported by zstd. */

/*

 * Size of the input and output buffers in multi-call mode.

 * Pick a larger size because it isn't used during kernel decompression,

 * since that is single pass, and we have to allocate a large buffer for

 * zstd's window anyway. The larger size speeds up initramfs decompression.

	/*

	 * zstd_get_error_name() cannot be used because error takes a char *

	 * not a const char *

/*

 * Handle the case where we have the entire input and output in one segment.

 * We can allocate less memory (no circular buffer for the sliding window),

 * and avoid some memcpy() calls.

	/*

	 * Find out how large the frame actually is, there may be junk at

	 * the end of the frame that zstd_decompress_dctx() can't handle.

	/*

	 * ZSTD decompression code won't be happy if the buffer size is so big

	 * that its end address overflows. When the size is not provided, make

	 * it as big as possible without having the end address overflow.

		/*

		 * We can decompress faster and with less memory when we have a

		 * single chunk.

	/*

	 * If in_buf is not provided, we must be using fill(), so allocate

	 * a large enough buffer. If it is provided, it must be at least

	 * ZSTD_IOBUF_SIZE large.

 Read the first chunk, since we need to decode the frame header. */

 Set the first non-empty input buffer. */

 Allocate the output buffer if we are using flush(). */

 Set the output buffer. */

	/*

	 * We need to know the window size to allocate the zstd_dstream.

	 * Since we are streaming, we need to allocate a buffer for the sliding

	 * window. The window size varies from 1 KB to ZSTD_WINDOWSIZE_MAX

	 * (8 MB), so it is important to use the actual value so as not to

	 * waste memory when it is smaller.

	/*

	 * Allocate the zstd_dstream now that we know how much memory is

	 * required.

	/*

	 * Decompression loop:

	 * Read more data if necessary (error if no more data can be read).

	 * Call the decompression function, which returns 0 when finished.

	 * Flush any data produced if using flush().

		/*

		 * If we need to reload data, either we have fill() and can

		 * try to get more data, or we don't and the input is truncated.

 Returns zero when the frame is complete. */

 Flush all of the data produced if using flush(). */

 SPDX-License-Identifier: GPL-2.0

/*

 * A fast, small, non-recursive O(n log n) sort for the Linux kernel

 *

 * This performs n*log2(n) + 0.37*n + o(n) comparisons on average,

 * and 1.5*n*log2(n) + O(n) in the (very contrived) worst case.

 *

 * Glibc qsort() manages n*log2(n) - 1.26*n for random inputs (1.63*n

 * better) at the expense of stack usage and much larger code to avoid

 * quicksort's O(n^2) worst case.

/**

 * is_aligned - is this pointer & size okay for word-wide copying?

 * @base: pointer to data

 * @size: size of each element

 * @align: required alignment (typically 4 or 8)

 *

 * Returns true if elements can be copied using word loads and stores.

 * The size must be a multiple of the alignment, and the base address must

 * be if we do not have CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS.

 *

 * For some reason, gcc doesn't know to optimize "if (a & mask || b & mask)"

 * to "if ((a | b) & mask)", so we do that by hand.

/**

 * swap_words_32 - swap two elements in 32-bit chunks

 * @a: pointer to the first element to swap

 * @b: pointer to the second element to swap

 * @n: element size (must be a multiple of 4)

 *

 * Exchange the two objects in memory.  This exploits base+index addressing,

 * which basically all CPUs have, to minimize loop overhead computations.

 *

 * For some reason, on x86 gcc 7.3.0 adds a redundant test of n at the

 * bottom of the loop, even though the zero flag is still valid from the

 * subtract (since the intervening mov instructions don't alter the flags).

 * Gcc 8.1.0 doesn't have that problem.

/**

 * swap_words_64 - swap two elements in 64-bit chunks

 * @a: pointer to the first element to swap

 * @b: pointer to the second element to swap

 * @n: element size (must be a multiple of 8)

 *

 * Exchange the two objects in memory.  This exploits base+index

 * addressing, which basically all CPUs have, to minimize loop overhead

 * computations.

 *

 * We'd like to use 64-bit loads if possible.  If they're not, emulating

 * one requires base+index+4 addressing which x86 has but most other

 * processors do not.  If CONFIG_64BIT, we definitely have 64-bit loads,

 * but it's possible to have 64-bit loads without 64-bit pointers (e.g.

 * x32 ABI).  Are there any cases the kernel needs to worry about?

 Use two 32-bit transfers to avoid base+index+4 addressing */

/**

 * swap_bytes - swap two elements a byte at a time

 * @a: pointer to the first element to swap

 * @b: pointer to the second element to swap

 * @n: element size

 *

 * This is the fallback if alignment doesn't allow using larger chunks.

/*

 * The values are arbitrary as long as they can't be confused with

 * a pointer, but small integers make for the smallest compare

 * instructions.

/*

 * The function pointer is last to make tail calls most efficient if the

 * compiler decides not to inline this function.

/**

 * parent - given the offset of the child, find the offset of the parent.

 * @i: the offset of the heap element whose parent is sought.  Non-zero.

 * @lsbit: a precomputed 1-bit mask, equal to "size & -size"

 * @size: size of each element

 *

 * In terms of array indexes, the parent of element j = @i/@size is simply

 * (j-1)/2.  But when working in byte offsets, we can't use implicit

 * truncation of integer divides.

 *

 * Fortunately, we only need one bit of the quotient, not the full divide.

 * @size has a least significant bit.  That bit will be clear if @i is

 * an even multiple of @size, and set if it's an odd multiple.

 *

 * Logically, we're doing "if (i & lsbit) i -= size;", but since the

 * branch is unpredictable, it's done with a bit of clever branch-free

 * code instead.

/**

 * sort_r - sort an array of elements

 * @base: pointer to data to sort

 * @num: number of elements

 * @size: size of each element

 * @cmp_func: pointer to comparison function

 * @swap_func: pointer to swap function or NULL

 * @priv: third argument passed to comparison function

 *

 * This function does a heapsort on the given array.  You may provide

 * a swap_func function if you need to do something more than a memory

 * copy (e.g. fix up pointers or auxiliary data), but the built-in swap

 * avoids a slow retpoline and so is significantly faster.

 *

 * Sorting time is O(n log n) both on average and worst-case. While

 * quicksort is slightly faster on average, it suffers from exploitable

 * O(n*n) worst-case behavior and extra memory requirements that make

 * it less suitable for kernel use.

 pre-scale counters for performance */

 Used to find parent */

 num < 2 || size == 0 */

	/*

	 * Loop invariants:

	 * 1. elements [a,n) satisfy the heap property (compare greater than

	 *    all of their children),

	 * 2. elements [n,num*size) are sorted, and

	 * 3. a <= b <= c <= d <= n (whenever they are valid).

 Building heap: sift down --a */

 Sorting: Extract root to --n */

 Sort complete */

		/*

		 * Sift element at "a" down into heap.  This is the

		 * "bottom-up" variant, which significantly reduces

		 * calls to cmp_func(): we find the sift-down path all

		 * the way to the leaves (one compare per level), then

		 * backtrack to find where to insert the target element.

		 *

		 * Because elements tend to sift down close to the leaves,

		 * this uses fewer compares than doing two per level

		 * on the way down.  (A bit more than half as many on

		 * average, 3/4 worst-case.)

 Special case last leaf with no sibling */

 Now backtrack from "b" to the correct location for "a" */

 Where "a" belongs */

 Shift it into place */

 SPDX-License-Identifier: GPL-2.0 OR MIT

/*

 * Test cases for arithmetic overflow checks.

 10^9 */, 10000000000ULL 
 Args are: value, shift, type, expected result, overflow expected */

 Sane shifts. */

 Sane shift: start and end with 0, without a too-wide shift. */

 Sane shift: start and end with 0, without reaching signed bit. */

 Overflow: shifted the bit off the end. */

 Overflow: shifted into the signed bit. */

 Overflow: high bit falls off unsigned types. */

 10010110 */

 1000100010010110 */

 10000100000010001000100010010110 */

 1000001000010000010000000100000010000100000010001000100010010110 */

 Overflow: bit shifted into signed bit on signed types. */

 01001011 */

 0100010001001011 */

 01000010000001000100010001001011 */

 0100000100001000001000000010000001000010000001000100010001001011 */

 Overflow: bit shifted past signed bit on signed types. */

 01001011 */

 0100010001001011 */

 01000010000001000100010001001011 */

 0100000100001000001000000010000001000010000001000100010001001011 */

 Overflow: values larger than destination type. */

 Nonsense: negative initial value. */

 Nonsense: negative shift values. */

 Overflow: shifted at or beyond entire type's bit width. */

	/*

	 * Corner case: for unsigned types, we fail when we've shifted

	 * through the entire width of bits. For signed types, we might

	 * want to match this behavior, but that would mean noticing if

	 * we shift through all but the signed bit, and this is not

	 * currently detected (but we'll notice an overflow into the

	 * signed bit). So, for now, we will test this condition but

	 * mark it as not expected to overflow.

/*

 * Deal with the various forms of allocator arguments. See comments above

 * the DEFINE_TEST_ALLOC() instances for mapping of the "bits".

 Wrap around to 16K */

 Tiny allocation test. */					\

 Wrapped allocation test. */					\

 Saturated allocation test. */				\

/*

 * Allocator uses a trailing node argument --------+  (e.g. kmalloc_node())

 * Allocator uses the gfp_t argument -----------+  |  (e.g. kmalloc())

 * Allocator uses a special leading argument +  |  |  (e.g. devm_kmalloc())

 *                                           |  |  |

 Create dummy device for devm_kmalloc()-family tests. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Implement the default iomap interfaces

 *

 * (C) Copyright 2004 Linus Torvalds

/*

 * Read/write from/to an (offsettable) iomem cookie. It might be a PIO

 * access or a MMIO access, these functions don't care. The info is

 * encoded in the hardware mapping set up by the mapping functions

 * (or the cookie itself, depending on implementation and hw).

 *

 * The generic routines don't assume any hardware mappings, and just

 * encode the PIO/MMIO as part of the cookie. They coldly assume that

 * the MMIO IO mappings are not in the low address range.

 *

 * Architectures for which this is not true can't use this generic

 * implementation and should do their own copy.

/*

 * We encode the physical PIO addresses (0-0xffff) into the

 * pointer by offsetting them with a constant (0x10000) and

 * assuming that all the low addresses are always PIO. That means

 * we can do some sanity checks on the low bits, and don't

 * need to just take things for granted.

/*

 * Ugly macros are a way of life.

 readq */

 readq */

/*

 * These are the "repeat MMIO read/write" functions.

 * Note the "__raw" accesses, since we don't want to

 * convert to CPU byte order. We write in "IO byte

 * order" (we also don't have IO barriers).

 Create a virtual mapping cookie for an IO port range */

 Nothing to do */

 CONFIG_HAS_IOPORT_MAP */

/* Hide the details if this is a MMIO or PIO address space and just do what

 nothing */, iounmap(addr));

 CONFIG_PCI */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 Robert Jarzmik <robert.jarzmik@free.fr>

 *

 * Scatterlist splitting helpers.

/**

 * sg_split - split a scatterlist into several scatterlists

 * @in: the input sg list

 * @in_mapped_nents: the result of a dma_map_sg(in, ...), or 0 if not mapped.

 * @skip: the number of bytes to skip in the input sg list

 * @nb_splits: the number of desired sg outputs

 * @split_sizes: the respective size of each output sg list in bytes

 * @out: an array where to store the allocated output sg lists

 * @out_mapped_nents: the resulting sg lists mapped number of sg entries. Might

 *                    be NULL if sglist not already mapped (in_mapped_nents = 0)

 * @gfp_mask: the allocation flag

 *

 * This function splits the input sg list into nb_splits sg lists, which are

 * allocated and stored into out.

 * The @in is split into :

 *  - @out[0], which covers bytes [@skip .. @skip + @split_sizes[0] - 1] of @in

 *  - @out[1], which covers bytes [@skip + split_sizes[0] ..

 *                                 @skip + @split_sizes[0] + @split_sizes[1] -1]

 * etc ...

 * It will be the caller's duty to kfree() out array members.

 *

 * Returns 0 upon success, or error code

	/*

	 * The order of these 3 calls is important and should be kept.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Kernel module for testing 'strscpy' family of functions.

/*

 * tc() - Run a specific test case.

 * @src: Source string, argument to strscpy_pad()

 * @count: Size of destination buffer, argument to strscpy_pad()

 * @expected: Expected return value from call to strscpy_pad()

 * @terminator: 1 if there should be a terminating null byte 0 otherwise.

 * @chars: Number of characters from the src string expected to be

 *         written to the dst buffer.

 * @pad: Number of pad characters expected (in the tail of dst buffer).

 *       (@pad does not include the null terminator byte.)

 *

 * Calls strscpy_pad() and verifies the return value and state of the

 * destination buffer after the call returns.

 Future proofing test suite, validate args */

 Space for null and to verify overflow */

 Space for the null */

 Check from the end back */

	/*

	 * tc() uses a destination buffer of size 6 and needs at

	 * least 2 characters spare (one for null and one to check for

	 * overflow).  This means we should only call tc() with

	 * strings up to a maximum of 4 characters long and 'count'

	 * should not exceed 4.  To test with longer strings increase

	 * the buffer size in tc().

 tc(src, count, expected, chars, terminator, pad) */

 SPDX-License-Identifier: GPL-2.0

/*

 * Extra Boot Config

 * Masami Hiramatsu <mhiramat@kernel.org>

 !__KERNEL__ */

/*

 * NOTE: This is only for tools/bootconfig, because tools/bootconfig will

 * run the parser sanity test.

 * This does NOT mean lib/bootconfig.c is available in the user space.

 * However, if you change this file, please make sure the tools/bootconfig

 * has no issue on building and running.

/*

 * Extra Boot Config (XBC) is given as tree-structured ascii text of

 * key-value pairs on memory.

 * xbc_parse() parses the text to build a simple tree. Each tree node is

 * simply a key word or a value. A key node may have a next key node or/and

 * a child node (both key and value). A value node may have a next value

 * node (for array).

 !__KERNEL__ */

/**

 * xbc_get_info() - Get the information of loaded boot config

 * @node_size: A pointer to store the number of nodes.

 * @data_size: A pointer to store the size of bootconfig data.

 *

 * Get the number of used nodes in @node_size if it is not NULL,

 * and the size of bootconfig data in @data_size if it is not NULL.

 * Return 0 if the boot config is initialized, or return -ENODEV.

/**

 * xbc_root_node() - Get the root node of extended boot config

 *

 * Return the address of root node of extended boot config. If the

 * extended boot config is not initiized, return NULL.

/**

 * xbc_node_index() - Get the index of XBC node

 * @node: A target node of getting index.

 *

 * Return the index number of @node in XBC node list.

/**

 * xbc_node_get_parent() - Get the parent XBC node

 * @node: An XBC node.

 *

 * Return the parent node of @node. If the node is top node of the tree,

 * return NULL.

/**

 * xbc_node_get_child() - Get the child XBC node

 * @node: An XBC node.

 *

 * Return the first child node of @node. If the node has no child, return

 * NULL.

/**

 * xbc_node_get_next() - Get the next sibling XBC node

 * @node: An XBC node.

 *

 * Return the NEXT sibling node of @node. If the node has no next sibling,

 * return NULL. Note that even if this returns NULL, it doesn't mean @node

 * has no siblings. (You also has to check whether the parent's child node

 * is @node or not.)

/**

 * xbc_node_get_data() - Get the data of XBC node

 * @node: An XBC node.

 *

 * Return the data (which is always a null terminated string) of @node.

 * If the node has invalid data, warn and return NULL.

/**

 * xbc_node_find_subkey() - Find a subkey node which matches given key

 * @parent: An XBC node.

 * @key: A key string.

 *

 * Search a key node under @parent which matches @key. The @key can contain

 * several words jointed with '.'. If @parent is NULL, this searches the

 * node from whole tree. Return NULL if no node is matched.

/**

 * xbc_node_find_value() - Find a value node which matches given key

 * @parent: An XBC node.

 * @key: A key string.

 * @vnode: A container pointer of found XBC node.

 *

 * Search a value node under @parent whose (parent) key node matches @key,

 * store it in *@vnode, and returns the value string.

 * The @key can contain several words jointed with '.'. If @parent is NULL,

 * this searches the node from whole tree. Return the value string if a

 * matched key found, return NULL if no node is matched.

 * Note that this returns 0-length string and stores NULL in *@vnode if the

 * key has no value. And also it will return the value of the first entry if

 * the value is an array.

/**

 * xbc_node_compose_key_after() - Compose partial key string of the XBC node

 * @root: Root XBC node

 * @node: Target XBC node.

 * @buf: A buffer to store the key.

 * @size: The size of the @buf.

 *

 * Compose the partial key of the @node into @buf, which is starting right

 * after @root (@root is not included.) If @root is NULL, this returns full

 * key words of @node.

 * Returns the total length of the key stored in @buf. Returns -EINVAL

 * if @node is NULL or @root is not the ancestor of @node or @root is @node,

 * or returns -ERANGE if the key depth is deeper than max depth.

 * This is expected to be used with xbc_find_node() to list up all (child)

 * keys under given key.

/**

 * xbc_node_find_next_leaf() - Find the next leaf node under given node

 * @root: An XBC root node

 * @node: An XBC node which starts from.

 *

 * Search the next leaf node (which means the terminal key node) of @node

 * under @root node (including @root node itself).

 * Return the next node or NULL if next leaf node is not found.

 First try */

 Leaf node may have a subkey */

 @root was a leaf, no child node. */

 User passed a node which is not uder parent */

/**

 * xbc_node_find_next_key_value() - Find the next key-value pair nodes

 * @root: An XBC root node

 * @leaf: A container pointer of XBC node which starts from.

 *

 * Search the next leaf node (which means the terminal key node) of *@leaf

 * under @root node. Returns the value and update *@leaf if next leaf node

 * is found, or NULL if no next leaf node is found.

 * Note that this returns 0-length string if the key has no value, or

 * the value of the first entry if the value is an array.

 tip must be passed */

 No value key */

 XBC parse and tree build */

 Ignore @head in this case */

 Push the last key as open brace */

/*

 * Return delimiter or error, no node added. As same as lib/cmdline.c,

 * you can use " around spaces, but can't escape " for value.

 A comment must be treated as a newline */

 the first level */

 Since the value node is the first child, skip it. */

 keep subkeys */

 op must be '+' */

 The value node should always be the first child */

 Array */

 k is updated in xbc_parse_key() */

 Brace closing */

 Empty tree */

 Key tree limitation check */

 Need to setup xbc_data and xbc_nodes before call this. */

/**

 * xbc_exit() - Clean up all parsed bootconfig

 *

 * This clears all data structures of parsed bootconfig on memory.

 * If you need to reuse xbc_init() with new boot config, you can

 * use this.

/**

 * xbc_init() - Parse given XBC file and build XBC internal tree

 * @data: The boot config text original data

 * @size: The size of @data

 * @emsg: A pointer of const char * to store the error message

 * @epos: A pointer of int to store the error position

 *

 * This parses the boot config text in @data. @size must be smaller

 * than XBC_DATA_MAX.

 * Return the number of stored nodes (>0) if succeeded, or -errno

 * if there is any error.

 * In error cases, @emsg will be updated with an error message and

 * @epos will be updated with the error position which is the byte offset

 * of @buf. If the error is not a parser error, @epos will be -1.

 SPDX-License-Identifier: GPL-2.0

/*

 * Test cases for memcat_p() in lib/memcat_p.c

 Size of each of the NULL-terminated input arrays */

 Expected number of non-NULL elements in the output array */

 lifted from test_sort.c */

 SPDX-License-Identifier: GPL-2.0-or-later

/* 

 * CRC32C

 *@Article{castagnoli-crc,

 * author =       { Guy Castagnoli and Stefan Braeuer and Martin Herrman},

 * title =        {{Optimization of Cyclic Redundancy-Check Codes with 24

 *                 and 32 Parity Bits}},

 * journal =      IEEE Transactions on Communication,

 * year =         {1993},

 * volume =       {41},

 * number =       {6},

 * pages =        {},

 * month =        {June},

 *}

 * Used by the iSCSI driver, possibly others, and derived from

 * the iscsi-crc.c module of the linux-iscsi driver at

 * http://linux-iscsi.sourceforge.net.

 *

 * Following the example of lib/crc32, this function is intended to be

 * flexible and useful for all users.  Modules that currently have their

 * own crc32c, but hopefully may be able to use this one are:

 *  net/sctp (please add all your doco to here if you change to

 *            use this one!)

 *  <endoflist>

 *

 * Copyright (c) 2004 Cisco Systems, Inc.

 SPDX-License-Identifier: GPL-2.0

/**

 * lib/minmax.c: windowed min/max tracker

 *

 * Kathleen Nichols' algorithm for tracking the minimum (or maximum)

 * value of a data stream over some fixed time interval.  (E.g.,

 * the minimum RTT over the past five minutes.) It uses constant

 * space and constant time per update yet almost always delivers

 * the same minimum as an implementation that has to keep all the

 * data in the window.

 *

 * The algorithm keeps track of the best, 2nd best & 3rd best min

 * values, maintaining an invariant that the measurement time of

 * the n'th best >= n-1'th best. It also makes sure that the three

 * values are widely separated in the time window since that bounds

 * the worse case error when that data is monotonically increasing

 * over the window.

 *

 * Upon getting a new min, we can forget everything earlier because

 * it has no value - the new min is <= everything else in the window

 * by definition and it's the most recent. So we restart fresh on

 * every new min and overwrites 2nd & 3rd choices. The same property

 * holds for 2nd & 3rd best.

 As time advances, update the 1st, 2nd, and 3rd choices. */

		/*

		 * Passed entire window without a new val so make 2nd

		 * choice the new val & 3rd choice the new 2nd choice.

		 * we may have to iterate this since our 2nd choice

		 * may also be outside the window (we checked on entry

		 * that the third choice was in the window).

		/*

		 * We've passed a quarter of the window without a new val

		 * so take a 2nd choice from the 2nd quarter of the window.

		/*

		 * We've passed half the window without finding a new val

		 * so take a 3rd choice from the last half of the window

 Check if new measurement updates the 1st, 2nd or 3rd choice max. */

 found new max? */

 nothing left in window? */

 forget earlier samples */

 Check if new measurement updates the 1st, 2nd or 3rd choice min. */

 found new min? */

 nothing left in window? */

 forget earlier samples */

/*

 * Test cases for lib/hexdump.c module.

 00 - 07 */

 08 - 0f */

 10 - 17 */

 18 - 1f */

 hex dump */

 ASCII part */

	/*

	 * Caller must provide the data length multiple of groupsize. The

	 * calculations below are made with that assumption in mind.

 hex */ + rs / gs  space */ + len 
 hex */ + 1  no trailing space */;

 do nothing */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2001 Momchil Velikov

 * Portions Copyright (C) 2001 Christoph Hellwig

 * Copyright (C) 2005 SGI, Christoph Lameter

 * Copyright (C) 2006 Nick Piggin

 * Copyright (C) 2012 Konstantin Khlebnikov

 * Copyright (C) 2016 Intel, Matthew Wilcox

 * Copyright (C) 2016 Intel, Ross Zwisler

 in_interrupt() */

/*

 * Radix tree node cache.

/*

 * The radix tree is variable-height, so an insert operation not only has

 * to build the branch to its corresponding item, it also has to build the

 * branch to existing items if the size has to be increased (by

 * radix_tree_extend).

 *

 * The worst case is a zero height tree with just a single item at index 0,

 * and then inserting an item at index ULONG_MAX. This requires 2 new branches

 * of RADIX_TREE_MAX_PATH size to be created, with only the root node shared.

 * Hence:

/*

 * The IDR does not have to be as high as the radix tree since it uses

 * signed integers, not unsigned longs.

 CHAR_BIT */ * sizeof(int) - 1)

/*

 * Per-cpu pool of preloaded nodes

/*

 * Returns 1 if any slot in the node has this tag set.

 * Otherwise returns 0.

/**

 * radix_tree_find_next_bit - find the next set bit in a memory region

 *

 * @node: where to begin the search

 * @tag: the tag index

 * @offset: the bitnumber to start searching at

 *

 * Unrollable variant of find_next_bit() for constant size arrays.

 * Tail bits starting from size to roundup(size, BITS_PER_LONG) must be zero.

 * Returns next bit offset, or size if nothing found.

/*

 * The maximum index which can be stored in a radix tree

/*

 * This assumes that the caller has performed appropriate preallocation, and

 * that the caller has pinned this thread of control to the current CPU.

	/*

	 * Preload code isn't irq safe and it doesn't make sense to use

	 * preloading during an interrupt anyway as all the allocations have

	 * to be atomic. So just do normal allocation when in interrupt.

		/*

		 * Even if the caller has preloaded, try to allocate from the

		 * cache first for the new node to get accounted to the memory

		 * cgroup.

		/*

		 * Provided the caller has preloaded here, we will always

		 * succeed in getting a node here (and never reach

		 * kmem_cache_alloc)

		/*

		 * Update the allocation stack trace as this is more useful

		 * for debugging.

	/*

	 * Must only free zeroed nodes into the slab.  We can be left with

	 * non-NULL entries by radix_tree_free_nodes, so clear the entries

	 * and tags here.

/*

 * Load up this CPU's radix_tree_node buffer with sufficient objects to

 * ensure that the addition of a single element in the tree cannot fail.  On

 * success, return zero, with preemption disabled.  On error, return -ENOMEM

 * with preemption not disabled.

 *

 * To make use of this facility, the radix tree must be initialised without

 * __GFP_DIRECT_RECLAIM being passed to INIT_RADIX_TREE().

	/*

	 * Nodes preloaded by one cgroup can be used by another cgroup, so

	 * they should never be accounted to any particular memory cgroup.

/*

 * Load up this CPU's radix_tree_node buffer with sufficient objects to

 * ensure that the addition of a single element in the tree cannot fail.  On

 * success, return zero, with preemption disabled.  On error, return -ENOMEM

 * with preemption not disabled.

 *

 * To make use of this facility, the radix tree must be initialised without

 * __GFP_DIRECT_RECLAIM being passed to INIT_RADIX_TREE().

 Warn on non-sensical use... */

/*

 * The same as above function, except we don't guarantee preloading happens.

 * We do it, if we decide it helps. On success, return zero with preemption

 * disabled. On error, return -ENOMEM with preemption not disabled.

 Preloading doesn't help anything with this gfp mask, skip it */

/*

 *	Extend a radix tree so it can store key @index.

 Figure out what the shift should be.  */

 Propagate the aggregated tag info to the new child */

 Moving a value entry root->xa_head to a node */

		/*

		 * entry was already in the radix tree, so we do not need

		 * rcu_assign_pointer here

/**

 *	radix_tree_shrink    -    shrink radix tree to minimum height

 *	@root:		radix tree root

		/*

		 * The candidate node has more than one child, or its child

		 * is not at the leftmost slot, we cannot shrink.

		/*

		 * For an IDR, we must not shrink entry 0 into the root in

		 * case somebody calls idr_replace() with a pointer that

		 * appears to be an internal entry

		/*

		 * We don't need rcu_assign_pointer(), since we are simply

		 * moving the node from one part of the tree to another: if it

		 * was safe to dereference the old pointer to it

		 * (node->slots[0]), it will be safe to dereference the new

		 * one (root->xa_head) as far as dependent read barriers go.

		/*

		 * We have a dilemma here. The node's slot[0] must not be

		 * NULLed in case there are concurrent lookups expecting to

		 * find the item. However if this was a bottom-level node,

		 * then it may be subject to the slot pointer being visible

		 * to callers dereferencing it. If item corresponding to

		 * slot[0] is subsequently deleted, these callers would expect

		 * their slot to become empty sooner or later.

		 *

		 * For example, lockless pagecache will look up a slot, deref

		 * the page pointer, and if the page has 0 refcount it means it

		 * was concurrently deleted from pagecache so try the deref

		 * again. Fortunately there is already a requirement for logic

		 * to retry the entire slot lookup -- the indirect pointer

		 * problem (replacing direct root node with an indirect pointer

		 * also results in a stale slot). So tag the slot as indirect

		 * to force callers to retry.

			/*

			 * Shouldn't the tags already have all been cleared

			 * by the caller?

/**

 *	__radix_tree_create	-	create a slot in a radix tree

 *	@root:		radix tree root

 *	@index:		index key

 *	@nodep:		returns node

 *	@slotp:		returns slot

 *

 *	Create, if necessary, and return the node and slot for an item

 *	at position @index in the radix tree @root.

 *

 *	Until there is more than one item in the tree, no nodes are

 *	allocated and @root->xa_head is used as a direct slot instead of

 *	pointing to a node, in which case *@nodep will be NULL.

 *

 *	Returns -ENOMEM, or 0 for success.

 Make sure the tree is high enough.  */

 Have to add a child node.  */

 Go a level down */

/*

 * Free any nodes below this node.  The tree is presumed to not need

 * shrinking, and any user data in the tree is presumed to not need a

 * destructor called on it.  If we need to add a destructor, we can

 * add that functionality later.  Note that we may not clear tags or

 * slots from the tree as an RCU walker may still have a pointer into

 * this subtree.  We could replace the entries with RADIX_TREE_RETRY,

 * but we'll still have to clear those in rcu_free.

/**

 *	radix_tree_insert    -    insert into a radix tree

 *	@root:		radix tree root

 *	@index:		index key

 *	@item:		item to insert

 *

 *	Insert an item into the radix tree at position @index.

/**

 *	__radix_tree_lookup	-	lookup an item in a radix tree

 *	@root:		radix tree root

 *	@index:		index key

 *	@nodep:		returns node

 *	@slotp:		returns slot

 *

 *	Lookup and return the item at position @index in the radix

 *	tree @root.

 *

 *	Until there is more than one item in the tree, no nodes are

 *	allocated and @root->xa_head is used as a direct slot instead of

 *	pointing to a node, in which case *@nodep will be NULL.

/**

 *	radix_tree_lookup_slot    -    lookup a slot in a radix tree

 *	@root:		radix tree root

 *	@index:		index key

 *

 *	Returns:  the slot corresponding to the position @index in the

 *	radix tree @root. This is useful for update-if-exists operations.

 *

 *	This function can be called under rcu_read_lock iff the slot is not

 *	modified by radix_tree_replace_slot, otherwise it must be called

 *	exclusive from other writers. Any dereference of the slot must be done

 *	using radix_tree_deref_slot.

/**

 *	radix_tree_lookup    -    perform lookup operation on a radix tree

 *	@root:		radix tree root

 *	@index:		index key

 *

 *	Lookup the item at the position @index in the radix tree @root.

 *

 *	This function can be called under rcu_read_lock, however the caller

 *	must manage lifetimes of leaf nodes (eg. RCU may also be used to free

 *	them safely). No RCU barriers are required to access or modify the

 *	returned item, however.

/*

 * IDR users want to be able to store NULL in the tree, so if the slot isn't

 * free, don't adjust the count, even if it's transitioning between NULL and

 * non-NULL.  For the IDA, we mark slots as being IDR_FREE while they still

 * have empty bits, but it only stores NULL in slots when they're being

 * deleted.

/**

 * __radix_tree_replace		- replace item in a slot

 * @root:		radix tree root

 * @node:		pointer to tree node

 * @slot:		pointer to slot in @node

 * @item:		new item to store in the slot.

 *

 * For use with __radix_tree_lookup().  Caller must hold tree write locked

 * across slot lookup and replacement.

	/*

	 * This function supports replacing value entries and

	 * deleting entries, but that needs accounting against the

	 * node unless the slot is root->xa_head.

/**

 * radix_tree_replace_slot	- replace item in a slot

 * @root:	radix tree root

 * @slot:	pointer to slot

 * @item:	new item to store in the slot.

 *

 * For use with radix_tree_lookup_slot() and

 * radix_tree_gang_lookup_tag_slot().  Caller must hold tree write locked

 * across slot lookup and replacement.

 *

 * NOTE: This cannot be used to switch between non-entries (empty slots),

 * regular entries, and value entries, as that requires accounting

 * inside the radix tree node. When switching from one type of entry or

 * deleting, use __radix_tree_lookup() and __radix_tree_replace() or

 * radix_tree_iter_replace().

/**

 * radix_tree_iter_replace - replace item in a slot

 * @root:	radix tree root

 * @iter:	iterator state

 * @slot:	pointer to slot

 * @item:	new item to store in the slot.

 *

 * For use with radix_tree_for_each_slot().

 * Caller must hold tree write locked.

/**

 *	radix_tree_tag_set - set a tag on a radix tree node

 *	@root:		radix tree root

 *	@index:		index key

 *	@tag:		tag index

 *

 *	Set the search tag (which must be < RADIX_TREE_MAX_TAGS)

 *	corresponding to @index in the radix tree.  From

 *	the root all the way down to the leaf node.

 *

 *	Returns the address of the tagged item.  Setting a tag on a not-present

 *	item is a bug.

 set the root's tag bit */

 clear the root's tag bit */

/**

 *	radix_tree_tag_clear - clear a tag on a radix tree node

 *	@root:		radix tree root

 *	@index:		index key

 *	@tag:		tag index

 *

 *	Clear the search tag (which must be < RADIX_TREE_MAX_TAGS)

 *	corresponding to @index in the radix tree.  If this causes

 *	the leaf node to have no tags set then clear the tag in the

 *	next-to-leaf node, etc.

 *

 *	Returns the address of the tagged item on success, else NULL.  ie:

 *	has the same return value and semantics as radix_tree_lookup().

/**

  * radix_tree_iter_tag_clear - clear a tag on the current iterator entry

  * @root: radix tree root

  * @iter: iterator state

  * @tag: tag to clear

/**

 * radix_tree_tag_get - get a tag on a radix tree node

 * @root:		radix tree root

 * @index:		index key

 * @tag:		tag index (< RADIX_TREE_MAX_TAGS)

 *

 * Return values:

 *

 *  0: tag not present or not set

 *  1: tag set

 *

 * Note that the return value of this function may not be relied on, even if

 * the RCU lock is held, unless tag modification and node deletion are excluded

 * from concurrency.

 Construct iter->tags bit-mask from node->tags[tag] array */

 This never happens if RADIX_TREE_TAG_LONGS == 1 */

 Pick tags from next element */

 Clip chunk size, here only BITS_PER_LONG tags */

/**

 * radix_tree_next_chunk - find next chunk of slots for iteration

 *

 * @root:	radix tree root

 * @iter:	iterator state

 * @flags:	RADIX_TREE_ITER_* flags and tag index

 * Returns:	pointer to chunk first slot, or NULL if iteration is over

	/*

	 * Catch next_index overflow after ~0UL. iter->index never overflows

	 * during iterating; it can be zero only at the beginning.

	 * And we cannot overflow iter->next_index in a single step,

	 * because RADIX_TREE_MAP_SHIFT < BITS_PER_LONG.

	 *

	 * This condition also used by radix_tree_next_slot() to stop

	 * contiguous iterating, and forbid switching to the next chunk.

 Single-slot tree */

 Hole detected */

 Overflow after ~0UL */

 Update the iterator state */

/**

 *	radix_tree_gang_lookup - perform multiple lookup on a radix tree

 *	@root:		radix tree root

 *	@results:	where the results of the lookup are placed

 *	@first_index:	start the lookup from this key

 *	@max_items:	place up to this many items at *results

 *

 *	Performs an index-ascending scan of the tree for present items.  Places

 *	them at *@results and returns the number of items which were placed at

 *	*@results.

 *

 *	The implementation is naive.

 *

 *	Like radix_tree_lookup, radix_tree_gang_lookup may be called under

 *	rcu_read_lock. In this case, rather than the returned results being

 *	an atomic snapshot of the tree at a single point in time, the

 *	semantics of an RCU protected gang lookup are as though multiple

 *	radix_tree_lookups have been issued in individual locks, and results

 *	stored in 'results'.

/**

 *	radix_tree_gang_lookup_tag - perform multiple lookup on a radix tree

 *	                             based on a tag

 *	@root:		radix tree root

 *	@results:	where the results of the lookup are placed

 *	@first_index:	start the lookup from this key

 *	@max_items:	place up to this many items at *results

 *	@tag:		the tag index (< RADIX_TREE_MAX_TAGS)

 *

 *	Performs an index-ascending scan of the tree for present items which

 *	have the tag indexed by @tag set.  Places the items at *@results and

 *	returns the number of items which were placed at *@results.

/**

 *	radix_tree_gang_lookup_tag_slot - perform multiple slot lookup on a

 *					  radix tree based on a tag

 *	@root:		radix tree root

 *	@results:	where the results of the lookup are placed

 *	@first_index:	start the lookup from this key

 *	@max_items:	place up to this many items at *results

 *	@tag:		the tag index (< RADIX_TREE_MAX_TAGS)

 *

 *	Performs an index-ascending scan of the tree for present items which

 *	have the tag indexed by @tag set.  Places the slots at *@results and

 *	returns the number of slots which were placed at *@results.

/**

 * radix_tree_iter_delete - delete the entry at this iterator position

 * @root: radix tree root

 * @iter: iterator state

 * @slot: pointer to slot

 *

 * Delete the entry at the position currently pointed to by the iterator.

 * This may result in the current node being freed; if it is, the iterator

 * is advanced so that it will not reference the freed memory.  This

 * function may be called without any locking if there are no other threads

 * which can access this tree.

/**

 * radix_tree_delete_item - delete an item from a radix tree

 * @root: radix tree root

 * @index: index key

 * @item: expected item

 *

 * Remove @item at @index from the radix tree rooted at @root.

 *

 * Return: the deleted entry, or %NULL if it was not present

 * or the entry at the given @index was not @item.

/**

 * radix_tree_delete - delete an entry from a radix tree

 * @root: radix tree root

 * @index: index key

 *

 * Remove the entry at @index from the radix tree rooted at @root.

 *

 * Return: The deleted entry, or %NULL if it was not present.

/**

 *	radix_tree_tagged - test whether any items in the tree are tagged

 *	@root:		radix tree root

 *	@tag:		tag to test

/**

 * idr_preload - preload for idr_alloc()

 * @gfp_mask: allocation mask to use for preloading

 *

 * Preallocate memory to use for the next call to idr_alloc().  This function

 * returns with preemption disabled.  It will be enabled by idr_preload_end().

 Have to add a child node.  */

/**

 * idr_destroy - release all internal memory from an IDR

 * @idr: idr handle

 *

 * After this function is called, the IDR is empty, and may be reused or

 * the data structure containing it may be freed.

 *

 * A typical clean-up sequence for objects stored in an idr tree will use

 * idr_for_each() to free all objects, if necessary, then idr_destroy() to

 * free the memory used to keep track of those objects.

 Free per-cpu pool of preloaded nodes */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Simple encoder primitives for ASN.1 BER/DER/CER

 *

 * Copyright (C) 2019 James.Bottomley@HansenPartnership.com

/**

 * asn1_encode_integer() - encode positive integer to ASN.1

 * @data:	pointer to the pointer to the data

 * @end_data:	end of data pointer, points one beyond last usable byte in @data

 * @integer:	integer to be encoded

 *

 * This is a simplified encoder: it only currently does

 * positive integers, but it should be simple enough to add the

 * negative case if a use comes along.

 need at least 3 bytes for tag, length and integer encoding */

 remaining length where at d (the start of the integer encoding) */

		/*

		 * for a positive number the first byte must have bit

		 * 7 clear in two's complement (otherwise it's a

		 * negative number) so prepend a leading zero if

		 * that's not the case

			/*

			 * no check needed here, we already know we

			 * have len >= 1

 calculate the base 128 digit values setting the top bit of the first octet */

 quick case */

/**

 * asn1_encode_oid() - encode an oid to ASN.1

 * @data:	position to begin encoding at

 * @end_data:	end of data pointer, points one beyond last usable byte in @data

 * @oid:	array of oids

 * @oid_len:	length of oid array

 *

 * this encodes an OID up to ASN.1 when presented as an array of OID values

 need at least 3 bytes for tag, length and OID encoding */

/**

 * asn1_encode_length() - encode a length to follow an ASN.1 tag

 * @data: pointer to encode at

 * @data_len: pointer to remaining length (adjusted by routine)

 * @len: length to encode

 *

 * This routine can encode lengths up to 65535 using the ASN.1 rules.

 * It will accept a negative length and place a zero length tag

 * instead (to keep the ASN.1 valid).  This convention allows other

 * encoder primitives to accept negative lengths as singalling the

 * sequence will be re-encoded when the length is known.

/**

 * asn1_encode_tag() - add a tag for optional or explicit value

 * @data:	pointer to place tag at

 * @end_data:	end of data pointer, points one beyond last usable byte in @data

 * @tag:	tag to be placed

 * @string:	the data to be tagged

 * @len:	the length of the data to be tagged

 *

 * Note this currently only handles short form tags < 31.

 *

 * Standard usage is to pass in a @tag, @string and @length and the

 * @string will be ASN.1 encoded with @tag and placed into @data.  If

 * the encoding would put data past @end_data then an error is

 * returned, otherwise a pointer to a position one beyond the encoding

 * is returned.

 *

 * To encode in place pass a NULL @string and -1 for @len and the

 * maximum allowable beginning and end of the data; all this will do

 * is add the current maximum length and update the data pointer to

 * the place where the tag contents should be placed is returned.  The

 * data should be copied in by the calling routine which should then

 * repeat the prior statement but now with the known length.  In order

 * to avoid having to keep both before and after pointers, the repeat

 * expects to be called with @data pointing to where the first encode

 * returned it and still NULL for @string but the real length in @len.

		/*

		 * we're recoding, so move back to the start of the

		 * tag and install a dummy length because the real

		 * data_len should be NULL

/**

 * asn1_encode_octet_string() - encode an ASN.1 OCTET STRING

 * @data:	pointer to encode at

 * @end_data:	end of data pointer, points one beyond last usable byte in @data

 * @string:	string to be encoded

 * @len:	length of string

 *

 * Note ASN.1 octet strings may contain zeros, so the length is obligatory.

 need minimum of 2 bytes for tag and length of zero length string */

/**

 * asn1_encode_sequence() - wrap a byte stream in an ASN.1 SEQUENCE

 * @data:	pointer to encode at

 * @end_data:	end of data pointer, points one beyond last usable byte in @data

 * @seq:	data to be encoded as a sequence

 * @len:	length of the data to be encoded as a sequence

 *

 * Fill in a sequence.  To encode in place, pass NULL for @seq and -1

 * for @len; then call again once the length is known (still with NULL

 * for @seq). In order to avoid having to keep both before and after

 * pointers, the repeat expects to be called with @data pointing to

 * where the first encode placed it.

		/*

		 * we're recoding, so move back to the start of the

		 * sequence and install a dummy length because the

		 * real length should be NULL

/**

 * asn1_encode_boolean() - encode a boolean value to ASN.1

 * @data:	pointer to encode at

 * @end_data:	end of data pointer, points one beyond last usable byte in @data

 * @val:	the boolean true/false value

 booleans are 3 bytes: tag, length == 1 and value == 0 or 1 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This module emits "Hello, world" on printk when loaded.

 *

 * It is designed to be used for basic evaluation of the module loading

 * subsystem (for example when validating module signing/verification). It

 * lacks any extra dependencies, and will not normally be loaded by the

 * system unless explicitly requested by name.

 SPDX-License-Identifier: GPL-2.0

/*

 * Note that the "cmpxchg()" reloads the "old" value for the

 * failure case.

/**

 * lockref_get - Increments reference count unconditionally

 * @lockref: pointer to lockref structure

 *

 * This operation is only valid if you already hold a reference

 * to the object, so you know the count cannot be zero.

/**

 * lockref_get_not_zero - Increments count unless the count is 0 or dead

 * @lockref: pointer to lockref structure

 * Return: 1 if count updated successfully or 0 if count was zero

/**

 * lockref_put_not_zero - Decrements count unless count <= 1 before decrement

 * @lockref: pointer to lockref structure

 * Return: 1 if count updated successfully or 0 if count would become zero

/**

 * lockref_get_or_lock - Increments count unless the count is 0 or dead

 * @lockref: pointer to lockref structure

 * Return: 1 if count updated successfully or 0 if count was zero

 * and we got the lock instead.

/**

 * lockref_put_return - Decrement reference count if possible

 * @lockref: pointer to lockref structure

 *

 * Decrement the reference count and return the new value.

 * If the lockref was dead or locked, return an error.

/**

 * lockref_put_or_lock - decrements count unless count <= 1 before decrement

 * @lockref: pointer to lockref structure

 * Return: 1 if count updated successfully or 0 if count <= 1 and lock taken

/**

 * lockref_mark_dead - mark lockref dead

 * @lockref: pointer to lockref structure

/**

 * lockref_get_not_dead - Increments count unless the ref is dead

 * @lockref: pointer to lockref structure

 * Return: 1 if count updated successfully or 0 if lockref was dead

 SPDX-License-Identifier: GPL-2.0-or-later

/*

   lru_cache.c



   This file is part of DRBD by Philipp Reisner and Lars Ellenberg.



   Copyright (C) 2003-2008, LINBIT Information Technologies GmbH.

   Copyright (C) 2003-2008, Philipp Reisner <philipp.reisner@linbit.com>.

   Copyright (C) 2003-2008, Lars Ellenberg <lars.ellenberg@linbit.com>.





 for memset */

 for seq_printf */

/* this is developers aid only.

 BUG() if e is not one of the elements tracked by lc */

/* We need to atomically

 *  - try to grab the lock (set LC_LOCKED)

 *  - only if there is no pending transaction

 *    (neither LC_DIRTY nor LC_STARVING is set)

 * Because of PARANOIA_ENTRY() above abusing lc->flags as well,

 * it is not sufficient to just say

 *	return 0 == cmpxchg(&lc->flags, 0, LC_LOCKED);

 Spin until no-one is inside a PARANOIA_ENTRY()/RETURN() section. */

	/* Alternative approach, spin in case someone enters or leaves a

/**

 * lc_create - prepares to track objects in an active set

 * @name: descriptive name only used in lc_seq_printf_stats and lc_seq_dump_details

 * @cache: cache root pointer

 * @max_pending_changes: maximum changes to accumulate until a transaction is required

 * @e_count: number of elements allowed to be active simultaneously

 * @e_size: size of the tracked objects

 * @e_off: offset to the &struct lc_element member in a tracked object

 *

 * Returns a pointer to a newly initialized struct lru_cache on success,

 * or NULL on (allocation) failure.

	/* e_count too big; would probably fail the allocation below anyways.

 preallocate all objects */

 else: could not allocate all elements, give up */

/**

 * lc_destroy - frees memory allocated by lc_create()

 * @lc: the lru cache to destroy

/**

 * lc_reset - does a full reset for @lc and the hash table slots.

 * @lc: the lru cache to operate on

 *

 * It is roughly the equivalent of re-allocating a fresh lru_cache object,

 * basically a short cut to lc_destroy(lc); lc = lc_create(...);

 re-init it */

/**

 * lc_seq_printf_stats - print stats about @lc into @seq

 * @seq: the seq_file to print into

 * @lc: the lru cache to print statistics of

	/* NOTE:

	 * total calls to lc_get are

	 * (starving + hits + misses)

	 * misses include "locked" count (update from an other thread in

	 * progress) and "changed", when this in fact lead to an successful

	 * update of the cache.

		/* "about to be changed" elements, pending transaction commit,

		 * are hashed by their "new number". "Normal" elements have

/**

 * lc_find - find element by label, if present in the hash table

 * @lc: The lru_cache object

 * @enr: element number

 *

 * Returns the pointer to an element, if the element with the requested

 * "label" or element number is present in the hash table,

 * or NULL if not found. Does not change the refcnt.

 * Ignores elements that are "about to be used", i.e. not yet in the active

 * set, but still pending transaction commit.

/**

 * lc_is_used - find element by label

 * @lc: The lru_cache object

 * @enr: element number

 *

 * Returns true, if the element with the requested "label" or element number is

 * present in the hash table, and is used (refcnt > 0).

 * Also finds elements that are not _currently_ used but only "about to be

 * used", i.e. on the "to_be_changed" list, pending transaction commit.

/**

 * lc_del - removes an element from the cache

 * @lc: The lru_cache object

 * @e: The element to remove

 *

 * @e must be unused (refcnt == 0). Moves @e from "lru" to "free" list,

 * sets @e->enr to %LC_FREE.

 something on the free list */

 something to evict */

 used as internal flags to __lc_get */

	/* if lc_new_number != lc_number,

	 * this enr is currently being pulled in already,

	 * and will be available once the pending transaction

			/* It has been found above, but on the "to_be_changed"

			 * list, not yet committed.  Don't pull it in twice,

			 * wait for the transaction, then try again...

			/* ... unless the caller is aware of the implications,

 else: lc_new_number == lc_number; a real hit. */

 Not evictable... */

 e == NULL */

	/* To avoid races with lc_try_lock(), first, mark us dirty

	/* ... only then check if it is locked anyways. If lc_unlock clears

	 * the dirty bit again, that's not a problem, we will come here again.

	/* In case there is nothing available and we can not kick out

	 * the LRU element, we have to wait ...

	/* It was not present in the active set.  We are going to recycle an

	 * unused (or even "free") element, but we won't accumulate more than

/**

 * lc_get - get element by label, maybe change the active set

 * @lc: the lru cache to operate on

 * @enr: the label to look up

 *

 * Finds an element in the cache, increases its usage count,

 * "touches" and returns it.

 *

 * In case the requested number is not present, it needs to be added to the

 * cache. Therefore it is possible that an other element becomes evicted from

 * the cache. In either case, the user is notified so he is able to e.g. keep

 * a persistent log of the cache changes, and therefore the objects in use.

 *

 * Return values:

 *  NULL

 *     The cache was marked %LC_STARVING,

 *     or the requested label was not in the active set

 *     and a changing transaction is still pending (@lc was marked %LC_DIRTY).

 *     Or no unused or free element could be recycled (@lc will be marked as

 *     %LC_STARVING, blocking further lc_get() operations).

 *

 *  pointer to the element with the REQUESTED element number.

 *     In this case, it can be used right away

 *

 *  pointer to an UNUSED element with some different element number,

 *          where that different number may also be %LC_FREE.

 *

 *          In this case, the cache is marked %LC_DIRTY,

 *          so lc_try_lock() will no longer succeed.

 *          The returned element pointer is moved to the "to_be_changed" list,

 *          and registered with the new element number on the hash collision chains,

 *          so it is possible to pick it up from lc_is_used().

 *          Up to "max_pending_changes" (see lc_create()) can be accumulated.

 *          The user now should do whatever housekeeping is necessary,

 *          typically serialize on lc_try_lock_for_transaction(), then call

 *          lc_committed(lc) and lc_unlock(), to finish the change.

 *

 * NOTE: The user needs to check the lc_number on EACH use, so he recognizes

 *       any cache set change.

/**

 * lc_get_cumulative - like lc_get; also finds to-be-changed elements

 * @lc: the lru cache to operate on

 * @enr: the label to look up

 *

 * Unlike lc_get this also returns the element for @enr, if it is belonging to

 * a pending transaction, so the return values are like for lc_get(),

 * plus:

 *

 * pointer to an element already on the "to_be_changed" list.

 * 	In this case, the cache was already marked %LC_DIRTY.

 *

 * Caller needs to make sure that the pending transaction is completed,

 * before proceeding to actually use this element.

/**

 * lc_try_get - get element by label, if present; do not change the active set

 * @lc: the lru cache to operate on

 * @enr: the label to look up

 *

 * Finds an element in the cache, increases its usage count,

 * "touches" and returns it.

 *

 * Return values:

 *  NULL

 *     The cache was marked %LC_STARVING,

 *     or the requested label was not in the active set

 *

 *  pointer to the element with the REQUESTED element number.

 *     In this case, it can be used right away

/**

 * lc_committed - tell @lc that pending changes have been recorded

 * @lc: the lru cache to operate on

 *

 * User is expected to serialize on explicit lc_try_lock_for_transaction()

 * before the transaction is started, and later needs to lc_unlock() explicitly

 * as well.

 count number of changes, not number of transactions */

/**

 * lc_put - give up refcnt of @e

 * @lc: the lru cache to operate on

 * @e: the element to put

 *

 * If refcnt reaches zero, the element is moved to the lru list,

 * and a %LC_STARVING (if set) is cleared.

 * Returns the new (post-decrement) refcnt.

 move it to the front of LRU. */

/**

 * lc_element_by_index

 * @lc: the lru cache to operate on

 * @i: the index of the element to return

/**

 * lc_index_of

 * @lc: the lru cache to operate on

 * @e: the element to query for its index position in lc->element

/**

 * lc_set - associate index with label

 * @lc: the lru cache to operate on

 * @enr: the label to set

 * @index: the element index to associate label with.

 *

 * Used to initialize the active set to some previously recorded state.

/**

 * lc_seq_dump_details - Dump a complete LRU cache to seq in textual form.

 * @lc: the lru cache to operate on

 * @seq: the &struct seq_file pointer to seq_printf into

 * @utext: user supplied additional "heading" or other info

 * @detail: function pointer the user may provide to dump further details

 * of the object the lc_element is embedded in. May be NULL.

 * Note: a leading space ' ' and trailing newline '\n' is implied.

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0

 Copyright (c) 2018 Mellanox Technologies. All rights reserved */

 First round of gets, the root objects should be created */

	/* Do the second round of gets, all roots are already created,

	 * make sure that no new root is created

 r: 1			d: */

 r: 1, 7		d: */

 r: 1, 7		d: 3^1 */

 r: 1, 7		d: 3^1, 5^1 */

 r: 1, 7		d: 3^1, 3^1, 5^1 */

 r: 1, 1, 7		d: 3^1, 3^1, 5^1 */

 r: 1, 1, 7, 30	d: 3^1, 3^1, 5^1 */

 r: 1, 1, 7, 30	d: 3^1, 3^1, 5^1, 8^7 */

 r: 1, 1, 7, 30	d: 3^1, 3^1, 5^1, 8^7, 8^7 */

 r: 1, 1, 7, 30	d: 3^1, 5^1, 8^7, 8^7 */

 r: 1, 1, 7, 30	d: 5^1, 8^7, 8^7 */

 r: 1, 7, 30		d: 5^1, 8^7, 8^7 */

 r: 7, 30		d: 5^1, 8^7, 8^7 */

 r: 7, 30		d: 8^7, 8^7 */

 r: 7, 30, 5		d: 8^7, 8^7 */

 r: 7, 30, 5		d: 8^7, 8^7, 6^5 */

 r: 7, 30, 5		d: 8^7, 8^7, 8^7, 6^5 */

 r: 7, 30, 5		d: 8^7, 8^7, 6^5 */

 r: 7, 30, 5		d: 8^7, 6^5 */

 r: 7, 30, 5		d: 6^5 */

 r: 7, 30, 5		d: 6^5, 8^5 */

 r: 30, 5		d: 6^5, 8^5 */

 r: 5			d: 6^5, 8^5 */

 r:			d: 6^5, 8^5 */

 r:			d: 6^5 */

 r:			d: */

			/* It is possible that one of the neighbor stats with

			 * same numbers have the correct key id, so check it

	/* This can only happen when action is not inversed.

	 * So in case of an error, cleanup by doing inverse action.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2007 Jens Axboe <jens.axboe@oracle.com>

 *

 * Scatterlist handling helpers.

/**

 * sg_next - return the next scatterlist entry in a list

 * @sg:		The current sg entry

 *

 * Description:

 *   Usually the next entry will be @sg@ + 1, but if this sg element is part

 *   of a chained scatterlist, it could jump to the start of a new

 *   scatterlist array.

 *

/**

 * sg_nents - return total count of entries in scatterlist

 * @sg:		The scatterlist

 *

 * Description:

 * Allows to know how many entries are in sg, taking into account

 * chaining as well

 *

/**

 * sg_nents_for_len - return total count of entries in scatterlist

 *                    needed to satisfy the supplied length

 * @sg:		The scatterlist

 * @len:	The total required length

 *

 * Description:

 * Determines the number of entries in sg that are required to meet

 * the supplied length, taking into account chaining as well

 *

 * Returns:

 *   the number of sg entries needed, negative error on failure

 *

/**

 * sg_last - return the last scatterlist entry in a list

 * @sgl:	First entry in the scatterlist

 * @nents:	Number of entries in the scatterlist

 *

 * Description:

 *   Should only be used casually, it (currently) scans the entire list

 *   to get the last entry.

 *

 *   Note that the @sgl@ pointer passed in need not be the first one,

 *   the important bit is that @nents@ denotes the number of entries that

 *   exist from @sgl@.

 *

/**

 * sg_init_table - Initialize SG table

 * @sgl:	   The SG table

 * @nents:	   Number of entries in table

 *

 * Notes:

 *   If this is part of a chained sg table, sg_mark_end() should be

 *   used only on the last table part.

 *

/**

 * sg_init_one - Initialize a single entry sg list

 * @sg:		 SG entry

 * @buf:	 Virtual address for IO

 * @buflen:	 IO length

 *

/*

 * The default behaviour of sg_alloc_table() is to use these kmalloc/kfree

 * helpers.

		/*

		 * Kmemleak doesn't track page allocations as they are not

		 * commonly used (in a raw form) for kernel data structures.

		 * As we chain together a list of pages and then a normal

		 * kmalloc (tracked by kmemleak), in order to for that last

		 * allocation not to become decoupled (and thus a

		 * false-positive) we need to inform kmemleak of all the

		 * intermediate allocations.

/**

 * __sg_free_table - Free a previously mapped sg table

 * @table:	The sg table header to use

 * @max_ents:	The maximum number of entries per single scatterlist

 * @nents_first_chunk: Number of entries int the (preallocated) first

 * 	scatterlist chunk, 0 means no such preallocated first chunk

 * @free_fn:	Free function

 * @num_ents:	Number of entries in the table

 *

 *  Description:

 *    Free an sg table previously allocated and setup with

 *    __sg_alloc_table().  The @max_ents value must be identical to

 *    that previously used with __sg_alloc_table().

 *

		/*

		 * If we have more than max_ents segments left,

		 * then assign 'next' to the sg table after the current one.

		 * sg_size is then one less than alloc size, since the last

		 * element is the chain pointer.

/**

 * sg_free_append_table - Free a previously allocated append sg table.

 * @table:	 The mapped sg append table header

 *

/**

 * sg_free_table - Free a previously allocated sg table

 * @table:	The mapped sg table header

 *

/**

 * __sg_alloc_table - Allocate and initialize an sg table with given allocator

 * @table:	The sg table header to use

 * @nents:	Number of entries in sg list

 * @max_ents:	The maximum number of entries the allocator returns per call

 * @nents_first_chunk: Number of entries int the (preallocated) first

 * 	scatterlist chunk, 0 means no such preallocated chunk provided by user

 * @gfp_mask:	GFP allocation mask

 * @alloc_fn:	Allocator to use

 *

 * Description:

 *   This function returns a @table @nents long. The allocator is

 *   defined to return scatterlist chunks of maximum size @max_ents.

 *   Thus if @nents is bigger than @max_ents, the scatterlists will be

 *   chained in units of @max_ents.

 *

 * Notes:

 *   If this function returns non-0 (eg failure), the caller must call

 *   __sg_free_table() to cleanup any leftover allocations.

 *

			/*

			 * Adjust entry count to reflect that the last

			 * entry of the previous table won't be used for

			 * linkage.  Without this, sg_kfree() may get

			 * confused.

		/*

		 * If this is the first mapping, assign the sg table header.

		 * If this is not the first mapping, chain previous part.

		/*

		 * If no more entries after this one, mark the end

/**

 * sg_alloc_table - Allocate and initialize an sg table

 * @table:	The sg table header to use

 * @nents:	Number of entries in sg list

 * @gfp_mask:	GFP allocation mask

 *

 *  Description:

 *    Allocate and initialize an sg table. If @nents@ is larger than

 *    SG_MAX_SINGLE_ALLOC a chained sg table will be setup.

 *

 Check if last entry should be keeped for chainning */

/**

 * sg_alloc_append_table_from_pages - Allocate and initialize an append sg

 *                                    table from an array of pages

 * @sgt_append:  The sg append table to use

 * @pages:       Pointer to an array of page pointers

 * @n_pages:     Number of pages in the pages array

 * @offset:      Offset from start of the first page to the start of a buffer

 * @size:        Number of valid bytes in the buffer (after offset)

 * @max_segment: Maximum size of a scatterlist element in bytes

 * @left_pages:  Left pages caller have to set after this call

 * @gfp_mask:	 GFP allocation mask

 *

 * Description:

 *    In the first call it allocate and initialize an sg table from a list of

 *    pages, else reuse the scatterlist from sgt_append. Contiguous ranges of

 *    the pages are squashed into a single scatterlist entry up to the maximum

 *    size specified in @max_segment.  A user may provide an offset at a start

 *    and a size of valid data in a buffer specified by the page array. The

 *    returned sg table is released by sg_free_append_table

 *

 * Returns:

 *   0 on success, negative error on failure

 *

 * Notes:

 *   If this function returns non-0 (eg failure), the caller must call

 *   sg_free_append_table() to cleanup any leftover allocations.

 *

 *   In the fist call, sgt_append must by initialized.

	/*

	 * The algorithm below requires max_segment to be aligned to PAGE_SIZE

	 * otherwise it can overshoot.

 Merge contiguous pages into the last SG */

 compute number of contiguous chunks */

 merging chunks and putting them into the scatterlist */

 look for the end of the current chunk */

 Pass how many chunks might be left */

			/*

			 * Adjust entry length to be as before function was

			 * called.

/**

 * sg_alloc_table_from_pages_segment - Allocate and initialize an sg table from

 *                                     an array of pages and given maximum

 *                                     segment.

 * @sgt:	 The sg table header to use

 * @pages:	 Pointer to an array of page pointers

 * @n_pages:	 Number of pages in the pages array

 * @offset:      Offset from start of the first page to the start of a buffer

 * @size:        Number of valid bytes in the buffer (after offset)

 * @max_segment: Maximum size of a scatterlist element in bytes

 * @gfp_mask:	 GFP allocation mask

 *

 *  Description:

 *    Allocate and initialize an sg table from a list of pages. Contiguous

 *    ranges of the pages are squashed into a single scatterlist node up to the

 *    maximum size specified in @max_segment. A user may provide an offset at a

 *    start and a size of valid data in a buffer specified by the page array.

 *

 *    The returned sg table is released by sg_free_table.

 *

 *  Returns:

 *   0 on success, negative error on failure

/**

 * sgl_alloc_order - allocate a scatterlist and its pages

 * @length: Length in bytes of the scatterlist. Must be at least one

 * @order: Second argument for alloc_pages()

 * @chainable: Whether or not to allocate an extra element in the scatterlist

 *	for scatterlist chaining purposes

 * @gfp: Memory allocation flags

 * @nent_p: [out] Number of entries in the scatterlist that have pages

 *

 * Returns: A pointer to an initialized scatterlist or %NULL upon failure.

 Check for integer overflow */

 Check for integer overflow */

/**

 * sgl_alloc - allocate a scatterlist and its pages

 * @length: Length in bytes of the scatterlist

 * @gfp: Memory allocation flags

 * @nent_p: [out] Number of entries in the scatterlist

 *

 * Returns: A pointer to an initialized scatterlist or %NULL upon failure.

/**

 * sgl_free_n_order - free a scatterlist and its pages

 * @sgl: Scatterlist with one or more elements

 * @nents: Maximum number of elements to free

 * @order: Second argument for __free_pages()

 *

 * Notes:

 * - If several scatterlists have been chained and each chain element is

 *   freed separately then it's essential to set nents correctly to avoid that a

 *   page would get freed twice.

 * - All pages in a chained scatterlist can be freed at once by setting @nents

 *   to a high number.

/**

 * sgl_free_order - free a scatterlist and its pages

 * @sgl: Scatterlist with one or more elements

 * @order: Second argument for __free_pages()

/**

 * sgl_free - free a scatterlist and its pages

 * @sgl: Scatterlist with one or more elements

 CONFIG_SGL_ALLOC */

/**

 * sg_miter_start - start mapping iteration over a sg list

 * @miter: sg mapping iter to be started

 * @sgl: sg list to iterate over

 * @nents: number of sg entries

 *

 * Description:

 *   Starts mapping iterator @miter.

 *

 * Context:

 *   Don't care.

/**

 * sg_miter_skip - reposition mapping iterator

 * @miter: sg mapping iter to be skipped

 * @offset: number of bytes to plus the current location

 *

 * Description:

 *   Sets the offset of @miter to its current location plus @offset bytes.

 *   If mapping iterator @miter has been proceeded by sg_miter_next(), this

 *   stops @miter.

 *

 * Context:

 *   Don't care.

 *

 * Returns:

 *   true if @miter contains the valid mapping.  false if end of sg

 *   list is reached.

/**

 * sg_miter_next - proceed mapping iterator to the next mapping

 * @miter: sg mapping iter to proceed

 *

 * Description:

 *   Proceeds @miter to the next mapping.  @miter should have been started

 *   using sg_miter_start().  On successful return, @miter->page,

 *   @miter->addr and @miter->length point to the current mapping.

 *

 * Context:

 *   May sleep if !SG_MITER_ATOMIC.

 *

 * Returns:

 *   true if @miter contains the next mapping.  false if end of sg

 *   list is reached.

	/*

	 * Get to the next page if necessary.

	 * __remaining, __offset is adjusted by sg_miter_stop

/**

 * sg_miter_stop - stop mapping iteration

 * @miter: sg mapping iter to be stopped

 *

 * Description:

 *   Stops mapping iterator @miter.  @miter should have been started

 *   using sg_miter_start().  A stopped iteration can be resumed by

 *   calling sg_miter_next() on it.  This is useful when resources (kmap)

 *   need to be released during iteration.

 *

 * Context:

 *   Don't care otherwise.

 drop resources from the last iteration */

/**

 * sg_copy_buffer - Copy data between a linear buffer and an SG list

 * @sgl:		 The SG list

 * @nents:		 Number of SG entries

 * @buf:		 Where to copy from

 * @buflen:		 The number of bytes to copy

 * @skip:		 Number of bytes to skip before copying

 * @to_buffer:		 transfer direction (true == from an sg list to a

 *			 buffer, false == from a buffer to an sg list)

 *

 * Returns the number of copied bytes.

 *

/**

 * sg_copy_from_buffer - Copy from a linear buffer to an SG list

 * @sgl:		 The SG list

 * @nents:		 Number of SG entries

 * @buf:		 Where to copy from

 * @buflen:		 The number of bytes to copy

 *

 * Returns the number of copied bytes.

 *

/**

 * sg_copy_to_buffer - Copy from an SG list to a linear buffer

 * @sgl:		 The SG list

 * @nents:		 Number of SG entries

 * @buf:		 Where to copy to

 * @buflen:		 The number of bytes to copy

 *

 * Returns the number of copied bytes.

 *

/**

 * sg_pcopy_from_buffer - Copy from a linear buffer to an SG list

 * @sgl:		 The SG list

 * @nents:		 Number of SG entries

 * @buf:		 Where to copy from

 * @buflen:		 The number of bytes to copy

 * @skip:		 Number of bytes to skip before copying

 *

 * Returns the number of copied bytes.

 *

/**

 * sg_pcopy_to_buffer - Copy from an SG list to a linear buffer

 * @sgl:		 The SG list

 * @nents:		 Number of SG entries

 * @buf:		 Where to copy to

 * @buflen:		 The number of bytes to copy

 * @skip:		 Number of bytes to skip before copying

 *

 * Returns the number of copied bytes.

 *

/**

 * sg_zero_buffer - Zero-out a part of a SG list

 * @sgl:		 The SG list

 * @nents:		 Number of SG entries

 * @buflen:		 The number of bytes to zero out

 * @skip:		 Number of bytes to skip before zeroing

 *

 * Returns the number of bytes zeroed.

 SPDX-License-Identifier: GPL-2.0-only

 CONFIG_HAVE_ARCH_BITREVERSE */

 SPDX-License-Identifier: GPL-2.0+

/*

 * test_ida.c: Test the IDA API

 * Copyright (c) 2016-2018 Microsoft Corporation

 * Copyright (c) 2018 Oracle Corporation

 * Author: Matthew Wilcox <willy@infradead.org>

/*

 * Straightforward checks that allocating and freeing IDs work.

 Destroy an IDA with a single entry at @base */

 Check that ida_destroy and ida_is_empty work */

 Destroy an already-empty IDA */

/*

 * Check what happens when we fill a leaf and then delete it.  This may

 * discover mishandling of IDR_FREE.

/*

 * Check allocations up to and slightly above the maximum allowed (2^31-1) ID.

 * Allocating up to 2^31-1 should succeed, and then allocating the next one

 * should fail.

/*

 * Check handling of conversions between exceptional entries and full bitmaps.

/*

 * Aug 8, 2011 Bob Pearson with help from Joakim Tjernlund and George Spelvin

 * cleaned up code to current version of sparse and added the slicing-by-8

 * algorithm to the closely similar existing slicing-by-4 algorithm.

 *

 * Oct 15, 2000 Matt Domsch <Matt_Domsch@dell.com>

 * Nicer crc32 functions/docs submitted by linux@horizon.com.  Thanks!

 * Code was from the public domain, copyright abandoned.  Code was

 * subsequently included in the kernel, thus was re-licensed under the

 * GNU GPL v2.

 *

 * Oct 12, 2000 Matt Domsch <Matt_Domsch@dell.com>

 * Same crc32 function was used in 5 other places in the kernel.

 * I made one version, and deleted the others.

 * There are various incantations of crc32().  Some use a seed of 0 or ~0.

 * Some xor at the end with ~0.  The generic crc32() function takes

 * seed as an argument, and doesn't xor at the end.  Then individual

 * users can do whatever they need.

 *   drivers/net/smc9194.c uses seed ~0, doesn't xor with ~0.

 *   fs/jffs2 uses seed 0, doesn't xor with ~0.

 *   fs/partitions/efi.c uses seed ~0, xor's with ~0.

 *

 * This source code is licensed under the GNU General Public License,

 * Version 2.  See the file COPYING for more details.

 see: Documentation/staging/crc32.rst for a description of algorithms */

 implements slicing-by-4 or slicing-by-8 algorithm */

 Align it */

 use pre increment for speed */

 And the last few bytes */

 use pre increment for speed */

 use pre increment for speed */

/**

 * crc32_le_generic() - Calculate bitwise little-endian Ethernet AUTODIN II

 *			CRC32/CRC32C

 * @crc: seed value for computation.  ~0 for Ethernet, sometimes 0 for other

 *	 uses, or the previous crc32/crc32c value if computing incrementally.

 * @p: pointer to buffer over which CRC32/CRC32C is run

 * @len: length of buffer @p

 * @tab: little-endian Ethernet table

 * @polynomial: CRC32/CRC32c LE polynomial

 aka Sarwate algorithm */

/*

 * This multiplies the polynomials x and y modulo the given modulus.

 * This follows the "little-endian" CRC convention that the lsbit

 * represents the highest power of x, and the msbit represents x^0.

/**

 * crc32_generic_shift - Append @len 0 bytes to crc, in logarithmic time

 * @crc: The original little-endian CRC (i.e. lsbit is x^31 coefficient)

 * @len: The number of bytes. @crc is multiplied by x^(8*@len)

 * @polynomial: The modulus used to reduce the result to 32 bits.

 *

 * It's possible to parallelize CRC computations by computing a CRC

 * over separate ranges of a buffer, then summing them.

 * This shifts the given CRC by 8*len bits (i.e. produces the same effect

 * as appending len bytes of zero to the data), in time proportional

 * to log(len).

 CRC of x^32 */

 Shift up to 32 bits in the simple linear way */

 "power" is x^(2^i), modulo the polynomial */

 Square power, advancing to x^(2^(i+1)) */

/**

 * crc32_be_generic() - Calculate bitwise big-endian Ethernet AUTODIN II CRC32

 * @crc: seed value for computation.  ~0 for Ethernet, sometimes 0 for

 *	other uses, or the previous crc32 value if computing incrementally.

 * @p: pointer to buffer over which CRC32 is run

 * @len: length of buffer @p

 * @tab: big-endian Ethernet table

 * @polynomial: CRC32 BE polynomial

 SPDX-License-Identifier: GPL-2.0

/*

 * decompress.c

 *

 * Detect the decompression method based on magic number

 Need at least this much... */

 SPDX-License-Identifier: GPL-2.0

/*

 * Ensure these tables do not accidentally become gigantic if some

 * huge errno makes it in. On most architectures, the first table will

 * only have about 140 entries, but mips and parisc have more sparsely

 * allocated errnos (with EHWPOISON = 257 on parisc, and EDQUOT = 1133

 * on mips), so this wastes a bit of space on those - though we

 * special case the EDQUOT case.

 ECANCELLED */

 EWOULDBLOCK */

 EREFUSED */

 EDEADLOCK */

 But why? */

 1133 */

/*

 * errname(EIO) -> "EIO"

 * errname(-EIO) -> "-EIO"

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/lib/cmdline.c

 * Helper functions generally used for parsing kernel command line

 * and module options.

 *

 * Code and copyrights come from init/main.c and arch/i386/kernel/setup.c.

 *

 * GNU Indent formatting options for this file: -kr -i8 -npsl -pcs

/*

 *	If a hyphen was found in get_option, this will handle the

 *	range of numbers, M-N.  This will expand the range and insert

 *	the values[M, M+1, ..., N] into the ints array in get_options.

/**

 *	get_option - Parse integer from an option string

 *	@str: option string

 *	@pint: (optional output) integer value parsed from @str

 *

 *	Read an int from an option string; if available accept a subsequent

 *	comma as well.

 *

 *	When @pint is NULL the function can be used as a validator of

 *	the current option in the string.

 *

 *	Return values:

 *	0 - no int in string

 *	1 - int found, no subsequent comma

 *	2 - int found including a subsequent comma

 *	3 - hyphen found to denote a range

 *

 *	Leading hyphen without integer is no integer case, but we consume it

 *	for the sake of simplification.

/**

 *	get_options - Parse a string into a list of integers

 *	@str: String to be parsed

 *	@nints: size of integer array

 *	@ints: integer array (must have room for at least one element)

 *

 *	This function parses a string containing a comma-separated

 *	list of integers, a hyphen-separated range of _positive_ integers,

 *	or a combination of both.  The parse halts when the array is

 *	full, or when no more numbers can be retrieved from the

 *	string.

 *

 *	When @nints is 0, the function just validates the given @str and

 *	returns the amount of parseable integers as described below.

 *

 *	Returns:

 *

 *	The first element is filled by the number of collected integers

 *	in the range. The rest is what was parsed from the @str.

 *

 *	Return value is the character in the string which caused

 *	the parse to end (typically a null terminator, if @str is

 *	completely parseable).

			/*

			 * Decrement the result by one to leave out the

			 * last number in the range.  The next iteration

			 * will handle the upper number in the range

/**

 *	memparse - parse a string with mem suffixes into a number

 *	@ptr: Where parse begins

 *	@retptr: (output) Optional pointer to next char after parse completes

 *

 *	Parses a string into a number.  The number stored at @ptr is

 *	potentially suffixed with K, M, G, T, P, E.

 local pointer to end of parsed string */

/**

 *	parse_option_str - Parse a string and check an option is set or not

 *	@str: String to be parsed

 *	@option: option name

 *

 *	This function parses a string containing a comma-separated list of

 *	strings like a=b,c.

 *

 *	Return true if there's such option in the string, or return false.

/*

 * Parse a string to get a param value pair.

 * You can use " around spaces, but can't escape ".

 * Hyphens and underscores equivalent in parameter names.

 Don't include quotes in value. */

 Chew up trailing spaces. */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Parse build id from the note segment. This logic can be shared between

 * 32-bit and 64-bit system, because Elf32_Nhdr and Elf64_Nhdr are

 * identical.

 overflow */

 check for overflow */

 only supports note that fits in the first page */

 Parse build ID from 32-bit ELF */

 only supports phdr that fits in one page */

 Parse build ID from 64-bit ELF */

 only supports phdr that fits in one page */

/*

 * Parse build ID of ELF file mapped to vma

 * @vma:      vma object

 * @build_id: buffer to store build id, at least BUILD_ID_SIZE long

 * @size:     returns actual build id size in case of success

 *

 * Return: 0 on success, -EINVAL otherwise

 only works for page backed storage  */

 page not mapped */

 compare magic x7f "ELF" */

 only support executable file and shared object file */

/**

 * build_id_parse_buf - Get build ID from a buffer

 * @buf:      Elf note section(s) to parse

 * @buf_size: Size of @buf in bytes

 * @build_id: Build ID parsed from @buf, at least BUILD_ID_SIZE_MAX long

 *

 * Return: 0 on success, -EINVAL otherwise

/**

 * init_vmlinux_build_id - Compute and stash the running kernel's build ID

 SPDX-License-Identifier: GPL-2.0

/*

 * Generate lookup table for the table-driven CRC64 calculation.

 *

 * gen_crc64table is executed in kernel build time and generates

 * lib/crc64table.h. This header is included by lib/crc64.c for

 * the table-driven CRC64 calculation.

 *

 * See lib/crc64.c for more information about which specification

 * and polynomial arithmetic that gen_crc64table.c follows to

 * generate the lookup table.

 *

 * Copyright 2018 SUSE Linux.

 *   Author: Coly Li <colyli@suse.de>

 this file is generated - do not edit */\n\n");

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Basic general purpose allocator for managing special purpose

 * memory, for example, memory that is not managed by the regular

 * kmalloc/kfree interface.  Uses for this includes on-device special

 * memory, uncached memory etc.

 *

 * It is safe to use the allocator in NMI handlers and other special

 * unblockable contexts that could otherwise deadlock on locks.  This

 * is implemented by using atomic operations and retries on any

 * conflicts.  The disadvantage is that there may be livelocks in

 * extreme cases.  For better scalability, one allocator can be used

 * for each CPU.

 *

 * The lockless operation only works if there is enough memory

 * available.  If new memory is added to the pool a lock has to be

 * still taken.  So any user relying on locklessness has to ensure

 * that sufficient memory is preallocated.

 *

 * The basic atomic operation of this allocator is cmpxchg on long.

 * On architectures that don't have NMI-safe cmpxchg implementation,

 * the allocator can NOT be used in NMI handler.  So code uses the

 * allocator in NMI handler should depend on

 * CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.

 *

 * Copyright 2005 (C) Jes Sorensen <jes@trained-monkey.org>

/*

 * bitmap_set_ll - set the specified number of bits at the specified position

 * @map: pointer to a bitmap

 * @start: a bit position in @map

 * @nr: number of bits to set

 *

 * Set @nr bits start from @start in @map lock-lessly. Several users

 * can set/clear the same bitmap simultaneously without lock. If two

 * users set the same bit, one user will return remain bits, otherwise

 * return 0.

/*

 * bitmap_clear_ll - clear the specified number of bits at the specified position

 * @map: pointer to a bitmap

 * @start: a bit position in @map

 * @nr: number of bits to set

 *

 * Clear @nr bits start from @start in @map lock-lessly. Several users

 * can set/clear the same bitmap simultaneously without lock. If two

 * users clear the same bit, one user will return remain bits,

 * otherwise return 0.

/**

 * gen_pool_create - create a new special memory pool

 * @min_alloc_order: log base 2 of number of bytes each bitmap bit represents

 * @nid: node id of the node the pool structure should be allocated on, or -1

 *

 * Create a new special memory pool that can be used to manage special purpose

 * memory not managed by the regular kmalloc/kfree interface.

/**

 * gen_pool_add_owner- add a new chunk of special memory to the pool

 * @pool: pool to add new memory chunk to

 * @virt: virtual starting address of memory chunk to add to pool

 * @phys: physical starting address of memory chunk to add to pool

 * @size: size in bytes of the memory chunk to add to pool

 * @nid: node id of the node the chunk structure and bitmap should be

 *       allocated on, or -1

 * @owner: private data the publisher would like to recall at alloc time

 *

 * Add a new chunk of special memory to the specified pool.

 *

 * Returns 0 on success or a -ve errno on failure.

/**

 * gen_pool_virt_to_phys - return the physical address of memory

 * @pool: pool to allocate from

 * @addr: starting address of memory

 *

 * Returns the physical address on success, or -1 on error.

/**

 * gen_pool_destroy - destroy a special memory pool

 * @pool: pool to destroy

 *

 * Destroy the specified special memory pool. Verifies that there are no

 * outstanding allocations.

/**

 * gen_pool_alloc_algo_owner - allocate special memory from the pool

 * @pool: pool to allocate from

 * @size: number of bytes to allocate from the pool

 * @algo: algorithm passed from caller

 * @data: data passed to algorithm

 * @owner: optionally retrieve the chunk owner

 *

 * Allocate the requested number of bytes from the specified pool.

 * Uses the pool allocation function (with first-fit algorithm by default).

 * Can not be used in NMI handler on architectures without

 * NMI-safe cmpxchg implementation.

/**

 * gen_pool_dma_alloc - allocate special memory from the pool for DMA usage

 * @pool: pool to allocate from

 * @size: number of bytes to allocate from the pool

 * @dma: dma-view physical address return value.  Use %NULL if unneeded.

 *

 * Allocate the requested number of bytes from the specified pool.

 * Uses the pool allocation function (with first-fit algorithm by default).

 * Can not be used in NMI handler on architectures without

 * NMI-safe cmpxchg implementation.

 *

 * Return: virtual address of the allocated memory, or %NULL on failure

/**

 * gen_pool_dma_alloc_algo - allocate special memory from the pool for DMA

 * usage with the given pool algorithm

 * @pool: pool to allocate from

 * @size: number of bytes to allocate from the pool

 * @dma: DMA-view physical address return value. Use %NULL if unneeded.

 * @algo: algorithm passed from caller

 * @data: data passed to algorithm

 *

 * Allocate the requested number of bytes from the specified pool. Uses the

 * given pool allocation function. Can not be used in NMI handler on

 * architectures without NMI-safe cmpxchg implementation.

 *

 * Return: virtual address of the allocated memory, or %NULL on failure

/**

 * gen_pool_dma_alloc_align - allocate special memory from the pool for DMA

 * usage with the given alignment

 * @pool: pool to allocate from

 * @size: number of bytes to allocate from the pool

 * @dma: DMA-view physical address return value. Use %NULL if unneeded.

 * @align: alignment in bytes for starting address

 *

 * Allocate the requested number bytes from the specified pool, with the given

 * alignment restriction. Can not be used in NMI handler on architectures

 * without NMI-safe cmpxchg implementation.

 *

 * Return: virtual address of the allocated memory, or %NULL on failure

/**

 * gen_pool_dma_zalloc - allocate special zeroed memory from the pool for

 * DMA usage

 * @pool: pool to allocate from

 * @size: number of bytes to allocate from the pool

 * @dma: dma-view physical address return value.  Use %NULL if unneeded.

 *

 * Allocate the requested number of zeroed bytes from the specified pool.

 * Uses the pool allocation function (with first-fit algorithm by default).

 * Can not be used in NMI handler on architectures without

 * NMI-safe cmpxchg implementation.

 *

 * Return: virtual address of the allocated zeroed memory, or %NULL on failure

/**

 * gen_pool_dma_zalloc_algo - allocate special zeroed memory from the pool for

 * DMA usage with the given pool algorithm

 * @pool: pool to allocate from

 * @size: number of bytes to allocate from the pool

 * @dma: DMA-view physical address return value. Use %NULL if unneeded.

 * @algo: algorithm passed from caller

 * @data: data passed to algorithm

 *

 * Allocate the requested number of zeroed bytes from the specified pool. Uses

 * the given pool allocation function. Can not be used in NMI handler on

 * architectures without NMI-safe cmpxchg implementation.

 *

 * Return: virtual address of the allocated zeroed memory, or %NULL on failure

/**

 * gen_pool_dma_zalloc_align - allocate special zeroed memory from the pool for

 * DMA usage with the given alignment

 * @pool: pool to allocate from

 * @size: number of bytes to allocate from the pool

 * @dma: DMA-view physical address return value. Use %NULL if unneeded.

 * @align: alignment in bytes for starting address

 *

 * Allocate the requested number of zeroed bytes from the specified pool,

 * with the given alignment restriction. Can not be used in NMI handler on

 * architectures without NMI-safe cmpxchg implementation.

 *

 * Return: virtual address of the allocated zeroed memory, or %NULL on failure

/**

 * gen_pool_free_owner - free allocated special memory back to the pool

 * @pool: pool to free to

 * @addr: starting address of memory to free back to pool

 * @size: size in bytes of memory to free

 * @owner: private data stashed at gen_pool_add() time

 *

 * Free previously allocated special memory back to the specified

 * pool.  Can not be used in NMI handler on architectures without

 * NMI-safe cmpxchg implementation.

/**

 * gen_pool_for_each_chunk - call func for every chunk of generic memory pool

 * @pool:	the generic memory pool

 * @func:	func to call

 * @data:	additional data used by @func

 *

 * Call @func for every chunk of generic memory pool.  The @func is

 * called with rcu_read_lock held.

/**

 * gen_pool_has_addr - checks if an address falls within the range of a pool

 * @pool:	the generic memory pool

 * @start:	start address

 * @size:	size of the region

 *

 * Check if the range of addresses falls within the specified pool. Returns

 * true if the entire range is contained in the pool and false otherwise.

/**

 * gen_pool_avail - get available free space of the pool

 * @pool: pool to get available free space

 *

 * Return available free space of the specified pool.

/**

 * gen_pool_size - get size in bytes of memory managed by the pool

 * @pool: pool to get size

 *

 * Return size in bytes of memory managed by the pool.

/**

 * gen_pool_set_algo - set the allocation algorithm

 * @pool: pool to change allocation algorithm

 * @algo: custom algorithm function

 * @data: additional data used by @algo

 *

 * Call @algo for each memory allocation in the pool.

 * If @algo is NULL use gen_pool_first_fit as default

 * memory allocation function.

/**

 * gen_pool_first_fit - find the first available region

 * of memory matching the size requirement (no alignment constraint)

 * @map: The address to base the search on

 * @size: The bitmap size in bits

 * @start: The bitnumber to start searching at

 * @nr: The number of zeroed bits we're looking for

 * @data: additional data - unused

 * @pool: pool to find the fit region memory from

 * @start_addr: not used in this function

/**

 * gen_pool_first_fit_align - find the first available region

 * of memory matching the size requirement (alignment constraint)

 * @map: The address to base the search on

 * @size: The bitmap size in bits

 * @start: The bitnumber to start searching at

 * @nr: The number of zeroed bits we're looking for

 * @data: data for alignment

 * @pool: pool to get order from

 * @start_addr: start addr of alloction chunk

/**

 * gen_pool_fixed_alloc - reserve a specific region

 * @map: The address to base the search on

 * @size: The bitmap size in bits

 * @start: The bitnumber to start searching at

 * @nr: The number of zeroed bits we're looking for

 * @data: data for alignment

 * @pool: pool to get order from

 * @start_addr: not used in this function

/**

 * gen_pool_first_fit_order_align - find the first available region

 * of memory matching the size requirement. The region will be aligned

 * to the order of the size specified.

 * @map: The address to base the search on

 * @size: The bitmap size in bits

 * @start: The bitnumber to start searching at

 * @nr: The number of zeroed bits we're looking for

 * @data: additional data - unused

 * @pool: pool to find the fit region memory from

 * @start_addr: not used in this function

/**

 * gen_pool_best_fit - find the best fitting region of memory

 * matching the size requirement (no alignment constraint)

 * @map: The address to base the search on

 * @size: The bitmap size in bits

 * @start: The bitnumber to start searching at

 * @nr: The number of zeroed bits we're looking for

 * @data: additional data - unused

 * @pool: pool to find the fit region memory from

 * @start_addr: not used in this function

 *

 * Iterate over the bitmap to find the smallest free region

 * which we can allocate the memory.

 NULL data matches only a pool without an assigned name */

/**

 * gen_pool_get - Obtain the gen_pool (if any) for a device

 * @dev: device to retrieve the gen_pool from

 * @name: name of a gen_pool or NULL, identifies a particular gen_pool on device

 *

 * Returns the gen_pool for the device if one is present, or NULL.

/**

 * devm_gen_pool_create - managed gen_pool_create

 * @dev: device that provides the gen_pool

 * @min_alloc_order: log base 2 of number of bytes each bitmap bit represents

 * @nid: node selector for allocated gen_pool, %NUMA_NO_NODE for all nodes

 * @name: name of a gen_pool or NULL, identifies a particular gen_pool on device

 *

 * Create a new special memory pool that can be used to manage special purpose

 * memory not managed by the regular kmalloc/kfree interface. The pool will be

 * automatically destroyed by the device management code.

 Check that genpool to be created is uniquely addressed on device */

/**

 * of_gen_pool_get - find a pool by phandle property

 * @np: device node

 * @propname: property name containing phandle(s)

 * @index: index into the phandle array

 *

 * Returns the pool that contains the chunk starting at the physical

 * address of the device tree node pointed at by the phandle property,

 * or NULL if not found.

 Check if named gen_pool is created by parent node device */

 CONFIG_OF */

 SPDX-License-Identifier: GPL-2.0

/*

 * kernel userspace event delivery

 *

 * Copyright (C) 2004 Red Hat, Inc.  All rights reserved.

 * Copyright (C) 2004 Novell, Inc.  All rights reserved.

 * Copyright (C) 2004 IBM, Inc. All rights reserved.

 *

 * Authors:

 *	Robert Love		<rml@novell.com>

 *	Kay Sievers		<kay.sievers@vrfy.org>

 *	Arjan van de Ven	<arjanv@redhat.com>

 *	Greg Kroah-Hartman	<greg@kroah.com>

 This lock protects uevent_seqnum and uevent_sock_list */

 the strings here must match the enum in include/linux/kobject.h */

 first arg is UUID */

	/*

	 * the rest are custom environment variables in KEY=VALUE

	 * format with ' ' delimiter between each KEY=VALUE pair

 skip the ' ', key must follow */

 skip the '=', value must follow */

/**

 * kobject_synth_uevent - send synthetic uevent with arguments

 *

 * @kobj: struct kobject for which synthetic uevent is to be generated

 * @buf: buffer containing action type and action args, newline is ignored

 * @count: length of buffer

 *

 * Returns 0 if kobject_synthetic_uevent() is completed with success or the

 * corresponding error when it fails.

 allocate message with maximum possible size */

 add header */

 send netlink message */

 ENOBUFS should be handled in userspace */

 fix credentials */

 fix uid */

 fix gid */

 ENOBUFS should be handled in userspace */

	/* kobjects currently only carry network namespace tags and they

	 * are the only tag relevant here since we want to decide which

	 * network namespaces to broadcast the uevent into.

/**

 * kobject_uevent_env - send an uevent with environmental data

 *

 * @kobj: struct kobject that the action is happening to

 * @action: action that is happening

 * @envp_ext: pointer to environmental data

 *

 * Returns 0 if kobject_uevent_env() is completed with success or the

 * corresponding error when it fails.

	/*

	 * Mark "remove" event done regardless of result, for some subsystems

	 * do not want to re-trigger "remove" event via automatic cleanup.

 search the kset we belong to */

 skip the event, if uevent_suppress is set*/

 skip the event, if the filter returns zero. */

 originating subsystem */

 environment buffer */

 complete object path */

 default keys */

 keys passed in from the caller */

 let the kset specific function add its stuff */

		/*

		 * Mark "add" event so we can make sure we deliver "remove"

		 * event to userspace during automatic cleanup. If

		 * the object did send an "add" event, "remove" will

		 * automatically generated by the core, if not already done

		 * by the caller.

 we will send an event, so request a new sequence number */

 call uevent_helper, usually only enabled during early boot */

 freed by cleanup_uevent_env */

/**

 * kobject_uevent - notify userspace by sending an uevent

 *

 * @kobj: struct kobject that the action is happening to

 * @action: action that is happening

 *

 * Returns 0 if kobject_uevent() is completed with success or the

 * corresponding error when it fails.

/**

 * add_uevent_var - add key value string to the environment buffer

 * @env: environment buffer structure

 * @format: printf format for the key=value pair

 *

 * Returns 0 if environment variable was added successfully or -ENOMEM

 * if no space was available.

 u64 to chars: 2^64 - 1 = 21 chars */

 bump and prepare sequence number */

 verify message does not overflow */

 copy skb and extend to accommodate sequence number */

 append sequence number */

 remove msg header */

 set portid 0 to inform userspace message comes from kernel */

 ENOBUFS should be handled in userspace */

	/*

	 * Verify that we are allowed to send messages to the target

	 * network namespace. The caller must have CAP_SYS_ADMIN in the

	 * owning user namespace of the target network namespace.

 Restrict uevents to initial user namespace. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Lock-less NULL terminated single linked list

 *

 * The basic atomic operation of this list is cmpxchg on long.  On

 * architectures that don't have NMI-safe cmpxchg implementation, the

 * list can NOT be used in NMI handlers.  So code that uses the list in

 * an NMI handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.

 *

 * Copyright 2010,2011 Intel Corp.

 *   Author: Huang Ying <ying.huang@intel.com>

/**

 * llist_add_batch - add several linked entries in batch

 * @new_first:	first entry in batch to be added

 * @new_last:	last entry in batch to be added

 * @head:	the head for your lock-less list

 *

 * Return whether list is empty before adding.

/**

 * llist_del_first - delete the first entry of lock-less list

 * @head:	the head for your lock-less list

 *

 * If list is empty, return NULL, otherwise, return the first entry

 * deleted, this is the newest added one.

 *

 * Only one llist_del_first user can be used simultaneously with

 * multiple llist_add users without lock.  Because otherwise

 * llist_del_first, llist_add, llist_add (or llist_del_all, llist_add,

 * llist_add) sequence in another user may change @head->first->next,

 * but keep @head->first.  If multiple consumers are needed, please

 * use llist_del_all or use lock between consumers.

/**

 * llist_reverse_order - reverse order of a llist chain

 * @head:	first item of the list to be reversed

 *

 * Reverse the order of a chain of llist entries and return the

 * new first entry.

 SPDX-License-Identifier: GPL-2.0

/*

 * This file exists solely to ensure debug information for some core

 * data structures is included in the final image even for

 * CONFIG_DEBUG_INFO_REDUCED. Please do not add actual code. However,

 * adding appropriate #includes is fine.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * lib/lshrdi3.c

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * A generic kernel FIFO implementation

 *

 * Copyright (C) 2009/2010 Stefani Seibold <stefani@seibold.net>

/*

 * internal helper to calculate the unused elements in a fifo

	/*

	 * round up to the next power of 2, since our 'let the indices

	 * wrap' technique works only in this case.

	/*

	 * make sure that the data in the fifo is up to date before

	 * incrementing the fifo->in index counter

	/*

	 * make sure that the data is copied before

	 * incrementing the fifo->out index counter

	/*

	 * make sure that the data in the fifo is up to date before

	 * incrementing the fifo->in index counter

 return the number of elements which are not copied */

	/*

	 * make sure that the data is copied before

	 * incrementing the fifo->out index counter

 return the number of elements which are not copied */

/*

 * __kfifo_peek_n internal helper function for determinate the length of

 * the next record in the fifo

/*

 * __kfifo_poke_n internal helper function for storing the length of

 * the record into the fifo

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * lib/ts_fsm.c	   A naive finite state machine text search approach

 *

 * Authors:	Thomas Graf <tgraf@suug.ch>

 *

 * ==========================================================================

 *

 *   A finite state machine consists of n states (struct ts_fsm_token)

 *   representing the pattern as a finite automaton. The data is read

 *   sequentially on an octet basis. Every state token specifies the number

 *   of recurrences and the type of value accepted which can be either a

 *   specific character or ctype based set of characters. The available

 *   type of recurrences include 1, (0|1), [0 n], and [1 n].

 *

 *   The algorithm differs between strict/non-strict mode specifying

 *   whether the pattern has to start at the first octet. Strict mode

 *   is enabled by default and can be disabled by inserting

 *   TS_FSM_HEAD_IGNORE as the first token in the chain.

 *

 *   The runtime performance of the algorithm should be around O(n),

 *   however while in strict mode the average runtime can be better.

 other values derived from ctype.h */

 ascii */

 wildcard */

 Map to _ctype flags and some magic numbers */

   0-  3 */

   4-  7 */

   8- 11 */

  12- 15 */

  16- 19 */

  20- 23 */

  24- 27 */

  28- 31 */

  32- 35 */

  36- 39 */

  40- 43 */

  44- 47 */

  48- 51 */

  52- 55 */

  56- 59 */

  60- 63 */

  64- 67 */

  68- 71 */

  72- 75 */

  76- 79 */

  80- 83 */

  84- 87 */

  88- 91 */

  92- 95 */

  96- 99 */

 100-103 */

 104-107 */

 108-111 */

 112-115 */

 116-119 */

 120-123 */

 124-127 */

 128-131 */

 132-135 */

 136-139 */

 140-143 */

 144-147 */

 148-151 */

 152-155 */

 156-159 */

 160-163 */

 164-167 */

 168-171 */

 172-175 */

 176-179 */

 180-183 */

 184-187 */

 188-191 */

 192-195 */

 196-199 */

 200-203 */

 204-207 */

 208-211 */

 212-215 */

 216-219 */

 220-223 */

 224-227 */

 228-231 */

 232-235 */

 236-239 */

 240-243 */

 244-247 */

 248-251 */

 252-255 */

		/*

		 * Optimization: Prefer small local loop over jumping

		 * back and forth until garbage at head is munched.

				/*

				 * Special case, don't start over upon

				 * a mismatch, give the user the

				 * chance to specify the type of data

				 * allowed to be ignored.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Test cases for <linux/hash.h> and <linux/stringhash.h>

 * This just verifies that various ways of computing a hash

 * produce the same thing and, for cases where a k-bit hash

 * value is requested, is of the requested size.

 *

 * We fill a buffer with a 255-byte null-terminated string,

 * and use both full_name_hash() and hashlen_string() to hash the

 * substrings from i to j, where 0 <= i < j < 256.

 *

 * The returned values are used to check that __hash_32() and

 * __hash_32_generic() compute the same thing.  Likewise hash_32()

 * and hash_64().

 32-bit XORSHIFT generator.  Seed must not be zero. */

 Given a non-zero x, returns a non-zero byte. */

 1 <= x <= 0x1fffe */

 1 <= x <= 0x2fd */

 1 <= x <= 0x100 */

 1 <= x <= 0xff */

 Fill the buffer with non-zero bytes. */

/*

 * Test the various integer hash functions.  h64 (or its low-order bits)

 * is the integer to hash.  hash_or accumulates the OR of the hash values,

 * which are later checked to see that they cover all the requested bits.

 *

 * Because these functions (as opposed to the string hashes) are all

 * inline, the code being tested is actually in the module, and you can

 * recompile and re-test the module without rebooting.

 Test __hash32 */

 Test k = 1..32 bits */

 Low k bits set */

 Test hash_32 */

 Test hash_64 */

 Suppress unused variable warning */

 Run time is cubic in SIZE */

 Test every possible non-empty substring in the buffer. */

 Check that hashlen_string gets the length right */

 Check that the hashes match */

 For use with hash_64 */

 i */

 j */

 The OR of all the hash values should cover all the bits */

 Test is pointless if results match */

 Likewise for all the i-bit hash values */

 Low i bits set */

 Issue notices about skipped tests. */

 Does everything */

 Does nothing */

 SPDX-License-Identifier: GPL-2.0-only

 identifiers for device / performance-differentiated memory regions */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

  Red Black Trees

  (C) 1999  Andrea Arcangeli <andrea@suse.de>

  (C) 2002  David Woodhouse <dwmw2@infradead.org>

  (C) 2012  Michel Lespinasse <walken@google.com>





  linux/lib/rbtree.c

/*

 * red-black trees properties:  https://en.wikipedia.org/wiki/Rbtree

 *

 *  1) A node is either red or black

 *  2) The root is black

 *  3) All leaves (NULL) are black

 *  4) Both children of every red node are black

 *  5) Every simple path from root to leaves contains the same number

 *     of black nodes.

 *

 *  4 and 5 give the O(log n) guarantee, since 4 implies you cannot have two

 *  consecutive red nodes in a path and every red node is therefore followed by

 *  a black. So if B is the number of black nodes on every simple path (as per

 *  5), then the longest possible path due to 4 is 2B.

 *

 *  We shall indicate color with case, where black nodes are uppercase and red

 *  nodes will be lowercase. Unknown color nodes shall be drawn as red within

 *  parentheses and have some accompanying text comment.

/*

 * Notes on lockless lookups:

 *

 * All stores to the tree structure (rb_left and rb_right) must be done using

 * WRITE_ONCE(). And we must not inadvertently cause (temporary) loops in the

 * tree structure as seen in program order.

 *

 * These two requirements will allow lockless iteration of the tree -- not

 * correct iteration mind you, tree rotations are not atomic so a lookup might

 * miss entire subtrees.

 *

 * But they do guarantee that any such traversal will only see valid elements

 * and that it will indeed complete -- does not get stuck in a loop.

 *

 * It also guarantees that if the lookup returns an element it is the 'correct'

 * one. But not returning an element does _NOT_ mean it's not present.

 *

 * NOTE:

 *

 * Stores to __rb_parent_color are not important for simple lookups so those

 * are left undone as of now. Nor did I check for loops involving parent

 * pointers.

/*

 * Helper function for rotations:

 * - old's parent and color get assigned to new

 * - old gets assigned new as a parent and 'color' as a color.

		/*

		 * Loop invariant: node is red.

			/*

			 * The inserted node is root. Either this is the

			 * first node, or we recursed at Case 1 below and

			 * are no longer violating 4).

		/*

		 * If there is a black parent, we are done.

		 * Otherwise, take some corrective action as,

		 * per 4), we don't want a red root or two

		 * consecutive red nodes.

 parent == gparent->rb_left */

				/*

				 * Case 1 - node's uncle is red (color flips).

				 *

				 *       G            g

				 *      / \          / \

				 *     p   u  -->   P   U

				 *    /            /

				 *   n            n

				 *

				 * However, since g's parent might be red, and

				 * 4) does not allow this, we need to recurse

				 * at g.

				/*

				 * Case 2 - node's uncle is black and node is

				 * the parent's right child (left rotate at parent).

				 *

				 *      G             G

				 *     / \           / \

				 *    p   U  -->    n   U

				 *     \           /

				 *      n         p

				 *

				 * This still leaves us in violation of 4), the

				 * continuation into Case 3 will fix that.

			/*

			 * Case 3 - node's uncle is black and node is

			 * the parent's left child (right rotate at gparent).

			 *

			 *        G           P

			 *       / \         / \

			 *      p   U  -->  n   g

			 *     /                 \

			 *    n                   U

 == parent->rb_right */

 Case 1 - color flips */

 Case 2 - right rotate at parent */

 Case 3 - left rotate at gparent */

 == parent->rb_left */

/*

 * Inline version for rb_erase() use - we want to be able to inline

 * and eliminate the dummy_rotate callback there

		/*

		 * Loop invariants:

		 * - node is black (or NULL on first iteration)

		 * - node is not the root (parent is not NULL)

		 * - All leaf paths going through parent and node have a

		 *   black node count that is 1 lower than other leaf paths.

 node == parent->rb_left */

				/*

				 * Case 1 - left rotate at parent

				 *

				 *     P               S

				 *    / \             / \

				 *   N   s    -->    p   Sr

				 *      / \         / \

				 *     Sl  Sr      N   Sl

					/*

					 * Case 2 - sibling color flip

					 * (p could be either color here)

					 *

					 *    (p)           (p)

					 *    / \           / \

					 *   N   S    -->  N   s

					 *      / \           / \

					 *     Sl  Sr        Sl  Sr

					 *

					 * This leaves us violating 5) which

					 * can be fixed by flipping p to black

					 * if it was red, or by recursing at p.

					 * p is red when coming from Case 1.

				/*

				 * Case 3 - right rotate at sibling

				 * (p could be either color here)

				 *

				 *   (p)           (p)

				 *   / \           / \

				 *  N   S    -->  N   sl

				 *     / \             \

				 *    sl  Sr            S

				 *                       \

				 *                        Sr

				 *

				 * Note: p might be red, and then both

				 * p and sl are red after rotation(which

				 * breaks property 4). This is fixed in

				 * Case 4 (in __rb_rotate_set_parents()

				 *         which set sl the color of p

				 *         and set p RB_BLACK)

				 *

				 *   (p)            (sl)

				 *   / \            /  \

				 *  N   sl   -->   P    S

				 *       \        /      \

				 *        S      N        Sr

				 *         \

				 *          Sr

			/*

			 * Case 4 - left rotate at parent + color flips

			 * (p and sl could be either color here.

			 *  After rotation, p becomes black, s acquires

			 *  p's color, and sl keeps its color)

			 *

			 *      (p)             (s)

			 *      / \             / \

			 *     N   S     -->   P   Sr

			 *        / \         / \

			 *      (sl) sr      N  (sl)

 Case 1 - right rotate at parent */

 Case 2 - sibling color flip */

 Case 3 - left rotate at sibling */

 Case 4 - right rotate at parent + color flips */

 Non-inline version for rb_erase_augmented() use */

/*

 * Non-augmented rbtree manipulation functions.

 *

 * We use dummy augmented callbacks here, and have the compiler optimize them

 * out of the rb_insert_color() and rb_erase() function definitions.

/*

 * Augmented rbtree manipulation functions.

 *

 * This instantiates the same __always_inline functions as in the non-augmented

 * case, but this time with user-defined callbacks.

/*

 * This function returns the first node (in sort order) of the tree.

	/*

	 * If we have a right-hand child, go down and then left as far

	 * as we can.

	/*

	 * No right-hand children. Everything down and left is smaller than us,

	 * so any 'next' node must be in the general direction of our parent.

	 * Go up the tree; any time the ancestor is a right-hand child of its

	 * parent, keep going up. First time it's a left-hand child of its

	 * parent, said parent is our 'next' node.

	/*

	 * If we have a left-hand child, go down and then right as far

	 * as we can.

	/*

	 * No left-hand children. Go up till we find an ancestor which

	 * is a right-hand child of its parent.

 Copy the pointers/colour from the victim to the replacement */

 Set the surrounding nodes to point to the replacement */

 Copy the pointers/colour from the victim to the replacement */

 Set the surrounding nodes to point to the replacement */

	/* Set the parent's pointer to the new node last after an RCU barrier

	 * so that the pointers onwards are seen to be set correctly when doing

	 * an RCU walk over the tree.

 If we're sitting on node, we've already seen our children */

		/* If we are the parent's left node, go to the parent's right

		/* Otherwise we are the parent's right node, and the parent

 SPDX-License-Identifier: GPL-2.0+

/*

 * Test cases for bitfield helpers.

	/*

	 * NOTE

	 * This whole function compiles (or at least should, if everything

	 * is going according to plan) to nothing after optimisation.

 these should fail compilation */

 this should at least give a warning */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/lib/kasprintf.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 Simplified asprintf. */

/*

 * If fmt contains no % (or is exactly %s), use kstrdup_const. If fmt

 * (or the sole vararg) points to rodata, we will then save a memory

 * allocation and string copy. In any case, the return value should be

 * freed using kfree_const().

 SPDX-License-Identifier: GPL-2.0-only

/*

 * A generic implementation of binary search for the Linux kernel

 *

 * Copyright (C) 2008-2009 Ksplice, Inc.

 * Author: Tim Abbott <tabbott@ksplice.com>

/*

 * bsearch - binary search an array of elements

 * @key: pointer to item being searched for

 * @base: pointer to first element to search

 * @num: number of elements

 * @size: size of each element

 * @cmp: pointer to comparison function

 *

 * This function does a binary search on the given array.  The

 * contents of the array should already be in ascending sorted order

 * under the provided comparison function.

 *

 * Note that the key need not have the same type as the elements in

 * the array, e.g. key could be a string and the comparison function

 * could compare the string with the struct's name field.  However, if

 * the key and elements in the array are of the same type, you can use

 * the same comparison function for both sort() and bsearch().

 SPDX-License-Identifier: GPL-2.0-or-later

/* saved per-CPU IRQ register pointer

 *

 * Copyright (C) 2006 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

 SPDX-License-Identifier: GPL-2.0-only

/*

 * UBSAN error reporting functions

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd.

 * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>

		/*

		 * This thread may hit another WARN() in the panic path.

		 * Resetting this prevents additional WARN() from panicking the

		 * system on this thread.  Other threads are blocked by the

		 * panic_mutex in panic().

 SPDX-License-Identifier: GPL-2.0-only

/*

 * crc4.c - simple crc-4 calculations.

/**

 * crc4 - calculate the 4-bit crc of a value.

 * @c:    starting crc4

 * @x:    value to checksum

 * @bits: number of bits in @x to checksum

 *

 * Returns the crc4 value of @x, using polynomial 0b10111.

 *

 * The @x value is treated as left-aligned, and bits above @bits are ignored

 * in the crc calculations.

 mask off anything above the top bit */

 Align to 4-bits */

 Calculate crc4 over four-bit nibbles, starting at the MSbit */

/*

 * The only reason this code can be compiled as a module is because the

 * ATA code that depends on it can be as well.  In practice, they're

 * both usually compiled in and the module overhead goes away.

/**

 * glob_match - Shell-style pattern matching, like !fnmatch(pat, str, 0)

 * @pat: Shell-style pattern to match, e.g. "*.[ch]".

 * @str: String to match.  The pattern must match the entire string.

 *

 * Perform shell-style glob matching, returning true (1) if the match

 * succeeds, or false (0) if it fails.  Equivalent to !fnmatch(@pat, @str, 0).

 *

 * Pattern metacharacters are ?, *, [ and \.

 * (And, inside character classes, !, - and ].)

 *

 * This is small and simple implementation intended for device blacklists

 * where a string is matched against a number of patterns.  Thus, it

 * does not preprocess the patterns.  It is non-recursive, and run-time

 * is at most quadratic: strlen(@str)*strlen(@pat).

 *

 * An example of the worst case is glob_match("*aaaaa", "aaaaaaaaaa");

 * it takes 6 passes over the pattern before matching the string.

 *

 * Like !fnmatch(@pat, @str, 0) and unlike the shell, this does NOT

 * treat / or leading . specially; it isn't actually used for pathnames.

 *

 * Note that according to glob(7) (and unlike bash), character classes

 * are complemented by a leading !; this does not support the regex-style

 * [^a-z] syntax.

 *

 * An opening bracket without a matching close is matched literally.

	/*

	 * Backtrack to previous * on mismatch and retry starting one

	 * character later in the string.  Because * matches all characters

	 * (no exception for /), it can be easily proved that there's

	 * never a need to backtrack multiple levels.

	/*

	 * Loop over each token (character or class) in pat, matching

	 * it against the remaining unmatched tail of str.  Return false

	 * on mismatch, or true after matching the trailing nul bytes.

 Wildcard: anything but nul */

 Any-length wildcard */

 Optimize trailing * case */

 Allow zero-length match */

 Character class */

			/*

			 * Iterate over each span in the character class.

			 * A span is either a single character a, or a

			 * range a-b.  The first span may begin with ']'.

 Malformed */

 Any special action if a > b? */

 Literal character */

 No point continuing */

 Try again from last *, one character later in str. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Wrapper for decompressing LZ4-compressed kernel, initramfs, and initrd

 *

 * Copyright (C) 2013, LG Electronics, Kyungsik Lee <kyungsik.lee@lge.com>

/*

 * Note: Uncompressed chunk size is used in the compressor side

 * (userspace side for compression).

 * It is hardcoded because there is not proper way to extract it

 * from the binary stream which is generated by the preliminary

 * version of LZ4 tool so far.

 empty or end-of-file */

 empty or end-of-file */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Resizable, Scalable, Concurrent Hash Table

 *

 * Copyright (c) 2015 Herbert Xu <herbert@gondor.apana.org.au>

 * Copyright (c) 2014-2015 Thomas Graf <tgraf@suug.ch>

 * Copyright (c) 2008-2014 Patrick McHardy <kaber@trash.net>

 *

 * Code partially derived from nft_hash

 * Rewritten with rehash code from br_multicast plus single list

 * pointer as suggested by Josh Triplett

	/* The top-level bucket entry does not need RCU protection

	 * because it's set at the same time as tbl->nest.

 Raced with another thread. */

 Need to preserved the bit lock. */

	/* Make insertions go into the new, empty table right away. Deletions

	 * and lookups will be attempted in both tables until we synchronize.

	 * As cmpxchg() provides strong barriers, we do not need

	 * rcu_assign_pointer().

 Publish the new table pointer. */

	/* Wait for readers. All new readers will see the new

	 * table, and thus no references to the old table will

	 * remain.

	 * We do this inside the locked region so that

	 * rhashtable_walk_stop() can use rcu_head_after_call_rcu()

	 * to check if it should not re-link the table.

/**

 * rhashtable_shrink - Shrink hash table while allowing concurrent lookups

 * @ht:		the hash table to shrink

 *

 * This function shrinks the hash table to fit, i.e., the smallest

 * size would not cause it to expand right away automatically.

 *

 * The caller must ensure that no concurrent resizing occurs by holding

 * ht->mutex.

 *

 * The caller must ensure that no concurrent table mutations take place.

 * It is however valid to have concurrent lookups if they are RCU protected.

 *

 * It is valid to have concurrent insertions and deletions protected by per

 * bucket locks or concurrent RCU protected lookups and traversals.

 Do not schedule more than one rehash */

 Do not fail the insert if someone else did a rehash. */

 Schedule async rehash to retry allocation in process context. */

 Need to preserve the bit lock */

	/* bkt is always the head of the list, so it holds

	 * the lock, which we need to preserve

 Failure is OK */

/**

 * rhashtable_walk_enter - Initialise an iterator

 * @ht:		Table to walk over

 * @iter:	Hash table Iterator

 *

 * This function prepares a hash table walk.

 *

 * Note that if you restart a walk after rhashtable_walk_stop you

 * may see the same object twice.  Also, you may miss objects if

 * there are removals in between rhashtable_walk_stop and the next

 * call to rhashtable_walk_start.

 *

 * For a completely stable walk you should construct your own data

 * structure outside the hash table.

 *

 * This function may be called from any process context, including

 * non-preemptable context, but cannot be called from softirq or

 * hardirq context.

 *

 * You must call rhashtable_walk_exit after this function returns.

/**

 * rhashtable_walk_exit - Free an iterator

 * @iter:	Hash table Iterator

 *

 * This function frees resources allocated by rhashtable_walk_enter.

/**

 * rhashtable_walk_start_check - Start a hash table walk

 * @iter:	Hash table iterator

 *

 * Start a hash table walk at the current iterator position.  Note that we take

 * the RCU lock in all cases including when we return an error.  So you must

 * always call rhashtable_walk_stop to clean up.

 *

 * Returns zero if successful.

 *

 * Returns -EAGAIN if resize event occurred.  Note that the iterator

 * will rewind back to the beginning and you may use it immediately

 * by calling rhashtable_walk_next.

 *

 * rhashtable_walk_start is defined as an inline variant that returns

 * void. This is preferred in cases where the caller would ignore

 * resize events and always continue.

		/*

		 * We need to validate that 'p' is still in the table, and

		 * if so, update 'skip'

		/* Need to validate that 'list' is still in the table, and

		 * if so, update 'skip' and 'p'.

/**

 * __rhashtable_walk_find_next - Find the next element in a table (or the first

 * one in case of a new walk).

 *

 * @iter:	Hash table iterator

 *

 * Returns the found object or NULL when the end of the table is reached.

 *

 * Returns -EAGAIN if resize event occurred.

 Ensure we see any new tables. */

/**

 * rhashtable_walk_next - Return the next object and advance the iterator

 * @iter:	Hash table iterator

 *

 * Note that you must call rhashtable_walk_stop when you are finished

 * with the walk.

 *

 * Returns the next object or NULL when the end of the table is reached.

 *

 * Returns -EAGAIN if resize event occurred.  Note that the iterator

 * will rewind back to the beginning and you may continue to use it.

		/* At the end of this slot, switch to next one and then find

		 * next entry from that point.

/**

 * rhashtable_walk_peek - Return the next object but don't advance the iterator

 * @iter:	Hash table iterator

 *

 * Returns the next object or NULL when the end of the table is reached.

 *

 * Returns -EAGAIN if resize event occurred.  Note that the iterator

 * will rewind back to the beginning and you may continue to use it.

 No object found in current iter, find next one in the table. */

		/* A nonzero skip value points to the next entry in the table

		 * beyond that last one that was found. Decrement skip so

		 * we find the current value. __rhashtable_walk_find_next

		 * will restore the original value of skip assuming that

		 * the table hasn't changed.

/**

 * rhashtable_walk_stop - Finish a hash table walk

 * @iter:	Hash table iterator

 *

 * Finish a hash table walk.  Does not reset the iterator to the start of the

 * hash table.

 This bucket table is being freed, don't re-link it. */

/**

 * rhashtable_init - initialize a new hash table

 * @ht:		hash table to be initialized

 * @params:	configuration parameters

 *

 * Initializes a new hash table based on the provided configuration

 * parameters. A table can be configured either with a variable or

 * fixed length key:

 *

 * Configuration Example 1: Fixed length keys

 * struct test_obj {

 *	int			key;

 *	void *			my_member;

 *	struct rhash_head	node;

 * };

 *

 * struct rhashtable_params params = {

 *	.head_offset = offsetof(struct test_obj, node),

 *	.key_offset = offsetof(struct test_obj, key),

 *	.key_len = sizeof(int),

 *	.hashfn = jhash,

 * };

 *

 * Configuration Example 2: Variable length keys

 * struct test_obj {

 *	[...]

 *	struct rhash_head	node;

 * };

 *

 * u32 my_hash_fn(const void *data, u32 len, u32 seed)

 * {

 *	struct test_obj *obj = data;

 *

 *	return [... hash ...];

 * }

 *

 * struct rhashtable_params params = {

 *	.head_offset = offsetof(struct test_obj, node),

 *	.hashfn = jhash,

 *	.obj_hashfn = my_hash_fn,

 * };

 Cap total entries at 2^31 to avoid nelems overflow. */

	/*

	 * This is api initialization and thus we need to guarantee the

	 * initial rhashtable allocation. Upon failure, retry with the

	 * smallest possible size with __GFP_NOFAIL semantics.

/**

 * rhltable_init - initialize a new hash list table

 * @hlt:	hash list table to be initialized

 * @params:	configuration parameters

 *

 * Initializes a new hash list table.

 *

 * See documentation for rhashtable_init.

/**

 * rhashtable_free_and_destroy - free elements and destroy hash table

 * @ht:		the hash table to destroy

 * @free_fn:	callback to release resources of element

 * @arg:	pointer passed to free_fn

 *

 * Stops an eventual async resize. If defined, invokes free_fn for each

 * element to releasal resources. Please note that RCU protected

 * readers may still be accessing the elements. Releasing of resources

 * must occur in a compatible manner. Then frees the bucket array.

 *

 * This function will eventually sleep to wait for an async resize

 * to complete. The caller is responsible that no further write operations

 * occurs in parallel.

 SPDX-License-Identifier: GPL-2.0

/**

 *	check_signature		-	find BIOS signatures

 *	@io_addr: mmio address to check

 *	@signature:  signature block

 *	@length: length of signature

 *

 *	Perform a signature comparison with the mmio address io_addr. This

 *	address should have been obtained by ioremap.

 *	Returns 1 on a match.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/lib/string.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

/*

 * This file should be used only for "library" routines that may have

 * alternative implementations on specific architectures (generally

 * found in <asm-xx/string.h>), or get overloaded by FORTIFY_SOURCE.

 * (Specifically, this file is built with __NO_FORTIFY.)

 *

 * Other helper functions should live in string_helpers.c.

/**

 * strncasecmp - Case insensitive, length-limited string comparison

 * @s1: One string

 * @s2: The other string

 * @len: the maximum number of characters to compare

 Yes, Virginia, it had better be unsigned */

/**

 * strcpy - Copy a %NUL terminated string

 * @dest: Where to copy the string to

 * @src: Where to copy the string from

 nothing */;

/**

 * strncpy - Copy a length-limited, C-string

 * @dest: Where to copy the string to

 * @src: Where to copy the string from

 * @count: The maximum number of bytes to copy

 *

 * The result is not %NUL-terminated if the source exceeds

 * @count bytes.

 *

 * In the case where the length of @src is less than  that  of

 * count, the remainder of @dest will be padded with %NUL.

 *

/**

 * strlcpy - Copy a C-string into a sized buffer

 * @dest: Where to copy the string to

 * @src: Where to copy the string from

 * @size: size of destination buffer

 *

 * Compatible with ``*BSD``: the result is always a valid

 * NUL-terminated string that fits in the buffer (unless,

 * of course, the buffer size is zero). It does not pad

 * out the result like strncpy() does.

/**

 * strscpy - Copy a C-string into a sized buffer

 * @dest: Where to copy the string to

 * @src: Where to copy the string from

 * @count: Size of destination buffer

 *

 * Copy the string, or as much of it as fits, into the dest buffer.  The

 * behavior is undefined if the string buffers overlap.  The destination

 * buffer is always NUL terminated, unless it's zero-sized.

 *

 * Preferred to strlcpy() since the API doesn't require reading memory

 * from the src string beyond the specified "count" bytes, and since

 * the return value is easier to error-check than strlcpy()'s.

 * In addition, the implementation is robust to the string changing out

 * from underneath it, unlike the current strlcpy() implementation.

 *

 * Preferred to strncpy() since it always returns a valid string, and

 * doesn't unnecessarily force the tail of the destination buffer to be

 * zeroed.  If zeroing is desired please use strscpy_pad().

 *

 * Returns:

 * * The number of characters copied (not including the trailing %NUL)

 * * -E2BIG if count is 0 or @src was truncated.

	/*

	 * If src is unaligned, don't cross a page boundary,

	 * since we don't know if the next page is mapped.

 If src or dest is unaligned, don't do word-at-a-time. */

 Hit buffer length without finding a NUL; force NUL-termination. */

/**

 * stpcpy - copy a string from src to dest returning a pointer to the new end

 *          of dest, including src's %NUL-terminator. May overrun dest.

 * @dest: pointer to end of string being copied into. Must be large enough

 *        to receive copy.

 * @src: pointer to the beginning of string being copied from. Must not overlap

 *       dest.

 *

 * stpcpy differs from strcpy in a key way: the return value is a pointer

 * to the new %NUL-terminating character in @dest. (For strcpy, the return

 * value is a pointer to the start of @dest). This interface is considered

 * unsafe as it doesn't perform bounds checking of the inputs. As such it's

 * not recommended for usage. Instead, its definition is provided in case

 * the compiler lowers other libcalls to stpcpy.

 nothing */;

/**

 * strcat - Append one %NUL-terminated string to another

 * @dest: The string to be appended to

 * @src: The string to append to it

/**

 * strncat - Append a length-limited, C-string to another

 * @dest: The string to be appended to

 * @src: The string to append to it

 * @count: The maximum numbers of bytes to copy

 *

 * Note that in contrast to strncpy(), strncat() ensures the result is

 * terminated.

/**

 * strlcat - Append a length-limited, C-string to another

 * @dest: The string to be appended to

 * @src: The string to append to it

 * @count: The size of the destination buffer.

 This would be a bug */

/**

 * strcmp - Compare two strings

 * @cs: One string

 * @ct: Another string

/**

 * strncmp - Compare two length-limited strings

 * @cs: One string

 * @ct: Another string

 * @count: The maximum number of bytes to compare

/**

 * strchr - Find the first occurrence of a character in a string

 * @s: The string to be searched

 * @c: The character to search for

 *

 * Note that the %NUL-terminator is considered part of the string, and can

 * be searched for.

/**

 * strchrnul - Find and return a character in a string, or end of string

 * @s: The string to be searched

 * @c: The character to search for

 *

 * Returns pointer to first occurrence of 'c' in s. If c is not found, then

 * return a pointer to the null byte at the end of s.

/**

 * strnchrnul - Find and return a character in a length limited string,

 * or end of string

 * @s: The string to be searched

 * @count: The number of characters to be searched

 * @c: The character to search for

 *

 * Returns pointer to the first occurrence of 'c' in s. If c is not found,

 * then return a pointer to the last character of the string.

/**

 * strrchr - Find the last occurrence of a character in a string

 * @s: The string to be searched

 * @c: The character to search for

/**

 * strnchr - Find a character in a length limited string

 * @s: The string to be searched

 * @count: The number of characters to be searched

 * @c: The character to search for

 *

 * Note that the %NUL-terminator is considered part of the string, and can

 * be searched for.

/**

 * strlen - Find the length of a string

 * @s: The string to be sized

 nothing */;

/**

 * strnlen - Find the length of a length-limited string

 * @s: The string to be sized

 * @count: The maximum number of bytes to search

 nothing */;

/**

 * strspn - Calculate the length of the initial substring of @s which only contain letters in @accept

 * @s: The string to be searched

 * @accept: The string to search for

/**

 * strcspn - Calculate the length of the initial substring of @s which does not contain letters in @reject

 * @s: The string to be searched

 * @reject: The string to avoid

/**

 * strpbrk - Find the first occurrence of a set of characters

 * @cs: The string to be searched

 * @ct: The characters to search for

/**

 * strsep - Split a string into tokens

 * @s: The string to be searched

 * @ct: The characters to search for

 *

 * strsep() updates @s to point after the token, ready for the next call.

 *

 * It returns empty tokens, too, behaving exactly like the libc function

 * of that name. In fact, it was stolen from glibc2 and de-fancy-fied.

 * Same semantics, slimmer shape. ;)

/**

 * memset - Fill a region of memory with the given value

 * @s: Pointer to the start of the area.

 * @c: The byte to fill the area with

 * @count: The size of the area.

 *

 * Do not use memset() to access IO space, use memset_io() instead.

/**

 * memset16() - Fill a memory area with a uint16_t

 * @s: Pointer to the start of the area.

 * @v: The value to fill the area with

 * @count: The number of values to store

 *

 * Differs from memset() in that it fills with a uint16_t instead

 * of a byte.  Remember that @count is the number of uint16_ts to

 * store, not the number of bytes.

/**

 * memset32() - Fill a memory area with a uint32_t

 * @s: Pointer to the start of the area.

 * @v: The value to fill the area with

 * @count: The number of values to store

 *

 * Differs from memset() in that it fills with a uint32_t instead

 * of a byte.  Remember that @count is the number of uint32_ts to

 * store, not the number of bytes.

/**

 * memset64() - Fill a memory area with a uint64_t

 * @s: Pointer to the start of the area.

 * @v: The value to fill the area with

 * @count: The number of values to store

 *

 * Differs from memset() in that it fills with a uint64_t instead

 * of a byte.  Remember that @count is the number of uint64_ts to

 * store, not the number of bytes.

/**

 * memcpy - Copy one area of memory to another

 * @dest: Where to copy to

 * @src: Where to copy from

 * @count: The size of the area.

 *

 * You should not use this function to access IO space, use memcpy_toio()

 * or memcpy_fromio() instead.

/**

 * memmove - Copy one area of memory to another

 * @dest: Where to copy to

 * @src: Where to copy from

 * @count: The size of the area.

 *

 * Unlike memcpy(), memmove() copes with overlapping areas.

/**

 * memcmp - Compare two areas of memory

 * @cs: One area of memory

 * @ct: Another area of memory

 * @count: The size of the area.

/**

 * bcmp - returns 0 if and only if the buffers have identical contents.

 * @a: pointer to first buffer.

 * @b: pointer to second buffer.

 * @len: size of buffers.

 *

 * The sign or magnitude of a non-zero return value has no particular

 * meaning, and architectures may implement their own more efficient bcmp(). So

 * while this particular implementation is a simple (tail) call to memcmp, do

 * not rely on anything but whether the return value is zero or non-zero.

/**

 * memscan - Find a character in an area of memory.

 * @addr: The memory area

 * @c: The byte to search for

 * @size: The size of the area.

 *

 * returns the address of the first occurrence of @c, or 1 byte past

 * the area if @c is not found

/**

 * strstr - Find the first substring in a %NUL terminated string

 * @s1: The string to be searched

 * @s2: The string to search for

/**

 * strnstr - Find the first substring in a length-limited string

 * @s1: The string to be searched

 * @s2: The string to search for

 * @len: the maximum number of characters to search

/**

 * memchr - Find a character in an area of memory.

 * @s: The memory area

 * @c: The byte to search for

 * @n: The size of the area.

 *

 * returns the address of the first occurrence of @c, or %NULL

 * if @c is not found

/**

 * memchr_inv - Find an unmatching character in an area of memory.

 * @start: The memory area

 * @c: Find a character other than c

 * @bytes: The size of the area.

 *

 * returns the address of the first character other than @c, or %NULL

 * if the whole buffer contains just @c.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * lib/ts_kmp.c		Knuth-Morris-Pratt text search implementation

 *

 * Authors:	Thomas Graf <tgraf@suug.ch>

 *

 * ==========================================================================

 * 

 *   Implements a linear-time string-matching algorithm due to Knuth,

 *   Morris, and Pratt [1]. Their algorithm avoids the explicit

 *   computation of the transition function DELTA altogether. Its

 *   matching time is O(n), for n being length(text), using just an

 *   auxiliary function PI[1..m], for m being length(pattern),

 *   precomputed from the pattern in time O(m). The array PI allows

 *   the transition function DELTA to be computed efficiently

 *   "on the fly" as needed. Roughly speaking, for any state

 *   "q" = 0,1,...,m and any character "a" in SIGMA, the value

 *   PI["q"] contains the information that is independent of "a" and

 *   is needed to compute DELTA("q", "a") [2]. Since the array PI

 *   has only m entries, whereas DELTA has O(m|SIGMA|) entries, we

 *   save a factor of |SIGMA| in the preprocessing time by computing

 *   PI rather than DELTA.

 *

 *   [1] Cormen, Leiserson, Rivest, Stein

 *       Introdcution to Algorithms, 2nd Edition, MIT Press

 *   [2] See finite automaton theory

/*

 * Copyright 2006, Red Hat, Inc., Dave Jones

 * Released under the General Public License (GPL).

 *

 * This file contains the linked list validation for DEBUG_LIST.

/*

 * Check that the data structures for the list manipulations are reasonably

 * valid. Failures here indicate memory corruption (and possibly an exploit

 * attempt).

 SPDX-License-Identifier: GPL-2.0

/*

 * KUnit test for the linear_ranges helper.

 *

 * Copyright (C) 2020, ROHM Semiconductors.

 * Author: Matti Vaittinen <matti.vaittien@fi.rohmeurope.com>

/* First things first. I deeply dislike unit-tests. I have seen all the hell

 * breaking loose when people who think the unit tests are "the silver bullet"

 * to kill bugs get to decide how a company should implement testing strategy...

 *

 * Believe me, it may get _really_ ridiculous. It is tempting to think that

 * walking through all the possible execution branches will nail down 100% of

 * bugs. This may lead to ideas about demands to get certain % of "test

 * coverage" - measured as line coverage. And that is one of the worst things

 * you can do.

 *

 * Ask people to provide line coverage and they do. I've seen clever tools

 * which generate test cases to test the existing functions - and by default

 * these tools expect code to be correct and just generate checks which are

 * passing when ran against current code-base. Run this generator and you'll get

 * tests that do not test code is correct but just verify nothing changes.

 * Problem is that testing working code is pointless. And if it is not

 * working, your test must not assume it is working. You won't catch any bugs

 * by such tests. What you can do is to generate a huge amount of tests.

 * Especially if you were are asked to proivde 100% line-coverage x_x. So what

 * does these tests - which are not finding any bugs now - do?

 *

 * They add inertia to every future development. I think it was Terry Pratchet

 * who wrote someone having same impact as thick syrup has to chronometre.

 * Excessive amount of unit-tests have this effect to development. If you do

 * actually find _any_ bug from code in such environment and try fixing it...

 * ...chances are you also need to fix the test cases. In sunny day you fix one

 * test. But I've done refactoring which resulted 500+ broken tests (which had

 * really zero value other than proving to managers that we do do "quality")...

 *

 * After this being said - there are situations where UTs can be handy. If you

 * have algorithms which take some input and should produce output - then you

 * can implement few, carefully selected simple UT-cases which test this. I've

 * previously used this for example for netlink and device-tree data parsing

 * functions. Feed some data examples to functions and verify the output is as

 * expected. I am not covering all the cases but I will see the logic should be

 * working.

 *

 * Here we also do some minor testing. I don't want to go through all branches

 * or test more or less obvious things - but I want to see the main logic is

 * working. And I definitely don't want to add 500+ test cases that break when

 * some simple fix is done x_x. So - let's only add few, well selected tests

 * which ensure as much logic is good as possible.

/*

 * Test Range 1:

 * selectors:	2	3	4	5	6

 * values (5):	10	20	30	40	50

 *

 * Test Range 2:

 * selectors:	7	8	9	10

 * values (4):	100	150	200	250

 2, 3, 4, 5, 6 */

 10, 20, 30, 40, 50 */

  7, 8, 9, 10 */

 100, 150, 200, 250 */

	/*

	 * Seek value greater than range max => get_selector_*_low should

	 * return Ok - but set found to false as value is not in range

 SPDX-License-Identifier: GPL-2.0-only

/*

 *	linux/lib/crc-ccitt.c

/*

 * This mysterious table is just the CRC of each possible byte. It can be

 * computed using the standard bit-at-a-time methods. The polynomial can

 * be seen in entry 128, 0x8408. This corresponds to x^0 + x^5 + x^12.

 * Add the implicit x^16, and you have the standard CRC-CCITT.

/*

 * Similar table to calculate CRC16 variant known as CRC-CCITT-FALSE

 * Reflected bits order, does not augment final value.

/**

 *	crc_ccitt - recompute the CRC (CRC-CCITT variant) for the data

 *	buffer

 *	@crc: previous CRC value

 *	@buffer: data pointer

 *	@len: number of bytes in the buffer

/**

 *	crc_ccitt_false - recompute the CRC (CRC-CCITT-FALSE variant)

 *	for the data buffer

 *	@crc: previous CRC value

 *	@buffer: data pointer

 *	@len: number of bytes in the buffer

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel module for testing copy_to/from_user infrastructure.

 *

 * Copyright 2013 Google Inc. All Rights Reserved

 *

 * Authors:

 *      Kees Cook       <keescook@chromium.org>

/*

 * Several 32-bit architectures support 64-bit {get,put}_user() calls.

 * As there doesn't appear to be anything that can safely determine

 * their capability at compile-time, we just have to opt-out certain archs.

	/*

	 * We want to cross a page boundary to exercise the code more

	 * effectively. We also don't want to make the size we scan too large,

	 * otherwise the test can take a long time and cause soft lockups. So

	 * scan a 1024 byte region across the page boundary.

	/*

	 * We conduct a series of check_nonzero_user() tests on a block of

	 * memory with the following byte-pattern (trying every possible

	 * [start,end] pair):

	 *

	 *   [ 00 ff 00 ff ... 00 00 00 00 ... ff 00 ff 00 ]

	 *

	 * And we verify that check_nonzero_user() acts identically to

	 * memchr_inv().

 Fill umem with a fixed byte pattern. */

 Check basic case -- (usize == ksize). */

 Old userspace case -- (usize < ksize). */

 New userspace (-E2BIG) case -- (usize > ksize). */

 New userspace (success) case -- (usize > ksize). */

	/*

	 * Legitimate usage: none of these copies should fail.

 Test usage of check_nonzero_user(). */

 Test usage of copy_struct_from_user(). */

	/*

	 * Invalid usage: none of these copies should succeed.

 Prepare kernel memory with check values. */

 Reject kernel-to-kernel copies through copy_from_user(). */

 Destination half of buffer should have been zeroed. */

	/*

	 * When running with SMAP/PAN/etc, this will Oops the kernel

	 * due to the zeroing of userspace memory on failure. This needs

	 * to be tested in LKDTM instead, since this test module does not

	 * expect to explode.

 SPDX-License-Identifier: GPL-2.0-only

 validate @native and @pcp counter values match @expected */

	/*

	 * volatile prevents compiler from optimizing it uses, otherwise the

	 * +ul_one/-ul_one below would replace with inc/dec instructions.

 Fail will directly unload the module */

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Test cases for the min max heap.

 Test with known set of values. */

 Test with randomly generated values. */

 Test with known set of values copied from data. */

 Test with randomly generated values. */

 Fill values with data to pop and replace. */

 Test with known set of values copied from data. */

 Test with randomly generated values. */

 do nothing */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Test cases for sscanf facility.

 Selection of interesting numbers to test, copied from test-kstrtox.c */

 should be overwritten */		\

/*

 * This gives a better variety of number "lengths" in a small sample than

 * the raw prandom*() functions (Not mathematically rigorous!!).

 * Variabilty of length and value is more important than perfect randomness.

/*

 * Define a pattern of negative and positive numbers to ensure we get

 * some of both within the small number of samples in a test string.

 00110010 01000110 */

/*

 * Convenience wrapper around snprintf() to append at buf_pos in buf,

 * updating buf_pos and returning the number of characters appended.

 * On error buf_pos is not changed and return value is 0.

/*

 * Convenience function to append the field delimiter string

 * to both the value string and format string buffers.

/*

 * List of numbers separated by delim. Each field width specifier is the

 * maximum possible digits for the given type and base.

/*

 * List of numbers separated by delim. Each field width specifier is the

 * exact length of the corresponding value digits in the string being scanned.

/*

 * Slice a continuous string of digits without field delimiters, containing

 * numbers of varying length, using the field width to extract each group

 * of digits. For example the hex values c0,3,bf01,303 would have a

 * string representation of "c03bf01303" and extracted with "%2x%1x%4x%3x".

/*

 * Number prefix is >= field width.

 * Expected behaviour is derived from testing userland sscanf.

	/*

	 * Negative decimal with a field of width 1, should quit scanning

	 * and return 0.

	/*

	 * 0x prefix in a field of width 1: 0 is a valid digit so should

	 * convert. Next field scan starts at the 'x' which isn't a digit so

	 * scan quits with one field converted.

	/*

	 * 0x prefix in a field of width 2 using %x conversion: first field

	 * converts to 0. Next field scan starts at the character after "0x".

	 * Both fields will convert.

	/*

	 * 0x prefix in a field of width 2 using %i conversion: first field

	 * converts to 0. Next field scan starts at the character after "0x",

	 * which will convert if can be interpreted as decimal but will fail

	 * if it contains any hex digits (since no 0x prefix).

 Selection of common delimiters/separators between numbers in a string. */

 String containing only one number. */

 String with multiple numbers separated by delimiter. */

 Field width may be longer than actual field digits. */

 Each field width exactly length of actual field digits. */

 Slice continuous sequence of digits using field widths. */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Intel Corporation

/* a tiny module only meant to test

 *

 *   set/clear_bit

 *   get_count_order/long

 use an enum because that's the most common BITMAP usage */

 SPDX-License-Identifier: GPL-2.0-only

	/*

	 * Limit the search scope to what the user defined.

	 * Otherwise we are merely measuring empty walks,

	 * which is pointless.

 Fail will directly unload the module */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *      crc7.c

/*

 * Table for CRC-7 (polynomial x^7 + x^3 + 1).

 * This is a big-endian CRC (msbit is highest power of x),

 * aligned so the msbit of the byte is the x^6 coefficient

 * and the lsbit is not used.

/**

 * crc7_be - update the CRC7 for the data buffer

 * @crc:     previous CRC7 value

 * @buffer:  data pointer

 * @len:     number of bytes in the buffer

 * Context: any

 *

 * Returns the updated CRC7 value.

 * The CRC7 is left-aligned in the byte (the lsbit is always 0), as that

 * makes the computation easier, and all callers want it in that form.

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * Test cases for memcpy(), memmove(), and memset().

 1 byte hole */

 Verify static initializers. */

 Verify assignment. */

 Verify complete overwrite. */

 Verify middle overwrite. */

 Verify argument side-effects aren't repeated. */

 Verify static initializers. */

 Verify assignment. */

 Verify complete overwrite. */

 Verify middle overwrite. */

 Verify argument side-effects aren't repeated. */

 Verify overlapping overwrite is correct. */

 Verify static initializers. */

 Verify assignment. */

 Verify complete overwrite. */

 Verify middle overwrite. */

 Verify argument side-effects aren't repeated. */

 Verify memset_after() */

 Verify memset_startat() */

/*

 * Test cases for lib/uuid.c module.

 no hyphen(s) */

 invalid character(s) */

 not enough data */

 LE */

 BE */

 LE */

 BE */

 do nothing */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This module provides an interface to trigger and test firmware loading.

 *

 * It is designed to be used for basic evaluation of the firmware loading

 * subsystem (for example when validating firmware verification). It lacks

 * any extra dependencies, and will not normally be loaded by the system

 * unless explicitly requested by name.

/**

 * test_config - represents configuration for the test for different triggers

 *

 * @name: the name of the firmware file to look for

 * @into_buf: when the into_buf is used if this is true

 *	request_firmware_into_buf() will be used instead.

 * @buf_size: size of buf to allocate when into_buf is true

 * @file_offset: file offset to request when calling request_firmware_into_buf

 * @partial: partial read opt when calling request_firmware_into_buf

 * @sync_direct: when the sync trigger is used if this is true

 *	request_firmware_direct() will be used instead.

 * @send_uevent: whether or not to send a uevent for async requests

 * @num_requests: number of requests to try per test case. This is trigger

 *	specific.

 * @reqs: stores all requests information

 * @read_fw_idx: index of thread from which we want to read firmware results

 *	from through the read_fw trigger.

 * @test_result: a test may use this to collect the result from the call

 *	of the request_firmware*() calls used in their tests. In order of

 *	priority we always keep first any setup error. If no setup errors were

 *	found then we move on to the first error encountered while running the

 *	API. Note that for async calls this typically will be a successful

 *	result (0) unless of course you've used bogus parameters, or the system

 *	is out of memory.  In the async case the callback is expected to do a

 *	bit more homework to figure out what happened, unfortunately the only

 *	information passed today on error is the fact that no firmware was

 *	found so we can only assume -ENOENT on async calls if the firmware is

 *	NULL.

 *

 *	Errors you can expect:

 *

 *	API specific:

 *

 *	0:		success for sync, for async it means request was sent

 *	-EINVAL:	invalid parameters or request

 *	-ENOENT:	files not found

 *

 *	System environment:

 *

 *	-ENOMEM:	memory pressure on system

 *	-ENODEV:	out of number of devices to test

 *	-EINVAL:	an unexpected error has occurred

 * @req_firmware: if @sync_direct is true this is set to

 *	request_firmware_direct(), otherwise request_firmware()

	/*

	 * These below don't belong her but we'll move them once we create

	 * a struct fw_test_device and stuff the misc_dev under there later.

/*

 * XXX: move to kstrncpy() once merged.

 *

 * Users should use kfree_const() when freeing these.

/*

 * As per sysfs_kf_seq_show() the buf is max PAGE_SIZE.

 Always return full write size even if we didn't consume all */

 Always return full write size even if we didn't consume all */

 Free 'name' ASAP, to test for race conditions */

 Free 'name' ASAP, to test for race conditions */

/*

 * We use a kthread as otherwise the kernel serializes all our sync requests

 * and we would not be able to mimic batched requests on a sync call. Batched

 * requests on a sync call can for instance happen on a device driver when

 * multiple cards are used and firmware loading happens outside of probe.

	/*

	 * We require an explicit release to enable more time and delay of

	 * calling release_firmware() to improve our chances of forcing a

	 * batched request. If we instead called release_firmware() right away

	 * then we might miss on an opportunity of having a successful firmware

	 * request pass on the opportunity to be come a batched request.

 Override any worker error if we had a general setup error */

/*

 * We wait for each callback to return with the lock held, no need to lock here

 forces *some* batched requests to queue up */

	/*

	 * Unfortunately the firmware API gives us nothing other than a null FW

	 * if the firmware was not found on async requests.  Best we can do is

	 * just assume -ENOENT. A better API would pass the actual return

	 * value to the callback.

	/*

	 * We require an explicit release to enable more time and delay of

	 * calling release_firmware() to improve our chances of forcing a

	 * batched request. If we instead called release_firmware() right away

	 * then we might miss on an opportunity of having a successful firmware

	 * request pass on the opportunity to be come a batched request.

 Override any worker error if we had a general setup error */

 These don't use the config at all - they could be ported! */

 These use the config and can use the test_result */

 SPDX-License-Identifier: GPL-2.0-only

 a simple boot-time regression test */

 SPDX-License-Identifier: GPL-2.0-only

 following fields used for testing augmented rbtree functionality */

 run checks */

 Fail will directly unload the module */

 SPDX-License-Identifier: GPL-2.0

/*

 * KUnit test for the Kernel Linked-list structures.

 *

 * Copyright (C) 2019, Google LLC.

 * Author: David Gow <davidgow@google.com>

 Test the different ways of initialising a list. */

 list_empty_careful() checks both next and prev. */

 should be [list] -> b -> a */

 should be [list] -> a -> b */

 before: [list] -> a -> b */

 now: [list] -> b */

 before: [list] -> a_old -> b */

 now: [list] -> a_new -> b */

 before: [list] -> a_old -> b */

 now: [list] -> a_new -> b */

 check a_old is empty (initialized) */

 before: [list] -> a -> b */

 after: [list] -> b -> a */

 before: [list] -> a -> b */

 after: [list] -> b, a initialised */

 before: [list1] -> a, [list2] -> b */

 after: [list1] empty, [list2] -> a -> b */

 before: [list1] -> a, [list2] -> b */

 after: [list1] empty, [list2] -> b -> a */

 before: [list1] -> x -> y, [list2] -> a -> b -> c -> d */

 after: [list1] -> x -> b -> c -> y, [list2] -> a -> d */

 This test doesn't check correctness under concurrent access */

 before: [list] -> a -> b */

 after: [list] -> b -> a */

 before: [list] -> a -> b -> c -> d */

 after: [list] -> c -> d -> a -> b */

 [list] empty */

 [list] -> a */

 [list] -> a -> b */

 before: [list1] -> entries[0] -> entries[1] -> entries[2] */

 after: [list2] -> entries[0] -> entries[1], [list1] -> entries[2] */

 before: [list1] -> entries[0] -> entries[1] -> entries[2] */

 after: [list2] -> entries[0], [list1] -> entries[1] -> entries[2] */

 before: [list1]->e[0]->e[1]->e[4], [list2]->e[2]->e[3] */

 after: [list1]->e[0]->e[1]->e[2]->e[3]->e[4], [list2] uninit */

 before: [list1]->e[0]->e[1]->e[4], [list2]->e[2]->e[3] */

 after: [list1]->e[0]->e[1]->e[2]->e[3]->e[4], [list2] uninit */

 before: [list1]->e[0]->e[1]->e[4], [list2]->e[2]->e[3] */

 after: [list1]->e[0]->e[1]->e[2]->e[3]->e[4], [list2] empty */

 before: [list1]->e[0]->e[1]->e[4], [list2]->e[2]->e[3] */

 after: [list1]->e[0]->e[1]->e[2]->e[3]->e[4], [list2] empty */

 SPDX-License-Identifier: GPL-2.0-or-later

/* ASN.1 Object identifier (OID) registry

 *

 * Copyright (C) 2012 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

/**

 * look_up_OID - Find an OID registration for the specified data

 * @data: Binary representation of the OID

 * @datasize: Size of the binary representation

 Hash the OID data */

	/* Binary search the OID registry.  OIDs are stored in ascending order

	 * of hash value then ascending order of size and then in ascending

	 * order of reverse value.

		/* Variation is most likely to be at the tail end of the

		 * OID, so do the comparison in reverse.

/**

 * parse_OID - Parse an OID from a bytestream

 * @data: Binary representation of the header + OID

 * @datasize: Size of the binary representation

 * @oid: Pointer to oid to return result

 *

 * Parse an OID from a bytestream that holds the OID in the format

 * ASN1_OID | length | oid. The length indicator must equal to datasize - 2.

 * -EBADMSG is returned if the bytestream is too short.

 we need 2 bytes of header and at least 1 byte for oid */

/*

 * sprint_OID - Print an Object Identifier into a buffer

 * @data: The encoded OID to print

 * @datasize: The size of the encoded OID

 * @buffer: The buffer to render into

 * @bufsize: The size of the buffer

 *

 * The OID is rendered into the buffer in "a.b.c.d" format and the number of

 * bytes is returned.  -EBADMSG is returned if the data could not be interpreted

 * and -ENOBUFS if the buffer was too small.

/**

 * sprint_OID - Print an Object Identifier into a buffer

 * @oid: The OID to print

 * @buffer: The buffer to render into

 * @bufsize: The size of the buffer

 *

 * The OID is rendered into the buffer in "a.b.c.d" format and the number of

 * bytes is returned.

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0

/*

 * Returns a list organized in an intermediate format suited

 * to chaining of merge() calls: null-terminated, no reserved or

 * sentinel head node, "prev" links not maintained.

 if equal, take 'a' -- important for sort stability */

/*

 * Combine final list merge with restoration of standard doubly-linked

 * list structure.  This approach duplicates code from merge(), but

 * runs faster than the tidier alternatives of either a separate final

 * prev-link restoration pass, or maintaining the prev links

 * throughout.

 if equal, take 'a' -- important for sort stability */

 Finish linking remainder of list b on to tail */

		/*

		 * If the merge is highly unbalanced (e.g. the input is

		 * already sorted), this loop may run many iterations.

		 * Continue callbacks to the client even though no

		 * element comparison is needed, so the client's cmp()

		 * routine can invoke cond_resched() periodically.

 And the final links to make a circular doubly-linked list */

/**

 * list_sort - sort a list

 * @priv: private data, opaque to list_sort(), passed to @cmp

 * @head: the list to sort

 * @cmp: the elements comparison function

 *

 * The comparison function @cmp must return > 0 if @a should sort after

 * @b ("@a > @b" if you want an ascending sort), and <= 0 if @a should

 * sort before @b *or* their original order should be preserved.  It is

 * always called with the element that came first in the input in @a,

 * and list_sort is a stable sort, so it is not necessary to distinguish

 * the @a < @b and @a == @b cases.

 *

 * This is compatible with two styles of @cmp function:

 * - The traditional style which returns <0 / =0 / >0, or

 * - Returning a boolean 0/1.

 * The latter offers a chance to save a few cycles in the comparison

 * (which is used by e.g. plug_ctx_cmp() in block/blk-mq.c).

 *

 * A good way to write a multi-word comparison is::

 *

 *	if (a->high != b->high)

 *		return a->high > b->high;

 *	if (a->middle != b->middle)

 *		return a->middle > b->middle;

 *	return a->low > b->low;

 *

 *

 * This mergesort is as eager as possible while always performing at least

 * 2:1 balanced merges.  Given two pending sublists of size 2^k, they are

 * merged to a size-2^(k+1) list as soon as we have 2^k following elements.

 *

 * Thus, it will avoid cache thrashing as long as 3*2^k elements can

 * fit into the cache.  Not quite as good as a fully-eager bottom-up

 * mergesort, but it does use 0.2*n fewer comparisons, so is faster in

 * the common case that everything fits into L1.

 *

 *

 * The merging is controlled by "count", the number of elements in the

 * pending lists.  This is beautifully simple code, but rather subtle.

 *

 * Each time we increment "count", we set one bit (bit k) and clear

 * bits k-1 .. 0.  Each time this happens (except the very first time

 * for each bit, when count increments to 2^k), we merge two lists of

 * size 2^k into one list of size 2^(k+1).

 *

 * This merge happens exactly when the count reaches an odd multiple of

 * 2^k, which is when we have 2^k elements pending in smaller lists,

 * so it's safe to merge away two lists of size 2^k.

 *

 * After this happens twice, we have created two lists of size 2^(k+1),

 * which will be merged into a list of size 2^(k+2) before we create

 * a third list of size 2^(k+1), so there are never more than two pending.

 *

 * The number of pending lists of size 2^k is determined by the

 * state of bit k of "count" plus two extra pieces of information:

 *

 * - The state of bit k-1 (when k == 0, consider bit -1 always set), and

 * - Whether the higher-order bits are zero or non-zero (i.e.

 *   is count >= 2^(k+1)).

 *

 * There are six states we distinguish.  "x" represents some arbitrary

 * bits, and "y" represents some arbitrary non-zero bits:

 * 0:  00x: 0 pending of size 2^k;           x pending of sizes < 2^k

 * 1:  01x: 0 pending of size 2^k; 2^(k-1) + x pending of sizes < 2^k

 * 2: x10x: 0 pending of size 2^k; 2^k     + x pending of sizes < 2^k

 * 3: x11x: 1 pending of size 2^k; 2^(k-1) + x pending of sizes < 2^k

 * 4: y00x: 1 pending of size 2^k; 2^k     + x pending of sizes < 2^k

 * 5: y01x: 2 pending of size 2^k; 2^(k-1) + x pending of sizes < 2^k

 * (merge and loop back to state 2)

 *

 * We gain lists of size 2^k in the 2->3 and 4->5 transitions (because

 * bit k-1 is set while the more significant bits are non-zero) and

 * merge them away in the 5->2 transition.  Note in particular that just

 * before the 5->2 transition, all lower-order bits are 11 (state 3),

 * so there is one list of each smaller size.

 *

 * When we reach the end of the input, we merge all the pending

 * lists, from smallest to largest.  If you work through cases 2 to

 * 5 above, you can see that the number of elements we merge with a list

 * of size 2^k varies from 2^(k-1) (cases 3 and 5 when x == 0) to

 * 2^(k+1) - 1 (second merge of case 5 when x == 2^(k-1) - 1).

 Count of pending */

 Zero or one elements */

 Convert to a null-terminated singly-linked list. */

	/*

	 * Data structure invariants:

	 * - All lists are singly linked and null-terminated; prev

	 *   pointers are not maintained.

	 * - pending is a prev-linked "list of lists" of sorted

	 *   sublists awaiting further merging.

	 * - Each of the sorted sublists is power-of-two in size.

	 * - Sublists are sorted by size and age, smallest & newest at front.

	 * - There are zero to two sublists of each size.

	 * - A pair of pending sublists are merged as soon as the number

	 *   of following pending elements equals their size (i.e.

	 *   each time count reaches an odd multiple of that size).

	 *   That ensures each later final merge will be at worst 2:1.

	 * - Each round consists of:

	 *   - Merging the two sublists selected by the highest bit

	 *     which flips when count is incremented, and

	 *   - Adding an element from the input as a size-1 sublist.

 Find the least-significant clear bit in count */

 Do the indicated merge */

 Install the merged result in place of the inputs */

 Move one element from input list to pending */

 End of input; merge together all the pending lists. */

 The final merge, rebuilding prev links */

/*

 * Aug 8, 2011 Bob Pearson with help from Joakim Tjernlund and George Spelvin

 * cleaned up code to current version of sparse and added the slicing-by-8

 * algorithm to the closely similar existing slicing-by-4 algorithm.

 *

 * Oct 15, 2000 Matt Domsch <Matt_Domsch@dell.com>

 * Nicer crc32 functions/docs submitted by linux@horizon.com.  Thanks!

 * Code was from the public domain, copyright abandoned.  Code was

 * subsequently included in the kernel, thus was re-licensed under the

 * GNU GPL v2.

 *

 * Oct 12, 2000 Matt Domsch <Matt_Domsch@dell.com>

 * Same crc32 function was used in 5 other places in the kernel.

 * I made one version, and deleted the others.

 * There are various incantations of crc32().  Some use a seed of 0 or ~0.

 * Some xor at the end with ~0.  The generic crc32() function takes

 * seed as an argument, and doesn't xor at the end.  Then individual

 * users can do whatever they need.

 *   drivers/net/smc9194.c uses seed ~0, doesn't xor with ~0.

 *   fs/jffs2 uses seed 0, doesn't xor with ~0.

 *   fs/partitions/efi.c uses seed ~0, xor's with ~0.

 *

 * This source code is licensed under the GNU General Public License,

 * Version 2.  See the file COPYING for more details.

 4096 random bytes */

 100 test cases */

 random starting crc */

 random 6 bit offset in buf */

 random 11 bit length of test */

 expected crc32_le result */

 expected crc32_be result */

 expected crc32c_le result */

	/* keep static to prevent cache warming code from

 pre-warm the cache */

 reduce OS noise */

	/* keep static to prevent cache warming code from

 pre-warm the cache */

 reduce OS noise */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Resizable, Scalable, Concurrent Hash Table

 *

 * Copyright (c) 2014-2015 Thomas Graf <tgraf@suug.ch>

 * Copyright (c) 2008-2014 Patrick McHardy <kaber@trash.net>

/**************************************************************************

 * Self Test

	/*

	 * Insertion Test:

	 * Insert entries into table with all keys even numbers

 Take the mutex to avoid RCU warning */

 two different values that map to same bucket */

	/* and another duplicate with same as [0] value

 count is 0 now, set it to -1 and wake up all threads together */

	/*

	 * rhltable_remove is very expensive, default values can cause test

	 * to run for 2 minutes or more,  use a smaller number instead.

 SPDX-License-Identifier: GPL-2.0

/*

 * An errseq_t is a way of recording errors in one place, and allowing any

 * number of "subscribers" to tell whether it has changed since a previous

 * point where it was sampled.

 *

 * It's implemented as an unsigned 32-bit value. The low order bits are

 * designated to hold an error code (between 0 and -MAX_ERRNO). The upper bits

 * are used as a counter. This is done with atomics instead of locking so that

 * these functions can be called from any context.

 *

 * The general idea is for consumers to sample an errseq_t value. That value

 * can later be used to tell whether any new errors have occurred since that

 * sampling was done.

 *

 * Note that there is a risk of collisions if new errors are being recorded

 * frequently, since we have so few bits to use as a counter.

 *

 * To mitigate this, one bit is used as a flag to tell whether the value has

 * been sampled since a new value was recorded. That allows us to avoid bumping

 * the counter if no one has sampled it since the last time an error was

 * recorded.

 *

 * A new errseq_t should always be zeroed out.  A errseq_t value of all zeroes

 * is the special (but common) case where there has never been an error. An all

 * zero value thus serves as the "epoch" if one wishes to know whether there

 * has ever been an error set since it was first initialized.

 The low bits are designated for error code (max of MAX_ERRNO) */

 This bit is used as a flag to indicate whether the value has been seen */

 The lowest bit of the counter */

/**

 * errseq_set - set a errseq_t for later reporting

 * @eseq: errseq_t field that should be set

 * @err: error to set (must be between -1 and -MAX_ERRNO)

 *

 * This function sets the error in @eseq, and increments the sequence counter

 * if the last sequence was sampled at some point in the past.

 *

 * Any error set will always overwrite an existing error.

 *

 * Return: The previous value, primarily for debugging purposes. The

 * return value should not be used as a previously sampled value in later

 * calls as it will not have the SEEN flag set.

 MAX_ERRNO must be able to serve as a mask */

	/*

	 * Ensure the error code actually fits where we want it to go. If it

	 * doesn't then just throw a warning and don't record anything. We

	 * also don't accept zero here as that would effectively clear a

	 * previous error.

 Clear out error bits and set new error */

 Only increment if someone has looked at it */

 If there would be no change, then call it done */

 Try to swap the new value into place */

		/*

		 * Call it success if we did the swap or someone else beat us

		 * to it for the same value.

 Raced with an update, try again */

/**

 * errseq_sample() - Grab current errseq_t value.

 * @eseq: Pointer to errseq_t to be sampled.

 *

 * This function allows callers to initialise their errseq_t variable.

 * If the error has been "seen", new callers will not see an old error.

 * If there is an unseen error in @eseq, the caller of this function will

 * see it the next time it checks for an error.

 *

 * Context: Any context.

 * Return: The current errseq value.

 If nobody has seen this error yet, then we can be the first. */

/**

 * errseq_check() - Has an error occurred since a particular sample point?

 * @eseq: Pointer to errseq_t value to be checked.

 * @since: Previously-sampled errseq_t from which to check.

 *

 * Grab the value that eseq points to, and see if it has changed @since

 * the given value was sampled. The @since value is not advanced, so there

 * is no need to mark the value as seen.

 *

 * Return: The latest error set in the errseq_t or 0 if it hasn't changed.

/**

 * errseq_check_and_advance() - Check an errseq_t and advance to current value.

 * @eseq: Pointer to value being checked and reported.

 * @since: Pointer to previously-sampled errseq_t to check against and advance.

 *

 * Grab the eseq value, and see whether it matches the value that @since

 * points to. If it does, then just return 0.

 *

 * If it doesn't, then the value has changed. Set the "seen" flag, and try to

 * swap it into place as the new eseq value. Then, set that value as the new

 * "since" value, and return whatever the error portion is set to.

 *

 * Note that no locking is provided here for concurrent updates to the "since"

 * value. The caller must provide that if necessary. Because of this, callers

 * may want to do a lockless errseq_check before taking the lock and calling

 * this.

 *

 * Return: Negative errno if one has been stored, or 0 if no new error has

 * occurred.

	/*

	 * Most callers will want to use the inline wrapper to check this,

	 * so that the common case of no error is handled without needing

	 * to take the lock that protects the "since" value.

		/*

		 * Set the flag and try to swap it into place if it has

		 * changed.

		 *

		 * We don't care about the outcome of the swap here. If the

		 * swap doesn't occur, then it has either been updated by a

		 * writer who is altering the value in some way (updating

		 * counter or resetting the error), or another reader who is

		 * just setting the "seen" flag. Either outcome is OK, and we

		 * can advance "since" and return an error based on what we

		 * have.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Unified UUID/GUID definition

 *

 * Copyright (C) 2009, 2016 Intel Corp.

 *	Huang Ying <ying.huang@intel.com>

/**

 * generate_random_uuid - generate a random UUID

 * @uuid: where to put the generated UUID

 *

 * Random UUID interface

 *

 * Used to create a Boot ID or a filesystem UUID/GUID, but can be

 * useful for other kernel drivers.

 Set UUID version to 4 --- truly random generation */

 Set the UUID variant to DCE */

 Set GUID version to 4 --- truly random generation */

 Set the GUID variant to DCE */

 reversion 0b10 */

 version 4 : random generation */

 version 4 : random generation */

/**

 * uuid_is_valid - checks if a UUID string is valid

 * @uuid:	UUID string to check

 *

 * Description:

 * It checks if the UUID string is following the format:

 *	xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx

 *

 * where x is a hex digit.

 *

 * Return: true if input is valid UUID string.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel module for testing static keys.

 *

 * Copyright 2015 Akamai Technologies Inc. All Rights Reserved

 *

 * Authors:

 *      Jason Baron       <jbaron@akamai.com>

 old keys */

 new keys */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * lib/clz_ctz.c

 *

 * Copyright (C) 2013 Chanho Min <chanho.min@lge.com>

 *

 * The functions in this file aren't called directly, but are required by

 * GCC builtins such as __builtin_ctz, and therefore they can't be removed

 * despite appearing unreferenced in kernel source.

 *

 * __c[lt]z[sd]i2 can be overridden by linking arch-specific versions.

 SPDX-License-Identifier: GPL-2.0-only

 for now */

 covers iovec and kvec alike */

 first chunk, usually the only one */

 Too bad - revert to non-atomic kmap */

 first chunk, usually the only one */

 Too bad - revert to non-atomic kmap */

 pipe must be non-empty

 must be at the last buffer...

 ... at the end of segment

 must be right after the last buffer

 merge with the last one */

/*

 * fault_in_iov_iter_readable - fault in iov iterator for reading

 * @i: iterator

 * @size: maximum length

 *

 * Fault in one or more iovecs of the given iov_iter, to a maximum length of

 * @size.  For each iovec, fault in each page that constitutes the iovec.

 *

 * Returns the number of bytes not faulted in (like copy_to_user() and

 * copy_from_user()).

 *

 * Always returns 0 for non-userspace iterators.

/*

 * fault_in_iov_iter_writeable - fault in iov iterator for writing

 * @i: iterator

 * @size: maximum length

 *

 * Faults in the iterator using get_user_pages(), i.e., without triggering

 * hardware page faults.  This is primarily useful when we already know that

 * some or all of the pages in @i aren't in memory.

 *

 * Returns the number of bytes not faulted in, like copy_to_user() and

 * copy_from_user().

 *

 * Always returns 0 for non-user-space iterators.

/**

 * _copy_mc_to_iter - copy to iter with source memory error exception handling

 * @addr: source kernel address

 * @bytes: total transfer length

 * @i: destination iterator

 *

 * The pmem driver deploys this for the dax operation

 * (dax_copy_to_iter()) for dax reads (bypass page-cache and the

 * block-layer). Upon #MC read(2) aborts and returns EIO or the bytes

 * successfully copied.

 *

 * The main differences between this and typical _copy_to_iter().

 *

 * * Typical tail/residue handling after a fault retries the copy

 *   byte-by-byte until the fault happens again. Re-triggering machine

 *   checks is potentially fatal so the implementation uses source

 *   alignment and poison alignment assumptions to avoid re-triggering

 *   hardware exceptions.

 *

 * * ITER_KVEC, ITER_PIPE, and ITER_BVEC can return short copies.

 *   Compare to copy_to_iter() where only ITER_IOVEC attempts might return

 *   a short copy.

 *

 * Return: number of bytes copied (may be %0)

 CONFIG_ARCH_HAS_COPY_MC */

/**

 * _copy_from_iter_flushcache - write destination through cpu cache

 * @addr: destination kernel address

 * @bytes: total transfer length

 * @i: source iterator

 *

 * The pmem driver arranges for filesystem-dax to use this facility via

 * dax_copy_from_iter() for ensuring that writes to persistent memory

 * are flushed through the CPU cache. It is differentiated from

 * _copy_from_iter_nocache() in that guarantees all data is flushed for

 * all iterator types. The _copy_from_iter_nocache() only attempts to

 * bypass the cache for the ITER_IOVEC case, and on some archs may use

 * instructions that strand dirty-data in the cache.

 *

 * Return: number of bytes copied (may be %0)

	/*

	 * The general case needs to access the page order in order

	 * to compute the page size.

	 * However, we mostly deal with order-0 pages and thus can

	 * avoid a possible cache line miss for requests that fit all

	 * page orders.

 first subpage

 make it relative to the beginning of buffer */

 ... and discard everything past that point */

 from beginning of current segment

 iovec and kvec have identical layouts */

		BUG(); /* We should never go beyond the start of the specified

			* range since we might then be straying into pages that

			* aren't pinned.

 same logics for iovec and kvec */

/*

 * Return the count of just the current iov_iter segment.

/**

 * iov_iter_xarray - Initialise an I/O iterator to use the pages in an xarray

 * @i: The iterator to initialise.

 * @direction: The direction of the transfer.

 * @xarray: The xarray to access.

 * @start: The start file position.

 * @count: The size of the I/O buffer in bytes.

 *

 * Set up an I/O iterator to either draw data out of the pages attached to an

 * inode or to inject data into those pages.  The pages *must* be prevented

 * from evaporation, either by taking a ref on them or locking them by the

 * caller.

/**

 * iov_iter_discard - Initialise an I/O iterator that discards data

 * @i: The iterator to initialise.

 * @direction: The direction of the transfer.

 * @count: The size of the I/O buffer in bytes.

 *

 * Set up an I/O iterator that just discards everything that's written to it.

 * It's only available as a READ iterator.

 iovec and kvec have identical layouts */

 if not the first one

 this start | previous end

 Amount of free space: some of this one + all after this one */

 Has the page moved or been split? */

 must be done on non-empty ITER_IOVEC one */

 if it had been empty, we wouldn't get called

 must be done on non-empty ITER_BVEC one */

 Amount of free space: some of this one + all after this one */

 for now */

 iovec and kvec have identical layouts */

 some of this one + all after this one */

 iovec and kvec have identical layout */

 check for compat_size_t not fitting in compat_ssize_t .. */

	/*

	 * SuS says "The readv() function *may* fail if the iovcnt argument was

	 * less than or equal to 0, or greater than {IOV_MAX}.  Linux has

	 * traditionally returned zero for zero segments, so...

	/*

	 * According to the Single Unix Specification we should return EINVAL if

	 * an element length is < 0 when cast to ssize_t or if the total length

	 * would overflow the ssize_t return value of the system call.

	 *

	 * Linux caps all read/write calls to MAX_RW_COUNT, and avoids the

	 * overflow case.

/**

 * import_iovec() - Copy an array of &struct iovec from userspace

 *     into the kernel, check that it is valid, and initialize a new

 *     &struct iov_iter iterator to access it.

 *

 * @type: One of %READ or %WRITE.

 * @uvec: Pointer to the userspace array.

 * @nr_segs: Number of elements in userspace array.

 * @fast_segs: Number of elements in @iov.

 * @iovp: (input and output parameter) Pointer to pointer to (usually small

 *     on-stack) kernel array.

 * @i: Pointer to iterator that will be initialized on success.

 *

 * If the array pointed to by *@iov is large enough to hold all @nr_segs,

 * then this function places %NULL in *@iov on return. Otherwise, a new

 * array will be allocated and the result placed in *@iov. This means that

 * the caller may call kfree() on *@iov regardless of whether the small

 * on-stack array was used or not (and regardless of whether this function

 * returns an error or not).

 *

 * Return: Negative error code on error, bytes imported on success

/**

 * iov_iter_restore() - Restore a &struct iov_iter to the same state as when

 *     iov_iter_save_state() was called.

 *

 * @i: &struct iov_iter to restore

 * @state: state to restore from

 *

 * Used after iov_iter_save_state() to bring restore @i, if operations may

 * have advanced it.

 *

 * Note: only works on ITER_IOVEC, ITER_BVEC, and ITER_KVEC

	/*

	 * For the *vec iters, nr_segs + iov is constant - if we increment

	 * the vec, then we also decrement the nr_segs count. Hence we don't

	 * need to track both of these, just one is enough and we can deduct

	 * the other from that. ITER_KVEC and ITER_IOVEC are the same struct

	 * size, so we can just increment the iov pointer as they are unionzed.

	 * ITER_BVEC _may_ be the same size on some archs, but on others it is

	 * not. Be safe and handle it separately.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Testsuite for BPF interpreter and BPF JIT compiler

 *

 * Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com

 General test specific settings */

 Few constants used to init test 'skb' */

 Redefine REGs to make tests less verbose */

 Flags that can be passed to test cases */

 Old BPF instructions only. */

 Extended instruction set.  */

 used when FLAG_EXPECTED_FAIL is set in the aux */

 for eBPF only, since tests don't call verifier */

 Custom run count, defaults to MAX_TESTRUNS if 0 */

 Large test cases need separate allocation and fill handler. */

 Hits 70 passes on x86_64 and triggers NOPs padding. */

 Hits exactly 11 passes on x86_64 JIT. */

/*

 * Branch conversion tests. Complex operations can expand to a lot

 * of instructions when JITed. This in turn may cause jump offsets

 * to overflow the field size of the native instruction, triggering

 * a branch conversion mechanism in some JITs.

 Branch taken by runtime decision */

 Branch not taken by runtime decision */

 Branch always taken, known at JIT time */

 Branch never taken, known at JIT time */

 ALU result computation used in tests */

 Test an ALU shift operation for all valid shift values */

 dword > 0, word < 0 */

 dowrd < 0, word > 0 */

 dowrd < 0, word < 0 */

 dword > 0, word > 0 */

 Perform operation */

			/*

			 * When debugging a JIT that fails this test, one

			 * can write the immediate value to R0 here to find

			 * out which operand values that fail.

 Load reference and check the result */

/*

 * Test an ALU register shift operation for all valid shift values

 * for the case when the source and destination are the same.

 Perform operation */

 Compute the reference result */

 Check the actual result */

/*

 * Common operand pattern generator for exhaustive power-of-two magnitudes

 * tests. The block size parameters can be adjusted to increase/reduce the

 * number of combinatons tested and thereby execution speed and memory

 * footprint.

 Total number of iterations for the two pattern */

 Compute the maximum number of insns and allocate the buffer */

 Add head instruction(s) */

	/*

	 * Pattern 1: all combinations of power-of-two magnitudes and sign,

	 * and with a block of contiguous values around each magnitude.

 Dst magnitudes */

 Src magnitudes */

 Sign combos */

	/*

	 * Pattern 2: all combinations for a larger block of values

	 * for each power-of-two magnitude and sign, where the magnitude is

	 * the same for both operands.

 Magnitude   */

 Sign combos */

 Append tail instructions */

/*

 * Block size parameters used in pattern tests below. une as needed to

 * increase/reduce the number combinations tested, see following examples.

 *        block   values per operand MSB

 * ----------------------------------------

 *           0     none

 *           1     (1 << MSB)

 *           2     (1 << MSB) + [-1, 0]

 *           3     (1 << MSB) + [-1, 0, 1]

 Number of test runs for a pattern test */

/*

 * Exhaustive tests of ALU operations for all combinations of power-of-two

 * magnitudes of the operands, both for positive and negative values. The

 * test is designed to verify e.g. the ALU and ALU64 operations for JITs that

 * emit different code depending on the magnitude of the immediate value.

 ALU64 immediate operations */

 ALU32 immediate operations */

 ALU64 register operations */

 ALU32 register operations */

/*

 * Test JITs that implement complex ALU operations as function

 * calls, and must re-arrange operands for argument passing.

 Operand and result values according to operation */

 Check all operand registers */

 ALU64 K registers */

 ALU32 K registers */

/*

 * Test JITs that implement complex ALU operations as function

 * calls, and must re-arrange operands for argument passing.

 Operand and result values according to operation */

 Check all combinations of operand registers */

 ALU64 X register combinations */

 ALU32 X register combinations */

/*

 * Exhaustive tests of atomic operations for all power-of-two operand

 * magnitudes, both for positive and negative values.

 Result unsuccessful */

 Result successful */

 Result unsuccessful */

 Zext always inserted by verifier */

 Result successful */

 Zext always inserted by verifier */

 64-bit atomic operations */

 32-bit atomic operations */

/*

 * Test JITs that implement ATOMIC operations as function calls or

 * other primitives, and must re-arrange operands for argument passing.

 Operand and memory values */

 BPF_W */

 Memory updated according to operation */

 Test all operand registers */

 Initialize value in memory */

 Initialize registers in order */

 Perform atomic operation */

 Check R0 register value */

 Expect value from memory */

 Aliased, checked below */

 Expect value to be preserved */

 Check source register value */

 Aliased with R0, checked above */

 Aliased with rd, checked below */

 Expect value to be preserved */

 Expect fetched value from mem */

 no fetch */

 Expect value to be preserved */

 Check destination register value */

 Check value in memory */

 No aliasing */

 Aliased, XCHG */

 Aliased, CMPXCHG */

 Aliased, ALU oper */

 width == BPF_W */

 64-bit atomic register tests */

 32-bit atomic register tests */

/*

 * Test the two-instruction 64-bit immediate load operation for all

 * power-of-two magnitudes of the immediate operand. For each MSB, a block

 * of immediate values centered around the power-of-two MSB are tested,

 * both for positive and negative values. The test is designed to verify

 * the operation for JITs that emit different code depending on the magnitude

 * of the immediate value. This is often the case if the native instruction

 * immediate field width is narrower than 32 bits.

 Increase for more tests per MSB position */

 Perform operation */

 Load reference */

 Check result */

/*

 * Test the two-instruction 64-bit immediate load operation for different

 * combinations of bytes. Each byte in the 64-bit word is constructed as

 * (base & mask) | (rand() & ~mask), where rand() is a deterministic LCG.

 * All patterns (base1, mask1) and (base2, mask2) bytes are tested.

 Update our LCG */

 Perform operation */

 Load reference */

 Check result */

/*

 * Exhaustive tests of JMP operations for all combinations of power-of-two

 * magnitudes of the operands, both for positive and negative values. The

 * test is designed to verify e.g. the JMP and JMP32 operations for JITs that

 * emit different code depending on the magnitude of the immediate value.

 JMP immediate tests */

 JMP32 immediate tests */

 JMP register tests */

 JMP32 register tests */

/*

 * Set up a sequence of staggered jumps, forwards and backwards with

 * increasing offset. This tests the conversion of relative jumps to

 * JITed native jumps. On some architectures, for example MIPS, a large

 * PC-relative jump offset may overflow the immediate field of the native

 * conditional branch instruction, triggering a conversion to use an

 * absolute jump instead. Since this changes the jump offsets, another

 * offset computation pass is necessary, and that may in turn trigger

 * another branch conversion. This jump sequence is particularly nasty

 * in that regard.

 *

 * The sequence generation is parameterized by size and jump type.

 * The size must be even, and the expected result is always size + 1.

 * Below is an example with size=8 and result=9.

 *

 *                     ________________________Start

 *                     R0 = 0

 *                     R1 = r1

 *                     R2 = r2

 *            ,------- JMP +4 * 3______________Preamble: 4 insns

 * ,----------|-ind 0- if R0 != 7 JMP 8 * 3 + 1 <--------------------.

 * |          |        R0 = 8                                        |

 * |          |        JMP +7 * 3               ------------------------.

 * | ,--------|-----1- if R0 != 5 JMP 7 * 3 + 1 <--------------.     |  |

 * | |        |        R0 = 6                                  |     |  |

 * | |        |        JMP +5 * 3               ------------------.  |  |

 * | | ,------|-----2- if R0 != 3 JMP 6 * 3 + 1 <--------.     |  |  |  |

 * | | |      |        R0 = 4                            |     |  |  |  |

 * | | |      |        JMP +3 * 3               ------------.  |  |  |  |

 * | | | ,----|-----3- if R0 != 1 JMP 5 * 3 + 1 <--.     |  |  |  |  |  |

 * | | | |    |        R0 = 2                      |     |  |  |  |  |  |

 * | | | |    |        JMP +1 * 3               ------.  |  |  |  |  |  |

 * | | | | ,--t=====4> if R0 != 0 JMP 4 * 3 + 1    1  2  3  4  5  6  7  8 loc

 * | | | | |           R0 = 1                     -1 +2 -3 +4 -5 +6 -7 +8 off

 * | | | | |           JMP -2 * 3               ---'  |  |  |  |  |  |  |

 * | | | | | ,------5- if R0 != 2 JMP 3 * 3 + 1 <-----'  |  |  |  |  |  |

 * | | | | | |         R0 = 3                            |  |  |  |  |  |

 * | | | | | |         JMP -4 * 3               ---------'  |  |  |  |  |

 * | | | | | | ,----6- if R0 != 4 JMP 2 * 3 + 1 <-----------'  |  |  |  |

 * | | | | | | |       R0 = 5                                  |  |  |  |

 * | | | | | | |       JMP -6 * 3               ---------------'  |  |  |

 * | | | | | | | ,--7- if R0 != 6 JMP 1 * 3 + 1 <-----------------'  |  |

 * | | | | | | | |     R0 = 7                                        |  |

 * | | Error | | |     JMP -8 * 3               ---------------------'  |

 * | | paths | | | ,8- if R0 != 8 JMP 0 * 3 + 1 <-----------------------'

 * | | | | | | | | |   R0 = 9__________________Sequence: 3 * size - 1 insns

 * `-+-+-+-+-+-+-+-+-> EXIT____________________Return: 1 insn

 *

 The maximum size parameter */

 We use a reduced number of iterations to get a reasonable execution time */

 Preamble */

 Sequence */

 Return */

 64-bit unconditional jump */

 64-bit immediate jumps */

 64-bit register jumps */

 32-bit immediate jumps */

 32-bit register jumps */

 A == -3 */

 X == len - 3 */

 A == len * 2 */

 ld #0 */

		/* 00:00:00:00:00:00 > 00:00:00:00:00:00, ethtype IPv4 (0x0800),

		 * length 98: 127.0.0.1 > 127.0.0.1: ICMP echo request,

		 * id 9737, seq 1, length 64

 M1 = 1 ^ len */

 M2 = 1 ^ len ^ 0x80000000 */

 M3 = len */

 IPv6 */

 IPv4 */

		/* 3c:07:54:43:e5:76 > 10:bf:48:d6:43:d6, ethertype IPv4(0x0800)

		 * length 114: 10.1.1.149.49700 > 10.1.2.10.22: Flags [P.],

		 * seq 1305692979:1305693027, ack 3650467037, win 65535,

		 * options [nop,nop,TS val 2502645400 ecr 3971138], length 48

 IP header */

 ip src */

 ip dst */

 dst port */ },

			/* tcpdump -nei eth0 'tcp port 22 and (((ip[2:2] -

			 * ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0) and

			 * (len > 115 or len < 30000000000)' -d

 libpcap emits K on TAX */

 libpcap emits K on TAX */

 IP header */

 ip src */

 ip dst */

 dst port */ },

 check that uninitialized X and A contain zeros */

		/* Have to test all register combinations, since

		 * JITing of different registers will produce

		 * different asm code.

 R0 == 155 */

 R1 == 456 */

 R2 == 1358 */

 R3 == 4063 */

 R4 == 12177 */

 R5 == 36518 */

 R6 == 109540 */

 R7 == 328605 */

 R8 == 985799 */

 R9 == 2957380 */

 R0 == 155 */

 R1 == 456 */

 R2 == 1358 */

 R3 == 4063 */

 R4 == 12177 */

 R5 == 36518 */

 R6 == 109540 */

 R7 == 328605 */

 R8 == 985799 */

 R9 == 2957380 */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 R0 <= 46 */

 R4 = 46 << 1 */

 R4 = 4 << 4 */

 R4 = 5 << 5 */

 seccomp insn, rejected in socket filter */

 IP header */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 Passes checker but fails during runtime. */

 R0 = 1 */

 BPF_ALU | BPF_MOV | BPF_X */

 BPF_ALU | BPF_MOV | BPF_K */

 BPF_ALU | BPF_ADD | BPF_X */

 BPF_ALU | BPF_ADD | BPF_K */

 BPF_ALU | BPF_SUB | BPF_X */

 BPF_ALU | BPF_SUB | BPF_K */

 BPF_ALU | BPF_MUL | BPF_X */

 BPF_ALU | BPF_MUL | BPF_K */

 BPF_ALU | BPF_DIV | BPF_X */

 BPF_ALU | BPF_DIV | BPF_K */

 BPF_ALU | BPF_MOD | BPF_X */

 BPF_ALU | BPF_MOD | BPF_K */

 BPF_ALU | BPF_AND | BPF_X */

 BPF_ALU | BPF_AND | BPF_K */

 BPF_ALU | BPF_OR | BPF_X */

 BPF_ALU | BPF_OR | BPF_K */

 BPF_ALU | BPF_XOR | BPF_X */

 BPF_ALU | BPF_XOR | BPF_K */

 BPF_ALU | BPF_LSH | BPF_X */

 BPF_ALU | BPF_LSH | BPF_K */

 BPF_ALU | BPF_RSH | BPF_X */

 BPF_ALU | BPF_RSH | BPF_K */

 BPF_ALU | BPF_ARSH | BPF_X */

 BPF_ALU | BPF_ARSH | BPF_K */

 BPF_ALU | BPF_NEG */

 BPF_ALU | BPF_END | BPF_FROM_BE */

 R1 = 0 */

 BPF_ALU | BPF_END | BPF_FROM_BE, reversed */

 R1 = 0 */

 BPF_ALU | BPF_END | BPF_FROM_LE */

 R1 = 0 */

 BPF_ALU | BPF_END | BPF_FROM_LE, reversed */

 R1 = 0 */

 BPF_LDX_MEM B/H/W/DW */

 BPF_STX_MEM B/H/W/DW */

 BPF_ST(X) | BPF_MEM | BPF_B/H/W/DW */

 BPF_STX | BPF_ATOMIC | BPF_W/DW */

	/*

	 * Exhaustive tests of atomic operation variants.

	 * Individual tests are expanded from template macros for all

	 * combinations of ALU operation, word size and fetching.

 BPF_ATOMIC | BPF_W: BPF_ADD */

 BPF_ATOMIC | BPF_W: BPF_ADD | BPF_FETCH */

 BPF_ATOMIC | BPF_DW: BPF_ADD */

 BPF_ATOMIC | BPF_DW: BPF_ADD | BPF_FETCH */

 BPF_ATOMIC | BPF_W: BPF_AND */

 BPF_ATOMIC | BPF_W: BPF_AND | BPF_FETCH */

 BPF_ATOMIC | BPF_DW: BPF_AND */

 BPF_ATOMIC | BPF_DW: BPF_AND | BPF_FETCH */

 BPF_ATOMIC | BPF_W: BPF_OR */

 BPF_ATOMIC | BPF_W: BPF_OR | BPF_FETCH */

 BPF_ATOMIC | BPF_DW: BPF_OR */

 BPF_ATOMIC | BPF_DW: BPF_OR | BPF_FETCH */

 BPF_ATOMIC | BPF_W: BPF_XOR */

 BPF_ATOMIC | BPF_W: BPF_XOR | BPF_FETCH */

 BPF_ATOMIC | BPF_DW: BPF_XOR */

 BPF_ATOMIC | BPF_DW: BPF_XOR | BPF_FETCH */

 BPF_ATOMIC | BPF_W: BPF_XCHG */

 BPF_ATOMIC | BPF_DW: BPF_XCHG */

 BPF_ATOMIC | BPF_W, BPF_CMPXCHG */

 BPF_ATOMIC | BPF_DW, BPF_CMPXCHG */

 BPF_JMP32 | BPF_JEQ | BPF_K */

 BPF_JMP32 | BPF_JEQ | BPF_X */

 BPF_JMP32 | BPF_JNE | BPF_K */

 BPF_JMP32 | BPF_JNE | BPF_X */

 BPF_JMP32 | BPF_JSET | BPF_K */

 BPF_JMP32 | BPF_JSET | BPF_X */

 BPF_JMP32 | BPF_JGT | BPF_K */

 BPF_JMP32 | BPF_JGT | BPF_X */

 BPF_JMP32 | BPF_JGE | BPF_K */

 BPF_JMP32 | BPF_JGE | BPF_X */

 BPF_JMP32 | BPF_JLT | BPF_K */

 BPF_JMP32 | BPF_JLT | BPF_X */

 BPF_JMP32 | BPF_JLE | BPF_K */

 BPF_JMP32 | BPF_JLE | BPF_X */

 BPF_JMP32 | BPF_JSGT | BPF_K */

 BPF_JMP32 | BPF_JSGT | BPF_X */

 BPF_JMP32 | BPF_JSGE | BPF_K */

 BPF_JMP32 | BPF_JSGE | BPF_X */

 BPF_JMP32 | BPF_JSLT | BPF_K */

 BPF_JMP32 | BPF_JSLT | BPF_X */

 BPF_JMP32 | BPF_JSLE | BPF_K */

 BPF_JMP32 | BPF_JSLE | BPF_K */

 BPF_JMP | BPF_EXIT */

 BPF_JMP | BPF_JA */

 BPF_JMP | BPF_JSLT | BPF_K */

 BPF_JMP | BPF_JSGT | BPF_K */

 BPF_JMP | BPF_JSLE | BPF_K */

 bad exit */

 good exit */

 bad exit */

 good exit */

 BPF_JMP | BPF_JSGE | BPF_K */

 bad exit */

 good exit */

 bad exit */

 good exit */

 BPF_JMP | BPF_JGT | BPF_K */

 BPF_JMP | BPF_JLT | BPF_K */

 BPF_JMP | BPF_JGE | BPF_K */

 BPF_JMP | BPF_JLE | BPF_K */

 BPF_JMP | BPF_JGT | BPF_K jump backwards */

 goto start */

 out: */

 start: */

 note: this takes 2 insns */

 goto out */

 BPF_JMP | BPF_JLT | BPF_K jump backwards */

 goto start */

 out: */

 start: */

 note: this takes 2 insns */

 goto out */

 BPF_JMP | BPF_JNE | BPF_K */

 BPF_JMP | BPF_JEQ | BPF_K */

 BPF_JMP | BPF_JSET | BPF_K */

 BPF_JMP | BPF_JSGT | BPF_X */

 BPF_JMP | BPF_JSLT | BPF_X */

 BPF_JMP | BPF_JSGE | BPF_X */

 BPF_JMP | BPF_JSLE | BPF_X */

 BPF_JMP | BPF_JGT | BPF_X */

 BPF_JMP | BPF_JLT | BPF_X */

 BPF_JMP | BPF_JGE | BPF_X */

 BPF_JMP | BPF_JLE | BPF_X */

 Mainly testing JIT + imm64 here. */

 BPF_JMP | BPF_JNE | BPF_X */

 BPF_JMP | BPF_JEQ | BPF_X */

 BPF_JMP | BPF_JSET | BPF_X */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

 Mainly checking JIT here. */

	/*

	 * LD_IND / LD_ABS on fragmented SKBs

	/*

	 * LD_IND / LD_ABS on non fragmented SKBs

		/*

		 * this tests that the JIT/interpreter correctly resets X

		 * before using it in an LD_IND instruction.

	/*

	 * verify that the interpreter or JIT correctly sets A and X

	 * to 0.

			/*

			 * A = 0x42

			 * A = A + X

			 * ret A

			/*

			 * A = A + 0x42

			 * ret A

			/*

			 * A = 0x66

			 * A = A - X

			 * ret A

			/*

			 * A = A - -0x66

			 * ret A

			/*

			 * A = 0x42

			 * A = A * X

			 * ret A

			/*

			 * A = A * 0x66

			 * ret A

			/*

			 * A = 0x42

			 * A = A / X ; this halt the filter execution if X is 0

			 * ret 0x42

			/*

			 * A = A / 1

			 * ret A

			/*

			 * A = 0x42

			 * A = A mod X ; this halt the filter execution if X is 0

			 * ret 0x42

			/*

			 * A = A mod 1

			 * ret A

			/*

			 * cmp A, 0x0, 0, 1

			 * ret 0x42

			 * ret 0x66

			/*

			 * A = 0x0

			 * cmp A, X, 0, 1

			 * ret 0x42

			 * ret 0x66

 Checking interpreter vs JIT wrt signed extended imms. */

 BPF_LDX_MEM with operand aliasing */

	/*

	 * Register (non-)clobbering tests for the case where a JIT implements

	 * complex ALU or ATOMIC operations via function calls. If so, the

	 * function call must be transparent to the eBPF registers. The JIT

	 * must therefore save and restore relevant registers across the call.

	 * The following tests check that the eBPF registers retain their

	 * values after such an operation. Mainly intended for complex ALU

	 * and atomic operation, but we run it for all. You never know...

	 *

	 * Note that each operations should be tested twice with different

	 * destinations, to check preservation for all registers.

 ALU64 operations, register clobbering */

 ALU32 immediate operations, register clobbering */

 ALU64 register operations, register clobbering */

 ALU32 register operations, register clobbering */

 64-bit atomic operations, register clobbering */

 32-bit atomic operations, register clobbering */

 Checking that ALU32 src is not zero extended in place */

 Checking that ATOMIC32 src is not zero extended in place */

 Checking that CMPXCHG32 src is not zero extended in place */

 Checking that JMP32 immediate src is not zero extended in place */

 Nop */			\

 Checking that JMP32 dst & src are not zero extended in place */

 Nop */			\

 ALU64 K register combinations */

 ALU32 K registers */

 ALU64 X register combinations */

 ALU32 X register combinations */

 Exhaustive test of ALU64 shift operations */

 Exhaustive test of ALU32 shift operations */

	/*

	 * Exhaustive test of ALU64 shift operations when

	 * source and destination register are the same.

	/*

	 * Exhaustive test of ALU32 shift operations when

	 * source and destination register are the same.

 ALU64 immediate magnitudes */

 ALU32 immediate magnitudes */

 ALU64 register magnitudes */

 ALU32 register magnitudes */

 LD_IMM64 immediate magnitudes and byte patterns */

 64-bit ATOMIC register combinations */

 32-bit ATOMIC register combinations */

 64-bit ATOMIC magnitudes */

 64-bit atomic magnitudes */

 JMP immediate magnitudes */

 JMP register magnitudes */

 JMP32 immediate magnitudes */

 JMP32 register magnitudes */

 Conditional jumps with constant decision */

 Short relative jumps */

 Conditional branch conversions */

 Staggered jump sequences, immediate */

 Staggered jump sequences, register */

 Staggered jump sequences, JMP32 immediate */

 Staggered jump sequences, JMP32 register */

 Initialize a fake skb with test pattern. */

	/* Test case expects an skb, so populate one. Various

	 * subtests generate skbs of different sizes based on

	 * the same data.

		/*

		 * when the test requires a fragmented skb, add a

		 * single fragment to the skb, filled with

		 * test->frag_data.

 Verifier rejected filter as expected. */

				/* Verifier didn't reject the test that's

				 * bad enough, just return!

 Type doesn't really matter here as long as it's not unspec. */

		/* We cannot error here as we don't need type compatibility

		 * checks.

		/*

		 * NOTE: Several sub-tests may be present, in which case

		 * a zero {data_size, result} tuple indicates the end of

		 * the sub-test array. The first test is always run,

		 * even if both data_size and result happen to be zero.

 this will set skb[i]->head_frag */

 skb_headlen(skb[i]): 8, skb[i]->head_frag = 1

 setup shinfo */

 adjust skb[0]'s len */

	/* skbs linked in a frag_list, both with linear data, with head_frag=0

	 * (data allocated by kmalloc), both have tcp data of 1308 bytes

	 * (total payload is 2616 bytes).

	 * Data offset is 72 bytes (40 ipv6 hdr, 32 tcp hdr). Some headroom.

	/* setup shinfo.

	 * mimic bpf_skb_proto_4_to_6, which resets gso_segs and assigns a

	 * reduced gso_size.

 adjust skb[0]'s len */

 Flags that can be passed to tail call test cases */

/*

 * Magic marker used in test snippets for tail calls below.

 * BPF_LD/MOV to R2 and R2 with this immediate value is replaced

 * with the proper values by the test runner.

 Special offset to indicate a NULL call target */

 Special offset to indicate an out-of-range index */

/*

 * A test function to be called from a BPF program, clobbering a lot of

 * CPU registers in the process. A JITed BPF program calling this function

 * must save and restore any caller-saved registers it uses for internal

 * state, for example the current tail call count.

/*

 * Tail call tests. Each test case may call any other test in the table,

 * including itself, specified as a relative index offset from the calling

 * test. The index TAIL_CALL_NULL can be used to specify a NULL target

 * function to test the JIT error path. Similarly, the index TAIL_CALL_INVALID

 * results in a target index that is out of range.

 Allocate the table of programs to be used for tall calls */

 Create all eBPF programs and populate the table */

 Compute the number of program instructions */

 Allocate and initialize the program */

 Relocate runtime tail call offsets and addresses */

 Skip: NOP */

 The last entry contains a NULL program pointer */

		/*

		 * if a test_id was specified, use test_range to

		 * cover only that test.

		/*

		 * if a test_name was specified, find it and setup

		 * test_range to cover only that test.

		/*

		 * check that the supplied test_range is valid.

	/*

	 * if test_suite is not specified, but test_id, test_name or test_range

	 * is specified, set 'test_bpf' as the default test suite.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * The pattern of set bits in the list length determines which cases

 * are hit in list_sort().

 not including head */

 `priv` is the test pointer so check() can fail the test if the list is invalid. */

 force some equivalencies */

 SPDX-License-Identifier: GPL-2.0

/**

 * cpumask_next - get the next cpu in a cpumask

 * @n: the cpu prior to the place to search (ie. return will be > @n)

 * @srcp: the cpumask pointer

 *

 * Returns >= nr_cpu_ids if no further cpus set.

 -1 is a legal arg here. */

/**

 * cpumask_next_and - get the next cpu in *src1p & *src2p

 * @n: the cpu prior to the place to search (ie. return will be > @n)

 * @src1p: the first cpumask pointer

 * @src2p: the second cpumask pointer

 *

 * Returns >= nr_cpu_ids if no further cpus set in both.

 -1 is a legal arg here. */

/**

 * cpumask_any_but - return a "random" in a cpumask, but not this one.

 * @mask: the cpumask to search

 * @cpu: the cpu to ignore.

 *

 * Often used to find any cpu but smp_processor_id() in a mask.

 * Returns >= nr_cpu_ids if no cpus set.

/**

 * cpumask_next_wrap - helper to implement for_each_cpu_wrap

 * @n: the cpu prior to the place to search

 * @mask: the cpumask pointer

 * @start: the start point of the iteration

 * @wrap: assume @n crossing @start terminates the iteration

 *

 * Returns >= nr_cpu_ids on completion

 *

 * Note: the @wrap argument is required for the start condition when

 * we cannot assume @start is set in @mask.

 These are not inline because of header tangles. */

/**

 * alloc_cpumask_var_node - allocate a struct cpumask on a given node

 * @mask: pointer to cpumask_var_t where the cpumask is returned

 * @flags: GFP_ flags

 *

 * Only defined when CONFIG_CPUMASK_OFFSTACK=y, otherwise is

 * a nop returning a constant 1 (in <linux/cpumask.h>)

 * Returns TRUE if memory allocation succeeded, FALSE otherwise.

 *

 * In addition, mask will be NULL if this fails.  Note that gcc is

 * usually smart enough to know that mask can never be NULL if

 * CONFIG_CPUMASK_OFFSTACK=n, so does code elimination in that case

 * too.

/**

 * alloc_cpumask_var - allocate a struct cpumask

 * @mask: pointer to cpumask_var_t where the cpumask is returned

 * @flags: GFP_ flags

 *

 * Only defined when CONFIG_CPUMASK_OFFSTACK=y, otherwise is

 * a nop returning a constant 1 (in <linux/cpumask.h>).

 *

 * See alloc_cpumask_var_node.

/**

 * alloc_bootmem_cpumask_var - allocate a struct cpumask from the bootmem arena.

 * @mask: pointer to cpumask_var_t where the cpumask is returned

 *

 * Only defined when CONFIG_CPUMASK_OFFSTACK=y, otherwise is

 * a nop (in <linux/cpumask.h>).

 * Either returns an allocated (zero-filled) cpumask, or causes the

 * system to panic.

/**

 * free_cpumask_var - frees memory allocated for a struct cpumask.

 * @mask: cpumask to free

 *

 * This is safe on a NULL mask.

/**

 * free_bootmem_cpumask_var - frees result of alloc_bootmem_cpumask_var

 * @mask: cpumask to free

/**

 * cpumask_local_spread - select the i'th cpu with local numa cpu's first

 * @i: index number

 * @node: local numa_node

 *

 * This function selects an online CPU according to a numa aware policy;

 * local cpus are returned first, followed by non-local ones, then it

 * wraps around.

 *

 * It's not very efficient, but useful for setup.

 Wrap: we always want a cpu. */

 NUMA first. */

 Skip NUMA nodes, done above. */

/**

 * Returns an arbitrary cpu within srcp1 & srcp2.

 *

 * Iterated calls using the same srcp1 and srcp2 will be distributed within

 * their intersection.

 *

 * Returns >= nr_cpu_ids if the intersection is empty.

 NOTE: our first selection will skip 0. */

 NOTE: our first selection will skip 0. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Return the bit number of a random bit set in the nodemask.

 * (returns NUMA_NO_NODE if nodemask is empty)

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd.

 * Author: Andrey Ryabinin <a.ryabinin@samsung.com>

	/*

	 * Temporarily enable multi-shot mode. Otherwise, KASAN would only

	 * report the first detected bug and panic the kernel if panic_on_warn

	 * is enabled.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Generic stack depot for storing stack traces.

 *

 * Some debugging tools need to save stack traces of certain events which can

 * be later presented to the user. For example, KASAN needs to safe alloc and

 * free stacks for each object, but storing two stack traces per object

 * requires too much memory (e.g. SLUB_DEBUG needs 256 bytes per object for

 * that).

 *

 * Instead, stack depot maintains a hashtable of unique stacktraces. Since alloc

 * and free stacks repeat a lot, we save about 100x space.

 * Stacks are never removed from depot, so we store them contiguously one after

 * another in a contiguous memory allocation.

 *

 * Author: Alexander Potapenko <glider@google.com>

 * Copyright (C) 2016 Google, Inc.

 *

 * Based on code by Dmitry Chernenkov.

 'Slab' size order for stack depot, 4 pages */

 The compact structure to store the reference to stacks. */

 Link in the hashtable */

 Hash in the hastable */

 Number of frames in the stack */

 Variable-sized array of entries. */

	/*

	 * This smp_load_acquire() pairs with smp_store_release() to

	 * |next_slab_inited| below and in depot_alloc_stack().

 If this is the last depot slab, do not touch the next one. */

		/*

		 * This smp_store_release pairs with smp_load_acquire() from

		 * |next_slab_inited| above and in stack_depot_save().

 Allocation of a new stack in raw storage */

		/*

		 * smp_store_release() here pairs with smp_load_acquire() from

		 * |next_slab_inited| in stack_depot_save() and

		 * init_stack_slab().

 Calculate hash for a stack */

/* Use our own, non-instrumented version of memcmp().

 *

 * We actually don't care about the order, just the equality.

 Find a stack that is equal to the one stored in entries in the hash */

/**

 * stack_depot_snprint - print stack entries from a depot into a buffer

 *

 * @handle:	Stack depot handle which was returned from

 *		stack_depot_save().

 * @buf:	Pointer to the print buffer

 *

 * @size:	Size of the print buffer

 *

 * @spaces:	Number of leading spaces to print

 *

 * Return:	Number of bytes printed.

/**

 * stack_depot_print - print stack entries from a depot

 *

 * @stack:		Stack depot handle which was returned from

 *			stack_depot_save().

 *

/**

 * stack_depot_fetch - Fetch stack entries from a depot

 *

 * @handle:		Stack depot handle which was returned from

 *			stack_depot_save().

 * @entries:		Pointer to store the entries address

 *

 * Return: The number of trace entries for this depot.

/**

 * __stack_depot_save - Save a stack trace from an array

 *

 * @entries:		Pointer to storage array

 * @nr_entries:		Size of the storage array

 * @alloc_flags:	Allocation gfp flags

 * @can_alloc:		Allocate stack slabs (increased chance of failure if false)

 *

 * Saves a stack trace from @entries array of size @nr_entries. If @can_alloc is

 * %true, is allowed to replenish the stack slab pool in case no space is left

 * (allocates using GFP flags of @alloc_flags). If @can_alloc is %false, avoids

 * any allocations and will fail if no space is left to store the stack trace.

 *

 * Context: Any context, but setting @can_alloc to %false is required if

 *          alloc_pages() cannot be used from the current context. Currently

 *          this is the case from contexts where neither %GFP_ATOMIC nor

 *          %GFP_NOWAIT can be used (NMI, raw_spin_lock).

 *

 * Return: The handle of the stack struct stored in depot, 0 on failure.

	/*

	 * Fast path: look the stack trace up without locking.

	 * The smp_load_acquire() here pairs with smp_store_release() to

	 * |bucket| below.

	/*

	 * Check if the current or the next stack slab need to be initialized.

	 * If so, allocate the memory - we won't be able to do that under the

	 * lock.

	 *

	 * The smp_load_acquire() here pairs with smp_store_release() to

	 * |next_slab_inited| in depot_alloc_stack() and init_stack_slab().

		/*

		 * Zero out zone modifiers, as we don't have specific zone

		 * requirements. Keep the flags related to allocation in atomic

		 * contexts and I/O.

			/*

			 * This smp_store_release() pairs with

			 * smp_load_acquire() from |bucket| above.

		/*

		 * We didn't need to store this stack trace, but let's keep

		 * the preallocated memory for the future.

 Nobody used this memory, ok to free it. */

/**

 * stack_depot_save - Save a stack trace from an array

 *

 * @entries:		Pointer to storage array

 * @nr_entries:		Size of the storage array

 * @alloc_flags:	Allocation gfp flags

 *

 * Context: Contexts where allocations via alloc_pages() are allowed.

 *          See __stack_depot_save() for more details.

 *

 * Return: The handle of the stack struct stored in depot, 0 on failure.

 SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0

/* Copyright 2016-2018 NXP

 * Copyright (c) 2018-2019, Vladimir Oltean <olteanv@gmail.com>

/**

 * packing - Convert numbers (currently u64) between a packed and an unpacked

 *	     format. Unpacked means laid out in memory in the CPU's native

 *	     understanding of integers, while packed means anything else that

 *	     requires translation.

 *

 * @pbuf: Pointer to a buffer holding the packed value.

 * @uval: Pointer to an u64 holding the unpacked value.

 * @startbit: The index (in logical notation, compensated for quirks) where

 *	      the packed value starts within pbuf. Must be larger than, or

 *	      equal to, endbit.

 * @endbit: The index (in logical notation, compensated for quirks) where

 *	    the packed value ends within pbuf. Must be smaller than, or equal

 *	    to, startbit.

 * @pbuflen: The length in bytes of the packed buffer pointed to by @pbuf.

 * @op: If PACK, then uval will be treated as const pointer and copied (packed)

 *	into pbuf, between startbit and endbit.

 *	If UNPACK, then pbuf will be treated as const pointer and the logical

 *	value between startbit and endbit will be copied (unpacked) to uval.

 * @quirks: A bit mask of QUIRK_LITTLE_ENDIAN, QUIRK_LSW32_IS_FIRST and

 *	    QUIRK_MSB_ON_THE_RIGHT.

 *

 * Return: 0 on success, EINVAL or ERANGE if called incorrectly. Assuming

 *	   correct usage, return code may be discarded.

 *	   If op is PACK, pbuf is modified.

 *	   If op is UNPACK, uval is modified.

	/* Number of bits for storing "uval"

	 * also width of the field to access in the pbuf

	/* Logical byte indices corresponding to the

	 * start and end of the field.

 startbit is expected to be larger than endbit */

 Invalid function call */

	/* Check if "uval" fits in "value_width" bits.

	 * If value_width is 64, the check will fail, but any

	 * 64-bit uval will surely fit.

		/* Cannot store "uval" inside "value_width" bits.

		 * Truncating "uval" is most certainly not desirable,

		 * so simply erroring out is appropriate.

 Initialize parameter */

	/* Iterate through an idealistic view of the pbuf as an u64 with

	 * no quirks, u8 by u8 (aligned at u8 boundaries), from high to low

	 * logical bit significance. "box" denotes the current logical u8.

 Bit indices into the currently accessed 8-bit box */

 Corresponding bits from the unpacked u64 parameter */

		/* This u8 may need to be accessed in its entirety

		 * (from bit 7 to bit 0), or not, depending on the

		 * input arguments startbit and endbit.

		/* We have determined the box bit start and end.

		 * Now we calculate where this (masked) u8 box would fit

		 * in the unpacked (CPU-readable) u64 - the u8 box's

		 * projection onto the unpacked u64. Though the

		 * box is u8, the projection is u64 because it may fall

		 * anywhere within the unpacked u64.

		/* Determine the offset of the u8 box inside the pbuf,

		 * adjusted for quirks. The adjusted box_addr will be used for

		 * effective addressing inside the pbuf (so it's not

		 * logical any longer).

 Read from pbuf, write to uval */

 Write to pbuf, read from uval */

 SPDX-License-Identifier: GPL-2.0

/*

 * Helper function for splitting a string into an argv-like array.

/**

 * argv_free - free an argv

 * @argv - the argument vector to be freed

 *

 * Frees an argv and the strings it points to.

/**

 * argv_split - split a string at whitespace, returning an argv

 * @gfp: the GFP mask used to allocate memory

 * @str: the string to be split

 * @argcp: returned argument count

 *

 * Returns an array of pointers to strings which are split out from

 * @str.  This is performed by strictly splitting on white-space; no

 * quote processing is performed.  Multiple whitespace characters are

 * considered to be a single argument separator.  The returned array

 * is always NULL-terminated.  Returns NULL on memory allocation

 * failure.

 *

 * The source string at `str' may be undergoing concurrent alteration via

 * userspace sysctl activity (at least).  The argv_split() implementation

 * attempts to handle this gracefully by taking a local copy to work on.

 SPDX-License-Identifier: GPL-2.0

/*

 * Convert integer string representation to an integer.

 * If an integer doesn't fit into specified type, -E is returned.

 *

 * Integer starts with optional sign.

 * kstrtou*() functions do not accept sign "-".

 *

 * Radix 0 means autodetection: leading "0x" implies radix 16,

 * leading "0" implies radix 8, otherwise radix is 10.

 * Autodetection hints work after optional sign, but not before.

 *

 * If -E is returned, result is not touched.

/*

 * Convert non-negative integer string representation in explicitly given radix

 * to an integer. A maximum of max_chars characters will be converted.

 *

 * Return number of characters consumed maybe or-ed with overflow bit.

 * If overflow occurs, result integer (incorrect) is still returned.

 *

 * Don't you dare use this function.

 don't tolower() this line */

		/*

		 * Check for overflow only if we are within range of

		 * it in the max base we support (16)

/**

 * kstrtoull - convert a string to an unsigned long long

 * @s: The start of the string. The string must be null-terminated, and may also

 *  include a single newline before its terminating null. The first character

 *  may also be a plus sign, but not a minus sign.

 * @base: The number base to use. The maximum supported base is 16. If base is

 *  given as 0, then the base of the string is automatically detected with the

 *  conventional semantics - If it begins with 0x the number will be parsed as a

 *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be

 *  parsed as an octal number. Otherwise it will be parsed as a decimal.

 * @res: Where to write the result of the conversion on success.

 *

 * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.

 * Preferred over simple_strtoull(). Return code must be checked.

/**

 * kstrtoll - convert a string to a long long

 * @s: The start of the string. The string must be null-terminated, and may also

 *  include a single newline before its terminating null. The first character

 *  may also be a plus sign or a minus sign.

 * @base: The number base to use. The maximum supported base is 16. If base is

 *  given as 0, then the base of the string is automatically detected with the

 *  conventional semantics - If it begins with 0x the number will be parsed as a

 *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be

 *  parsed as an octal number. Otherwise it will be parsed as a decimal.

 * @res: Where to write the result of the conversion on success.

 *

 * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.

 * Preferred over simple_strtoll(). Return code must be checked.

 Internal, do not use. */

 Internal, do not use. */

/**

 * kstrtouint - convert a string to an unsigned int

 * @s: The start of the string. The string must be null-terminated, and may also

 *  include a single newline before its terminating null. The first character

 *  may also be a plus sign, but not a minus sign.

 * @base: The number base to use. The maximum supported base is 16. If base is

 *  given as 0, then the base of the string is automatically detected with the

 *  conventional semantics - If it begins with 0x the number will be parsed as a

 *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be

 *  parsed as an octal number. Otherwise it will be parsed as a decimal.

 * @res: Where to write the result of the conversion on success.

 *

 * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.

 * Preferred over simple_strtoul(). Return code must be checked.

/**

 * kstrtoint - convert a string to an int

 * @s: The start of the string. The string must be null-terminated, and may also

 *  include a single newline before its terminating null. The first character

 *  may also be a plus sign or a minus sign.

 * @base: The number base to use. The maximum supported base is 16. If base is

 *  given as 0, then the base of the string is automatically detected with the

 *  conventional semantics - If it begins with 0x the number will be parsed as a

 *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be

 *  parsed as an octal number. Otherwise it will be parsed as a decimal.

 * @res: Where to write the result of the conversion on success.

 *

 * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.

 * Preferred over simple_strtol(). Return code must be checked.

/**

 * kstrtobool - convert common user inputs into boolean values

 * @s: input string

 * @res: result

 *

 * This routine returns 0 iff the first character is one of 'Yy1Nn0', or

 * [oO][NnFf] for "on" and "off". Otherwise it will return -EINVAL.  Value

 * pointed to by res is updated upon finding a match.

/*

 * Since "base" would be a nonsense argument, this open-codes the

 * _from_user helper instead of using the helper macro below.

 Longest string needed to differentiate, newline, terminator */

 sign, base 2 representation, newline, terminator */		\

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Nokia Corporation

 * Copyright (C) 2011 Intel Corporation

 *

 * Author:

 * Dmitry Kasatkin <dmitry.kasatkin@nokia.com>

 *                 <dmitry.kasatkin@intel.com>

 *

 * File: sign.c

 *	implements signature (RSA) verification

 *	pkcs decoding is based on LibTomCrypt code

 test message size */

 separate encoded message */

 separator check */

		/* There was no octet with hexadecimal value 0x00

/*

 * RSA Signature verification with public key

 key was revoked before we acquired its semaphore */

/**

 * digsig_verify() - digital signature verification with public key

 * @keyring:	keyring to search key in

 * @sig:	digital signature

 * @siglen:	length of the signature

 * @data:	data

 * @datalen:	length of the data

 *

 * Returns 0 on success, -EINVAL otherwise

 *

 * Verifies data integrity against digital signature.

 * Currently only RSA is supported.

 * Normally hash of the content is used as a data for this function.

 *

 search in specific keyring */

 pass signature mpis address */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * lib/bitmap.c

 * Helper functions for bitmap.h.

/**

 * DOC: bitmap introduction

 *

 * bitmaps provide an array of bits, implemented using an

 * array of unsigned longs.  The number of valid bits in a

 * given bitmap does _not_ need to be an exact multiple of

 * BITS_PER_LONG.

 *

 * The possible unused bits in the last, partially used word

 * of a bitmap are 'don't care'.  The implementation makes

 * no particular effort to keep them zero.  It ensures that

 * their value will not affect the results of any operation.

 * The bitmap operations that return Boolean (bitmap_empty,

 * for example) or scalar (bitmap_weight, for example) results

 * carefully filter out these unused bits from impacting their

 * results.

 *

 * The byte ordering of bitmaps is more natural on little

 * endian architectures.  See the big-endian headers

 * include/asm-ppc64/bitops.h and include/asm-s390/bitops.h

 * for the best explanations of this ordering.

/**

 * __bitmap_shift_right - logical right shift of the bits in a bitmap

 *   @dst : destination bitmap

 *   @src : source bitmap

 *   @shift : shift by this many bits

 *   @nbits : bitmap size, in bits

 *

 * Shifting right (dividing) means moving bits in the MS -> LS bit

 * direction.  Zeros are fed into the vacated MS positions and the

 * LS bits shifted off the bottom are lost.

		/*

		 * If shift is not word aligned, take lower rem bits of

		 * word above and make them the top rem bits of result.

/**

 * __bitmap_shift_left - logical left shift of the bits in a bitmap

 *   @dst : destination bitmap

 *   @src : source bitmap

 *   @shift : shift by this many bits

 *   @nbits : bitmap size, in bits

 *

 * Shifting left (multiplying) means moving bits in the LS -> MS

 * direction.  Zeros are fed into the vacated LS bit positions

 * and those MS bits shifted off the top are lost.

		/*

		 * If shift is not word aligned, take upper rem bits of

		 * word below and make them the bottom rem bits of result.

/**

 * bitmap_cut() - remove bit region from bitmap and right shift remaining bits

 * @dst: destination bitmap, might overlap with src

 * @src: source bitmap

 * @first: start bit of region to be removed

 * @cut: number of bits to remove

 * @nbits: bitmap size, in bits

 *

 * Set the n-th bit of @dst iff the n-th bit of @src is set and

 * n is less than @first, or the m-th bit of @src is set for any

 * m such that @first <= n < nbits, and m = n + @cut.

 *

 * In pictures, example for a big-endian 32-bit architecture:

 *

 * The @src bitmap is::

 *

 *   31                                   63

 *   |                                    |

 *   10000000 11000001 11110010 00010101  10000000 11000001 01110010 00010101

 *                   |  |              |                                    |

 *                  16  14             0                                   32

 *

 * if @cut is 3, and @first is 14, bits 14-16 in @src are cut and @dst is::

 *

 *   31                                   63

 *   |                                    |

 *   10110000 00011000 00110010 00010101  00010000 00011000 00101110 01000010

 *                      |              |                                    |

 *                      14 (bit 17     0                                   32

 *                          from @src)

 *

 * Note that @dst and @src might overlap partially or entirely.

 *

 * This is implemented in the obvious way, with a shift and carry

 * step for each moved bit. Optimisation is left as an exercise

 * for the compiler.

/**

 * bitmap_find_next_zero_area_off - find a contiguous aligned zero area

 * @map: The address to base the search on

 * @size: The bitmap size in bits

 * @start: The bitnumber to start searching at

 * @nr: The number of zeroed bits we're looking for

 * @align_mask: Alignment mask for zero area

 * @align_offset: Alignment offset for zero area.

 *

 * The @align_mask should be one less than a power of 2; the effect is that

 * the bit offset of all zero areas this function finds plus @align_offset

 * is multiple of that power of 2.

 Align allocation */

/*

 * Bitmap printing & parsing functions: first version by Nadia Yvette Chambers,

 * second version by Paul Jackson, third by Joe Korty.

/**

 * bitmap_parse_user - convert an ASCII hex string in a user buffer into a bitmap

 *

 * @ubuf: pointer to user buffer containing string.

 * @ulen: buffer size in bytes.  If string is smaller than this

 *    then it must be terminated with a \0.

 * @maskp: pointer to bitmap array that will contain result.

 * @nmaskbits: size of bitmap, in bits.

/**

 * bitmap_print_to_pagebuf - convert bitmap to list or hex format ASCII string

 * @list: indicates whether the bitmap must be list

 * @buf: page aligned buffer into which string is placed

 * @maskp: pointer to bitmap to convert

 * @nmaskbits: size of bitmap, in bits

 *

 * Output format is a comma-separated list of decimal numbers and

 * ranges if list is specified or hex digits grouped into comma-separated

 * sets of 8 digits/set. Returns the number of characters written to buf.

 *

 * It is assumed that @buf is a pointer into a PAGE_SIZE, page-aligned

 * area and that sufficient storage remains at @buf to accommodate the

 * bitmap_print_to_pagebuf() output. Returns the number of characters

 * actually printed to @buf, excluding terminating '\0'.

/**

 * bitmap_print_to_buf  - convert bitmap to list or hex format ASCII string

 * @list: indicates whether the bitmap must be list

 *      true:  print in decimal list format

 *      false: print in hexadecimal bitmask format

/**

 * bitmap_print_bitmask_to_buf  - convert bitmap to hex bitmask format ASCII string

 *

 * The bitmap_print_to_pagebuf() is used indirectly via its cpumap wrapper

 * cpumap_print_to_pagebuf() or directly by drivers to export hexadecimal

 * bitmask and decimal list to userspace by sysfs ABI.

 * Drivers might be using a normal attribute for this kind of ABIs. A

 * normal attribute typically has show entry as below:

 * static ssize_t example_attribute_show(struct device *dev,

 * 		struct device_attribute *attr, char *buf)

 * {

 * 	...

 * 	return bitmap_print_to_pagebuf(true, buf, &mask, nr_trig_max);

 * }

 * show entry of attribute has no offset and count parameters and this

 * means the file is limited to one page only.

 * bitmap_print_to_pagebuf() API works terribly well for this kind of

 * normal attribute with buf parameter and without offset, count:

 * bitmap_print_to_pagebuf(bool list, char *buf, const unsigned long *maskp,

 * 			   int nmaskbits)

 * {

 * }

 * The problem is once we have a large bitmap, we have a chance to get a

 * bitmask or list more than one page. Especially for list, it could be

 * as complex as 0,3,5,7,9,... We have no simple way to know it exact size.

 * It turns out bin_attribute is a way to break this limit. bin_attribute

 * has show entry as below:

 * static ssize_t

 * example_bin_attribute_show(struct file *filp, struct kobject *kobj,

 * 		struct bin_attribute *attr, char *buf,

 * 		loff_t offset, size_t count)

 * {

 * 	...

 * }

 * With the new offset and count parameters, this makes sysfs ABI be able

 * to support file size more than one page. For example, offset could be

 * >= 4096.

 * bitmap_print_bitmask_to_buf(), bitmap_print_list_to_buf() wit their

 * cpumap wrapper cpumap_print_bitmask_to_buf(), cpumap_print_list_to_buf()

 * make those drivers be able to support large bitmask and list after they

 * move to use bin_attribute. In result, we have to pass the corresponding

 * parameters such as off, count from bin_attribute show entry to this API.

 *

 * @buf: buffer into which string is placed

 * @maskp: pointer to bitmap to convert

 * @nmaskbits: size of bitmap, in bits

 * @off: in the string from which we are copying, We copy to @buf

 * @count: the maximum number of bytes to print

 *

 * The role of cpumap_print_bitmask_to_buf() and cpumap_print_list_to_buf()

 * is similar with cpumap_print_to_pagebuf(),  the difference is that

 * bitmap_print_to_pagebuf() mainly serves sysfs attribute with the assumption

 * the destination buffer is exactly one page and won't be more than one page.

 * cpumap_print_bitmask_to_buf() and cpumap_print_list_to_buf(), on the other

 * hand, mainly serves bin_attribute which doesn't work with exact one page,

 * and it can break the size limit of converted decimal list and hexadecimal

 * bitmask.

 *

 * WARNING!

 *

 * This function is not a replacement for sprintf() or bitmap_print_to_pagebuf().

 * It is intended to workaround sysfs limitations discussed above and should be

 * used carefully in general case for the following reasons:

 *  - Time complexity is O(nbits^2/count), comparing to O(nbits) for snprintf().

 *  - Memory complexity is O(nbits), comparing to O(1) for snprintf().

 *  - @off and @count are NOT offset and number of bits to print.

 *  - If printing part of bitmap as list, the resulting string is not a correct

 *    list representation of bitmap. Particularly, some bits within or out of

 *    related interval may be erroneously set or unset. The format of the string

 *    may be broken, so bitmap_parselist-like parser may fail parsing it.

 *  - If printing the whole bitmap as list by parts, user must ensure the order

 *    of calls of the function such that the offset is incremented linearly.

 *  - If printing the whole bitmap as list by parts, user must keep bitmap

 *    unchanged between the very first and very last call. Otherwise concatenated

 *    result may be incorrect, and format may be broken.

 *

 * Returns the number of characters actually printed to @buf

/**

 * bitmap_print_list_to_buf  - convert bitmap to decimal list format ASCII string

 *

 * Everything is same with the above bitmap_print_bitmask_to_buf() except

 * the print format.

/*

 * Region 9-38:4/10 describes the following bitmap structure:

 * 0	   9  12    18			38	     N

 * .........****......****......****..................

 *	    ^  ^     ^			 ^	     ^

 *      start  off   group_len	       end	 nbits

/*

 * The format allows commas and whitespaces at the beginning

 * of the region.

/**

 * bitmap_parselist - convert list format ASCII string to bitmap

 * @buf: read user string from this buffer; must be terminated

 *    with a \0 or \n.

 * @maskp: write resulting mask here

 * @nmaskbits: number of bits in mask to be written

 *

 * Input format is a comma-separated list of decimal numbers and

 * ranges.  Consecutively set bits are shown as two hyphen-separated

 * decimal numbers, the smallest and largest bit numbers set in

 * the range.

 * Optionally each range can be postfixed to denote that only parts of it

 * should be set. The range will divided to groups of specific size.

 * From each group will be used only defined amount of bits.

 * Syntax: range:used_size/group_size

 * Example: 0-1023:2/256 ==> 0,1,256,257,512,513,768,769

 * The value 'N' can be used as a dynamically substituted token for the

 * maximum allowed value; i.e (nmaskbits - 1).  Keep in mind that it is

 * dynamic, so if system changes cause the bitmap width to change, such

 * as more cores in a CPU list, then any ranges using N will also change.

 *

 * Returns: 0 on success, -errno on invalid input strings. Error values:

 *

 *   - ``-EINVAL``: wrong region format

 *   - ``-EINVAL``: invalid character in string

 *   - ``-ERANGE``: bit number specified too large for mask

 *   - ``-EOVERFLOW``: integer overflow in the input parameters

/**

 * bitmap_parselist_user()

 *

 * @ubuf: pointer to user buffer containing string.

 * @ulen: buffer size in bytes.  If string is smaller than this

 *    then it must be terminated with a \0.

 * @maskp: pointer to bitmap array that will contain result.

 * @nmaskbits: size of bitmap, in bits.

 *

 * Wrapper for bitmap_parselist(), providing it with user buffer.

/**

 * bitmap_parse - convert an ASCII hex string into a bitmap.

 * @start: pointer to buffer containing string.

 * @buflen: buffer size in bytes.  If string is smaller than this

 *    then it must be terminated with a \0 or \n. In that case,

 *    UINT_MAX may be provided instead of string length.

 * @maskp: pointer to bitmap array that will contain result.

 * @nmaskbits: size of bitmap, in bits.

 *

 * Commas group hex digits into chunks.  Each chunk defines exactly 32

 * bits of the resultant bitmask.  No chunk may specify a value larger

 * than 32 bits (%-EOVERFLOW), and if a chunk specifies a smaller value

 * then leading 0-bits are prepended.  %-EINVAL is returned for illegal

 * characters. Grouping such as "1,,5", ",44", "," or "" is allowed.

 * Leading, embedded and trailing whitespace accepted.

/**

 * bitmap_pos_to_ord - find ordinal of set bit at given position in bitmap

 *	@buf: pointer to a bitmap

 *	@pos: a bit position in @buf (0 <= @pos < @nbits)

 *	@nbits: number of valid bit positions in @buf

 *

 * Map the bit at position @pos in @buf (of length @nbits) to the

 * ordinal of which set bit it is.  If it is not set or if @pos

 * is not a valid bit position, map to -1.

 *

 * If for example, just bits 4 through 7 are set in @buf, then @pos

 * values 4 through 7 will get mapped to 0 through 3, respectively,

 * and other @pos values will get mapped to -1.  When @pos value 7

 * gets mapped to (returns) @ord value 3 in this example, that means

 * that bit 7 is the 3rd (starting with 0th) set bit in @buf.

 *

 * The bit positions 0 through @bits are valid positions in @buf.

/**

 * bitmap_ord_to_pos - find position of n-th set bit in bitmap

 *	@buf: pointer to bitmap

 *	@ord: ordinal bit position (n-th set bit, n >= 0)

 *	@nbits: number of valid bit positions in @buf

 *

 * Map the ordinal offset of bit @ord in @buf to its position in @buf.

 * Value of @ord should be in range 0 <= @ord < weight(buf). If @ord

 * >= weight(buf), returns @nbits.

 *

 * If for example, just bits 4 through 7 are set in @buf, then @ord

 * values 0 through 3 will get mapped to 4 through 7, respectively,

 * and all other @ord values returns @nbits.  When @ord value 3

 * gets mapped to (returns) @pos value 7 in this example, that means

 * that the 3rd set bit (starting with 0th) is at position 7 in @buf.

 *

 * The bit positions 0 through @nbits-1 are valid positions in @buf.

/**

 * bitmap_remap - Apply map defined by a pair of bitmaps to another bitmap

 *	@dst: remapped result

 *	@src: subset to be remapped

 *	@old: defines domain of map

 *	@new: defines range of map

 *	@nbits: number of bits in each of these bitmaps

 *

 * Let @old and @new define a mapping of bit positions, such that

 * whatever position is held by the n-th set bit in @old is mapped

 * to the n-th set bit in @new.  In the more general case, allowing

 * for the possibility that the weight 'w' of @new is less than the

 * weight of @old, map the position of the n-th set bit in @old to

 * the position of the m-th set bit in @new, where m == n % w.

 *

 * If either of the @old and @new bitmaps are empty, or if @src and

 * @dst point to the same location, then this routine copies @src

 * to @dst.

 *

 * The positions of unset bits in @old are mapped to themselves

 * (the identify map).

 *

 * Apply the above specified mapping to @src, placing the result in

 * @dst, clearing any bits previously set in @dst.

 *

 * For example, lets say that @old has bits 4 through 7 set, and

 * @new has bits 12 through 15 set.  This defines the mapping of bit

 * position 4 to 12, 5 to 13, 6 to 14 and 7 to 15, and of all other

 * bit positions unchanged.  So if say @src comes into this routine

 * with bits 1, 5 and 7 set, then @dst should leave with bits 1,

 * 13 and 15 set.

 following doesn't handle inplace remaps */

 identity map */

/**

 * bitmap_bitremap - Apply map defined by a pair of bitmaps to a single bit

 *	@oldbit: bit position to be mapped

 *	@old: defines domain of map

 *	@new: defines range of map

 *	@bits: number of bits in each of these bitmaps

 *

 * Let @old and @new define a mapping of bit positions, such that

 * whatever position is held by the n-th set bit in @old is mapped

 * to the n-th set bit in @new.  In the more general case, allowing

 * for the possibility that the weight 'w' of @new is less than the

 * weight of @old, map the position of the n-th set bit in @old to

 * the position of the m-th set bit in @new, where m == n % w.

 *

 * The positions of unset bits in @old are mapped to themselves

 * (the identify map).

 *

 * Apply the above specified mapping to bit position @oldbit, returning

 * the new bit position.

 *

 * For example, lets say that @old has bits 4 through 7 set, and

 * @new has bits 12 through 15 set.  This defines the mapping of bit

 * position 4 to 12, 5 to 13, 6 to 14 and 7 to 15, and of all other

 * bit positions unchanged.  So if say @oldbit is 5, then this routine

 * returns 13.

/**

 * bitmap_onto - translate one bitmap relative to another

 *	@dst: resulting translated bitmap

 * 	@orig: original untranslated bitmap

 * 	@relmap: bitmap relative to which translated

 *	@bits: number of bits in each of these bitmaps

 *

 * Set the n-th bit of @dst iff there exists some m such that the

 * n-th bit of @relmap is set, the m-th bit of @orig is set, and

 * the n-th bit of @relmap is also the m-th _set_ bit of @relmap.

 * (If you understood the previous sentence the first time your

 * read it, you're overqualified for your current job.)

 *

 * In other words, @orig is mapped onto (surjectively) @dst,

 * using the map { <n, m> | the n-th bit of @relmap is the

 * m-th set bit of @relmap }.

 *

 * Any set bits in @orig above bit number W, where W is the

 * weight of (number of set bits in) @relmap are mapped nowhere.

 * In particular, if for all bits m set in @orig, m >= W, then

 * @dst will end up empty.  In situations where the possibility

 * of such an empty result is not desired, one way to avoid it is

 * to use the bitmap_fold() operator, below, to first fold the

 * @orig bitmap over itself so that all its set bits x are in the

 * range 0 <= x < W.  The bitmap_fold() operator does this by

 * setting the bit (m % W) in @dst, for each bit (m) set in @orig.

 *

 * Example [1] for bitmap_onto():

 *  Let's say @relmap has bits 30-39 set, and @orig has bits

 *  1, 3, 5, 7, 9 and 11 set.  Then on return from this routine,

 *  @dst will have bits 31, 33, 35, 37 and 39 set.

 *

 *  When bit 0 is set in @orig, it means turn on the bit in

 *  @dst corresponding to whatever is the first bit (if any)

 *  that is turned on in @relmap.  Since bit 0 was off in the

 *  above example, we leave off that bit (bit 30) in @dst.

 *

 *  When bit 1 is set in @orig (as in the above example), it

 *  means turn on the bit in @dst corresponding to whatever

 *  is the second bit that is turned on in @relmap.  The second

 *  bit in @relmap that was turned on in the above example was

 *  bit 31, so we turned on bit 31 in @dst.

 *

 *  Similarly, we turned on bits 33, 35, 37 and 39 in @dst,

 *  because they were the 4th, 6th, 8th and 10th set bits

 *  set in @relmap, and the 4th, 6th, 8th and 10th bits of

 *  @orig (i.e. bits 3, 5, 7 and 9) were also set.

 *

 *  When bit 11 is set in @orig, it means turn on the bit in

 *  @dst corresponding to whatever is the twelfth bit that is

 *  turned on in @relmap.  In the above example, there were

 *  only ten bits turned on in @relmap (30..39), so that bit

 *  11 was set in @orig had no affect on @dst.

 *

 * Example [2] for bitmap_fold() + bitmap_onto():

 *  Let's say @relmap has these ten bits set::

 *

 *		40 41 42 43 45 48 53 61 74 95

 *

 *  (for the curious, that's 40 plus the first ten terms of the

 *  Fibonacci sequence.)

 *

 *  Further lets say we use the following code, invoking

 *  bitmap_fold() then bitmap_onto, as suggested above to

 *  avoid the possibility of an empty @dst result::

 *

 *	unsigned long *tmp;	// a temporary bitmap's bits

 *

 *	bitmap_fold(tmp, orig, bitmap_weight(relmap, bits), bits);

 *	bitmap_onto(dst, tmp, relmap, bits);

 *

 *  Then this table shows what various values of @dst would be, for

 *  various @orig's.  I list the zero-based positions of each set bit.

 *  The tmp column shows the intermediate result, as computed by

 *  using bitmap_fold() to fold the @orig bitmap modulo ten

 *  (the weight of @relmap):

 *

 *      =============== ============== =================

 *      @orig           tmp            @dst

 *      0                0             40

 *      1                1             41

 *      9                9             95

 *      10               0             40 [#f1]_

 *      1 3 5 7          1 3 5 7       41 43 48 61

 *      0 1 2 3 4        0 1 2 3 4     40 41 42 43 45

 *      0 9 18 27        0 9 8 7       40 61 74 95

 *      0 10 20 30       0             40

 *      0 11 22 33       0 1 2 3       40 41 42 43

 *      0 12 24 36       0 2 4 6       40 42 45 53

 *      78 102 211       1 2 8         41 42 74 [#f1]_

 *      =============== ============== =================

 *

 * .. [#f1]

 *

 *     For these marked lines, if we hadn't first done bitmap_fold()

 *     into tmp, then the @dst result would have been empty.

 *

 * If either of @orig or @relmap is empty (no set bits), then @dst

 * will be returned empty.

 *

 * If (as explained above) the only set bits in @orig are in positions

 * m where m >= W, (where W is the weight of @relmap) then @dst will

 * once again be returned empty.

 *

 * All bits in @dst not set by the above rule are cleared.

 same meaning as in above comment */

 following doesn't handle inplace mappings */

	/*

	 * The following code is a more efficient, but less

	 * obvious, equivalent to the loop:

	 *	for (m = 0; m < bitmap_weight(relmap, bits); m++) {

	 *		n = bitmap_ord_to_pos(orig, m, bits);

	 *		if (test_bit(m, orig))

	 *			set_bit(n, dst);

	 *	}

 m == bitmap_pos_to_ord(relmap, n, bits) */

/**

 * bitmap_fold - fold larger bitmap into smaller, modulo specified size

 *	@dst: resulting smaller bitmap

 *	@orig: original larger bitmap

 *	@sz: specified size

 *	@nbits: number of bits in each of these bitmaps

 *

 * For each bit oldbit in @orig, set bit oldbit mod @sz in @dst.

 * Clear all other bits in @dst.  See further the comment and

 * Example [2] for bitmap_onto() for why and how to use this.

 following doesn't handle inplace mappings */

 CONFIG_NUMA */

/*

 * Common code for bitmap_*_region() routines.

 *	bitmap: array of unsigned longs corresponding to the bitmap

 *	pos: the beginning of the region

 *	order: region size (log base 2 of number of bits)

 *	reg_op: operation(s) to perform on that region of bitmap

 *

 * Can set, verify and/or release a region of bits in a bitmap,

 * depending on which combination of REG_OP_* flag bits is set.

 *

 * A region of a bitmap is a sequence of bits in the bitmap, of

 * some size '1 << order' (a power of two), aligned to that same

 * '1 << order' power of two.

 *

 * Returns 1 if REG_OP_ISFREE succeeds (region is all zero bits).

 * Returns 0 in all other cases and reg_ops.

 true if region is all zero bits */

 set all bits in region */

 clear all bits in region */

 number of bits in region */

 index first long of region in bitmap */

 bit offset region in bitmap[index] */

 num longs spanned by region in bitmap */

 num bits of region in each spanned long */

 bitmask for one long of region */

 scans bitmap by longs */

 return value */

	/*

	 * Either nlongs_reg == 1 (for small orders that fit in one long)

	 * or (offset == 0 && mask == ~0UL) (for larger multiword orders.)

	/*

	 * Can't do "mask = (1UL << nbitsinlong) - 1", as that

	 * overflows if nbitsinlong == BITS_PER_LONG.

 all bits in region free (zero) */

/**

 * bitmap_find_free_region - find a contiguous aligned mem region

 *	@bitmap: array of unsigned longs corresponding to the bitmap

 *	@bits: number of bits in the bitmap

 *	@order: region size (log base 2 of number of bits) to find

 *

 * Find a region of free (zero) bits in a @bitmap of @bits bits and

 * allocate them (set them to one).  Only consider regions of length

 * a power (@order) of two, aligned to that power of two, which

 * makes the search algorithm much faster.

 *

 * Return the bit offset in bitmap of the allocated region,

 * or -errno on failure.

 scans bitmap by regions of size order */

/**

 * bitmap_release_region - release allocated bitmap region

 *	@bitmap: array of unsigned longs corresponding to the bitmap

 *	@pos: beginning of bit region to release

 *	@order: region size (log base 2 of number of bits) to release

 *

 * This is the complement to __bitmap_find_free_region() and releases

 * the found region (by clearing it in the bitmap).

 *

 * No return value.

/**

 * bitmap_allocate_region - allocate bitmap region

 *	@bitmap: array of unsigned longs corresponding to the bitmap

 *	@pos: beginning of bit region to allocate

 *	@order: region size (log base 2 of number of bits) to allocate

 *

 * Allocate (set bits in) a specified region of a bitmap.

 *

 * Return 0 on success, or %-EBUSY if specified region wasn't

 * free (not all bits were zero).

/**

 * bitmap_copy_le - copy a bitmap, putting the bits into little-endian order.

 * @dst:   destination buffer

 * @src:   bitmap to copy

 * @nbits: number of bits in the bitmap

 *

 * Require nbits % BITS_PER_LONG == 0.

/**

 * bitmap_from_arr32 - copy the contents of u32 array of bits to bitmap

 *	@bitmap: array of unsigned longs, the destination bitmap

 *	@buf: array of u32 (in host byte order), the source bitmap

 *	@nbits: number of bits in @bitmap

 Clear tail bits in last word beyond nbits. */

/**

 * bitmap_to_arr32 - copy the contents of bitmap to a u32 array of bits

 *	@buf: array of u32 (in host byte order), the dest bitmap

 *	@bitmap: array of unsigned longs, the source bitmap

 *	@nbits: number of bits in @bitmap

 Clear tail bits in last element of array beyond nbits. */

 SPDX-License-Identifier: GPL-2.0

/**

 * memweight - count the total number of bits set in memory area

 * @ptr: pointer to the start of the area

 * @bytes: the size of the area

	/*

	 * The reason that this last loop is distinct from the preceding

	 * bitmap_weight() call is to compute 1-bits in the last region smaller

	 * than sizeof(long) properly on big-endian systems.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 w.s.high = 1..1 or 0..0 */

 SPDX-License-Identifier: GPL-2.0

/*

 * This is a maximally equidistributed combined Tausworthe generator

 * based on code from GNU Scientific Library 1.5 (30 Jun 2004)

 *

 * lfsr113 version:

 *

 * x_n = (s1_n ^ s2_n ^ s3_n ^ s4_n)

 *

 * s1_{n+1} = (((s1_n & 4294967294) << 18) ^ (((s1_n <<  6) ^ s1_n) >> 13))

 * s2_{n+1} = (((s2_n & 4294967288) <<  2) ^ (((s2_n <<  2) ^ s2_n) >> 27))

 * s3_{n+1} = (((s3_n & 4294967280) <<  7) ^ (((s3_n << 13) ^ s3_n) >> 21))

 * s4_{n+1} = (((s4_n & 4294967168) << 13) ^ (((s4_n <<  3) ^ s4_n) >> 12))

 *

 * The period of this generator is about 2^113 (see erratum paper).

 *

 * From: P. L'Ecuyer, "Maximally Equidistributed Combined Tausworthe

 * Generators", Mathematics of Computation, 65, 213 (1996), 203--213:

 * http://www.iro.umontreal.ca/~lecuyer/myftp/papers/tausme.ps

 * ftp://ftp.iro.umontreal.ca/pub/simulation/lecuyer/papers/tausme.ps

 *

 * There is an erratum in the paper "Tables of Maximally Equidistributed

 * Combined LFSR Generators", Mathematics of Computation, 68, 225 (1999),

 * 261--269: http://www.iro.umontreal.ca/~lecuyer/myftp/papers/tausme2.ps

 *

 *      ... the k_j most significant bits of z_j must be non-zero,

 *      for each j. (Note: this restriction also applies to the

 *      computer code given in [4], but was mistakenly not mentioned

 *      in that paper.)

 *

 * This affects the seeding procedure by imposing the requirement

 * s1 > 1, s2 > 7, s3 > 15, s4 > 127.

/**

 *	prandom_u32_state - seeded pseudo-random number generator.

 *	@state: pointer to state structure holding seeded state.

 *

 *	This is used for pseudo-randomness with no outside seeding.

 *	For more random results, use prandom_u32().

/**

 *	prandom_bytes_state - get the requested number of pseudo-random bytes

 *

 *	@state: pointer to state structure holding seeded state.

 *	@buf: where to copy the pseudo-random bytes to

 *	@bytes: the requested number of bytes

 *

 *	This is used for pseudo-randomness with no outside seeding.

 *	For more random results, use prandom_bytes().

 Calling RNG ten times to satisfy recurrence condition */

 Test cases against taus113 from GSL library. */

 super-duper LCG */

/*

 * The prandom_u32() implementation is now completely separate from the

 * prandom_state() functions, which are retained (for now) for compatibility.

 *

 * Because of (ab)use in the networking code for choosing random TCP/UDP port

 * numbers, which open DoS possibilities if guessable, we want something

 * stronger than a standard PRNG.  But the performance requirements of

 * the network code do not allow robust crypto for this application.

 *

 * So this is a homebrew Junior Spaceman implementation, based on the

 * lowest-latency trustworthy crypto primitive available, SipHash.

 * (The authors of SipHash have not been consulted about this abuse of

 * their work.)

 *

 * Standard SipHash-2-4 uses 2n+4 rounds to hash n words of input to

 * one word of output.  This abbreviated version uses 2 rounds per word

 * of output.

/*

 * This is the core CPRNG function.  As "pseudorandom", this is not used

 * for truly valuable things, just intended to be a PITA to guess.

 * For maximum speed, we do just two SipHash rounds per word.  This is

 * the same rate as 4 rounds per 64 bits that SipHash normally uses,

 * so hopefully it's reasonably secure.

 *

 * There are two changes from the official SipHash finalization:

 * - We omit some constants XORed with v2 in the SipHash spec as irrelevant;

 *   they are there only to make the output rounds distinct from the input

 *   rounds, and this application has no input rounds.

 * - Rather than returning v0^v1^v2^v3, return v1+v3.

 *   If you look at the SipHash round, the last operation on v3 is

 *   "v3 ^= v0", so "v0 ^ v3" just undoes that, a waste of time.

 *   Likewise "v1 ^= v2".  (The rotate of v2 makes a difference, but

 *   it still cancels out half of the bits in v2 for no benefit.)

 *   Second, since the last combining operation was xor, continue the

 *   pattern of alternating xor/add for a tiny bit of extra non-linearity.

/**

 *	prandom_u32 - pseudo random number generator

 *

 *	A 32 bit pseudo-random number is generated using a fast

 *	algorithm suitable for simulation. This algorithm is NOT

 *	considered safe for cryptographic use.

/**

 *	prandom_bytes - get the requested number of pseudo-random bytes

 *	@buf: where to copy the pseudo-random bytes to

 *	@bytes: the requested number of bytes

/**

 *	prandom_seed - add entropy to pseudo random number generator

 *	@entropy: entropy value

 *

 *	Add some additional seed material to the prandom pool.

 *	The "entropy" is actually our IP address (the only caller is

 *	the network code), not for unpredictability, but to ensure that

 *	different machines are initialized differently.

/*

 *	Generate some initially weak seeding values to allow

 *	the prandom_u32() engine to be started.

 Stronger reseeding when available, and periodically thereafter. */

	/*

	 * Reinitialize each CPU's PRNG with 128 bits of key.

	 * No locking on the CPUs, but then somewhat random results are,

	 * well, expected.

		/*

		 * On 32-bit machines, hash in two extra words to

		 * approximate 128-bit key length.  Not that the hash

		 * has that much security, but this prevents a trivial

		 * 64-bit brute force.

		/*

		 * Probably impossible in practice, but there is a

		 * theoretical risk that a race between this reseeding

		 * and the target CPU writing its state back could

		 * create the all-zero SipHash fixed point.

		 *

		 * To ensure that never happens, ensure the state

		 * we write contains no zero words.

 reseed every ~60 seconds, in [40 .. 80) interval with slack */

/*

 * The random ready callback can be called from almost any interrupt.

 * To avoid worrying about whether it's safe to delay that interrupt

 * long enough to seed all CPUs, just schedule an immediate timer event.

/* Principle: True 32-bit random numbers will all have 16 differing bits on

 * average. For each 32-bit number, there are 601M numbers differing by 16

 * bits, and 89% of the numbers differ by at least 12 bits. Note that more

 * than 16 differing bits also implies a correlation with inverted bits. Thus

 * we take 1024 random numbers and compare each of them to the other ones,

 * counting the deviation of correlated bits to 16. Constants report 32,

 * counters 32-log2(TEST_SIZE), and pure randoms, around 6 or lower. With the

 * u32 total, TEST_SIZE may be as large as 4096 samples.

	/* We'll return the average deviation as 2*sqrt(corr/samples), which

	 * is also sqrt(4*corr/samples) which provides a better resolution.

  CONFIG_RANDOM32_SELFTEST */

/*

 * Start periodic full reseeding as soon as strong

 * random numbers are available.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Test for find_*_bit functions.

 *

 * Copyright (c) 2017 Cavium.

/*

 * find_bit functions are widely used in kernel, so the successful boot

 * is good enough test for correctness.

 *

 * This test is focused on performance of traversing bitmaps. Two typical

 * scenarios are reproduced:

 * - randomly filled bitmap with approximately equal number of set and

 *   cleared bits;

 * - sparse bitmap with few set bits at random positions.

/*

 * This is Schlemiel the Painter's algorithm. It should be called after

 * all other tests for the same bitmap because it sets all bits of bitmap to 1.

	/*

	 * test_find_first_bit() may take some time, so

	 * traverse only part of bitmap to avoid soft lockup.

	/*

	 * Everything is OK. Return error just to let user run benchmark

	 * again without annoying rmmod.

 SPDX-License-Identifier: GPL-2.0

/*

 * IOMMU helper functions for the free area management

 We don't want the last of the limit */

/* Copyright (C) 2016 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.

 *

 * This file is provided under a dual BSD/GPLv2 license.

 *

 * SipHash: a fast short-input PRF

 * https://131002.net/siphash/

 *

 * This implementation is specifically for SipHash2-4 for a secure PRF

 * and HalfSipHash1-3/SipHash1-3 for an insecure PRF only suitable for

 * hashtables.

/**

 * siphash_1u64 - compute 64-bit siphash PRF value of a u64

 * @first: first u64

 * @key: the siphash key

/**

 * siphash_2u64 - compute 64-bit siphash PRF value of 2 u64

 * @first: first u64

 * @second: second u64

 * @key: the siphash key

/**

 * siphash_3u64 - compute 64-bit siphash PRF value of 3 u64

 * @first: first u64

 * @second: second u64

 * @third: third u64

 * @key: the siphash key

/**

 * siphash_4u64 - compute 64-bit siphash PRF value of 4 u64

 * @first: first u64

 * @second: second u64

 * @third: third u64

 * @forth: forth u64

 * @key: the siphash key

/* Note that on 64-bit, we make HalfSipHash1-3 actually be SipHash1-3, for

 * performance reasons. On 32-bit, below, we actually implement HalfSipHash1-3.

/**

 * hsiphash_1u32 - compute 64-bit hsiphash PRF value of a u32

 * @first: first u32

 * @key: the hsiphash key

/**

 * hsiphash_2u32 - compute 32-bit hsiphash PRF value of 2 u32

 * @first: first u32

 * @second: second u32

 * @key: the hsiphash key

/**

 * hsiphash_3u32 - compute 32-bit hsiphash PRF value of 3 u32

 * @first: first u32

 * @second: second u32

 * @third: third u32

 * @key: the hsiphash key

/**

 * hsiphash_4u32 - compute 32-bit hsiphash PRF value of 4 u32

 * @first: first u32

 * @second: second u32

 * @third: third u32

 * @forth: forth u32

 * @key: the hsiphash key

/**

 * hsiphash_1u32 - compute 32-bit hsiphash PRF value of a u32

 * @first: first u32

 * @key: the hsiphash key

/**

 * hsiphash_2u32 - compute 32-bit hsiphash PRF value of 2 u32

 * @first: first u32

 * @second: second u32

 * @key: the hsiphash key

/**

 * hsiphash_3u32 - compute 32-bit hsiphash PRF value of 3 u32

 * @first: first u32

 * @second: second u32

 * @third: third u32

 * @key: the hsiphash key

/**

 * hsiphash_4u32 - compute 32-bit hsiphash PRF value of 4 u32

 * @first: first u32

 * @second: second u32

 * @third: third u32

 * @forth: forth u32

 * @key: the hsiphash key

 SPDX-License-Identifier: GPL-2.0

/*

 * Provide a default dump_stack() function for architectures

 * which don't implement their own.

/**

 * dump_stack_set_arch_desc - set arch-specific str to show with task dumps

 * @fmt: printf-style format string

 * @...: arguments for the format string

 *

 * The configured string will be printed right after utsname during task

 * dumps.  Usually used to add arch-specific system identifiers.  If an

 * arch wants to make use of such an ID string, it should initialize this

 * as soon as possible during boot.

/**

 * dump_stack_print_info - print generic debug info for dump_stack()

 * @log_lvl: log level

 *

 * Arch-specific dump_stack() implementations can use this function to

 * print out the same debug information as the generic dump_stack().

/**

 * show_regs_print_info - print generic debug info for show_regs()

 * @log_lvl: log level

 *

 * show_regs() implementations can use this function to print out generic

 * debug information.

/**

 * dump_stack_lvl - dump the current task information and its stack trace

 * @log_lvl: log level

 *

 * Architectures can override this implementation by implementing its own.

	/*

	 * Permit this cpu to perform nested stack dumps while serialising

	 * against other CPUs

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Initially, a percpu refcount is just a set of percpu counters. Initially, we

 * don't try to detect the ref hitting 0 - which means that get/put can just

 * increment or decrement the local counter. Note that the counter on a

 * particular cpu can (and will) wrap - this is fine, when we go to shutdown the

 * percpu counters will all sum to the correct value

 *

 * (More precisely: because modular arithmetic is commutative the sum of all the

 * percpu_count vars will be equal to what it would have been if all the gets

 * and puts were done to a single integer, even if some of the percpu integers

 * overflow or underflow).

 *

 * The real trick to implementing percpu refcounts is shutdown. We can't detect

 * the ref hitting 0 on every put - this would require global synchronization

 * and defeat the whole purpose of using percpu refs.

 *

 * What we do is require the user to keep track of the initial refcount; we know

 * the ref can't hit 0 before the user drops the initial ref, so as long as we

 * convert to non percpu mode before the initial ref is dropped everything

 * works.

 *

 * Converting to non percpu mode is done with some RCUish stuff in

 * percpu_ref_kill. Additionally, we need a bias value so that the

 * atomic_long_t can't hit 0 before we've added up all the percpu refs.

/**

 * percpu_ref_init - initialize a percpu refcount

 * @ref: percpu_ref to initialize

 * @release: function which will be called when refcount hits 0

 * @flags: PERCPU_REF_INIT_* flags

 * @gfp: allocation mask to use

 *

 * Initializes @ref.  @ref starts out in percpu mode with a refcount of 1 unless

 * @flags contains PERCPU_REF_INIT_ATOMIC or PERCPU_REF_INIT_DEAD.  These flags

 * change the start state to atomic with the latter setting the initial refcount

 * to 0.  See the definitions of PERCPU_REF_INIT_* flags for flag behaviors.

 *

 * Note that @release must not sleep - it may potentially be called from RCU

 * callback context by percpu_ref_kill().

 non-NULL confirm_switch indicates switching in progress */

/**

 * percpu_ref_exit - undo percpu_ref_init()

 * @ref: percpu_ref to exit

 *

 * This function exits @ref.  The caller is responsible for ensuring that

 * @ref is no longer in active use.  The usual places to invoke this

 * function from are the @ref->release() callback or in init failure path

 * where percpu_ref_init() succeeded but other parts of the initialization

 * of the embedding object failed.

 drop ref from percpu_ref_switch_to_atomic() */

	/*

	 * It's crucial that we sum the percpu counters _before_ adding the sum

	 * to &ref->count; since gets could be happening on one cpu while puts

	 * happen on another, adding a single cpu's count could cause

	 * @ref->count to hit 0 before we've got a consistent value - but the

	 * sum of all the counts will be consistent and correct.

	 *

	 * Subtracting the bias value then has to happen _after_ adding count to

	 * &ref->count; we need the bias value to prevent &ref->count from

	 * reaching 0 before we add the percpu counts. But doing it at the same

	 * time is equivalent and saves us atomic operations:

 @ref is viewed as dead on all CPUs, send out switch confirmation */

 switching from percpu to atomic */

	/*

	 * Non-NULL ->confirm_switch is used to indicate that switching is

	 * in progress.  Use noop one if unspecified.

 put after confirmation */

	/*

	 * Restore per-cpu operation.  smp_store_release() is paired

	 * with READ_ONCE() in __ref_is_percpu() and guarantees that the

	 * zeroing is visible to all percpu accesses which can see the

	 * following __PERCPU_REF_ATOMIC clearing.

	/*

	 * If the previous ATOMIC switching hasn't finished yet, wait for

	 * its completion.  If the caller ensures that ATOMIC switching

	 * isn't in progress, this function can be called from any context.

/**

 * percpu_ref_switch_to_atomic - switch a percpu_ref to atomic mode

 * @ref: percpu_ref to switch to atomic mode

 * @confirm_switch: optional confirmation callback

 *

 * There's no reason to use this function for the usual reference counting.

 * Use percpu_ref_kill[_and_confirm]().

 *

 * Schedule switching of @ref to atomic mode.  All its percpu counts will

 * be collected to the main atomic counter.  On completion, when all CPUs

 * are guaraneed to be in atomic mode, @confirm_switch, which may not

 * block, is invoked.  This function may be invoked concurrently with all

 * the get/put operations and can safely be mixed with kill and reinit

 * operations.  Note that @ref will stay in atomic mode across kill/reinit

 * cycles until percpu_ref_switch_to_percpu() is called.

 *

 * This function may block if @ref is in the process of switching to atomic

 * mode.  If the caller ensures that @ref is not in the process of

 * switching to atomic mode, this function can be called from any context.

/**

 * percpu_ref_switch_to_atomic_sync - switch a percpu_ref to atomic mode

 * @ref: percpu_ref to switch to atomic mode

 *

 * Schedule switching the ref to atomic mode, and wait for the

 * switch to complete.  Caller must ensure that no other thread

 * will switch back to percpu mode.

/**

 * percpu_ref_switch_to_percpu - switch a percpu_ref to percpu mode

 * @ref: percpu_ref to switch to percpu mode

 *

 * There's no reason to use this function for the usual reference counting.

 * To re-use an expired ref, use percpu_ref_reinit().

 *

 * Switch @ref to percpu mode.  This function may be invoked concurrently

 * with all the get/put operations and can safely be mixed with kill and

 * reinit operations.  This function reverses the sticky atomic state set

 * by PERCPU_REF_INIT_ATOMIC or percpu_ref_switch_to_atomic().  If @ref is

 * dying or dead, the actual switching takes place on the following

 * percpu_ref_reinit().

 *

 * This function may block if @ref is in the process of switching to atomic

 * mode.  If the caller ensures that @ref is not in the process of

 * switching to atomic mode, this function can be called from any context.

/**

 * percpu_ref_kill_and_confirm - drop the initial ref and schedule confirmation

 * @ref: percpu_ref to kill

 * @confirm_kill: optional confirmation callback

 *

 * Equivalent to percpu_ref_kill() but also schedules kill confirmation if

 * @confirm_kill is not NULL.  @confirm_kill, which may not block, will be

 * called after @ref is seen as dead from all CPUs at which point all

 * further invocations of percpu_ref_tryget_live() will fail.  See

 * percpu_ref_tryget_live() for details.

 *

 * This function normally doesn't block and can be called from any context

 * but it may block if @confirm_kill is specified and @ref is in the

 * process of switching to atomic mode by percpu_ref_switch_to_atomic().

 *

 * There are no implied RCU grace periods between kill and release.

/**

 * percpu_ref_is_zero - test whether a percpu refcount reached zero

 * @ref: percpu_ref to test

 *

 * Returns %true if @ref reached zero.

 *

 * This function is safe to call as long as @ref is between init and exit.

 protect us from being destroyed */

/**

 * percpu_ref_reinit - re-initialize a percpu refcount

 * @ref: perpcu_ref to re-initialize

 *

 * Re-initialize @ref so that it's in the same state as when it finished

 * percpu_ref_init() ignoring %PERCPU_REF_INIT_DEAD.  @ref must have been

 * initialized successfully and reached 0 but not exited.

 *

 * Note that percpu_ref_tryget[_live]() are safe to perform on @ref while

 * this function is in progress.

/**

 * percpu_ref_resurrect - modify a percpu refcount from dead to live

 * @ref: perpcu_ref to resurrect

 *

 * Modify @ref so that it's in the same state as before percpu_ref_kill() was

 * called. @ref must be dead but must not yet have exited.

 *

 * If @ref->release() frees @ref then the caller is responsible for

 * guaranteeing that @ref->release() does not get called while this

 * function is in progress.

 *

 * Note that percpu_ref_tryget[_live]() are safe to perform on @ref while

 * this function is in progress.

 SPDX-License-Identifier: GPL-2.0

/*

 * This is an implementation of the notion of "decrement a

 * reference count, and return locked if it decremented to zero".

 *

 * NOTE NOTE NOTE! This is _not_ equivalent to

 *

 *	if (atomic_dec_and_test(&atomic)) {

 *		spin_lock(&lock);

 *		return 1;

 *	}

 *	return 0;

 *

 * because the spin-lock and the decrement must be

 * "atomic".

 Subtract 1 from counter unless that drops it to 0 (ie. it was 1) */

 Otherwise do it the slow way */

 Subtract 1 from counter unless that drops it to 0 (ie. it was 1) */

 Otherwise do it the slow way */

 SPDX-License-Identifier: GPL-2.0

/*

 * Test cases for SL[AOU]B/page initialization at alloc/free time.

 Calculate the number of uninitialized bytes in the buffer. */

 Fill a buffer with garbage, skipping |skip| first bytes. */

 Test the page allocator by calling alloc_pages with different orders. */

 Test kmalloc() with given parameters. */

 Test vmalloc() with given parameters. */

 Test kmalloc()/vmalloc() by allocating objects of different sizes. */

 Initialize the first 4 bytes of the object. */

/*

 * Check the invariants for the buffer allocated from a slab cache.

 * If the cache has a test constructor, the first 4 bytes of the object must

 * always remain equal to CTOR_PATTERN.

 * If the cache isn't an RCU-typesafe one, or if the allocation is done with

 * __GFP_ZERO, then the object contents must be zeroed after allocation.

 * If the cache is an RCU-typesafe one, the object contents must never be

 * zeroed after the first use. This is checked by memcmp() in

 * do_kmem_cache_size().

/*

 * Test kmem_cache with given parameters:

 *  want_ctor - use a constructor;

 *  want_rcu - use SLAB_TYPESAFE_BY_RCU;

 *  want_zero - use __GFP_ZERO.

 Do a test of bulk allocations */

 Check that buf is zeroed, if it must be. */

		/*

		 * If this is an RCU cache, use a critical section to ensure we

		 * can touch objects after they're freed.

		/*

		 * Copy the buffer to check that it's not wiped on

		 * free().

		/*

		 * Check that |buf| is intact after kmem_cache_free().

		 * |want_zero| is false, because we wrote garbage to

		 * the buffer already.

/*

 * Check that the data written to an RCU-allocated object survives

 * reallocation.

	/*

	 * Run for a fixed number of iterations. If we never hit saved_ptr,

	 * assume the test passes.

/*

 * Test kmem_cache allocation by creating caches of different sizes, with and

 * without constructors, with and without SLAB_TYPESAFE_BY_RCU.

 Test the behavior of SLAB_TYPESAFE_BY_RCU caches of different sizes. */

/*

 * Run the tests. Each test function returns the number of executed tests and

 * updates |failures| with the number of failed tests.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *      crc-itu-t.c

* CRC table for the CRC ITU-T V.41 0x1021 (x^16 + x^12 + x^15 + 1) */

/**

 * crc_itu_t - Compute the CRC-ITU-T for the data buffer

 *

 * @crc:     previous CRC value

 * @buffer:  data pointer

 * @len:     number of bytes in the buffer

 *

 * Returns the updated CRC value

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Helpers for formatting and printing strings

 *

 * Copyright 31 August 2008 James Bottomley

 * Copyright (C) 2013, Intel Corporation

/**

 * string_get_size - get the size in the specified units

 * @size:	The size to be converted in blocks

 * @blk_size:	Size of the block (use 1 for size in bytes)

 * @units:	units to use (powers of 1000 or 1024)

 * @buf:	buffer to format to

 * @len:	length of buffer

 *

 * This function returns a string formatted to 3 significant figures

 * giving the size in the required units.  @buf should have room for

 * at least 9 bytes and will always be zero terminated.

 *

	/* This is Napier's algorithm.  Reduce the original block size to

	 *

	 * coefficient * divisor[units]^i

	 *

	 * we do the reduction so both coefficients are just under 32 bits so

	 * that multiplying them together won't overflow 64 bits and we keep

	 * as much precision as possible in the numbers.

	 *

	 * Note: it's safe to throw away the remainders here because all the

	 * precision is in the coefficients.

	/* now perform the actual multiplication keeping i as the sum of the

 and logarithmically reduce it until it's just under the divisor */

	/* work out in j how many digits of precision we need from the

		/* express the remainder as a decimal.  It's currently the

		 * numerator of a fraction whose denominator is

	/* add a 5 to the digit below what will be printed to ensure

/**

 * string_unescape - unquote characters in the given string

 * @src:	source buffer (escaped)

 * @dst:	destination buffer (unescaped)

 * @size:	size of the destination buffer (0 to unlimit)

 * @flags:	combination of the flags.

 *

 * Description:

 * The function unquotes characters in the given string.

 *

 * Because the size of the output will be the same as or less than the size of

 * the input, the transformation may be performed in place.

 *

 * Caller must provide valid source and destination pointers. Be aware that

 * destination buffer will always be NULL-terminated. Source string must be

 * NULL-terminated as well.  The supported flags are::

 *

 *	UNESCAPE_SPACE:

 *		'\f' - form feed

 *		'\n' - new line

 *		'\r' - carriage return

 *		'\t' - horizontal tab

 *		'\v' - vertical tab

 *	UNESCAPE_OCTAL:

 *		'\NNN' - byte with octal value NNN (1 to 3 digits)

 *	UNESCAPE_HEX:

 *		'\xHH' - byte with hexadecimal value HH (1 to 2 digits)

 *	UNESCAPE_SPECIAL:

 *		'\"' - double quote

 *		'\\' - backslash

 *		'\a' - alert (BEL)

 *		'\e' - escape

 *	UNESCAPE_ANY:

 *		all previous together

 *

 * Return:

 * The amount of the characters processed to the destination buffer excluding

 * trailing '\0' is returned.

/**

 * string_escape_mem - quote characters in the given memory buffer

 * @src:	source buffer (unescaped)

 * @isz:	source buffer size

 * @dst:	destination buffer (escaped)

 * @osz:	destination buffer size

 * @flags:	combination of the flags

 * @only:	NULL-terminated string containing characters used to limit

 *		the selected escape class. If characters are included in @only

 *		that would not normally be escaped by the classes selected

 *		in @flags, they will be copied to @dst unescaped.

 *

 * Description:

 * The process of escaping byte buffer includes several parts. They are applied

 * in the following sequence.

 *

 *	1. The character is not matched to the one from @only string and thus

 *	   must go as-is to the output.

 *	2. The character is matched to the printable and ASCII classes, if asked,

 *	   and in case of match it passes through to the output.

 *	3. The character is matched to the printable or ASCII class, if asked,

 *	   and in case of match it passes through to the output.

 *	4. The character is checked if it falls into the class given by @flags.

 *	   %ESCAPE_OCTAL and %ESCAPE_HEX are going last since they cover any

 *	   character. Note that they actually can't go together, otherwise

 *	   %ESCAPE_HEX will be ignored.

 *

 * Caller must provide valid source and destination pointers. Be aware that

 * destination buffer will not be NULL-terminated, thus caller have to append

 * it if needs. The supported flags are::

 *

 *	%ESCAPE_SPACE: (special white space, not space itself)

 *		'\f' - form feed

 *		'\n' - new line

 *		'\r' - carriage return

 *		'\t' - horizontal tab

 *		'\v' - vertical tab

 *	%ESCAPE_SPECIAL:

 *		'\"' - double quote

 *		'\\' - backslash

 *		'\a' - alert (BEL)

 *		'\e' - escape

 *	%ESCAPE_NULL:

 *		'\0' - null

 *	%ESCAPE_OCTAL:

 *		'\NNN' - byte with octal value NNN (3 digits)

 *	%ESCAPE_ANY:

 *		all previous together

 *	%ESCAPE_NP:

 *		escape only non-printable characters, checked by isprint()

 *	%ESCAPE_ANY_NP:

 *		all previous together

 *	%ESCAPE_HEX:

 *		'\xHH' - byte with hexadecimal value HH (2 digits)

 *	%ESCAPE_NA:

 *		escape only non-ascii characters, checked by isascii()

 *	%ESCAPE_NAP:

 *		escape only non-printable or non-ascii characters

 *	%ESCAPE_APPEND:

 *		append characters from @only to be escaped by the given classes

 *

 * %ESCAPE_APPEND would help to pass additional characters to the escaped, when

 * one of %ESCAPE_NP, %ESCAPE_NA, or %ESCAPE_NAP is provided.

 *

 * One notable caveat, the %ESCAPE_NAP, %ESCAPE_NP and %ESCAPE_NA have the

 * higher priority than the rest of the flags (%ESCAPE_NAP is the highest).

 * It doesn't make much sense to use either of them without %ESCAPE_OCTAL

 * or %ESCAPE_HEX, because they cover most of the other character classes.

 * %ESCAPE_NAP can utilize %ESCAPE_SPACE or %ESCAPE_SPECIAL in addition to

 * the above.

 *

 * Return:

 * The total size of the escaped output that would be generated for

 * the given input and flags. To check whether the output was

 * truncated, compare the return value to osz. There is room left in

 * dst for a '\0' terminator if and only if ret < osz.

		/*

		 * Apply rules in the following sequence:

		 *	- the @only string is supplied and does not contain a

		 *	  character under question

		 *	- the character is printable and ASCII, when @flags has

		 *	  %ESCAPE_NAP bit set

		 *	- the character is printable, when @flags has

		 *	  %ESCAPE_NP bit set

		 *	- the character is ASCII, when @flags has

		 *	  %ESCAPE_NA bit set

		 *	- the character doesn't fall into a class of symbols

		 *	  defined by given @flags

		 * In these cases we just pass through a character to the

		 * output buffer.

		 *

		 * When %ESCAPE_APPEND is passed, the characters from @only

		 * have been excluded from the %ESCAPE_NAP, %ESCAPE_NP, and

		 * %ESCAPE_NA cases.

 ESCAPE_OCTAL and ESCAPE_HEX always go last */

/*

 * Return an allocated string that has been escaped of special characters

 * and double quotes, making it safe to log in quotes.

/*

 * Returns allocated NULL-terminated string containing process

 * command line, with inter-argument NULLs replaced with spaces,

 * and other special characters escaped.

 Collapse trailing NULLs, leave res pointing to last non-NULL. */

 Replace inter-argument NULLs. */

 Make sure result is printable. */

/*

 * Returns allocated NULL-terminated string containing pathname,

 * with special characters escaped, able to be safely logged. If

 * there is an error, the leading character will be "<".

 We add 11 spaces for ' (deleted)' to be appended */

/**

 * kfree_strarray - free a number of dynamically allocated strings contained

 *                  in an array and the array itself

 *

 * @array: Dynamically allocated array of strings to free.

 * @n: Number of strings (starting from the beginning of the array) to free.

 *

 * Passing a non-NULL @array and @n == 0 as well as NULL @array are valid

 * use-cases. If @array is NULL, the function does nothing.

/**

 * strscpy_pad() - Copy a C-string into a sized buffer

 * @dest: Where to copy the string to

 * @src: Where to copy the string from

 * @count: Size of destination buffer

 *

 * Copy the string, or as much of it as fits, into the dest buffer.  The

 * behavior is undefined if the string buffers overlap.  The destination

 * buffer is always %NUL terminated, unless it's zero-sized.

 *

 * If the source string is shorter than the destination buffer, zeros

 * the tail of the destination buffer.

 *

 * For full explanation of why you may want to consider using the

 * 'strscpy' functions please see the function docstring for strscpy().

 *

 * Returns:

 * * The number of characters copied (not including the trailing %NUL)

 * * -E2BIG if count is 0 or @src was truncated.

/**

 * skip_spaces - Removes leading whitespace from @str.

 * @str: The string to be stripped.

 *

 * Returns a pointer to the first non-whitespace character in @str.

/**

 * strim - Removes leading and trailing whitespace from @s.

 * @s: The string to be stripped.

 *

 * Note that the first trailing whitespace is replaced with a %NUL-terminator

 * in the given string @s. Returns a pointer to the first non-whitespace

 * character in @s.

/**

 * sysfs_streq - return true if strings are equal, modulo trailing newline

 * @s1: one string

 * @s2: another string

 *

 * This routine returns true iff two strings are equal, treating both

 * NUL and newline-then-NUL as equivalent string terminations.  It's

 * geared for use with sysfs input strings, which generally terminate

 * with newlines but are compared against values without newlines.

/**

 * match_string - matches given string in an array

 * @array:	array of strings

 * @n:		number of strings in the array or -1 for NULL terminated arrays

 * @string:	string to match with

 *

 * This routine will look for a string in an array of strings up to the

 * n-th element in the array or until the first NULL element.

 *

 * Historically the value of -1 for @n, was used to search in arrays that

 * are NULL terminated. However, the function does not make a distinction

 * when finishing the search: either @n elements have been compared OR

 * the first NULL element was found.

 *

 * Return:

 * index of a @string in the @array if matches, or %-EINVAL otherwise.

/**

 * __sysfs_match_string - matches given string in an array

 * @array: array of strings

 * @n: number of strings in the array or -1 for NULL terminated arrays

 * @str: string to match with

 *

 * Returns index of @str in the @array or -EINVAL, just like match_string().

 * Uses sysfs_streq instead of strcmp for matching.

 *

 * This routine will look for a string in an array of strings up to the

 * n-th element in the array or until the first NULL element.

 *

 * Historically the value of -1 for @n, was used to search in arrays that

 * are NULL terminated. However, the function does not make a distinction

 * when finishing the search: either @n elements have been compared OR

 * the first NULL element was found.

/**

 * strreplace - Replace all occurrences of character in string.

 * @s: The string to operate on.

 * @old: The character being replaced.

 * @new: The character @old is replaced with.

 *

 * Returns pointer to the nul byte at the end of @s.

/**

 * memcpy_and_pad - Copy one buffer to another with padding

 * @dest: Where to copy to

 * @dest_len: The destination buffer size

 * @src: Where to copy from

 * @count: The number of bytes to copy

 * @pad: Character to use for padding if space is left in destination.

 CONFIG_FORTIFY_SOURCE */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Expecting three errors.

	 * One for the corrupted freechain and the other one for the wrong

	 * count of objects in use. The third error is fixing broken cache.

	/*

	 * Try to repair corrupted freepointer.

	 * Still expecting two errors. The first for the wrong count

	 * of objects in use.

	 * The second error is for fixing broken cache.

	/*

	 * Previous validation repaired the count of objects in use.

	 * Now expecting no error.

 SPDX-License-Identifier: GPL-2.0-or-later

/* Generic associative array implementation.

 *

 * See Documentation/core-api/assoc_array.rst for information.

 *

 * Copyright (C) 2013 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

#define DEBUG

/*

 * Iterate over an associative array.  The caller must hold the RCU read lock

 * or better.

 Descend through a shortcut */

 Address dependency. */

	/* We perform two passes of each node.

	 *

	 * The first pass does all the leaves in this node.  This means we

	 * don't miss any leaves if the node is split up by insertion whilst

	 * we're iterating over the branches rooted here (we may, however, see

	 * some leaves twice).

 Address dependency. */

			/* We need a barrier between the read of the pointer,

			 * which is supplied by the above READ_ONCE().

 Invoke the callback */

	/* The second pass attends to all the metadata pointers.  If we follow

	 * one of these we may find that we don't come back here, but rather go

	 * back to a replacement node with the leaves in a different layout.

	 *

	 * We are guaranteed to make progress, however, as the slot number for

	 * a particular portion of the key space cannot change - and we

	 * continue at the back pointer + 1.

 Address dependency. */

 Move up to the parent (may need to skip back over a shortcut) */

 Address dependency. */

 Address dependency. */

 Ascend to next slot in parent node */

/**

 * assoc_array_iterate - Pass all objects in the array to a callback

 * @array: The array to iterate over.

 * @iterator: The callback function.

 * @iterator_data: Private data for the callback function.

 *

 * Iterate over all the objects in an associative array.  Each one will be

 * presented to the iterator function.

 *

 * If the array is being modified concurrently with the iteration then it is

 * possible that some objects in the array will be passed to the iterator

 * callback more than once - though every object should be passed at least

 * once.  If this is undesirable then the caller must lock against modification

 * for the duration of this function.

 *

 * The function will return 0 if no objects were in the array or else it will

 * return the result of the last iterator function called.  Iteration stops

 * immediately if any call to the iteration function results in a non-zero

 * return.

 *

 * The caller should hold the RCU read lock or better if concurrent

 * modification is possible.

 Address dependency. */

 Node in which leaf might be found */

/*

 * Navigate through the internal tree looking for the closest node to the key.

 Address dependency. */

	/* Use segments from the key for the new leaf to navigate through the

	 * internal tree, skipping through nodes and shortcuts that are on

	 * route to the destination.  Eventually we'll come to a slot that is

	 * either empty or contains a leaf at which point we've found a node in

	 * which the leaf we're looking for might be found or into which it

	 * should be inserted.

 Address dependency. */

		/* The node doesn't have a node/shortcut pointer in the slot

		 * corresponding to the index key that we have to follow.

		/* There is a pointer to a node in the slot corresponding to

		 * this index key segment, so we need to follow it.

	/* There is a shortcut in the slot corresponding to the index key

	 * segment.  We follow the shortcut if its partial index key matches

	 * this leaf's.  Otherwise we need to split the shortcut.

		/* Check the leaf against the shortcut's index key a word at a

		 * time, trimming the final word (the shortcut stores the index

		 * key completely from the root to the shortcut's target).

 Trim segments that are beyond the shortcut */

 This shortcut points elsewhere */

 The shortcut matches the leaf's index to this point. */

 Address dependency. */

/**

 * assoc_array_find - Find an object by index key

 * @array: The associative array to search.

 * @ops: The operations to use.

 * @index_key: The key to the object.

 *

 * Find an object in an associative array by walking through the internal tree

 * to the node that should contain the object and then searching the leaves

 * there.  NULL is returned if the requested object was not found in the array.

 *

 * The caller must hold the RCU read lock or better.

	/* If the target key is available to us, it's has to be pointed to by

	 * the terminal node.

 Address dependency. */

			/* We need a barrier between the read of the pointer

			 * and dereferencing the pointer - but only if we are

			 * actually going to dereference it.

/*

 * Destructively iterate over an associative array.  The caller must prevent

 * other simultaneous accesses.

 Descend through a shortcut */

 Done */

	/* Move back up to the parent (may need to free a shortcut on

 Ascend to next slot in parent node */

/**

 * assoc_array_destroy - Destroy an associative array

 * @array: The array to destroy.

 * @ops: The operations to use.

 *

 * Discard all metadata and free all objects in an associative array.  The

 * array will be empty and ready to use again upon completion.  This function

 * cannot fail.

 *

 * The caller must prevent all other accesses whilst this takes place as no

 * attempt is made to adjust pointers gracefully to permit RCU readlock-holding

 * accesses to continue.  On the other hand, no memory allocation is required.

/*

 * Handle insertion into an empty tree.

/*

 * Handle insertion into a terminal node.

	/* We arrived at a node which doesn't have an onward node or shortcut

	 * pointer that we have to follow.  This means that (a) the leaf we

	 * want must go here (either by insertion or replacement) or (b) we

	 * need to split this node and insert in one of the fragments.

	/* Firstly, we have to check the leaves in this node to see if there's

	 * a matching one we should replace in place.

	/* If there is a free slot in this node then we can just insert the

	 * leaf here.

	/* The node has no spare slots - so we're either going to have to split

	 * it or insert another node before it.

	 *

	 * Whatever, we're going to need at least two new nodes - so allocate

	 * those now.  We may also need a new shortcut, but we deal with that

	 * when we need it.

 We need to find out how similar the leaves are. */

 The node contains only leaves */

		/* The old leaves all cluster in the same slot.  We will need

		 * to insert a shortcut if the new node wants to cluster with them.

		/* Otherwise all the old leaves cluster in the same slot, but

		 * the new leaf wants to go into a different slot - so we

		 * create a new node (n0) to hold the new leaf and a pointer to

		 * a new node (n1) holding all the old leaves.

		 *

		 * This can be done by falling through to the node splitting

		 * path.

	/* We need to split the current node.  The node must contain anything

	 * from a single leaf (in the one leaf case, this leaf will cluster

	 * with the new leaf) and the rest meta-pointers, to all leaves, some

	 * of which may cluster.

	 *

	 * It won't contain the case in which all the current leaves plus the

	 * new leaves want to cluster in the same slot.

	 *

	 * We need to expel at least two leaves out of a set consisting of the

	 * leaves in the node and the new leaf.  The current meta pointers can

	 * just be copied as they shouldn't cluster with any of the leaves.

	 *

	 * We need a new node (n0) to replace the current one and a new node to

	 * take the expelled nodes (n1).

 Need to calculate this */

	/* Begin by finding two matching leaves.  There have to be at least two

	 * that match - even if there are meta pointers - because any leaf that

	 * would match a slot with a meta pointer in it must be somewhere

	 * behind that meta pointer and cannot be here.  Further, given N

	 * remaining leaf slots, we now have N+1 leaves to go in them.

 Metadata pointers cannot change slot */

 Filter the leaf pointers between the new nodes */

	/* All the leaves, new and old, want to cluster together in this node

	 * in the same slot, so we have to replace this node with a shortcut to

	 * skip over the identical parts of the key and then place a pair of

	 * nodes, one inside the other, at the end of the shortcut and

	 * distribute the keys between them.

	 *

	 * Firstly we need to work out where the leaves start diverging as a

	 * bit position into their keys so that we know how big the shortcut

	 * needs to be.

	 *

	 * We only need to make a single pass of N of the N+1 leaves because if

	 * any keys differ between themselves at bit X then at least one of

	 * them must also differ with the base key at bit X or before.

 Need to calculate this */

	/* This now reduces to a node splitting exercise for which we'll need

	 * to regenerate the disparity table.

/*

 * Handle insertion into the middle of a shortcut.

	/* We need to split a shortcut and insert a node between the two

	 * pieces.  Zero-length pieces will be dispensed with entirely.

	 *

	 * First of all, we need to find out in which level the first

	 * difference was.

 Create a new node now since we're going to need it anyway */

	/* Insert a new shortcut before the new node if this segment isn't of

	 * zero length - otherwise we just connect the new node directly to the

	 * parent.

	/* We need to know which slot in the new node is going to take a

	 * metadata pointer.

	/* Determine whether we need to follow the new node with a replacement

	 * for the current shortcut.  We could in theory reuse the current

	 * shortcut if its parent slot number doesn't change - but that's a

	 * 1-in-16 chance so not worth expending the code upon.

		/* We don't have to replace the pointed-to node as long as we

		 * use memory barriers to make sure the parent slot number is

		 * changed before the back pointer (the parent slot number is

		 * irrelevant to the old parent shortcut).

 Install the new leaf in a spare slot in the new node. */

/**

 * assoc_array_insert - Script insertion of an object into an associative array

 * @array: The array to insert into.

 * @ops: The operations to use.

 * @index_key: The key to insert at.

 * @object: The object to insert.

 *

 * Precalculate and preallocate a script for the insertion or replacement of an

 * object in an associative array.  This results in an edit script that can

 * either be applied or cancelled.

 *

 * The function returns a pointer to an edit script or -ENOMEM.

 *

 * The caller should lock against other modifications and must continue to hold

 * the lock until assoc_array_apply_edit() has been called.

 *

 * Accesses to the tree may take place concurrently with this function,

 * provided they hold the RCU read lock.

	/* The leaf pointer we're given must not have the bottom bit set as we

	 * use those for type-marking the pointer.  NULL pointers are also not

	 * allowed as they indicate an empty slot but we have to allow them

	 * here as they can be updated later.

 Allocate a root node if there isn't one yet */

		/* We found a node that doesn't have a node/shortcut pointer in

		 * the slot corresponding to the index key that we have to

		 * follow.

		/* We found a shortcut that didn't match our key in a slot we

		 * needed to follow.

 Clean up after an out of memory error */

/**

 * assoc_array_insert_set_object - Set the new object pointer in an edit script

 * @edit: The edit script to modify.

 * @object: The object pointer to set.

 *

 * Change the object to be inserted in an edit script.  The object pointed to

 * by the old object is not freed.  This must be done prior to applying the

 * script.

/*

 * Subtree collapse to node iterator.

/**

 * assoc_array_delete - Script deletion of an object from an associative array

 * @array: The array to search.

 * @ops: The operations to use.

 * @index_key: The key to the object.

 *

 * Precalculate and preallocate a script for the deletion of an object from an

 * associative array.  This results in an edit script that can either be

 * applied or cancelled.

 *

 * The function returns a pointer to an edit script if the object was found,

 * NULL if the object was not found or -ENOMEM.

 *

 * The caller should lock against other modifications and must continue to hold

 * the lock until assoc_array_apply_edit() has been called.

 *

 * Accesses to the tree may take place concurrently with this function,

 * provided they hold the RCU read lock.

		/* We found a node that should contain the leaf we've been

		 * asked to remove - *if* it's in the tree.

	/* In the simplest form of deletion we just clear the slot and release

	 * the leaf after a suitable interval.

	/* If that concludes erasure of the last leaf, then delete the entire

	 * internal array.

	/* However, we'd also like to clear up some metadata blocks if we

	 * possibly can.

	 *

	 * We go for a simple algorithm of: if this node has FAN_OUT or fewer

	 * leaves in it, then attempt to collapse it - and attempt to

	 * recursively collapse up the tree.

	 *

	 * We could also try and collapse in partially filled subtrees to take

	 * up space in this node.

		/* First of all, we need to know if this node has metadata so

		 * that we don't try collapsing if all the leaves are already

		 * here.

		/* Look further up the tree to see if we can collapse this node

		 * into a more proximal node too.

		/* There's no point collapsing if the original node has no meta

		 * pointers to discard and if we didn't merge into one of that

		 * node's ancestry.

 Create a new node to collapse into */

 Clean up after an out of memory error */

/**

 * assoc_array_clear - Script deletion of all objects from an associative array

 * @array: The array to clear.

 * @ops: The operations to use.

 *

 * Precalculate and preallocate a script for the deletion of all the objects

 * from an associative array.  This results in an edit script that can either

 * be applied or cancelled.

 *

 * The function returns a pointer to an edit script if there are objects to be

 * deleted, NULL if there are no objects in the array or -ENOMEM.

 *

 * The caller should lock against other modifications and must continue to hold

 * the lock until assoc_array_apply_edit() has been called.

 *

 * Accesses to the tree may take place concurrently with this function,

 * provided they hold the RCU read lock.

/*

 * Handle the deferred destruction after an applied edit.

/**

 * assoc_array_apply_edit - Apply an edit script to an associative array

 * @edit: The script to apply.

 *

 * Apply an edit script to an associative array to effect an insertion,

 * deletion or clearance.  As the edit script includes preallocated memory,

 * this is guaranteed not to fail.

 *

 * The edit script, dead objects and dead metadata will be scheduled for

 * destruction after an RCU grace period to permit those doing read-only

 * accesses on the array to continue to do so under the RCU read lock whilst

 * the edit is taking place.

/**

 * assoc_array_cancel_edit - Discard an edit script.

 * @edit: The script to discard.

 *

 * Free an edit script and all the preallocated data it holds without making

 * any changes to the associative array it was intended for.

 *

 * NOTE!  In the case of an insertion script, this does _not_ release the leaf

 * that was to be inserted.  That is left to the caller.

 Clean up after an out of memory error */

/**

 * assoc_array_gc - Garbage collect an associative array.

 * @array: The array to clean.

 * @ops: The operations to use.

 * @iterator: A callback function to pass judgement on each object.

 * @iterator_data: Private data for the callback function.

 *

 * Collect garbage from an associative array and pack down the internal tree to

 * save memory.

 *

 * The iterator function is asked to pass judgement upon each object in the

 * array.  If it returns false, the object is discard and if it returns true,

 * the object is kept.  If it returns true, it must increment the object's

 * usage count (or whatever it needs to do to retain it) before returning.

 *

 * This function returns 0 if successful or -ENOMEM if out of memory.  In the

 * latter case, the array is not changed.

 *

 * The caller should lock against other modifications and must continue to hold

 * the lock until assoc_array_apply_edit() has been called.

 *

 * Accesses to the tree may take place concurrently with this function,

 * provided they hold the RCU read lock.

	/* If this point is a shortcut, then we need to duplicate it and

	 * advance the target cursor.

 Duplicate the node at this position */

 Filter across any leaves and gc any subtrees */

				/* The iterator will have done any reference

				 * counting on the object for us.

	/* Count up the number of empty slots in this node and work out the

	 * subtree leaf count.

 See what we can fold in */

 Fold the child node into this one */

			/* We would already have reaped an intervening shortcut

			 * on the way back up the tree.

 Excise this node if it is singly occupied by a shortcut */

 We can discard any preceding shortcut also */

	/* Excise any shortcuts we might encounter that point to nodes that

	 * only contain leaves.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Generic implementation of 64-bit atomics using spinlocks,

 * useful on processors that don't have 64-bit atomic instructions.

 *

 * Copyright © 2009 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>

/*

 * We use a hashed array of spinlocks to provide exclusive access

 * to each atomic64_t variable.  Since this is expected to used on

 * systems with small numbers of CPUs (<= 4 or so), we use a

 * relatively small array of 16 spinlocks to avoid wasting too much

 * memory on the spinlock array.

/*

 * Ensure each lock is in a separate cacheline.

 SPDX-License-Identifier: GPL-2.0

/*

 * Normal 64-bit CRC calculation.

 *

 * This is a basic crc64 implementation following ECMA-182 specification,

 * which can be found from,

 * https://www.ecma-international.org/publications/standards/Ecma-182.htm

 *

 * Dr. Ross N. Williams has a great document to introduce the idea of CRC

 * algorithm, here the CRC64 code is also inspired by the table-driven

 * algorithm and detail example from this paper. This paper can be found

 * from,

 * http://www.ross.net/crc/download/crc_v3.txt

 *

 * crc64table[256] is the lookup table of a table-driven 64-bit CRC

 * calculation, which is generated by gen_crc64table.c in kernel build

 * time. The polynomial of crc64 arithmetic is from ECMA-182 specification

 * as well, which is defined as,

 *

 * x^64 + x^62 + x^57 + x^55 + x^54 + x^53 + x^52 + x^47 + x^46 + x^45 +

 * x^40 + x^39 + x^38 + x^37 + x^35 + x^33 + x^32 + x^31 + x^29 + x^27 +

 * x^24 + x^23 + x^22 + x^21 + x^19 + x^17 + x^13 + x^12 + x^10 + x^9 +

 * x^7 + x^4 + x + 1

 *

 * Copyright 2018 SUSE Linux.

 *   Author: Coly Li <colyli@suse.de>

/**

 * crc64_be - Calculate bitwise big-endian ECMA-182 CRC64

 * @crc: seed value for computation. 0 or (u64)~0 for a new CRC calculation,

 *       or the previous crc64 value if computing incrementally.

 * @p: pointer to buffer over which CRC64 is run

 * @len: length of buffer @p

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/lib/vsprintf.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 vsprintf.c -- Lars Wirzenius & Linus Torvalds. */

/*

 * Wirzenius wrote this portably, Torvalds fucked it up :-)

/*

 * Fri Jul 13 2001 Crutcher Dunnavant <crutcher+kernel@datastacks.com>

 * - changed to provide snprintf and vsnprintf functions

 * So Feb  1 16:51:32 CET 2004 Juergen Quade <quade@hsnr.de>

 * - scnprintf and vscnprintf

 for KSYM_SYMBOL_LEN */

 For the trace_print_flags arrays */

 for PAGE_SIZE */

 cpu_to_le16 */

 FIXME */

 Field too short for prefix + digit, skip over without converting */

/**

 * simple_strtoull - convert a string to an unsigned long long

 * @cp: The start of the string

 * @endp: A pointer to the end of the parsed string will be placed here

 * @base: The number base to use

 *

 * This function has caveats. Please use kstrtoull instead.

/**

 * simple_strtoul - convert a string to an unsigned long

 * @cp: The start of the string

 * @endp: A pointer to the end of the parsed string will be placed here

 * @base: The number base to use

 *

 * This function has caveats. Please use kstrtoul instead.

/**

 * simple_strtol - convert a string to a signed long

 * @cp: The start of the string

 * @endp: A pointer to the end of the parsed string will be placed here

 * @base: The number base to use

 *

 * This function has caveats. Please use kstrtol instead.

	/*

	 * simple_strntoull() safely handles receiving max_chars==0 in the

	 * case cp[0] == '-' && max_chars == 1.

	 * If max_chars == 0 we can drop through and pass it to simple_strntoull()

	 * and the content of *cp is irrelevant.

/**

 * simple_strtoll - convert a string to a signed long long

 * @cp: The start of the string

 * @endp: A pointer to the end of the parsed string will be placed here

 * @base: The number base to use

 *

 * This function has caveats. Please use kstrtoll instead.

/*

 * Decimal conversion is by far the most typical, and is used for

 * /proc and /sys data. This directly impacts e.g. top performance

 * with many processes running. We optimize it for speed by emitting

 * two characters at a time, using a 200 byte lookup table. This

 * roughly halves the number of multiplications compared to computing

 * the digits one at a time. Implementation strongly inspired by the

 * previous version, which in turn used ideas described at

 * <http://www.cs.uiowa.edu/~jones/bcd/divide.html> (with permission

 * from the author, Douglas W. Jones).

 *

 * It turns out there is precisely one 26 bit fixed-point

 * approximation a of 64/100 for which x/100 == (x * (u64)a) >> 32

 * holds for all x in [0, 10^8-1], namely a = 0x28f5c29. The actual

 * range happens to be somewhat larger (x <= 1073741898), but that's

 * irrelevant for our purpose.

 *

 * For dividing a number in the range [10^4, 10^6-1] by 100, we still

 * need a 32x32->64 bit multiply, so we simply use the same constant.

 *

 * For dividing a number in the range [100, 10^4-1] by 100, there are

 * several options. The simplest is (x * 0x147b) >> 19, which is valid

 * for all x <= 43698.

/*

 * This will print a single '0' even if r == 0, since we would

 * immediately jump to out_r where two 0s would be written but only

 * one of them accounted for in buf. This is needed by ip4_string

 * below. All other callers pass a non-zero value of r.

 1 <= r < 10^8 */

 100 <= r < 10^8 */

 1 <= q < 10^6 */

  100 <= q < 10^6 */

 1 <= r < 10^4 */

 100 <= r < 10^4 */

 1 <= q < 100 */

 1 <= r < 100 */

 0 <= r < 10^8 */

 0 <= q < 10^6 */

 0 <= r < 10^4 */

 0 <= q < 100 */

 1 <= n <= 1.6e11 */

 1 <= n < 1e8 */

 0 <= r < 10^4 */

 0 <= q < 100 */

/*

 * Call put_dec_full4 on x % 10000, return x / 10000.

 * The approximation x/10000 == (x * 0x346DC5D7) >> 43

 * holds for all x < 1,128,869,999.  The largest value this

 * helper will ever be asked to convert is 1,125,520,955.

 * (second call in the put_dec code, assuming n is all-ones).

/* Based on code by Douglas W. Jones found at

 * <http://www.cs.uiowa.edu/~jones/bcd/decimal.html#sixtyfour>

 * (with permission from the author).

 * Performs no 64-bit division and hence should be fast on 32-bit machines.

 implicit "& 0xffff" */

 implicit "& 0xffff" */

	/* n = 2^48 d3 + 2^32 d2 + 2^16 d1 + d0

/*

 * Convert passed number to decimal string.

 * Returns the length of string.  On buffer overflow, returns 0.

 *

 * If speed is not important, use snprintf(). It's easy to read the code.

 put_dec requires 2-byte alignment of the buffer. */

 put_dec() may work incorrectly for num = 0 (generate "", not "0") */

 unsigned/signed, must be 1 */

 left justified */

 show plus */

 space if plus */

 pad with zero, must be 16 == '0' - ' ' */

 use lowercase in hex (must be 32 == 0x20) */

 prefix hex with "0x", octal with "0" */

 Just a string part */

 format_type enum */

 width of output field */

 flags to number() */

 number base, 8, 10 or 16 only */

 # of digits/chars */

 put_dec requires 2-byte alignment of the buffer. */

	/* locase = 0 or 0x20. ORing digits or letters with 'locase'

 generate full string in tmp[], in reverse order */

 8 or 16 */

 base 10 */

 printing 100 using %2d gives "100", not "00" */

 leading space padding */

 sign */

 "0x" / "0" prefix */

 zero or space padding */

 hmm even more zero padding? */

 actual digits of result */

 trailing space padding */

 0x + hex */

 nowhere to put anything */

/*

 * Handle field width padding for a string.

 * @buf: current buffer position

 * @n: length of string

 * @end: end of output buffer

 * @spec: for field width and flags

 * Returns: new buffer position after padding.

 we want to pad the sucker */

 Handle string from a well known address. */

	/*

	 * Somebody passed ERR_PTR(-1234) or some other non-existing

	 * Efoo - or perhaps CONFIG_SYMBOLIC_ERRNAME=n. Fall back to

	 * printing it as its decimal representation.

 Be careful: error messages must fit into the given buffer. */

	/*

	 * Hard limit to avoid a completely insane messages. It actually

	 * works pretty well because most error messages are in

	 * the many pointer format modifiers.

/*

 * Do not call any complex external code here. Nested printk()/vsprintf()

 * might cause infinite loops. Failures might break printk() and would

 * be hard to debug.

 Make pointers available for printing early in the boot sequence. */

 Needs to run from preemptible context */

 This may be in an interrupt handler. */

 Use hw RNG if available. */

 This is in preemptible context */

 Maps a pointer to a 32 bit unique identifier. */

	/*

	 * Mask off the first 32 bits, this makes explicit that we have

	 * modified the address (and 32 bits is plenty for a unique ID).

	/*

	 * Print the real pointer value for NULL and error pointers,

	 * as they are not actual addresses.

 When debugging early boot use non-cryptographically secure hash. */

 string length must be less than default_width */

 Handle as %p, hash and do _not_ leak addresses. */

		/*

		 * kptr_restrict==1 cannot be used in IRQ context

		 * because its test for CAP_SYSLOG would be meaningless.

		/*

		 * Only print the real pointer value if the current

		 * process has CAP_SYSLOG and is running with the

		 * same credentials it started with. This is because

		 * access to files is checked at open() time, but %pK

		 * checks permission at read() time. We don't want to

		 * leak pointer values if a binary opens a file using

		 * %pK and then elevates privileges before reading it.

 Always print 0's for %pK */

	/* 32-bit res (sizeof==4): 10 chars in dec, 10 in hex ("0x" + 8)

	int i, len = 1;		/* if we pass '%ph[CDN]', field width remains

 nothing to print */

 reused to print numbers */

 current bit is 'cur', most recently seen range is [rbot, rtop] */

 hold each IP quad in reverse order */

 reverse the digits in the quad */

 find position of longest 0 run */

 don't compress a single 0 */

 emit address */

 hex u16 without leading 0s */

 nothing to print */

	/*

	 * string_escape_mem() writes as many characters as it can to

	 * the given buffer, and returns the total size of the output

	 * had the buffer been big enough.

 Print non-control ASCII characters as-is, dot otherwise */

 Page flags from the main area. */

 Page flags from the fields area */

 Skip undefined fields. */

 Format: Flag Name + '=' (equals sign) + Number + '|' (separator) */

 Loop starting from the root node to the current node. */

 simple case without anything any more format specifiers */

 full_name */

 name */

 phandle */

 path-spec */

 flags */

 major compatible string */

 full compatible string */

 name */

 full_name */

 Disable pointer hashing if requested */

/*

 * Show a '%p' thing.  A kernel extension is that the '%p' is followed

 * by an extra set of alphanumeric characters that are extended format

 * specifiers.

 *

 * Please update scripts/checkpatch.pl when adding/removing conversion

 * characters.  (Search for "check for vsprintf extension").

 *

 * Right now we handle:

 *

 * - 'S' For symbolic direct pointers (or function descriptors) with offset

 * - 's' For symbolic direct pointers (or function descriptors) without offset

 * - '[Ss]R' as above with __builtin_extract_return_addr() translation

 * - 'S[R]b' as above with module build ID (for use in backtraces)

 * - '[Ff]' %pf and %pF were obsoleted and later removed in favor of

 *	    %ps and %pS. Be careful when re-using these specifiers.

 * - 'B' For backtraced symbolic direct pointers with offset

 * - 'Bb' as above with module build ID (for use in backtraces)

 * - 'R' For decoded struct resource, e.g., [mem 0x0-0x1f 64bit pref]

 * - 'r' For raw struct resource, e.g., [mem 0x0-0x1f flags 0x201]

 * - 'b[l]' For a bitmap, the number of bits is determined by the field

 *       width which must be explicitly specified either as part of the

 *       format string '%32b[l]' or through '%*b[l]', [l] selects

 *       range-list format instead of hex format

 * - 'M' For a 6-byte MAC address, it prints the address in the

 *       usual colon-separated hex notation

 * - 'm' For a 6-byte MAC address, it prints the hex address without colons

 * - 'MF' For a 6-byte MAC FDDI address, it prints the address

 *       with a dash-separated hex notation

 * - '[mM]R' For a 6-byte MAC address, Reverse order (Bluetooth)

 * - 'I' [46] for IPv4/IPv6 addresses printed in the usual way

 *       IPv4 uses dot-separated decimal without leading 0's (1.2.3.4)

 *       IPv6 uses colon separated network-order 16 bit hex with leading 0's

 *       [S][pfs]

 *       Generic IPv4/IPv6 address (struct sockaddr *) that falls back to

 *       [4] or [6] and is able to print port [p], flowinfo [f], scope [s]

 * - 'i' [46] for 'raw' IPv4/IPv6 addresses

 *       IPv6 omits the colons (01020304...0f)

 *       IPv4 uses dot-separated decimal with leading 0's (010.123.045.006)

 *       [S][pfs]

 *       Generic IPv4/IPv6 address (struct sockaddr *) that falls back to

 *       [4] or [6] and is able to print port [p], flowinfo [f], scope [s]

 * - '[Ii][4S][hnbl]' IPv4 addresses in host, network, big or little endian order

 * - 'I[6S]c' for IPv6 addresses printed as specified by

 *       https://tools.ietf.org/html/rfc5952

 * - 'E[achnops]' For an escaped buffer, where rules are defined by combination

 *                of the following flags (see string_escape_mem() for the

 *                details):

 *                  a - ESCAPE_ANY

 *                  c - ESCAPE_SPECIAL

 *                  h - ESCAPE_HEX

 *                  n - ESCAPE_NULL

 *                  o - ESCAPE_OCTAL

 *                  p - ESCAPE_NP

 *                  s - ESCAPE_SPACE

 *                By default ESCAPE_ANY_NP is used.

 * - 'U' For a 16 byte UUID/GUID, it prints the UUID/GUID in the form

 *       "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"

 *       Options for %pU are:

 *         b big endian lower case hex (default)

 *         B big endian UPPER case hex

 *         l little endian lower case hex

 *         L little endian UPPER case hex

 *           big endian output byte order is:

 *             [0][1][2][3]-[4][5]-[6][7]-[8][9]-[10][11][12][13][14][15]

 *           little endian output byte order is:

 *             [3][2][1][0]-[5][4]-[7][6]-[8][9]-[10][11][12][13][14][15]

 * - 'V' For a struct va_format which contains a format string * and va_list *,

 *       call vsnprintf(->format, *->va_list).

 *       Implements a "recursive vsnprintf".

 *       Do not use this feature without some mechanism to verify the

 *       correctness of the format string and va_list arguments.

 * - 'K' For a kernel pointer that should be hidden from unprivileged users.

 *       Use only for procfs, sysfs and similar files, not printk(); please

 *       read the documentation (path below) first.

 * - 'NF' For a netdev_features_t

 * - '4cc' V4L2 or DRM FourCC code, with endianness and raw numerical value.

 * - 'h[CDN]' For a variable-length buffer, it prints it as a hex string with

 *            a certain separator (' ' by default):

 *              C colon

 *              D dash

 *              N no separator

 *            The maximum supported length is 64 bytes of the input. Consider

 *            to use print_hex_dump() for the larger input.

 * - 'a[pd]' For address types [p] phys_addr_t, [d] dma_addr_t and derivatives

 *           (default assumed to be phys_addr_t, passed by reference)

 * - 'd[234]' For a dentry name (optionally 2-4 last components)

 * - 'D[234]' Same as 'd' but for a struct file

 * - 'g' For block_device name (gendisk + partition number)

 * - 't[RT][dt][r][s]' For time and date as represented by:

 *      R    struct rtc_time

 *      T    time64_t

 * - 'C' For a clock, it prints the name (Common Clock Framework) or address

 *       (legacy clock framework) of the clock

 * - 'Cn' For a clock, it prints the name (Common Clock Framework) or address

 *        (legacy clock framework) of the clock

 * - 'G' For flags to be printed as a collection of symbolic strings that would

 *       construct the specific value. Supported flags given by option:

 *       p page flags (see struct page) given as pointer to unsigned long

 *       g gfp flags (GFP_* and __GFP_*) given as pointer to gfp_t

 *       v vma flags (VM_*) given as pointer to unsigned long

 * - 'OF[fnpPcCF]'  For a device tree object

 *                  Without any optional arguments prints the full_name

 *                  f device node full_name

 *                  n device node name

 *                  p device node phandle

 *                  P device node path spec (name + @unit)

 *                  F device node flags

 *                  c major compatible string

 *                  C full compatible string

 * - 'fw[fP]'	For a firmware node (struct fwnode_handle) pointer

 *		Without an option prints the full name of the node

 *		f full name

 *		P node name, including a possible unit address

 * - 'x' For printing the address unmodified. Equivalent to "%lx".

 *       Please read the documentation (path below) before using!

 * - '[ku]s' For a BPF/tracing related format specifier, e.g. used out of

 *           bpf_trace_printk() where [ku] prefix specifies either kernel (k)

 *           or user (u) memory to probe, and:

 *              s a string, equivalent to "%s" on direct vsnprintf() use

 *

 * ** When making changes please also update:

 *	Documentation/core-api/printk-formats.rst

 *

 * Note: The default behaviour (unadorned %p) is to hash the address,

 * rendering it useful as a unique identifier.

 Colon separated: 00:01:02:03:04:05 */

 Contiguous: 000102030405 */

 [mM]F (FDDI) */

 [mM]R (Reverse order; Bluetooth) */

	case 'I':			/* Formatted IP supported

					 * 4:	1.2.3.4

					 * 6:	0001:0203:...:0708

					 * 6c:	1::708 or 1::1.2.3.4

	case 'i':			/* Contiguous:

					 * 4:	001.002.003.004

					 * 6:   000102...0f

 %pe with a non-ERR_PTR gets treated as plain %p */

	/*

	 * default is to _not_ leak addresses, so hash before printing,

	 * unless no_hash_pointers is specified on the command line.

/*

 * Helper function to decode printf style format.

 * Each call decode a token from the format and return the

 * number of characters read (or likely the delta where it wants

 * to go on the next call).

 * The decoded token is returned through the parameters

 *

 * 'h', 'l', or 'L' for integer fields

 * 'z' support added 23/7/1999 S.H.

 * 'z' changed to 'Z' --davidm 1/25/99

 * 'Z' changed to 'z' --adobriyan 2017-01-25

 * 't' added for ptrdiff_t

 *

 * @fmt: the format string

 * @type of the token returned

 * @flags: various flags such as +, -, # tokens..

 * @field_width: overwritten width

 * @base: base of the number (octal, hex, ...)

 * @precision: precision of a number

 * @qualifier: qualifier of a number (long, size_t, ...)

 we finished early by reading the field width */

 we finished early by reading the precision */

 By default */

 Return the current non-format string */

 Process flags */

 this also skips first '%' */

 get field width */

 it's the next argument */

 get the precision */

 it's the next argument */

 get the conversion qualifier */

 default base */

 integer number formats - set up the flags and "break" */

		/*

		 * Since %n poses a greater security risk than

		 * utility, treat it as any other invalid or

		 * unsupported format specifier.

/**

 * vsnprintf - Format a string and place it in a buffer

 * @buf: The buffer to place the result into

 * @size: The size of the buffer, including the trailing null space

 * @fmt: The format string to use

 * @args: Arguments for the format string

 *

 * This function generally follows C99 vsnprintf, but has some

 * extensions and a few limitations:

 *

 *  - ``%n`` is unsupported

 *  - ``%p*`` is handled by pointer()

 *

 * See pointer() or Documentation/core-api/printk-formats.rst for more

 * extensive description.

 *

 * **Please update the documentation in both places when making changes**

 *

 * The return value is the number of characters which would

 * be generated for the given input, excluding the trailing

 * '\0', as per ISO C99. If you want to have the exact

 * number of characters written into @buf as return value

 * (not including the trailing '\0'), use vscnprintf(). If the

 * return is greater than or equal to @size, the resulting

 * string is truncated.

 *

 * If you're not already dealing with a va_list consider using snprintf().

	/* Reject out-of-range values early.  Large positive sizes are

 Make sure end is always >= buf */

			/*

			 * Presumably the arguments passed gcc's type

			 * checking, but there is no safe or sane way

			 * for us to continue parsing the format and

			 * fetching from the va_list; the remaining

			 * specifiers and arguments would be out of

			 * sync.

 the trailing null byte doesn't count towards the total */

/**

 * vscnprintf - Format a string and place it in a buffer

 * @buf: The buffer to place the result into

 * @size: The size of the buffer, including the trailing null space

 * @fmt: The format string to use

 * @args: Arguments for the format string

 *

 * The return value is the number of characters which have been written into

 * the @buf not including the trailing '\0'. If @size is == 0 the function

 * returns 0.

 *

 * If you're not already dealing with a va_list consider using scnprintf().

 *

 * See the vsnprintf() documentation for format string extensions over C99.

/**

 * snprintf - Format a string and place it in a buffer

 * @buf: The buffer to place the result into

 * @size: The size of the buffer, including the trailing null space

 * @fmt: The format string to use

 * @...: Arguments for the format string

 *

 * The return value is the number of characters which would be

 * generated for the given input, excluding the trailing null,

 * as per ISO C99.  If the return is greater than or equal to

 * @size, the resulting string is truncated.

 *

 * See the vsnprintf() documentation for format string extensions over C99.

/**

 * scnprintf - Format a string and place it in a buffer

 * @buf: The buffer to place the result into

 * @size: The size of the buffer, including the trailing null space

 * @fmt: The format string to use

 * @...: Arguments for the format string

 *

 * The return value is the number of characters written into @buf not including

 * the trailing '\0'. If @size is == 0 the function returns 0.

/**

 * vsprintf - Format a string and place it in a buffer

 * @buf: The buffer to place the result into

 * @fmt: The format string to use

 * @args: Arguments for the format string

 *

 * The function returns the number of characters written

 * into @buf. Use vsnprintf() or vscnprintf() in order to avoid

 * buffer overflows.

 *

 * If you're not already dealing with a va_list consider using sprintf().

 *

 * See the vsnprintf() documentation for format string extensions over C99.

/**

 * sprintf - Format a string and place it in a buffer

 * @buf: The buffer to place the result into

 * @fmt: The format string to use

 * @...: Arguments for the format string

 *

 * The function returns the number of characters written

 * into @buf. Use snprintf() or scnprintf() in order to avoid

 * buffer overflows.

 *

 * See the vsnprintf() documentation for format string extensions over C99.

/*

 * bprintf service:

 * vbin_printf() - VA arguments to binary data

 * bstr_printf() - Binary data to text string

/**

 * vbin_printf - Parse a format string and place args' binary value in a buffer

 * @bin_buf: The buffer to place args' binary value

 * @size: The size of the buffer(by words(32bits), not characters)

 * @fmt: The format string to use

 * @args: Arguments for the format string

 *

 * The format follows C99 vsnprintf, except %n is ignored, and its argument

 * is skipped.

 *

 * The return value is the number of words(32bits) which would be generated for

 * the given input.

 *

 * NOTE:

 * If the return value is greater than @size, the resulting bin_buf is NOT

 * valid for bstr_printf().

 Pointers may require the width */

 Dereferenced pointers must be done now */

 Dereference of functions is still OK */

 Must be nul terminated */

 skip all alphanumeric pointer suffixes */

/**

 * bstr_printf - Format a string from binary arguments and place it in a buffer

 * @buf: The buffer to place the result into

 * @size: The size of the buffer, including the trailing null space

 * @fmt: The format string to use

 * @bin_buf: Binary arguments for the format string

 *

 * This function like C99 vsnprintf, but the difference is that vsnprintf gets

 * arguments from stack, and bstr_printf gets arguments from @bin_buf which is

 * a binary buffer that generated by vbin_printf.

 *

 * The format follows C99 vsnprintf, but has some extensions:

 *  see vsnprintf comment for details.

 *

 * The return value is the number of characters which would

 * be generated for the given input, excluding the trailing

 * '\0', as per ISO C99. If you want to have the exact

 * number of characters written into @buf as return value

 * (not including the trailing '\0'), use vscnprintf(). If the

 * return is greater than or equal to @size, the resulting

 * string is truncated.

 Make sure end is always >= buf */

 Non function dereferences were already done */

 Pointer dereference was already processed */

 default: */

 switch(spec.type) */

 while(*fmt) */

 the trailing null byte doesn't count towards the total */

/**

 * bprintf - Parse a format string and place args' binary value in a buffer

 * @bin_buf: The buffer to place args' binary value

 * @size: The size of the buffer(by words(32bits), not characters)

 * @fmt: The format string to use

 * @...: Arguments for the format string

 *

 * The function returns the number of words(u32) written

 * into @bin_buf.

 CONFIG_BINARY_PRINTF */

/**

 * vsscanf - Unformat a buffer into a list of arguments

 * @buf:	input buffer

 * @fmt:	format of buffer

 * @args:	arguments

 skip any white space in format */

		/* white space in format matches any amount of

		 * white space, including none, in the input.

 anything that is not a conversion must match exactly */

		/* skip this conversion.

		 * advance both strings to next white space

 '%*[' not yet supported, invalid format */

 get field width */

 get conversion qualifier */

 return number of characters read so far */

 first, skip leading white space in buffer */

 now copy until next white space */

		/*

		 * Warning: This implementation of the '[' conversion specifier

		 * deviates from its glibc counterpart in the following ways:

		 * (1) It does NOT support ranges i.e. '-' is NOT a special

		 *     character

		 * (2) It cannot match the closing bracket ']' itself

		 * (3) A field width is required

		 * (4) '%*[' (discard matching input) is currently not supported

		 *

		 * Example usage:

		 * ret = sscanf("00:0a:95","%2[^:]:%2[^:]:%2[^:]",

		 *		buf1, buf2, buf3);

		 * if (ret < 3)

		 *    // etc..

 field width is required */

 no ']' or no character set found */

 exclude null '\0' byte */

 match must be non-empty */

 looking for '%' in str */

 invalid format; stop here */

		/* have some sort of integer conversion.

		 * first, skip white space in buffer.

 that's 'hh' in format */

/**

 * sscanf - Unformat a buffer into a list of arguments

 * @buf:	input buffer

 * @fmt:	formatting of buffer

 * @...:	resulting arguments

 SPDX-License-Identifier: GPL-2.0

/*

 * lib/smp_processor_id.c

 *

 * DEBUG_PREEMPT variant of smp_processor_id().

	/*

	 * It is valid to assume CPU-locality during early bootup:

	/*

	 * Avoid recursion:

 SPDX-License-Identifier: GPL-2.0

/*

 * helpers to map values in a linear range to range index

 *

 * Original idea borrowed from regulator framework

 *

 * It might be useful if we could support also inversely proportional ranges?

 * Copyright 2020 ROHM Semiconductors

/**

 * linear_range_values_in_range - return the amount of values in a range

 * @r:		pointer to linear range where values are counted

 *

 * Compute the amount of values in range pointed by @r. Note, values can

 * be all equal - range with selectors 0,...,2 with step 0 still contains

 * 3 values even though they are all equal.

 *

 * Return: the amount of values in range pointed by @r

/**

 * linear_range_values_in_range_array - return the amount of values in ranges

 * @r:		pointer to array of linear ranges where values are counted

 * @ranges:	amount of ranges we include in computation.

 *

 * Compute the amount of values in ranges pointed by @r. Note, values can

 * be all equal - range with selectors 0,...,2 with step 0 still contains

 * 3 values even though they are all equal.

 *

 * Return: the amount of values in first @ranges ranges pointed by @r

/**

 * linear_range_get_max_value - return the largest value in a range

 * @r:		pointer to linear range where value is looked from

 *

 * Return: the largest value in the given range

/**

 * linear_range_get_value - fetch a value from given range

 * @r:		pointer to linear range where value is looked from

 * @selector:	selector for which the value is searched

 * @val:	address where found value is updated

 *

 * Search given ranges for value which matches given selector.

 *

 * Return: 0 on success, -EINVAL given selector is not found from any of the

 * ranges.

/**

 * linear_range_get_value_array - fetch a value from array of ranges

 * @r:		pointer to array of linear ranges where value is looked from

 * @ranges:	amount of ranges in an array

 * @selector:	selector for which the value is searched

 * @val:	address where found value is updated

 *

 * Search through an array of ranges for value which matches given selector.

 *

 * Return: 0 on success, -EINVAL given selector is not found from any of the

 * ranges.

/**

 * linear_range_get_selector_low - return linear range selector for value

 * @r:		pointer to linear range where selector is looked from

 * @val:	value for which the selector is searched

 * @selector:	address where found selector value is updated

 * @found:	flag to indicate that given value was in the range

 *

 * Return selector for which range value is closest match for given

 * input value. Value is matching if it is equal or smaller than given

 * value. If given value is in the range, then @found is set true.

 *

 * Return: 0 on success, -EINVAL if range is invalid or does not contain

 * value smaller or equal to given value

/**

 * linear_range_get_selector_low_array - return linear range selector for value

 * @r:		pointer to array of linear ranges where selector is looked from

 * @ranges:	amount of ranges to scan from array

 * @val:	value for which the selector is searched

 * @selector:	address where found selector value is updated

 * @found:	flag to indicate that given value was in the range

 *

 * Scan array of ranges for selector for which range value matches given

 * input value. Value is matching if it is equal or smaller than given

 * value. If given value is found to be in a range scanning is stopped and

 * @found is set true. If a range with values smaller than given value is found

 * but the range max is being smaller than given value, then the range's

 * biggest selector is updated to @selector but scanning ranges is continued

 * and @found is set to false.

 *

 * Return: 0 on success, -EINVAL if range array is invalid or does not contain

 * range with a value smaller or equal to given value

/**

 * linear_range_get_selector_high - return linear range selector for value

 * @r:		pointer to linear range where selector is looked from

 * @val:	value for which the selector is searched

 * @selector:	address where found selector value is updated

 * @found:	flag to indicate that given value was in the range

 *

 * Return selector for which range value is closest match for given

 * input value. Value is matching if it is equal or higher than given

 * value. If given value is in the range, then @found is set true.

 *

 * Return: 0 on success, -EINVAL if range is invalid or does not contain

 * value greater or equal to given value

/**

 * linear_range_get_selector_within - return linear range selector for value

 * @r:		pointer to linear range where selector is looked from

 * @val:	value for which the selector is searched

 * @selector:	address where found selector value is updated

 *

 * Return selector for which range value is closest match for given

 * input value. Value is matching if it is equal or lower than given

 * value. But return maximum selector if given value is higher than

 * maximum value.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Derived from arch/ppc/mm/extable.c and arch/i386/mm/extable.c.

 *

 * Copyright (C) 2004 Paul Mackerras, IBM Corp.

 ARCH_HAS_RELATIVE_EXTABLE */

/*

 * The exception table needs to be sorted so that the binary

 * search that we use to find entries in it works properly.

 * This is used both for the kernel exception table and for

 * the exception tables of modules that get loaded.

 avoid overflow */

/*

 * If the exception table is sorted, any referring to the module init

 * will be at the beginning or the end.

trim the beginning*/

trim the end*/

 CONFIG_MODULES */

 avoid overflow */

/*

 * Search one exception table for an entry corresponding to the

 * given instruction address, and return the address of the entry,

 * or NULL if none is found.

 * We use a binary search, and thus we assume that the table is

 * already sorted.

 SPDX-License-Identifier: GPL-2.0-only

/**

 * sg_free_table_chained - Free a previously mapped sg table

 * @table:	The sg table header to use

 * @nents_first_chunk: size of the first_chunk SGL passed to

 *		sg_alloc_table_chained

 *

 *  Description:

 *    Free an sg table previously allocated and setup with

 *    sg_alloc_table_chained().

 *

 *    @nents_first_chunk has to be same with that same parameter passed

 *    to sg_alloc_table_chained().

 *

/**

 * sg_alloc_table_chained - Allocate and chain SGLs in an sg table

 * @table:	The sg table header to use

 * @nents:	Number of entries in sg list

 * @first_chunk: first SGL

 * @nents_first_chunk: number of the SGL of @first_chunk

 *

 *  Description:

 *    Allocate and chain SGLs in an sg table. If @nents@ is larger than

 *    @nents_first_chunk a chained sg table will be setup. @first_chunk is

 *    ignored if nents_first_chunk <= 1 because user expects the SGL points

 *    non-chain SGL.

 *

 User supposes that the 1st SGL includes real entry */

 SPDX-License-Identifier: GPL-2.0

 Protect surrounding memory. */

 Protect surrounding memory. */

 "((aligned(8)))" helps this not into be misaligned for ptr-access. */

 Excluded because they Oops the module. */

 do nothing */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * lib/hexdump.c

/**

 * hex_to_bin - convert a hex digit to its real value

 * @ch: ascii character represents hex digit

 *

 * hex_to_bin() converts one hex digit to its actual value or -1 in case of bad

 * input.

/**

 * hex2bin - convert an ascii hexadecimal string to its binary representation

 * @dst: binary result

 * @src: ascii hexadecimal string

 * @count: result length

 *

 * Return 0 on success, -EINVAL in case of bad input.

/**

 * bin2hex - convert binary data to an ascii hexadecimal string

 * @dst: ascii hexadecimal result

 * @src: binary data

 * @count: binary data length

/**

 * hex_dump_to_buffer - convert a blob of data to "hex ASCII" in memory

 * @buf: data blob to dump

 * @len: number of bytes in the @buf

 * @rowsize: number of bytes to print per line; must be 16 or 32

 * @groupsize: number of bytes to print at a time (1, 2, 4, 8; default = 1)

 * @linebuf: where to put the converted data

 * @linebuflen: total size of @linebuf, including space for terminating NUL

 * @ascii: include ASCII after the hex output

 *

 * hex_dump_to_buffer() works on one "line" of output at a time, i.e.,

 * 16 or 32 bytes of input data converted to hex + ASCII output.

 *

 * Given a buffer of u8 data, hex_dump_to_buffer() converts the input data

 * to a hex + ASCII dump at the supplied memory location.

 * The converted output is always NUL-terminated.

 *

 * E.g.:

 *   hex_dump_to_buffer(frame->data, frame->len, 16, 1,

 *			linebuf, sizeof(linebuf), true);

 *

 * example output buffer:

 * 40 41 42 43 44 45 46 47 48 49 4a 4b 4c 4d 4e 4f  @ABCDEFGHIJKLMNO

 *

 * Return:

 * The amount of bytes placed in the buffer without terminating NUL. If the

 * output was truncated, then the return value is the number of bytes

 * (excluding the terminating NUL) which would have been written to the final

 * string if enough space had been available.

 limit to one line at a time */

 no mixed size output */

/**

 * print_hex_dump - print a text hex dump to syslog for a binary blob of data

 * @level: kernel log level (e.g. KERN_DEBUG)

 * @prefix_str: string to prefix each line with;

 *  caller supplies trailing spaces for alignment if desired

 * @prefix_type: controls whether prefix of an offset, address, or none

 *  is printed (%DUMP_PREFIX_OFFSET, %DUMP_PREFIX_ADDRESS, %DUMP_PREFIX_NONE)

 * @rowsize: number of bytes to print per line; must be 16 or 32

 * @groupsize: number of bytes to print at a time (1, 2, 4, 8; default = 1)

 * @buf: data blob to dump

 * @len: number of bytes in the @buf

 * @ascii: include ASCII after the hex output

 *

 * Given a buffer of u8 data, print_hex_dump() prints a hex + ASCII dump

 * to the kernel log at the specified kernel log level, with an optional

 * leading prefix.

 *

 * print_hex_dump() works on one "line" of output at a time, i.e.,

 * 16 or 32 bytes of input data converted to hex + ASCII output.

 * print_hex_dump() iterates over the entire input @buf, breaking it into

 * "line size" chunks to format and print.

 *

 * E.g.:

 *   print_hex_dump(KERN_DEBUG, "raw data: ", DUMP_PREFIX_ADDRESS,

 *		    16, 1, frame->data, frame->len, true);

 *

 * Example output using %DUMP_PREFIX_OFFSET and 1-byte mode:

 * 0009ab42: 40 41 42 43 44 45 46 47 48 49 4a 4b 4c 4d 4e 4f  @ABCDEFGHIJKLMNO

 * Example output using %DUMP_PREFIX_ADDRESS and 4-byte mode:

 * ffffffff88089af0: 73727170 77767574 7b7a7978 7f7e7d7c  pqrstuvwxyz{|}~.

 defined(CONFIG_PRINTK) */

/*	Small bzip2 deflate implementation, by Rob Landley (rob@landley.net).



	Based on bzip2 decompression code by Julian R Seward (jseward@acm.org),

	which also acknowledges contributions by Mike Burrows, David Wheeler,

	Peter Fenwick, Alistair Moffat, Radford Neal, Ian H. Witten,

	Robert Sedgewick, and Jon L. Bentley.



	This code is licensed under the LGPLv2:

		LGPL (http://www.gnu.org/copyleft/lgpl.html

/*

	Size and speed optimizations by Manuel Novoa III  (mjn3@codepoet.org).



	More efficient reading of Huffman codes, a streamlined read_bunzip()

	function, and various other tweaks.  In (limited) tests, approximately

	20% faster than bzcat on x86 and about 10% faster on arm.



	Note that about 2/3 of the time is spent in read_unzip() reversing

	the Burrows-Wheeler transformation.  Much of that time is delay

	resulting from cache misses.



	I would ask that anyone benefiting from this work, especially those

	using it in commercial products, consider making a donation to my local

	non-profit hospice organization in the name of the woman I loved, who

	passed away Feb. 12, 2003.



		In memory of Toni W. Hagan



		Hospice of Acadiana, Inc.

		2600 Johnston St., Suite 200

		Lafayette, LA 70503-3240



		Phone (337) 232-1234 or 1-800-738-2226

		Fax   (337) 232-1297



		https://www.hospiceacadiana.com/



	Manuel

/*

	Made it fit for running in Linux Kernel by Alain Knaff (alain@knaff.lu)

 STATIC */

 Constants for Huffman coding */

 64 would have been more efficient */

 Longest Huffman code allowed */

 256 literals + RUNA + RUNB */

 Status return values */

 Other housekeeping constants */

 This is what we know about each Huffman coding group */

 We have an extra slot at the end of limit[] for a sentinel value. */

/* Structure holding all the housekeeping data, including IO buffers and

 State for interrupting output loop */

 I/O tracking data (file handles, buffers, positions, etc.) */

, outbufPos*/;

,*outbuf*/;

	/* The CRC values stored in the block header and calculated from the

 Intermediate buffer and its size (in bytes) */

 These things are a bit too big to go on the stack */

 nSelectors = 15 bits */

 Huffman coding tables */

 non-zero if we have IO error */

/* Return the next nnn bits of input.  All reads from the compressed input

	/* If we need to get more data from the byte buffer, do so.

	   (Loop getting one byte at a time to enforce endianness and avoid

		/* If we need to read more data from file into byte buffer, do

 Avoid 32-bit overflow (dump bit buffer to top of output) */

 Grab next 8 bits of input from buffer. */

 Calculate result */

 Unpacks the next block and sets up for the inverse burrows-wheeler step. */

	/* Read in header signature and CRC, then validate signature.

	/* We can add support for blockRandomised if anybody complains.

	   There was some code for this in busybox 1.0.0-pre3, but nobody ever

	/* mapping table: if some byte values are never used (encoding things

	   like ascii text), the compression code removes the gaps to have fewer

	   symbols to deal with, and writes a sparse bitfield indicating which

	   values were present.  We make a translation table to convert the

 How many different Huffman coding groups does this block use? */

	/* nSelectors: Every GROUP_SIZE many symbols we select a new

	   Huffman coding group.  Read in the group selector list,

	   which is stored as MTF encoded bit runs.  (MTF = Move To

	   Front, as each value is used it's moved to the start of the

 Get next value */

 Decode MTF to get the next selector */

	/* Read the Huffman coding tables for each group, which code

	   for symTotal literal symbols, plus two run symbols (RUNA,

		/* Read Huffman code lengths for each symbol.  They're

		   stored in a way similar to mtf; record a starting

		   value for the first symbol, and an offset from the

		   previous value for everys symbol after that.

		   (Subtracting 1 before the loop and then adding it

		   back at the end is an optimization that makes the

		   test inside the loop simpler: symbol length 0

		   becomes negative, so an unsigned inequality catches

				/* If first bit is 0, stop.  Else

				   second bit indicates whether to

				   increment or decrement the value.

				   Optimization: grab 2 bits and unget

				/* Add one if second bit 1, else

			/* Correct for the initial -1, to get the

 Find largest and smallest lengths in this group */

		/* Calculate permute[], base[], and limit[] tables from

		 * length[].

		 *

		 * permute[] is the lookup table for converting

		 * Huffman coded symbols into decoded symbols.  base[]

		 * is the amount to subtract from the value of a

		 * Huffman symbol of a given length when using

		 * permute[].

		 *

		 * limit[] indicates the largest numerical value a

		 * symbol with a given number of bits can have.  This

		 * is how the Huffman codes can vary in length: each

		 * code with a value > limit[length] needs another

		 * bit.

		/* Note that minLen can't be smaller than 1, so we

		   adjust the base and limit array pointers so we're

		   not always wasting the first entry.  We do this

		/* Calculate permute[].  Concurrently, initialize

 Count symbols coded for at each bit length */

		/* Calculate limit[] (the largest symbol-coding value

		 *at each bit length, which is (previous limit <<

		 *1)+symbols at this level), and base[] (number of

		 *symbols to ignore at each bit length, which is limit

		 *minus the cumulative count of symbols coded for

			/* We read the largest possible symbol size

			   and then unget bits after determining how

			   many we need, and those extra bits could be

			   set to anything.  (They're noise from

			   future symbols.)  At each level we're

			   really only interested in the first few

			   bits, so here we set all the trailing

			   to-be-ignored bits to 1 so they don't

			   affect the value > limit[length]

		limit[maxLen+1] = INT_MAX; /* Sentinel value for

	/* We've finished reading and digesting the block header.  Now

	   read this block's Huffman coded symbols from the file and

	   undo the Huffman coding and run length encoding, saving the

	/* Initialize symbol occurrence counters and symbol Move To

 Loop through compressed symbols. */

 Determine which Huffman coding group to use. */

 Read next Huffman-coded symbol. */

		/* Note: It is far cheaper to read maxLen bits and

		   back up than it is to read minLen bits and then an

		   additional bit at a time, testing as we go.

		   Because there is a trailing last block (with file

		   CRC), there is no danger of the overread causing an

		   unexpected EOF for a valid compressed file.  As a

		   further optimization, we do the read inline

		   (falling back to a call to get_bits if the buffer

		   runs dry).  The following (up to got_huff_bits:) is

		   equivalent to j = get_bits(bd, hufGroup->maxLen);

		/* Figure how many bits are in next symbol and

 Huffman decode value to get nextSym (with bounds checking) */

		/* We have now decoded the symbol, which indicates

		   either a new literal byte, or a repeated run of the

		   most recent literal byte.  First, check if nextSym

		   indicates a repeated run, and if so loop collecting

 RUNA or RUNB */

			/* If this is the start of a new run, zero out

			/* Neat trick that saves 1 symbol: instead of

			   or-ing 0 or 1 at each bit position, add 1

			   or 2 instead.  For example, 1011 is 1 << 0

			   + 1 << 1 + 2 << 2.  1010 is 2 << 0 + 2 << 1

			   + 1 << 2.  You can make any bit pattern

			   that way using 1 less symbol than the basic

			   or 0/1 method (except all bits 0, which

			   would use no symbols, but a run of length 0

			   doesn't mean anything in this context).

 +runPos if RUNA; +2*runPos if RUNB */

		/* When we hit the first non-run symbol after a run,

		   we now know how many times to repeat the last

		   literal, so append that many copies to our buffer

		   of decoded symbols (dbuf) now.  (The last literal

		   used is the one at the head of the mtfSymbol

 Is this the terminating symbol? */

		/* At this point, nextSym indicates a new literal

		   character.  Subtract one to get the position in the

		   MTF array at which this literal is currently to be

		   found.  (Note that the result can't be -1 or 0,

		   because 0 and 1 are RUNA and RUNB.  But another

		   instance of the first symbol in the mtf array,

		   position 0, would have been handled as part of a

		   run above.  Therefore 1 unused mtf position minus 2

		/* Adjust the MTF array.  Since we typically expect to

		 *move only a small number of symbols, and are bound

		 *by 256 in any case, using memmove here would

		 *typically be bigger and slower due to function call

 We have our literal byte.  Save it into dbuf. */

	/* At this point, we've read all the Huffman-coded symbols

	   (and repeated runs) for this block from the input stream,

	   and decoded them into the intermediate buffer.  There are

	   dbufCount many decoded bytes in dbuf[].  Now undo the

	   Burrows-Wheeler transform on dbuf.  See

	   http://dogma.net/markn/articles/bwt/bwt.htm

 Turn byteCount into cumulative occurrence counts of 0 to n-1. */

 Figure out what order dbuf would be in if we sorted it. */

	/* Decode first byte by hand to initialize "previous" byte.

	   Note that it doesn't get output, and if the first three

	   characters are identical it doesn't qualify as a run (hence

/* Undo burrows-wheeler transform on intermediate buffer to produce output.

   If start_bunzip was initialized with out_fd =-1, then up to len bytes of

   data are written to outbuf.  Return value is number of bytes written or

   error (all errors are negative numbers).  If out_fd!=-1, outbuf and len

   are ignored, data is written to out_fd and return is RETVAL_OK or error.

 If last read was short due to end of file, return last block now */

	/* We will always have pending decoded data to write into the output

	   buffer unless this is the very first call (in which case we haven't

 Inside the loop, writeCopies means extra copies (beyond 1) */

 Loop outputting bytes */

			/* If the output buffer is full, snapshot

 Write next byte into output buffer, updating CRC */

			/* Loop now if we're outputting multiple

			/* Follow sequence vector to undo

			/* After 3 consecutive copies of the same

			   byte, the 4th is a repeat count.  We count

			   down from 4 instead *of counting up because

				/* We have a repeated run, this byte

				/* Sometimes there are just 3 bytes

				/* Subtract the 1 copy we'd output

 Decompression of this block completed successfully */

 If this block had a CRC error, force file level CRC error. */

	/* Refill the intermediate buffer by Huffman-decoding next

 (previous is just a convenient unused temp variable here) */

/* Allocate the structure, read file header.  If in_fd ==-1, inbuf must contain

   a complete bunzip file (len bytes long).  If in_fd!=-1, inbuf and len are

 Figure out how much data to allocate */

 Allocate bunzip_data.  Most fields initialize to zero. */

 Setup input buffer */

 Init the CRC32 table (big endian) */

 Ensure that file starts with "BZh['1'-'9']." */

	/* Fourth byte (ascii '1'-'9'), indicates block size in units of 100k of

/* Example usage: decompress src_fd to dst_fd.  (Stops at end of bzip2 data,

 Check CRC and release memory */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * A generic version of devmem_is_allowed.

 *

 * Based on arch/arm64/mm/mmap.c

 *

 * Copyright (C) 2020 Google, Inc.

 * Copyright (C) 2012 ARM Ltd.

/*

 * devmem_is_allowed() checks to see if /dev/mem access to a certain address

 * is valid. The argument is a physical page number.  We mimic x86 here by

 * disallowing access to system RAM as well as device-exclusive MMIO regions.

 * This effectively disable read()/write() on /dev/mem.

/*

 * lib/test_parman.c - Test module for parman

 * Copyright (c) 2017 Mellanox Technologies. All rights reserved.

 * Copyright (c) 2017 Jiri Pirko <jiri@mellanox.com>

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions are met:

 *

 * 1. Redistributions of source code must retain the above copyright

 *    notice, this list of conditions and the following disclaimer.

 * 2. Redistributions in binary form must reproduce the above copyright

 *    notice, this list of conditions and the following disclaimer in the

 *    documentation and/or other materials provided with the distribution.

 * 3. Neither the names of the copyright holders nor the names of its

 *    contributors may be used to endorse or promote products derived from

 *    this software without specific prior written permission.

 *

 * Alternatively, this software may be distributed under the terms of the

 * GNU General Public License ("GPL") version 2 as published by the Free

 * Software Foundation.

 *

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"

 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE

 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE

 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE

 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR

 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF

 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS

 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN

 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)

 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE

 * POSSIBILITY OF SUCH DAMAGE.

 defines number of prios for testing */

#define TEST_PARMAN_ITEM_SHIFT 13 /* defines a total number

				   * of items for testing

 Assign random uniqueue priority to each prio structure */

 Assign random prio to each item structure */

 SPDX-License-Identifier: GPL-2.0

/*

 * Out-of-line refcount functions.

/**

 * refcount_dec_if_one - decrement a refcount if it is 1

 * @r: the refcount

 *

 * No atomic_t counterpart, it attempts a 1 -> 0 transition and returns the

 * success thereof.

 *

 * Like all decrement operations, it provides release memory order and provides

 * a control dependency.

 *

 * It can be used like a try-delete operator; this explicit case is provided

 * and not cmpxchg in generic, because that would allow implementing unsafe

 * operations.

 *

 * Return: true if the resulting refcount is 0, false otherwise

/**

 * refcount_dec_not_one - decrement a refcount if it is not 1

 * @r: the refcount

 *

 * No atomic_t counterpart, it decrements unless the value is 1, in which case

 * it will return false.

 *

 * Was often done like: atomic_add_unless(&var, -1, 1)

 *

 * Return: true if the decrement operation was successful, false otherwise

/**

 * refcount_dec_and_mutex_lock - return holding mutex if able to decrement

 *                               refcount to 0

 * @r: the refcount

 * @lock: the mutex to be locked

 *

 * Similar to atomic_dec_and_mutex_lock(), it will WARN on underflow and fail

 * to decrement when saturated at REFCOUNT_SATURATED.

 *

 * Provides release memory ordering, such that prior loads and stores are done

 * before, and provides a control dependency such that free() must come after.

 * See the comment on top.

 *

 * Return: true and hold mutex if able to decrement refcount to 0, false

 *         otherwise

/**

 * refcount_dec_and_lock - return holding spinlock if able to decrement

 *                         refcount to 0

 * @r: the refcount

 * @lock: the spinlock to be locked

 *

 * Similar to atomic_dec_and_lock(), it will WARN on underflow and fail to

 * decrement when saturated at REFCOUNT_SATURATED.

 *

 * Provides release memory ordering, such that prior loads and stores are done

 * before, and provides a control dependency such that free() must come after.

 * See the comment on top.

 *

 * Return: true and hold spinlock if able to decrement refcount to 0, false

 *         otherwise

/**

 * refcount_dec_and_lock_irqsave - return holding spinlock with disabled

 *                                 interrupts if able to decrement refcount to 0

 * @r: the refcount

 * @lock: the spinlock to be locked

 * @flags: saved IRQ-flags if the is acquired

 *

 * Same as refcount_dec_and_lock() above except that the spinlock is acquired

 * with disabled interrupts.

 *

 * Return: true and hold spinlock if able to decrement refcount to 0, false

 *         otherwise

/* Lzma decompressor for Linux kernel. Shamelessly snarfed

 *from busybox 1.1.1

 *

 *Linux kernel adaptation

 *Copyright (C) 2006  Alain < alain@knaff.lu >

 *

 *Based on small lzma deflate implementation/Small range coder

 *implementation for lzma.

 *Copyright (C) 2006  Aurelien Jacobs < aurel@gnuage.org >

 *

 *Based on LzmaDecode.c from the LZMA SDK 4.22 (https://www.7-zip.org/)

 *Copyright (C) 1999-2005  Igor Pavlov

 *

 *Copyrights of the parts, see headers below.

 *

 *

 *This program is free software; you can redistribute it and/or

 *modify it under the terms of the GNU Lesser General Public

 *License as published by the Free Software Foundation; either

 *version 2.1 of the License, or (at your option) any later version.

 *

 *This program is distributed in the hope that it will be useful,

 *but WITHOUT ANY WARRANTY; without even the implied warranty of

 *MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU

 *Lesser General Public License for more details.

 *

 *You should have received a copy of the GNU Lesser General Public

 *License along with this library; if not, write to the Free Software

 *Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA

 STATIC */

/* Small range coder implementation for lzma.

 *Copyright (C) 2006  Aurelien Jacobs < aurel@gnuage.org >

 *

 *Based on LzmaDecode.c from the LZMA SDK 4.22 (https://www.7-zip.org/)

 *Copyright (c) 1999-2005  Igor Pavlov

 Called twice: once at startup and once in rc_normalize() */

 Called once */

 Called twice, but one callsite is in inline'd rc_is_bit_0_helper() */

 Called 9 times */

/* Why rc_is_bit_0_helper exists?

 *Because we want to always expose (rc->code < rc->bound) to optimizer

 Called ~10 times, but very small, thus inlined */

 Called 4 times in unlzma loop */

 Called once */

 Called twice */

/*

 * Small lzma deflate implementation.

 * Copyright (C) 2006  Aurelien Jacobs < aurel@gnuage.org >

 *

 * Based on LzmaDecode.c from the LZMA SDK 4.22 (https://www.7-zip.org/)

 * Copyright (C) 1999-2005  Igor Pavlov

/*

 * Extracted fronm glob.c

 Boot with "glob.verbose=1" to show successful tests, too */

 Can't get string literals into a particular section, so... */

/*

 * The tests are all jammed together in one array to make it simpler

 * to place that array in the .init.rodata section.  The obvious

 * "array of structures containing char *" has no way to force the

 * pointed-to strings to be in a particular section.

 *

 * Anyway, a test consists of:

 * 1. Expected glob_match result: '1' or '0'.

 * 2. Pattern to match: null-terminated string

 * 3. String to match against: null-terminated string

 *

 * The list of tests is terminated with a final '\0' instead of

 * a glob_match result character.

 Some basic tests */

 Simple character class tests */

 Corner cases in character class parsing */

 Simple wild cards */

 Asterisk wild cards (backtracking) */

 Multiple asterisks (complex backtracking) */

	/*

	 * Tests are jammed together in a string.  The first byte is '1'

	 * or '0' to indicate the expected outcome, or '\0' to indicate the

	 * end of the tests.  Then come two null-terminated strings: the

	 * pattern and the string to match it against.

 What's the errno for "kernel bug detected"?  Guess... */

 We need a dummy exit function to allow unload */

 SPDX-License-Identifier: GPL-2.0

/*

 *  NMI backtrace support

 *

 * Gratuitously copied from arch/x86/kernel/apic/hw_nmi.c by Russell King,

 * with the following header:

 *

 *  HW NMI watchdog support

 *

 *  started by Don Zickus, Copyright (C) 2010 Red Hat, Inc.

 *

 *  Arch specific calls to support NMI watchdog

 *

 *  Bits copied from original nmi.c file

 For reliability, we're prepared to waste bits here. */

 "in progress" flag of arch_trigger_cpumask_backtrace */

/*

 * When raise() is called it will be passed a pointer to the

 * backtrace_mask. Architectures that call nmi_cpu_backtrace()

 * directly from their raise() functions may rely on the mask

 * they are passed being updated as a side effect of this call.

		/*

		 * If there is already a trigger_all_cpu_backtrace() in progress

		 * (backtrace_flag == 1), don't output double cpu dump infos.

	/*

	 * Don't try to send an NMI to this cpu; it may work on some

	 * architectures, but on others it may not, and we'll get

	 * information at least as useful just by doing a dump_stack() here.

	 * Note that nmi_cpu_backtrace(NULL) will clear the cpu bit.

 Wait for up to 10 seconds for all CPUs to do the backtrace */

	/*

	 * Force flush any remote buffers that might be stuck in IRQ context

	 * and therefore could not run their irq_work.

 Dump stacks even for idle CPUs.

		/*

		 * Allow nested NMI backtraces while serializing

		 * against other CPUs.

 SPDX-License-Identifier: GPL-2.0

/*

 * This module tests the blackhole_dev that is created during the

 * net subsystem initialization. The test this module performs is

 * by injecting an skb into the stack with skb->dev as the

 * blackhole_dev and expects kernel to behave in a sane manner

 * (in other words, *not crash*)!

 *

 * Copyright (c) 2018, Mahesh Bandewar <maheshb@google.com>

 Ether + IPv6 + UDP */

 random tail-room */

 Reserve head-room for the headers */

 Add data to the skb */

 Add protocol data */

 (Transport) UDP */

 (Network) IPv6 */

 Ether */

 Now attempt to send the packet */

/*

 * Copyright (c) 2011 Broadcom Corporation

 *

 * Permission to use, copy, modify, and/or distribute this software for any

 * purpose with or without fee is hereby granted, provided that the above

 * copyright notice and this permission notice appear in all copies.

 *

 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES

 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF

 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY

 * SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES

 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION

 * OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN

 * CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

/**

 * crc8_populate_msb - fill crc table for given polynomial in reverse bit order.

 *

 * @table:	table to be filled.

 * @polynomial:	polynomial for which table is to be filled.

/**

 * crc8_populate_lsb - fill crc table for given polynomial in regular bit order.

 *

 * @table:	table to be filled.

 * @polynomial:	polynomial for which table is to be filled.

/**

 * crc8 - calculate a crc8 over the given input data.

 *

 * @table: crc table used for calculation.

 * @pdata: pointer to data buffer.

 * @nbytes: number of bytes in data buffer.

 * @crc: previous returned crc8 value.

 loop over the buffer data */

 SPDX-License-Identifier: GPL-2.0

/*

 * lib/bust_spinlocks.c

 *

 * Provides a minimal bust_spinlocks for architectures which don't

 * have one of their own.

 *

 * bust_spinlocks() clears any spinlocks which would prevent oops, die(), BUG()

 * and panic() information from reaching the user.

 SPDX-License-Identifier: GPL-2.0

/*

 * NETLINK      Netlink attributes

 *

 * 		Authors:	Thomas Graf <tgraf@suug.ch>

 * 				Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>

/* For these data types, attribute length should be exactly the given

 * size. However, to maintain compatibility with broken commands, if the

 * attribute length does not match the expected size a warning is emitted

 * to the user that the command is sending invalid data and needs to be fixed.

/*

 * Nested policies might refer back to the original

 * policy in some cases, and userspace could try to

 * abuse that and recurse by nesting in the right

 * ways. Limit recursion to avoid this problem.

disallow invalid bit selector */

disallow invalid bit values */

disallow valid bit values that are not selected*/

 this assumes min <= max (don't validate against min) */

		/* a nested attributes is allowed to be empty; if its not,

		 * it must have a size of at least NLA_HDRLEN.

				/*

				 * return directly to preserve the inner

				 * error message/attribute pointer

		/* a nested array attribute is allowed to be empty; if its not,

		 * it must have a size of at least NLA_HDRLEN.

				/*

				 * return directly to preserve the inner

				 * error message/attribute pointer

 further validation */

 nothing to do */

/**

 * __nla_validate - Validate a stream of attributes

 * @head: head of attribute stream

 * @len: length of attribute stream

 * @maxtype: maximum attribute type to be expected

 * @policy: validation policy

 * @validate: validation strictness

 * @extack: extended ACK report struct

 *

 * Validates all attributes in the specified attribute stream against the

 * specified policy. Validation depends on the validate flags passed, see

 * &enum netlink_validation for more details on that.

 * See documentation of struct nla_policy for more details.

 *

 * Returns 0 on success or a negative error code.

/**

 * nla_policy_len - Determine the max. length of a policy

 * @policy: policy to use

 * @n: number of policies

 *

 * Determines the max. length of the policy.  It is currently used

 * to allocated Netlink buffers roughly the size of the actual

 * message.

 *

 * Returns 0 on success or a negative error code.

/**

 * __nla_parse - Parse a stream of attributes into a tb buffer

 * @tb: destination array with maxtype+1 elements

 * @maxtype: maximum attribute type to be expected

 * @head: head of attribute stream

 * @len: length of attribute stream

 * @policy: validation policy

 * @validate: validation strictness

 * @extack: extended ACK pointer

 *

 * Parses a stream of attributes and stores a pointer to each attribute in

 * the tb array accessible via the attribute type.

 * Validation is controlled by the @validate parameter.

 *

 * Returns 0 on success or a negative error code.

/**

 * nla_find - Find a specific attribute in a stream of attributes

 * @head: head of attribute stream

 * @len: length of attribute stream

 * @attrtype: type of attribute to look for

 *

 * Returns the first attribute in the stream matching the specified type.

/**

 * nla_strscpy - Copy string attribute payload into a sized buffer

 * @dst: Where to copy the string to.

 * @nla: Attribute to copy the string from.

 * @dstsize: Size of destination buffer.

 *

 * Copies at most dstsize - 1 bytes into the destination buffer.

 * Unlike strlcpy the destination buffer is always padded out.

 *

 * Return:

 * * srclen - Returns @nla length (not including the trailing %NUL).

 * * -E2BIG - If @dstsize is 0 or greater than U16_MAX or @nla length greater

 *            than @dstsize.

 Zero pad end of dst. */

/**

 * nla_strdup - Copy string attribute payload into a newly allocated buffer

 * @nla: attribute to copy the string from

 * @flags: the type of memory to allocate (see kmalloc).

 *

 * Returns a pointer to the allocated buffer or NULL on error.

/**

 * nla_memcpy - Copy a netlink attribute into another memory area

 * @dest: where to copy to memcpy

 * @src: netlink attribute to copy from

 * @count: size of the destination area

 *

 * Note: The number of bytes copied is limited by the length of

 *       attribute's payload. memcpy

 *

 * Returns the number of bytes copied.

/**

 * nla_memcmp - Compare an attribute with sized memory area

 * @nla: netlink attribute

 * @data: memory area

 * @size: size of memory area

/**

 * nla_strcmp - Compare a string attribute against a string

 * @nla: netlink string attribute

 * @str: another string

/**

 * __nla_reserve - reserve room for attribute on the skb

 * @skb: socket buffer to reserve room on

 * @attrtype: attribute type

 * @attrlen: length of attribute payload

 *

 * Adds a netlink attribute header to a socket buffer and reserves

 * room for the payload but does not copy it.

 *

 * The caller is responsible to ensure that the skb provides enough

 * tailroom for the attribute header and payload.

/**

 * __nla_reserve_64bit - reserve room for attribute on the skb and align it

 * @skb: socket buffer to reserve room on

 * @attrtype: attribute type

 * @attrlen: length of attribute payload

 * @padattr: attribute type for the padding

 *

 * Adds a netlink attribute header to a socket buffer and reserves

 * room for the payload but does not copy it. It also ensure that this

 * attribute will have a 64-bit aligned nla_data() area.

 *

 * The caller is responsible to ensure that the skb provides enough

 * tailroom for the attribute header and payload.

/**

 * __nla_reserve_nohdr - reserve room for attribute without header

 * @skb: socket buffer to reserve room on

 * @attrlen: length of attribute payload

 *

 * Reserves room for attribute payload without a header.

 *

 * The caller is responsible to ensure that the skb provides enough

 * tailroom for the payload.

/**

 * nla_reserve - reserve room for attribute on the skb

 * @skb: socket buffer to reserve room on

 * @attrtype: attribute type

 * @attrlen: length of attribute payload

 *

 * Adds a netlink attribute header to a socket buffer and reserves

 * room for the payload but does not copy it.

 *

 * Returns NULL if the tailroom of the skb is insufficient to store

 * the attribute header and payload.

/**

 * nla_reserve_64bit - reserve room for attribute on the skb and align it

 * @skb: socket buffer to reserve room on

 * @attrtype: attribute type

 * @attrlen: length of attribute payload

 * @padattr: attribute type for the padding

 *

 * Adds a netlink attribute header to a socket buffer and reserves

 * room for the payload but does not copy it. It also ensure that this

 * attribute will have a 64-bit aligned nla_data() area.

 *

 * Returns NULL if the tailroom of the skb is insufficient to store

 * the attribute header and payload.

/**

 * nla_reserve_nohdr - reserve room for attribute without header

 * @skb: socket buffer to reserve room on

 * @attrlen: length of attribute payload

 *

 * Reserves room for attribute payload without a header.

 *

 * Returns NULL if the tailroom of the skb is insufficient to store

 * the attribute payload.

/**

 * __nla_put - Add a netlink attribute to a socket buffer

 * @skb: socket buffer to add attribute to

 * @attrtype: attribute type

 * @attrlen: length of attribute payload

 * @data: head of attribute payload

 *

 * The caller is responsible to ensure that the skb provides enough

 * tailroom for the attribute header and payload.

/**

 * __nla_put_64bit - Add a netlink attribute to a socket buffer and align it

 * @skb: socket buffer to add attribute to

 * @attrtype: attribute type

 * @attrlen: length of attribute payload

 * @data: head of attribute payload

 * @padattr: attribute type for the padding

 *

 * The caller is responsible to ensure that the skb provides enough

 * tailroom for the attribute header and payload.

/**

 * __nla_put_nohdr - Add a netlink attribute without header

 * @skb: socket buffer to add attribute to

 * @attrlen: length of attribute payload

 * @data: head of attribute payload

 *

 * The caller is responsible to ensure that the skb provides enough

 * tailroom for the attribute payload.

/**

 * nla_put - Add a netlink attribute to a socket buffer

 * @skb: socket buffer to add attribute to

 * @attrtype: attribute type

 * @attrlen: length of attribute payload

 * @data: head of attribute payload

 *

 * Returns -EMSGSIZE if the tailroom of the skb is insufficient to store

 * the attribute header and payload.

/**

 * nla_put_64bit - Add a netlink attribute to a socket buffer and align it

 * @skb: socket buffer to add attribute to

 * @attrtype: attribute type

 * @attrlen: length of attribute payload

 * @data: head of attribute payload

 * @padattr: attribute type for the padding

 *

 * Returns -EMSGSIZE if the tailroom of the skb is insufficient to store

 * the attribute header and payload.

/**

 * nla_put_nohdr - Add a netlink attribute without header

 * @skb: socket buffer to add attribute to

 * @attrlen: length of attribute payload

 * @data: head of attribute payload

 *

 * Returns -EMSGSIZE if the tailroom of the skb is insufficient to store

 * the attribute payload.

/**

 * nla_append - Add a netlink attribute without header or padding

 * @skb: socket buffer to add attribute to

 * @attrlen: length of attribute payload

 * @data: head of attribute payload

 *

 * Returns -EMSGSIZE if the tailroom of the skb is insufficient to store

 * the attribute payload.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * test_kprobes.c - simple sanity test for *probes

 *

 * Copyright IBM Corp. 2008

 This is for preventing inlining the function */

 addr and flags should be cleard for reusing kprobe. */

 addr and flags should be cleard for reusing kprobe. */

	/*

	 * Test stacktrace inside the kretprobe handler, this will involves

	 * kretprobe trampoline, but must include correct return address

	 * of the target function.

	/*

	 * Test stacktrace from pt_regs at the return address. Thus the stack

	 * trace must start from the target return address.

	/*

	 * Run the stacktrace_driver() to record correct return address in

	 * stacktrace_target() and ensure stacktrace_driver() call is not

	 * inlined by checking the return address of stacktrace_driver()

	 * and the return address of this function is different.

	/*

	 * Test stacktrace inside the kretprobe handler for nested case.

	 * The unwinder will find the kretprobe_trampoline address on the

	 * return address, and kretprobe must solve that.

 Ditto for the regs version. */

KUNIT_ASSERT_NE(test, myretaddr, stacktrace_driver());

 CONFIG_ARCH_CORRECT_STACKTRACE_ON_KRETPROBE */

 CONFIG_KRETPROBES */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * lib/btree.c	- Simple In-memory B+Tree

 *

 * Copyright (c) 2007-2008 Joern Engel <joern@purestorage.com>

 * Bits and pieces stolen from Peter Zijlstra's code, which is

 * Copyright 2007, Red Hat Inc. Peter Zijlstra

 *

 * see http://programming.kicks-ass.net/kernel-patches/vma_lookup/btree.patch

 *

 * A relatively simple B+Tree implementation.  I have written it as a learning

 * exercise to understand how B+Trees work.  Turned out to be useful as well.

 *

 * B+Trees can be used similar to Linux radix trees (which don't have anything

 * in common with textbook radix trees, beware).  Prerequisite for them working

 * well is that access to a random tree node is much faster than a large number

 * of operations within each node.

 *

 * Disks have fulfilled the prerequisite for a long time.  More recently DRAM

 * has gained similar properties, as memory access times, when measured in cpu

 * cycles, have increased.  Cacheline sizes have increased as well, which also

 * helps B+Trees.

 *

 * Compared to radix trees, B+Trees are more efficient when dealing with a

 * sparsely populated address space.  Between 25% and 50% of the memory is

 * occupied with valid pointers.  When densely populated, radix trees contain

 * ~98% pointers - hard to beat.  Very sparse radix trees contain only ~2%

 * pointers.

 *

 * This particular implementation stores pointers identified by a long value.

 * Storing NULL pointers is illegal, lookup will return NULL when no entry

 * was found.

 *

 * A tricks was used that is not commonly found in textbooks.  The lowest

 * values are to the right, not to the left.  All used slots within a node

 * are on the left, all unused slots contain NUL values.  Most operations

 * simply loop once over all slots and terminate on the first NUL.

/*

 * Usually this function is quite similar to normal lookup.  But the key of

 * a parent node may be smaller than the smallest key of all its siblings.

 * In such a case we cannot just return NULL, as we have only proven that no

 * key smaller than __key, but larger than this parent key exists.

 * So we set __key to the parent key and retry.  We have to use the smallest

 * such parent key, which is the last parent key we encountered.

/*

 * locate the correct leaf node in the btree

 right-most key is too large, update it */

			/* FIXME: If the right-most key on higher levels is

 two identical keys are not allowed */

 need to split node */

 shift and insert */

 Move all keys to the left */

 Exchange left and right child in parent */

 Remove left (formerly right) child from parent */

		/* Because we don't steal entries from a neighbour, this case

		 * can happen.  Parent node contains a single child, this

		 * node, so merging with a sibling never happens.

	/*

	 * We could also try to steal one entry from the left or right

	 * neighbor.  By not doing so we changed the invariant from

	 * "all nodes are at least half full" to "no two neighboring

	 * nodes can be merged".  Which means that the average fill of

	 * all nodes is still half or better.

 we recursed all the way up */

 remove and shift */

 target is empty, just copy fields over */

	/* TODO: This needs some optimizations.  Currently we do three tree

	 * walks to remove a single object from the victim.

		/* We must make a copy of the key, as the original will get

 If core code starts using btree, initialization should happen even earlier */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Generic show_mem() implementation

 *

 * Copyright (C) 2008 Johannes Weiner <hannes@saeurebad.de>

/* Allocate an array of spinlocks to be accessed by a hash. Two arguments

 * indicate the number of elements to allocate in the array. max_size

 * gives the maximum number of elements to allocate. cpu_mult gives

 * the number of locks per CPU to allocate. The size is rounded up

 * to a power of 2 to be suitable as a hash table.

 SPDX-License-Identifier: GPL-2.0-only

		/*

		 * Create debugfs r/w file containing action->error. If

		 * notifier call chain is called with action->val, it will

		 * fail with the error code

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Generic Timer-queue

 *

 *  Manages a simple queue of timers, ordered by expiration time.

 *  Uses rbtrees for quick list adds and expiration.

 *

 *  NOTE: All of the following functions need to be serialized

 *  to avoid races. No locking is done by this library code.

/**

 * timerqueue_add - Adds timer to timerqueue.

 *

 * @head: head of timerqueue

 * @node: timer node to be added

 *

 * Adds the timer node to the timerqueue, sorted by the node's expires

 * value. Returns true if the newly added timer is the first expiring timer in

 * the queue.

 Make sure we don't add nodes that are already added */

/**

 * timerqueue_del - Removes a timer from the timerqueue.

 *

 * @head: head of timerqueue

 * @node: timer node to be removed

 *

 * Removes the timer node from the timerqueue. Returns true if the queue is

 * not empty after the remove.

/**

 * timerqueue_iterate_next - Returns the timer after the provided timer

 *

 * @node: Pointer to a timer.

 *

 * Provides the timer that is after the given node. This is used, when

 * necessary, to iterate through the list of timers in a timer list

 * without modifying the list.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * lib/ts_bm.c		Boyer-Moore text search implementation

 *

 * Authors:	Pablo Neira Ayuso <pablo@eurodev.net>

 *

 * ==========================================================================

 * 

 *   Implements Boyer-Moore string matching algorithm:

 *

 *   [1] A Fast String Searching Algorithm, R.S. Boyer and Moore.

 *       Communications of the Association for Computing Machinery, 

 *       20(10), 1977, pp. 762-772.

 *       https://www.cs.utexas.edu/users/moore/publications/fstrpos.pdf

 *

 *   [2] Handbook of Exact String Matching Algorithms, Thierry Lecroq, 2004

 *       http://www-igm.univ-mlv.fr/~lecroq/string/string.pdf

 *

 *   Note: Since Boyer-Moore (BM) performs searches for matchings from right 

 *   to left, it's still possible that a matching could be spread over 

 *   multiple blocks, in that case this algorithm won't find any coincidence.

 *   

 *   If you're willing to ensure that such thing won't ever happen, use the

 *   Knuth-Pratt-Morris (KMP) implementation instead. In conclusion, choose 

 *   the proper string search algorithm depending on your setting. 

 *

 *   Say you're using the textsearch infrastructure for filtering, NIDS or 

 *   any similar security focused purpose, then go KMP. Otherwise, if you 

 *   really care about performance, say you're classifying packets to apply

 *   Quality of Service (QoS) policies, and you don't mind about possible

 *   matchings spread over multiple fragments, then go BM.

 Alphabet size, use ASCII */

 London calling... */

 Now jumping to... */

	/* Compute the good shift array, used to match reocurrences 

 SPDX-License-Identifier: GPL-2.0

 XX:XX:XX:XX:XX:XX */

 Don't dirty result unless string is valid MAC. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * lib/parser.c - simple parser for mount, etc. options.

/**

 * match_one - Determines if a string matches a simple pattern

 * @s: the string to examine for presence of the pattern

 * @p: the string containing the pattern

 * @args: array of %MAX_OPT_ARGS &substring_t elements. Used to return match

 * locations.

 *

 * Description: Determines if the pattern @p is present in string @s. Can only

 * match extremely simple token=arg style patterns. If the pattern is found,

 * the location(s) of the arguments will be returned in the @args array.

/**

 * match_token - Find a token (and optional args) in a string

 * @s: the string to examine for token/argument pairs

 * @table: match_table_t describing the set of allowed option tokens and the

 * arguments that may be associated with them. Must be terminated with a

 * &struct match_token whose pattern is set to the NULL pointer.

 * @args: array of %MAX_OPT_ARGS &substring_t elements. Used to return match

 * locations.

 *

 * Description: Detects which if any of a set of token strings has been passed

 * to it. Tokens can include up to %MAX_OPT_ARGS instances of basic c-style

 * format identifiers which will be taken into account when matching the

 * tokens, and whose locations will be returned in the @args array.

/**

 * match_number - scan a number in the given base from a substring_t

 * @s: substring to be scanned

 * @result: resulting integer on success

 * @base: base to use when converting string

 *

 * Description: Given a &substring_t and a base, attempts to parse the substring

 * as a number in that base.

 *

 * Return: On success, sets @result to the integer represented by the

 * string and returns 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.

/**

 * match_u64int - scan a number in the given base from a substring_t

 * @s: substring to be scanned

 * @result: resulting u64 on success

 * @base: base to use when converting string

 *

 * Description: Given a &substring_t and a base, attempts to parse the substring

 * as a number in that base.

 *

 * Return: On success, sets @result to the integer represented by the

 * string and returns 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.

/**

 * match_int - scan a decimal representation of an integer from a substring_t

 * @s: substring_t to be scanned

 * @result: resulting integer on success

 *

 * Description: Attempts to parse the &substring_t @s as a decimal integer.

 *

 * Return: On success, sets @result to the integer represented by the string

 * and returns 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.

/**

 * match_uint - scan a decimal representation of an integer from a substring_t

 * @s: substring_t to be scanned

 * @result: resulting integer on success

 *

 * Description: Attempts to parse the &substring_t @s as a decimal integer.

 *

 * Return: On success, sets @result to the integer represented by the string

 * and returns 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.

/**

 * match_u64 - scan a decimal representation of a u64 from

 *                  a substring_t

 * @s: substring_t to be scanned

 * @result: resulting unsigned long long on success

 *

 * Description: Attempts to parse the &substring_t @s as a long decimal

 * integer.

 *

 * Return: On success, sets @result to the integer represented by the string

 * and returns 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.

/**

 * match_octal - scan an octal representation of an integer from a substring_t

 * @s: substring_t to be scanned

 * @result: resulting integer on success

 *

 * Description: Attempts to parse the &substring_t @s as an octal integer.

 *

 * Return: On success, sets @result to the integer represented by the string

 * and returns 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.

/**

 * match_hex - scan a hex representation of an integer from a substring_t

 * @s: substring_t to be scanned

 * @result: resulting integer on success

 *

 * Description: Attempts to parse the &substring_t @s as a hexadecimal integer.

 *

 * Return: On success, sets @result to the integer represented by the string

 * and returns 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.

/**

 * match_wildcard - parse if a string matches given wildcard pattern

 * @pattern: wildcard pattern

 * @str: the string to be parsed

 *

 * Description: Parse the string @str to check if matches wildcard

 * pattern @pattern. The pattern may contain two types of wildcards:

 *   '*' - matches zero or more characters

 *   '?' - matches one character

 *

 * Return: If the @str matches the @pattern, return true, else return false.

/**

 * match_strlcpy - Copy the characters from a substring_t to a sized buffer

 * @dest: where to copy to

 * @src: &substring_t to copy

 * @size: size of destination buffer

 *

 * Description: Copy the characters in &substring_t @src to the

 * c-style string @dest.  Copy no more than @size - 1 characters, plus

 * the terminating NUL.

 *

 * Return: length of @src.

/**

 * match_strdup - allocate a new string with the contents of a substring_t

 * @s: &substring_t to copy

 *

 * Description: Allocates and returns a string filled with the contents of

 * the &substring_t @s. The caller is responsible for freeing the returned

 * string with kfree().

 *

 * Return: the address of the newly allocated NUL-terminated string or

 * %NULL on error.

 SPDX-License-Identifier: GPL-2.0+

/*

 * test_free_pages.c: Check that free_pages() doesn't leak memory

 * Copyright (c) 2020 Oracle

 * Author: Matthew Wilcox <willy@infradead.org>

 Simulate page cache getting a speculative reference */

/*

 * Generic binary BCH encoding/decoding library

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of the GNU General Public License version 2 as published by

 * the Free Software Foundation.

 *

 * This program is distributed in the hope that it will be useful, but WITHOUT

 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or

 * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for

 * more details.

 *

 * You should have received a copy of the GNU General Public License along with

 * this program; if not, write to the Free Software Foundation, Inc., 51

 * Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.

 *

 * Copyright © 2011 Parrot S.A.

 *

 * Author: Ivan Djelic <ivan.djelic@parrot.com>

 *

 * Description:

 *

 * This library provides runtime configurable encoding/decoding of binary

 * Bose-Chaudhuri-Hocquenghem (BCH) codes.

 *

 * Call bch_init to get a pointer to a newly allocated bch_control structure for

 * the given m (Galois field order), t (error correction capability) and

 * (optional) primitive polynomial parameters.

 *

 * Call bch_encode to compute and store ecc parity bytes to a given buffer.

 * Call bch_decode to detect and locate errors in received data.

 *

 * On systems supporting hw BCH features, intermediate results may be provided

 * to bch_decode in order to skip certain steps. See bch_decode() documentation

 * for details.

 *

 * Option CONFIG_BCH_CONST_PARAMS can be used to force fixed values of

 * parameters m and t; thus allowing extra compiler optimizations and providing

 * better (up to 2x) encoding performance. Using this option makes sense when

 * (m,t) are fixed and known in advance, e.g. when using BCH error correction

 * on a particular NAND flash device.

 *

 * Algorithmic details:

 *

 * Encoding is performed by processing 32 input bits in parallel, using 4

 * remainder lookup tables.

 *

 * The final stage of decoding involves the following internal steps:

 * a. Syndrome computation

 * b. Error locator polynomial computation using Berlekamp-Massey algorithm

 * c. Error locator root finding (by far the most expensive step)

 *

 * In this implementation, step c is not performed using the usual Chien search.

 * Instead, an alternative approach described in [1] is used. It consists in

 * factoring the error locator polynomial using the Berlekamp Trace algorithm

 * (BTA) down to a certain degree (4), after which ad hoc low-degree polynomial

 * solving techniques [2] are used. The resulting algorithm, called BTZ, yields

 * much better performance than Chien search for usual (m,t) values (typically

 * m >= 13, t < 32, see [1]).

 *

 * [1] B. Biswas, V. Herbert. Efficient root finding of polynomials over fields

 * of characteristic 2, in: Western European Workshop on Research in Cryptology

 * - WEWoRC 2009, Graz, Austria, LNCS, Springer, July 2009, to appear.

 * [2] [Zin96] V.A. Zinoviev. On the solution of equations of degree 10 over

 * finite fields GF(2^q). In Rapport de recherche INRIA no 2829, 1996.

 2KB */

 64 bit correction */

/*

 * represent a polynomial over GF(2^m)

 polynomial degree */

 polynomial terms */

 given its degree, compute a polynomial size in bytes */

 polynomial of degree 1 */

/*

 * same as bch_encode(), but process input data one byte at a time

/*

 * convert ecc bytes to aligned, zero-padded 32-bit ecc words

/*

 * convert 32-bit ecc words to ecc bytes

/**

 * bch_encode - calculate BCH ecc parity of data

 * @bch:   BCH control structure

 * @data:  data to encode

 * @len:   data length in bytes

 * @ecc:   ecc parity data, must be initialized by caller

 *

 * The @ecc parity array is used both as input and output parameter, in order to

 * allow incremental computations. It should be of the size indicated by member

 * @ecc_bytes of @bch, and should be initialized to 0 before the first call.

 *

 * The exact number of computed ecc parity bits is given by member @ecc_bits of

 * @bch; it may be less than m*t for large values of t.

 load ecc parity bytes into internal 32-bit buffer */

 process first unaligned data bytes */

 process 32-bit aligned data words */

	/*

	 * split each 32-bit word into 4 polynomials of weight 8 as follows:

	 *

	 * 31 ...24  23 ...16  15 ... 8  7 ... 0

	 * xxxxxxxx  yyyyyyyy  zzzzzzzz  tttttttt

	 *                               tttttttt  mod g = r0 (precomputed)

	 *                     zzzzzzzz  00000000  mod g = r1 (precomputed)

	 *           yyyyyyyy  00000000  00000000  mod g = r2 (precomputed)

	 * xxxxxxxx  00000000  00000000  00000000  mod g = r3 (precomputed)

	 * xxxxxxxx  yyyyyyyy  zzzzzzzz  tttttttt  mod g = r0^r1^r2^r3

 input data is read in big-endian format */

 process last unaligned bytes */

 store ecc parity bytes into original parity buffer */

/*

 * shorter and faster modulo function, only works when v < 2N.

 polynomial degree is the most-significant bit index */

	/*

	 * public domain code snippet, lifted from

	 * http://www-graphics.stanford.edu/~seander/bithacks.html

 Galois field basic operations: multiply, divide, inverse, etc. */

/*

 * compute 2t syndromes of ecc polynomial, i.e. ecc(a^j) for j=1..2t

 make sure extra bits in last ecc word are cleared */

 compute v(a^j) for j=1 .. 2t-1 */

 v(a^(2j)) = v(a^j)^2 */

 use simplified binary Berlekamp-Massey algorithm */

 e[i+1](X) = e[i](X)+di*dp^-1*X^2(i-p)*e[p](X) */

 compute l[i+1] = max(l[i]->c[l[p]+2*(i-p]) */

 di+1 = S(2i+3)+elp[i+1].1*S(2i+2)+...+elp[i+1].lS(2i+3-l) */

/*

 * solve a m x m linear system in GF(2) with an expected number of solutions,

 * and return the number of found solutions

 Gaussian elimination */

 find suitable row for elimination */

 perform elimination on remaining rows */

 elimination not needed, store defective row index */

 rewrite system, inserting fake parameter rows */

 system has no solution */

 unexpected number of solutions */

 set parameters for p-th solution */

 compute unique solution */

/*

 * this function builds and solves a linear system for finding roots of a degree

 * 4 affine monic polynomial X^4+aX^2+bX+c over GF(2^m).

 build linear system to solve X^4+aX^2+bX+c = 0 */

	/*

	 * transpose 16x16 matrix before passing it to linear solver

	 * warning: this code assumes m < 16

/*

 * compute root r of a degree 1 polynomial over GF(2^m) (returned as log(1/r))

 poly[X] = bX+c with c!=0, root=c/b */

/*

 * compute roots of a degree 2 polynomial over GF(2^m)

 using z=a/bX, transform aX^2+bX+c into z^2+z+u (u=ac/b^2) */

		/*

		 * let u = sum(li.a^i) i=0..m-1; then compute r = sum(li.xi):

		 * r^2+r = sum(li.(xi^2+xi)) = sum(li.(a^i+Tr(a^i).a^k)) =

		 * u + sum(li.Tr(a^i).a^k) = u+a^k.Tr(sum(li.a^i)) = u+a^k.Tr(u)

		 * i.e. r and r+1 are roots iff Tr(u)=0

 verify root */

 reverse z=a/bX transformation and compute log(1/r) */

/*

 * compute roots of a degree 3 polynomial over GF(2^m)

 transform polynomial into monic X^3 + a2X^2 + b2X + c2 */

 (X+a2)(X^3+a2X^2+b2X+c2) = X^4+aX^2+bX+c (affine) */

 c = a2c2      */

 b = a2b2 + c2 */

 a = a2^2 + b2 */

 find the 4 roots of this affine polynomial */

 remove a2 from final list of roots */

/*

 * compute roots of a degree 4 polynomial over GF(2^m)

 transform polynomial into monic X^4 + aX^3 + bX^2 + cX + d */

 use Y=1/X transformation to get an affine polynomial */

 first, eliminate cX by using z=X+e with ae^2+c=0 */

 compute e such that e^2 = c/a */

			/*

			 * use transformation z=X+e:

			 * z^4+e^4 + a(z^3+ez^2+e^2z+e^3) + b(z^2+e^2) +cz+ce+d

			 * z^4 + az^3 + (ae+b)z^2 + (ae^2+c)z+e^4+be^2+ae^3+ce+d

			 * z^4 + az^3 + (ae+b)z^2 + e^4+be^2+d

			 * z^4 + az^3 +     b'z^2 + d'

 now, use Y=1/X to get Y^4 + b/dY^2 + a/dY + 1/d */

 assume all roots have multiplicity 1 */

 polynomial is already affine */

 find the 4 roots of this affine polynomial */

 post-process roots (reverse transformations) */

/*

 * build monic, log-based representation of a polynomial

 represent 0 values with -1; warning, rep[d] is not set to 1 */

/*

 * compute polynomial Euclidean division remainder in GF(2^m)[X]

 reuse or compute log representation of denominator */

/*

 * compute polynomial Euclidean division quotient in GF(2^m)[X]

 compute a mod b (modifies a) */

 quotient is stored in upper part of polynomial a */

/*

 * compute polynomial GCD (Greatest Common Divisor) in GF(2^m)[X]

/*

 * Given a polynomial f and an integer k, compute Tr(a^kX) mod f

 * This is used in Berlekamp Trace algorithm for splitting polynomials

 z contains z^2j mod f */

 compute f log representation only once */

 add a^(k*2^i)(z^(2^i) mod f) and compute (z^(2^i) mod f)^2 */

 z^(2(i+1)) mod f = (z^(2^i) mod f)^2 mod f */

/*

 * factor a polynomial using Berlekamp Trace algorithm (BTA)

 tk = Tr(a^k.X) mod f */

 compute g = gcd(f, tk) (destructive operation) */

 compute h=f/gcd(f,tk); this will modify f and q */

 store g and h in-place (clobbering f) */

/*

 * find roots of a polynomial, using BTZ algorithm; see the beginning of this

 * file for details

 handle low degree polynomials with ad hoc techniques */

 factor polynomial using Berlekamp Trace Algorithm (BTA) */

/*

 * exhaustive root search (Chien) implementation - not used, included only for

 * reference/comparison tests

 use a log-based representation of polynomial */

 compute elp(a^i) */

 USE_CHIEN_SEARCH */

/**

 * bch_decode - decode received codeword and find bit error locations

 * @bch:      BCH control structure

 * @data:     received data, ignored if @calc_ecc is provided

 * @len:      data length in bytes, must always be provided

 * @recv_ecc: received ecc, if NULL then assume it was XORed in @calc_ecc

 * @calc_ecc: calculated ecc, if NULL then calc_ecc is computed from @data

 * @syn:      hw computed syndrome data (if NULL, syndrome is calculated)

 * @errloc:   output array of error locations

 *

 * Returns:

 *  The number of errors found, or -EBADMSG if decoding failed, or -EINVAL if

 *  invalid parameters were provided

 *

 * Depending on the available hw BCH support and the need to compute @calc_ecc

 * separately (using bch_encode()), this function should be called with one of

 * the following parameter configurations -

 *

 * by providing @data and @recv_ecc only:

 *   bch_decode(@bch, @data, @len, @recv_ecc, NULL, NULL, @errloc)

 *

 * by providing @recv_ecc and @calc_ecc:

 *   bch_decode(@bch, NULL, @len, @recv_ecc, @calc_ecc, NULL, @errloc)

 *

 * by providing ecc = recv_ecc XOR calc_ecc:

 *   bch_decode(@bch, NULL, @len, NULL, ecc, NULL, @errloc)

 *

 * by providing syndrome results @syn:

 *   bch_decode(@bch, NULL, @len, NULL, NULL, @syn, @errloc)

 *

 * Once bch_decode() has successfully returned with a positive value, error

 * locations returned in array @errloc should be interpreted as follows -

 *

 * if (errloc[n] >= 8*len), then n-th error is located in ecc (no need for

 * data correction)

 *

 * if (errloc[n] < 8*len), then n-th error is located in data and can be

 * corrected with statement data[errloc[n]/8] ^= 1 << (errloc[n] % 8);

 *

 * Note that this function does not perform any data correction by itself, it

 * merely indicates error locations.

 sanity check: make sure data length can be handled */

 if caller does not provide syndromes, compute them */

 compute received data ecc into an internal buffer */

 load provided calculated ecc */

 load received ecc or assume it was XORed in calc_ecc */

 XOR received and calculated ecc */

 no error found */

 post-process raw error locations for easier correction */

/*

 * generate Galois field lookup tables

 primitive polynomial must be of degree m */

 polynomial is not primitive (a^i=1 with 0<i<2^m-1) */

/*

 * compute generator polynomial remainder tables for fast encoding

 p(X)=i is a small polynomial of weight <= 8 */

 we want to compute (p(X).X^(8*b+deg(g))) mod g(X) */

 subtract X^d.g(X) from p(X).X^(8*b+deg(g)) */

/*

 * build a base for factoring degree 2 polynomials

 find k s.t. Tr(a^k) = 1 and 0 <= k < m */

 find xi, i=0..m-1 such that xi^2+xi = a^i+Tr(a^i).a^k */

 should not happen but check anyway */

/*

 * compute generator polynomial for given (m,t) parameters.

 enumerate all roots of g(X) */

 build generator polynomial g(X) */

 multiply g(X) by (X+root) */

 store left-justified binary representation of g(X) */

/**

 * bch_init - initialize a BCH encoder/decoder

 * @m:          Galois field order, should be in the range 5-15

 * @t:          maximum error correction capability, in bits

 * @prim_poly:  user-provided primitive polynomial (or 0 to use default)

 * @swap_bits:  swap bits within data and syndrome bytes

 *

 * Returns:

 *  a newly allocated BCH control structure if successful, NULL otherwise

 *

 * This initialization can take some time, as lookup tables are built for fast

 * encoding/decoding; make sure not to call this function from a time critical

 * path. Usually, bch_init() should be called on module/driver init and

 * bch_free() should be called to release memory on exit.

 *

 * You may provide your own primitive polynomial of degree @m in argument

 * @prim_poly, or let bch_init() use its default polynomial.

 *

 * Once bch_init() has successfully returned a pointer to a newly allocated

 * BCH control structure, ecc length in bytes is given by member @ecc_bytes of

 * the structure.

 default primitive polynomials */

		/*

		 * values of m greater than 15 are not currently supported;

		 * supporting m > 15 would require changing table base type

		 * (uint16_t) and a small patch in matrix transposition

		/*

		 * we can support larger than 64 bits if necessary, at the

		 * cost of higher stack usage.

 sanity checks */

 invalid t value */

 select a primitive polynomial for generating GF(2^m) */

 use generator polynomial for computing encoding tables */

/**

 *  bch_free - free the BCH control structure

 *  @bch:    BCH control structure to release

 SPDX-License-Identifier: GPL-2.0

/**

 * devm_ioremap - Managed ioremap()

 * @dev: Generic device to remap IO address for

 * @offset: Resource address to map

 * @size: Size of map

 *

 * Managed ioremap().  Map is automatically unmapped on driver detach.

/**

 * devm_ioremap_uc - Managed ioremap_uc()

 * @dev: Generic device to remap IO address for

 * @offset: Resource address to map

 * @size: Size of map

 *

 * Managed ioremap_uc().  Map is automatically unmapped on driver detach.

/**

 * devm_ioremap_wc - Managed ioremap_wc()

 * @dev: Generic device to remap IO address for

 * @offset: Resource address to map

 * @size: Size of map

 *

 * Managed ioremap_wc().  Map is automatically unmapped on driver detach.

/**

 * devm_ioremap_np - Managed ioremap_np()

 * @dev: Generic device to remap IO address for

 * @offset: Resource address to map

 * @size: Size of map

 *

 * Managed ioremap_np().  Map is automatically unmapped on driver detach.

/**

 * devm_iounmap - Managed iounmap()

 * @dev: Generic device to unmap for

 * @addr: Address to unmap

 *

 * Managed iounmap().  @addr must have been mapped using devm_ioremap*().

/**

 * devm_ioremap_resource() - check, request region, and ioremap resource

 * @dev: generic device to handle the resource for

 * @res: resource to be handled

 *

 * Checks that a resource is a valid memory region, requests the memory

 * region and ioremaps it. All operations are managed and will be undone

 * on driver detach.

 *

 * Usage example:

 *

 *	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);

 *	base = devm_ioremap_resource(&pdev->dev, res);

 *	if (IS_ERR(base))

 *		return PTR_ERR(base);

 *

 * Return: a pointer to the remapped memory or an ERR_PTR() encoded error code

 * on failure.

/**

 * devm_ioremap_resource_wc() - write-combined variant of

 *				devm_ioremap_resource()

 * @dev: generic device to handle the resource for

 * @res: resource to be handled

 *

 * Return: a pointer to the remapped memory or an ERR_PTR() encoded error code

 * on failure.

/*

 * devm_of_iomap - Requests a resource and maps the memory mapped IO

 *		   for a given device_node managed by a given device

 *

 * Checks that a resource is a valid memory region, requests the memory

 * region and ioremaps it. All operations are managed and will be undone

 * on driver detach of the device.

 *

 * This is to be used when a device requests/maps resources described

 * by other device tree nodes (children or otherwise).

 *

 * @dev:	The device "managing" the resource

 * @node:       The device-tree node where the resource resides

 * @index:	index of the MMIO range in the "reg" property

 * @size:	Returns the size of the resource (pass NULL if not needed)

 *

 * Usage example:

 *

 *	base = devm_of_iomap(&pdev->dev, node, 0, NULL);

 *	if (IS_ERR(base))

 *		return PTR_ERR(base);

 *

 * Please Note: This is not a one-to-one replacement for of_iomap() because the

 * of_iomap() function does not track whether the region is already mapped.  If

 * two drivers try to map the same memory, the of_iomap() function will succeed

 * but the devm_of_iomap() function will return -EBUSY.

 *

 * Return: a pointer to the requested and mapped memory or an ERR_PTR() encoded

 * error code on failure.

/*

 * Generic iomap devres

/**

 * devm_ioport_map - Managed ioport_map()

 * @dev: Generic device to map ioport for

 * @port: Port to map

 * @nr: Number of ports to map

 *

 * Managed ioport_map().  Map is automatically unmapped on driver

 * detach.

 *

 * Return: a pointer to the remapped memory or NULL on failure.

/**

 * devm_ioport_unmap - Managed ioport_unmap()

 * @dev: Generic device to unmap for

 * @addr: Address to unmap

 *

 * Managed ioport_unmap().  @addr must have been mapped using

 * devm_ioport_map().

 CONFIG_HAS_IOPORT_MAP */

/*

 * PCI iomap devres

/**

 * pcim_iomap_table - access iomap allocation table

 * @pdev: PCI device to access iomap table for

 *

 * Access iomap allocation table for @dev.  If iomap table doesn't

 * exist and @pdev is managed, it will be allocated.  All iomaps

 * recorded in the iomap table are automatically unmapped on driver

 * detach.

 *

 * This function might sleep when the table is first allocated but can

 * be safely called without context and guaranteed to succeed once

 * allocated.

/**

 * pcim_iomap - Managed pcim_iomap()

 * @pdev: PCI device to iomap for

 * @bar: BAR to iomap

 * @maxlen: Maximum length of iomap

 *

 * Managed pci_iomap().  Map is automatically unmapped on driver

 * detach.

 duplicate mappings not allowed */

/**

 * pcim_iounmap - Managed pci_iounmap()

 * @pdev: PCI device to iounmap for

 * @addr: Address to unmap

 *

 * Managed pci_iounmap().  @addr must have been mapped using pcim_iomap().

/**

 * pcim_iomap_regions - Request and iomap PCI BARs

 * @pdev: PCI device to map IO resources for

 * @mask: Mask of BARs to request and iomap

 * @name: Name used when requesting regions

 *

 * Request and iomap regions specified by @mask.

/**

 * pcim_iomap_regions_request_all - Request all BARs and iomap specified ones

 * @pdev: PCI device to map IO resources for

 * @mask: Mask of BARs to iomap

 * @name: Name used when requesting regions

 *

 * Request all PCI BARs and iomap regions specified by @mask.

/**

 * pcim_iounmap_regions - Unmap and release PCI BARs

 * @pdev: PCI device to map IO resources for

 * @mask: Mask of BARs to unmap and release

 *

 * Unmap and release regions specified by @mask.

 CONFIG_PCI */

/**

 * devm_arch_phys_wc_add - Managed arch_phys_wc_add()

 * @dev: Managed device

 * @base: Memory base address

 * @size: Size of memory range

 *

 * Adds a WC MTRR using arch_phys_wc_add() and sets up a release callback.

 * See arch_phys_wc_add() for more information.

/**

 * devm_arch_io_reserve_memtype_wc - Managed arch_io_reserve_memtype_wc()

 * @dev: Managed device

 * @start: Memory base address

 * @size: Size of memory range

 *

 * Reserves a memory range with WC caching using arch_io_reserve_memtype_wc()

 * and sets up a release callback See arch_io_reserve_memtype_wc() for more

 * information.

/*

 * xxHash - Extremely Fast Hash algorithm

 * Copyright (C) 2012-2016, Yann Collet.

 *

 * BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions are

 * met:

 *

 *   * Redistributions of source code must retain the above copyright

 *     notice, this list of conditions and the following disclaimer.

 *   * Redistributions in binary form must reproduce the above

 *     copyright notice, this list of conditions and the following disclaimer

 *     in the documentation and/or other materials provided with the

 *     distribution.

 *

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT

 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,

 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT

 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,

 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY

 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 * This program is free software; you can redistribute it and/or modify it under

 * the terms of the GNU General Public License version 2 as published by the

 * Free Software Foundation. This program is dual-licensed; you may select

 * either version 2 of the GNU General Public License ("GPL") or BSD license

 * ("BSD").

 *

 * You can contact the author at:

 * - xxHash homepage: https://cyan4973.github.io/xxHash/

 * - xxHash source repository: https://github.com/Cyan4973/xxHash

/*-*************************************

 * Macros

/*-*************************************

 * Constants

/*-**************************

 *  Utils

/*-***************************

 * Simple Hash Functions

/*-**************************************************

 * Advanced Hash Functions

 use a local state for memcpy() to avoid strict-aliasing warnings */

 use a local state for memcpy() to avoid strict-aliasing warnings */

 fill in tmp buffer */

 some data left from previous update */

 == seed */ + PRIME32_5;

 fill in tmp buffer */

 tmp buffer is full */

/*

 * Test cases for lib/string_helpers.c module.

 Copy string to in buffer */

 Copy expected result for given flags */

 Call string_unescape and compare result */

 terminator */

 terminator */

 terminator */

 terminator */

 terminator */

 terminator */

 terminator */

 terminator */

 terminator */

 terminator */

 terminator */

 Test cases are NULL-aware */

 ESCAPE_OCTAL has a higher priority */

 NULL injection */

 '\0' passes isascii() test */

 Don't try strings that have no output */

 Copy string to in buffer */

 Copy expected result for given flags */

 small values */

 normal values */

 weird block sizes */

 huge values */

 Without dictionary */

 With dictionary */

 Test string_get_size() */

 Test string upper(), string_lower() */

 SPDX-License-Identifier: GPL-2.0-or-later

/* Decoder for ASN.1 BER/DER/CER encoded bytestream

 *

 * Copyright (C) 2012 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

					OPC TAG JMP ACT */

/*

 * Find the length of an indefinite length object

 * @data: The data buffer

 * @datalen: The end of the innermost containing element in the buffer

 * @_dp: The data parse cursor (updated before returning)

 * @_len: Where to return the size of the element.

 * @_errmsg: Where to return a pointer to an error message on error

 Extract a tag from the data */

 It appears to be an EOC. */

 Extract the length */

 Indefinite length */

/**

 * asn1_ber_decoder - Decoder BER/DER/CER ASN.1 according to pattern

 * @decoder: The decoder definition (produced by asn1_compiler)

 * @context: The caller's context (to be passed to the action functions)

 * @data: The encoded data

 * @datalen: The size of the encoded data

 *

 * Decode BER/DER/CER encoded ASN.1 data according to a bytecode pattern

 * produced by asn1_compiler.  Action functions are called on marked tags to

 * allow the caller to retrieve significant data.

 *

 * LIMITATIONS:

 *

 * To keep down the amount of stack used by this function, the following limits

 * have been imposed:

 *

 *  (1) This won't handle datalen > 65535 without increasing the size of the

 *	cons stack elements and length_too_long checking.

 *

 *  (2) The stack of constructed types is 10 deep.  If the depth of non-leaf

 *	constructed types exceeds this, the decode will fail.

 *

 *  (3) The SET type (not the SET OF type) isn't really supported as tracking

 *	what members of the set have been seen is a pain.

 Last tag matched */

#define FLAG_CONS		0x20 /* Corresponds to CONS bit in the opcode tag

				      * - ie. whether or not we are going to parse

				      *   a compound type.

	/* If this command is meant to match a tag, then do that before

	 * evaluating the command.

 Skip conditional matches if possible */

 Extract a tag from the data */

			/* Extract the tag from the machine

			 * - Either CONS or PRIM are permitted in the data if

			 *   CONS is not set in the op stream, otherwise CONS

			 *   is mandatory.

 Determine whether the tag matched */

 All odd-numbered tags are MATCH_OR_SKIP. */

 Indefinite length */

			/* For expected compound forms, we stack the positions

			 * of the start and end of the data.

 Decide how to handle the operation */

 Indefinite length - check for the EOC. */

 Shouldn't reach here */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * lib/debug_locks.c

 *

 * Generic place for common debugging facilities for various locks:

 * spinlocks, rwlocks, mutexes and rwsems.

 *

 * Started by Ingo Molnar:

 *

 *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

/*

 * We want to turn all lock-debugging facilities on/off at once,

 * via a global flag. The reason is that once a single bug has been

 * detected and reported, there might be cascade of followup bugs

 * that would just muddy the log. So we report the first one and

 * shut up after that.

/*

 * The locking-testsuite uses <debug_locks_silent> to get a

 * 'silent failure': nothing is printed to the console when

 * a locking bug is detected.

/*

 * Generic 'turn off all lock debugging' function:

 SPDX-License-Identifier: GPL-2.0

		/* Keep sparse happy by restoring an even lock count on

		 * this lock. In case we return here, we don't call into

		 * __do_once_done but return early in the DO_ONCE() macro.

 SPDX-License-Identifier: GPL-2.0

/*

 * lib/locking-selftest.c

 *

 * Testsuite for various locking APIs: spinlocks, rwlocks,

 * mutexes and rw-semaphores.

 *

 * It is checking both false positives and false negatives.

 *

 * Started by Ingo Molnar:

 *

 *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

/*

 * Change this to 1 if you want to see the failure printouts:

/*

 * Normal standalone locks, for the circular and irq-context

 * dependency tests:

/*

 * Locks that we initialize dynamically as well so that

 * e.g. X1 and X2 becomes two instances of the same class,

 * but X* and Y* are different classes. We do this so that

 * we do not trigger a real lockup:

/*

 * non-inlined runtime initializers, to let separate locks share

 * the same lock-class:

/*

 * For spinlocks and rwlocks we also do hardirq-safe / softirq-safe tests.

 * The following functions use a lock from a simulated hardirq/softirq

 * context, causing the locks to be marked as hardirq-safe/softirq-safe:

/*

 * Shortcuts for lock/unlock API variants, to keep

 * the testcases compact:

/*

 * Generate different permutations of the same testcase, using

 * the same basic lock-dependency/state events:

/*

 * AA deadlock:

 this one should fail */

/*

 * 6 testcases:

/*

 * Special-case for read-locking, they are

 * allowed to recurse on the same lock class:

 this one should NOT fail

 this one should NOT fail

 this one should fail

 this one should fail

/*

 * The mixing of read and write locks is not allowed:

 this one should fail

 this one should fail

 this one should fail

 this one should fail

/*

 * read_lock(A)

 * spin_lock(B)

 *		spin_lock(B)

 *		write_lock(A)

 should fail

 should fail

/*

 * read_lock(A)

 * spin_lock(B)

 *		spin_lock(B)

 *		write_lock(A)

 *

 * This test case is aimed at poking whether the chain cache prevents us from

 * detecting a read-lock/lock-write deadlock: if the chain cache doesn't differ

 * read/write locks, the following case may happen

 *

 * 	{ read_lock(A)->lock(B) dependency exists }

 *

 * 	P0:

 * 	lock(B);

 * 	read_lock(A);

 *

 *	{ Not a deadlock, B -> A is added in the chain cache }

 *

 *	P1:

 *	lock(B);

 *	write_lock(A);

 *

 *	{ B->A found in chain cache, not reported as a deadlock }

 *

 should fail

/*

 * read_lock(A)

 * spin_lock(B)

 *		spin_lock(B)

 *		read_lock(A)

 should NOT fail

 should fail

/*

 * write_lock(A)

 * spin_lock(B)

 *		spin_lock(B)

 *		write_lock(A)

 should fail

 should fail

/*

 * ABBA deadlock:

 fail */

/*

 * 6 testcases:

/*

 * AB BC CA deadlock:

 fail */

/*

 * 6 testcases:

/*

 * AB CA BC deadlock:

 fail */

/*

 * 6 testcases:

/*

 * AB BC CD DA deadlock:

 fail */

/*

 * 6 testcases:

/*

 * AB CD BD DA deadlock:

 fail */

/*

 * 6 testcases:

/*

 * AB CD BC DA deadlock:

 fail */

/*

 * 6 testcases:

/*

 * Double unlock:

 fail */

/*

 * 6 testcases:

/*

 * initializing a held lock:

 fail */

/*

 * 6 testcases:

/*

 * locking an irq-safe lock with irqs enabled:

/*

 * Generate 24 testcases:

/*

 * Enabling hardirqs with a softirq-safe lock held:

/*

 * Generate 12 testcases:

/*

 * Enabling irqs with an irq-safe lock held:

/*

 * Generate 24 testcases:

/*

 * Acquiring a irq-unsafe lock while holding an irq-safe-lock:

/*

 * Generate 36 testcases:

/*

 * If a lock turns into softirq-safe, but earlier it took

 * a softirq-unsafe lock:

/*

 * Generate 36 testcases:

/*

 * read-lock / write-lock irq inversion.

 *

 * Deadlock scenario:

 *

 * CPU#1 is at #1, i.e. it has write-locked A, but has not

 * taken B yet.

 *

 * CPU#2 is at #2, i.e. it has locked B.

 *

 * Hardirq hits CPU#2 at point #2 and is trying to read-lock A.

 *

 * The deadlock occurs because CPU#1 will spin on B, and CPU#2

 * will spin on A.

/*

 * Generate 36 testcases:

/*

 * write-read / write-read / write-read deadlock even if read is recursive

/*

 * write-write / read-read / write-read deadlock even if read is recursive

/*

 * write-write / read-read / read-write is not deadlock when read is recursive

/*

 * write-read / read-read / write-write is not deadlock when read is recursive

/*

 * read-lock / write-lock recursion that is actually safe.

/*

 * Generate 24 testcases:

/*

 * read-lock / write-lock recursion that is unsafe.

/*

 * Generate 24 testcases:

/*

 * read-lock / write-lock recursion that is unsafe.

 *

 * A is a ENABLED_*_READ lock

 * B is a USED_IN_*_READ lock

 *

 * read_lock(A);

 *			write_lock(B);

 * <interrupt>

 * read_lock(B);

 * 			write_lock(A); // if this one is read_lock(), no deadlock

/*

 * Generate 24 testcases:

	/*

	 * Filter out expected failures:

	/*

	 * Some tests (e.g. double-unlock) might corrupt the preemption

	 * count, so restore it:

/*

 * 'read' variant: rlocks must not trigger.

 No lockdep test, pure API */

	/*

	 * None of the ww_mutex codepaths should be taken in the 'normal'

	 * mutex calls. The easiest way to verify this is by using the

	 * normal mutex calls, and making sure o.ctx is unmodified.

 mutex_lock (and indirectly, mutex_lock_nested) */

 mutex_lock_interruptible (and *_nested) */

 mutex_lock_killable (and *_nested) */

 trylock, succeeding */

 trylock, failing */

 nest_lock */

 This is not a deadlock, because we have X1 to serialize Y1 and Y2 */

/*

 * <in hardirq handler>

 * read_lock(&A);

 *			<hardirq disable>

 *			spin_lock(&B);

 * spin_lock(&B);

 *			read_lock(&A);

 *

 * is a deadlock.

/*

 * <in hardirq handler>

 * spin_lock(&B);

 *			<hardirq disable>

 *			read_lock(&A);

 * read_lock(&A);

 *			spin_lock(&B);

 *

 * is not a deadlock.

/*

 * <hardirq disable>

 * spin_lock(&B);

 *			read_lock(&A);

 *			<in hardirq handler>

 *			spin_lock(&B);

 * read_lock(&A);

 *

 * is a deadlock. Because the two read_lock()s are both non-recursive readers.

/*

 * wait contexts (considering PREEMPT_RT)

 *

 * o: inner is allowed in outer

 * x: inner is disallowed in outer

 *

 *       \  inner |  RCU  | RAW_SPIN | SPIN | MUTEX

 * outer  \       |       |          |      |

 * ---------------+-------+----------+------+-------

 * HARDIRQ        |   o   |    o     |  o   |  x

 * ---------------+-------+----------+------+-------

 * NOTTHREADED_IRQ|   o   |    o     |  x   |  x

 * ---------------+-------+----------+------+-------

 * SOFTIRQ        |   o   |    o     |  o   |  x

 * ---------------+-------+----------+------+-------

 * RCU            |   o   |    o     |  o   |  x

 * ---------------+-------+----------+------+-------

 * RCU_BH         |   o   |    o     |  o   |  x

 * ---------------+-------+----------+------+-------

 * RCU_SCHED      |   o   |    o     |  x   |  x

 * ---------------+-------+----------+------+-------

 * RAW_SPIN       |   o   |    o     |  x   |  x

 * ---------------+-------+----------+------+-------

 * SPIN           |   o   |    o     |  o   |  x

 * ---------------+-------+----------+------+-------

 * MUTEX          |   o   |    o     |  o   |  o

 * ---------------+-------+----------+------+-------

 the outer context allows all kinds of preemption */

/*

 * the outer context only allows the preemption introduced by spinlock_t (which

 * is a sleepable lock for PREEMPT_RT)

 the outer doesn't allows any kind of preemption */

 IRQ-ON */

 IN-IRQ */

 IN-IRQ <-> IRQ-ON cycle, false */

 IRQ-ON */

 IRQ-ON */

 IN-IRQ */

 IN-IRQ <-> IRQ-ON cycle only if we count local_lock(), false */

 IRQ-ON */

 IRQ-ON */

 IN-IRQ */

 IN-IRQ <-> IRQ-ON cycle only if we count local_lock(), false */

 IN-IRQ <-> IRQ-ON cycle, true */

 mutex_A is hardirq-unsafe and softirq-unsafe */

 mutex_A -> lock_C */

 lock_A is hardirq-safe */

 lock_A -> lock_B */

 lock_B -> lock_C */

 lock_D is softirq-safe */

 And lock_D is hardirq-unsafe */

	/*

	 * mutex_A -> lock_C -> lock_D is softirq-unsafe -> softirq-safe, not

	 * deadlock.

	 *

	 * lock_A -> lock_B -> lock_C -> lock_D is hardirq-safe ->

	 * hardirq-unsafe, deadlock.

	/*

	 * Got a locking failure before the selftest ran?

	/*

	 * treats read_lock() as recursive read locks for testing purpose

	/*

	 * Run the testsuite:

	/*

	 * irq-context testcases:

	/*

	 * queued_read_lock() specific test cases can be put here

 Wait context test cases that are specific for RAW_LOCK_NESTING */

/*

 * lib/dynamic_debug.c

 *

 * make pr_debug()/dev_dbg() calls runtime configurable based upon their

 * source module.

 *

 * Copyright (C) 2008 Jason Baron <jbaron@redhat.com>

 * By Greg Banks <gnb@melbourne.sgi.com>

 * Copyright (c) 2008 Silicon Graphics Inc.  All Rights Reserved.

 * Copyright (C) 2011 Bart Van Assche.  All Rights Reserved.

 * Copyright (C) 2013 Du, Changbin <changbin.du@gmail.com>

 Return the path relative to source root */

 prefix mismatch, don't skip */

 format a string into buf[] which describes the _ddebug's flags */

 trim any trailing newlines */

/*

 * Search the tables for _ddebug's which match the given `query' and

 * apply the `flags' and `mask' to them.  Returns number of matching

 * callsites, normally the same as number of changes.  If verbose,

 * logs the changes.  Takes ddebug_lock.

 search for matching ddebugs */

 match against the module name */

 match against the source filename */

 match against the function */

 match against the format */

 anchored search. match must be at beginning */

 match against the line number range */

/*

 * Split the buffer `buf' into space-separated words.

 * Handles simple " and ' quoting, i.e. without nested,

 * embedded or escaped \".  Return the number of words

 * or <0 on error.

 Skip leading whitespace */

 oh, it was trailing whitespace */

 token starts comment, skip rest of line */

 find `end' of word, whitespace separated or quoted */

 unclosed quote */

 `buf' is start of word, `end' is one past its end */

 ran out of words[] before bytes */

 terminate the word */

/*

 * Parse a single line number.  Note that the empty string ""

 * is treated as a special case and converted to zero, which

 * is later treated as a "don't care" value.

 range <first>-<last> */

 special case for last lineno not specified */

/*

 * Parse words[] as a ddebug query specification, which is a series

 * of (keyword, value) pairs chosen from these possibilities:

 *

 * func <function-name>

 * file <full-pathname>

 * file <base-filename>

 * module <module-name>

 * format <escaped-string-to-find-in-format>

 * line <lineno>

 * line <first-lineno>-<last-lineno> // where either may be empty

 *

 * Only 1 of each type is allowed.

 * Returns 0 on success, <0 on error.

 check we have an even number of words */

 support $modname.dyndbg=<multiple queries> */

 tail :$info is function or line-range */

 take as function name */

/*

 * Parse `str' as a flags specification, format [-+=][p]+.

 * Sets up *maskp and *flagsp to be used when changing the

 * flags fields of matched _ddebug's.  Returns 0 on success

 * or <0 on error.

 calculate final flags, mask based upon op */

 modifiers->flags already set */

 check flags 1st (last arg) so query is pairs of spec,val */

 actually go and implement the change */

/* handle multiple queries in query string, continue on error, return

   last error or number of matching callsites.  Module name is either

   in param (for boot arg) or perhaps in query string.

/**

 * dynamic_debug_exec_queries - select and change dynamic-debug prints

 * @query: query-string described in admin-guide/dynamic-debug-howto

 * @modname: string containing module name, usually &module.mod_name

 *

 * This uses the >/proc/dynamic_debug/control reader, allowing module

 * authors to modify their dynamic-debug callsites. The modname is

 * canonically struct module.mod_name, but can also be null or a

 * module-wildcard, for example: "drm*".

 writable copy of query */

/*

 * Install a noop handler to make dyndbg look like a normal kernel cli param.

 * This avoids warnings about dyndbg being an unknown cli param when supplied

 * by a user.

/*

 * File_ops->write method for <debugfs>/dynamic_debug/control.  Gathers the

 * command text from userspace, parses and executes it.

/*

 * Set the iterator to point to the first _ddebug object

 * and return a pointer to that first object.  Returns

 * NULL if there are no _ddebugs at all.

/*

 * Advance the iterator to point to the next _ddebug

 * object from the one the iterator currently points at,

 * and returns a pointer to the new _ddebug.  Returns

 * NULL if the iterator has seen all the _ddebugs.

 iterate to next table */

/*

 * Seq_ops start method.  Called at the start of every

 * read() call from userspace.  Takes the ddebug_lock and

 * seeks the seq_file's iterator to the given position.

/*

 * Seq_ops next method.  Called several times within a read()

 * call from userspace, with ddebug_lock held.  Walks to the

 * next _ddebug object with a special case for the header line.

/*

 * Seq_ops show method.  Called several times within a read()

 * call from userspace, with ddebug_lock held.  Formats the

 * current _ddebug as a single human-readable line, with a

 * special case for the header line.

/*

 * Seq_ops stop method.  Called at the end of each read()

 * call from userspace.  Drops ddebug_lock.

/*

 * Allocate a new ddebug_table for the given module

 * and add it to the global list.

	/*

	 * For built-in modules, name lives in .rodata and is

	 * immortal. For loaded modules, name points at the name[]

	 * member of struct module, which lives at least as long as

	 * this struct ddebug_table.

 helper for ddebug_dyndbg_(boot|module)_param_cb */

 needed only for ddebug_dyndbg_boot_param_cb */

 determined by caller */

 query failure shouldn't stop module load */

 handle both dyndbg and $module.dyndbg params at boot */

/*

 * modprobe foo finds foo.params in boot-args, strips "foo.", and

 * passes them to load_module().  This callback gets unknown params,

 * processes dyndbg params, rejects others.

/*

 * Called in response to a module being unloaded.  Removes

 * any ddebug_table's which point at the module.

 Create the control file in debugfs if it is enabled */

 Also create the control file in procfs */

	/* now that ddebug tables are loaded, process all boot args

	 * again to find and activate queries given in dyndbg params.

	 * While this has already been done for known boot params, it

	 * ignored the unknown ones (dyndbg in particular).  Reusing

	 * parse_args avoids ad-hoc parsing.  This will also attempt

	 * to activate queries for not-yet-loaded modules, which is

	 * slightly noisy if verbose, but harmless.

 Allow early initialization for boot messages via boot param */

 Debugfs setup must be done later */

 SPDX-License-Identifier: GPL-2.0-only

 Interior node: */

 Leaf: */

/*

 * Returns size (of data, in bytes) that a tree of a given depth holds:

 depth that's needed for a genradix that can address up to ULONG_MAX: */

/*

 * Returns pointer to the specified byte @offset within @radix, or NULL if not

 * allocated

	/*

	 * We're using pages (not slab allocations) directly for kernel data

	 * structures, so we need to explicitly inform kmemleak of them in order

	 * to avoid false positive memory leak reports.

/*

 * Returns pointer to the specified byte @offset within @radix, allocating it if

 * necessary - newly allocated slots are always zeroed out:

 Increase tree depth if necessary: */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 1999 ARM Limited

 * Copyright (C) 2000 Deep Blue Solutions Ltd

 * Copyright 2006-2007,2010 Freescale Semiconductor, Inc. All Rights Reserved.

 * Copyright 2008 Juergen Beisert, kernel@pengutronix.de

 * Copyright 2009 Ilya Yanok, Emcraft Systems Ltd, yanok@emcraft.com

 * Copyright (C) 2011 Wolfram Sang, Pengutronix e.K.

/*

 * Clear the bit and poll it cleared.  This is usually called with

 * a reset address and mask being either SFTRST(bit 31) or CLKGATE

 * (bit 30).

 nothing */;

 clear and poll SFTRST */

 clear CLKGATE */

 set SFTRST to reset the block */

 poll CLKGATE becoming set */

 nothing */;

 clear and poll SFTRST */

 clear and poll CLKGATE */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2016 Facebook

 * Copyright (C) 2013-2014 Jens Axboe

 If the map is full, a hint won't do us much good. */

 Only update the hint if we used it. */

/*

 * See if we have deferred clears that we can batch move

	/*

	 * First get a stable cleared mask, setting the old mask to 0.

	/*

	 * Now clear the masked bits in our free word

 don't wrap if starting from 0 */

			/*

			 * We started with an offset, and we didn't reset the

			 * offset to 0 in a failure case, so start from 0 to

			 * exhaust the map.

	/*

	 * Unless we're doing round robin tag allocation, just use the

	 * alloc_hint to find the right word index. No point in looping

	 * twice in find_next_zero_bit() for that case.

 Jump to next index. */

 Jump to next index. */

	/*

	 * For each batch, we wake up one queue. We need to make sure that our

	 * batch size is small enough that the full depth of the bitmap,

	 * potentially limited by a shallow depth, is enough to wake up all of

	 * the queues.

	 *

	 * Each full word of the bitmap has bits_per_word bits, and there might

	 * be a partial word. There are depth / bits_per_word full words and

	 * depth % bits_per_word bits left over. In bitwise arithmetic:

	 *

	 * bits_per_word = 1 << shift

	 * depth / bits_per_word = depth >> shift

	 * depth % bits_per_word = depth & ((1 << shift) - 1)

	 *

	 * Each word can be limited to sbq->min_shallow_depth bits.

		/*

		 * Pairs with the memory barrier in sbitmap_queue_wake_up()

		 * to ensure that the batch size is updated before the wait

		 * counts.

 Jump to next index. */

		/*

		 * Pairs with the memory barrier in sbitmap_queue_resize() to

		 * ensure that we see the batch size update before the wait

		 * count is reset.

		/*

		 * For concurrent callers of this, the one that failed the

		 * atomic_cmpxhcg() race should call this function again

		 * to wakeup a new batch on a different 'ws'.

 since we're clearing a batch, skip the deferred map */

	/*

	 * Once the clear bit is set, the bit may be allocated out.

	 *

	 * Orders READ/WRITE on the associated instance(such as request

	 * of blk_mq) by this bit for avoiding race with re-allocation,

	 * and its pair is the memory barrier implied in __sbitmap_get_word.

	 *

	 * One invariant is that the clear bit has to be zero when the bit

	 * is in use.

	/*

	 * Pairs with the memory barrier in set_current_state() to ensure the

	 * proper ordering of clear_bit_unlock()/waitqueue_active() in the waker

	 * and test_and_set_bit_lock()/prepare_to_wait()/finish_wait() in the

	 * waiter. See the comment on waitqueue_active().

	/*

	 * Pairs with the memory barrier in set_current_state() like in

	 * sbitmap_queue_wake_up().

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Test cases for bitmap API.

 Fibonacci sequence */

 exp3_0_1 = (exp2[0] & ~exp2_to_exp3_mask) | (exp2[1] & exp2_to_exp3_mask) */

 exp3_1_0 = (exp2[1] & ~exp2_to_exp3_mask) | (exp2[0] & exp2_to_exp3_mask) */

 Known way to set all bits */

 single-word bitmaps */

 cross boundaries operations */

 Zeroing entire area */

 Known way to clear all bits */

 single-word bitmaps */

 cross boundaries operations */

 Zeroing entire area */

 single-word bitmaps */

 multi-word bitmaps */

	/* the following tests assume a 32- or 64-bit arch (even 128b

	 * if we care)

 ... but 0-padded til word length */

 ... but aligned on word length */

 1 bit set */

 non-edge 1 bit set */

 zero bits set */

 3 bits set across 4-bit boundary */

 Repeated clump */

 4 bits set */

 all bits set */

 non-adjacent 2 bits set */

 set bitmap to test case */

 0x01 */

 0x02 */

 0x28 */

 0x28 */

 0x0F */

 0xFF */

 0x05 - part 1 */

 0x05 - part 2 */

 Partial overlap */

 more than 4KB */

 test by non-zero offset */

/*

 * proc sysctl test driver

 *

 * Copyright (C) 2017 Luis R. Rodriguez <mcgrof@kernel.org>

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of the GNU General Public License as published by the Free

 * Software Foundation; either version 2 of the License, or at your option any

 * later version; or, when distributed separately from the Linux kernel or

 * when incorporated into other software packages, subject to the following

 * license:

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of copyleft-next (version 0.3.1 or later) as published

 * at http://copyleft-next.org/.

/*

 * This module provides an interface to the proc sysctl interfaces.  This

 * driver requires CONFIG_PROC_SYSCTL. It will not normally be loaded by the

 * system unless explicitly requested by name. You can also build this driver

 * into your kernel.

 These are all under /proc/sys/debug/test_sysctl/ */

/* Test cases for siphash.c

 *

 * Copyright (C) 2016 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.

 *

 * This file is provided under a dual BSD/GPLv2 license.

 *

 * SipHash: a fast short-input PRF

 * https://131002.net/siphash/

 *

 * This implementation is specifically for SipHash2-4 for a secure PRF

 * and HalfSipHash1-3/SipHash1-3 for an insecure PRF only suitable for

 * hashtables.

/* Test vectors taken from reference source available at:

 *     https://github.com/veorq/SipHash

 SPDX-License-Identifier: GPL-2.0-only

/*

 * T10 Data Integrity Field CRC16 calculation

 *

 * Copyright (c) 2007 Oracle Corporation.  All rights reserved.

 * Written by Martin K. Petersen <martin.petersen@oracle.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * lib/textsearch.c	Generic text search interface

 *

 * Authors:	Thomas Graf <tgraf@suug.ch>

 * 		Pablo Neira Ayuso <pablo@netfilter.org>

 *

 * ==========================================================================

/**

 * DOC: ts_intro

 * INTRODUCTION

 *

 *   The textsearch infrastructure provides text searching facilities for

 *   both linear and non-linear data. Individual search algorithms are

 *   implemented in modules and chosen by the user.

 *

 * ARCHITECTURE

 *

 * .. code-block:: none

 *

 *     User

 *     +----------------+

 *     |        finish()|<--------------(6)-----------------+

 *     |get_next_block()|<--------------(5)---------------+ |

 *     |                |                     Algorithm   | |

 *     |                |                    +------------------------------+

 *     |                |                    |  init()   find()   destroy() |

 *     |                |                    +------------------------------+

 *     |                |       Core API           ^       ^          ^

 *     |                |      +---------------+  (2)     (4)        (8)

 *     |             (1)|----->| prepare()     |---+       |          |

 *     |             (3)|----->| find()/next() |-----------+          |

 *     |             (7)|----->| destroy()     |----------------------+

 *     +----------------+      +---------------+

 *

 *   (1) User configures a search by calling textsearch_prepare() specifying

 *       the search parameters such as the pattern and algorithm name.

 *   (2) Core requests the algorithm to allocate and initialize a search

 *       configuration according to the specified parameters.

 *   (3) User starts the search(es) by calling textsearch_find() or

 *       textsearch_next() to fetch subsequent occurrences. A state variable

 *       is provided to the algorithm to store persistent variables.

 *   (4) Core eventually resets the search offset and forwards the find()

 *       request to the algorithm.

 *   (5) Algorithm calls get_next_block() provided by the user continuously

 *       to fetch the data to be searched in block by block.

 *   (6) Algorithm invokes finish() after the last call to get_next_block

 *       to clean up any leftovers from get_next_block. (Optional)

 *   (7) User destroys the configuration by calling textsearch_destroy().

 *   (8) Core notifies the algorithm to destroy algorithm specific

 *       allocations. (Optional)

 *

 * USAGE

 *

 *   Before a search can be performed, a configuration must be created

 *   by calling textsearch_prepare() specifying the searching algorithm,

 *   the pattern to look for and flags. As a flag, you can set TS_IGNORECASE

 *   to perform case insensitive matching. But it might slow down

 *   performance of algorithm, so you should use it at own your risk.

 *   The returned configuration may then be used for an arbitrary

 *   amount of times and even in parallel as long as a separate struct

 *   ts_state variable is provided to every instance.

 *

 *   The actual search is performed by either calling

 *   textsearch_find_continuous() for linear data or by providing

 *   an own get_next_block() implementation and

 *   calling textsearch_find(). Both functions return

 *   the position of the first occurrence of the pattern or UINT_MAX if

 *   no match was found. Subsequent occurrences can be found by calling

 *   textsearch_next() regardless of the linearity of the data.

 *

 *   Once you're done using a configuration it must be given back via

 *   textsearch_destroy.

 *

 * EXAMPLE::

 *

 *   int pos;

 *   struct ts_config *conf;

 *   struct ts_state state;

 *   const char *pattern = "chicken";

 *   const char *example = "We dance the funky chicken";

 *

 *   conf = textsearch_prepare("kmp", pattern, strlen(pattern),

 *                             GFP_KERNEL, TS_AUTOLOAD);

 *   if (IS_ERR(conf)) {

 *       err = PTR_ERR(conf);

 *       goto errout;

 *   }

 *

 *   pos = textsearch_find_continuous(conf, &state, example, strlen(example));

 *   if (pos != UINT_MAX)

 *       panic("Oh my god, dancing chickens at %d\n", pos);

 *

 *   textsearch_destroy(conf);

 ========================================================================== */

/**

 * textsearch_register - register a textsearch module

 * @ops: operations lookup table

 *

 * This function must be called by textsearch modules to announce

 * their presence. The specified &@ops must have %name set to a

 * unique identifier and the callbacks find(), init(), get_pattern(),

 * and get_pattern_len() must be implemented.

 *

 * Returns 0 or -EEXISTS if another module has already registered

 * with same name.

/**

 * textsearch_unregister - unregister a textsearch module

 * @ops: operations lookup table

 *

 * This function must be called by textsearch modules to announce

 * their disappearance for examples when the module gets unloaded.

 * The &ops parameter must be the same as the one during the

 * registration.

 *

 * Returns 0 on success or -ENOENT if no matching textsearch

 * registration was found.

/**

 * textsearch_find_continuous - search a pattern in continuous/linear data

 * @conf: search configuration

 * @state: search state

 * @data: data to search in

 * @len: length of data

 *

 * A simplified version of textsearch_find() for continuous/linear data.

 * Call textsearch_next() to retrieve subsequent matches.

 *

 * Returns the position of first occurrence of the pattern or

 * %UINT_MAX if no occurrence was found.

/**

 * textsearch_prepare - Prepare a search

 * @algo: name of search algorithm

 * @pattern: pattern data

 * @len: length of pattern

 * @gfp_mask: allocation mask

 * @flags: search flags

 *

 * Looks up the search algorithm module and creates a new textsearch

 * configuration for the specified pattern.

 *

 * Note: The format of the pattern may not be compatible between

 *       the various search algorithms.

 *

 * Returns a new textsearch configuration according to the specified

 * parameters or a ERR_PTR(). If a zero length pattern is passed, this

 * function returns EINVAL.

	/*

	 * Why not always autoload you may ask. Some users are

	 * in a situation where requesting a module may deadlock,

	 * especially when the module is located on a NFS mount.

/**

 * textsearch_destroy - destroy a search configuration

 * @conf: search configuration

 *

 * Releases all references of the configuration and frees

 * up the memory.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * klist.c - Routines for manipulating klists.

 *

 * Copyright (C) 2005 Patrick Mochel

 *

 * This klist interface provides a couple of structures that wrap around

 * struct list_head to provide explicit list "head" (struct klist) and list

 * "node" (struct klist_node) objects. For struct klist, a spinlock is

 * included that protects access to the actual list itself. struct

 * klist_node provides a pointer to the klist that owns it and a kref

 * reference count that indicates the number of current users of that node

 * in the list.

 *

 * The entire point is to provide an interface for iterating over a list

 * that is safe and allows for modification of the list during the

 * iteration (e.g. insertion and removal), including modification of the

 * current node on the list.

 *

 * It works using a 3rd object type - struct klist_iter - that is declared

 * and initialized before an iteration. klist_next() is used to acquire the

 * next element in the list. It returns NULL if there are no more items.

 * Internally, that routine takes the klist's lock, decrements the

 * reference count of the previous klist_node and increments the count of

 * the next klist_node. It then drops the lock and returns.

 *

 * There are primitives for adding and removing nodes to/from a klist.

 * When deleting, klist_del() will simply decrement the reference count.

 * Only when the count goes to 0 is the node removed from the list.

 * klist_remove() will try to delete the node from the list and block until

 * it is actually removed. This is useful for objects (like devices) that

 * have been removed from the system and must be freed (but must wait until

 * all accessors have finished).

/*

 * Use the lowest bit of n_klist to mark deleted nodes and exclude

 * dead ones from iteration.

 no knode deserves to start its life dead */

 and no knode should die twice ever either, see we're very humane */

/**

 * klist_init - Initialize a klist structure.

 * @k: The klist we're initializing.

 * @get: The get function for the embedding object (NULL if none)

 * @put: The put function for the embedding object (NULL if none)

 *

 * Initialises the klist structure.  If the klist_node structures are

 * going to be embedded in refcounted objects (necessary for safe

 * deletion) then the get/put arguments are used to initialise

 * functions that take and release references on the embedding

 * objects.

/**

 * klist_add_head - Initialize a klist_node and add it to front.

 * @n: node we're adding.

 * @k: klist it's going on.

/**

 * klist_add_tail - Initialize a klist_node and add it to back.

 * @n: node we're adding.

 * @k: klist it's going on.

/**

 * klist_add_behind - Init a klist_node and add it after an existing node

 * @n: node we're adding.

 * @pos: node to put @n after

/**

 * klist_add_before - Init a klist_node and add it before an existing node

 * @n: node we're adding.

 * @pos: node to put @n after

/**

 * klist_del - Decrement the reference count of node and try to remove.

 * @n: node we're deleting.

/**

 * klist_remove - Decrement the refcount of node and wait for it to go away.

 * @n: node we're removing.

/**

 * klist_node_attached - Say whether a node is bound to a list or not.

 * @n: Node that we're testing.

/**

 * klist_iter_init_node - Initialize a klist_iter structure.

 * @k: klist we're iterating.

 * @i: klist_iter we're filling.

 * @n: node to start with.

 *

 * Similar to klist_iter_init(), but starts the action off with @n,

 * instead of with the list head.

/**

 * klist_iter_init - Iniitalize a klist_iter structure.

 * @k: klist we're iterating.

 * @i: klist_iter structure we're filling.

 *

 * Similar to klist_iter_init_node(), but start with the list head.

/**

 * klist_iter_exit - Finish a list iteration.

 * @i: Iterator structure.

 *

 * Must be called when done iterating over list, as it decrements the

 * refcount of the current node. Necessary in case iteration exited before

 * the end of the list was reached, and always good form.

/**

 * klist_prev - Ante up prev node in list.

 * @i: Iterator structure.

 *

 * First grab list lock. Decrement the reference count of the previous

 * node, if there was one. Grab the prev node, increment its reference

 * count, drop the lock, and return that prev node.

/**

 * klist_next - Ante up next node in list.

 * @i: Iterator structure.

 *

 * First grab list lock. Decrement the reference count of the previous

 * node, if there was one. Grab the next node, increment its reference

 * count, drop the lock, and return that next node.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  LZO1X Decompressor from LZO

 *

 *  Copyright (C) 1996-2012 Markus F.X.J. Oberhumer <markus@oberhumer.com>

 *

 *  The full LZO package can be found at:

 *  http://www.oberhumer.com/opensource/lzo/

 *

 *  Changed for Linux kernel use by:

 *  Nitin Gupta <nitingupta910@gmail.com>

 *  Richard Purdie <rpurdie@openedhand.com>

/* This MAX_255_COUNT is the maximum number of times we can add 255 to a base

 * count without overflowing an integer. The multiply will overflow when

 * multiplying 255 by more than MAXINT/255. The sum will overflow earlier

 * depending on the base count. Since the base count is taken from a u8

 * and a few bits, it is safe to assume that it will always be lower than

 * or equal to 2*255, thus we can always prevent any overflow by accepting

 * two less 255 steps. See Documentation/staging/lzo.rst for more information.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  LZO1X Compressor from LZO

 *

 *  Copyright (C) 1996-2012 Markus F.X.J. Oberhumer <markus@oberhumer.com>

 *

 *  The full LZO package can be found at:

 *  http://www.oberhumer.com/opensource/lzo/

 *

 *  Changed for Linux kernel use by:

 *  Nitin Gupta <nitingupta910@gmail.com>

 *  Richard Purdie <rpurdie@openedhand.com>

 Under lzo-rle, block copies

 for 261 <= length <= 264 and

 (distance & 0x80f3) == 0x80f3

 can result in ambiguous

 output. Adjust length

 to 260 to prevent ambiguity.

 LZO v0 will never write 17 as first byte (except for zero-length

 input), so this is used to version the bitstream

/*

 * LZ4 HC - High Compression Mode of LZ4

 * Copyright (C) 2011-2015, Yann Collet.

 *

 * BSD 2 - Clause License (http://www.opensource.org/licenses/bsd - license.php)

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions are

 * met:

 *	* Redistributions of source code must retain the above copyright

 *	  notice, this list of conditions and the following disclaimer.

 *	* Redistributions in binary form must reproduce the above

 * copyright notice, this list of conditions and the following disclaimer

 * in the documentation and/or other materials provided with the

 * distribution.

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT

 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,

 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT

 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,

 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY

 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 * You can contact the author at :

 *	- LZ4 homepage : http://www.lz4.org

 *	- LZ4 source repository : https://github.com/lz4/lz4

 *

 *	Changed for kernel usage by:

 *	Sven Schmidt <4sschmid@informatik.uni-hamburg.de>

/*-************************************

 *	Dependencies

 memset */

/* *************************************

 *	Local Constants and types

 faster */

/**************************************

 *	HC Compression

 Update chains up to ip (excluded) */

 Index table will be updated */

 HC4 match finder */

 virtual matchpos */

 First Match */

 Encode Literal length */

 Check output limit */

 Copy Literals */

 Encode Offset */

 Encode MatchLength */

 Check output limit */

 Prepare next loop */

 init */

 Main Loop */

 saved, in case we would skip too much */

 No better match */

 empirical */

 Here, start0 == ip */

 First Match too small : removed */

		/*

		* Currently we have :

		* ml2 > ml1, and

		* ip1 + 3 <= ip2 (usually < ip1 + ml1)

		/*

		 * Now, we have start2 = ip + new_ml,

		 * with new_ml = min(ml, OPTIMAL_ML = 18)

 No better match : 2 sequences to encode */

 ip & ref are known; Now for ml */

 Now, encode 2 sequences */

 Not enough space for match 2 : remove it */

				/* can write Seq1 immediately

				 * ==> Seq2 is removed,

				 * so Seq3 becomes Seq1

		/*

		* OK, now we have 3 ascending matches;

		* let's write at least the first one

		* ip & ref are known; Now for ml

 Encode Last Literals */

 Check output limit */

 End */

		/* Error : state is not aligned

		 * for pointers (32 or 64 bits)

/**************************************

 *	Streaming Functions

 compression */

 Referencing remaining dictionary content */

	/*

	 * Only one memory segment for extDict,

	 * so any previous extDict is lost at this stage

 match referencing will resume from there */

 auto - init if forgotten */

 Check overflow */

 Check if blocks follow each other */

 Check overlapping input/dictionary space */

 dictionary saving */

/*

 * LZ4 - Fast LZ compression algorithm

 * Copyright (C) 2011 - 2016, Yann Collet.

 * BSD 2 - Clause License (http://www.opensource.org/licenses/bsd - license.php)

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions are

 * met:

 *	* Redistributions of source code must retain the above copyright

 *	  notice, this list of conditions and the following disclaimer.

 *	* Redistributions in binary form must reproduce the above

 * copyright notice, this list of conditions and the following disclaimer

 * in the documentation and/or other materials provided with the

 * distribution.

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT

 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,

 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT

 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,

 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY

 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 * You can contact the author at :

 *	- LZ4 homepage : http://www.lz4.org

 *	- LZ4 source repository : https://github.com/lz4/lz4

 *

 *	Changed for kernel usage by:

 *	Sven Schmidt <4sschmid@informatik.uni-hamburg.de>

/*-************************************

 *	Dependencies

/*-*****************************

 *	Decompression functions

 disabled */

/*

 * LZ4_decompress_generic() :

 * This generic decompression function covers all use cases.

 * It shall be instantiated several times, using different sets of directives.

 * Note that it is important for performance that this function really get inlined,

 * in order to remove useless branches during compilation optimization.

		/*

		 * If endOnInput == endOnInputSize,

		 * this value is `dstCapacity`

 endOnOutputSize, endOnInputSize */

 full, partial */

 noDict, withPrefix64k, usingExtDict */

 always <= dst, == dst when no prefix */

 only if dict == usingExtDict */

 note : = 0 if noDict */

 Set up the "end" pointers for the shortcut. */

maxLL*/ - 2 
maxLL*/ - 18 
 Special cases */

 Empty output buffer */

 Main Loop : decode sequences */

 get literal length */

 ip < iend before the increment */

		/*

		 * A two-stage shortcut for the most common case:

		 * 1) If the literal length is 0..14, and there is enough

		 * space, enter the shortcut and copy 16 bytes on behalf

		 * of the literals (in the fast mode, only 8 bytes can be

		 * safely copied this way).

		 * 2) Further if the match length is 4..18, copy 18 bytes

		 * in a similar manner; but we ensure that there's enough

		 * space in the output for those 18 bytes earlier, upon

		 * entering the shortcut (in other words, there is a

		 * combined check for both stages).

		 *

		 * The & in the likely() below is intentionally not && so that

		 * some compilers can produce better parallelized runtime code

		   /*

		    * strictly "less than" on input, to re-enter

		    * the loop with at least one byte

 Copy the literals */

			/*

			 * The second stage:

			 * prepare for match copying, decode full info.

			 * If it doesn't work out, the info won't be wasted.

 match length */

 check overflow */

 Do not deal with overlapping matches. */

 Copy the match. */

 Both stages worked, load the next token. */

			/*

			 * The second stage didn't work out, but the info

			 * is ready. Propel it right to the point of match

			 * copying.

 decode literal length */

 overflow detection */

 overflow detection */

 overflow detection */

 copy literals */

					/*

					 * Partial decoding :

					 * stop in the middle of literal segment

					/*

					 * Error :

					 * read attempt beyond

					 * end of input buffer

					/*

					 * Error :

					 * block decoding must

					 * stop exactly there

					/*

					 * Error :

					 * input must be consumed

			/*

			 * supports overlapping memory regions; only matters

			 * for in-place decompression scenarios

 Necessarily EOF, due to parsing restrictions */

 may overwrite up to WILDCOPYLENGTH beyond cpy */

 get offset */

 get matchlength */

 Error : offset outside buffers */

 costs ~1%; silence an msan warning when offset == 0 */

		/*

		 * note : when partialDecoding, there is no guarantee that

		 * at least 4 bytes remain available in output buffer

 overflow detection */

 match starting within external dictionary */

 doesn't respect parsing restriction */

				/*

				 * match fits entirely within external

				 * dictionary : just copy

				/*

				 * match stretches into both external

				 * dictionary and current block

 overlap copy */

 copy match within block */

		/*

		 * partialDecoding :

		 * may not respect endBlock parsing restrictions

 overlap copy */

				/*

				 * Error : last LASTLITERALS bytes

				 * must be literals (uncompressed)

 wildcopy correction */

 end of decoding */

 Nb of output bytes decoded */

 Nb of input bytes read */

 Overflow error detected */

 ===== Instantiate a few more decoding cases, used more than once. ===== */

/*

 * The "double dictionary" mode, for use with e.g. ring buffers: the first part

 * of the dictionary is passed as prefix, and the second via dictStart + dictSize.

 * These routines are used only once, in LZ4_decompress_*_continue().

 ===== streaming decompression functions ===== */

/*

 * *_continue() :

 * These decoding functions allow decompression of multiple blocks

 * in "streaming" mode.

 * Previously decoded blocks must still be available at the memory

 * position where they were decoded.

 * If it's not possible, save the relevant part of

 * decoded data into a safe buffer,

 * and indicate where it stands using LZ4_setStreamDecode()

 The first call, no dictionary yet. */

 They're rolling the current segment. */

		/*

		 * The buffer wraps around, or they're

		 * switching to another buffer.

/*

 * LZ4 - Fast LZ compression algorithm

 * Copyright (C) 2011 - 2016, Yann Collet.

 * BSD 2 - Clause License (http://www.opensource.org/licenses/bsd - license.php)

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions are

 * met:

 *	* Redistributions of source code must retain the above copyright

 *	  notice, this list of conditions and the following disclaimer.

 *	* Redistributions in binary form must reproduce the above

 * copyright notice, this list of conditions and the following disclaimer

 * in the documentation and/or other materials provided with the

 * distribution.

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT

 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,

 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT

 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,

 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY

 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 * You can contact the author at :

 *	- LZ4 homepage : http://www.lz4.org

 *	- LZ4 source repository : https://github.com/lz4/lz4

 *

 *	Changed for kernel usage by:

 *	Sven Schmidt <4sschmid@informatik.uni-hamburg.de>

/*-************************************

 *	Dependencies

/*-******************************

 *	Compression functions

 default, to ensure a return */

/*

 * LZ4_compress_generic() :

 * inlined, to ensure branches are decided at compilation time

 Init conditions */

 Unsupported inputSize, too large (or negative) */

 Size too large (not within 64K limit) */

 Input too small, no compression (all literals) */

 First Byte */

 Main Loop */

 Find a match */

 Catch up */

 Encode Literals */

 Check output buffer overflow */

 Copy Literals */

 Encode Offset */

 Encode MatchLength */

 Check output buffer overflow */

 Test end of chunk */

 Fill table */

 Test next position */

 Prepare next loop */

 Encode Last Literals */

 Check output buffer overflow */

 End */

/*-******************************

 *	*_destSize() variant

 offset */

 because 8 + MINMATCH == MFLIMIT */ - 1 
 token */);

 token */;

 Init conditions */

 Impossible to store anything */

 Unsupported input size, too large (or negative) */

 Size too large (not within 64K limit) */

 Input too small, no compression (all literals) */

 First Byte */

 Main Loop */

 Find a match */

 Catch up */

 Encode Literal length */

 Not enough space for a last match */

 Copy Literals */

 Encode Offset */

 Encode MatchLength */

 Match description too long : reduce it */

 Test end of block */

 Fill table */

 Test next position */

 Prepare next loop */

 Encode Last Literals */

 token */

 litLength */

 literals */ > oend) {

 adapt lastRunSize to fill 'dst' */

 End */

 compression success is guaranteed */

/*-******************************

 *	Streaming functions

 Uninitialized structure, or reuse overflow */

 address space overflow */

 rescale hash table */

 useless to define a dictionary > 64 * KB */

 Uninitialized structure detected */

 Check overlapping input/dictionary space */

 prefix mode : source data follows dictionary */

 external dictionary mode */

 SPDX-License-Identifier: Zlib

/*

 * Compress.

 Check for kernel dfltcc command line parameter */

 Unsupported compression settings */

 Unsupported hardware */

        /* The remaining data is located in pending_out[0:pending]. If someone

         * calls put_byte() - this might happen in deflate() - the byte will be

         * placed into pending_buf[pending], which is incorrect. Move the

         * remaining data to the beginning of pending_buf so that put_byte() is

         * usable again.

    /* Trailing empty block. Switch to software, except when Continuation Flag

     * is set, which means that DFLTCC has buffered some output in the

     * parameter block and needs to be called again in order to flush it.

            /* A block is still open, and the hardware does not support closing

             * blocks without adding data. Thus, close it manually.

    /* There is an open non-BFINAL block, we are not going to close it just

     * yet, we have compressed more than DFLTCC_BLOCK_SIZE bytes and we see

     * more than DFLTCC_DHT_MIN_SAMPLE_SIZE bytes. Open a new block with a new

     * DHT in order to adapt to a possibly changed input data distribution.

            /* We need to flush the DFLTCC buffer before writing the

             * End-of-block Symbol. Mask the input data and proceed as usual.

            /* DFLTCC buffer is empty, so we can manually write the

             * End-of-block Symbol right away.

    /* The caller gave us too much data. Pass only one block worth of

     * uncompressed data to DFLTCC and mask the rest, so that on the next

     * iteration we start a new block.

    /* When we have an open non-BFINAL deflate block and caller indicates that

     * the stream is ending, we need to close an open deflate block and open a

     * BFINAL one.

 Translate stream to parameter block */

        /* We need to close a block. Always do this in software - when there is

        /* We are about to open a BFINAL block, set Block Header Final bit

         * until the stream ends.

    /* DFLTCC-CMPR will write to next_out, so make sure that buffers with

     * higher precedence are empty.

 Honor history */

 When opening a block, choose a Huffman-Table Type */

 Deflate */

            /* We are about to call DFLTCC with a small input buffer, which is

             * inefficient. Since there is masked data, there will be at least

             * one more DFLTCC call, so skip the current one and make the next

             * one handle more data.

 Translate parameter block to stream */

 Avoid accessing next_out */

 Unmask the input data */

 If we encounter an error, it means there is a bug in DFLTCC call */

    /* Update Block-Continuation Flag. It will be used to check whether to call

     * GDHT the next time.

 Make the current deflate() call also close the stream */

 Clear history */

 deflate() must use all input or all output */

 SPDX-License-Identifier: Zlib

 dfltcc.c - SystemZ DEFLATE CONVERSION CALL support. */

 Successful completion */

 Ignore for pre-boot decompressor */

 Initialize available functions */

 Initialize parameter block */

 Initialize tuning parameters */

 SPDX-License-Identifier: Zlib

/*

 * Expand.

 Check for kernel dfltcc command line parameter */

 Unsupported compression settings */

 Unsupported hardware */

        /* DFLTCC has already decompressed some data. Since there is not

         * enough information to resume decompression in software, the call

         * must fail.

 DFLTCC was not used yet - decompress in software */

 DFLTCC does not support stopping on block boundaries */

 Translate stream to parameter block */

 Software and hardware history formats match */

 Honor history for the first block */

 Inflate */

 Translate parameter block to stream */

 Report an error if stream is corrupted */

 Break if operands are exhausted, otherwise continue looping */

 SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB

/*

 * Copyright (c) 2019, Mellanox Technologies inc.  All rights reserved.

 DIM_GOING_LEFT */

 u32 holds up to 71 minutes, should be enough */

 SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB

/*

 * Copyright (c) 2019, Mellanox Technologies inc.  All rights reserved.

 first stat */

 SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB

/*

 * Copyright (c) 2018, Mellanox Technologies inc.  All rights reserved.

/*

 * Net DIM profiles:

 *        There are different set of profiles for each CQ period mode.

 *        There are different set of profiles for RX/TX CQs.

 *        Each profile size must be of NET_DIM_PARAMS_NUM_PROFILES

 SPDX-License-Identifier: GPL-2.0

/*

 * C++ stream style string builder used in KUnit for building messages.

 *

 * Copyright (C) 2019, Google LLC.

 * Author: Brendan Higgins <brendanhiggins@google.com>

 Make a copy because `vsnprintf` could change it */

 Need space for null byte. */

 +1 for null byte. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Assertion and expectation serialization API.

 *

 * Copyright (C) 2019, Google LLC.

 * Author: Brendan Higgins <brendanhiggins@google.com>

 Checks if `text` is a literal representing `value`, e.g. "5" and 5 */

/* Checks if KUNIT_EXPECT_STREQ() args were string literals.

 * Note: `text` will have ""s where as `value` will not.

 SPDX-License-Identifier: GPL-2.0

/*

 * These symbols point to the .kunit_test_suites section and are defined in

 * include/asm-generic/vmlinux.lds.h, and consequently must be extern.

 glob_match() needs NULL terminated strings, so we need a copy of filter_glob_param. */

 Split "suite_glob.test_glob" into two. Assumes filter_glob is not empty. */

 Create a copy of suite with only tests that match test_glob. */

 Use memcpy to workaround copy->name being const. */

 won't be able to run anything, return an empty set */

 Hack: print a tap header so kunit.py can find the start of KUnit output. */

 a copy was made of each array */

 IS_BUILTIN(CONFIG_KUNIT) */

 SPDX-License-Identifier: GPL-2.0

/*

 * KUnit test for core test infrastructure.

 *

 * Copyright (C) 2019, Google LLC.

 * Author: Brendan Higgins <brendanhiggins@google.com>

/*

 * Context for testing test managed resources

 * is_resource_initialized is used to test arbitrary resources

/*

 * Note: tests below use kunit_alloc_and_get_resource(), so as a consequence

 * they have a reference to the associated resource that they must release

 * via kunit_put_resource().  In normal operation, users will only

 * have to do this for cases where they use kunit_find_resource(), and the

 * kunit_alloc_resource() function will be used (which does not take a

 * resource reference).

/*

 * TODO(brendanhiggins@google.com): replace the arrays that keep track of the

 * order of allocation and freeing with strict mocks using the IN_SEQUENCE macro

 * to assert allocation and freeing order when the feature becomes available.

 fake_resource_1 allocates a fake_resource_2 in its init. */

	/*

	 * Since fake_resource_2_init calls KUNIT_RESOURCE_TEST_MARK_ORDER

	 * before returning to fake_resource_1_init, it should be the first to

	 * put its key in the allocate_order array.

	/*

	 * Because fake_resource_2 finishes allocation before fake_resource_1,

	 * fake_resource_1 should be freed first since it could depend on

	 * fake_resource_2.

 Before: Should be SUCCESS with no comment. */

 Mark the test as skipped. */

 After: Should be SKIPPED with our comment. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020, Oracle and/or its affiliates.

 *    Author: Alan Maguire <alan.maguire@oracle.com>

/*

 * Create a debugfs representation of test suites:

 *

 * Path						Semantics

 * /sys/kernel/debug/kunit/<testsuite>/results	Show results of last run for

 *						testsuite

 *

/*

 * /sys/kernel/debug/kunit/<testsuite>/results shows all results for testsuite.

 Allocate logs before creating debugfs representation. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Base unit test (KUnit) API.

 *

 * Copyright (C) 2019, Google LLC.

 * Author: Brendan Higgins <brendanhiggins@google.com>

/*

 * Fail the current test and print an error message to the log.

 kunit_err() only accepts literals, so evaluate the args first. */

/*

 * KUnit statistic mode:

 * 0 - disabled

 * 1 - only when there is more than one subtest

 * 2 - enabled

/*

 * Append formatted message to log, size of which is limited to

 * KUNIT_LOG_SIZE bytes (including null terminating byte).

	/*

	 * We do not log the test suite results as doing so would

	 * mean debugfs display would consist of the test suite

	 * description and status prior to individual test results.

	 * Hence directly printk the suite status, and we will

	 * separately seq_printf() the suite status for the debugfs

	 * representation.

 Does not return. */

	/*

	 * Throw could not abort from test.

	 *

	 * XXX: we should never reach this line! As kunit_try_catch_throw is

	 * marked __noreturn.

/*

 * Initializes and runs test case. Does not clean up or do post validations.

/*

 * Performs post validations and cleanup after a test case was run.

 * XXX: Should ONLY BE CALLED AFTER kunit_run_case_internal!

	/*

	 * kunit_run_case_internal may encounter a fatal error; if it does,

	 * abort will be called, this thread will exit, and finally the parent

	 * thread will resume control and handle any necessary clean up.

 This line may never be reached. */

		/*

		 * Test case could not finish, we have no idea what state it is

		 * in, so don't do clean up.

		/*

		 * Unknown internal error occurred preventing test case from

		 * running, so there is nothing to clean up.

	/*

	 * Test case was run, but aborted. It is the test case's business as to

	 * whether it failed or not, we just need to clean up.

/*

 * Performs all logic to run a test case. It also catches most errors that

 * occur in a test case and reports them as failures.

 Propagate the parameter result to the test case. */

 Get initial param. */

 Get next param. */

/*

 * Used for static resources and when a kunit_resource * has been created by

 * kunit_alloc_resource().  When an init function is supplied, @data is passed

 * into the init function; otherwise, we simply set the resource data field to

 * the data value passed in.

 refcount for list is established by kref_init() */

		/*

		 * bump refcount for get; kunit_resource_put() should be called

		 * when done.

 We have a reference also via _find(); drop it. */

	/*

	 * Removing the resource from the list of resources drops the

	 * reference count to 1; the final put will trigger the free.

	/*

	 * test->resources is a stack - each allocation must be freed in the

	 * reverse order from which it was added since one resource may depend

	 * on another for its entire lifetime.

	 * Also, we cannot use the normal list_for_each constructs, even the

	 * safe ones because *arbitrary* nodes may be deleted when

	 * kunit_resource_free is called; the list_for_each_safe variants only

	 * protect against the current node being deleted, not the next.

		/*

		 * Need to unlock here as a resource may remove another

		 * resource, and this can't happen if the test->lock

		 * is held.

 SPDX-License-Identifier: GPL-2.0

/*

 * Example KUnit test to show how to use KUnit.

 *

 * Copyright (C) 2019, Google LLC.

 * Author: Brendan Higgins <brendanhiggins@google.com>

/*

 * This is the most fundamental element of KUnit, the test case. A test case

 * makes a set EXPECTATIONs and ASSERTIONs about the behavior of some code; if

 * any expectations or assertions are not met, the test fails; otherwise, the

 * test passes.

 *

 * In KUnit, a test case is just a function with the signature

 * `void (*)(struct kunit *)`. `struct kunit` is a context object that stores

 * information about the current test.

	/*

	 * This is an EXPECTATION; it is how KUnit tests things. When you want

	 * to test a piece of code, you set some expectations about what the

	 * code should do. KUnit then runs the test and verifies that the code's

	 * behavior matched what was expected.

/*

 * This is run once before each test case, see the comment on

 * example_test_suite for more information.

/*

 * This test should always be skipped.

 This line should run */

 Skip (and abort) the test */

 This line should not execute */

/*

 * This test should always be marked skipped.

 This line should run */

 Skip (but do not abort) the test */

 This line should run */

/*

 * Here we make a list of all the test cases we want to add to the test suite

 * below.

	/*

	 * This is a helper to create a test case object from a test case

	 * function; its exact function is not important to understand how to

	 * use KUnit, just know that this is how you associate test cases with a

	 * test suite.

/*

 * This defines a suite or grouping of tests.

 *

 * Test cases are defined as belonging to the suite by adding them to

 * `kunit_cases`.

 *

 * Often it is desirable to run some function which will set up things which

 * will be used by every test; this is accomplished with an `init` function

 * which runs before each test case is invoked. Similarly, an `exit` function

 * may be specified which runs after every test case and can be used to for

 * cleanup. For clarity, running tests in a test suite would behave as follows:

 *

 * suite.init(test);

 * suite.test_case[0](test);

 * suite.exit(test);

 * suite.init(test);

 * suite.test_case[1](test);

 * suite.exit(test);

 * ...;

/*

 * This registers the above test suite telling KUnit that this is a suite of

 * tests that need to be run.

 SPDX-License-Identifier: GPL-2.0

/*

 * KUnit test for struct string_stream.

 *

 * Copyright (C) 2019, Google LLC.

 * Author: Brendan Higgins <brendanhiggins@google.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * KUnit test for the KUnit executor.

 *

 * Copyright (C) 2021, Google LLC.

 * Author: Daniel Latypov <dlatypov@google.com>

 .run_case is not important, just needs to be non-NULL */

 Want: suite1, suite2, NULL -> suite2, NULL */

 Validate we just have suite2 */

 Want: suite1, suite2, NULL -> suite2 (just test1), NULL */

 Validate we just have suite2 */

 Now validate we just have test2 */

 just in case */

 Suites per-file are stored as a NULL terminated array */

 Match the memory layout of suite_set */

 Emulate two files, each having one suite */

 Filter out suite1 */

 let us use ASSERTs without leaking */

 Test helpers */

/* Use the resource API to register a call to kfree(to_free).

 * Since we never actually use the resource, it's safe to use on const data.

 kfree() handles NULL already, but avoid allocating a no-op cleanup. */

 We normally never expect to allocate suites, hence the non-const cast. */

 SPDX-License-Identifier: GPL-2.0

/*

 * An API to allow a function, that may fail, to be executed, and recover in a

 * controlled manner.

 *

 * Copyright (C) 2019, Google LLC.

 * Author: Brendan Higgins <brendanhiggins@google.com>

	/*

	 * TODO(brendanhiggins@google.com): We should probably have some type of

	 * variable timeout here. The only question is what that timeout value

	 * should be.

	 *

	 * The intention has always been, at some point, to be able to label

	 * tests with some type of size bucket (unit/small, integration/medium,

	 * large/system/end-to-end, etc), where each size bucket would get a

	 * default timeout value kind of like what Bazel does:

	 * https://docs.bazel.build/versions/master/be/common-definitions.html#test.size

	 * There is still some debate to be had on exactly how we do this. (For

	 * one, we probably want to have some sort of test runner level

	 * timeout.)

	 *

	 * For more background on this topic, see:

	 * https://mike-bland.com/2011/11/01/small-medium-large.html

	 *

	 * If tests timeout due to exceeding sysctl_hung_task_timeout_secs,

	 * the task will be killed and an oops generated.

 5 min */

 SPDX-License-Identifier: GPL-2.0-or-later

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright 2002 H. Peter Anvin - All Rights Reserved

 *

/*

 * raid6/algos.c

 *

 * Algorithm list and algorithm selection for RAID-6

 In .bss so it's zeroed */

 Need more time to be stable in userspace */

 work on the second half of the disks */

 Try to pick the best algorithm */

 This code uses the gfmul table as convenient data set to abuse */

 prepare the buffer and fill it circularly with gfmul table */

 select raid gen_syndrome function */

 select raid recover functions */

 SPDX-License-Identifier: GPL-2.0

/*

 * RAID-6 data recovery in dual failure mode based on the XC instruction.

 *

 * Copyright IBM Corp. 2016

 * Author(s): Martin Schwidefsky <schwidefsky@de.ibm.com>

 Recover two failed data blocks. */

 P multiplier table for B data */

 Q multiplier table (for both) */

	/* Compute syndrome with zero for the missing data pages

	   Use the dead data pages as temporary storage for

 Restore pointer table */

 Now, pick the proper data tables */

 Now do it... */

 Recover failure of one data block plus the P block */

 Q multiplier table */

	/* Compute syndrome with zero for the missing data page

 Restore pointer table */

 Now, pick the proper data tables */

 Now do it... */

 SPDX-License-Identifier: GPL-2.0-or-later

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright 2002 H. Peter Anvin - All Rights Reserved

 *

/*

 * raid6/recov.c

 *

 * RAID-6 data recovery in dual failure mode.  In single failure mode,

 * use the RAID-5 algorithm (or, in the case of Q failure, just reconstruct

 * the syndrome.)

 Recover two failed data blocks. */

 P multiplier table for B data */

 Q multiplier table (for both) */

	/* Compute syndrome with zero for the missing data pages

	   Use the dead data pages as temporary storage for

 Restore pointer table */

 Now, pick the proper data tables */

 Now do it... */

 Reconstructed B */

 Reconstructed A */

 Recover failure of one data block plus the P block */

 Q multiplier table */

	/* Compute syndrome with zero for the missing data page

 Restore pointer table */

 Now, pick the proper data tables */

 Now do it... */

 Testing only */

 Recover two failed blocks. */

 P+Q failure.  Just rebuild the syndrome. */

			/* data+Q failure.  Reconstruct data from P,

 NOT IMPLEMENTED - equivalent to RAID-5 */

 data+P failure. */

 data+data failure. */

 SPDX-License-Identifier: GPL-2.0-or-later

/* -*- linux-c -*- --------------------------------------------------------

 *

 *   Copyright (C) 2016 Intel Corporation

 *

 *   Author: Gayatri Kammela <gayatri.kammela@intel.com>

 *   Author: Megha Dey <megha.dey@linux.intel.com>

 *

 *   Based on avx2.c: Copyright 2012 Yuanhan Liu All Rights Reserved

 *   Based on sse2.c: Copyright 2002 H. Peter Anvin - All Rights Reserved

 *

 * -----------------------------------------------------------------------

/*

 * AVX512 implementation of RAID-6 syndrome functions

 *

 Highest data disk */

 XOR parity */

 RS syndrome */

 Zero temp */

 P[0] */

 Q[0] */

 P/Q right side optimization */

 XOR parity */

 RS syndrome */

 P/Q data pages */

 P/Q left side optimization */

 Don't use movntdq for r/w memory area < cache line */

 Has cache hints */

/*

 * Unrolled-by-2 AVX512 implementation

 Highest data disk */

 XOR parity */

 RS syndrome */

 Zero temp */

 We uniformly assume a single prefetch covers at least 64 bytes */

 P[0] */

 P[1] */

 Q[0] */

 Q[1] */

 P/Q right side optimization */

 XOR parity */

 RS syndrome */

 P/Q data pages */

 P/Q left side optimization */

			     /* Don't use movntdq for r/w

			      * memory area < cache line

 Has cache hints */

/*

 * Unrolled-by-4 AVX2 implementation

 Highest data disk */

 XOR parity */

 RS syndrome */

 Zero temp */

 P[0] */

 P[1] */

 Q[0] */

 Q[1] */

 P[2] */

 P[3] */

 Q[2] */

 Q[3] */

 P/Q right side optimization */

 XOR parity */

 RS syndrome */

 P/Q data pages */

 P/Q left side optimization */

 Has cache hints */

 CONFIG_AS_AVX512 */

 SPDX-License-Identifier: GPL-2.0-or-later

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright 2002 H. Peter Anvin - All Rights Reserved

 *

/*

 * raid6/mmx.c

 *

 * MMX implementation of RAID-6 syndrome functions

 Shared with raid6/sse1.c */

 Not really "boot_cpu" but "all_cpus" */

/*

 * Plain MMX implementation

 Highest data disk */

 XOR parity */

 RS syndrome */

 Zero temp */

 P[0] */

 Q[0] */

 XOR not yet implemented */

/*

 * Unrolled-by-2 MMX implementation

 Highest data disk */

 XOR parity */

 RS syndrome */

 Zero temp */

 Zero temp */

 P[0] */

 Q[0] */

 Q[1] */

 XOR not yet implemented */

 SPDX-License-Identifier: GPL-2.0-or-later

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright 2002 H. Peter Anvin - All Rights Reserved

 *

/*

 * raid6/sse2.c

 *

 * SSE-2 implementation of RAID-6 syndrome functions

 *

 Not really boot_cpu but "all_cpus" */

/*

 * Plain SSE2 implementation

 Highest data disk */

 XOR parity */

 RS syndrome */

 Zero temp */

 P[0] */

 Q[0] */

 P/Q right side optimization */

 XOR parity */

 RS syndrome */

 P/Q data pages */

 P/Q left side optimization */

 Don't use movntdq for r/w memory area < cache line */

 Has cache hints */

/*

 * Unrolled-by-2 SSE2 implementation

 Highest data disk */

 XOR parity */

 RS syndrome */

 Zero temp */

 Zero temp */

 We uniformly assume a single prefetch covers at least 32 bytes */

 P[0] */

 P[1] */

 Q[0] */

 Q[1] */

 P/Q right side optimization */

 XOR parity */

 RS syndrome */

 P/Q data pages */

 P/Q left side optimization */

 Don't use movntdq for r/w memory area < cache line */

 Has cache hints */

/*

 * Unrolled-by-4 SSE2 implementation

 Highest data disk */

 XOR parity */

 RS syndrome */

 P[0] */

 P[1] */

 Q[0] */

 Zero temp */

 Q[1] */

 Zero temp */

 P[2] */

 P[3] */

 Q[2] */

 Zero temp */

 Q[3] */

 Zero temp */

 The second prefetch seems to improve performance... */

 P/Q right side optimization */

 XOR parity */

 RS syndrome */

 P/Q data pages */

 P/Q left side optimization */

 Has cache hints */

 CONFIG_X86_64 */

 SPDX-License-Identifier: GPL-2.0-or-later

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright 2002 H. Peter Anvin - All Rights Reserved

 *

/*

 * raid6/sse1.c

 *

 * SSE-1/MMXEXT implementation of RAID-6 syndrome functions

 *

 * This is really an MMX implementation, but it requires SSE-1 or

 * AMD MMXEXT for prefetch support and a few other features.  The

 * support for nontemporal memory accesses is enough to make this

 * worthwhile as a separate implementation.

 Defined in raid6/mmx.c */

 Not really boot_cpu but "all_cpus" */

/*

 * Plain SSE1 implementation

 Highest data disk */

 XOR parity */

 RS syndrome */

 Zero temp */

 P[0] */

 Q[0] */

 XOR not yet implemented */

 Has cache hints */

/*

 * Unrolled-by-2 SSE1 implementation

 Highest data disk */

 XOR parity */

 RS syndrome */

 Zero temp */

 Zero temp */

 We uniformly assume a single prefetch covers at least 16 bytes */

 P[0] */

 P[1] */

 Q[0] */

 Q[1] */

 XOR not yet implemented */

 Has cache hints */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Intel Corporation

 P multiplier table for B data */

 Q multiplier table (for both) */

	/* Compute syndrome with zero for the missing data pages

	   Use the dead data pages as temporary storage for

 Restore pointer table */

 Now, pick the proper data tables */

 Now do it... */

 xmm6, xmm14, xmm15 */

 xmm0/8 = px */

 xmm2/10 = px */

 xmm5/13 = qx */

 xmm1/9 = pbmul[px] */

 xmm1/9 = db = DQ */

		/* 1 = dq ^ q

		 * 0 = dp ^ p

 xmm2 = px */

 xmm5 = qx */

 xmm1 = pbmul[px] */

 xmm1 = db = DQ */

 Q multiplier table */

	/* Compute syndrome with zero for the missing data page

 Restore pointer table */

 Now, pick the proper data tables */

 xmm3 = q[0] ^ dq[0] */

 xmm4 = q[16] ^ dq[16] */

 xmm4 = xmm8 = q[16] ^ dq[16] */

 xmm1 = qmul[q[0] ^ dq[0]] */

 xmm11 = qmul[q[16] ^ dq[16]] */

 xmm2 = p[0] ^ qmul[q[0] ^ dq[0]] */

 xmm12 = p[16] ^ qmul[q[16] ^ dq[16]] */

 xmm3 = *q ^ *dq */

 xmm1 = qmul[*q ^ *dq */

 xmm2 = *p ^ qmul[*q ^ *dq] */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Intel Corporation

 * Copyright (C) 2017 Linaro Ltd. <ard.biesheuvel@linaro.org>

/*

 * AArch32 does not provide this intrinsic natively because it does not

 * implement the underlying instruction. AArch32 only provides a 64-bit

 * wide vtbl.8 instruction, so use that instead.

	/*

	 * while ( bytes-- ) {

	 *	uint8_t px, qx, db;

	 *

	 *	px    = *p ^ *dp;

	 *	qx    = qmul[*q ^ *dq];

	 *	*dq++ = db = pbmul[px] ^ qx;

	 *	*dp++ = db ^ px;

	 *	p++; q++;

	 * }

	/*

	 * while (bytes--) {

	 *	*p++ ^= *dq = qmul[*q ^ *dq];

	 *	q++; dq++;

	 * }

/*

 * Copyright 2017, Matt Brown, IBM Corp.

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License

 * as published by the Free Software Foundation; either version

 * 2 of the License, or (at your option) any later version.

 *

 * vpermxor$#.c

 *

 * Based on H. Peter Anvin's paper - The mathematics of RAID-6

 *

 * $#-way unrolled portable integer math RAID-6 instruction set

 * This file is postprocessed using unroll.awk

 *

 * vpermxor$#.c makes use of the vpermxor instruction to optimise the RAID6 Q

 * syndrome calculations.

 * This can be run on systems which have both Altivec and vpermxor instruction.

 *

 * This instruction was introduced in POWER8 - ISA v2.07.

 Highest data disk */

 XOR parity */

 RS syndrome */

 P syndrome */

 Q syndrome */

 Check if arch has both altivec and the vpermxor instructions */

 SPDX-License-Identifier: GPL-2.0-or-later

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright 2002-2007 H. Peter Anvin - All Rights Reserved

 *

/*

 * mktables.c

 *

 * Make RAID-6 tables.  This is a host user space program to be run at

 * compile time.

 Compute multiplication table */

 Compute vector multiplication table */

 Compute power-of-2 table (exponent) */

 For entry 255, not a real entry */

 Compute log-of-2 table */

 Compute inverse table x^-1 == x^254 */

 Compute inv(2^x + 1) (exponent-xor-inverse) table */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Intel Corporation

 * Author: Jim Kukunas <james.t.kukunas@linux.intel.com>

 P multiplier table for B data */

 Q multiplier table (for both) */

	/* Compute syndrome with zero for the missing data pages

	   Use the dead data pages as temporary storage for

 Restore pointer table */

 Now, pick the proper data tables */

 ymm0 = x0f[16] */

		/*

		 * 1 = dq[0]  ^ q[0]

		 * 9 = dq[32] ^ q[32]

		 * 0 = dp[0]  ^ p[0]

		 * 8 = dp[32] ^ p[32]

		/*

		 * 5 = qx[0]

		 * 15 = qx[32]

		/*

		 * 1  = pbmul[px[0]]

		 * 13 = pbmul[px[32]]

		/*

		 * 1 = db = DQ

		 * 13 = db[32] = DQ[32]

 1 = dq ^ q;  0 = dp ^ p */

		/*

		 * 1 = dq ^ q

		 * 3 = dq ^ p >> 4

 5 = qx */

 1 = pbmul[px] */

 1 = db = DQ */

 Q multiplier table */

	/* Compute syndrome with zero for the missing data page

 Restore pointer table */

 Now, pick the proper data tables */

		/*

		 * 3 = q[0] ^ dq[0]

		 * 8 = q[32] ^ dq[32]

		/*

		 * 1  = qmul[q[0]  ^ dq[0]]

		 * 14 = qmul[q[32] ^ dq[32]]

		/*

		 * 2  = p[0]  ^ qmul[q[0]  ^ dq[0]]

		 * 12 = p[32] ^ qmul[q[32] ^ dq[32]]

 3 = q ^ dq */

 1 = qmul[q ^ dq] */

 2 = p ^ qmul[q ^ dq] */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2016 Intel Corporation

 *

 * Author: Gayatri Kammela <gayatri.kammela@intel.com>

 * Author: Megha Dey <megha.dey@linux.intel.com>

 P multiplier table for B data */

 Q multiplier table (for both) */

	/*

	 * Compute syndrome with zero for the missing data pages

	 * Use the dead data pages as temporary storage for

	 * delta p and delta q

 Restore pointer table */

 Now, pick the proper data tables */

 zmm0 = x0f[16] */

		/*

		 * 1 = dq[0]  ^ q[0]

		 * 9 = dq[64] ^ q[64]

		 * 0 = dp[0]  ^ p[0]

		 * 8 = dp[64] ^ p[64]

		/*

		 * 5 = qx[0]

		 * 15 = qx[64]

		/*

		 * 1  = pbmul[px[0]]

		 * 13 = pbmul[px[64]]

		/*

		 * 1 = db = DQ

		 * 13 = db[64] = DQ[64]

 1 = dq ^ q;  0 = dp ^ p */

		/*

		 * 1 = dq ^ q

		 * 3 = dq ^ p >> 4

 5 = qx */

 1 = pbmul[px] */

 1 = db = DQ */

 Q multiplier table */

	/*

	 * Compute syndrome with zero for the missing data page

	 * Use the dead data page as temporary storage for delta q

 Restore pointer table */

 Now, pick the proper data tables */

		/*

		 * 3 = q[0] ^ dq[0]

		 * 8 = q[64] ^ dq[64]

		/*

		 * 1  = qmul[q[0]  ^ dq[0]]

		 * 14 = qmul[q[64] ^ dq[64]]

		/*

		 * 2  = p[0]  ^ qmul[q[0]  ^ dq[0]]

		 * 12 = p[64] ^ qmul[q[64] ^ dq[64]]

 3 = q ^ dq */

 1 = qmul[q ^ dq] */

 2 = p ^ qmul[q ^ dq] */

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright 2002-2004 H. Peter Anvin - All Rights Reserved

 *

 *   This program is free software; you can redistribute it and/or modify

 *   it under the terms of the GNU General Public License as published by

 *   the Free Software Foundation, Inc., 53 Temple Place Ste 330,

 *   Boston MA 02111-1307, USA; either version 2 of the License, or

 *   (at your option) any later version; incorporated herein by reference.

 *

/*

 * int$#.c

 *

 * $#-way unrolled portable integer math RAID-6 instruction set

 *

 * This file is postprocessed using unroll.awk

/*

 * This is the C data type to use

 Change this from BITS_PER_LONG if there is something better... */

/*

 * IA-64 wants insane amounts of unrolling.  On other architectures that

 * is just a waste of space.

/*

 * These sub-operations are separate inlines since they can sometimes be

 * specially optimized using architecture-specific hacks.

/*

 * The SHLBYTE() operation shifts each byte left by 1, *not*

 * rolling over into the next byte

/*

 * The MASK() operation returns 0xFF in any byte for which the high

 * bit is 1, 0x00 for any byte for which the high bit is 0.

 Overflow on the top bit is OK */

 Highest data disk */

 XOR parity */

 RS syndrome */

 P/Q right side optimization */

 XOR parity */

 RS syndrome */

 P/Q data pages */

 P/Q left side optimization */

 always valid */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Intel Corporation

 * Copyright (C) 2017 Linaro Ltd. <ard.biesheuvel@linaro.org>

 P multiplier table for B data */

 Q multiplier table (for both) */

	/*

	 * Compute syndrome with zero for the missing data pages

	 * Use the dead data pages as temporary storage for

	 * delta p and delta q

 Restore pointer table */

 Now, pick the proper data tables */

 Q multiplier table */

	/*

	 * Compute syndrome with zero for the missing data page

	 * Use the dead data page as temporary storage for delta q

 Restore pointer table */

 Now, pick the proper data tables */

 SPDX-License-Identifier: GPL-2.0

/*

 * raid6_vx$#.c

 *

 * $#-way unrolled RAID6 gen/xor functions for s390

 * based on the vector facility

 *

 * Copyright IBM Corp. 2016

 * Author(s): Martin Schwidefsky <schwidefsky@de.ibm.com>

 *

 * This file is postprocessed using unroll.awk.

/*

 * The SHLBYTE() operation shifts each of the 16 bytes in

 * vector register y left by 1 bit and stores the result in

 * vector register x.

/*

 * For each of the 16 bytes in the vector register y the MASK()

 * operation returns 0xFF if the high bit of the byte is 1,

 * or 0x00 if the high bit is 0. The result is stored in vector

 * register x.

 Highest data disk */

 XOR parity */

 RS syndrome */

 P/Q right side optimization */

 XOR parity */

 RS syndrome */

 P/Q data pages */

 P/Q left side optimization */

/* -----------------------------------------------------------------------

 *

 *   neon.uc - RAID-6 syndrome calculation using ARM NEON instructions

 *

 *   Copyright (C) 2012 Rob Herring

 *   Copyright (C) 2015 Linaro Ltd. <ard.biesheuvel@linaro.org>

 *

 *   Based on altivec.uc:

 *     Copyright 2002-2004 H. Peter Anvin - All Rights Reserved

 *

 *   This program is free software; you can redistribute it and/or modify

 *   it under the terms of the GNU General Public License as published by

 *   the Free Software Foundation, Inc., 53 Temple Place Ste 330,

 *   Boston MA 02111-1307, USA; either version 2 of the License, or

 *   (at your option) any later version; incorporated herein by reference.

 *

/*

 * neon$#.c

 *

 * $#-way unrolled NEON intrinsics math RAID-6 instruction set

 *

 * This file is postprocessed using unroll.awk

/*

 * The SHLBYTE() operation shifts each byte left by 1, *not*

 * rolling over into the next byte

/*

 * The MASK() operation returns 0xFF in any byte for which the high

 * bit is 1, 0x00 for any byte for which the high bit is 0.

 Highest data disk */

 XOR parity */

 RS syndrome */

 P/Q right side optimization */

 XOR parity */

 RS syndrome */

 P/Q data pages */

 P/Q left side optimization */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/lib/raid6/neon.c - RAID6 syndrome calculation using ARM NEON intrinsics

 *

 * Copyright (C) 2013 Linaro Ltd <ard.biesheuvel@linaro.org>

/*

 * There are 2 reasons these wrappers are kept in a separate compilation unit

 * from the actual implementations in neonN.c (generated from neon.uc by

 * unroll.awk):

 * - the actual implementations use NEON intrinsics, and the GCC support header

 *   (arm_neon.h) is not fully compatible (type wise) with the kernel;

 * - the neonN.c files are compiled with -mfpu=neon and optimization enabled,

 *   and we have to make sure that we never use *any* NEON/VFP instructions

 *   outside a kernel_neon_begin()/kernel_neon_end() pair.

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright 2002-2004 H. Peter Anvin - All Rights Reserved

 *

 *   This program is free software; you can redistribute it and/or modify

 *   it under the terms of the GNU General Public License as published by

 *   the Free Software Foundation, Inc., 53 Temple Place Ste 330,

 *   Boston MA 02111-1307, USA; either version 2 of the License, or

 *   (at your option) any later version; incorporated herein by reference.

 *

/*

 * raid6altivec$#.c

 *

 * $#-way unrolled portable integer math RAID-6 instruction set

 *

 * This file is postprocessed using unroll.awk

 *

 * <benh> hpa: in process,

 * you can just "steal" the vec unit with enable_kernel_altivec() (but

 * bracked this with preempt_disable/enable or in a lock)

 __KERNEL__ */

/*

 * This is the C data type to use.  We use a vector of

 * signed char so vec_cmpgt() will generate the right

 * instruction.

/*

 * The SHLBYTE() operation shifts each byte left by 1, *not*

 * rolling over into the next byte

/*

 * The MASK() operation returns 0xFF in any byte for which the high

 * bit is 1, 0x00 for any byte for which the high bit is 0.

 vec_cmpgt returns a vector bool char; thus the need for the cast */

/* This is noinline to make damned sure that gcc doesn't move any of the

 Highest data disk */

 XOR parity */

 RS syndrome */

 This assumes either all CPUs have Altivec or none does */

 XOR not yet implemented */

 CONFIG_ALTIVEC */

 SPDX-License-Identifier: GPL-2.0-or-later

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 2012 Intel Corporation

 *   Author: Yuanhan Liu <yuanhan.liu@linux.intel.com>

 *

 *   Based on sse2.c: Copyright 2002 H. Peter Anvin - All Rights Reserved

 *

/*

 * AVX2 implementation of RAID-6 syndrome functions

 *

/*

 * Plain AVX2 implementation

 Highest data disk */

 XOR parity */

 RS syndrome */

 Zero temp */

 P[0] */

 Q[0] */

 P/Q right side optimization */

 XOR parity */

 RS syndrome */

 P/Q data pages */

 P/Q left side optimization */

 Don't use movntdq for r/w memory area < cache line */

 Has cache hints */

/*

 * Unrolled-by-2 AVX2 implementation

 Highest data disk */

 XOR parity */

 RS syndrome */

 Zero temp */

 We uniformly assume a single prefetch covers at least 32 bytes */

 P[0] */

 P[1] */

 Q[0] */

 Q[1] */

 P/Q right side optimization */

 XOR parity */

 RS syndrome */

 P/Q data pages */

 P/Q left side optimization */

 Don't use movntdq for r/w memory area < cache line */

 Has cache hints */

/*

 * Unrolled-by-4 AVX2 implementation

 Highest data disk */

 XOR parity */

 RS syndrome */

 Zero temp */

 P[0] */

 P[1] */

 Q[0] */

 Q[1] */

 P[2] */

 P[3] */

 Q[2] */

 Q[3] */

 P/Q right side optimization */

 XOR parity */

 RS syndrome */

 P/Q data pages */

 P/Q left side optimization */

 Has cache hints */

 SPDX-License-Identifier: GPL-2.0-or-later

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright 2002-2007 H. Peter Anvin - All Rights Reserved

 *

/*

 * raid6test.c

 *

 * Test RAID-6 recovery with various algorithms

 Including P and Q */

		/* We don't implement the DQ failure scenario, since it's

 Nuke syndromes */

 Generate assumed good syndrome */

 Simulate rmw run */

 Pick the best algorithm test */

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic userspace implementations of gettimeofday() and similar.

/*

 * Default implementation which works for all sane clocksources. That

 * obviously excludes x86/TSC.

 Add the namespace offset */

	/*

	 * Do this outside the loop: a race inside the loop could result

	 * in __iter_div_u64_rem() being extremely slow.

 Allows to compile the high resolution parts out */

		/*

		 * Open coded to handle VDSO_CLOCKMODE_TIMENS. Time namespace

		 * enabled tasks have a special VVAR page installed which

		 * has vd->seq set to 1 and vd->clock_mode set to

		 * VDSO_CLOCKMODE_TIMENS. For non time namespace affected tasks

		 * this does not affect performance because if vd->seq is

		 * odd, i.e. a concurrent update is in progress the extra

		 * check for vd->clock_mode is just a few extra

		 * instructions while spin waiting for vd->seq to become

		 * even again.

	/*

	 * Do this outside the loop: a race inside the loop could result

	 * in __iter_div_u64_rem() being extremely slow.

 Add the namespace offset */

	/*

	 * Do this outside the loop: a race inside the loop could result

	 * in __iter_div_u64_rem() being extremely slow.

		/*

		 * Open coded to handle VDSO_CLOCK_TIMENS. See comment in

		 * do_hres().

 Check for negative values or invalid clocks */

	/*

	 * Convert the clockid to a bitmask and use it to check which

	 * clocks are handled in the VDSO directly.

 For ret == 0 */

 BUILD_VDSO32 */

 VDSO_HAS_TIME */

 Check for negative values or invalid clocks */

	/*

	 * Convert the clockid to a bitmask and use it to check which

	 * clocks are handled in the VDSO directly.

		/*

		 * Preserves the behaviour of posix_get_hrtimer_res().

		/*

		 * Preserves the behaviour of posix_get_coarse_res().

 BUILD_VDSO32 */

 VDSO_HAS_CLOCK_GETRES */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2019 SUSE

 Version 2 supports migration. */

 Executed on object patching (ie, patch enablement) */

 Executed on object unpatching (ie, patch disablement) */

 Executed on object unpatching (ie, patch disablement) */

 Executed on object unpatching (ie, patch disablement) */

 vmlinux */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2014 Seth Jennings <sjenning@redhat.com>

 name being NULL means vmlinux */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2019 SUSE

 The console loglevel fix is the same in the next cumulative patch. */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Joe Lawrence <joe.lawrence@redhat.com>

 name being NULL means vmlinux */

 set .replace in the init function below for demo purposes */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Joe Lawrence <joe.lawrence@redhat.com>

 Executed on object patching (ie, patch enablement) */

 Executed on object unpatching (ie, patch disablement) */

 Executed on object unpatching (ie, patch disablement) */

 Executed on object unpatching (ie, patch disablement) */

 vmlinux */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Joe Lawrence <joe.lawrence@redhat.com>

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Joe Lawrence <joe.lawrence@redhat.com>

/*

 * Keep a small list of pointers so that we can print address-agnostic

 * pointer values.  Use a rolling integer count to differentiate the values.

 * Ironically we could have used the shadow variable API to do this, but

 * let's not lean too heavily on the very code we're testing.

/*

 * Shadow variable wrapper functions that echo the function and arguments

 * to the kernel log for testing verification.  Don't display raw pointers,

 * but use the ptr_id() value instead.

 Shadow variable constructor - remember simple pointer data */

/*

 * With more than one item to free in the list, order is not determined and

 * shadow_dtor will not be passed to shadow_free_all() which would make the

 * test fail. (see pass 6)

 number of objects we simulate that need shadow vars */

 dynamically created obj fields have the following shadow var id values */

/*

 * The main test case adds/removes new fields (shadow var) to each of these

 * test structure instances. The last group of fields in the struct represent

 * the idea that shadow variables may be added and removed to and from the

 * struct during execution.

 add anything here below and avoid to define an empty struct */

 these represent shadow vars added and removed with SV_ID{1,2} */

 char nfield1; */

 int  nfield2; */

	/*

	 * With an empty shadow variable hash table, expect not to find

	 * any matches.

 pass 1: init & alloc a char+int pair of svars for each objs */

 pass 2: verify we find allocated svars and where they point to */

 check the "char" svar for all objects */

 check the "int" svar for all objects */

 pass 3: verify that 'get_or_alloc' returns already allocated svars */

 pass 4: free <objs[*], SV_ID1> pairs of svars, verify removal */

 'char' pairs */

 pass 5: check we still find <objs[*], SV_ID2> svar pairs */

 'int' pairs */

 pass 6: free all the <objs[*], SV_ID2> svar pairs too. */

 'int' pairs */

 'char' pairs */

 'int' pairs */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2019 SUSE

 Version 1 does not support migration. */

 Executed on object patching (ie, patch enablement) */

 Executed on object unpatching (ie, patch disablement) */

 Executed on object unpatching (ie, patch disablement) */

 Executed on object unpatching (ie, patch disablement) */

 vmlinux */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Joe Lawrence <joe.lawrence@redhat.com>

 Executed on object patching (ie, patch enablement) */

 Executed on object unpatching (ie, patch disablement) */

 Executed on object unpatching (ie, patch disablement) */

 Executed on object unpatching (ie, patch disablement) */

 vmlinux */

 set .replace in the init function below for demo purposes */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Joe Lawrence <joe.lawrence@redhat.com>

 load/run-time control from sysfs writer  */

		/*

		 * Busy-wait until the sysfs writer has acknowledged a

		 * blocked transition and clears the flag.

		/*

		 * Serialize output: print all messages from the work

		 * function before returning from init().

/*

 * CRC32 using the polynomial from IEEE-802.3

 *

 * Authors: Lasse Collin <lasse.collin@tukaani.org>

 *          Igor Pavlov <https://7-zip.org/>

 *

 * This file has been put into the public domain.

 * You can do whatever you want with this file.

/*

 * This is not the fastest implementation, but it is pretty compact.

 * The fastest versions of xz_crc32() on modern CPUs without hardware

 * accelerated CRC instruction are 3-5 times as fast as this version,

 * but they are bigger and use more memory for the lookup table.

/*

 * STATIC_RW_DATA is used in the pre-boot environment on some architectures.

 * See <linux/decompress/mm.h> for details.

/*

 * Branch/Call/Jump (BCJ) filter decoders

 *

 * Authors: Lasse Collin <lasse.collin@tukaani.org>

 *          Igor Pavlov <https://7-zip.org/>

 *

 * This file has been put into the public domain.

 * You can do whatever you want with this file.

/*

 * The rest of the file is inside this ifdef. It makes things a little more

 * convenient when building without support for any BCJ filters.

 Type of the BCJ filter being used */

 x86 or x86-64 */

 Big endian only */

 Big or little endian */

 Little endian only */

 Little endian only */

 Big or little endian */

	/*

	 * Return value of the next filter in the chain. We need to preserve

	 * this information across calls, because we must not call the next

	 * filter anymore once it has returned XZ_STREAM_END.

 True if we are operating in single-call mode. */

	/*

	 * Absolute position relative to the beginning of the uncompressed

	 * data (in a single .xz Block). We care only about the lowest 32

	 * bits so this doesn't need to be uint64_t even with big files.

 x86 filter state */

 Temporary space to hold the variables from struct xz_buf */

 Amount of already filtered data in the beginning of buf */

 Total amount of data currently stored in buf  */

		/*

		 * Buffer to hold a mix of filtered and unfiltered data. This

		 * needs to be big enough to hold Alignment + 2 * Look-ahead:

		 *

		 * Type         Alignment   Look-ahead

		 * x86              1           4

		 * PowerPC          4           0

		 * IA-64           16           0

		 * ARM              4           0

		 * ARM-Thumb        2           2

		 * SPARC            4           0

/*

 * This is used to test the most significant byte of a memory address

 * in an x86 instruction.

	/*

	 * The local variables take a little bit stack space, but it's less

	 * than what LZMA2 decoder takes, so it doesn't make sense to reduce

	 * stack usage here without doing that for the LZMA2 decoder too.

 Loop counters */

 Instruction slot (0, 1, or 2) in the 128-bit instruction word */

 Bitwise offset of the instruction indicated by slot */

 bit_pos split into byte and bit parts */

 Address part of an instruction */

 Mask used to detect which instructions to convert */

 41-bit instruction stored somewhere in the lowest 48 bits */

 Instruction normalized with bit_res for easier manipulation */

/*

 * Apply the selected BCJ filter. Update *pos and s->pos to match the amount

 * of data that got filtered.

 *

 * NOTE: This is implemented as a switch statement to avoid using function

 * pointers, which could be problematic in the kernel boot code, which must

 * avoid pointers to static data (at least on x86).

 Never reached but silence compiler warnings. */

/*

 * Flush pending filtered data from temp to the output buffer.

 * Move the remaining mixture of possibly filtered and unfiltered

 * data to the beginning of temp.

/*

 * The BCJ filter functions are primitive in sense that they process the

 * data in chunks of 1-16 bytes. To hide this issue, this function does

 * some buffering.

	/*

	 * Flush pending already filtered data to the output buffer. Return

	 * immediately if we couldn't flush everything, or if the next

	 * filter in the chain had already returned XZ_STREAM_END.

	/*

	 * If we have more output space than what is currently pending in

	 * temp, copy the unfiltered data from temp to the output buffer

	 * and try to fill the output buffer by decoding more data from the

	 * next filter in the chain. Apply the BCJ filter on the new data

	 * in the output buffer. If everything cannot be filtered, copy it

	 * to temp and rewind the output buffer position accordingly.

	 *

	 * This needs to be always run when temp.size == 0 to handle a special

	 * case where the output buffer is full and the next filter has no

	 * more output coming but hasn't returned XZ_STREAM_END yet.

		/*

		 * As an exception, if the next filter returned XZ_STREAM_END,

		 * we can do that too, since the last few bytes that remain

		 * unfiltered are meant to remain unfiltered.

		/*

		 * If there wasn't enough input to the next filter to fill

		 * the output buffer with unfiltered data, there's no point

		 * to try decoding more data to temp.

	/*

	 * We have unfiltered data in temp. If the output buffer isn't full

	 * yet, try to fill the temp buffer by decoding more data from the

	 * next filter. Apply the BCJ filter on temp. Then we hopefully can

	 * fill the actual output buffer by copying filtered data from temp.

	 * A mix of filtered and unfiltered data may be left in temp; it will

	 * be taken care on the next call to this function.

 Make b->out{,_pos,_size} temporarily point to s->temp. */

		/*

		 * If the next filter returned XZ_STREAM_END, we mark that

		 * everything is filtered, since the last unfiltered bytes

		 * of the stream are meant to be left as is.

 Unsupported Filter ID */

/*

 * XZ decoder tester

 *

 * Author: Lasse Collin <lasse.collin@tukaani.org>

 *

 * This file has been put into the public domain.

 * You can do whatever you want with this file.

 Maximum supported dictionary size */

 Device name to pass to register_chrdev(). */

 Dynamically allocated device major number */

/*

 * We reuse the same decoder state, and thus can decode only one

 * file at a time.

 XZ decoder state */

/*

 * Return value of xz_dec_run(). We need to avoid calling xz_dec_run() after

 * it has returned XZ_STREAM_END, so we make this static.

/*

 * Input and output buffers. The input buffer is used as a temporary safe

 * place for the data coming from the userspace.

/*

 * Structure to pass the input and output buffers to the XZ decoder.

 * A few of the fields are never modified so we initialize them here.

/*

 * CRC32 of uncompressed data. This is used to give the user a simple way

 * to check that the decoder produces correct output.

/*

 * Decode the data given to us from the userspace. CRC32 of the uncompressed

 * data is calculated and is printed at the end of successful decoding. The

 * uncompressed data isn't stored anywhere for further use.

 *

 * The .xz file must have exactly one Stream and no Stream Padding. The data

 * after the first Stream is considered to be garbage.

 Allocate the XZ decoder state and register the character device. */

/*

 * This code is in the public domain, but in Linux it's simplest to just

 * say it's GPL and consider the authors as the copyright holders.

/*

 * LZMA2 decoder

 *

 * Authors: Lasse Collin <lasse.collin@tukaani.org>

 *          Igor Pavlov <https://7-zip.org/>

 *

 * This file has been put into the public domain.

 * You can do whatever you want with this file.

/*

 * Range decoder initialization eats the first five bytes of each LZMA chunk.

/*

 * Minimum number of usable input buffer to safely decode one LZMA symbol.

 * The worst case is that we decode 22 bits using probabilities and 26

 * direct bits. This may decode at maximum of 20 bytes of input. However,

 * lzma_main() does an extra normalization before returning, thus we

 * need to put 21 here.

/*

 * Dictionary (history buffer)

 *

 * These are always true:

 *    start <= pos <= full <= end

 *    pos <= limit <= end

 *

 * In multi-call mode, also these are true:

 *    end == size

 *    size <= size_max

 *    allocated <= size

 *

 * Most of these variables are size_t to support single-call mode,

 * in which the dictionary variables address the actual output

 * buffer directly.

 Beginning of the history buffer */

 Old position in buf (before decoding more data) */

 Position in buf */

	/*

	 * How full dictionary is. This is used to detect corrupt input that

	 * would read beyond the beginning of the uncompressed stream.

 Write limit; we don't write to buf[limit] or later bytes. */

	/*

	 * End of the dictionary buffer. In multi-call mode, this is

	 * the same as the dictionary size. In single-call mode, this

	 * indicates the size of the output buffer.

	/*

	 * Size of the dictionary as specified in Block Header. This is used

	 * together with "full" to detect corrupt input that would make us

	 * read beyond the beginning of the uncompressed stream.

	/*

	 * Maximum allowed dictionary size in multi-call mode.

	 * This is ignored in single-call mode.

	/*

	 * Amount of memory currently allocated for the dictionary.

	 * This is used only with XZ_DYNALLOC. (With XZ_PREALLOC,

	 * size_max is always the same as the allocated size.)

 Operation mode */

 Range decoder */

	/*

	 * Number of initializing bytes remaining to be read

	 * by rc_read_init().

	/*

	 * Buffer from which we read our input. It can be either

	 * temp.buf or the caller-provided input buffer.

 Probabilities for a length decoder. */

 Probability of match length being at least 10 */

 Probability of match length being at least 18 */

 Probabilities for match lengths 2-9 */

 Probabilities for match lengths 10-17 */

 Probabilities for match lengths 18-273 */

 Distances of latest four matches */

 Types of the most recently seen LZMA symbols */

	/*

	 * Length of a match. This is updated so that dict_repeat can

	 * be called again to finish repeating the whole match.

	/*

	 * LZMA properties or related bit masks (number of literal

	 * context bits, a mask derived from the number of literal

	 * position bits, and a mask derived from the number

	 * position bits)

 (1 << lp) - 1 */

 (1 << pb) - 1 */

 If 1, it's a match. Otherwise it's a single 8-bit literal. */

 If 1, it's a repeated match. The distance is one of rep0 .. rep3. */

	/*

	 * If 0, distance of a repeated match is rep0.

	 * Otherwise check is_rep1.

	/*

	 * If 0, distance of a repeated match is rep1.

	 * Otherwise check is_rep2.

 If 0, distance of a repeated match is rep2. Otherwise it is rep3. */

	/*

	 * If 1, the repeated match has length of one byte. Otherwise

	 * the length is decoded from rep_len_decoder.

	/*

	 * Probability tree for the highest two bits of the match

	 * distance. There is a separate probability tree for match

	 * lengths of 2 (i.e. MATCH_LEN_MIN), 3, 4, and [5, 273].

	/*

	 * Probility trees for additional bits for match distance

	 * when the distance is in the range [4, 127].

	/*

	 * Probability tree for the lowest four bits of a match

	 * distance that is equal to or greater than 128.

 Length of a normal match */

 Length of a repeated match */

 Probabilities of literals */

 Position in xz_dec_lzma2_run(). */

 Next position after decoding the compressed size of the chunk. */

 Uncompressed size of LZMA chunk (2 MiB at maximum) */

	/*

	 * Compressed size of LZMA chunk or compressed/uncompressed

	 * size of uncompressed chunk (64 KiB at maximum)

	/*

	 * True if dictionary reset is needed. This is false before

	 * the first chunk (LZMA or uncompressed).

	/*

	 * True if new LZMA properties are needed. This is false

	 * before the first LZMA chunk.

	/*

	 * The order below is important on x86 to reduce code size and

	 * it shouldn't hurt on other platforms. Everything up to and

	 * including lzma.pos_mask are in the first 128 bytes on x86-32,

	 * which allows using smaller instructions to access those

	 * variables. On x86-64, fewer variables fit into the first 128

	 * bytes, but this is still the best order without sacrificing

	 * the readability by splitting the structures.

	/*

	 * Temporary buffer which holds small number of input bytes between

	 * decoder calls. See lzma2_lzma() for details.

/**************

 * Dictionary *

/*

 * Reset the dictionary state. When in single-call mode, set up the beginning

 * of the dictionary to point to the actual output buffer.

 Set dictionary write limit */

 Return true if at least one byte can be written into the dictionary. */

/*

 * Get a byte from the dictionary at the given distance. The distance is

 * assumed to valid, or as a special case, zero when the dictionary is

 * still empty. This special case is needed for single-call decoding to

 * avoid writing a '\0' to the end of the destination buffer.

/*

 * Put one byte into the dictionary. It is assumed that there is space for it.

/*

 * Repeat given number of bytes from the given distance. If the distance is

 * invalid, false is returned. On success, true is returned and *len is

 * updated to indicate how many bytes were left to be repeated.

 Copy uncompressed data as is from input to dictionary and output buffers. */

		/*

		 * If doing in-place decompression in single-call mode and the

		 * uncompressed size of the file is larger than the caller

		 * thought (i.e. it is invalid input!), the buffers below may

		 * overlap and cause undefined behavior with memcpy().

		 * With valid inputs memcpy() would be fine here.

			/*

			 * Like above but for multi-call mode: use memmove()

			 * to avoid undefined behavior with invalid input.

/*

 * Flush pending data from dictionary to b->out. It is assumed that there is

 * enough space in b->out. This is guaranteed because caller uses dict_limit()

 * before decoding data into the dictionary.

		/*

		 * These buffers cannot overlap even if doing in-place

		 * decompression because in multi-call mode dict->buf

		 * has been allocated by us in this file; it's not

		 * provided by the caller like in single-call mode.

		 *

		 * With MicroLZMA, b->out can be NULL to skip bytes that

		 * the caller doesn't need. This cannot be done with XZ

		 * because it would break BCJ filters.

/*****************

 * Range decoder *

 Reset the range decoder. */

/*

 * Read the first five initial bytes into rc->code if they haven't been

 * read already. (Yes, the first byte gets completely ignored.)

 Return true if there may not be enough input for the next decoding loop. */

/*

 * Return true if it is possible (from point of view of range decoder) that

 * we have reached the end of the LZMA chunk.

 Read the next input byte if needed. */

/*

 * Decode one bit. In some versions, this function has been split in three

 * functions so that the compiler is supposed to be able to more easily avoid

 * an extra branch. In this particular version of the LZMA decoder, this

 * doesn't seem to be a good idea (tested with GCC 3.3.6, 3.4.6, and 4.3.3

 * on x86). Using a non-split version results in nicer looking code too.

 *

 * NOTE: This must return an int. Do not make it return a bool or the speed

 * of the code generated by GCC 3.x decreases 10-15 %. (GCC 4.3 doesn't care,

 * and it generates 10-20 % faster code than GCC 3.x from this file anyway.)

 Decode a bittree starting from the most significant bit. */

 Decode a bittree starting from the least significant bit. */

 Decode direct bits (fixed fifty-fifty probability) */

/********

 * LZMA *

 Get pointer to literal coder probability array. */

 Decode a literal (one 8-bit byte) */

 Decode the length of the match into s->lzma.len. */

 Decode a match. The distance will be stored in s->lzma.rep0. */

/*

 * Decode a repeated match. The distance is one of the four most recently

 * seen matches. The distance will be stored in s->lzma.rep0.

 LZMA decoder core */

	/*

	 * If the dictionary was reached during the previous call, try to

	 * finish the possibly pending repeat in the dictionary.

	/*

	 * Decode more LZMA symbols. One iteration may consume up to

	 * LZMA_IN_REQUIRED - 1 bytes.

	/*

	 * Having the range decoder always normalized when we are outside

	 * this function makes it easier to correctly handle end of the chunk.

/*

 * Reset the LZMA decoder and range decoder state. Dictionary is not reset

 * here, because LZMA state may be reset without resetting the dictionary.

	/*

	 * All probabilities are initialized to the same value. This hack

	 * makes the code smaller by avoiding a separate loop for each

	 * probability array.

	 *

	 * This could be optimized so that only that part of literal

	 * probabilities that are actually required. In the common case

	 * we would write 12 KiB less.

/*

 * Decode and validate LZMA properties (lc/lp/pb) and calculate the bit masks

 * from the decoded lp and pb values. On success, the LZMA decoder state is

 * reset and true is returned.

/*********

 * LZMA2 *

/*

 * The LZMA decoder assumes that if the input limit (s->rc.in_limit) hasn't

 * been exceeded, it is safe to read up to LZMA_IN_REQUIRED bytes. This

 * wrapper function takes care of making the LZMA decoder's assumption safe.

 *

 * As long as there is plenty of input left to be decoded in the current LZMA

 * chunk, we decode directly from the caller-supplied input buffer until

 * there's LZMA_IN_REQUIRED bytes left. Those remaining bytes are copied into

 * s->temp.buf, which (hopefully) gets filled on the next call to this

 * function. We decode a few bytes from the temporary buffer so that we can

 * continue decoding from the caller-supplied input buffer again.

/*

 * Take care of the LZMA2 control layer, and forward the job of actual LZMA

 * decoding or copying of uncompressed chunks to other functions.

			/*

			 * LZMA2 control byte

			 *

			 * Exact values:

			 *   0x00   End marker

			 *   0x01   Dictionary reset followed by

			 *          an uncompressed chunk

			 *   0x02   Uncompressed chunk (no dictionary reset)

			 *

			 * Highest three bits (s->control & 0xE0):

			 *   0xE0   Dictionary reset, new properties and state

			 *          reset, followed by LZMA compressed chunk

			 *   0xC0   New properties and state reset, followed

			 *          by LZMA compressed chunk (no dictionary

			 *          reset)

			 *   0xA0   State reset using old properties,

			 *          followed by LZMA compressed chunk (no

			 *          dictionary reset)

			 *   0x80   LZMA chunk (no dictionary or state reset)

			 *

			 * For LZMA compressed chunks, the lowest five bits

			 * (s->control & 1F) are the highest bits of the

			 * uncompressed size (bits 16-20).

			 *

			 * A new LZMA2 stream must begin with a dictionary

			 * reset. The first LZMA chunk must set new

			 * properties and reset the LZMA state.

			 *

			 * Values that don't match anything described above

			 * are invalid and we return XZ_DATA_ERROR.

					/*

					 * When there are new properties,

					 * state reset is done at

					 * SEQ_PROPERTIES.

			/*

			 * Set dictionary limit to indicate how much we want

			 * to be encoded at maximum. Decode new data into the

			 * dictionary. Flush the new data from dictionary to

			 * b->out. Check if we finished decoding this chunk.

			 * In case the dictionary got full but we didn't fill

			 * the output buffer yet, we may run this loop

			 * multiple times without changing s->lzma2.sequence.

 This limits dictionary size to 3 GiB to keep parsing simpler. */

 This is a wrapper struct to have a nice struct name in the public API. */

	/*

	 * sequence is SEQ_PROPERTIES before the first input byte,

	 * SEQ_LZMA_PREPARE until a total of five bytes have been read,

	 * and SEQ_LZMA_RUN for the rest of the input stream.

 One byte is needed for the props. */

			/*

			 * Don't increment b->in_pos here. The same byte is

			 * also passed to rc_read_init() which will ignore it.

		/*

		 * xz_dec_microlzma_reset() doesn't validate the compressed

		 * size so we do it here. We have to limit the maximum size

		 * to avoid integer overflows in lzma2_lzma(). 3 GiB is a nice

		 * round number and much more than users of this code should

		 * ever need.

 This is to allow increasing b->out_size between calls. */

 Restrict dict_size to the same range as in the LZMA2 code. */

	/*

	 * comp_size is validated in xz_dec_microlzma_run().

	 * uncomp_size can safely be anything.

/*

 * .xz Stream decoder

 *

 * Author: Lasse Collin <lasse.collin@tukaani.org>

 *

 * This file has been put into the public domain.

 * You can do whatever you want with this file.

 Hash used to validate the Index field */

 Position in dec_main() */

 Position in variable-length integers and Check fields */

 Variable-length integer decoded by dec_vli() */

 Saved in_pos and out_pos */

 CRC32 value in Block or Index */

 Type of the integrity check calculated from uncompressed data */

 Operation mode */

	/*

	 * True if the next call to xz_dec_run() is allowed to return

	 * XZ_BUF_ERROR.

 Information stored in Block Header */

		/*

		 * Value stored in the Compressed Size field, or

		 * VLI_UNKNOWN if Compressed Size is not present.

		/*

		 * Value stored in the Uncompressed Size field, or

		 * VLI_UNKNOWN if Uncompressed Size is not present.

 Size of the Block Header field */

 Information collected when decoding Blocks */

 Observed compressed size of the current Block */

 Observed uncompressed size of the current Block */

 Number of Blocks decoded so far */

		/*

		 * Hash calculated from the Block sizes. This is used to

		 * validate the Index field.

 Variables needed when verifying the Index field */

 Position in dec_index() */

 Size of the Index in bytes */

 Number of Records (matches block.count in valid files) */

		/*

		 * Hash calculated from the Records (matches block.hash in

		 * valid files).

	/*

	 * Temporary buffer needed to hold Stream Header, Block Header,

	 * and Stream Footer. The Block Header is the biggest (1 KiB)

	 * so we reserve space according to that. buf[] has to be aligned

	 * to a multiple of four bytes; the size_t variables before it

	 * should guarantee this.

 Sizes of the Check field with different Check IDs */

/*

 * Fill s->temp by copying data starting from b->in[b->in_pos]. Caller

 * must have set s->temp.pos to indicate how much data we are supposed

 * to copy into s->temp.buf. Return true once s->temp.pos has reached

 * s->temp.size.

 Decode a variable-length integer (little-endian base-128 encoding) */

 Don't allow non-minimal encodings. */

/*

 * Decode the Compressed Data field from a Block. Update and validate

 * the observed compressed and uncompressed sizes of the Block so that

 * they don't exceed the values possibly stored in the Block Header

 * (validation assumes that no integer overflow occurs, since vli_type

 * is normally uint64_t). Update the CRC32 if presence of the CRC32

 * field was indicated in Stream Header.

 *

 * Once the decoding is finished, validate that the observed sizes match

 * the sizes possibly stored in the Block Header. Update the hash and

 * Block count, which are later used to validate the Index field.

	/*

	 * There is no need to separately check for VLI_UNKNOWN, since

	 * the observed sizes are always smaller than VLI_UNKNOWN.

 Update the Index size and the CRC32 value. */

/*

 * Decode the Number of Records, Unpadded Size, and Uncompressed Size

 * fields from the Index field. That is, Index Padding and CRC32 are not

 * decoded by this function.

 *

 * This can return XZ_OK (more input needed), XZ_STREAM_END (everything

 * successfully decoded), or XZ_DATA_ERROR (input is corrupt).

			/*

			 * Validate that the Number of Records field

			 * indicates the same number of Records as

			 * there were Blocks in the Stream.

/*

 * Validate that the next four input bytes match the value of s->crc32.

 * s->pos must be zero when starting to validate the first byte.

/*

 * Skip over the Check field when the Check ID is not supported.

 * Returns true once the whole Check field has been skipped over.

 Decode the Stream Header field (the first 12 bytes of the .xz Stream). */

	/*

	 * Of integrity checks, we support only none (Check ID = 0) and

	 * CRC32 (Check ID = 1). However, if XZ_DEC_ANY_CHECK is defined,

	 * we will accept other check types too, but then the check won't

	 * be verified and a warning (XZ_UNSUPPORTED_CHECK) will be given.

 Decode the Stream Footer field (the last 12 bytes of the .xz Stream) */

	/*

	 * Validate Backward Size. Note that we never added the size of the

	 * Index CRC32 field to s->index.size, thus we use s->index.size / 4

	 * instead of s->index.size / 4 - 1.

	/*

	 * Use XZ_STREAM_END instead of XZ_OK to be more convenient

	 * for the caller.

 Decode the Block Header and initialize the filter chain. */

	/*

	 * Validate the CRC32. We know that the temp buffer is at least

	 * eight bytes so this is safe.

	/*

	 * Catch unsupported Block Flags. We support only one or two filters

	 * in the chain, so we catch that with the same test.

 Compressed Size */

 Uncompressed Size */

 If there are two filters, the first one must be a BCJ filter. */

		/*

		 * We don't support custom start offset,

		 * so Size of Properties must be zero.

 Valid Filter Flags always take at least two bytes. */

 Filter ID = LZMA2 */

 Size of Properties = 1-byte Filter Properties */

 Filter Properties contains LZMA2 dictionary size. */

 The rest must be Header Padding. */

	/*

	 * Store the start position for the case when we are in the middle

	 * of the Index field.

			/*

			 * Stream Header is copied to s->temp, and then

			 * decoded from there. This way if the caller

			 * gives us only little input at a time, we can

			 * still keep the Stream Header decoding code

			 * simple. Similar approach is used in many places

			 * in this file.

			/*

			 * If dec_stream_header() returns

			 * XZ_UNSUPPORTED_CHECK, it is still possible

			 * to continue decoding if working in multi-call

			 * mode. Thus, update s->sequence before calling

			 * dec_stream_header().

 We need one byte of input to continue. */

 See if this is the beginning of the Index field. */

			/*

			 * Calculate the size of the Block Header and

			 * prepare to decode it.

			/*

			 * Size of Compressed Data + Block Padding

			 * must be a multiple of four. We don't need

			 * s->block.compressed for anything else

			 * anymore, so we use it here to test the size

			 * of the Block Padding field.

 Finish the CRC32 value and Index size. */

 Compare the hashes to validate the Index field. */

 Never reached */

/*

 * xz_dec_run() is a wrapper for dec_main() to handle some special cases in

 * multi-call and single-call decoding.

 *

 * In multi-call mode, we must return XZ_BUF_ERROR when it seems clear that we

 * are not going to make any progress anymore. This is to prevent the caller

 * from calling us infinitely when the input file is truncated or otherwise

 * corrupt. Since zlib-style API allows that the caller fills the input buffer

 * only when the decoder doesn't produce any new output, we have to be careful

 * to avoid returning XZ_BUF_ERROR too easily: XZ_BUF_ERROR is returned only

 * after the second consecutive call to xz_dec_run() that makes no progress.

 *

 * In single-call mode, if we couldn't decode everything and no error

 * occurred, either the input is truncated or the output buffer is too small.

 * Since we know that the last input byte never produces any output, we know

 * that if all the input was consumed and decoding wasn't finished, the file

 * must be corrupt. Otherwise the output buffer has to be too small or the

 * file is corrupt in a way that decoding it produces too big output.

 *

 * If single-call decoding fails, we reset b->in_pos and b->out_pos back to

 * their original values. This is because with some filter chains there won't

 * be any valid uncompressed data in the output buffer unless the decoding

 * actually succeeds (that's the price to pay of using the output buffer as

 * the workspace).

/*

 * XZ decoder module information

 *

 * Author: Lasse Collin <lasse.collin@tukaani.org>

 *

 * This file has been put into the public domain.

 * You can do whatever you want with this file.

/*

 * This code is in the public domain, but in Linux it's simplest to just

 * say it's GPL and consider the authors as the copyright holders.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * 842 Software Compression

 *

 * Copyright (C) 2015 Dan Streetman, IBM Corp

 *

 * See 842.h for details of the 842 compressed format.

/* By default, we allow compressing input buffers of any length, but we must

 * use the non-standard "short data" template so the decompressor can correctly

 * reproduce the uncompressed data buffer at the right length.  However the

 * hardware 842 compressor will not recognize the "short data" template, and

 * will fail to decompress any compressed buffer containing it (I have no idea

 * why anyone would want to use software to compress and hardware to decompress

 * but that's beside the point).  This parameter forces the compression

 * function to simply reject any input buffer that isn't a multiple of 8 bytes

 * long, instead of using the "short data" template, so that all compressed

 * buffers produced by this function will be decompressable by the 842 hardware

 * decompressor.  Unless you have a specific need for that, leave this disabled

 * so that any length buffer can be compressed.

 params size in bits */

 8 */

 18 */

 25 */

 25 */

 32 */

 33 */

 33 */

 33 */

 33 */

 40 */

 40 */

 40 */

 40 */

 41 */

 41 */

 48 */

 48 */

 48 */

 48 */

 48 */

 48 */

 56 */

 56 */

 56 */

 56 */

 64 */

	/* split this up if writing to > 8 bytes (i.e. n == 64 && p->bit > 0),

	 * or if we're at the end of the output buffer and would write past end

 repeat param is 0-based */

/* update the hashtable entries.

 * only call this after finding/adding the current template

 * the dataN fields for the current 8 byte block must be already updated

/* find the next template to use, and add it

 * the p->dataN fields must already be set for the current 8 byte block

 check up to OPS_MAX - 1; last op is our fallback */

/**

 * sw842_compress

 *

 * Compress the uncompressed buffer of length @ilen at @in to the output buffer

 * @out, using no more than @olen bytes, using the 842 compression format.

 *

 * Returns: 0 on success, error on failure.  The @olen parameter

 * will contain the number of output bytes written on success, or

 * 0 on error.

 if using strict mode, we can only compress a multiple of 8 */

 let's compress at least 8 bytes, mkay? */

 make initial 'last' different so we don't match the first time */

		/* must get the next data, as we need to update the hashtable

		 * entries with the new data every time

		/* we don't care about endianness in last or next;

		 * we're just comparing 8 bytes to another 8 bytes,

		 * they're both the same endianness

 repeat count bits are 0-based, so we stop at +1 */

 reached max repeat bits */

	/*

	 * crc(0:31) is appended to target data starting with the next

	 * bit after End of stream template.

	 * nx842 calculates CRC for data in big-endian format. So doing

	 * same here so that sw842 decompression can be used for both

	 * compressed data.

 pad compressed length to multiple of 8 */

 we were so close! */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * 842 Software Decompression

 *

 * Copyright (C) 2015 Dan Streetman, IBM Corp

 *

 * See 842.h for details of the 842 compressed format.

 rolling fifo sizes */

	/* split this up if reading > 8 bytes, or if we're at the end of

	 * the input buffer and would read past the end

 a ring buffer of fsize is used; correct the offset */

 this is where the current fifo is */

 the current pos in the fifo */

		/* if the offset is past/at the pos, we need to

		 * go back to the last fifo section

/**

 * sw842_decompress

 *

 * Decompress the 842-compressed buffer of length @ilen at @in

 * to the output buffer @out, using no more than @olen bytes.

 *

 * The compressed buffer must be only a single 842-compressed buffer,

 * with the standard format described in the comments in 842.h

 * Processing will stop when the 842 "END" template is detected,

 * not the end of the buffer.

 *

 * Returns: 0 on success, error on failure.  The @olen parameter

 * will contain the number of output bytes written on success, or

 * 0 on error.

 no previous bytes */

 copy rep + 1 */

 use template */

	/*

	 * crc(0:31) is saved in compressed data starting with the

	 * next bit after End of stream template.

	/*

	 * Validate CRC saved in compressed data.

 SPDX-License-Identifier: GPL-2.0+ OR BSD-3-Clause

/*

 * Copyright (c) Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

 Common symbols. zstd_compress must depend on zstd_decompress. */

 Decompression symbols. */

 SPDX-License-Identifier: GPL-2.0+ OR BSD-3-Clause

/*

 * Copyright (c) Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

 0 means unknown in linux zstd API but means 0 in new zstd API */

/* ******************************************************************

 * Common functions of New Generation Entropy library

 * Copyright (c) Yann Collet, Facebook, Inc.

 *

 *  You can contact the author at :

 *  - FSE+HUF source repository : https://github.com/Cyan4973/FiniteStateEntropy

 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/* *************************************

*  Dependencies

 ERR_*, ERROR */

 FSE_MIN_TABLELOG */

 HUF_TABLELOG_ABSOLUTEMAX */

===   Version   ===*/

===   Error Management   ===*/

/*-**************************************************************

*  FSE NCount encoding-decoding

 GCC Intrinsic */

 Software version */

 This function only works when hbSize >= 8 */

 init */

 all symbols not present in NCount have a frequency of 0 */

 extract tableLog */

            /* Count the number of repeats. Each time the

             * 2-bit repeat code is 0b11 there is another

             * repeat.

             * Avoid UB by setting the high bit to 1.

 Add the final repeat which isn't 0b11. */

            /* This is an error, but break and return an error

             * at the end, because returning out of a loop makes

             * it harder for the compiler to optimize.

            /* We don't need to set the normalized count to 0

             * because we already memset the whole buffer to 0.

 For first condition to work */

 extra accuracy */

            /* When it matters (small blocks), this is a

             * predictable branch, because we don't use -1.

                /* This branch can be folded into the

                 * threshold update condition because we

                 * know that threshold > 1.

 Only possible when there are too many zeros. */

 Avoids the FORCE_INLINE of the _body() function. */

 bmi2 */ 0);

/*! HUF_readStats() :

    Read compact Huffman tree, saved by HUF_writeCTable().

    `huffWeight` is destination buffer.

    `rankStats` is assumed to be a table of at least HUF_TABLELOG_MAX U32.

    @return : size read from `src` , or an error Code .

    Note : Needed by HUF_readCTable() and HUF_readDTableX?() .

 bmi2 */ 0);

* is not necessary, even though some analyzer complain ... */
 is not necessary, even though some analyzer complain ... */

 special header */

 header compressed with FSE (normal case) */

 max (hwSize-1) values decoded, as last one is implied */

 collect weight stats */

 get last non-null symbol weight (implied, total must be 2^n) */

 determine last weight */

 last value must be a clean power of 2 */

 check tree construction validity */

 by construction : at least 2 elts of rank 1, must be even */

 results */

 Avoids the FORCE_INLINE of the _body() function. */

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/*-*************************************

*  Dependencies

 ZSTD_malloc, ZSTD_calloc, ZSTD_free, ZSTD_memset */

/*-****************************************

*  Version

/*-****************************************

*  ZSTD Error Management

 defined within zstd_internal.h */

/*! ZSTD_isError() :

 *  tells if a return value is an error code

/*! ZSTD_getErrorName() :

/*! ZSTD_getError() :

/*! ZSTD_getErrorString() :

/*=**************************************************************

*  Custom allocator

        /* calloc implemented as malloc+memset;

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

 The purpose of this file is to have a single list of error strings embedded in binary */

 following error codes are not stable and may be removed or changed in a future version */

/* ******************************************************************

 * FSE : Finite State Entropy decoder

 * Copyright (c) Yann Collet, Facebook, Inc.

 *

 *  You can contact the author at :

 *  - FSE source repository : https://github.com/Cyan4973/FiniteStateEntropy

 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/* **************************************************************

*  Includes

 assert */

/* **************************************************************

*  Error Management

 use only *after* variable declarations */

/* **************************************************************

*  Templates

/*

  designed to be included

  for type-specific functions (template emulation in C)

  Objective is to write these functions only once, for improved maintenance

 safety checks */

 Function names */

 Function templates */

 because *dt is unsigned, 32-bits aligned on 32-bits */

 Sanity Checks */

 Init, lay down lowprob symbols */

 Spread symbols */

        /* First lay down the symbols in order.

         * We use a uint64_t to lay down 8 bytes at a time. This reduces branch

         * misses since small blocks generally have small table logs, so nearly

         * all symbols have counts <= 8. We ensure we have 8 bytes at the end of

         * our buffer to handle the over-write.

        /* Now we spread those positions across the table.

         * The benefit of doing it in two stages is that we avoid the the

         * variable size inner loop, which caused lots of branch misses.

         * Now we can run through all the positions without any branch misses.

         * We unroll the loop twice, since that is what emperically worked best.

 FSE_MIN_TABLELOG is 5 */

 lowprob area */

 position must reach all cells once, otherwise normalizedCounter is incorrect */

 Build Decoding table */

/*-*******************************************************

*  Decompression (Byte symbols)

 Sanity checks */

 min size */

 Build Decoding Table */

 Init */

 4 symbols per loop */

 This test must be static */

 This test must be static */

 This test must be static */

 tail */

 note : BIT_reloadDStream(&bitD) >= FSE_DStream_partiallyFilled; Ends at exactly BIT_DStream_completed */

 select fast mode (static) */

 bmi2 */ 0);

 Dynamically sized */

 normal FSE decoding mode */

 select fast mode (static) */

 Avoids the FORCE_INLINE of the _body() function. */

 FSE_COMMONDEFS_ONLY */

/* ******************************************************************

 * debug

 * Part of FSE library

 * Copyright (c) Yann Collet, Facebook, Inc.

 *

 * You can contact the author at :

 * - Source repository : https://github.com/Cyan4973/FiniteStateEntropy

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/*

 * This module only hosts one global variable

 * which can be used to dynamically influence the verbosity of traces,

 * such as DEBUGLOG and RAWLOG

/* ******************************************************************

 * FSE : Finite State Entropy encoder

 * Copyright (c) Yann Collet, Facebook, Inc.

 *

 *  You can contact the author at :

 *  - FSE source repository : https://github.com/Cyan4973/FiniteStateEntropy

 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/* **************************************************************

*  Includes

 U32, U16, etc. */

 assert, DEBUGLOG */

 HIST_count_wksp */

 ZSTD_malloc, ZSTD_free, ZSTD_memcpy, ZSTD_memset */

/* **************************************************************

*  Error Management

/* **************************************************************

*  Templates

/*

  designed to be included

  for type-specific functions (template emulation in C)

  Objective is to write these functions only once, for improved maintenance

 safety checks */

 Function names */

 Function templates */

/* FSE_buildCTable_wksp() :

 * Same as FSE_buildCTable(), but using an externally allocated scratch buffer (`workSpace`).

 * wkspSize should be sized to handle worst case situation, which is `1<<max_tableLog * sizeof(FSE_FUNCTION_TYPE)`

 * workSpace must also be properly aligned with FSE_FUNCTION_TYPE requirements

 header */ + (tableLog ? tableSize>>1 : 1) ;

 Must be 4 byte aligned */

 CTable header */

 required for threshold strategy to work */

    /* For explanations on how to distribute symbol values over the table :

fastcompression.blogspot.fr/2014/02/fse-distributing-symbol-values.html */

 useless initialization, just to keep scan-build happy */

 symbol start positions */

 Low proba symbol */

 Spread symbols */

 Low proba area */

 Must have initialized all positions */

 Build table */

 note : static analyzer may not understand tableSymbol is properly initialized */

 TableU16 : sorted by symbol order; gives next state value */

 Build Symbol Transformation Table */

 filling nonetheless, for compatibility with FSE_getMaxNbBits() */

 debug : symbol costs */

/*-**************************************************************

*  FSE NCount encoding

 maxSymbolValue==0 ? use default */

 Table Size */

 Init */

 +1 for extra accuracy */

 stops at 1 */

 incorrect distribution */

 Buffer overflow */

 Buffer overflow */

 +1 for extra accuracy */

 [0..max[ [max..threshold[ (...) [threshold+max 2*threshold[ */

 Buffer overflow */

 incorrect normalized distribution */

 flush remaining bitStream */

 Buffer overflow */

 Unsupported */

 Unsupported */

 write in buffer is safe */);

/*-**************************************************************

*  FSE Compression Code

 provides the minimum logSize to safely represent a distribution */

 Not supported, RLE should be used instead */

 Not supported, RLE should be used instead */

 Accuracy can be reduced */

 Need a minimum to safely represent all symbol values */

/* Secondary normalization method.

 Init */

 risk of rounding to zero */

        /* all values are pretty poor;

           probably incompressible data (should have already been detected);

 all of the symbols were low enough for the lowOne or lowThreshold */

 scale on remaining */

 Sanity checks */

 Unsupported size */

 Unsupported size */

 Too small tableLog, compression potentially impossible */

 <== here, one division ! */

 rle special case */

 corner case, need another normalization method */

 Print Table (debug) */

 fake FSE_CTable, for raw (uncompressed) input */

 header */ + (tableSize>>1);   
 Sanity checks */

 min size */

 header */

 Build table */

 Build Symbol Transformation Table */

 fake FSE_CTable, for rle input (always same symbol) */

 header */

 Build table */

 just in case */

 Build Symbol Transformation Table */

 init */

 not enough space available to write a bitstream */ }

 join to mod 4 */

 test bit 2 */

 2 or 4 encoding per loop */

 this test must be static */

 this test must be static */

 FSE_COMMONDEFS_ONLY */

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

 /*-*************************************

 *  Dependencies

/*

 * -log2(x / 256) lookup table for x in [0, 256).

 * If x == 0: Return 0

 * Else: Return floor(-log2(x / 256) * 256)

/*

 * Returns true if we should use ncount=-1 else we should

 * use ncount=1 for low probability symbols instead.

    /* Heuristic: This should cover most blocks <= 16K and

     * start to fade out after 16K to about 32K depending on

     * comprssibility.

/*

 * Returns the cost in bytes of encoding the normalized count header.

 * Returns an error if any of the helper functions return an error.

/*

 * Returns the cost in bits of encoding the distribution described by count

 * using the entropy bound.

/*

 * Returns the cost in bits of encoding the distribution in count using ctable.

 * Returns an error if ctable cannot represent all the symbols in count.

/*

 * Returns the cost in bits of encoding the distribution in count using the

 * table described by norm. The max symbol support by norm is assumed >= max.

 * norm must be valid for every symbol with non-zero probability in count.

            /* Prefer set_basic over set_rle when there are 2 or less symbols,

             * since RLE uses 1 byte, but set_basic uses 5-6 bits per symbol.

             * If basic encoding isn't possible, always choose RLE.

 28-36 for offset, 56-72 for lengths */

 xx_DEFAULTNORMLOG */

                /* The format allows default tables to be repeated, but it isn't useful.

                 * When using simple heuristics to select encoding type, we don't want

                 * to confuse these tables with dictionaries. When running more careful

                 * analysis, we don't need to waste time checking both repeating tables

                 * and default tables.

 note : could be pre-calculated */

 overflow protected */

 first symbols */

 intentional underflow */

 32b*/  
 (7)*/  
 15 */  
 24 */  
 (7)*/

 16 */  
 (7)*/

 (7)*/

 31 */

 31 */

 (7)*/

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

 /*-*************************************

 *  Dependencies

 ZSTD_getSequenceLength */

 HIST_countFast_wksp */

/*-*************************************

*  Superblock entropy buffer structs

/* ZSTD_hufCTablesMetadata_t :

 *  Stores Literals Block Type for a super-block in hType, and

 *  huffman tree description in hufDesBuffer.

 *  hufDesSize refers to the size of huffman tree description in bytes.

/* ZSTD_fseCTablesMetadata_t :

 *  Stores symbol compression modes for a super-block in {ll, ol, ml}Type, and

 *  fse tables in fseTablesBuffer.

 *  fseTablesSize refers to the size of fse tables in bytes.

 This is to account for bug in 1.3.4. More detail in ZSTD_compressSubBlock_sequences() */

/* ZSTD_buildSuperBlockEntropy_literal() :

 *  Builds entropy for the super-block literals.

 *  Stores literals block type (raw, rle, compressed, repeat) and

 *  huffman description table to hufMetadata.

 Prepare nextEntropy assuming reusing the existing table */

 small ? don't even attempt compression (speed opt) */

 Scan input and build symbol stats */

 Validate the previous Huffman table */

 Build Huffman Tree */

 Build and write the CTable */

 Check against repeating the previous CTable */

/* ZSTD_buildSuperBlockEntropy_sequences() :

 *  Builds entropy for the super-block sequences.

 *  Stores symbol compression modes and fse table to fseMetadata.

 convert length/distances into codes */

 build CTable for Literal Lengths */

 can't fail */

 We don't copy tables */

 build CTable for Offsets */

 can't fail */

 We can only use the basic table if max <= DefaultMaxOff, otherwise the offsets are too large */

 We don't copy tables */

 build CTable for MatchLengths */

 can't fail */

 We don't copy tables */

/* ZSTD_buildSuperBlockEntropy() :

 *  Builds entropy for the super-block.

/* ZSTD_compressSubBlock_literal() :

 *  Compresses literals section for a sub-block.

 *  When we have to write the Huffman table we will sometimes choose a header

 *  size larger than necessary. This is because we have to pick the header size

 *  before we know the table size + compressed size, so we have a bound on the

 *  table size. If we guessed incorrectly, we fall back to uncompressed literals.

 *

 *  We write the header when writeEntropy=1 and set entropyWritten=1 when we succeeded

 *  in writing the header, otherwise it is set to 0.

 *

 *  hufMetadata->hType has literals block type info.

 *      If it is set_basic, all sub-blocks literals section will be Raw_Literals_Block.

 *      If it is set_rle, all sub-blocks literals section will be RLE_Literals_Block.

 *      If it is set_compressed, first sub-block's literals section will be Compressed_Literals_Block

 *      If it is set_compressed, first sub-block's literals section will be Treeless_Literals_Block

 *      and the following sub-blocks' literals sections will be Treeless_Literals_Block.

 *  @return : compressed size of literals section of a sub-block

 *            Or 0 if it unable to compress.

 TODO bmi2... */

 TODO bmi2 */

 If we expand and we aren't writing a header then emit uncompressed */

 If we are writing headers then allow expansion that doesn't change our header size. */

 Build header */

 2 - 2 - 10 - 10 */

 2 - 2 - 14 - 14 */

 2 - 2 - 18 - 18 */

 not possible : lhSize is {3,4,5} */

 Only used by assert(), suppress unused variable warnings in production. */

/* ZSTD_compressSubBlock_sequences() :

 *  Compresses sequences section for a sub-block.

 *  fseMetadata->llType, fseMetadata->ofType, and fseMetadata->mlType have

 *  symbol compression modes for the super-block.

 *  The first successfully compressed block will have these in its header.

 *  We set entropyWritten=1 when we succeed in compressing the sequences.

 *  The following sub-blocks will always have repeat mode.

 *  @return : compressed size of sequences section of a sub-block

 *            Or 0 if it is unable to compress

 Sequences Header */

max nbSeq Size*/ + 1 
 seqHead : flags for FSE encoding type */

        /* zstd versions <= 1.3.4 mistakenly report corruption when

         * FSE_readNCount() receives a buffer < 4 bytes.

         * Fixed by https://github.com/facebook/zstd/pull/1146.

         * This can happen when the last set_compressed table present is 2

         * bytes and the bitstream is only one byte.

         * In this exceedingly rare case, we will simply emit an uncompressed

         * block, since it isn't worth optimizing.

 NCountSize >= 2 && bitstreamSize > 0 ==> lastCountSize == 3 */

    /* zstd versions <= 1.4.0 mistakenly report error when

     * sequences section body size is less than 3 bytes.

     * Fixed by https://github.com/facebook/zstd/pull/1664.

     * This can happen when the previous sequences section block is compressed

     * with rle mode and the current block's sequences section is compressed

     * with repeat mode where sequences section body size can be 1 byte.

/* ZSTD_compressSubBlock() :

 *  Compresses a single sub-block.

 *  @return : compressed size of the sub-block

 Write block header */

 Use hard coded size of 3 bytes */

 impossible */

 can't fail */

 We selected this encoding type, so it must be valid. */

 for offset, offset code is also the number of additional bits */

 Use hard coded size of 3 bytes */

/* ZSTD_compressSubBlock_multi() :

 *  Breaks super-block into multiple sub-blocks and compresses them.

 *  Entropy will be written to the first block.

 *  The following blocks will use repeat mode to compress.

 *  All sub-blocks are compressed blocks (no raw or rle blocks).

 *  @return : compressed size of the super block (which is multiple ZSTD blocks)

        /* I think there is an optimization opportunity here.

         * Calling ZSTD_estimateSubBlockSize for every sequence can be wasteful

         * since it recalculates estimate from scratch.

         * For example, it would recount literal distribution and symbol codes everytime.

 Entropy only needs to be written once */

        /* If we haven't written our entropy tables, then we've violated our contract and

         * must emit an uncompressed block.

 We have to regenerate the repcodes because we've skipped some sequences */

 statically allocated in resetCCtx */), "");

 statically allocated in resetCCtx */);

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

    /* Always insert every fastHashFillStep position into the hash tables.

     * Insert the other positions into the large hash table if their entry

     * is empty.

 Only load extra positions for ZSTD_dtlm_full */

 template */, ZSTD_dictMode_e const dictMode)

 presumes that, if there is a dictionary, it must be using Attach mode */

 if a dictionary is attached, it must be within window range */

 init */

        /* dictMatchState repCode checks don't currently handle repCode == 0

 Main Search Loop */

 < instead of <=, because repcode check at (ip+1) */

 update hash tables */

 check dictMatchState repcode */

 intentional underflow */)

 check noDict repcode */

 check prefix long match */

 catch up */

 check dictMatchState long match */

 catch up */

 check prefix short match */

 check dictMatchState short match */

 check prefix long +1 match */

 catch up */

 check dict long +1 match */

 catch up */

 if no long +1 match, explore the short match we found */

 catch up */

 catch up */

 match found */

 Complementary insertion */

 done after iLimit test, as candidates could be > iend-8 */

 check immediate repcode */

 intentional overflow */)

 swap offset_2 <=> offset_1 */

 store sequence */

 swap offset_2 <=> offset_1 */

 faster when present ... (?) */

 while (ip < ilimit) */

 save reps for next block */

 Return the last literals size */

 includes case 3 */

 includes case 3 */

 template */)

 if extDict is invalidated due to maxDistance, switch to "regular" variant */

 Search Loop */

 < instead of <=, because (ip+1) */

 offset_1 expected <= curr +1 */

 update hash table */

 intentional underflow : ensure repIndex doesn't overlap dict + prefix */

 catch up */

 catch up */

 catch up */

 move to next sequence start */

 Complementary insertion */

 done after iLimit test, as candidates could be > iend-8 */

 check immediate repcode */

 intentional overflow : ensure repIndex2 doesn't overlap dict + prefix */

 swap offset_2 <=> offset_1 */

 save reps for next block */

 Return the last literals size */

 includes case 3 */

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

 ZSTD_hashPtr, ZSTD_count, ZSTD_storeSeq */

    /* Always insert every fastHashFillStep position into the hash table.

     * Insert the other positions if their hash entry is empty.

 Only load extra positions for ZSTD_dtlm_full */

 not yet filled */

 support stepSize of 0 */

 We check ip0 (ip + 0) and ip1 (ip + 1) each loop */

 init */

 Main Search Loop */

    /* From intel 'The vector pragma indicates that the loop should be

     * vectorized if it is legal to do so'. Can be used together with

     * #pragma ivdep (but have opted to exclude that because intel

 < instead of <=, because check at ip0+2 */

 update hash table */

 update hash table */

 found a regular match */

 found a regular match after one literal */

 Requires: ip0, match0 */

 Compute the offset code */

 Count the backwards match length */

 catch up */

 Requires: ip0, match0, offcode */

 Count the forward length */

 match found */

 Fill Table */

 check base overflow */

 here because current+2 could be > iend-8 */

 offset_2==0 means offset_2 is invalidated */

 store sequence */

 swap offset_2 <=> offset_1 */

litLen*/, anchor, iend, 0 
 faster when present (confirmed on gcc-8) ... (?) */

 save reps for next block */

 Return the last literals size */

 includes case 3 */

 support stepSize of 0 */

    /* if a dictionary is still attached, it necessarily means that

 these variables are not used when assert() is disabled */

    /* ensure there will be no underflow

 init */

    /* dictMatchState repCode checks don't currently handle repCode == 0

 Main Search Loop */

 < instead of <=, because repcode check at (ip+1) */

 update hash table */

 intentional underflow : ensure repIndex isn't overlapping dict + prefix */

 found a dict match */

 catch up */

 it's not a match, and we're not going to check the dictionary */

 found a regular match */

 catch up */

 match found */

 Fill Table */

 check base overflow */

 here because curr+2 could be > iend-8 */

 check immediate repcode */

 intentional overflow */)

 swap offset_2 <=> offset_1 */

 save reps for next block */

 Return the last literals size */

 includes case 3 */

 support stepSize of 0 */

 switch to "regular" variant if extDict is invalidated due to maxDistance */

 Search Loop */

 < instead of <=, because (ip+1) */

 update hash table */

 check repIndex */

 intentional underflow */ & (repIndex > dictStartIndex))

 catch up */

 update offset history */

 Fill Table */

 check immediate repcode */

 intentional overflow */

 swap offset_2 <=> offset_1 */

litlen*/, anchor, iend, 0 
 save reps for next block */

 Return the last literals size */

 includes case 3 */

/* ******************************************************************

 * Huffman encoder, part of New Generation Entropy library

 * Copyright (c) Yann Collet, Facebook, Inc.

 *

 *  You can contact the author at :

 *  - FSE+HUF source repository : https://github.com/Cyan4973/FiniteStateEntropy

 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/* **************************************************************

*  Compiler specifics

/* **************************************************************

*  Includes

 ZSTD_memcpy, ZSTD_memset */

 FSE_optimalTableLog_internal */

 header compression */

/* **************************************************************

*  Error Management

 use only *after* variable declarations */

/* **************************************************************

*  Utils

/* *******************************************************

*  HUF : Huffman block compression

/* HUF_compressWeights() :

 * Same as FSE_compress(), but dedicated to huff0's weights compression.

 * The use case needs much less stack memory.

 * Note : all elements within weightTable are supposed to be <= HUF_TABLELOG_MAX.

 init conditions */

 Not compressible */

 Scan input and build symbol stats */

 never fails */

 only a single symbol in src : rle */

 each symbol present maximum once => not compressible */

 useLowProbCount */ 0) );

 Write table description header */

 Compress */

 not enough space for compressed data */

 precomputed conversion table */

 check conditions */

 convert to weight */

 attempt weights compression by FSE */

 FSE compressed */

 write raw values as 4-bits (max : 15) */

 should not happen : likely means source cannot be compressed */

 not enough space within dst buffer */

special case*/ + (maxSymbolValue-1));

 to be sure it doesn't cause msan issue in final combination */

/*! HUF_writeCTable() :

    `CTable` : Huffman tree to save, using huf representation.

 init not required, even though some static analyzer may complain */

 large enough for values from 0 to 16 */

 get symbol weights */

 check result */

 Prepare base value per rank */

 fill nbBits */

 fill val */

 support w=0=>n=tableLog+1 */

 determine stating value per rank */

 for w==0 */

 start at n=tablelog <-> w=1 */

 get starting value within each rank */

 assign value within rank, symbol order */

/*

 * HUF_setMaxHeight():

 * Enforces maxNbBits on the Huffman tree described in huffNode.

 *

 * It sets all nodes with nbBits > maxNbBits to be maxNbBits. Then it adjusts

 * the tree to so that it is a valid canonical Huffman tree.

 *

 * @pre               The sum of the ranks of each symbol == 2^largestBits,

 *                    where largestBits == huffNode[lastNonNull].nbBits.

 * @post              The sum of the ranks of each symbol == 2^largestBits,

 *                    where largestBits is the return value <= maxNbBits.

 *

 * @param huffNode    The Huffman tree modified in place to enforce maxNbBits.

 * @param lastNonNull The symbol with the lowest count in the Huffman tree.

 * @param maxNbBits   The maximum allowed number of bits, which the Huffman tree

 *                    may not respect. After this function the Huffman tree will

 *                    respect maxNbBits.

 * @return            The maximum number of bits of the Huffman tree after adjustment,

 *                    necessarily no more than maxNbBits.

 early exit : no elt > maxNbBits, so the tree is already valid. */

 there are several too large elements (at least >= 2) */

        /* Adjust any ranks > maxNbBits to maxNbBits.

         * Compute totalCost, which is how far the sum of the ranks is

         * we are over 2^largestBits after adjust the offending ranks.

 n stops at huffNode[n].nbBits <= maxNbBits */

 n end at index of smallest symbol using < maxNbBits */

        /* renorm totalCost from 2^largestBits to 2^maxNbBits

 repay normalized cost */

 Get pos of last (smallest = lowest cum. count) symbol per rank */

 < maxNbBits */

                /* Try to reduce the next power of 2 above totalCost because we

                 * gain back half the rank.

                    /* Decrease highPos if no symbols of lowPos or if it is

                     * not cheaper to remove 2 lowPos than highPos.

 only triggered when no more rank 1 symbol left => find closest one (note : there is necessarily at least one !) */

 HUF_MAX_TABLELOG test just to please gcc 5+; but it should not be necessary */

 Increase the number of bits to gain back half the rank cost. */

                /* Fix up the new rank.

                 * If the new rank was empty, this symbol is now its smallest.

                 * Otherwise, this symbol will be the largest in the new rank so no adjustment.

                /* Fix up the old rank.

                 * If the symbol was at position 0, meaning it was the highest weight symbol in the tree,

                 * it must be the only symbol in its rank, so the old rank now has no symbols.

                 * Otherwise, since the Huffman nodes are sorted by count, the previous position is now

                 * the smallest node in the rank. If the previous position belongs to a different rank,

                 * then the rank is now empty.

 special case, reached largest symbol */

 this rank is now empty */

 while (totalCost > 0) */

            /* If we've removed too much weight, then we have to add it back.

             * To avoid overshooting again, we only adjust the smallest rank.

             * We take the largest nodes from the lowest rank 0 and move them

             * to rank 1. There's guaranteed to be enough rank 0 symbols because

             * TODO.

 Sometimes, cost correction overshoot */

                /* special case : no rank 1 symbol (using maxNbBits-1);

                 * let's create one from largest rank 0 (using maxNbBits).

 repay normalized cost */

 there are several too large elements (at least >= 2) */

/*

 * HUF_sort():

 * Sorts the symbols [0, maxSymbolValue] by count[symbol] in decreasing order.

 *

 * @param[out] huffNode       Sorted symbols by decreasing count. Only members `.count` and `.byte` are filled.

 *                            Must have (maxSymbolValue + 1) entries.

 * @param[in]  count          Histogram of the symbols.

 * @param[in]  maxSymbolValue Maximum symbol value.

 * @param      rankPosition   This is a scratch workspace. Must have RANK_POSITION_TABLE_SIZE entries.

    /* Compute base and set curr to base.

     * For symbol s let lowerRank = BIT_highbit32(count[n]+1) and rank = lowerRank + 1.

     * Then 2^lowerRank <= count[n]+1 <= 2^rank.

     * We attribute each symbol to lowerRank's base value, because we want to know where

     * each rank begins in the output, so for rank R we want to count ranks R+1 and above.

 Sort */

        /* Insert into the correct position in the rank.

         * We have at most 256 symbols, so this insertion should be fine.

/* HUF_buildCTable_wksp() :

 *  Same as HUF_buildCTable(), but using externally allocated scratch buffer.

 *  `workSpace` must be aligned on 4-bytes boundaries, and be at least as large as sizeof(HUF_buildCTable_wksp_tables).

/* HUF_buildTree():

 * Takes the huffNode array sorted by HUF_sort() and builds an unlimited-depth Huffman tree.

 *

 * @param huffNode        The array sorted by HUF_sort(). Builds the Huffman tree in this array.

 * @param maxSymbolValue  The maximum symbol value.

 * @return                The smallest node in the Huffman tree (by count).

 init for parents */

 fake entry, strong barrier */

 create parents */

 distribute weights (unlimited tree height) */

/*

 * HUF_buildCTableFromTree():

 * Build the CTable given the Huffman tree in huffNode.

 *

 * @param[out] CTable         The output Huffman CTable.

 * @param      huffNode       The Huffman tree.

 * @param      nonNullRank    The last and smallest node in the Huffman tree.

 * @param      maxSymbolValue The maximum symbol value.

 * @param      maxNbBits      The exact maximum number of bits used in the Huffman tree.

 fill result into ctable (val, nbBits) */

 determine starting value per rank */

 get starting value within each rank */

 push nbBits per symbol, symbol order */

 assign value within rank, symbol order */

 safety checks */

 must be aligned on 4-bytes boundaries */

 sort, decreasing order */

 build tree */

 enforce maxTableLog */

 check fit into table */

 init */

 not enough space to compress */

 join to mod 4 */

 note : n&3==0 at this stage */

 bmi2 */ 0);

 first 3 segments */

 minimum space to compress successfully */

 no saving possible : too small input */

 jumpTable */

 bmi2 */ 0);

 uncompressible */

 check compressibility */

/* HUF_compress_internal() :

 * `workSpace_align4` must be aligned on 4-bytes boundaries,

 must be aligned on 4-bytes boundaries */

 checks & inits */

 Uncompressed */

 cannot fit anything within dst budget */

 current block size limit */

 Heuristic : If old table is valid, use it for small inputs */

 Scan input and build symbol stats */

 single symbol, rle */

 heuristic : probably not compressible enough */

 Check validity of previous table */

 Heuristic : use existing table for small inputs */

 Build Huffman Tree */

 Zero unused symbols in CTable, so we can check it for validity */

 Write table description header */

 Check if using previous huffman table is beneficial */

 Use the new huffman table */

 Save new table */

bmi2*/);

/* HUF_compress4X_repeat():

 * compress input using 4 streams.

bmi2*/);

/* HUF_compress4X_repeat():

 * compress input using 4 streams.

/*

 * Copyright (c) Przemyslaw Skibinski, Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/*

 * Disable inlining for the optimal parser for the kernel build.

 * It is unlikely to be used in the kernel, and where it is used

 * latency shouldn't matter because it is very slow to begin with.

 * We prefer a ~180KB binary size win over faster optimal parsing.

 *

 * TODO(https://github.com/facebook/zstd/issues/2862):

 * Improve the code size of the optimal parser in general, so we

 * don't need this hack for the kernel build.

 scaling factor for litFreq, so that frequencies adapt faster to new stats */

 log factor when using previous stats to init next stats */

 if srcSize < ZSTD_PREDEF_THRESHOLD, symbols' cost is assumed static, directly determined by pre-defined distributions */

/*-*************************************

*  Price functions for optimal parser

 approximation at bit level */

 fractional bit accuracy */

 opt==approx, ultra==accurate */

/* debugging function,

 * @return price in bytes as fractional value

/* ZSTD_downscaleStat() :

 * reduce all elements in table by a factor 2^(ZSTD_FREQ_DIV+malus)

/* ZSTD_rescaleFreqs() :

 * if first block (detected by optPtr->litLengthSum == 0) : init statistics

 *    take hints from dictionary if there is one

 *    or init from zero, using src for literals stats, or flat 1 for match symbols

 * otherwise downscale existing stats, to be used as seed for next block.

 first block : init */

 heuristic */

 huffman table presumed generated by dictionary */

 scale to 2K */

minimum to calculate cost*/;

 scale to 1K */

minimum to calculate cost*/;

minimum to calculate cost*/;

minimum to calculate cost*/;

 not a dictionary */

 use raw first block to init statistics */

 new block : re-use previous statistics, scaled down */

/* ZSTD_rawLiteralsCost() :

 * price of literals (only) in specified segment (which length can be 0).

 Uncompressed - 8 bytes per literal. */

 6 bit per literal - no statistic used */

 dynamic statistics */

 literal cost should never be negative */

/* ZSTD_litLengthPrice() :

 dynamic statistics */

/* ZSTD_getMatchPrice() :

 * Provides the cost of the match part (offset + matchLength) of a sequence

 * Must be combined with ZSTD_fullLiteralsCost() to get the full cost of a sequence.

 fixed scheme, do not use statistics */

 dynamic statistics */

static*/ && offCode >= 20)

 handicap for long distance offsets, favor decompression speed */

 match Length */

 heuristic : make matches a bit more costly to favor less sequences -> faster decompression speed */

/* ZSTD_updateStats() :

 literals */

 literal Length */

 match offset code (0-2=>repCode; 3+=>offset+2) */

 match Length */

/* ZSTD_readMINMATCH() :

 * function safe only for comparisons

/* Update hashTable3 up to ip (excluded)

/*-*************************************

*  Binary Tree search

/* ZSTD_insertBt1() : add one or multiple positions to tree.

 *  ip : assumed <= iend-8 .

 to be nullified at the end */

 ZSTD_C_PREDICT */

 required for h calculation */

 Update Hash Table */

 guaranteed minimum nb of common bytes */

 note : can create issues when hlog small <= 11 */

 written this way, as bt is a roll buffer */

 no need to check length, result known */

 beyond tree size, stop the search */

 new "smaller" => larger of match */

 new matchIndex larger than previous (closer to current) */

 beyond tree size, stop the search */

 might be wrong if actually extDict */

 to prepare for next usage of match[matchLength] */

 equal : no way to know if inf or sup */

 drop , to guarantee consistency ; miss a bit of compression, but other solutions can corrupt tree */

 necessarily within buffer */

 match is smaller than current */

 update smaller idx */

 all smaller will now have at least this guaranteed common length */

 beyond tree size, stop searching */

 new "candidate" => larger than match, which was smaller than target */

 new matchIndex, larger than previous and closer to current */

 match is larger than current */

 beyond tree size, stop searching */

 speed optimization */

 store result (found matches) in this table (presumed large enough) */

 tells if associated literal length is 0 or not. This value must be 0 or 1 */

 template */)

 farthest referenced position of any match => detects repetitive patterns */

 to be nullified at the end */

 check repCode */

 necessarily 1 or 0 */

 intentional overflow, discards 0 and -1 */ < curr-dictLimit) {  
                /* We must validate the repcode offset because when we're using a dictionary the

                 * valid offset range shrinks when the dictionary goes out of bounds.

 repIndex < dictLimit || repIndex >= curr */

intentional overflow*/ < curr - windowLow)  
 intentional overflow : do not test positions overlapping 2 memory segments */)

intentional overflow*/ < curr - (dmsLowLimit + dmsIndexDelta))  
 intentional overflow : do not test positions overlapping 2 memory segments */

 save longer solution */

 best possible */

 HC3 match finder */

static*/ && (bestLength < mls)) {

heuristic : longer distance likely too expensive*/ ) {

static*/ || (dictMode == ZSTD_dictMatchState) 
 save best solution */

 == 3 > bestLength */) {

 no prior solution */

 best possible length */

 skip insertion */

 no dictMatchState lookup: dicts don't have a populated HC3 table */

 Update Hash Table */

 guaranteed minimum nb of common bytes */

 ensure the condition is correct when !extDict */

 ensure early section of match is equal as expected */

 ensure early section of match is equal as expected */

 prepare for match[matchLength] read */

 equal : no way to know if inf or sup */) {

 break should also skip searching dms */

 drop, to preserve bt consistency (miss a little bit of compression) */

 match smaller than current */

 update smaller idx */

 all smaller will now have at least this guaranteed common length */

 beyond tree size, stop the search */

 new candidate => larger than match, which was smaller than current */

 new matchIndex, larger than previous, closer to current */

 beyond tree size, stop the search */

 Check we haven't underflowed. */

 guaranteed minimum nb of common bytes */

 to prepare for next usage of match[matchLength] */

 equal : no way to know if inf or sup */) {

 drop, to guarantee consistency (miss a little bit of compression) */

 beyond tree size, stop the search */

 all smaller will now have at least this guaranteed common length */

 new matchIndex larger than previous (closer to current) */

 match is larger than current */

 skip repetitive patterns */

 store result (match found, increasing size) in this table */

 skipped area */

/* ***********************

*  LDM helper functions  *

 Struct containing info needed to make decision about ldm inclusion */

 External match candidates store for this block */

 Start position of the current match candidate */

 End position of the current match candidate */

 Offset of the match candidate */

/* ZSTD_optLdm_skipRawSeqStoreBytes():

 * Moves forward in rawSeqStore by nbBytes, which will update the fields 'pos' and 'posInSequence'.

/* ZSTD_opt_getNextMatchAndUpdateSeqStore():

 * Calculates the beginning and end of the next match in the current block.

 * Updates 'pos' and 'posInSequence' of the ldmSeqStore.

 Setting match end position to MAX to ensure we never use an LDM during this block */

    /* Calculate appropriate bytes left in matchLength and litLength after adjusting

 If there are more literal bytes than bytes remaining in block, no ldm is possible */

    /* Matches may be < MINMATCH by this process. In that case, we will reject them

 Match ends after the block ends, we can't use the whole match */

 Consume nb of bytes equal to size of sequence left */

/* ZSTD_optLdm_maybeAddMatch():

 * Adds a match if it's long enough, based on it's 'matchStartPosInBlock'

 * and 'matchEndPosInBlock', into 'matches'. Maintains the correct ordering of 'matches'

 Note: ZSTD_match_t actually contains offCode and matchLength (before subtracting MINMATCH) */

 Ensure that current block position is not outside of the match */

/* ZSTD_optLdm_processMatchCandidate():

 * Wrapper function to update ldm seq store and call ldm functions as necessary.

            /* The position at which ZSTD_optLdm_processMatchCandidate() is called is not necessarily

             * at the end of a match from the ldm seq store, and will often be some bytes

             * over beyond matchEndPosInBlock. As such, we need to correct for these "overshoots"

/*-*******************************

*  Optimal parser

 debug */

 RAWLOG(2, "%3i:%3i,  ", enb, table[enb]); */

 init */

 Match Loop */

 find first match */

 initialize opt[0] */

 means is_a_literal */

            /* We don't need to include the actual price of the literals because

             * it is static for the duration of the forward pass, and is included

             * in every price. We include the literal length to avoid negative

             * prices when we subtract the previous literal length.

 large match -> immediate encoding */

 set prices for first matches starting position == 0 */

 mlen, litlen and price will be fixed during forward scanning */

 check further positions */

 Fix current position with one literal if cheaper */

 overflow check */

            /* Set the repcodes of the current position. We must do it here

             * because we rely on the repcodes of the 2nd to last sequence being

             * correct to set the next chunks repcodes during the backward

             * traversal.

 last match must start at a minimum distance of 8 from oend */

static_test*/

 skip unpromising positions; about ~+6% speed, -0.01 ratio */

 last sequence is actually only literals, fix cur to last match - note : may underflow, in which case, it's first sequence, and it's okay */

 underflow => first match */

 set prices using matches found at position == cur */

 scan downward */

 fill empty positions */

 early update abort; gets ~+10% speed for about -0.01 ratio loss */

 for (cur = 1; cur <= last_pos; cur++) */

 single sequence, and it starts before `ip` */

 control overflow*/

 cur, last_pos, best_mlen, best_off have to be set */

        /* Set the next chunk's repcodes based on the repcodes of the beginning

         * of the last match, and the last sequence. This avoids us having to

         * update them while traversing the sequences.

 save sequences */

 only literals => must be last "sequence", actually starting a new stream of sequences */

 must be last sequence */

 last "sequence" is a bunch of literals => don't progress anchor */

 will finish */

 while (ip < ilimit) */

 Return the last literals size */

optLevel*/, ZSTD_noDict);

 used in 2-pass strategy */

 used in 2-pass strategy */

/* ZSTD_initStats_ultra():

 * make a first compression pass, just to seed stats with more accurate starting values.

 * only works on first block, with no dictionary and no ldm.

 * this function cannot error, hence its contract must be respected.

 updated rep codes will sink here */

 first block */

 no ldm */

 no dictionary */

 no prefix (note: intentional overflow, defined as 2-complement) */

optLevel*/, ZSTD_noDict);   
 invalidate first scan from history */

 re-inforce weight of collected statistics */

optLevel*/, ZSTD_noDict);

    /* 2-pass strategy:

     * this strategy makes a first pass over first block to collect statistics

     * and seed next round's statistics with it.

     * After 1st pass, function forgets everything, and starts a new block.

     * Consequently, this can only work if no data has been previously loaded in tables,

     * aka, no dictionary, no prefix, no ldm preprocessing.

     * The compression ratio gain is generally small (~0.5% on first block),

 first block */

 no ldm */

 no dictionary */

 start of frame, nothing already loaded nor skipped */

optLevel*/, ZSTD_noDict);

optLevel*/, ZSTD_dictMatchState);

optLevel*/, ZSTD_dictMatchState);

optLevel*/, ZSTD_extDict);

optLevel*/, ZSTD_extDict);

/* note : no btultra2 variant for extDict nor dictMatchState,

 * because btultra2 is not meant to work with dictionaries

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/*-*************************************

*  Dependencies

 INT_MAX, ZSTD_memset, ZSTD_memcpy */

 HIST_countFast_wksp */

 FSE_encodeSymbol */

/* ***************************************************************

*  Tuning parameters

/*!

 * COMPRESS_HEAPMODE :

 * Select how default decompression function ZSTD_compress() allocates its context,

 * on stack (0, default), or into heap (1).

 * Note that functions with explicit context such as ZSTD_compressCCtx() are unaffected.

/*-*************************************

*  Helper functions

/* ZSTD_compressBound()

 * Note that the result from this function is only compatible with the "normal"

 * full-block strategy.

 * When there are a lot of small blocks due to frequent flush in streaming mode

 * the overhead of headers can make the compressed data to be larger than the

 * return value of ZSTD_compressBound().

/*-*************************************

*  Context memory management

 The dictContentType the CDict was created with */

 entropy workspace of HUF_WORKSPACE_SIZE bytes */

 0 indicates that advanced API was used to select CDict params */

 typedef'd to ZSTD_CDict within "zstd.h" */

 minimum size */

 must be 8-aligned */

 statically sized space. entropyWorkspace never moves (but prev/next block swap places) */

/*

 * Clears and frees all of the dictionaries in the CCtx.

 support free on NULL */

 support sizeof on NULL */

 cctx may be in the workspace */

 same object */

 private API call, for dictBuilder only */

/* Returns 1 if compression parameters are such that we should

 * enable long distance matching (wlog >= 27, strategy >= btopt).

 * Returns 0 otherwise.

 should not matter, as all cParams are presumed properly defined */

 LDM is enabled by default for optimal parser and window size >= 128MB */

/*

 * Initializes the cctxParams from params and compressionLevel.

 * @param compressionLevel If params are derived from a compression level then that compression level, otherwise ZSTD_NO_CLEVEL.

    /* Should not matter, as all cParams are presumed properly defined.

     * But, set it for tracing anyway.

/*

 * Sets cctxParams' cParams and fParams from params, but otherwise leaves them alone.

 * @param param Validated zstd parameters.

    /* Should not matter, as all cParams are presumed properly defined.

     * But, set it for tracing anyway.

 experimental parameters */

 note : how to ensure at compile time that this is the highest value enum ? */

 note : how to ensure at compile time that this is the highest value enum ? */

/* ZSTD_cParam_clampBounds:

 * Clamps the value into the bounded range.

 0 == default */

 return type (size_t) cannot represent negative values */

 0 => use default */

 0 => use default */

 0 => use default */

 0 => use default */

 0 => use default */

 0 => use default */

 Content size written in frame header _when known_ (default:1) */

 A 32-bits content checksum will be calculated and written at end of frame (default:0) */

 When applicable, dictionary's dictID is provided in frame header (default:1) */

 0 ==> auto */

 0 ==> default */

 0 ==> default */

 0 ==> default */

 0 ==> default */

 0 ==> default */

/* ZSTD_CCtx_setParametersUsingCCtxParams() :

 *  just applies `params` into `cctx`

 *  no action is performed, parameters are merely stored.

 *  If ZSTDMT is enabled, parameters are pushed to cctx->mtctx.

 *    This is possible even if a compression is ongoing.

 *    In which case, new parameters will be applied on the fly, starting with next compression job.

/*

 * Initializes the local dict using the requested parameters.

 * NOTE: This does not use the pledged src size, because it may be used for more

 * than one compression.

 No local dictionary. */

 Local dictionary already initialized. */

 in case one already exists */

 no dictionary mode */

 Free the existing local cdict (if any) to save memory. */

/*! ZSTD_CCtx_reset() :

/* ZSTD_checkCParams() :

    control CParam values remain within authorized range.

/* ZSTD_clampCParams() :

 *  make CParam values within valid range.

/* ZSTD_cycleLog() :

/* ZSTD_dictAndWindowLog() :

 * Returns an adjusted window log that is large enough to fit the source and the dictionary.

 * The zstd format says that the entire dictionary is valid if one byte of the dictionary

 * is within the window. So the hashLog and chainLog should be large enough to reference both

 * the dictionary and the window. So we must use this adjusted dictAndWindowLog when downsizing

 * the hashLog and windowLog.

 * NOTE: srcSize must not be ZSTD_CONTENTSIZE_UNKNOWN.

 No dictionary ==> No change */

 Handled in ZSTD_adjustCParams_internal() */

        /* If the window size is already large enough to fit both the source and the dictionary

         * then just use the window size. Otherwise adjust so that it fits the dictionary and

         * the window.

 Window size large enough already */

 Larger than max window log */

/* ZSTD_adjustCParams_internal() :

 *  optimize `cPar` for a specified input (`srcSize` and `dictSize`).

 *  mostly downsize to reduce memory consumption and initialization latency.

 * `srcSize` can be ZSTD_CONTENTSIZE_UNKNOWN when not known.

 * `mode` is the mode for parameter adjustment. See docs for `ZSTD_cParamMode_e`.

 *  note : `srcSize==0` means 0!

 (1<<9) + 1 */

        /* If we don't know the source size, don't make any

         * assumptions about it. We will already have selected

         * smaller parameters if a dictionary is in use.

        /* Assume a small source size when creating a dictionary

         * with an unkown source size.

        /* Dictionary has its own dedicated parameters which have

         * already been selected. We are selecting parameters

         * for only the source.

 resize windowLog if input is small enough, to use less memory */

 minimum wlog required for valid frame header */

 resulting cPar is necessarily valid (all parameters within range) */

 srcSizeHint == 0 means 0 */

    /* We don't use ZSTD_cwksp_alloc_size() here because the tables aren't

 forCCtx */ 1);

    /* estimateCCtxSize is for one-shot compression. So no buffers should

     * be needed. However, we still allocate two 0-sized buffers, which can

 Choose the set of cParams for a given level across all srcSizes that give the largest cctxSize */

 Ensure monotonically increasing memory usage as compression level increases */

/* ZSTD_getFrameProgression():

 * tells how much data has been consumed (input) and produced (output) for current frame.

 * able to count progression inside worker threads (non-blocking mode).

 simplified; some data might still be left within streaming output buffer */

/*! ZSTD_toFlushNow()

 *  Only useful for multithreading scenarios currently (nbWorkers >= 1).

 over-simplification; could also check if context is currently running in streaming mode, and in which case, report how many bytes are left to be flushed within output buffer */

/*! ZSTD_invalidateMatchState()

 *  Invalidate all the matches in the match finder tables.

 *  Requires nextSrc and base to be set (can be NULL).

 force reset of btopt stats */

/*

 * Controls, for this matchState reset, whether the tables need to be cleared /

 * prepared for the coming compression (ZSTDcrp_makeClean), or whether the

 * tables can be left unclean (ZSTDcrp_leaveDirty), because we know that a

 * subsequent operation will overwrite the table space anyways (e.g., copying

 * the matchState contents in from a CDict).

/*

 * Controls, for this matchState reset, whether indexing can continue where it

 * left off (ZSTDirp_continue), or whether it needs to be restarted from zero

 * (ZSTDirp_reset).

 check that allocation hasn't already failed */

 table Space */

 reset tables only */

 opt parser space */

/* ZSTD_indexTooCloseToMax() :

 * minor optimization : prefer memset() rather than reduceIndex()

 * which is measurably slow in some circumstances (reported for Visual Studio).

 * Works when re-using a context for a lot of smallish inputs :

 * if all inputs are smaller than ZSTD_INDEXOVERFLOW_MARGIN,

 * memset() will be triggered before reduceIndex().

/*! ZSTD_resetCCtx_internal() :

 Adjust long distance matching parameters */

 Check if workspace is large enough, alloc a new one if needed */

                /* Statically sized space.

                 * entropyWorkspace never moves,

 init params */

        /* ZSTD_wildcopy() is used to copy into the literals buffer,

         * so we have to oversize the buffer by WILDCOPY_OVERLENGTH bytes.

 buffers */

 ldm bucketOffsets table */

 TODO: avoid memset? */

 sequences storage */

 ldm hash table */

 TODO: avoid memset? */

        /* Due to alignment, when reusing a workspace, we can actually consume

         * up to 3 extra bytes for alignment. See the comments in zstd_cwksp.h

/* ZSTD_invalidateRepCodes() :

 * ensures next compression will not use repcodes from previous block.

 * Note : only works with regular variant;

/* These are the approximate sizes for each strategy past which copying the

 * dictionary tables into the working context is faster than using them

 * in-place.

 unused */

 ZSTD_fast */

 ZSTD_dfast */

 ZSTD_greedy */

 ZSTD_lazy */

 ZSTD_lazy2 */

 ZSTD_btlazy2 */

 ZSTD_btopt */

 ZSTD_btultra */

 ZSTD_btultra2 */

          && !params->forceWindow ); /* dictMatchState isn't correctly

        /* Resize working context table params for input only, since the dict

 pledgedSrcSize == 0 means 0! */

 don't even attach dictionaries with no contents */

            /* prep working match state so dict matches never have negative indices

 loadedDictEnd is expressed within the referential of the active context */

 copy block state */

 Copy only compression parameters related to tables. */

 copy tables */

 Zero the hashTable3, since the cdict never fills it */

 copy dictionary offsets */

 copy block state */

/* We have a choice between copying the dictionary context into the working

 * context, or referencing the dictionary context from the working context

/*! ZSTD_copyCCtx_internal() :

 *  Duplicate an existing context `srcCCtx` into another one `dstCCtx`.

 *  Only works during stage ZSTDcs_init (i.e. after creation, but before first call to ZSTD_compressContinue()).

 *  The "context", in this case, refers to the hash and chain tables,

 *  entropy tables, and dictionary references.

 * `windowLog` value is enforced if != 0, otherwise value is copied from srcCCtx.

 Copy only compression parameters related to tables. */

 copy tables */

 copy dictionary offsets */

 copy block state */

/*! ZSTD_copyCCtx() :

 *  Duplicate an existing context `srcCCtx` into another one `dstCCtx`.

 *  Only works during stage ZSTDcs_init (i.e. after creation, but before first call to ZSTD_compressContinue()).

 *  pledgedSrcSize==0 means "unknown".

content*/, 0 noDictID*/ };

/*! ZSTD_reduceTable() :

 *  reduce table indexes by `reducerValue`, or squash to zero.

 *  PreserveMark preserves "unsorted mark" for btlazy2 strategy.

 *  It must be set to a clear 0/1 value, to remove branch during inlining.

 *  Presume table size is a multiple of ZSTD_ROWSIZE

 multiple of ZSTD_ROWSIZE */

 can be casted to int */

/*! ZSTD_reduceIndex() :

/*-*******************************************************

*  Block entropic compression

 See doc/zstd_compression_format.md for detailed format description */

/* ZSTD_useTargetCBlockSize():

 * Returns if target compressed block size param is being used.

 * If used, compression will do best effort to make a compressed block size to be around targetCBlockSize.

/* ZSTD_entropyCompressSequences_internal():

 compressed, raw or rle */

 Compress literals */

 Sequences Header */

max nbSeq Size*/ + 1 
 Copy the old tables over as if we repeated them */

 seqHead : flags for FSE encoding type */

 convert length/distances into codes */

 build CTable for Literal Lengths */

 can't fail */

 We don't copy tables */

 build CTable for Offsets */

 can't fail */

 We can only use the basic table if max <= DefaultMaxOff, otherwise the offsets are too large */

 We don't copy tables */

 build CTable for MatchLengths */

 can't fail */

 We don't copy tables */

        /* zstd versions <= 1.3.4 mistakenly report corruption when

         * FSE_readNCount() receives a buffer < 4 bytes.

         * Fixed by https://github.com/facebook/zstd/pull/1146.

         * This can happen when the last set_compressed table present is 2

         * bytes and the bitstream is only one byte.

         * In this exceedingly rare case, we will simply emit an uncompressed

         * block, since it isn't worth optimizing.

 NCountSize >= 2 && bitstreamSize > 0 ==> lastCountSize == 3 */

    /* When srcSize <= dstCapacity, there is enough space to write a raw uncompressed block.

     * Since we ran out of space, block must be not compressible, so fall back to raw uncompressed block.

 block not compressed */

 Check compressibility */

 block not compressed */

/* ZSTD_selectBlockCompressor() :

 * Not static, but internal use only (used by long distance matcher)

 default for 0 */,

 default for 0 */,

 default for 0 */,

 default for 0 */,

 Assert that we have correctly flushed the ctx params into the ms's copy */

 don't even attempt compression below a certain srcSize */

 required for optimal parser to read stats from dictionary */

 tell the optimal parser how we expect to compress literals */

    /* a gap between an attached dict and the current window is not safe,

     * they must remain adjacent,

 limited update after a very long match */

 ensure no overflow */

 select and store sequences */

 Updates ldmSeqStore.pos */

 Updates ldmSeqStore.size */

 Updates ldmSeqStore.pos */

 not long range mode */

 Ensure we have enough space for last literals "sequence" */

 Derive the correct offset corresponding to a repcode */

        /* seqStoreSeqs[i].offset == offCode+1, and ZSTD_updateRep() expects offCode

    /* Insert last literals (if any exist) in the block as a sequence with ml == off == 0.

     * If there are no last literals, then we'll emit (of: 0, ml: 0, ll: 0), which is a marker

     * for the block boundary, according to the API.

 Unrolled loop to read four size_ts of input at a time. Returns 1 if is RLE, 0 if not. */

 Check if prefix is RLE first before using unrolled loop */

/* Returns true if the given block may be RLE.

 * This is just a heuristic based on the compressibility.

 * It may return both false positives and false negatives.

    /* This the upper bound for the length of an rle block.

     * This isn't the actual upper bound. Finding the real threshold

     * needs further investigation.

 encode sequences and literals */

 statically allocated in resetCCtx */,

        /* We don't want to emit our first block as a RLE even if it qualifies because

         * doing so will cause the decoder (cli only) to throw a "should consume all input error."

         * This is only an issue for zstd <= v1.4.3

    /* We check that dictionaries have offset codes available for the first

     * block. After the first block, the offcode table might not have large

     * enough codes to represent the offsets in the data.

        if (/* We don't want to emit our first block as a RLE even if it qualifies because

            * doing so will cause the decoder (cli only) to throw a "should consume all input error."

            * This is only an issue for zstd <= v1.4.3

        /* Attempt superblock compression.

         *

         * Note that compressed size of ZSTD_compressSuperBlock() is not bound by the

         * standard ZSTD_compressBound(). This is a problem, because even if we have

         * space now, taking an extra byte now could cause us to run out of space later

         * and violate ZSTD_compressBound().

         *

         * Define blockBound(blockSize) = blockSize + ZSTD_blockHeaderSize.

         *

         * In order to respect ZSTD_compressBound() we must attempt to emit a raw

         * uncompressed block in these cases:

         *   * cSize == 0: Return code for an uncompressed block.

         *   * cSize == dstSize_tooSmall: We may have expanded beyond blockBound(srcSize).

         *     ZSTD_noCompressBlock() will return dstSize_tooSmall if we are really out of

         *     output space.

         *   * cSize >= blockBound(srcSize): We have expanded the block too much so

         *     emit an uncompressed block.

    /* Superblock compression failed, attempt to emit a single no compress block.

     * The decoder will be able to stream this block since it is uncompressed.

 invalidate dictionaries on overflow correction */

/*! ZSTD_compress_frameChunk() :

*   Compress a chunk of data into one or multiple blocks.

*   All blocks will be terminated, all input will be consumed.

*   Function will issue an error if there is not enough `dstCapacity` to hold the compressed content.

*   Frame is supposed already started (header already produced)

*   @return : compressed size, or an error code

 Ensure hash/chain table insertion resumes no sooner than lowlimit */

 frame */);

 block is not compressible */

 0-3 */

 0-3 */

 0-3 */

 impossible */

 impossible */

/* ZSTD_writeSkippableFrame_advanced() :

 * Writes out a skippable frame with the specified magic number variant (16 are supported),

 * from ZSTD_MAGIC_SKIPPABLE_START to ZSTD_MAGIC_SKIPPABLE_START+15, and the desired source data.

 *

 * Returns the total number of bytes written, or a ZSTD error code.

 Skippable frame overhead */,

/* ZSTD_writeLastEmptyBlock() :

 * output an empty Block with end-of-frame mark to complete a frame

 * @return : size of data written into `dst` (== ZSTD_blockHeaderSize (defined in zstd_internal.h))

 *           or an error code if `dstCapacity` is too small (<ZSTD_blockHeaderSize)

lastBlock*/ + (((U32)bt_raw)<<1);  
 do not generate an empty block if no input */

 overflow check and correction for block mode */

 frame */);

 control src size */

 frame mode */, 0 
 frame mode */, 0 
/*! ZSTD_loadDictionaryContent() :

 *  @return : 0, or an error code

 Assert that we the ms params match the params we're being given */

 must load everything in one go */

 we want the dictionary table fully sorted */

 not possible : not a valid strategy id */

/* Dictionaries that assign zero probability to symbols that show up causes problems

 * when FSE encoding. Mark dictionaries with zero probability symbols as FSE_repeat_check

 * and only dictionaries with 100% valid symbols can be assumed valid.

 skip magic num and dict ID */

        /* We only set the loaded table as valid if it contains all non-zero

 fill all offset symbols to avoid garbage at end of table */

 Defer checking offcodeMaxValue because we need to know the size of the dictionary content */

 The maximum offset that must be supported */

 Calculate minimum offset code required to represent maxOffset */

 All offset values <= dictContentSize + 128 KB must be representable for a valid table */

 All repCodes must be <= dictContentSize and != 0 */

/* Dictionary format :

 * See :

 * https://github.com/facebook/zstd/blob/release/doc/zstd_compression_format.md#dictionary-format

/*! ZSTD_loadZstdDictionary() :

 * @return : dictID, or an error code

 *  assumptions : magic number supposed already checked

 *                dictSize supposed >= 8

 skip magic number */ );

/* ZSTD_compress_insertDictionary() :

 dict restricted modes */

 impossible */

 dict as full zstd dictionary */

/*! ZSTD_compressBegin_internal() :

 params are supposed to be fully validated at this point */

 either dict or cdict, not both */

 compression parameters verification and optimization */

/*! ZSTD_compressBegin_advanced() :

cdict*/,

/*! ZSTD_writeEpilogue() :

*   Ends a frame.

 special case : empty frame */

 write one last empty block, make it the "last" block */

 last block */ + (((U32)bt_raw)<<1) + 0;

 return to "created but no init" status */

 frame mode */, 1 
 control src size */

 Internal */

 =====  Dictionary API  ===== */

/*! ZSTD_estimateCDictSize_advanced() :

 forCCtx */ 0)

 support sizeof on NULL */

 cdict may be in the workspace */

 Reset the state to no dictionary */

    /* (Maybe) load the dictionary

     * Skips loading the dictionary if it is < 8 bytes.

 forCCtx */ 0) +

 signals advanced API usage */

 Fall back to non-DDSS params */

 support free on NULL */

/*! ZSTD_initStaticCDict_advanced() :

 *  Generate a digested dictionary in provided memory area.

 *  workspace: The memory area to emplace the dictionary into.

 *             Provided pointer must 8-bytes aligned.

 *             It must outlive dictionary usage.

 *  workspaceSize: Use ZSTD_estimateCDictSize()

 *                 to determine how large workspace must be.

 *  cParams : use ZSTD_getCParams() to transform a compression level

 *            into its relevants cParams.

 * @return : pointer to ZSTD_CDict*, or NULL if error (size too small)

 *  Note : there is no corresponding "free" function.

 *         Since workspace was allocated externally, it must be freed externally.

 forCCtx */ 0);

 8-aligned */

/*! ZSTD_getDictID_fromCDict() :

 *  Provides the dictID of the dictionary loaded into `cdict`.

 *  If @return == 0, the dictionary is not conformant to Zstandard specification, or empty.

/* ZSTD_compressBegin_usingCDict_advanced() :

 Initialize the cctxParams from the cdict */

    /* Increase window log to fit the entire dictionary and source if the

     * source size is known. Limit the increase to 19, which is the

     * window log for compression level 1 with the largest source size.

/* ZSTD_compressBegin_usingCDict() :

 * pledgedSrcSize=0 means "unknown"

content*/, 0 noDictID*/ };

 will check if cdict != NULL */

/*! ZSTD_compress_usingCDict() :

 *  Compression using a digested Dictionary.

 *  Faster startup than ZSTD_compress_usingDict(), recommended when same dictionary is used multiple times.

 *  Note that compression parameters are decided at CDict creation time

content*/, 0 noDictID*/ };

/* ******************************************************************

*  Streaming

 CStream and CCtx are now same object */

 same object */

======   Initialization   ======*/

 32-bits hash */ ;

/* ZSTD_resetCStream():

    /* temporary : 0 interpreted as "unknown" during transition period.

     * Users willing to specify "unknown" **must** use ZSTD_CONTENTSIZE_UNKNOWN.

     * 0 will be interpreted as "empty" in the future.

/*! ZSTD_initCStream_internal() :

 *  Note : for lib/compress only. Used by zstdmt_compress.c.

 *  Assumption 1 : params are valid

 either dict or cdict, not both */

 Dictionary is cleared if !cdict */

/* ZSTD_initCStream_usingCDict_advanced() :

 note : cdict must outlive compression session */

/* ZSTD_initCStream_advanced() :

 * pledgedSrcSize must be exact.

 * if srcSize is not known at init time, use value ZSTD_CONTENTSIZE_UNKNOWN.

    /* for compatibility with older programs relying on this behavior.

     * Users should now specify ZSTD_CONTENTSIZE_UNKNOWN.

     * This line will be removed in the future.

    /* temporary : 0 interpreted as "unknown" during transition period.

     * Users willing to specify "unknown" **must** use ZSTD_CONTENTSIZE_UNKNOWN.

     * 0 will be interpreted as "empty" in the future.

======   Compression   ======*/

/* ZSTD_compressStream_generic():

 *  internal function for all *compressStream*() variants

 *  non-static, because can be called from zstdmt_compress.c

 check expectations */

 Enough output space */

 OR we are allowed to return dstSizeTooSmall */

 shortcut to compression pass directly into output buffer */

 complete loading into inBuffer in buffered mode */

 not enough input to fill full block : stop here */

 empty */

 compress current block (note : this stage cannot be stopped in the middle) */

 compress into output buffer, to skip flush stage */

 prepare next block */

 Already validated */);

 Consume the input prior to error checking to mirror buffered mode. */

 no need to flush */

 pass-through to flush stage */

 flush not fully completed, presumably because dst is too small */

 impossible */

/* After a compression call set the expected input/output buffer.

 * This is validated at the start of the next compression call.

/* Validate that the input/output buffers match the expectations set by

 * ZSTD_setBufferExpectations.

 Init the local dict if present. */

 single usage */

 only one can be set */

 let cdict take priority in terms of compression level */

 auto-fix pledgedSrcSize */

 Enable LDM by default for optimal parser and window size >= 128MB */

            /* for small input: avoid automatic flush on reaching end of block, since

            * it would require to add a 3-bytes null block to end frame

 check conditions */

 transparent initialization stage */

 Set initial buffer expectations now that we've initialized */

 end of transparent initialization stage */

 compression stage */

 remaining to flush */

 ZSTD_compressStream2() will check validity of dstPos and srcPos */

 Enable stable input/output buffers. */

 Reset to the original values. */

 compression not completed, due to lack of output space */

 all input is expected consumed */

 Index in array of ZSTD_Sequence */

 Position within sequence at idx */

 Number of bytes given by sequences provided so far */

 Returns a ZSTD error code if sequence is not valid */

    /* posInSrc represents the amount of data the the decoder would decode up to this point.

     * As long as the amount of data decoded is less than or equal to window size, offsets may be

     * larger than the total length of output decoded in order to reference the dict, even larger than

     * window size. After output surpasses windowSize, we're limited to windowSize offsets again.

 Returns an offset code, given a sequence's raw offset, the ongoing repcode array, and whether litLength == 0 */

 ZSTD_storeSeq expects a number in the range [0, 2] to represent a repcode */

/* Returns 0 on success, and a ZSTD_error otherwise. This function scans through an array of

 * ZSTD_Sequence, storing the sequences it finds, until it reaches a block delimiter.

/* Returns the number of bytes to move the current read position back by. Only non-zero

 * if we ended up splitting a sequence. Otherwise, it may return a ZSTD error if something

 * went wrong.

 *

 * This function will attempt to scan through blockSize bytes represented by the sequences

 * in inSeqs, storing any (partial) sequences.

 *

 * Occasionally, we may want to change the actual number of bytes we consumed from inSeqs to

 * avoid splitting a match, or to avoid splitting a match such that it would produce a match

 * smaller than MINMATCH. In this case, we return the number of bytes that we didn't read from this block.

 May be adjusted if we decide to process fewer than blockSize bytes */

 Modify the sequence depending on where endPosInSequence lies */

 Move to the next sequence */

            /* This is the final (partial) sequence we're adding from inSeqs, and endPosInSequence

 Only ever split the match if it is larger than the block size */

 Move the endPosInSequence backward so that it creates match of minMatch length */

                    /* Flag that we split the last match - after storing the sequence, exit the loop,

                    /* Move the position in sequence backwards so that we don't split match, and break to store

                     * the last literals. We use the original currSeq.litLength as a marker for where endPosInSequence

                     * should go. We prefer to do this whenever it is not necessary to split the match, or if doing so

                     * would cause the first half of the match to be too small

 This sequence ends inside the literals, break to store the last literals */

 Check if this offset can be represented with a repcode */

 Store any last literals */

/* Compress, block-by-block, all of the sequences given.

 *

 * Returns the cumulative size of all compressed blocks (including their headers), otherwise a ZSTD error.

 Special case: empty frame */

 last block */ + (((U32)bt_raw)<<1);

 If blocks are too small, emit as a nocompress block */

 Leave space for block header */, dstCapacity - ZSTD_blockHeaderSize,

 statically allocated in resetCCtx */,

            /* We don't want to emit our first block as a RLE even if it qualifies because

            * doing so will cause the decoder (cli only) to throw a "should consume all input error."

            * This is only an issue for zstd <= v1.4.3

 ZSTD_noCompressBlock writes the block header as well */

 Error checking and repcodes update */

 Write block header into beginning of block*/

 Transparent initialization stage, same as compressStream2() */

 Begin writing output, starting with frame header */

 cSize includes block header size and compressed sequences size */

======   Finalize   ======*/

/*! ZSTD_flushStream() :

 minimal estimation */

 single thread mode : attempt to calculate remaining to flush more precisely */

-=====  Pre-defined compression levels  =====-*/

 "default" - for any srcSize > 256 KB */

 W,  C,  H,  S,  L, TL, strat */

 base for negative levels */

 level  1 */

 level  2 */

 level  3 */

 level  4 */

 level  5 */

 level  6 */

 level  7 */

 level  8 */

 level  9 */

 level 10 */

 level 11 */

 level 12 */

 level 13 */

 level 14 */

 level 15 */

 level 16 */

 level 17 */

 level 18 */

 level 19 */

 level 20 */

 level 21 */

 level 22 */

 for srcSize <= 256 KB */

 W,  C,  H,  S,  L,  T, strat */

 base for negative levels */

 level  1 */

 level  2 */

 level  3 */

 level  4.*/

 level  5.*/

 level  6.*/

 level  7 */

 level  8 */

 level  9 */

 level 10 */

 level 11.*/

 level 12.*/

 level 13 */

 level 14.*/

 level 15.*/

 level 16.*/

 level 17.*/

 level 18.*/

 level 19.*/

 level 20.*/

 level 21.*/

 level 22.*/

 for srcSize <= 128 KB */

 W,  C,  H,  S,  L,  T, strat */

 base for negative levels */

 level  1 */

 level  2 */

 level  3 */

 level  4 */

 level  5 */

 level  6 */

 level  7 */

 level  8 */

 level  9 */

 level 10 */

 level 11 */

 level 12 */

 level 13.*/

 level 14.*/

 level 15.*/

 level 16.*/

 level 17.*/

 level 18.*/

 level 19.*/

 level 20.*/

 level 21.*/

 level 22.*/

 for srcSize <= 16 KB */

 W,  C,  H,  S,  L,  T, strat */

 base for negative levels */

 level  1 */

 level  2 */

 level  3 */

 level  4 */

 level  5.*/

 level  6 */

 level  7 */

 level  8.*/

 level  9.*/

 level 10.*/

 level 11.*/

 level 12.*/

 level 13.*/

 level 14.*/

 level 15.*/

 level 16.*/

 level 17.*/

 level 18.*/

 level 19.*/

 level 20.*/

 level 21.*/

 level 22.*/

/*

 * Reverses the adjustment applied to cparams when enabling dedicated dict

 * search. This is used to recover the params set to be used in the working

 * context. (Otherwise, those tables would also grow.)

/*! ZSTD_getCParams_internal() :

 * @return ZSTD_compressionParameters structure for a selected compression level, srcSize and dictSize.

 *  Note: srcSizeHint 0 means 0, use ZSTD_CONTENTSIZE_UNKNOWN for unknown.

 *        Use dictSize == 0 for unknown or unused.

 row */

 0 == default */

 entry 0 is baseline for fast mode */

 acceleration factor */

 refine parameters based on srcSize & dictSize */

/*! ZSTD_getCParams() :

 * @return ZSTD_compressionParameters structure for a selected compression level, srcSize and dictSize.

/*! ZSTD_getParams() :

 *  same idea as ZSTD_getCParams()

 * @return a `ZSTD_parameters` structure (instead of `ZSTD_compressionParameters`).

/*! ZSTD_getParams() :

 *  same idea as ZSTD_getCParams()

 * @return a `ZSTD_parameters` structure (instead of `ZSTD_compressionParameters`).

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/*-*************************************

*  Binary Tree search

 condition for ZSTD_hashPtr */

 condition for valid base+idx */

 assumption : ip + 8 <= iend */

 Update Hash Table */

 update BT like a chain */

/* ZSTD_insertDUBT1() :

 *  sort one already inserted but unsorted position

 *  assumption : curr >= btlow == (curr - btmask)

 this candidate is unsorted : next sorted candidate is reached through *smallerPtr, while *largerPtr contains previous unsorted candidate (which is already saved and can be overwritten) */

 to be nullified at the end */

 condition for ZSTD_count */

 guaranteed minimum nb of common bytes */

        /* note : all candidates are now supposed sorted,

         * but it's still possible to have nextPtr[1] == ZSTD_DUBT_UNSORTED_MARK

 both in current segment*/

 both in extDict */) {

 might be wrong if extDict is incorrectly set to 0 */

 preparation for next read of match[matchLength] */

 equal : no way to know if inf or sup */

 drop , to guarantee consistency ; miss a bit of compression, but other solutions can corrupt tree */

 necessarily within buffer */

 match is smaller than current */

 update smaller idx */

 all smaller will now have at least this guaranteed common length */

 beyond tree size, stop searching */

 new "candidate" => larger than match, which was smaller than target */

 new matchIndex, larger than previous and closer to current */

 match is larger than current */

 beyond tree size, stop searching */

 guaranteed minimum nb of common bytes */

 to prepare for next usage of match[matchLength] */

 reached end of input : ip[matchLength] is not valid, no way to know if it's larger or smaller than match */

 drop, to guarantee consistency (miss a little bit of compression) */

 beyond tree size, stop the search */

 all smaller will now have at least this guaranteed common length */

 new matchIndex larger than previous (closer to current) */

 match is larger than current */

 beyond tree size, stop the search */

 required for h calculation */

 reach end of unsorted candidates list */

 the unsortedMark becomes a reversed chain, to move up back to original position */

    /* nullify last candidate if it's still unsorted

 batch sort stacked candidates */

 will end on matchIndex == 0 */

 find longest match */

 to be nullified at the end */

 Update Hash Table */

 guaranteed minimum nb of common bytes */

 to prepare for next usage of match[matchLength] */

 equal : no way to know if inf or sup */

                        nbCompares = 0; /* in addition to avoiding checking any

                                         * further in this loop, make sure we

 drop, to guarantee consistency (miss a little bit of compression) */

 match is smaller than current */

 update smaller idx */

 all smaller will now have at least this guaranteed common length */

 beyond tree size, stop the search */

 new "smaller" => larger of match */

 new matchIndex larger than previous (closer to current) */

 match is larger than current */

 beyond tree size, stop the search */

 Check we haven't underflowed. */

 ensure nextToUpdate is increased */

 skip repetitive patterns */

 ZSTD_BtFindBestMatch() : Tree updater, providing best match */

 template */,

 skipped area */

 includes case 3 */

 includes case 3 */

 includes case 3 */

/* *********************************

*  Hash Chain

/* Update chains up to ip (excluded)

 catch up */

    /* We know the hashtable is oversized by a factor of `bucketSize`.

     * We are going to temporarily pretend `bucketSize == 1`, keeping only a

     * single entry. We will use the rest of the space to construct a temporary

     * chaintable.

 fill conventional hash table and conventional chain table */

 sort chains into ddss chain table */

                /* skip through the chain to the first position that won't be

                            /* only allow pulling `cacheSize` number of entries

                             * into the cache or chainTable beyond `minChain`,

                             * to replace the entries pulled out of the

                             * chainTable into the cache. This lets us reach

                             * back further without increasing the total number

                             * of entries in the chainTable, guaranteeing the

                             * DDSS chain table will fit into the space

 I believe this is guaranteed... */

 move chain pointers into the last entry of each hash bucket */

 fill the buckets of the hash table */

 Shift hash cache down 1. */

 inlining is important to hardwire a hot branch (template emulation) */

 HC4 match finder */

 ensures this is true if dictMode != ZSTD_extDict */

 potentially better */

 assumption : matchIndex <= dictLimit-4 (by table construction) */

 save best solution */

 best possible, avoids read overflow on next attempt */

 Check we haven't underflowed. */

 guaranteed by table construction */

 assumption : matchIndex <= dictLimit-4 (by table construction) */

 save best solution */

 best possible, avoids read overflow on next attempt */

 guaranteed by table construction */

 assumption : matchIndex <= dictLimit-4 (by table construction) */

 save best solution */

 best possible, avoids read overflow on next attempt */

 assumption : matchIndex <= dictLimit-4 (by table construction) */

 save best solution */

 best possible, avoids read overflow on next attempt */

 includes case 3 */

 includes case 3 */

 includes case 3 */

 includes case 3 */

/* *******************************

*  Common parser - lazy strategy

    /*

     * This table is indexed first by the four ZSTD_dictMode_e values, and then

     * by the two searchMethod_e values. NULLs are placed for configurations

     * that should never occur (extDict modes go to the other implementation

     * below and there is no DDSS for binary tree search yet).

 init */

        /* dictMatchState repCode checks don't currently handle repCode == 0

 Match Loop */

    /* I've measured random a 5% speed loss on levels 5 & 6 (greedy) when the

     * code alignment is perturbed. To fix the instability align the loop on 32-bytes.

 check repCode */

 intentional underflow */)

 first search (depth 0) */

 jump faster over incompressible sections */

 let's try to find a better solution */

 intentional underflow */)

 raw approx */

 search a better one */

 let's find an even better one */

 intentional underflow */)

 raw approx */

 nothing found : store previous solution */

        /* NOTE:

         * start[-offset+ZSTD_REP_MOVE-1] is undefined behavior.

         * (-offset+ZSTD_REP_MOVE-1) is unsigned, and is added to start, which

         * overflows the pointer, which is undefined behavior.

 catch up */

 only search for offset within prefix */

 catch up */

 store sequence */

 check immediate repcode */

 intentional overflow */)

 swap offset_2 <=> offset_1 */

 store sequence */

 swap repcodes */

 faster when present ... (?) */

 Save reps for next block */

 Return the last literals size */

 init */

 Match Loop */

    /* I've measured random a 5% speed loss on levels 5 & 6 (greedy) when the

     * code alignment is perturbed. To fix the instability align the loop on 32-bytes.

 check repCode */

 intentional overflow */

 repcode detected we should take it */

 first search (depth 0) */

 jump faster over incompressible sections */

 let's try to find a better solution */

 check repCode */

 intentional overflow */

 repcode detected */

 search match, depth 1 */

 raw approx */

 search a better one */

 let's find an even better one */

 check repCode */

 intentional overflow */

 repcode detected */

 search match, depth 2 */

 raw approx */

 nothing found : store previous solution */

 catch up */

 catch up */

 store sequence */

 check immediate repcode */

 intentional overflow */

 repcode detected we should take it */

 swap offset history */

 faster when present ... (?) */

 Save reps for next block */

 Return the last literals size */

/* ******************************************************************

 * hist : Histogram functions

 * part of Finite State Entropy project

 * Copyright (c) Yann Collet, Facebook, Inc.

 *

 *  You can contact the author at :

 *  - FSE source repository : https://github.com/Cyan4973/FiniteStateEntropy

 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

 --- dependencies --- */

 U32, BYTE, etc. */

 assert, DEBUGLOG */

 ERROR */

 --- Error management --- */

/*-**************************************************************

 *  Histogram functions

/* HIST_count_parallel_wksp() :

 * store histogram into 4 intermediate tables, recombined at the end.

 * this design makes better use of OoO cpus,

 * and is noticeably faster when some values are heavily repeated.

 * But it needs some additional workspace for intermediate tables.

 * `workSpace` must be a U32 table of size >= HIST_WKSP_SIZE_U32.

 * @return : largest histogram frequency,

 safety checks */

 by stripes of 16 bytes */

 finish last symbols */

 in case count & Counting1 are overlapping */

/* HIST_countFast_wksp() :

 * Same as HIST_countFast(), but using an externally provided scratch buffer.

 * `workSpace` is a writable buffer which must be 4-bytes aligned,

 * `workSpaceSize` must be >= HIST_WKSP_SIZE

 heuristic threshold */

 must be aligned on 4-bytes boundaries */

/* HIST_count_wksp() :

 * Same as HIST_count(), but using an externally provided scratch buffer.

 must be aligned on 4-bytes boundaries */

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

 /*-*************************************

 *  Dependencies

 2 - 1 - 5 */

 2 - 2 - 12 */

 2 - 2 - 20 */

 not necessary : flSize is {1,2,3} */

 dstCapacity already guaranteed to be >=4, hence large enough */

 2 - 1 - 5 */

 2 - 2 - 12 */

 2 - 2 - 20 */

 not necessary : flSize is {1,2,3} */

 Prepare nextEntropy assuming reusing the existing table */

 small ? don't even attempt compression (speed opt) */

 reused the existing table */

 using a newly constructed table */

 Build header */

 2 - 2 - 10 - 10 */

 2 - 2 - 14 - 14 */

 2 - 2 - 18 - 18 */

 not possible : lhSize is {3,4,5} */

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

 ZSTD_fillHashTable() */

 ZSTD_fillDoubleHashTable() */

/* ZSTD_ldm_gear_init():

 *

 * Initializes the rolling hash state such that it will honor the

    /* The choice of the splitting criterion is subject to two conditions:

     *   1. it has to trigger on average every 2^(hashRateLog) bytes;

     *   2. ideally, it has to depend on a window of minMatchLength bytes.

     *

     * In the gear hash algorithm, bit n depends on the last n bytes;

     * so in order to obtain a good quality splitting criterion it is

     * preferable to use bits with high weight.

     *

     * To match condition 1 we use a mask with hashRateLog bits set

     * and, because of the previous remark, we make sure these bits

     * have the highest possible weight while still respecting

     * condition 2.

 In this degenerate case we simply honor the hash rate. */

/* ZSTD_ldm_gear_feed():

 *

 * Registers in the splits array all the split points found in the first

 * size bytes following the data pointer. This function terminates when

 * either all the data has been processed or LDM_BATCH_SIZE splits are

 * present in the splits array.

 *

 * Precondition: The splits array must not be full.

/* ZSTD_ldm_getBucket() :

/* ZSTD_ldm_insertEntry() :

/* ZSTD_ldm_countBackwardsMatch() :

 *  Returns the number of bytes that match backwards before pIn and pMatch.

 *

/* ZSTD_ldm_countBackwardsMatch_2segments() :

 *  Returns the number of bytes that match backwards from pMatch,

 *  even with the backwards match spanning 2 different segments.

 *

 If backwards match is entirely in the extDict or prefix, immediately return */

/* ZSTD_ldm_fillFastTables() :

 *

 *  Fills the relevant tables for the ZSTD_fast and ZSTD_dfast strategies.

 *  This is similar to ZSTD_loadDictionaryContent.

 *

 *  The tables for the other strategies are filled within their

 not possible : not a valid strategy id */

/* ZSTD_ldm_limitTableUpdate() :

 *

 *  Sets cctx->nextToUpdate to a position corresponding closer to anchor

 *  if it is far way

 LDM parameters */

 Prefix and extDict parameters */

 Input bounds */

 Input positions */

 Rolling hash state */

 Arrays for staged-processing */

 Initialize the rolling hash state with the first minMatchLength bytes */

            /* If a split point would generate a sequence overlapping with

             * the previous one, we merely register it in the hash table and

 !extDict */

            /* No match found -- insert an entry into the hash table

 Match found */

 Out of sequence storage */

            /* Insert the current entry into the hash table --- it must be

/*! ZSTD_ldm_reduceTable() :

    /* Check that ZSTD_window_update() has been called for this chunk prior

     * to passing it to this function.

    /* The input could be very large (in zstdmt), so it must be broken up into

     * chunks to enforce the maximum distance and handle overflow correction.

 1. Perform overflow correction if necessary. */

 cycleLog */ 0, maxDist, chunkStart);

 invalidate dictionaries on overflow correction */

        /* 2. We enforce the maximum offset allowed.

         *

         * kMaxChunkSize should be small enough that we don't lose too much of

         * the window through early invalidation.

         * TODO: * Test the chunk size.

         *       * Try invalidation after the sequence generation and test the

         *         the offset against maxDist directly.

         *

         * NOTE: Because of dictionaries + sequence splitting we MUST make sure

         * that any offset used is valid at the END of the sequence, since it may

         * be split into two sequences. This condition holds when using

         * ZSTD_window_enforceMaxDist(), but if we move to checking offsets

         * against maxDist directly, we'll have to carefully handle that case.

 3. Generate the sequences for the chunk, and get newLeftoverSize. */

        /* 4. We add the leftover literals from previous iterations to the first

         *    newly generated sequence, or add the `newLeftoverSize` if none are

         *    generated.

 Prepend the leftover literals from the last call */

 Skip past srcSize literals */

 Skip past the first srcSize of the match */

 The match is too short, omit it */

/*

 * If the sequence length is longer than remaining then the sequence is split

 * between this block and the next.

 *

 * Returns the current sequence to handle, or if the rest of the block should

 * be literals, it returns a sequence with offset == 0.

 Likely: No partial sequence */

 Cut the sequence short (offset == 0 ==> rest is literals). */

 Skip past `remaining` bytes for the future sequences. */

 Input bounds */

 Input positions */

 If using opt parser, use LDMs only as candidates rather than always accepting them */

 Loop through each sequence and apply the block compressor to the literals */

 maybeSplitSequence updates rawSeqStore->pos */

 End signal */

 Fill tables for block compressor */

 Run the block compressor */

 Update the repcodes */

 Store the sequence */

 Fill the tables for the block compressor */

 Compress the last literals */

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/* zstd_decompress_block :

/*-*******************************************************

*  Dependencies

 ZSTD_memcpy, ZSTD_memmove, ZSTD_memset */

 prefetch */

 bmi2 */

 low level memory routines */

 ZSTD_DCtx */

 ZSTD_DDictDictContent */

/*_*******************************************************

*  Macros

/* These two optional macros force the use one way or another of the two

 * ZSTD_decompressSequences implementations. You can't force in both directions

 * at the same time.

/*_*******************************************************

*  Memory operations

/*-*************************************************************

 *   Block decoding

/*! ZSTD_getcBlockSize() :

 only useful for RLE */

 Hidden declaration for fullbench */

/*! ZSTD_decodeLiteralsBlock() :

 * @return : nb of bytes read from src (< srcSize )

 note : srcSize < BLOCKSIZE */

 note : default is impossible, since lhlCode into [0..3] */

 2 - 2 - 10 - 10 */

 2 - 2 - 14 - 14 */

 2 - 2 - 18 - 18 */

 prefetch huffman table if cold */

 heuristic */)) {

 note : default is impossible, since lhlCode into [0..3] */

 risk reading beyond src buffer with wildcopy */

 direct reference into compressed stream */

 note : default is impossible, since lhlCode into [0..3] */

/* Default FSE distribution tables.

 * These are pre-calculated FSE decoding tables using default distributions as defined in specification :

 * https://github.com/facebook/zstd/blob/release/doc/zstd_compression_format.md#default-distributions

 * They were generated programmatically with following method :

 * - start from default distributions, present in /lib/common/zstd_internal.h

 * - generate tables normally, using ZSTD_buildFSETable()

 * - printout the content of tables

 Default FSE distribution table for Literal Lengths */

 header : fastMode, tableLog */

 nextState, nbAddBits, nbBits, baseVal */

 LL_defaultDTable */

 Default FSE distribution table for Offset Codes */

 header : fastMode, tableLog */

 nextState, nbAddBits, nbBits, baseVal */

 OF_defaultDTable */

 Default FSE distribution table for Match Lengths */

 header : fastMode, tableLog */

 nextState, nbAddBits, nbBits, baseVal */

 ML_defaultDTable */

/* ZSTD_buildFSETable() :

 * generate FSE decoding table for one symbol (ll, ml or off)

 * cannot fail if input is valid =>

 Sanity Checks */

 Init, lay down lowprob symbols */

 Spread symbols */

    /* Specialized symbol spreading for the case when there are

     * no low probability (-1 count) symbols. When compressing

     * small blocks we avoid low probability symbols to hit this

     * case, since header decoding speed matters more.

        /* First lay down the symbols in order.

         * We use a uint64_t to lay down 8 bytes at a time. This reduces branch

         * misses since small blocks generally have small table logs, so nearly

         * all symbols have counts <= 8. We ensure we have 8 bytes at the end of

         * our buffer to handle the over-write.

        /* Now we spread those positions across the table.

         * The benefit of doing it in two stages is that we avoid the the

         * variable size inner loop, which caused lots of branch misses.

         * Now we can run through all the positions without any branch misses.

         * We unroll the loop twice, since that is what emperically worked best.

 FSE_MIN_TABLELOG is 5 */

 lowprob area */

 position must reach all cells once, otherwise normalizedCounter is incorrect */

 Build Decoding table */

 Avoids the FORCE_INLINE of the _body() function. */

/*! ZSTD_buildSeqTable() :

 * @return : nb bytes read from src,

 prefetch FSE table if used */

 heuristic */)) {

 check */

 SeqHead */

 FSE table descriptors */

 minimum possible size: 1 byte for symbol encoding types */

 Build DTables */

/*! ZSTD_overlapCopy8() :

 *  Copies 8 bytes from ip to op and updates op and ip where ip <= op.

 *  If the offset is < 8 then the offset is spread to at least 8 bytes.

 *

 *  Precondition: *ip <= *op

 *  Postcondition: *op - *op >= 8

 close range match, overlap */

 added */

 subtracted */

/*! ZSTD_safecopy() :

 *  Specialized version of memcpy() that is allowed to READ up to WILDCOPY_OVERLENGTH past the input buffer

 *  and write up to 16 bytes past oend_w (op >= oend_w is allowed).

 *  This function is only called in the uncommon case where the sequence is near the end of the block. It

 *  should be fast for a single long sequence, but can be slow for several short sequences.

 *

 *  @param ovtype controls the overlap detection

 *         - ZSTD_no_overlap: The source and destination are guaranteed to be at least WILDCOPY_VECLEN bytes apart.

 *         - ZSTD_overlap_src_before_dst: The src and dst may overlap and may be any distance apart.

 *           The src buffer must be before the dst buffer.

 Handle short lengths. */

 Copy 8 bytes and ensure the offset >= 8 when there can be overlap. */

 No risk of overwrite. */

 Wildcopy until we get close to the end. */

 Handle the leftovers. */

/* ZSTD_execSequenceEnd():

 * This version handles cases that are near the end of the output buffer. It requires

 * more careful checks to make sure there is no overflow. By separating out these hard

 * and unlikely cases, we can speed up the common cases.

 *

 * NOTE: This function needs to be fast for a single long sequence, but doesn't need

 * to be optimized for many small sequences, since those fall into ZSTD_execSequence().

 bounds checks : careful of address space overflow in 32-bit mode */

 copy literals */

 copy Match */

 offset beyond prefix */

 span extDict & currentPrefixSegment */

 risk : address space overflow (32-bits) */

 risk : address space underflow on oend=NULL */

 Precondition */);

 No underflow */);

    /* Handle edge cases in a slow path:

     *   - Read beyond end of literals

     *   - Match end is within WILDCOPY_OVERLIMIT of oend

     *   - 32-bit mode and the match length overflows

 Assumptions (everything else goes into ZSTD_execSequenceEnd()) */

 No overflow */);

 Non-zero match & no overflow */);

 No underflow */);

 Literal length is in bounds */);

 Can wildcopy literals */);

 Can wildcopy matches */);

    /* Copy Literals:

     * Split out litLength <= 16 since it is nearly always true. +1.6% on gcc-9.

     * We likely don't need the full 32-byte wildcopy.

 update for next sequence */

 Copy Match */

 offset beyond prefix -> go into extDict */

 span extDict & currentPrefixSegment */

 Match within prefix of 1 or more bytes */

    /* Nearly all offsets are >= WILDCOPY_VECLEN bytes, which means we can use wildcopy

     * without overlap checking.

        /* We bet on a full wildcopy for matches, since we expect matches to be

         * longer than literals (in general). In silesia, ~10% of matches are longer

         * than 16 bytes.

 Copy 8 bytes and spread the offset to be >= 8. */

 If the match length is > 8 bytes, then continue with the wildcopy. */

/* We need to add at most (ZSTD_WINDOWLOG_MAX_32 - 1) bits to read the maximum

 * offset bits. But we can only read at most (STREAM_ACCUMULATOR_MIN_32 - 1)

 * bits before reloading. This value is the maximum number of bytes we read

 * after reloading when we are decoding long offsets.

 sequence */

 to avoid another reload */

>0*/);   
 0 is not valid; input is corrupted; force offset to 1 */

>0*/);

 Ensure there are enough bits to read the rest of data in 64-bit mode. */

>0*/);

        seq.match = matchBase + pos - seq.offset;  /* note : this operation can overflow when seq.offset is really too large, which can only happen when input is corrupted.

    /* ANS state update

     * gcc-9.0.0 does 2.5% worse with ZSTD_updateFseStateWithDInfo().

     * clang-9.2.0 does 7% worse with ZSTD_updateFseState().

     * Naturally it seems like ZSTD_updateFseStateWithDInfo() should be the

     * better option, so it is the default for other compilers. But, if you

     * measure that it is worse, please put up a pull request.

 <=  9 bits */

 <=  9 bits */

 <= 18 bits */

 <=  8 bits */

 <=  9 bits */

 <=  9 bits */

 <= 18 bits */

 <=  8 bits */

 No dictionary used. */

 Dictionary is our prefix. */

 Dictionary is not our ext-dict. */

 Dictionary is not within our window size. */

 Dictionary is active. */

 Offset must be within the dictionary. */

 Offset must be within our window. */

 Regen sequences */

        /* Align the decompression loop to 32 + 16 bytes.

         *

         * zstd compiled with gcc-9 on an Intel i9-9900k shows 10% decompression

         * speed swings based on the alignment of the decompression loop. This

         * performance swing is caused by parts of the decompression loop falling

         * out of the DSB. The entire decompression loop should fit in the DSB,

         * when it can't we get much worse performance. You can measure if you've

         * hit the good case or the bad case with this perf command for some

         * compressed file test.zst:

         *

         *   perf stat -e cycles -e instructions -e idq.all_dsb_cycles_any_uops \

         *             -e idq.all_mite_cycles_any_uops -- ./zstd -tq test.zst

         *

         * If you see most cycles served out of the MITE you've hit the bad case.

         * If you see most cycles served out of the DSB you've hit the good case.

         * If it is pretty even then you may be in an okay case.

         *

         * I've been able to reproduce this issue on the following CPUs:

         *   - Kabylake: Macbook Pro (15-inch, 2019) 2.4 GHz Intel Core i9

         *               Use Instruments->Counters to get DSB/MITE cycles.

         *               I never got performance swings, but I was able to

         *               go from the good case of mostly DSB to half of the

         *               cycles served from MITE.

         *   - Coffeelake: Intel i9-9900k

         *

         * I haven't been able to reproduce the instability or DSB misses on any

         * of the following CPUS:

         *   - Haswell

         *   - Broadwell: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GH

         *   - Skylake

         *

         * If you are seeing performance stability this script can help test.

         * It tests on 4 commits in zstd where I saw performance change.

         *

         *   https://gist.github.com/terrelln/9889fc06a423fd5ca6e99351564473f4

            /* gcc and clang both don't like early returns in this loop.

             * Instead break and check for an error at the end of the loop.

 check if reached exact end */

 save reps for next block */

 last literal segment */

 ZSTD_FORCE_DECOMPRESS_SEQUENCES_LONG */

 Regen sequences */

 prepare in advance */

 note : it's safe to invoke PREFETCH() on any memory address, including invalid ones */

 decode and decompress */

 note : it's safe to invoke PREFETCH() on any memory address, including invalid ones */

 finish queue */

 save reps for next block */

 last literal segment */

 ZSTD_FORCE_DECOMPRESS_SEQUENCES_SHORT */

 ZSTD_FORCE_DECOMPRESS_SEQUENCES_LONG */

 ZSTD_FORCE_DECOMPRESS_SEQUENCES_SHORT */

 DYNAMIC_BMI2 */

 ZSTD_FORCE_DECOMPRESS_SEQUENCES_LONG */

/* ZSTD_decompressSequencesLong() :

 * decompression function triggered when a minimum share of offsets is considered "long",

 * aka out of cache.

 * note : "long" definition seems overloaded here, sometimes meaning "wider than bitstream register", and sometimes meaning "farther than memory cache distance".

 ZSTD_FORCE_DECOMPRESS_SEQUENCES_SHORT */

/* ZSTD_getLongOffsetsShare() :

 * condition : offTable must be valid

 * @return : "share" of long offsets (arbitrarily defined as > (1<<23))

 max not too large */

 scale to OffFSELog */

 blockType == blockCompressed */

    /* isLongOffset must be true if there are long offsets.

     * Offsets are long if they are larger than 2^STREAM_ACCUMULATOR_MIN.

     * We don't expect that to be the case in 64-bit mode.

     * In block mode, window size is not known, so we have to be conservative.

     * (note: but it could be evaluated from current-lowLimit)

 Decode literals section */

 Build Decoding Tables */

        /* These macros control at build-time which decompressor implementation

         * we use. If neither is defined, we do some inspection and dispatch at

         * runtime.

 could probably use a larger nbSeq limit */

 heuristic values, correspond to 2.73% and 7.81% */

 else */

 not contiguous */

 frame */ 0);

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/* ***************************************************************

*  Tuning parameters

/*!

 * HEAPMODE :

 * Select how default decompression function ZSTD_decompress() allocates its context,

 * on stack (0), or into heap (1, default; requires malloc()).

 * Note that functions with explicit context such as ZSTD_decompressDCtx() are unaffected.

/*!

*  LEGACY_SUPPORT :

*  if set to 1+, ZSTD_decompress() can decode older formats (v0.1+)

/*!

 *  MAXWINDOWSIZE_DEFAULT :

 *  maximum window size accepted by DStream __by default__.

 *  Frames requiring more memory will be rejected.

 *  It's possible to set a different limit using ZSTD_DCtx_setMaxWindowSize().

/*!

 *  NO_FORWARD_PROGRESS_MAX :

 *  maximum allowed nb of calls to ZSTD_decompressStream()

 *  without any forward progress

 *  (defined as: no byte read from input, and no byte flushed to output)

 *  before triggering an error.

/*-*******************************************************

*  Dependencies

 ZSTD_memcpy, ZSTD_memmove, ZSTD_memset */

 bmi2 */

 low level memory routines */

 xxh64_reset, xxh64_update, xxh64_digest, XXH64 */

 blockProperties_t */

 ZSTD_DCtx */

 ZSTD_DDictDictContent */

 ZSTD_decompressBlock_internal */

/* ***********************************

 * Multiple DDicts Hashset internals *

#define DDICT_HASHSET_MAX_LOAD_FACTOR_SIZE_MULT 3   /* These two constants represent SIZE_MULT/COUNT_MULT load factor without using a float.

                                                     * Currently, that means a 0.75 load factor.

                                                     * So, if count * COUNT_MULT / size * SIZE_MULT != 0, then we've exceeded

                                                     * the load factor of the ddict hash set.

/* Hash function to determine starting position of dict insertion within the table

 * Returns an index between [0, hashSet->ddictPtrTableSize]

 DDict ptr table size is a multiple of 2, use size - 1 as mask to get index within [0, hashSet->ddictPtrTableSize) */

/* Adds DDict to a hashset without resizing it.

 * If inserting a DDict with a dictID that already exists in the set, replaces the one in the set.

 * Returns 0 if successful, or a zstd error code if something went wrong.

 Replace existing ddict if inserting ddict with same dictID */

/* Expands hash table by factor of DDICT_HASHSET_RESIZE_FACTOR and

 * rehashes all values, allocates new table, frees old table.

 * Returns 0 on success, otherwise a zstd error code.

/* Fetches a DDict with the given dictID

 * Returns the ZSTD_DDict* with the requested dictID. If it doesn't exist, then returns NULL.

 currDictID == 0 implies a NULL ddict entry */

 Goes to start of table when we reach the end */

/* Allocates space for and returns a ddict hash set

 * The hash set's ZSTD_DDict* table has all values automatically set to NULL to begin with.

 * Returns NULL if allocation failed.

/* Frees the table of ZSTD_DDict* within a hashset, then frees the hashset itself.

 * Note: The ZSTD_DDict* within the table are NOT freed.

/* Public function: Adds a DDict into the ZSTD_DDictHashSet, possibly triggering a resize of the hash set.

 * Returns 0 on success, or a ZSTD error.

/*-*************************************************************

*   Context management

 support sizeof NULL */

 only supports formats ZSTD_f_zstd1 and ZSTD_f_zstd1_magicless */

 8-aligned */

 minimum size */

 support free on NULL */

 no longer useful */

 no need to copy workspace */

/* Given a dctx with a digested frame params, re-selects the correct ZSTD_DDict based on

 * the requested dict ID from the frame. If there exists a reference to the correct ZSTD_DDict, then

 * accordingly sets the ddict to be used to decompress the frame.

 *

 * If no DDict is found, then no action is taken, and the ZSTD_DCtx::ddict remains as-is.

 *

 * ZSTD_d_refMultipleDDicts must be enabled for this function to be called.

/*-*************************************************************

 *   Frame header decoding

/*! ZSTD_isFrame() :

 *  Tells if the content of `buffer` starts with a valid Frame Identifier.

 *  Note : Frame Identifier is 4 bytes. If `size < 4`, @return will always be 0.

 *  Note 2 : Legacy Frame Identifiers are considered valid only if Legacy Support is enabled.

/* ZSTD_frameHeaderSize_internal() :

 *  srcSize must be large enough to reach header size fields.

 *  note : only works for formats ZSTD_f_zstd1 and ZSTD_f_zstd1_magicless.

 * @return : size of the Frame Header

/* ZSTD_frameHeaderSize() :

 *  srcSize must be >= ZSTD_frameHeaderSize_prefix.

 * @return : size of the Frame Header,

/* ZSTD_getFrameHeader_advanced() :

 *  decode Frame Header, or require larger `srcSize`.

 *  note : only works for formats ZSTD_f_zstd1 and ZSTD_f_zstd1_magicless

 * @return : 0, `zfhPtr` is correctly filled,

 *          >0, `srcSize` is too small, value is wanted `srcSize` amount,

 not strictly necessary, but static analyzer do not understand that zfhPtr is only going to be read only if return value is zero, since they are 2 different signals */

 skippable frame */

 magic number + frame length */

 ensure there is enough `srcSize` to fully read/decode frame header */

 impossible */

 impossible */

/* ZSTD_getFrameHeader() :

 *  decode Frame Header, or require larger `srcSize`.

 *  note : this function does not consume input, it only reads it.

 * @return : 0, `zfhPtr` is correctly filled,

 *          >0, `srcSize` is too small, value is wanted `srcSize` amount,

/* ZSTD_getFrameContentSize() :

 *  compatible with legacy mode

 * @return : decompressed size of the single frame pointed to be `src` if known, otherwise

 *         - ZSTD_CONTENTSIZE_UNKNOWN if the size cannot be determined

/* ZSTD_findDecompressedSize() :

 *  compatible with legacy mode

 *  `srcSize` must be the exact length of some number of ZSTD compressed and/or

 *      skippable frames

 check for overflow */

 while (srcSize >= ZSTD_frameHeaderSize_prefix) */

/* ZSTD_getDecompressedSize() :

 *  compatible with legacy mode

 * @return : decompressed size if known, 0 otherwise

             note : 0 can mean any of the following :

                   - frame content is empty

                   - decompressed size field is not present in frame header

                   - frame header unknown / not supported

/* ZSTD_decodeFrameHeader() :

 * `headerSize` must be the size provided by ZSTD_frameHeaderSize().

 * If multiple DDict references are enabled, also will choose the correct DDict to use.

 invalid header */

 Reference DDict requested by frame if dctx references multiple ddicts */

    /* Skip the dictID check in fuzzing mode, because it makes the search

     * harder.

 Extract Frame Header */

 Iterate over each block */

 Final frame content checksum */

/* ZSTD_findFrameCompressedSize() :

 *  compatible with legacy mode

 *  `src` must point to the start of a ZSTD frame, ZSTD legacy frame, or skippable frame

 *  `srcSize` must be at least as large as the frame contained

/* ZSTD_decompressBound() :

 *  compatible with legacy mode

 *  `src` must point to the start of a ZSTD frame or a skippeable frame

 *  `srcSize` must be at least as large as the frame contained

 *  @return : the maximum decompressed size of the compressed source

 Iterate over each frame */

/*-*************************************************************

 *   Frame decoding

/* ZSTD_insertBlock() :

/*! ZSTD_decompressFrame() :

 * @dctx must be properly initialized

 *  will update *srcPtr and *srcSizePtr,

 check */

 Frame Header */

 Loop on each block */

 frame */ 1);

 Frame content checksum verification */

 streaming */ 0);

 Allow caller to get size read */

 either dict or ddict set, not both */

 we were called from ZSTD_decompress_usingDDict */

            /* this will initialize correctly with no dict if dict == NULL, so

 while (srcSize >= ZSTD_frameHeaderSize_prefix) */

 Impossible */);

 stack mode */

/*-**************************************

*   Advanced Streaming Decompression API

*   Bufferless and synchronous

/*

 * Similar to ZSTD_nextSrcSizeToDecompress(), but when when a block input can be streamed,

 * we allow taking a partial block as the input. Currently only raw uncompressed blocks can

 * be streamed.

 *

 * For blocks that can be streamed, this allows us to reduce the latency until we produce

 * output, and avoid copying the input.

 *

 * @param inputSize - The total amount of input that the caller currently has.

 should not happen */

/* ZSTD_decompressContinue() :

 *  srcSize : must be the exact nb of bytes expected (see ZSTD_nextSrcSizeToDecompress())

 *  @return : nb of bytes generated into `dst` (necessarily <= `dstCapacity)

 Sanity check */

 allows header */

 to read skippable magic number */

 skippable frame */

 remaining to load to get full skippable frame header */

 empty block */

 end of frame */

 jump to next header */

 frame */ 1);

 Streaming not supported */

 Streaming not supported */

 should never happen */

 Stay on the same stage until we are finished streaming the block. */

 end of frame */

 another round for frame checksum */

 streaming */ 1);

 ends here */

 guaranteed by dctx->expected */

 streaming */ 1);

 complete skippable header */

 note : dctx->expected can grow seriously large, beyond local buffer size */

 impossible */

 some compiler require default to do something */

/*! ZSTD_loadDEntropy() :

 *  dict : must point at beginning of a valid zstd dictionary.

 dict must be valid */

 skip header = magic + dictID */

 use fse tables as temporary workspace; implies fse tables are grouped together */

 in minimal huffman, we always use X1 variants */

 bmi2 */0);

 bmi2 */ 0);

 bmi2 */ 0);

 pure content mode */

 load entropy tables */

 reference dictionary content */

 dctx->format must be properly set */

 cover both little and big endian */

 initial repcodes */

 ======   ZSTD_DDict   ====== */

 NULL ddict is equivalent to no dictionary */

/*! ZSTD_getDictID_fromDict() :

 *  Provides the dictID stored within dictionary.

 *  if @return == 0, the dictionary is not conformant with Zstandard specification.

/*! ZSTD_getDictID_fromFrame() :

 *  Provides the dictID required to decompress frame stored within `src`.

 *  If @return == 0, the dictID could not be decoded.

 *  This could for one of the following reasons :

 *  - The frame does not require a dictionary (most common case).

 *  - The frame was built with dictID intentionally removed.

 *    Needed dictionary is a hidden information.

 *    Note : this use case also happens when using a non-conformant dictionary.

 *  - `srcSize` is too small, and as a result, frame header could not be decoded.

 *    Note : possible if `srcSize < ZSTD_FRAMEHEADERSIZE_MAX`.

 *  - This is not a Zstandard frame.

 *  When identifying the exact failure cause, it's possible to use

/*! ZSTD_decompress_usingDDict() :

*   Decompression using a pre-digested Dictionary

 pass content and size in case legacy frames are encountered */

/*=====================================

*   Streaming decompression

 ***  Initialization  *** */

/* ZSTD_initDStream_usingDict() :

 * return : expected size, aka ZSTD_startingInputLength().

 note : this variant can't fail */

/* ZSTD_initDStream_usingDDict() :

 * ddict will just be referenced, and must outlive decompression session

/* ZSTD_resetDStream() :

 * return : expected size, aka ZSTD_startingInputLength().

 Impossible: ddictSet cannot have been allocated if static dctx */

/* ZSTD_DCtx_setMaxWindowSize() :

 * note : no direct equivalence in ZSTD_DCtx_setParameter,

/* ZSTD_dParam_withinBounds:

 * @return 1 if value is within dParam bounds,

 no block can be larger */

 note : should be user-selectable, but requires an additional parameter (or a dctx) */

 *****   Decompression   ***** */

 Checks that the output buffer hasn't changed if ZSTD_obm_stable is used. */

 No requirement when ZSTD_obm_stable is not enabled. */

    /* Any buffer is allowed in zdss_init, this must be the same for every other call until

     * the context is reset.

 The buffer must match our expectation exactly. */

/* Calls ZSTD_decompressContinue() with the right parameters for ZSTD_decompressStream()

 * and updates the stage and the output buffer state. This call is extracted so it can be

 * used both when reading directly from the ZSTD_inBuffer, and in buffered input mode.

 * NOTE: You must break after calling this function since the streamStage is modified.

 Write directly into the output buffer */

 Flushing is not needed. */

 error */

 need more input */

 if hSize!=0, hSize > zds->lhSize */

 not enough input to load full header */

 remaining header bytes + next block header */

 check for single-pass mode opportunity */

 shortcut : using single-pass mode */

 Check output buffer is large enough for ZSTD_odm_stable. */

 Consume header (see ZSTDds_decodeFrameHeader) */

 skippable frame */

 control buffer memory usage */

 Adapt buffer sizes to frame header instructions */

 frame checksum */);

 static DCtx */

 controlled at init */

 end of frame */

 decode directly from src */

 Function modifies the stage so we must break */

 no more input */

 At this point we shouldn't be decompressing a block that we can stream. */

 not enough input, wait for more */

 decode loaded input */

 input is consumed */

 Function modifies the stage so we must break */

 flush completed */

 cannot complete flush */

 impossible */

 some compiler require default to do something */

 result */

 Update the expected output buffer for ZSTD_obm_stable. */

 no forward progress */

 frame fully decoded */

 output fully flushed */

 can't release hostage (not present) */

 release hostage */

 zds->hostageByte */

 zds->outEnd == zds->outStart */

 output not fully flushed; keep last byte as hostage; will be released when all output is flushed */

 note : pos > 0, otherwise, impossible to finish reading last block */

 nextSrcSizeHint==0 */

 preload header of next block */

 part already loaded*/

 ZSTD_compress_generic() will check validity of dstPos and srcPos */

/*

 * Copyright (c) Yann Collet, Facebook, Inc.

 * All rights reserved.

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/* zstd_ddict.c :

/*-*******************************************************

*  Dependencies

 ZSTD_memcpy, ZSTD_memmove, ZSTD_memset */

 bmi2 */

 low level memory routines */

/*-*******************************************************

*  Types

 typedef'd to ZSTD_DDict within "zstd.h" */

 only accept specified dictionaries */

 pure content mode */

 only accept specified dictionaries */

 pure content mode */

 load entropy tables */

 cover both little and big endian */

 parse dictionary content */

/*! ZSTD_createDDict() :

*   Create a digested dictionary, to start decompression without startup delay.

*   `dict` content is copied inside DDict.

/*! ZSTD_createDDict_byReference() :

 *  Create a digested dictionary, to start decompression without startup delay.

 *  Dictionary content is simply referenced, it will be accessed during decompression.

 8-aligned */

 local copy */

 support free on NULL */

/*! ZSTD_estimateDDictSize() :

 *  Estimate amount of memory that will be needed to create a dictionary for decompression.

 support sizeof on NULL */

/*! ZSTD_getDictID_fromDDict() :

 *  Provides the dictID of the dictionary loaded into `ddict`.

 *  If @return == 0, the dictionary is not conformant to Zstandard specification, or empty.

/* ******************************************************************

 * huff0 huffman decoder,

 * part of Finite State Entropy library

 * Copyright (c) Yann Collet, Facebook, Inc.

 *

 *  You can contact the author at :

 *  - FSE+HUF source repository : https://github.com/Cyan4973/FiniteStateEntropy

 *

 * This source code is licensed under both the BSD-style license (found in the

 * LICENSE file in the root directory of this source tree) and the GPLv2 (found

 * in the COPYING file in the root directory of this source tree).

 * You may select, at your option, one of the above-listed licenses.

/* **************************************************************

*  Dependencies

 ZSTD_memcpy, ZSTD_memset */

 BIT_* */

 to compress headers */

/* **************************************************************

*  Macros

/* These two optional macros force the use one way or another of the two

 * Huffman decompression implementations. You can't force in both directions

 * at the same time.

/* **************************************************************

*  Error Management

/* **************************************************************

*  Byte alignment for workSpace management

/* **************************************************************

*  BMI2 Variant Wrappers

-***************************/

  generic DTableDesc       */

-***************************/

-***************************/

  single-symbol decoding   */

-***************************/

 single-symbol decoding */

/*

 * Packs 4 HUF_DEltX1 structs into a U64. This is used to lay down 4 entries at

 * a time.

 bmi2 */ 0);

 ZSTD_memset(huffWeight, 0, sizeof(huffWeight)); */   
 Table header */

 DTable too small, Huffman tree cannot fit in */

    /* Compute symbols and rankStart given rankVal:

     *

     * rankVal already contains the number of values of each weight.

     *

     * symbols contains the symbols ordered by weight. First are the rankVal[0]

     * weight 0 symbols, followed by the rankVal[1] weight 1 symbols, and so on.

     * symbols[0] is filled (but unused) to avoid a branch.

     *

     * rankStart contains the offset where each rank belongs in the DTable.

     * rankStart[0] is not filled because there are no entries in the table for

     * weight 0.

    /* fill DTable

     * We fill all entries of each weight in order.

     * That way length is a constant for each iteration of the outter loop.

     * We can switch based on the length to a different inner loop which is

     * optimized for that particular case.

 note : dtLog >= 1 */

 up to 4 symbols at a time */

 [0-3] symbols remaining */

 no more data to retrieve from bitstream, no need to reload */

 Check */

 strict minimum : jump table + 1 byte per stream */

 Init */

 jumpTable */

 overflow */

 up to 16 symbols per loop (4 symbols per stream) in 64-bit mode */

 check corruption */

        /* note : should not be necessary : op# advance in lock step, and we control op4.

 note : op4 supposed already verified within main loop */

 finish bitStreams one by one */

 check */

 decoded size */

 bmi2 */ 0);

 bmi2 */ 0);

 bmi2 */ 0);

 HUF_FORCE_DECOMPRESS_X2 */

 *************************/

 double-symbols decoding */

 *************************/

 double-symbols decoding */

/* HUF_fillDTableX2Level2() :

 get pre-calculated rankVal */

 fill skipped values */

 fill DTable */

 note : sortedSymbols already skipped */

 since length >= 1 */

 note : targetLog >= srcLog, hence scaleLog <= 1 */

 fill DTable */

 enough room for a second symbol */

 force compiler to avoid strict-aliasing */

 if compiler fails here, assertion is wrong */

 ZSTD_memset(weightList, 0, sizeof(weightList)); */  
 bmi2 */ 0);

 check result */

 DTable can't fit code depth */

 find maxWeight */

 necessarily finds a solution before 0 */

 Get start index of each weight */

 put all 0w symbols at the end of sorted list*/

 sort symbols by weight */

 forget 0w symbols; this is beginning of weight(1) */

 Build rankVal */

 tableLog <= maxTableLog */

 note : dtLog >= 1 */

 note : dtLog >= 1 */

 ugly hack; works only because it's the last symbol. Note : can't easily extract nbBits from just this symbol */

 up to 8 symbols at a time */

 closer to end : up to 2 symbols at a time */

 no need to reload : reached the end of DStream */

 Init */

 decode */

 force compiler to not use strict-aliasing */

 check */

 decoded size */

 strict minimum : jump table + 1 byte per stream */

 Init */

 jumpTable */

 overflow */

 16-32 symbols per loop (4-8 symbols per stream) */

 check corruption */

 note : op4 already verified within main loop */

 finish bitStreams one by one */

 check */

 decoded size */

 bmi2 */ 0);

 bmi2 */ 0);

 bmi2 */ 0);

 bmi2 */ 0);

 HUF_FORCE_DECOMPRESS_X1 */

 ***********************************/

 Universal decompression selectors */

 ***********************************/

 bmi2 */ 0);

 bmi2 */ 0);

 bmi2 */ 0) :

 bmi2 */ 0);

 bmi2 */ 0);

 bmi2 */ 0);

 bmi2 */ 0) :

 bmi2 */ 0);

 Quantization */][3 
 single, double, quad */

 Q==0 : impossible */

 Q==1 : impossible */

 Q == 2 : 12-18% */

 Q == 3 : 18-25% */

 Q == 4 : 25-32% */

 Q == 5 : 32-38% */

 Q == 6 : 38-44% */

 Q == 7 : 44-50% */

 Q == 8 : 50-56% */

 Q == 9 : 56-62% */

 Q ==10 : 62-69% */

 Q ==11 : 69-75% */

 Q ==12 : 75-81% */

 Q ==13 : 81-87% */

 Q ==14 : 87-93% */

 Q ==15 : 93-99% */

/* HUF_selectDecoder() :

 *  Tells which decoder is likely to decode faster,

 *  based on a set of pre-computed metrics.

 * @return : 0==HUF_decompress4X1, 1==HUF_decompress4X2 .

 decoder timing evaluation */

 Q < 16 */

 advantage to algorithm using less memory, to reduce cache eviction */

 validation checks */

 validation checks */

 invalid */

 not compressed */

 RLE */

 validation checks */

/* inflate.c -- zlib decompression

 * Copyright (C) 1995-2005 Mark Adler

 * For conditions of distribution and use, see copyright notice in zlib.h

 *

 * Based on zlib 1.2.3 but modified for the Linux Kernel by

 * Richard Purdie <richard@openedhand.com>

 *

 * Changes mainly for static instead of dynamic memory allocation

 *

 architecture-specific bits */

 to support ill-conceived Java test suite */

 Initialise Window */

 in case we return an error */

    /*

     * DFLTCC requires the window to be page aligned.

     * Thus, we overallocate and take the aligned portion of the buffer.

/*

   Return state with length and distance decoding tables and index sizes set to

   fixed code decoding.  This returns fixed tables from inffixed.h.

/*

   Update the window with the last wsize (normally 32K) bytes written before

   returning. This is only called when a window is already in use, or when

   output has been written during this inflate call, but the end of the deflate

   stream has not been reached yet. It is also called to window dictionary data

   when a dictionary is loaded.



   Providing output buffers larger than 32K to inflate() should provide a speed

   advantage, since only the last 32K of output is copied to the sliding window

   upon return from inflate(), and since all distances after the first 32K of

   output will fall in the output data, making match copies simpler and faster.

   The advantage may be dependent on the size of the processor's data caches.

 copy state->wsize or less output bytes into the circular window */

/*

 * At the end of a Deflate-compressed PPP packet, we expect to have seen

 * a `stored' block type value but not the (zero) length bytes.

/*

   Returns true if inflate is currently at the end of a block generated by

   Z_SYNC_FLUSH or Z_FULL_FLUSH. This function is used by one PPP

   implementation to provide an additional safety check. PPP uses

   Z_SYNC_FLUSH but removes the length bytes of the resulting empty stored

   block. When decompressing, PPP checks that at the end of input packet,

   inflate is waiting for these length bytes.

 Macros for inflate(): */

 check function to use adler32() for zlib or crc32() for gzip */

 Load registers with state in inflate() for speed */

 Restore state from registers in inflate() */

 Clear the input bit accumulator */

/* Get a byte of input into the bit accumulator, or return from inflate()

/* Assure that there are at least n bits in the bit accumulator.  If there is

 Return the low n bits of the bit accumulator (n < 16) */

 Remove n bits from the bit accumulator */

 Remove zero to seven bits as needed to go to a byte boundary */

/*

   inflate() uses a state machine to process as much input data and generate as

   much output data as possible before returning.  The state machine is

   structured roughly as follows:



    for (;;) switch (state) {

    ...

    case STATEn:

        if (not enough input data or output space to make progress)

            return;

        ... make progress ...

        state = STATEm;

        break;

    ...

    }



   so when inflate() is called again, the same case is attempted again, and

   if the appropriate resources are provided, the machine proceeds to the

   next state.  The NEEDBITS() macro is usually the way the state evaluates

   whether it can proceed or should return.  NEEDBITS() does the return if

   the requested bits are not available.  The typical use of the BITS macros

   is:



        NEEDBITS(n);

        ... do something with BITS(n) ...

        DROPBITS(n);



   where NEEDBITS(n) either returns from inflate() if there isn't enough

   input left to load n bits into the accumulator, or it continues.  BITS(n)

   gives the low n bits in the accumulator.  When done, DROPBITS(n) drops

   the low n bits off the accumulator.  INITBITS() clears the accumulator

   and sets the number of available bits to zero.  BYTEBITS() discards just

   enough bits to put the accumulator on a byte boundary.  After BYTEBITS()

   and a NEEDBITS(8), then BITS(8) would return the next byte in the stream.



   NEEDBITS(n) uses PULLBYTE() to get an available byte of input, or to return

   if there is no input available.  The decoding of variable length codes uses

   PULLBYTE() directly in order to pull just enough bytes to decode the next

   code, and no more.



   Some states loop until they get enough input, making sure that enough

   state information is maintained to continue the loop where it left off

   if NEEDBITS() returns in the loop.  For example, want, need, and keep

   would all have to actually be part of the saved state in case NEEDBITS()

   returns:



    case STATEw:

        while (want < need) {

            NEEDBITS(n);

            keep[want++] = BITS(n);

            DROPBITS(n);

        }

        state = STATEx;

    case STATEx:



   As shown above, if the next state is also the next case, then the break

   is omitted.



   A state may also return if there is not enough output space available to

   complete that state.  Those states are copying stored data, writing a

   literal byte, and copying a matching string.



   When returning, a "goto inf_leave" is used to update the total counters,

   update the check value, and determine whether any progress has been made

   during that inflate() call in order to return the proper return code.

   Progress is defined as a change in either strm->avail_in or strm->avail_out.

   When there is a window, goto inf_leave will update the window with the last

   output written.  If a goto inf_leave occurs in the middle of decompression

   and there is no window currently, goto inf_leave will create one and copy

   output to the window for the next call of inflate().



   In this implementation, the flush parameter of inflate() only affects the

   return code (per zlib.h).  inflate() always writes as much as possible to

   strm->next_out, given the space available and the provided input--the effect

   documented in zlib.h of Z_SYNC_FLUSH.  Furthermore, inflate() always defers

   the allocation of and copying into a sliding window until necessary, which

   provides the effect documented in zlib.h for Z_FINISH when the entire input

   stream available.  So the only thing the flush parameter actually does is:

   when flush is set to Z_FINISH, inflate() cannot return Z_OK.  Instead it

   will return Z_BUF_ERROR if it has not reached the end of the stream.

 next input */

 next output */

 available input and output */

 bit buffer */

 bits in bit buffer */

 save starting available input and output */

 number of stored or match bytes to copy */

 where to copy match bytes from */

 current decoding table entry */

 parent table entry */

 length to copy for repeats, bits to drop */

 return code */

 permutation of code lengths */

    /* Do not check for strm->next_out == NULL here as ppc zImage

 skip check */

 stored block */

 fixed block */

 decode codes */

 dynamic block */

 go to byte boundary */

 handle error breaks in while */

 build code tables */

 copy from window */

 copy from output */

    /*

       Return from inflate(), updating the total counts and the check value.

       If there was no progress during the inflate() call, return a buffer

       error.  Call zlib_updatewindow() to create and/or update the window state.

/*

 * This subroutine adds the data at next_in/avail_in to the output history

 * without performing any output.  The output buffer must be "caught up";

 * i.e. no pending output but this should always be the case. The state must

 * be waiting on the start of a block (i.e. mode == TYPE or HEAD).  On exit,

 * the output will also be caught up, and the checksum will have been updated

 * if need be.

 Setup some variables to allow misuse of updateWindow */

 Restore saved variables */

/* Utility function: initialize zlib, unpack binary blob, clean up zlib,

 * return len or negative error code.

	/* gzip header (1f,8b,08... 10 bytes total + possible asciz filename)

	 * expected to be stripped from input

 after Z_FINISH, only Z_STREAM_END is "we unpacked it all" */

 returns Z_OK (0) if successful */

/* inffast.c -- fast decoding

 * Copyright (C) 1995-2004 Mark Adler

 * For conditions of distribution and use, see copyright notice in zlib.h

 Endian independent version */

/*

   Decode literal, length, and distance codes and write out the resulting

   literal and match bytes until either not enough input or output is

   available, an end-of-block is encountered, or a data error is encountered.

   When large enough input and output buffers are supplied to inflate(), for

   example, a 16K input buffer and a 64K output buffer, more than 95% of the

   inflate execution time is spent in this routine.



   Entry assumptions:



        state->mode == LEN

        strm->avail_in >= 6

        strm->avail_out >= 258

        start >= strm->avail_out

        state->bits < 8



   On return, state->mode is one of:



        LEN -- ran out of enough output space or enough available input

        TYPE -- reached end of block code, inflate() to interpret next block

        BAD -- error in block data



   Notes:



    - The maximum input bits used by a length/distance pair is 15 bits for the

      length code, 5 bits for the length extra, 15 bits for the distance code,

      and 13 bits for the distance extra.  This totals 48 bits, or six bytes.

      Therefore if strm->avail_in >= 6, then there is enough input to avoid

      checking for available input while decoding.



    - The maximum bytes that a single length/distance pair can output is 258

      bytes, which is the maximum length that can be coded.  inflate_fast()

      requires strm->avail_out >= 258 for each loop to avoid checking for

      output space.



    - @start:	inflate()'s starting value for strm->avail_out

 local strm->next_in */

 while in < last, enough input available */

 local strm->next_out */

 inflate()'s initial strm->next_out */

 while out < end, enough space available */

 maximum distance from zlib header */

 window size or zero if not using window */

 valid bytes in the window */

 window write index */

 allocated sliding window, if wsize != 0 */

 local strm->hold */

 local strm->bits */

 local strm->lencode */

 local strm->distcode */

 mask for first level of length codes */

 mask for first level of distance codes */

 retrieved table entry */

 code bits, operation, extra bits, or */

  window position, window bytes to copy */

 match length, unused bytes */

 match distance */

 where to copy match from */

 copy state to local variables */

    /* decode literals and length/distances until end-of-block or not enough

 literal */

 length base */

 number of extra bits */

 distance base */

 number of extra bits */

 max distance in output */

 see if copy from window */

 distance back in window */

 very common case */

 some from window */

 rest from output */

 wrap around window */

 some from end of window */

 some from start of window */

 rest from output */

 contiguous in window */

 some from window */

 rest from output */

 copy direct from output */

 minimum length is three */

 Align out addr */

 dist == 1 or dist == 2 */

 copy one char pattern to both bytes */

 2nd level distance code */

 2nd level length code */

 end-of-block */

 return unused bytes (on entry, bits < 8, so in won't go too far back) */

 update state and return */

/*

   inflate_fast() speedups that turned out slower (on a PowerPC G3 750CXe):

   - Using bit fields for code structure

   - Different op definition to avoid & for extra bits (do & for table bits)

   - Three separate decoding do-loops for direct, window, and write == 0

   - Special case for distance > 1 copies to do overlapped load and store copy

   - Explicit branch predictions (based on measured branch probabilities)

   - Deferring match copy and interspersed it with decoding subsequent codes

   - Swapping literal/length else

   - Swapping window/direct else

   - Larger unrolled copy loops (three is about right)

   - Moving len -= 3 statement into middle of loop

 !ASMINF */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/lib/zlib_inflate/inflate_syms.c

 *

 * Exported symbols for the inflate functionality.

 *

/* inftrees.c -- generate Huffman trees for efficient decoding

 * Copyright (C) 1995-2005 Mark Adler

 * For conditions of distribution and use, see copyright notice in zlib.h

/*

   Build a set of tables to decode the provided canonical Huffman code.

   The code lengths are lens[0..codes-1].  The result starts at *table,

   whose indices are 0..2^bits-1.  work is a writable array of at least

   lens shorts, which is used as a work area.  type is the type of code

   to be generated, CODES, LENS, or DISTS.  On return, zero is success,

   -1 is an invalid code, and +1 means that ENOUGH isn't enough.  table

   on return points to the next available entry's address.  bits is the

   requested root table index bits, and on return it is the actual root

   table index bits.  It will differ if the request is greater than the

   longest code or if it is less than the shortest code.

 a code's length in bits */

 index of code symbols */

 minimum and maximum code lengths */

 number of index bits for root table */

 number of index bits for current table */

 code bits to drop for sub-table */

 number of prefix codes available */

 code entries in table used */

 Huffman code */

 for incrementing code, index */

 index for replicating entries */

 low bits for current root entry */

 mask for low root bits */

 table entry for duplication */

 next available space in table */

 base value table to use */

 extra bits table to use */

 use base and extra for symbol > end */

 number of codes of each length */

 offsets in table for each length */

 Length codes 257..285 base */

 Length codes 257..285 extra */

 Distance codes 0..29 base */

 Distance codes 0..29 extra */

    /*

       Process a set of code lengths to create a canonical Huffman code.  The

       code lengths are lens[0..codes-1].  Each length corresponds to the

       symbols 0..codes-1.  The Huffman code is generated by first sorting the

       symbols by length from short to long, and retaining the symbol order

       for codes with equal lengths.  Then the code starts with all zero bits

       for the first code of the shortest length, and the codes are integer

       increments for the same length, and zeros are appended as the length

       increases.  For the deflate format, these bits are stored backwards

       from their more natural integer increment ordering, and so when the

       decoding tables are built in the large loop below, the integer codes

       are incremented backwards.



       This routine assumes, but does not check, that all of the entries in

       lens[] are in the range 0..MAXBITS.  The caller must assure this.

       1..MAXBITS is interpreted as that code length.  zero means that that

       symbol does not occur in this code.



       The codes are sorted by computing a count of codes for each length,

       creating from that a table of starting indices for each length in the

       sorted table, and then entering the symbols in order in the sorted

       table.  The sorted table is work[], with that space being provided by

       the caller.



       The length counts are used for other purposes as well, i.e. finding

       the minimum and maximum length codes, determining if there are any

       codes at all, checking for a valid set of lengths, and looking ahead

       at length counts to determine sub-table sizes when building the

       decoding tables.

 accumulate lengths for codes (assumes lens[] all in 0..MAXBITS) */

 bound code lengths, force root to be within code lengths */

 no symbols to code at all */

 invalid code marker */

 make a table to force an error */

 no symbols, but wait for decoding to report error */

 check for an over-subscribed or incomplete set of lengths */

 over-subscribed */

 incomplete set */

 generate offsets into symbol table for each length for sorting */

 sort symbols by length, by symbol order within each length */

    /*

       Create and fill in decoding tables.  In this loop, the table being

       filled is at next and has curr index bits.  The code being used is huff

       with length len.  That code is converted to an index by dropping drop

       bits off of the bottom.  For codes where len is less than drop + curr,

       those top drop + curr - len bits are incremented through all values to

       fill the table with replicated entries.



       root is the number of index bits for the root table.  When len exceeds

       root, sub-tables are created pointed to by the root entry with an index

       of the low root bits of huff.  This is saved in low to check for when a

       new sub-table should be started.  drop is zero when the root table is

       being filled, and drop is root when sub-tables are being filled.



       When a new sub-table is needed, it is necessary to look ahead in the

       code lengths to determine what size sub-table is needed.  The length

       counts are used for this, and so count[] is decremented as codes are

       entered in the tables.



       used keeps track of how many table entries have been allocated from the

       provided *table space.  It is checked when a LENS table is being made

       against the space in *table, ENOUGH, minus the maximum space needed by

       the worst case distance code, MAXD.  This should never happen, but the

       sufficiency of ENOUGH has not been proven exhaustively, hence the check.

       This assumes that when type == LENS, bits == 9.



       sym increments through all symbols, and the loop terminates when

       all codes of length max, i.e. all codes, have been processed.  This

       routine permits incomplete codes, so another loop after this one fills

       in the rest of the decoding tables with invalid code markers.

 set up for code type */

 dummy value--not used */

 DISTS */

 initialize state for loop */

 starting code */

 starting code symbol */

 starting code length */

 current table to fill in */

 current table index bits */

 current bits to drop from code for index */

 trigger new sub-table when len > root */

 use root table entries */

 mask for comparing low */

 check available table space */

 process all codes and make table entries */

 create table entry */

 end of block */

 replicate for those indices with low len bits equal to huff */

 save offset to next table */

 backwards increment the len-bit code huff */

 go to next symbol, update count, len */

 create new sub-table if needed */

 if first time, transition to sub-tables */

 increment past last table */

 here min is 1 << curr */

 determine length of next table */

 check for enough space */

 point entry in root table to sub-table */

    /*

       Fill in rest of table for incomplete codes.  This loop is similar to the

       loop above in incrementing huff for table indices.  It is assumed that

       len is equal to curr + drop, so there is no loop needed to increment

       through high index bits.  When the current sub-table is filled, the loop

       drops back to the root table to fill in any remaining entries there.

 invalid code marker */

 when done with sub-table, drop back to root table */

 put invalid code marker in table */

 backwards increment the len-bit code huff */

 set return parameters */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018-2019, Intel Corporation. */

/* Internal structure used to store details about the PLDM image file as it is

 * being validated and processed.

 current offset of firmware image */

 PLDM Firmware Package Header */

 length of the component bitmap */

 Start of the component image information */

 Start pf the firmware device id records */

 The CRC at the end of the package header */

/**

 * pldm_check_fw_space - Verify that the firmware image has space left

 * @data: pointer to private data

 * @offset: offset to start from

 * @length: length to check for

 *

 * Verify that the firmware data can hold a chunk of bytes with the specified

 * offset and length.

 *

 * Returns: zero on success, or -EFAULT if the image does not have enough

 * space left to fit the expected length.

/**

 * pldm_move_fw_offset - Move the current firmware offset forward

 * @data: pointer to private data

 * @bytes_to_move: number of bytes to move the offset forward by

 *

 * Check that there is enough space past the current offset, and then move the

 * offset forward by this amount.

 *

 * Returns: zero on success, or -EFAULT if the image is too small to fit the

 * expected length.

/**

 * pldm_parse_header - Validate and extract details about the PLDM header

 * @data: pointer to private data

 *

 * Performs initial basic verification of the PLDM image, up to the first

 * firmware record.

 *

 * This includes the following checks and extractions

 *

 *   * Verify that the UUID at the start of the header matches the expected

 *     value as defined in the DSP0267 PLDM specification

 *   * Check that the revision is 0x01

 *   * Extract the total header_size and verify that the image is large enough

 *     to contain at least the length of this header

 *   * Extract the size of the component bitmap length

 *   * Extract a pointer to the start of the record area

 *

 * Returns: zero on success, or a negative error code on failure.

	/* extract a pointer to the record area, which just follows the main

	 * PLDM header data.

/**

 * pldm_check_desc_tlv_len - Check that the length matches expectation

 * @data: pointer to image details

 * @type: the descriptor type

 * @size: the length from the descriptor header

 *

 * If the descriptor type is one of the documented descriptor types according

 * to the standard, verify that the provided length matches.

 *

 * If the type is not recognized or is VENDOR_DEFINED, return zero.

 *

 * Returns: zero on success, or -EINVAL if the specified size of a standard

 * TLV does not match the expected value defined for that TLV.

 Do not report an error on an unexpected TLV */

/**

 * pldm_parse_desc_tlvs - Check and skip past a number of TLVs

 * @data: pointer to private data

 * @record: pointer to the record this TLV belongs too

 * @desc_count: descriptor count

 *

 * From the current offset, read and extract the descriptor TLVs, updating the

 * current offset each time.

 *

 * Returns: zero on success, or a negative error code on failure.

 According to DSP0267, this only includes the data field */

 check that we have space and move the offset forward */

/**

 * pldm_parse_one_record - Verify size of one PLDM record

 * @data: pointer to image details

 * @__record: pointer to the record to check

 *

 * This function checks that the record size does not exceed either the size

 * of the firmware file or the total length specified in the header section.

 *

 * It also verifies that the recorded length of the start of the record

 * matches the size calculated by adding the static structure length, the

 * component bitmap length, the version string length, the length of all

 * descriptor TLVs, and the length of the package data.

 *

 * Returns: zero on success, or a negative error code on failure.

 Make a copy and insert it into the record list */

 Then check that we have space and move the offset */

 check that we have space for the component bitmap length */

 Scan through the descriptor TLVs and find the end */

/**

 * pldm_parse_records - Locate the start of the component area

 * @data: pointer to private data

 *

 * Extract the record count, and loop through each record, searching for the

 * component area.

 *

 * Returns: zero on success, or a negative error code on failure.

	/* Extract a pointer to the component area, which just follows the

	 * PLDM device record data.

/**

 * pldm_parse_components - Locate the CRC header checksum

 * @data: pointer to private data

 *

 * Extract the component count, and find the pointer to the component area.

 * Scan through each component searching for the end, which should point to

 * the package header checksum.

 *

 * Extract the package header CRC and save it for verification.

 *

 * Returns: zero on success, or a negative error code on failure.

 Make sure that we reached the expected offset */

/**

 * pldm_verify_header_crc - Verify that the CRC in the header matches

 * @data: pointer to private data

 *

 * Calculates the 32-bit CRC using the standard IEEE 802.3 CRC polynomial and

 * compares it to the value stored in the header.

 *

 * Returns: zero on success if the CRC matches, or -EBADMSG on an invalid CRC.

	/* Calculate the 32-bit CRC of the header header contents up to but

	 * not including the checksum. Note that the Linux crc32_le function

	 * does not perform an expected final XOR.

/**

 * pldmfw_free_priv - Free memory allocated while parsing the PLDM image

 * @data: pointer to the PLDM data structure

 *

 * Loops through and clears all allocated memory associated with each

 * allocated descriptor, record, and component.

/**

 * pldm_parse_image - parse and extract details from PLDM image

 * @data: pointer to private data

 *

 * Verify that the firmware file contains valid data for a PLDM firmware

 * file. Extract useful pointers and data from the firmware file and store

 * them in the data structure.

 *

 * The PLDM firmware file format is defined in DMTF DSP0267 1.0.0. Care

 * should be taken to use get_unaligned_le* when accessing data from the

 * pointers in data.

 *

 * Returns: zero on success, or a negative error code on failure.

 these are u32 so that we can store PCI_ANY_ID */

/**

 * pldmfw_op_pci_match_record - Check if a PCI device matches the record

 * @context: PLDM fw update structure

 * @record: list of records extracted from the PLDM image

 *

 * Determine of the PCI device associated with this device matches the record

 * data provided.

 *

 * Searches the descriptor TLVs and extracts the relevant descriptor data into

 * a pldm_pci_record_id. This is then compared against the PCI device ID

 * information.

 *

 * Returns: true if the device matches the record, false otherwise.

 Skip unrelated TLVs */

		/* A value of zero for one of the descriptors is sometimes

		 * used when the record should ignore this field when matching

		 * device. For example if the record applies to any subsystem

		 * device or vendor.

/**

 * pldm_find_matching_record - Find the first matching PLDM record

 * @data: pointer to private data

 *

 * Search through PLDM records and find the first matching entry. It is

 * expected that only one entry matches.

 *

 * Store a pointer to the matching record, if found.

 *

 * Returns: zero on success, or -ENOENT if no matching record is found.

/**

 * pldm_send_package_data - Send firmware the package data for the record

 * @data: pointer to private data

 *

 * Send the package data associated with the matching record to the firmware,

 * using the send_pkg_data operation.

 *

 * Returns: zero on success, or a negative error code on failure.

/**

 * pldm_send_component_tables - Send component table information to firmware

 * @data: pointer to private data

 *

 * Loop over each component, sending the applicable components to the firmware

 * via the send_component_table operation.

 *

 * Returns: zero on success, or a negative error code on failure.

 Skip components which are not intended for this device */

		/* determine whether this is the start, middle, end, or both

		 * the start and end of the component tables

/**

 * pldm_flash_components - Program each component to device flash

 * @data: pointer to private data

 *

 * Loop through each component that is active for the matching device record,

 * and send it to the device driver for flashing.

 *

 * Returns: zero on success, or a negative error code on failure.

 Skip components which are not intended for this device */

/**

 * pldm_finalize_update - Finalize the device flash update

 * @data: pointer to private data

 *

 * Tell the device driver to perform any remaining logic to complete the

 * device update.

 *

 * Returns: zero on success, or a PLFM_FWU error indicating the reason for

 * failure.

/**

 * pldmfw_flash_image - Write a PLDM-formatted firmware image to the device

 * @context: ops and data for firmware update

 * @fw: firmware object pointing to the relevant firmware file to program

 *

 * Parse the data for a given firmware file, verifying that it is a valid PLDM

 * formatted image that matches this device.

 *

 * Extract the device record Package Data and Component Tables and send them

 * to the device firmware. Extract and write the flash data for each of the

 * components indicated in the firmware file.

 *

 * Returns: zero on success, or a negative error code on failure.

 SPDX-License-Identifier: GPL-2.0

/*

 * rational fractions

 *

 * Copyright (C) 2009 emlix GmbH, Oskar Schirmer <oskar@scara.com>

 * Copyright (C) 2019 Trent Piepho <tpiepho@gmail.com>

 *

 * helper functions when coping with rational numbers

/*

 * calculate best rational approximation for a given fraction

 * taking into account restricted register size, e.g. to find

 * appropriate values for a pll with 5 bit denominator and

 * 8 bit numerator register fields, trying to set up with a

 * frequency ratio of 3.1415, one would say:

 *

 * rational_best_approximation(31415, 10000,

 *		(1 << 8) - 1, (1 << 5) - 1, &n, &d);

 *

 * you may look at given_numerator as a fixed point number,

 * with the fractional part size described in given_denominator.

 *

 * for theoretical background, see:

 * https://en.wikipedia.org/wiki/Continued_fraction

	/* n/d is the starting rational, which is continually

	 * decreased each iteration using the Euclidean algorithm.

	 *

	 * dp is the value of d from the prior iteration.

	 *

	 * n2/d2, n1/d1, and n0/d0 are our successively more accurate

	 * approximations of the rational.  They are, respectively,

	 * the current, previous, and two prior iterations of it.

	 *

	 * a is current term of the continued fraction.

		/* Find next term in continued fraction, 'a', via

		 * Euclidean algorithm.

		/* Calculate the current rational approximation (aka

		 * convergent), n2/d2, using the term just found and

		 * the two prior approximations.

		/* If the current convergent exceeds the maxes, then

		 * return either the previous convergent or the

		 * largest semi-convergent, the final term of which is

		 * found below as 't'.

			/* This tests if the semi-convergent is closer than the previous

			 * convergent.  If d1 is zero there is no previous convergent as this

			 * is the 1st iteration, so always choose the semi-convergent.

 SPDX-License-Identifier: GPL-2.0-only

 Lowest common multiple */

 SPDX-License-Identifier: GPL-2.0

/*

 * For a description of the algorithm please have a look at

 * include/linux/reciprocal_div.h

 ceil(log2(d)) */

	/* NOTE: mlow/mhigh could overflow u64 when l == 32. This case needs to

	 * be handled before calling "reciprocal_value_adv", please see the

	 * comment at include/linux/reciprocal_div.h.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2003 Bernardo Innocenti <bernie@develer.com>

 *

 * Based on former do_div() implementation from asm-parisc/div64.h:

 *	Copyright (C) 1999 Hewlett-Packard Co

 *	Copyright (C) 1999 David Mosberger-Tang <davidm@hpl.hp.com>

 *

 *

 * Generic C version of 64bit/32bit division and modulo, with

 * 64bit result and 32bit remainder.

 *

 * The fast case for (n>>32 == 0) is handled inline by do_div().

 *

 * Code generated for this function might be very inefficient

 * for some CPUs. __div64_32() can be overridden by linking arch-specific

 * assembly versions such as arch/ppc/lib/div64.S and arch/sh/lib/div64.S

 * or by defining a preprocessor macro in arch/include/asm/div64.h.

 Not needed on 64bit architectures */

 Reduce the thing a bit first */

/**

 * div_s64_rem - signed 64bit divide with 64bit divisor and remainder

 * @dividend:	64bit dividend

 * @divisor:	64bit divisor

 * @remainder:  64bit remainder

/**

 * div64_u64_rem - unsigned 64bit divide with 64bit divisor and remainder

 * @dividend:	64bit dividend

 * @divisor:	64bit divisor

 * @remainder:  64bit remainder

 *

 * This implementation is a comparable to algorithm used by div64_u64.

 * But this operation, which includes math for calculating the remainder,

 * is kept distinct to avoid slowing down the div64_u64 operation on 32bit

 * systems.

/**

 * div64_u64 - unsigned 64bit divide with 64bit divisor

 * @dividend:	64bit dividend

 * @divisor:	64bit divisor

 *

 * This implementation is a modified version of the algorithm proposed

 * by the book 'Hacker's Delight'.  The original source and full proof

 * can be found here and is available for use without restriction.

 *

 * 'http://www.hackersdelight.org/hdcodetxt/divDouble.c.txt'

/**

 * div64_s64 - signed 64bit divide with 64bit divisor

 * @dividend:	64bit dividend

 * @divisor:	64bit divisor

 BITS_PER_LONG == 32 */

/*

 * Iterative div/mod for use when dividend is not expected to be much

 * bigger than divisor.

 can a * b overflow ? */

		/*

		 * (b * a) / c is equal to

		 *

		 *      (b / c) * a +

		 *      (b % c) * a / c

		 *

		 * if nothing overflows. Can the 1st multiplication

		 * overflow? Yes, but we do not care: this can only

		 * happen if the end result can't fit in u64 anyway.

		 *

		 * So the code below does

		 *

		 *      res = (b / c) * a;

		 *      b = b % c;

 drop precision */

 SPDX-License-Identifier: GPL-2.0-only

	/* Betrand's Postulate (or Chebyshev's theorem) states that if n > 3,

	 * there is always at least one prime p between n and 2n - 2.

	 * Equivalently, if n > 1, then there is always at least one prime p

	 * such that n < p < 2n.

	 *

	 * http://mathworld.wolfram.com/BertrandsPostulate.html

	 * https://en.wikipedia.org/wiki/Bertrand's_postulate

	/* Where memory permits, track the primes using the

	 * Sieve of Eratosthenes. The sieve is to remove all multiples of known

	 * primes from the set, what remains in the set is therefore prime.

/**

 * next_prime_number - return the next prime number

 * @x: the starting point for searching to test

 *

 * A prime number is an integer greater than 1 that is only divisible by

 * itself and 1.  The set of prime numbers is computed using the Sieve of

 * Eratoshenes (on finding a prime, all multiples of that prime are removed

 * from the set) enabling a fast lookup of the next prime number larger than

 * @x. If the sieve fails (memory limitation), the search falls back to using

 * slow trial-divison, up to the value of ULONG_MAX (which is reported as the

 * final prime as a sentinel).

 *

 * Returns: the next prime number larger than @x

/**

 * is_prime_number - test whether the given number is prime

 * @x: the number to test

 *

 * A prime number is an integer greater than 1 that is only divisible by

 * itself and 1. Internally a cache of prime numbers is kept (to speed up

 * searching for sequential primes, see next_prime_number()), but if the number

 * falls outside of that cache, its primality is tested using trial-divison.

 *

 * Returns: true if @x is prime, false for composite numbers.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2013 Davidlohr Bueso <davidlohr.bueso@hp.com>

 *

 *  Based on the shift-and-subtract algorithm for computing integer

 *  square root from Guy L. Steele.

/**

 * int_sqrt - computes the integer square root

 * @x: integer of which to calculate the sqrt

 *

 * Computes: floor(sqrt(x))

/**

 * int_sqrt64 - strongly typed int_sqrt function when minimum 64 bit input

 * is expected.

 * @x: 64bit integer of which to calculate the sqrt

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This implements the binary GCD algorithm. (Often attributed to Stein,

 * but as Knuth has noted, appears in a first-century Chinese math text.)

 *

 * This is faster than the division-based algorithm even on x86, which

 * has decent hardware division.

 If __ffs is available, the even/odd algorithm benchmarks slower. */

/**

 * gcd - calculate and return the greatest common divisor of 2 unsigned longs

 * @a: first value

 * @b: second value

 If normalization is done by loops, the even/odd algorithm is a win. */

 Isolate lsbit of r */

 SPDX-License-Identifier: GPL-2.0

/*

 * An integer based power function

 *

 * Derived from drivers/video/backlight/pwm_bl.c

/**

 * int_pow - computes the exponentiation of the given base and exponent

 * @base: base which will be raised to the given power

 * @exp: power to be raised to

 *

 * Computes: pow(base, exp), i.e. @base raised to the @exp power

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2021  Maciej W. Rozycki

/*

 * This needs to be a macro, because we don't want to rely on the compiler

 * to do constant propagation, and `do_div' may take a different path for

 * constants, so we do want to verify that as well.

/*

 * Run calculation for the same divisor value expressed as a constant

 * and as a variable, so as to verify the implementation for both cases

 * should they be handled by different code execution paths.

 SPDX-License-Identifier: GPL-2.0

 Creates function rational_gen_params */

/*

 * Copyright (c) 2011 Broadcom Corporation

 *

 * Permission to use, copy, modify, and/or distribute this software for any

 * purpose with or without fee is hereby granted, provided that the above

 * copyright notice and this permission notice appear in all copies.

 *

 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES

 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF

 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY

 * SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES

 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION

 * OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN

 * CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

/*

 * cordic_calc_iq() - calculates the i/q coordinate for given angle

 *

 * theta: angle in degrees for which i/q coordinate is to be calculated

 * coord: function output parameter holding the i/q coordinate

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0

 0 */

 1 */

 2 */

 3 */

 4 */

 5 */

 6 */

 7 */

 8 */

 9 */

 10 */

 11 */

 12 */

 13 */

 14 */

 15 */

 16 */

 17 */

 18 */

 19 */

 20 */

 21 */

 22 */

 23 */

 24 */

 25 */

 26 */

 27 */

 28 */

 29 */

 30 */

 31 */

 32 */

 33 */

 34 */

 35 */

 36 */

 37 */

 38 */

 39 */

 40 */

 41 */

 42 */

 43 */

 44 */

 45 */

 46 */

 47 */

 48 */

 49 */

 50 */

 51 */

 52 */

 53 */

 54 */

 55 */

 56 */

 57 */

 58 */

 59 */

 60 */

 61 */

 62 */

 63 */

 64 */

 65 */

 66 */

 67 */

 68 */

 69 */

 70 */

 71 */

 72 */

 73 */

 74 */

 75 */

 76 */

 77 */

 78 */

 79 */

 80 */

 81 */

 82 */

 83 */

 84 */

 85 */

 86 */

 87 */

 88 */

 89 */

 90 */

 91 */

 92 */

 93 */

 94 */

 95 */

 96 */

 97 */

 98 */

 99 */

 100 */

 101 */

 102 */

 103 */

 104 */

 105 */

 106 */

 107 */

 108 */

 109 */

 110 */

 111 */

 112 */

 113 */

 114 */

 115 */

 116 */

 117 */

 118 */

 119 */

 120 */

 121 */

 122 */

 123 */

 124 */

 125 */

 126 */

 127 */

 128 */

 129 */

 130 */

 131 */

 132 */

 133 */

 134 */

 135 */

 136 */

 137 */

 138 */

 139 */

 140 */

 141 */

 142 */

 143 */

 144 */

 145 */

 146 */

 147 */

 148 */

 149 */

 150 */

 151 */

 152 */

 153 */

 154 */

 155 */

 156 */

 157 */

 158 */

 159 */

 160 */

 161 */

 162 */

 163 */

 164 */

 165 */

 166 */

 167 */

 168 */

 169 */

 170 */

 171 */

 172 */

 173 */

 174 */

 175 */

 176 */

 177 */

 178 */

 179 */

 180 */

 181 */

 182 */

 183 */

 184 */

 185 */

 186 */

 187 */

 188 */

 189 */

 190 */

 191 */

 192 */

 193 */

 194 */

 195 */

 196 */

 197 */

 198 */

 199 */

 200 */

 201 */

 202 */

 203 */

 204 */

 205 */

 206 */

 207 */

 208 */

 209 */

 210 */

 211 */

 212 */

 213 */

 214 */

 215 */

 216 */

 217 */

 218 */

 219 */

 220 */

 221 */

 222 */

 223 */

 224 */

 225 */

 226 */

 227 */

 228 */

 229 */

 230 */

 231 */

 232 */

 233 */

 234 */

 235 */

 236 */

 237 */

 238 */

 239 */

 240 */

 241 */

 242 */

 243 */

 244 */

 245 */

 246 */

 247 */

 248 */

 249 */

 250 */

 251 */

 252 */

 253 */

 254 */

 255 */

/* Hand composed "Minuscule" 4x6 font, with binary data generated using

 * Perl stub.

 *

 * Use 'perl -x mini_4x6.c < mini_4x6.c > new_version.c' to regenerate

 * binary data.

 *

 * Created by Kenneth Albanowski.

 * No rights reserved, released to the public domain.

 *

 * Version 1.0

/*



#!/usr/bin/perl -pn



s{((0x)?[0-9a-fA-F]+)(.*\[([\*\ ]{4})\])}{



	($num,$pat,$bits) = ($1,$3,$4);

	

	$bits =~ s/([^\s0])|(.)/ defined($1) + 0 /ge;

	

	$num = ord(pack("B8", $bits));

	$num |= $num >> 4;

	$num = sprintf("0x%.2x", $num);

	

	#print "$num,$pat,$bits\n";

	

	$num . $pat;

}ge;



__END__;

/* Note: binary data consists of one byte for each row of each character top

   to bottom, character 0 to character 255, six bytes per character. Each

   byte contains the same four character bits in both nybbles.

   MSBit to LSBit = left to right.

{*/

   Char 0: ' '  */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 1: ' '  */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 2: ' '  */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 3: ' '  */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 4: ' '  */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 5: ' '  */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 6: ' '  */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 7: ' '  */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 8: ' '  */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 9: ' '  */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 10: '' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 11: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 12: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 13: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 14: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 15: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 16: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 17: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 18: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 19: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 20: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 21: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 22: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 23: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 24: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 25: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 26: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 27: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 28: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 29: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 30: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 31: ' ' */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

}*/

{*/

   Char 32: ' ' */

=  [    ]       */

=  [    ]       */

=  [    ]       */

=  [    ]       */

=  [    ]       */

=  [    ]       */

}*/

{*/

   Char 33: '!' */

=  [ *  ]       */

=  [ *  ]       */

=  [ *  ]       */

=  [    ]       */

=  [ *  ]       */

=  [    ]       */

}*/

{*/

   Char 34: '"' */

=  [* * ]       */

=  [* * ]       */

=  [    ]       */

=  [    ]       */

=  [    ]       */

=  [    ]       */

}*/

{*/

   Char 35: '#' */

=  [* * ]       */

=  [****]       */

=  [****]       */

=  [* * ]       */

=  [    ]       */

=  [    ]       */

}*/

{*/

   Char 36: '$' */

=  [ *  ]       */

=  [ ** ]       */

=  [*** ]       */

=  [**  ]       */

=  [ *  ]       */

=  [    ]       */

}*/

{*/

   Char 37: '%' */

=  [* * ]       */

=  [  * ]       */

=  [ *  ]       */

=  [*   ]       */

=  [* * ]       */

=  [    ]       */

}*/

{*/

   Char 38: '&' */

=  [ ** ]       */

=  [*  *]       */

=  [ ** ]       */

=  [* * ]       */

=  [** *]       */

=  [    ]       */

}*/

{*/

   Char 39: ''' */

=  [  * ]       */

=  [ *  ]       */

=  [    ]       */

=  [    ]       */

=  [    ]       */

=  [    ]       */

}*/

{*/

   Char 40: '(' */

=  [  * ]       */

=  [ *  ]       */

=  [ *  ]       */

=  [ *  ]       */

=  [  * ]       */

=  [    ]       */

}*/

{*/

   Char 41: ')' */

=  [ *  ]       */

=  [  * ]       */

=  [  * ]       */

=  [  * ]       */

=  [ *  ]       */

=  [    ]       */

}*/

{*/

   Char 42: '*' */

=  [    ]       */

=  [*** ]       */

=  [*** ]       */

=  [*** ]       */

=  [    ]       */

=  [    ]       */

}*/

{*/

   Char 43: '+' */

=  [    ]       */

=  [ *  ]       */

=  [*** ]       */

=  [ *  ]       */

=  [    ]       */

=  [    ]       */

}*/

{*/

   Char 44: ',' */

=  [    ]       */

=  [    ]       */

=  [    ]       */

=  [ *  ]       */

=  [*   ]       */

=  [    ]       */

}*/

{*/

   Char 45: '-' */

=  [    ]       */

=  [    ]       */

=  [*** ]       */

=  [    ]       */

=  [    ]       */

=  [    ]       */

}*/

{*/

   Char 46: '.' */

=  [    ]       */

=  [    ]       */

=  [    ]       */

=  [    ]       */

=  [ *  ]       */

=  [    ]       */

}*/

{*/

   Char 47: '/' */

=  [    ]       */

=  [  * ]       */

=  [ *  ]       */

=  [*   ]       */

=  [    ]       */

=  [    ]       */

}*/

{*/

   Char 48: '0'   */

=   [ *  ]        */

=   [* * ]        */

=   [* * ]        */

=   [* * ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/

   Char 49: '1'   */

=   [ *  ]        */

=   [**  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/

   Char 50: '2'   */

=   [**  ]        */

=   [  * ]        */

=   [ *  ]        */

=   [*   ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/

   Char 51: '3'   */

=   [*** ]        */

=   [  * ]        */

=   [ ** ]        */

=   [  * ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [* * ]        */

=   [* * ]        */

=   [*** ]        */

=   [  * ]        */

=   [  * ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*   ]        */

=   [*** ]        */

=   [  * ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*   ]        */

=   [*** ]        */

=   [* * ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [  * ]        */

=   [  * ]        */

=   [  * ]        */

=   [  * ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [* * ]        */

=   [*** ]        */

=   [* * ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [* * ]        */

=   [*** ]        */

=   [  * ]        */

=   [  * ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [ *  ]        */

=   [    ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [ *  ]        */

=   [    ]        */

=   [ *  ]        */

=   [*   ]        */

}*/

{*/ 	
=   [  * ]        */

=   [ *  ]        */

=   [*   ]        */

=   [ *  ]        */

=   [  * ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [*** ]        */

=   [    ]        */

=   [*** ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [*   ]        */

=   [ *  ]        */

=   [  * ]        */

=   [ *  ]        */

=   [*   ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [  * ]        */

=   [ ** ]        */

=   [    ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [*** ]        */

=   [*** ]        */

=   [*   ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [* * ]        */

=   [*** ]        */

=   [* * ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [**  ]        */

=   [* * ]        */

=   [**  ]        */

=   [* * ]        */

=   [**  ]        */

=   [    ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [*   ]        */

=   [*   ]        */

=   [*   ]        */

=   [ ** ]        */

=   [    ]        */

}*/

{*/ 	
=   [**  ]        */

=   [* * ]        */

=   [* * ]        */

=   [* * ]        */

=   [**  ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*   ]        */

=   [*** ]        */

=   [*   ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*   ]        */

=   [*** ]        */

=   [*   ]        */

=   [*   ]        */

=   [    ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [*   ]        */

=   [*** ]        */

=   [* * ]        */

=   [ ** ]        */

=   [    ]        */

}*/

{*/ 	
=   [* * ]        */

=   [* * ]        */

=   [*** ]        */

=   [* * ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [  * ]        */

=   [  * ]        */

=   [  * ]        */

=   [* * ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [* * ]        */

=   [* * ]        */

=   [**  ]        */

=   [* * ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [*   ]        */

=   [*   ]        */

=   [*   ]        */

=   [*   ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [* * ]        */

=   [*** ]        */

=   [*** ]        */

=   [* * ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [* * ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [* * ]        */

=   [* * ]        */

=   [* * ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [**  ]        */

=   [* * ]        */

=   [**  ]        */

=   [*   ]        */

=   [*   ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [* * ]        */

=   [* * ]        */

=   [*** ]        */

=   [ ** ]        */

=   [    ]        */

}*/

{*/ 	
=   [**  ]        */

=   [* * ]        */

=   [*** ]        */

=   [**  ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [*   ]        */

=   [ *  ]        */

=   [  * ]        */

=   [**  ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [* * ]        */

=   [* * ]        */

=   [* * ]        */

=   [* * ]        */

=   [ ** ]        */

=   [    ]        */

}*/

{*/ 	
=   [* * ]        */

=   [* * ]        */

=   [* * ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [* * ]        */

=   [* * ]        */

=   [*** ]        */

=   [*** ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [* * ]        */

=   [* * ]        */

=   [ *  ]        */

=   [* * ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [* * ]        */

=   [* * ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [  * ]        */

=   [ *  ]        */

=   [*   ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ ** ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [*   ]        */

=   [ *  ]        */

=   [  * ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [  * ]        */

=   [  * ]        */

=   [  * ]        */

=   [ ** ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [* * ]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

=   [****]        */

}*/

{*/ 	
=   [*   ]        */

=   [ *  ]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [ ** ]        */

=   [* * ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*   ]        */

=   [*   ]        */

=   [**  ]        */

=   [* * ]        */

=   [**  ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [ ** ]        */

=   [*   ]        */

=   [ ** ]        */

=   [    ]        */

}*/

{*/ 	
=   [  * ]        */

=   [  * ]        */

=   [ ** ]        */

=   [* * ]        */

=   [ ** ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [*** ]        */

=   [*** ]        */

=   [*   ]        */

=   [ ** ]        */

=   [    ]        */

}*/

{*/ 	
=   [  * ]        */

=   [ *  ]        */

=   [*** ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [ ** ]        */

=   [* * ]        */

=   [ ** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*   ]        */

=   [*   ]        */

=   [**  ]        */

=   [* * ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [    ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [    ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [*   ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [*   ]        */

=   [* * ]        */

=   [**  ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [**  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [*** ]        */

=   [*** ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [**  ]        */

=   [* * ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [ *  ]        */

=   [* * ]        */

=   [* * ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [**  ]        */

=   [* * ]        */

=   [**  ]        */

=   [*   ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [ ** ]        */

=   [* * ]        */

=   [ ** ]        */

=   [  * ]        */

}*/

{*/ 	
=   [    ]        */

=   [**  ]        */

=   [* * ]        */

=   [*   ]        */

=   [*   ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [ ** ]        */

=   [**  ]        */

=   [  * ]        */

=   [**  ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [ *  ]        */

=   [*** ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [* * ]        */

=   [* * ]        */

=   [ ** ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [* * ]        */

=   [*** ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [* * ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [* * ]        */

=   [ *  ]        */

=   [* * ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [* * ]        */

=   [*** ]        */

=   [  * ]        */

=   [**  ]        */

}*/

{*/ 	
=   [    ]        */

=   [*** ]        */

=   [ ** ]        */

=   [**  ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [  * ]        */

=   [ *  ]        */

=   [**  ]        */

=   [ *  ]        */

=   [  * ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [    ]        */

}*/

{*/ 	
=   [*   ]        */

=   [ *  ]        */

=   [ ** ]        */

=   [ *  ]        */

=   [*   ]        */

=   [    ]        */

}*/

{*/ 	
=   [ * *]        */

=   [* * ]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [* * ]        */

=   [* * ]        */

=   [*** ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [ ** ]        */

=   [**  ]        */

=   [ ** ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [**  ]        */

=   [ ** ]        */

=   [**  ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [*   ]        */

=   [  * ]        */

=   [*   ]        */

=   [  * ]        */

=   [*   ]        */

=   [  * ]        */

}*/

{*/ 	
=   [* * ]        */

=   [ * *]        */

=   [* * ]        */

=   [ * *]        */

=   [* * ]        */

=   [ * *]        */

}*/

{*/ 	
=   [** *]        */

=   [* **]        */

=   [** *]        */

=   [* **]        */

=   [** *]        */

=   [* **]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [**  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [**  ]        */

=   [**  ]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [*** ]        */

=   [ ** ]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [*** ]        */

=   [ ** ]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [**  ]        */

=   [**  ]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [*** ]        */

=   [*** ]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [ ** ]        */

=   [ ** ]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [*** ]        */

=   [*** ]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [*** ]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [**  ]        */

=   [**  ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [**  ]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [ ***]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [****]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [****]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [ ***]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [****]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [****]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [ ***]        */

=   [ ***]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [ ***]        */

=   [ ** ]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [ ***]        */

=   [ ***]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [ ***]        */

=   [ ***]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [****]        */

=   [****]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [****]        */

=   [****]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [ ***]        */

=   [ ***]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [****]        */

=   [****]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [****]        */

=   [****]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [****]        */

=   [****]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [****]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [****]        */

=   [****]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [****]        */

=   [ ** ]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [ ***]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [ ***]        */

=   [ ***]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [ ***]        */

=   [ ***]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [ ***]        */

=   [ ** ]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [ ** ]        */

=   [ ** ]        */

=   [****]        */

=   [ ** ]        */

=   [ ** ]        */

=   [ ** ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [****]        */

=   [****]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [ *  ]        */

=   [ *  ]        */

=   [**  ]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [ ***]        */

=   [ *  ]        */

=   [ *  ]        */

=   [ *  ]        */

}*/

{*/ 	
=   [****]        */

=   [****]        */

=   [****]        */

=   [****]        */

=   [****]        */

=   [****]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [    ]        */

=   [****]        */

=   [****]        */

=   [****]        */

}*/

{*/ 	
=   [**  ]        */

=   [**  ]        */

=   [**  ]        */

=   [**  ]        */

=   [**  ]        */

=   [**  ]        */

}*/

{*/ 	
=   [  **]        */

=   [  **]        */

=   [  **]        */

=   [  **]        */

=   [  **]        */

=   [  **]        */

}*/

{*/ 	
=   [****]        */

=   [****]        */

=   [****]        */

=   [    ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

{*/ 	
=   [    ]        */

=   [    ]        */

=   [ ** ]        */

=   [ ** ]        */

=   [    ]        */

=   [    ]        */

}*/

{*/ 	
=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [*** ]        */

=   [    ]        */

}*/

