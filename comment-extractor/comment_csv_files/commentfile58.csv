 SPDX-License-Identifier: GPL-2.0

 64 bit rw */

 64 bit read only */

	/*

	 * Not defined in KVM:

	 *

	 * EVMCS1_FIELD(0x00006402, exit_io_instruction_ecx,

	 *		HV_VMX_ENLIGHTENED_CLEAN_FIELD_NONE);

	 * EVMCS1_FIELD(0x00006404, exit_io_instruction_esi,

	 *		HV_VMX_ENLIGHTENED_CLEAN_FIELD_NONE);

	 * EVMCS1_FIELD(0x00006406, exit_io_instruction_esi,

	 *		HV_VMX_ENLIGHTENED_CLEAN_FIELD_NONE);

	 * EVMCS1_FIELD(0x00006408, exit_io_instruction_eip,

	 *		HV_VMX_ENLIGHTENED_CLEAN_FIELD_NONE);

	/*

	 * No mask defined in the spec as Hyper-V doesn't currently support

	 * these. Future proof by resetting the whole clean field mask on

	 * access.

 32 bit rw */

 32 bit read only */

 No mask defined in the spec (not used) */

 16 bit rw */

	/*

	 * vmcs_version represents the range of supported Enlightened VMCS

	 * versions: lower 8 bits is the minimal version, higher 8 bits is the

	 * maximum supported version. KVM supports versions from 1 to

	 * KVM_EVMCS_VERSION.

	/*

	 * Hyper-V 2016 and 2019 try using these features even when eVMCS

	 * is enabled but there are no corresponding fields.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * We maintain a per-CPU linked-list of vCPU, so in wakeup_handler() we

 * can find which vCPU should be waken up.

	/*

	 * In case of hot-plug or hot-unplug, we may have to undo

	 * vmx_vcpu_pi_put even if there is no assigned device.  And we

	 * always keep PI.NDST up to date for simplicity: it makes the

	 * code easier, and CPU migration is not a fast path.

	/*

	 * If the 'nv' field is POSTED_INTR_WAKEUP_VECTOR, do not change

	 * PI.NDST: pi_post_block is the one expected to change PID.NDST and the

	 * wakeup handler expects the vCPU to be on the blocked_vcpu_list that

	 * matches PI.NDST. Otherwise, a vcpu may not be able to be woken up

	 * correctly.

 The full case.  */

	/*

	 * Clear SN before reading the bitmap.  The VT-d firmware

	 * writes the bitmap and reads SN atomically (5.2.3 in the

	 * spec), so it doesn't really have a memory barrier that

	 * pairs with this, but we cannot do that and we need one.

 Set SN when the vCPU is preempted */

 set 'NV' to 'notification vector' */

/*

 * This routine does the following things for vCPU which is going

 * to be blocked if VT-d PI is enabled.

 * - Store the vCPU to the wakeup list, so when interrupts happen

 *   we can find the right vCPU to wake up.

 * - Change the Posted-interrupt descriptor as below:

 *      'NDST' <-- vcpu->pre_pcpu

 *      'NV' <-- POSTED_INTR_WAKEUP_VECTOR

 * - If 'ON' is set during this process, which means at least one

 *   interrupt is posted for this vCPU, we cannot block it, in

 *   this case, return 1, otherwise, return 0.

 *

		/*

		 * Since vCPU can be preempted during this process,

		 * vcpu->cpu could be different with pre_pcpu, we

		 * need to set pre_pcpu as the destination of wakeup

		 * notification event, then we can find the right vCPU

		 * to wakeup in wakeup handler if interrupts happen

		 * when the vCPU is in blocked state.

 set 'NV' to 'wakeup vector' */

 We should not block the vCPU if an interrupt is posted for it.  */

/*

 * Handler for POSTED_INTERRUPT_WAKEUP_VECTOR.

/*

 * Bail out of the block loop if the VM has an assigned

 * device, but the blocking vCPU didn't reconfigure the

 * PI.NV to the wakeup vector, i.e. the assigned device

 * came along after the initial check in pi_pre_block().

/*

 * pi_update_irte - set IRTE for Posted-Interrupts

 *

 * @kvm: kvm

 * @host_irq: host irq of the interrupt

 * @guest_irq: gsi of the interrupt

 * @set: set or unset PI

 * returns 0 on success, < 0 on failure

		/*

		 * VT-d PI cannot support posting multicast/broadcast

		 * interrupts to a vCPU, we still use interrupt remapping

		 * for these kind of interrupts.

		 *

		 * For lowest-priority interrupts, we only support

		 * those with single CPU as the destination, e.g. user

		 * configures the interrupts via /proc/irq or uses

		 * irqbalance to make the interrupts single-CPU.

		 *

		 * We will support full lowest-priority interrupt later.

		 *

		 * In addition, we can only inject generic interrupts using

		 * the PI mechanism, refuse to route others through it.

			/*

			 * Make sure the IRTE is in remapped mode if

			 * we don't handle it in posted mode.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel-based Virtual Machine driver for Linux

 *

 * This module enables machines with Intel VT-x extensions to run virtual

 * machines without emulation or binary translation.

 *

 * Copyright (C) 2006 Qumranet, Inc.

 * Copyright 2010 Red Hat, Inc. and/or its affiliates.

 *

 * Authors:

 *   Avi Kivity   <avi@qumranet.com>

 *   Yaniv Kamay  <yaniv@qumranet.com>

/*

 * If nested=1, nested virtualization is supported, i.e., guests may use

 * VMX and be a hypervisor for its own guests. If nested=0, guests may not

 * use VMX instructions.

 Guest_tsc -> host_tsc conversion requires 64-bit division.  */

/*

 * List of MSRs that can be directly passed to the guest.

 * In addition to these x2apic and PT MSRs are handled specially.

/*

 * These 2 parameters are used to config the controls for Pause-Loop Exiting:

 * ple_gap:    upper bound on the amount of time between two successive

 *             executions of PAUSE in a loop. Also indicate if ple enabled.

 *             According to test, this time is usually smaller than 128 cycles.

 * ple_window: upper bound on the amount of time a guest is allowed to execute

 *             in a PAUSE loop. Tests indicate that most spinlocks are held for

 *             less than 2^12 cycles

 * Time is measured based on a counter that runs at the same rate as the TSC,

 * refer SDM volume 3b section 21.6.13 & 22.1.3.

 Default doubles per-vcpu window every exit. */

 Default resets per-vcpu window every exit to ple_window. */

 Default is to compute the maximum so we can never overflow. */

 Default is SYSTEM mode, 1 for host-guest mode */

 Storage for pre module init parameter parsing */

 If set to auto use the default l1tf mitigation method */

		/*

		 * This allocation for vmx_l1d_flush_pages is not tied to a VM

		 * lifetime and so should not be charged to a memcg.

		/*

		 * Initialize each page with a different pattern in

		 * order to protect against KSM in the nested

		 * virtualization case.

	/*

	 * Has vmx_init() run already? If not then this is the pre init

	 * parameter parsing. In that case just store the value and let

	 * vmx_init() do the proper setup after enable_ept has been

	 * established.

/*

 * We maintain a per-CPU linked-list of VMCS loaded on that CPU. This is needed

 * when a CPU is brought down, and we need to VMCLEAR all VMCSs loaded on it.

	/*

	 * Synthetic VM-Exit is not enabled in current code and so All

	 * evmcs in singe VM shares same assist page.

 IS_ENABLED(CONFIG_HYPERV) */

/*

 * Comment's format: document - errata name - stepping - processor name.

 * Refer from

 * https://www.virtualbox.org/svn/vbox/trunk/src/VBox/VMM/VMMR0/HMR0.cpp

 323344.pdf - BA86   - D0 - Xeon 7500 Series */

 323056.pdf - AAX65  - C2 - Xeon L3406 */

 322814.pdf - AAT59  - C2 - i7-600, i5-500, i5-400 and i3-300 Mobile */

 322911.pdf - AAU65  - C2 - i5-600, i3-500 Desktop and Pentium G6950 */

 322911.pdf - AAU65  - K0 - i5-600, i3-500 Desktop and Pentium G6950 */

 322373.pdf - AAO95  - B1 - Xeon 3400 Series */

 322166.pdf - AAN92  - B1 - i7-800 and i5-700 Desktop */

/*

 * 320767.pdf - AAP86  - B1 -

 * i7-900 Mobile Extreme, i7-800 and i7-700 Mobile

 321333.pdf - AAM126 - C0 - Xeon 3500 */

 321333.pdf - AAM126 - C1 - Xeon 3500 */

 320836.pdf - AAJ124 - C0 - i7-900 Desktop Extreme and i7-900 Desktop */

 321333.pdf - AAM126 - D0 - Xeon 3500 */

 321324.pdf - AAK139 - D0 - Xeon 5500 */

 320836.pdf - AAJ124 - D0 - i7-900 Extreme and i7-900 Desktop */

 Xeon E3-1220 V2 */

 Clear the reserved bits */

 x2APIC MSRs. These are handled in vmx_update_msr_bitmap_x2apic() */

 PT MSRs. These are handled in pt_update_intercept_for_msr() */

 LBR MSRs. These are handled in vmx_update_intercept_for_lbr_msrs() */

 CONFIG_KEXEC_CORE */

 vcpu migration can race with cpu offline */

	/*

	 * Ensure all writes to loaded_vmcs, including deleting it from its

	 * current percpu list, complete before setting loaded_vmcs->vcpu to

	 * -1, otherwise a different cpu can see vcpu == -1 first and add

	 * loaded_vmcs to its percpu list before it's deleted from this cpu's

	 * list. Pairs with the smp_rmb() in vmx_vcpu_load_vmcs().

	/*

	 * Guest access to VMware backdoor ports could legitimately

	 * trigger #GP because of TSS I/O permission bitmap.

	 * We intercept those #GP and allow access to them anyway

	 * as VMware does.

	/* When we are running a nested L2 guest and L1 specified for it a

	 * certain exception bitmap, we must trap the same exceptions and pass

	 * them to L1. When running L2, we will only handle the exceptions

	 * specified above if L1 did not want them.

			/*

			 * If EPT is enabled, #PF is currently only intercepted

			 * if MAXPHYADDR is smaller on the guest than on the

			 * host.  In that case we only care about present,

			 * non-reserved faults.  For vmcs02, however, PFEC_MASK

			 * and PFEC_MATCH are set in prepare_vmcs02_rare.

/*

 * Check if MSR is intercepted for currently loaded MSR bitmap.

		/* PEBS needs a quiescent period after being disabled (to write

		 * a record).  Disabling PEBS through VMX MSR swapping doesn't

		 * provide that period, so a CPU could write host's record into

		 * guest's memory.

 Shadow paging assumes NX to be available.  */

	/*

	 * LMA and LME handled by hardware; SCE meaningless outside long mode.

 SCE is meaningful only in long mode on Intel */

	/*

	 * On EPT, we can't emulate NX, so we must switch EFER atomically.

	 * On CPUs that support "load IA32_EFER", always switch EFER

	 * atomically, since it's faster than switching it manually.

/*

 * On 32-bit kernels, VM exits still load the FS and GS bases from the

 * VMCS rather than the segment table.  KVM uses this helper to figure

 * out the current bases to poke them into the VMCS before entry.

 The base must be 128-byte aligned and a legal physical address. */

	/*

	 * GUEST_IA32_RTIT_CTL is already set in the VMCS.

	 * Save host state before VM entry.

	/*

	 * KVM requires VM_EXIT_CLEAR_IA32_RTIT_CTL to expose PT to the guest,

	 * i.e. RTIT_CTL is always cleared on VM-Exit.  Restore it if necessary.

	/*

	 * Note that guest MSRs to be saved/restored can also be changed

	 * when guest state is loaded. This happens when guest transitions

	 * to/from long-mode by setting MSR_EFER.LMA.

	/*

	 * Set host fs and gs selectors.  Unfortunately, 22.2.3 does not

	 * allow segment selectors with cpl > 0 or ti == 1.

		/*

		 * Ensure loaded_vmcs->cpu is read before adding loaded_vmcs to

		 * this cpu's percpu list, otherwise it may not yet be deleted

		 * from its previous cpu's percpu list.  Pairs with the

		 * smb_wmb() in __loaded_vmcs_clear().

		/*

		 * No indirect branch prediction barrier needed when switching

		 * the active VMCS within a guest, e.g. on nested VM-Enter.

		 * The L1 VMM can protect itself with retpolines, IBPB or IBRS.

		/*

		 * Flush all EPTP/VPID contexts, the new pCPU may have stale

		 * TLB entries from its previous association with the vCPU.

		/*

		 * Linux uses per-cpu TSS and GDT, so set these when switching

		 * processors.  See 22.2.4.

 22.2.4 */

 22.2.3 */

/*

 * Switches to specified vcpu, until a matching vcpu_put(), but assumes

 * vcpu mutex is already taken.

	/*

	 * Any MSR write that attempts to change bits marked reserved will

	 * case a #GP fault.

	/*

	 * Any attempt to modify IA32_RTIT_CTL while TraceEn is set will

	 * result in a #GP unless the same write also clears TraceEn.

	/*

	 * WRMSR to IA32_RTIT_CTL that sets TraceEn but clears this bit

	 * and FabricEn would cause #GP, if

	 * CPUID.(EAX=14H, ECX=0):ECX.SNGLRGNOUT[bit 2] = 0

	/*

	 * MTCFreq, CycThresh and PSBFreq encodings check, any MSR write that

	 * utilize encodings marked reserved will cause a #GP fault.

	/*

	 * If ADDRx_CFG is reserved or the encodings is >2 will

	 * cause a #GP fault.

	/*

	 * Emulation of instructions in SGX enclaves is impossible as RIP does

	 * not point  tthe failing instruction, and even if it did, the code

	 * stream is inaccessible.  Inject #UD instead of exiting to userspace

	 * so that guest userspace can't DoS the guest simply by triggering

	 * emulation (enclaves are CPL3 only).

	/*

	 * Using VMCS.VM_EXIT_INSTRUCTION_LEN on EPT misconfig depends on

	 * undefined behavior: Intel's SDM doesn't mandate the VMCS field be

	 * set when EPT misconfig occurs.  In practice, real hardware updates

	 * VM_EXIT_INSTRUCTION_LEN on EPT misconfig, but other hypervisors

	 * (namely Hyper-V) don't set it due to it being undefined behavior,

	 * i.e. we end up advancing IP with some random value.

		/*

		 * Emulating an enclave's instructions isn't supported as KVM

		 * cannot access the enclave's memory or its true RIP, e.g. the

		 * vmcs.GUEST_RIP points at the exit point of the enclave, not

		 * the RIP that actually triggered the VM-Exit.  But, because

		 * most instructions that cause VM-Exit will #UD in an enclave,

		 * most instruction-based VM-Exits simply do not occur.

		 *

		 * There are a few exceptions, notably the debug instructions

		 * INT1ICEBRK and INT3, as they are allowed in debug enclaves

		 * and generate #DB/#BP as expected, which KVM might intercept.

		 * But again, the CPU does the dirty work and saves an instr

		 * length of zero so VMMs don't shoot themselves in the foot.

		 * WARN if KVM tries to skip a non-zero length instruction on

		 * a VM-Exit from an enclave.

		/*

		 * We need to mask out the high 32 bits of RIP if not in 64-bit

		 * mode, but just finding out that we are in 64-bit mode is

		 * quite expensive.  Only do it if there was a carry.

 skipping an emulated instruction also counts */

/*

 * Recognizes a pending MTF VM-exit and records the nested state for later

 * delivery.

	/*

	 * Per the SDM, MTF takes priority over debug-trap exceptions besides

	 * T-bit traps. As instruction emulation is completed (i.e. at the

	 * instruction boundary), any #DB exception pending delivery must be a

	 * debug-trap. Record the pending MTF state to be delivered in

	 * vmx_check_nested_events().

	/*

	 * Ensure that we clear the HLT state in the VMCS.  We don't need to

	 * explicitly skip the instruction because if the HLT state is set,

	 * then the instruction is already executing and RIP has already been

	 * advanced.

/*

 * Configuring user return MSRs to automatically save, load, and restore MSRs

 * that need to be shoved into hardware when running the guest.  Note, omitting

 * an MSR here does _NOT_ mean it's not emulated, only that it will not be

 * loaded into hardware when running the guest.

	/*

	 * The SYSCALL MSRs are only needed on long mode guests, and only

	 * when EFER.SCE is set.

	/*

	 * hle=0, rtm=0, tsx_ctrl=1 can be found with some combinations of new

	 * kernel and old userspace.  If those guests run on a tsx=off host, do

	 * allow guests to use TSX_CTRL, but don't change the value in hardware

	 * so that TSX remains always disabled.

	/*

	 * The set of MSRs to load may have changed, reload MSRs before the

	 * next VM-Enter.

/*

 * nested_vmx_allowed() checks whether a guest should be allowed to use VMX

 * instructions and MSRs (i.e., nested VMX). Nested VMX is disabled for

 * all guests if the "nested" module option is off, and can also be disabled

 * for a single guest by disabling its VMX cpuid bit.

/*

 * Reads an msr value (of 'msr_index') into 'pdata'.

 * Returns 0 on success, non-0 otherwise.

 * Assumes vcpu_load() was already called.

		/*

		 * Enlightened VMCS v1 doesn't have certain VMCS fields but

		 * instead of just ignoring the features, different Hyper-V

		 * versions are either trying to use them and fail or do some

		 * sanity checking and refuse to boot. Filter all unsupported

		 * features out.

/*

 * Writes msr value into the appropriate "register".

 * Returns 0 on success, non-0 otherwise.

 * Assumes vcpu_load() was already called.

 The reserved bit 1 and non-32 bit [63:32] should be zero */

		/*

		 * For non-nested:

		 * When it's written (to non-zero) for the first time, pass

		 * it through.

		 *

		 * For nested:

		 * The handling of the MSR bitmap for L2 guests is done in

		 * nested_vmx_prepare_msr_bitmap. We should not touch the

		 * vmcs02.msr_bitmap here since it gets completely overwritten

		 * in the merging. We update the vmcs01 here for L1 as well

		 * since it will end up touching the MSR anyway now.

		/*

		 * For non-nested:

		 * When it's written (to non-zero) for the first time, pass

		 * it through.

		 *

		 * For nested:

		 * The handling of the MSR bitmap for L2 guests is done in

		 * nested_vmx_prepare_msr_bitmap. We should not touch the

		 * vmcs02.msr_bitmap here since it gets completely overwritten

		 * in the merging.

 SGX may be enabled/disabled by guest's firmware */

		/*

		 * On real hardware, the LE hash MSRs are writable before

		 * the firmware sets bit 0 in MSR 0x7a ("activating" SGX),

		 * at which point SGX related bits in IA32_FEATURE_CONTROL

		 * become writable.

		 *

		 * KVM does not emulate SGX activation for simplicity, so

		 * allow writes to the LE hash MSRs if IA32_FEATURE_CONTROL

		 * is unlocked.  This is technically not architectural

		 * behavior, but it's close enough.

 they are read-only */

		/*

		 * When intercepting CR3 loads, e.g. for shadowing paging, KVM's

		 * CR3 is loaded into hardware, not the guest's CR3.

	/*

	 * This can happen if we hot-added a CPU but failed to allocate

	 * VP assist page for it.

/*

 * There is no X86_FEATURE for SGX yet, but anyway we need to query CPUID

 * directly instead of going through cpu_has(), to ensure KVM is trapping

 * ENCLS whenever it's supported in hardware.  It does not matter whether

 * the host OS supports or has enabled SGX.

 bit == 0 in high word ==> must be zero */

 bit == 1 in low word  ==> must be one  */

 Ensure minimum (required) set of control bits are supported. */

		/* CR3 accesses and invlpg don't need to cause VM Exits when EPT

	/*

	 * Some cpus support VM_{ENTRY,EXIT}_IA32_PERF_GLOBAL_CTRL but they

	 * can't be used due to an errata where VM Exit may incorrectly clear

	 * IA32_PERF_GLOBAL_CTRL[34:32].  Workaround the errata by using the

	 * MSR load mechanism to switch IA32_PERF_GLOBAL_CTRL.

 AAK155 */

 AAP115 */

 AAT100 */

 BC86,AAY89,BD102 */

 BA97 */

 IA-32 SDM Vol 3B: VMCS size is never greater than 4kB. */

 IA-32 SDM Vol 3B: 64-bit CPUs always have VMX_BASIC_MSR[48]==0. */

 Require Write-Back (WB) memory type for VMCS accesses. */

 KVM supports Enlightened VMCS v1 only */

/*

 * Free a VMCS, but before that VMCLEAR it on the CPU where it was last loaded

		/*

		 * When eVMCS is enabled, alloc_vmcs_cpu() sets

		 * vmcs->revision_id to KVM_EVMCS_VERSION instead of

		 * revision_id reported by MSR_IA32_VMX_BASIC.

		 *

		 * However, even though not explicitly documented by

		 * TLFS, VMXArea passed as VMXON argument should

		 * still be marked with revision_id reported by

		 * physical CPU.

		/*

		 * CS and SS RPL should be equal during guest entry according

		 * to VMX spec, but in reality it is not always so. Since vcpu

		 * is in the middle of the transition from real mode to

		 * protected mode it is safe to assume that RPL 0 is a good

		 * default value.

	/*

	 * Update real mode segment cache. It may be not up-to-date if segment

	 * register was written while vcpu was in a guest mode.

	/*

	 * Very old userspace does not call KVM_SET_TSS_ADDR before entering

	 * vcpu. Warn the user that an update is overdue.

 Nothing to do if hardware doesn't support EFER. */

	/*

	 * INVEPT must be issued when EPT is enabled, irrespective of VPID, as

	 * the CPU is not required to invalidate guest-physical mappings on

	 * VM-Entry, even if VPID is disabled.  Guest-physical mappings are

	 * associated with the root EPT structure and not any particular VPID

	 * (INVVPID also isn't required to invalidate guest-physical mappings).

 No flush required if the current context is invalid. */

	/*

	 * vpid_sync_vcpu_addr() is a nop if vmx->vpid==0, see the comment in

	 * vmx_flush_tlb_guest() for an explanation of why this is ok.

	/*

	 * vpid_sync_context() is a nop if vmx->vpid==0, e.g. if enable_vpid==0

	 * or a vpid couldn't be allocated for this vCPU.  VM-Enter and VM-Exit

	 * are required to flush GVA->{G,H}PA mappings from the TLB if vpid is

	 * disabled (VM-Enter with vpid enabled and vpid==0 is disallowed),

	 * i.e. no explicit INVVPID is necessary.

		/*

		 * Ensure KVM has an up-to-date snapshot of the guest's CR3.  If

		 * the below code _enables_ CR3 exiting, vmx_cache_reg() will

		 * (correctly) stop reading vmcs.GUEST_CR3 because it thinks

		 * KVM's CR3 is installed.

		/*

		 * When running with EPT but not unrestricted guest, KVM must

		 * intercept CR3 accesses when paging is _disabled_.  This is

		 * necessary because restricted guests can't actually run with

		 * paging disabled, and so KVM stuffs its own CR3 in order to

		 * run the guest when identity mapped page tables.

		 *

		 * Do _NOT_ check the old CR0.PG, e.g. to optimize away the

		 * update, it may be stale with respect to CR3 interception,

		 * e.g. after nested VM-Enter.

		 *

		 * Lastly, honor L1's desires, i.e. intercept CR3 loads and/or

		 * stores to forward them to L1, even if KVM does not need to

		 * intercept them to preserve its identity mapped page tables.

 Note, vmx_set_cr4() consumes the new vcpu->arch.cr0. */

 depends on vcpu->arch.cr0 to be set to a new value */

 vmcs01.GUEST_CR3 is already up-to-date. */

	/*

	 * We operate under the default treatment of SMM, so VMX cannot be

	 * enabled under SMM.  Note, whether or not VMXE is allowed at all is

	 * handled by kvm_is_valid_cr4().

	/*

	 * Pass through host's Machine Check Enable value to hw_cr4, which

	 * is in force while we are in guest mode.  Do not let guests control

	 * this bit, even if host CR4.MCE == 0.

		/*

		 * SMEP/SMAP/PKU is disabled if CPU is in non-paging mode in

		 * hardware.  To emulate this behavior, SMEP/SMAP/PKU needs

		 * to be manually disabled when guest switches to non-paging

		 * mode.

		 *

		 * If !enable_unrestricted_guest, the CPU is always running

		 * with CR0.PG=1 and CR4 needs to be modified.

		 * If enable_unrestricted_guest, the CPU automatically

		 * disables SMEP/SMAP/PKU when the guest sets CR0.PG=0.

	/*

	 * Some userspaces do not preserve unusable property. Since usable

	 * segment has to be present according to VMX spec we can use present

	 * property to amend userspace bug by making unusable segment always

	 * nonpresent. vmx_segment_access_rights() already marks nonpresent

	 * segment as unusable.

	/*

	 *   Fix the "Accessed" bit in AR field of segment registers for older

	 * qemu binaries.

	 *   IA32 arch specifies that at the time of processor reset the

	 * "Accessed" bit in the AR field of segment registers is 1. And qemu

	 * is setting it to 0 in the userland code. This causes invalid guest

	 * state vmexit when "unrestricted guest" mode is turned on.

	 *    Fix for this setup issue in cpu_reset is being pushed in the qemu

	 * tree. Newer qemu binaries with that qemu fix would not need this

	 * kvm hack.

 Accessed */

 TODO: Add Reserved field check, this'll require a new member in the kvm_segment_field structure */

 DPL != RPL */

 DPL < RPL */

	/* TODO: Add other members to kvm_segment_field to allow checking for other access

	 * rights flags

 TI = 1 */

 TODO: Check if guest is in IA32e mode */

 TI = 1 */

/*

 * Check if guest state is valid. Returns true if valid, false if

 * not.

 * We assume that registers are always usable

 real mode guest state checks */

 protected mode guest state checks */

	/* TODO:

	 * - Add checks on RIP

	 * - Add checks on RFLAGS

 Protect kvm_vmx->ept_identity_pagetable_done. */

 Set up identity-mapping pagetable for EPT in real mode */

 code segment */

	/*

	 * Do not pin the page in memory, so that memory hot-unplug

	 * is able to migrate it.

	/*

	 * Mark the desired intercept state in shadow bitmap, this is needed

	 * for resync when the MSR filters change.

	/*

	 * Mark the desired intercept state in shadow bitmap, this is needed

	 * for resync when the MSR filter changes.

	/*

	 * TPR reads and writes can be virtualized even if virtual interrupt

	 * delivery is not in use.

	/*

	 * Set intercept permissions for all potentially passed through MSRs

	 * again. They will automatically get filtered through the MSR filter,

	 * so we are back in sync after this.

		/*

		 * The vector of interrupt to be delivered to vcpu had

		 * been set in PIR before this function.

		 *

		 * Following cases will be reached in this block, and

		 * we always send a notification event in all cases as

		 * explained below.

		 *

		 * Case 1: vcpu keeps in non-root mode. Sending a

		 * notification event posts the interrupt to vcpu.

		 *

		 * Case 2: vcpu exits to root mode and is still

		 * runnable. PIR will be synced to vIRR before the

		 * next vcpu entry. Sending a notification event in

		 * this case has no effect, as vcpu is not in root

		 * mode.

		 *

		 * Case 3: vcpu exits to root mode and is blocked.

		 * vcpu_block() has already synced PIR to vIRR and

		 * never blocks vcpu if vIRR is not cleared. Therefore,

		 * a blocked vcpu here does not wait for any requested

		 * interrupts in PIR, and sending a notification event

		 * which has no effect is safe here.

		/*

		 * If a posted intr is not recognized by hardware,

		 * we will accomplish it in the next vmentry.

 the PIR and ON have been set by L1. */

/*

 * Send interrupt to vcpu via posted interrupt way.

 * 1. If target vcpu is running(non-root mode), send posted interrupt

 * notification to vcpu and hardware will sync PIR to vIRR atomically.

 * 2. If target vcpu isn't running(root mode), kick it to pick up the

 * interrupt from PIR in next vmentry.

 If a previous notification has sent the IPI, nothing to do.  */

/*

 * Set up the vmcs's constant host-state fields, i.e., host-state fields that

 * will not change in the lifetime of the guest.

 * Note that host-state that does change is set elsewhere. E.g., host-state

 * that is set differently for each CPU is set in vmx_vcpu_load(), not here.

 22.2.3 */

	/*

	 * Save the most likely value for this task's CR3 in the VMCS.

	 * We can't use __get_current_cr3_fast() because we're not atomic.

 22.2.3  FIXME: shadow tables */

 Save the most likely value for this task's CR4 in the VMCS. */

 22.2.3, 22.2.5 */

 22.2.4 */

	/*

	 * Load null selectors, so we can avoid reloading them in

	 * vmx_prepare_switch_to_host(), in case userspace uses

	 * the null selectors too (the expected case).

 22.2.4 */

 22.2.4 */

 22.2.4 */

 22.2.4 */

 22.2.4 */

 22.2.5 */

 22.2.3 */

 Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */

 Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */

/*

 * Adjust a single secondary execution control bit to intercept/allow an

 * instruction in the guest.  This is usually done based on whether or not a

 * feature has been exposed to the guest in order to correctly emulate faults.

	/*

	 * If the control is for an opt-in feature, clear the control if the

	 * feature is not exposed to the guest, i.e. not enabled.  If the

	 * control is opt-out, i.e. an exiting control, clear the control if

	 * the feature _is_ exposed to the guest, i.e. exiting/interception is

	 * disabled for the associated instruction.  Note, the caller is

	 * responsible presetting exec_control to set all supported bits.

	/*

	 * Update the nested MSR settings so that a nested VMM can/can't set

	 * controls for features that are/aren't exposed to the guest.

/*

 * Wrapper macro for the common case of adjusting a secondary execution control

 * based on a single guest CPUID bit, with a dedicated feature bit.  This also

 * verifies that the control is actually supported by KVM and hardware.

 More macro magic for ENABLE_/opt-in versus _EXITING/opt-out controls. */

	/* SECONDARY_EXEC_DESC is enabled/disabled on writes to CR4.UMIP,

	/* SECONDARY_EXEC_SHADOW_VMCS is enabled when L1 executes VMPTRLD

	   (handle_vmptrld).

	   We can NOT enable shadow_vmcs here because we don't have yet

	   a current VMCS12

	/*

	 * PML is enabled/disabled when dirty logging of memsmlots changes, but

	 * it needs to be set here when dirty logging is already active, e.g.

	 * if this vCPU was created after dirty logging was enabled.

 Exposing XSAVES only when XSAVE is exposed */

	/*

	 * RDPID is also gated by ENABLE_RDTSCP, turn on the control if either

	 * feature is exposed to the guest.  This creates a virtualization hole

	 * if both are supported in hardware but only one is exposed to the

	 * guest, but letting the guest execute RDTSCP or RDPID when either one

	 * is advertised is preferable to emulating the advertised instruction

	 * in KVM on #UD, and obviously better than incorrectly injecting #UD.

 22.3.1.5 */

 Control */

 22.2.1 */

 22.2.4 */

 22.2.4 */

 22.2.4 */

 22.2.4 */

 22.2.1, 20.8.1 */

 Bit[6~0] are forced to 1, writes are ignored. */

	/*

	 * Enforce invariant: pi_desc.nv is always either POSTED_INTR_VECTOR

	 * or POSTED_INTR_WAKEUP_VECTOR.

 22.2.1 */

		/*

		 * Tracking the NMI-blocked state in software is built upon

		 * finding the next open IRQ window. This, in turn, depends on

		 * well-behaving guests: They have to keep IRQs disabled at

		 * least as long as the NMI handler runs. Otherwise we may

		 * cause NMI nesting, maybe breaking the guest. But as this is

		 * highly unlikely, we can live with the residual risk.

 An NMI must not be injected into L2 if it's supposed to VM-Exit.  */

       /*

        * An IRQ must not be injected into L2 if it's supposed to VM-Exit,

        * e.g. if the IRQ arrived asynchronously after checking nested events.

		/*

		 * Update instruction length as we may reinject the exception

		 * from user space while in guest debugging mode.

	/*

	 * Instruction with address size override prefix opcode 0x67

	 * Cause the #SS fault with 0 error code in VM86 mode.

	/*

	 * Forward all other exceptions that are valid in real mode.

	 * FIXME: Breaks guest debugging in real mode, needs to be fixed with

	 *        the required debugging infrastructure rework.

 handled by vmx_vcpu_run() */

/*

 * If the host has split lock detection disabled, then #AC is

 * unconditionally injected into the guest, which is the pre split lock

 * detection behaviour.

 *

 * If the host has split lock detection enabled then #AC is

 * only injected into the guest when:

 *  - Guest CPL == 3 (user mode)

 *  - Guest has #AC detection enabled in CR0

 *  - Guest EFLAGS has AC bit set

 handled by handle_exception_nmi_irqoff() */

		/*

		 * VMware backdoor emulation on #GP interception only handles

		 * IN{S}, OUT{S}, and RDPMC, none of which generate a non-zero

		 * error code on #GP.

	/*

	 * The #PF with PFEC.RSVD = 1 indicates the guest is accessing

	 * MMIO, it is better to report an internal error.

	 * See the comments in vmx_handle_exit.

			/*

			 * EPT will cause page fault only if we need to

			 * detect illegal GPAs.

		/*

		 * Update instruction length as we may reinject #BP from

		 * user space while in guest debugging mode. Reading it for

		 * #DB as well causes no harm, it is not used in that case.

		/*

		 * Handle split lock. Depending on detection mode this will

		 * either warn and disable split lock detection for this

		 * task or force SIGBUS on it.

	/*

	 * Patch in the VMCALL instruction:

 called to set cr0 as appropriate for a mov-to-cr0 exit. */

		/*

		 * We get here when L2 changed cr0 in a way that did not change

		 * any of L1's shadowed bits (see nested_vmx_exit_handled_cr),

		 * but did change L0 shadowed bits. So we first calculate the

		 * effective cr0 value that L1 would like to write into the

		 * hardware. It consists of the L2-owned bits from the new

		 * value combined with the L1-owned bits from L1's guest_cr0.

 analogously to handle_set_cr0 */

 mov to cr */

				/*

				 * TODO: we might be squashing a

				 * KVM_GUESTDBG_SINGLESTEP-triggered

				 * KVM_EXIT_DEBUG here.

 clts */

mov from cr*/

 lmsw */

 First, if DR does not exist, trigger UD */

		/*

		 * As the vm-exit takes precedence over the debug trap, we

		 * need to emulate the latter, either for the host or the

		 * guest debugging itself.

		/*

		 * No more DR vmexits; force a reload of the debug registers

		 * and reenter on this instruction.  The next vmexit will

		 * retrieve the full state of the debug registers.

	/*

	 * exc_debug expects dr6 to be cleared after it runs, avoid that it sees

	 * a stale dr6 from the guest.

		/*

		 * Sane guest uses MOV to write EOI, with written value

		 * not cared. So make a short-circuit here by avoiding

		 * heavy instruction emulation.

 EOI-induced VM exit is trap-like and thus no need to adjust IP */

 APIC-write VM exit is trap-like and thus no need to adjust IP */

	/*

	 * TODO: What about debug traps on tss switch?

	 *       Are we supposed to inject them and update dr6?

	/*

	 * EPT violation happened while executing iret from NMI,

	 * "blocked by NMI" bit has to be set before next VM entry.

	 * There are errata that may cause this bit to not be set:

	 * AAK134, BY25.

 Is it a read fault? */

 Is it a write fault? */

 Is it a fetch fault? */

 ept page table entry is present? */

	/*

	 * Check that the GPA doesn't exceed physical memory limits, as that is

	 * a guest page fault.  We have to emulate the instruction here, because

	 * if the illegal address is that of a paging structure, then

	 * EPT_VIOLATION_ACC_WRITE bit is set.  Alternatively, if supported we

	 * would also use advanced VM-exit information for EPT violations to

	 * reconstruct the page fault error code.

	/*

	 * A nested guest cannot optimize MMIO vmexits, because we have an

	 * nGPA here instead of the required GPA.

		/*

		 * Note, return 1 and not 0, vcpu_run() will invoke

		 * xfer_to_guest_mode() which will create a proper return

		 * code.

/*

 * Indicate a busy-waiting vcpu in spinlock. We do not enable the PAUSE

 * exiting, so only get here on cpu with PAUSE-Loop-Exiting.

	/*

	 * Intel sdm vol3 ch-25.1.3 says: The "PAUSE-loop exiting"

	 * VM-execution control is ignored if CPL > 0. OTOH, KVM

	 * never set PAUSE_EXITING and just set PLE if supported,

	 * so the vcpu must be CPL=0 if it gets a PAUSE exit.

	/* According to the Intel instruction reference, the memory operand

	 * is read even if it isn't needed (e.g., for type==all)

	/*

	 * PML buffer FULL happened while executing iret from NMI,

	 * "blocked by NMI" bit has to be set before next VM entry.

	/*

	 * PML buffer already flushed at beginning of VMEXIT. Nothing to do

	 * here.., and there's no userspace involvement needed for PML.

/*

 * When nested=0, all VMX instruction VM Exits filter here.  The handlers

 * are overwritten by nested_vmx_setup() when nested=1.

	/*

	 * SGX virtualization is disabled.  There is no software enable bit for

	 * SGX, so KVM intercepts all ENCLS leafs and injects a #UD to prevent

	 * the guest from executing ENCLS (when SGX is supported by hardware).

 CONFIG_X86_SGX_KVM */

	/*

	 * Hardware may or may not set the BUS_LOCK_DETECTED flag on BUS_LOCK

	 * VM-Exits. Unconditionally set the flag here and leave the handling to

	 * vmx_handle_exit().

/*

 * The exit handlers return 1 if the exit was handled fully and guest execution

 * may resume.  Otherwise they set the kvm_run parameter to indicate what needs

 * to be done to userspace and return 0.

 Do nothing if PML buffer is empty */

 PML index always points to next available PML buffer entity */

 reset PML index */

/*

 * The guest has exited.  See if we can fix it or if we need userspace

 * assistance.

	/*

	 * Flush logged GPAs PML buffer, this will make dirty_bitmap more

	 * updated. Another good is, in kvm_vm_ioctl_get_dirty_log, before

	 * querying dirty_bitmap, we only need to kick all vcpus out of guest

	 * mode as if vcpus is in root mode, the PML buffer must has been

	 * flushed already.  Note, PML is never enabled in hardware while

	 * running L2.

	/*

	 * We should never reach this point with a pending nested VM-Enter, and

	 * more specifically emulation of L2 due to invalid guest state (see

	 * below) should never happen as that means we incorrectly allowed a

	 * nested VM-Enter with an invalid vmcs12.

 If guest state is invalid, start emulating */

		/*

		 * PML is never enabled when running L2, bail immediately if a

		 * PML full exit occurs as something is horribly wrong.

		/*

		 * The host physical addresses of some pages of guest memory

		 * are loaded into the vmcs02 (e.g. vmcs12's Virtual APIC

		 * Page). The CPU may write to these pages via their host

		 * physical address while L2 is running, bypassing any

		 * address-translation-based dirty tracking (e.g. EPT write

		 * protection).

		 *

		 * Mark them dirty on every exit from L2 to prevent them from

		 * getting out of sync with dirty tracking.

	/*

	 * Note:

	 * Do not try to fix EXIT_REASON_EPT_MISCONFIG if it caused by

	 * delivery event since it indicates guest is accessing MMIO.

	 * The vm-exit can be triggered again after return to guest that

	 * will cause infinite loop.

			/*

			 * This CPU don't support us in finding the end of an

			 * NMI-blocked window if the guest runs with IRQs

			 * disabled. So we pull the trigger after 1 s of

			 * futile waiting, but inform the user about this.

	/*

	 * Exit to user space when bus lock detected to inform that there is

	 * a bus lock in guest.

/*

 * Software based L1D cache flush which is used when microcode providing

 * the cache control MSR is not loaded.

 *

 * The L1D cache is 32 KiB on Nehalem and later microarchitectures, but to

 * flush it is required to read in 64 KiB because the replacement algorithm

 * is not exactly LRU. This could be sized at runtime via topology

 * information but as all relevant affected CPUs have 32KiB L1D cache size

 * there is no point in doing so.

	/*

	 * This code is only executed when the the flush mode is 'cond' or

	 * 'always'

		/*

		 * Clear the per-vcpu flush bit, it gets set again

		 * either from vcpu_run() or from one of the unsafe

		 * VMEXIT handlers.

		/*

		 * Clear the per-cpu flush bit, it gets set again from

		 * the interrupt handlers.

 First ensure the pages are in the TLB */

 Now fill the cache */

 Postpone execution until vmcs01 is the current VMCS. */

			/*

			 * Flush the TLB, reloading the APIC access page will

			 * only do so if its physical address has changed, but

			 * the guest may have inserted a non-APIC mapping into

			 * the TLB while the APIC access page was disabled.

 Defer reload until vmcs01 is the current VMCS. */

	/*

	 * Do not pin apic access page in memory, the MMU notifier

	 * will call us again if it is migrated or swapped out.

	/*

	 * When running L2, updating RVI is only relevant when

	 * vmcs12 virtual-interrupt-delivery enabled.

	 * However, it can be enabled only when L1 also

	 * intercepts external-interrupts and in that case

	 * we should not update vmcs02 RVI but instead intercept

	 * interrupt. Therefore, do nothing when running L2.

		/*

		 * IOMMU can write to PID.ON, so the barrier matters even on UP.

		 * But on x86 this is just a compiler barrier anyway.

		/*

		 * If we are running L2 and L1 has a new pending interrupt

		 * which can be injected, this may cause a vmexit or it may

		 * be injected into L2.  Either way, this interrupt will be

		 * processed via KVM_REQ_EVENT, not RVI, because we do not use

		 * virtual interrupt delivery to inject L1 interrupts into L2.

 if exit due to PF check for async PF */

 Handle machine checks before interrupts are enabled */

 We need to handle NMIs before interrupts are enabled */

/*

 * The kvm parameter can be NULL (module initialization, or invocation before

 * VM creation). Be sure to check the kvm parameter before using it.

		/*

		 * We cannot do SMM unless we can run the guest in big

		 * real mode.

 This is AMD only.  */

		/*

		 * SDM 3: 27.7.1.2 (September 2008)

		 * Re-set bit "block by NMI" before VM entry if vmexit caused by

		 * a guest IRET fault.

		 * SDM 3: 23.2.2 (September 2008)

		 * Bit 12 is undefined in any of the following cases:

		 *  If the VM exit sets the valid bit in the IDT-vectoring

		 *   information field.

		 *  If the VM exit is due to a double fault.

		/*

		 * SDM 3: 27.7.1.2 (September 2008)

		 * Clear bit "block by NMI" before VM entry if a NMI

		 * delivery faulted.

 Note, nr_msrs may be garbage if perf_guest_get_msrs() returns NULL. */

 set_hv_timer ensures the delta fits in 32-bits */

 L1D Flush includes CPU buffer clear to mitigate MDS */

 Record the guest's net vcpu time for enforced NMI injections. */

	/*

	 * Don't enter VMX if guest state is invalid, let the exit handler

	 * start emulation until we arrive back to a valid state.  Synthesize a

	 * consistency check VM-Exit due to invalid guest state and bail.

 We don't emulate invalid state of a nested guest */

	/*

	 * We did this in prepare_switch_to_guest, because it needs to

	 * be within srcu_read_lock.

 When KVM_DEBUGREG_WONT_EXIT, dr6 is accessible in guest. */

	/* When single-stepping over STI and MOV SS, we must clear the

	 * corresponding interruptibility bits in the guest state. Otherwise

	 * vmentry fails as it then expects bit 14 (BS) in pending debug

	 * exceptions being set, but that's not correct for the guest debugging

	/*

	 * If this vCPU has touched SPEC_CTRL, restore the guest's value if

	 * it's non-zero. Since vmentry is serialising on affected CPUs, there

	 * is no need to worry about the conditional branch over the wrmsr

	 * being speculatively taken.

 The actual VMENTER/EXIT is in the .noinstr.text section. */

	/*

	 * We do not use IBRS in the kernel. If this vCPU has used the

	 * SPEC_CTRL MSR it may have left it on; save the value and

	 * turn it off. This is much more efficient than blindly adding

	 * it to the atomic save/restore list. Especially as the former

	 * (Saving guest MSRs on vmexit) doesn't even exist in KVM.

	 *

	 * For non-nested case:

	 * If the L01 MSR bitmap does not intercept the MSR, then we need to

	 * save it.

	 *

	 * For nested case:

	 * If the L02 MSR bitmap does not intercept the MSR, then we need to

	 * save it.

 All fields are clean at this point */

 MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */

	/*

	 * The sysexit path does not restore ds/es, so we must set them to

	 * a reasonable value ourselves.

	 *

	 * We can't defer this to vmx_prepare_switch_to_host() since that

	 * function may be executed in interrupt context, which saves and

	 * restore segments around it, nullifying its effect.

		/*

		 * Track VMLAUNCH/VMRESUME that have made past guest state

		 * checking.

	/*

	 * If PML is turned on, failure on enabling PML just results in failure

	 * of creating the vcpu, therefore we can simplify PML logic (by

	 * avoiding dealing with cases, such as enabling PML partially on vcpus

	 * for the guest), etc.

		/*

		 * TSX_CTRL_CPUID_CLEAR is handled in the CPUID interception.

		 * Keep the host value unchanged to avoid changing CPUID bits

		 * under the host kernel's feet.

 The MSR bitmap starts with all ones */

www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html for details.\n"

www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html for details.\n"

 'I explicitly don't care' is set */

			/*

			 * Warn upon starting the first VM in a potentially

			 * insecure environment.

 Flush is enforced */

	/* We wanted to honor guest CD/MTRR/PAT, but doing so could result in

	 * memory aliases with conflicting memory types and sometimes MCEs.

	 * We have to be careful as to what are honored and when.

	 *

	 * For MMIO, guest CD/MTRR are ignored.  The EPT memory type is set to

	 * UC.  The effective memory type is UC or WC depending on guest PAT.

	 * This was historically the source of MCEs and we want to be

	 * conservative.

	 *

	 * When there is no need to deal with noncoherent DMA (e.g., no VT-d

	 * or VT-d has snoop control), guest CD/MTRR/PAT are all ignored.  The

	 * EPT memory type is set to WB.  The effective memory type is forced

	 * WB.

	 *

	 * Otherwise, we trust guest.  Guest CD/MTRR/PAT are all honored.  The

	 * EPT memory type is used to emulate guest CD/MTRR.

	/*

	 * These bits in the secondary execution controls field

	 * are dynamic, the others are mostly based on the hypervisor

	 * architecture and the guest's CPUID.  Do not touch the

	 * dynamic bits.

/*

 * Generate MSR_IA32_VMX_CR{0,4}_FIXED1 according to CPUID. Only set bits

 * (indicating "allowed-1") if they are supported in the guest's CPUID.

 Get the number of configurable Address Ranges for filtering */

 Initialize and clear the no dependency bits */

	/*

	 * If CPUID.(EAX=14H,ECX=0):EBX[0]=1 CR3Filter can be set otherwise

	 * will inject an #GP

	/*

	 * If CPUID.(EAX=14H,ECX=0):EBX[1]=1 CYCEn, CycThresh and

	 * PSBFreq can be set

	/*

	 * If CPUID.(EAX=14H,ECX=0):EBX[3]=1 MTCEn and MTCFreq can be set

 If CPUID.(EAX=14H,ECX=0):EBX[4]=1 FUPonPTW and PTWEn can be set */

 If CPUID.(EAX=14H,ECX=0):EBX[5]=1 PwrEvEn can be set */

 If CPUID.(EAX=14H,ECX=0):ECX[0]=1 ToPA can be set */

 If CPUID.(EAX=14H,ECX=0):ECX[3]=1 FabricEn can be set */

 unmask address range configure area */

 xsaves_enabled is recomputed in vmx_compute_secondary_exec_control(). */

 Refresh #PF interception to account for MAXPHYADDR changes. */

 CPUID 0x1 */

 CPUID 0x7 */

 CPUID 0xD.1 */

 CPUID 0x80000001 and 0x7 (RDPID) */

	/*

	 * If the 'use IO bitmaps' VM-execution control is 0, IO instruction

	 * VM-exits depend on the 'unconditional IO exiting' VM-execution

	 * control.

	 *

	 * Otherwise, IO instruction VM-exits are controlled by the IO bitmaps.

 FIXME: produce nested vmexit and return X86EMUL_INTERCEPTED.  */

	/*

	 * RDPID causes #UD if disabled through secondary execution controls.

	 * Because it is marked as EmulateOnUD, we need to intercept it here.

	 * Note, RDPID is hidden behind ENABLE_RDTSCP.

 FIXME: produce nested vmexit and return X86EMUL_INTERCEPTED.  */

 TODO: check more intercepts... */

 (a << shift) / divisor, return 1 if overflow otherwise 0 */

 To avoid the overflow on divq */

 Low hold the result, high hold rem which is discarded */

 Convert to host delta tsc if tsc scaling is enabled */

	/*

	 * If the delta tsc can't fit in the 32 bit after the multi shift,

	 * we can't use the preemption timer.

	 * It's possible that it fits on later vmentries, but checking

	 * on every vmentry is costly so we just use an hrtimer.

	/*

	 * Note, cpu_dirty_logging_count can be changed concurrent with this

	 * code, but in that case another update request will be made and so

	 * the guest will never run with a stale PML value.

 we need a nested vmexit to enter SMM, postpone if run is pending */

 RSM will cause a vmexit anyway.  */

	/*

	 * Though SYSCALL is only supported in 64-bit mode on Intel CPUs, kvm

	 * will emulate SYSCALL in legacy mode if the vendor string in guest

	 * CPUID.0:{EBX,ECX,EDX} is "AuthenticAMD" or "AMDisbetter!" To

	 * support this emulation, MSR_STAR is included in the list for i386,

	 * but is never loaded into hardware.  MSR_CSTAR is also never loaded

	 * into hardware and is here purely for emulation purposes.

 NX support is required for shadow paging. */

	/*

	 * set_apic_access_page_addr() is used to reload apic access

	 * page upon invalidation.  No need to do anything if not

	 * using the APIC_ACCESS_ADDR VMCS field.

 0 is reserved for host */

	/*

	 * Only enable PML when hardware supports PML feature, and both EPT

	 * and EPT A/D bit features are enabled -- PML depends on them to work.

		/*

		 * KVM "disables" the preemption timer by setting it to its max

		 * value.  Don't use the timer if it might cause spurious exits

		 * at a rate faster than 0.1 Hz (of uninterrupted guest time).

 Restore state so sysfs ignores VMX */

		/*

		 * Reset everything to support using non-enlightened VMCS

		 * access later (e.g. when we reload the module with

		 * enlightened_vmcs=0)

	/*

	 * Enlightened VMCS usage should be recommended and the host needs

	 * to support eVMCS v1 or above. We can also disable eVMCS support

	 * with module parameter.

 Check that we have assist pages on all online CPUs */

	/*

	 * Must be called after kvm_init() so enable_ept is properly set

	 * up. Hand the parameter mitigation value in which was stored in

	 * the pre module init parser. If no parameter was given, it will

	 * contain 'auto' which will be turned into the default 'cond'

	 * mitigation mode.

	/*

	 * Shadow paging doesn't have a (further) performance penalty

	 * from GUEST_MAXPHYADDR < HOST_MAXPHYADDR so enable it

	 * by default

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel-based Virtual Machine driver for Linux

 *

 * AMD SVM support

 *

 * Copyright (C) 2006 Qumranet, Inc.

 * Copyright 2010 Red Hat, Inc. and/or its affiliates.

 *

 * Authors:

 *   Yaniv Kamay  <yaniv@qumranet.com>

 *   Avi Kivity   <avi@qumranet.com>

		/*

		 * TODO: track the cause of the nested page fault, and

		 * correctly fill in the high bits of exit_info_1.

	/*

	 * The NPT format depends on L1's CR4 and EFER, which is in vmcb01.  Note,

	 * when called via KVM_SET_NESTED_STATE, that state may _not_ match current

	 * vCPU state.  CR0.WP is explicitly ignored, while CR0.PG is required.

 We only want the cr8 intercept bits of L1 */

		/*

		 * Once running L2 with HF_VINTR_MASK, EFLAGS.IF does not

		 * affect any interrupt we may want to inject; therefore,

		 * interrupt window vmexits are irrelevant to L0.

 We don't want to see VMMCALLs from a nested guest */

 If SMI is not intercepted, ignore guest SMI intercept as well  */

 asid not copied, it is handled manually for svm->vmcb.  */

	/*

	 * This function merges the msr permission bitmaps of kvm and the

	 * nested vmcb. It is optimized in that it only merges the parts where

	 * the kvm msr permission bitmap may contain zero bits

/*

 * Bits 11:0 of bitmap address are ignored by hardware

 Nested FLUSHBYASID is not supported yet.  */

	/*

	 * These checks are also performed by KVM_SET_SREGS,

	 * except that EFER.LMA is not checked by SVM against

	 * CR0.PG && EFER.LME.

 Common checks that apply to both L1 and L2 state.  */

	/*

	 * FIXME: these should be done after copying the fields,

	 * to avoid TOC/TOU races.  For these save area checks

	 * the possible damage is limited since kvm_set_cr0 and

	 * kvm_set_cr4 handle failure; EFER_SVME is an exception

	 * so it is force-set later in nested_prepare_vmcb_save.

 Copy it here because nested_svm_check_controls will check it.  */

/*

 * Synchronize fields that are written by the processor, so that

 * they can be copied back into the vmcb12.

 Only a few fields of int_ctl are written by the processor.  */

		/*

		 * In order to request an interrupt window, L0 is usurping

		 * svm->vmcb->control.int_ctl and possibly setting V_IRQ

		 * even if it was clear in L1's VMCB.  Restoring it would be

		 * wrong.  However, in this case V_IRQ will remain true until

		 * interrupt_window_interception calls svm_clear_vintr and

		 * restores int_ctl.  We can just leave it aside.

/*

 * Transfer any event that L0 or L1 wanted to inject into L2 to

 * EXIT_INT_INFO.

	/*

	 * TODO: optimize unconditional TLB flush/MMU sync.  A partial list of

	 * things to fix before this can be conditional:

	 *

	 *  - Flush TLBs for both L1 and L2 remote TLB flush

	 *  - Honor L1's request to flush an ASID on nested VMRUN

	 *  - Sync nested NPT MMU on VMRUN that flushes L2's ASID[*]

	 *  - Don't crush a pending TLB flush in vmcb02 on nested VMRUN

	 *  - Flush L1's ASID on KVM_REQ_TLB_FLUSH_GUEST

	 *

	 * [*] Unlike nested EPT, SVM's ASID management can invalidate nested

	 *     NPT guest-physical mappings on VMRUN.

/*

 * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true

 * if we are emulating VM-Entry into a guest with NPT enabled.

 Re-initialize the MMU, e.g. to pick up CR4 MMU role changes. */

 FIXME: merge g_pat from vmcb01 and vmcb12.  */

 Load the nested guest state */

	/*

	 * Force-set EFER_SVME even though it is checked earlier on the

	 * VMCB12, because the guest can flip the bit between the check

	 * and now.  Clearing EFER_SVME would call svm_free_nested.

 In case we don't even reach vcpu_run, the fields are not updated */

 These bits will be set properly on the first execution when new_vmc12 is true */

	/*

	 * Filled at exit: exit_code, exit_code_hi, exit_info_1, exit_info_2,

	 * exit_int_info, exit_int_info_err, next_rip, insn_len, insn_bytes.

	/*

	 * Also covers avic_vapic_bar, avic_backing_page, avic_logical_id,

	 * avic_physical_id.

 Copied from vmcb01.  msrpm_base can be overwritten later.  */

 Done at vmrun: asid.  */

 Also overwritten later if necessary.  */

 nested_cr3.  */

 Enter Guest-Mode */

	/*

	 * Merge guest and host intercepts - must be called with vcpu in

	 * guest-mode to take effect.

	/*

	 * Some VMCB state is shared between L1 and L2 and thus has to be

	 * moved at the time of nested vmrun and vmexit.

	 *

	 * VMLOAD/VMSAVE state would also belong in this category, but KVM

	 * always performs VMLOAD and VMSAVE from the VMCB01.

	/*

	 * Since vmcb01 is not in use, we can use it to store some of the L1

	 * state.

 Copy state save area fields which are handled by VMRUN */

 Triple faults in L2 should never escape. */

 Exit Guest-Mode */

 in case we halted in L2 */

 Give the current vmcb to the guest */

	/*

	 * On vmexit the  GIF is set to false and

	 * no event can be injected in L1.

	/*

	 * Restore processor state that had been saved in vmcb01

	/*

	 * Drop what we picked up for L2 via svm_complete_interrupts() so it

	 * doesn't end up in L1.

	/*

	 * If we are here following the completion of a VMRUN that

	 * is being single-stepped, queue the pending #DB intercept

	 * right now so that it an be accounted for before we execute

	 * L1's next instruction.

	/*

	 * When last_vmcb12_gpa matches the current vmcb12 gpa,

	 * some vmcb12 fields are not loaded if they are marked clean

	 * in the vmcb12, since in this case they are up to date already.

	 *

	 * When the vmcb02 is freed, this optimization becomes invalid.

/*

 * Forcibly leave nested mode in order to be able to reset the VCPU later on.

 Offset is in 32 bit units but need in 8 bit units */

		/*

		 * Host-intercepted exceptions have been checked already in

		 * nested_svm_exit_special.  There is nothing to do here,

		 * the vmexit is injected by svm_check_nested_events.

	/*

	 * EXITINFO2 is undefined for all exception intercepts other

	 * than #PF.

 See inject_pending_event.  */

		/*

		 * Only a pending nested run can block a pending exception.

		 * Otherwise an injected NMI/interrupt should either be

		 * lost or delivered to the nested hypervisor in the EXITINTINFO

		 * vmcb field, while delivering the pending exception.

 Trap async PF even if not shadowing */

 First fill in the header and copy it out.  */

	/*

	 * Copy over the full size of the VMCB rather than just the size

	 * of the structs.

	/*

	 * If in guest mode, vcpu->arch.efer actually refers to the L2 guest's

	 * EFER.SVME, but EFER.SVME still has to be 1 for VMRUN to succeed.

 GIF=1 and no guest mode are required if SVME=0.  */

 SMM temporarily disables SVM, so we cannot be in guest mode.  */

	/*

	 * Processor state contains L2 state.  Check that it is

	 * valid for guest mode (see nested_vmcb_check_save).

	/*

	 * Validate host state saved from before VMRUN (see

	 * nested_svm_check_permissions).

	/*

	 * While the nested guest CR3 is already checked and set by

	 * KVM_SET_SREGS, it was set when nested state was yet loaded,

	 * thus MMU might not be initialized correctly.

	 * Set it again to fix this.

	/*

	 * All checks done, we can enter guest mode. Userspace provides

	 * vmcb12.control, which will be combined with L1 and stored into

	 * vmcb02, and the L1 save state which we store in vmcb01.

	 * L2 registers if needed are moved from the current VMCB to VMCB02.

		/*

		 * Reload the guest's PDPTRs since after a migration

		 * the guest CR3 might be restored prior to setting the nested

		 * state which can lead to a load of wrong PDPTRs.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel-based Virtual Machine driver for Linux

 *

 * AMD SVM-SEV support

 *

 * Copyright 2010 Red Hat, Inc. and/or its affiliates.

/*

 * When this config is not defined, SEV feature is not supported and APIs in

 * this file are not used but this file still gets compiled into the KVM AMD

 * module.

 *

 * We will not have MISC_CG_RES_SEV and MISC_CG_RES_SEV_ES entries in the enum

 * misc_res_type {} defined in linux/misc_cgroup.h.

 *

 * Below macros allow compilation to succeed.

 enable/disable SEV support */

 enable/disable SEV-ES support */

 CONFIG_KVM_AMD_SEV */

 Called with the sev_bitmap_lock held, or on shutdown  */

 Check if there are any ASIDs to reclaim before performing a flush */

	/*

	 * DEACTIVATE will clear the WBINVD indicator causing DF_FLUSH to fail,

	 * so it must be guarded.

 Must be called with the sev_bitmap_lock held */

 The flush process will flush all reclaimable SEV and SEV-ES ASIDs */

	/*

	 * SEV-enabled guests must use asid from min_sev_asid to max_sev_asid.

	 * SEV-ES-enabled guest can use from 1 to min_sev_asid - 1.

 Guard DEACTIVATE against WBINVD/DF_FLUSH used in ASID recycling */

 activate ASID on the given handle */

 create memory encryption context */

 Bind ASID to this guest */

 return handle to userspace */

 Calculate number of pages. */

 Avoid using vmalloc for smaller buffers. */

 Pin the user virtual address. */

 find the number of contiguous pages starting from idx */

 Lock the user memory. */

	/*

	 * Flush (on non-coherent CPUs) before LAUNCH_UPDATE encrypts pages in

	 * place; the cache may contain the data that was written unencrypted.

		/*

		 * If the user buffer is not page-aligned, calculate the offset

		 * within the page.

 Calculate the number of pages that can be encrypted in one go. */

 content of memory is updated, mark pages dirty */

 unlock the user pages */

 Check some debug related fields before encrypting the VMSA */

 Sync registgers */

 Sync some non-GPR registers before encrypting */

	/*

	 * SEV-ES will use a VMSA that is pointed to by the VMCB, not

	 * the traditional VMSA that is part of the VMCB. Copy the

	 * traditional VMSA as it has been built so far (in prep

	 * for LAUNCH_UPDATE_VMSA) to be the initial SEV-ES state.

 Perform some pre-encryption checks against the VMSA */

	/*

	 * The LAUNCH_UPDATE_VMSA command will perform in-place encryption of

	 * the VMSA memory content (i.e it will write the same memory region

	 * with the guest's key), so invalidate it first.

 User wants to query the blob length */

	/*

	 * If we query the session length, FW responded with expected data.

	/*

	 * Its safe to read more than we are asked, caller should ensure that

	 * destination has enough space.

 if inputs are not 16-byte then use intermediate buffer */

 If source buffer is not aligned then use an intermediate buffer */

	/*

	 *  If destination buffer or length is not aligned then do read-modify-write:

	 *   - decrypt destination in an intermediate buffer

	 *   - copy the source buffer in an intermediate buffer

	 *   - use the intermediate buffer as source buffer

		/*

		 *  If source is kernel buffer then use memcpy() otherwise

		 *  copy_from_user().

 lock userspace source and destination page */

		/*

		 * Flush (on non-coherent CPUs) before DBG_{DE,EN}CRYPT read or modify

		 * the pages; flush the destination too so that future accesses do not

		 * see stale data.

		/*

		 * Since user buffer may not be page aligned, calculate the

		 * offset within the page.

	/*

	 * Flush (on non-coherent CPUs) before LAUNCH_SECRET encrypts pages in

	 * place; the cache may contain the data that was written unencrypted.

	/*

	 * The secret must be copied into contiguous memory region, lets verify

	 * that userspace memory pages are contiguous before we issue command.

 content of memory is updated, mark pages dirty */

 User wants to query the blob length */

	/*

	 * If we query the session length, FW responded with expected data.

 Userspace wants to query session length. */

 if session_len is zero, userspace wants to query the session length */

 some sanity checks */

 allocate the memory to hold the session data blob */

 copy the certificate blobs from userspace */

 populate the FW SEND_START field with system physical address */

 Userspace wants to query either header or trans length. */

 userspace wants to query either header or trans length */

 Check if we are crossing the page boundary */

 Pin guest memory */

 allocate memory for header and transport buffer */

 The SEND_UPDATE_DATA command requires C-bit to be always set. */

 copy transport buffer to user space */

 Copy packet header to userspace. */

 Get parameter from the userspace */

 some sanity checks */

 create memory encryption context */

 Bind ASID to this guest */

 Check if we are crossing the page boundary */

 Pin guest memory */

	/*

	 * Flush (on non-coherent CPUs) before RECEIVE_UPDATE_DATA, the PSP

	 * encrypts the written data with the guest's key, and the cache may

	 * contain dirty, unencrypted data.

 The RECEIVE_UPDATE_DATA command requires C-bit to be always set. */

	/*

	 * Allow mirrors VM to call KVM_SEV_LAUNCH_UPDATE_VMSA to enable SEV-ES

	 * active mirror VMs. Also allow the debugging and status commands.

	/*

	 * Bail if this VM is already involved in a migration to avoid deadlock

	 * between two VMs trying to migrate to/from each other.

		/*

		 * Transfer VMSA and GHCB state to the destination.  Nullify and

		 * clear source fields as appropriate, the state now belongs to

		 * the destination.

 Operates on the source on success, on the destination on failure.  */

 Only the enc_context_owner handles some memory enc operations. */

 If kvm is mirroring encryption context it isn't responsible for it */

	/*

	 * The guest may change the memory encryption attribute from C=0 -> C=1

	 * or vice versa for this memory range. Lets make sure caches are

	 * flushed to ensure that guest data gets written into memory with

	 * correct C-bit.

 If kvm is mirroring encryption context it isn't responsible for it */

	/*

	 * Ensure that all guest tagged cache entries are flushed before

	 * releasing the pages back to the system for use. CLFLUSH will

	 * not do this, so issue a WBINVD.

 Mirrors of mirrors should work, but let's not get silly */

	/*

	 * The mirror kvm holds an enc_context_owner ref so its asid can't

	 * disappear until we're done with it

	/*

	 * Disallow out-of-band SEV/SEV-ES init if the target is already an

	 * SEV guest, or if vCPUs have been created.  KVM relies on vCPUs being

	 * created after SEV/SEV-ES initialization, e.g. to init intercepts.

 Set enc_context_owner and copy its encryption context over */

	/*

	 * Do not copy ap_jump_table. Since the mirror does not share the same

	 * KVM contexts as the original, and they may have different

	 * memory-views.

 If this is a mirror_kvm release the enc_context_owner and skip sev cleanup */

	/*

	 * Ensure that all guest tagged cache entries are flushed before

	 * releasing the pages back to the system for use. CLFLUSH will

	 * not do this, so issue a WBINVD.

	/*

	 * if userspace was terminated before unregistering the memory regions

	 * then lets unpin all the registered memory.

 Does the CPU support SEV? */

 Retrieve SEV CPUID information */

 Set encryption bit location for SEV-ES guests */

 Maximum number of encrypted guests supported simultaneously */

 Minimum ASID value that should be used for SEV guest */

	/*

	 * Initialize SEV ASID bitmaps. Allocate space for ASID 0 in the bitmap,

	 * even though it's never used, so that the bitmap is indexed by the

	 * actual ASID.

 SEV-ES support requested? */

 Does the CPU support SEV-ES? */

 Has the system been allocated ASIDs for SEV-ES? */

 No need to take sev_bitmap_lock, all VMs have been destroyed. */

/*

 * Pages used by hardware to hold guest encrypted state must be flushed before

 * returning them to the system.

	/*

	 * If hardware enforced cache coherency for encrypted mappings of the

	 * same physical page is supported, nothing to do.

	/*

	 * If the VM Page Flush MSR is supported, use it to flush the page

	 * (using the page virtual address and the guest ASID).

 Align start and stop to page boundaries. */

	/*

	 * Hardware should always have one of the above features,

	 * but if not, use WBINVD and issue a warning.

 Re-use the dump_invalid_vmcb module parameter */

	/*

	 * The GHCB protocol so far allows for the following data

	 * to be returned:

	 *   GPRs RAX, RBX, RCX, RDX

	 *

	 * Copy their values, even if they may not have been written during the

	 * VM-Exit.  It's the guest's responsibility to not consume random data.

	/*

	 * The GHCB protocol so far allows for the following data

	 * to be supplied:

	 *   GPRs RAX, RBX, RCX, RDX

	 *   XCR0

	 *   CPL

	 *

	 * VMMCALL allows the guest to provide extra registers. KVM also

	 * expects RSI for hypercalls, so include that, too.

	 *

	 * Copy their values to the appropriate location if supplied.

 Copy the GHCB exit information into the VMCB fields */

 Clear the valid entries fields */

 Only GHCB Usage code 0 is supported */

	/*

	 * Retrieve the exit code now even though is may not be marked valid

	 * as it could help with debugging.

		/*

		 * The scratch area lives outside the GHCB, so there is a

		 * buffer that, depending on the operation performed, may

		 * need to be synced, then freed.

 Assign the asid allocated with this SEV guest */

	/*

	 * Flush guest TLB:

	 *

	 * 1) when different VMCB for the same ASID is to be run on the same host CPU.

	 * 2) or this VMCB was executed on different host CPU in previous VMRUNs.

 Scratch area begins within GHCB */

		/*

		 * If the scratch area begins within the GHCB, it must be

		 * completely contained in the GHCB shared buffer area.

		/*

		 * The guest memory must be read into a kernel buffer, so

		 * limit the size

 Unable to copy scratch area from guest */

		/*

		 * The scratch area is outside the GHCB. The operation will

		 * dictate whether the buffer needs to be synced before running

		 * the vCPU next time (i.e. a read was requested so the data

		 * must be written back to the guest memory).

 Initialize the registers needed by the CPUID intercept */

 Validate the GHCB */

 Unable to map GHCB from guest */

 Set AP jump table address */

 Get AP jump table address */

	/*

	 * An SEV-ES guest requires a VMSA area that is a separate from the

	 * VMCB page. Do not include the encryption mask on the VMSA physical

	 * address since hardware will access it using the guest key.

 Can't intercept CR register access, HV can't modify CR registers */

 Track EFER/CR register changes */

 No support for enable_vmware_backdoor */

 Can't intercept XSETBV, HV can't modify XCR0 directly */

 Clear intercepts on selected MSRs */

	/*

	 * Set the GHCB MSR value as per the GHCB specification when emulating

	 * vCPU RESET for an SEV-ES guest.

	/*

	 * As an SEV-ES guest, hardware will restore the host state on VMEXIT,

	 * of which one step is to perform a VMLOAD. Since hardware does not

	 * perform a VMSAVE on VMRUN, the host savearea must be updated.

 XCR0 is restored on VMEXIT, save the current host value */

 PKRU is restored on VMEXIT, save the current host value */

 MSR_IA32_XSS is restored on VMEXIT, save the currnet host value */

 First SIPI: Use the values as initially set by the VMM */

	/*

	 * Subsequent SIPI: Return from an AP Reset Hold VMGEXIT, where

	 * the guest will set the CS and RIP. Set SW_EXIT_INFO_2 to a

	 * non-zero value.

/*

 * Set osvw_len to higher value when updated Revision Guides

 * are published and we know what the new status bits are

 Index of the MSR */

 True if intercept is initially cleared */

/*

 * These 2 parameters are used to config the controls for Pause-Loop Exiting:

 * pause_filter_count: On processors that support Pause filtering(indicated

 *	by CPUID Fn8000_000A_EDX), the VMCB provides a 16 bit pause filter

 *	count value. On VMRUN this value is loaded into an internal counter.

 *	Each time a pause instruction is executed, this counter is decremented

 *	until it reaches zero at which time a #VMEXIT is generated if pause

 *	intercept is enabled. Refer to  AMD APM Vol 2 Section 15.14.4 Pause

 *	Intercept Filtering for more details.

 *	This also indicate if ple logic enabled.

 *

 * pause_filter_thresh: In addition, some processor families support advanced

 *	pause filtering (indicated by CPUID Fn8000_000A_EDX) upper bound on

 *	the amount of time a guest is allowed to execute in a pause loop.

 *	In this mode, a 16-bit pause filter threshold field is added in the

 *	VMCB. The threshold value is a cycle count that is used to reset the

 *	pause counter. As with simple pause filtering, VMRUN loads the pause

 *	count value from VMCB into an internal counter. Then, on each pause

 *	instruction the hardware checks the elapsed number of cycles since

 *	the most recent pause instruction against the pause filter threshold.

 *	If the elapsed cycle count is greater than the pause filter threshold,

 *	then the internal pause count is reloaded from the VMCB and execution

 *	continues. If the elapsed cycle count is less than the pause filter

 *	threshold, then the internal pause count is decremented. If the count

 *	value is less than zero and PAUSE intercept is enabled, a #VMEXIT is

 *	triggered. If advanced pause filtering is supported and pause filter

 *	threshold field is set to zero, the filter will operate in the simpler,

 *	count only mode.

 Default doubles per-vcpu window every exit. */

 Default resets per-vcpu window every exit to pause_filter_count. */

 Default is to compute the maximum so we can never overflow. */

/*

 * Use nested page tables by default.  Note, NPT may get forced off by

 * svm_hardware_setup() if it's unsupported by hardware or the host kernel.

 allow nested virtualization in KVM/SVM */

 enable/disable Next RIP Save */

 enable/disable Virtual VMLOAD VMSAVE */

 enable/disable Virtual GIF */

 enable/disable LBR virtualization */

/*

 * enable / disable AVIC.  Because the defaults differ for APICv

 * support between VMX and SVM we cannot use module_param_named.

/*

 * Only MSR_TSC_AUX is switched via the user return hook.  EFER is switched via

 * the VMCB, and the SYSCALL/SYSENTER MSRs are handled by VMLOAD/VMSAVE.

 *

 * RDTSCP and RDPID are not used in the kernel, specifically to allow KVM to

 * defer the restoration of TSC_AUX until the CPU returns to userspace.

 4 msrs per u8 */

 add range offset */

 Now we have the u8 offset - but need the u32 offset */

 MSR not in any range */

 Shadow paging assumes NX to be available.  */

 #GP intercept is still needed for vmware backdoor */

			/*

			 * Free the nested guest state, unless we are in SMM.

			 * In this case we will return to the nested guest

			 * as soon as we leave SMM.

	/*

	 * SEV-ES does not expose the next RIP. The RIP update is controlled by

	 * the type of exit and the #VC handler in the guest.

		/*

		 * For guest debugging where we have to reinject #BP if some

		 * INT3 is guest-owned:

		 * Emulate nRIP by moving RIP forward. Will fail if injection

		 * raises a fault that is not intercepted. Still better than

		 * failing in all cases.

 Use _safe variants to not break nested virtualization */

	/*

	 * Guests should see errata 400 and 415 as fixed (assuming that

	 * HLT and IO instructions are intercepted).

	/*

	 * By increasing VCPU's osvw.length to 3 we are telling the guest that

	 * all osvw.status bits inside that length, including bit 0 (which is

	 * reserved for erratum 298), are valid. However, if host processor's

	 * osvw_len is 0 then osvw_status[0] carries no information. We need to

	 * be conservative here and therefore we tell the guest that erratum 298

	 * is present (because we really don't know).

 Make sure we clean up behind us */

		/*

		 * Set the default value, even if we don't use TSC scaling

		 * to avoid having stale value in the msr

	/*

	 * Get OSVW bits.

	 *

	 * Note that it is possible to have a system with mixed processor

	 * revisions and therefore different OSVW bits. If bits are not the same

	 * on different processors then choose the worst case (i.e. if erratum

	 * is present on one processor and not on another then assume that the

	 * erratum is present everywhere).

 Set the shadow bitmaps to the desired intercept states */

	/*

	 * If this warning triggers extend the direct_access_msrs list at the

	 * beginning of the file

 Enforce non allowed MSRs to trap */

	/*

	 * Set intercept permissions for all direct access MSRs again. They

	 * will automatically get filtered through the MSR filter, so we are

	 * back in sync after this.

 Offset already in list? */

 Slot used by another offset? */

 Add offset to list */

	/*

	 * If this BUG triggers the msrpm_offsets table has an overflow. Just

	 * increase MSRPM_OFFSETS in this case.

 Clear our flags if they were not set by the guest */

/*

 * The default MMIO mask is a single bit (excluding the present bit),

 * which could conflict with the memory encryption bit. Check for

 * memory encryption support and override the default MMIO mask if

 * memory encryption is enabled.

 If there is no memory encryption support, use existing mask */

 If memory encryption is not enabled, use existing mask */

 Increment the mask bit if it is the same as the encryption bit */

	/*

	 * If the mask bit location is below 52, then some bits above the

	 * physical addressing limit will always be reserved, so use the

	 * rsvd_bits() function to generate the mask. This mask, along with

	 * the present bit, will be used to generate a page fault with

	 * PFER.RSV = 1.

	 *

	 * If the mask bit location is 52 (or above), then clear the mask.

 CPUID 0x80000001 and 0x8000000A (SVM features) */

 Nested VM can receive #VMEXIT instead of triggering #GP */

 CPUID 0x80000008 */

 CPUID 0x8000001F (SME/SEV features) */

	/*

	 * NX is required for shadow paging and for NPT if the NX huge pages

	 * mitigation is enabled.

 Check for pause filtering support */

	/*

	 * KVM's MMU doesn't support using 2-level paging for itself, and thus

	 * NPT isn't supported if the host is using 2-level paging since host

	 * CR4 is unchanged on VMRUN.

 Force VM NPT level equal to the host's max NPT level */

 Note, SEV setup consumes npt_enabled. */

	/*

	 * It seems that on AMD processors PTE's accessed bit is

	 * being set by the CPU hardware before the NPF vmexit.

	 * This is not expected behaviour and our tests fail because

	 * of it.

	 * A workaround here is to disable support for

	 * GUEST_MAXPHYADDR < HOST_MAXPHYADDR if NPT is enabled.

	 * In this case userspace can know if there is support using

	 * KVM_CAP_SMALLER_MAXPHYADDR extension and decide how to handle

	 * it

	 * If future AMD CPU models change the behaviour described above,

	 * this variable can be changed accordingly

 Read/Write Data Segment */

 Evaluate instruction intercepts that depend on guest CPUID features. */

	/*

	 * Intercept INVPCID if shadow paging is enabled to sync/free shadow

	 * roots, or if INVPCID is disabled in the guest to inject #UD.

		/*

		 * We must intercept SYSENTER_EIP and SYSENTER_ESP

		 * accesses because the processor only stores 32 bits.

		 * For the same reason we cannot use virtual VMLOAD/VMSAVE.

		/*

		 * If hardware supports Virtual VMLOAD VMSAVE then enable it

		 * in VMCB and clear intercepts to avoid #VMEXIT.

 No need to intercept these MSRs */

	/*

	 * Guest access to VMware backdoor ports could legitimately

	 * trigger #GP because of TSS I/O permission bitmap.

	 * We intercept those #GP and allow access to them anyway

	 * as VMware does.

 Executable/Readable Code Segment */

 Setup VMCB for Nested Paging */

	/*

	 * If the host supports V_SPEC_CTRL then disable the interception

	 * of MSR_IA32_SPEC_CTRL.

 Perform SEV-ES specific VMCB updates */

		/*

		 * SEV-ES guests require a separate VMSA page used to contain

		 * the encrypted register state of the guest.

		/*

		 * SEV-ES guests maintain an encrypted version of their FPU

		 * state which is restored and saved on VMRUN and VMEXIT.

		 * Mark vcpu->arch.guest_fpu->fpstate as scratch so it won't

		 * do xsave/xrstor on it.

	/* We initialize this flag to true to make sure that the is_running

	 * bit would be set the first time the vcpu is loaded.

	/*

	 * The vmcb page can be recycled, causing a false negative in

	 * svm_vcpu_load(). So, ensure that no logical CPU has this

	 * vmcb page recorded as its current vmcb.

	/*

	 * Save additional host state that will be restored on VMEXIT (sev-es)

	 * or subsequent vmload of host save area.

 Hide our flags if they were not set by the guest */

       /*

        * Any change of EFLAGS.VM is accompanied by a reload of SS

        * (caused by either a task switch or an inter-privilege IRET),

        * so we do not need to update the CPL here.

	/*

	 * The following fields are ignored when AVIC is enabled

	/*

	 * This is just a dummy VINTR to actually cause a vmexit to happen.

	 * Actual injection of virtual interrupts happens through EVENTINJ.

control->int_vector >> 4*/ 0xf) << V_INTR_PRIO_SHIFT);

 Drop int_ctl fields related to VINTR injection.  */

	/*

	 * AMD CPUs circa 2014 track the G bit for all segments except CS.

	 * However, the SVM spec states that the G bit is not observed by the

	 * CPU, and some VMware virtual CPUs drop the G bit for all segments.

	 * So let's synthesize a legal G bit for all segments, this helps

	 * running KVM nested. It also helps cross-vendor migration, because

	 * Intel's vmentry has a check on the 'G' bit.

	/*

	 * AMD's VMCB does not have an explicit unusable field, so emulate it

	 * for cross vendor migration purposes by "not present"

		/*

		 * Work around a bug where the busy flag in the tr selector

		 * isn't exposed

		/*

		 * The accessed bit must always be set in the segment

		 * descriptor cache, although it can be cleared in the

		 * descriptor, the cached bit always remains at 1. Since

		 * Intel has a check on this, set it here to support

		 * cross-vendor migration.

		/*

		 * On AMD CPUs sometimes the DB bit in the segment

		 * descriptor is left as 1, although the whole segment has

		 * been made unusable. Clear it here to pass an Intel VMX

		 * entry check when cross vendor migrating.

 This is symmetric with svm_set_segment() */

	/*

	 * re-enable caching here because the QEMU bios

	 * does not do it - this results in some delay at

	 * reboot

	/*

	 * SEV-ES guests must always keep the CR intercepts cleared. CR

	 * tracking is done using the CR write traps.

 Selective CR0 write remains on.  */

	/*

	 * This is always accurate, except if SYSRET returned to a segment

	 * with SS.DPL != 3.  Intel does not have this quirk, and always

	 * forces SS.DPL to 3 on sysret, so we ignore that case; fixing it

	 * would entail passing the CPL to userspace and back.

 This is symmetric with svm_get_segment() */

	/*

	 * We cannot reset svm->vmcb->save.dr6 to DR6_ACTIVE_LOW here,

	 * because db_interception might need it.  We can do it before vmentry.

 Make sure we check for pending NMIs upon entry */

 Bit 62 may or may not be set for this mce */

 Clear MCi_STATUS registers */

 Flush tlb to evict multi-match entries */

		/*

		 * Erratum 383 triggered. Guest state is corrupt so kill the

		 * guest.

	/*

	 * On an #MC intercept the MCE handler is not called automatically in

	 * the host. So do it by hand here.

	/*

	 * The VM save area has already been encrypted so it

	 * cannot be reinitialized - just terminate.

	/*

	 * VMCB is undefined after a SHUTDOWN intercept.  INIT the vCPU to put

	 * the VMCB in a known good state.  Unfortuately, KVM doesn't have

	 * KVM_MP_STATE_SHUTDOWN and can't add it without potentially breaking

	 * userspace.  At a platform view, INIT is acceptable behavior as

	 * there exist bare metal platforms that automatically INIT the CPU

	 * in response to shutdown.

 address size bug? */

 Return NONE_SVM_INSTR if not SVM instrs, otherwise return decode result */

 VMRUN */

 VMLOAD */

 VMSAVE */

 Returns '1' or -errno on failure, '0' on success. */

/*

 * #GP handling code. Note that #GP can be triggered under the following two

 * cases:

 *   1) SVM VM-related instructions (VMRUN/VMSAVE/VMLOAD) that trigger #GP on

 *      some AMD CPUs when EAX of these instructions are in the reserved memory

 *      regions (e.g. SMM memory on host).

 *   2) VMware backdoor

 Both #GP cases have zero error_code */

 All SVM instructions expect page aligned RAX */

 Decode the instruction for usage later */

		/*

		 * VMware backdoor emulation on #GP interception only handles

		 * IN{S}, OUT{S}, and RDPMC.

		/*

		 * If VGIF is enabled, the STGI intercept is only added to

		 * detect the opening of the SMI/NMI window; remove it now.

		 * Likewise, clear the VINTR intercept, we will set it

		 * again while processing KVM_REQ_EVENT if needed.

		/*

		 * After a CLGI no interrupts should come.  But if vGIF is

		 * in use, we still rely on the VINTR intercept (rather than

		 * STGI) to detect an open interrupt window.

 FIXME: Handle an address size prefix. */

 Let's treat INVLPGA the same as INVLPG (can be optimized!) */

 mov to cr */

 mov from cr */

		/*

		 * No more DR vmexits; force a reload of the debug registers

		 * and reenter on this instruction.  The next vmexit will

		 * retrieve the full state of the debug registers.

 mov to DRn  */

 instruction emulation calls kvm_set_cr8() */

	/*

	 * Clear the EFER_SVME bit from EFER. The SVM code always sets this

	 * bit in svm_set_efer(), but __kvm_valid_efer() checks it against

	 * whether the guest has X86_FEATURE_SVM - this avoids a failure if

	 * the guest doesn't have X86_FEATURE_SVM.

	/*

	 * Nobody will change the following 5 values in the VMCB so we can

	 * safely return them on rdmsr. They will always be 0 until LBRV is

	 * implemented.

 check for svm_disable while efer.svme is set */

		/*

		 * For non-nested:

		 * When it's written (to non-zero) for the first time, pass

		 * it through.

		 *

		 * For nested:

		 * The handling of the MSR bitmap for L2 guests is done in

		 * nested_svm_vmrun_msrpm.

		 * We update the L1 MSR bit as well since it will end up

		 * touching the MSR anyway now.

		/*

		 * We only intercept the MSR_IA32_SYSENTER_{EIP|ESP} msrs

		 * when we spoof an Intel vendor ID (for cross vendor migration).

		 * In this case we use this intercept to track the high

		 * 32 bit part of these msrs to support Intel's

		 * implementation of SYSENTER/SYSEXIT.

		/*

		 * TSC_AUX is usually changed only during boot and never read

		 * directly.  Intercept TSC_AUX instead of exposing it to the

		 * guest via direct_access_msrs, and switch it via user return.

		/*

		 * Old kernels did not validate the value written to

		 * MSR_VM_HSAVE_PA.  Allow KVM_SET_MSR to set an invalid

		 * value to allow live migrating buggy or malicious guests

		 * originating from those kernels.

 Check the supported bits */

 Don't allow the guest to change a bit, #GP */

	/*

	 * For AVIC, the only reason to end up here is ExtINTs.

	 * In this case AVIC was temporarily disabled for

	 * requesting the IRQ window and we have to re-enable it.

	/*

	 * CPL is not made available for an SEV-ES guest, therefore

	 * vcpu->arch.preempted_in_kernel can never be true.  Just

	 * set in_kernel to false as well.

	/*

	 * For an INVPCID intercept:

	 * EXITINFO1 provides the linear address of the memory operand.

	 * EXITINFO2 provides the contents of the register operand.

 SEV-ES guests must use the CR write traps to track CR registers. */

 available 32/64-bit TSS */

	/*

	 * If the previous vmrun of the vmcb occurred on a different physical

	 * cpu, then mark the vmcb dirty and assign a new asid.  Hardware's

	 * vmcb clean bits are per logical CPU, as are KVM's asid assignments.

 FIXME: handle wraparound of asid_generation */

	/*

	 * SEV-ES guests must always keep the CR intercepts cleared. CR

	 * tracking is done using the CR write traps.

 An NMI must not be injected into L2 if it's supposed to VM-Exit.  */

		/*

		 * SEV-ES guests to not expose RFLAGS. Use the VMCB interrupt mask

		 * bit to determine the state of the IF flag.

 As long as interrupts are being delivered...  */

 ... vmexits aren't blocked by the interrupt shadow  */

	/*

	 * An IRQ must not be injected into L2 if it's supposed to VM-Exit,

	 * e.g. if the IRQ arrived asynchronously after checking nested events.

	/*

	 * In case GIF=0 we can't rely on the CPU to tell us when GIF becomes

	 * 1, because that's a separate STGI/VMRUN intercept.  The next time we

	 * get that intercept, this function will be called again though and

	 * we'll get the vintr intercept. However, if the vGIF feature is

	 * enabled, the STGI interception will not occur. Enable the irq

	 * window under the assumption that the hardware will set the GIF.

		/*

		 * IRQ window is not needed when AVIC is enabled,

		 * unless we have pending ExtINT since it cannot be injected

		 * via AVIC. In such case, we need to temporarily disable AVIC,

		 * and fallback to injecting IRQ via V_IRQ.

 IRET will cause a vm exit */

 STGI will cause a vm exit */

	/*

	 * Something prevents NMI from been injected. Single step over possible

	 * problem (IRET or exception injection or interrupt shadow)

	/*

	 * Flush only the current ASID even if the TLB flush was invoked via

	 * kvm_flush_remote_tlbs().  Although flushing remote TLBs requires all

	 * ASIDs to be flushed, KVM uses a single ASID for L1 and L2, and

	 * unconditionally does a TLB flush on both nested VM-Enter and nested

	 * VM-Exit (via kvm_mmu_reset_context()).

	/*

	 * If we've made progress since setting HF_IRET_MASK, we've

	 * executed an IRET and can allow NMI injection.

		/*

		 * Never re-inject a #VC exception.

		/*

		 * In case of software exceptions, do not reinject the vector,

		 * but re-execute the instruction instead. Rewind RIP first

		 * if we emulated INT3 before.

		/*

		 * Use a single vmcb (vmcb01 because it's always valid) for

		 * context switching guest state via VMLOAD/VMSAVE, that way

		 * the state doesn't need to be copied between vmcb01 and

		 * vmcb02 when switching vmcbs for nested virtualization.

	/*

	 * Disable singlestep if we're injecting an interrupt/exception.

	 * We don't want our modified rflags to be pushed on the stack where

	 * we might not be able to easily reset them if we disabled NMI

	 * singlestep later.

		/*

		 * Event injection happens before external interrupts cause a

		 * vmexit and interrupts are disabled here, so smp_send_reschedule

		 * is enough to force an immediate vmexit.

	/*

	 * Run with all-zero DR6 unless needed, so that we can get the exact cause

	 * of a #DB.

	/*

	 * If this vCPU has touched SPEC_CTRL, restore the guest's value if

	 * it's non-zero. Since vmentry is serialising on affected CPUs, there

	 * is no need to worry about the conditional branch over the wrmsr

	 * being speculatively taken.

	/*

	 * We do not use IBRS in the kernel. If this vCPU has used the

	 * SPEC_CTRL MSR it may have left it on; save the value and

	 * turn it off. This is much more efficient than blindly adding

	 * it to the atomic save/restore list. Especially as the former

	 * (Saving guest MSRs on vmexit) doesn't even exist in KVM.

	 *

	 * For non-nested case:

	 * If the L01 MSR bitmap does not intercept the MSR, then we need to

	 * save it.

	 *

	 * For nested case:

	 * If the L02 MSR bitmap does not intercept the MSR, then we need to

	 * save it.

 Any pending NMI will happen here */

 Track VMRUNs that have made past consistency checking */

 if exit due to PF check for async PF */

	/*

	 * We need to handle MC intercepts here before the vcpu has a chance to

	 * change the physical cpu

 Loading L2's CR3 is handled by enter_svm_guest_mode.  */

 PCID in the guest should be impossible with a 32-bit MMU. */

	/*

	 * Patch in the VMMCALL instruction:

/*

 * The kvm parameter can be NULL (module initialization, or invocation before

 * VM creation). Be sure to check the kvm parameter before using it.

 SEV-ES guests do not support SMM, so report false */

 Update nrips enabled cache */

 For sev guests, the memory encryption bit is not reserved in CR3.  */

		/*

		 * AVIC does not work with an x2APIC mode guest. If the X2APIC feature

		 * is exposed to the guest, disable AVIC.

		/*

		 * Currently, AVIC does not work with nested virtualization.

		 * So, we disable AVIC when cpuid for SVM is set in the L1 guest.

 lmsw can't clear PE - catch this here */

		/*

		 * We get this for NOP only, but pause

		 * is rep not, check this here

 TODO: Advertise NRIPS to guest hypervisor unconditionally */

 [63:9] are reserved. */

 Per APM Vol.2 15.22.2 "Response to SMI" */

 An SMI must not be injected into L2 if it's supposed to VM-Exit.  */

 FED8h - SVM Guest */

 FEE0h - SVM Guest VMCB Physical Address */

	/*

	 * KVM uses VMCB01 to store L1 host state while L2 runs but

	 * VMCB01 is going to be used during SMM and thus the state will

	 * be lost. Temporary save non-VMLOAD/VMSAVE state to the host save

	 * area pointed to by MSR_VM_HSAVE_PA. APM guarantees that the

	 * format of the area is identical to guest save area offsetted

	 * by 0x400 (matches the offset of 'struct vmcb_save_area'

	 * within 'struct vmcb'). Note: HSAVE area may also be used by

	 * L1 hypervisor to save additional host context (e.g. KVM does

	 * that, see svm_prepare_guest_switch()) which must be

	 * preserved.

 Non-zero if SMI arrived while vCPU was in guest mode. */

	/*

	 * Restore L1 host state from L1 HSAVE area as VMCB01 was

	 * used during SMM (see svm_enter_smm())

	/*

	 * Enter the nested guest now

 STGI will cause a vm exit */

 We must be in SMM; RSM will cause a vmexit anyway.  */

	/*

	 * When the guest is an SEV-ES guest, emulation is not possible.

	/*

	 * Detect and workaround Errata 1096 Fam_17h_00_0Fh.

	 *

	 * Errata:

	 * When CPU raise #NPF on guest data access and vCPU CR4.SMAP=1, it is

	 * possible that CPU microcode implementing DecodeAssist will fail

	 * to read bytes of instruction which caused #NPF. In this case,

	 * GuestIntrBytes field of the VMCB on a VMEXIT will incorrectly

	 * return 0 instead of the correct guest instruction bytes.

	 *

	 * This happens because CPU microcode reading instruction bytes

	 * uses a special opcode which attempts to read data using CPL=0

	 * privileges. The microcode reads CS:RIP and if it hits a SMAP

	 * fault, it gives up and returns no instruction bytes.

	 *

	 * Detection:

	 * We reach here in case CPU supports DecodeAssist, raised #NPF and

	 * returned 0 in GuestIntrBytes field of the VMCB.

	 * First, errata can only be triggered in case vCPU CR4.SMAP=1.

	 * Second, if vCPU CR4.SMEP=1, errata could only be triggered

	 * in case vCPU CPL==3 (Because otherwise guest would have triggered

	 * a SMEP fault instead of #NPF).

	 * Otherwise, vCPU CR4.SMEP=0, errata could be triggered by any vCPU CPL.

	 * As most guests enable SMAP if they have also enabled SMEP, use above

	 * logic in order to attempt minimize false-positive of detecting errata

	 * while still preserving all cases semantic correctness.

	 *

	 * Workaround:

	 * To determine what instruction the guest was executing, the hypervisor

	 * will have to decode the instruction at the instruction pointer.

	 *

	 * In non SEV guest, hypervisor will be able to read the guest

	 * memory to decode the instruction pointer when insn_len is zero

	 * so we return true to indicate that decoding is possible.

	 *

	 * But in the SEV guest, the guest memory is encrypted with the

	 * guest specific key and hypervisor will not be able to decode the

	 * instruction pointer so we will not able to workaround it. Lets

	 * print the error and request to kill the guest.

	/*

	 * If RIP is invalid, go ahead with emulation which will cause an

	 * internal error exit.

	/*

	 * TODO: Last condition latch INIT signals on vCPU when

	 * vCPU is in guest-mode and vmcb12 defines intercept on INIT.

	 * To properly emulate the INIT intercept,

	 * svm_check_nested_events() should call nested_svm_vmexit()

	 * if an INIT signal is pending.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * KVM PMU support for AMD

 *

 * Copyright 2015, Red Hat, Inc. and/or its affiliates.

 *

 * Author:

 *   Wei Huang <wei@redhat.com>

 *

 * Implementation is based on pmu_intel.c file

 duplicated from amd_perfmon_event_map, K7 and above should work. */

 return PERF_COUNT_HW_MAX as AMD doesn't have fixed events */

/* check if a PMC is enabled by comparing it against global_ctrl bits. Because

 * AMD CPU doesn't have global_ctrl MSR, all PMCs are enabled (return TRUE).

		/*

		 * The idx is contiguous. The MSRs are not. The counter MSRs

		 * are interleaved with the event select MSRs.

 idx is the ECX register of RDPMC instruction */

 All MSRs refer to exactly one PMC, so msr_idx_to_pmc is enough.  */

 MSR_PERFCTRn */

 MSR_EVNTSELn */

 MSR_PERFCTRn */

 MSR_EVNTSELn */

 not applicable to AMD; but clean them to prevent any fall out */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel-based Virtual Machine driver for Linux

 *

 * AMD SVM support

 *

 * Copyright (C) 2006 Qumranet, Inc.

 * Copyright 2010 Red Hat, Inc. and/or its affiliates.

 *

 * Authors:

 *   Yaniv Kamay  <yaniv@qumranet.com>

 *   Avi Kivity   <avi@qumranet.com>

/*

 * 0xff is broadcast, so the max index allowed for physical APIC ID

 * table is 0xfe.  APIC IDs above 0xff are reserved.

 AVIC GATAG is encoded using VM and VCPU IDs */

/* Note:

 * This hash table is used to map VM_ID to a struct kvm_svm,

 * when handling AMD IOMMU GALOG notification to schedule in

 * a particular vCPU.

/*

 * This is a wrapper of struct amd_iommu_ir_data.

 Used by SVM for per-vcpu ir_list */

 Storing pointer to struct amd_ir_data */

/* Note:

 * This function is called from IOMMU driver to notify

 * SVM to schedule in a particular vCPU of a particular VM.

	/* Note:

	 * At this point, the IOMMU should have already set the pending

	 * bit in the vAPIC backing page. So, we just need to schedule

	 * in the vcpu.

 Allocating physical APIC ID table (4KB) */

 Allocating logical APIC ID table (4KB) */

 id is 1-based, zero is not okay */

 Is it still in use? Only possible if wrapped at least once */

/*

 * Note:

 * AVIC hardware walks the nested page table to check permissions,

 * but does not use the SPA address specified in the leaf page

 * table entry since it uses  address in the AVIC_BACKING_PAGE pointer

 * field of the VMCB. Therefore, we set up the

 * APIC_ACCESS_PAGE_PRIVATE_MEMSLOT (4KB) here.

 Setting AVIC backing page address in the phy APIC ID table */

		/*

		 * AVIC hardware handles the generation of

		 * IPIs when the specified Message Type is Fixed

		 * (also known as fixed delivery mode) and

		 * the Trigger Mode is edge-triggered. The hardware

		 * also supports self and broadcast delivery modes

		 * specified via the Destination Shorthand(DSH)

		 * field of the ICRL. Logical and physical APIC ID

		 * formats are supported. All other IPI types cause

		 * a #VMEXIT, which needs to emulated.

		/*

		 * At this point, we expect that the AVIC HW has already

		 * set the appropriate IRR bits on the valid target

		 * vcpus. So, we just need to kick the appropriate vcpu.

 flat */

 cluster */

 We need to move physical_id_entry to new offset */

	/*

	 * Also update the guest physical APIC ID in the logical

	 * APIC ID table entry if already setup the LDR.

 Handling Trap */

 Handling Fault */

	/*

	 * Here, we go through the per-vcpu ir_list to update all existing

	 * interrupt remapping table entry targeting this vcpu.

		/**

		 * During AVIC temporary deactivation, guest could update

		 * APIC ID, DFR and LDR registers, which would not be trapped

		 * by avic_unaccelerated_access_interception(). In this case,

		 * we need to check and update the AVIC logical APIC ID table

		 * accordingly before re-activating.

	/**

	 * In some cases, the existing irte is updated and re-set,

	 * so we need to check here if it's already been * added

	 * to the ir_list.

	/**

	 * Allocating new amd_iommu_pi_data, which will get

	 * add to the per-vcpu ir_list.

/*

 * Note:

 * The HW cannot support posting multicast/broadcast

 * interrupts to a vCPU. So, we still use legacy interrupt

 * remapping for these kind of interrupts.

 *

 * For lowest-priority interrupts, we only support

 * those with single CPU as the destination, e.g. user

 * configures the interrupts via /proc/irq or uses

 * irqbalance to make the interrupts single-CPU.

/*

 * svm_update_pi_irte - set IRTE for Posted-Interrupts

 *

 * @kvm: kvm

 * @host_irq: host irq of the interrupt

 * @guest_irq: gsi of the interrupt

 * @set: set or unset PI

 * returns 0 on success, < 0 on failure

		/**

		 * Here, we setup with legacy mode in the following cases:

		 * 1. When cannot target interrupt to a specific vcpu.

		 * 2. Unsetting posted interrupt.

		 * 3. APIC virtualization is disabled for the vcpu.

		 * 4. IRQ has incompatible delivery mode (SMI, INIT, etc)

 Try to enable guest_mode in IRTE */

			/**

			 * Here, we successfully setting up vcpu affinity in

			 * IOMMU guest mode. Now, we need to store the posted

			 * interrupt information in a per-vcpu ir_list so that

			 * we can reference to them directly when we update vcpu

			 * scheduling information in IOMMU irte.

 Use legacy mode in IRTE */

			/**

			 * Here, pi is used to:

			 * - Tell IOMMU to use legacy mode for this interrupt.

			 * - Retrieve ga_tag of prior interrupt remapping data.

			/**

			 * Check if the posted interrupt was previously

			 * setup with the guest_mode by checking if the ga_tag

			 * was cached. If so, we need to clean up the per-vcpu

			 * ir_list.

	/*

	 * Here, we go through the per-vcpu ir_list to update all existing

	 * interrupt remapping table entry targeting this vcpu.

 ID = 0xff (broadcast), ID > 0xff (reserved) */

	/*

	 * Since the host physical APIC id is 8 bits,

	 * we can support host APIC ID upto 255.

/*

 * This function is called during VCPU halt/unhalt.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * KVM L1 hypervisor optimizations on Hyper-V for SVM.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * x86 decoder sanity test - based on test_get_insn.c

 *

 * Copyright (C) IBM Corporation, 2009

 * Copyright (C) Hitachi, Ltd., 2011

/*

 * Test of instruction analysis against tampering.

 * Feed random binary to instruction decoder and ensure not to

 * access out-of-instruction-buffer.

 Program name */

 Verbosity */

 x86-64 bit mode flag */

 Random seed */

 Start of iteration number */

 End of iteration number */

 Input file name */

 Input a decoded instruction sequence directly */

 Give a seed and iteration number */

 Read given instruction sequence from the input file */

 Fills buffer with random binary up to MAX_INSN_SIZE */

 Check errors */

 Initialize random seed */

 No seed is given */

 Prepare stop bytes with NOPs */

 Skip to given iteration number */

 Decode an instruction */

 Access out-of-range memory */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) IBM Corporation, 2009

/*

 * Test of instruction analysis in general and insn_get_length() in

 * particular.  See if insn_get_length() and the disassembler agree

 * on the length of each instruction in an elf disassembly.

 *

 * Usage: objdump -d a.out | awk -f objdump_reformat.awk | ./insn_decoder_test

 Symbol line */

 Characters beyond tab2 aren't examined */

 Decode an instruction */

 SPDX-License-Identifier: GPL-2.0

 This is included from relocs_32/64.c */

/*

 * Following symbols have been audited. There values are constant and do

 * not change if bzImage is loaded at a different physical address than

 * the address for which it has been compiled. Don't warn user about

 * absolute relocations present w.r.t these symbols.

/*

 * These symbols are known to be relative, even if the linker marks them

 * as absolute (typically defined outside any section in the linker script.)

/*

 * These symbols are known to be relative, even if the linker marks them

 * as absolute (typically defined outside any section in the linker script.)

/*

 * These are 16-bit segment symbols when compiling 16-bit code.

/*

 * These are offsets belonging to segments, as opposed to linear addresses,

 * when compiling 16-bit code.

 calculate offset of sym from head of table. */

 Convert the fields to native endian */

			/* Absolute symbols are not relocated if bzImage is

			 * loaded at a non-compiled address. Display a warning

			 * to user at compile time about the absolute

			 * relocations present.

			 *

			 * User need to audit the code to make sure

			 * some symbols which should have been section

			 * relative have not become absolute because of some

			 * linker optimization or wrong programming usage.

			 *

			 * Before warning check if this absolute symbol

			 * relocation is harmless.

 Walk through the relocations */

/*

 * The .data..percpu section is a special case for x86_64 SMP kernels.

 * It is used to initialize the actual per_cpu areas and to provide

 * definitions for the per_cpu variables that correspond to their offsets

 * within the percpu area. Since the values of all of the symbols need

 * to be offsets from the start of the per_cpu area the virtual address

 * (sh_addr) of .data..percpu is 0 in SMP kernels.

 *

 * This means that:

 *

 *	Relocations that reference symbols in the per_cpu area do not

 *	need further relocation (since the value is an offset relative

 *	to the start of the per_cpu area that does not change).

 *

 *	Relocations that apply to the per_cpu area need to have their

 *	offset adjusted by by the value of __per_cpu_load to make them

 *	point to the correct place in the loaded image (because the

 *	virtual address of .data..percpu is 0).

 *

 * For non SMP kernels .data..percpu is linked as part of the normal

 * kernel data and does not require special treatment.

 *

 non SMP kernel */

/*

 * Check to see if a symbol lies in the .data..percpu section.

 *

 * The linker incorrectly associates some symbols with the

 * .data..percpu section so we also need to check the symbol

 * name to make sure that we classify the symbol correctly.

 *

 * The GNU linker incorrectly associates:

 *	__init_begin

 *	__per_cpu_load

 *

 * The "gold" linker incorrectly associates:

 *	init_per_cpu__fixed_percpu_data

 *	init_per_cpu__gdt_page

	/*

	 * Adjust the offset if this reloc applies to the percpu section.

 NONE can be ignored. */

		/*

		 * PC relative relocations don't need to be adjusted unless

		 * referencing a percpu symbol.

		 *

		 * NB: R_X86_64_PLT32 can be treated as R_X86_64_PC32.

		/*

		 * Only used by jump labels

		/*

		 * References to the percpu area don't need to be adjusted.

			/*

			 * Whitelisted absolute symbols do not require

			 * relocation.

		/*

		 * Relocation offsets for 64 bit kernels are output

		 * as 32 bits and sign extended back to 64 bits when

		 * the relocations are processed.

		 * Make sure that the offset will fit.

		/*

		 * NONE can be ignored and PC relative relocations don't need

		 * to be adjusted. Because sym must be defined, R_386_PLT32 can

		 * be treated the same way as R_386_PC32.

			/*

			 * Whitelisted absolute symbols do not require

			 * relocation.

		/*

		 * NONE can be ignored and PC relative relocations don't need

		 * to be adjusted. Because sym must be defined, R_386_PLT32 can

		 * be treated the same way as R_386_PC32.

			/*

			 * Whitelisted absolute symbols do not require

			 * relocation.

			/*

			 * Whitelisted absolute symbols do not require

			 * relocation.

 Collect up the relocations */

 Order the relocations for more efficient processing */

 Print the relocations */

		/* Print the relocations in a form suitable that

		 * gas will like.

 Print a stop */

 Now print each relocation */

 Print a stop */

 Now print each inverse 32-bit relocation */

 Print a stop */

 Now print each relocation */

/*

 * As an aid to debugging problems with different linkers

 * print summary information about the relocs.

 * Since different linkers tend to emit the sections in

 * different orders we use the section names in the output.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright(c) 2017 Intel Corporation. All rights reserved.

 *

 * This code is based in part on work published here:

 *

 *	https://github.com/IAIK/KAISER

 *

 * The original work was written by and and signed off by for the Linux

 * kernel by:

 *

 *   Signed-off-by: Richard Fellner <richard.fellner@student.tugraz.at>

 *   Signed-off-by: Moritz Lipp <moritz.lipp@iaik.tugraz.at>

 *   Signed-off-by: Daniel Gruss <daniel.gruss@iaik.tugraz.at>

 *   Signed-off-by: Michael Schwarz <michael.schwarz@iaik.tugraz.at>

 *

 * Major changes to the original code by: Dave Hansen <dave.hansen@intel.com>

 * Mostly rewritten by Thomas Gleixner <tglx@linutronix.de> and

 *		       Andy Lutomirsky <luto@amacapital.net>

 Backporting helper */

/*

 * Define the page-table levels we clone for user-space on 32

 * and 64 bit.

 Assume mode is auto unless overridden. */

	/*

	 * Changes to the high (kernel) portion of the kernelmode page

	 * tables are not automatically propagated to the usermode tables.

	 *

	 * Users should keep in mind that, unlike the kernelmode tables,

	 * there is no vmalloc_fault equivalent for the usermode tables.

	 * Top-level entries added to init_mm's usermode pgd after boot

	 * will not be automatically propagated to other mms.

	/*

	 * The user page tables get the full PGD, accessible from

	 * userspace:

	/*

	 * If this is normal user memory, make it NX in the kernel

	 * pagetables so that, if we somehow screw up and return to

	 * usermode with the kernel CR3 loaded, we'll get a page fault

	 * instead of allowing user code to execute with the wrong CR3.

	 *

	 * As exceptions, we don't set NX if:

	 *  - _PAGE_USER is not set.  This could be an executable

	 *     EFI runtime mapping or something similar, and the kernel

	 *     may execute from it

	 *  - we don't have NX support

	 *  - we're clearing the PGD (i.e. the new pgd is not present).

 return the copy of the PGD we want the kernel to use: */

/*

 * Walk the user copy of the page tables (optionally) trying to allocate

 * page table pages on the way down.

 *

 * Returns a pointer to a P4D on success, or NULL on failure.

/*

 * Walk the user copy of the page tables (optionally) trying to allocate

 * page table pages on the way down.

 *

 * Returns a pointer to a PMD on success, or NULL on failure.

 The user page tables do not use large mappings: */

/*

 * Walk the shadow copy of the page tables (optionally) trying to allocate

 * page table pages on the way down.  Does not support large pages.

 *

 * Note: this is only used when mapping *new* kernel data into the

 * user/shadow page tables.  It is never used for userspace data.

 *

 * Returns a pointer to a PTE on success, or NULL on failure.

 We can't do anything sensible if we hit a large mapping. */

	/*

	 * Clone the populated PMDs which cover start to end. These PMD areas

	 * can have holes.

 Overflow check */

			/*

			 * Only clone present PMDs.  This ensures only setting

			 * _PAGE_GLOBAL on present PMDs.  This should only be

			 * called on well-known addresses anyway, so a non-

			 * present PMD would be a surprise.

			/*

			 * Setting 'target_pmd' below creates a mapping in both

			 * the user and kernel page tables.  It is effectively

			 * global, so set it as global in both copies.  Note:

			 * the X86_FEATURE_PGE check is not _required_ because

			 * the CPU ignores _PAGE_GLOBAL when PGE is not

			 * supported.  The check keeps consistency with

			 * code that only set this bit when supported.

			/*

			 * Copy the PMD.  That is, the kernelmode and usermode

			 * tables will share the last-level page tables of this

			 * address range

 Walk the page-table down to the pte level */

 Only clone present PTEs */

 Allocate PTE in the user page-table */

 Set GLOBAL bit in both PTEs */

 Clone the PTE */

/*

 * Clone a single p4d (i.e. a top-level entry on 4-level systems and a

 * next-level entry on 5-level systems.

/*

 * Clone the CPU_ENTRY_AREA and associated data into the user space visible

 * page table.

		/*

		 * The SYSCALL64 entry code needs one word of scratch space

		 * in which to spill a register.  It lives in the sp2 slot

		 * of the CPU's TSS.

		 *

		 * This is done for all possible CPUs during boot to ensure

		 * that it's propagated to all mms.

 CONFIG_X86_64 */

/*

 * On 32 bit PAE systems with 1GB of Kernel address space there is only

 * one pgd/p4d for the whole kernel. Cloning that would map the whole

 * address space into the user page-tables, making PTI useless. So clone

 * the page-table on the PMD level to prevent that.

 CONFIG_X86_64 */

/*

 * Clone the ESPFIX P4D into the user space visible page table

/*

 * Clone the populated PMDs of the entry text and force it RO.

/*

 * Global pages and PCIDs are both ways to make kernel TLB entries

 * live longer, reduce TLB misses and improve kernel performance.

 * But, leaving all kernel text Global makes it potentially accessible

 * to Meltdown-style attacks which make it trivial to find gadgets or

 * defeat KASLR.

 *

 * Only use global pages when it is really worth it.

	/*

	 * Systems with PCIDs get little benefit from global

	 * kernel text and are not worth the downsides.

	/*

	 * Only do global kernel image for pti=auto.  Do the most

	 * secure thing (not global) if pti=on specified.

	/*

	 * K8 may not tolerate the cleared _PAGE_RW on the userspace

	 * global kernel image pages.  Do the safe thing (disable

	 * global kernel image).  This is unlikely to ever be

	 * noticed because PTI is disabled by default on AMD CPUs.

	/*

	 * RANDSTRUCT derives its hardening benefits from the

	 * attacker's lack of knowledge about the layout of kernel

	 * data structures.  Keep the kernel image non-global in

	 * cases where RANDSTRUCT is in use to help keep the layout a

	 * secret.

/*

 * For some configurations, map all of kernel text into the user page

 * tables.  This reduces TLB misses, especially on non-PCID systems.

	/*

	 * rodata is part of the kernel image and is normally

	 * readable on the filesystem or on the web.  But, do not

	 * clone the areas past rodata, they might contain secrets.

	/*

	 * Note that this will undo _some_ of the work that

	 * pti_set_kernel_image_nonglobal() did to clear the

	 * global bit.

	/*

	 * pti_clone_pgtable() will set the global bit in any PMDs

	 * that it clones, but we also need to get any PTEs in

	 * the last level for areas that are not huge-page-aligned.

 Set the global bit for normal non-__init kernel text: */

	/*

	 * The identity map is created with PMDs, regardless of the

	 * actual length of the kernel.  We need to clear

	 * _PAGE_GLOBAL up to a PMD boundary, not just to the end

	 * of the image.

	/*

	 * This clears _PAGE_GLOBAL from the entire kernel image.

	 * pti_clone_kernel_text() map put _PAGE_GLOBAL back for

	 * areas that are mapped to userspace.

/*

 * Initialize kernel page table isolation

	/*

	 * We check for X86_FEATURE_PCID here. But the init-code will

	 * clear the feature flag on 32 bit because the feature is not

	 * supported on 32 bit anyway. To print the warning we need to

	 * check with cpuid directly again.

 Use printk to work around pr_fmt() */

 Undo all global bits from the init pagetables in head_64.S: */

 Replace some of the global bits just for shared entry text: */

/*

 * Finalize the kernel mappings in the userspace page-table. Some of the

 * mappings for the kernel image might have changed since pti_init()

 * cloned them. This is because parts of the kernel image have been

 * mapped RO and/or NX.  These changes need to be cloned again to the

 * userspace page-table.

	/*

	 * We need to clone everything (again) that maps parts of the

	 * kernel image.

 SPDX-License-Identifier: GPL-2.0-only

 for totalram_pages */

	/*

	 * Explicitly reset zone->managed_pages because set_highmem_pages_init()

	 * is invoked before memblock_free_all()

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 1995  Linus Torvalds

 *

 *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999

/*

 * Creates a middle page table and puts a pointer to it in the

 * given global directory entry. This only returns the gd entry

 * in non-PAE compilation mode, since the middle layer is folded.

/*

 * Create a page table and place a pointer to it in a middle page

 * directory entry:

	/*

	 * Something (early fixmap) may already have put a pte

	 * page here, which causes the page table allocation

	 * to become nonlinear. Attempt to fix it, and if it

	 * is still nonlinear then we have to bug.

/*

 * This function initializes a certain range of kernel virtual memory

 * with new bootmem page tables, everywhere page tables are missing in

 * the given range.

 *

 * NOTE: The pagetables are allocated contiguous on the physical space

 * so we can cache the place of the first one and move around without

 * checking the pgd every time.

/*

 * This maps the physical memory to kernel virtual address space, a total

 * of max_low_pfn pages, by creating page tables starting from address

 * PAGE_OFFSET:

	/*

	 * First iteration will setup identity mapping using large/small pages

	 * based on use_pse, with other attributes same as set by

	 * the early code in head_32.S

	 *

	 * Second iteration will setup the appropriate attributes (NX, GLOBAL..)

	 * as desired for the kernel identity mapping.

	 *

	 * This two pass mechanism conforms to the TLB app note which says:

	 *

	 *     "Software should not write to a paging-structure entry in a way

	 *      that would change, for any linear address, both the page size

	 *      and either the page frame or attributes."

			/*

			 * Map with big pages if possible, otherwise

			 * create normal page tables:

				/*

				 * first pass will use the same initial

				 * identity mapping attribute + _PAGE_PSE.

				/*

				 * first pass will use the same initial

				 * identity mapping attribute.

		/*

		 * update direct mapping page count only in the first

		 * iteration.

		/*

		 * local global flush tlb, which will flush the previous

		 * mappings present in both small and large page TLB's.

		/*

		 * Second iteration will set the actual desired PTE attributes.

 CONFIG_HIGHMEM */

	/*

	 * sync back low identity map too.  It is used for example

	 * in the 32-bit EFI stub.

	/*

	 * Remove any mappings which extend past the end of physical

	 * memory from the boot time page table.

	 * In virtual address space, we should have at least two pages

	 * from VMALLOC_END to pkmap or fixmap according to VMALLOC_END

	 * definition. And max_low_pfn is set to VMALLOC_END physical

	 * address. If initial memory mapping is doing right job, we

	 * should have pte used near max_low_pfn or one pmd is not present.

 should not be large page here */

/*

 * Build a proper pagetable for the kernel mappings.  Up until this

 * point, we've been running on some set of pagetables constructed by

 * the boot process.

 *

 * If we're booting on native hardware, this will be a pagetable

 * constructed in arch/x86/kernel/head_32.S.  The root of the

 * pagetable will be swapper_pg_dir.

 *

 * If we're booting paravirtualized under a hypervisor, then there are

 * more options: we may already be running PAE, and the pagetable may

 * or may not be based in swapper_pg_dir.  In any case,

 * paravirt_pagetable_init() will set up swapper_pg_dir

 * appropriately for the rest of the initialization to work.

 *

 * In general, pagetable_init() assumes that the pagetable may already

 * be partially populated, and so it avoids stomping on any existing

 * mappings.

	/*

	 * Fixed mappings, only the page table structure has to be

	 * created - mappings will be set by set_fixmap():

 Bits supported by the hardware: */

 Bits allowed in normal kernel mappings: */

 Used in PAGE_KERNEL_* macros which are reasonably used out-of-tree: */

 user-defined highmem size */

/*

 * highmem=size forces highmem to be exactly 'size' bytes.

 * This works even on boxes that have no highmem otherwise.

 * This also works to reduce highmem size on bigger boxes.

/*

 * All of RAM fits into lowmem - but if user wants highmem

 * artificially via the highmem=x boot parameter then create

 * it:

 max_low_pfn is 0, we already have early_res support */

/*

 * We have more RAM than fits into lowmem - we try to put it into

 * highmem, also taking the highmem=x boot parameter into account:

 Maximum memory usable is what is directly addressable */

 !CONFIG_HIGHMEM */

 !CONFIG_HIGHMEM64G */

 !CONFIG_HIGHMEM */

/*

 * Determine low and high memory ranges:

 it could update max_pfn */

 !CONFIG_NUMA */

/*

 * paging_init() sets up the page tables - note that the first 8MB are

 * already mapped by head.S.

 *

 * This routines also unmaps the page at virtual kernel address 0, so

 * that we can trap those pesky NULL-reference errors in the kernel.

	/*

	 * NOTE: at this point the bootmem allocator is fully available.

/*

 * Test if the WP bit works in supervisor mode. It isn't supported on 386's

 * and also on some strange 486's. All 586+'s are OK. This used to involve

 * black magic jumps to work around some nasty CPU bugs, but fortunately the

 * switch to using exceptions got rid of all that.

	/*

	 * With CONFIG_DEBUG_PAGEALLOC initialization of highmem pages has to

	 * be done before memblock_free_all(). Memblock use free low memory for

	 * temporary data (see find_range_array()) and for this purpose can use

	 * pages that was already passed to the buddy allocator, hence marked as

	 * not accessible in the page tables when compiled with

	 * CONFIG_DEBUG_PAGEALLOC. Otherwise order of initialization is not

	 * important here.

 this will put all low memory onto the freelists */

	/*

	 * Check boundaries twice: Some fundamental inconsistencies can

	 * be detected at build time already.

	/*

	 * When this called, init has already been executed and released,

	 * so everything past _etext should be NX.

	/*

	 * This comes from is_x86_32_kernel_text upper limit. Also HPAGE where used:

 SPDX-License-Identifier: GPL-2.0-only

/*

 *	TLB flushing, formerly SMP-only

 *		c/o Linus Torvalds.

 *

 *	These mean you can really definitely utterly forget about

 *	writing to user space from interrupts. (Its not allowed anyway).

 *

 *	Optimizations Manfred Spraul <manfred@colorfullife.com>

 *

 *	More scalable flush, from Andi Kleen

 *

 *	Implement flush IPI by CALL_FUNCTION_VECTOR, Alex Shi

/*

 * Bits to mangle the TIF_SPEC_* state into the mm pointer which is

 * stored in cpu_tlb_state.last_user_mm_spec.

 Bits to set when tlbstate and flush is (re)initialized */

/*

 * The x86 feature is called PCID (Process Context IDentifier). It is similar

 * to what is traditionally called ASID on the RISC processors.

 *

 * We don't use the traditional ASID implementation, where each process/mm gets

 * its own ASID and flush/restart when we run out of ASID space.

 *

 * Instead we have a small per-cpu array of ASIDs and cache the last few mm's

 * that came by on this CPU, allowing cheaper switch_mm between processes on

 * this CPU.

 *

 * We end up with different spaces for different things. To avoid confusion we

 * use different names for each of them:

 *

 * ASID  - [0, TLB_NR_DYN_ASIDS-1]

 *         the canonical identifier for an mm

 *

 * kPCID - [1, TLB_NR_DYN_ASIDS]

 *         the value we write into the PCID part of CR3; corresponds to the

 *         ASID+1, because PCID 0 is special.

 *

 * uPCID - [2048 + 1, 2048 + TLB_NR_DYN_ASIDS]

 *         for KPTI each mm has two address spaces and thus needs two

 *         PCID values, but we can still do with a single ASID denomination

 *         for each mm. Corresponds to kPCID + 2048.

 *

 There are 12 bits of space for ASIDS in CR3 */

/*

 * When enabled, PAGE_TABLE_ISOLATION consumes a single bit for

 * user/kernel switches

/*

 * ASIDs are zero-based: 0->MAX_AVAIL_ASID are valid.  -1 below to account

 * for them being zero-based.  Another -1 is because PCID 0 is reserved for

 * use by non-PCID-aware users.

/*

 * Given @asid, compute kPCID

	/*

	 * Make sure that the dynamic ASID space does not conflict with the

	 * bit we are using to switch between user and kernel ASIDs.

	/*

	 * The ASID being passed in here should have respected the

	 * MAX_ASID_AVAILABLE and thus never have the switch bit set.

	/*

	 * The dynamically-assigned ASIDs that get passed in are small

	 * (<TLB_NR_DYN_ASIDS).  They never have the high switch bit set,

	 * so do not bother to clear it.

	 *

	 * If PCID is on, ASID-aware code paths put the ASID+1 into the

	 * PCID bits.  This serves two purposes.  It prevents a nasty

	 * situation in which PCID-unaware code saves CR3, loads some other

	 * value (with PCID == 0), and then restores CR3, thus corrupting

	 * the TLB for ASID 0 if the saved ASID was nonzero.  It also means

	 * that any bugs involving loading a PCID-enabled CR3 with

	 * CR4.PCIDE off will trigger deterministically.

/*

 * Given @asid, compute uPCID

	/*

	 * Use boot_cpu_has() instead of this_cpu_has() as this function

	 * might be called during early boot. This should work even after

	 * boot because all CPU's the have same capabilities:

/*

 * We get here when we do something requiring a TLB invalidation

 * but could not go invalidate all of the contexts.  We do the

 * necessary invalidation by clearing out the 'ctx_id' which

 * forces a TLB flush when the context is loaded.

	/*

	 * This is only expected to be set if we have disabled

	 * kernel _PAGE_GLOBAL pages.

 Do not need to flush the current asid */

		/*

		 * Make sure the next time we go to switch to

		 * this asid, we do a flush:

	/*

	 * We don't currently own an ASID slot on this CPU.

	 * Allocate a slot.

/*

 * Given an ASID, flush the corresponding user ASID.  We can delay this

 * until the next time we switch to it.

 *

 * See SWITCH_TO_USER_CR3.

 There is no user ASID if address space separation is off */

	/*

	 * We only have a single ASID if PCID is off and the CR3

	 * write will have flushed it.

	/*

	 * Caution: many callers of this function expect

	 * that load_cr3() is serializing and orders TLB

	 * fills with respect to the mm_cpumask writes.

	/*

	 * It's plausible that we're in lazy TLB mode while our mm is init_mm.

	 * If so, our callers still expect us to flush the TLB, but there

	 * aren't any user TLB entries in init_mm to worry about.

	 *

	 * This needs to happen before any other sanity checks due to

	 * intel_idle's shenanigans.

 Warn if we're not lazy. */

/*

 * Invoked from return to user/guest by a task that opted-in to L1D

 * flushing but ended up running on an SMT enabled core due to wrong

 * affinity settings or CPU hotplug. This is part of the paranoid L1D flush

 * contract which this task requested.

 Flush L1D if the outgoing task requests it */

 Check whether the incoming task opted in for L1D flush */

	/*

	 * Validate that it is not running on an SMT sibling as this would

	 * make the excercise pointless because the siblings share L1D. If

	 * it runs on a SMT sibling, notify it with SIGBUS on return to

	 * user/guest

	/*

	 * Ensure that the bit shift above works as expected and the two flags

	 * end up in bit 0 and 1.

	/*

	 * Avoid user/user BTB poisoning by flushing the branch predictor

	 * when switching between processes. This stops one process from

	 * doing Spectre-v2 attacks on another.

	 *

	 * Both, the conditional and the always IBPB mode use the mm

	 * pointer to avoid the IBPB when switching between tasks of the

	 * same process. Using the mm pointer instead of mm->context.ctx_id

	 * opens a hypothetical hole vs. mm_struct reuse, which is more or

	 * less impossible to control by an attacker. Aside of that it

	 * would only affect the first schedule so the theoretically

	 * exposed data is not really interesting.

		/*

		 * This is a bit more complex than the always mode because

		 * it has to handle two cases:

		 *

		 * 1) Switch from a user space task (potential attacker)

		 *    which has TIF_SPEC_IB set to a user space task

		 *    (potential victim) which has TIF_SPEC_IB not set.

		 *

		 * 2) Switch from a user space task (potential attacker)

		 *    which has TIF_SPEC_IB not set to a user space task

		 *    (potential victim) which has TIF_SPEC_IB set.

		 *

		 * This could be done by unconditionally issuing IBPB when

		 * a task which has TIF_SPEC_IB set is either scheduled in

		 * or out. Though that results in two flushes when:

		 *

		 * - the same user space task is scheduled out and later

		 *   scheduled in again and only a kernel thread ran in

		 *   between.

		 *

		 * - a user space task belonging to the same process is

		 *   scheduled in after a kernel thread ran in between

		 *

		 * - a user space task belonging to the same process is

		 *   scheduled in immediately.

		 *

		 * Optimize this with reasonably small overhead for the

		 * above cases. Mangle the TIF_SPEC_IB bit into the mm

		 * pointer of the incoming task which is stored in

		 * cpu_tlbstate.last_user_mm_spec for comparison.

		 *

		 * Issue IBPB only if the mm's are different and one or

		 * both have the IBPB bit set.

		/*

		 * Only flush when switching to a user space task with a

		 * different context than the user space task which ran

		 * last on this CPU.

		/*

		 * Flush L1D when the outgoing task requested it and/or

		 * check whether the incoming task requested L1D flushing

		 * and ended up on an SMT sibling.

		/*

		 * Clear the existing dirty counters to

		 * prevent the leak for an RDPMC task.

	/*

	 * NB: The scheduler will call us with prev == next when switching

	 * from lazy TLB mode to normal mode if active_mm isn't changing.

	 * When this happens, we don't assume that CR3 (and hence

	 * cpu_tlbstate.loaded_mm) matches next.

	 *

	 * NB: leave_mm() calls us with prev == NULL and tsk == NULL.

 We don't want flush_tlb_func() to run concurrently with us. */

	/*

	 * Verify that CR3 is what we think it is.  This will catch

	 * hypothetical buggy code that directly switches to swapper_pg_dir

	 * without going through leave_mm() / switch_mm_irqs_off() or that

	 * does something like write_cr3(read_cr3_pa()).

	 *

	 * Only do this check if CONFIG_DEBUG_VM=y because __read_cr3()

	 * isn't free.

		/*

		 * If we were to BUG here, we'd be very likely to kill

		 * the system so hard that we don't see the call trace.

		 * Try to recover instead by ignoring the error and doing

		 * a global flush to minimize the chance of corruption.

		 *

		 * (This is far from being a fully correct recovery.

		 *  Architecturally, the CPU could prefetch something

		 *  back into an incorrect ASID slot and leave it there

		 *  to cause trouble down the road.  It's better than

		 *  nothing, though.)

	/*

	 * The membarrier system call requires a full memory barrier and

	 * core serialization before returning to user-space, after

	 * storing to rq->curr, when changing mm.  This is because

	 * membarrier() sends IPIs to all CPUs that are in the target mm

	 * to make them issue memory barriers.  However, if another CPU

	 * switches to/from the target mm concurrently with

	 * membarrier(), it can cause that CPU not to receive an IPI

	 * when it really should issue a memory barrier.  Writing to CR3

	 * provides that full memory barrier and core serializing

	 * instruction.

		/*

		 * Even in lazy TLB mode, the CPU should stay set in the

		 * mm_cpumask. The TLB shootdown code can figure out from

		 * cpu_tlbstate_shared.is_lazy whether or not to send an IPI.

		/*

		 * If the CPU is not in lazy TLB mode, we are just switching

		 * from one thread in a process to another thread in the same

		 * process. No TLB flush required.

		/*

		 * Read the tlb_gen to check whether a flush is needed.

		 * If the TLB is up to date, just use it.

		 * The barrier synchronizes with the tlb_gen increment in

		 * the TLB shootdown code.

		/*

		 * TLB contents went out of date while we were in lazy

		 * mode. Fall through to the TLB switching code below.

		/*

		 * Apply process to process speculation vulnerability

		 * mitigations if applicable.

		/*

		 * Stop remote flushes for the previous mm.

		 * Skip kernel threads; we never send init_mm TLB flushing IPIs,

		 * but the bitmap manipulation can cause cache line contention.

		/*

		 * Start remote flushes and then read tlb_gen.

 Let nmi_uaccess_okay() know that we're changing CR3. */

 The new ASID is already up to date. */

 Make sure we write CR3 before loaded_mm. */

/*

 * Please ignore the name of this function.  It should be called

 * switch_to_kernel_thread().

 *

 * enter_lazy_tlb() is a hint from the scheduler that we are entering a

 * kernel thread or other context without an mm.  Acceptable implementations

 * include doing nothing whatsoever, switching to init_mm, or various clever

 * lazy tricks to try to minimize TLB flushes.

 *

 * The scheduler reserves the right to call enter_lazy_tlb() several times

 * in a row.  It will notify us that we're going back to a real mm by

 * calling switch_mm_irqs_off().

/*

 * Call this when reinitializing a CPU.  It fixes the following potential

 * problems:

 *

 * - The ASID changed from what cpu_tlbstate thinks it is (most likely

 *   because the CPU was taken down and came back up with CR3's PCID

 *   bits clear.  CPU hotplug can do this.

 *

 * - The TLB contains junk in slots corresponding to inactive ASIDs.

 *

 * - The CPU went so far out to lunch that it may have missed a TLB

 *   flush.

 Assert that CR3 already references the right mm. */

	/*

	 * Assert that CR4.PCIDE is set if needed.  (CR4.PCIDE initialization

	 * doesn't work like other CR4 bits because it can only be set from

	 * long mode.)

 Force ASID 0 and force a TLB flush. */

 Reinitialize tlbstate. */

/*

 * flush_tlb_func()'s memory ordering requirement is that any

 * TLB fills that happen after we flush the TLB are ordered after we

 * read active_mm's tlb_gen.  We don't need any explicit barriers

 * because all x86 flush operations are serializing and the

 * atomic64_read operation won't be reordered by the compiler.

	/*

	 * We have three different tlb_gen values in here.  They are:

	 *

	 * - mm_tlb_gen:     the latest generation.

	 * - local_tlb_gen:  the generation that this CPU has already caught

	 *                   up to.

	 * - f->new_tlb_gen: the generation that the requester of the flush

	 *                   wants us to catch up to.

 This code cannot presently handle being reentered. */

 Can only happen on remote CPUs */

		/*

		 * We're in lazy mode.  We need to at least flush our

		 * paging-structure cache to avoid speculatively reading

		 * garbage into our TLB.  Since switching to init_mm is barely

		 * slower than a minimal flush, just switch to init_mm.

		 *

		 * This should be rare, with native_flush_tlb_multi() skipping

		 * IPIs to lazy TLB mode CPUs.

		/*

		 * There's nothing to do: we're already up to date.  This can

		 * happen if two concurrent flushes happen -- the first flush to

		 * be handled can catch us all the way up, leaving no work for

		 * the second flush.

	/*

	 * If we get to this point, we know that our TLB is out of date.

	 * This does not strictly imply that we need to flush (it's

	 * possible that f->new_tlb_gen <= local_tlb_gen), but we're

	 * going to need to flush in the very near future, so we might

	 * as well get it over with.

	 *

	 * The only question is whether to do a full or partial flush.

	 *

	 * We do a partial flush if requested and two extra conditions

	 * are met:

	 *

	 * 1. f->new_tlb_gen == local_tlb_gen + 1.  We have an invariant that

	 *    we've always done all needed flushes to catch up to

	 *    local_tlb_gen.  If, for example, local_tlb_gen == 2 and

	 *    f->new_tlb_gen == 3, then we know that the flush needed to bring

	 *    us up to date for tlb_gen 3 is the partial flush we're

	 *    processing.

	 *

	 *    As an example of why this check is needed, suppose that there

	 *    are two concurrent flushes.  The first is a full flush that

	 *    changes context.tlb_gen from 1 to 2.  The second is a partial

	 *    flush that changes context.tlb_gen from 2 to 3.  If they get

	 *    processed on this CPU in reverse order, we'll see

	 *     local_tlb_gen == 1, mm_tlb_gen == 3, and end != TLB_FLUSH_ALL.

	 *    If we were to use __flush_tlb_one_user() and set local_tlb_gen to

	 *    3, we'd be break the invariant: we'd update local_tlb_gen above

	 *    1 without the full flush that's needed for tlb_gen 2.

	 *

	 * 2. f->new_tlb_gen == mm_tlb_gen.  This is purely an optimization.

	 *    Partial TLB flushes are not all that much cheaper than full TLB

	 *    flushes, so it seems unlikely that it would be a performance win

	 *    to do a partial flush if that won't bring our TLB fully up to

	 *    date.  By doing a full flush instead, we can increase

	 *    local_tlb_gen all the way to mm_tlb_gen and we can probably

	 *    avoid another flush in the very near future.

 Partial flush */

 Full flush. */

 Both paths above update our state to mm_tlb_gen. */

 Tracing is done in a unified manner to reduce the code size */

	/*

	 * Do accounting and tracing. Note that there are (and have always been)

	 * cases in which a remote TLB flush will be traced, but eventually

	 * would not happen.

	/*

	 * If no page tables were freed, we can skip sending IPIs to

	 * CPUs in lazy TLB mode. They will flush the CPU themselves

	 * at the next context switch.

	 *

	 * However, if page tables are getting freed, we need to send the

	 * IPI everywhere, to prevent CPUs in lazy TLB mode from tripping

	 * up on the new contents of what used to be page tables, while

	 * doing a speculative memory access.

		/*

		 * Although we could have used on_each_cpu_cond_mask(),

		 * open-coding it has performance advantages, as it eliminates

		 * the need for indirect calls or retpolines. In addition, it

		 * allows to use a designated cpumask for evaluating the

		 * condition, instead of allocating one.

		 *

		 * This code works under the assumption that there are no nested

		 * TLB flushes, an assumption that is already made in

		 * flush_tlb_mm_range().

		 *

		 * cond_cpumask is logically a stack-local variable, but it is

		 * more efficient to have it off the stack and not to allocate

		 * it on demand. Preemption is disabled and this code is

		 * non-reentrant.

/*

 * See Documentation/x86/tlb.rst for details.  We choose 33

 * because it is large enough to cover the vast majority (at

 * least 95%) of allocations, and is small enough that we are

 * confident it will not cause too much overhead.  Each single

 * flush is about 100 ns, so this caps the maximum overhead at

 * _about_ 3,000 ns.

 *

 * This is in units of pages.

	/*

	 * Ensure that the following code is non-reentrant and flush_tlb_info

	 * is not overwritten. This means no TLB flushing is initiated by

	 * interrupt handlers and machine-check exception handlers.

 Complete reentrancy prevention checks */

 Should we flush just the requested range? */

 This is also a barrier that synchronizes with switch_mm(). */

	/*

	 * flush_tlb_multi() is not optimized for the common case in which only

	 * a local TLB flush is needed. Optimize this use-case by calling

	 * flush_tlb_func_local() directly in this case.

 flush range by one by one 'invlpg' */

 Balance as user space task's flush, a bit conservative */

/*

 * This can be used from process context to figure out what the value of

 * CR3 is without needing to do a (slow) __read_cr3().

 *

 * It's intended to be used for code like KVM that sneakily changes CR3

 * and needs to restore it.  It needs to be used very carefully.

 For now, be very restrictive about when this can be called. */

/*

 * Flush one page in the kernel mapping

	/*

	 * If PTI is off, then __flush_tlb_one_user() is just INVLPG or its

	 * paravirt equivalent.  Even with PCID, this is sufficient: we only

	 * use PCID if we also use global PTEs for the kernel mapping, and

	 * INVLPG flushes global translations across all address spaces.

	 *

	 * If PTI is on, then the kernel is mapped with non-global PTEs, and

	 * __flush_tlb_one_user() will flush the given address for the current

	 * kernel address space and for its usermode counterpart, but it does

	 * not flush it for other address spaces.

	/*

	 * See above.  We need to propagate the flush to all other address

	 * spaces.  In principle, we only need to propagate it to kernelmode

	 * address spaces, but the extra bookkeeping we would need is not

	 * worth it.

/*

 * Flush one page in the user mapping

	/*

	 * Some platforms #GP if we call invpcid(type=1/2) before CR4.PCIDE=1.

	 * Just use invalidate_user_asid() in case we are called early.

/*

 * Flush everything

		/*

		 * Using INVPCID is considerably faster than a pair of writes

		 * to CR4 sandwiched inside an IRQ flag save/restore.

		 *

		 * Note, this works with CR4.PCIDE=0 or 1.

	/*

	 * Read-modify-write to CR4 - protect it from preemption and

	 * from interrupts. (Use the raw variant because this code can

	 * be called from deep inside debugging code.)

 toggle PGE */

 write old PGE again and flush TLBs */

/*

 * Flush the entire current user mapping

	/*

	 * Preemption or interrupts must be disabled to protect the access

	 * to the per CPU variable and to prevent being preempted between

	 * read_cr3() and write_cr3().

 If current->mm == NULL then the read_cr3() "borrows" an mm */

/*

 * Flush everything

	/*

	 * This is to catch users with enabled preemption and the PGE feature

	 * and don't trigger the warning in __native_flush_tlb().

		/*

		 * !PGE -> !PCID (setup_pcid()), thus every flush is total.

	/*

	 * flush_tlb_multi() is not optimized for the common case in which only

	 * a local TLB flush is needed. Optimize this use-case by calling

	 * flush_tlb_func_local() directly in this case.

/*

 * Blindly accessing user memory from NMI context can be dangerous

 * if we're in the middle of switching the current user task or

 * switching the loaded mm.  It can also be dangerous if we

 * interrupted some kernel code that was temporarily using a

 * different mm.

	/*

	 * The condition we want to check is

	 * current_mm->pgd == __va(read_cr3_pa()).  This may be slow, though,

	 * if we're running in a VM with shadow paging, and nmi_uaccess_okay()

	 * is supposed to be reasonably fast.

	 *

	 * Instead, we check the almost equivalent but somewhat conservative

	 * condition below, and we rely on the fact that switch_mm_irqs_off()

	 * sets loaded_mm to LOADED_MM_SWITCHING before writing to CR3.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Flexible mmap layout support

 *

 * Based on code by Ingo Molnar and Andi Kleen, copyrighted

 * as follows:

 *

 * Copyright 2003-2009 Red Hat Inc.

 * All Rights Reserved.

 * Copyright 2005 Andi Kleen, SUSE Labs.

 * Copyright 2007 Jiri Kosina, SUSE Labs.

 Values close to RLIM_INFINITY can overflow. */

	/*

	 * Top of mmap area (just below the process stack).

	 * Leave an at least ~128 MB hole with possible stack randomization.

/*

 * This function, called very early during the creation of a new

 * process VM image, sets up which VM layout function to use:

	/*

	 * The mmap syscall mapping base decision depends solely on the

	 * syscall type (64-bit or compat). This applies for 64bit

	 * applications and 32bit applications. The 64bit syscall uses

	 * mmap_base, the compat syscall uses mmap_compat_base.

/**

 * mmap_address_hint_valid - Validate the address hint of mmap

 * @addr:	Address hint

 * @len:	Mapping length

 *

 * Check whether @addr and @addr + @len result in a valid mapping.

 *

 * On 32bit this only checks whether @addr + @len is <= TASK_SIZE.

 *

 * On 64bit with 5-level page tables another sanity check is required

 * because mappings requested by mmap(@addr, 0) which cross the 47-bit

 * virtual address boundary can cause the following theoretical issue:

 *

 *  An application calls mmap(addr, 0), i.e. without MAP_FIXED, where @addr

 *  is below the border of the 47-bit address space and @addr + @len is

 *  above the border.

 *

 *  With 4-level paging this request succeeds, but the resulting mapping

 *  address will always be within the 47-bit virtual address space, because

 *  the hint address does not result in a valid mapping and is

 *  ignored. Hence applications which are not prepared to handle virtual

 *  addresses above 47-bit work correctly.

 *

 *  With 5-level paging this request would be granted and result in a

 *  mapping which crosses the border of the 47-bit virtual address

 *  space. If the application cannot handle addresses above 47-bit this

 *  will lead to misbehaviour and hard to diagnose failures.

 *

 * Therefore ignore address hints which would result in a mapping crossing

 * the 47-bit virtual address boundary.

 *

 * Note, that in the same scenario with MAP_FIXED the behaviour is

 * different. The request with @addr < 47-bit and @addr + @len > 47-bit

 * fails on a 4-level paging machine but succeeds on a 5-level paging

 * machine. It is reasonable to expect that an application does not rely on

 * the failure of such a fixed mapping request, so the restriction is not

 * applied.

 Can we access it for direct reading/writing? Must be RAM: */

 Can we access it through mmap? Must be a valid physical address: */

/*

 * Only allow root to set high MMIO mappings to PROT_NONE.

 * This prevents an unpriv. user to set them to PROT_NONE and invert

 * them, then pointing to valid memory for L1TF speculation.

 *

 * Note: for locked down kernels may want to disable the root override.

 If it's real memory always allow */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) IBM Corporation, 2005

 *               Jeff Muizelaar, 2006, 2007

 *               Pekka Paalanen, 2008 <pq@iki.fi>

 *

 * Derived from the read-mod example from relay-examples by Tom Zanussi.

 for ISA_START_ADDRESS */

 Accessed per-cpu. */

 struct remap_trace */

/*

 * Locking in this file:

 * - mmiotrace_mutex enforces enable/disable_mmiotrace() critical sections.

 * - mmiotrace_enabled may be modified only when holding mmiotrace_mutex

 *   and trace_lock.

 * - Routines depending on is_enabled() must take trace_lock.

 * - trace_list users must hold trace_lock.

 * - is_enabled() guarantees that mmio_trace_{rw,mapping} are allowed.

 * - pre/post callbacks assume the effect of is_enabled() being true.

 module parameters */

/*

 * For some reason the pre/post pairs have been called in an

 * unmatched order. Report and die.

 it doesn't make sense to have more than one active trace per cpu */

	/*

	 * Only record the program counter when requested.

	 * It may taint clean-room reverse engineering.

	/*

	 * XXX: the timestamp recorded will be *after* the tracing has been

	 * done, not at the time we hit the instruction. SMP implications

	 * on event ordering?

 this should always return the active_trace count to 0 */

 These are page-unaligned. */

 recheck and proper locking in *_core() */

 unregister_kmmio_probe() requirement */

 recheck and proper locking in *_core() */

	/*

	 * No locking required, because the caller ensures we are in a

	 * critical section via mutex, and is_enabled() is false,

	 * i.e. nothing can traverse or modify this list.

	 * Caller also ensures is_enabled() cannot change.

 unregister_kmmio_probe() requirement */

 !CONFIG_HOTPLUG_CPU */

 guarantees: no more kmmio callbacks */

 SPDX-License-Identifier: GPL-2.0

/*

 * ACPI 3.0 based NUMA setup

 * Copyright 2004 Andi Kleen, SuSE Labs.

 *

 * Reads the ACPI SRAT table to figure out what memory belongs to which CPUs.

 *

 * Called from acpi_numa_init while reading the SRAT and SLIT tables.

 * Assumes all memory regions belonging to a single proximity domain

 * are in one chunk. Holes between them will be included in the node.

 Callback for Proximity Domain -> x2APIC mapping */

 Callback for Proximity Domain -> LAPIC mapping */

 SPDX-License-Identifier: GPL-2.0

/*

 * NUMA emulation

/*

 * Sets up nid to range from @start to @end.  The return value is -errno if

 * something went wrong, 0 otherwise.

/*

 * Sets up nr_nodes fake nodes interleaved over physical nodes ranging from addr

 * to max_addr.

 *

 * Returns zero on success or negative on error.

	/*

	 * Calculate target node size.  x86_32 freaks on __udivdi3() so do

	 * the division in ulong number of pages and convert back.

	/*

	 * Calculate the number of big nodes that can be allocated as a result

	 * of consolidating the remainder.

	/*

	 * Continue to fill physical nodes with fake nodes until there is no

	 * memory left on any of them.

			/*

			 * Continue to add memory to this fake node if its

			 * non-reserved memory is less than the per-node size.

			/*

			 * If there won't be at least FAKE_NODE_MIN_SIZE of

			 * non-reserved memory in ZONE_DMA32 for the next node,

			 * this one must extend to the boundary.

			/*

			 * If there won't be enough non-reserved memory for the

			 * next node, this one must extend to the end of the

			 * physical node.

/*

 * Returns the end address of a node so that there is at least `size' amount of

 * non-reserved memory or `max_addr' is reached.

/*

 * Sets up fake nodes of `size' interleaved over physical nodes ranging from

 * `addr' to `max_addr'.

 *

 * Returns zero on success or negative on error.

	/*

	 * In the 'uniform' case split the passed in physical node by

	 * nr_nodes, in the non-uniform case, ignore the passed in

	 * physical block and try to create nodes of at least size

	 * @size.

	 *

	 * In the uniform case, split the nodes strictly by physical

	 * capacity, i.e. ignore holes. In the non-uniform case account

	 * for holes and treat @size as a minimum floor.

		/*

		 * The limit on emulated nodes is MAX_NUMNODES, so the

		 * size per node is increased accordingly if the

		 * requested size is too small.  This creates a uniform

		 * distribution of node sizes across the entire machine

		 * (but not necessarily over physical nodes).

	/*

	 * Fill physical nodes with fake nodes of size until there is no memory

	 * left on any of them.

			/*

			 * If there won't be at least FAKE_NODE_MIN_SIZE of

			 * non-reserved memory in ZONE_DMA32 for the next node,

			 * this one must extend to the boundary.

			/*

			 * If there won't be enough non-reserved memory for the

			 * next node, this one must extend to the end of the

			 * physical node.

/**

 * numa_emulation - Emulate NUMA nodes

 * @numa_meminfo: NUMA configuration to massage

 * @numa_dist_cnt: The size of the physical NUMA distance table

 *

 * Emulate NUMA nodes according to the numa=fake kernel parameter.

 * @numa_meminfo contains the physical memory configuration and is modified

 * to reflect the emulated configuration on success.  @numa_dist_cnt is

 * used to determine the size of the physical distance table.

 *

 * On success, the following modifications are made.

 *

 * - @numa_meminfo is updated to reflect the emulated nodes.

 *

 * - __apicid_to_node[] is updated such that APIC IDs are mapped to the

 *   emulated nodes.

 *

 * - NUMA distance table is rebuilt to represent distances between emulated

 *   nodes.  The distances are determined considering how emulated nodes

 *   are mapped to physical nodes and match the actual distances.

 *

 * - emu_nid_to_phys[] reflects how emulated nodes are mapped to physical

 *   nodes.  This is used by numa_add_cpu() and numa_remove_cpu().

 *

 * If emulation is not enabled or fails, emu_nid_to_phys[] is filled with

 * identity mapping and no other modification is made.

	/*

	 * If the numa=fake command-line contains a 'M' or 'G', it represents

	 * the fixed node size.  Otherwise, if it is just a single number N,

	 * split the system RAM into N fake nodes.

			/*

			 * The reason we pass in blk[0] is due to

			 * numa_remove_memblk_from() called by

			 * emu_setup_memblk() will delete entry 0

			 * and then move everything else up in the pi.blk

			 * array. Therefore we should always be looking

			 * at blk[0].

 copy the physical distance table */

	/*

	 * Determine the max emulated nid and the default phys nid to use

	 * for unmapped nodes.

 commit */

 Make sure numa_nodes_parsed only contains emulated nodes */

	/*

	 * Transform __apicid_to_node table to use emulated nids by

	 * reverse-mapping phys_nid.  The maps should always exist but fall

	 * back to zero just in case.

 make sure all emulated nodes are mapped to a physical node */

 transform distance table */

 free the copied physical distance table */

 No emulation.  Build identity emu_nid_to_phys[] for numa_add_cpu() */

	/*

	 * Map the cpu to each emulated node that is allocated on the physical

	 * node of the cpu's apic id.

 !CONFIG_DEBUG_PER_CPU_MAPS */

 early_cpu_to_node() already emits a warning and trace */

 !CONFIG_DEBUG_PER_CPU_MAPS */

 SPDX-License-Identifier: GPL-2.0

 use the carry flag to determine if x was < __START_KERNEL_map */

 carry flag will be set if starting x was >= PAGE_OFFSET */

 only check upper bounds since lower bounds will trigger carry */

 use the carry flag to determine if x was < __START_KERNEL_map */

 carry flag will be set if starting x was >= PAGE_OFFSET */

 VMALLOC_* aren't constants  */

 max_low_pfn is set early, but not _that_ early */

 CONFIG_X86_64 */

 SPDX-License-Identifier: GPL-2.0

/*

 * noexec = on|off

 *

 * Control non-executable mappings for processes.

 *

 * on      Enable

 * off     Disable

 32bit non-PAE kernel, NX cannot be used */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright © 2008 Ingo Molnar

 There is no way to map greater than 1 << 32 address without PAE */

 Filter out unsupported __PAGE_KERNEL* bits: */

	/*

	 * For non-PAT systems, translate non-WB request to UC- just in

	 * case the caller set the PWT bit to prot directly without using

	 * pgprot_writecombine(). UC- translates to uncached if the MTRR

	 * is UC or WC. UC- gets the real intention, of the user, which is

	 * "WC if the MTRR is WC, UC if you can't do that."

 Filter out unsupported __PAGE_KERNEL* bits: */

 for MAX_DMA_PFN */

/*

 * We need to define the tracepoints somewhere, and tlb.c

 * is only compiled when SMP=y.

/*

 * Tables translating between page_cache_type_t and pte encoding.

 *

 * The default values are defined statically as minimal supported mode;

 * WC and WT fall back to UC-.  pat_init() updates these values to support

 * more cache modes, WC and WT, when it is safe to do so.  See pat_init()

 * for the details.  Note, __early_ioremap() used during early boot-time

 * takes pgprot_t (pte encoding) and does not use these tables.

 *

 *   Index into __cachemode2pte_tbl[] is the cachemode.

 *

 *   Index into __pte2cachemode_tbl[] are the caching attribute bits of the pte

 *   (_PAGE_PWT, _PAGE_PCD, _PAGE_PAT) at index bit positions 0, 1, 2.

 Check that the write-protect PAT entry is set for write-protect */

/*

 * Pages returned are already directly mapped.

 *

 * Changing that is likely to break Xen, see commit:

 *

 *    279b706 x86,xen: introduce x86_init.mapping.pagetable_reserve

 *

 * for detailed information.

/*

 * By default need to be able to allocate page tables below PGD firstly for

 * the 0-ISA_END_ADDRESS range and secondly for the initial PMD_SIZE mapping.

 * With KASLR memory randomization, depending on the machine e820 memory and the

 * PUD alignment, twice that many pages may be needed when KASLR memory

 * randomization is enabled.

/*

 * Save some of cr4 feature set we're using (e.g.  Pentium 4MB

 * enable and PPro Global page enable), so that any CPU's that boot

 * up after us can get the correct flags. Invoked on the boot CPU.

	/*

	 * For pagealloc debugging, identity mapping will use small pages.

	 * This will simplify cpa(), which otherwise needs to support splitting

	 * large pages into small in interrupt context, etc.

 Enable PSE if available */

 Enable PGE if available */

 By the default is everything supported: */

 Except when with PTI where the kernel is mostly non-Global: */

 Enable 1 GB linear kernel mappings if available: */

		/*

		 * This can't be cr4_set_bits_and_update_boot() -- the

		 * trampoline code can't handle CR4.PCIDE and it wouldn't

		 * do any good anyway.  Despite the name,

		 * cr4_set_bits_and_update_boot() doesn't actually cause

		 * the bits in question to remain set all the way through

		 * the secondary boot asm.

		 *

		 * Instead, we brute-force it and set CR4.PCIDE manually in

		 * start_secondary().

		/*

		 * INVPCID's single-context modes (2/3) only work if we set

		 * X86_CR4_PCIDE, *and* we INVPCID support.  It's unusable

		 * on systems that have X86_CR4_PCIDE clear, or that have

		 * no INVPCID support at all.

		/*

		 * flush_tlb_all(), as currently implemented, won't work if

		 * PCID is on but PGE is not.  Since that combination

		 * doesn't exist on real hardware, there's no reason to try

		 * to fully support it, but it's polite to avoid corrupting

		 * data if we're on an improperly configured VM.

 CONFIG_X86_64 */

/*

 * adjust the page_size_mask for small range to go with

 *	big page size instead small one if nearby are ram too.

	/*

	 * 32-bit without PAE has a 4M large page size.

	 * PG_LEVEL_2M is misnamed, but we can at least

	 * print out the right size in the string.

 head if not big page alignment ? */

	/*

	 * Don't use a large page for the first 2/4MB of memory

	 * because there are often fixed size MTRRs in there

	 * and overlapping MTRRs into large pages can cause

	 * slowdowns.

 CONFIG_X86_64 */

 big page (2M) range */

 CONFIG_X86_64 */

 big page (1G) range */

 tail is not big page (1G) alignment */

 tail is not big page (2M) alignment */

 try to merge same page size and continuous */

 move it */

/*

 * Setup the direct mapping of the physical memory at PAGE_OFFSET.

 * This runs before bootmem is initialized and gets pages directly from

 * the physical memory. To access them they are temporarily mapped.

/*

 * We need to iterate through the E820 memory map and create direct mappings

 * for only E820_TYPE_RAM and E820_KERN_RESERVED regions. We cannot simply

 * create direct mappings for all pfns from [0 to max_low_pfn) and

 * [4GB to max_pfn) because of possible memory holes in high addresses

 * that cannot be marked as UC by fixed/variable range MTRRs.

 * Depending on the alignment of E820 ranges, this may possibly result

 * in using smaller size (i.e. 4K instead of 2M or 1G) page tables.

 *

 * init_mem_mapping() calls init_range_memory_mapping() with big range.

 * That range would have hole in the middle or ends, and only ram parts

 * will be mapped in init_range_memory_mapping().

		/*

		 * if it is overlapping with brk pgt, we need to

		 * alloc pgt buf from memblock instead.

	/*

	 * Initial mapped size is PMD_SIZE (2M).

	 * We can not set step_size to be PUD_SIZE (1G) yet.

	 * In worse case, when we cross the 1G boundary, and

	 * PG_LEVEL_2M is not set, we will need 1+1+512 pages (2M + 8k)

	 * to map 1G range with PTE. Hence we use one less than the

	 * difference of page table level shifts.

	 *

	 * Don't need to worry about overflow in the top-down case, on 32bit,

	 * when step_size is 0, round_down() returns 0 for start, and that

	 * turns it into 0x100000000ULL.

	 * In the bottom-up case, round_up(x, 0) returns 0 though too, which

	 * needs to be taken into consideration by the code below.

/**

 * memory_map_top_down - Map [map_start, map_end) top down

 * @map_start: start address of the target memory range

 * @map_end: end address of the target memory range

 *

 * This function will setup direct mapping for memory range

 * [map_start, map_end) in top-down. That said, the page tables

 * will be allocated at the end of the memory, and we map the

 * memory in top-down.

	/*

	 * Systems that have many reserved areas near top of the memory,

	 * e.g. QEMU with less than 1G RAM and EFI enabled, or Xen, will

	 * require lots of 4K mappings which may exhaust pgt_buf.

	 * Start with top-most PMD_SIZE range aligned at PMD_SIZE to ensure

	 * there is enough mapped memory that can be allocated from

	 * memblock.

 step_size need to be small so pgt_buf from BRK could cover it */

 will get exact value next */

	/*

	 * We start from the top (end of memory) and go to the bottom.

	 * The memblock_find_in_range() gets us a block of RAM from the

	 * end of RAM in [min_pfn_mapped, max_pfn_mapped) used as new pages

	 * for page table.

/**

 * memory_map_bottom_up - Map [map_start, map_end) bottom up

 * @map_start: start address of the target memory range

 * @map_end: end address of the target memory range

 *

 * This function will setup direct mapping for memory range

 * [map_start, map_end) in bottom-up. Since we have limited the

 * bottom-up allocation above the kernel, the page tables will

 * be allocated just above the kernel and we map the memory

 * in [map_start, map_end) in bottom-up.

 step_size need to be small so pgt_buf from BRK could cover it */

	/*

	 * We start from the bottom (@map_start) and go to the top (@map_end).

	 * The memblock_find_in_range() gets us a block of RAM from the

	 * end of RAM in [min_pfn_mapped, max_pfn_mapped) used as new pages

	 * for page table.

/*

 * The real mode trampoline, which is required for bootstrapping CPUs

 * occupies only a small area under the low 1MB.  See reserve_real_mode()

 * for details.

 *

 * If KASLR is disabled the first PGD entry of the direct mapping is copied

 * to map the real mode trampoline.

 *

 * If KASLR is enabled, copy only the PUD which covers the low 1MB

 * area. This limits the randomization granularity to 1GB for both 4-level

 * and 5-level paging.

 the ISA range is always mapped regardless of memory holes */

 Init the trampoline, possibly with KASLR memory offset */

	/*

	 * If the allocation is in bottom-up direction, we setup direct mapping

	 * in bottom-up, otherwise we setup direct mapping in top-down.

		/*

		 * we need two separate calls here. This is because we want to

		 * allocate page tables above the kernel. So we first map

		 * [kernel_end, end) to make memory above the kernel be mapped

		 * as soon as possible. And then use page tables allocated above

		 * the kernel to map [ISA_END_ADDRESS, kernel_end).

 can we preserve max_low_pfn ?*/

/*

 * Initialize an mm_struct to be used during poking and a pointer to be used

 * during patching.

	/*

	 * Randomize the poking address, but make sure that the following page

	 * will be mapped at the same PMD. We need 2 pages, so find space for 3,

	 * and adjust the address if the PMD ends after the first one.

	/*

	 * We need to trigger the allocation of the page-tables that will be

	 * needed for poking now. Later, poking may be performed in an atomic

	 * section, which might cause allocation to fail.

/*

 * devmem_is_allowed() checks to see if /dev/mem access to a certain address

 * is valid. The argument is a physical page number.

 *

 * On x86, access has to be given to the first megabyte of RAM because that

 * area traditionally contains BIOS code and data regions used by X, dosemu,

 * and similar apps. Since they map the entire memory range, the whole range

 * must be allowed (for mapping), but any areas that would otherwise be

 * disallowed are flagged as being "zero filled" instead of rejected.

 * Access has to be given to non-kernel-ram areas as well, these contain the

 * PCI mmio resources as well as potential bios/acpi data regions.

		/*

		 * For disallowed memory regions in the low 1MB range,

		 * request that the page be shown as all zeros.

	/*

	 * This must follow RAM test, since System RAM is considered a

	 * restricted resource under CONFIG_STRICT_IOMEM.

 Low 1MB bypasses iomem restrictions. */

 Make sure boundaries are page aligned */

	/*

	 * If debugging page accesses then do not free this memory but

	 * mark them not present - any buggy init-section access will

	 * create a kernel page fault:

		/*

		 * Inform kmemleak about the hole in the memory since the

		 * corresponding pages will be unmapped.

		/*

		 * We just marked the kernel text read only above, now that

		 * we are going to free part of that, we need to make that

		 * writeable and non-executable first.

/*

 * begin/end can be in the direct map or the "high kernel mapping"

 * used for the kernel image only.  free_init_pages() will do the

 * right thing for either kind of address.

	/*

	 * PTI maps some of the kernel into userspace.  For performance,

	 * this includes some kernel areas that do not contain secrets.

	 * Those areas might be adjacent to the parts of the kernel image

	 * being freed, which may contain secrets.  Remove the "high kernel

	 * image mapping" for these freed areas, ensuring they are not even

	 * potentially vulnerable to Meltdown regardless of the specific

	 * optimizations PTI is currently using.

	 *

	 * The "noalias" prevents unmapping the direct map alias which is

	 * needed to access the freed pages.

	 *

	 * This is only valid for 64bit kernels. 32bit has only one mapping

	 * which can't be treated in this way for obvious reasons.

	/*

	 * end could be not aligned, and We can not align that,

	 * decompressor could be confused by aligned initrd_end

	 * We already reserve the end partial page before in

	 *   - i386_start_kernel()

	 *   - x86_64_start_kernel()

	 *   - relocate_initrd()

	 * So here We can do PAGE_ALIGN() safely to get partial page to be freed

/*

 * Calculate the precise size of the DMA zone (first 16 MB of RAM),

 * and pass it to the MM layer - to help it set zone watermarks more

 * accurately.

 *

 * Done on 64-bit systems only for the time being, although 32-bit systems

 * might benefit from this as well.

	/*

	 * Iterate over all memory ranges (free and reserved ones alike),

	 * to calculate the total number of pages in the first 16 MB of RAM:

	/*

	 * Iterate over free memory ranges to calculate the number of free

	 * pages in the DMA zone, while not counting potential partial

	 * pages at the beginning or the end of the range:

 fail hard if we screw up cr4 shadow initialization */

 entry 0 MUST be WB (hardwired to speed up translations) */

 Limit the swap file size to MAX_PA/2 for L1TF workaround */

		/*

		 * We encode swap offsets also with 3 bits below those for pfn

		 * which makes the usable limit higher.

 SPDX-License-Identifier: GPL-2.0

 Is called from entry code, so must be noinstr */

	/*

	 * The cpu_entry_area is shared between the user and kernel

	 * page tables.  All of its ptes can safely be global.

	 * _PAGE_GLOBAL gets reused to help indicate PROT_NONE for

	 * non-present PTEs, so be careful not to set it in that

	 * case to avoid confusion.

	/*

	 * Force the population of PMDs for not yet allocated per cpu

	 * memory like debug store buffers.

	/*

	 * The exceptions stack mappings in the per cpu area are protected

	 * by guard pages so each stack must be mapped separately. DB2 is

	 * not mapped; it just exists to catch triple nesting of #DB.

 Setup the fixmap mappings only once per-processor */

 On 64-bit systems, we use a read-only fixmap GDT and TSS. */

	/*

	 * On native 32-bit systems, the GDT cannot be read-only because

	 * our double fault handler uses a task gate, and entering through

	 * a task gate needs to change an available TSS to busy.  If the

	 * GDT is read-only, that will triple fault.  The TSS cannot be

	 * read-only because the CPU writes to it on task switches.

	 *

	 * On Xen PV, the GDT must be read-only because the hypervisor

	 * requires it.

	/*

	 * The Intel SDM says (Volume 3, 7.2.1):

	 *

	 *  Avoid placing a page boundary in the part of the TSS that the

	 *  processor reads during a task switch (the first 104 bytes). The

	 *  processor may not correctly perform address translations if a

	 *  boundary occurs in this area. During a task switch, the processor

	 *  reads and writes into the first 104 bytes of each TSS (using

	 *  contiguous physical addresses beginning with the physical address

	 *  of the first byte of the TSS). So, after TSS access begins, if

	 *  part of the 104 bytes is not physically contiguous, the processor

	 *  will access incorrect information without generating a page-fault

	 *  exception.

	 *

	 * There are also a lot of errata involving the TSS spanning a page

	 * boundary.  Assert that we're not doing that.

	/*

	 * VMX changes the host TR limit to 0x67 after a VM exit. This is

	 * okay, since 0x67 covers the size of struct x86_hw_tss. Make sure

	 * that this is correct.

 The +1 is for the readonly IDT: */

 Careful here: start + PMD_SIZE might wrap around */

	/*

	 * This is the last essential update to swapper_pgdir which needs

	 * to be synchronized to initial_page_table on 32bit.

 SPDX-License-Identifier: GPL-2.0-only

 Common code for 32 and 64-bit NUMA */

/*

 * apicid, cpu, node mappings

/*

 * Map cpu index to node index

 early setting, no percpu area yet */

/*

 * Allocate node_to_cpumask_map based on number of available nodes

 * Requires node_possible_map to be valid.

 *

 * Note: cpumask_of_node() is not valid until after this is done.

 * (Use CONFIG_DEBUG_PER_CPU_MAPS to check this.)

 setup nr_node_ids if not done yet */

 allocate the map */

 cpumask_of_node() will now work */

 ignore zero length blks */

 whine about and ignore invalid blks */

/**

 * numa_remove_memblk_from - Remove one numa_memblk from a numa_meminfo

 * @idx: Index of memblk to remove

 * @mi: numa_meminfo to remove memblk from

 *

 * Remove @idx'th numa_memblk from @mi by shifting @mi->blk[] and

 * decrementing @mi->nr_blks.

/**

 * numa_move_tail_memblk - Move a numa_memblk from one numa_meminfo to another

 * @dst: numa_meminfo to append block to

 * @idx: Index of memblk to remove

 * @src: numa_meminfo to remove memblk from

/**

 * numa_add_memblk - Add one numa_memblk to numa_meminfo

 * @nid: NUMA node ID of the new memblk

 * @start: Start address of the new memblk

 * @end: End address of the new memblk

 *

 * Add a new memblk to the default numa_meminfo.

 *

 * RETURNS:

 * 0 on success, -errno on failure.

 Allocate NODE_DATA for a node on the local memory */

	/*

	 * Allocate node data.  Try node-local memory and then any node.

	 * Never allocate in DMA zone.

 report and initialize */

/**

 * numa_cleanup_meminfo - Cleanup a numa_meminfo

 * @mi: numa_meminfo to clean up

 *

 * Sanitize @mi by merging and removing unnecessary memblks.  Also check for

 * conflicts and clear unused memblks.

 *

 * RETURNS:

 * 0 on success, -errno on failure.

 first, trim all entries */

 move / save reserved memory ranges */

 make sure all non-reserved blocks are inside the limits */

 preserve info for non-RAM areas above 'max_pfn': */

 and there's no empty block */

 merge neighboring / overlapping entries */

			/*

			 * See whether there are overlapping blocks.  Whine

			 * about but allow overlaps of the same nid.  They

			 * will be merged below.

			/*

			 * Join together blocks on the same node, holes

			 * between which don't overlap with memory on other

			 * nodes.

 clear unused ones */

/*

 * Set nodes, which have memory in @mi, in *@nodemask.

/**

 * numa_reset_distance - Reset NUMA distance table

 *

 * The current table is freed.  The next numa_set_distance() call will

 * create a new one.

 numa_distance could be 1LU marking allocation failure, test cnt */

 enable table creation */

 size the new table and allocate it */

 don't retry until explicitly reset */

 fill with the default distances */

/**

 * numa_set_distance - Set NUMA distance from one NUMA to another

 * @from: the 'from' node to set distance

 * @to: the 'to'  node to set distance

 * @distance: NUMA distance

 *

 * Set the distance from node @from to @to to @distance.  If distance table

 * doesn't exist, one which is large enough to accommodate all the currently

 * known nodes will be created.

 *

 * If such table cannot be allocated, a warning is printed and further

 * calls are ignored until the distance table is reset with

 * numa_reset_distance().

 *

 * If @from or @to is higher than the highest known node or lower than zero

 * at the time of table creation or @distance doesn't make sense, the call

 * is ignored.

 * This is to allow simplification of specific NUMA config implementations.

/*

 * Sanity check to catch more bad NUMA configurations (they are amazingly

 * common).  Make sure the nodes cover all memory.

 We seem to lose 3 pages somewhere. Allow 1M of slack. */

/*

 * Mark all currently memblock-reserved physical memory (which covers the

 * kernel's own memory ranges) as hot-unswappable.

	/*

	 * We have to do some preprocessing of memblock regions, to

	 * make them suitable for reservation.

	 *

	 * At this time, all memory regions reserved by memblock are

	 * used by the kernel, but those regions are not split up

	 * along node boundaries yet, and don't necessarily have their

	 * node ID set yet either.

	 *

	 * So iterate over all memory known to the x86 architecture,

	 * and use those ranges to set the nid in memblock.reserved.

	 * This will split up the memblock regions along node

	 * boundaries and will set the node IDs as well.

	/*

	 * Now go over all reserved memblock regions, to construct a

	 * node mask of all kernel reserved memory areas.

	 *

	 * [ Note, when booting with mem=nn[kMG] or in a kdump kernel,

	 *   numa_meminfo might not include all memblock.reserved

	 *   memory ranges, because quirks such as trim_snb_memory()

	 *   reserve specific pages for Sandy Bridge graphics. ]

	/*

	 * Finally, clear the MEMBLOCK_HOTPLUG flag for all memory

	 * belonging to the reserved node mask.

	 *

	 * Note that this will include memory regions that reside

	 * on nodes that contain kernel memory - entire nodes

	 * become hot-unpluggable:

 Account for nodes with cpus and no memory */

	/*

	 * At very early time, the kernel have to use some memory such as

	 * loading the kernel image. We cannot prevent this anyway. So any

	 * node the kernel resides in should be un-hotpluggable.

	 *

	 * And when we come here, alloc node data won't fail.

	/*

	 * If sections array is gonna be used for pfn -> nid mapping, check

	 * whether its granularity is fine enough.

 Finally register nodes. */

		/*

		 * Don't confuse VM with a node that doesn't have the

		 * minimum amount of memory:

 Dump memblock with node info and return. */

/*

 * There are unfortunately some poorly designed mainboards around that

 * only connect memory to a single CPU. This breaks the 1:1 cpu->node

 * mapping. To avoid this fill in the mapping for all possible CPUs,

 * as the number of CPUs is not known yet. We round robin the existing

 * nodes.

 In case that parsing SRAT failed. */

	/*

	 * We reset memblock back to the top-down direction

	 * here because if we configured ACPI_NUMA, we have

	 * parsed SRAT in init_func(). It is ok to have the

	 * reset here even if we did't configure ACPI_NUMA

	 * or acpi numa init fails and fallbacks to dummy

	 * numa init.

/**

 * dummy_numa_init - Fallback dummy NUMA init

 *

 * Used if there's no underlying NUMA architecture, NUMA initialization

 * fails, or NUMA is disabled on the command line.

 *

 * Must online at least one node and add memory blocks that cover all

 * allowed memory.  This function must not fail.

/**

 * x86_numa_init - Initialize NUMA

 *

 * Try each configured NUMA initialization method until one succeeds.  The

 * last fallback is dummy single node config encompassing whole memory and

 * never fails.

 Allocate and initialize node data. Memory-less node is now online.*/

	/*

	 * All zonelists will be built later in start_kernel() after per cpu

	 * areas are initialized.

/*

 * A node may exist which has one or more Generic Initiators but no CPUs and no

 * memory.

 *

 * This function must be called after init_cpu_to_node(), to ensure that any

 * memoryless CPU nodes have already been brought online, and before the

 * node_data[nid] is needed for zone list setup in build_all_zonelists().

 *

 * When this function is called, any nodes containing either memory and/or CPUs

 * will already be online and there is no need to do anything extra, even if

 * they also contain one or more Generic Initiators.

/*

 * Setup early cpu_to_node.

 *

 * Populate cpu_to_node[] only if x86_cpu_to_apicid[],

 * and apicid_to_node[] tables have valid entries for a CPU.

 * This means we skip cpu_to_node[] initialisation for NUMA

 * emulation and faking node case (when running a kernel compiled

 * for NUMA on a non NUMA box), which is OK as cpu_to_node[]

 * is already initialized in a round robin manner at numa_init_array,

 * prior to this call, and this initialization is good enough

 * for the fake NUMA cases.

 *

 * Called before the per_cpu areas are setup.

 !CONFIG_NUMA_EMU */

 !CONFIG_DEBUG_PER_CPU_MAPS */

/*

 * Same function as cpu_to_node() but used if called before the

 * per_cpu areas are setup.

 early_cpu_to_node() already emits a warning and trace */

 !CONFIG_NUMA_EMU */

/*

 * Returns a pointer to the bitmask of CPUs on Node 'node'.

 !CONFIG_DEBUG_PER_CPU_MAPS */

	/*

	 * Prefer online nodes, but if reserved memory might be

	 * hot-added continue the search with reserved ranges.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel Memory Protection Keys management

 * Copyright (c) 2015, Intel Corporation.

 debugfs_create_u32()		*/

 mm_struct, vma, etc...       */

 PKEY_*                       */

 boot_cpu_has, ...            */

 vma_pkey()                   */

 Do we need to assign a pkey for mm's execute-only maps? */

 Go allocate one to use, which might fail */

	/*

	 * We do not want to go through the relatively costly

	 * dance to set PKRU if we do not need to.  Check it

	 * first and assume that if the execute-only pkey is

	 * write-disabled that we do not have to set it

	 * ourselves.

	/*

	 * Set up PKRU so that it denies access for everything

	 * other than execution.

	/*

	 * If the PKRU-set operation failed somehow, just return

	 * 0 and effectively disable execute-only support.

 We got one, store it and use it from here on out */

 Do this check first since the vm_flags should be hot */

/*

 * This is only called for *plain* mprotect calls.

	/*

	 * Is this an mprotect_pkey() call?  If so, never

	 * override the value that came from the user.

	/*

	 * The mapping is execute-only.  Go try to get the

	 * execute-only protection key.  If we fail to do that,

	 * fall through as if we do not have execute-only

	 * support in this mm.

		/*

		 * Protections are *not* PROT_EXEC, but the mapping

		 * is using the exec-only pkey.  This mapping was

		 * PROT_EXEC and will no longer be.  Move back to

		 * the default pkey.

	/*

	 * This is a vanilla, non-pkey mprotect (or we failed to

	 * setup execute-only), inherit the pkey from the VMA we

	 * are working on.

/*

 * Make the default PKRU value (at execve() time) as restrictive

 * as possible.  This ensures that any threads clone()'d early

 * in the process's lifetime will not accidentally get access

 * to data which is pkey-protected later on.

 Make the buffer a valid string that we can not overrun */

	/*

	 * Don't allow insane settings that will blow the system

	 * up immediately if someone attempts to disable access

	 * or writes to pkey 0.

 Do not expose the file if pkeys are not supported. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic VM initialization for x86-64 NUMA setups.

 * Copyright 2002,2003 Andi Kleen, SuSE Labs.

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1995  Linus Torvalds

 *  Copyright (C) 2001, 2002 Andi Kleen, SuSE Labs.

 *  Copyright (C) 2008-2009, Red Hat Inc., Ingo Molnar

 test_thread_flag(), ...	*/

 task_stack_*(), ...		*/

 oops_begin/end, ...		*/

 search_exception_tables	*/

 max_low_pfn			*/

 kfence_handle_page_fault	*/

 NOKPROBE_SYMBOL, ...		*/

 kmmio_handler, ...		*/

 perf_sw_event		*/

 hstate_index_to_shift	*/

 prefetchw			*/

 exception_enter(), ...	*/

 faulthandler_disabled()	*/

 efi_crash_gracefully_on_page_fault()*/

 boot_cpu_has, ...		*/

 dotraplinkage, ...		*/

 VSYSCALL_ADDR		*/

 emulate_vsyscall		*/

 struct vm86			*/

 vma_pkey()			*/

 efi_crash_gracefully_on_page_fault()*/

 store_idt(), ...		*/

 exception stack		*/

 VMALLOC_START, ...		*/

 kvm_handle_async_pf		*/

 fixup_vdso_exception()	*/

/*

 * Returns 0 if mmiotrace is disabled, or if the fault is not

 * handled by mmiotrace:

/*

 * Prefetch quirks:

 *

 * 32-bit mode:

 *

 *   Sometimes AMD Athlon/Opteron CPUs report invalid exceptions on prefetch.

 *   Check that here and ignore it.  This is AMD erratum #91.

 *

 * 64-bit mode:

 *

 *   Sometimes the CPU reports invalid exceptions on prefetch.

 *   Check that here and ignore it.

 *

 * Opcode checker based on code by Richard Brunner.

		/*

		 * Values 0x26,0x2E,0x36,0x3E are valid x86 prefixes.

		 * In X86_64 long mode, the CPU will signal invalid

		 * opcode if some of these prefixes are present so

		 * X86_64 will never get here anyway

		/*

		 * In 64-bit mode 0x40..0x4F are valid REX prefixes

 0x64 thru 0x67 are valid prefixes in all modes. */

 0xF0, 0xF2, 0xF3 are valid prefixes in all modes. */

 Prefetch instruction is 0x0F0D or 0x0F18 */

 Erratum #91 affects AMD K8, pre-NPT CPUs */

	/*

	 * If it was a exec (instruction fetch) fault on NX page, then

	 * do not ignore the fault:

	/*

	 * This code has historically always bailed out if IP points to a

	 * not-present page (e.g. due to a race).  No one has ever

	 * complained about this.

	/*

	 * set_pgd(pgd, *pgd_k); here would be useless on PAE

	 * and redundant with the set_pmd() on non-PAE. As would

	 * set_p4d/set_pud.

/*

 *   Handle a fault on the vmalloc or module mapping area

 *

 *   This is needed because there is a race condition between the time

 *   when the vmalloc mapping code updates the PMD to the point in time

 *   where it synchronizes this update with the other page-tables in the

 *   system.

 *

 *   In this race window another thread/CPU can map an area on the same

 *   PMD, finds it already present and does not synchronize it with the

 *   rest of the system yet. As a result v[mz]alloc might return areas

 *   which are not mapped in every page-table in the system, causing an

 *   unhandled page-fault when they are accessed.

 Make sure we are in vmalloc area: */

	/*

	 * Synchronize this task's top level page-table

	 * with the 'reference' page table.

	 *

	 * Do _not_ use "current" here. We might be inside

	 * an interrupt in the middle of a task switch..

 the pgt_lock only for Xen */

	/*

	 * We must not directly access the pte in the highpte

	 * case if the page table is located in highmem.

	 * And let's rather not kmap-atomic the pte, just in case

	 * it's allocated already:

 CONFIG_X86_64: */

 CONFIG_X86_64 */

/*

 * Workaround for K8 erratum #93 & buggy BIOS.

 *

 * BIOS SMM functions are required to use a specific workaround

 * to avoid corruption of the 64bit RIP register on C stepping K8.

 *

 * A lot of BIOS that didn't get tested properly miss this.

 *

 * The OS sees this as a page fault with the upper 32bits of RIP cleared.

 * Try to work around it here.

 *

 * Note we only handle faults in kernel here.

 * Does nothing on 32-bit.

/*

 * Work around K8 erratum #100 K8 in compat mode occasionally jumps

 * to illegal addresses >4GB.

 *

 * We catch this in the page fault handler because these addresses

 * are not reachable. Just detect this case and return.  Any code

 * segment in LDT is compatibility mode.

 Pentium F0 0F C7 C8 bug workaround: */

		/*

		 * This can happen for quite a few reasons.  The more obvious

		 * ones are faults accessing the GDT, or LDT.  Perhaps

		 * surprisingly, if the CPU tries to deliver a benign or

		 * contributory exception from user code and gets a page fault

		 * during delivery, the page fault can be delivered as though

		 * it originated directly from user code.  This could happen

		 * due to wrong permissions on the IDT, GDT, LDT, TSS, or

		 * kernel or IST stack.

 Usable even on Xen PV -- it's just slow. */

	/*

	 * To avoid leaking information about the kernel page

	 * table layout, pretend that user-mode accesses to

	 * kernel addresses are always protection faults.

	 *

	 * NB: This means that failed vsyscalls with vsyscall=none

	 * will have the PROT bit.  This doesn't leak any

	 * information and does not appear to cause any problems.

		/*

		 * Implicit kernel access from user mode?  Skip the stack

		 * overflow and EFI special cases.

	/*

	 * Stack overflow?  During boot, we can fault near the initial

	 * stack in the direct map, but that's not an overflow -- check

	 * that we're in vmalloc space to avoid this.

		/*

		 * We're likely to be running with very little stack space

		 * left.  It's plausible that we'd hit this condition but

		 * double-fault even before we get this far, in which case

		 * we're fine: the double-fault handler will deal with it.

		 *

		 * We don't want to make it all the way into the oops code

		 * and then double-fault, though, because we're likely to

		 * break the console driver and lose most of the stack dump.

	/*

	 * Buggy firmware could access regions which might page fault.  If

	 * this happens, EFI has a special OOPS path that will try to

	 * avoid hanging the system.

 Only not-present faults should be handled by KFENCE. */

	/*

	 * Oops. The kernel tried to access some bad page. We'll have to

	 * terminate things with extreme prejudice:

 Executive summary in case the body of the oops scrolled away */

 Are we prepared to handle this kernel fault? */

		/*

		 * Any interrupt that takes a fault gets the fixup. This makes

		 * the below recursive fault logic only apply to a faults from

		 * task context.

		/*

		 * Per the above we're !in_interrupt(), aka. task context.

		 *

		 * In this case we need to make sure we're not recursively

		 * faulting through the emulate_vsyscall() logic.

 XXX: hwpoison faults will set the wrong code. */

		/*

		 * Barring that, we can do the fixup and be happy.

	/*

	 * AMD erratum #91 manifests as a spurious page fault on a PREFETCH

	 * instruction.

/*

 * Print out info about fatal segfaults, if the show_unhandled_signals

 * sysctl is set:

/*

 * The (legacy) vsyscall page is the long page in the kernel portion

 * of the address space that has user-accessible permissions.

 Implicit user access to kernel memory -- just oops */

	/*

	 * User mode accesses just cause a SIGSEGV.

	 * It's possible to have interrupts off here:

	/*

	 * Valid to do another page fault here because this one came

	 * from user space:

	/*

	 * Something tried to access memory that isn't in our memory map..

	 * Fix it, but check if it's kernel or user first..

 This code is always called on the current mm */

 this checks permission keys on the VMA: */

	/*

	 * This OSPKE check is not strictly necessary at runtime.

	 * But, doing it this way allows compiler optimizations

	 * if pkeys are compiled out.

		/*

		 * A protection key fault means that the PKRU value did not allow

		 * access to some PTE.  Userspace can figure out what PKRU was

		 * from the XSAVE state.  This function captures the pkey from

		 * the vma and passes it to userspace so userspace can discover

		 * which protection key was set on the PTE.

		 *

		 * If we get here, we know that the hardware signaled a X86_PF_PK

		 * fault and that there was a VMA once we got in the fault

		 * handler.  It does *not* guarantee that the VMA we find here

		 * was the one that we faulted on.

		 *

		 * 1. T1   : mprotect_key(foo, PAGE_SIZE, pkey=4);

		 * 2. T1   : set PKRU to deny access to pkey=4, touches page

		 * 3. T1   : faults...

		 * 4.    T2: mprotect_key(foo, PAGE_SIZE, pkey=5);

		 * 5. T1   : enters fault handler, takes mmap_lock, etc...

		 * 6. T1   : reaches here, sees vma_pkey(vma)=5, when we really

		 *	     faulted on a pte with its pkey=4.

 Kernel mode? Handle exceptions or die: */

 User-space => ok to do another page fault: */

/*

 * Handle a spurious fault caused by a stale TLB entry.

 *

 * This allows us to lazily refresh the TLB when increasing the

 * permissions of a kernel page (RO -> RW or NX -> X).  Doing it

 * eagerly is very expensive since that implies doing a full

 * cross-processor TLB flush, even if no stale TLB entries exist

 * on other processors.

 *

 * Spurious faults may only occur if the TLB contains an entry with

 * fewer permission than the page table entry.  Non-present (P = 0)

 * and reserved bit (R = 1) faults are never spurious.

 *

 * There are no security implications to leaving a stale TLB when

 * increasing the permissions on a page.

 *

 * Returns non-zero if a spurious fault was handled, zero otherwise.

 *

 * See Intel Developer's Manual Vol 3 Section 4.10.4.3, bullet 3

 * (Optional Invalidation).

	/*

	 * Only writes to RO or instruction fetches from NX may cause

	 * spurious faults.

	 *

	 * These could be from user or supervisor accesses but the TLB

	 * is only lazily flushed after a kernel mapping protection

	 * change, so user accesses are not expected to cause spurious

	 * faults.

	/*

	 * Make sure we have permissions in PMD.

	 * If not, then there's a bug in the page tables:

 This is only called for the current mm, so: */

	/*

	 * Read or write was blocked by protection keys.  This is

	 * always an unconditional error and can never result in

	 * a follow-up action to resolve the fault, like a COW.

	/*

	 * SGX hardware blocked the access.  This usually happens

	 * when the enclave memory contents have been destroyed, like

	 * after a suspend/resume cycle. In any case, the kernel can't

	 * fix the cause of the fault.  Handle the fault as an access

	 * error even in cases where no actual access violation

	 * occurred.  This allows userspace to rebuild the enclave in

	 * response to the signal.

	/*

	 * Make sure to check the VMA so that we do not perform

	 * faults just to hit a X86_PF_PK as soon as we fill in a

	 * page.

 write, present and write, not present: */

 read, present: */

 read, not present: */

	/*

	 * On 64-bit systems, the vsyscall page is at an address above

	 * TASK_SIZE_MAX, but is not considered part of the kernel

	 * address space.

/*

 * Called for all faults where 'address' is part of the kernel address

 * space.  Might get called for faults that originate from *code* that

 * ran in userspace or the kernel.

	/*

	 * Protection keys exceptions only happen on user pages.  We

	 * have no user pages in the kernel portion of the address

	 * space, so do not expect them here.

	/*

	 * We can fault-in kernel-space virtual memory on-demand. The

	 * 'reference' page table is init_mm.pgd.

	 *

	 * NOTE! We MUST NOT take any locks for this case. We may

	 * be in an interrupt or a critical region, and should

	 * only copy the information from the master page table,

	 * nothing more.

	 *

	 * Before doing this on-demand faulting, ensure that the

	 * fault is not any of the following:

	 * 1. A fault on a PTE with a reserved bit set.

	 * 2. A fault caused by a user-mode access.  (Do not demand-

	 *    fault kernel memory due to user-mode accesses).

	 * 3. A fault caused by a page-level protection violation.

	 *    (A demand fault would be on a non-present page which

	 *     would have X86_PF_PROT==0).

	 *

	 * This is only needed to close a race condition on x86-32 in

	 * the vmalloc mapping/unmapping code. See the comment above

	 * vmalloc_fault() for details. On x86-64 the race does not

	 * exist as the vmalloc mappings don't need to be synchronized

	 * there.

 Was the fault spurious, caused by lazy TLB invalidation? */

 kprobes don't want to hook the spurious faults: */

	/*

	 * Note, despite being a "bad area", there are quite a few

	 * acceptable reasons to get here, such as erratum fixups

	 * and handling kernel code that can fault, like get_user().

	 *

	 * Don't take the mm semaphore here. If we fixup a prefetch

	 * fault we could otherwise deadlock:

/*

 * Handle faults in the user portion of the address space.  Nothing in here

 * should check X86_PF_USER without a specific justification: for almost

 * all purposes, we should treat a normal kernel access to user memory

 * (e.g. get_user(), put_user(), etc.) the same as the WRUSS instruction.

 * The one exception is AC flag handling, which is, per the x86

 * architecture, special for WRUSS.

		/*

		 * Whoops, this is kernel mode code trying to execute from

		 * user memory.  Unless this is AMD erratum #93, which

		 * corrupts RIP such that it looks like a user address,

		 * this is unrecoverable.  Don't even try to look up the

		 * VMA or look for extable entries.

 kprobes don't want to hook the spurious faults: */

	/*

	 * Reserved bits are never expected to be set on

	 * entries in the user portion of the page tables.

	/*

	 * If SMAP is on, check for invalid kernel (supervisor) access to user

	 * pages in the user address space.  The odd case here is WRUSS,

	 * which, according to the preliminary documentation, does not respect

	 * SMAP and will have the USER bit set so, in all cases, SMAP

	 * enforcement appears to be consistent with the USER bit.

		/*

		 * No extable entry here.  This was a kernel access to an

		 * invalid pointer.  get_kernel_nofault() will not get here.

	/*

	 * If we're in an interrupt, have no user context or are running

	 * in a region with pagefaults disabled then we must not take the fault

	/*

	 * It's safe to allow irq's after cr2 has been saved and the

	 * vmalloc fault has been handled.

	 *

	 * User-mode registers count as a user access even for any

	 * potential system fault or CPU buglet:

	/*

	 * Faults in the vsyscall page might need emulation.  The

	 * vsyscall page is at a high address (>PAGE_OFFSET), but is

	 * considered to be part of the user address space.

	 *

	 * The vsyscall page does not have a "real" VMA, so do this

	 * emulation before we go searching for VMAs.

	 *

	 * PKRU never rejects instruction fetches, so we don't need

	 * to consider the PF_PK bit.

	/*

	 * Kernel-mode access to the user address space should only occur

	 * on well-defined single instructions listed in the exception

	 * tables.  But, an erroneous kernel fault occurring outside one of

	 * those areas which also holds mmap_lock might deadlock attempting

	 * to validate the fault against the address space.

	 *

	 * Only do the expensive exception table search when we might be at

	 * risk of a deadlock.  This happens if we

	 * 1. Failed to acquire mmap_lock, and

	 * 2. The access did not originate in userspace.

			/*

			 * Fault from code in kernel from

			 * which we do not expect faults.

		/*

		 * The above down_read_trylock() might have succeeded in

		 * which case we'll have missed the might_sleep() from

		 * down_read():

	/*

	 * Ok, we have a good vm_area for this memory access, so

	 * we can handle it..

	/*

	 * If for any reason at all we couldn't handle the fault,

	 * make sure we exit gracefully rather than endlessly redo

	 * the fault.  Since we never set FAULT_FLAG_RETRY_NOWAIT, if

	 * we get VM_FAULT_RETRY back, the mmap_lock has been unlocked.

	 *

	 * Note that handle_userfault() may also release and reacquire mmap_lock

	 * (and not return with VM_FAULT_RETRY), when returning to userland to

	 * repeat the page fault later with a VM_FAULT_NOPAGE retval

	 * (potentially after handling any pending signal during the return to

	 * userland). The return to userland is identified whenever

	 * FAULT_FLAG_USER|FAULT_FLAG_KILLABLE are both set in flags.

		/*

		 * Quick path to respond to signals.  The core mm code

		 * has unlocked the mm for us if we get here.

	/*

	 * If we need to retry the mmap_lock has already been released,

	 * and if there is a fatal signal pending there is no guarantee

	 * that we made any progress. Handle this case first.

 Kernel mode? Handle exceptions or die: */

		/*

		 * We ran out of memory, call the OOM killer, and return the

		 * userspace (which will retry the fault, or kill us if we got

		 * oom-killed):

 Was the fault on kernel-controlled part of the address space? */

		/*

		 * User address page fault handling might have reenabled

		 * interrupts. Fixing up all potential exit points of

		 * do_user_addr_fault() and its leaf functions is just not

		 * doable w/o creating an unholy mess or turning the code

		 * upside down.

	/*

	 * KVM uses #PF vector to deliver 'page not present' events to guests

	 * (asynchronous page fault mechanism). The event happens when a

	 * userspace task is trying to access some valid (from guest's point of

	 * view) memory which is not currently mapped by the host (e.g. the

	 * memory is swapped out). Note, the corresponding "page ready" event

	 * which is injected when the memory becomes available, is delivered via

	 * an interrupt mechanism and not a #PF exception

	 * (see arch/x86/kernel/kvm.c: sysvec_kvm_asyncpf_interrupt()).

	 *

	 * We are relying on the interrupted context being sane (valid RSP,

	 * relevant locks not held, etc.), which is fine as long as the

	 * interrupted context had IF=1.  We are also relying on the KVM

	 * async pf type field and CR2 being read consistently instead of

	 * getting values from real and async page faults mixed up.

	 *

	 * Fingers crossed.

	 *

	 * The async #PF handling code takes care of idtentry handling

	 * itself.

	/*

	 * Entry handling for valid #PF from kernel mode is slightly

	 * different: RCU is already watching and rcu_irq_enter() must not

	 * be invoked because a kernel fault on a user space address might

	 * sleep.

	 *

	 * In case the fault hit a RCU idle region the conditional entry

	 * code reenabled RCU to avoid subsequent wreckage which helps

	 * debuggability.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/arch/x86_64/mm/init.c

 *

 *  Copyright (C) 1995  Linus Torvalds

 *  Copyright (C) 2000  Pavel Machek <pavel@ucw.cz>

 *  Copyright (C) 2002,2003 Andi Kleen <ak@suse.de>

/*

 * NOTE: pagetable_init alloc all the fixmap pagetables contiguous on the

 * physical space so we can cache the place of the first one and move

 * around without checking the pgd every time.

 Bits supported by the hardware: */

 Bits allowed in normal kernel mappings: */

 Used in PAGE_KERNEL_* macros which are reasonably used out-of-tree: */

/*

 * noexec32=on|off

 * Control non executable heap for 32bit processes.

 * To control the stack too use noexec=off

 *

 * on	PROT_READ does not imply PROT_EXEC for 32-bit processes (default)

 * off	PROT_READ implies PROT_EXEC

 Check for overflow */

 the pgt_lock only for Xen */

		/*

		 * With folded p4d, pgd_none() is always false, we need to

		 * handle synchronization on p4d level.

 the pgt_lock only for Xen */

/*

 * When memory was added make sure all the processes MM have

 * suitable PGD entries in the local PGD level page.

/*

 * NOTE: This function is marked __ref because it calls __init function

 * (alloc_bootmem_pages). It's safe to do it ONLY when after_bootmem == 0.

	/*

	 * It's enough to flush this one mapping.

	 * (PGE mappings get flushed as well)

/*

 * Create large page table mappings for a range of physical addresses.

/*

 * The head.S code sets up the kernel high mapping:

 *

 *   from __START_KERNEL_map to __START_KERNEL_map + size (== _end-_text)

 *

 * phys_base holds the negative offset to the kernel, which is added

 * to the compile time generated pmds. This results in invalid pmds up

 * to the point where we hit the physaddr 0 mapping.

 *

 * We limit the mappings to the region from _text to _brk_end.  _brk_end

 * is rounded up to the 2MB boundary. This catches the invalid pmds as

 * well, as they are located before _text:

	/*

	 * Native path, max_pfn_mapped is not set yet.

	 * Xen has valid max_pfn_mapped set in

	 *	arch/x86/xen/mmu.c:xen_setup_kernel_pagetable().

/*

 * Create PTE level page table mapping for physical addresses.

 * It returns the last physical address mapped.

		/*

		 * We will re-use the existing mapping.

		 * Xen for example has some special requirements, like mapping

		 * pagetable pages as RO. So assume someone who pre-setup

		 * these mappings are more intelligent.

/*

 * Create PMD level page table mapping for physical addresses. The virtual

 * and physical address have to be aligned at this level.

 * It returns the last physical address mapped.

			/*

			 * If we are ok with PG_LEVEL_2M mapping, then we will

			 * use the existing mapping,

			 *

			 * Otherwise, we will split the large page mapping but

			 * use the same existing protection bits except for

			 * large page, so that we don't violate Intel's TLB

			 * Application note (317080) which says, while changing

			 * the page sizes, new and old translations should

			 * not differ with respect to page frame and

			 * attributes.

/*

 * Create PUD level page table mapping for physical addresses. The virtual

 * and physical address do not have to be aligned at this level. KASLR can

 * randomize virtual addresses up to this level.

 * It returns the last physical address mapped.

			/*

			 * If we are ok with PG_LEVEL_1G mapping, then we will

			 * use the existing mapping.

			 *

			 * Otherwise, we will split the gbpage mapping but use

			 * the same existing protection  bits except for large

			 * page, so that we don't violate Intel's TLB

			 * Application note (317080) which says, while changing

			 * the page sizes, new and old translations should

			 * not differ with respect to page frame and

			 * attributes.

/*

 * Create page table mapping for the physical memory for specific physical

 * addresses. Note that it can only be used to populate non-present entries.

 * The virtual and physical addresses have to be aligned on PMD level

 * down. It returns the last physical address mapped.

/*

 * This function is similar to kernel_physical_mapping_init() above with the

 * exception that it uses set_{pud,pmd}() instead of the set_{pud,pte}_safe()

 * when updating the mapping. The caller is responsible to flush the TLBs after

 * the function returns.

	/*

	 * clear the default setting with node 0

	 * note: don't use nodes_clear here, that is really clearing when

	 *	 numa support is not compiled in, and later node_set_state

	 *	 will not set it back.

/*

 * The unused vmemmap range, which was not yet memset(PAGE_UNUSED), ranges

 * from unused_pmd_start to next PMD_SIZE boundary.

	/*

	 * Clears (unused_pmd_start, PMD_END]

 Returns true if the PMD is completely unused and thus it can be freed */

	/*

	 * Flush the unused range cache to ensure that memchr_inv() will work

	 * for the whole range.

	/*

	 * As we expect to add in the same granularity as we remove, it's

	 * sufficient to mark only some piece used to block the memmap page from

	 * getting removed when removing some other adjacent memmap (just in

	 * case the first memmap never gets initialized e.g., because the memory

	 * block never gets onlined).

	/*

	 * We only optimize if the new used range directly follows the

	 * previously unused range (esp., when populating consecutive sections).

	/*

	 * If the range does not contiguously follows previous one, make sure

	 * to mark the unused range of the previous one so it can be removed.

	/*

	 * Could be our memmap page is filled with PAGE_UNUSED already from a

	 * previous remove. Make sure to reset it.

	/*

	 * Mark with PAGE_UNUSED the unused parts of the new memmap range

	/*

	 * We want to avoid memset(PAGE_UNUSED) when populating the vmemmap of

	 * consecutive sections. Remember for the last added PMD where the

	 * unused range begins.

/*

 * Memory hotplug specific functions

/*

 * After memory hotplug the variables max_pfn, max_low_pfn and high_memory need

 * updating.

 update max_pfn, max_low_pfn and high_memory */

 bootmem page has reserved flag */

 free a pte talbe */

 free a pmd talbe */

 free a pud talbe */

		/*

		 * We mapped [0,1G) memory as identity mapping when

		 * initializing, in arch/x86/kernel/head_64.S. These

		 * pagetables cannot be removed.

 For non-direct mapping, pages means nothing. */

 Call free_pte_table() in remove_pmd_table(). */

 Call free_pmd_table() in remove_pud_table(). */

		/*

		 * For 4-level page tables we do not want to free PUDs, but in the

		 * 5-level case we should free them. This code will have to change

		 * to adapt for boot-time switching between 4 and 5 level page tables.

 start and end are both virtual address. */

 CONFIG_MEMORY_HOTPLUG */

/*

 * Pre-allocates page-table pages for the vmalloc area in the kernel page-table.

 * Only the level which needs to be synchronized between all page-tables is

 * allocated because the synchronization can be expensive.

		/*

		 * The goal here is to allocate all possibly required

		 * hardware page tables pointed to by the top hardware

		 * level.

		 *

		 * On 4-level systems, the P4D layer is folded away and

		 * the above code does no preallocation.  Below, go down

		 * to the pud _software_ level to ensure the second

		 * hardware level is allocated on 4-level systems too.

	/*

	 * The pages have to be there now or they will be missing in

	 * process page-tables later.

 clear_bss() already clear the empty_zero_page */

 this will put all memory onto the freelists */

	/*

	 * Must be done after boot memory is put on freelist, because here we

	 * might set fields in deferred struct pages that have not yet been

	 * initialized, and memblock_free_all() initializes all the reserved

	 * deferred pages for us.

 Register memory areas for /proc/kcore */

	/*

	 * More CPUs always led to greater speedups on tested systems, up to

	 * all the nodes' CPUs.  Use all since the system is otherwise idle

	 * now.

	/*

	 * The rodata/data/bss/brk section (but not the kernel text!)

	 * should also be not-executable.

	 *

	 * We align all_end to PMD_SIZE because the existing mapping

	 * is a full PMD. If we would align _brk_end to PAGE_SIZE we

	 * split the PMD and the reminder between _brk_end and the end

	 * of the PMD will remain mapped executable.

	 *

	 * Any PMD which was setup after the one which covers _brk_end

	 * has been zapped already via cleanup_highmem().

/*

 * Block size is the minimum amount of memory which can be hotplugged or

 * hotremoved. It must be power of two and must be equal or larger than

 * MIN_MEMORY_BLOCK_SIZE.

 Amount of ram needed to start using large blocks */

 Adjustable memory block size */

 If memory block size has been set, then use it */

 Use regular block if RAM is smaller than MEM_SIZE_FOR_LARGE_BLOCK */

	/*

	 * Use max block size to minimize overhead on bare metal, where

	 * alignment for memory hotplug isn't a concern.

 Find the largest allowed block size that aligns to memory end */

/*

 * Initialise the sparsemem vmemmap using huge-pages at the PMD level.

 check to see if we have contiguous blocks */

 no fallback */

 SPDX-License-Identifier: GPL-2.0

/*

 * IA-32 Huge TLB Page Support for Kernel.

 *

 * Copyright (C) 2002, Rohit Seth <rohit.seth@intel.com>

 This is just for testing */

 hugetlb should be locked, and hence, prefaulted */

/*

 * pmd_huge() returns 1 if @pmd is hugetlb related entry, that is normal

 * hugetlb entry or non-present (migration or hwpoisoned) hugetlb entry.

 * Otherwise, returns 0.

	/*

	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area

	 * in the full address space.

	/*

	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area

	 * in the full address space.

	/*

	 * A failed mmap() very likely causes application failure,

	 * so fall back to the bottom-up function here. This scenario

	 * can happen with large stack limits and large mmap()

	 * allocations.

 No address checking. See comment at mmap_address_hint_valid() */

 CONFIG_HUGETLB_PAGE */

 With compaction or CMA we can allocate gigantic pages at runtime */

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0

	/*

	 * "userpte=nohigh" disables allocation of user pagetables in

	 * high memory.

	/*

	 * NOTE! For PAE, any changes to the top page-directory-pointer-table

	 * entries need a full cr3 reload to flush.

 CONFIG_PGTABLE_LEVELS > 4 */

 CONFIG_PGTABLE_LEVELS > 3 */

 CONFIG_PGTABLE_LEVELS > 2 */

	/* If the pgd points to a shared pagetable level (either the

	   ptes in non-PAE, or shared PMD in PAE), then just copy the

 list required to sync kernel mapping updates */

/*

 * List of all pgd's needed for non-PAE so it can invalidate entries

 * in both cached and uncached pgd's; not needed for PAE since the

 * kernel pmd is shared. If PAE were not to share the pmd a similar

 * tactic would be needed. This is essentially codepath-based locking

 * against pageattr.c; it is the unique case in which a valid change

 * of kernel pagetables can't be lazily synchronized by vmalloc faults.

 * vmalloc faults work because attached pagetables are never freed.

 * -- nyc

/*

 * In PAE mode, we need to do a cr3 reload (=tlb flush) when

 * updating the top-level pagetable entries to guarantee the

 * processor notices the update.  Since this is expensive, and

 * all 4 top-level entries are used almost immediately in a

 * new process's life, we just pre-populate them here.

 *

 * Also, if we're in a paravirt environment where the kernel pmd is

 * not shared between pagetables (!SHARED_KERNEL_PMDS), we allocate

 * and initialize the kernel pmds here.

/*

 * We allocate separate PMDs for the kernel part of the user page-table

 * when PTI is enabled. We need them to map the per-process LDT into the

 * user-space page-table.

	/* Note: almost everything apart from _PAGE_PRESENT is

	/*

	 * According to Intel App note "TLBs, Paging-Structure Caches,

	 * and Their Invalidation", April 2007, document 317080-001,

	 * section 8.1: in PAE mode we explicitly have to flush the

	 * TLB via cr3 if the top-level pgd is changed...

 !CONFIG_X86_PAE */

 No need to prepopulate any pagetable entries in non-PAE modes. */

 CONFIG_X86_PAE */

/*

 * Mop up any pmd pages which may still be attached to the pgd.

 * Normally they will be freed by munmap/exit_mmap, but any pmd we

 * preallocate which never got a corresponding vma will need to be

 * freed manually.

 Work around gcc-3.4.x bug */

/*

 * Xen paravirt assumes pgd table should be in one page. 64 bit kernel also

 * assumes that pgd should be in one page.

 *

 * But kernel with PAE paging that is not running as a Xen domain

 * only needs to allocate 32 bytes for pgd instead of one page.

	/*

	 * When PAE kernel is running as a Xen domain, it does not use

	 * shared kernel pmd. And this requires a whole page for pgd.

	/*

	 * when PAE kernel is not running as a Xen domain, it uses

	 * shared kernel pmd. Shared kernel pmd does not require a whole

	 * page for pgd. We are able to just allocate a 32-byte for pgd.

	 * During boot time, we create a 32-byte slab for pgd table allocation.

	/*

	 * If no SHARED_KERNEL_PMD, PAE kernel is running as a Xen domain.

	 * We allocate one page for pgd.

	/*

	 * Now PAE kernel is not running as a Xen domain. We can allocate

	 * a 32-byte slab for pgd to save memory space.

 CONFIG_X86_PAE */

	/*

	 * Make sure that pre-populating the pmds is atomic with

	 * respect to anything walking the pgd_list, so that they

	 * never see a partially populated pgd.

/*

 * Used to set accessed or dirty bits in the page table entries

 * on other architectures. On x86, the accessed and dirty bits

 * are tracked by hardware. However, do_wp_page calls this function

 * to also make the pte writeable at the same time the dirty bit is

 * set. In that case we do actually need to write the PTE.

		/*

		 * We had a write-protection fault here and changed the pmd

		 * to to more permissive. No need to flush the TLB for that,

		 * #PF is architecturally guaranteed to do that and in the

		 * worst-case we'll generate a spurious fault.

		/*

		 * We had a write-protection fault here and changed the pud

		 * to to more permissive. No need to flush the TLB for that,

		 * #PF is architecturally guaranteed to do that and in the

		 * worst-case we'll generate a spurious fault.

	/*

	 * On x86 CPUs, clearing the accessed bit without a TLB flush

	 * doesn't cause data corruption. [ It could cause incorrect

	 * page aging and the (mistaken) reclaim of hot pages, but the

	 * chance of that should be relatively low. ]

	 *

	 * So as a performance optimization don't flush the TLB when

	 * clearing the accessed bit, it will eventually be flushed by

	 * a context switch or a VM operation anyway. [ In the rare

	 * event of it not getting flushed for a long time the delay

	 * shouldn't really matter because there's no real memory

	 * pressure for swapout to react to. ]

/**

 * reserve_top_address - reserves a hole in the top of kernel address space

 * @reserve - size of hole to reserve

 *

 * Can be used to relocate the fixmap area and poke a hole in the top

 * of kernel address space to make room for a hypervisor.

       /*

	* Ensure that the static initial page tables are covering the

	* fixmap completely.

 enum fixed_addresses */ idx,

 Sanitize 'prot' against any unsupported bits: */

/**

 * p4d_set_huge - setup kernel P4D mapping

 *

 * No 512GB pages yet -- always return 0

/**

 * p4d_clear_huge - clear kernel P4D mapping when it is set

 *

 * No 512GB pages yet -- always return 0

/**

 * pud_set_huge - setup kernel PUD mapping

 *

 * MTRRs can override PAT memory types with 4KiB granularity. Therefore, this

 * function sets up a huge page only if any of the following conditions are met:

 *

 * - MTRRs are disabled, or

 *

 * - MTRRs are enabled and the range is completely covered by a single MTRR, or

 *

 * - MTRRs are enabled and the corresponding MTRR memory type is WB, which

 *   has no effect on the requested PAT memory type.

 *

 * Callers should try to decrease page size (1GB -> 2MB -> 4K) if the bigger

 * page mapping attempt fails.

 *

 * Returns 1 on success and 0 on failure.

 Bail out if we are we on a populated non-leaf entry: */

/**

 * pmd_set_huge - setup kernel PMD mapping

 *

 * See text over pud_set_huge() above.

 *

 * Returns 1 on success and 0 on failure.

 Bail out if we are we on a populated non-leaf entry: */

/**

 * pud_clear_huge - clear kernel PUD mapping when it is set

 *

 * Returns 1 on success and 0 on failure (no PUD map is found).

/**

 * pmd_clear_huge - clear kernel PMD mapping when it is set

 *

 * Returns 1 on success and 0 on failure (no PMD map is found).

/**

 * pud_free_pmd_page - Clear pud entry and free pmd page.

 * @pud: Pointer to a PUD.

 * @addr: Virtual address associated with pud.

 *

 * Context: The pud range has been unmapped and TLB purged.

 * Return: 1 if clearing the entry succeeded. 0 otherwise.

 *

 * NOTE: Callers must allow a single page allocation.

 INVLPG to clear all paging-structure caches */

/**

 * pmd_free_pte_page - Clear pmd entry and free pte page.

 * @pmd: Pointer to a PMD.

 * @addr: Virtual address associated with pmd.

 *

 * Context: The pmd range has been unmapped and TLB purged.

 * Return: 1 if clearing the entry succeeded. 0 otherwise.

 INVLPG to clear all paging-structure caches */

 !CONFIG_X86_64 */

/*

 * Disable free page handling on x86-PAE. This assures that ioremap()

 * does not update sync'd pmd entries. See vmalloc_sync_one().

 CONFIG_X86_64 */

 CONFIG_HAVE_ARCH_HUGE_VMAP */

 SPDX-License-Identifier: GPL-2.0

/*

 * AMD NUMA support.

 * Discover the memory map and associated nodes.

 *

 * This version reads it directly from the AMD northbridge.

 *

 * Copyright 2002,2003 Andi Kleen, SuSE Labs.

 Could sort here, but pun for now. Should not happen anyroads. */

	/*

	 * We seem to have valid NUMA configuration.  Map apicids to nodes

	 * using the coreid bits from early_identify_cpu.

	/*

	 * get boot-time SMP configuration:

 SPDX-License-Identifier: GPL-2.0

/* Support for MMIO probes.

 * Benefit many code from kprobes

 * (C) 2002 Louis Zhuang <louis.zhuang@intel.com>.

 *     2007 Alexander Eichner

 *     2008 Pekka Paalanen <pq@iki.fi>

 the requested address */

 page presence prior to arming */

	/*

	 * Number of times this page has been registered as a part

	 * of a probe. If zero, page is disarmed and this may be freed.

	 * Used only by writers (RCU) and post_kmmio_handler().

	 * Protected by kmmio_lock, when linked into kmmio_page_table.

 Protected by kmmio_lock */

 Read-protected by RCU, write-protected by kmmio_lock. */

 Accessed per-cpu */

/*

 * this is basically a dynamic stabbing problem:

 * Could use the existing prio tree code or

 * Possible better implementations:

 * The Interval Skip List: A Data Structure for Finding All Intervals That

 * Overlap a Point (might be simple)

 * Space Efficient Dynamic Stabbing with Fast Queries - Mikkel Thorup

 Get the kmmio at this addr (if any). You must be holding RCU read lock. */

 You must be holding RCU read lock. */

 Presume this has been called with clear==true previously */

 Nothing should care about address */

 Presume this has been called with clear==true previously */

/*

 * Mark the given page as not present. Access to it will trigger a fault.

 *

 * Struct kmmio_fault_page is protected by RCU and kmmio_lock, but the

 * protection is ignored here. RCU read lock is assumed held, so the struct

 * will not disappear unexpectedly. Furthermore, the caller must guarantee,

 * that double arming the same virtual address (page) cannot occur.

 *

 * Double disarming on the other hand is allowed, and may occur when a fault

 * and mmiotrace shutdown happen simultaneously.

* Restore the given page to saved presence state. */

/*

 * This is being called from do_page_fault().

 *

 * We may be in an interrupt or a critical section. Also prefecthing may

 * trigger a page fault. We may be in the middle of process switch.

 * We cannot take any locks, because we could be executing especially

 * within a kmmio critical section.

 *

 * Local interrupts are disabled, so preemption cannot happen.

 * Do not enable interrupts, do not sleep, and watch out for other CPUs.

/*

 * Interrupts are disabled on entry as trap3 is an interrupt gate

 * and they remain disabled throughout this function.

 default to fault not handled */

	/*

	 * Preemption is now disabled to prevent process switch during

	 * single stepping. We can only handle one active kmmio trace

	 * per cpu, so ensure that we finish it before something else

	 * gets to run. We also hold the RCU read lock over single

	 * stepping to avoid looking up the probe and kmmio_fault_page

	 * again.

		/*

		 * Either this page fault is not caused by kmmio, or

		 * another CPU just pulled the kmmio probe from under

		 * our feet. The latter case should not be possible.

			/*

			 * A second fault on the same page means some other

			 * condition needs handling by do_page_fault(), the

			 * page really not being present is the most common.

			/*

			 * Prevent overwriting already in-flight context.

			 * This should not happen, let's hope disarming at

			 * least prevents a panic.

	/*

	 * Enable single-stepping and disable interrupts for the faulting

	 * context. Local interrupts must not get enabled during stepping.

 Now we set present bit in PTE and single step. */

	/*

	 * If another cpu accesses the same page while we are stepping,

	 * the access will not be caught. It will simply succeed and the

	 * only downside is we lose the event. If this becomes a problem,

	 * the user should drop to single cpu before tracing.

 fault handled */

/*

 * Interrupts are disabled on entry as trap1 is an interrupt gate

 * and they remain disabled throughout this function.

 * This must always get called as the pair to kmmio_handler().

		/*

		 * debug traps without an active context are due to either

		 * something external causing them (f.e. using a debugger while

		 * mmio tracing enabled), or erroneous behaviour

 Prevent racing against release_kmmio_fault_page(). */

 These were acquired in kmmio_handler(). */

	/*

	 * if somebody else is singlestepping across a probe point, flags

	 * will have TF set, in which case, continue the remaining processing

	 * of do_debug, as if this is not a probe hit.

 You must be holding kmmio_lock. */

 You must be holding kmmio_lock. */

/*

 * With page-unaligned ioremaps, one or two armed pages may contain

 * addresses from outside the intended mapping. Events for these addresses

 * are currently silently dropped. The events may result only from programming

 * mistakes by accessing addresses before the beginning or past the end of a

 * mapping.

	/*

	 * XXX: What should I do here?

	 * Here was a call to global_flush_tlb(), but it does not exist

	 * anymore. It seems it's not needed after all.

 This is the real RCU destroy call. */

/*

 * Remove a kmmio probe. You have to synchronize_rcu() before you can be

 * sure that the callbacks will not be called anymore. Only after that

 * you may actually release your struct kmmio_probe.

 *

 * Unregistering a kmmio fault page has three steps:

 * 1. release_kmmio_fault_page()

 *    Disarm the page, wait a grace period to let all faults finish.

 * 2. remove_kmmio_fault_pages()

 *    Remove the pages from kmmio_page_table.

 * 3. rcu_free_kmmio_fault_pages()

 *    Actually free the kmmio_fault_page structs as with RCU.

	/*

	 * This is not really RCU here. We have just disarmed a set of

	 * pages so that they cannot trigger page faults anymore. However,

	 * we cannot remove the pages from kmmio_page_table,

	 * because a probe hit might be in flight on another CPU. The

	 * pages are collected into a list, and they will be removed from

	 * kmmio_page_table when it is certain that no probe hit related to

	 * these pages can be in flight. RCU grace period sounds like a

	 * good choice.

	 *

	 * If we removed the pages too early, kmmio page fault handler might

	 * not find the respective kmmio_fault_page and determine it's not

	 * a kmmio fault, when it actually is. This would lead to madness.

			/*

			 * Reset the BS bit in dr6 (pointed by args->err) to

			 * denote completion of processing

 SPDX-License-Identifier: GPL-2.0-only

/*

 * AMD Memory Encryption Support

 *

 * Copyright (C) 2016 Advanced Micro Devices, Inc.

 *

 * Author: Tom Lendacky <thomas.lendacky@amd.com>

/*

 * Since SME related variables are set early in the boot process they must

 * reside in the .data section so as not to be zeroed out when the .bss

 * section is later cleared.

 Buffer used for early in-place encryption by BSP, no locking needed */

/*

 * This routine does not change the underlying encryption setting of the

 * page(s) that map this memory. It assumes that eventually the memory is

 * meant to be accessed as either encrypted or decrypted but the contents

 * are currently not in the desired state.

 *

 * This routine follows the steps outlined in the AMD64 Architecture

 * Programmer's Manual Volume 2, Section 7.10.8 Encrypt-in-Place.

	/*

	 * There are limited number of early mapping slots, so map (at most)

	 * one page at time.

		/*

		 * Create mappings for the current and desired format of

		 * the memory. Use a write-protected mapping for the source.

		/*

		 * If a mapping can't be obtained to perform the operation,

		 * then eventual access of that area in the desired mode

		 * will cause a crash.

		/*

		 * Use a temporary buffer, of cache-line multiple size, to

		 * avoid data corruption as documented in the APM.

 Use early_pmd_flags but remove the encryption mask */

 Get the command line address before unmapping the real_mode_data */

 Get the command line address after mapping the real_mode_data */

 Update the protection map with memory encryption mask */

	/*

	 * For SEV, all DMA has to occur via shared/unencrypted pages.

	 * SEV uses SWIOTLB to make this happen without changing device

	 * drivers. However, depending on the workload being run, the

	 * default 64MB of SWIOTLB may not be enough and SWIOTLB may

	 * run out of buffers for DMA, resulting in I/O errors and/or

	 * performance degradation especially with high I/O workloads.

	 *

	 * Adjust the default size of SWIOTLB for SEV guests using

	 * a percentage of guest memory for SWIOTLB buffers.

	 * Also, as the SWIOTLB bounce buffer memory is allocated

	 * from low memory, ensure that the adjusted size is within

	 * the limits of low available memory.

	 *

	 * The percentage of guest memory used here for SWIOTLB buffers

	 * is more of an approximation of the static adjustment which

	 * 64MB for <1G, and ~128M to 256M for 1G-to-4G, i.e., the 6%

 If prot is same then do nothing. */

	/*

	 * We are going to perform in-place en-/decryption and change the

	 * physical page attribute from C=1 to C=0 or vice versa. Flush the

	 * caches to ensure that data gets accessed with the correct C-bit.

 Encrypt/decrypt the contents in-place */

 Change the page encryption mask. */

		/*

		 * Check whether we can change the large page in one go.

		 * We request a split when the address is not aligned and

		 * the number of pages to set/clear encryption bit is smaller

		 * than the number of pages in the large page.

		/*

		 * The virtual address is part of a larger page, create the next

		 * level page table mapping (4K or 2M). If it is part of a 2M

		 * page then we request a split of the large page into 4K

		 * chunks. A 1GB large page is split into 2M pages, resp.

		/*

		 * kernel_physical_mapping_change() does not flush the TLBs, so

		 * a TLB flush is required after we exit from the for loop.

 Override for DMA direct allocation check - ARCH_HAS_FORCE_DMA_UNENCRYPTED */

	/*

	 * For SEV, all DMA must be to unencrypted addresses.

	/*

	 * For SME, all DMA must be to unencrypted addresses if the

	 * device does not support DMA to addresses that include the

	 * encryption mask.

	/*

	 * The unused memory range was mapped decrypted, change the encryption

	 * attribute from decrypted to encrypted before freeing it.

 Secure Memory Encryption */

		/*

		 * SME is mutually exclusive with any of the SEV

		 * features below.

 Secure Encrypted Virtualization */

 Encrypted Register State */

 Architecture __weak replacement functions */

 Call into SWIOTLB to update the SWIOTLB DMA buffers */

	/*

	 * With SEV, we need to unroll the rep string I/O instructions,

	 * but SEV-ES supports them through the #VC handler.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Written by Pekka Paalanen, 2008-2009 <pq@iki.fi>

/*

 * Tests how mmiotrace behaves in face of multiple ioremap / iounmaps in

 * a short time. We had a bug in deferred freeing procedure which tried

 * to free this region multiple times (ioremap can reuse the same address

 * for many mappings).

 Force freeing. If it will crash we will know why. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Helper routines for building identity mapping page tables. This is

 * included by both the compressed kernel and the regular kernel.

 Set the default pagetable flags if not supplied */

 Filter out unsupported __PAGE_KERNEL_* bits: */

			/*

			 * With p4d folded, pgd is equal to p4d.

			 * The pgd entry has to point to the pud page table in this case.

 SPDX-License-Identifier: GPL-2.0

/*

 * This file implements KASLR memory randomization for x86_64. It randomizes

 * the virtual address space of kernel memory regions (physical memory

 * mapping, vmalloc & vmemmap) for x86_64. This security feature mitigates

 * exploits relying on predictable kernel addresses.

 *

 * Entropy is generated using the KASLR early boot functions now shared in

 * the lib directory (originally written by Kees Cook). Randomization is

 * done on PGD & P4D/PUD page table levels to increase possible addresses.

 * The physical memory mapping code was adapted to support P4D/PUD level

 * virtual addresses. This implementation on the best configuration provides

 * 30,000 possible virtual addresses in average for each memory region.

 * An additional low memory page is used to ensure each CPU can start with

 * a PGD aligned virtual address (for realmode).

 *

 * The order of each memory region is not changed. The feature looks at

 * the available space for the regions based on different configuration

 * options and randomizes the base and space between each. The size of the

 * physical memory mapping is the available physical memory.

/*

 * The end address could depend on more configuration options to make the

 * highest amount of space for randomization available, but that's too hard

 * to keep straight and caused issues already.

/*

 * Memory regions randomized by KASLR (except modules that use a separate logic

 * earlier during boot). The list is ordered based on virtual addresses. This

 * order is kept after randomization.

 Get size in bytes used by the memory region */

 Initialize base and padding for each memory region randomized with KASLR */

	/*

	 * These BUILD_BUG_ON checks ensure the memory layout is consistent

	 * with the vaddr_start/vaddr_end variables. These checks are very

	 * limited....

	/*

	 * Update Physical memory mapping to available and

	 * add padding if needed (especially for memory hotplug support).

 Adapt physical memory region size based on available memory */

	/*

	 * Calculate the vmemmap region size in TBs, aligned to a TB

	 * boundary.

 Calculate entropy available between regions */

		/*

		 * Select a random virtual address using the extra entropy

		 * available.

		/*

		 * Jump the region and add a minimum padding based on

		 * randomization alignment.

	/*

	 * There are two mappings for the low 1MB area, the direct mapping

	 * and the 1:1 mapping for the real mode trampoline:

	 *

	 * Direct mapping: virt_addr = phys_addr + PAGE_OFFSET

	 * 1:1 mapping:    virt_addr = phys_addr

 SPDX-License-Identifier: GPL-2.0-only

	/*

	 * Range covering the highest possible canonical userspace address

	 * as well as non-canonical address range. For the canonical range

	 * we also need to include the userspace guard page.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Handler for when we fail to restore a task's FPU state.  We should never get

 * here because the FPU state of a task using the FPU (task->thread.fpu.state)

 * should always be valid.  However, past bugs have allowed userspace to set

 * reserved bits in the XSAVE area using PTRACE_SETREGSET or sys_rt_sigreturn().

 * These caused XRSTOR to fail when switching to the task, leaking the FPU

 * registers of the task previously executing on the CPU.  Mitigate this class

 * of vulnerability by restoring from the initial state (essentially, zeroing

 * out all the FPU registers) if we can't restore from the task's FPU state.

 Pretend that the read succeeded and returned 0. */

 Pretend that the write succeeded. */

 Restricted version used during very early boot */

 Ignore early NMIs. */

	/*

	 * Old CPUs leave the high bits of CS on the stack

	 * undefined.  I'm not sure which CPUs do this, but at least

	 * the 486 DX works this way.

	 * Xen pv domains are not using the default __KERNEL_CS.

	/*

	 * The full exception fixup machinery is available as soon as

	 * the early IDT is loaded.  This means that it is the

	 * responsibility of extable users to either function correctly

	 * when handlers are invoked early or to simply avoid causing

	 * exceptions before they're ready to handle them.

	 *

	 * This is better than filtering which handlers can be used,

	 * because refusing to call a handler here is guaranteed to

	 * result in a hard-to-debug panic.

	 *

	 * Keep in mind that not all vectors actually get here.  Early

	 * page faults, for example, are special.

 Skip the ud2. */

		/*

		 * If this was a BUG and report_bug returns or if this

		 * was just a normal #UD, we want to continue onward and

		 * crash.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Re-map IO memory to kernel address space so that we can access it.

 * This is needed for high PCI addresses that aren't mapped in the

 * 640k-1MB IO memory area on PC's

 *

 * (C) Copyright 1995 1996 Linus Torvalds

/*

 * Descriptor controlling ioremap() behavior.

/*

 * Fix up the linear direct mapping of the kernel to avoid cache attribute

 * conflicts.

 Does the range (or a subset of) contain normal RAM? */

/*

 * In a SEV guest, NONE and RESERVED should not be mapped encrypted because

 * there the whole memory is already encrypted.

/*

 * The EFI runtime services data area is not covered by walk_mem_res(), but must

 * be mapped encrypted when SEV is active.

/*

 * To avoid multiple resource walks, this function walks resources marked as

 * IORESOURCE_MEM and IORESOURCE_BUSY and looking for system RAM and/or a

 * resource described not as IORES_DESC_NONE (e.g. IORES_DESC_ACPI_TABLES).

 *

 * After that, deal with misc other ranges in __ioremap_check_other() which do

 * not fall into the above category.

/*

 * Remap an arbitrary physical address space into the kernel virtual

 * address space. It transparently creates kernel huge I/O mapping when

 * the physical address is aligned by a huge page size (1GB or 2MB) and

 * the requested size is at least the huge page size.

 *

 * NOTE: MTRRs can override PAT memory types with a 4KB granularity.

 * Therefore, the mapping code falls back to use a smaller page toward 4KB

 * when a mapping range is covered by non-WB type of MTRRs.

 *

 * NOTE! We need to allow non-page-aligned mappings too: we will obviously

 * have to convert them into an offset in a page-aligned mapping, but the

 * caller shouldn't need to know that small detail.

 Don't allow wraparound or zero size */

	/*

	 * Don't allow anybody to remap normal RAM that we're using..

	/*

	 * Mappings have to be page-aligned

	/*

	 * If the page being mapped is in memory and SEV is active then

	 * make sure the memory encryption attribute is enabled in the

	 * resulting mapping.

	/*

	 * Ok, go for it..

	/*

	 * Check if the request spans more than any BAR in the iomem resource

	 * tree.

/**

 * ioremap     -   map bus memory into CPU space

 * @phys_addr:    bus address of the memory

 * @size:      size of the resource to map

 *

 * ioremap performs a platform specific sequence of operations to

 * make bus memory CPU accessible via the readb/readw/readl/writeb/

 * writew/writel functions and the other mmio helpers. The returned

 * address is not guaranteed to be usable directly as a virtual

 * address.

 *

 * This version of ioremap ensures that the memory is marked uncachable

 * on the CPU as well as honouring existing caching rules from things like

 * the PCI bus. Note that there are other caches and buffers on many

 * busses. In particular driver authors should read up on PCI writes

 *

 * It's useful if some control registers are in such an area and

 * write combining or read caching is not desirable:

 *

 * Must be freed with iounmap.

	/*

	 * Ideally, this should be:

	 *	pat_enabled() ? _PAGE_CACHE_MODE_UC : _PAGE_CACHE_MODE_UC_MINUS;

	 *

	 * Till we fix all X drivers to use ioremap_wc(), we will use

	 * UC MINUS. Drivers that are certain they need or can already

	 * be converted over to strong UC can use ioremap_uc().

/**

 * ioremap_uc     -   map bus memory into CPU space as strongly uncachable

 * @phys_addr:    bus address of the memory

 * @size:      size of the resource to map

 *

 * ioremap_uc performs a platform specific sequence of operations to

 * make bus memory CPU accessible via the readb/readw/readl/writeb/

 * writew/writel functions and the other mmio helpers. The returned

 * address is not guaranteed to be usable directly as a virtual

 * address.

 *

 * This version of ioremap ensures that the memory is marked with a strong

 * preference as completely uncachable on the CPU when possible. For non-PAT

 * systems this ends up setting page-attribute flags PCD=1, PWT=1. For PAT

 * systems this will set the PAT entry for the pages as strong UC.  This call

 * will honor existing caching rules from things like the PCI bus. Note that

 * there are other caches and buffers on many busses. In particular driver

 * authors should read up on PCI writes.

 *

 * It's useful if some control registers are in such an area and

 * write combining or read caching is not desirable:

 *

 * Must be freed with iounmap.

/**

 * ioremap_wc	-	map memory into CPU space write combined

 * @phys_addr:	bus address of the memory

 * @size:	size of the resource to map

 *

 * This version of ioremap ensures that the memory is marked write combining.

 * Write combining allows faster writes to some hardware devices.

 *

 * Must be freed with iounmap.

/**

 * ioremap_wt	-	map memory into CPU space write through

 * @phys_addr:	bus address of the memory

 * @size:	size of the resource to map

 *

 * This version of ioremap ensures that the memory is marked write through.

 * Write through stores data into memory while keeping the cache up-to-date.

 *

 * Must be freed with iounmap.

/**

 * iounmap - Free a IO remapping

 * @addr: virtual address from ioremap_*

 *

 * Caller must ensure there is only one unmapping for the same pointer.

	/*

	 * The PCI/ISA range special-casing was removed from __ioremap()

	 * so this check, in theory, can be removed. However, there are

	 * cases where iounmap() is called for addresses not obtained via

	 * ioremap() (vga16fb for example). Add a warning so that these

	 * cases can be caught and fixed.

	/* Use the vm area unlocked, assuming the caller

	   ensures there isn't another iounmap for the same address

	   in parallel. Reuse of the virtual address is prevented by

	   leaving it in the global lists until we're done with it.

 Finally remove it */

/*

 * Convert a physical pointer to a virtual kernel pointer for /dev/mem

 * access

 memremap() maps if RAM, otherwise falls back to ioremap() */

 Only add the offset on success and return NULL if memremap() failed */

/*

 * Examine the physical address to determine if it is an area of memory

 * that should be mapped decrypted.  If the memory is not part of the

 * kernel usable area it was accessed and created decrypted, so these

 * areas should be mapped decrypted. And since the encryption key can

 * change across reboots, persistent memory should also be mapped

 * decrypted.

 *

 * If SEV is active, that implies that BIOS/UEFI also ran encrypted so

 * only persistent memory should be mapped decrypted.

	/*

	 * Check if the address is part of a persistent memory region.

	 * This check covers areas added by E820, EFI and ACPI.

	/*

	 * Check if the non-volatile attribute is set for an EFI

	 * reserved area.

 Check if the address is outside kernel usable area */

 For SEV, these areas are encrypted */

/*

 * Examine the physical address to determine if it is EFI data. Check

 * it against the boot params structure and EFI tables and memory types.

 Check if the address is part of EFI boot/runtime data */

/*

 * Examine the physical address to determine if it is boot data by checking

 * it against the boot params setup_data chain.

/*

 * Examine the physical address to determine if it is boot data by checking

 * it against the boot params setup_data chain (early boot version).

/*

 * Architecture function to determine if RAM remap is allowed. By default, a

 * RAM remap will map the data as encrypted. Determine if a RAM remap should

 * not be done so that the data will be mapped decrypted.

/*

 * Architecture override of __weak function to adjust the protection attributes

 * used when remapping memory. By default, early_memremap() will map the data

 * as encrypted. Determine if an encrypted mapping should not be done and set

 * the appropriate protection attributes.

 Remap memory with encryption */

/*

 * Remap memory with encryption and write-protected - cannot be called

 * before pat_init() is called

 Remap memory without encryption */

/*

 * Remap memory without encryption and write-protected - cannot be called

 * before pat_init() is called

 CONFIG_AMD_MEM_ENCRYPT */

 Don't assume we're using swapper_pg_dir at this point */

	/*

	 * The boot-ioremap range spans multiple pmds, for which

	 * we are not prepared:

 Sanitize 'prot' against any unsupported bits: */

 SPDX-License-Identifier: GPL-2.0

 cpu_feature_enabled() cannot be used this early */

 See comment in kasan_init() */

		/*

		 * With folded p4d, pgd_clear() is nop, use p4d_clear()

		 * instead.

 See comment in kasan_init() */

		/*

		 * we need to populate p4ds to be synced when running in

		 * four level mode - see sync_global_pgds_l4()

 Mask out unsupported __PAGE_KERNEL bits: */

	/*

	 * We use the same shadow offset for 4- and 5-level paging to

	 * facilitate boot-time switching between paging modes.

	 * As result in 5-level paging mode KASAN_SHADOW_START and

	 * KASAN_SHADOW_END are not aligned to PGD boundary.

	 *

	 * KASAN_SHADOW_START doesn't share PGD with anything else.

	 * We claim whole PGD entry to make things easier.

	 *

	 * KASAN_SHADOW_END lands in the last PGD entry and it collides with

	 * bunch of things like kernel code, modules, EFI mapping, etc.

	 * We need to take extra steps to not overwrite them.

	/*

	 * If we're in full vmalloc mode, don't back vmalloc space with early

	 * shadow pages. Instead, prepopulate pgds/p4ds so they are synced to

	 * the global table and we can populate the lower levels on demand.

	/*

	 * kasan_early_shadow_page has been used as early shadow memory, thus

	 * it may contain some garbage. Now we can clear and write protect it,

	 * since after the TLB flush no one should write to it.

 Flush TLBs again to be sure that write protection applied. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Debug helper to dump the current kernel pagetables of the system

 * so that we can see what the various memory ranges are set to.

 *

 * (C) Copyright 2008 Intel Corporation

 *

 * Author: Arjan van de Ven <arjan@linux.intel.com>

/*

 * The dumper groups pagetable entries of the same type into one, and for

 * that it needs to keep some state when walking, and flush this state

 * when a "break" in the continuity is found.

 Address space markers hints */

	/*

	 * These fields get initialized with the (dynamic)

	 * KASAN_SHADOW_{START,END} values in pt_dump_init().

 CONFIG_X86_64 */

 !CONFIG_X86_64 */

 Multipliers for offsets within the PTEs */

/*

 * Print a readable form of a pgprot_t to the seq_file

 Not present */

 Bit 7 has a different meaning on level 3 vs 4 */

	/*

	 * If PCI BIOS is enabled, the PCI BIOS area is forced to WX.

	 * Inform about it, but avoid the warning.

 Account the WX pages */

/*

 * This function gets called on a break in a continuous series

 * of PTE entries; the next one is different so we need to

 * print what we collected so far.

	/*

	 * If we have a "break" in the series, we need to flush the state that

	 * we have now. "break" is either changing perms, levels or

	 * address space marker.

 First entry */

		/*

		 * Now print the actual finished series

		/*

		 * We print markers for special areas of address space,

		 * such as the start of vmalloc space etc.

		 * This helps in the interpretation.

	/*

	 * Various markers are not compile-time constants, so assign them

	 * here.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Fault Injection Test harness (FI)

 *  Copyright (C) Intel Crop.

/*  Id: pf_in.c,v 1.1.1.1 2002/11/12 05:56:32 brlock Exp

 *  Copyright by Intel Crop., 2002

 *  Louis Zhuang (louis.zhuang@intel.com)

 *

 *  Bjorn Steinbrink (B.Steinbrink@gmx.de), 2007

 struct pt_regs */

 IA32 Manual 3, 2-1 */

 IA32 Manual 3, 3-432*/

 IA32 Manual 3, 3-432*/

 not __i386__ */

 REX Prefixes */

 AMD64 Manual 3, Appendix A*/

 8 bit only */

 16 bit only */

 16 or 32 bit */

 16, 32 or 64 bit */

 not __i386__ */

 0x0F is extension instruction */

/*

 * Define register ident in mod/rm byte.

 * Note: these are NOT the same as in ptrace-abi.h.

		/*

		 * If REX prefix exists, access low bytes of SI etc.

		 * instead of AH etc.

 for STOS, source register is fixed */

 if r/m is 5 we have a 32 disp (IA32 Manual 3, Table 2-2)  */

 AMD64: XXX Check for address size prefix? */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * AMD Memory Encryption Support

 *

 * Copyright (C) 2016 Advanced Micro Devices, Inc.

 *

 * Author: Tom Lendacky <thomas.lendacky@amd.com>

/*

 * Since we're dealing with identity mappings, physical and virtual

 * addresses are the same, so override these defines which are ultimately

 * used by the headers in misc.h.

/*

 * Special hack: we have to be careful, because no indirections are

 * allowed here, and paravirt_ops is a kind of one. As it will only run in

 * baremetal anyway, we just keep it from happening. (This list needs to

 * be extended when new paravirt and debugging variants are added.)

/*

 * This code runs before CPU feature bits are set. By default, the

 * pgtable_l5_enabled() function uses bit X86_FEATURE_LA57 to determine if

 * 5-level paging is active, so that won't work here. USE_EARLY_PGTABLE_L5

 * is provided to handle this situation and, instead, use a variable that

 * has been set by the early boot code.

/*

 * This work area lives in the .init.scratch section, which lives outside of

 * the kernel proper. It is sized to hold the intermediate copy buffer and

 * more than enough pagetable pages.

 *

 * By using this section, the kernel can be encrypted in place and it

 * avoids any possibility of boot parameters or initramfs images being

 * placed such that the in-place encryption logic overwrites them.  This

 * section is 2MB aligned to allow for simple pagetable setup using only

 * PMD entries (see vmlinux.lds.S).

 Save original end value since we modify the struct value */

 If start is not 2MB aligned, create PTE entries */

 Create PMD entries */

 If end is not 2MB aligned, create PTE entries */

	/*

	 * Perform a relatively simplistic calculation of the pagetable

	 * entries that are needed. Those mappings will be covered mostly

	 * by 2MB PMD entries so we can conservatively calculate the required

	 * number of P4D, PUD and PMD structures needed to perform the

	 * mappings.  For mappings that are not 2MB aligned, PTE mappings

	 * would be needed for the start and end portion of the address range

	 * that fall outside of the 2MB alignment.  This results in, at most,

	 * two extra pages to hold PTE entries for each range that is mapped.

	 * Incrementing the count for each covers the case where the addresses

	 * cross entries.

 PGDIR_SIZE is equal to P4D_SIZE on 4-level machine. */

	/*

	 * Now calculate the added pagetable structures needed to populate

	 * the new pagetables.

	/*

	 * This is early code, use an open coded check for SME instead of

	 * using cc_platform_has(). This eliminates worries about removing

	 * instrumentation or checking boot_cpu_data in the cc_platform_has()

	 * function.

	/*

	 * Prepare for encrypting the kernel and initrd by building new

	 * pagetables with the necessary attributes needed to encrypt the

	 * kernel in place.

	 *

	 *   One range of virtual addresses will map the memory occupied

	 *   by the kernel and initrd as encrypted.

	 *

	 *   Another range of virtual addresses will map the memory occupied

	 *   by the kernel and initrd as decrypted and write-protected.

	 *

	 *     The use of write-protect attribute will prevent any of the

	 *     memory from being cached.

 Physical addresses gives us the identity mapped virtual addresses */

	/*

	 * We're running identity mapped, so we must obtain the address to the

	 * SME encryption workarea using rip-relative addressing.

	/*

	 * Calculate required number of workarea bytes needed:

	 *   executable encryption area size:

	 *     stack page (PAGE_SIZE)

	 *     encryption routine page (PAGE_SIZE)

	 *     intermediate copy buffer (PMD_PAGE_SIZE)

	 *   pagetable structures for the encryption of the kernel

	 *   pagetable structures for workarea (in case not currently mapped)

	/*

	 * One PGD for both encrypted and decrypted mappings and a set of

	 * PUDs and PMDs for each of the encrypted and decrypted mappings.

 PUDs and PMDs needed in the current pagetables for the workarea */

	/*

	 * The total workarea includes the executable encryption area and

	 * the pagetable area. The start of the workarea is already 2MB

	 * aligned, align the end of the workarea on a 2MB boundary so that

	 * we don't try to create/allocate PTE entries from the workarea

	 * before it is mapped.

	/*

	 * Set the address to the start of where newly created pagetable

	 * structures (PGDs, PUDs and PMDs) will be allocated. New pagetable

	 * structures are created when the workarea is added to the current

	 * pagetables and when the new encrypted and decrypted kernel

	 * mappings are populated.

	/*

	 * Make sure the current pagetable structure has entries for

	 * addressing the workarea.

 Flush the TLB - no globals so cr3 is enough */

	/*

	 * A new pagetable structure is being built to allow for the kernel

	 * and initrd to be encrypted. It starts with an empty PGD that will

	 * then be populated with new PUDs and PMDs as the encrypted and

	 * decrypted kernel mappings are created.

	/*

	 * A different PGD index/entry must be used to get different

	 * pagetable entries for the decrypted mapping. Choose the next

	 * PGD index and convert it to a virtual address to be used as

	 * the base of the mapping.

 Add encrypted kernel (identity) mappings */

 Add decrypted, write-protected kernel (non-identity) mappings */

 Add encrypted initrd (identity) mappings */

		/*

		 * Add decrypted, write-protected initrd (non-identity) mappings

 Add decrypted workarea mappings to both kernel mappings */

 Perform the encryption */

	/*

	 * At this point we are running encrypted.  Remove the mappings for

	 * the decrypted areas - all that is needed for this is to remove

	 * the PGD entry/entries.

 Flush the TLB - no globals so cr3 is enough */

 Check for the SME/SEV support leaf */

	/*

	 * Check for the SME/SEV feature:

	 *   CPUID Fn8000_001F[EAX]

	 *   - Bit 0 - Secure Memory Encryption support

	 *   - Bit 1 - Secure Encrypted Virtualization support

	 *   CPUID Fn8000_001F[EBX]

	 *   - Bits 5:0 - Pagetable bit position used to indicate encryption

 Check whether SEV or SME is supported */

 Check the SEV MSR whether SEV or SME is enabled */

 Check if memory encryption is enabled */

		/*

		 * No SME if Hypervisor bit is set. This check is here to

		 * prevent a guest from trying to enable SME. For running as a

		 * KVM guest the MSR_AMD64_SYSCFG will be sufficient, but there

		 * might be other hypervisors which emulate that MSR as non-zero

		 * or even pass it through to the guest.

		 * A malicious hypervisor can still trick a guest into this

		 * path, but there is no way to protect against that.

 For SME, check the SYSCFG MSR */

 SEV state cannot be controlled by a command line option */

	/*

	 * Fixups have not been applied to phys_base yet and we're running

	 * identity mapped, so we must obtain the address to the SME command

	 * line argument data using rip-relative addressing.

/*

 * Written by: Patricia Gaughen <gone@us.ibm.com>, IBM Corporation

 * August 2002: added remote node KVA remap - Martin J. Bligh 

 *

 * Copyright (C) 2002, IBM Corp.

 *

 * All rights reserved.          

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License as published by

 * the Free Software Foundation; either version 2 of the License, or

 * (at your option) any later version.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or

 * NON INFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this program; if not, write to the Free Software

 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.

 SPDX-License-Identifier: GPL-2.0

/*

 * Associate a virtual page frame with a given physical page frame 

 * and protection flags for that frame.

	/*

	 * It's enough to flush this one mapping.

	 * (PGE mappings get flushed as well)

/*

 * vmalloc=size forces the vmalloc area to be exactly 'size'

 * bytes. This can be used to increase (or decrease) the

 * vmalloc area - the default is 128m.

 Add VMALLOC_OFFSET to the parsed value due to vm area guard hole*/

/*

 * reservetop=size reserves a hole at the top of the kernel address space which

 * a hypervisor can load into later.  Needed for dynamically loaded hypervisors,

 * so relocating the fixmap can be done before paging initialization.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2002 Andi Kleen, SuSE Labs.

 * Thanks to Ben LaHaise for precious feedback.

/*

 * The current flushing context - we pass it instead of 5 arguments:

/*

 * Serialize cpa() (for !DEBUG_PAGEALLOC which uses large identity mappings)

 * using cpa_lock. So that we don't allow any other cpu, with stale large tlb

 * entries change the page attribute in parallel to some other cpu

 * splitting a large page entry along with changing the attribute.

 Do not search for aliases */

 Protect against CPA */

 Do not reference physical address outside the kernel. */

	/*

	 * Kernel text has an alias mapping at a high address, known

	 * here as "highmap".

 There is no highmap on 32-bit */

/*

 * See set_mce_nospec().

 *

 * Machine check recovery code needs to change cache mode of poisoned pages to

 * UC to avoid speculative access logging another error. But passing the

 * address of the 1:1 mapping to set_memory_uc() is a fine way to encourage a

 * speculative access. So we cheat and flip the top bit of the address. This

 * works fine for the code that updates the page tables. But at the end of the

 * process we need to flush the TLB and cache and the non-canonical address

 * causes a #GP fault when used by the INVLPG and CLFLUSH instructions.

 *

 * But in the common case we already have a canonical address. This code

 * will fix the top bit if needed and is a no-op otherwise.

/*

 * Flushing functions

/**

 * clflush_cache_range - flush a cache range with clflush

 * @vaddr:	virtual start address

 * @size:	number of bytes to flush

 *

 * CLFLUSHOPT is an unordered instruction which needs fencing with MFENCE or

 * SFENCE to avoid ordering issues.

	/*

	 * Flush all to work around Errata in early athlons regarding

	 * large page flushing.

		/*

		 * Only flush present addresses:

/*

 * The BIOS area between 640k and 1Mb needs to be executable for PCI BIOS

 * based config access (CONFIG_PCI_GOBIOS) support.

/*

 * The .rodata section needs to be read-only. Using the pfn catches all

 * aliases.  This also includes __ro_after_init, so do not enforce until

 * kernel_set_to_readonly is true.

	/*

	 * Note: __end_rodata is at page aligned and not inclusive, so

	 * subtract 1 to get the last enforced PFN in the rodata area.

/*

 * Protect kernel text against becoming non executable by forbidding

 * _PAGE_NX.  This protects only the high kernel mapping (_text -> _etext)

 * out of which the kernel actually executes.  Do not protect the low

 * mapping.

 *

 * This does not cover __inittext since that is gone after boot.

/*

 * Once the kernel maps the text as RO (kernel_set_to_readonly is set),

 * kernel text mappings for the large page aligned text, rodata sections

 * will be always read-only. For the kernel identity mappings covering the

 * holes caused by this alignment can be anything that user asks.

 *

 * This will preserve the large page mappings for kernel text/data at no

 * extra cost.

	/*

	 * Don't enforce the !RW mapping for the kernel text mapping, if

	 * the current mapping is already using small page mapping.  No

	 * need to work hard to preserve large page mappings in this case.

	 *

	 * This also fixes the Linux Xen paravirt guest boot failure caused

	 * by unexpected read-only mappings for kernel identity

	 * mappings. In this paravirt guest case, the kernel text mapping

	 * and the kernel identity mapping share the same page-table pages,

	 * so the protections for kernel text and identity mappings have to

	 * be the same.

/*

 * Certain areas of memory on x86 require very specific protection flags,

 * for example the BIOS area or kernel text. Callers don't always get this

 * right (again, ioremap() on BIOS memory is not uncommon) so this function

 * checks and fixes these known static required protection bits.

	/*

	 * There is no point in checking RW/NX conflicts when the requested

	 * mapping is setting the page !PRESENT.

 Operate on the virtual address */

	/*

	 * Special case to preserve a large page. If the change spawns the

	 * full large page mapping then there is no point to split it

	 * up. Happens with ftrace and is going to be removed once ftrace

	 * switched to text_poke().

 Check the PFN directly */

/*

 * Lookup the page table entry for a virtual address in a specific pgd.

 * Return a pointer to the entry and the level of the mapping.

/*

 * Lookup the page table entry for a virtual address. Return a pointer

 * to the entry and the level of the mapping.

 *

 * Note: We return pud and pmd either when the entry is marked large

 * or when the present bit is not set. Otherwise we would return a

 * pointer to a nonexisting mapping.

/*

 * Lookup the page table entry for a virtual address in a given mm. Return a

 * pointer to the entry and the level of the mapping.

/*

 * Lookup the PMD entry for a virtual address. Return a pointer to the entry

 * or NULL if not present.

/*

 * This is necessary because __pa() does not work on some

 * kinds of memory, like vmalloc() or the alloc_remap()

 * areas on 32-bit NUMA systems.  The percpu areas can

 * end up in this kind of memory, for instance.

 *

 * This could be optimized, but it is only intended to be

 * used at initialization time, and keeping it

 * unoptimized should increase the testing coverage for

 * the more obscure platforms.

	/*

	 * pXX_pfn() returns unsigned long, which must be cast to phys_addr_t

	 * before being left-shifted PAGE_SHIFT bits -- this trick is to

	 * make 32-PAE kernel work correctly.

/*

 * Set the new pmd in all the pgds we know about:

 change init_mm */

	/*

	 * _PAGE_GLOBAL means "global page" for present PTEs.

	 * But, it is also used to indicate _PAGE_PROTNONE

	 * for non-present PTEs.

	 *

	 * This ensures that a _PAGE_GLOBAL PTE going from

	 * present to non-present is not confused as

	 * _PAGE_PROTNONE.

	/*

	 * Check for races, another CPU might have split this page

	 * up already:

	/*

	 * Calculate the number of pages, which fit into this large

	 * page starting at address:

	/*

	 * We are safe now. Check whether the new pgprot is the same:

	 * Convert protection attributes to 4k-format, as cpa->mask* are set

	 * up accordingly.

 Clear PSE (aka _PAGE_PAT) and move PAT bit to correct position */

	/*

	 * req_prot is in format of 4k pages. It must be converted to large

	 * page format: the caching mode includes the PAT bit located at

	 * different bit positions in the two formats.

	/*

	 * old_pfn points to the large page base pfn. So we need to add the

	 * offset of the virtual address:

	/*

	 * Calculate the large page base address and the number of 4K pages

	 * in the large page

	/*

	 * Sanity check that the existing mapping is correct versus the static

	 * protections. static_protections() guards against !PRESENT, so no

	 * extra conditional required here.

		/*

		 * Split the large page and tell the split code to

		 * enforce static protections.

	/*

	 * Optimization: If the requested pgprot is the same as the current

	 * pgprot, then the large page can be preserved and no updates are

	 * required independent of alignment and length of the requested

	 * range. The above already established that the current pgprot is

	 * correct, which in consequence makes the requested pgprot correct

	 * as well if it is the same. The static protection scan below will

	 * not come to a different conclusion.

	/*

	 * If the requested range does not cover the full page, split it up

	/*

	 * Check whether the requested pgprot is conflicting with a static

	 * protection requirement in the large page.

	/*

	 * If there is a conflict, split the large page.

	 *

	 * There used to be a 4k wise evaluation trying really hard to

	 * preserve the large pages, but experimentation has shown, that this

	 * does not help at all. There might be corner cases which would

	 * preserve one large page occasionally, but it's really not worth the

	 * extra code and cycles for the common case.

 All checks passed. Update the large page mapping. */

	/*

	 * If should_split_large_page() discovered an inconsistent mapping,

	 * remove the invalid protection in the split mapping.

 Hand in lpsize = 0 to enforce the protection mechanism */

	/*

	 * If this is splitting a PMD, fix it up. PUD splits cannot be

	 * fixed trivially as that would require to rescan the newly

	 * installed PMD mappings after returning from split_large_page()

	 * so an eventual further split can allocate the necessary PTE

	 * pages. Warn for now and revisit it in case this actually

	 * happens.

	/*

	 * Check for races, another CPU might have split this page

	 * up for us already:

		/*

		 * Clear PSE (aka _PAGE_PAT) and move

		 * PAT bit to correct position.

		/*

		 * Clear the PSE flags if the PRESENT flag is not set

		 * otherwise pmd_present/pmd_huge will return true

		 * even on a non present pmd.

	/*

	 * Get the target pfn from the original entry:

	/*

	 * Install the new, split up pagetable.

	 *

	 * We use the standard kernel pagetable protections for the new

	 * pagetable protections, the actual ptes set above control the

	 * primary protection behavior:

	/*

	 * Do a global flush tlb after splitting the large page

	 * and before we do the actual change page attribute in the PTE.

	 *

	 * Without this, we violate the TLB application note, that says:

	 * "The TLBs may contain both ordinary and large-page

	 *  translations for a 4-KByte range of linear addresses. This

	 *  may occur if software modifies the paging structures so that

	 *  the page size used for the address range changes. If the two

	 *  translations differ with respect to page frame or attributes

	 *  (e.g., permissions), processor behavior is undefined and may

	 *  be implementation-specific."

	 *

	 * We do this global tlb flush inside the cpa_lock, so that we

	 * don't allow any other cpu, with stale tlb entries change the

	 * page attribute in parallel, that also falls into the

	 * just split large page entry.

	/*

	 * Not on a 2MB page boundary?

	/*

	 * Try to unmap in 2M chunks.

	/*

	 * 4K leftovers?

	/*

	 * Try again to free the PMD page if haven't succeeded above.

	/*

	 * Not on a GB page boundary?

	/*

	 * Try to unmap in 1G chunks?

	/*

	 * 2M leftovers?

	/*

	 * No need to try to free the PUD page because we'll free it in

	 * populate_pgd's error path

	/*

	 * Not on a 2M boundary?

		/*

		 * Need a PTE page?

	/*

	 * We mapped them all?

		/*

		 * We cannot use a 1G page so allocate a PMD page if needed.

	/*

	 * Map trailing 4K pages.

	/*

	 * Not on a Gb page boundary? => map everything up to it with

	 * smaller pages.

		/*

		 * Need a PMD page?

 We mapped them all? */

	/*

	 * Map everything starting from the Gb boundary, possibly with 1G pages

 Map trailing leftover */

/*

 * Restrictions for kernel page table do not necessarily apply when mapping in

 * an alternate PGD.

 shut up gcc */

	/*

	 * Allocate a PUD page and hand it down for mapping.

		/*

		 * Leave the PUD page in place in case some other CPU or thread

		 * already found it, but remove any useless entries we just

		 * added to it.

		/*

		 * Right now, we only execute this code path when mapping

		 * the EFI virtual memory map regions, no other users

		 * provide a ->pgd value. This may change in the future.

	/*

	 * Ignore all non primary paths.

	/*

	 * Ignore the NULL PTE for kernel identity mapping, as it is expected

	 * to have holes.

	 * Also set numpages to '1' indicating that we processed cpa req for

	 * one virtual address page and its pfn. TBD: numpages can be set based

	 * on the initial value and the level returned by lookup_address().

 Faults in the highmap are OK, so do not warn: */

 Hand in lpsize = 0 to enforce the protection mechanism */

		/*

		 * We need to keep the pfn from the existing PTE,

		 * after all we're only going to change it's attributes

		 * not the memory it points to

		/*

		 * Do we really change anything ?

	/*

	 * Check, whether we can keep the large page intact

	 * and just change the pte:

	/*

	 * When the range fits into the existing large page,

	 * return. cp->numpages and cpa->tlbflush have been updated in

	 * try_large_page:

	/*

	 * We have to split the large page:

	/*

	 * No need to redo, when the primary call touched the direct

	 * mapping already:

	/*

	 * If the primary call didn't touch the high mapping already

	 * and the physical address is inside the kernel map, we need

	 * to touch the high mapped kernel as well:

		/*

		 * The high mapping range is imprecise, so ignore the

		 * return value.

		/*

		 * Store the remaining nr of pages for the large page

		 * preservation check.

 for array changes, we can't use large page */

		/*

		 * Adjust the number of pages with the result of the

		 * CPA operation. Either a large page has been

		 * preserved or a single page update happened.

 Restore the original numpages */

	/*

	 * Check, if we are requested to set a not supported

	 * feature.  Clearing non-supported features is OK.

 Ensure we are PAGE_SIZE aligned */

		/*

		 * in_flag of CPA_PAGES_ARRAY implies it is aligned.

		 * No need to check in that case

			/*

			 * People should not be passing in unaligned addresses:

 Must avoid aliasing mappings in the highmem code */

 No alias checking for _NX bit modifications */

 Has caller explicitly disabled alias checking? */

	/*

	 * Check whether we really changed something:

	/*

	 * No need to flush, when we did not set any of the caching

	 * attributes:

	/*

	 * On error; flush everything to be sure.

/*

 * _set_memory_prot is an internal helper for callers that have been passed

 * a pgprot_t value from upper layers and a reservation has already been taken.

 * If you want to set the pgprot to a specific page protocol, use the

 * set_memory_xx() functions.

	/*

	 * for now UC MINUS. see comments in ioremap()

	 * If you really need strong UC use ioremap_uc(), but note

	 * that you cannot override IO areas with set_memory_*() as

	 * these helpers cannot work with IO memory.

	/*

	 * for now UC MINUS. see comments in ioremap()

 WB cache mode is hard wired to all cache attribute bits being 0 */

/*

 * __set_memory_enc_pgtable() is used for the hypervisors that get

 * informed about "encryption" status via page tables.

 Should not be working on unaligned addresses */

 Must avoid aliasing mappings in the highmem code */

	/*

	 * Before changing the encryption attribute, we need to flush caches.

	/*

	 * After changing the encryption attribute, we need to flush TLBs again

	 * in case any speculative TLB caching occurred (but no need to flush

	 * caches again).  We could just use cpa_flush_all(), but in case TLB

	 * flushing gets optimized in the cpa_flush() path use the same logic

	 * as above.

	/*

	 * Notify hypervisor that a given memory range is mapped encrypted

	 * or decrypted.

 If WC, set to UC- first and then WC */

 Success */

 WB cache mode is hard wired to all cache attribute bits being 0 */

	/*

	 * No alias checking needed for setting present flag. otherwise,

	 * we may need to break large pages for 64-bit kernel text

	 * mappings (this adds to complexity if we want to do this from

	 * atomic context especially). Let's keep it simple!

	/*

	 * No alias checking needed for setting not present flag. otherwise,

	 * we may need to break large pages for 64-bit kernel text

	 * mappings (this adds to complexity if we want to do this from

	 * atomic context especially). Let's keep it simple!

	/*

	 * The return value is ignored as the calls cannot fail.

	 * Large pages for identity mappings are not used at boot time

	 * and hence no memory allocations during large page split.

	/*

	 * We should perform an IPI and flush all tlbs,

	 * but that can deadlock->flush only current cpu.

	 * Preemption needs to be disabled around __flush_tlb_all() due to

	 * CR3 reload in __native_flush_tlb().

 CONFIG_DEBUG_PAGEALLOC */

/*

 * __flush_tlb_all() flushes mappings only on current CPU and hence this

 * function shouldn't be used in an SMP environment. Presently, it's used only

 * during boot (way before smp_init()) by EFI subsystem and hence is ok.

	/*

	 * The typical sequence for unmapping is to find a pte through

	 * lookup_address_in_pgd() (ideally, it should never return NULL because

	 * the address is already mapped) and change it's protections. As pfn is

	 * the *target* of a mapping, it's not useful while unmapping.

/*

 * The testcases use internal knowledge of the implementation that shouldn't

 * be exposed to the rest of the kernel. Include these directly here.

 SPDX-License-Identifier: GPL-2.0

/*

 * Handle caching attributes in page tables (PAT)

 *

 * Authors: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>

 *          Suresh B Siddha <suresh.b.siddha@intel.com>

 *

 * Interval tree used to store the PAT memory type reservations.

/*

 * The memtype tree keeps track of memory type for specific

 * physical memory areas. Without proper tracking, conflicting memory

 * types in different mappings can cause CPU cache corruption.

 *

 * The tree is an interval tree (augmented rbtree) which tree is ordered

 * by the starting address. The tree can contain multiple entries for

 * different regions which overlap. All the aliases have the same

 * cache attributes of course, as enforced by the PAT logic.

 *

 * memtype_lock protects the rbtree.

 Returns NULL if there is no match */

	/*

	 * Since the memtype_rbroot tree allows overlapping ranges,

	 * memtype_erase() checks with EXACT_MATCH first, i.e. free

	 * a whole node for the munmap case.  If no such entry is found,

	 * it then checks with END_MATCH, i.e. shrink the size of a node

	 * from the end for the mremap case.

 munmap: erase this node */

 mremap: update the end value of this node */

/*

 * Debugging helper, copy the Nth entry of the tree into a

 * a copy for printout. This allows us to print out the tree

 * via debugfs, without holding the memtype_lock too long:

 pos == i */

 SPDX-License-Identifier: GPL-2.0

/*

 * self test for change_page_attr.

 *

 * Clears the a test pte bit on random pages in the direct mapping,

 * then reverts and compares page tables forwards and afterwards.

/*

 * Only print the results of the first pass:

 Change the global bit on random pages in the direct mapping */

 shut gcc up */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Page Attribute Table (PAT) support: handle memory caching attributes in page tables.

 *

 * Authors: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>

 *          Suresh B Siddha <suresh.b.siddha@intel.com>

 *

 * Loosely based on earlier PAT patchset from Eric Biederman and Andi Kleen.

 *

 * Basic principles:

 *

 * PAT is a CPU feature supported by all modern x86 CPUs, to allow the firmware and

 * the kernel to set one of a handful of 'caching type' attributes for physical

 * memory ranges: uncached, write-combining, write-through, write-protected,

 * and the most commonly used and default attribute: write-back caching.

 *

 * PAT support supercedes and augments MTRR support in a compatible fashion: MTRR is

 * a hardware interface to enumerate a limited number of physical memory ranges

 * and set their caching attributes explicitly, programmed into the CPU via MSRs.

 * Even modern CPUs have MTRRs enabled - but these are typically not touched

 * by the kernel or by user-space (such as the X server), we rely on PAT for any

 * additional cache attribute logic.

 *

 * PAT doesn't work via explicit memory ranges, but uses page table entries to add

 * cache attribute information to the mapped memory range: there's 3 bits used,

 * (_PAGE_PWT, _PAGE_PCD, _PAGE_PAT), with the 8 possible values mapped by the

 * CPU to actual cache attributes via an MSR loaded into the CPU (MSR_IA32_CR_PAT).

 *

 * ( There's a metric ton of finer details, such as compatibility with CPU quirks

 *   that only support 4 types of PAT entries, and interaction with MTRRs, see

 *   below for details. )

/*

 * PAT support is enabled by default, but can be disabled for

 * various user-requested or hardware-forced reasons:

/*

 * X86 PAT uses page flags arch_1 and uncached together to keep track of

 * memory type of pages that have backing page struct.

 *

 * X86 PAT supports 4 different memory types:

 *  - _PAGE_CACHE_MODE_WB

 *  - _PAGE_CACHE_MODE_WC

 *  - _PAGE_CACHE_MODE_UC_MINUS

 *  - _PAGE_CACHE_MODE_WT

 *

 * _PAGE_CACHE_MODE_WB is the default type.

 uncached */

 Write combining */

 Write Through */

 Write Protected */

 Write Back (default) */

 UC, but can be overridden by MTRR */

/*

 * Update the cache mode to pgprot translation tables according to PAT

 * configuration.

 * Using lower indices is preferred, so we start with highest index.

		/*

		 * If this happens we are on a secondary CPU, but switched to

		 * PAT on the boot CPU. We have no way to undo PAT.

		/*

		 * CPU supports PAT. Set PAT table to be consistent with

		 * PAT MSR. This case supports "nopat" boot option, and

		 * virtual machine environments which support PAT without

		 * MTRRs. In specific, Xen has unique setup to PAT MSR.

		 *

		 * If PAT MSR returns 0, it is considered invalid and emulates

		 * as No PAT.

		/*

		 * No PAT. Emulate the PAT table that corresponds to the two

		 * cache bits, PWT (Write Through) and PCD (Cache Disable).

		 * This setup is also the same as the BIOS default setup.

		 *

		 * PTE encoding:

		 *

		 *       PCD

		 *       |PWT  PAT

		 *       ||    slot

		 *       00    0    WB : _PAGE_CACHE_MODE_WB

		 *       01    1    WT : _PAGE_CACHE_MODE_WT

		 *       10    2    UC-: _PAGE_CACHE_MODE_UC_MINUS

		 *       11    3    UC : _PAGE_CACHE_MODE_UC

		 *

		 * NOTE: When WC or WP is used, it is redirected to UC- per

		 * the default setup in __cachemode2pte_tbl[].

/**

 * pat_init - Initialize the PAT MSR and PAT table on the current CPU

 *

 * This function initializes PAT MSR and PAT table with an OS-defined value

 * to enable additional cache attributes, WC, WT and WP.

 *

 * This function must be called on all CPUs using the specific sequence of

 * operations defined in Intel SDM. mtrr_rendezvous_handler() provides this

 * procedure for PAT.

		/*

		 * PAT support with the lower four entries. Intel Pentium 2,

		 * 3, M, and 4 are affected by PAT errata, which makes the

		 * upper four entries unusable. To be on the safe side, we don't

		 * use those.

		 *

		 *  PTE encoding:

		 *      PAT

		 *      |PCD

		 *      ||PWT  PAT

		 *      |||    slot

		 *      000    0    WB : _PAGE_CACHE_MODE_WB

		 *      001    1    WC : _PAGE_CACHE_MODE_WC

		 *      010    2    UC-: _PAGE_CACHE_MODE_UC_MINUS

		 *      011    3    UC : _PAGE_CACHE_MODE_UC

		 * PAT bit unused

		 *

		 * NOTE: When WT or WP is used, it is redirected to UC- per

		 * the default setup in __cachemode2pte_tbl[].

		/*

		 * Full PAT support.  We put WT in slot 7 to improve

		 * robustness in the presence of errata that might cause

		 * the high PAT bit to be ignored.  This way, a buggy slot 7

		 * access will hit slot 3, and slot 3 is UC, so at worst

		 * we lose performance without causing a correctness issue.

		 * Pentium 4 erratum N46 is an example for such an erratum,

		 * although we try not to use PAT at all on affected CPUs.

		 *

		 *  PTE encoding:

		 *      PAT

		 *      |PCD

		 *      ||PWT  PAT

		 *      |||    slot

		 *      000    0    WB : _PAGE_CACHE_MODE_WB

		 *      001    1    WC : _PAGE_CACHE_MODE_WC

		 *      010    2    UC-: _PAGE_CACHE_MODE_UC_MINUS

		 *      011    3    UC : _PAGE_CACHE_MODE_UC

		 *      100    4    WB : Reserved

		 *      101    5    WP : _PAGE_CACHE_MODE_WP

		 *      110    6    UC-: Reserved

		 *      111    7    WT : _PAGE_CACHE_MODE_WT

		 *

		 * The reserved slots are unused, but mapped to their

		 * corresponding types in the presence of PAT errata.

 protects memtype accesses */

/*

 * Does intersection of PAT memory type and MTRR memory type and returns

 * the resulting memory type as PAT understands it.

 * (Type in pat and mtrr will not have same value)

 * The intersection is based on "Effective Memory Type" tables in IA-32

 * SDM vol 3a

	/*

	 * Look for MTRR hint to get the effective type in case where PAT

	 * request is for WB.

	/*

	 * For legacy reasons, physical address range in the legacy ISA

	 * region is tracked as non-RAM. This will allow users of

	 * /dev/mem to map portions of legacy ISA region, even when

	 * some of those portions are listed(or not even listed) with

	 * different e820 types(RAM/reserved/..)

/*

 * For RAM pages, we use page flags to mark the pages with appropriate type.

 * The page flags are limited to four types, WB (default), WC, WT and UC-.

 * WP request fails with -EINVAL, and UC gets redirected to UC-.  Setting

 * a new memory type is only allowed for a page mapped with the default WB

 * type.

 *

 * Here we do two passes:

 * - Find the memtype of all the pages in the range, look for any conflicts.

 * - In case of no conflicts, set the new memtype for pages in the range.

 We do not support strong UC */

	/*

	 * When changing the memtype for pages containing poison allow

	 * for a "decoy" virtual address (bit 63 clear) passed to

	 * set_memory_X(). __pa() on a "decoy" address results in a

	 * physical address with bit 63 set.

	 *

	 * Decoy addresses are not present for 32-bit builds, see

	 * set_mce_nospec().

/*

 * req_type typically has one of the:

 * - _PAGE_CACHE_MODE_WB

 * - _PAGE_CACHE_MODE_WC

 * - _PAGE_CACHE_MODE_UC_MINUS

 * - _PAGE_CACHE_MODE_UC

 * - _PAGE_CACHE_MODE_WT

 *

 * If new_type is NULL, function will return an error if it cannot reserve the

 * region with req_type. If new_type is non-NULL, function will return

 * available type in new_type in case of no error. In case of any error

 * it will return a negative return value.

	/*

	 * The end address passed into this function is exclusive, but

	 * sanitize_phys() expects an inclusive address.

 This is identical to page table setting without PAT */

 Low ISA region is always mapped WB in page table. No need to track */

	/*

	 * Call mtrr_lookup to get the type hint. This is an

	 * optimization for /dev/mem mmap'ers into WB memory (BIOS

	 * tools and ACPI tools). Use WB request for WB memory and use

	 * UC_MINUS otherwise.

 Low ISA region is always mapped WB. No need to track */

/**

 * lookup_memtype - Looks up the memory type for a physical address

 * @paddr: physical address of which memory type needs to be looked up

 *

 * Only to be called when PAT is enabled

 *

 * Returns _PAGE_CACHE_MODE_WB, _PAGE_CACHE_MODE_WC, _PAGE_CACHE_MODE_UC_MINUS

 * or _PAGE_CACHE_MODE_WT.

/**

 * pat_pfn_immune_to_uc_mtrr - Check whether the PAT memory type

 * of @pfn cannot be overridden by UC MTRR memory type.

 *

 * Only to be called when PAT is enabled.

 *

 * Returns true, if the PAT memory type of @pfn is UC, UC-, or WC.

 * Returns false in other cases.

/**

 * memtype_reserve_io - Request a memory type mapping for a region of memory

 * @start: start (physical address) of the region

 * @end: end (physical address) of the region

 * @type: A pointer to memtype, with requested type. On success, requested

 * or any other compatible type that was available for the region is returned

 *

 * On success, returns 0

 * On failure, returns non-zero

/**

 * memtype_free_io - Release a memory type mapping for a region of memory

 * @start: start (physical address) of the region

 * @end: end (physical address) of the region

 This check is done in drivers/char/mem.c in case of STRICT_DEVMEM */

 This check is needed to avoid cache aliasing when PAT is enabled */

 CONFIG_STRICT_DEVMEM */

/*

 * Change the memory type for the physical address range in kernel identity

 * mapping space if that range is a part of identity map.

	/*

	 * Some areas in the middle of the kernel identity range

	 * are not mapped, for example the PCI space.

/*

 * Internal interface to reserve a range of physical memory with prot.

 * Reserved non RAM regions only and after successful memtype_reserve,

 * this func also keeps identity mapping (if any) in sync with this new prot.

	/*

	 * reserve_pfn_range() for RAM pages. We do not refcount to keep

	 * track of number of mappings of RAM pages. We can assert that

	 * the type requested matches the type of first page in the range.

		/*

		 * We allow returning different type than the one requested in

		 * non strict case.

/*

 * Internal interface to free a range of physical memory.

 * Frees non RAM regions only.

/*

 * track_pfn_copy is called when vma that is covering the pfnmap gets

 * copied through copy_page_range().

 *

 * If the vma has a linear pfn mapping for the entire range, we get the prot

 * from pte and reserve the entire vma range with single reserve_pfn_range call.

		/*

		 * reserve the whole chunk covered by vma. We need the

		 * starting address and protection from pte.

/*

 * prot is passed in as a parameter for the new mapping. If the vma has

 * a linear pfn mapping for the entire range, or no vma is provided,

 * reserve the entire pfn + size range with single reserve_pfn_range

 * call.

 reserve the whole chunk starting from paddr */

	/*

	 * For anything smaller than the vma size we set prot based on the

	 * lookup.

 Check memtype for the remaining pages */

 Set prot based on lookup */

/*

 * untrack_pfn is called while unmapping a pfnmap for a region.

 * untrack can be called for a specific region indicated by pfn and size or

 * can be for the entire vma (in which case pfn, size are zero).

 free the chunk starting from pfn or the whole chunk */

/*

 * untrack_pfn_moved is called, while mremapping a pfnmap for a new region,

 * with the old vma after its pfnmap page table has been removed.  The new

 * vma has a new pfnmap to the same pfn & cache type with VM_PAT set.

/*

 * We are allocating a temporary printout-entry to be passed

 * between seq_start()/next() and seq_show():

 Free it on error: */

 CONFIG_DEBUG_FS && CONFIG_X86_PAT */

 SPDX-License-Identifier: GPL-2.0

/*

 * sys_alloc_thread_area: get a yet unused TLS descriptor index.

	/*

	 * For historical reasons (i.e. no one ever documented how any

	 * of the segmentation APIs work), user programs can and do

	 * assume that a struct user_desc that's all zeros except for

	 * entry_number means "no segment at all".  This never actually

	 * worked.  In fact, up to Linux 3.19, a struct user_desc like

	 * this would create a 16-bit read-write segment with base and

	 * limit both equal to zero.

	 *

	 * That was close enough to "no segment at all" until we

	 * hardened this function to disallow 16-bit TLS segments.  Fix

	 * it up by interpreting these zeroed segments the way that they

	 * were almost certainly intended to be interpreted.

	 *

	 * The correct way to ask for "no segment at all" is to specify

	 * a user_desc that satisfies LDT_empty.  To keep everything

	 * working, we accept both.

	 *

	 * Note that there's a similar kludge in modify_ldt -- look at

	 * the distinction between modes 1 and 0x11.

	/*

	 * espfix is required for 16-bit data segments, but espfix

	 * only works for LDT segments.

 Only allow data segments in the TLS array. */

	/*

	 * Non-present segments with DPL 3 present an interesting attack

	 * surface.  The kernel should handle such segments correctly,

	 * but TLS is very difficult to protect in a sandbox, so prevent

	 * such segments from being created.

	 *

	 * If userspace needs to remove a TLS entry, it can still delete

	 * it outright.

	/*

	 * We must not get preempted while modifying the TLS.

/*

 * Set a given TLS descriptor:

	/*

	 * index -1 means the kernel should try to find and

	 * allocate an empty descriptor:

	/*

	 * If DS, ES, FS, or GS points to the modified segment, forcibly

	 * refresh it.  Only needed on x86_64 because x86_32 reloads them

	 * on return to user mode.

/*

 * Get the current Thread-Local Storage area:

 SPDX-License-Identifier: GPL-2.0-or-later

/*  paravirtual clock -- common code used by kvm/xen



	/*

	 * Assumption here is that last_value, a global accumulator, always goes

	 * forward. If we are less than that, we should not be much smaller.

	 * We assume there is an error margin we're inside, and then the correction

	 * does not sacrifice accuracy.

	 *

	 * For reads: global may have changed between test and return,

	 * but this means someone else updated poked the clock at a later time.

	 * We just need to make sure we are not seeing a backwards event.

	 *

	 * For updates: last_value = ret is not enough, since two vcpus could be

	 * updating at the same time, and one of them could be slightly behind,

	 * making the assumption that last_value always go forward fail to hold.

 get wallclock at system boot */

 fetch version before time */

		/*

		 * Note: wall_clock->sec is a u32 value, so it can

		 * only store dates between 1970 and 2106. To allow

		 * times beyond that, we need to create a new hypercall

		 * interface with an extended pvclock_wall_clock structure

		 * like ARM has.

 fetch time before checking version */

 time since system boot */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * per-CPU TSS segments. Threads are completely 'soft' on Linux,

 * no more per-task TSS's. The TSS size is kept cacheline-aligned

 * so they are allowed to end up in the .data..cacheline_aligned

 * section. Since TSS's are completely CPU-local, we want them

 * on exact cacheline boundaries, to eliminate cacheline ping-pong.

		/*

		 * .sp0 is only used when entering ring 0 from a lower

		 * privilege level.  Since the init task never runs anything

		 * but ring 0 code, there is no need for a valid value here.

		 * Poison it.

/*

 * this gets called so that we can store lazy state into memory and copy the

 * current task into the new thread.

 Drop the copied pointer to current's fpstate */

/*

 * Free thread data structures etc..

	/*

	 * Clear all status flags including IF and set fixed bit. 64bit

	 * does not have this initialization as the frame does not contain

	 * flags. The flags consistency (especially vs. AC) is there

	 * ensured via objtool, which lacks 32bit support.

 Kernel thread ? */

	/*

	 * Clone current's PKRU value from hardware. tsk->thread.pkru

	 * is only valid when scheduled out.

		/*

		 * An IO thread is a user space thread, but it doesn't

		 * return to ret_after_fork().

		 *

		 * In order to indicate that to tools like gdb,

		 * we reset the stack and instruction pointers.

		 *

		 * It does the same kernel frame setup to return to a kernel

		 * function that a kernel thread does.

 Set a new TLS for the child thread? */

	/*

	 * If PKRU is enabled the default PKRU value has to be loaded into

	 * the hardware right here (similar to context switch).

		/*

		 * Must flip the CPU state synchronously with

		 * TIF_NOTSC in the current running context.

		/*

		 * Must flip the CPU state synchronously with

		 * TIF_NOTSC in the current running context.

		/*

		 * Must flip the CPU state synchronously with

		 * TIF_NOCPUID in the current running context.

		/*

		 * Must flip the CPU state synchronously with

		 * TIF_NOCPUID in the current running context.

/*

 * Called immediately after a successful exec.

 If cpuid was previously disabled for this task, re-enable it. */

	/*

	 * Don't inherit TIF_SSBD across exec boundary when

	 * PR_SPEC_DISABLE_NOEXEC is used.

	/*

	 * Invalidate I/O bitmap if the previous task used it. This prevents

	 * any possible leakage of an active I/O bitmap.

	 *

	 * If the next task has an I/O bitmap it will handle it on exit to

	 * user mode.

	/*

	 * Copy at least the byte range of the incoming tasks bitmap which

	 * covers the permitted I/O ports.

	 *

	 * If the previous task which used an I/O bitmap had more bits

	 * permitted, then the copy needs to cover those as well so they

	 * get turned off.

	/*

	 * Store the new max and the sequence number of this bitmap

	 * and a pointer to the bitmap itself.

/**

 * tss_update_io_bitmap - Update I/O bitmap before exiting to usermode

		/*

		 * Only copy bitmap data when the sequence number differs. The

		 * update time is accounted to the incoming task.

 Enable the bitmap */

	/*

	 * Make sure that the TSS limit is covering the IO bitmap. It might have

	 * been cut down by a VMEXIT to 0x67 which would cause a subsequent I/O

	 * access from user space to trigger a #GP because tbe bitmap is outside

	 * the TSS limit.

 CONFIG_X86_IOPL_IOPERM */

	/*

	 * Shared state setup happens once on the first bringup

	 * of the CPU. It's not destroyed on CPU hotunplug.

	/*

	 * Go over HT siblings and check whether one of them has set up the

	 * shared state pointer already.

 Link it to the state of the sibling: */

	/*

	 * First HT sibling to come up on the core.  Link shared state of

	 * the first HT sibling to itself. The siblings on the same core

	 * which come up later will see the shared state pointer and link

	 * themselves to the state of this CPU.

/*

 * Logic is: First HT sibling enables SSBD for both siblings in the core

 * and last sibling to disable it, disables it for the whole core. This how

 * MSR_SPEC_CTRL works in "hardware":

 *

 *  CORE_SPEC_CTRL = THREAD0_SPEC_CTRL | THREAD1_SPEC_CTRL

		/*

		 * Since this can race with prctl(), block reentry on the

		 * same CPU.

 First sibling enables SSBD: */

	/*

	 * SSBD has the same definition in SPEC_CTRL and VIRT_SPEC_CTRL,

	 * so ssbd_tif_to_spec_ctrl() just works.

/*

 * Update the MSRs managing speculation control, during context switch.

 *

 * tifp: Previous task's thread flags

 * tifn: Next task's thread flags

 Handle change of TIF_SSBD depending on the mitigation method. */

 Only evaluate TIF_SPEC_IB if conditional STIBP is enabled. */

 Return the updated threadinfo flags*/

 Forced update. Make sure all relevant TIF flags are different */

 Called from seccomp/prctl update */

 Enforce MSR update to ensure consistent state */

/*

 * Idle related variables and functions

/*

 * Called from the generic idle code.

/*

 * We use this if we don't have any better idle routine..

	/*

	 * Remove this CPU:

	/*

	 * Use wbinvd on processors that support SME. This provides support

	 * for performing a successful kexec when going from SME inactive

	 * to SME active (or vice-versa). The cache must be cleared so that

	 * if there are entries with the same physical address, both with and

	 * without the encryption bit, they don't race each other when flushed

	 * and potentially end up with the wrong entry being committed to

	 * memory.

		/*

		 * Use native_halt() so that memory contents don't change

		 * (stack usage and variables) after possibly issuing the

		 * native_wbinvd() above.

/*

 * AMD Erratum 400 aware idle routine. We handle it the same way as C3 power

 * states (local apic timer and TSC stop).

 *

 * XXX this function is completely buggered vs RCU and tracing.

	/*

	 * We cannot use static_cpu_has_bug() here because X86_BUG_AMD_APIC_C1E

	 * gets set after static_cpu_has() places have been converted via

	 * alternatives.

	/*

	 * The switch back from broadcast mode needs to be called with

	 * interrupts disabled.

/*

 * Intel Core2 and older machines prefer MWAIT over HALT for C1.

 * We can't rely on cpuidle installing MWAIT, because it will not load

 * on systems that support only C1 -- so the boot default must be MWAIT.

 *

 * Some AMD machines are the opposite, they depend on using HALT.

 *

 * So for default C1, which is used during boot until cpuidle loads,

 * use MWAIT-C1 on Intel HW that has it, else use HALT.

/*

 * MONITOR/MWAIT with no hints, used for default C1 state. This invokes MWAIT

 * with interrupts enabled and no flags, which is backwards compatible with the

 * original MWAIT implementation.

 quirk */

 quirk */

	/*

	 * AMD E400 detection needs to happen after ACPI has been enabled. If

	 * the machine is affected K8_INTP_C1E_ACTIVE_MASK bits are set in

	 * MSR_K8_INT_PENDING_MSG.

		/*

		 * When the boot option of idle=halt is added, halt is

		 * forced to be used for CPU idle. In such case CPU C2/C3

		 * won't be used again.

		 * To continue to load the CPU idle driver, don't touch

		 * the boot_option_idle_override.

		/*

		 * If the boot option of "idle=nomwait" is added,

		 * it means that mwait will be disabled for CPU C2/C3

		 * states. In such case it won't touch the variable

		 * of boot_option_idle_override.

/*

 * Called from fs/proc with a reference on @p to find the function

 * which called into schedule(). This needs to be done carefully

 * because the task might wake up and we might look at a stack

 * changing under us.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Architecture specific sysfs attributes in /sys/kernel

 *

 * Copyright (C) 2007, Intel Corp.

 *      Huang Ying <ying.huang@intel.com>

 * Copyright (C) 2013, 2013 Red Hat, Inc.

 *      Dave Young <dyoung@redhat.com>

 should be enough for setup_data nodes numbers */

 SPDX-License-Identifier: GPL-2.0

/*

 * The compat_siginfo_t structure and handing code is very easy

 * to break in several ways.  It must always be updated when new

 * updates are made to the main siginfo_t, and

 * copy_siginfo_to_user32() must be updated when the

 * (arch-independent) copy_siginfo_to_user() is updated.

 *

 * It is also easy to put a new member in the compat_siginfo_t

 * which has implicit alignment which can move internal structure

 * alignment around breaking the ABI.  This can happen if you,

 * for instance, put a plain 64-bit value in there.

	/*

	 * If adding a new si_code, there is probably new data in

	 * the siginfo.  Make sure folks bumping the si_code

	 * limits also have to look at this code.  Make sure any

	 * new fields are handled in copy_siginfo_to_user32()!

 This is part of the ABI and can never change in size: */

 This is a part of the ABI and can never change in alignment */

	/*

	 * The offsets of all the (unioned) si_fields are fixed

	 * in the ABI, of course.  Make sure none of them ever

	 * move and are always at the beginning:

	 /*

	 * Ensure that the size of each si_field never changes.

	 * If it does, it is a sign that the

	 * copy_siginfo_to_user32() code below needs to updated

	 * along with the size in the CHECK_SI_SIZE().

	 *

	 * We repeat this check for both the generic and compat

	 * siginfos.

	 *

	 * Note: it is OK for these to grow as long as the whole

	 * structure stays within the padding size (checked

	 * above).

 no _sigchld_x32 in the generic siginfo_t */

 any new si_fields should be added here */

 SPDX-License-Identifier: GPL-2.0

/*

 * AMD Encrypted Register State Support

 *

 * Author: Joerg Roedel <jroedel@suse.de>

 *

 * This file is not compiled stand-alone. It contains code shared

 * between the pre-decompression boot code and the running Linux kernel

 * and is included directly into both code-bases.

	/*

	 * Tell the hypervisor what went wrong - only reason-set 0 is

	 * currently supported.

 Request Guest Termination from Hypvervisor */

 Do the GHCB protocol version negotiation */

 Exceptions don't require to decode the instruction */

 Check if exception information from hypervisor is sane. */

 Fill in protocol and format specifiers */

	/*

	 * Hyper-V unenlightened guests use a paravisor for communicating and

	 * GHCB pages are being allocated and set up by that paravisor. Linux

	 * should not change the GHCB page's physical address.

/*

 * Boot VC Handler - This is the first VC handler during boot, there is no GHCB

 * page yet, so it only supports the MSR based communication with the

 * hypervisor and only the CPUID exit-code.

 Only CPUID is supported via MSR protocol */

	/*

	 * This is a VC handler and the #VC is only raised when SEV-ES is

	 * active, which means SEV must be active too. Do sanity checks on the

	 * CPUID results to make sure the hypervisor does not trick the kernel

	 * into the no-sev path. This could map sensitive data unencrypted and

	 * make it accessible to the hypervisor.

	 *

	 * In particular, check for:

	 *	- Availability of CPUID leaf 0x8000001f

	 *	- SEV CPUID bit.

	 *

	 * The hypervisor might still report the wrong C-bit position, but this

	 * can't be checked here.

 SEV leaf check */

 SEV bit */

 Skip over the CPUID two-byte opcode */

 Terminate the guest */

 INS opcodes */

 OUTS opcodes */

 IN immediate opcodes */

 OUT immediate opcodes */

 IN register opcodes */

 OUT register opcodes */

 Single byte opcodes */

 Length determined by instruction parsing */

 (REP) INS/OUTS */

		/*

		 * For the string variants with rep prefix the amount of in/out

		 * operations per #VC exception is limited so that the kernel

		 * has a chance to take interrupts and re-schedule while the

		 * instruction is emulated.

 Read bytes of OUTS into the shared buffer */

		/*

		 * Issue an VMGEXIT to the HV to consume the bytes from the

		 * shared buffer or to have it write them into the shared buffer

		 * depending on the instruction: OUTS or INS.

 Read bytes from shared buffer into the guest's destination. */

 IN/OUT into/from rAX */

 Safe to read xcr0 */

 xgetbv will cause #GP - use reset value for xcr0 */

 SPDX-License-Identifier: GPL-2.0

/*

 * This is a good place to put board specific reboot fixups.

 *

 * List of supported fixups:

 * geode-gx1/cs5530a - Jaya Kumar <jayalk@intworks.biz>

 * geode-gx/lx/cs5536 - Andres Salomon <dilinger@debian.org>

 *

	/* writing 1 to the reset control register, 0x44 causes the

 shouldn't get here but be safe and spin-a-while */

 writing 1 to the LSB of this MSR causes a hard reset */

 shouldn't get here but be safe and spin a while */

 Voluntary reset the watchdog timer */

 Generate a CPU reset on next tick */

 Use the minimum timer resolution */

/*

 * PCI ids solely used for fixups_table go here

/*

 * we see if any fixup is available for our current hardware. if there

 * is a fixup, we call it and we expect to never return from it. if we

 * do return, we keep looking and then eventually fall back to the

 * standard mach_reboot on return.

	/* we can be called from sysrq-B code. In such a case it is

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Confidential Computing Platform Capability checks

 *

 * Copyright (C) 2021 Advanced Micro Devices, Inc.

 *

 * Author: Tom Lendacky <thomas.lendacky@amd.com>

/*

 * SME and SEV are very similar but they are not the same, so there are

 * times that the kernel will need to distinguish between SME and SEV. The

 * cc_platform_has() function is used for this.  When a distinction isn't

 * needed, the CC_ATTR_MEM_ENCRYPT attribute can be used.

 *

 * The trampoline code is a good example for this requirement.  Before

 * paging is activated, SME will access all memory as decrypted, but SEV

 * will access all memory as encrypted.  So, when APs are being brought

 * up under SME the trampoline area cannot be encrypted, whereas under SEV

 * the trampoline area must be encrypted.

 SPDX-License-Identifier: GPL-2.0-or-later

/* ----------------------------------------------------------------------- *

 *

 *   Copyright 2000-2008 H. Peter Anvin - All Rights Reserved

 *   Copyright 2009 Intel Corporation; author: H. Peter Anvin

 *

/*

 * x86 MSR access device

 *

 * This device is accessed by lseek() to the appropriate register number

 * and then read/write in chunks of 8 bytes.  A larger size means multiple

 * reads or writes of the same register.

 *

 * This driver uses /dev/cpu/%d/msr where %d is the minor number, and on

 * an SMP box will direct the access to CPU %d.

 Invalid chunk size */

	/*

	 * MSRs writes usually happen all at once, and can easily saturate kmsg.

	 * Only allow one message every 30 seconds.

	 *

	 * It's possible to be smarter here and do it (for example) per-MSR, but

	 * it would certainly be more complex, and this is enough at least to

	 * avoid saturating the ring buffer.

git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git/about for details.\n");

 Invalid chunk size */

 No such CPU */

 MSR not supported */

/*

 * File operations we support

 val is NUL-terminated, see kernfs_fop_write() */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Interrupt descriptor table related code

 Interrupt gate */

 System interrupt gate */

/*

 * Interrupt gate with interrupt stack. The _ist index is the index in

 * the tss.ist[] array, but for the descriptor it needs to start at 1.

 Task gate */

/*

 * Early traps running on the DEFAULT_STACK because the other interrupt

 * stacks work only after cpu_init().

	/*

	 * Not possible on 64-bit. See idt_setup_early_pf() for details.

/*

 * The default IDT entries which are set up in trap_init() before

 * cpu_init() is invoked. Interrupt stacks cannot be used at that point and

 * the traps which use them are reinitialized with IST after cpu_init() has

 * set up TSS.

/*

 * The APIC and SMP idt entries

 Must be page-aligned because the real IDT is used in the cpu entry area */

/**

 * idt_setup_early_traps - Initialize the idt table with early traps

 *

 * On X8664 these traps do not use interrupt stacks as they can't work

 * before cpu_init() is invoked and sets up TSS. The IST variants are

 * installed after that.

/**

 * idt_setup_traps - Initialize the idt table with default traps

/*

 * Early traps running on the DEFAULT_STACK because the other interrupt

 * stacks work only after cpu_init().

/**

 * idt_setup_early_pf - Initialize the idt table with early pagefault handler

 *

 * On X8664 this does not use interrupt stacks as they can't work before

 * cpu_init() is invoked and sets up TSS. The IST variant is installed

 * after that.

 *

 * Note, that X86_64 cannot install the real #PF handler in

 * idt_setup_early_traps() because the memory initialization needs the #PF

 * handler from the early_idt_handler_array to initialize the early page

 * tables.

	/*

	 * Set the IDT descriptor to a fixed read-only location in the cpu

	 * entry area, so that the "sidt" instruction will not leak the

	 * location of the kernel, and to defend the IDT against arbitrary

	 * memory write vulnerabilities.

/**

 * idt_setup_apic_and_irq_gates - Setup APIC/SMP and normal interrupt gates

		/*

		 * Don't set the non assigned system vectors in the

		 * system_vectors bitmap. Otherwise they show up in

		 * /proc/interrupts.

 Map IDT into CPU entry area and reload it. */

 Make the IDT table read only */

/**

 * idt_setup_early_handler - Initializes the idt table with early handlers

/**

 * idt_invalidate - Invalidate interrupt descriptor table

 SPDX-License-Identifier: GPL-2.0

/*

 * x86 single-step support code, common to 32-bit and 64-bit.

	/*

	 * We'll assume that the code segments in the GDT

	 * are all zero-based. That is largely true: the

	 * TLS segments are used for data, and the PNPBIOS

	 * and APM bios ones we just ignore here.

 bogus selector, access would fault */

 16-bit code segment? */

 popf and iret */

 CHECKME: 64 65 */

 opcode and address size prefixes */

 irrelevant prefixes (segment overrides and repeats) */

 32-bit mode: register increment */

 64-bit mode: REX prefix */

 CHECKME: f2, f3 */

		/*

		 * pushf: NOTE! We should probably not let

		 * the user see the TF bit being set. But

		 * it's more pain than it's worth to avoid

		 * it, and a debugger could emulate this

		 * all in user space if it _really_ cares.

/*

 * Enable single-stepping.  Return nonzero if user mode is not using TF itself.

	/*

	 * If we stepped into a sysenter/syscall insn, it trapped in

	 * kernel mode; do_debug() cleared TF and set TIF_SINGLESTEP.

	 * If user-mode had set TF itself, then it's still clear from

	 * do_debug() and we need to set it again to restore the user

	 * state so we don't wrongly set TIF_FORCED_TF below.

	 * If enable_single_step() was used last and that is what

	 * set TIF_SINGLESTEP, then both TF and TIF_FORCED_TF are

	 * already set and our bookkeeping is fine.

	/*

	 * Always set TIF_SINGLESTEP.  This will also

	 * cause us to set TF when returning to user mode.

	/*

	 * Ensure that a trap is triggered once stepping out of a system

	 * call prior to executing any user instruction.

 Set TF on the kernel stack.. */

	/*

	 * ..but if TF is changed by the instruction we will trace,

	 * don't mark it as being "us" that set it, so that we

	 * won't clear it by hand later.

	 *

	 * Note that if we don't actually execute the popf because

	 * of a signal arriving right now or suchlike, we will lose

	 * track of the fact that it really was "us" that set it.

	/*

	 * If TF was already set, check whether it was us who set it.

	 * If not, we should never attempt a block step.

	/*

	 * Ensure irq/preemption can't change debugctl in between.

	 * Note also that both TIF_BLOCKSTEP and debugctl should

	 * be changed atomically wrt preemption.

	 *

	 * NOTE: this means that set/clear TIF_BLOCKSTEP is only safe if

	 * task is current or it can't be running, otherwise we can race

	 * with __switch_to_xtra(). We rely on ptrace_freeze_traced() but

	 * PTRACE_KILL is not safe.

/*

 * Enable single or block step.

	/*

	 * Make sure block stepping (BTF) is not enabled unless it should be.

	 * Note that we don't try to worry about any is_setting_trap_flag()

	 * instructions after the first when using block stepping.

	 * So no one should try to use debugger block stepping in a program

	 * that uses user-mode single stepping itself.

	/*

	 * Make sure block stepping (BTF) is disabled.

 Always clear TIF_SINGLESTEP... */

 But touch TF only if it was set by us.. */

 SPDX-License-Identifier: GPL-2.0-or-later

 /*

 *	x86 SMP booting functions

 *

 *	(c) 1995 Alan Cox, Building #3 <alan@lxorguk.ukuu.org.uk>

 *	(c) 1998, 1999, 2000, 2009 Ingo Molnar <mingo@redhat.com>

 *	Copyright 2001 Andi Kleen, SuSE Labs.

 *

 *	Much of the core SMP work is based on previous work by Thomas Radke, to

 *	whom a great many thanks are extended.

 *

 *	Thanks to Intel for making available several different Pentium,

 *	Pentium Pro and Pentium-II/Xeon MP machines.

 *	Original development of Linux SMP code supported by Caldera.

 *

 *	Fixes

 *		Felix Koop	:	NR_CPUS used properly

 *		Jose Renau	:	Handle single CPU case.

 *		Alan Cox	:	By repeated request 8) - Total BogoMIPS report.

 *		Greg Wright	:	Fix for kernel stacks panic.

 *		Erich Boleyn	:	MP v1.4 and additional changes.

 *	Matthias Sattler	:	Changes for 2.1 kernel map.

 *	Michel Lespinasse	:	Changes for 2.1 kernel map.

 *	Michael Chastain	:	Change trampoline.S to gnu as.

 *		Alan Cox	:	Dumb bug: 'B' step PPro's are fine

 *		Ingo Molnar	:	Added APIC timers, based on code

 *					from Jose Renau

 *		Ingo Molnar	:	various cleanups and rewrites

 *		Tigran Aivazian	:	fixed "0.00 in /proc/uptime on SMP" bug.

 *	Maciej W. Rozycki	:	Bits for genuine 82489DX APICs

 *	Andi Kleen		:	Changed for SMP boot into long mode.

 *		Martin J. Bligh	: 	Added support for multi-quad systems

 *		Dave Jones	:	Report invalid combinations of Athlon CPUs.

 *		Rusty Russell	:	Hacked into shape for new "hotplug" boot process.

 *      Andi Kleen              :       Converted to new state machine.

 *	Ashok Raj		: 	CPU hotplug support

 *	Glauber Costa		:	i386 and x86_64 integration

 representing HT siblings of each logical CPU */

 representing HT and core siblings of each logical CPU */

 representing HT, core, and die siblings of each logical CPU */

 Per CPU bogomips and other parameters */

 Logical package management. We might want to allocate that dynamically */

 Maximum number of SMT threads on any online core */

 Flag to indicate if a complete sched domain rebuild is required */

	/*

	 * Paranoid:  Set warm reset code and vector here back

	 * to default values.

/*

 * Report back to the Boot Processor during boot time or to the caller processor

 * during CPU online.

	/*

	 * If waken up by an INIT in an 82489DX configuration

	 * cpu_callout_mask guarantees we don't get here before

	 * an INIT_deassert IPI reaches our local APIC, so it is

	 * now safe to touch our local APIC.

	/*

	 * the boot CPU has finished the init stage and is spinning

	 * on callin_map until we finish. We are free to set up this

	 * CPU, first the APIC. (this is probably redundant on most

	 * boards)

	/*

	 * Save our processor parameters. Note: this information

	 * is needed for clock calibration.

	/*

	 * The topology information must be up to date before

	 * calibrate_delay() and notify_cpu_starting().

	/*

	 * Get our bogomips.

	 * Update loops_per_jiffy in cpu_data. Previous call to

	 * smp_store_cpu_info() stored a value that is close but not as

	 * accurate as the value just calculated.

	/*

	 * Allow the master to continue.

/*

 * Activate a secondary processor.

	/*

	 * Don't put *anything* except direct CPU state initialization

	 * before cpu_init(), SMP booting is too fragile that we want to

	 * limit the things done here to the most necessary things.

 switch away from the initial page table */

 otherwise gcc will move up smp_processor_id before the cpu_init */

	/*

	 * Check TSC synchronization with the boot CPU:

	/*

	 * Lock vector_lock, set CPU online and bring the vector

	 * allocator online. Online must be set with vector_lock held

	 * to prevent a concurrent irq setup/teardown from seeing a

	 * half valid vector space.

 enable local interrupts */

/**

 * topology_is_primary_thread - Check whether CPU is the primary SMT thread

 * @cpu:	CPU to check

/**

 * topology_smt_supported - Check whether SMT is supported by the CPUs

/**

 * topology_phys_to_logical_pkg - Map a physical package id to a logical

 *

 * Returns logical package id or -1 if not found

/**

 * topology_phys_to_logical_die - Map a physical die id to logical

 *

 * Returns logical die id or -1 if not found

/**

 * topology_update_package_map - Update the physical to logical package map

 * @pkg:	The physical package id as retrieved via CPUID

 * @cpu:	The cpu for which this is updated

 Already available somewhere? */

/**

 * topology_update_die_map - Update the physical to logical die map

 * @die:	The die id as retrieved via CPUID

 * @cpu:	The cpu for which this is updated

 Already available somewhere? */

 CPU 0 */

/*

 * The bootstrap kernel entry code has set these up. Save them for

 * a given CPU

 Copy boot_cpu_data only on the first bringup */

	/*

	 * During boot time, CPU0 has this setup already. Save the info when

	 * bringing up AP or offlined CPU0.

 If the arch didn't set up l2c_id, fall back to SMT */

 Do not match if L2 cache id does not match: */

/*

 * Unlike the other levels, we do not enforce keeping a

 * multicore group inside a NUMA node.  If this happens, we will

 * discard the MC level of the topology later.

/*

 * Define intel_cod_cpu[] for Intel COD (Cluster-on-Die) CPUs.

 *

 * Any Intel CPU that has multiple nodes per package and does not

 * match intel_cod_cpu[] has the SNC (Sub-NUMA Cluster) topology.

 *

 * When in SNC mode, these CPUs enumerate an LLC that is shared

 * by multiple NUMA nodes. The LLC is shared for off-package data

 * access but private to the NUMA node (half of the package) for

 * on-package access. CPUID (the source of the information about

 * the LLC) can only enumerate the cache as shared or unshared,

 * but not this particular configuration.

 COD */

 COD */

 SNC */

 Do not match if we do not have a valid APICID for cpu: */

 Do not match if LLC id does not match: */

	/*

	 * Allow the SNC topology without warning. Return of false

	 * means 'c' does not share the LLC of 'o'. This will be

	 * reflected to userspace.

/*

 * Set if a package/die has multiple NUMA nodes inside.

 * AMD Magny-Cours, Intel Cluster-on-Die, and Intel

 * Sub-NUMA Clustering have this.

	/*

	 * This needs a separate iteration over the cpus because we rely on all

	 * topology_sibling_cpumask links to be set-up.

			/*

			 *  Does this new cpu bringup a new core?

				/*

				 * for each core in package, increment

				 * the booted_cores for this new cpu

				/*

				 * increment the core count for all

				 * the other cpus in this package

 maps the cpu to the sched domain representing multi-core */

	/*

	 * Allow the user to impress friends.

		/*

		 * Wait for idle.

/*

 * The Multiprocessor Specification 1.4 (1997) example code suggests

 * that there should be a 10ms delay between the BSP asserting INIT

 * and de-asserting INIT, when starting a remote processor.

 * But that slows boot and resume on modern processors, which include

 * many cores and don't require that delay.

 *

 * Cmdline "init_cpu_udelay=" is available to over-ride this delay.

 * Modern processor families are quirked to remove the delay entirely.

 if cmdline changed it from default, leave it alone */

 if modern processor, use no delay */

 else, use legacy delay */

/*

 * Poke the other CPU in the eye via NMI to wake it up. Remember that the normal

 * INIT, INIT, STARTUP sequence will reset the chip hard for us, and this

 * won't ... remember to clear down the APIC, etc later.

 Target chip */

 Boot on the stack */

 Kick the second */

	/*

	 * Give the other CPU some time to accept the IPI.

 Due to the Pentium erratum 3AP.  */

	/*

	 * Be paranoid about clearing APIC errors.

 Due to the Pentium erratum 3AP.  */

	/*

	 * Turn INIT on target chip

	/*

	 * Send IPI

 Target chip */

 Send IPI */

	/*

	 * Should we send STARTUP IPIs ?

	 *

	 * Determine this based on the APIC version.

	 * If we don't have an integrated APIC, don't send the STARTUP IPIs.

	/*

	 * Run STARTUP IPI loop.

 Due to the Pentium erratum 3AP.  */

		/*

		 * STARTUP IPI

 Target chip */

 Boot on the stack */

 Kick the second */

		/*

		 * Give the other CPU some time to accept the IPI.

		/*

		 * Give the other CPU some time to accept the IPI.

 Due to the Pentium erratum 3AP.  */

 reduce the number of lines printed when booting a large cpu count system */

 + '#' sign */

 + '#' */

 Add padding for the BSP */

/*

 * Wake up AP by INIT, INIT, STARTUP sequence.

 *

 * Instead of waiting for STARTUP after INITs, BSP will execute the BIOS

 * boot-strap code which is not a desired behavior for waking up BSP. To

 * void the boot-strap code, wake up CPU0 by NMI instead.

 *

 * This works to wake up soft offlined CPU0 only. If CPU0 is hard offlined

 * (i.e. physically hot removed and then hot added), NMI won't wake it up.

 * We'll change this code in the future to wake up hard offlined CPU0 if

 * real platform and request are available.

	/*

	 * Wake up AP by INIT, INIT, STARTUP sequence.

	/*

	 * Wake up BSP by nmi.

	 *

	 * Register a NMI handler to help wake up CPU0.

 Just in case we booted with a single CPU. */

 Initialize the interrupt stack(s) */

 Stack for startup_32 can be just as for start_secondary onwards */

/*

 * NOTE - on most systems this is a PHYSICAL apic ID, but on multiquad

 * (ie clustered apic addressing mode), this is a LOGICAL apic ID.

 * Returns zero if CPU booted OK, else error code from

 * ->wakeup_secondary_cpu.

 start_ip had better be page-aligned! */

 Enable the espfix hack for this CPU */

 So we see what's up */

	/*

	 * This grunge runs the startup process for

	 * the targeted processor.

		/*

		 * Be paranoid about clearing APIC errors.

	/*

	 * AP might wait on cpu_callout_mask in cpu_init() with

	 * cpu_initialized_mask set if previous attempt to online

	 * it timed-out. Clear cpu_initialized_mask so that after

	 * INIT/SIPI it could start with a clean state.

	/*

	 * Wake up a CPU in difference cases:

	 * - Use the method in the APIC driver if it's defined

	 * Otherwise,

	 * - Use an INIT boot APIC message for APs or NMI for BSP.

		/*

		 * Wait 10s total for first sign of life from AP

				/*

				 * Tell AP to proceed with initialization

		/*

		 * Wait till AP completes initial initialization

			/*

			 * Allow other tasks to run while we wait for the

			 * AP to come online. This also gives a chance

			 * for the MTRR work(triggered by the AP coming online)

			 * to be completed in the stop machine context.

		/*

		 * Cleanup possible dangling ends...

	/*

	 * Already booted CPU?

	/*

	 * Save current MTRR state in case it was changed since early boot

	 * (e.g. by the ACPI SMI) to initialize new CPUs with MTRRs in sync:

 x86 CPUs take themselves offline, so delayed offline is OK. */

 the FPU context is blank, nobody can own it */

	/*

	 * Check TSC synchronization with the AP (keep irqs disabled

	 * while doing so):

	/*

	 * Clean up the nmi handler. Do this after the callin and callout sync

	 * to avoid impact of possible long unregister time.

/**

 * arch_disable_smp_support() - disables SMP support for x86 at runtime

/*

 * Fall back to non SMP mode after errors.

 *

 * RED-PEN audit/test this more. I bet there is more state messed up here.

/*

 * Various sanity checks.

	/*

	 * Should not be necessary because the MP table should list the boot

	 * CPU too, but we do it for the sake of robustness anyway.

 mark all to hotplug */

	/*

	 * Setup boot CPU information

 Final full version of the data */

	/*

	 * Set 'default' x86 topology, this matches default_topology() in that

	 * it has NUMA nodes as a topology level. See also

	 * native_smp_cpus_done().

	 *

	 * Must be done before set_cpus_sibling_map() is ran.

/*

 * Prepare for SMP bootup.

 * @max_cpus: configured maximum number of CPUs, It is a legacy parameter

 *            for common interface support.

 Setup local timer */

 Setup local timer */

/*

 * Early setup to make printk work.

 already set me in cpu_online_mask in boot_cpu_init() */

	/*

	 * Today neither Intel nor AMD support heterogeneous systems so

	 * extrapolate the boot cpu's data to all packages.

/*

 * cpu_possible_mask should be static, it cannot change as cpu's

 * are onlined, or offlined. The reason is per-cpu data-structures

 * are allocated by some modules at init time, and don't expect to

 * do this dynamically on cpu arrival/departure.

 * cpu_present_mask on the other hand can change dynamically.

 * In case when cpu_hotplug is not compiled, then we resort to current

 * behaviour, which is cpu_possible == cpu_present.

 * - Ashok Raj

 *

 * Three ways to find out the number of additional hotplug CPUs:

 * - If the BIOS specified disabled CPUs in ACPI/mptables use that.

 * - The user can overwrite it with possible_cpus=NUM

 * - Otherwise don't reserve additional CPUs.

 * We do this because additional CPUs waste a lot of memory.

 * -AK

 No boot processor was found in mptable or ACPI MADT */

 Make sure boot cpu is enumerated */

 nr_cpu_ids could be reduced via nr_cpus= */

 Recompute SMT state for all CPUs on offline */

/

		 */

		if (cpumask_weight(topology_sibling_cpumask(cpu)) == 1)

			cpu_data(sibling).booted_cores--;

	}



	for_each_cpu(sibling, topology_die_cpumask(cpu))

		cpumask_clear_cpu(cpu, topology_die_cpumask(sibling));



	for_each_cpu(sibling, topology_sibling_cpumask(cpu)) {

		cpumask_clear_cpu(cpu, topology_sibling_cpumask(sibling));

		if (cpumask_weight(topology_sibling_cpumask(sibling)) == 1)

			cpu_data(sibling).smt_active = false;

	}



	for_each_cpu(sibling, cpu_llc_shared_mask(cpu))

		cpumask_clear_cpu(cpu, cpu_llc_shared_mask(sibling));

	for_each_cpu(sibling, cpu_l2c_shared_mask(cpu))

		cpumask_clear_cpu(cpu, cpu_l2c_shared_mask(sibling));

	cpumask_clear(cpu_llc_shared_mask(cpu));

	cpumask_clear(cpu_l2c_shared_mask(cpu));

	cpumask_clear(topology_sibling_cpumask(cpu));

	cpumask_clear(topology_core_cpumask(cpu));

	cpumask_clear(topology_die_cpumask(cpu));

	c->cpu_core_id = 0;

	c->booted_cores = 0;

	cpumask_clear_cpu(cpu, cpu_sibling_setup_mask);

	recompute_smt_state();

}



static void remove_cpu_from_maps(int cpu)

{

	set_cpu_online(cpu, false);

	cpumask_clear_cpu(cpu, cpu_callout_mask);

	cpumask_clear_cpu(cpu, cpu_callin_mask);

	/* was set by cpu_init() */

	cpumask_clear_cpu(cpu, cpu_initialized_mask);

	numa_remove_cpu(cpu);

}



void cpu_disable_common(void)

{

	int cpu = smp_processor_id();



	remove_siblinginfo(cpu);



	/* It's now safe to remove this processor from the online map */

	lock_vector_lock();

	remove_cpu_from_maps(cpu);

	unlock_vector_lock();

	fixup_irqs();

	lapic_offline();

}



int native_cpu_disable(void)

{

	int ret;



	ret = lapic_can_unplug_cpu();

	if (ret)

		return ret;



	cpu_disable_common();





         */

	apic_soft_disable();



	return 0;

}



int common_cpu_die(unsigned int cpu)

{

	int ret = 0;



	/* We don't do anything here: idle task is faking death itself. */



	/* They ack this in play_dead() by setting CPU_DEAD */

	if (cpu_wait_death(cpu, 5)) {

		if (system_state == SYSTEM_RUNNING)

			pr_info("CPU %u is now offline\n", cpu);

	} else {

		pr_err("CPU %u didn't die...\n", cpu);

		ret = -1;

	}



	return ret;

}



void native_cpu_die(unsigned int cpu)

{

	common_cpu_die(cpu);

}



void play_dead_common(void)

{

	idle_task_exit();



	/* Ack it */

	(void)cpu_report_death();





	 */

	local_irq_disable();

}



*

 */

void cond_wakeup_cpu0(void)

{

	if (smp_processor_id() == 0 && enable_start_cpu0)

		start_cpu0();

}

EXPORT_SYMBOL_GPL(cond_wakeup_cpu0);





 */

static inline void mwait_play_dead(void)

{

	unsigned int eax, ebx, ecx, edx;

	unsigned int highest_cstate = 0;

	unsigned int highest_subcstate = 0;

	void *mwait_ptr;

	int i;



	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD ||

	    boot_cpu_data.x86_vendor == X86_VENDOR_HYGON)

		return;

	if (!this_cpu_has(X86_FEATURE_MWAIT))

		return;

	if (!this_cpu_has(X86_FEATURE_CLFLUSH))

		return;

	if (__this_cpu_read(cpu_info.cpuid_level) < CPUID_MWAIT_LEAF)

		return;



	eax = CPUID_MWAIT_LEAF;

	ecx = 0;

	native_cpuid(&eax, &ebx, &ecx, &edx);





	 */

	if (!(ecx & CPUID5_ECX_EXTENSIONS_SUPPORTED)) {

		eax = 0;

	} else {

		edx >>= MWAIT_SUBSTATE_SIZE;

		for (i = 0; i < 7 && edx; i++, edx >>= MWAIT_SUBSTATE_SIZE) {

			if (edx & MWAIT_SUBSTATE_MASK) {

				highest_cstate = i;

				highest_subcstate = edx & MWAIT_SUBSTATE_MASK;

			}

		}

		eax = (highest_cstate << MWAIT_SUBSTATE_SIZE) |

			(highest_subcstate - 1);

	}





	 */

	mwait_ptr = &current_thread_info()->flags;



	wbinvd();



	while (1) {



		 */

		mb();

		clflush(mwait_ptr);

		mb();

		__monitor(mwait_ptr, 0, 0);

		mb();

		__mwait(eax, 0);



		cond_wakeup_cpu0();

	}

}



void hlt_play_dead(void)

{

	if (__this_cpu_read(cpu_info.x86) >= 4)

		wbinvd();



	while (1) {

		native_halt();



		cond_wakeup_cpu0();

	}

}



void native_play_dead(void)

{

	play_dead_common();

	tboot_shutdown(TB_SHUTDOWN_WFS);



	mwait_play_dead();	/* Only returns on failure */

	if (cpuidle_play_dead())

		hlt_play_dead();

}



#else /* ... !CONFIG_HOTPLUG_CPU */

int native_cpu_disable(void)

{

	return -ENOSYS;

}



void native_cpu_die(unsigned int cpu)

{

	/* We said "no" in __cpu_disable */

	BUG();

}



void native_play_dead(void)

{

	BUG();

}



#endif



#ifdef CONFIG_X86_64



 */



DEFINE_STATIC_KEY_FALSE(arch_scale_freq_key);



static DEFINE_PER_CPU(u64, arch_prev_aperf);

static DEFINE_PER_CPU(u64, arch_prev_mperf);

static u64 arch_turbo_freq_ratio = SCHED_CAPACITY_SCALE;

static u64 arch_max_freq_ratio = SCHED_CAPACITY_SCALE;



void arch_set_max_freq_ratio(bool turbo_disabled)

{

	arch_max_freq_ratio = turbo_disabled ? SCHED_CAPACITY_SCALE :

					arch_turbo_freq_ratio;

}

EXPORT_SYMBOL_GPL(arch_set_max_freq_ratio);



static bool turbo_disabled(void)

{

	u64 misc_en;

	int err;



	err = rdmsrl_safe(MSR_IA32_MISC_ENABLE, &misc_en);

	if (err)

		return false;



	return (misc_en & MSR_IA32_MISC_ENABLE_TURBO_DISABLE);

}



static bool slv_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq)

{

	int err;



	err = rdmsrl_safe(MSR_ATOM_CORE_RATIOS, base_freq);

	if (err)

		return false;



	err = rdmsrl_safe(MSR_ATOM_CORE_TURBO_RATIOS, turbo_freq);

	if (err)

		return false;



	*base_freq = (*base_freq >> 16) & 0x3F;     /* max P state */

	*turbo_freq = *turbo_freq & 0x3F;           /* 1C turbo    */



	return true;

}



#define X86_MATCH(model)					\

	X86_MATCH_VENDOR_FAM_MODEL_FEATURE(INTEL, 6,		\

		INTEL_FAM6_##model, X86_FEATURE_APERFMPERF, NULL)



static const struct x86_cpu_id has_knl_turbo_ratio_limits[] = {

	X86_MATCH(XEON_PHI_KNL),

	X86_MATCH(XEON_PHI_KNM),

	{}

};



static const struct x86_cpu_id has_skx_turbo_ratio_limits[] = {

	X86_MATCH(SKYLAKE_X),

	{}

};



static const struct x86_cpu_id has_glm_turbo_ratio_limits[] = {

	X86_MATCH(ATOM_GOLDMONT),

	X86_MATCH(ATOM_GOLDMONT_D),

	X86_MATCH(ATOM_GOLDMONT_PLUS),

	{}

};



static bool knl_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq,

				int num_delta_fratio)

{

	int fratio, delta_fratio, found;

	int err, i;

	u64 msr;



	err = rdmsrl_safe(MSR_PLATFORM_INFO, base_freq);

	if (err)

		return false;



	*base_freq = (*base_freq >> 8) & 0xFF;	    /* max P state */



	err = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, &msr);

	if (err)

		return false;



	fratio = (msr >> 8) & 0xFF;

	i = 16;

	found = 0;

	do {

		if (found >= num_delta_fratio) {

			*turbo_freq = fratio;

			return true;

		}



		delta_fratio = (msr >> (i + 5)) & 0x7;



		if (delta_fratio) {

			found += 1;

			fratio -= delta_fratio;

		}



		i += 8;

	} while (i < 64);



	return true;

}



static bool skx_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq, int size)

{

	u64 ratios, counts;

	u32 group_size;

	int err, i;



	err = rdmsrl_safe(MSR_PLATFORM_INFO, base_freq);

	if (err)

		return false;



	*base_freq = (*base_freq >> 8) & 0xFF;      /* max P state */



	err = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, &ratios);

	if (err)

		return false;



	err = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT1, &counts);

	if (err)

		return false;



	for (i = 0; i < 64; i += 8) {

		group_size = (counts >> i) & 0xFF;

		if (group_size >= size) {

			*turbo_freq = (ratios >> i) & 0xFF;

			return true;

		}

	}



	return false;

}



static bool core_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq)

{

	u64 msr;

	int err;



	err = rdmsrl_safe(MSR_PLATFORM_INFO, base_freq);

	if (err)

		return false;



	err = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, &msr);

	if (err)

		return false;



	*base_freq = (*base_freq >> 8) & 0xFF;    /* max P state */

	*turbo_freq = (msr >> 24) & 0xFF;         /* 4C turbo    */



	/* The CPU may have less than 4 cores */

	if (!*turbo_freq)

		*turbo_freq = msr & 0xFF;         /* 1C turbo    */



	return true;

}



static bool intel_set_max_freq_ratio(void)

{

	u64 base_freq, turbo_freq;

	u64 turbo_ratio;



	if (slv_set_max_freq_ratio(&base_freq, &turbo_freq))

		goto out;



	if (x86_match_cpu(has_glm_turbo_ratio_limits) &&

	    skx_set_max_freq_ratio(&base_freq, &turbo_freq, 1))

		goto out;



	if (x86_match_cpu(has_knl_turbo_ratio_limits) &&

	    knl_set_max_freq_ratio(&base_freq, &turbo_freq, 1))

		goto out;



	if (x86_match_cpu(has_skx_turbo_ratio_limits) &&

	    skx_set_max_freq_ratio(&base_freq, &turbo_freq, 4))

		goto out;



	if (core_set_max_freq_ratio(&base_freq, &turbo_freq))

		goto out;



	return false;



out:



	 */

	if (!base_freq || !turbo_freq) {

		pr_debug("Couldn't determine cpu base or turbo frequency, necessary for scale-invariant accounting.\n");

		return false;

	}



	turbo_ratio = div_u64(turbo_freq * SCHED_CAPACITY_SCALE, base_freq);

	if (!turbo_ratio) {

		pr_debug("Non-zero turbo and base frequencies led to a 0 ratio.\n");

		return false;

	}



	arch_turbo_freq_ratio = turbo_ratio;

	arch_set_max_freq_ratio(turbo_disabled());



	return true;

}



#ifdef CONFIG_ACPI_CPPC_LIB

static bool amd_set_max_freq_ratio(void)

{

	struct cppc_perf_caps perf_caps;

	u64 highest_perf, nominal_perf;

	u64 perf_ratio;

	int rc;



	rc = cppc_get_perf_caps(0, &perf_caps);

	if (rc) {

		pr_debug("Could not retrieve perf counters (%d)\n", rc);

		return false;

	}



	highest_perf = amd_get_highest_perf();

	nominal_perf = perf_caps.nominal_perf;



	if (!highest_perf || !nominal_perf) {

		pr_debug("Could not retrieve highest or nominal performance\n");

		return false;

	}



	perf_ratio = div_u64(highest_perf * SCHED_CAPACITY_SCALE, nominal_perf);

	/* midpoint between max_boost and max_P */

	perf_ratio = (perf_ratio + SCHED_CAPACITY_SCALE) >> 1;

	if (!perf_ratio) {

		pr_debug("Non-zero highest/nominal perf values led to a 0 ratio\n");

		return false;

	}



	arch_turbo_freq_ratio = perf_ratio;

	arch_set_max_freq_ratio(false);



	return true;

}

#else

static bool amd_set_max_freq_ratio(void)

{

	return false;

}

#endif



static void init_counter_refs(void)

{

	u64 aperf, mperf;



	rdmsrl(MSR_IA32_APERF, aperf);

	rdmsrl(MSR_IA32_MPERF, mperf);



	this_cpu_write(arch_prev_aperf, aperf);

	this_cpu_write(arch_prev_mperf, mperf);

}



#ifdef CONFIG_PM_SLEEP

static struct syscore_ops freq_invariance_syscore_ops = {

	.resume = init_counter_refs,

};



static void register_freq_invariance_syscore_ops(void)

{

	/* Bail out if registered already. */

	if (freq_invariance_syscore_ops.node.prev)

		return;



	register_syscore_ops(&freq_invariance_syscore_ops);

}

#else

static inline void register_freq_invariance_syscore_ops(void) {}

#endif



static void init_freq_invariance(bool secondary, bool cppc_ready)

{

	bool ret = false;



	if (!boot_cpu_has(X86_FEATURE_APERFMPERF))

		return;



	if (secondary) {

		if (static_branch_likely(&arch_scale_freq_key)) {

			init_counter_refs();

		}

		return;

	}



	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)

		ret = intel_set_max_freq_ratio();

	else if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {

		if (!cppc_ready) {

			return;

		}

		ret = amd_set_max_freq_ratio();

	}



	if (ret) {

		init_counter_refs();

		static_branch_enable(&arch_scale_freq_key);

		register_freq_invariance_syscore_ops();

		pr_info("Estimated ratio of average max frequency by base frequency (times 1024): %llu\n", arch_max_freq_ratio);

	} else {

		pr_debug("Couldn't determine max cpu frequency, necessary for scale-invariant accounting.\n");

	}

}



#ifdef CONFIG_ACPI_CPPC_LIB

static DEFINE_MUTEX(freq_invariance_lock);



void init_freq_invariance_cppc(void)

{

	static bool secondary;



	mutex_lock(&freq_invariance_lock);



	init_freq_invariance(secondary, true);

	secondary = true;



	mutex_unlock(&freq_invariance_lock);

}

#endif



static void disable_freq_invariance_workfn(struct work_struct *work)

{

	static_branch_disable(&arch_scale_freq_key);

}



static DECLARE_WORK(disable_freq_invariance_work,

		    disable_freq_invariance_workfn);



DEFINE_PER_CPU(unsigned long, arch_freq_scale) = SCHED_CAPACITY_SCALE;



void arch_scale_freq_tick(void)

{

	u64 freq_scale;

	u64 aperf, mperf;

	u64 acnt, mcnt;



	if (!arch_scale_freq_invariant())

		return;



	rdmsrl(MSR_IA32_APERF, aperf);

	rdmsrl(MSR_IA32_MPERF, mperf);



	acnt = aperf - this_cpu_read(arch_prev_aperf);

	mcnt = mperf - this_cpu_read(arch_prev_mperf);



	this_cpu_write(arch_prev_aperf, aperf);

	this_cpu_write(arch_prev_mperf, mperf);



	if (check_shl_overflow(acnt, 2*SCHED_CAPACITY_SHIFT, &acnt))

		goto error;



	if (check_mul_overflow(mcnt, arch_max_freq_ratio, &mcnt) || !mcnt)

		goto error;



	freq_scale = div64_u64(acnt, mcnt);

	if (!freq_scale)

		goto error;



	if (freq_scale > SCHED_CAPACITY_SCALE)

		freq_scale = SCHED_CAPACITY_SCALE;



	this_cpu_write(arch_freq_scale, freq_scale);

	return;



error:

	pr_warn("Scheduler frequency invariance went wobbly, disabling!\n");

	schedule_work(&disable_freq_invariance_work);

}

#else

static inline void init_freq_invariance(bool secondary, bool cppc_ready)

{

}

#endif /* CONFIG_X86_64 */

/*

 * Copyright (C) 2009 Thomas Gleixner <tglx@linutronix.de>

 *

 *  For licencing details see kernel-base/COPYING

/*

 * Allow devicetree configured systems to disable the RTC by setting the

 * corresponding DT node's status property to disabled. Code is optimized

 * out for CONFIG_OF=n builds.

/*

 * The platform setup functions are preset with the default functions

 * for standard PC hardware.

 MSI arch specific hooks */

/*

 * Stack trace management functions

 *

 *  Copyright (C) 2006-2009 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 Success path for user tasks */

			/*

			 * Kernel mode registers on the stack indicate an

			 * in-kernel interrupt or exception (e.g., preemption

			 * or a page fault), which can make frame pointers

			 * unreliable.

		/*

		 * A NULL or invalid return address probably means there's some

		 * generated code which __kernel_text_address() doesn't know

		 * about.

 Check for stack corruption */

 Userspace stacktrace - based on kernel/trace/trace_sysprof.c */

 SPDX-License-Identifier: GPL-2.0-or-later

/* ----------------------------------------------------------------------- *

 *

 *   Copyright 2000-2008 H. Peter Anvin - All Rights Reserved

 *

/*

 * x86 CPUID access device

 *

 * This device is accessed by lseek() to the appropriate CPUID level

 * and then read in chunks of 16 bytes.  A larger size means multiple

 * reads of consecutive levels.

 *

 * The lower 32 bits of the file position is used as the incoming %eax,

 * and the upper 32 bits of the file position as the incoming %ecx,

 * the latter intended for "counting" eax levels like eax=4.

 *

 * This driver uses /dev/cpu/%d/cpuid where %d is the minor number, and on

 * an SMP box will direct the access to CPU %d.

 Invalid chunk size */

 No such CPU */

 CPUID not supported */

/*

 * File operations we support

/*

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *  Copyright (C) 2000, 2001, 2002 Andi Kleen, SuSE Labs

 *

 *  Pentium III FXSR, SSE support

 *	Gareth Hughes <gareth@valinux.com>, May 2000

/*

 * Handle hardware traps and faults.

	/*

	 * We got #UD, if the text isn't readable we'd have gotten

	 * a different exception.

		/*

		 * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.

		 * On nmi (interrupt 2), do_trap should not be called.

	/*

	 * We want error_code and trap_nr set for userspace faults and

	 * kernelspace faults which result in die(), but not

	 * kernelspace faults which are fixed up.  die() gives the

	 * process no chance to handle the signal and notice the

	 * kernel fault information, so that won't result in polluting

	 * the information about previously queued, but not yet

	 * delivered, faults.  See also exc_general_protection below.

/*

 * Posix requires to provide the address of the faulting instruction for

 * SIGILL (#UD) and SIGFPE (#DE) in the si_addr member of siginfo_t.

 *

 * This address is usually regs->ip, but when an uprobe moved the code out

 * of line then regs->ip points to the XOL code which would confuse

 * anything which analyzes the fault address vs. the unmodified binary. If

 * a trap happened in XOL code then uprobe maps regs->ip back to the

 * original instruction address.

	/*

	 * All lies, just get the WARN/BUG out.

	/*

	 * Since we're emulating a CALL with exceptions, restore the interrupt

	 * state to what it was at the exception site.

	/*

	 * We use UD2 as a short encoding for 'CALL __WARN', as such

	 * handle it before exception entry to avoid recursive WARN

	 * in case exception entry is the one triggering WARNs.

 Be absolutely certain we don't return. */

/*

 * Runs on an IST stack for x86_64 and on a special task stack for x86_32.

 *

 * On x86_64, this is more or less a normal kernel entry.  Notwithstanding the

 * SDM's warnings about double faults being unrecoverable, returning works as

 * expected.  Presumably what the SDM actually means is that the CPU may get

 * the register state wrong on entry, so returning could be a bad idea.

 *

 * Various CPU engineers have promised that double faults due to an IRET fault

 * while the stack is read-only are, in fact, recoverable.

 *

 * On x86_32, this is entered through a task gate, and regs are synthesized

 * from the TSS.  Returning is, in principle, okay, but changes to regs will

 * be lost.  If, for some reason, we need to return to a context with modified

 * regs, the shim code could be adjusted to synchronize the registers.

 *

 * The 32bit #DF shim provides CR2 already as an argument. On 64bit it needs

 * to be read before doing anything else.

	/*

	 * If IRET takes a non-IST fault on the espfix64 stack, then we

	 * end up promoting it to a doublefault.  In that case, take

	 * advantage of the fact that we're not using the normal (TSS.sp0)

	 * stack right now.  We can write a fake #GP(0) frame at TSS.sp0

	 * and then modify our own IRET frame so that, when we return,

	 * we land directly at the #GP(0) vector with the stack already

	 * set up according to its expectations.

	 *

	 * The net result is that our #GP handler will think that we

	 * entered from usermode with the bad user context.

	 *

	 * No need for nmi_enter() here because we don't use RCU.

		/*

		 * regs->sp points to the failing IRET frame on the

		 * ESPFIX64 stack.  Copy it to the entry stack.  This fills

		 * in gpregs->ss through gpregs->ip.

		 *

 Missing (lost) #GP error code */

		/*

		 * Adjust our frame so that we return straight to the #GP

		 * vector with the expected RSP value.  This is safe because

		 * we won't enable interrupts or schedule before we invoke

		 * general_protection, so nothing will clobber the stack

		 * frame we just set up.

		 *

		 * We will enter general_protection with kernel GSBASE,

		 * which is what the stub expects, given that the faulting

		 * RIP will be the IRET instruction.

	/*

	 * If we overflow the stack into a guard page, the CPU will fail

	 * to deliver #PF and will send #DF instead.  Similarly, if we

	 * take any non-IST exception while too close to the bottom of

	 * the stack, the processor will get a page fault while

	 * delivering the exception and will generate a double fault.

	 *

	 * According to the SDM (footnote in 6.15 under "Interrupt 14 -

	 * Page-Fault Exception (#PF):

	 *

	 *   Processors update CR2 whenever a page fault is detected. If a

	 *   second page fault occurs while an earlier page fault is being

	 *   delivered, the faulting linear address of the second fault will

	 *   overwrite the contents of CR2 (replacing the previous

	 *   address). These updates to CR2 occur even if the page fault

	 *   results in a double fault or occurs during the delivery of a

	 *   double fault.

	 *

	 * The logic below has a small possibility of incorrectly diagnosing

	 * some errors as stack overflows.  For example, if the IDT or GDT

	 * gets corrupted such that #GP delivery fails due to a bad descriptor

	 * causing #GP and we hit this condition while CR2 coincidentally

	 * points to the stack guard page, we'll think we overflowed the

	 * stack.  Given that we're going to panic one way or another

	 * if this happens, this isn't necessarily worth fixing.

	 *

	 * If necessary, we could improve the test by only diagnosing

	 * a stack overflow if the saved RSP points within 47 bytes of

	 * the bottom of the stack: if RSP == tsk_stack + 48 and we

	 * take an exception, the stack is already aligned and there

	 * will be enough room SS, RSP, RFLAGS, CS, RIP, and a

	 * possible error code, so a stack overflow would *not* double

	 * fault.  With any less space left, exception delivery could

	 * fail, and, as a practical matter, we've overflowed the

	 * stack even if the actual trigger for the double fault was

	 * something else.

/*

 * When an uncaught #GP occurs, try to determine the memory address accessed by

 * the instruction and return that address to the caller. Also, try to figure

 * out whether any part of the access to that address was non-canonical.

	/*

	 * Check that:

	 *  - the operand is not in the kernel half

	 *  - the last byte of the operand is not in the user canonical half

	/*

	 * To be potentially processing a kprobe fault and to trust the result

	 * from kprobe_running(), we have to be non-preemptible.

	/*

	 * KASAN is interested only in the non-canonical case, clear it

	 * otherwise.

 CONFIG_KGDB_LOW_LEVEL_TRAP */

	/*

	 * poke_int3_handler() is completely self contained code; it does (and

	 * must) *NOT* call out to anything, lest it hits upon yet another

	 * INT3.

	/*

	 * irqentry_enter_from_user_mode() uses static_branch_{,un}likely()

	 * and therefore can trigger INT3, hence poke_int3_handler() must

	 * be done before. If the entry came from kernel mode, then use

	 * nmi_enter() because the INT3 could have been hit in any context

	 * including NMI.

/*

 * Help handler running on a per-cpu (IST or entry trampoline) stack

 * to switch to the normal thread stack if the interrupted code was in

 * user mode. The actual stack switch is done in entry_64.S

	/*

	 * In the SYSCALL entry path the RSP value comes from user-space - don't

	 * trust it and switch to the current kernel stack

	/*

	 * From here on the RSP value is trusted. Now check whether entry

	 * happened from a safe stack. Not safe are the entry or unknown stacks,

	 * use the fall-back stack instead in this case.

	/*

	 * Found a safe stack - switch to it as if the entry didn't happen via

	 * IST stack. The code below only copies pt_regs, the real switch happens

	 * in assembly code.

	/*

	 * This is called from entry_64.S early in handling a fault

	 * caused by a bad iret to user mode.  To handle the fault

	 * correctly, we want to move our stack frame to where it would

	 * be had we entered directly on the entry stack (rather than

	 * just below the IRET frame) and we want to pretend that the

	 * exception came from the IRET target.

 Copy the IRET target to the temporary storage. */

 Copy the remainder of the stack from the current stack. */

 Update the entry stack */

	/*

	 * We don't try for precision here.  If we're anywhere in the region of

	 * code that can be single-stepped in the SYSENTER entry path, then

	 * assume that this is a useless single-step trap due to SYSENTER

	 * being invoked with TF set.  (We don't know in advance exactly

	 * which instructions will be hit because BTF could plausibly

	 * be set.)

	/*

	 * The Intel SDM says:

	 *

	 *   Certain debug exceptions may clear bits 0-3. The remaining

	 *   contents of the DR6 register are never cleared by the

	 *   processor. To avoid confusion in identifying debug

	 *   exceptions, debug handlers should clear the register before

	 *   returning to the interrupted task.

	 *

	 * Keep it simple: clear DR6 immediately.

 Flip to positive polarity */

/*

 * Our handling of the processor debug registers is non-trivial.

 * We do not clear them on entry and exit from the kernel. Therefore

 * it is possible to get a watchpoint trap here from inside the kernel.

 * However, the code in ./ptrace.c has ensured that the user can

 * only set watchpoints on userspace addresses. Therefore the in-kernel

 * watchpoint trap can only occur in code which is reading/writing

 * from user space. Such code must not hold kernel locks (since it

 * can equally take a page fault), therefore it is safe to call

 * force_sig_info even though that claims and releases locks.

 *

 * Code in ./signal.c ensures that the debug control register

 * is restored before we deliver any signal, and therefore that

 * user code runs with the correct debug control register even though

 * we clear it here.

 *

 * Being careful here means that we don't have to be as careful in a

 * lot of more complicated places (task switching can be a bit lazy

 * about restoring all the debug state, and ptrace doesn't have to

 * find every occurrence of the TF bit that could be saved away even

 * by user code)

 *

 * May run on IST stack.

	/*

	 * Notifiers will clear bits in @dr6 to indicate the event has been

	 * consumed - hw_breakpoint_handler(), single_stop_cont().

	 *

	 * Notifiers will set bits in @virtual_dr6 to indicate the desire

	 * for signals - ptrace_triggered(), kgdb_hw_overflow_handler().

	/*

	 * Disable breakpoints during exception handling; recursive exceptions

	 * are exceedingly 'fun'.

	 *

	 * Since this function is NOKPROBE, and that also applies to

	 * HW_BREAKPOINT_X, we can't hit a breakpoint before this (XXX except a

	 * HW_BREAKPOINT_W on our stack)

	 *

	 * Entry text is excluded for HW_BP_X and cpu_entry_area, which

	 * includes the entry stack is excluded for everything.

	/*

	 * If something gets miswired and we end up here for a user mode

	 * #DB, we will malfunction.

		/*

		 * The SDM says "The processor clears the BTF flag when it

		 * generates a debug exception." but PTRACE_BLOCKSTEP requested

		 * it for userspace, but we just took a kernel #DB, so re-set

		 * BTF.

	/*

	 * Catch SYSENTER with TF set and clear DR_STEP. If this hit a

	 * watchpoint at the same time then that will still be handled.

	/*

	 * The kernel doesn't use INT1

	/*

	 * The kernel doesn't use TF single-step outside of:

	 *

	 *  - Kprobes, consumed through kprobe_debug_handler()

	 *  - KGDB, consumed through notify_debug()

	 *

	 * So if we get here with DR_STEP set, something is wonky.

	 *

	 * A known way to trigger this is through QEMU's GDB stub,

	 * which leaks #DB into the guest and causes IST recursion.

	/*

	 * If something gets miswired and we end up here for a kernel mode

	 * #DB, we will malfunction.

	/*

	 * NB: We can't easily clear DR7 here because

	 * irqentry_exit_to_usermode() can invoke ptrace, schedule, access

	 * user memory, etc.  This means that a recursive #DB is possible.  If

	 * this happens, that #DB will hit exc_debug_kernel() and clear DR7.

	 * Since we're not on the IST stack right now, everything will be

	 * fine.

	/*

	 * Start the virtual/ptrace DR6 value with just the DR_STEP mask

	 * of the real DR6. ptrace_triggered() will set the DR_TRAPn bits.

	 *

	 * Userspace expects DR_STEP to be visible in ptrace_get_debugreg(6)

	 * even if it is not the result of PTRACE_SINGLESTEP.

	/*

	 * The SDM says "The processor clears the BTF flag when it

	 * generates a debug exception."  Clear TIF_BLOCKSTEP to keep

	 * TIF_BLOCKSTEP in sync with the hardware BTF flag.

	/*

	 * If dr6 has no reason to give us about the origin of this trap,

	 * then it's very likely the result of an icebp/int01 trap.

	 * User wants a sigtrap for that.

 It's safe to allow irq's after DR6 has been saved */

 #DB for bus lock can only be triggered from userspace. */

 Add the virtual_dr6 bits for signals. */

 IST stack entry */

 User entry, runs on regular task stack */

 32 bit does not have separate entry points. */

/*

 * Note that we play around with the 'TS' bit in an attempt to get

 * the correct behaviour even in the presence of the asynchronous

 * IRQ13 behaviour

	/*

	 * Synchronize the FPU register state to the memory register state

	 * if necessary. This allows the exception handler to inspect it.

 Retry when we get spurious exceptions: */

 AMD 486 bug: INVD in CPL 0 raises #XF instead of #GP */

	/*

	 * This addresses a Pentium Pro Erratum:

	 *

	 * PROBLEM: If the APIC subsystem is configured in mixed mode with

	 * Virtual Wire mode implemented through the local APIC, an

	 * interrupt vector of 0Fh (Intel reserved encoding) may be

	 * generated by the local APIC (Int 15).  This vector may be

	 * generated upon receipt of a spurious interrupt (an interrupt

	 * which is removed before the system receives the INTA sequence)

	 * instead of the programmed 8259 spurious interrupt vector.

	 *

	 * IMPLICATION: The spurious interrupt vector programmed in the

	 * 8259 is normally handled by an operating system's spurious

	 * interrupt handler. However, a vector of 0Fh is unknown to some

	 * operating systems, which would crash if this erratum occurred.

	 *

	 * In theory this could be limited to 32bit, but the handler is not

	 * hurting and who knows which other CPUs suffer from this.

 Die if that happens in kernel space */

 This should not happen. */

 Try to fix it up and carry on. */

		/*

		 * Something terrible happened, and we're better off trying

		 * to kill the task than getting stuck in a never-ending

		 * loop of #NM faults.

 Init cpu_entry_area before IST entries are set up */

 Init GHCB memory pages when running as an SEV-ES guest */

 Initialize TSS before setting up traps so ISTs work */

 Setup traps as cpu_init() might #GP */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2015, Christoph Hellwig.

 * Copyright (c) 2015, Intel Corporation.

	/*

	 * See drivers/nvdimm/e820.c for the implementation, this is

	 * simply here to trigger the module to load on demand.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * EISA specific code

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (C) 1995  Linus Torvalds

 *

 * This file contains the setup_arch() code, which handles the architecture-dependent

 * parts of early kernel initialization.

/*

 * max_low_pfn_mapped: highest directly mapped pfn < 4 GB

 * max_pfn_mapped:     highest directly mapped pfn > 4 GB

 *

 * The direct mapping only covers E820_TYPE_RAM regions, so the ranges and gaps are

 * represented by pfn_mapped[].

/*

 * Range of the BSS area. The size of the BSS area is determined

 * at link time, with RESERVE_BRK() facility reserving additional

 * chunks.

/*

 * These are the four main kernel memory regions, we put them into

 * the resource tree so that kdump tools and other debugging tools

 * recover it:

 CPU data as detected by the assembly code in head_32.S */

 Common CPU data for all CPUs */

 Boot loader ID and version as integers, for the benefit of proc_dointvec */

/*

 * Setup options

/**

 * copy_edd() - Copy the BIOS EDD information

 *              from boot_params into a safe place.

 *

	/* Mark brk area as locked down and no longer taking any

 Assume only end is not page aligned */

 We need to move the initrd down into directly mapped mem */

 Assume only end is not page aligned */

 No initrd provided by bootloader */

 Assume only end is not page aligned */

 No initrd provided by bootloader */

 All are mapped, easy case */

 CONFIG_BLK_DEV_INITRD */

/*

 * --------- Crashkernel reservation ------------------------------

 16M alignment for crash kernel regions */

/*

 * Keep the crash kernel below this limit.

 *

 * Earlier 32-bits kernels would limit the kernel to the low 512 MB range

 * due to mapping restrictions.

 *

 * 64-bit kdump kernels need to be restricted to be under 64 TB, which is

 * the upper limit of system RAM in 4-level paging mode. Since the kdump

 * jump could be from 5-level paging to 4-level paging, the jump will fail if

 * the kernel is put above 64 TB, and during the 1st kernel bootup there's

 * no good way to detect the paging mode of the target kernel which will be

 * loaded for dumping.

 crashkernel=Y,low */

		/*

		 * two parts from kernel/dma/swiotlb.c:

		 * -swiotlb size: user-specified with swiotlb= or default.

		 *

		 * -swiotlb overflow buffer: now hardcoded to 32k. We round it

		 * to 8M for other buffers that may need to stay low too. Also

		 * make sure we allocate enough extra low memory so that we

		 * don't run out of DMA buffers for 32-bit devices.

 passed with crashkernel=0,low ? */

 crashkernel=XM */

 crashkernel=X,high */

 0 means: find the address automatically */

		/*

		 * Set CRASH_ADDR_LOW_MAX upper bound for crash memory,

		 * crashkernel=x,high reserves memory over 4G, also allocates

		 * 256M extra low memory for DMA buffers and swiotlb.

		 * But the extra memory is not required for all machines.

		 * So try low memory first and fall back to high memory

		 * unless "crashkernel=size[KMG],high" is specified.

 request I/O space for devices used on all i[345]86 PCs */

 Assume no if something weird is going on with PCI */

/*

 * Sandy Bridge graphics has trouble with certain ranges, exclude

 * them from allocation.

	/*

	 * SandyBridge integrated graphics devices have a bug that prevents

	 * them from accessing certain memory ranges, namely anything below

	 * 1M and in the pages listed in bad_pages[] above.

	 *

	 * To avoid these pages being ever accessed by SNB gfx devices reserve

	 * bad_pages that have not already been reserved at boot time.

	 * All memory below the 1 MB mark is anyway reserved later during

	 * setup_arch(), so there is no need to reserve it here.

	/*

	 * A special case is the first 4Kb of memory;

	 * This is a BIOS owned area, not kernel ram, but generally

	 * not listed as such in the E820 table.

	 *

	 * This typically reserves additional memory (64KiB by default)

	 * since some BIOSes are known to corrupt low memory.  See the

	 * Kconfig help text for X86_RESERVE_LOW.

	/*

	 * special case: Some BIOSes report the PC BIOS

	 * area (640Kb -> 1Mb) as RAM even though it is not.

	 * take them out.

 called before trim_bios_range() to spare extra sanitize */

	/*

	 * Complain if .text .data and .bss are not marked as E820_TYPE_RAM and

	 * attempt to fix it by adding the range. We may have a confused BIOS,

	 * or the user may have used memmap=exactmap or memmap=xxM$yyM to

	 * exclude kernel range. If we really are running on top non-RAM,

	 * we will crash later anyways.

	/*

	 * Reserve the memory occupied by the kernel between _text and

	 * __end_of_kernel_reserve symbols. Any kernel sections after the

	 * __end_of_kernel_reserve symbol must be explicitly reserved with a

	 * separate memblock_reserve() or they will be discarded.

	/*

	 * The first 4Kb of memory is a BIOS owned area, but generally it is

	 * not listed as such in the E820 table.

	 *

	 * Reserve the first 64K of memory since some BIOSes are known to

	 * corrupt low memory. After the real mode trampoline is allocated the

	 * rest of the memory below 640k is reserved.

	 *

	 * In addition, make sure page 0 is always reserved because on

	 * systems with L1TF its contents can be leaked to user processes.

/*

 * Dump out kernel offset information on panic.

 append boot loader cmdline to builtin */

/*

 * Determine if we were loaded by an EFI loader.  If so, then we have also been

 * passed the efi memmap, systab, etc., so we should use these data structures

 * for initialization.  Note, the efi init code path is determined by the

 * global efi_enabled. This allows the same kernel image to be used on existing

 * systems (with a traditional BIOS) as well as on EFI systems.

/*

 * setup_arch - architecture-specific boot-time initializations

 *

 * Note: On x86_64, fixmaps are ready for use even before this is called.

	/*

	 * copy kernel address range established so far and switch

	 * to the proper swapper page table

	/*

	 * Note: Quark X1000 CPUs advertise PGE incorrectly and require

	 * a cr3 based tlb flush, so the following __flush_tlb_all()

	 * will not flush anything because the CPU quirk which clears

	 * X86_FEATURE_PGE has not been invoked yet. Though due to the

	 * load_cr3() above the TLB has been flushed already. The

	 * quirk is invoked before subsequent calls to __flush_tlb_all()

	 * so proper operation is guaranteed.

	/*

	 * If we have OLPC OFW, we might end up relocating the fixmap due to

	 * reserve_top(), so do this before touching the ioremap area.

	/*

	 * x86_configure_nx() is called before parse_early_param() (called by

	 * prepare_command_line()) to detect whether hardware doesn't support

	 * NX (so that the early EHCI debug console setup can safely call

	 * set_fixmap()). It may then be called again from within noexec_setup()

	 * during parsing early parameters to honor the respective command line

	 * option.

	/*

	 * This parses early params and it needs to run before

	 * early_reserve_memory() because latter relies on such settings

	 * supplied as early params.

	/*

	 * Do some memory reservations *before* memory is added to memblock, so

	 * memblock allocations won't overwrite it.

	 *

	 * After this point, everything still needed from the boot loader or

	 * firmware or kernel text should be early reserved or marked not RAM in

	 * e820. All other memory is free game.

	 *

	 * This call needs to happen before e820__memory_setup() which calls the

	 * xen_memory_setup() on Xen dom0 which relies on the fact that those

	 * early reservations have happened already.

	/*

	 * Memory used by the kernel cannot be hot-removed because Linux

	 * cannot migrate the kernel pages. When memory hotplug is

	 * enabled, we should prevent memblock from allocating memory

	 * for the kernel.

	 *

	 * ACPI SRAT records all hotpluggable memory ranges. But before

	 * SRAT is parsed, we don't know about it.

	 *

	 * The kernel image is loaded into memory at very early time. We

	 * cannot prevent this anyway. So on NUMA system, we set any

	 * node the kernel resides in as un-hotpluggable.

	 *

	 * Since on modern servers, one node could have double-digit

	 * gigabytes memory, we can assume the memory around the kernel

	 * image is also un-hotpluggable. So before SRAT is parsed, just

	 * allocate memory near the kernel image to try the best to keep

	 * the kernel away from hotpluggable memory.

	/*

	 * VMware detection requires dmi to be available, so this

	 * needs to be done after dmi_setup(), for the boot CPU.

 after parse_early_param, so could debug it */

	/*

	 * partially used pages are not usable - thus

	 * we are rounding upwards:

 update e820 for memory not covered by WB MTRRs */

	/*

	 * This call is required when the CPU does not support PAT. If

	 * mtrr_bp_init() invoked it already via pat_init() the call has no

	 * effect.

	/*

	 * Define random base addresses for memory sections after max_pfn is

	 * defined and before each memory section base is used.

 max_low_pfn get updated here */

 How many end-of-memory variables you have, grandma! */

 need this before calling reserve_initrd */

	/*

	 * Find and reserve possible boot-time SMP configuration:

	/*

	 * Need to conclude brk, before e820__memblock_setup()

	 * it could use memblock_find_in_range, could overlap with

	 * brk area.

	/*

	 * Needs to run after memblock setup because it needs the physical

	 * memory size.

	/*

	 * The EFI specification says that boot service code won't be

	 * called after ExitBootServices(). This is, in fact, a lie.

 preallocate 4k for mptable mpc */

	/*

	 * Find free memory for the real mode trampoline and place it there. If

	 * there is not enough free memory under 1M, on EFI-enabled systems

	 * there will be additional attempt to reclaim the memory for the real

	 * mode trampoline at efi_free_boot_services().

	 *

	 * Unconditionally reserve the entire first 1M of RAM because BIOSes

	 * are known to corrupt low memory and several hundred kilobytes are not

	 * worth complex detection what memory gets clobbered. Windows does the

	 * same thing for very similar reasons.

	 *

	 * Moreover, on machines with SandyBridge graphics or in setups that use

	 * crashkernel the entire 1M is reserved anyway.

	/*

	 * Update mmu_cr4_features (and, indirectly, trampoline_cr4_features)

	 * with the current CR4 value.  This may not be necessary, but

	 * auditing all the early-boot CR4 manipulation would be needed to

	 * rule it out.

	 *

	 * Mask off features that don't work outside long mode (just

	 * PCIDE for now).

	/*

	 * NOTE: On x86-32, only from this point on, fixmaps are ready for use.

 Allocate bigger log buffer */

 Look for ACPI tables and reserve memory occupied by them. */

	/*

	 * Reserve memory for crash kernel after SRAT is parsed so that it

	 * won't consume hotpluggable memory.

	/*

	 * Sync back kernel address range.

	 *

	 * FIXME: Can the later sync in setup_cpu_entry_areas() replace

	 * this call?

	/*

	 * Read APIC and some other early information from ACPI tables.

	/*

	 * get boot-time SMP configuration:

	/*

	 * Systems w/o ACPI and mptables might not have it mapped the local

	 * APIC yet, but prefill_possible_map() might need to access it.

	/*

	 * This needs to run before setup_local_APIC() which soft-disables the

	 * local APIC temporarily and that masks the thermal LVT interrupt,

	 * leading to softlockups on machines which have configured SMI

	 * interrupt delivery.

 CONFIG_X86_32 */

/*

 * umip.c Emulation for instruction protected by the User-Mode Instruction

 * Prevention feature

 *

 * Copyright (c) 2017, Intel Corporation.

 * Ricardo Neri <ricardo.neri-calderon@linux.intel.com>

/** DOC: Emulation for User-Mode Instruction Prevention (UMIP)

 *

 * User-Mode Instruction Prevention is a security feature present in recent

 * x86 processors that, when enabled, prevents a group of instructions (SGDT,

 * SIDT, SLDT, SMSW and STR) from being run in user mode by issuing a general

 * protection fault if the instruction is executed with CPL > 0.

 *

 * Rather than relaying to the user space the general protection fault caused by

 * the UMIP-protected instructions (in the form of a SIGSEGV signal), it can be

 * trapped and emulate the result of such instructions to provide dummy values.

 * This allows to both conserve the current kernel behavior and not reveal the

 * system resources that UMIP intends to protect (i.e., the locations of the

 * global descriptor and interrupt descriptor tables, the segment selectors of

 * the local descriptor table, the value of the task state register and the

 * contents of the CR0 register).

 *

 * This emulation is needed because certain applications (e.g., WineHQ and

 * DOSEMU2) rely on this subset of instructions to function.

 *

 * The instructions protected by UMIP can be split in two groups. Those which

 * return a kernel memory address (SGDT and SIDT) and those which return a

 * value (SLDT, STR and SMSW).

 *

 * For the instructions that return a kernel memory address, applications

 * such as WineHQ rely on the result being located in the kernel memory space,

 * not the actual location of the table. The result is emulated as a hard-coded

 * value that, lies close to the top of the kernel memory. The limit for the GDT

 * and the IDT are set to zero.

 *

 * The instruction SMSW is emulated to return the value that the register CR0

 * has at boot time as set in the head_32.

 * SLDT and STR are emulated to return the values that the kernel programmatically

 * assigns:

 * - SLDT returns (GDT_ENTRY_LDT * 8) if an LDT has been set, 0 if not.

 * - STR returns (GDT_ENTRY_TSS * 8).

 *

 * Emulation is provided for both 32-bit and 64-bit processes.

 *

 * Care is taken to appropriately emulate the results when segmentation is

 * used. That is, rather than relying on USER_DS and USER_CS, the function

 * insn_get_addr_ref() inspects the segment descriptor pointed by the

 * registers in pt_regs. This ensures that we correctly obtain the segment

 * base address and the address and operand sizes even if the user space

 * application uses a local descriptor table.

/*

 * The SGDT and SIDT instructions store the contents of the global descriptor

 * table and interrupt table registers, respectively. The destination is a

 * memory operand of X+2 bytes. X bytes are used to store the base address of

 * the table and 2 bytes are used to store the limit. In 32-bit processes X

 * has a value of 4, in 64-bit processes X has a value of 8.

 0F 01 /0 */

 0F 01 /1 */

 0F 01 /4 */

 0F 00 /0 */

 0F 00 /1 */

/**

 * umip_printk() - Print a rate-limited message

 * @regs:	Register set with the context in which the warning is printed

 * @log_level:	Kernel log level to print the message

 * @fmt:	The text string to print

 *

 * Print the text contained in @fmt. The print rate is limited to bursts of 5

 * messages every two minutes. The purpose of this customized version of

 * printk() is to print messages when user space processes use any of the

 * UMIP-protected instructions. Thus, the printed text is prepended with the

 * task name and process ID number of the current task as well as the

 * instruction and stack pointers in @regs as seen when entering kernel mode.

 *

 * Returns:

 *

 * None.

 Bursts of 5 messages every two minutes */

/**

 * identify_insn() - Identify a UMIP-protected instruction

 * @insn:	Instruction structure with opcode and ModRM byte.

 *

 * From the opcode and ModRM.reg in @insn identify, if any, a UMIP-protected

 * instruction that can be emulated.

 *

 * Returns:

 *

 * On success, a constant identifying a specific UMIP-protected instruction that

 * can be emulated.

 *

 * -EINVAL on error or when not an UMIP-protected instruction that can be

 * emulated.

 By getting modrm we also get the opcode. */

 All the instructions of interest start with 0x0f. */

/**

 * emulate_umip_insn() - Emulate UMIP instructions and return dummy values

 * @insn:	Instruction structure with operands

 * @umip_inst:	A constant indicating the instruction to emulate

 * @data:	Buffer into which the dummy result is stored

 * @data_size:	Size of the emulated result

 * @x86_64:	true if process is 64-bit, false otherwise

 *

 * Emulate an instruction protected by UMIP and provide a dummy result. The

 * result of the emulation is saved in @data. The size of the results depends

 * on both the instruction and type of operand (register vs memory address).

 * The size of the result is updated in @data_size. Caller is responsible

 * of providing a @data buffer of at least UMIP_GDT_IDT_BASE_SIZE +

 * UMIP_GDT_IDT_LIMIT_SIZE bytes.

 *

 * Returns:

 *

 * 0 on success, -EINVAL on error while emulating.

	/*

	 * These two instructions return the base address and limit of the

	 * global and interrupt descriptor table, respectively. According to the

	 * Intel Software Development manual, the base address can be 24-bit,

	 * 32-bit or 64-bit. Limit is always 16-bit. If the operand size is

	 * 16-bit, the returned value of the base address is supposed to be a

	 * zero-extended 24-byte number. However, it seems that a 32-byte number

	 * is always returned irrespective of the operand size.

 SGDT and SIDT do not use registers operands. */

		/*

		 * 64-bit processes use the entire dummy base address.

		 * 32-bit processes use the lower 32 bits of the base address.

		 * dummy_base_addr is always 64 bits, but we memcpy the correct

		 * number of bytes from it to the destination.

		/*

		 * For these 3 instructions, the number

		 * of bytes to be copied in the result buffer is determined

		 * by whether the operand is a register or a memory location.

		 * If operand is a register, return as many bytes as the operand

		 * size. If operand is memory, return only the two least

		 * significant bytes.

/**

 * force_sig_info_umip_fault() - Force a SIGSEGV with SEGV_MAPERR

 * @addr:	Address that caused the signal

 * @regs:	Register set containing the instruction pointer

 *

 * Force a SIGSEGV signal with SEGV_MAPERR as the error code. This function is

 * intended to be used to provide a segmentation fault when the result of the

 * UMIP emulation could not be copied to the user space memory.

 *

 * Returns: none

/**

 * fixup_umip_exception() - Fixup a general protection fault caused by UMIP

 * @regs:	Registers as saved when entering the #GP handler

 *

 * The instructions SGDT, SIDT, STR, SMSW and SLDT cause a general protection

 * fault if executed with CPL > 0 (i.e., from user space). This function fixes

 * the exception up and provides dummy results for SGDT, SIDT and SMSW; STR

 * and SLDT are not fixed up.

 *

 * If operands are memory addresses, results are copied to user-space memory as

 * indicated by the instruction pointed by eIP using the registers indicated in

 * the instruction operands. If operands are registers, results are copied into

 * the context that was saved when entering kernel mode.

 *

 * Returns:

 *

 * True if emulation was successful; false if not.

 10 bytes is the maximum size of the result of UMIP instructions */

	/*

	 * Give up on emulation if fetching the instruction failed. Should a

	 * page fault or a #GP be issued?

	/*

	 * If operand is a register, write result to the copy of the register

	 * value that was pushed to the stack when entering into kernel mode.

	 * Upon exit, the value we write will be restored to the actual hardware

	 * register.

		/*

		 * Negative values are usually errors. In memory addressing,

		 * the exception is -EDOM. Since we expect a register operand,

		 * all negative values are errors.

			/*

			 * If copy fails, send a signal and tell caller that

			 * fault was fixed up.

 increase IP to let the program keep going */

 SPDX-License-Identifier: GPL-2.0

/* does this oprom support the given pci device, or any of the devices

 * that the driver supports?

 video rom */

 0 < length <= 0x7f * 512, historically */

 if checksum okay, trust length byte */

 system rom */

 check for extension rom (ignore length byte!) */

 check for adapter roms on 2k boundaries */

 0 < length <= 0x7f * 512, historically */

 but accept any length that fits if checksum okay */

 SPDX-License-Identifier: GPL-2.0

/*

 * X86 trace clocks

/*

 * trace_clock_x86_tsc(): A clock that is just the cycle counter.

 *

 * Unlike the other clocks, this is not in nanoseconds.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * itmt.c: Support Intel Turbo Boost Max Technology 3.0

 *

 * (C) Copyright 2016 Intel Corporation

 * Author: Tim Chen <tim.c.chen@linux.intel.com>

 *

 * On platforms supporting Intel Turbo Boost Max Technology 3.0, (ITMT),

 * the maximum turbo frequencies of some cores in a CPU package may be

 * higher than for the other cores in the same package.  In that case,

 * better performance can be achieved by making the scheduler prefer

 * to run tasks on the CPUs with higher max turbo frequencies.

 *

 * This file provides functions and data structures for enabling the

 * scheduler to favor scheduling on cores can be boosted to a higher

 * frequency under ITMT.

 Boolean to track if system has ITMT capabilities */

/*

 * Boolean to control whether we want to move processes to cpu capable

 * of higher turbo frequency for cpus supporting Intel Turbo Boost Max

 * Technology 3.0.

 *

 * It can be set via /proc/sys/kernel/sched_itmt_enabled

/**

 * sched_set_itmt_support() - Indicate platform supports ITMT

 *

 * This function is used by the OS to indicate to scheduler that the platform

 * is capable of supporting the ITMT feature.

 *

 * The current scheme has the pstate driver detects if the system

 * is ITMT capable and call sched_set_itmt_support.

 *

 * This must be done only after sched_set_itmt_core_prio

 * has been called to set the cpus' priorities.

 * It must not be called with cpu hot plug lock

 * held as we need to acquire the lock to rebuild sched domains

 * later.

 *

 * Return: 0 on success

/**

 * sched_clear_itmt_support() - Revoke platform's support of ITMT

 *

 * This function is used by the OS to indicate that it has

 * revoked the platform's support of ITMT feature.

 *

 * It must not be called with cpu hot plug lock

 * held as we need to acquire the lock to rebuild sched domains

 * later.

 disable sched_itmt if we are no longer ITMT capable */

/**

 * sched_set_itmt_core_prio() - Set CPU priority based on ITMT

 * @prio:	Priority of cpu core

 * @core_cpu:	The cpu number associated with the core

 *

 * The pstate driver will find out the max boost frequency

 * and call this function to set a priority proportional

 * to the max boost frequency. CPU with higher boost

 * frequency will receive higher priority.

 *

 * No need to rebuild sched domain after updating

 * the CPU priorities. The sched domains have no

 * dependency on CPU priorities.

		/*

		 * Ensure that the siblings are moved to the end

		 * of the priority chain and only used when

		 * all other high priority cpus are out of capacity.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (c) 1991,1992,1995  Linus Torvalds

 *  Copyright (c) 1994  Alan Modra

 *  Copyright (c) 1995  Markus Kuhn

 *  Copyright (c) 1996  Ingo Molnar

 *  Copyright (c) 1998  Andrea Arcangeli

 *  Copyright (c) 2002,2006  Vojtech Pavlik

 *  Copyright (c) 2003  Andi Kleen

 *

		/*

		 * Return address is either directly at stack pointer

		 * or above a saved flags. Eflags has bits 22-31 zero,

		 * kernel addresses don't.

/*

 * Default timer interrupt handler for PIT/HPET

	/*

	 * Unconditionally register the legacy timer interrupt; even

	 * without legacy PIC/PIT we need this for the HPET0 in legacy

	 * replacement mode.

 Default timer init function */

	/*

	 * Before PIT/HPET init, select the interrupt mode. This is required

	 * to make the decision whether PIT should be initialized correct.

 Setup the legacy timers */

	/*

	 * After PIT/HPET timers init, set up the final interrupt mode for

	 * delivering IRQs.

/*

 * Initialize TSC and delay the periodic timer init to

 * late x86_late_time_init() so ioremap works.

/*

 * Sanity check the vdso related archdata content.

 SPDX-License-Identifier: GPL-2.0

/*

 * pci_swiotlb_detect_override - set swiotlb to 1 if necessary

 *

 * This returns non-zero if we are forced to use swiotlb (by the boot

 * option).

/*

 * If 4GB or more detected (and iommu=off not set) or if SME is active

 * then set swiotlb to 1 and return 1.

 don't initialize swiotlb if iommu=off (no_iommu=1) */

	/*

	 * Set swiotlb to 1 so that bounce buffers are allocated and used for

	 * devices that can't support DMA to encrypted memory.

 An IOMMU turned us off. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * KVM paravirt_ops implementation

 *

 * Copyright (C) 2007, Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 * Copyright IBM Corporation, 2007

 *   Authors: Anthony Liguori <aliguori@us.ibm.com>

/*

 * No need for any "IO delay" on KVM

 dummy entry exist -> wake up was delivered ahead of PF */

/*

 * kvm_async_pf_task_wait_schedule - Wait for pagefault to be handled

 * @token:	Token to identify the sleep node entry

 *

 * Invoked from the async pagefault handling code or from the VM exit page

 * fault handler. In both cases RCU is watching.

		/*

		 * async PF was not yet handled.

		 * Add dummy entry for the token.

			/*

			 * Allocation failed! Busy wait while other cpu

			 * handles async PF.

	/*

	 * If the host managed to inject an async #PF into an interrupt

	 * disabled region, then die hard as this is not going to end well

	 * and the host side is seriously broken.

 Page is swapped out by the host. */

	/**

	 * This relies on __test_and_clear_bit to modify the memory

	 * in a way that is atomic with respect to the local CPU.

	 * The hypervisor only accesses this memory from the local CPU so

	 * there's no need for lock or memory barriers.

	 * An optimization barrier is implied in apic write.

 Size alignment is implied but just to make it explicit. */

/*

 * Iterate through all possible CPUs and map the memory region pointed

 * by apf_reason, steal_time and kvm_apic_eoi as decrypted at once.

 *

 * Note: we iterate through all possible CPUs to ensure that CPUs

 * hotplugged will have their per-cpu variable already mapped as

 * decrypted.

 Get variable contents into buffer */

/*

 * Set the IPI entry points

 Make sure other vCPUs get a chance to run if they need to. */

	/*

	 * We have to call flush only on online vCPUs. And

	 * queue flush_on_enter for pre-empted vCPUs

		/*

		 * The local vCPU is never preempted, so we do not explicitly

		 * skip check for local vCPU - it will never be cleared from

		 * flushmask.

	/*

	 * Map the per-cpu variables as decrypted before kvm_guest_cpu_init()

	 * shares the guest physical address with the hypervisor.

/*

 * After a PV feature is registered, the host will keep writing to the

 * registered memory location. If the guest happens to shutdown, this memory

 * won't be valid. In cases like kexec, in which you install a new kernel, this

 * means a random memory location will be kept being written.

	/*

	 * Hard lockup detection is enabled by default. Disable it, as guests

	 * can get false positives too easily, for example if the host is

	 * overcommitted.

 So we don't blow up on old processors */

		/*

		 * Reset the host's shared pages list related to kernel

		 * specific page encryption status settings before we load a

		 * new kernel by kexec. Reset the page encryption status

		 * during early boot intead of just before kexec to avoid SMP

		 * races during kvm_pv_guest_cpu_reboot().

		 * NOTE: We cannot reset the complete shared pages list

		 * here as we need to retain the UEFI/OVMF firmware

		 * specific settings.

		/*

		 * Ensure that _bss_decrypted section is marked as decrypted in the

		 * shared pages list.

		/*

		 * If not booted using EFI, enable Live migration support.

 RAX and CPL are already in the GHCB */

 No checking of the return state needed */

 Kick a cpu by its apicid. Used to wake up a halted vcpu */

	/*

	 * halt until it's our turn and kicked. Note that we do safe halt

	 * for irq enabled case to avoid hang when lock info is overwritten

	 * in irq spinlock slowpath and no spurious interrupt occur to save us.

 safe_halt() will enable IRQ */

/*

 * Hand-optimize version for x86-64 to avoid 8 64-bit register saving and

 * restoring to/from the stack.

/*

 * Setup pv_lock_ops to exploit KVM_FEATURE_PV_UNHALT if present.

	/*

	 * In case host doesn't support KVM_FEATURE_PV_UNHALT there is still an

	 * advantage of keeping virt_spin_lock_key enabled: virt_spin_lock() is

	 * preferred over native qspinlock when vCPU is preempted.

	/*

	 * Disable PV spinlocks and use native qspinlock when dedicated pCPUs

	 * are available.

	/*

	 * When PV spinlock is enabled which is preferred over

	 * virt_spin_lock(), virt_spin_lock_key's value is meaningless.

	 * Just disable it anyway.

 CONFIG_PARAVIRT_SPINLOCKS */

 Enable guest halt poll disables host halt poll */

 Disable guest halt poll enables host halt poll */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * 8237A DMA controller suspend functions.

 *

 * Written by Pierre Ossman, 2005.

/*

 * This module just handles suspend/resume issues with the

 * 8237A DMA controller (used for ISA and LPC).

 * Allocation is handled in kernel/dma.c and normal usage is

 * in asm/dma.h.

 DMA count is a bit weird so this is not 0 */

 Enable cascade DMA or channel 0-3 won't work */

	/*

	 * From SKL PCH onwards, the legacy DMA device is removed in which the

	 * I/O ports (81h-83h, 87h, 89h-8Bh, 8Fh) related to it are removed

	 * as well. All removed ports must return 0xff for a inb() request.

	 *

	 * Note: DMA_PAGE_2 (port 0x81) should not be checked for detecting

	 * the presence of DMA device since it may be used by BIOS to decode

	 * LPC traffic for POST codes. Original LPC only decodes one byte of

	 * port 0x80 but some BIOS may choose to enhance PCH LPC port 0x8x

	 * decoding.

	/*

	 * It is not required to load this driver as newer SoC may not

	 * support 8237 DMA or bus mastering from LPC. Platform firmware

	 * must announce the support for such legacy devices via

	 * ACPI_FADT_LEGACY_DEVICES field in FADT table.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) 2007 Alan Stern

 * Copyright (C) 2009 IBM Corporation

 * Copyright (C) 2009 Frederic Weisbecker <fweisbec@gmail.com>

 *

 * Authors: Alan Stern <stern@rowland.harvard.edu>

 *          K.Prasad <prasad@linux.vnet.ibm.com>

 *          Frederic Weisbecker <fweisbec@gmail.com>

/*

 * HW_breakpoint: a unified kernel/user-space hardware breakpoint facility,

 * using the CPU's debug registers.

 Per cpu debug control register value */

 Per cpu debug address registers values */

/*

 * Stores the breakpoints currently in use on each breakpoint address

 * register for each cpus

/*

 * Encode the length, type, Exact, and Enable bits for a particular breakpoint

 * as stored in debug register 7.

/*

 * Decode the length and type bits for a particular breakpoint as

 * stored in debug register 7.  Return the "enabled" status.

/*

 * Install a perf counter breakpoint.

 *

 * We seek a free debug address register and use it for this

 * breakpoint. Eventually we enable it in the debug control register.

 *

 * Atomic: we hold the counter->ctx->lock and we only handle variables

 * and registers local to this cpu.

	/*

	 * Ensure we first write cpu_dr7 before we set the DR7 register.

	 * This ensures an NMI never see cpu_dr7 0 when DR7 is not.

/*

 * Uninstall the breakpoint contained in the given counter.

 *

 * First we search the debug address register it uses and then we disable

 * it.

 *

 * Atomic: we hold the counter->ctx->lock and we only handle variables

 * and registers local to this cpu.

	/*

	 * Ensure the write to cpu_dr7 is after we've set the DR7 register.

	 * This ensures an NMI never see cpu_dr7 0 when DR7 is not.

 Type */

 Len */

/*

 * Check for virtual address in kernel space.

	/*

	 * We don't need to worry about va + len - 1 overflowing:

	 * we already require that va is aligned to a multiple of len.

/*

 * Checks whether the range [addr, end], overlaps the area [base, base + size).

/*

 * Checks whether the range from addr to end, inclusive, overlaps the fixed

 * mapped CPU entry area range or other ranges used for CPU entry.

 CPU entry erea is always used for CPU entry */

	/*

	 * When FSGSBASE is enabled, paranoid_entry() fetches the per-CPU

	 * GSBASE value via __per_cpu_offset or pcpu_unit_offsets.

 The original rw GDT is being used after load_direct_gdt() */

		/*

		 * cpu_tss_rw is not directly referenced by hardware, but

		 * cpu_tss_rw is also used in CPU entry code,

		/*

		 * cpu_tlbstate.user_pcid_flush_mask is used for CPU entry.

		 * If a data breakpoint on it, it will cause an unwanted #DB.

		 * Protect the full cpu_tlbstate structure to be sure.

		/*

		 * When in guest (X86_FEATURE_HYPERVISOR), local_db_save()

		 * will read per-cpu cpu_dr7 before clear dr7 register.

	/*

	 * Prevent any breakpoint of any type that overlaps the CPU

	 * entry area and data.  This protects the IST stacks and also

	 * reduces the chance that we ever find out what happens if

	 * there's a data breakpoint on the GDT, IDT, or TSS.

 Type */

		/*

		 * We don't allow kernel breakpoints in places that are not

		 * acceptable for kprobes.  On non-kprobes kernels, we don't

		 * allow kernel breakpoints at all.

		/*

		 * x86 inst breakpoints need to have a specific undefined len.

		 * But we still need to check userspace is not trying to setup

		 * an unsupported length, to get a range breakpoint for example.

 Len */

 AMD range breakpoint */

		/*

		 * It's impossible to use a range breakpoint to fake out

		 * user vs kernel detection because bp_len - 1 can't

		 * have the high bit set.  If we ever allow range instruction

		 * breakpoints, then we'll have to check for kprobe-blacklisted

		 * addresses anywhere in the range.

/*

 * Validate the arch-specific HW Breakpoint register settings

	/*

	 * Check that the low-order bits of the address are appropriate

	 * for the alignment implied by len.

/*

 * Release the user breakpoints used by ptrace

/*

 * Handle debug exception notifications.

 *

 * Return value is either NOTIFY_STOP or NOTIFY_DONE as explained below.

 *

 * NOTIFY_DONE returned if one of the following conditions is true.

 * i) When the causative address is from user-space and the exception

 * is a valid one, i.e. not triggered as a result of lazy debug register

 * switching

 * ii) When there are more bits than trap<n> set in DR6 register (such

 * as BD, BS or BT) indicating that more than one debug condition is

 * met and requires some more action in do_debug().

 *

 * NOTIFY_STOP returned for all other cases

 *

 The DR6 value is pointed by args->err */

 Do an early return if no trap bits are set in DR6 */

 Handle all the breakpoints that were triggered */

		/*

		 * TF and data breakpoints are traps and can be merged, however

		 * instruction breakpoints are faults and will be raised

		 * separately.

		 *

		 * However DR6 can indicate both TF and instruction

		 * breakpoints. In that case take TF as that has precedence and

		 * delay the instruction breakpoint for the next exception.

		/*

		 * Reset the 'i'th TRAP bit in dr6 to denote completion of

		 * exception handling

		/*

		 * Set up resume flag to avoid breakpoint recursion when

		 * returning back to origin.

	/*

	 * Further processing in do_debug() is needed for a) user-space

	 * breakpoints (to generate signals) and b) when the system has

	 * taken exception due to multiple causes

/*

 * Handle debug exception notifications.

 TODO */

 SPDX-License-Identifier: GPL-2.0

 no conflict */

 Keep the area above or below the conflict, whichever is larger */

	/*

	 * Trim out BIOS area (high 2MB) and E820 regions. We do not remove

	 * the low 1MB unconditionally, as this area is needed for some ISA

	 * cards requiring a memory range, e.g. the i82365 PCMCIA controller.

 SPDX-License-Identifier: GPL-2.0

/*

 * 8253/PIT functions

 *

/*

 * HPET replaces the PIT, when enabled. So we need to know, which of

 * the two timers is used

/*

 * Modern chipsets can disable the PIT clock which makes it unusable. It

 * would be possible to enable the clock but the registers are chipset

 * specific and not discoverable. Avoid the whack a mole game.

 *

 * These platforms have discoverable TSC/CPU frequencies but this also

 * requires to know the local APIC timer frequency as it normally is

 * calibrated against the PIT interrupt.

 This also returns true when APIC is disabled */

	 /*

	  * Several reasons not to register PIT as a clocksource:

	  *

	  * - On SMP PIT does not scale due to i8253_lock

	  * - when HPET is enabled

	  * - when local APIC timer is active (PIT is switched off)

 !CONFIG_X86_64 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * AMD Memory Encryption Support

 *

 * Copyright (C) 2019 SUSE

 *

 * Author: Joerg Roedel <jroedel@suse.de>

 For show_regs() */

 For early boot hypervisor communication in SEV-ES enabled guests */

/*

 * Needs to be in the .data section because we need it NULL before bss is

 * cleared

 #VC handler runtime per-CPU data */

	/*

	 * Reserve one page per CPU as backup storage for the unencrypted GHCB.

	 * It is needed when an NMI happens while the #VC handler uses the real

	 * GHCB, and the NMI handler itself is causing another #VC exception. In

	 * that case the GHCB content of the first handler needs to be backed up

	 * and restored.

	/*

	 * Mark the per-cpu GHCBs as in-use to detect nested #VC exceptions.

	 * There is no need for it to be atomic, because nothing is written to

	 * the GHCB between the read and the write of ghcb_active. So it is safe

	 * to use it when a nested #VC exception happens before the write.

	 *

	 * This is necessary for example in the #VC->NMI->#VC case when the NMI

	 * happens while the first #VC handler uses the GHCB. When the NMI code

	 * raises a second #VC handler it might overwrite the contents of the

	 * GHCB written by the first handler. To avoid this the content of the

	 * GHCB is saved and restored when the GHCB is detected to be in use

	 * already.

	/*

	 * Cached DR7 value - write it on DR7 writes and return it on reads.

	 * That value will never make it to the real hardware DR7 as debugging

	 * is currently unsupported in SEV-ES guests.

 Needed in vc_early_forward_exception */

 User-mode RSP is not trusted */

 SYSCALL gap still has user-mode RSP */

/*

 * This function handles the case when an NMI is raised in the #VC

 * exception handler entry code, before the #VC handler has switched off

 * its IST stack. In this case, the IST entry for #VC must be adjusted,

 * so that any nested #VC exception will not overwrite the stack

 * contents of the interrupted #VC handler.

 *

 * The IST entry is adjusted unconditionally so that it can be also be

 * unconditionally adjusted back in __sev_es_ist_exit(). Otherwise a

 * nested sev_es_ist_exit() call may adjust back the IST entry too

 * early.

 *

 * The __sev_es_ist_enter() and __sev_es_ist_exit() functions always run

 * on the NMI IST stack, as they are only called from NMI handling code

 * right now.

 Read old IST entry */

	/*

	 * If NMI happened while on the #VC IST stack, set the new IST

	 * value below regs->sp, so that the interrupted stack frame is

	 * not overwritten by subsequent #VC exceptions.

	/*

	 * Reserve additional 8 bytes and store old IST value so this

	 * adjustment can be unrolled in __sev_es_ist_exit().

 Set new IST entry */

 Read IST entry */

 Read back old IST entry and write it to the TSS */

/*

 * Nothing shall interrupt this code path while holding the per-CPU

 * GHCB. The backup GHCB is only for NMIs interrupting this path.

 *

 * Callers must disable local interrupts around it.

 GHCB is already in use - save its contents */

			/*

			 * Backup-GHCB is also already in use. There is no way

			 * to continue here so just kill the machine. To make

			 * panic() work, mark GHCBs inactive so that messages

			 * can be printed out.

 Mark backup_ghcb active before writing to it */

 Backup GHCB content */

 Needed in vc_early_forward_exception */

 Nothing could be copied */

 Effective RIP could not be calculated */

	/*

	 * This function uses __put_user() independent of whether kernel or user

	 * memory is accessed. This works fine because __put_user() does no

	 * sanity checks of the pointer being accessed. All that it does is

	 * to report when the access failed.

	 *

	 * Also, this function runs in atomic context, so __put_user() is not

	 * allowed to sleep. The page-fault handler detects that it is running

	 * in atomic context and will not try to take mmap_sem and handle the

	 * fault, so additional pagefault_enable()/disable() calls are not

	 * needed.

	 *

	 * The access can't be done via copy_to_user() here because

	 * vc_write_mem() must not use string instructions to access unsafe

	 * memory. The reason is that MOVS is emulated by the #VC handler by

	 * splitting the move up into a read and a write and taking a nested #VC

	 * exception on whatever of them is the MMIO access. Using string

	 * instructions here would cause infinite nesting.

	/*

	 * This function uses __get_user() independent of whether kernel or user

	 * memory is accessed. This works fine because __get_user() does no

	 * sanity checks of the pointer being accessed. All that it does is

	 * to report when the access failed.

	 *

	 * Also, this function runs in atomic context, so __get_user() is not

	 * allowed to sleep. The page-fault handler detects that it is running

	 * in atomic context and will not try to take mmap_sem and handle the

	 * fault, so additional pagefault_enable()/disable() calls are not

	 * needed.

	 *

	 * The access can't be done via copy_from_user() here because

	 * vc_read_mem() must not use string instructions to access unsafe

	 * memory. The reason is that MOVS is emulated by the #VC handler by

	 * splitting the move up into a read and a write and taking a nested #VC

	 * exception on whatever of them is the MMIO access. Using string

	 * instructions here would cause infinite nesting.

 Emulated MMIO to/from encrypted memory not supported */

 Include code shared with pre-decompression boot stage */

 Restore GHCB from Backup */

		/*

		 * Invalidate the GHCB so a VMGEXIT instruction issued

		 * from userspace won't appear to be valid.

 On UP guests there is no jump table so this is not a failure */

 Check if AP Jump Table is page-aligned */

/*

 * This is needed by the OVMF UEFI firmware which will use whatever it finds in

 * the GHCB MSR as its GHCB to talk to the hypervisor. So make sure the per-cpu

 * runtime GHCBs used by the kernel are also mapped in the EFI page-table.

 Is it a WRMSR? */

/*

 * This function runs on the first #VC exception after the kernel

 * switched to virtual addresses.

 First make sure the hypervisor talks a supported protocol. */

	/*

	 * Clear the boot_ghcb. The first exception comes in before the bss

	 * section is cleared.

 Alright - Make the boot-ghcb public */

 Wakeup signal? */

/*

 * Play_dead handler when running under SEV-ES. This is needed because

 * the hypervisor can't deliver an SIPI request to restart the AP.

 * Instead the kernel has to issue a VMGEXIT to halt the VCPU until the

 * hypervisor wakes it up again.

 IRQs now disabled */

	/*

	 * If we get here, the VCPU was woken up again. Jump to CPU

	 * startup code to get it back online.

 CONFIG_HOTPLUG_CPU */

 CONFIG_HOTPLUG_CPU */

 Enable SEV-ES special handling */

 Initialize per-cpu GHCB pages */

 Secondary CPUs use the runtime #VC handler */

 Can never be greater than 8 */

 MMIO Read w/ zero-extension */

 Zero extend based on operand size */

 MMIO Read w/ sign-extension */

 Sign extend based on operand size */

/*

 * The MOVS instruction has two memory operands, which raises the

 * problem that it is not known whether the access to the source or the

 * destination caused the #VC exception (and hence whether an MMIO read

 * or write operation needs to be emulated).

 *

 * Instead of playing games with walking page-tables and trying to guess

 * whether the source or destination is an MMIO range, split the move

 * into two operations, a read and a write with only one memory operand.

 * This will cause a nested #VC exception on the MMIO address which can

 * then be handled.

 *

 * This implementation has the benefit that it also supports MOVS where

 * source _and_ destination are MMIO regions.

 *

 * It will slow MOVS on MMIO down a lot, but in SEV-ES guests it is a

 * rare operation. If it turns out to be a performance problem the split

 * operations can be moved to memcpy_fromio() and memcpy_toio().

 MMIO Write */

 MMIO Read */

 Zero-extend for 32-bit operation */

 MOVS instruction */

 Two-Byte Opcodes */

 Upper 32 bits must be written as zeroes */

 Clear out other reserved bits and set bit 10 */

 Early non-zero writes to DR7 are not supported */

 Using a value of 0 for ExitInfo1 means RAX holds the value */

	/*

	 * Treat it as a NOP and do not leak a physical address to the

	 * hypervisor.

 Treat the same as MONITOR/MONITORX */

	/*

	 * Call sev_es_hcall_finish() after regs->ax is already set.

	 * This allows the hypervisor handler to overwrite it again if

	 * necessary.

	/*

	 * Calling ecx_alignment_check() directly does not work, because it

	 * enables IRQs and the GHCB is active. Forward the exception and call

	 * it later from vc_forward_exception().

		/*

		 * Unexpected #VC exception

	/*

	 * If the code was already executing on the VC2 stack when the #VC

	 * happened, let it proceed to the normal handling routine. This way the

	 * code executing on the VC2 stack can cause #VC exceptions to get handled.

 Done - now check the result */

 Nothing to do */

		/*

		 * Emulating the instruction which caused the #VC exception

		 * failed - can't continue so print debug information

/*

 * Runtime #VC exception handler when raised from kernel mode. Runs in NMI mode

 * and will panic when an error happens.

	/*

	 * With the current implementation it is always possible to switch to a

	 * safe stack because #VC exceptions only happen at known places, like

	 * intercepted instructions or accesses to MMIO areas/IO ports. They can

	 * also happen with code instrumentation when the hypervisor intercepts

	 * #DB, but the critical paths are forbidden to be instrumented, so #DB

	 * exceptions currently also only happen in safe places.

	 *

	 * But keep this here in case the noinstr annotations are violated due

	 * to bug elsewhere.

	/*

	 * Handle #DB before calling into !noinstr code to avoid recursive #DB.

 Show some debug info */

 Ask hypervisor to sev_es_terminate */

 If that fails and we get here - just panic */

/*

 * Runtime #VC exception handler when raised from user mode. Runs in IRQ mode

 * and will kill the current task with SIGBUS when an error happens.

	/*

	 * Handle #DB before calling into !noinstr code to avoid recursive #DB.

		/*

		 * Do not kill the machine if user-space triggered the

		 * exception. Send SIGBUS instead and let user-space deal with

		 * it.

 Do initial setup or terminate the guest */

 Done - now check the result */

 Nothing to do */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/i386/kernel/head32.c -- prepare to run common code

 *

 *  Copyright (C) 2000 Andrea Arcangeli <andrea@suse.de> SuSE

 *  Copyright (C) 2007 Eric Biederman <ebiederm@xmission.com>

 Initialize 32bit specific setup functions */

 Make sure IDT is set up before any exception happens */

 Call the subarch specific early setup function */

/*

 * Initialize page tables.  This creates a PDE and a set of page

 * tables, which are located immediately beyond __brk_base.  The variable

 * _brk_end is set up to point to the first "safe" location.

 * Mappings are created both at virtual address 0 (identity mapping)

 * and PAGE_OFFSET for up to _end.

 *

 * In PAE mode initial_page_table is statically defined to contain

 * enough entries to cover the VMSPLIT option (that is the top 1, 2 or 3

 * entries). The identity mapping is handled by pointing two PGD entries

 * to the first kernel PMD. Note the upper half of each PMD or PTE are

 * always zero at this stage.

 Enough space to fit pagetables for the low memory linear map */

 Kernel PDE entry */

 Can't use pte_pfn() since it's a call with CONFIG_PARAVIRT */

 SPDX-License-Identifier: GPL-2.0

/*

 * On x86_64 symbols referenced from code should be reachable using

 * 32bit relocations.  Reserve space for static percpu variables in

 * modules so that they are always served from the first chunk which

 * is located at the percpu segment base.  On x86_32, anything can

 * address anywhere.  No need to reserve space in the first chunk.

/**

 * pcpu_need_numa - determine percpu allocation needs to consider NUMA

 *

 * If NUMA is not configured or there is only one NUMA node available,

 * there is no reason to consider NUMA.  This function determines

 * whether percpu allocation should consider NUMA or not.

 *

 * RETURNS:

 * true if NUMA should be considered; otherwise, false.

/**

 * pcpu_alloc_bootmem - NUMA friendly alloc_bootmem wrapper for percpu

 * @cpu: cpu to allocate for

 * @size: size allocation in bytes

 * @align: alignment

 *

 * Allocate @size bytes aligned at @align for cpu @cpu.  This wrapper

 * does the right thing for NUMA regardless of the current

 * configuration.

 *

 * RETURNS:

 * Pointer to the allocated area on success, NULL on failure.

/*

 * Helpers for first chunk memory allocation

	/*

	 * Allocate percpu area.  Embedding allocator is our favorite;

	 * however, on NUMA configurations, it can result in very

	 * sparse unit mapping and vmalloc area isn't spacious enough

	 * on 32bit.  Use page in that case.

		/*

		 * On 64bit, use PMD_SIZE for atom_size so that embedded

		 * percpu areas are aligned to PMD.  This, in the future,

		 * can also allow using PMD mappings in vmalloc area.  Use

		 * PAGE_SIZE on 32bit as vmalloc space is highly contended

		 * and large vmalloc area allocs can easily fail.

 alrighty, percpu areas up and running */

		/*

		 * Copy data used in early init routines from the

		 * initial arrays to the per cpu data areas.  These

		 * arrays then become expendable and the *_early_ptr's

		 * are zeroed indicating that the static arrays are

		 * gone.

		/*

		 * Ensure that the boot cpu numa_node is correct when the boot

		 * cpu is on a node that doesn't have memory installed.

		 * Also cpu_up() will call cpu_to_node() for APs when

		 * MEMORY_HOTPLUG is defined, before per_cpu(numa_node) is set

		 * up later with c_init aka intel_init/amd_init.

		 * So set them all (boot cpu and all APs).

		/*

		 * Up to this point, the boot CPU has been using .init.data

		 * area.  Reload any changed state for the boot CPU.

 indicate the early static arrays will soon be gone */

 Setup node to cpumask map */

 Setup cpu initialized, callin, callout masks */

	/*

	 * Sync back kernel address range again.  We already did this in

	 * setup_arch(), but percpu data also needs to be available in

	 * the smpboot asm and arch_sync_kernel_mappings() doesn't sync to

	 * swapper_pg_dir on 32-bit. The per-cpu mappings need to be available

	 * there too.

	 *

	 * FIXME: Can the later sync in setup_cpu_entry_areas() replace

	 * this call?

 SPDX-License-Identifier: GPL-2.0

 site call */

 site cond-call */

 tramp / site tail-call */

 tramp / site cond-tail-call */

/*

 * data16 data16 xorq %rax, %rax - a single 5 byte instruction that clears %rax

 * The REX.W cancels the effect of any data16.

	/*

	 * If we ever trigger this, our text is corrupt, we'll probably not live long.

	/*

	 * Encode the following table without branches:

	 *

	 *	tail	null	insn

	 *	-----+-------+------

	 *	  0  |   0   |  CALL

	 *	  0  |   1   |  NOP

	 *	  1  |   0   |  JMP

	 *	  1  |   1   |  RET

 SPDX-License-Identifier: GPL-2.0

 Simple VGA output */

 scroll 1 line up */

 Serial functions loosely based on a similar package from Klaus P. Gerlicher */

 ttyS0 */

  Transmit register (WRITE) */

  Receive register  (READ)  */

  Interrupt Enable          */

  Interrupt ID              */

  FIFO control              */

  Line control              */

  Modem control             */

  Line Status               */

  Modem Status              */

  Divisor Latch Low         */

  Divisor latch High        */

 8n1 */

 no interrupt */

 no fifo */

 DTR + RTS */

 Convert from baud to divisor value */

 These will always be IO based ports */

 Set up the HW */

 shift implied by pointer type */

 shift implied by pointer type */

/*

 * early_pci_serial_init()

 *

 * This function is invoked when the early_printk param starts with "pciserial"

 * The rest of the param should be "[force],B:D.F,baud", where B, D & F describe

 * the location of a PCI device that must be a UART device. "force" is optional

 * and overrides the use of an UART device with a wrong PCI class code.

 Force the use of an UART device with wrong class code */

	/*

	 * Part the param to get the BDF values

 A baud might be following */

	/*

	 * Find the device from the BDF

	/*

	 * Verify it is a UART type device

 16550 I/F at BAR0 */ {

	/*

	 * Determine if it is IO or memory mapped

 it is IO mapped */

 It is memory mapped - assume 32-bit alignment */

 WARNING! assuming the address is always in the first 4G */

	/*

	 * Initialize the hardware

			/* Sometimes, we want to leave the UART alone

			 * and assume the BIOS has set it up correctly.

			 * "nocfg" tells us this is the case, and we

			 * should do no more setup.

 Convert from baud to divisor value */

 Set up the HW */

 Keep from match the above "serial" */

 SPDX-License-Identifier: GPL-2.0

/*

 * RTC related functions

/*

 * This is a special lock that is owned by the CPU and holds the index

 * register we are working with.  It is required for NMI access to the

 * CMOS/RTC registers.  See include/asm-i386/mc146818rtc.h for details.

 CONFIG_X86_32 */

 For two digit years assume time is always after that */

/*

 * In order to set the CMOS clock precisely, set_rtc_mmss has to be

 * called 500 ms after the second nowtime has started, because when

 * nowtime is written into the registers of the CMOS clock, it will

 * jump to the next second precisely 500 ms later. Check the Motorola

 * MC146818A or Dallas DS12887 data sheet for details.

	/*

	 * If pm_trace abused the RTC as storage, set the timespec to 0,

	 * which tells the caller that this RTC value is unusable.

	/*

	 * If UIP is clear, then we have >= 244 microseconds before

	 * RTC registers will be updated.  Spec sheet says that this

	 * is the reliable way to read RTC - registers. If UIP is set

	 * then the register access might be invalid.

 Routines for accessing the CMOS RAM/RTC. */

 not static: needed by APM */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *  Copyright (C) 2000, 2001, 2002 Andi Kleen, SuSE Labs

	/*

	 * This is a software stack, so 'end' can be a valid stack pointer.

	 * It just means the stack is empty.

	/*

	 * See irq_32.c -- the next stack pointer is stored at the beginning of

	 * the stack.

	/*

	 * This is a software stack, so 'end' can be a valid stack pointer.

	 * It just means the stack is empty.

	/*

	 * The next stack pointer is stored at the beginning of the stack.

	 * See irq_32.c.

	/*

	 * Make sure we don't iterate through any given stack more than once.

	 * If it comes up a second time then there's something wrong going on:

	 * just break out and report an unknown stack type.

/*

 * Populate sysfs with topology information

 *

 * Written by: Matthew Dobson, IBM Corporation

 * Original Code: Paul Dorwin, IBM Corporation, Patrick Mochel, OSDL

 *

 * Copyright (C) 2002, IBM Corp.

 *

 * All rights reserved.

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License as published by

 * the Free Software Foundation; either version 2 of the License, or

 * (at your option) any later version.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or

 * NON INFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this program; if not, write to the Free Software

 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.

 *

 * Send feedback to <colpatch@us.ibm.com>

/*

 * This function offlines a CPU as early as possible and allows userspace to

 * boot up without the CPU. The CPU can be onlined back by user after boot.

 *

 * This is only called for debugging CPU offline/online feature.

 CONFIG_DEBUG_HOTPLUG_CPU0 */

	/*

	 * Currently CPU0 is only hotpluggable on Intel platforms. Other

	 * vendors can add hotplug support later.

	 * Xen PV guests don't support CPU0 hotplug at all.

	/*

	 * Two known BSP/CPU0 dependencies: Resume from suspend/hibernate

	 * depends on BSP. PIC interrupts depend on BSP.

	 *

	 * If the BSP dependencies are under control, one can tell kernel to

	 * enable BSP hotplug. This basically adds a control file and

	 * one can attempt to offline BSP.

		/*

		 * We won't take down the boot processor on i386 if some

		 * interrupts only are able to be serviced by the BSP in PIC.

 CONFIG_HOTPLUG_CPU */

 CONFIG_HOTPLUG_CPU */

 SPDX-License-Identifier: GPL-2.0

/*

 * Align a virtual address to avoid aliasing in the I$ on AMD F15h.

 handle 32- and 64-bit case with a single conditional */

/*

 * To avoid aliasing in the I$ on AMD F15h, the bits defined by the

 * va_align.bits, [12:upper_bit), are set to a random value instead of

 * zeroing them. This random value is computed once per boot. This form

 * of ASLR is known as "per-boot ASLR".

 *

 * To achieve this, the random value is added to the info.align_offset

 * value before calling vm_unmapped_area() or ORed directly to the

 * address.

 guard against enabling this on other CPU families */

		/* This is usually used needed to map code in small

		   model, so it needs to be in the first 31bit. Limit

		   it to that.  This means we need to move the

		   unmapped base down for this case. This can give

		   conflicts with the heap, but we assume that glibc

		   malloc knows how to fall back to mmap. Give it 1GB

 requested length too big for entire address space */

 No address checking. See comment at mmap_address_hint_valid() */

 for MAP_32BIT mappings we force the legacy mmap base */

 requesting a specific address */

	/*

	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area

	 * in the full address space.

	 *

	 * !in_32bit_syscall() check to avoid high addresses for x32

	 * (and make it no op on native i386).

	/*

	 * A failed mmap() very likely causes application failure,

	 * so fall back to the bottom-up function here. This scenario

	 * can happen with large stack limits and large mmap()

	 * allocations.

 SPDX-License-Identifier: GPL-2.0

/*

 *	Copyright (C) 1992, 1998 Linus Torvalds, Ingo Molnar

 *

 * This file contains the lowest level x86_64-specific interrupt

 * entry and irq statistics code. All the remaining irq logic is

 * done by the generic kernel/irq/ code and in the

 * x86_64-specific irq controller code. (e.g. i8259.c and

 * io_apic.c.)

/*

 * VMAP the backing store with guard pages

 Store actual TOS to avoid adjustment in the hotpath */

/*

 * If VMAP stacks are disabled due to KASAN, just use the per cpu

 * backing store without guard pages.

 Store actual TOS to avoid adjustment in the hotpath */

 SPDX-License-Identifier: GPL-2.0-only

/* ----------------------------------------------------------------------- *

 *

 *   Copyright 2014 Intel Corporation; author: H. Peter Anvin

 *

/*

 * The IRET instruction, when returning to a 16-bit segment, only

 * restores the bottom 16 bits of the user space stack pointer.  This

 * causes some 16-bit software to break, but it also leaks kernel state

 * to user space.

 *

 * This works around this by creating percpu "ministacks", each of which

 * is mapped 2^16 times 64K apart.  When we detect that the return SS is

 * on the LDT, we copy the IRET frame to the ministack and use the

 * relevant alias to return to userspace.  The ministacks are mapped

 * readonly, so if the IRET fault we promote #GP to #DF which is an IST

 * vector and thus has its own stack; we then do the fixup in the #DF

 * handler.

 *

 * This file sets up the ministacks and the related page tables.  The

 * actual ministack invocation is in entry_64.S.

/*

 * Note: we only need 6*8 = 48 bytes for the espfix stack, but round

 * it up to a cache line to avoid unnecessary sharing.

 There is address space for how many espfix pages? */

 This contains the *bottom* address of the espfix stack */

 Initialization mutex - should this be a spinlock? */

 Page allocation bitmap - each page serves ESPFIX_STACKS_PER_PAGE CPUs */

/*

 * This returns the bottom address of the espfix stack for a specific CPU.

 * The math allows for a non-power-of-two ESPFIX_STACK_SIZE, in which case

 * we have to account for some amount of padding at the end of each page.

	/*

	 * This is run before the entropy pools are initialized,

	 * but this is hopefully better than nothing.

 The constant is an arbitrary large prime */

 Install the espfix pud into the kernel page directory */

 Randomize the locations */

 The rest is the same as for any other processor */

 We only have to do this once... */

 Already initialized */

 Did another CPU already set this up? */

 Did we race on the lock? */

	/*

	 * __PAGE_KERNEL_* includes _PAGE_GLOBAL, which we want since

	 * this is mapped to userspace.

 Job is done for this CPU and any CPU which shares this page */

 SPDX-License-Identifier: GPL-2.0

/*

 * I/O delay strategies for inb_p/outb_p

 *

 * Allow for a DMI based override of port 0x80, needed for certain HP laptops

 * and possibly other systems. Also allow for the gradual elimination of

 * outb_p/inb_p API uses.

/*

 * Paravirt wants native_io_delay to be a constant.

		/*

		 * 2 usecs is an upper-bound for the outb delay but

		 * note that udelay doesn't have the bus-level

		 * side-effects that outb does, nor does udelay() have

		 * precise timings during very early bootup (the delays

		 * are shorter until calibrated):

/*

 * Quirk table for systems that misbehave (lock up, etc.) if port

 * 0x80 is used:

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0

/*

 * Some BIOSes seem to corrupt the low 64k of memory during events

 * like suspend/resume and unplugging an HDMI cable.  Reserve all

 * remaining free memory in that area and fill it with a distinct

 * pattern.

 seconds */

 Assume we've already mapped this early memory */

 First time we run the checks right away */

 SPDX-License-Identifier: GPL-2.0

/*

 * jump label x86 support

 *

 * Copyright (C) 2009 Jason Baron <jbaron@redhat.com>

 *

		/*

		 * The location is not an op that we were expecting.

		 * Something went wrong. Crash the box, as something could be

		 * corrupting the kernel.

	/*

	 * As long as only a single processor is running and the code is still

	 * not marked as RO, text_poke_early() can be used; Checking that

	 * system_state is SYSTEM_BOOTING guarantees it. It will be set to

	 * SYSTEM_SCHEDULING before other cores are awaken and before the

	 * code is write-protected.

	 *

	 * At the time the change is being done, just ignore whether we

	 * are doing nop -> jump or jump -> nop transition, and assume

	 * always nop being the 'currently valid' instruction

		/*

		 * Fallback to the non-batching mode.

 SPDX-License-Identifier: GPL-2.0

/*

 * Dynamic function tracing support.

 *

 * Copyright (C) 2007-2008 Steven Rostedt <srostedt@redhat.com>

 *

 * Thanks goes to Ingo Molnar, for suggesting the idea.

 * Mathieu Desnoyers, for suggesting postponing the modifications.

 * Arjan van de Ven, for keeping me straight, and explaining to me

 * the dangers of modifying code on the run.

	/*

	 * Need to grab text_mutex to prevent a race from module loading

	 * and live kernel patching from changing the text permissions while

	 * ftrace has it set to "read/write".

	/*

	 * ftrace_make_{call,nop}() may be called during

	 * module load, and we need to finish the text_poke_queue()

	 * that they do, here.

	/*

	 * Note:

	 * We are paranoid about modifying text, as if a bug was to happen, it

	 * could cause us to read or write to someplace that could cause harm.

	 * Carefully read and modify the code with probe_kernel_*(), and make

	 * sure what we read is what we expected it to be before modifying it.

 read the text we want to modify */

 Make sure it is what we expect it to be */

/*

 * Marked __ref because it calls text_poke_early() which is .init.text. That is

 * ok because that call will happen early, during boot, when .init sections are

 * still present.

 replace the text with the new text */

	/*

	 * On boot up, and when modules are loaded, the MCOUNT_ADDR

	 * is converted to a nop, and will never become MCOUNT_ADDR

	 * again. This code is either running before SMP (on boot up)

	 * or before the code will ever be executed (module load).

	 * We do not want to use the breakpoint version in this case,

	 * just modify the code directly.

	/*

	 * x86 overrides ftrace_replace_code -- this function will never be used

	 * in this case.

 Should only be called when module is loaded */

/*

 * Should never be called:

 *  As it is only called by __ftrace_replace_code() which is called by

 *  ftrace_replace_code() that x86 overrides, and by ftrace_update_code()

 *  which is called to turn mcount into nops or nops into function calls

 *  but not to convert a function from not using regs to one that uses

 *  regs, which ftrace_modify_call() is for.

 Currently only x86_64 supports dynamic trampolines */

 Module allocation simplifies allocating memory for code */

 Trampolines can only be created if modules are supported */

 Defined as markers to the end of the ftrace default trampolines */

 movq function_trace_op(%rip), %rdx */

 0x48 0x8b 0x15 <offset-to-ftrace_trace_op (4 bytes)> */

/*

 * The ftrace_ops is passed to the function callback. Since the

 * trampoline only services a single ftrace_ops, we can pass in

 * that ops directly.

 *

 * The ftrace_op_code_union is used to create a pointer to the

 * ftrace_ops that will be passed to the callback function.

 48 8b 15 <offset> is movq <offset>(%rip), %rdx */

	/*

	 * Allocate enough size to store the ftrace_caller code,

	 * the iret , as well as the address of the ftrace_ops this

	 * trampoline is used for.

 Copy ftrace_caller onto the trampoline memory */

 The trampoline ends with ret(q) */

 No need to test direct calls on created trampolines */

 NOP the jnz 1f; but make sure it's a 2 byte jnz */

	/*

	 * The address of the ftrace_ops that is used for this trampoline

	 * is stored at the end of the trampoline. This will be used to

	 * load the third parameter for the callback. Basically, that

	 * location at the end of the trampoline takes the place of

	 * the global function_trace_op variable.

 Are we pointing to the reference? */

 Load the contents of ptr into the callback parameter */

 put in the new offset to the ftrace_ops */

 put in the call to the function */

 ALLOC_TRAMP flags lets us know we created it */

	/*

	 * The ftrace_ops caller may set up its own trampoline.

	 * In such a case, this code must not modify it.

 Do a safe modify in case the trampoline is executing */

 Return the address of the function the trampoline calls */

 Make sure this is a call */

/*

 * If the ops->trampoline was not allocated, then it probably

 * has a static trampoline func, or is the ftrace caller itself.

		/*

		 * We only know about function graph tracer setting as static

		 * trampoline.

 If we didn't allocate this trampoline, consider it static */

 CONFIG_X86_64 */

 CONFIG_DYNAMIC_FTRACE */

 !CONFIG_HAVE_DYNAMIC_FTRACE_WITH_ARGS */

 CONFIG_HAVE_DYNAMIC_FTRACE_WITH_ARGS */

 !CONFIG_DYNAMIC_FTRACE */

/*

 * Hook the return address and push it in the stack of return addrs

 * in current thread info.

	/*

	 * When resuming from suspend-to-ram, this function can be indirectly

	 * called from early CPU startup code while the CPU is in real mode,

	 * which would fail miserably.  Make sure the stack pointer is a

	 * virtual address.

	 *

	 * This check isn't as accurate as virt_addr_valid(), but it should be

	 * good enough for this purpose, and it's fast.

 CONFIG_FUNCTION_GRAPH_TRACER */

 SPDX-License-Identifier: GPL-2.0

/*

 *	Implement 'Simple Boot Flag Specification 2.0'

 set via acpi_boot_init() */

 Reserved bits */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * handle transition of Linux booting another kernel

 * Copyright (C) 2002-2005 Eric Biederman  <ebiederm@xmission.com>

/*

 * A architecture hook called to validate the

 * proposed image and prepare the control pages

 * as needed.  The pages for KEXEC_CONTROL_PAGE_SIZE

 * have been allocated, but the segments have yet

 * been copied into the kernel.

 *

 * Do what every setup is needed on image and the

 * reboot code buffer to allow us to avoid allocations

 * later.

 *

 * - Make control page executable.

 * - Allocate page tables

 * - Setup page tables

/*

 * Undo anything leftover by machine_kexec_prepare

 * when an image is freed.

/*

 * Do not allocate memory (or fail in any way) in machine_kexec().

 * We are past the point of no return, committed to rebooting now.

 Interrupts aren't acceptable while we reboot */

		/*

		 * We need to put APICs in legacy mode so that we can

		 * get timer interrupts in second kernel. kexec/kdump

		 * paths already have calls to restore_boot_irq_mode()

		 * in one form or other. kexec jump path also need one.

	/*

	 * The segment registers are funny things, they have both a

	 * visible and an invisible part.  Whenever the visible part is

	 * set to a specific selector, the invisible part is loaded

	 * with from a table in memory.  At no other time is the

	 * descriptor table in memory accessed.

	 *

	 * I take advantage of this here by force loading the

	 * segments, before I zap the gdt with an invalid value.

	/*

	 * The gdt & idt are now invalid.

	 * If you want to load them you must set up your own idt & gdt.

 now call it */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (C) 1995  Linus Torvalds

 *

 *  Pentium III FXSR, SSE support

 *	Gareth Hughes <gareth@valinux.com>, May 2000

 *

 *  X86-64 port

 *	Andi Kleen.

 *

 *	CPU hotplug support - ashok.raj@intel.com

/*

 * This file handles the architecture-dependent parts of process handling..

 Not included via unistd.h */

 Prints also some state that isn't saved in the pt_regs */

 Only print out debug registers if they are in their non-default state. */

/*

 * Out of line to be protected from kprobes and tracing. If this would be

 * traced or probed than any access to a per CPU variable happens with

 * the wrong GS.

 *

 * It is not used on Xen paravirt. When paravirt support is needed, it

 * needs to be renamed with native_ prefix.

/*

 * Out of line to be protected from kprobes and tracing. If this would be

 * traced or probed than any access to a per CPU variable happens with

 * the wrong GS.

 *

 * It is not used on Xen paravirt. When paravirt support is needed, it

 * needs to be renamed with native_ prefix.

/*

 * Saves the FS or GS base for an outgoing thread if FSGSBASE extensions are

 * not available.  The goal is to be reasonably fast on non-FSGSBASE systems.

 * It's forcibly inlined because it'll generate better code and this function

 * is hot.

		/*

		 * On Intel (without X86_BUG_NULL_SEG), the segment base could

		 * be the pre-existing saved base or it could be zero.  On AMD

		 * (with X86_BUG_NULL_SEG), the segment base could be almost

		 * anything.

		 *

		 * This branch is very hot (it's hit twice on almost every

		 * context switch between 64-bit programs), and avoiding

		 * the RDMSR helps a lot, so we just assume that whatever

		 * value is already saved is correct.  This matches historical

		 * Linux behavior, so it won't break existing applications.

		 *

		 * To avoid leaking state, on non-X86_BUG_NULL_SEG CPUs, if we

		 * report that the base is zero, it needs to actually be zero:

		 * see the corresponding logic in load_seg_legacy.

		/*

		 * If the selector is 1, 2, or 3, then the base is zero on

		 * !X86_BUG_NULL_SEG CPUs and could be anything on

		 * X86_BUG_NULL_SEG CPUs.  In the latter case, Linux

		 * has never attempted to preserve the base across context

		 * switches.

		 *

		 * If selector > 3, then it refers to a real segment, and

		 * saving the base isn't necessary.

		/*

		 * If FSGSBASE is enabled, we can't make any useful guesses

		 * about the base, and user code expects us to save the current

		 * value.  Fortunately, reading the base directly is efficient.

/*

 * While a process is running,current->thread.fsbase and current->thread.gsbase

 * may not match the corresponding CPU registers (see save_base_legacy()).

 Interrupts need to be off for FSGSBASE */

		/*

		 * The next task is using 64-bit TLS, is not using this

		 * segment at all, or is having fun with arcane CPU features.

			/*

			 * Nasty case: on AMD CPUs, we need to forcibly zero

			 * the base.

				/*

				 * We could try to exhaustively detect cases

				 * under which we can skip the segment load,

				 * but there's really only one case that matters

				 * for performance: if both the previous and

				 * next states are fully zeroed, we can skip

				 * the load.

				 *

				 * (This assumes that prev_base == 0 has no

				 * false positives.  This is the case on

				 * Intel-style CPUs.)

		/*

		 * The next task is using a real segment.  Loading the selector

		 * is sufficient.

/*

 * Store prev's PKRU value and load next's PKRU value if they differ. PKRU

 * is not XSTATE managed on context switch because that would require a

 * lookup in the task's FPU xsave buffer and require to keep that updated

 * in various places.

 Stash the prev task's value: */

	/*

	 * PKRU writes are slightly expensive.  Avoid them when not

	 * strictly necessary:

 Update the FS and GS selectors if they could have changed. */

 Update the bases. */

		/*

		 * There are no user segments in the GDT with nonzero bases

		 * other than the TLS segments.

		/*

		 * If performance here mattered, we could protect the LDT

		 * with RCU.  This is a slow path, though, so we can just

		 * take the mutex.

 Loading zero below won't clear the base. */

/*

 *	switch_to(x,y) should switch tasks from x to y.

 *

 * This could still be optimized:

 * - fold all the options into a flag word and test it with a single test.

 * - could test fs/gs bitsliced

 *

 * Kprobes not supported here. Set the probe on schedule instead.

 * Function graph tracer not supported too.

	/* We must save %fs and %gs before load_TLS() because

	 * %fs and %gs may be cleared by load_TLS().

	 *

	 * (e.g. xen_load_tls())

	/*

	 * Load TLS before restoring any segments so that segment loads

	 * reference the correct GDT entries.

	/*

	 * Leave lazy mode, flushing any hypercalls made here.  This

	 * must be done after loading TLS entries in the GDT but before

	 * loading segments that might reference them.

	/* Switch DS and ES.

	 *

	 * Reading them only returns the selectors, but writing them (if

	 * nonzero) loads the full descriptor from the GDT or LDT.  The

	 * LDT for next is loaded in switch_mm, and the GDT is loaded

	 * above.

	 *

	 * We therefore need to write new values to the segment

	 * registers on every context switch unless both the new and old

	 * values are zero.

	 *

	 * Note that we don't need to do anything for CS and SS, as

	 * those are saved and restored as part of pt_regs.

	/*

	 * Switch the PDA and FPU contexts.

 Reload sp0. */

		/*

		 * AMD CPUs have a misfeature: SYSRET sets the SS selector but

		 * does not update the cached descriptor.  As a result, if we

		 * do SYSRET while SS is NULL, we'll end up in user mode with

		 * SS apparently equal to __USER_DS but actually unusable.

		 *

		 * The straightforward workaround would be to fix it up just

		 * before SYSRET, but that would slow down the system call

		 * fast paths.  Instead, we ensure that SS is never NULL in

		 * system call context.  We do this by replacing NULL SS

		 * selectors at every context switch.  SYSCALL sets up a valid

		 * SS, so the only way to get NULL is to re-enter the kernel

		 * from CPL 3 through an interrupt.  Since that can't happen

		 * in the same task as a running syscall, we are guaranteed to

		 * context switch between every interrupt vector entry and a

		 * subsequent SYSRET.

		 *

		 * We read SS first because SS reads are much faster than

		 * writes.  Out of caution, we force SS to __KERNEL_DS even if

		 * it previously had a different non-NULL value.

 Load the Intel cache allocation PQR MSR. */

 inherit personality from parent */

 Make sure to be in 64bit mode */

 Pretend that this comes from a 64bit execve */

	/* TBD: overwrites user setup. Should have two bits.

	   But 64bit processes have always behaved this way,

	   so it's not too bad. The main problem is just that

	/*

	 * in_32bit_syscall() uses the presence of the x32 syscall bit

	 * flag to determine compat status.  The x86 mmap() code relies on

	 * the syscall bitness so set x32 syscall bit right here to make

	 * in_32bit_syscall() work during exec().

	 *

	 * Pretend to come from a x32 execve.

		/*

		 * uprobes applied to this MM need to know this and

		 * cannot use user_64bit_mode() at that time.

 Prepare the first "return" to user space */

 Make sure to be in 32bit mode */

		/*

		 * ARCH_SET_GS has always overwritten the index

		 * and the base. Zero is the most sensible value

		 * to put in the index, and is the only value that

		 * makes any sense if FSGSBASE is unavailable.

			/*

			 * On non-FSGSBASE systems, save_base_legacy() expects

			 * that we also fill in thread.gsbase.

		/*

		 * Not strictly needed for %fs, but do it for symmetry

		 * with %gs

		/*

		 * Set the selector to 0 for the same reason

		 * as %gs above.

			/*

			 * On non-FSGSBASE systems, save_base_legacy() expects

			 * that we also fill in thread.fsbase.

 SPDX-License-Identifier: GPL-2.0

		/* We are bit sneaky here. We use the memory address to figure

		 * out if the node we depend on is past our point, if so, swap.

 Simple cyclic dependency checker. */

 Heavy handed way..*/

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains work-arounds for x86 and x86_64 platform bugs.

	/* BIOS may enable hardware IRQ balancing for

	 * E7520/E7320/E7525(revision ID 0x9 and below)

	 * based platforms.

	 * Disable SW irqbalance/affinity on those platforms.

 enable access to config space*/

	/*

	 * read xTPR register.  We may not have a pci_dev for device 8

	 * because it might be hidden until the above write.

 put back the original value for config space*/

 read the Function Disable register, dword mode only */

 HPET disabled in HPTC. Trying to enable */

 use bits 31:14, 16 kB aligned */

 read the Function Disable register, dword mode only */

 HPET is enabled in HPTC. Just not reported by BIOS */

 HPET disabled in HPTC. Trying to enable */

 ICH10 */

	/*

	 * Bit 17 is HPET enable bit.

	 * Bit 16:15 control the HPET base address.

	/*

	 * HPET is disabled. Trying enabling at FED00000 and check

	 * whether it sticks

 HPET is enabled in HPTC. Just not reported by BIOS */

/*

 * Undocumented chipset features. Make sure that the user enforced

 * this.

	/*

	 * Bit 7 is HPET enable bit.

	 * Bit 31:10 is HPET base address (contrary to what datasheet claims)

	/*

	 * HPET is disabled. Trying enabling at FED00000 and check

	 * whether it sticks

 base address */

 enable interrupt */

/*

 * Undocumented chipset feature taken from LinuxBIOS.

 ISA Bridges */

 LPC bridges */

/*

 * According to the datasheet e6xx systems have the HPET hardwired to

 * 0xfed00000

/*

 * HPET MSI on some boards (ATI SB700/SB800) has side effect on

 * floppy DMA. Disable HPET MSI on such platforms.

 * See erratum #27 (Misinterpreted MSI Requests May Result in

 * Corrupted LPC DMA Data) in AMD Publication #46837,

 * "SB700 Family Product Errata", Rev. 1.0, March 2010.

 Set correct numa_node information for AMD NB functions */

	/*

	 * Some hardware may return an invalid node ID,

	 * so check it first:

/*

 * Processor does not ensure DRAM scrub read/write sequence

 * is atomic wrt accesses to CC6 save state area. Therefore

 * if a concurrent scrub read/write access is to same address

 * the entry may appear as if it is not written. This quirk

 * applies to Fam16h models 00h-0Fh

 *

 * See "Revision Guide" for AMD F16h models 00h-0fh,

 * document 51810 rev. 3.04, Nov 2013

	/*

	 * Suggested workaround:

	 * set D18F3x58[4:0] = 00h and set D18F3x5C[0] = 0b

 Ivy Bridge, Haswell, Broadwell */

 Skylake */

	/*

	 * CAPID0{7:6} indicate whether this is an advanced RAS SKU

	 * CAPID5{8:5} indicate that various NVDIMM usage modes are

	 * enabled, so memory machine check recovery is also enabled.

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *  Copyright (C) 2000, 2001, 2002 Andi Kleen, SuSE Labs

 Called from get_stack_info_noinstr - so must be noinstr too */

 The user space code from other tasks cannot be accessed. */

	/*

	 * Make sure userspace isn't trying to trick us into dumping kernel

	 * memory by pointing the userspace instruction pointer at it.

	/*

	 * Even if named copy_from_user_nmi() this can be invoked from

	 * other contexts and will not try to resolve a pagefault, which is

	 * the correct thing to do here as this code can be called from any

	 * context.

/*

 * There are a couple of reasons for the 2/3rd prologue, courtesy of Linus:

 *

 * In case where we don't have the exact kernel image (which, if we did, we can

 * simply disassemble and navigate to the RIP), the purpose of the bigger

 * prologue is to have more context and to be able to correlate the code from

 * the different toolchains better.

 *

 * In addition, it helps in recreating the register allocation of the failing

 * kernel and thus make sense of the register dump.

 *

 * What is more, the additional complication of a variable length insn arch like

 * x86 warrants having longer byte sequence before rIP so that the disassembler

 * can "sync" up properly and find instruction boundaries when decoding the

 * opcode bytes.

 *

 * Thus, the 2/3rds prologue and 64 byte OPCODE_BUFSIZE is just a random

 * guesstimate in attempt to achieve all of the above.

 No access to the user space stack of other tasks. Ignore. */

	/*

	 * These on_stack() checks aren't strictly necessary: the unwind code

	 * has already validated the 'regs' pointer.  The checks are done for

	 * ordering reasons: if the registers are on the next stack, we don't

	 * want to print them out yet.  Otherwise they'll be shown as part of

	 * the wrong stack.  Later, when show_trace_log_lvl() switches to the

	 * next stack, this function will be called again with the same regs so

	 * they can be printed in the right context.

		/*

		 * When an interrupt or exception occurs in entry code, the

		 * full pt_regs might not have been saved yet.  In that case

		 * just print the iret frame.

	/*

	 * Iterate through the stacks, starting with the current stack pointer.

	 * Each stack has a pointer to the next one.

	 *

	 * x86-64 can have several stacks:

	 * - task stack

	 * - interrupt stack

	 * - HW exception stacks (double fault, nmi, debug, mce)

	 * - entry stack

	 *

	 * x86-32 can have up to four stacks:

	 * - task stack

	 * - softirq stack

	 * - hardirq stack

	 * - entry stack

			/*

			 * We weren't on a valid stack.  It's possible that

			 * we overflowed a valid stack into a guard page.

			 * See if the next page up is valid so that we can

			 * generate some kind of backtrace if this happens.

		/*

		 * Scan the stack, printing any text addresses we find.  At the

		 * same time, follow proper stack frames with the unwinder.

		 *

		 * Addresses found during the scan which are not reported by

		 * the unwinder are considered to be additional clues which are

		 * sometimes useful for debugging and are prefixed with '?'.

		 * This also serves as a failsafe option in case the unwinder

		 * goes off in the weeds.

			/*

			 * Don't print regs->ip again if it was already printed

			 * by show_regs_if_on_stack().

			/*

			 * When function graph tracing is enabled for a

			 * function, its return address on the stack is

			 * replaced with the address of an ftrace handler

			 * (return_to_handler).  In that case, before printing

			 * the "real" address, we want to print the handler

			 * address as an "unreliable" hint that function graph

			 * tracing was involved.

			/*

			 * Get the next frame from the unwinder.  No need to

			 * check for an error: if anything goes wrong, the rest

			 * of the addresses will just be printed as unreliable.

 if the frame has entry regs, print them */

	/*

	 * Stack frames below this one aren't interesting.  Don't show them

	 * if we're printing for %current.

 racy, but better than risking deadlock. */

 nested oops. should stop eventually */;

 Nest count reaches zero, release the lock. */

 Executive summary in case the oops scrolled away */

	/*

	 * We're not going to return, but we might be on an IST stack or

	 * have very little stack space left.  Rewind the stack and kill

	 * the task.

	 * Before we rewind the stack, we have to tell KASAN that we're going to

	 * reuse the task stack and that existing poisons are invalid.

 Save the regs of the first oops for the executive summary later. */

/*

 * This is gone through when something in the kernel has done something bad

 * and is about to be terminated:

	/*

	 * When in-kernel, we also print out the stack at the time of the fault..

 SPDX-License-Identifier: GPL-2.0

 Set this to 1 if there is a HW IOMMU in the system */

/*

 * See <Documentation/x86/x86_64/boot-options.rst> for the iommu kernel

 * parameter documentation.

 gart_parse_options has more force support */

 Must execute after PCI subsystem */

 Many VIA bridges seem to corrupt data for DAC. Disable it here */

 SPDX-License-Identifier: GPL-2.0

/*

 * check TSC synchronization.

 *

 * Copyright (C) 2006, Red Hat, Inc., Ingo Molnar

 *

 * We check whether all boot CPUs have their TSC's synchronized,

 * print a warning if not and turn off the TSC clock-source.

 *

 * The warp-check is point-to-point between two CPUs, the CPU

 * initiating the bootup is the 'source CPU', the freshly booting

 * CPU is the 'target CPU'.

 *

 * Only two CPUs may participate - they can enter in any order.

 * ( The serial nature of the boot logic and the CPU hotplug lock

 *   protects against more than 2 CPUs entering this code. )

/*

 * TSC's on different sockets may be reset asynchronously.

 * This may cause the TSC ADJUST value on socket 0 to be NOT 0.

 Skip unnecessary error messages if TSC already unstable */

 Rate limit the MSR check */

 Restore the original value */

	/*

	 * First online CPU in a package stores the boot value in the

	 * adjustment value. This value might change later via the sync

	 * mechanism. If that fails we still can yell about boot values not

	 * being consistent.

	 *

	 * On the boot cpu we just force set the ADJUST value to 0 if it's

	 * non zero. We don't do that on non boot cpus because physical

	 * hotplug should have set the ADJUST register to a value > 0 so

	 * the TSC is in sync with the already running cpus.

	 *

	 * Also don't force the ADJUST value to zero if that is a valid value

	 * for socket 0 as determined by the system arch.  This is required

	 * when multiple sockets are reset asynchronously with each other

	 * and socket 0 may not have an TSC ADJUST value of 0.

 Skip unnecessary error messages if TSC already unstable */

 !CONFIG_SMP */

/*

 * Store and check the TSC ADJUST MSR if available

	/*

	 * If a non-zero TSC value for socket 0 may be valid then the default

	 * adjusted value cannot assumed to be zero either.

	/*

	 * Check whether this CPU is the first in a package to come up. In

	 * this case do not check the boot value against another package

	 * because the new package might have been physically hotplugged,

	 * where TSC_ADJUST is expected to be different. When called on the

	 * boot CPU topology_core_cpumask() might not be available yet.

	/*

	 * Compare the boot value and complain if it differs in the

	 * package.

	/*

	 * The TSC_ADJUST values in a package must be the same. If the boot

	 * value on this newly upcoming CPU differs from the adjustment

	 * value of the already online CPU in this package, set it to that

	 * adjusted value.

	/*

	 * We have the TSCs forced to be in sync on this package. Skip sync

	 * test:

/*

 * Entry/exit counters that make sure that both CPUs

 * run the measurement code at once:

/*

 * We use a raw spinlock in this exceptional case, because

 * we want to have the fastest, inlined, non-debug version

 * of a critical section, to be able to prove TSC time-warps:

/*

 * TSC-warp measurement loop running on both CPUs.  This is not called

 * if there is no TSC.

	/*

	 * The measurement runs for 'timeout' msecs:

		/*

		 * We take the global lock, measure TSC, save the

		 * previous TSC that was measured (possibly on

		 * another CPU) and update the previous TSC timestamp.

		/*

		 * Be nice every now and then (and also check whether

		 * measurement is done [we also insert a 10 million

		 * loops safety exit, so we dont lock up in case the

		 * TSC readout is totally broken]):

		/*

		 * Outside the critical section we can now see whether

		 * we saw a time-warp of the TSC going backwards:

			/*

			 * Check whether this bounces back and forth. Only

			 * one CPU should observe time going backwards.

/*

 * If the target CPU coming online doesn't have any of its core-siblings

 * online, a timeout of 20msec will be used for the TSC-warp measurement

 * loop. Otherwise a smaller timeout of 2msec will be used, as we have some

 * information about this socket already (and this information grows as we

 * have more and more logical-siblings in that socket).

 *

 * Ideally we should be able to skip the TSC sync check on the other

 * core-siblings, if the first logical CPU in a socket passed the sync test.

 * But as the TSC is per-logical CPU and can potentially be modified wrongly

 * by the bios, TSC sync test for smaller duration should be able

 * to catch such errors. Also this will catch the condition where all the

 * cores in the socket don't get reset at the same time.

/*

 * Source CPU calls into this - it waits for the freshly booted

 * target CPU to arrive and then starts the measurement:

	/*

	 * No need to check if we already know that the TSC is not

	 * synchronized or if we have no TSC.

	/*

	 * Set the maximum number of test runs to

	 *  1 if the CPU does not provide the TSC_ADJUST MSR

	 *  3 if the MSR is available, so the target can try to adjust

	/*

	 * Wait for the target to start or to skip the test:

	/*

	 * Trigger the target to continue into the measurement too:

	/*

	 * If the test was successful set the number of runs to zero and

	 * stop. If not, decrement the number of runs an check if we can

	 * retry. In case of random warps no retry is attempted.

 Force it to 0 if random warps brought us here */

	/*

	 * Reset it - just in case we boot another CPU later:

	/*

	 * Let the target continue with the bootup:

	/*

	 * Retry, if there is a chance to do so.

/*

 * Freshly booted CPUs call into this:

 Also aborts if there is no TSC. */

	/*

	 * Store, verify and sanitize the TSC adjust register. If

	 * successful skip the test.

	 *

	 * The test is also skipped when the TSC is marked reliable. This

	 * is true for SoCs which have no fallback clocksource. On these

	 * SoCs the TSC is frequency synchronized, but still the TSC ADJUST

	 * register might have been wreckaged by the BIOS..

	/*

	 * Register this CPU's participation and wait for the

	 * source CPU to start the measurement:

	/*

	 * Store the maximum observed warp value for a potential retry:

	/*

	 * Ok, we are done:

	/*

	 * Wait for the source CPU to print stuff:

	/*

	 * Reset it for the next sync test:

	/*

	 * Check the number of remaining test runs. If not zero, the test

	 * failed and a retry with adjusted TSC is possible. If zero the

	 * test was either successful or failed terminally.

	/*

	 * If the warp value of this CPU is 0, then the other CPU

	 * observed time going backwards so this TSC was ahead and

	 * needs to move backwards.

	/*

	 * Add the result to the previous adjustment value.

	 *

	 * The adjustment value is slightly off by the overhead of the

	 * sync mechanism (observed values are ~200 TSC cycles), but this

	 * really depends on CPU, node distance and frequency. So

	 * compensating for this is hard to get right. Experiments show

	 * that the warp is not longer detectable when the observed warp

	 * value is used. In the worst case the adjustment needs to go

	 * through a 3rd run for fine tuning.

 CONFIG_SMP */

 SPDX-License-Identifier: GPL-2.0

/*

 * Firmware replacement code.

 *

 * Work around broken BIOSes that don't set an aperture, only set the

 * aperture in the AGP bridge, or set too small aperture.

 *

 * If all fails map the aperture over some low memory.  This is cheaper than

 * doing bounce buffering. The memory is lost. This is done at early boot

 * because only the bootmem allocator can allocate 32+MB.

 *

 * Copyright 2002 Andi Kleen, SuSE Labs.

/*

 * Using 512M as goal, in case kexec will load kernel_big

 * that will do the on-position decompress, and could overlap with

 * with the gart aperture that is used.

 * Sequence:

 * kernel_small

 * ==> kexec (with kdump trigger path or gart still enabled)

 * ==> kernel_small (gart area become e820_reserved)

 * ==> kexec (with kdump trigger path or gart still enabled)

 * ==> kerne_big (uncompressed size will be big than 64M or 128M)

 * So don't use 512M below as gart iommu, leave the space for kernel

 * code for safe.

 64MB */

/*

 * If the first kernel maps the aperture over e820 RAM, the kdump kernel will

 * use the same range because it will remain configured in the northbridge.

 * Trying to dump this area via /proc/vmcore may crash the machine, so exclude

 * it from vmcore.

/* This code runs before the PCI subsystem is initialized, so just

 aper_size should <= 1G */

	/*

	 * Aperture has to be naturally aligned. This means a 2GB aperture

	 * won't have much chance of finding a place in the lower 4GB of

	 * memory. Unfortunately we cannot move it up because that would

	 * make the IOMMU useless.

 Find a PCI capability */

 Read a standard AGPv3 bridge header */

 old_order could be the value from NB gart setting */

 Some BIOS use weird encodings not in the AGPv3 table. */

 < 32MB */

	/*

	 * On some sick chips, APSIZE is 0. It means it wants 4G

	 * so let double check that order, and lets trust AMD NB settings:

/*

 * Look for an AGP bridge. Windows only expects the aperture in the

 * AGP bridge and some BIOS forget to initialize the Northbridge too.

 * Work around this here.

 *

 * Do an PCI bus scan by hand because we're running before the PCI

 * subsystem.

 *

 * All AMD AGP bridges are AGPv3 compliant, so we can do this scan

 * generically. It's probably overkill to always scan all slots because

 * the AGP bridges should be always an own bus on the HT hierarchy,

 * but do it here for future safety.

 Poor man's PCI discovery */

 needed? */

 AGP bridge? */

 No multi-function device? */

/*

 * With kexec/kdump, if the first kernel doesn't shut down the GART and the

 * second kernel allocates a different GART region, there might be two

 * overlapping GART regions present:

 *

 * - the first still used by the GART initialized in the first kernel.

 * - (sub-)set of it used as normal RAM by the second kernel.

 *

 * which leads to memory corruptions and a kernel panic eventually.

 *

 * This can also happen if the BIOS has forgotten to mark the GART region

 * as reserved.

 *

 * Try to update the e820 map to mark that new region as reserved.

 This is mostly duplicate of iommu_hole_init */

 reserve it, so we can reuse it in second kernel */

 disable them all at first */

			/*

			 * Before we do anything else disable the GART. It may

			 * still be enabled if we boot into a crash-kernel here.

			 * Reconfiguring the GART while it is enabled could have

			 * unknown side-effects.

 the same between two setting from NB and agp */

			/*

			 * If this is the kdump kernel, the first kernel

			 * may have allocated the range over its e820 RAM

			 * and fixed up the northbridge

 Got the aperture from the AGP bridge */

			/*

			 * Could disable AGP and IOMMU here, but it's

			 * probably not worth it. But the later users

			 * cannot deal with bad apertures and turning

			 * on the aperture over memory causes very

			 * strange problems, so it's better to panic

			 * early.

	/*

	 * If this is the kdump kernel _and_ the first kernel did not

	 * configure the aperture in the northbridge, this range may

	 * overlap with the first kernel's memory. We can't access the

	 * range through vmcore even though it should be part of the dump.

 Fix up the north bridges */

		/*

		 * Don't enable translation yet but enable GART IO and CPU

		 * accesses and set DISTLBWALKPRB since GART table memory is UC.

 SPDX-License-Identifier: GPL-2.0

/*

 *  prepare to run common code

 *

 *  Copyright (C) 2000 Andrea Arcangeli <andrea@suse.de> SuSE

 cpu_feature_enabled() cannot be used this early */

/*

 * Manage page tables very early on.

/*

 * GDT used on the boot CPU before switching to virtual addresses.

/*

 * Address needs to be set at runtime because it references the startup_gdt

 * while the kernel still uses a direct mapping.

	/*

	 * 5-level paging is detected and enabled at kernel decompression

	 * stage. Only check if it has been enabled there.

/* Code in __startup_64() can be relocated during execution, but the compiler

 * doesn't have to generate PC-relative relocations when accessing globals from

 * that function. Clang actually does not generate them, which leads to

 * boot-time crashes. To work around this problem, every global pointer must

 * be adjusted using fixup_pointer().

 Is the address too large? */

	/*

	 * Compute the delta between the address I am compiled to run at

	 * and the address I am actually running at.

 Is the address not 2M aligned? */

 Activate Secure Memory Encryption (SME) if supported and enabled */

 Include the SME encryption mask in the fixup value */

 Fixup the physical addresses in the page table */

	/*

	 * Set up the identity mapping for the switchover.  These

	 * entries should *NOT* have the global bit set!  This also

	 * creates a bunch of nonsense entries but that is fine --

	 * it avoids problems around wraparound.

 Filter out unsupported __PAGE_KERNEL_* bits: */

	/*

	 * Fixup the kernel text+data virtual addresses. Note that

	 * we might write invalid pmds, when the kernel is relocated

	 * cleanup_highmap() fixes this up along with the mappings

	 * beyond _end.

	 *

	 * Only the region occupied by the kernel image has so far

	 * been checked against the table of usable memory regions

	 * provided by the firmware, so invalidate pages outside that

	 * region. A page table entry that maps to a reserved area of

	 * memory would allow processor speculation into that area,

	 * and on some hardware (particularly the UV platform) even

	 * speculative access to some reserved areas is caught as an

	 * error, causing the BIOS to halt the system.

 invalidate pages before the kernel image */

 fixup pages that are part of the kernel image */

 invalidate pages after the kernel image */

	/*

	 * Fixup phys_base - remove the memory encryption mask to obtain

	 * the true physical address.

 Encrypt the kernel and related (if SME is active) */

	/*

	 * Clear the memory encryption mask from the .bss..decrypted section.

	 * The bss section will be memset to zero later in the initialization so

	 * there is no need to zero it after changing the memory encryption

	 * attribute.

	 *

	 * This is early code, use an open coded check for SME instead of

	 * using cc_platform_has(). This eliminates worries about removing

	 * instrumentation or checking boot_cpu_data in the cc_platform_has()

	 * function.

	/*

	 * Return the SME encryption mask (if SME is active) to be used as a

	 * modifier for the initial pgdir entry programmed into CR3.

	/*

	 * Return the SME encryption mask (if SME is active) to be used as a

	 * modifier for the initial pgdir entry programmed into CR3.

 Wipe all early page tables except for the kernel symbol map */

 Create a new PMD entry */

 Invalid address or early pgt is done ?  */

	/*

	 * The use of __START_KERNEL_map rather than __PAGE_OFFSET here is

	 * critical -- __PAGE_OFFSET would point us back into the dynamic

	 * range and we might end up looping forever...

/* Don't add a printk in there. printk relies on the PDA which is not initialized 

	/*

	 * If SME is active, this will create decrypted mappings of the

	 * boot data in advance of the copy operations.

	/*

	 * The old boot data is no longer needed and won't be reserved,

	 * freeing up that memory for use by the system. If SME is active,

	 * we need to remove the mappings that were created so that the

	 * memory doesn't remain mapped as decrypted.

	/*

	 * Build-time sanity checks on the kernel image and module

	 * area mappings. (these are purely build-time and produce no code)

 Kill off the identity-map trampoline */

	/*

	 * SME support may update early_pmd_flags to include the memory

	 * encryption mask, so it needs to be called before anything

	 * that may generate a page fault.

	/*

	 * Load microcode early on BSP.

 set init_top_pgt kernel high mapping*/

 version is always not zero if it is copied */

/*

 * Data structures and code used for IDT setup in head_64.S. The bringup-IDT is

 * used until the idt_table takes over. On the boot CPU this happens in

 * x86_64_start_kernel(), on secondary CPUs in start_secondary(). In both cases

 * this happens in the functions called from head_64.S.

 *

 * The idt_table can't be used that early because all the code modifying it is

 * in idt.c and can be instrumented by tracing or KASAN, which both don't work

 * during early CPU bringup. Also the idt_table has the runtime vectors

 * configured which require certain CPU state to be setup already (like TSS),

 * which also hasn't happened yet in early CPU bringup.

 Set at runtime */

 This runs while still in the direct mapping */

 VMM Communication Exception */

 This is used when running on kernel addresses */

 VMM Communication Exception */

/*

 * Setup boot CPU state needed before kernel switches to virtual addresses.

 Load GDT */

 New GDT is live - reload data segment registers */

 SPDX-License-Identifier: GPL-2.0-only

	/*

	 * Do a binary range search to find the rightmost duplicate of a given

	 * starting address.  Some entries are section terminators which are

	 * "weak" entries for ensuring there are no gaps.  They should be

	 * ignored when they conflict with a real entry.

/*

 * Ftrace dynamic trampolines do not have orc entries of their own.

 * But they are copies of the ftrace entries that are static and

 * defined in ftrace_*.S, which do have orc entries.

 *

 * If the unwinder comes across a ftrace trampoline, then find the

 * ftrace function that was used to create it, and use that ftrace

 * function's orc entry, as the placement of the return code in

 * the stack will be identical.

 Prevent unlikely recursion */

/*

 * If we crash with IP==0, the last successfully executed instruction

 * was probably an indirect function call with a NULL function pointer,

 * and we don't have unwind information for NULL.

 * This hardcoded ORC entry for IP==0 allows us to unwind from a NULL function

 * pointer into its parent and then continue normally from there.

 Fake frame pointer entry -- used as a fallback for generated code */

 For non-init vmlinux addresses, use the fast lookup table: */

 vmlinux .init slow lookup: */

 Module lookup: */

 Swap the .orc_unwind_ip entries: */

 Swap the corresponding .orc_unwind entries: */

	/*

	 * The "weak" section terminator entries need to always be on the left

	 * to ensure the lookup code skips them in favor of real entries.

	 * These terminator entries exist to handle any gaps created by

	 * whitelisted .o files which didn't get objtool generation.

	/*

	 * The 'cur_orc_*' globals allow the orc_sort_swap() callback to

	 * associate an .orc_unwind_ip table entry with its corresponding

	 * .orc_unwind entry so they can both be swapped.

	/*

	 * Note, the orc_unwind and orc_unwind_ip tables were already

	 * sorted at build time via the 'sorttable' tool.

	 * It's ready for binary search straight away, no need to sort it.

 Initialize the fast lookup table: */

 Initialize the ending block: */

 x86-32 support will be more complicated due to the &regs->sp hack */

/*

 * If state->regs is non-NULL, and points to a full pt_regs, just get the reg

 * value from state->regs.

 *

 * Otherwise, if state->regs just points to IRET regs, and the previous frame

 * had full regs, it's safe to get the value from the previous regs.  This can

 * happen when early/late IRQ entry code gets interrupted by an NMI.

 Don't let modules unload while we're reading their ORC data. */

 End-of-stack check for user tasks: */

	/*

	 * Find the orc_entry associated with the text address.

	 *

	 * For a call frame (as opposed to a signal frame), state->ip points to

	 * the instruction after the call.  That instruction's stack layout

	 * could be different from the call instruction's layout, for example

	 * if the call was to a noreturn function.  So get the ORC data for the

	 * call instruction itself.

		/*

		 * As a fallback, try to assume this code uses a frame pointer.

		 * This is useful for generated code, like BPF, which ORC

		 * doesn't know about.  This is just a guess, so the rest of

		 * the unwind is no longer considered reliable.

 End-of-stack check for kernel threads: */

 Find the previous frame's stack: */

 Find IP, SP and possibly regs: */

		/*

		 * There is a small chance to interrupt at the entry of

		 * __kretprobe_trampoline() where the ORC info doesn't exist.

		 * That point is right after the RET to __kretprobe_trampoline()

		 * which was modified return address.

		 * At that point, the @addr_p of the unwind_recover_kretprobe()

		 * (this has to point the address of the stack entry storing

		 * the modified return address) must be "SP - (a stack entry)"

		 * because SP is incremented by the RET.

 See UNWIND_HINT_TYPE_REGS case comment. */

 Find BP: */

 Prevent a recursive loop due to bad ORC data: */

	/*

	 * Refuse to unwind the stack of a task while it's executing on another

	 * CPU.  This check is racy, but that's ok: the unwinder has other

	 * checks to prevent it from going off the rails.

		/*

		 * We weren't on a valid stack.  It's possible that

		 * we overflowed a valid stack into a guard page.

		 * See if the next page up is valid so that we can

		 * generate some kind of backtrace if this happens.

	/*

	 * The caller can provide the address of the first frame directly

	 * (first_frame) or indirectly (regs->sp) to indicate which stack frame

	 * to start unwinding at.  Skip ahead until we reach it.

 When starting from regs, skip the regs frame: */

 Otherwise, skip ahead to the user-specified starting frame: */

 SPDX-License-Identifier: GPL-2.0

/*

 * Split spinlock implementation out into its own file, so it can be

 * compiled in a FTRACE-compatible way.

 SPDX-License-Identifier: GPL-2.0

/*

 * ISA PIC or low IO-APIC triggered (INTA-cycle or APIC) interrupts:

 * (these are usually mapped to vectors 0x30-0x3f)

/*

 * The IO-APIC gives us many more interrupt sources. Most of these

 * are unused but an SMP system is supposed to have enough memory ...

 * sometimes (mostly wrt. hw bugs) we get corrupted vectors all

 * across the spectrum, so we really want to be prepared to get all

 * of these. Plus, more powerful systems might have more than 64

 * IO-APIC registers.

 *

 * (these are usually mapped into the 0x30-0xff vector range)

	/*

	 * Try to set up the through-local-APIC virtual wire mode earlier.

	 *

	 * On some 32-bit UP machines, whose APIC has been disabled by BIOS

	 * and then got re-enabled by "lapic", it hangs at boot time without this.

	/*

	 * On cpu 0, Assign ISA_IRQ_VECTOR(irq) to IRQ 0..15.

	 * If these IRQ's are handled by legacy interrupt-controllers like PIC,

	 * then this configuration will likely be static after the boot. If

	 * these IRQs are handled by more modern controllers like IO-APIC,

	 * then this vector space can be freed and re-used dynamically as the

	 * irq's migrate etc.

 Execute any quirks before the call gates are initialised: */

 IRQ2 is cascade interrupt to second interrupt controller */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * HPET address is set in acpi/boot.c, when an ACPI entry exists

 OS timer block num */

/*

 * HPET command line enable / disable

/**

 * is_hpet_enabled - Check whether the legacy HPET timer interrupt is enabled

/*

 * When the HPET driver (/dev/hpet) is enabled, we need to reserve

 * timer 0 and timer 1 in case of RTC emulation.

	/*

	 * NOTE that hd_irq[] reflects IOAPIC input pins (LEGACY_8254

	 * is wrong for i8259!) not the output IRQ.  Many BIOS writers

	 * don't bother configuring *any* comparator interrupts.

 Associate the first unused channel to /dev/hpet */

 Common HPET functions */

	/*

	 * HPET on AMD 81xx needs a second write (with HPET_TN_SETVAL

	 * cleared) to T0_CMP to set the period. The HPET_TN_SETVAL

	 * bit is automatically cleared after the first write.

	 * (See AMD-8111 HyperTransport I/O Hub Data Sheet,

	 * Publication # 24674)

	/*

	 * HPETs are a complete disaster. The compare register is

	 * based on a equal comparison and neither provides a less

	 * than or equal functionality (which would require to take

	 * the wraparound into account) nor a simple count down event

	 * mode. Further the write to the comparator register is

	 * delayed internally up to two HPET clock cycles in certain

	 * chipsets (ATI, ICH9,10). Some newer AMD chipsets have even

	 * longer delays. We worked around that by reading back the

	 * compare register, but that required another workaround for

	 * ICH9,10 chips where the first readout after write can

	 * return the old stale value. We already had a minimum

	 * programming delta of 5us enforced, but a NMI or SMI hitting

	 * between the counter readout and the comparator write can

	 * move us behind that point easily. Now instead of reading

	 * the compare register back several times, we make the ETIME

	 * decision based on the following: Return ETIME if the

	 * counter value after the write is less than HPET_MIN_CYCLES

	 * away from the event or if the counter is already ahead of

	 * the event. The minimum programming delta for the generic

	 * clockevents code is set to 1.5 * HPET_MIN_CYCLES.

	/*

	 * Start HPET with the boot CPU's cpumask and make it global after

	 * the IO_APIC has been initialized.

	/*

	 * Legacy horrors and sins from the past. HPET used periodic mode

	 * unconditionally forever on the legacy channel 0. Removing the

	 * below hack and using the conditional in hpet_init_clockevent()

	 * makes at least Qemu and one hardware machine fail to boot.

	 * There are two issues which cause the boot failure:

	 *

	 * #1 After the timer delivery test in IOAPIC and the IOAPIC setup

	 *    the next interrupt is not delivered despite the HPET channel

	 *    being programmed correctly. Reprogramming the HPET after

	 *    switching to IOAPIC makes it work again. After fixing this,

	 *    the next issue surfaces:

	 *

	 * #2 Due to the unconditional periodic mode availability the Local

	 *    APIC timer calibration can hijack the global clockevents

	 *    event handler without causing damage. Using oneshot at this

	 *    stage makes if hang because the HPET does not get

	 *    reprogrammed due to the handler hijacking. Duh, stupid me!

	 *

	 * Both issues require major surgery and especially the kick HPET

	 * again after enabling IOAPIC results in really nasty hackery.

	 * This 'assume periodic works' magic has survived since HPET

	 * support got added, so it's questionable whether this should be

	 * fixed. Both Qemu and the failing hardware machine support

	 * periodic mode despite the fact that both don't advertise it in

	 * the configuration register and both need that extra kick after

	 * switching to IOAPIC. Seems to be a feature...

 Start HPET legacy interrupts */

/*

 * HPET MSI Support

 Restore the MSI msg and unmask the interrupt */

 Invoked from the hotplug callback on @cpu */

 No point if MSI is disabled or CPU has an Always Runing APIC Timer */

 Only consider HPET channel with MSI support */

/*

 * Clock source related code

/*

 * Reading the HPET counter is a very slow operation. If a large number of

 * CPUs are trying to access the HPET counter simultaneously, it can cause

 * massive delays and slow down system performance dramatically. This may

 * happen when HPET is the default clock source instead of TSC. For a

 * really large system with hundreds of CPUs, the slowdown may be so

 * severe, that it can actually crash the system because of a NMI watchdog

 * soft lockup, for example.

 *

 * If multiple CPUs are trying to access the HPET counter at the same time,

 * we don't actually need to read the counter multiple times. Instead, the

 * other CPUs can use the counter value read by the first CPU in the group.

 *

 * This special feature is only enabled on x86-64 systems. It is unlikely

 * that 32-bit x86 systems will have enough CPUs to require this feature

 * with its associated locking overhead. We also need 64-bit atomic read.

 *

 * The lock and the HPET value are stored together and can be read in a

 * single atomic 64-bit read. It is explicitly assumed that arch_spinlock_t

 * is 32 bits in size.

	/*

	 * Read HPET directly if in NMI.

	/*

	 * Read the current state of the lock and HPET value atomically.

		/*

		 * Use WRITE_ONCE() to prevent store tearing.

	/*

	 * Contended case

	 * --------------

	 * Wait until the HPET value change or the lock is free to indicate

	 * its value is up-to-date.

	 *

	 * It is possible that old.value has already contained the latest

	 * HPET value while the lock holder was in the process of releasing

	 * the lock. Checking for lock state change will enable us to return

	 * the value immediately instead of waiting for the next HPET reader

	 * to come along.

/*

 * For UP or 32-bit.

/*

 * AMD SB700 based systems with spread spectrum enabled use a SMM based

 * HPET emulation to provide proper frequency setting.

 *

 * On such systems the SMM code is initialized with the first HPET register

 * access and takes some time to complete. During this time the config

 * register reads 0xffffffff. We check for max 1000 loops whether the

 * config register reads a non-0xffffffff value to make sure that the

 * HPET is up and running before we proceed any further.

 *

 * A counting loop is safe, as the HPET access takes thousands of CPU cycles.

 *

 * On non-SB700 based machines this check is only done once and has no

 * side effects.

	/*

	 * We don't know the TSC frequency yet, but waiting for

	 * 200000 TSC cycles is safe:

	 * 4 GHz == 50us

	 * 1 GHz == 200us

/*

 * Check whether the system supports PC10. If so force disable HPET as that

 * stops counting in PC10. This check is overbroad as it does not take any

 * of the following into account:

 *

 *	- ACPI tables

 *	- Enablement of intel_idle

 *	- Command line arguments which limit intel_idle C-state support

 *

 * That's perfectly fine. HPET is a piece of hardware designed by committee

 * and the only reasons why it is still in use on modern systems is the

 * fact that it is impossible to reliably query TSC and CPU frequency via

 * CPUID or firmware.

 *

 * If HPET is functional it is useful for calibrating TSC, but this can be

 * done via PMTIMER as well which seems to be the last remaining timer on

 * X86/INTEL platforms that has not been completely wreckaged by feature

 * creep.

 *

 * In theory HPET support should be removed altogether, but there are older

 * systems out there which depend on it because TSC and APIC timer are

 * dysfunctional in deeper C-states.

 *

 * It's only 20 years now that hardware people have been asked to provide

 * reliable and discoverable facilities which can be used for timekeeping

 * and per CPU timer interrupts.

 *

 * The probability that this problem is going to be solved in the

 * forseeable future is close to zero, so the kernel has to be cluttered

 * with heuristics to keep up with the ever growing amount of hardware and

 * firmware trainwrecks. Hopefully some day hardware people will understand

 * that the approach of "This can be fixed in software" is not sustainable.

 * Hope dies last...

 Check whether PC10 substates are supported */

 Check whether PC10 is enabled in PKG C-state limit */

/**

 * hpet_enable - Try to setup the HPET timer. Returns 1 on success.

 Validate that the config register is working */

	/*

	 * Read the period and check for a sane value:

 The period is a femtoseconds value. Convert it to a frequency. */

	/*

	 * Read the HPET ID register to retrieve the IRQ routing

	 * information and the number of channels

 This is the HPET channel number which is zero based */

	/*

	 * The legacy routing mode needs at least two channels, tick timer

	 * and the rtc emulation channel.

 Read, store and sanitize the global configuration */

 Read, store and sanitize the per channel configuration */

	/*

	 * Validate that the counter is counting. This needs to be done

	 * after sanitizing the config registers to properly deal with

	 * force enabled HPETs.

/*

 * The late initialization runs after the PCI quirks have been invoked

 * which might have detected a system on which the HPET can be enforced.

 *

 * Also, the MSI machinery is not working yet when the HPET is initialized

 * early.

 *

 * If the HPET is enabled, then:

 *

 *  1) Reserve one channel for /dev/hpet if CONFIG_HPET=y

 *  2) Reserve up to num_possible_cpus() channels as per CPU clockevents

 *  3) Setup /dev/hpet if CONFIG_HPET=y

 *  4) Register hotplug callbacks when clockevents are available

 Restore boot configuration with the enable bit cleared */

 Restore the channel boot configuration */

 If the HPET was enabled at boot time, reenable it */

/*

 * HPET in LegacyReplacement mode eats up the RTC interrupt line. When HPET

 * is enabled, we support RTC interrupt functionality in software.

 *

 * RTC has 3 kinds of interrupts:

 *

 *  1) Update Interrupt - generate an interrupt, every second, when the

 *     RTC clock is updated

 *  2) Alarm Interrupt - generate an interrupt at a specific time of day

 *  3) Periodic Interrupt - generate periodic interrupt, with frequencies

 *     2Hz-8192Hz (2Hz-64Hz for non-root user) (all frequencies in powers of 2)

 *

 * (1) and (2) above are implemented using polling at a frequency of 64 Hz:

 * DEFAULT_RTC_INT_FREQ.

 *

 * The exact frequency is a tradeoff between accuracy and interrupt overhead.

 *

 * For (3), we use interrupts at 64 Hz, or the user specified periodic frequency,

 * if it's higher.

/*

 * Check that the HPET counter c1 is ahead of c2

/*

 * Registers a IRQ handler.

/*

 * Deregisters the IRQ handler registered with hpet_register_irq_handler()

 * and does cleanup.

/*

 * Channel 1 for RTC emulation. We use one shot mode, as periodic mode

 * is not supported by all HPET implementations for channel 1.

 *

 * hpet_rtc_timer_init() is called when the rtc is initialized.

/*

 * The functions below are called from rtc driver.

 * Return 0 if HPET is not being used.

 * Otherwise do the necessary changes and return 1.

	/*

	 * Increment the comparator value until we are ahead of the

	 * current count.

 SPDX-License-Identifier: GPL-2.0

/*

 * sys_ia32.c: Conversion between 32bit and 64bit native syscalls. Based on

 *             sys_sparc32

 *

 * Copyright (C) 2000		VA Linux Co

 * Copyright (C) 2000		Don Dugger <n0ano@valinux.com>

 * Copyright (C) 1999		Arun Sharma <arun.sharma@intel.com>

 * Copyright (C) 1997,1998	Jakub Jelinek (jj@sunsite.mff.cuni.cz)

 * Copyright (C) 1997		David S. Miller (davem@caip.rutgers.edu)

 * Copyright (C) 2000		Hewlett-Packard Co.

 * Copyright (C) 2000		David Mosberger-Tang <davidm@hpl.hp.com>

 * Copyright (C) 2000,2001,2002	Andi Kleen, SuSE Labs (x86-64 port)

 *

 * These routines maintain argument size conversion between 32bit and 64bit

 * environment. In 2.5 most of this should be moved to a generic directory.

 *

 * This file assumes that there is a hole at the end of user address space.

 *

 * Some of the functions are LE specific currently. These are

 * hopefully all marked.  This should be fixed.

 warning: next two assume little endian */

/*

 * Some system calls that need sign extended arguments. This could be

 * done by a generic wrapper.

/*

 * Another set for IA32/LFS -- x86_64 struct stat is different due to

 * support for 64bit inode numbers.

/*

 * Linux/i386 didn't use to be able to handle more than

 * 4 system call parameters, so these system calls used a memory

 * block for parameter passing..

/*

 * The 32-bit clone ABI is CONFIG_CLONE_BACKWARDS

 CONFIG_IA32_EMULATION */

 SPDX-License-Identifier: GPL-2.0-only

	/*

	 * The caller can provide the address of the first frame directly

	 * (first_frame) or indirectly (regs->sp) to indicate which stack frame

	 * to start unwinding at.  Skip ahead until we reach it.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/x86/kernel/nmi-selftest.c

 *

 * Testsuite for NMI: IPIs

 *

 * Started by Don Zickus:

 * (using lib/locking-selftest.c as a guide)

 *

 *   Copyright (C) 2011 Red Hat, Inc., Don Zickus <dzickus@redhat.com>

 check to see if NMI IPIs work on this machine */

 trap all the unknown NMIs we may generate */

 sync above data before sending NMI */

 Don't wait longer than a second */

 What happens if we timeout, do we still unregister?? */

	/*

	 * Filter out expected failures:

        /*

	 * Run the testsuite:

 SPDX-License-Identifier: GPL-2.0-only

/*

 * vSMPowered(tm) systems specific initialization

 * Copyright (C) 2005 ScaleMP Inc.

 *

 * Ravikiran Thirumalai <kiran@scalemp.com>,

 * Shai Fultheim <shai@scalemp.com>

 * Paravirt ops integration: Glauber de Oliveira Costa <gcosta@redhat.com>,

 *			     Ravikiran Thirumalai <kiran@scalemp.com>

 set vSMP magic bits to indicate vSMP capable kernel */

 If possible, let the vSMP foundation route the interrupt optimally */

 Don't let users change irq affinity via procfs */

 Check if we are running on a ScaleMP vSMPowered box */

	/*

	 * CONFIG_X86_VSMP is not configured, so limit the number CPUs to the

	 * ones present in the first board, unless explicitly overridden by

	 * setup_max_cpus

 Read the vSMP Foundation topology register */

 The value 0 should be decoded as 8 */

 need to update phys_pkg_id */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Architecture specific (i386/x86_64) functions for kexec based crash dumps.

 *

 * Created by: Hariprasad Nellitheertha (hari@in.ibm.com)

 *

 * Copyright (C) IBM Corporation, 2004. All rights reserved.

 * Copyright (C) Red Hat Inc., 2014. All rights reserved.

 * Authors:

 *      Vivek Goyal <vgoyal@redhat.com>

 *

 Used while preparing memory map entries for second kernel */

 Type of memory */

/*

 * This is used to VMCLEAR all VMCSs loaded on the

 * processor. And when loading kvm_intel module, the

 * callback function pointer will be assigned.

 *

 * protected by rcu.

	/*

	 * VMCLEAR VMCSs loaded on all cpus if needed.

	/* Disable VMX or SVM if needed.

	 *

	 * We need to disable virtualization on all CPUs.

	 * Having VMX or SVM enabled on any CPU may break rebooting

	 * after the kdump kernel has finished its task.

	/*

	 * Disable Intel PT to stop its logging

 Override the weak function in kernel/panic.c */

 There are no cpus to shootdown */

	/* This function is only called after the system

	 * has panicked or is otherwise in a critical state.

	 * The minimum amount of code to allow a kexec'd kernel

	 * to run successfully needs to happen here.

	 *

	 * In practice this means shooting down the other cpus in

	 * an SMP system.

 The kernel is broken so disable interrupts */

	/*

	 * VMCLEAR VMCSs loaded on this cpu if needed.

	/* Booting kdump kernel with VMX or SVM enabled won't work,

	 * because (among other limitations) we can't disable paging

	 * with the virt flags.

	/*

	 * Disable Intel PT to stop its logging

 Prevent crash_kexec() from deadlocking on ioapic_lock. */

 Gather all the required information to prepare elf headers for ram regions */

	/*

	 * Exclusion of crash region and/or crashk_low_res may cause

	 * another range split. So add extra two slots here.

/*

 * Look for any unwanted ranges between mstart, mend and remove them. This

 * might lead to split and split ranges are put in cmem->ranges[] array

 Exclude the low 1M because it is always reserved */

 Exclude crashkernel region */

 Prepare elf headers. Return addr and size */

 Exclude unwanted mem ranges */

 By default prepare 64bit headers */

 Exclude elf header region */

 Prepare memory map for crash dump kernel */

 Add the low 1M */

 Add ACPI tables */

 Add ACPI Non-volatile Storage */

 Add e820 reserved ranges */

 Add crashk_low_res region */

 Exclude some ranges from crashk_res and add rest to memmap */

 If entry is less than a page, skip it */

 Prepare elf headers and add a segment */

 CONFIG_KEXEC_FILE */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *  Copyright (C) 2000, 2001, 2002 Andi Kleen, SuSE Labs

		/*

		 * On 64-bit, we have a generic entry stack that we

		 * use for all the kernel entry points, including

		 * SYSENTER.

/**

 * struct estack_pages - Page descriptor for exception stacks

 * @offs:	Offset from the start of the exception stack area

 * @size:	Size of the exception stack

 * @type:	Type to store in the stack_info struct

/*

 * Array of exception stack page descriptors. If the stack is larger than

 * PAGE_SIZE, all pages covering a particular stack will have the same

 * info. The guard pages including the not mapped DB2 stack are zeroed

 * out.

	/*

	 * Handle the case where stack trace is collected _before_

	 * cea_exception_stacks had been initialized.

 Bail if @stack is outside the exception stack area. */

 Calc page offset from start of exception stacks */

 Lookup the page descriptor */

 Guard page? */

	/*

	 * @end points directly to the top most stack entry to avoid a -8

	 * adjustment in the stack switch hotpath. Adjust it back before

	 * calculating @begin.

	/*

	 * Due to the switching logic RSP can never be == @end because the

	 * final operation is 'popq %rsp' which means after that RSP points

	 * to the original stack and not to @end.

	/*

	 * The next stack pointer is stored at the top of the irq stack

	 * before switching to the irq stack. Actual stack entries are all

	 * below that.

	/*

	 * Make sure we don't iterate through any given stack more than once.

	 * If it comes up a second time then there's something wrong going on:

	 * just break out and report an unknown stack type.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Intel SMP support routines.

 *

 *	(c) 1995 Alan Cox, Building #3 <alan@lxorguk.ukuu.org.uk>

 *	(c) 1998-99, 2000, 2009 Ingo Molnar <mingo@redhat.com>

 *      (c) 2002,2003 Andi Kleen, SuSE Labs.

 *

 *	i386 and x86_64 integration by Glauber Costa <gcosta@redhat.com>

/*

 *	Some notes on x86 processor bugs affecting SMP operation:

 *

 *	Pentium, Pentium Pro, II, III (and all CPUs) have bugs.

 *	The Linux implications for SMP are handled as follows:

 *

 *	Pentium III / [Xeon]

 *		None of the E1AP-E3AP errata are visible to the user.

 *

 *	E1AP.	see PII A1AP

 *	E2AP.	see PII A2AP

 *	E3AP.	see PII A3AP

 *

 *	Pentium II / [Xeon]

 *		None of the A1AP-A3AP errata are visible to the user.

 *

 *	A1AP.	see PPro 1AP

 *	A2AP.	see PPro 2AP

 *	A3AP.	see PPro 7AP

 *

 *	Pentium Pro

 *		None of 1AP-9AP errata are visible to the normal user,

 *	except occasional delivery of 'spurious interrupt' as trap #15.

 *	This is very rare and a non-problem.

 *

 *	1AP.	Linux maps APIC as non-cacheable

 *	2AP.	worked around in hardware

 *	3AP.	fixed in C0 and above steppings microcode update.

 *		Linux does not use excessive STARTUP_IPIs.

 *	4AP.	worked around in hardware

 *	5AP.	symmetric IO mode (normal Linux operation) not affected.

 *		'noapic' mode has vector 0xf filled out properly.

 *	6AP.	'noapic' mode might be affected - fixed in later steppings

 *	7AP.	We do not assume writes to the LVT deasserting IRQs

 *	8AP.	We do not enable low power mode (deep sleep) during MP bootup

 *	9AP.	We do not use mixed mode

 *

 *	Pentium

 *		There is a marginal case where REP MOVS on 100MHz SMP

 *	machines with B stepping processors can fail. XXX should provide

 *	an L1cache=Writethrough or L1cache=off option.

 *

 *		B stepping CPUs may hang. There are hardware work arounds

 *	for this. We warn about it in case your board doesn't have the work

 *	arounds. Basically that's so I can tell anyone with a B stepping

 *	CPU and SMP problems "tough".

 *

 *	Specific items [From Pentium Processor Specification Update]

 *

 *	1AP.	Linux doesn't use remote read

 *	2AP.	Linux doesn't trust APIC errors

 *	3AP.	We work around this

 *	4AP.	Linux never generated 3 interrupts of the same priority

 *		to cause a lost local interrupt.

 *	5AP.	Remote read is never used

 *	6AP.	not affected - worked around in hardware

 *	7AP.	not affected - worked around in hardware

 *	8AP.	worked around in hardware - we get explicit CS errors if not

 *	9AP.	only 'noapic' mode affected. Might generate spurious

 *		interrupts, we log only the first one and count the

 *		rest silently.

 *	10AP.	not affected - worked around in hardware

 *	11AP.	Linux reads the APIC between writes to avoid this, as per

 *		the documentation. Make sure you preserve this as it affects

 *		the C stepping chips too.

 *	12AP.	not affected - worked around in hardware

 *	13AP.	not affected - worked around in hardware

 *	14AP.	we always deassert INIT during bootup

 *	15AP.	not affected - worked around in hardware

 *	16AP.	not affected - worked around in hardware

 *	17AP.	not affected - worked around in hardware

 *	18AP.	not affected - worked around in hardware

 *	19AP.	not affected - worked around in BIOS

 *

 *	If this sounds worrying believe me these bugs are either ___RARE___,

 *	or are signal timing bugs worked around in hardware and there's

 *	about nothing of note with C stepping upwards.

 We are registered on stopping cpu too, avoid spurious NMI */

/*

 * this function calls the 'stop' function on all other CPUs in the system.

	/*

	 * Use an own vector here because smp_call_function

	 * does lots of things not suitable in a panic situation.

	/*

	 * We start by using the REBOOT_VECTOR irq.

	 * The irq is treated as a sync point to allow critical

	 * regions of code on other cpus to release their spin locks

	 * and re-enable irqs.  Jumping straight to an NMI might

	 * accidentally cause deadlocks with further shutdown/panic

	 * code.  By syncing, we give the cpus up to one second to

	 * finish their work before we force them off with the NMI.

 did someone beat us here? */

 sync above data before sending IRQ */

		/*

		 * Don't wait longer than a second for IPI completion. The

		 * wait request is not checked here because that would

		 * prevent an NMI shutdown attempt in case that not all

		 * CPUs reach shutdown state.

 if the REBOOT_VECTOR didn't work, try with the NMI */

		/*

		 * If NMI IPI is enabled, try to register the stop handler

		 * and send the IPI. In any case try to wait for the other

		 * CPUs to stop.

 Sync above data before sending IRQ */

		/*

		 * Don't wait longer than 10 ms if the caller didn't

		 * request it. If wait is true, the machine hangs here if

		 * one or more CPUs do not reach shutdown state.

/*

 * Reschedule call back. KVM uses this interrupt to force a cpu out of

 * guest mode.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Architecture specific debugfs files

 *

 * Copyright (C) 2007, Intel Corp.

 *	Huang Ying <ying.huang@intel.com>

 Is it direct data or invalid indirect one? */

 CONFIG_DEBUG_BOOT_PARAMS */

 SPDX-License-Identifier: GPL-2.0

/*

 *	Memory preserving reboot related code.

 *

 *	Created by: Hariprasad Nellitheertha (hari@in.ibm.com)

 *	Copyright (C) IBM Corporation, 2004. All rights reserved

/**

 * copy_oldmem_page - copy one page of memory

 * @pfn: page frame number to be copied

 * @buf: target memory address for the copy; this can be in kernel address

 *	space or user address space (see @userbuf)

 * @csize: number of bytes to copy

 * @offset: offset in bytes into the page (based on pfn) to begin the copy

 * @userbuf: if set, @buf is in user address space, use copy_to_user(),

 *	otherwise @buf is in kernel address space, use memcpy().

 *

 * Copy a page from the old kernel's memory. For this page, there is no pte

 * mapped in the current kernel. We stitch up a pte, similar to kmap_atomic.

/**

 * copy_oldmem_page_encrypted - same as copy_oldmem_page() above but ioremap the

 * memory with the encryption mask set to accommodate kdump on SME-enabled

 * machines.

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *  Copyright (C) 2000, 2001, 2002 Andi Kleen SuSE Labs

 *

 *  1997-11-28  Modified for POSIX.1b signals by Richard Henderson

 *  2000-06-20  Pentium III FXSR, SSE support by Gareth Hughes

 *  2000-2002   x86-64 support by Andi Kleen

 CONFIG_X86_64 */

/*

 * If regs->ss will cause an IRET fault, change it.  Otherwise leave it

 * alone.  Using this generally makes no sense unless

 * user_64bit_mode(regs) would return true.

 If invalid: */

 set ar = 0 */

	/*

	 * For a valid 64-bit user context, we need DPL 3, type

	 * read-write data or read-write exp-down data, and S and P

	 * set.  We can't use VERW because VERW doesn't check the

	 * P bit.

 Always make any pending restarted system calls return -EINTR */

 CONFIG_X86_32 */

 CONFIG_X86_64 */

 Get CS/SS and force CPL3 */

 disable syscall checks */

	/*

	 * Fix up SS if needed for the benefit of old DOSEMU and

	 * CRIU.

 CONFIG_X86_32 */

 CONFIG_X86_64 */

 !CONFIG_X86_32 */

 CONFIG_X86_32 */

 non-iBCS2 extensions.. */

/*

 * Set up a signal frame.

 x86 ABI requires 16-byte alignment */

/*

 * Determine which stack to use..

	/*

	 * Align the stack pointer according to the i386 ABI,

	 * i.e. so that on function entry ((sp + 4) & 15) == 0.

 !CONFIG_X86_32 */

 Default to using normal stack */

 redzone */

 This is the X/Open sanctioned signal stack switching.  */

		/*

		 * This checks nested_altstack via sas_ss_flags(). Sensible

		 * programs use SS_AUTODISARM, which disables that check, and

		 * programs that don't use SS_AUTODISARM get compatible.

 This is the legacy signal stack switching. */

	/*

	 * If we are on the alternate signal stack and would overflow it, don't.

	 * Return an always-bogus address instead so we will die with SIGSEGV.

 save i387 and extended state */

 popl %eax; movl $..., %eax */

 int $0x80 */

 movl $..., %eax */

 int $0x80 */

 Set up to return from userspace.  */

	/*

	 * This is popl %eax ; movl $__NR_sigreturn, %eax ; int $0x80

	 *

	 * WE DO NOT USE IT ANY MORE! It's only left here for historical

	 * reasons and because gdb uses it as a signature to notice

	 * signal handler stack frames.

 Set up registers for signal handler */

 Create the ucontext.  */

 Set up to return from userspace.  */

	/*

	 * This is movl $__NR_rt_sigreturn, %ax ; int $0x80

	 *

	 * WE DO NOT USE IT ANY MORE! It's only left here for historical

	 * reasons and because gdb uses it as a signature to notice

	 * signal handler stack frames.

 Set up registers for signal handler */

 !CONFIG_X86_32 */

 x86-64 should always use SA_RESTORER. */

 Create the ucontext.  */

	/* Set up to return from userspace.  If provided, use a stub

 Set up registers for signal handler */

 In case the signal handler was declared without prototypes */

	/* This also works for non SA_SIGINFO handlers because they expect the

	/*

	 * Set up the CS and SS registers to run signal handlers in

	 * 64-bit mode, even if the handler happens to be interrupting

	 * 32-bit or 16-bit code.

	 *

	 * SS is subtle.  In 64-bit mode, we don't need any particular

	 * SS descriptor, but we do need SS to be valid.  It's possible

	 * that the old SS is entirely bogus -- this can happen if the

	 * signal we're trying to deliver is #GP or #SS caused by a bad

	 * SS value.  We also have a compatibility issue here: DOSEMU

	 * relies on the contents of the SS register indicating the

	 * SS value at the time of the signal, even though that code in

	 * DOSEMU predates sigreturn's ability to restore SS.  (DOSEMU

	 * avoids relying on sigreturn to restore SS; instead it uses

	 * a trampoline.)  So we do our best: if the old SS was valid,

	 * we keep it.  Otherwise we replace it.

 CONFIG_X86_32 */

 CONFIG_X86_X32_ABI */

 Create the ucontext.  */

 Set up registers for signal handler */

 We use the x32 calling convention here... */

 CONFIG_X86_X32_ABI */

/*

 * Do a signal return; undo the signal stack.

	/*

	 * x86_32 has no uc_flags bits relevant to restore_sigcontext.

	 * Save a few cycles by skipping the __get_user.

 CONFIG_X86_32 */

/*

 * There are four different struct types for signal frame: sigframe_ia32,

 * rt_sigframe_ia32, rt_sigframe_x32, and rt_sigframe. Use the worst case

 * -- the largest size. It means the size for 64-bit apps is a bit more

 * than needed, but this keeps the code simple.

/*

 * The FP state frame contains an XSAVE buffer which must be 64-byte aligned.

 * If a signal frame starts at an unaligned address, extra space is required.

 * This is the max alignment padding, conservatively.

/*

 * The frame data is composed of the following areas and laid out as:

 *

 * -------------------------

 * | alignment padding     |

 * -------------------------

 * | (f)xsave frame        |

 * -------------------------

 * | fsave header          |

 * -------------------------

 * | alignment padding     |

 * -------------------------

 * | siginfo + ucontext    |

 * -------------------------

 max_frame_size tells userspace the worst case signal stack size. */

 Userspace expects an aligned size. */

 Perform fixup for the pre-signal frame. */

 Set up the stack frame */

 Are we from a system call? */

 If so, check system call restarting.. */

	/*

	 * If TF is set due to a debugger (TIF_FORCED_TF), clear TF now

	 * so that register information in the sigcontext is correct and

	 * then notify the tracer before entering the signal handler.

		/*

		 * Clear the direction flag as per the ABI for function entry.

		 *

		 * Clear RF when entering the signal handler, because

		 * it might disable possible debug exception from the

		 * signal handler.

		 *

		 * Clear TF for the case when it wasn't set by debugger to

		 * avoid the recursive send_sigtrap() in SIGTRAP handler.

		/*

		 * Ensure the signal handler starts with the new fpu state.

/*

 * Note that 'init' is a special process: it doesn't get signals it doesn't

 * want to handle. Thus you cannot kill init even with a SIGKILL even by

 * mistake.

 Whee! Actually deliver the signal.  */

 Did we come from a system call? */

 Restart the system call - no handlers present */

	/*

	 * If there's no signal to deliver, we just put the saved sigmask

	 * back.

/*

 * MINSIGSTKSZ is 2048 and can't be changed despite the fact that AVX512

 * exceeds that size already. As such programs might never use the

 * sigaltstack they just continued to work. While always checking against

 * the real size would be correct, this might be considered a regression.

 *

 * Therefore avoid the sanity check, unless enforced by kernel

 * configuration or command line option.

 *

 * When dynamic FPU features are supported, the check is also enforced when

 * the task has permissions to use dynamic features. Tasks which have no

 * permission are checked against the size of the non-dynamic feature set

 * if strict checking is enabled. This avoids forcing all tasks on the

 * system to allocate large sigaltstacks even if they are never going

 * to use a dynamic feature. As this is serialized via sighand::siglock

 * any permission request for a dynamic feature either happened already

 * or will see the newly install sigaltstack size in the permission checks.

 CONFIG_DYNAMIC_SIGFRAME */

 SPDX-License-Identifier: GPL-2.0-only

	/*

	 * GCC can occasionally decide to realign the stack pointer and change

	 * the offset of the stack frame in the prologue of a function called

	 * by head/entry code.  Examples:

	 *

	 * <start_secondary>:

	 *      push   %edi

	 *      lea    0x8(%esp),%edi

	 *      and    $0xfffffff8,%esp

	 *      pushl  -0x4(%edi)

	 *      push   %ebp

	 *      mov    %esp,%ebp

	 *

	 * <x86_64_start_kernel>:

	 *      lea    0x8(%rsp),%r10

	 *      and    $0xfffffffffffffff0,%rsp

	 *      pushq  -0x8(%r10)

	 *      push   %rbp

	 *      mov    %rsp,%rbp

	 *

	 * After aligning the stack, it pushes a duplicate copy of the return

	 * address before pushing the frame pointer.

	/*

	 * When unwinding from an ftrace handler of a function called by entry

	 * code, the stack layout of the last frame is:

	 *

	 *   bp

	 *   parent ret addr

	 *   bp

	 *   function ret addr

	 *   parent ret addr

	 *   pt_regs

	 *   -----------------

/*

 * This determines if the frame pointer actually contains an encoded pointer to

 * pt_regs on the stack.  See ENCODE_FRAME_POINTER.

 Is the next frame pointer an encoded pointer to pt_regs? */

	/*

	 * If the next bp isn't on the current stack, switch to the next one.

	 *

	 * We may have to traverse multiple stacks to deal with the possibility

	 * that info->next_sp could point to an empty stack and the next bp

	 * could be on a subsequent stack.

 Make sure it only unwinds up and doesn't overlap the prev frame: */

 Move state to the next frame: */

 Save the return address: */

 Save the original stack pointer for unwind_dump(): */

 Have we reached the end? */

		/*

		 * kthreads (other than the boot CPU's idle thread) have some

		 * partial regs at the end of their stack which were placed

		 * there by copy_thread().  But the regs don't have any

		 * useful information, so we can skip them.

		 *

		 * This user_mode() check is slightly broader than a PF_KTHREAD

		 * check because it also catches the awkward situation where a

		 * newly forked kthread transitions into a user task by calling

		 * kernel_execve(), which eventually clears PF_KTHREAD.

		/*

		 * We're almost at the end, but not quite: there's still the

		 * syscall regs frame.  Entry code doesn't encode the regs

		 * pointer for syscalls, so we have to set it manually.

 Get the next frame pointer: */

 Move to the next frame if it's safe: */

	/*

	 * When unwinding a non-current task, the task might actually be

	 * running on another CPU, in which case it could be modifying its

	 * stack while we're reading it.  This is generally not a problem and

	 * can be ignored as long as the caller understands that unwinding

	 * another task will not always succeed.

	/*

	 * Don't warn if the unwinder got lost due to an interrupt in entry

	 * code or in the C handler before the first frame pointer got set up:

	/*

	 * There are some known frame pointer issues on 32-bit.  Disable

	 * unwinder warnings on 32-bit until it gets objtool support.

 Don't even attempt to start from user mode regs: */

	/*

	 * If we crash with IP==0, the last successfully executed instruction

	 * was probably an indirect function call with a NULL function pointer.

	 * That means that SP points into the middle of an incomplete frame:

	 * *SP is a return pointer, and *(SP-sizeof(unsigned long)) is where we

	 * would have written a frame pointer if we hadn't crashed.

	 * Pretend that the frame is complete and that BP points to it, but save

	 * the real BP so that we can use it when looking for the next frame.

 Initialize stack info and make sure the frame data is accessible: */

	/*

	 * The caller can provide the address of the first frame directly

	 * (first_frame) or indirectly (regs->sp) to indicate which stack frame

	 * to start unwinding at.  Skip ahead until we reach it.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

/*

 * Copyright (C) 2004 Amit S. Kale <amitkale@linsyssoft.com>

 * Copyright (C) 2000-2001 VERITAS Software Corporation.

 * Copyright (C) 2002 Andi Kleen, SuSE Labs

 * Copyright (C) 2004 LinSysSoft Technologies Pvt. Ltd.

 * Copyright (C) 2007 MontaVista Software, Inc.

 * Copyright (C) 2007-2008 Jason Wessel, Wind River Systems, Inc.

/****************************************************************************

 *  Contributor:     Lake Stevens Instrument Division$

 *  Written by:      Glenn Engel $

 *  Updated by:	     Amit Kale<akale@veritas.com>

 *  Updated by:	     Tom Rini <trini@kernel.crashing.org>

 *  Updated by:	     Jason Wessel <jason.wessel@windriver.com>

 *  Modified for 386 by Jim Kingdon, Cygnus Support.

 *  Original kgdb, compatibility with 2.1.xx kernel by

 *  David Grothe <dave@gcom.com>

 *  Integrated into 2.2.5 kernel by Tigran Aivazian <tigran@sco.com>

 *  X86_64 changes from Andi Kleen's patch merged by Jim Houston

/**

 *	sleeping_thread_to_gdb_regs - Convert ptrace regs to GDB regs

 *	@gdb_regs: A pointer to hold the registers in the order GDB wants.

 *	@p: The &struct task_struct of the desired process.

 *

 *	Convert the register values of the sleeping process in @p to

 *	the format that GDB expects.

 *	This function is called when kgdb does not have access to the

 *	&struct pt_regs and therefore it should fill the gdb registers

 *	@gdb_regs with what has	been saved in &struct thread_struct

 *	thread field during switch_to.

			/*

			 * The debugger is responsible for handing the retry on

			 * remove failure.

/**

 *	kgdb_disable_hw_debug - Disable hardware debugging while we in kgdb.

 *	@regs: Current &struct pt_regs.

 *

 *	This function will be called if the particular architecture must

 *	disable hardware debugging while it is processing gdb packets or

 *	handling exception.

 Disable hardware debugging while we are in kgdb: */

/**

 *	kgdb_roundup_cpus - Get other CPUs into a holding pattern

 *

 *	On SMP systems, we need to get the attention of the other CPUs

 *	and get them be in a known state.  This should do what is needed

 *	to get the other CPUs to call kgdb_wait(). Note that on some arches,

 *	the NMI approach is not used for rounding up all the CPUs. For example,

 *	in case of MIPS, smp_call_function() is used to roundup CPUs.

 *

 *	On non-SMP systems, this is not called.

/**

 *	kgdb_arch_handle_exception - Handle architecture specific GDB packets.

 *	@e_vector: The error vector of the exception that happened.

 *	@signo: The signal number of the exception that happened.

 *	@err_code: The error code of the exception that happened.

 *	@remcomInBuffer: The buffer of the packet we have read.

 *	@remcomOutBuffer: The buffer of %BUFMAX bytes to write a packet into.

 *	@linux_regs: The &struct pt_regs of the current process.

 *

 *	This function MUST handle the 'c' and 's' command packets,

 *	as well packets to set / remove a hardware breakpoint, if used.

 *	If there are additional packets which the hardware needs to handle,

 *	they are handled here.  The code should return -1 if it wants to

 *	process more packets, and a %0 or %1 if it wants to exit from the

 *	kgdb callback.

 try to read optional parameter, pc unchanged if no parm */

 clear the trace bit */

 set the trace bit if we're stepping */

 this means that we do not want to exit from the handler: */

	/*

	 * Single step exception from kernel space to user space so

	 * eat the exception and continue the process:

	/*

	 * Reset the BS bit in dr6 (pointed by args->err) to

	 * denote completion of processing

 KGDB CPU roundup */

 do nothing */

			/* This means a user thread is single stepping

			 * a system call which should be ignored

 Must touch watchdog before return to normal operation */

/**

 *	kgdb_arch_init - Perform any architecture specific initialization.

 *

 *	This function will handle the initialization of any architecture

 *	specific callbacks.

	/*

	 * Pre-allocate the hw breakpoint instructions in the non-atomic

	 * portion of kgdb because this operation requires mutexs to

	 * complete.

/**

 *	kgdb_arch_exit - Perform any architecture specific uninitalization.

 *

 *	This function will handle the uninitalization of any architecture

 *	specific callbacks, for dynamic registration and unregistration.

/**

 *

 *	kgdb_skipexception - Bail out of KGDB when we've been triggered.

 *	@exception: Exception vector number

 *	@regs: Current &struct pt_regs.

 *

 *	On some architectures we need to skip a breakpoint exception when

 *	it occurs after a breakpoint has been removed.

 *

 * Skip an int3 exception when it occurs after a breakpoint has been

 * removed. Backtrack eip by 1 since the int3 would have caused it to

 * increment by 1.

	/*

	 * It is safe to call text_poke_kgdb() because normal kernel execution

	 * is stopped on all cores, so long as the text_mutex is not locked.

	/*

	 * It is safe to call text_poke_kgdb() because normal kernel execution

	 * is stopped on all cores, so long as the text_mutex is not locked.

 Breakpoint instruction: */

 SPDX-License-Identifier: GPL-2.0

/*

 * AMD Family 10h mmconfig enablement

 need to avoid (0xfd<<32), (0xfe<<32), and (0xff<<32), ht used space */

 only try to get setting from BSP */

 SYS_CFG */

 TOP_MEM2 is not enabled? */

 TOP_MEM2 */

	/*

	 * need to check if the range is in the high mmio range that is

	 * above 4G

 39:16 on 31:8*/

 39:16 on 31:8*/

 sort the range */

 need to find one window */

 need to find window between ranges */

 try to make sure that AP's setting is identical to BSP setting */

 only trust the one handle 256 buses, if acpi=off */

	/*

	 * if it is not enabled, try to enable it and assume only one segment

	 * with 256 buses

 Called from a non __init function, but only on the BSP. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Jailhouse paravirt_ops implementation

 *

 * Copyright (c) Siemens AG, 2015-2017

 *

 * Authors:

 *  Jan Kiszka <jan.kiszka@siemens.com>

	/*

	 * We do not have access to IR inside Jailhouse non-root cells.  So

	 * we have to run in physical mode.

	/*

	 * This will trigger the switch to apic_x2apic_phys.  Empty OEM IDs

	 * ensure that only this APIC driver picks up the call.

 Register 1:1 mapping for legacy UART IRQs 3 and 4 */

	/*

	 * There are no bridges on the virtual PCI root bus under Jailhouse,

	 * thus no other way to discover all devices than a full scan.

	 * Respect any overrides via the command line, though.

 Deactivate UART if access isn't allowed */

	/*

	 * There are flags inside setup_data that indicate availability of

	 * platform UARTs since setup data version 2.

	 *

	 * In case of version 1, we don't know which UARTs belong Linux. In

	 * this case, unconditionally register 1:1 mapping for legacy UART IRQs

	 * 3 and 4.

 !CONFIG_SERIAL_8250 */

 CONFIG_SERIAL_8250 */

 setup data must at least contain the header */

	/*

	 * Avoid that the kernel complains about missing ACPI tables - there

	 * are none in a non-root cell.

	/*

	 * The x2APIC is only available if the root cell enabled it. Jailhouse

	 * does not support switching between xAPIC and x2APIC.

 SPDX-License-Identifier: GPL-2.0-only

 By Ross Biro 1/23/92 */

/*

 * Pentium III FXSR, SSE support

 *	Gareth Hughes <gareth@valinux.com>, May 2000

/**

 * regs_query_register_offset() - query register offset from its name

 * @name:	the name of a register

 *

 * regs_query_register_offset() returns the offset of a register in struct

 * pt_regs from its name. If the name is invalid, this returns -EINVAL;

/**

 * regs_query_register_name() - query register name from its offset

 * @offset:	the offset of a register in struct pt_regs.

 *

 * regs_query_register_name() returns the name of a register from its

 * offset in struct pt_regs. If the @offset is invalid, this returns NULL;

/*

 * does not yet catch signals sent when the child dies.

 * in exit.c or in signal.c.

/*

 * Determines which flags the user has access to [1 = access, 0 = no access].

/*

 * Determines whether a value may be installed in a segment register.

	/*

	 * Returning the value truncates it to 16 bits.

	/*

	 * The value argument was already truncated to 16 bits.

	/*

	 * For %cs and %ss we cannot permit a null selector.

	 * We can permit a bogus selector as long as it has USER_RPL.

	 * Null selectors are fine for other segment registers, but

	 * we will never get back to user mode with invalid %cs or %ss

	 * and will take the trap in iret instead.  Much code relies

	 * on user_mode() to distinguish a user trap frame (which can

	 * safely use invalid selectors) from a kernel trap frame.

 CONFIG_X86_64 */

	/*

	 * Returning the value truncates it to 16 bits.

 Older gas can't assemble movq %?s,%r?? */

	/*

	 * The value argument was already truncated to 16 bits.

	/*

	 * Writes to FS and GS will change the stored selector.  Whether

	 * this changes the segment base as well depends on whether

	 * FSGSBASE is enabled.

		/*

		 * Can't actually change these in 64-bit mode.

 CONFIG_X86_32 */

	/*

	 * If the debugger set TF, hide it from the readout.

	/*

	 * If the user value contains TF, mark that

	 * it was not "us" (the debugger) that set it.

	 * If not, make sure it stays set if we had.

	/*

	 * Store in the virtual DR6 register the fact that the breakpoint

	 * was hit so the thread's debugger will see it.

/*

 * Walk through every ptrace breakpoints for this thread and

 * build the dr7 value on top of their attributes.

 *

/*

 * Handle ptrace writes to debug register 7.

 Restore if the first pass failed, second_pass shouldn't fail. */

/*

 * Handle PTRACE_PEEKUSR calls for the debug register area.

 Flip back to arch polarity */

		/*

		 * Put stub len and type to create an inactive but correct bp.

		 *

		 * CHECKME: the previous code returned -EIO if the addr wasn't

		 * a valid task virtual addr. The new one will return -EINVAL in

		 *  this case.

		 * -EINVAL may be what we want for in-kernel breakpoints users,

		 * but -EIO looks better for ptrace, since we refuse a register

		 * writing for the user. And anyway this is the previous

		 * behaviour.

/*

 * Handle PTRACE_POKEUSR calls for the debug register area.

 There are no DR4 or DR5 registers */

 Flip to positive polarity */

/*

 * These access the current or another (stopped) task's io permission

 * bitmap for debugging or core dump.

/*

 * Called by kernel/ptrace.c when detaching..

 *

 * Make sure the single step bit is not set.

 Initialized below. */

 Initialized below. */

 This is native 64-bit ptrace() */

 This is native 32-bit ptrace() */

 read the word at location addr in the USER area. */

 Default return condition */

 write the word at location addr in the USER area */

 Get all gp regs from the child. */

 Set all gp regs in the child. */

 Get the child FPU state. */

 Set the child FPU state. */

 Get the child extended FPU state. */

 Set the child extended FPU state. */

		/* normal 64bit interface to access TLS data.

		   Works just like arch_prctl, except that the arguments

	/*

	 * A 32-bit ptracer on a 64-bit kernel expects that writing

	 * FS or GS will also update the base.  This is needed for

	 * operations like PTRACE_SETREGS to fully restore a saved

	 * CPU state.

		/*

		 * Warning: bizarre corner case fixup here.  A 32-bit

		 * debugger setting orig_eax to -1 wants to disable

		 * syscall restart.  Make sure that the syscall

		 * restart code sign-extends orig_ax.  Also make sure

		 * we interpret the -ERESTART* codes correctly if

		 * loaded into regs->ax in case the task is not

		 * actually still sitting at the exit from a 32-bit

		 * syscall with TS_COMPAT still set.

		/*

		 * Other dummy fields in the virtual user structure

		 * are ignored

		/*

		 * Other dummy fields in the virtual user structure

		 * are ignored

 Get all gp regs from the child. */

 Set all gp regs in the child. */

 Get the child FPU state. */

 Set the child FPU state. */

 Get the child extended FPU state. */

 Set the child extended FPU state. */

 CONFIG_IA32_EMULATION */

	/* Read 32bits at location addr in the USER area.  Only allow

 Default return condition */

	/* Write the word at location addr in the USER area.  Only allow

	   to update segment and debug registers with the upper 32bits

 Get all gp regs from the child. */

 Set all gp regs in the child. */

 Get the child FPU state. */

 Set the child FPU state. */

 CONFIG_COMPAT */

 CONFIG_X86_32 */

 CONFIG_X86_64 */

/*

 * This represents bytes 464..511 in the memory layout exported through

 * the REGSET_XSTATE interface.

/*

 * This is used by the core dump code to decide which regset to dump.  The

 * core dump code writes out the resulting .e_machine and the corresponding

 * regsets.  This is suboptimal if the task is messing around with its CS.L

 * field, but at worst the core dump will end up missing some information.

 *

 * Unfortunately, it is also used by the broken PTRACE_GETREGSET and

 * PTRACE_SETREGSET APIs.  These APIs look at the .regsets field but have

 * no way to make sure that the e_machine they use matches the caller's

 * expectations.  The result is that the data format returned by

 * PTRACE_GETREGSET depends on the returned CS field (and even the offset

 * of the returned CS field depends on its value!) and the data format

 * accepted by PTRACE_SETREGSET is determined by the old CS value.  The

 * upshot is that it is basically impossible to use these APIs correctly.

 *

 * The best way to fix it in the long run would probably be to add new

 * improved ptrace() APIs to read and write registers reliably, possibly by

 * allowing userspace to select the ELF e_machine variant that they expect.

 Send us the fake SIGTRAP */

 SPDX-License-Identifier: GPL-2.0-or-later

/*  Paravirtualization interfaces

    Copyright (C) 2006 Rusty Russell IBM Corporation





    2007 - x86_64 support added by Glauber de Oliveira Costa, Red Hat Inc

/*

 * nop stub, which must not clobber anything *including the stack* to

 * avoid confusing the entry prologues.

 stub always returning 0. */

 Undefined instruction for dealing with missing ops pointers. */

 Kernel might not be viable if patching fails, bail out: */

 call */

 identity function, which can be inlined */

	/*

	 * Neat trick to map patch type back to the call within the

	 * corresponding structure.

 If there's no function, patch it with paravirt_BUG() */

 Otherwise call the function. */

 These are in entry.S */

/*

 * Reserve the whole legacy IO space to prevent any legacy drivers

 * from wasting time probing for their hardware.  This is a fairly

 * brute-force approach to disabling all non-virtual drivers.

 *

 * Note that this must be called very early to have any effect.

 64-bit pagetable entries */

 Cpu ops. */

 Irq ops. */

 CONFIG_PARAVIRT_XXL */

 Mmu ops. */

 CONFIG_PGTABLE_LEVELS >= 5 */

 CONFIG_PARAVIRT_XXL */

 Lock ops. */

 SMP */

 SPDX-License-Identifier: GPL-2.0

/*

 * x86 specific code for irq_work

 *

 * Copyright (C) 2010 Red Hat, Inc., Peter Zijlstra

 SPDX-License-Identifier: GPL-2.0

/*

 *	Memory preserving reboot related code.

 *

 *	Created by: Hariprasad Nellitheertha (hari@in.ibm.com)

 *	Copyright (C) IBM Corporation, 2004. All rights reserved

	/*

	 * non-PAE kdump kernel executed from a PAE one will crop high pte

	 * bits and poke unwanted space counting again from address 0, we

	 * don't want that. pte must fit into unsigned long. In fact the

	 * test checks high 12 bits for being zero (pfn will be shifted left

	 * by PAGE_SHIFT).

/**

 * copy_oldmem_page - copy one page from "oldmem"

 * @pfn: page frame number to be copied

 * @buf: target memory address for the copy; this can be in kernel address

 *	space or user address space (see @userbuf)

 * @csize: number of bytes to copy

 * @offset: offset in bytes into the page (based on pfn) to begin the copy

 * @userbuf: if set, @buf is in user address space, use copy_to_user(),

 *	otherwise @buf is in kernel address space, use memcpy().

 *

 * Copy a page from "oldmem". For this page, there might be no pte mapped

 * in the current kernel.

/*

 *  Copyright (C) 1995  Linus Torvalds

 *

 *  Pentium III FXSR, SSE support

 *	Gareth Hughes <gareth@valinux.com>, May 2000

/*

 * This file handles the architecture-dependent parts of process handling..

 Only print out debug registers if they are in their non-default state. */

/*

 *	switch_to(x,y) should switch tasks from x to y.

 *

 * We fsave/fwait so that an exception goes off at the right time

 * (as a call from the fsave or fwait in effect) rather than to

 * the wrong process. Lazy FP saving no longer makes any sense

 * with modern CPU's, and this simplifies a lot of things (SMP

 * and UP become the same).

 *

 * NOTE! We used to use the x86 hardware context switching. The

 * reason for not using it any more becomes apparent when you

 * try to recover gracefully from saved state that is no longer

 * valid (stale segment register values in particular). With the

 * hardware task-switch, there is no way to fix up bad state in

 * a reasonable manner.

 *

 * The fact that Intel documents the hardware task-switching to

 * be slow is a fairly red herring - this code is not noticeably

 * faster. However, there _is_ some room for improvement here,

 * so the performance issues may eventually be a valid point.

 * More important, however, is the fact that this allows us much

 * more flexibility.

 *

 * The return value (in %ax) will be the "prev" task after

 * the task-switch, and shows up in ret_from_fork in entry.S,

 * for example.

 never put a printk in __switch_to... printk() calls wake_up*() indirectly */

	/*

	 * Save away %gs. No need to save %fs, as it was saved on the

	 * stack on entry.  No need to save %es and %ds, as those are

	 * always kernel segments while inside the kernel.  Doing this

	 * before setting the new TLS descriptors avoids the situation

	 * where we temporarily have non-reloadable segments in %fs

	 * and %gs.  This could be an issue if the NMI handler ever

	 * used %fs or %gs (it does not today), or if the kernel is

	 * running inside of a hypervisor layer.

	/*

	 * Load the per-thread Thread-Local Storage descriptor.

	/*

	 * Leave lazy mode, flushing any hypercalls made here.

	 * This must be done before restoring TLS segments so

	 * the GDT and LDT are properly updated.

	/*

	 * Reload esp0 and cpu_current_top_of_stack.  This changes

	 * current_thread_info().  Refresh the SYSENTER configuration in

	 * case prev or next is vm86.

	/*

	 * Restore %gs if needed (which is common)

 Load the Intel cache allocation PQR MSR. */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * The pt_regs struct does not store

	 * ds, es, fs, gs in 64 bit mode.

 CONFIG_X86_64 */

	/*

	 * If we're in an NMI that interrupted task_pt_regs setup, then

	 * we can't sample user regs at all.  This check isn't really

	 * sufficient, though, as we could be in an NMI inside an interrupt

	 * that happened during task_pt_regs setup.

	/*

	 * These registers are always saved on 64-bit syscall entry.

	 * On 32-bit entry points, they are saved too except r8..r11.

	/*

	 * Store user space frame-pointer value on sample

	 * to facilitate stack unwinding for cases when

	 * user space executable code has such support

	 * enabled at compile time:

	/*

	 * For this to be at all useful, we need a reasonable guess for

	 * the ABI.  Be careful: we're in NMI context, and we're

	 * considering current to be the current task, so we should

	 * be careful not to look at any other percpu variables that might

	 * change during context switches.

 CONFIG_X86_32 */

 SPDX-License-Identifier: GPL-2.0

/*

 *	Copyright (C) 1992, 1998 Linus Torvalds, Ingo Molnar

 *

 * This file contains the lowest level x86-specific interrupt

 * entry, irq-stacks and irq statistics code. All the remaining

 * irq logic is done by the generic kernel/irq/ code and

 * by the x86-specific irq controller code. (e.g. i8259.c and

 * io_apic.c.)

 Debugging check for stack overflow: is there less than 1KB free? */

	/*

	 * this is where we switch to the IRQ stack. However, if we are

	 * already using the IRQ stack (because we interrupted a hardirq

	 * handler) we can't do that and just have to keep using the

	 * current stack (which is the irq stack already after all)

 Save the next esp at the bottom of the stack */

/*

 * Allocate per-cpu stacks for hardirq and softirq processing

 build the stack frame on the softirq stack */

 Push the previous esp onto the stack */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * tboot.c: main implementation of helper functions used by kernel for

 *          runtime support of Intel(R) Trusted Execution Technology

 *

 * Copyright (c) 2006-2009, Intel Corporation

 Global pointer to shared data; NULL means no measured launch. */

 timeout for APs (in secs) to enter wait-for-SIPI state during shutdown */

 noinline to prevent gcc from warning about dereferencing constant fixaddr */

 Look for valid page-aligned address for shared page. */

	/*

	 * also verify that it is mapped as we expect it before calling

	 * set_fixmap(), to reduce chance of garbage value causing crash

 Map and check for tboot UUID. */

	/*

	 * PTI poisons low addresses in the kernel page tables in the

	 * name of making them unusable for userspace.  To execute

	 * code at such a low address, the poison must be cleared.

	 *

	 * Note: 'pgd' actually gets set in p4d_alloc() _or_

	 * pud_alloc() depending on 4/5-level paging.

 Reuse the original kernel mapping */

 Create identity map for tboot shutdown code. */

 no CONFIG_ACPI_SLEEP */

 S3 shutdown requested, but S3 not supported by the kernel... */

	/*

	 * if we're being called before the 1:1 mapping is set up then just

	 * return and let the normal shutdown happen; this should only be

	 * due to very early panic()

 if this is S3 then set regions to MAC */

 should not reach here */

	/*

	 * We need phys addr of waking vector, but can't use virt_to_phys() on

	 * &acpi_gbl_FACS because it is ioremap'ed, so calc from FACS phys

	 * addr.

 S0,1,2: */ -1, -1, -1,

 S3: */ TB_SHUTDOWN_S3,

 S4: */ TB_SHUTDOWN_S4,

 S5: */ TB_SHUTDOWN_S5 };

 we always use the 32b wakeup vector */

 CONFIG_DEBUG_FS */

/*

 * TXT configuration registers (offsets from TXT_{PUB, PRIV}_CONFIG_REGS_BASE)

 # pages for each config regs space - used by fixmap */

 offsets from pub/priv config space */

 currently 6 */

	/*

	 * ACPI tables may not be DMA protected by tboot, so use DMAR copy

	 * SINIT saved in SinitMleData in TXT heap (which is DMA protected)

 map config space in order to get heap addr */

 now map TXT heap */

 walk heap to SinitMleData */

 skip BiosData */

 skip OsMleData */

 skip OsSinitData */

 now points to SinitMleDataSize; set to SinitMleData */

 get addr of DMAR table */

 don't unmap heap because dmar.c needs access to this */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * User-space Probes (UProbes) for x86

 *

 * Copyright (C) IBM Corporation, 2008-2011

 * Authors:

 *	Srikar Dronamraju

 *	Jim Keniston

 Post-execution fixups. */

 Adjust IP back to vicinity of actual insn */

 Adjust the return address of a call insn */

 Instruction will modify TF, don't change it */

 Adaptations for mhiramat x86 decoder v14. */

/*

 * Good-instruction tables for 32-bit apps.  This is non-const and volatile

 * to keep gcc from statically optimizing it out, as variable_test_bit makes

 * some versions of gcc to think only *(unsigned long*) is used.

 *

 * Opcodes we'll probably never support:

 * 6c-6f - ins,outs. SEGVs if used in userspace

 * e4-e7 - in,out imm. SEGVs if used in userspace

 * ec-ef - in,out acc. SEGVs if used in userspace

 * cc - int3. SIGTRAP if used in userspace

 * ce - into. Not used in userspace - no kernel support to make it useful. SEGVs

 *	(why we support bound (62) then? it's similar, and similarly unused...)

 * f1 - int1. SIGTRAP if used in userspace

 * f4 - hlt. SEGVs if used in userspace

 * fa - cli. SEGVs if used in userspace

 * fb - sti. SEGVs if used in userspace

 *

 * Opcodes which need some work to be supported:

 * 07,17,1f - pop es/ss/ds

 *	Normally not used in userspace, but would execute if used.

 *	Can cause GP or stack exception if tries to load wrong segment descriptor.

 *	We hesitate to run them under single step since kernel's handling

 *	of userspace single-stepping (TF flag) is fragile.

 *	We can easily refuse to support push es/cs/ss/ds (06/0e/16/1e)

 *	on the same grounds that they are never used.

 * cd - int N.

 *	Used by userspace for "int 80" syscall entry. (Other "int N"

 *	cause GP -> SEGV since their IDT gates don't allow calls from CPL 3).

 *	Not supported since kernel's handling of userspace single-stepping

 *	(TF flag) is fragile.

 * cf - iret. Normally not used in userspace. Doesn't SEGV unless arguments are bad

      0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f         */

      ----------------------------------------------         */

 00 */

 10 */

 20 */

 30 */

 40 */

 50 */

 60 */

 70 */

 80 */

 90 */

 a0 */

 b0 */

 c0 */

 d0 */

 e0 */

 f0 */

      ----------------------------------------------         */

      0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f         */

/* Good-instruction tables for 64-bit apps.

 *

 * Genuinely invalid opcodes:

 * 06,07 - formerly push/pop es

 * 0e - formerly push cs

 * 16,17 - formerly push/pop ss

 * 1e,1f - formerly push/pop ds

 * 27,2f,37,3f - formerly daa/das/aaa/aas

 * 60,61 - formerly pusha/popa

 * 62 - formerly bound. EVEX prefix for AVX512 (not yet supported)

 * 82 - formerly redundant encoding of Group1

 * 9a - formerly call seg:ofs

 * ce - formerly into

 * d4,d5 - formerly aam/aad

 * d6 - formerly undocumented salc

 * ea - formerly jmp seg:ofs

 *

 * Opcodes we'll probably never support:

 * 6c-6f - ins,outs. SEGVs if used in userspace

 * e4-e7 - in,out imm. SEGVs if used in userspace

 * ec-ef - in,out acc. SEGVs if used in userspace

 * cc - int3. SIGTRAP if used in userspace

 * f1 - int1. SIGTRAP if used in userspace

 * f4 - hlt. SEGVs if used in userspace

 * fa - cli. SEGVs if used in userspace

 * fb - sti. SEGVs if used in userspace

 *

 * Opcodes which need some work to be supported:

 * cd - int N.

 *	Used by userspace for "int 80" syscall entry. (Other "int N"

 *	cause GP -> SEGV since their IDT gates don't allow calls from CPL 3).

 *	Not supported since kernel's handling of userspace single-stepping

 *	(TF flag) is fragile.

 * cf - iret. Normally not used in userspace. Doesn't SEGV unless arguments are bad

      0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f         */

      ----------------------------------------------         */

 00 */

 10 */

 20 */

 30 */

 40 */

 50 */

 60 */

 70 */

 80 */

 90 */

 a0 */

 b0 */

 c0 */

 d0 */

 e0 */

 f0 */

      ----------------------------------------------         */

      0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f         */

/* Using this for both 64-bit and 32-bit apps.

 * Opcodes we don't support:

 * 0f 00 - SLDT/STR/LLDT/LTR/VERR/VERW/-/- group. System insns

 * 0f 01 - SGDT/SIDT/LGDT/LIDT/SMSW/-/LMSW/INVLPG group.

 *	Also encodes tons of other system insns if mod=11.

 *	Some are in fact non-system: xend, xtest, rdtscp, maybe more

 * 0f 05 - syscall

 * 0f 06 - clts (CPL0 insn)

 * 0f 07 - sysret

 * 0f 08 - invd (CPL0 insn)

 * 0f 09 - wbinvd (CPL0 insn)

 * 0f 0b - ud2

 * 0f 30 - wrmsr (CPL0 insn) (then why rdmsr is allowed, it's also CPL0 insn?)

 * 0f 34 - sysenter

 * 0f 35 - sysexit

 * 0f 37 - getsec

 * 0f 78 - vmread (Intel VMX. CPL0 insn)

 * 0f 79 - vmwrite (Intel VMX. CPL0 insn)

 *	Note: with prefixes, these two opcodes are

 *	extrq/insertq/AVX512 convert vector ops.

 * 0f ae - group15: [f]xsave,[f]xrstor,[v]{ld,st}mxcsr,clflush[opt],

 *	{rd,wr}{fs,gs}base,{s,l,m}fence.

 *	Why? They are all user-executable.

      0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f         */

      ----------------------------------------------         */

 00 */

 10 */

 20 */

 30 */

 40 */

 50 */

 60 */

 70 */

 80 */

 90 */

 a0 */

 b0 */

 c0 */

 d0 */

 e0 */

 f0 */

      ----------------------------------------------         */

      0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f         */

/*

 * opcodes we may need to refine support for:

 *

 *  0f - 2-byte instructions: For many of these instructions, the validity

 *  depends on the prefix and/or the reg field.  On such instructions, we

 *  just consider the opcode combination valid if it corresponds to any

 *  valid instruction.

 *

 *  8f - Group 1 - only reg = 0 is OK

 *  c6-c7 - Group 11 - only reg = 0 is OK

 *  d9-df - fpu insns with some illegal encodings

 *  f2, f3 - repnz, repz prefixes.  These are also the first byte for

 *  certain floating-point instructions, such as addsd.

 *

 *  fe - Group 4 - only reg = 0 or 1 is OK

 *  ff - Group 5 - only reg = 0-6 is OK

 *

 * others -- Do we need to support these?

 *

 *  0f - (floating-point?) prefetch instructions

 *  07, 17, 1f - pop es, pop ss, pop ds

 *  26, 2e, 36, 3e - es:, cs:, ss:, ds: segment prefixes --

 *	but 64 and 65 (fs: and gs:) seem to be used, so we support them

 *  67 - addr16 prefix

 *  ce - into

 *  f0 - lock prefix

/*

 * TODO:

 * - Where necessary, examine the modrm byte and allow only valid instructions

 * in the different Groups and fpu instructions.

 We should not singlestep on the exception masking instructions */

/*

 * If arch_uprobe->insn doesn't use rip-relative addressing, return

 * immediately.  Otherwise, rewrite the instruction so that it accesses

 * its memory operand indirectly through a scratch register.  Set

 * defparam->fixups accordingly. (The contents of the scratch register

 * will be saved before we single-step the modified instruction,

 * and restored afterward).

 *

 * We do this because a rip-relative instruction can access only a

 * relatively small area (+/- 2 GB from the instruction), and the XOL

 * area typically lies beyond that area.  At least for instructions

 * that store to memory, we can't execute the original instruction

 * and "fix things up" later, because the misdirected store could be

 * disastrous.

 *

 * Some useful facts about rip-relative instructions:

 *

 *  - There's always a modrm byte with bit layout "00 reg 101".

 *  - There's never a SIB byte.

 *  - The displacement is always 4 bytes.

 *  - REX.B=1 bit in REX prefix, which normally extends r/m field,

 *    has no effect on rip-relative mode. It doesn't make modrm byte

 *    with r/m=101 refer to register 1101 = R13.

	/*

	 * insn_rip_relative() would have decoded rex_prefix, vex_prefix, modrm.

	 * Clear REX.b bit (extension of MODRM.rm field):

	 * we want to encode low numbered reg, not r8+.

 REX byte has 0100wrxb layout, clearing REX.b bit */

	/*

	 * Similar treatment for VEX3/EVEX prefix.

	 * TODO: add XOP treatment when insn decoder supports them

		/*

		 * vex2:     c5    rvvvvLpp   (has no b bit)

		 * vex3/xop: c4/8f rxbmmmmm wvvvvLpp

		 * evex:     62    rxbR00mm wvvvv1pp zllBVaaa

		 * Setting VEX3.b (setting because it has inverted meaning).

		 * Setting EVEX.x since (in non-SIB encoding) EVEX.x

		 * is the 4th bit of MODRM.rm, and needs the same treatment.

		 * For VEX3-encoded insns, VEX3.x value has no effect in

		 * non-SIB encoding, the change is superfluous but harmless.

	/*

	 * Convert from rip-relative addressing to register-relative addressing

	 * via a scratch register.

	 *

	 * This is tricky since there are insns with modrm byte

	 * which also use registers not encoded in modrm byte:

	 * [i]div/[i]mul: implicitly use dx:ax

	 * shift ops: implicitly use cx

	 * cmpxchg: implicitly uses ax

	 * cmpxchg8/16b: implicitly uses dx:ax and bx:cx

	 *   Encoding: 0f c7/1 modrm

	 *   The code below thinks that reg=1 (cx), chooses si as scratch.

	 * mulx: implicitly uses dx: mulx r/m,r1,r2 does r1:r2 = dx * r/m.

	 *   First appeared in Haswell (BMI2 insn). It is vex-encoded.

	 *   Example where none of bx,cx,dx can be used as scratch reg:

	 *   c4 e2 63 f6 0d disp32   mulx disp32(%rip),%ebx,%ecx

	 * [v]pcmpistri: implicitly uses cx, xmm0

	 * [v]pcmpistrm: implicitly uses xmm0

	 * [v]pcmpestri: implicitly uses ax, dx, cx, xmm0

	 * [v]pcmpestrm: implicitly uses ax, dx, xmm0

	 *   Evil SSE4.2 string comparison ops from hell.

	 * maskmovq/[v]maskmovdqu: implicitly uses (ds:rdi) as destination.

	 *   Encoding: 0f f7 modrm, 66 0f f7 modrm, vex-encoded: c5 f9 f7 modrm.

	 *   Store op1, byte-masked by op2 msb's in each byte, to (ds:rdi).

	 *   AMD says it has no 3-operand form (vex.vvvv must be 1111)

	 *   and that it can have only register operands, not mem

	 *   (its modrm byte must have mode=11).

	 *   If these restrictions will ever be lifted,

	 *   we'll need code to prevent selection of di as scratch reg!

	 *

	 * Summary: I don't know any insns with modrm byte which

	 * use SI register implicitly. DI register is used only

	 * by one insn (maskmovq) and BX register is used

	 * only by one too (cmpxchg8b).

	 * BP is stack-segment based (may be a problem?).

	 * AX, DX, CX are off-limits (many implicit users).

	 * SP is unusable (it's stack pointer - think about "pop mem";

	 * also, rsp+disp32 needs sib encoding -> insn length change).

 Fetch modrm.reg */

 Fetch vex.vvvv */

	/*

	 * TODO: add XOP vvvv reading.

	 *

	 * vex.vvvv field is in bits 6-3, bits are inverted.

	 * But in 32-bit mode, high-order bit may be ignored.

	 * Therefore, let's consider only 3 low-order bits.

	/*

	 * Register numbering is ax,cx,dx,bx, sp,bp,si,di, r8..r15.

	 *

	 * Choose scratch reg. Order is important: must not select bx

	 * if we can use si (cmpxchg8b case!)

 TODO (paranoia): force maskmovq to not use di */

	/*

	 * Point cursor at the modrm byte.  The next 4 bytes are the

	 * displacement.  Beyond the displacement, for some instructions,

	 * is the immediate operand.

	/*

	 * Change modrm from "00 reg 101" to "10 reg reg2". Example:

	 * 89 05 disp32  mov %eax,disp32(%rip) becomes

	 * 89 86 disp32  mov %eax,disp32(%rsi)

/*

 * If we're emulating a rip-relative instruction, save the contents

 * of the scratch register and store the target address in that register.

 32-bit: */

/*

 * No RIP-relative addressing on 32-bit

 CONFIG_X86_64 */

	/*

	 * Check registers for mode as in_xxx_syscall() does not apply here.

/*

 * We have to fix things up as follows:

 *

 * Typically, the new ip is relative to the copied instruction.  We need

 * to make it relative to the original instruction (FIX_IP).  Exceptions

 * are return instructions and absolute or indirect jump or call instructions.

 *

 * If the single-stepped instruction was a call, the return address that

 * is atop the stack is the address following the copied instruction.  We

 * need to make it the address following the original instruction (FIX_CALL).

 *

 * If the original instruction was a rip-relative instruction such as

 * "movl %edx,0xnnnn(%rip)", we have instead executed an equivalent

 * instruction using a scratch register -- e.g., "movl %edx,0xnnnn(%rsi)".

 * We need to restore the contents of the scratch register

 * (FIX_RIP_reg).

 Pop incorrect return address */

 popf; tell the caller to not touch TF */

 not a conditional jmp */

		/*

		 * If it fails we execute this (mangled, see the comment in

		 * branch_clear_offset) insn out-of-line. In the likely case

		 * this should trigger the trap, and the probed application

		 * should die or restart the same insn after it handles the

		 * signal, arch_uprobe_post_xol() won't be even called.

		 *

		 * But there is corner case, see the comment in ->post_xol().

	/*

	 * We can only get here if branch_emulate_op() failed to push the ret

	 * address _and_ another thread expanded our stack before the (mangled)

	 * "call" insn was executed out-of-line. Just restore ->sp and restart.

	 * We could also restore ->ip and try to call branch_emulate_op() again.

	/*

	 * Turn this insn into "call 1f; 1:", this is what we will execute

	 * out-of-line if ->emulate() fails. We only need this to generate

	 * a trap, so that the probed task receives the correct signal with

	 * the properly filled siginfo.

	 *

	 * But see the comment in ->post_xol(), in the unlikely case it can

	 * succeed. So we need to ensure that the new ->ip can not fall into

	 * the non-canonical area and trigger #GP.

	 *

	 * We could turn it into (say) "pushf", but then we would need to

	 * divorce ->insn[] and ->ixol[]. We need to preserve the 1st byte

	 * of ->insn[] for set_orig_insn().

 Returns -ENOSYS if branch_xol_ops doesn't handle this insn */

 jmp 8 */

 jmp 32 */

 prefix* + nop; same as jmp with .offs = 0 */

 call relative */

		/*

		 * If it is a "near" conditional jmp, OPCODE2() - 0x10 matches

		 * OPCODE1() of the "short" jmp which checks the same condition.

	/*

	 * 16-bit overrides such as CALLW (66 e8 nn nn) are not supported.

	 * Intel and AMD behavior differ in 64-bit mode: Intel ignores 66 prefix.

	 * No one uses these insns, reject any branch insns with such prefix.

 Returns -ENOSYS if push_xol_ops doesn't handle this insn */

 only support rex_prefix 0x41 (x64 only) */

/**

 * arch_uprobe_analyze_insn - instruction analysis including validity and fixups.

 * @auprobe: the probepoint information.

 * @mm: the probed address space.

 * @addr: virtual address at which to install the probepoint

 * Return 0 on success or a -ve number on error.

	/*

	 * Figure out which fixups default_post_xol_op() will need to perform,

	 * and annotate defparam->fixups accordingly.

 popf */

 ret or lret -- ip is correct */

 jmp absolute -- ip is correct */

 call absolute - Fix return addr, not ip */

 call or lcall, indirect */

 jmp or ljmp, indirect */

/*

 * arch_uprobe_pre_xol - prepare to execute out of line.

 * @auprobe: the probepoint information.

 * @regs: reflects the saved user state of current task.

/*

 * If xol insn itself traps and generates a signal(Say,

 * SIGILL/SIGSEGV/etc), then detect the case where a singlestepped

 * instruction jumps back to its own address. It is assumed that anything

 * like do_page_fault/do_trap/etc sets thread.trap_nr != -1.

 *

 * arch_uprobe_pre_xol/arch_uprobe_post_xol save/restore thread.trap_nr,

 * arch_uprobe_xol_was_trapped() simply checks that ->trap_nr is not equal to

 * UPROBE_TRAP_NR == -1 set by arch_uprobe_pre_xol().

/*

 * Called after single-stepping. To avoid the SMP problems that can

 * occur when we temporarily put back the original opcode to

 * single-step, we single-stepped a copy of the instruction.

 *

 * This function prepares to resume execution after the single-step.

			/*

			 * Restore ->ip for restart or post mortem analysis.

			 * ->post_xol() must not return -ERESTART unless this

			 * is really possible.

	/*

	 * arch_uprobe_pre_xol() doesn't save the state of TIF_BLOCKSTEP

	 * so we can get an extra SIGTRAP if we do not clear TF. We need

	 * to examine the opcode to make it right.

 callback routine for handling exceptions. */

 We are only interested in userspace traps */

/*

 * This function gets called when XOL instruction either gets trapped or

 * the thread has a fatal signal. Reset the instruction pointer to its

 * probed address for the potential restart or for post mortem analysis.

 clear TF if it was set by us in arch_uprobe_pre_xol() */

 clear high bits for 32-bit apps */

 check whether address has been already hijacked */

 sp was just decremented by "call" insn */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Dynamic DMA mapping support for AMD Hammer.

 *

 * Use the integrated AGP GART in the Hammer northbridge as an IOMMU for PCI.

 * This allows to use PCI devices that only support 32bit addresses on systems

 * with more than 4GB.

 *

 * See Documentation/core-api/dma-api-howto.rst for the interface specification.

 *

 * Copyright 2002 Andi Kleen, SuSE Labs.

 GART remapping area (physical) */

 size of remapping area bytes */

 .. and in pages */

 Remapping table */

/*

 * If this is disabled the IOMMU will use an optimized flushing strategy

 * of only flushing when an mapping is reused. With it true the GART is

 * flushed for every mapping. Problem is that doing the lazy flush seems

 * to trigger bugs with some popular PCI cards, in particular 3ware (but

 * has been also also seen with Qlogic at least).

 Allocation bitmap for the remapping area: */

 Guarded by iommu_bitmap_lock: */

 GART can only remap to physical addresses < 1TB */

 backdoor interface to AGP driver */

 protected by iommu_bitmap_lock */

 global flush state. set for each gart wrap */

/*

 * Use global flush state to avoid races with multiple flushers.

 Debugging aid for drivers that don't free their IOMMU tables */

	/*

	 * Ran out of IOMMU space for this operation. This is very bad.

	 * Unfortunately the drivers cannot handle this operation properly.

	 * Return some non mapped prereserved space in the aperture and

	 * let the Northbridge deal with it. This will result in garbage

	 * in the IO operation. When the size exceeds the prereserved space

	 * memory corruption will occur or random memory will be DMAed

	 * out. Hopefully no network devices use single mappings that big.

/* Map a single continuous physical area into the IOMMU.

 * Caller needs to check if the iommu is needed and flush.

 Map a single area into the IOMMU */

/*

 * Free a DMA mapping.

	/*

	 * This driver will not always use a GART mapping, but might have

	 * created a direct mapping instead.  If that is the case there is

	 * nothing to unmap here.

/*

 * Wrapper for pci_unmap_single working with scatterlists.

 Fallback for dma_map_sg in case of overflow */

 Map multiple scatterlist entries continuous into the first. */

/*

 * DMA map all entries in a scatterlist.

 * Merge chunks that have page aligned sizes into a continuous mapping.

 shut up gcc */

 Handle the previous not yet processed entries */

			/*

			 * Can only merge when the last chunk ends on a

			 * page boundary and the new one doesn't have an

			 * offset.

 When it was forced or merged try again in a dumb way */

 allocate and map a coherent mapping */

 free a coherent mapping */

 Flush the GART-TLB to remove stale entries */

/*

 * If fix_up_north_bridges is set, the north bridges have to be fixed up on

 * resume in the same way as they are handled in gart_iommu_hole_init().

		/*

		 * Don't enable translations just yet.  That is the next

		 * step.  Restore the pre-suspend aperture settings.

/*

 * Private Northbridge GATT initialization in case we cannot use the

 * AGP driver for some reason.

 Should not happen anymore */

 don't shutdown it if there is AGP installed */

 Makefile puts PCI initialization via subsys_initcall first. */

 Add other AMD AGP bridge drivers here */

 need to map that range */

	/*

	 * Unmap the IOMMU part of the GART. The alias of the page is

	 * always mapped with cache enabled and there is no full cache

	 * coherency across the GART remapping. The unmapping avoids

	 * automatic prefetches from the CPU allocating cache lines in

	 * there. All CPU accesses are done via the direct mapping to

	 * the backing memory. The GART address is only used by PCI

	 * devices.

	/*

	 * Tricky. The GART table remaps the physical memory range,

	 * so the CPU wont notice potential aliases and if the memory

	 * is remapped to UC later on, we might surprise the PCI devices

	 * with a stray writeout of a cacheline. So play it sure and

	 * do an explicit, full-scale wbinvd() _after_ having marked all

	 * the pages as Not-Present:

	/*

	 * Now all caches are flushed and we can safely enable

	 * GART hardware.  Doing it early leaves the possibility

	 * of stale cache entries that can lead to GART PTE

	 * errors.

	/*

	 * Try to workaround a bug (thanks to BenH):

	 * Set unmapped entries to a scratch page instead of 0.

	 * Any prefetches that hit unmapped entries won't get an bus abort

	 * then. (P2P bridge may be prefetching on DMA reads).

 duplicated from pci-dma.c */

 SPDX-License-Identifier: GPL-2.0-or-later

/*  KVM paravirtual clock driver. A clocksource implementation

    Copyright (C) 2008 Glauber de Oliveira Costa, Red Hat Inc.

 Aligned to page sizes to match whats mapped via vsyscalls to userspace */

/*

 * The wallclock is the time of day when we booted. Since then, some time may

 * have elapsed since the hypervisor wrote the data. So we try to account for

 * that with system time

/*

 * If we don't do that, there is the possibility that the guest

 * will calibrate under heavy load - thus, getting a lower lpj -

 * and execute the delays themselves without load. This is wrong,

 * because no delay loop can finish beforehand.

 * Any heuristics is subject to fail, because ultimately, a large

 * poll of guests can be running and trouble each other. So we preset

 * lpj here

	/*

	 * hvclock is shared between the guest and the hypervisor, must

	 * be mapped decrypted.

	/*

	 * The per cpu area setup replicates CPU0 data to all cpu

	 * pointers. So carefully check. CPU0 has been set up in init

	 * already.

 Use the static page for the first CPUs, allocate otherwise */

	/*

	 * X86_FEATURE_NONSTOP_TSC is TSC runs at constant rate

	 * with P/T states and does not stop in deep C-states.

	 *

	 * Invariant TSC exposed by host means kvmclock is not necessary:

	 * can use TSC as clocksource.

	 *

 SPDX-License-Identifier: GPL-2.0

/*

 * Architecture specific OF callbacks.

/*

 * CE4100 ids. Will be moved to machine_device_initcall() once we have it.

 Did the boot loader setup the local APIC ? */

 CONFIG_X86_LOCAL_APIC */

 SPDX-License-Identifier: GPL-2.0-or-later

/*  Kernel module help for x86.

    Copyright (C) 2001 Rusty Russell.



 Mutex protects the module_load_offset. */

		/*

		 * Calculate the module_load_offset the first time this

		 * code is called. Once calculated it stays the same until

		 * reboot.

 This is where to make the change */

		/* This is the symbol it is referring to.  Note that all

 We add the value into the location given */

 Add the value, subtract its position */

X86_64*/

 This is where to make the change */

		/* This is the symbol it is referring to.  Note that all

 patch .altinstructions */

 make jump label nops */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kexec bzImage loader

 *

 * Copyright (C) 2014 Red Hat Inc.

 * Authors:

 *      Vivek Goyal <vgoyal@redhat.com>

 elfcorehdr=0x<64bit-value> */

/*

 * Defines lowest physical address for various segments. Not sure where

 * exactly these limits came from. Current bzimage64 loader in kexec-tools

 * uses these so I am retaining it. It can be changed over time as we gain

 * more insight.

/*

 * This is a place holder for all boot loader specific data structure which

 * gets allocated in one call but gets freed much later during cleanup

 * time. Right now there is only one field but it can grow as need be.

	/*

	 * Temporary buffer to hold bootparams buffer. This should be

	 * freed once the bootparam segment has been loaded.

 TODO: Pass entries more than E820_MAX_ENTRIES_ZEROPAGE in bootparams setup data */

 Add setup data */

 CONFIG_EFI */

 Get subarch from existing bootparams */

 Copying screen_info will do? */

 Fill in memsize later */

 Always fill in RSDP: it is either 0 or a valid value */

 Default APM info */

 Default drive info */

 64M*/

 Setup EFI state */

 Setup EDD info */

 kernel should be at least two sectors long */

	/*

	 * Can't handle 32bit EFI as it does not allow loading kernel

	 * above 4G. This should be handled by 32bit bzImage loader

 I've got a bzImage */

	/*

	 * In case of crash dump, we will append elfcorehdr=<addr> to

	 * command line. Make sure it does not overflow

 Allocate and load backup region */

	/*

	 * Load purgatory. For 64bit entry point, purgatory  code can be

	 * anywhere.

	/*

	 * Load Bootparams and cmdline and space for efi stuff.

	 *

	 * Allocate memory together for multiple data structures so

	 * that they all can go in single area/segment and we don't

	 * have to create separate segment for each. Keeps things

	 * little bit simple

 Copy setup header onto bootparams. Documentation/x86/boot.rst */

 Is there a limit on setup header size? */

 Load kernel */

 Load initrd high */

 bootloader info. Do we need a separate ID for kexec kernel loader? */

 Setup purgatory regs for entry */

 Bootstrap Processor */

 Allocate loader specific data */

	/*

	 * Store pointer to params so that it could be freed after loading

	 * params segment has been loaded and contents have been copied

	 * somewhere else.

 This cleanup function is called after various segments have been loaded */

 SPDX-License-Identifier: GPL-2.0

 workaround for a warning with -Wmissing-prototypes */

	/*

	 * Offset from the entry stack to task stack stored in TSS. Kernel entry

	 * happens on the per-cpu entry-stack, and the asm code switches to the

	 * task-stack pointer stored in x86_tss.sp1, which is a copy of

	 * task->thread.sp0 where entry code can find it.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Shared support code for AMD K8 northbridges and derivatives.

 * Copyright 2006 Andi Kleen, SUSE Labs.

 Protect the PCI config register pairs used for SMN and DF indirect access. */

/*

 * Data Fabric Indirect Access uses FICAA/FICAD.

 *

 * Fabric Indirect Configuration Access Address (FICAA): Constructed based

 * on the device's Instance Id and the PCI function and register offset of

 * the desired register.

 *

 * Fabric Indirect Configuration Access Data (FICAD): There are FICAD LO

 * and FICAD HI registers but so far we only need the LO register.

		/*

		 * There should be _exactly_ N roots for each DF/SMN

		 * interface.

		/*

		 * If there are more PCI root devices than data fabric/

		 * system management network interfaces, then the (N)

		 * PCI roots per DF/SMN interface are functionally the

		 * same (for DF/SMN access) and N-1 are redundant.  N-1

		 * PCI roots should be skipped per DF/SMN interface so

		 * the following DF/SMN interfaces get mapped to

		 * correct PCI roots.

	/*

	 * Check for L3 cache presence.

	/*

	 * Some CPU families support L3 Cache Index Disable. There are some

	 * limitations because of E382 and E388 on family 0x10.

 L3 cache partitioning is supported on family 0x15 */

/*

 * Ignores subdevice/subvendor but as far as I can figure out

 * they're useless anyways

 assume all cpus from fam10h have mmconfig */

 mmconfig is not enabled */

 if necessary, collect reset state of L3 partitioning and BAN mode */

 deactivate BAN mode if any subcaches are to be disabled */

 reset BAN mode if L3 partitioning returned to reset state */

	/*

	 * Avoid races between AGP and IOMMU. In theory it's not needed

	 * but I'm not sure if the hardware won't lose flush requests

	 * when another is pending. This whole thing is so expensive anyways

	 * that it doesn't matter to serialize more. -AK

 Make sure the hardware actually executed the flush*/

 Apply erratum 688 fix so machines without a BIOS fix work. */

 This has to go after the PCI subsystem */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * handle transition of Linux booting another kernel

 * Copyright (C) 2002-2005 Eric Biederman  <ebiederm@xmission.com>

/*

 * Used while adding mapping for ACPI tables.

 * Can be reused when other iomem regions need be mapped

 ACPI tables could be located in ACPI Non-volatile Storage region */

	/*

	 * segments's mem ranges could be outside 0 ~ max_pfn,

	 * for example when jump back to original kernel from kexeced kernel.

	 * or first kernel is booted with user mem map, and second kernel

	 * could be loaded out of that range.

	/*

	 * Prepare EFI systab and ACPI tables for kexec kernel since they are

	 * not covered by pfn_mapped.

 Calculate the offsets */

 Setup the identity mapped 64bit page table */

/*

 * Do not allocate memory (or fail in any way) in machine_kexec().

 * We are past the point of no return, committed to rebooting now.

 Interrupts aren't acceptable while we reboot */

		/*

		 * We need to put APICs in legacy mode so that we can

		 * get timer interrupts in second kernel. kexec/kdump

		 * paths already have calls to restore_boot_irq_mode()

		 * in one form or other. kexec jump path also need one.

	/*

	 * The segment registers are funny things, they have both a

	 * visible and an invisible part.  Whenever the visible part is

	 * set to a specific selector, the invisible part is loaded

	 * with from a table in memory.  At no other time is the

	 * descriptor table in memory accessed.

	 *

	 * I take advantage of this here by force loading the

	 * segments, before I zap the gdt with an invalid value.

	/*

	 * The gdt & idt are now invalid.

	 * If you want to load them you must set up your own idt & gdt.

 now call it */

 arch-dependent functionality related to kexec file-based syscall */

/*

 * Apply purgatory relocations.

 *

 * @pi:		Purgatory to be relocated.

 * @section:	Section relocations applying to.

 * @relsec:	Section containing RELAs.

 * @symtabsec:	Corresponding symtab.

 *

 * TODO: Some of the code belongs to generic code. Move that in kexec.c.

 String & section header string table */

		/*

		 * rel[i].r_offset contains byte offset from beginning

		 * of section to the storage unit affected.

		 *

		 * This is location to update. This is temporary buffer

		 * where section is currently loaded. This will finally be

		 * loaded to a different address later, pointed to by

		 * ->sh_addr. kexec takes care of moving it

		 *  (kexec_load_segment()).

 Final address of the location */

		/*

		 * rel[i].r_info contains information about symbol table index

		 * w.r.t which relocation must be made and type of relocation

		 * to apply. ELF64_R_SYM() and ELF64_R_TYPE() macros get

		 * these respectively.

 CONFIG_KEXEC_FILE */

	/*

	 * For physical range: [start, end]. We must skip the unassigned

	 * crashk resource with zero-valued "end" member.

 Don't touch the control code page used in crash_kexec().*/

 Control code page is located in the 2nd page. */

/*

 * During a traditional boot under SME, SME will encrypt the kernel,

 * so the SME kexec kernel also needs to be un-encrypted in order to

 * replicate a normal SME boot.

 *

 * During a traditional boot under SEV, the kernel has already been

 * loaded encrypted, so the SEV kexec kernel needs to be encrypted in

 * order to replicate a normal SEV boot.

	/*

	 * If host memory encryption is active we need to be sure that kexec

	 * pages are not encrypted because when we boot to the new kernel the

	 * pages won't be accessed encrypted (initially).

	/*

	 * If host memory encryption is active we need to reset the pages back

	 * to being an encrypted mapping before freeing them.

 SPDX-License-Identifier: GPL-2.0

/*

 * This function reserves all conventional PC system BIOS related

 * firmware memory areas (some of which are data, some of which

 * are code), that must not be used by the kernel as available

 * RAM.

 *

 * The BIOS places the EBDA/XBDA at the top of conventional

 * memory, and usually decreases the reported amount of

 * conventional memory (int 0x12) too.

 *

 * This means that as a first approximation on most systems we can

 * guess the reserved BIOS area by looking at the low BIOS RAM size

 * value and assume that everything above that value (up to 1MB) is

 * reserved.

 *

 * But life in firmware country is not that simple:

 *

 * - This code also contains a quirk for Dell systems that neglect

 *   to reserve the EBDA area in the 'RAM size' value ...

 *

 * - The same quirk also avoids a problem with the AMD768MPX

 *   chipset: reserve a page before VGA to prevent PCI prefetch

 *   into it (errata #56). (Usually the page is reserved anyways,

 *   unless you have no PS/2 mouse plugged in.)

 *

 * - Plus paravirt systems don't have a reliable value in the

 *   'BIOS RAM size' pointer we can rely on, so we must quirk

 *   them too.

 *

 * Due to those various problems this function is deliberately

 * very conservative and tries to err on the side of reserving

 * too much, to not risk reserving too little.

 *

 * Losing a small amount of memory in the bottom megabyte is

 * rarely a problem, as long as we have enough memory to install

 * the SMP bootup trampoline which *must* be in this area.

 *

 * Using memory that is in use by the BIOS or by some DMA device

 * the BIOS didn't shut down *is* a big problem to the kernel,

 * obviously.

 128K, less than this is insane */

 640K, absolute maximum */

	/*

	 * NOTE: In a paravirtual environment the BIOS reserved

	 * area is absent. We'll just have to assume that the

	 * paravirt case can handle memory setup correctly,

	 * without our help.

	/*

	 * BIOS RAM size is encoded in kilobytes, convert it

	 * to bytes to get a first guess at where the BIOS

	 * firmware area starts:

	/*

	 * If bios_start is less than 128K, assume it is bogus

	 * and bump it up to 640K.  Similarly, if bios_start is above 640K,

	 * don't trust it.

 Get the start address of the EBDA page: */

	/*

	 * If the EBDA start address is sane and is below the BIOS region,

	 * then also reserve everything from the EBDA start address up to

	 * the BIOS region.

 Reserve all memory between bios_start and the 1MB mark: */

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 Use this to add nops to a buffer, then text_poke the whole buffer. */

/*

 * Are we looking at a near JMP with a 1 or 4-byte displacement.

 next_rip of the replacement JMP */

 target rip of the replacement JMP */

 negative offset */

/*

 * optimize_nops_range() - Optimize a sequence of single byte NOPs (0x90)

 *

 * @instr: instruction byte stream

 * @instrlen: length of the above

 * @off: offset within @instr where the first NOP has been detected

 *

 * Return: number of NOPs found (and replaced).

/*

 * "noinline" to cause control flow change and thus invalidate I$ and

 * cause refetch after modification.

	/*

	 * Jump over the non-NOP insns and optimize single-byte NOPs into bigger

	 * ones.

		/*

		 * See if this and any potentially following NOPs can be

		 * optimized.

/*

 * Replace instructions with better alternatives for this CPU type. This runs

 * before SMP is initialized to avoid SMP problems with self modifying code.

 * This implies that asymmetric systems where APs have less capabilities than

 * the boot processor are not handled. Tough. Make sure you disable such

 * features by hand.

 *

 * Marked "noinline" to cause control flow change and thus insn cache

 * to refetch changed I$ lines.

	/*

	 * The scan order should be from start to end. A later scanned

	 * alternative code can overwrite previously scanned alternative code.

	 * Some kernel functions (e.g. memcpy, memset, etc) use this order to

	 * patch code.

	 *

	 * So be careful if you want to change the scan order to any other

	 * order.

 Mask away "NOT" flag bit for feature to test. */

		/*

		 * Patch if either:

		 * - feature is present

		 * - feature not present but ALTINSTR_FLAG_INV is set to mean,

		 *   patch if feature is *NOT* present.

		/*

		 * 0xe8 is a relative jump; fix the offset.

		 *

		 * Instruction length is checked before the opcode to avoid

		 * accessing uninitialized bytes for zero-length replacements.

/*

 * CALL/JMP *%\reg

 Reg = 2; CALL r/m */

 Reg = 4; JMP r/m */

 REX.B prefix */

 Mod = 3 */

 opcode */

/*

 * Rewrite the compiler generated retpoline thunk calls.

 *

 * For spectre_v2=off (!X86_FEATURE_RETPOLINE), rewrite them into immediate

 * indirect instructions, avoiding the extra indirection.

 *

 * For example, convert:

 *

 *   CALL __x86_indirect_thunk_\reg

 *

 * into:

 *

 *   CALL *%\reg

 *

 * It also tries to inline spectre_v2=retpoline,amd when size permits.

 If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */

	/*

	 * Convert:

	 *

	 *   Jcc.d32 __x86_indirect_thunk_\reg

	 *

	 * into:

	 *

	 *   Jncc.d8 1f

	 *   [ LFENCE ]

	 *   JMP *%\reg

	 *   [ NOP ]

	 * 1:

 Jcc.d32 second opcode byte is in the range: 0x80-0x8f */

 invert condition */

 Jcc.d8 */

 sizeof(Jcc.d8) == 2 */

 Continue as if: JMP.d32 __x86_indirect_thunk_\reg */

	/*

	 * For RETPOLINE_AMD: prepend the indirect CALL/JMP with an LFENCE.

 LFENCE */

/*

 * Generated by 'objtool --retpoline'.

 escape */

 !RETPOLINES || !CONFIG_STACK_VALIDATION */

 CONFIG_RETPOLINE && CONFIG_STACK_VALIDATION */

 turn DS segment override prefix into lock prefix */

 turn lock prefix into DS segment override prefix */

 what is this ??? */

 ptrs to lock prefixes */

 .text segment, needed to avoid patching init code ;) */

 protected by text_mutex */

 Don't bother remembering, we'll never have to undo it. */

 we'll run the (safe but slow) SMP code then ... */

 Why bother if there are no other CPUs? */

/*

 * Return 1 if the address range is reserved for SMP-alternatives.

 * Must hold text_mutex.

 CONFIG_SMP */

 prep the buffer with the original instructions */

 Pad the rest with nops */

 CONFIG_PARAVIRT */

/*

 * Self-test for the INT3 based CALL emulation code.

 *

 * This exercises int3_emulate_call() to make sure INT3 pt_regs are set up

 * properly and that there is a stack gap between the INT3 frame and the

 * previous context. Without this gap doing a virtual PUSH on the interrupted

 * stack would corrupt the INT3 IRET frame.

 *

 * See entry_{32,64}.S for more details.

/*

 * We define the int3_magic() function in assembly to control the calling

 * convention such that we can 'call' it from assembly.

 defined in asm */

 defined in asm below */

 last */

	/*

	 * Basically: int3_magic(&val); but really complicated :-)

	 *

	 * Stick the address of the INT3 instruction into int3_selftest_ip,

	 * then trigger the INT3, padded with NOPs to match a CALL instruction

	 * length.

	/*

	 * The patching is not fully atomic, so try to avoid local

	 * interruptions that might execute the to be patched code.

	 * Other CPUs are not running.

	/*

	 * Don't stop machine check exceptions while patching.

	 * MCEs only happen when something got corrupted and in this

	 * case we must do something about the corruption.

	 * Ignoring it is worse than an unlikely patching race.

	 * Also machine checks tend to be broadcast and if one CPU

	 * goes into machine check the others follow quickly, so we don't

	 * expect a machine check to cause undue problems during to code

	 * patching.

	/*

	 * Paravirt patching and alternative patching can be combined to

	 * replace a function call with a short direct code sequence (e.g.

	 * by setting a constant return value instead of doing that in an

	 * external function).

	 * In order to make this work the following sequence is required:

	 * 1. set (artificial) features depending on used paravirt

	 *    functions which can later influence alternative patching

	 * 2. apply paravirt patching (generally replacing an indirect

	 *    function call with a direct one)

	 * 3. apply alternative patching (e.g. replacing a direct function

	 *    call with a custom code sequence)

	 * Doing paravirt patching after alternative patching would clobber

	 * the optimization of the custom code with a function call again.

	/*

	 * First patch paravirt functions, such that we overwrite the indirect

	 * call with the direct call.

	/*

	 * Rewrite the retpolines, must be done before alternatives since

	 * those can rewrite the retpoline thunks.

	/*

	 * Then patch alternatives, such that those paravirt calls that are in

	 * alternatives can be overwritten by their immediate fragments.

 Patch to UP if other cpus not imminent. */

/**

 * text_poke_early - Update instructions on a live kernel at boot time

 * @addr: address to modify

 * @opcode: source of the copy

 * @len: length to copy

 *

 * When you use this code to patch more than one byte of an instruction

 * you need to make sure that other CPUs cannot execute this code in parallel.

 * Also no thread must be currently preempted in the middle of these

 * instructions. And on the local CPU you need to be protected against NMI or

 * MCE handlers seeing an inconsistent instruction while you patch.

		/*

		 * Modules text is marked initially as non-executable, so the

		 * code cannot be running and speculative code-fetches are

		 * prevented. Just change the code.

		/*

		 * Could also do a CLFLUSH here to speed up CPU recovery; but

		 * that causes hangs on some VIA CPUs.

/*

 * Using a temporary mm allows to set temporary mappings that are not accessible

 * by other CPUs. Such mappings are needed to perform sensitive memory writes

 * that override the kernel memory protections (e.g., W^X), without exposing the

 * temporary page-table mappings that are required for these write operations to

 * other CPUs. Using a temporary mm also allows to avoid TLB shootdowns when the

 * mapping is torn down.

 *

 * Context: The temporary mm needs to be used exclusively by a single core. To

 *          harden security IRQs must be disabled while the temporary mm is

 *          loaded, thereby preventing interrupt handler bugs from overriding

 *          the kernel memory protection.

	/*

	 * Make sure not to be in TLB lazy mode, as otherwise we'll end up

	 * with a stale address space WITHOUT being in lazy mode after

	 * restoring the previous mm.

	/*

	 * If breakpoints are enabled, disable them while the temporary mm is

	 * used. Userspace might set up watchpoints on addresses that are used

	 * in the temporary mm, which would lead to wrong signals being sent or

	 * crashes.

	 *

	 * Note that breakpoints are not disabled selectively, which also causes

	 * kernel breakpoints (e.g., perf's) to be disabled. This might be

	 * undesirable, but still seems reasonable as the code that runs in the

	 * temporary mm should be short.

	/*

	 * Restore the breakpoints if they were disabled before the temporary mm

	 * was loaded.

	/*

	 * While boot memory allocator is running we cannot use struct pages as

	 * they are not yet initialized. There is no way to recover.

	/*

	 * If something went wrong, crash and burn since recovery paths are not

	 * implemented.

	/*

	 * Map the page without the global bit, as TLB flushing is done with

	 * flush_tlb_mm_range(), which is intended for non-global PTEs.

	/*

	 * The lock is not really needed, but this allows to avoid open-coding.

	/*

	 * This must not fail; preallocated in poking_init().

	/*

	 * Loading the temporary mm behaves as a compiler barrier, which

	 * guarantees that the PTE will be set at the time memcpy() is done.

	/*

	 * Ensure that the PTE is only cleared after the instructions of memcpy

	 * were issued by using a compiler barrier.

	/*

	 * Loading the previous page-table hierarchy requires a serializing

	 * instruction that already allows the core to see the updated version.

	 * Xen-PV is assumed to serialize execution in a similar manner.

	/*

	 * Flushing the TLB might involve IPIs, which would require enabled

	 * IRQs, but not if the mm is not used, as it is in this point.

	/*

	 * If the text does not match what we just wrote then something is

	 * fundamentally screwy; there's nothing we can really do about that.

/**

 * text_poke - Update instructions on a live kernel

 * @addr: address to modify

 * @opcode: source of the copy

 * @len: length to copy

 *

 * Only atomic text poke/set should be allowed when not doing early patching.

 * It means the size must be writable atomically and the address must be aligned

 * in a way that permits an atomic write. It also makes sure we fit on a single

 * page.

 *

 * Note that the caller must ensure that if the modified code is part of a

 * module, the module would not be removed during poking. This can be achieved

 * by registering a module notifier, and ordering module removal and patching

 * trough a mutex.

/**

 * text_poke_kgdb - Update instructions on a live kernel by kgdb

 * @addr: address to modify

 * @opcode: source of the copy

 * @len: length to copy

 *

 * Only atomic text poke/set should be allowed when not doing early patching.

 * It means the size must be writable atomically and the address must be aligned

 * in a way that permits an atomic write. It also makes sure we fit on a single

 * page.

 *

 * Context: should only be used by kgdb, which ensures no other core is running,

 *	    despite the fact it does not hold the text_mutex.

 addr := _stext + rel_addr */

 rcu_dereference */

	/*

	 * Having observed our INT3 instruction, we now must observe

	 * bp_desc:

	 *

	 *	bp_desc = desc			INT3

	 *	WMB				RMB

	 *	write INT3			if (desc)

	/*

	 * Discount the INT3. See text_poke_bp_batch().

	/*

	 * Skip the binary search if there is a single member in the vector.

		/*

		 * Someone poked an explicit INT3, they'll want to handle it,

		 * do not consume.

/**

 * text_poke_bp_batch() -- update instructions on live kernel on SMP

 * @tp:			vector of instructions to patch

 * @nr_entries:		number of entries in the vector

 *

 * Modify multi-byte instruction by using int3 breakpoint on SMP.

 * We completely avoid stop_machine() here, and achieve the

 * synchronization using int3 breakpoint.

 *

 * The way it is done:

 *	- For each entry in the vector:

 *		- add a int3 trap to the address that will be patched

 *	- sync cores

 *	- For each entry in the vector:

 *		- update all but the first byte of the patched range

 *	- sync cores

 *	- For each entry in the vector:

 *		- replace the first byte (int3) by the first byte of

 *		  replacing opcode

 *	- sync cores

 rcu_assign_pointer */

	/*

	 * Corresponding read barrier in int3 notifier for making sure the

	 * nr_entries and handler are correctly ordered wrt. patching.

	/*

	 * First step: add a int3 trap to the address that will be patched.

	/*

	 * Second step: update all but the first byte of the patched range.

		/*

		 * Emit a perf event to record the text poke, primarily to

		 * support Intel PT decoding which must walk the executable code

		 * to reconstruct the trace. The flow up to here is:

		 *   - write INT3 byte

		 *   - IPI-SYNC

		 *   - write instruction tail

		 * At this point the actual control flow will be through the

		 * INT3 and handler and not hit the old or new instruction.

		 * Intel PT outputs FUP/TIP packets for the INT3, so the flow

		 * can still be decoded. Subsequently:

		 *   - emit RECORD_TEXT_POKE with the new instruction

		 *   - IPI-SYNC

		 *   - write first byte

		 *   - IPI-SYNC

		 * So before the text poke event timestamp, the decoder will see

		 * either the old instruction flow or FUP/TIP of INT3. After the

		 * text poke event timestamp, the decoder will see either the

		 * new instruction flow or FUP/TIP of INT3. Thus decoders can

		 * use the timestamp as the point at which to modify the

		 * executable code.

		 * The old instruction is recorded so that the event can be

		 * processed forwards or backwards.

		/*

		 * According to Intel, this core syncing is very likely

		 * not necessary and we'd be safe even without it. But

		 * better safe than sorry (plus there's not only Intel).

	/*

	 * Third step: replace the first byte (int3) by the first byte of

	 * replacing opcode.

	/*

	 * Remove and synchronize_rcu(), except we have a very primitive

	 * refcount based completion.

 RCU_INIT_POINTER */

 assume NOP */

 NOP2 -- emulate as JMP8+0 */

 NOP5 -- emulate as JMP32+0 */

 unknown instruction */

/*

 * We hard rely on the tp_vec being ordered; ensure this is so by flushing

 * early if needed.

 force */

/**

 * text_poke_bp() -- update instructions on live kernel on SMP

 * @addr:	address to patch

 * @opcode:	opcode of new instruction

 * @len:	length to copy

 * @emulate:	instruction to be emulated

 *

 * Update a single instruction with the vector in the stack, avoiding

 * dynamically allocated memory. This function should be used when it is

 * not possible to allocate memory.

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1994  Linus Torvalds

 *

 *  29 dec 2001 - Fixed oopses caused by unchecked access to the vm86

 *                stack - Manfred Spraul <manfred@colorfullife.com>

 *

 *  22 mar 2002 - Manfred detected the stackfaults, but didn't handle

 *                them correctly. Now the emulation will be in a

 *                consistent state after stackfaults - Kasper Dupont

 *                <kasperd@daimi.au.dk>

 *

 *  22 mar 2002 - Added missing clear_IF in set_vflags_* Kasper Dupont

 *                <kasperd@daimi.au.dk>

 *

 *  ?? ??? 2002 - Fixed premature returns from handle_vm86_fault

 *                caused by Kasper Dupont's changes - Stas Sergeev

 *

 *   4 apr 2002 - Fixed CHECK_IF_IN_TRAP broken by Stas' changes.

 *                Kasper Dupont <kasperd@daimi.au.dk>

 *

 *   9 apr 2002 - Changed syntax of macros in handle_vm86_fault.

 *                Kasper Dupont <kasperd@daimi.au.dk>

 *

 *   9 apr 2002 - Changed stack access macros to jump to a label

 *                instead of returning to userspace. This simplifies

 *                do_int, and is needed by handle_vm6_fault. Kasper

 *                Dupont <kasperd@daimi.au.dk>

 *

/*

 * Known problems:

 *

 * Interrupt handling is not guaranteed:

 * - a real x86 will disable all interrupts for one instruction

 *   after a "mov ss,xx" to make stack handling atomic even without

 *   the 'lss' instruction. We can't guarantee this in v86 mode,

 *   as the next instruction might result in a page fault or similar.

 * - a real x86 will have interrupts disabled for one instruction

 *   past the 'sti' that enables them. We don't bother with all the

 *   details yet.

 *

 * Let's hope these problems do not actually matter for anything.

/*

 * 8- and 16-bit register defines..

/*

 * virtual flags (16 and 32-bit versions)

	/*

	 * This gets called from entry.S with interrupts disabled, but

	 * from process context. Enable interrupts here, before trying

	 * to access user space.

	/*

	 * Don't write screen_bitmap in case some user had a value there

	 * and expected it to remain unchanged.

		/*

		 * NOTE: on old vm86 stuff this will return the error

		 *  from access_ok(), because the subfunction is

		 *  interpreted as (invalid) address to vm86_struct.

		 *  So the installation check works.

 we come here only for functions VM86_ENTER, VM86_ENTER_NO_BYPASS */

		/*

		 * vm86 cannot virtualize the address space, so vm86 users

		 * need to manage the low 1MB themselves using mmap.  Given

		 * that BIOS places important data in the first page, vm86

		 * is essentially useless if mmap_min_addr != 0.  DOSEMU,

		 * for example, won't even bother trying to use vm86 if it

		 * can't map a page at virtual address 0.

		 *

		 * To reduce the available kernel attack surface, simply

		 * disallow vm86(old) for users who cannot mmap at va 0.

		 *

		 * The implementation of security_mmap_addr will allow

		 * suitably privileged users to map va 0 even if

		 * vm.mmap_min_addr is set above 0, and we want this

		 * behavior for vm86 as well, as it ensures that legacy

		 * tools like vbetool will not fail just because of

		 * vm.mmap_min_addr.

 VM86_SCREEN_BITMAP had numerous bugs and appears to have no users. */

/*

 * The flags register is also special: we cannot trust that the user

 * has set it up safely, so this makes sure interrupt etc flags are

 * inherited from protected mode.

/*

 * Save old state

 make room for real-mode segments */

/*

 * It is correct to call set_IF(regs) from the set_vflags_*

 * functions. However someone forgot to call clear_IF(regs)

 * in the opposite case.

 * After the command sequence CLI PUSHF STI POPF you should

 * end up with interrupts disabled, but you ended up with

 * interrupts enabled.

 *  ( I was testing my own changes, but the only bug I

 *    could find was in a function I had not changed. )

 * [KD]

/* There are so many possible reasons for this function to return

 * VM86_INTx, so adding another doesn't bother me. We can expect

 * userspace programs to be able to handle it. (Getting a problem

 * in userspace is always better than an Oops anyway.) [KD]

 we let this handle by the calling routine */

 32-bit data */     data32 = 1; break;

 32-bit address */  break;

 CS */              break;

 DS */              break;

 ES */              break;

 SS */              break;

 GS */              break;

 FS */              break;

 repnz */       break;

 rep */             break;

 pushf */

 popf */

 int xx */

 iret */

 cli */

 sti */

	/*

	 * Damn. This is incorrect: the 'sti' instruction should actually

	 * enable interrupts after the /next/ instruction. Not good.

	 *

	 * Probably needs some horsing around with the TF flag. Aiee..

	/* FIXME: After a long discussion with Stas we finally

	 *        agreed, that this is wrong. Here we should

	 *        really send a SIGSEGV to the user program.

	 *        But how do we create the correct context? We

	 *        are inside a general protection fault handler

	 *        and has just returned from a page fault handler.

	 *        The correct context for the signal handler

	 *        should be a mixture of the two, but how do we

	 *        get the information? [KD]

 ---------------- vm86 special IRQ passing stuff ----------------- */

 0 = don't send a signal */ \

	/*

	 * IRQ will be re-enabled when user asks for the irq (whether

	 * polling or as a result of the signal)

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *  Copyright (C) 2000, 2001, 2002 Andi Kleen, SuSE Labs

 *  Copyright (C) 2011	Don Zickus Red Hat, Inc.

 *

 *  Pentium III FXSR, SSE support

 *	Gareth Hughes <gareth@valinux.com>, May 2000

/*

 * Handle hardware traps and faults.

/*

 * Prevent NMI reason port (0x61) being accessed simultaneously, can

 * only be used in NMI handler.

	/*

	 * NMIs are edge-triggered, which means if you have enough

	 * of them concurrently, you can lose some because only one

	 * can be latched at any given time.  Walk the whole list

	 * to handle those situations.

 return total number of NMI events handled */

	/*

	 * Indicate if there are multiple registrations on the

	 * internal NMI handler call chains (SERR and IO_CHECK).

	/*

	 * some handlers need to be executed first otherwise a fake

	 * event confuses some handlers (kdump uses this flag)

		/*

		 * the name passed in to describe the nmi handler

		 * is used as the lookup key

 check to see if anyone registered against these types of errors */

 Clear and disable the PCI SERR error line. */

 check to see if anyone registered against these types of errors */

		/*

		 * If we end up here, it means we have received an NMI while

		 * processing panic(). Simply return without delaying and

		 * re-enabling NMIs.

 Re-enable the IOCK line, wait for a few seconds */

	/*

	 * Use 'false' as back-to-back NMIs are dealt with one level up.

	 * Of course this makes having multiple 'unknown' handlers useless

	 * as only the first one is ever run (unless it can actually determine

	 * if it caused the NMI)

	/*

	 * CPU-specific NMI must be processed before non-CPU-specific

	 * NMI, otherwise we may lose it, because the CPU-specific

	 * NMI can not be detected/processed on other CPUs.

	/*

	 * Back-to-back NMIs are interesting because they can either

	 * be two NMI or more than two NMIs (any thing over two is dropped

	 * due to NMI being edge-triggered).  If this is the second half

	 * of the back-to-back NMI, assume we dropped things and process

	 * more handlers.  Otherwise reset the 'swallow' NMI behaviour

		/*

		 * There are cases when a NMI handler handles multiple

		 * events in the current NMI.  One of these events may

		 * be queued for in the next NMI.  Because the event is

		 * already handled, the next NMI will result in an unknown

		 * NMI.  Instead lets flag this for a potential NMI to

		 * swallow.

	/*

	 * Non-CPU-specific NMI: NMI sources can be processed on any CPU.

	 *

	 * Another CPU may be processing panic routines while holding

	 * nmi_reason_lock. Check if the CPU issued the IPI for crash dumping,

	 * and if so, call its callback directly.  If there is no CPU preparing

	 * crash dump, we simply loop here.

		/*

		 * Reassert NMI in case it became active

		 * meanwhile as it's edge-triggered:

	/*

	 * Only one NMI can be latched at a time.  To handle

	 * this we may process multiple nmi handlers at once to

	 * cover the case where an NMI is dropped.  The downside

	 * to this approach is we may process an NMI prematurely,

	 * while its real NMI is sitting latched.  This will cause

	 * an unknown NMI on the next run of the NMI processing.

	 *

	 * We tried to flag that condition above, by setting the

	 * swallow_nmi flag when we process more than one event.

	 * This condition is also only present on the second half

	 * of a back-to-back NMI, so we flag that condition too.

	 *

	 * If both are true, we assume we already processed this

	 * NMI previously and we swallow it.  Otherwise we reset

	 * the logic.

	 *

	 * There are scenarios where we may accidentally swallow

	 * a 'real' unknown NMI.  For example, while processing

	 * a perf NMI another perf NMI comes in along with a

	 * 'real' unknown NMI.  These two NMIs get combined into

	 * one (as described above).  When the next NMI gets

	 * processed, it will be flagged by perf as handled, but

	 * no one will know that there was a 'real' unknown NMI sent

	 * also.  As a result it gets swallowed.  Or if the first

	 * perf NMI returns two events handled then the second

	 * NMI will get eaten by the logic below, again losing a

	 * 'real' unknown NMI.  But this is the best we can do

	 * for now.

/*

 * NMIs can page fault or hit breakpoints which will cause it to lose

 * its NMI context with the CPU when the breakpoint or page fault does an IRET.

 *

 * As a result, NMIs can nest if NMIs get unmasked due an IRET during

 * NMI processing.  On x86_64, the asm glue protects us from nested NMIs

 * if the outer NMI came from kernel mode, but we can still nest if the

 * outer NMI came from user mode.

 *

 * To handle these nested NMIs, we have three states:

 *

 *  1) not running

 *  2) executing

 *  3) latched

 *

 * When no NMI is in progress, it is in the "not running" state.

 * When an NMI comes in, it goes into the "executing" state.

 * Normally, if another NMI is triggered, it does not interrupt

 * the running NMI and the HW will simply latch it so that when

 * the first NMI finishes, it will restart the second NMI.

 * (Note, the latch is binary, thus multiple NMIs triggering,

 *  when one is running, are ignored. Only one NMI is restarted.)

 *

 * If an NMI executes an iret, another NMI can preempt it. We do not

 * want to allow this new NMI to run, but we want to execute it when the

 * first one finishes.  We set the state to "latched", and the exit of

 * the first NMI will perform a dec_return, if the result is zero

 * (NOT_RUNNING), then it will simply exit the NMI handler. If not, the

 * dec_return would have set the state to NMI_EXECUTING (what we want it

 * to be when we are running). In this case, we simply jump back to

 * rerun the NMI handler again, and restart the 'latched' NMI.

 *

 * No trap (breakpoint or page fault) should be hit before nmi_restart,

 * thus there is no race between the first check of state for NOT_RUNNING

 * and setting it to NMI_EXECUTING. The HW will prevent nested NMIs

 * at this point.

 *

 * In case the NMI takes a page fault, we need to save off the CR2

 * because the NMI could have preempted another page fault and corrupt

 * the CR2 that is about to be read. As nested NMIs must be restarted

 * and they can not take breakpoints or page faults, the update of the

 * CR2 must be done before converting the nmi state back to NOT_RUNNING.

 * Otherwise, there would be a race of another nested NMI coming in

 * after setting state to NOT_RUNNING but before updating the nmi_cr2.

	/*

	 * Re-enable NMIs right here when running as an SEV-ES guest. This might

	 * cause nested NMIs, but those can be handled safely.

	/*

	 * Needs to happen before DR7 is accessed, because the hypervisor can

	 * intercept DR7 reads/writes, turning those into #VC exceptions.

 reset the back-to-back NMI logic */

 SPDX-License-Identifier: GPL-2.0

/*

 * Power off function, if any

/*

 * This is set if we need to go through the 'emergency' path.

 * When machine_emergency_restart() is called, we may be on

 * an inconsistent state and won't be able to do a clean cleanup

 This is set by the PCI code if either type 1 or type 2 PCI is detected */

/*

 * Reboot options and system auto-detection code provided by

 * Dell Inc. so their systems "just work". :-)

/*

 * Some machines require the "reboot=a" commandline options

/*

 * Some machines require the "reboot=b" or "reboot=k"  commandline options,

 * this quirk makes that automatic.

/*

 * Some machines don't handle the default ACPI reboot method and

 * require the EFI reboot method:

	/*

	 * Write zero to CMOS register number 0x0f, which the BIOS POST

	 * routine will recognize as telling it to do a proper reboot.  (Well

	 * that's what this book in front of me says -- it may only apply to

	 * the Phoenix BIOS though, it's not clear).  At the same time,

	 * disable NMIs by setting the top bit in the CMOS address register,

	 * as we're about to do peculiar things to the CPU.  I'm not sure if

	 * `outb_p' is needed instead of just `outb'.  Use it to be on the

	 * safe side.  (Yes, CMOS_WRITE does outb_p's. -  Paul G.)

	/*

	 * Switch back to the initial page table.

 Exiting long mode will fail if CR4.PCIDE is set. */

 Jump to the identity-mapped low memory code */

/*

 * Some Apple MacBook and MacBookPro's needs reboot=p to be able to reboot

/*

 * This is a single dmi_table handling all reboot quirks.

 Acer */

 Handle reboot issue on Acer Aspire one */

 Handle reboot issue on Acer TravelMate X514-51T */

 Apple */

 Handle problems with rebooting on Apple MacBook5 */

 Handle problems with rebooting on Apple MacBook6,1 */

 Handle problems with rebooting on Apple MacBookPro5 */

 Handle problems with rebooting on Apple Macmini3,1 */

 Handle problems with rebooting on the iMac9,1. */

 Handle problems with rebooting on the iMac10,1. */

 ASRock */

 Handle problems with rebooting on ASRock Q1900DC-ITX */

 ASUS */

 Handle problems with rebooting on ASUS P4S800 */

 Handle problems with rebooting on ASUS EeeBook X205TA */

 Handle problems with rebooting on ASUS EeeBook X205TAW */

 Certec */

 Handle problems with rebooting on Certec BPC600 */

 Dell */

 Handle problems with rebooting on Dell DXP061 */

 Handle problems with rebooting on Dell E520's */

 Handle problems with rebooting on the Latitude E5410. */

 Handle problems with rebooting on the Latitude E5420. */

 Handle problems with rebooting on the Latitude E6320. */

 Handle problems with rebooting on the Latitude E6420. */

 Handle problems with rebooting on Dell Optiplex 330 with 0KP561 */

 Handle problems with rebooting on Dell Optiplex 360 with 0T656F */

 Handle problems with rebooting on Dell Optiplex 745's SFF */

 Handle problems with rebooting on Dell Optiplex 745's DFF */

 Handle problems with rebooting on Dell Optiplex 745 with 0KW626 */

 Handle problems with rebooting on Dell OptiPlex 760 with 0G919G */

 Handle problems with rebooting on the OptiPlex 990. */

 Handle problems with rebooting on Dell 300's */

 Handle problems with rebooting on Dell 1300's */

 Handle problems with rebooting on Dell 2400's */

 Handle problems with rebooting on the Dell PowerEdge C6100. */

 Handle problems with rebooting on the Precision M6600. */

 Handle problems with rebooting on Dell T5400's */

 Handle problems with rebooting on Dell T7400's */

 Handle problems with rebooting on Dell XPS710 */

 Handle problems with rebooting on Dell Optiplex 7450 AIO */

 Hewlett-Packard */

 Handle problems with rebooting on HP laptops */

 PCIe Wifi card isn't detected after reboot otherwise */

 Sony */

 Handle problems with rebooting on Sony VGN-Z540N */

	/*

	 * Only do the DMI check if reboot_type hasn't been overridden

	 * on the command line

	/*

	 * The DMI quirks table takes precedence. If no quirks entry

	 * matches and the ACPI Hardware Reduced bit is set and EFI

	 * runtime services are enabled, force EFI reboot.

 Use NMIs as IPIs to tell all CPUs to disable virtualization */

 Just make sure we won't change CPUs while doing this */

	/*

	 * Disable VMX on all CPUs before rebooting, otherwise we risk hanging

	 * the machine, because the CPU blocks INIT when it's in VMX root.

	 *

	 * We can't take any locks and we may be on an inconsistent state, so

	 * use NMIs as IPIs to tell the other CPUs to exit VMX root and halt.

	 *

	 * Do the NMI shootdown even if VMX if off on _this_ CPU, as that

	 * doesn't prevent a different CPU from being in VMX root operation.

 Safely force _this_ CPU out of VMX root operation. */

 Halt and exit VMX root operation on the other CPUs. */

/*

 * To the best of our knowledge Windows compatible x86 hardware expects

 * the following on reboot:

 *

 * 1) If the FADT has the ACPI reboot register flag set, try it

 * 2) If still alive, write to the keyboard controller

 * 3) If still alive, write to the ACPI reboot register again

 * 4) If still alive, write to the keyboard controller again

 * 5) If still alive, call the EFI runtime service to reboot

 * 6) If no EFI runtime service, call the BIOS to do a reboot

 *

 * We default to following the same pattern. We also have

 * two other reboot methods: 'triple fault' and 'PCI', which

 * can be triggered via the reboot= kernel boot option or

 * via quirks.

 *

 * This means that this function can never return, it can misbehave

 * by not rebooting properly and hanging.

 Tell the BIOS if we want cold or warm reboot */

	/*

	 * If an EFI capsule has been registered with the firmware then

	 * override the reboot= parameter.

 Could also try the reset bit in the Hammer NB */

 For board specific fixups */

 Pulse reset low */

 We're probably dead after this, but... */

 Request hard reset */

 Actually do the reset */

 We're probably dead after this, but... */

 Stop the cpus and apics */

	/*

	 * Disabling IO APIC before local APIC is a workaround for

	 * erratum AVR31 in "Intel Atom Processor C2000 Product Family

	 * Specification Update". In this situation, interrupts that target

	 * a Logical Processor whose Local APIC is either in the process of

	 * being hardware disabled or software disabled are neither delivered

	 * nor discarded. When this erratum occurs, the processor may hang.

	 *

	 * Even without the erratum, it still makes sense to quiet IO APIC

	 * before disabling Local APIC.

	/*

	 * Stop all of the others. Also disable the local irq to

	 * not receive the per-cpu timer interrupt which may trigger

	 * scheduler's load balance.

 Stop other cpus and apics */

 A fallback in case there is no PM info available */

 This is the CPU performing the emergency shutdown work. */

	/*

	 * Don't do anything if this handler is invoked on crashing cpu.

	 * Otherwise, system will completely hang. Crashing cpu can get

	 * an NMI if system was initially booted with nmi_watchdog parameter.

 Assume hlt works */

/*

 * Halt all other CPUs, calling the specified function on each of them

 *

 * This function can be used to halt all other CPUs on crash

 * or emergency reboot time. The function passed as parameter

 * will be called inside a NMI handler on all CPUs.

 Make a note of crashing cpu. Will be used in NMI callback. */

 Would it be better to replace the trap vector here? */

 Return what? */

	/*

	 * Ensure the new callback function is set before sending

	 * out the NMI

 Kick CPUs looping in NMI context. */

 Wait at most a second for the other cpus to stop */

 Leave the nmi callback set */

/*

 * Check if the crash dumping IPI got issued and if so, call its callback

 * directly. This function is used when we have already been in NMI handler.

 * It doesn't return.

 Override the weak function in kernel/panic.c */

 If no CPU is preparing crash dump, we simply loop here. */

 !CONFIG_SMP */

 No other CPUs to shoot down */

 SPDX-License-Identifier: GPL-2.0

/*

 * This contains the io-permission bitmap code - written by obz, with changes

 * by Linus. 32/64 bits code unification by Miguel Botón.

 Can be NULL when current->thread.iopl_emul == 3 */

		/*

		 * Take a refcount on current's bitmap. It can be used by

		 * both tasks as long as none of them changes the bitmap.

 TSS update is handled on exit to user space */

 Invalidate TSS */

/*

 * This changes the io permissions bitmap in the current task.

	/*

	 * If it's the first ioperm() call in this thread's lifetime, set the

	 * IO bitmap up. ioperm() is much less timing critical than clone(),

	 * this is why we delay this operation until now:

 No point to allocate a bitmap just to clear permissions */

	/*

	 * If the bitmap is not shared, then nothing can take a refcount as

	 * current can obviously not fork at the same time. If it's shared

	 * duplicate it and drop the refcount on the original one.

	/*

	 * Store the bitmap pointer (might be the same if the task already

	 * head one). Must be done here so freeing the bitmap when all

	 * permissions are dropped has the pointer set up.

 Mark it active for context switching and exit to user mode */

	/*

	 * Update the tasks bitmap. The update of the TSS bitmap happens on

	 * exit to user mode. So this needs no protection.

	/*

	 * Search for a (possibly new) maximum. This is simple and stupid,

	 * to keep it obviously correct:

 All permissions dropped? */

	/*

	 * Update the sequence number to force a TSS update on return to

	 * user mode.

/*

 * The sys_iopl functionality depends on the level argument, which if

 * granted for the task is used to enable access to all 65536 I/O ports.

 *

 * This does not use the IOPL mechanism provided by the CPU as that would

 * also allow the user space task to use the CLI/STI instructions.

 *

 * Disabling interrupts in a user space task is dangerous as it might lock

 * up the machine and the semantics vs. syscalls and exceptions is

 * undefined.

 *

 * Setting IOPL to level 0-2 is disabling I/O permissions. Level 3

 * 3 enables them.

 *

 * IOPL is strictly per thread and inherited on fork.

 No point in going further if nothing changes */

 Trying to gain more privileges? */

 CONFIG_X86_IOPL_IOPERM */

 SPDX-License-Identifier: GPL-2.0

/*

 * Called by double_fault with CR0.TS and EFLAGS.NT cleared.  The CPU thinks

 * we're running the doublefault task.  Cannot return.

 Reset back to the normal kernel task. */

	/*

	 * Fill in pt_regs.  A downside of doing this in C is that the unwinder

	 * won't see it (no ENCODE_FRAME_POINTER), so a nested stack dump

	 * won't successfully unwind to the source of the double fault.

	 * The main dump from exc_double_fault() is fine, though, since it

	 * uses these regs directly.

	 *

	 * If anyone ever cares, this could be moved to asm.

 We won't go through the entry asm, so we can leave __csh as 0. */

	/*

	 * x86_32 does not save the original CR3 anywhere on a task switch.

	 * This means that, even if we wanted to return, we would need to find

	 * some way to reconstruct CR3.  We could make a credible guess based

	 * on cpu_tlbstate, but that would be racy and would not account for

	 * PTI.

                /*

                 * No sp0 or ss0 -- we never run CPL != 0 with this TSS

                 * active.  sp is filled in later.

 Set up doublefault TSS pointer in the GDT */

	/*

	 * The linker isn't smart enough to initialize percpu variables that

	 * point to other places in percpu space.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Common interrupt code for 32 and 64 bit

/*

 * 'what should we do if we get a hw irq event on an illegal vector'.

 * each architecture has to answer this themselves.

	/*

	 * Currently unexpected vectors happen only on SMP and APIC.

	 * We _must_ ack these because every local APIC has only N

	 * irq slots per priority level, and a 'hanging, unacked' IRQ

	 * holds up an irq slot - in excessive cases (when multiple

	 * unexpected vectors occur) that might lock up the APIC

	 * completely.

	 * But only ack when the APIC is enabled -AK

/*

 * /proc/interrupts printing for arch specific interrupts

/*

 * /proc/stat helpers

/*

 * common_interrupt() handles all normal device IRQ's (the special SMP

 * cross-CPU interrupts have their own entry points).

 entry code tells RCU that we're not quiescent.  Check it. */

 Function pointer for generic interrupt vector handling */

/*

 * Handler for X86_PLATFORM_IPI_VECTOR.

/*

 * Handler for POSTED_INTERRUPT_VECTOR.

/*

 * Handler for POSTED_INTERRUPT_WAKEUP_VECTOR.

/*

 * Handler for POSTED_INTERRUPT_NESTED_VECTOR.

 A cpu has been removed from cpu_online_mask.  Reset irq affinities. */

	/*

	 * We can remove mdelay() and then send spurious interrupts to

	 * new cpu targets for all the irqs that were handled previously by

	 * this cpu. While it works, I have seen spurious interrupt messages

	 * (nothing wrong but still...).

	 *

	 * So for now, retain mdelay(1) and check the IRR and then send those

	 * interrupts to new targets as this cpu is already offlined...

	/*

	 * We can walk the vector array of this cpu without holding

	 * vector_lock because the cpu is already marked !online, so

	 * nothing else will touch it.

 SPDX-License-Identifier: GPL-2.0

/*

 * This is the 'legacy' 8259A Programmable Interrupt Controller,

 * present in the majority of PC/AT boxes.

 * plus some generic x86 specific things if generic specifics makes

 * any sense at all.

/*

 * 8259A PIC functions to handle ISA devices:

/*

 * This contains the irq mask for both 8259A irq controllers,

/*

 * Not all IRQs can be routed through the IO-APIC, eg. on certain (older)

 * boards the timer interrupt is not really connected to any IO-APIC pin,

 * it's fed to the master 8259A's IR0 line only.

 *

 * Any '1' bit in this mask means the IRQ is routed through the IO-APIC.

 * this 'mixed mode' IRQ handling costs nothing because it's only used

 * at IRQ setup time.

/*

 * This function assumes to be called rarely. Switching between

 * 8259A registers is slow.

 * This has to be protected by the irq controller spinlock

 * before being called.

 ISR register */

 back to the IRR register */

 ISR register */

 back to the IRR register */

/*

 * Careful! The 8259A is a fragile beast, it pretty

 * much _has_ to be done exactly like this (mask it

 * first, _then_ send the EOI, and the order of EOI

 * to the two 8259s is important!

	/*

	 * Lightweight spurious IRQ detection. We do not want

	 * to overdo spurious IRQ handling - it's usually a sign

	 * of hardware problems, so we only do the checks we can

	 * do without slowing down good hardware unnecessarily.

	 *

	 * Note that IRQ7 and IRQ15 (the two spurious IRQs

	 * usually resulting from the 8259A-1|2 PICs) occur

	 * even if the IRQ is masked in the 8259A. Thus we

	 * can check spurious 8259A IRQs without doing the

	 * quite slow i8259A_irq_real() call for every IRQ.

	 * This does not cover 100% of spurious interrupts,

	 * but should be enough to warn the user that there

	 * is something bad going on ...

 DUMMY - (do we need this?) */

 'Specific EOI' to slave */

 'Specific EOI' to master-IRQ2 */

 DUMMY - (do we need this?) */

 'Specific EOI to master */

	/*

	 * this is the slow path - should happen rarely.

		/*

		 * oops, the IRQ _is_ in service according to the

		 * 8259A - not spurious, go handle it.

		/*

		 * At this point we can be sure the IRQ is spurious,

		 * lets ACK and report it. [once per IRQ]

		/*

		 * Theoretically we do not have to handle this IRQ,

		 * but in Linux this does not cause problems and is

		 * simpler for us.

/**

 * ELCR registers (0x4d0, 0x4d1) control edge/level of IRQ

 IRQ 0,1,2,8,13 are marked as reserved */

	/* Put the i8259A into a quiescent state that

	 * the kernel initialization code can get it

	 * out of.

 mask all of 8259A-1 */

 mask all of 8259A-2 */

 mask all of 8259A-1 */

 mask all of 8259A-2 */

 restore master IRQ mask */

 restore slave IRQ mask */

	/*

	 * Check to see if we have a PIC.

	 * Mask all except the cascade and read

	 * back the value we just wrote. If we don't

	 * have a PIC, we will read 0xff as opposed to the

	 * value we wrote.

 mask all of 8259A-2 */

 mask all of 8259A-1 */

	/*

	 * outb_pic - this has to work on a wide range of PC hardware.

 ICW1: select 8259A-1 init */

 ICW2: 8259A-1 IR0-7 mapped to ISA_IRQ_VECTOR(0) */

 8259A-1 (the master) has a slave on IR2 */

 master does Auto EOI */

 master expects normal EOI */

 ICW1: select 8259A-2 init */

 ICW2: 8259A-2 IR0-7 mapped to ISA_IRQ_VECTOR(8) */

 8259A-2 is a slave on master's IR2 */

 (slave's support for AEOI in flat mode is to be investigated) */

		/*

		 * In AEOI mode we just have to mask the interrupt

		 * when acking.

 wait for 8259A to initialize */

 restore master IRQ mask */

 restore slave IRQ mask */

/*

 * make i8259 a driver so that we can select pic functions at run time. the goal

 * is to make x86 binary compatible among pc compatible and non-pc compatible

 * platforms, such as x86 MID.

/*

 * trace_intel_irq_entry - record intel specific IRQ entry

/*

 * trace_intel_irq_exit - record intel specific IRQ exit

/*

 * register_intel_irq_tp - Register intel specific IRQ entry tracepoints

 CONFIG_X86_THERMAL_VECTOR */

 CONFIG_X86_MCE_THRESHOLD */

 CONFIG_SMP */

 CONFIG_X86_MCE_AMD */

 CONFIG_X86_THERMAL_VECTOR */

 CONFIG_X86_THERMAL_VECTOR */

 CONFIG_OSNOISE_TRACER && CONFIG_X86_LOCAL_APIC */

 SPDX-License-Identifier: GPL-2.0

/* Various workarounds for chipset bugs.

   This code runs very early and can't use the regular PCI subsystem

   The entries are keyed to PCI bridges which usually identify chipsets

   uniquely.

   This is only for whole classes of chipsets with specific problems which

   need early invasive action (e.g. before the timers are initialized).

   Most PCI device specific workarounds can be done later and should be

   in standard PCI quirks

   Mainboard specific bugs should be handled by DMI entries.

	/*

	 * we found a hypertransport bus

	 * make sure that we are broadcasting

	 * interrupts to all cpus on the ht bus

	 * if we're using extended apic ids

 CONFIG_X86_IO_APIC */

 CONFIG_ACPI */

	/*

	 * Only applies to Nvidia root ports (bus 0) and not to

	 * Nvidia graphics cards with PCI ports on secondary buses.

	/*

	 * All timer overrides on Nvidia are

	 * wrong unless HPET is enabled.

	 * Unfortunately that's not true on many Asus boards.

	 * We don't know yet how to detect this automatically, but

	 * at least allow a command line override.

 RED-PEN skip them on mptables too? */

 check for IRQ0 interrupt swap */

	/*

	 * SB600: revisions 0x11, 0x12, 0x13, 0x14, ...

	 * SB700: revisions 0x39, 0x3a, ...

	 * SB800: revisions 0x40, 0x41, ...

 check for IRQ0 interrupt swap */

	/*

	 * Revision <= 13 of all triggering devices id in this quirk

	 * have a problem draining interrupts when irq remapping is

	 * enabled, and should be flagged as broken. Additionally

	 * revision 0x22 of device id 0x3405 has this problem.

/*

 * Systems with Intel graphics controllers set aside memory exclusively

 * for gfx driver use.  This memory is not marked in the E820 as reserved

 * or as RAM, and so is subject to overlap from E820 manipulation later

 * in the boot process.  On some systems, MMIO space is allocated on top,

 * despite the efforts of the "RAM buffer" approach, which simply rounds

 * memory boundaries up to 64M to try to catch space that may decode

 * as RAM and so is not suitable for MMIO.

/*

 * On 830/845/85x the stolen memory base isn't available in any

 * register. We need to calculate it as TOM-TSEG_SIZE-stolen_size.

	/* Almost universally we can find the Graphics Base of Stolen Memory

	 * at register BSM (0x5c) in the igfx configuration space. On a few

	 * (desktop) machines this is also mirrored in the bridge device at

	 * different locations, or in the MCHBAR.

 local memory isn't part of the normal address space */

	/*

	 * 0x0  to 0x10: 32MB increments starting at 0MB

	 * 0x11 to 0x16: 4MB increments starting at 8MB

	 * 0x17 to 0x1d: 4MB increments start at 36MB

 0x0  to 0xef: 32MB increments starting at 0MB */

 0xf0 to 0xfe: 4MB increments starting at 4MB */

 Mark this space as reserved */

 Card may have been put into PCI_D3hot by grub quirk */

	/*

	 * HPET on the current version of the Baytrail platform has accuracy

	 * problems: it will halt in deep idle state - so we disable it.

	 *

	 * More details can be found in section 18.10.1.3 of the datasheet:

	 *

	 *    http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/atom-z8000-datasheet-vol-1.pdf

/**

 * check_dev_quirk - apply early quirks to a given PCI device

 * @num: bus number

 * @slot: slot number

 * @func: PCI function

 *

 * Check the vendor & device ID against the early quirks table.

 *

 * If the device is single function, let early_pci_scan_bus() know so we don't

 * poke at this device again.

 no class, treat as single function */

 Poor man's PCI discovery */

 Only probe function 0 on single fn devices */

 SPDX-License-Identifier: GPL-2.0

/*

 * Code for supporting irq vector tracepoints.

 *

 * Copyright (C) 2013 Seiji Aguchi <seiji.aguchi@hds.com>

 *

 SPDX-License-Identifier: GPL-2.0

/*

 *	Intel Multiprocessor Specification 1.1 and 1.4

 *	compliant MP-table parsing routines.

 *

 *	(c) 1995 Alan Cox, Building #3 <alan@lxorguk.ukuu.org.uk>

 *	(c) 1998, 1999, 2000, 2009 Ingo Molnar <mingo@redhat.com>

 *      (c) 2008 Alexey Starikovskiy <astarikovskiy@suse.de>

/*

 * Checksum an MP configuration block.

 CONFIG_X86_IO_APIC */

 CONFIG_X86_IO_APIC */

/*

 * Read/parse the MPC

 Initialize the lapic mapping */

 Now process the configuration blocks. */

 ACPI may have already provided this data */

 wrong mptable */

	/*

	 *  If true, we have an ISA/PCI system with no IRQ entries

	 *  in the MP table. To prevent the PCI interrupts from being set up

	 *  incorrectly, we try to use the ELCR. The sanity check to see if

	 *  there is good ELCR data is very simple - IRQ0, 1, 2 and 13 can

	 *  never be level sensitive, so we simply see if the ELCR agrees.

	 *  If it does, we assume it's valid.

 IRQ0 & IRQ13 not connected */

 IRQ2 is never connected */

			/*

			 *  If the ELCR indicates a level-sensitive interrupt, we

			 *  copy that information over to the MP table in the

			 *  irqflag field (level sensitive, active high polarity).

 IRQ0 to INTIN2 */

 8259A to INTIN0 */

	/*

	 * We set up most of the low 16 IO-APIC pins according to MPS rules.

	/*

	 * local APIC has default address

	/*

	 * 2 CPUs, numbered 0 & 1.

 Either an integrated APIC or a discrete 82489DX. */

	/*

	 * Read the physical hardware table.  Anything here will

	 * override the defaults.

	/*

	 * If there are no explicit MP IRQ entries, then we are

	 * broken.  We set up most of the low 16 IO-APIC pins to

	 * ISA defaults and hope it will work.

/*

 * Scan the memory blocks for an SMP configuration block.

	/*

	 * MPS doesn't support hyperthreading, aka only have

	 * thread 0 apic id in MPS table

	/*

	 * Now see if we need to read further.

			/*

			 * local APIC has default address

	/*

	 * Only use the first configuration found.

	/*

	 * FIXME: Linux assumes you have 640K of base ram..

	 * this continues the error...

	 *

	 * 1) Scan the bottom 1K for a signature

	 * 2) Scan the top 1K of base RAM

	 * 3) Scan the 64K of bios

	/*

	 * If it is an SMP machine we should know now, unless the

	 * configuration is in an EISA bus machine with an

	 * extended bios data area.

	 *

	 * there is a real-mode segmented pointer pointing to the

	 * 4K EBDA area at 0x40E, calculate and scan it here.

	 *

	 * NOTE! There are Linux loaders that will corrupt the EBDA

	 * area, and as such this kind of SMP config may be less

	 * trustworthy, simply because the SMP table may have been

	 * stomped on during early boot. These loaders are buggy and

	 * should be fixed.

	 *

	 * MP1.4 SPEC states to only scan first 1K of 4K EBDA.

 not legacy */

 already claimed */

 not found */

 legacy, do nothing */

		/*

		 * not found (-1), or duplicated (-2) are invalid entries,

		 * we need to use the slot later

 CONFIG_X86_IO_APIC */

 CONFIG_X86_IO_APIC */

 wrong mptable */

 update checksum */

 alloc_mptable or alloc_mptable=4k */

	/*

	 * Now see if we need to go further.

 check if we can change the position */

 check if we can modify that */

 steal 16 bytes from [0, 1k) */

	/*

	 * only replace the one with mp_INT and

	 *	 MP_IRQ_TRIGGER_LEVEL|MP_IRQ_POLARITY_LOW,

	 * already in mp_irqs , stored by ... and mp_config_acpi_gsi,

	 * may need pci=routeirq for all coverage

 SPDX-License-Identifier: GPL-2.0

/*

 * TSC frequency enumeration via MSR

 *

 * Copyright (C) 2013, 2018 Intel Corporation

 * Author: Bin Gao <bin.gao@intel.com>

 4 bits to select the frequency */

/*

 * The frequency numbers in the SDM are e.g. 83.3 MHz, which does not contain a

 * lot of accuracy which leads to clock drift. As far as we know Bay Trail SoCs

 * use a 25 MHz crystal and Cherry Trail uses a 19.2 MHz crystal, the crystal

 * is the source clk for a root PLL which outputs 1600 and 100 MHz. It is

 * unclear if the root PLL outputs are used directly by the CPU clock PLL or

 * if there is another PLL in between.

 * This does not matter though, we can model the chain of PLLs as a single PLL

 * with a quotient equal to the quotients of all PLLs in the chain multiplied.

 * So we can create a simplified model of the CPU clock setup using a reference

 * clock of 100 MHz plus a quotient which gets us as close to the frequency

 * from the SDM as possible.

 * For the 83.3 MHz example from above this would give us 100 MHz * 5 / 6 =

 * 83 and 1/3 MHz, which matches exactly what has been measured on actual hw.

/*

 * If MSR_PERF_STAT[31] is set, the maximum resolved bus ratio can be

 * read in MSR_PLATFORM_ID[12:8], otherwise in MSR_PERF_STAT[44:40].

 * Unfortunately some Intel Atom SoCs aren't quite compliant to this,

 * so we need manually differentiate SoC families. This is what the

 * field use_msr_plat does.

	/*

	 * Some CPU frequencies in the SDM do not map to known PLL freqs, in

	 * that case the muldiv array is empty and the freqs array is used.

/*

 * Penwell and Clovertrail use spread spectrum clock,

 * so the freq number is not exactly the same as reported

 * by MSR based on SDM.

/*

 * Bay Trail SDM MSR_FSB_FREQ frequencies simplified PLL model:

 *  000:   100 *  5 /  6  =  83.3333 MHz

 *  001:   100 *  1 /  1  = 100.0000 MHz

 *  010:   100 *  4 /  3  = 133.3333 MHz

 *  011:   100 *  7 /  6  = 116.6667 MHz

 *  100:   100 *  4 /  5  =  80.0000 MHz

/*

 * Cherry Trail SDM MSR_FSB_FREQ frequencies simplified PLL model:

 * 0000:   100 *  5 /  6  =  83.3333 MHz

 * 0001:   100 *  1 /  1  = 100.0000 MHz

 * 0010:   100 *  4 /  3  = 133.3333 MHz

 * 0011:   100 *  7 /  6  = 116.6667 MHz

 * 0100:   100 *  4 /  5  =  80.0000 MHz

 * 0101:   100 * 14 / 15  =  93.3333 MHz

 * 0110:   100 *  9 / 10  =  90.0000 MHz

 * 0111:   100 *  8 /  9  =  88.8889 MHz

 * 1000:   100 *  7 /  8  =  87.5000 MHz

/*

 * Merriefield SDM MSR_FSB_FREQ frequencies simplified PLL model:

 * 0001:   100 *  1 /  1  = 100.0000 MHz

 * 0010:   100 *  4 /  3  = 133.3333 MHz

/*

 * Moorefield SDM MSR_FSB_FREQ frequencies simplified PLL model:

 * 0000:   100 *  5 /  6  =  83.3333 MHz

 * 0001:   100 *  1 /  1  = 100.0000 MHz

 * 0010:   100 *  4 /  3  = 133.3333 MHz

 * 0011:   100 *  1 /  1  = 100.0000 MHz

/*

 * 24 MHz crystal? : 24 * 13 / 4 = 78 MHz

 * Frequency step for Lightning Mountain SoC is fixed to 78 MHz,

 * so all the frequency entries are 78000.

/*

 * MSR-based CPU/TSC frequency discovery for certain CPUs.

 *

 * Set global "lapic_timer_period" to bus_clock_cycles/jiffy

 * Return processor base frequency in KHz, or 0 on failure.

 Get FSB FREQ ID */

	/*

	 * Note this also catches cases where the index points to an unpopulated

	 * part of muldiv, in that case the else will set freq and res to 0.

		/*

		 * Multiplying by ratio before the division has better

		 * accuracy than just calculating freq * ratio.

	/*

	 * TSC frequency determined by MSR is always considered "known"

	 * because it is reported by HW.

	 * Another fact is that on MSR capable platforms, PIT/HPET is

	 * generally not available so calibration won't work at all.

	/*

	 * Unfortunately there is no way for hardware to tell whether the

	 * TSC is reliable.  We were told by silicon design team that TSC

	 * on Atom SoCs are always "reliable". TSC is also the only

	 * reliable clocksource on these SoCs (HPET is either not present

	 * or not functional) so mark TSC reliable which removes the

	 * requirement for a watchdog clocksource.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Low level x86 E820 memory map handling functions.

 *

 * The firmware and bootloader passes us the "E820 table", which is the primary

 * physical memory layout description available about x86 systems.

 *

 * The kernel takes the E820 memory layout and optionally modifies it with

 * quirks and other tweaks, and feeds that into the generic Linux memory

 * allocation code routines via a platform independent interface (memblock, etc.).

/*

 * We organize the E820 table into three main data structures:

 *

 * - 'e820_table_firmware': the original firmware version passed to us by the

 *   bootloader - not modified by the kernel. It is composed of two parts:

 *   the first 128 E820 memory entries in boot_params.e820_table and the remaining

 *   (if any) entries of the SETUP_E820_EXT nodes. We use this to:

 *

 *       - inform the user about the firmware's notion of memory layout

 *         via /sys/firmware/memmap

 *

 *       - the hibernation code uses it to generate a kernel-independent CRC32

 *         checksum of the physical memory layout of a system.

 *

 * - 'e820_table_kexec': a slightly modified (by the kernel) firmware version

 *   passed to us by the bootloader - the major difference between

 *   e820_table_firmware[] and this one is that, the latter marks the setup_data

 *   list created by the EFI boot stub as reserved, so that kexec can reuse the

 *   setup_data information in the second kernel. Besides, e820_table_kexec[]

 *   might also be modified by the kexec itself to fake a mptable.

 *   We use this to:

 *

 *       - kexec, which is a bootloader in disguise, uses the original E820

 *         layout to pass to the kexec-ed kernel. This way the original kernel

 *         can have a restricted E820 map while the kexec()-ed kexec-kernel

 *         can have access to full memory - etc.

 *

 * - 'e820_table': this is the main E820 table that is massaged by the

 *   low level x86 platform code, or modified by boot parameters, before

 *   passed on to higher level MM layers.

 *

 * Once the E820 map has been converted to the standard Linux memory layout

 * information its role stops - modifying it has no effect and does not get

 * re-propagated. So itsmain role is a temporary bootstrap storage of firmware

 * specific memory layout data during early bootup.

 For PCI or other memory-mapped resources */

/*

 * This function checks if any part of the range <start,end> is mapped

 * with type.

/*

 * This function checks if the entire <start,end> range is mapped with 'type'.

 *

 * Note: this function only works correctly once the E820 table is sorted and

 * not-overlapping (at least for the range specified), which is the case normally.

 Is the region (part) in overlap with the current region? */

		/*

		 * If the region is at the beginning of <start,end> we move

		 * 'start' to the end of the region since it's ok until there

		/*

		 * If 'start' is now at or beyond 'end', we're done, full

		 * coverage of the desired range exists:

/*

 * This function checks if the entire range <start,end> is mapped with type.

/*

 * This function returns the type associated with the range <start,end>.

/*

 * Add a memory region to the kernel E820 map.

 Fall through: */

 Fall through: */

/*

 * Sanitize an E820 map.

 *

 * Some E820 layouts include overlapping entries. The following

 * replaces the original E820 map with a new one, removing overlaps,

 * and resolving conflicting memory types in favor of highest

 * numbered type.

 *

 * The input parameter 'entries' points to an array of 'struct

 * e820_entry' which on entry has elements in the range [0, *nr_entries)

 * valid, and which has space for up to max_nr_entries entries.

 * On return, the resulting sanitized E820 map entries will be in

 * overwritten in the same location, starting at 'entries'.

 *

 * The integer pointed to by nr_entries must be valid on entry (the

 * current number of valid entries located at 'entries'). If the

 * sanitizing succeeds the *nr_entries will be updated with the new

 * number of valid entries (something no more than max_nr_entries).

 *

 * The return value from e820__update_table() is zero if it

 * successfully 'sanitized' the map entries passed in, and is -1

 * if it did nothing, which can happen if either of (1) it was

 * only passed one map entry, or (2) any of the input map entries

 * were invalid (start + size < start, meaning that the size was

 * so big the described memory range wrapped around through zero.)

 *

 *	Visually we're performing the following

 *	(1,2,3,4 = memory types)...

 *

 *	Sample memory map (w/overlaps):

 *	   ____22__________________

 *	   ______________________4_

 *	   ____1111________________

 *	   _44_____________________

 *	   11111111________________

 *	   ____________________33__

 *	   ___________44___________

 *	   __________33333_________

 *	   ______________22________

 *	   ___________________2222_

 *	   _________111111111______

 *	   _____________________11_

 *	   _________________4______

 *

 *	Sanitized equivalent (no overlap):

 *	   1_______________________

 *	   _44_____________________

 *	   ___1____________________

 *	   ____22__________________

 *	   ______11________________

 *	   _________1______________

 *	   __________3_____________

 *	   ___________44___________

 *	   _____________33_________

 *	   _______________2________

 *	   ________________1_______

 *	   _________________4______

 *	   ___________________2____

 *	   ____________________33__

 *	   ______________________4_

 Pointer to the original entry: */

 Address for this change point: */

	/*

	 * Inputs are pointers to two elements of change_point[].  If their

	 * addresses are not equal, their difference dominates.  If the addresses

	 * are equal, then consider one that represents the end of its region

	 * to be greater than one that does not.

	/*

	 * These types may indicate distinct platform ranges aligned to

	 * numa node, protection domain, performance domain, or other

	 * boundaries. Do not merge them.

 If there's only one memory region, don't bother: */

 Bail out if we find any unreasonable addresses in the map: */

 Create pointers for initial change-point information (for sorting): */

	/*

	 * Record all known change-points (starting and ending addresses),

	 * omitting empty memory regions:

 Sort change-point list by memory addresses (low -> high): */

 Create a new memory map, removing overlaps: */

 Number of entries in the overlap table */

 Index for creating new map entries */

 Start with undefined memory type */

 Start with 0 as last starting address */

 Loop through change-points, determining effect on the new map: */

 Keep track of all overlapping entries */

 Add map entry to overlap list (> 1 entry implies an overlap) */

 Remove entry from list (order independent, so swap with last): */

		/*

		 * If there are overlapping entries, decide which

		 * "type" to use (larger value takes precedence --

		 * 1=usable, 2,3,4,4+=unusable)

 Continue building up new map based on this information: */

 Move forward only if the new size was non-zero: */

 No more space left for new entries? */

 Copy the new entries into the original location: */

 Ignore the entry on 64-bit overflow: */

/*

 * Copy the BIOS E820 map into a safe place.

 *

 * Sanity-check it while we're at it..

 *

 * If we're lucky and live on a modern system, the setup code

 * will have given us a memory map that we can use to properly

 * set up memory.  If we aren't, we'll fake a memory map.

 Only one memory region (or negative)? Ignore it */

 Completely covered by new range? */

 New range is completely covered? */

 Partially covered: */

		/*

		 * Left range could be head or tail, so need to update

		 * its size first:

 Remove a range of memory from the E820 table: */

 Completely covered? */

 Is the new range completely covered? */

 Partially covered: */

		/*

		 * Left range could be head or tail, so need to update

		 * the size first:

/*

 * Search for a gap in the E820 memory space from 0 to MAX_GAP_END (4GB).

		/*

		 * Since "last" is at most 4GB, we know we'll

		 * fit in 32 bits if this condition is true:

/*

 * Search for the biggest gap in the low 32 bits of the E820

 * memory space. We pass this space to the PCI subsystem, so

 * that it can assign MMIO resources for hotplug or

 * unconfigured devices in.

 *

 * Hopefully the BIOS let enough space left.

	/*

	 * e820__reserve_resources_late() protects stolen RAM already:

/*

 * Called late during init, in free_initmem().

 *

 * Initial e820_table and e820_table_kexec are largish __initdata arrays.

 *

 * Copy them to a (usually much smaller) dynamically allocated area that is

 * sized precisely after the number of e820 entries.

 *

 * This is done after we've performed all the fixes and tweaks to the tables.

 * All functions which modify them are __init functions, which won't exist

 * after free_initmem().

/*

 * Because of the small fixed size of struct boot_params, only the first

 * 128 E820 memory entries are passed to the kernel via boot_params.e820_table,

 * the remaining (if any) entries are passed via the SETUP_E820_EXT node of

 * struct setup_data, which is parsed here.

/*

 * Find the ranges of physical addresses that do not correspond to

 * E820 RAM areas and register the corresponding pages as 'nosave' for

 * hibernation (32-bit) or software suspend and suspend to RAM (64-bit).

 *

 * This function requires the E820 map to be sorted and without any

 * overlapping entries.

/*

 * Register ACPI NVS memory regions, so that we can save/restore them during

 * hibernation and the subsequent resume:

/*

 * Allocate the requested number of bytes with the requested alignment

 * and return (the physical address) to the caller. Also register this

 * range in the 'kexec' E820 table as a reserved range.

 *

 * This allows kexec to fake a new mptable, as if it came from the real

 * system.

 CONFIG_X86_32 */

/*

 * Find the highest page frame number we have available

 The "mem=nopentium" boot option disables 4MB page tables on 32-bit kernels: */

 Don't remove all memory when getting "mem={invalid}" parameter: */

/*

 * Reserve all entries from the bootloader's extensible data nodes list,

 * because if present we are going to use it later on to fetch e820

 * entries from it:

		/*

		 * SETUP_EFI is supplied by kexec and does not need to be

		 * reserved.

/*

 * Called after parse_early_param(), after early parameters (such as mem=)

 * have been processed, in which case we already have an E820 table filled in

 * via the parameter callback function(s), but it's not sorted and printed yet:

 Fall-through: */

 Fall-through: */

 Fall-through: */

 Fall-through: */

 Fall-through: */

 Fall-through: */

 Fall-through: */

 Fall-through: */

 Fall-through: */

 Fall-through: */

 Fall-through: */

 Fall-through: */

 this is the legacy bios/dos rom-shadow + mmio region */

	/*

	 * Treat persistent memory and other special memory ranges like

	 * device memory, i.e. reserve it for exclusive use of a driver

/*

 * Mark E820 reserved areas as busy for the resource manager:

		/*

		 * Don't register the region that could be conflicted with

		 * PCI device BAR resources and insert them later in

		 * pcibios_resource_survey():

 Expose the bootloader-provided memory layout to the sysfs. */

/*

 * How much should we pad the end of RAM, depending on where it is?

 To 64kB in the first megabyte */

 To 1MB in the first 16MB */

 To 64MB for anything above that */

	/*

	 * Try to bump up RAM regions to reasonable boundaries, to

	 * avoid stolen RAM:

/*

 * Pass the firmware (bootloader) E820 map to the kernel and process it:

	/*

	 * Try to copy the BIOS-supplied E820-map.

	 *

	 * Otherwise fake a memory map; one section from 0k->640k,

	 * the next section from 1mb->appropriate_mem_k

 Compare results from other methods and take the one that gives more RAM: */

 We just appended a lot of ranges, sanitize the table: */

/*

 * Calls e820__memory_setup_default() in essence to pick up the firmware/bootloader

 * E820 map - with an optional platform quirk available for virtual platforms

 * to override this method of boot environment processing:

 This is a firmware interface ABI - make sure we don't break it: */

	/*

	 * The bootstrap memblock region count maximum is 128 entries

	 * (INIT_MEMBLOCK_REGIONS), but EFI might pass us more E820 entries

	 * than that - so allow memblock resizing.

	 *

	 * This is safe, because this call happens pretty late during x86 setup,

	 * so we know about reserved memory regions already. (This is important

	 * so that memblock resizing does no stomp over reserved areas.)

 Throw away partial pages: */

 SPDX-License-Identifier: GPL-2.0-only

 TSC clocks / usec, not used here */

/*

 * TSC can be unstable due to cpufreq or due to unsynced TSCs

  0 + 2*16 = 32 */

 32 + 4    = 36 */

 fits one cacheline */

/*

 * Accelerators for sched_clock()

 * convert from cycles(64bits) => nanoseconds (64bits)

 *  basic equation:

 *              ns = cycles / (freq / ns_per_sec)

 *              ns = cycles * (ns_per_sec / freq)

 *              ns = cycles * (10^9 / (cpu_khz * 10^3))

 *              ns = cycles * (10^6 / cpu_khz)

 *

 *      Then we use scaling math (suggested by george@mvista.com) to get:

 *              ns = cycles * (10^6 * SC / cpu_khz) / SC

 *              ns = cycles * cyc2ns_scale / SC

 *

 *      And since SC is a constant power of two, we can convert the div

 *  into a shift. The larger SC is, the more accurate the conversion, but

 *  cyc2ns_scale needs to be a 32-bit value so that 32-bit multiplication

 *  (64-bit result) can be used.

 *

 *  We can use khz divisor instead of mhz to keep a better precision.

 *  (mathieu.desnoyers@polymtl.ca)

 *

 *                      -johnstul@us.ibm.com "math is hard, lets go shopping!"

	/*

	 * Compute a new multiplier as per the above comment and ensure our

	 * time function is continuous; see the comment near struct

	 * cyc2ns_data.

	/*

	 * cyc2ns_shift is exported via arch_perf_update_userpage() where it is

	 * not expected to be greater than 31 due to the original published

	 * conversion algorithm shifting a 32-bit value (now specifies a 64-bit

	 * value) - refer perf_event_mmap_page documentation in perf_event.h.

/*

 * Initialize cyc2ns for boot cpu

/*

 * Secondary CPUs do not run through tsc_init(), so set up

 * all the scale factors for all CPUs, assuming the same

 * speed as the bootup CPU.

/*

 * Scheduler clock - returns current time in nanosec units.

 return the value in ns */

	/*

	 * Fall back to jiffies if there's no TSC available:

	 * ( But note that we still use it if the TSC is marked

	 *   unstable. We do this because unlike Time Of Day,

	 *   the scheduler clock tolerates small errors and it's

	 *   very important for it to be as fast as the platform

	 *   can achieve it. )

 No locking but a rare wrong value is not a big deal: */

/*

 * Generate a sched_clock if you already have a TSC value.

/* We need to define a real function for sched_clock, to override the

/*

 * disable flag for tsc. Takes effect by clearing the TSC cpu flag

 * in cpu/common.c

/*

 * Read TSC and the reference counters. Take care of any disturbances

/*

 * Calculate the TSC frequency from HPET reference

/*

 * Calculate the TSC frequency from PMTimer reference

/*

 * Try to calibrate the TSC against the Programmable

 * Interrupt Timer and return the frequency of the TSC

 * in kHz.

 *

 * Return ULONG_MAX on failure to calibrate.

		/*

		 * Relies on tsc_early_delay_calibrate() to have given us semi

		 * usable udelay(), wait for the same 50ms we would have with

		 * the PIT loop below.

 Set the Gate high, disable speaker */

	/*

	 * Setup CTC channel 2* for mode 0, (interrupt on terminal

	 * count mode), binary count. Set the latch register to 50ms

	 * (LSB then MSB) to begin countdown.

	/*

	 * Sanity checks:

	 *

	 * If we were not able to read the PIT more than loopmin

	 * times, then we have been hit by a massive SMI

	 *

	 * If the maximum is 10 times larger than the minimum,

	 * then we got hit by an SMI as well.

 Calculate the PIT value */

/*

 * This reads the current MSB of the PIT counter, and

 * checks if we are running on sufficiently fast and

 * non-virtualized hardware.

 *

 * Our expectations are:

 *

 *  - the PIT is running at roughly 1.19MHz

 *

 *  - each IO is going to take about 1us on real hardware,

 *    but we allow it to be much faster (by a factor of 10) or

 *    _slightly_ slower (ie we allow up to a 2us read+counter

 *    update - anything else implies a unacceptably slow CPU

 *    or PIT for the fast calibration to work.

 *

 *  - with 256 PIT ticks to read the value, we have 214us to

 *    see the same MSB (and overhead like doing a single TSC

 *    read per MSB value etc).

 *

 *  - We're doing 2 reads per loop (LSB, MSB), and we expect

 *    them each to take about a microsecond on real hardware.

 *    So we expect a count value of around 100. But we'll be

 *    generous, and accept anything over 50.

 *

 *  - if the PIT is stuck, and we see *many* more reads, we

 *    return early (and the next caller of pit_expect_msb()

 *    then consider it a failure when they don't see the

 *    next expected value).

 *

 * These expectations mean that we know that we have seen the

 * transition from one expected value to another with a fairly

 * high accuracy, and we didn't miss any events. We can thus

 * use the TSC value at the transitions to calculate a pretty

 * good value for the TSC frequency.

 Ignore LSB */

	/*

	 * We require _some_ success, but the quality control

	 * will be based on the error terms on the TSC values.

/*

 * How many MSB values do we want to see? We aim for

 * a maximum error rate of 500ppm (in practice the

 * real error is much smaller), but refuse to spend

 * more than 50ms on it.

 Set the Gate high, disable speaker */

	/*

	 * Counter 2, mode 0 (one-shot), binary count

	 *

	 * NOTE! Mode 2 decrements by two (and then the

	 * output is flipped each time, giving the same

	 * final output frequency as a decrement-by-one),

	 * so mode 0 is much better when looking at the

	 * individual counts.

 Start at 0xffff */

	/*

	 * The PIT starts counting at the next edge, so we

	 * need to delay for a microsecond. The easiest way

	 * to do that is to just read back the 16-bit counter

	 * once from the PIT.

			/*

			 * Extrapolate the error and fail fast if the error will

			 * never be below 500 ppm.

			/*

			 * Iterate until the error is less than 500 ppm

			/*

			 * Check the PIT one more time to verify that

			 * all TSC reads were stable wrt the PIT.

			 *

			 * This also guarantees serialization of the

			 * last cycle read ('d2') in pit_expect_msb.

	/*

	 * Ok, if we get here, then we've seen the

	 * MSB of the PIT decrement 'i' times, and the

	 * error has shrunk to less than 500 ppm.

	 *

	 * As a result, we can depend on there not being

	 * any odd delays anywhere, and the TSC reads are

	 * reliable (within the error).

	 *

	 * kHz = ticks / time-in-seconds / 1000;

	 * kHz = (t2 - t1) / (I * 256 / PIT_TICK_RATE) / 1000

	 * kHz = ((t2 - t1) * PIT_TICK_RATE) / (I * 256 * 1000)

/**

 * native_calibrate_tsc

 * Determine TSC frequency via CPUID, else return 0.

 CPUID 15H TSC/Crystal ratio, plus optionally Crystal Hz */

	/*

	 * Denverton SoCs don't report crystal clock, and also don't support

	 * CPUID.0x16 for the calculation below, so hardcode the 25MHz crystal

	 * clock.

	/*

	 * TSC frequency reported directly by CPUID is a "hardware reported"

	 * frequency and is the most accurate one so far we have. This

	 * is considered a known frequency.

	/*

	 * Some Intel SoCs like Skylake and Kabylake don't report the crystal

	 * clock, but we can easily calculate it to a high degree of accuracy

	 * by considering the crystal ratio and the CPU speed.

	/*

	 * For Atom SoCs TSC is the only reliable clocksource.

	 * Mark TSC reliable so no watchdog on it.

	/*

	 * The local APIC appears to be fed by the core crystal clock

	 * (which sounds entirely sensible). We can set the global

	 * lapic_timer_period here to avoid having to calibrate the APIC

	 * timer later.

/*

 * calibrate cpu using pit, hpet, and ptimer methods. They are available

 * later in boot after acpi is initialized.

	/*

	 * Run 5 calibration loops to get the lowest frequency value

	 * (the best estimate). We use two different calibration modes

	 * here:

	 *

	 * 1) PIT loop. We set the PIT Channel 2 to oneshot mode and

	 * load a timeout of 50ms. We read the time right after we

	 * started the timer and wait until the PIT count down reaches

	 * zero. In each wait loop iteration we read the TSC and check

	 * the delta to the previous read. We keep track of the min

	 * and max values of that delta. The delta is mostly defined

	 * by the IO time of the PIT access, so we can detect when

	 * any disturbance happened between the two reads. If the

	 * maximum time is significantly larger than the minimum time,

	 * then we discard the result and have another try.

	 *

	 * 2) Reference counter. If available we use the HPET or the

	 * PMTIMER as a reference to check the sanity of that value.

	 * We use separate TSC readouts and check inside of the

	 * reference read for any possible disturbance. We discard

	 * disturbed values here as well. We do that around the PIT

	 * calibration delay loop as we have to wait for a certain

	 * amount of time anyway.

 Preset PIT loop values */

		/*

		 * Read the start value and the reference count of

		 * hpet/pmtimer when available. Then do the PIT

		 * calibration, which will take at least 50ms, and

		 * read the end value.

 Pick the lowest PIT TSC calibration so far */

 hpet or pmtimer available ? */

 Check, whether the sampling was disturbed */

 Check the reference deviation */

		/*

		 * If both calibration results are inside a 10% window

		 * then we can be sure, that the calibration

		 * succeeded. We break out of the loop right away. We

		 * use the reference value, as it is more precise.

		/*

		 * Check whether PIT failed more than once. This

		 * happens in virtualized environments. We need to

		 * give the virtual PC a slightly longer timeframe for

		 * the HPET/PMTIMER to make the result precise.

	/*

	 * Now check the results.

 PIT gave no useful value */

 We don't have an alternative source, disable TSC */

 The alternative source failed as well, disable TSC */

 Use the alternative source */

 We don't have an alternative source, use the PIT calibration value */

 The alternative source failed, use the PIT calibration value */

	/*

	 * The calibration values differ too much. In doubt, we use

	 * the PIT value as we know that there are PMTIMERs around

	 * running at double speed. At least we let the user know:

/**

 * native_calibrate_cpu_early - can calibrate the cpu early in boot

/**

 * native_calibrate_cpu - calibrate the cpu

/*

 * Even on processors with invariant TSC, TSC gets reset in some the

 * ACPI system sleep states. And in some systems BIOS seem to reinit TSC to

 * arbitrary value (still sync'd across cpu's) during resume from such sleep

 * states. To cope up with this, recompute the cyc2ns_offset for each cpu so

 * that sched_clock() continues from the point where it was left off during

 * suspend.

	/*

	 * We're coming out of suspend, there's no concurrency yet; don't

	 * bother being nice about the RCU stuff, just write to both

	 * data fields.

/*

 * Frequency scaling support. Adjust the TSC based timer when the CPU frequency

 * changes.

 *

 * NOTE: On SMP the situation is not fixable in general, so simply mark the TSC

 * as unstable and give up in those cases.

 *

 * Should fix up last_tsc too. Currently gettimeofday in the

 * first tick after the change will be slightly wrong.

 CONFIG_CPU_FREQ */

/*

 * If ART is present detect the numerator:denominator to convert to TSC

	/*

	 * Don't enable ART in a VM, non-stop TSC and TSC_ADJUST required,

	 * and the TSC counter resets must not occur asynchronously.

 Make this sticky over multiple CPU init calls */

 clocksource code */

/*

 * We used to compare the TSC to the cycle_last value in the clocksource

 * structure to avoid a nasty time-warp. This can be observed in a

 * very small window right after one CPU updated cycle_last under

 * xtime/vsyscall_gtod lock and the other CPU reads a TSC value which

 * is smaller than the cycle_last reference value due to a TSC which

 * is slightly behind. This delta is nowhere else observable, but in

 * that case it results in a forward time jump in the range of hours

 * due to the unsigned delta calculation of the time keeping core

 * code, which is necessary to support wrapping clocksources like pm

 * timer.

 *

 * This sanity check is now done in the core timekeeping code.

 * checking the result of read_tsc() - cycle_last for being negative.

 * That works because CLOCKSOURCE_MASK(64) does not mask out any bit.

/*

 * .mask MUST be CLOCKSOURCE_MASK(64). See comment above read_tsc()

/*

 * Must mark VALID_FOR_HRES early such that when we unregister tsc_early

 * this one will immediately take over. We will only register if TSC has

 * been found good.

 RTSC counts during suspend */

 Geode_LX - the OLPC CPU has a very reliable TSC */

/*

 * Make an educated guess if the TSC is trustworthy and synchronized

 * over all CPUs.

	/*

	 * Intel systems are normally all synchronized.

	 * Exceptions must mark TSC as unstable:

 assume multi socket systems are not synchronized: */

/*

 * Convert ART to TSC given numerator/denominator found in detect_art()

/**

 * convert_art_ns_to_tsc() - Convert ART in nanoseconds to TSC.

 * @art_ns: ART (Always Running Timer) in unit of nanoseconds

 *

 * PTM requires all timestamps to be in units of nanoseconds. When user

 * software requests a cross-timestamp, this function converts system timestamp

 * to TSC.

 *

 * This is valid when CPU feature flag X86_FEATURE_TSC_KNOWN_FREQ is set

 * indicating the tsc_khz is derived from CPUID[15H]. Drivers should check

 * that this flag is set before conversion to TSC is attempted.

 *

 * Return:

 * struct system_counterval_t - system counter value with the pointer to the

 *	corresponding clocksource

 *	@cycles:	System counter value

 *	@cs:		Clocksource corresponding to system counter value. Used

 *			by timekeeping code to verify comparability of two cycle

 *			values.

/**

 * tsc_refine_calibration_work - Further refine tsc freq calibration

 * @work - ignored.

 *

 * This functions uses delayed work over a period of a

 * second to further refine the TSC freq value. Since this is

 * timer based, instead of loop based, we don't block the boot

 * process while this longer calibration is done.

 *

 * If there are any calibration anomalies (too many SMIs, etc),

 * or the refined calibration is off by 1% of the fast early

 * calibration, we throw out the new calibration and use the

 * early calibration.

 Don't bother refining TSC on unstable systems */

	/*

	 * Since the work is started early in boot, we may be

	 * delayed the first time we expire. So set the workqueue

	 * again once we know timers are working.

		/*

		 * Only set hpet once, to avoid mixing hardware

		 * if the hpet becomes enabled later.

 hpet or pmtimer available ? */

 Check, whether the sampling was disturbed */

 Make sure we're within 1% */

 Inform the TSC deadline clockevent devices about the recalibration */

 Update the sched_clock() rate to match the clocksource one */

	/*

	 * When TSC frequency is known (retrieved via MSR or CPUID), we skip

	 * the refined calibration and directly register it as a clocksource.

/*

 * We use device_initcall here, to ensure we run after the hpet

 * is fully initialized, which may occur at fs_initcall time.

 Make sure that cpu and tsc are not already calibrated */

 We should not be here with non-native cpu calibration */

	/*

	 * Trust non-zero tsc_khz as authoritative,

	 * and use it to sanity check cpu_khz,

	 * which will be off if system timer is off.

 Sanitize TSC ADJUST before cyc2ns gets initialized */

 Don't change UV TSC multi-chassis synchronization */

	/*

	 * native_calibrate_cpu_early can only calibrate using methods that are

	 * available early in boot.

 We failed to determine frequencies earlier, try again */

/*

 * If we have a constant TSC and are using the TSC for the delay loop,

 * we can skip clock calibration if another cpu in the same socket has already

 * been calibrated. This assumes that CONSTANT_TSC applies to all

 * cpus in the socket - this should be a safe assumption.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1992 Krishna Balasubramanian and Linus Torvalds

 * Copyright (C) 1999 Ingo Molnar <mingo@redhat.com>

 * Copyright (C) 2002 Andi Kleen

 *

 * This handles calls from both 32bit and 64bit mode.

 *

 * Lock order:

 *	contex.ldt_usr_sem

 *	  mmap_lock

 *	    context.lock

 This is a multiple of PAGE_SIZE. */

 READ_ONCE synchronizes with smp_store_release */

	/*

	 * Any change to mm->context.ldt is followed by an IPI to all

	 * CPUs with the mm active.  The LDT will not be freed until

	 * after the IPI is handled by all such CPUs.  This means that,

	 * if the ldt_struct changes before we return, the values we see

	 * will be safe, and the new values will be loaded before we run

	 * any user code.

	 *

	 * NB: don't try to convert this to use RCU without extreme care.

	 * We would still need IRQs off, because we don't want to change

	 * the local LDT after an IPI loaded a newer value than the one

	 * that we can see.

				/*

				 * Whoops -- either the new LDT isn't mapped

				 * (if slot == -1) or is mapped into a bogus

				 * slot (if slot > 1).

			/*

			 * If page table isolation is enabled, ldt->entries

			 * will not be mapped in the userspace pagetables.

			 * Tell the CPU to access the LDT through the alias

			 * at ldt_slot_va(ldt->slot).

	/*

	 * Load the LDT if either the old or new mm had an LDT.

	 *

	 * An mm will never go from having an LDT to not having an LDT.  Two

	 * mms never share an LDT, so we don't gain anything by checking to

	 * see whether the LDT changed.  There's also no guarantee that

	 * prev->context.ldt actually matches LDTR, but, if LDTR is non-NULL,

	 * then prev->context.ldt will also be non-NULL.

	 *

	 * If we really cared, we could optimize the case where prev == next

	 * and we're exiting lazy mode.  Most of the time, if this happens,

	 * we don't actually need to reload LDTR, but modify_ldt() is mostly

	 * used by legacy code and emulators where we don't need this level of

	 * performance.

	 *

	 * This uses | instead of || because it generates better code.

	/*

	 * Make sure that the cached DS and ES descriptors match the updated

	 * LDT.

 context.lock is held by the task which issued the smp function call */

 The caller must call finalize_ldt_struct on the result. LDT starts zeroed. */

	/*

	 * Xen is very picky: it requires a page-aligned LDT that has no

	 * trailing nonzero bytes in any page that contains LDT descriptors.

	 * Keep it simple: zero the whole allocation and never allocate less

	 * than PAGE_SIZE.

 The new LDT isn't aliased for PTI yet. */

		/*

		 * We already had an LDT.  The top-level entry should already

		 * have been allocated and synchronized with the usermode

		 * tables.

		/*

		 * This is the first time we're mapping an LDT for this process.

		 * Sync the pgd to the usermode tables.

 !CONFIG_X86_PAE */

 CONFIG_X86_PAE */

/*

 * If PTI is enabled, this maps the LDT into the kernelmode and

 * usermode tables for the given mm.

	/*

	 * Any given ldt_struct should have map_ldt_struct() called at most

	 * once.

 Check if the current mappings are sane */

		/*

		 * Treat the PTI LDT range as a *userspace* range.

		 * get_locked_pte() will allocate all needed pagetables

		 * and account for them in this mm.

		/*

		 * Map it RO so the easy to find address is not a primary

		 * target via some kernel interface which misses a

		 * permission check.

 Filter out unsuppored __PAGE_KERNEL* bits: */

 Propagate LDT mapping to the user page-table */

 LDT map/unmap is only required for PTI */

 !CONFIG_PAGE_TABLE_ISOLATION */

 CONFIG_PAGE_TABLE_ISOLATION */

	/*

	 * Although free_pgd_range() is intended for freeing user

	 * page-tables, it also works out for kernel mappings on x86.

	 * We use tlb_gather_mmu_fullmm() to avoid confusing the

	 * range-tracking logic in __tlb_adjust_range().

 After calling this, the LDT is immutable. */

 Synchronizes with READ_ONCE in load_mm_ldt. */

 Activate the LDT for all CPUs using currents mm. */

/*

 * Called on fork from arch_dup_mmap(). Just copy the current LDT state,

 * the new task is not running, so nothing can be installed.

/*

 * No need to lock the MM as we are the last user

 *

 * 64bit: Don't touch the LDT register - we're already in the next thread.

 Zero-fill the rest and pretend we read bytecount bytes. */

 CHECKME: Can we use _one_ random number ? */

	/*

	 * Xen PV does not implement ESPFIX64, which means that 16-bit

	 * segments will not work correctly.  Until either Xen PV implements

	 * ESPFIX64 and can signal this fact to the guest or unless someone

	 * provides compelling evidence that allowing broken 16-bit segments

	 * is worthwhile, disallow 16-bit segments under Xen PV.

 The user wants to clear the entry. */

	/*

	 * If we are using PTI, map the new LDT into the userspace pagetables.

	 * If there is already an LDT, use the other slot so that other CPUs

	 * will continue to use the old LDT until install_ldt() switches

	 * them over to the new LDT.

		/*

		 * This only can fail for the first LDT setup. If an LDT is

		 * already installed then the PTE page is already

		 * populated. Mop up a half populated page table.

	/*

	 * The SYSCALL_DEFINE() macros give us an 'unsigned long'

	 * return type, but tht ABI for sys_modify_ldt() expects

	 * 'int'.  This cast gives us an int-sized value in %rax

	 * for the return code.  The 'unsigned' is necessary so

	 * the compiler does not try to sign-extend the negative

	 * return codes into the high half of the register when

	 * taking the value from int->long.

 SPDX-License-Identifier: GPL-2.0-or-later

/* -*- linux-c -*-

 * APM BIOS driver for Linux

 * Copyright 1994-2001 Stephen Rothwell (sfr@canb.auug.org.au)

 *

 * Initial development of this driver was funded by NEC Australia P/L

 *	and NEC Corporation

 *

 * October 1995, Rik Faith (faith@cs.unc.edu):

 *    Minor enhancements and updates (to the patch set) for 1.3.x

 *    Documentation

 * January 1996, Rik Faith (faith@cs.unc.edu):

 *    Make /proc/apm easy to format (bump driver version)

 * March 1996, Rik Faith (faith@cs.unc.edu):

 *    Prohibit APM BIOS calls unless apm_enabled.

 *    (Thanks to Ulrich Windl <Ulrich.Windl@rz.uni-regensburg.de>)

 * April 1996, Stephen Rothwell (sfr@canb.auug.org.au)

 *    Version 1.0 and 1.1

 * May 1996, Version 1.2

 * Feb 1998, Version 1.3

 * Feb 1998, Version 1.4

 * Aug 1998, Version 1.5

 * Sep 1998, Version 1.6

 * Nov 1998, Version 1.7

 * Jan 1999, Version 1.8

 * Jan 1999, Version 1.9

 * Oct 1999, Version 1.10

 * Nov 1999, Version 1.11

 * Jan 2000, Version 1.12

 * Feb 2000, Version 1.13

 * Nov 2000, Version 1.14

 * Oct 2001, Version 1.15

 * Jan 2002, Version 1.16

 * Oct 2002, Version 1.16ac

 *

 * History:

 *    0.6b: first version in official kernel, Linux 1.3.46

 *    0.7: changed /proc/apm format, Linux 1.3.58

 *    0.8: fixed gcc 2.7.[12] compilation problems, Linux 1.3.59

 *    0.9: only call bios if bios is present, Linux 1.3.72

 *    1.0: use fixed device number, consolidate /proc/apm into this file,

 *         Linux 1.3.85

 *    1.1: support user-space standby and suspend, power off after system

 *         halted, Linux 1.3.98

 *    1.2: When resetting RTC after resume, take care so that the time

 *         is only incorrect by 30-60mS (vs. 1S previously) (Gabor J. Toth

 *         <jtoth@princeton.edu>); improve interaction between

 *         screen-blanking and gpm (Stephen Rothwell); Linux 1.99.4

 *    1.2a:Simple change to stop mysterious bug reports with SMP also added

 *	   levels to the printk calls. APM is not defined for SMP machines.

 *         The new replacement for it is, but Linux doesn't yet support this.

 *         Alan Cox Linux 2.1.55

 *    1.3: Set up a valid data descriptor 0x40 for buggy BIOS's

 *    1.4: Upgraded to support APM 1.2. Integrated ThinkPad suspend patch by

 *         Dean Gaudet <dgaudet@arctic.org>.

 *         C. Scott Ananian <cananian@alumni.princeton.edu> Linux 2.1.87

 *    1.5: Fix segment register reloading (in case of bad segments saved

 *         across BIOS call).

 *         Stephen Rothwell

 *    1.6: Cope with compiler/assembler differences.

 *         Only try to turn off the first display device.

 *         Fix OOPS at power off with no APM BIOS by Jan Echternach

 *                   <echter@informatik.uni-rostock.de>

 *         Stephen Rothwell

 *    1.7: Modify driver's cached copy of the disabled/disengaged flags

 *         to reflect current state of APM BIOS.

 *         Chris Rankin <rankinc@bellsouth.net>

 *         Reset interrupt 0 timer to 100Hz after suspend

 *         Chad Miller <cmiller@surfsouth.com>

 *         Add CONFIG_APM_IGNORE_SUSPEND_BOUNCE

 *         Richard Gooch <rgooch@atnf.csiro.au>

 *         Allow boot time disabling of APM

 *         Make boot messages far less verbose by default

 *         Make asm safer

 *         Stephen Rothwell

 *    1.8: Add CONFIG_APM_RTC_IS_GMT

 *         Richard Gooch <rgooch@atnf.csiro.au>

 *         change APM_NOINTS to CONFIG_APM_ALLOW_INTS

 *         remove dependency on CONFIG_PROC_FS

 *         Stephen Rothwell

 *    1.9: Fix small typo.  <laslo@wodip.opole.pl>

 *         Try to cope with BIOS's that need to have all display

 *         devices blanked and not just the first one.

 *         Ross Paterson <ross@soi.city.ac.uk>

 *         Fix segment limit setting it has always been wrong as

 *         the segments needed to have byte granularity.

 *         Mark a few things __init.

 *         Add hack to allow power off of SMP systems by popular request.

 *         Use CONFIG_SMP instead of __SMP__

 *         Ignore BOUNCES for three seconds.

 *         Stephen Rothwell

 *   1.10: Fix for Thinkpad return code.

 *         Merge 2.2 and 2.3 drivers.

 *         Remove APM dependencies in arch/i386/kernel/process.c

 *         Remove APM dependencies in drivers/char/sysrq.c

 *         Reset time across standby.

 *         Allow more initialisation on SMP.

 *         Remove CONFIG_APM_POWER_OFF and make it boot time

 *         configurable (default on).

 *         Make debug only a boot time parameter (remove APM_DEBUG).

 *         Try to blank all devices on any error.

 *   1.11: Remove APM dependencies in drivers/char/console.c

 *         Check nr_running to detect if we are idle (from

 *         Borislav Deianov <borislav@lix.polytechnique.fr>)

 *         Fix for bioses that don't zero the top part of the

 *         entrypoint offset (Mario Sitta <sitta@al.unipmn.it>)

 *         (reported by Panos Katsaloulis <teras@writeme.com>).

 *         Real mode power off patch (Walter Hofmann

 *         <Walter.Hofmann@physik.stud.uni-erlangen.de>).

 *   1.12: Remove CONFIG_SMP as the compiler will optimize

 *         the code away anyway (smp_num_cpus == 1 in UP)

 *         noted by Artur Skawina <skawina@geocities.com>.

 *         Make power off under SMP work again.

 *         Fix thinko with initial engaging of BIOS.

 *         Make sure power off only happens on CPU 0

 *         (Paul "Rusty" Russell <rusty@rustcorp.com.au>).

 *         Do error notification to user mode if BIOS calls fail.

 *         Move entrypoint offset fix to ...boot/setup.S

 *         where it belongs (Cosmos <gis88564@cis.nctu.edu.tw>).

 *         Remove smp-power-off. SMP users must now specify

 *         "apm=power-off" on the kernel command line. Suggested

 *         by Jim Avera <jima@hal.com>, modified by Alan Cox

 *         <alan@lxorguk.ukuu.org.uk>.

 *         Register the /proc/apm entry even on SMP so that

 *         scripts that check for it before doing power off

 *         work (Jim Avera <jima@hal.com>).

 *   1.13: Changes for new pm_ interfaces (Andy Henroid

 *         <andy_henroid@yahoo.com>).

 *         Modularize the code.

 *         Fix the Thinkpad (again) :-( (CONFIG_APM_IGNORE_MULTIPLE_SUSPENDS

 *         is now the way life works).

 *         Fix thinko in suspend() (wrong return).

 *         Notify drivers on critical suspend.

 *         Make kapmd absorb more idle time (Pavel Machek <pavel@ucw.cz>

 *         modified by sfr).

 *         Disable interrupts while we are suspended (Andy Henroid

 *         <andy_henroid@yahoo.com> fixed by sfr).

 *         Make power off work on SMP again (Tony Hoyle

 *         <tmh@magenta-logic.com> and <zlatko@iskon.hr>) modified by sfr.

 *         Remove CONFIG_APM_SUSPEND_BOUNCE.  The bounce ignore

 *         interval is now configurable.

 *   1.14: Make connection version persist across module unload/load.

 *         Enable and engage power management earlier.

 *         Disengage power management on module unload.

 *         Changed to use the sysrq-register hack for registering the

 *         power off function called by magic sysrq based upon discussions

 *         in irc://irc.openprojects.net/#kernelnewbies

 *         (Crutcher Dunnavant <crutcher+kernel@datastacks.com>).

 *         Make CONFIG_APM_REAL_MODE_POWER_OFF run time configurable.

 *         (Arjan van de Ven <arjanv@redhat.com>) modified by sfr.

 *         Work around byte swap bug in one of the Vaio's BIOS's

 *         (Marc Boucher <marc@mbsi.ca>).

 *         Exposed the disable flag to dmi so that we can handle known

 *         broken APM (Alan Cox <alan@lxorguk.ukuu.org.uk>).

 *   1.14ac: If the BIOS says "I slowed the CPU down" then don't spin

 *         calling it - instead idle. (Alan Cox <alan@lxorguk.ukuu.org.uk>)

 *         If an APM idle fails log it and idle sensibly

 *   1.15: Don't queue events to clients who open the device O_WRONLY.

 *         Don't expect replies from clients who open the device O_RDONLY.

 *         (Idea from Thomas Hood)

 *         Minor waitqueue cleanups. (John Fremlin <chief@bandits.org>)

 *   1.16: Fix idle calling. (Andreas Steinmetz <ast@domdv.de> et al.)

 *         Notify listeners of standby or suspend events before notifying

 *         drivers. Return EBUSY to ioctl() if suspend is rejected.

 *         (Russell King <rmk@arm.linux.org.uk> and Thomas Hood)

 *         Ignore first resume after we generate our own resume event

 *         after a suspend (Thomas Hood)

 *         Daemonize now gets rid of our controlling terminal (sfr).

 *         CONFIG_APM_CPU_IDLE now just affects the default value of

 *         idle_threshold (sfr).

 *         Change name of kernel apm daemon (as it no longer idles) (sfr).

 *   1.16ac: Fix up SMP support somewhat. You can now force SMP on and we

 *	   make _all_ APM calls on the CPU#0. Fix unsafe sign bug.

 *	   TODO: determine if its "boot CPU" or "CPU0" we want to lock to.

 *

 * APM 1.1 Reference:

 *

 *   Intel Corporation, Microsoft Corporation. Advanced Power Management

 *   (APM) BIOS Interface Specification, Revision 1.1, September 1993.

 *   Intel Order Number 241704-001.  Microsoft Part Number 781-110-X01.

 *

 * [This document is available free from Intel by calling 800.628.8686 (fax

 * 916.356.6100) or 800.548.4725; or from

 * http://www.microsoft.com/whdc/archive/amp_12.mspx  It is also

 * available from Microsoft by calling 206.882.8080.]

 *

 * APM 1.2 Reference:

 *   Intel Corporation, Microsoft Corporation. Advanced Power Management

 *   (APM) BIOS Interface Specification, Revision 1.2, February 1996.

 *

 * [This document is available from Microsoft at:

 *    http://www.microsoft.com/whdc/archive/amp_12.mspx]

/*

 * The apm_bios device is one of the misc char devices.

 * This is its minor number.

/*

 * Various options can be changed at boot time as follows:

 * (We allow underscores for compatibility with the modules code)

 *	apm=on/off			enable/disable APM

 *	    [no-]allow[-_]ints		allow interrupts during BIOS calls

 *	    [no-]broken[-_]psr		BIOS has a broken GetPowerStatus call

 *	    [no-]realmode[-_]power[-_]off	switch to real mode before

 *	    					powering off

 *	    [no-]debug			log some debugging messages

 *	    [no-]power[-_]off		power off on shutdown

 *	    [no-]smp			Use apm even on an SMP box

 *	    bounce[-_]interval=<n>	number of ticks to ignore suspend

 *	    				bounces

 *          idle[-_]threshold=<n>       System idle percentage above which to

 *                                      make APM BIOS idle calls. Set it to

 *                                      100 to disable.

 *          idle[-_]period=<n>          Period (in 1/100s of a second) over

 *                                      which the idle percentage is

 *                                      calculated.

/* KNOWN PROBLEM MACHINES:

 *

 * U: TI 4000M TravelMate: BIOS is *NOT* APM compliant

 *                         [Confirmed by TI representative]

 * ?: ACER 486DX4/75: uses dseg 0040, in violation of APM specification

 *                    [Confirmed by BIOS disassembly]

 *                    [This may work now ...]

 * P: Toshiba 1950S: battery life information only gets updated after resume

 * P: Midwest Micro Soundbook Elite DX2/66 monochrome: screen blanking

 * 	broken in BIOS [Reported by Garst R. Reese <reese@isn.net>]

 * ?: AcerNote-950: oops on reading /proc/apm - workaround is a WIP

 * 	Neale Banks <neale@lowendale.com.au> December 2000

 *

 * Legend: U = unusable with APM patches

 *         P = partially usable with APM patches

/*

 * Define as 1 to make the driver always call the APM BIOS busy

 * routine even if the clock was not reported as slowed by the

 * idle routine.  Otherwise, define as 0.

/*

 * Define to make the APM BIOS calls zero all data segment registers (so

 * that an incorrect BIOS implementation will cause a kernel panic if it

 * tries to write to arbitrary memory).

/*

 * Define to re-initialize the interrupt 0 timer to 100 Hz after a suspend.

 * This patched by Chad Miller <cmiller@surfsouth.com>, original code by

 * David Chen <chen@ctpa04.mit.edu>

/*

 * Need to poll the APM BIOS every second

/*

 * Ignore suspend events for this amount of time after a resume

/*

 * Maximum number of events stored

/*

 * The per-file APM data

/*

 * The magic number in apm_user

/*

 * idle percentage above which bios idle calls are done

 entry 0 is for polling */ },

 entry 1 is for APM idle */

 WAG */

 WAG */

/*

 * Local variables

/*

 * Set up a segment that references the real mode segment 0x40

 * that extends up to the end of page zero (that we have reserved).

 * This is for buggy BIOS's that refer to (real mode) segment 0x40

 * even though they are called in protected mode.

 no spaces */

/*

 *	APM event names taken from the APM 1.2 specification. These are

 *	the message codes that the BIOS uses to tell us about events

/*

 *	The BIOS returns a set of standard error codes in AX when the

 *	carry flag is set.

 N/A	{ APM_SUCCESS,		"Operation succeeded" }, */

 N/A	{ APM_16_UNSUPPORTED,	"16 bit interface not supported" }, */

 N/A	{ APM_NO_EVENTS,	"No events pending" }, */

/**

 *	apm_error	-	display an APM error

 *	@str: information string

 *	@err: APM BIOS return code

 *

 *	Write a meaningful log entry to the kernel log in the event of

 *	an APM error.  Note that this also handles (negative) kernel errors.

/*

 * These are the actual BIOS calls.  Depending on APM_ZERO_SEGS and

 * apm_info.allow_ints, we are being really paranoid here!  Not only

 * are interrupts disabled, but all the segment registers (except SS)

 * are saved and zeroed this means that if the BIOS tries to reference

 * any data without explicitly loading the segment registers, the kernel

 * will fault immediately rather than have some unforeseen circumstances

 * for the rest of the kernel.  And it will be very obvious!  :-) Doing

 * this depends on CS referring to the same physical memory as DS so that

 * DS can be zeroed before the call. Unfortunately, we can't do anything

 * about the stack segment/pointer.  Also, we tell the compiler that

 * everything could change.

 *

 * Also, we KNOW that for the non error case of apm_bios_call, there

 * is no useful data returned in the low order 8 bits of eax.

 In and out */

 Out only */

 Error: -ENOMEM, or bits 8-15 of eax */

/**

 *	__apm_bios_call - Make an APM BIOS 32bit call

 *	@_call: pointer to struct apm_bios_call.

 *

 *	Make an APM call using the 32bit protected mode interface. The

 *	caller is responsible for knowing if APM BIOS is configured and

 *	enabled. This call can disable interrupts for a long period of

 *	time on some laptops.  The return value is in AH and the carry

 *	flag is loaded into AL.  If there is an error, then the error

 *	code is returned in AH (bits 8-15 of eax) and this function

 *	returns non-zero.

 *

 *	Note: this makes the call on the current CPU.

 Run __apm_bios_call or __apm_bios_call_simple on CPU 0 */

	/* Don't bother with work_on_cpu in the common case, so we don't

 work_on_cpu can fail with -ENOMEM */

/**

 *	apm_bios_call	-	Make an APM BIOS 32bit call (on CPU 0)

 *	@call: the apm_bios_call registers.

 *

 *	If there is an error, it is returned in @call.err.

/**

 *	__apm_bios_call_simple - Make an APM BIOS 32bit call (on CPU 0)

 *	@_call: pointer to struct apm_bios_call.

 *

 *	Make a BIOS call that returns one value only, or just status.

 *	If there is an error, then the error code is returned in AH

 *	(bits 8-15 of eax) and this function returns non-zero (it can

 *	also return -ENOMEM). This is used for simpler BIOS operations.

 *	This call may hold interrupts off for a long time on some laptops.

 *

 *	Note: this makes the call on the current CPU.

/**

 *	apm_bios_call_simple	-	make a simple APM BIOS 32bit call

 *	@func: APM function to invoke

 *	@ebx_in: EBX register value for BIOS call

 *	@ecx_in: ECX register value for BIOS call

 *	@eax: EAX register on return from the BIOS call

 *	@err: bits

 *

 *	Make a BIOS call that returns one value only, or just status.

 *	If there is an error, then the error code is returned in @err

 *	and this function returns non-zero. This is used for simpler

 *	BIOS operations.  This call may hold interrupts off for a long

 *	time on some laptops.

/**

 *	apm_driver_version	-	APM driver version

 *	@val:	loaded with the APM version on return

 *

 *	Retrieve the APM version supported by the BIOS. This is only

 *	supported for APM 1.1 or higher. An error indicates APM 1.0 is

 *	probably present.

 *

 *	On entry val should point to a value indicating the APM driver

 *	version with the high byte being the major and the low byte the

 *	minor number both in BCD

 *

 *	On return it will hold the BIOS revision supported in the

 *	same format.

/**

 *	apm_get_event	-	get an APM event from the BIOS

 *	@event: pointer to the event

 *	@info: point to the event information

 *

 *	The APM BIOS provides a polled information for event

 *	reporting. The BIOS expects to be polled at least every second

 *	when events are pending. When a message is found the caller should

 *	poll until no more messages are present.  However, this causes

 *	problems on some laptops where a suspend event notification is

 *	not cleared until it is acknowledged.

 *

 *	Additional information is returned in the info pointer, providing

 *	that APM 1.2 is in use. If no messages are pending the value 0x80

 *	is returned (No power management events pending).

 indicate info not valid */

/**

 *	set_power_state	-	set the power management state

 *	@what: which items to transition

 *	@state: state to transition to

 *

 *	Request an APM change of state for one or more system devices. The

 *	processor state must be transitioned last of all. what holds the

 *	class of device in the upper byte and the device number (0xFF for

 *	all) for the object to be transitioned.

 *

 *	The state holds the state to transition to, which may in fact

 *	be an acceptance of a BIOS requested state change.

/**

 *	set_system_power_state - set system wide power state

 *	@state: which state to enter

 *

 *	Transition the entire system into a new APM power state.

/**

 *	apm_do_idle	-	perform power saving

 *

 *	This function notifies the BIOS that the processor is (in the view

 *	of the OS) idle. It returns -1 in the event that the BIOS refuses

 *	to handle the idle request. On a success the function returns 1

 *	if the BIOS did clock slowing or 0 otherwise.

		/* This always fails on some SMP boards running UP kernels.

		 * Only report the failure the first 5 times.

/**

 *	apm_do_busy	-	inform the BIOS the CPU is busy

 *

 *	Request that the BIOS brings the CPU back to full performance.

/*

 * If no process has really been interested in

 * the CPU for some time, we want to call BIOS

 * power management - we probably want

 * to conserve power.

/**

 * apm_cpu_idle		-	cpu idling for APM capable Linux

 *

 * This is the idling function the kernel executes when APM is available. It

 * tries to do BIOS powermanagement based on the average system idle time.

 * Furthermore it calls the system default idle routine.

 = 0 */

 = 0 */

 = 0 */

 BIOS refused */

/**

 *	apm_power_off	-	ask the BIOS to power off

 *

 *	Handle the power off sequence. This is the one piece of code we

 *	will execute even on SMP machines. In order to deal with BIOS

 *	bugs we support real mode APM BIOS power off calls. We also make

 *	the SMP call on CPU0 as some systems will only honour this call

 *	on their first cpu.

 Some bioses don't like being called from CPU != 0 */

/**

 *	apm_enable_power_management - enable BIOS APM power management

 *	@enable: enable yes/no

 *

 *	Enable or disable the APM BIOS power services.

/**

 *	apm_get_power_status	-	get current power state

 *	@status: returned status

 *	@bat: battery info

 *	@life: estimated life

 *

 *	Obtain the current power status from the APM BIOS. We return a

 *	status which gives the rough battery status, and current power

 *	source. The bat value returned give an estimate as a percentage

 *	of life and a status value for the battery. The estimated life

 *	if reported is a lifetime in seconds/minutes at current power

 *	consumption.

 pretend we only have one battery. */

/**

 *	apm_engage_power_management	-	enable PM on a device

 *	@device: identity of device

 *	@enable: on/off

 *

 *	Activate or deactivate power management on either a specific device

 *	or the entire system (%APM_DEVICE_ALL).

/**

 *	apm_console_blank	-	blank the display

 *	@blank: on/off

 *

 *	Attempt to blank the console, firstly by blanking just video device

 *	zero, and if that fails (some BIOSes don't support it) then it blanks

 *	all video devices. Typically the BIOS will do laptop backlight and

 *	monitor powerdown for us.

 silence gcc */

 set the clock to HZ */

 binary, mode 2, LSB/MSB, ch 0 */

 LSB */

 MSB */

 silence gcc */

 we don't use the eventinfo */

			/*

			 * If we are already processing a SUSPEND,

			 * then further SUSPEND events from the BIOS

			 * will be ignored.  We also return here to

			 * cope with the fact that the Thinkpads keep

			 * sending a SUSPEND event until something else

			 * happens!

 If needed, notify drivers here */

			/*

			 * We are not allowed to reject a critical suspend.

/*

 * This is the APM thread main loop.

		/*

		 * Ok, check all events, check for idle (and mark us sleeping

		 * so as not to count towards the load average)..

	/*

	 * XXX - this is a tiny bit broken, when we consider BSD

	 * process accounting. If the device is opened by root, we

	 * instantly flag that we used superuser privs. Who knows,

	 * we might close the device immediately without doing a

	 * privileged operation -- cevans

	/* Arguments, with symbols from linux/apm_bios.h.  Information is

	   from the Get Power Status (0x0a) call unless otherwise noted.



	   0) Linux driver version (this will change if format changes)

	   1) APM BIOS Version.  Usually 1.0, 1.1 or 1.2.

	   2) APM flags from APM Installation Check (0x00):

	      bit 0: APM_16_BIT_SUPPORT

	      bit 1: APM_32_BIT_SUPPORT

	      bit 2: APM_IDLE_SLOWS_CLOCK

	      bit 3: APM_BIOS_DISABLED

	      bit 4: APM_BIOS_DISENGAGED

	   3) AC line status

	      0x00: Off-line

	      0x01: On-line

	      0x02: On backup power (BIOS >= 1.1 only)

	      0xff: Unknown

	   4) Battery status

	      0x00: High

	      0x01: Low

	      0x02: Critical

	      0x03: Charging

	      0x04: Selected battery not present (BIOS >= 1.2 only)

	      0xff: Unknown

	   5) Battery flag

	      bit 0: High

	      bit 1: Low

	      bit 2: Critical

	      bit 3: Charging

	      bit 7: No system battery

	      0xff: Unknown

	   6) Remaining battery life (percentage of charge):

	      0-100: valid

	      -1: Unknown

	   7) Remaining battery life (time units):

	      Number of remaining minutes or seconds

	      -1: Unknown

	/* 2002/08/01 - WT

	 * This is to avoid random crashes at boot time during initialization

	 * on SMP systems in case of "apm=power-off" mode. Seen on ASUS A7M266D.

	 * Some bioses don't like being called from CPU != 0.

	 * Method suggested by Ingo Molnar.

			/*

			 * We only support BIOSs up to version 1.2

 Fall back to an APM 1.0 connection. */

		/*

		 * This call causes my NEC UltraLite Versa 33/C to hang if it

		 * is booted with PM disabled but not in the docking station.

		 * Unfortunate ...

 Install our power off handler.. */

 Simple "print if true" callback */

/*

 * Some Bioses enable the PS/2 mouse (touchpad) at resume, even if it was

 * disabled before the suspend. Linux used to get terribly confused by that.

 Some bioses have a broken protected mode poweroff and need to use realmode */

 Some laptops require interrupts to be enabled during APM calls */

 Some APM bioses corrupt memory or just plain do not work */

 Some APM bioses hang on APM idle calls */

/*

 *  Check for clue free BIOS implementations who use

 *  the following QA technique

 *

 *      [ Write BIOS Code ]<------

 *               |                ^

 *      < Does it Compile >----N--

 *               |Y               ^

 *	< Does it Boot Win98 >-N--

 *               |Y

 *           [Ship It]

 *

 *	Phoenix A04  08/24/2000 is known bad (Dell Inspiron 5000e)

 *	Phoenix A07  09/29/2000 is known good (Dell Inspiron 5000)

/*

 * This bios swaps the APM minute reporting bytes over (Many sony laptops

 * have this problem).

 Handle problems with APM on the C600 */

 Allow interrupts during suspend on Dell Latitude laptops*/

 APM crashes */

 Allow interrupts during suspend on Dell Inspiron laptops*/

 Handle problems with APM on Inspiron 5000e */

 Handle problems with APM on Inspiron 2500 */

 APM crashes */

 Allow interrupts during suspend on Compaq Laptops*/

 Allow interrupts during APM or the clock goes slow */

 APM blows on shutdown */

 APM crashes */

 APM crashes */

 APM crashes */

 APM crashes */

 APM crashes */

 APM crashes */

 APM crashes */

 APM idle hangs */

 APM idle hangs */

 Handle problems with APM on Sony Vaio PCG-N505X(DE) */

 Handle problems with APM on Sony Vaio PCG-N505VX */

 Handle problems with APM on Sony Vaio PCG-XG29 */

 Handle problems with APM on Sony Vaio PCG-Z600NE */

 Handle problems with APM on Sony Vaio PCG-Z600NE */

 Handle problems with APM on Sony Vaio PCG-Z600LEK(DE) */

 Handle problems with APM on Sony Vaio PCG-Z505LS */

 Handle problems with APM on Sony Vaio PCG-Z505LS */

 Handle problems with APM on Sony Vaio PCG-Z505LS (with updated BIOS) */

 Handle problems with APM on Sony Vaio PCG-F104K */

 Handle problems with APM on Sony Vaio PCG-C1VN/C1VE */

 Handle problems with APM on Sony Vaio PCG-C1VE */

 Handle problems with APM on Sony Vaio PCG-C1VE */

 broken PM poweroff bios */

 Generic per vendor APM settings  */

 Allow interrupts during suspend on IBM laptops */

/*

 * Just start the APM thread. We do NOT want to do APM BIOS

 * calls from anything but the APM thread, if for no other reason

 * than the fact that we don't trust the APM BIOS. This way,

 * most common APM BIOS problems that lead to protection errors

 * etc will have at least some level of being contained...

 *

 * In short, if something bad happens, at least we have a choice

 * of just killing the apm thread..

 User can override, but default is to trust DMI */

	/*

	 * Fix for the Compaq Contura 3/25c which reports BIOS version 0.1

	 * but is reportedly a 1.0 BIOS.

 BIOS < 1.2 doesn't set cseg_16_len */

 64k */

	/*

	 * Set up the long jump entry point to the APM BIOS, which is called

	 * from inline assembly.

	/*

	 * The APM 1.1 BIOS is supposed to provide limit information that it

	 * recognizes.  Many machines do this correctly, but many others do

	 * not restrict themselves to their claimed limit.  When this happens,

	 * they will cause a segmentation violation in the kernel at boot time.

	 * Most BIOS's, however, will respect a 64k limit, so we use that.

	 *

	 * Note we only set APM segments on CPU zero, since we pin the APM

	 * code to that CPU.

	/*

	 * Note we don't actually care if the misc_device cannot be registered.

	 * this driver can do its job without it, even if userspace can't

	 * control it.  just log the error

 SPDX-License-Identifier: GPL-2.0

/*

 * Generate definitions needed by assembly language modules.

 * This code generates raw asm output which is post-processed to extract

 * and format the required data.

 TLB state for the entry code */

 Layout info for cpu_entry_area */

 Offset for fields in tss_struct */

 SPDX-License-Identifier: GPL-2.0

/*

 * FPU register's regset abstraction, for ptrace, core dumps, etc.

/*

 * The xstateregs_active() routine is the same as the regset_fpregs_active() routine,

 * as the "regset->n" for the xstate regset will be updated based on the feature

 * capabilities supported by the xsave.

/*

 * The regset get() functions are invoked from:

 *

 *   - coredump to dump the current task's fpstate. If the current task

 *     owns the FPU then the memory state has to be synchronized and the

 *     FPU register state preserved. Otherwise fpstate is already in sync.

 *

 *   - ptrace to dump fpstate of a stopped task, in which case the registers

 *     have already been saved to fpstate on context switch.

/*

 * Invalidate cached FPU registers before modifying the stopped target

 * task's fpstate.

 *

 * This forces the target task on resume to restore the FPU registers from

 * modified fpstate. Otherwise the task might skip the restore and operate

 * with the cached FPU registers which discards the modifications.

	/*

	 * Only stopped child tasks can be used to modify the FPU

	 * state in the fpstate buffer:

 No funny business with partial or oversized writes is permitted. */

 Do not allow an invalid MXCSR value. */

 Copy the state  */

 Clear xmm8..15 */

 Mark FP and SSE as in use when XSAVE is enabled */

	/*

	 * A whole standard-format XSAVE buffer is needed:

/*

 * FPU tag word conversions.

 to avoid 16 bit prefixes in the code */

 Transform each pair of bits into 01 (valid) or 00 (empty) */

 0V0V0V0V0V0V0V0V */

 and move the valid bits to the lower byte. */

 00VV00VV00VV00VV */

 0000VVVV0000VVVV */

 00000000VVVVVVVV */

/*

 * FXSR floating point environment conversions.

	/*

	 * should be actually ds/cs at fpu exception time, but

	 * that information is not available in 64bit mode.

 cs and ds ignored */

 Handle init state optimized xstate correctly */

 No funny business with partial or oversized writes is permitted. */

	/*

	 * Update the header bit in the xsave header, indicating the

	 * presence of FP.

 CONFIG_X86_32 || CONFIG_IA32_EMULATION */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * x86 FPU boot time init code:

/*

 * Initialize the registers found in all CPUs, CR0 and CR4:

 clear TS and EM */

 Flush out any pending x87 state: */

/*

 * Enable all supported FPU features. Called when a CPU is brought online:

/*

 * Boot time FPU feature detection code:

 Static because GCC does not get 16-byte stack alignment right: */

		/*

		 * If zero then use the default features mask,

		 * which has all features set, except the

		 * denormals-are-zero feature bit:

/*

 * Once per bootup FPU initialization sequences that will run on most x86 CPUs:

	/*

	 * Set up the legacy init FPU context. Will be updated when the

	 * CPU supports XSAVE[S].

 Get alignment of the TYPE. */

/*

 * Enforce that 'MEMBER' is the last field of 'TYPE'.

 *

 * Align the computed size with alignment of the TYPE,

 * because that's how C aligns structs.

/*

 * We append the 'struct fpu' to the task_struct:

	/*

	 * Subtract off the static size of the register state.

	 * It potentially has a bunch of padding.

	/*

	 * Add back the dynamically-calculated register state

	 * size.

	/*

	 * We dynamically size 'struct fpu', so we require that

	 * it be at the end of 'thread_struct' and that

	 * 'thread_struct' be at the end of 'task_struct'.  If

	 * you hit a compile error here, check the structure to

	 * see if something got added to the end.

/*

 * Set up the user and kernel xstate sizes based on the legacy FPU context size.

 *

 * We set this up first, and later it will be overwritten by

 * fpu__init_system_xstate() if the CPU knows about xstates.

	/*

	 * Note that the size configuration might be overwritten later

	 * during fpu__init_system_xstate().

 Bring init_fpstate size and features up to date */

/*

 * Called on the boot CPU once per system bootup, to set up the initial

 * FPU state that is later cloned into all processes:

	/*

	 * The FPU has to be operational for some of the

	 * later FPU init activities:

 SPDX-License-Identifier: GPL-2.0

/*

 * x86 FPU bug checks:

/*

 * Boot time CPU/FPU FDIV bug detection code:

/*

 * This used to check for exceptions..

 * However, it turns out that to support that,

 * the XMM trap handlers basically had to

 * be buggy. So let's have a correct XMM trap

 * handler, and forget about printing out

 * some status at boot.

 *

 * We should really only care about bugs here

 * anyway. Not features.

 kernel_fpu_begin/end() relies on patched alternative instructions. */

	/*

	 * trap_init() enabled FXSR and company _before_ testing for FP

	 * problems here.

	 *

	 * Test for the divl bug: http://en.wikipedia.org/wiki/Fdiv_bug

 SPDX-License-Identifier: GPL-2.0

/*

 * FPU signal frame handling routines.

/*

 * Check for the presence of extended state information in the

 * user fpstate pointer in the sigcontext.

 Check for the first magic field and other error scenarios. */

	/*

	 * Check for the presence of second magic word at the end of memory

	 * layout. This detects the case where the user just copied the legacy

	 * fpstate layout with out copying the extended state information

	 * in the memory layout.

 Set the parameters for fx only state */

/*

 * Signal frame handlers.

/*

 * Prepare the SW reserved portion of the fxsave memory layout, indicating

 * the presence of the extended state information in the memory layout

 * pointed to by the fpstate pointer in the sigcontext.

 * This is saved when ever the FP and extended state context is

 * saved on the user stack during the signal handler delivery to the user.

 Setup the bytes not touched by the [f]xsave and reserved for SW. */

	/*

	 * Read the xfeatures which we copied (directly from the cpu or

	 * from the state in task struct) to the user buffers.

	/*

	 * For legacy compatible, we always set FP/SSE bits in the bit

	 * vector while saving the state to the user context. This will

	 * enable us capturing any changes(during sigreturn) to

	 * the FP/SSE bits by the legacy applications which don't touch

	 * xfeatures in the xsave header.

	 *

	 * xsave aware apps can change the xfeatures in the xsave

	 * header as well as change any contents in the memory layout.

	 * xrestore as part of sigreturn will capture all the changes.

/*

 * Save the fpu, extended register state to the user signal frame.

 *

 * 'buf_fx' is the 64-byte aligned pointer at which the [f|fx|x]save

 *  state is copied.

 *  'buf' points to the 'buf_fx' or to the fsave header followed by 'buf_fx'.

 *

 *	buf == buf_fx for 64-bit frames and 32-bit fsave frame.

 *	buf != buf_fx for 32-bit frames with fxstate.

 *

 * Save it directly to the user frame with disabled page fault handler. If

 * that faults, try to clear the frame which handles the page fault.

 *

 * If this is a 32-bit frame with fxstate, put a fsave header before

 * the aligned state at 'buf_fx'.

 *

 * For [f]xsave state, update the SW reserved fields in the [f]xsave frame

 * indicating the absence/presence of the extended state to the user.

		/*

		 * Clear the xsave header first, so that reserved fields are

		 * initialized to zero.

	/*

	 * Load the FPU registers if they are not valid for the current task.

	 * With a valid FPU state we can attempt to save the state directly to

	 * userland's stack frame which will likely succeed. If it does not,

	 * resolve the fault in the user memory and try again.

 Save the fsave header for the 32-bit frames. */

/*

 * Attempt to restore the FPU registers directly from user memory.

 * Pagefaults are handled and any errors returned are fatal.

 Ensure that XFD is up to date */

		/*

		 * The above did an FPU restore operation, restricted to

		 * the user portion of the registers, and failed, but the

		 * microcode might have modified the FPU registers

		 * nevertheless.

		 *

		 * If the FPU registers do not belong to current, then

		 * invalidate the FPU register state otherwise the task

		 * might preempt current and return to user space with

		 * corrupted FPU registers.

 Try to handle #PF, but anything else is fatal. */

	/*

	 * Restore supervisor states: previous context switch etc has done

	 * XSAVES and saved the supervisor states in the kernel buffer from

	 * which they can be restored now.

	 *

	 * It would be optimal to handle this with a single XRSTORS, but

	 * this does not work because the rest of the FPU registers have

	 * been restored from a user buffer directly.

 Restore the FPU registers directly from user memory. */

	/*

	 * Copy the legacy state because the FP portion of the FX frame has

	 * to be ignored for histerical raisins. The legacy state is folded

	 * in once the larger state has been copied.

	/*

	 * By setting TIF_NEED_FPU_LOAD it is ensured that our xstate is

	 * not modified on context switch and that the xstate is considered

	 * to be loaded again on return to userland (overriding last_cpu avoids

	 * the optimisation).

		/*

		 * If supervisor states are available then save the

		 * hardware state in current's fpstate so that the

		 * supervisor state is preserved. Save the full state for

		 * simplicity. There is no point in optimizing this by only

		 * saving the supervisor states and then shuffle them to

		 * the right place in memory. It's ia32 mode. Shrug.

 Reject invalid MXCSR values. */

 Mask invalid bits out for historical reasons (broken hardware). */

 Enforce XFEATURE_MASK_FPSSE when XSAVE is enabled */

 Fold the legacy FP storage */

		/*

		 * Remove all UABI feature bits not set in user_xfeatures

		 * from the memory xstate header which makes the full

		 * restore below bring them into init state. This works for

		 * fx_only mode as well because that has only FP and SSE

		 * set in user_xfeatures.

		 *

		 * Preserve supervisor states!

/*

 * Restore FPU state from a sigframe:

	/*

	 * Only FXSR enabled systems need the FX state quirk.

	 * FRSTOR does not need it and can use the fast path.

	/*

	 * This space is needed on (most) 32-bit kernels, or when a 32-bit

	 * app is running on a 64-bit kernel. To keep things simple, just

	 * assume the worst case and always include space for 'freg_state',

	 * even for 64-bit apps on 64-bit kernels. This wastes a bit of

	 * space, but keeps the code simple.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (C) 1994 Linus Torvalds

 *

 *  Pentium III FXSR, SSE support

 *  General FPU state handling cleanups

 *	Gareth Hughes <gareth@valinux.com>, May 2000

 The FPU state configuration data for kernel and user space */

/*

 * Represents the initial FPU state. It's mostly (but not completely) zeroes,

 * depending on the FPU hardware format:

/*

 * Track whether the kernel is using the FPU state

 * currently.

 *

 * This flag is used:

 *

 *   - by IRQ context code to potentially use the FPU

 *     if it's unused.

 *

 *   - to debug kernel_fpu_begin()/end() correctness

/*

 * Track which context is using the FPU on the CPU:

/*

 * Were we in user mode (or vm86 mode) when we were

 * interrupted?

 *

 * Doing kernel_fpu_begin/end() is ok if we are running

 * in an interrupt context from user mode - we'll just

 * save the FPU state as required.

/*

 * Can we use the FPU in kernel mode with the

 * whole "kernel_fpu_begin/end()" sequence?

 *

 * It's always ok in process context (ie "not interrupt")

 * but it is sometimes ok even from an irq.

/*

 * Save the FPU register state in fpu->fpstate->regs. The register state is

 * preserved.

 *

 * Must be called with fpregs_lock() held.

 *

 * The legacy FNSAVE instruction clears all FPU state unconditionally, so

 * register state has to be reloaded. That might be a pointless exercise

 * when the FPU is going to be used by another task right after that. But

 * this only affects 20+ years old 32bit systems and avoids conditionals all

 * over the place.

 *

 * FXSAVE and all XSAVE variants preserve the FPU register state.

		/*

		 * AVX512 state is tracked here because its use is

		 * known to slow the max clock speed of the core.

	/*

	 * Legacy FPU register saving, FNSAVE always clears FPU registers,

	 * so we have to reload them from the memory state.

	/*

	 * AMD K7/K8 and later CPUs up to Zen don't save/restore

	 * FDP/FIP/FOP unless an exception is pending. Clear the x87 state

	 * here by setting it to fixed values.  "m" is a random variable

	 * that should be in L1.

 set F?P to defined value */

		/*

		 * Dynamically enabled features are enabled in XCR0, but

		 * usage requires also that the corresponding bits in XFD

		 * are cleared.  If the bits are set then using a related

		 * instruction will raise #NM. This allows to do the

		 * allocation of the larger FPU buffer lazy from #NM or if

		 * the task has no permission to kill it which would happen

		 * via #UD if the feature is disabled in XCR0.

		 *

		 * XFD state is following the same life time rules as

		 * XSTATE and to restore state correctly XFD has to be

		 * updated before XRSTORS otherwise the component would

		 * stay in or go into init state even if the bits are set

		 * in fpstate::regs::xsave::xfeatures.

		/*

		 * Restoring state always needs to modify all features

		 * which are in @mask even if the current task cannot use

		 * extended features.

		 *

		 * So fpstate->xfeatures cannot be used here, because then

		 * a feature for which the task has no permission but was

		 * used by the previous task would not go into init state.

 Swap fpstate */

 Includes XFD update */

		/*

		 * XSTATE is restored by firmware from encrypted

		 * memory. Make sure XFD state is correct while

		 * running with guest fpstate

 Make it restorable on a XSAVE enabled host */

 Retrieve PKRU if not in init state */

 Ensure that XCOMP_BV is set up for XSAVES */

 CONFIG_KVM */

 Put sane initial values into the control registers. */

/*

 * Sync the FPU register state to current's memory register state when the

 * current task owns the FPU. The hardware register state is preserved.

 XSAVE(S) just needs the legacy and the xstate header part */

/*

 * Legacy x87 fpstate state init:

/*

 * Used in two places:

 * 1) Early boot to setup init_fpstate for non XSAVE systems

 * 2) fpu_init_fpstate_user() which is invoked from KVM

 Initialize sizes and feature masks */

 Set the fpstate pointer to the default fpstate */

 Initialize the permission related info in fpu */

 Fork also inherits the permissions of the parent */

 Clone current's FPU state on fork */

 The new task's FPU state cannot be valid in the hardware. */

	/*

	 * Enforce reload for user space tasks and prevent kernel threads

	 * from trying to save the FPU registers on context switch.

	/*

	 * No FPU state inheritance for kernel threads and IO

	 * worker threads.

 Clear out the minimal state */

	/*

	 * If a new feature is added, ensure all dynamic features are

	 * caller-saved from here!

	/*

	 * Save the default portion of the current FPU state into the

	 * clone. Assume all dynamic features to be defined as caller-

	 * saved, which enables skipping both the expansion of fpstate

	 * and the copying of any dynamic state.

	 *

	 * Do not use memcpy() when TIF_NEED_FPU_LOAD is set because

	 * copying is not valid when current uses non-default states.

/*

 * Whitelist the FPU register state embedded into task_struct for hardened

 * usercopy.

/*

 * Drops current FPU state: deactivates the fpregs and

 * the fpstate. NOTE: it still leaves previous contents

 * in the fpregs in the eager-FPU case.

 *

 * This function can be used in cases where we know that

 * a state-restore is coming: either an explicit one,

 * or a reschedule.

 Ignore delayed exceptions from user space */

/*

 * Clear FPU registers by setting them up from the init fpstate.

 * Caller must do fpregs_[un]lock() around it.

/*

 * Reset current->fpu memory state to the init values.

	/*

	 * This does not change the actual hardware registers. It just

	 * resets the memory image and sets TIF_NEED_FPU_LOAD so a

	 * subsequent return to usermode will reload the registers from the

	 * task's memory image.

	 *

	 * Do not use fpstate_init() here. Just copy init_fpstate which has

	 * the correct content already except for PKRU.

	 *

	 * PKRU handling does not rely on the xstate when restoring for

	 * user space as PKRU is eagerly written in switch_to() and

	 * flush_thread().

/*

 * Reset current's user FPU states to the init states.  current's

 * supervisor states, if any, are not modified by this function.  The

 * caller guarantees that the XSTATE header in memory is intact.

	/*

	 * Ensure that current's supervisor states are loaded into their

	 * corresponding registers.

 Reset user states in registers. */

	/*

	 * Now all FPU registers have their desired values.  Inform the FPU

	 * state machine that current's FPU registers are in the hardware

	 * registers. The memory image does not need to be updated because

	 * any operation relying on it has to save the registers first when

	 * current's FPU is marked active.

/*

 * Load FPU context before returning to userspace.

/*

 * If current FPU state according to its tracking (loaded FPU context on this

 * CPU) is not valid then we must have TIF_NEED_FPU_LOAD set so the context is

 * loaded on return to userland.

/*

 * x87 math exception handling:

		/*

		 * (~cwd & swd) will mask out exceptions that are not set to unmasked

		 * status.  0x3f is the exception bits in these regs, 0x200 is the

		 * C1 reg you need in case of a stack fault, 0x040 is the stack

		 * fault bit.  We should only be taking one exception at a time,

		 * so if this combination doesn't produce any single exception,

		 * then we have a bad program that isn't synchronizing its FPU usage

		 * and it will suffer the consequences since we won't be able to

		 * fully reproduce the context of the exception.

		/*

		 * The SIMD FPU exceptions are handled a little differently, as there

		 * is only a single status/control register.  Thus, to determine which

		 * unmasked exception was caught we must mask the exception mask bits

		 * at 0x1f80, and then use these to mask the exception bits at 0x3f.

 Invalid op */

		/*

		 * swd & 0x240 == 0x040: Stack Underflow

		 * swd & 0x240 == 0x240: Stack Overflow

		 * User must clear the SF bit (0x40) if set

 Divide by Zero */

 Overflow */

 Denormal, Underflow */

 Precision */

	/*

	 * If we're using IRQ 13, or supposedly even some trap

	 * X86_TRAP_MF implementations, it's possible

	 * we get a spurious trap, which is not an error.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * xsave/xrstor support.

 *

 * Author: Suresh Siddha <suresh.b.siddha@intel.com>

/*

 * Although we spell it out in here, the Processor Trace

 * xfeature is completely unused.  We use other mechanisms

 * to save/restore PT state in Linux.

/*

 * Return whether the system supports a given xfeature.

 *

 * Also return the name of the (most advanced) feature that the caller requested:

		/*

		 * So we use FLS here to be able to print the most advanced

		 * feature that was requested but is missing. So if a driver

		 * asks about "XFEATURE_MASK_SSE | XFEATURE_MASK_YMM" we'll print the

		 * missing AVX feature - this is the most informative message

		 * to users:

	/*

	 * Extended State Enumeration Sub-leaves (EAX = 0DH, ECX = n, n > 1)

	 * returns ECX[0] set to (1) for a supervisor state, and cleared (0)

	 * for a user state.

/*

 * Enable the extended processor state save/restore feature.

 * Called once per CPU onlining.

	/*

	 * Must happen after CR4 setup and before xsetbv() to allow KVM

	 * lazy passthrough.  Write independent of the dynamic state static

	 * key as that does not work on the boot CPU. This also ensures

	 * that any stale state is wiped out from XFD.

	/*

	 * XCR_XFEATURE_ENABLED_MASK (aka. XCR0) sets user features

	 * managed by XSAVE{C, OPT, S} and XRSTOR{S}.  Only XSAVE user

	 * states can be set here.

	/*

	 * MSR_IA32_XSS sets supervisor states managed by XSAVES.

/*

 * Record the offsets and sizes of various xstates contained

 * in the XSAVE state memory layout.

 start at the beginning of the "extended state" */

	/*

	 * The FP xstates and SSE xstates are legacy states. They are always

	 * in the fixed offsets in the xsave area in either compacted form

	 * or standard form.

		/*

		 * If an xfeature is supervisor state, the offset in EBX is

		 * invalid, leave it to -1.

		/*

		 * In our xstate size checks, we assume that the highest-numbered

		 * xstate feature has the highest offset in the buffer.  Ensure

		 * it does.

/*

 * Print out all the supported xstate features:

/*

 * This check is important because it is easy to get XSTATE_*

 * confused with XSTATE_BIT_*.

/*

 * We could cache this like xstate_size[], but we only use

 * it here, so it would be a waste of space.

	/*

	 * The value returned by ECX[1] indicates the alignment

	 * of state component 'i' when the compacted format

	 * of the extended region of an XSAVE area is used:

/*

 * This function sets up offsets and sizes of all extended states in

 * xsave area. This supports both standard format and compacted format

 * of the xsave area.

	/*

	 * The FP xstates and SSE xstates are legacy states. They are always

	 * in the fixed offsets in the xsave area in either compacted form

	 * or standard form.

/*

 * Setup offsets of a supervisor-state-only XSAVES buffer:

 *

 * The offsets stored in xstate_comp_offsets[] only work for one specific

 * value of the Requested Feature BitMap (RFBM).  In cases where a different

 * RFBM value is used, a different set of offsets is required.  This set of

 * offsets is for when RFBM=xfeatures_mask_supervisor().

/*

 * Print out xstate component offsets and sizes

/*

 * This function is called only during boot time when x86 caps are not set

 * up and alternative can not be used yet.

	/*

	 * We should never fault when copying from a kernel buffer, and the FPU

	 * state we set at boot time should be valid.

/*

 * All supported features have either init state all zeros or are

 * handled in setup_init_fpu() individually. This is an explicit

 * feature list and does not use XFEATURE_MASK*SUPPORTED to catch

 * newly added supported features at build time and make people

 * actually look at the init state for the new feature.

/*

 * setup the xstate image representing the init state

	/*

	 * Init all the features state with header.xfeatures being 0x0

	/*

	 * All components are now in init state. Read the state back so

	 * that init_fpstate contains all non-zero init state. This only

	 * works with XSAVE, but not with XSAVEOPT and XSAVES because

	 * those use the init optimization which skips writing data for

	 * components in init state.

	 *

	 * XSAVE could be used, but that would require to reshuffle the

	 * data when XSAVES is available because XSAVES uses xstate

	 * compaction. But doing so is a pointless exercise because most

	 * components have an all zeros init state except for the legacy

	 * ones (FP and SSE). Those can be saved with FXSAVE into the

	 * legacy area. Adding new features requires to ensure that init

	 * state is all zeroes or if not to add the necessary handling

	 * here.

	/*

	 * Only XSAVES supports supervisor states and it uses compacted

	 * format. Checking a supervisor state's uncompacted offset is

	 * an error.

 Validate an xstate header supplied by userspace (ptrace or sigreturn) */

 No unknown or supervisor features may be set */

 Userspace must use the uncompacted format */

	/*

	 * If 'reserved' is shrunken to add a new field, make sure to validate

	 * that new field here!

 No reserved bits may be set */

	/*

	 * Dump out a few leaves past the ones that we support

	 * just in case there are some goodies up there

/**

 * check_xtile_data_against_struct - Check tile data state size.

 *

 * Calculate the state size by multiplying the single tile size which is

 * recorded in a C struct, and the number of tiles that the CPU informs.

 * Compare the provided size with the calculation.

 *

 * @size:	The tile data state size

 *

 * Returns:	0 on success, -EINVAL on mismatch.

	/*

	 * Check the maximum palette id:

	 *   eax: the highest numbered palette subleaf.

	/*

	 * Cross-check each tile size and find the maximum number of

	 * supported tiles.

		/*

		 * Check the tile size info:

		 *   eax[31:16]:  bytes per title

		 *   ebx[31:16]:  the max names (or max number of tiles)

/*

 * We have a C struct for each 'xstate'.  We need to ensure

 * that our software representation matches what the CPU

 * tells us about the state's size.

	/*

	 * Ask the CPU for the size of the state.

	/*

	 * Match each CPU state with the corresponding software

	 * structure.

 The tile data size varies between implementations. */

	/*

	 * Make *SURE* to add any feature numbers in below if

	 * there are "holes" in the xsave state component

	 * numbers.

 Align from the end of the previous feature */

		/*

		 * In compacted format the enabled features are packed,

		 * i.e. disabled features do not occupy space.

		 *

		 * In non-compacted format the offsets are fixed and

		 * disabled states still occupy space in the memory buffer.

		/*

		 * Add the feature size even for non-compacted format

		 * to make the end result correct

/*

 * This essentially double-checks what the cpu told us about

 * how large the XSAVE buffer needs to be.  We are recalculating

 * it to be safe.

 *

 * Independent XSAVE features allocate their own buffers and are not

 * covered by these checks. Only the size of the buffer for task->fpu

 * is checked here.

		/*

		 * Supervisor state components can be managed only by

		 * XSAVES.

/*

 * Get total size of enabled xstates in XCR0 | IA32_XSS.

 *

 * Note the SDM's wording here.  "sub-function 0" only enumerates

 * the size of the *user* states.  If we use it to size a buffer

 * that we use 'XSAVES' on, we could potentially overflow the

 * buffer because 'XSAVES' saves system states too.

	/*

	 * - CPUID function 0DH, sub-function 1:

	 *    EBX enumerates the size (in bytes) required by

	 *    the XSAVES instruction for an XSAVE area

	 *    containing all the state components

	 *    corresponding to bits currently set in

	 *    XCR0 | IA32_XSS.

/*

 * Get the total size of the enabled xstates without the independent supervisor

 * features.

 Disable independent features. */

	/*

	 * Ask the hardware what size is required of the buffer.

	 * This is the size required for the task->fpu buffer.

 Re-enable independent features so XSAVES will work on them again. */

	/*

	 * - CPUID function 0DH, sub-function 0:

	 *    EBX enumerates the size (in bytes) required by

	 *    the XSAVE instruction for an XSAVE area

	 *    containing all the *user* state components

	 *    corresponding to bits currently set in XCR0.

/*

 * Will the runtime-enumerated 'xstate_size' fit in the init

 * task's statically-allocated buffer?

 Recompute the context size for enabled features: */

 Uncompacted user space size */

	/*

	 * XSAVES kernel size includes supervisor states and

	 * uses compacted format when available.

	 *

	 * XSAVE does not support supervisor states so

	 * kernel and user size is identical.

 Ensure we have the space to store all default enabled features. */

/*

 * We enabled the XSAVE hardware, but something went wrong and

 * we can not use it.  Disable it.

 Restore the legacy size.*/

	/*

	 * Prevent enabling the static branch which enables writes to the

	 * XFD MSR.

/*

 * Enable and initialize the xsave feature.

 * Called once per system bootup.

	/*

	 * Find user xstates supported by the processor.

	/*

	 * Find supervisor xstates supported by the processor.

		/*

		 * This indicates that something really unexpected happened

		 * with the enumeration.  Disable XSAVE and try to continue

		 * booting without it.  This is too early to BUG().

	/*

	 * Clear XSAVE features that are disabled in the normal CPUID.

 Careful: X86_FEATURE_FPU is 0! */

 Clean out dynamic features from default */

 Store it for paranoia check at the end */

	/*

	 * Initialize the default XFD state in initfp_state and enable the

	 * dynamic sizing mechanism if dynamic states are available.  The

	 * static key cannot be enabled here because this runs before

	 * jump_label_init(). This is delayed to an initcall.

 Enable xstate instructions to be able to continue with initialization: */

 Reset the state for the current task */

	/*

	 * Update info used for ptrace frames; use standard-format size and no

	 * supervisor xstates:

	/*

	 * Paranoia check whether something in the setup modified the

	 * xfeatures mask.

 something went wrong, try to boot without any XSAVE support */

/*

 * Restore minimal FPU state after suspend:

	/*

	 * Restore XCR0 on xsave capable CPUs:

	/*

	 * Restore IA32_XSS. The same CPUID bit enumerates support

	 * of XSAVES and MSR_IA32_XSS.

/*

 * Given an xstate feature nr, calculate where in the xsave

 * buffer the state is.  Callers should ensure that the buffer

 * is valid.

/*

 * Given the xsave area and a state inside, this function returns the

 * address of the state.

 *

 * This is the API that is called to get xstate address in either

 * standard format or compacted format of xsave area.

 *

 * Note that if there is no data for the field in the xsave buffer

 * this will return NULL.

 *

 * Inputs:

 *	xstate: the thread's storage area for all FPU data

 *	xfeature_nr: state which is defined in xsave.h (e.g. XFEATURE_FP,

 *	XFEATURE_SSE, etc...)

 * Output:

 *	address of the state in the xsave area, or NULL if the

 *	field is not present in the xsave buffer.

	/*

	 * Do we even *have* xsave state?

	/*

	 * We should not ever be requesting features that we

	 * have not enabled.

	/*

	 * This assumes the last 'xsave*' instruction to

	 * have requested that 'xfeature_nr' be saved.

	 * If it did not, we might be seeing and old value

	 * of the field in the buffer.

	 *

	 * This can happen because the last 'xsave' did not

	 * request that this feature be saved (unlikely)

	 * or because the "init optimization" caused it

	 * to not be saved.

/*

 * This will go out and modify PKRU register to set the access

 * rights for @pkey to @init_val.

	/*

	 * This check implies XSAVE support.  OSPKE only gets

	 * set if we enable XSAVE and we enable PKU in XCR0.

	/*

	 * This code should only be called with valid 'pkey'

	 * values originating from in-kernel users.  Complain

	 * if a bad value is observed.

 Set the bits we need in PKRU:  */

 Shift the bits in to the correct place in PKRU for pkey: */

 Get old PKRU and mask off any old bits in place: */

 Write old part along with new part: */

 ! CONFIG_ARCH_HAS_PKEYS */

/**

 * __copy_xstate_to_uabi_buf - Copy kernel saved xstate to a UABI buffer

 * @to:		membuf descriptor

 * @fpstate:	The fpstate buffer from which to copy

 * @pkru_val:	The PKRU value to store in the PKRU component

 * @copy_mode:	The requested copy mode

 *

 * Converts from kernel XSAVE or XSAVES compacted format to UABI conforming

 * format, i.e. from the kernel internal hardware dependent storage format

 * to the requested @mode. UABI XSTATE is always uncompacted!

 *

 * It supports partial copy but @to.pos always starts from zero.

 Mask out the feature bits depending on copy mode */

 Copy FP state up to MXCSR */

 Copy MXCSR when SSE or YMM are set in the feature mask */

 Copy the remaining FP state */

 Copy the SSE state - shared with YMM, but independently managed */

 Zero the padding area */

 Copy xsave->i387.sw_reserved */

 Copy the user space relevant state of @xsave->header */

	/*

	 * The ptrace buffer is in non-compacted XSAVE format.  In

	 * non-compacted format disabled features still occupy state space,

	 * but there is no state to copy from in the compacted

	 * init_fpstate. The gap tracking will zero these states.

		/*

		 * If there was a feature or alignment gap, zero the space

		 * in the destination buffer.

			/*

			 * PKRU is not necessarily up to date in the

			 * XSAVE buffer. Use the provided value.

		/*

		 * Keep track of the last copied state in the non-compacted

		 * target buffer for gap zeroing.

/**

 * copy_xstate_to_uabi_buf - Copy kernel saved xstate to a UABI buffer

 * @to:		membuf descriptor

 * @tsk:	The task from which to copy the saved xstate

 * @copy_mode:	The requested copy mode

 *

 * Converts from kernel XSAVE or XSAVES compacted format to UABI conforming

 * format, i.e. from the kernel internal hardware dependent storage format

 * to the requested @mode. UABI XSTATE is always uncompacted!

 *

 * It supports partial copy but @to.pos always starts from zero.

 Validate MXCSR when any of the related features is in use */

 Reserved bits in MXCSR must be zero. */

 SSE and YMM require MXCSR even when FP is not in use. */

	/*

	 * The state that came in from userspace was user-state only.

	 * Mask all the user states out of 'xfeatures':

	/*

	 * Add back in the features that came in from userspace:

/*

 * Convert from a ptrace standard-format kernel buffer to kernel XSAVE[S]

 * format and copy to the target thread. Used by ptrace and KVM.

/*

 * Convert from a sigreturn standard-format user-space buffer to kernel

 * XSAVE[S] format and copy to the target thread. This is called from the

 * sigreturn() and rt_sigreturn() system calls.

/**

 * xsaves - Save selected components to a kernel xstate buffer

 * @xstate:	Pointer to the buffer

 * @mask:	Feature mask to select the components to save

 *

 * The @xstate buffer must be 64 byte aligned and correctly initialized as

 * XSAVES does not write the full xstate header. Before first use the

 * buffer should be zeroed otherwise a consecutive XRSTORS from that buffer

 * can #GP.

 *

 * The feature mask must be a subset of the independent features.

/**

 * xrstors - Restore selected components from a kernel xstate buffer

 * @xstate:	Pointer to the buffer

 * @mask:	Feature mask to select the components to restore

 *

 * The @xstate buffer must be 64 byte aligned and correctly initialized

 * otherwise XRSTORS from that buffer can #GP.

 *

 * Proper usage is to restore the state which was saved with

 * xsaves() into @xstate.

 *

 * The feature mask must be a subset of the independent features.

/*

 * Ensure that a subsequent XSAVE* or XRSTOR* instruction with RFBM=@mask

 * can safely operate on the @fpstate buffer.

	 /*

	  * The XFD MSR does not match fpstate->xfd. That's invalid when

	  * the passed in fpstate is current's fpstate.

	/*

	 * XRSTOR(S) from init_fpstate are always correct as it will just

	 * bring all components into init state and not read from the

	 * buffer. XSAVE(S) raises #PF after init.

	/*

	 * XSAVE(S): clone(), fpu_swap_kvm_fpu()

	 * XRSTORS(S): fpu_swap_kvm_fpu()

	/*

	 * No XSAVE/XRSTOR instructions (except XSAVE itself) touch

	 * the buffer area for XFD-disabled state components.

	/*

	 * Remove features which are valid in fpstate. They

	 * have space allocated in fpstate.

	/*

	 * Any remaining state components in 'mask' might be written

	 * by XSAVE/XRSTOR. Fail validation it found.

 CONFIG_X86_DEBUG_FPU */

	/*

	 * If init_fpstate.xfd has bits set then dynamic features are

	 * available and the dynamic sizing must be enabled.

/**

 * fpu_install_fpstate - Update the active fpstate in the FPU

 *

 * @fpu:	A struct fpu * pointer

 * @newfps:	A struct fpstate * pointer

 *

 * Returns:	A null pointer if the last active fpstate is the embedded

 *		one or the new fpstate is already installed;

 *		otherwise, a pointer to the old fpstate which has to

 *		be freed by the caller.

/**

 * fpstate_realloc - Reallocate struct fpstate for the requested new features

 *

 * @xfeatures:	A bitmap of xstate features which extend the enabled features

 *		of that task

 * @ksize:	The required size for the kernel buffer

 * @usize:	The required size for user space buffers

 *

 * Note vs. vmalloc(): If the task with a vzalloc()-allocated buffer

 * terminates quickly, vfree()-induced IPIs may be a concern, but tasks

 * with large states are likely to live longer.

 *

 * Returns: 0 on success, -ENOMEM on allocation error.

	/*

	 * Ensure that the current state is in the registers before

	 * swapping fpstate as that might invalidate it due to layout

	 * changes.

 Do the final updates within the locked region */

 get_sigframe_size() is based on fpu_user_cfg.max_size */

	/*

	 * This deliberately does not exclude !XSAVES as we still might

	 * decide to optionally context switch XCR0 or talk the silicon

	 * vendors into extending XFD for the pre AMX states, especially

	 * AVX512.

 Check whether fully enabled */

 Calculate the resulting kernel state size */

 Calculate the resulting user state size */

 Pairs with the READ_ONCE() in xstate_get_group_perm() */

 Protected by sighand lock */

/*

 * Permissions array to map facilities with more than one component

	/*

	 * Look up the facility mask which can require more than

	 * one xstate component.

 Lockless quick check */

 Protect against concurrent modifications */

 Protect against concurrent modifications */

 If not permitted let it die */

	/*

	 * The feature is permitted. State size is sufficient.  Dropping

	 * the lock is safe here even if more features are added from

	 * another task, the retrieved buffer sizes are valid for the

	 * currently requested feature(s).

	/*

	 * Try to allocate a new fpstate. If that fails there is no way

	 * out.

 CONFIG_X86_64 */

 !CONFIG_X86_64 */

/**

 * fpu_xstate_prctl - xstate permission operations

 * @tsk:	Redundant pointer to current

 * @option:	A subfunction of arch_prctl()

 * @arg2:	option argument

 * Return:	0 if successful; otherwise, an error code

 *

 * Option arguments:

 *

 * ARCH_GET_XCOMP_SUPP: Pointer to user space u64 to store the info

 * ARCH_GET_XCOMP_PERM: Pointer to user space u64 to store the info

 * ARCH_REQ_XCOMP_PERM: Facility number requested

 *

 * For facilities which require more than one XSTATE component, the request

 * must be the highest state component number related to that facility,

 * e.g. for AMX which requires XFEATURE_XTILE_CFG(17) and

 * XFEATURE_XTILE_DATA(18) this would be XFEATURE_XTILE_DATA(18).

		/*

		 * Lockless snapshot as it can also change right after the

		 * dropping the lock.

/*

 * Report the amount of time elapsed in millisecond since last AVX512

 * use in the task.

		/*

		 * Report -1 if no AVX512 usage

		/*

		 * Cap to LONG_MAX if time difference > LONG_MAX

/*

 * Report architecture specific information

	/*

	 * Report AVX512 state if the processor and build option supported.

 CONFIG_PROC_PID_ARCH_STATUS */

 SPDX-License-Identifier: GPL-2.0

/*

 * NOOP APIC driver.

 *

 * Does almost nothing and should be substituted by a real apic driver via

 * probe routine.

 *

 * Though in case if apic is disabled (for some reason) we try

 * to not uglify the caller's code and allow to call (some) apic routines

 * like self-ipi, etc...

	/*

	 * NOOP apic should not ever be

	 * enabled via probe routine

	/*

	 * if we would be really "pedantic"

	 * we should pass read_apic_id() here

	 * but since NOOP suppose APIC ID = 0

	 * lets save a few cycles

 SPDX-License-Identifier: GPL-2.0-only

/*

 *	Local APIC handling, local APIC timers

 *

 *	(c) 1999, 2000, 2009 Ingo Molnar <mingo@redhat.com>

 *

 *	Fixes

 *	Maciej W. Rozycki	:	Bits for genuine 82489DX APICs;

 *					thanks to Eric Gilmore

 *					and Rolf G. Tews

 *					for testing these extensively.

 *	Maciej W. Rozycki	:	Various updates and fixes.

 *	Mikael Pettersson	:	Power Management for UP-APIC.

 *	Pavel Machek and

 *	Mikael Pettersson	:	PM converted to driver model.

 Processor that is doing the boot up */

/*

 * The highest APIC ID seen during enumeration.

/*

 * Bitmask of physically existing CPUs:

/*

 * Processor to be disabled specified by kernel parameter

 * disable_cpu_apicid=<int>, mostly used for the kdump 2nd kernel to

 * avoid undefined behaviour caused by sending INIT from AP to BSP.

/*

 * This variable controls which CPUs receive external NMIs.  By default,

 * external NMIs are delivered only to the BSP.

/*

 * Hypervisor supports 15 bits of APIC ID in MSI Extended Destination ID

/*

 * Map cpu index to physical APIC ID

/*

 * On x86_32, the mapping between cpu and logical apicid may vary

 * depending on apic in use.  The following early percpu variable is

 * used for the mapping.  This is where the behaviors of x86_64 and 32

 * actually diverge.  Let's keep it ugly for now.

 Local APIC was disabled by the BIOS and enabled by the kernel */

/*

 * Handle interrupt mode configuration register (IMCR).

 * This register controls whether the interrupt signals

 * that reach the BSP come from the master PIC or from the

 * local APIC. Before entering Symmetric I/O Mode, either

 * the BIOS or the operating system must switch out of

 * PIC Mode by changing the IMCR.

 NMI and 8259 INTR go through APIC */

 NMI and 8259 INTR go directly to BSP */

/*

 * Knob to control our willingness to enable the local APIC.

 *

 * +1=force-enable

/*

 * APIC command line parameters

 Disable local APIC timer from the kernel commandline or via dmi quirk */

 Local APIC timer works in C2 */

/*

 * Debug level, exported for io_apic.c

 Have we found an MP table */

/*

 * Get the LAPIC version

/*

 * Check, if the APIC is integrated or a separate chip

/*

 * Check, whether this is a modern or a first generation APIC

 AMD systems use old APIC versions, so check the CPU */

 Hygon systems use modern APIC */

/*

 * right after this call apic become NOOP driven

 * so apic->write/read doesn't do anything

/**

 * get_physical_broadcast - Get number of physical broadcast IDs

/**

 * lapic_get_maxlvt - get the maximum number of local vector table entries

	/*

	 * - we always have APIC integrated on 64bit mode

	 * - 82489DXs do not report # of LVT entries

/*

 * Local APIC timer

 Clock divisor */

/*

 * This function sets up the local APIC timer, with a timeout of

 * 'clocks' APIC bus clock. During calibration we actually call

 * this function twice on the boot CPU, once with a bogus timeout

 * value, second time for real. The other (noncalibrating) CPUs

 * call this function only once, with the real, calibrated value.

 *

 * We do reads before writes even if unnecessary, to get around the

 * P5 APIC double write bug.

		/*

		 * See Intel SDM: TSC-Deadline Mode chapter. In xAPIC mode,

		 * writing to the APIC LVTT and TSC_DEADLINE MSR isn't serialized.

		 * According to Intel, MFENCE can do the serialization here.

	/*

	 * Divide PICLK by 16

/*

 * Setup extended LVT, AMD specific

 *

 * Software should use the LVT offsets the BIOS provides.  The offsets

 * are determined by the subsystems using it like those for MCE

 * threshold or IBS.  On K8 only offset 0 (APIC500) and MCE interrupts

 * are supported. Beginning with family 10h at least 4 offsets are

 * available.

 *

 * Since the offsets must be consistent for all cores, we keep track

 * of the LVT offsets in software and reserve the offset for the same

 * vector also to be used on other cores. An offset is freed by

 * setting the entry to APIC_EILVT_MASKED.

 *

 * If the BIOS is right, there should be no conflicts. Otherwise a

 * "[Firmware Bug]: ..." error message is generated. However, if

 * software does not properly determines the offsets, it is not

 * necessarily a BIOS bug.

 0: unassigned */

 may not change if vectors are different */

/*

 * If mask=1, the LVT entry does not generate interrupts while mask=0

 * enables the vector. See also the BKDGs. Must be called with

 * preemption disabled.

/*

 * Program the next event, relative to now

 This MSR is special and need a special fence: */

 Lapic used as dummy for broadcast ? */

 Lapic used as dummy for broadcast ? */

/*

 * Local APIC timer broadcast function

/*

 * The local apic timer can be used for any function which is CPU local.

 EP */

 EX */

/*

 * Setup the local APIC timer for this CPU. Copy the initialized values

 * of the boot CPU and register the clock event in the framework.

 Make LAPIC timer preferable over percpu HPET */

/*

 * Install the updated TSC frequency from recalibration at the TSC

 * deadline clockevent devices.

	/*

	 * The clockevent device's ->mult and ->shift can both be

	 * changed. In order to avoid races, schedule the frequency

	 * update code on each CPU.

/*

 * In this functions we calibrate APIC bus clocks to the external timer.

 *

 * We want to do the calibration only once since we want to have local timer

 * irqs synchronous. CPUs connected by the same APIC bus have the very same bus

 * frequency.

 *

 * This was previously done by reading the PIT/HPET and waiting for a wrap

 * around to find out, that a tick has elapsed. I have a box, where the PIT

 * readout is broken, so it never gets out of the wait loop again. This was

 * also reported by others.

 *

 * Monitoring the jiffies value is inaccurate and the clockevents

 * infrastructure allows us to do a simple substitution of the interrupt

 * handler.

 *

 * The calibration routine also uses the pm_timer when possible, as the PIT

 * happens to run way too slow (factor 2.3 on my VAIO CoreDuo, which goes

 * back to normal later in the boot process).

/*

 * Temporary interrupt handler and polled calibration function.

 Check, if the PM timer is available */

 Correct the lapic counter value */

 Correct the tsc counter value */

 Calculate the scaled math multiplication factor */

	/*

	 * If the frequencies are not known, PIT is required for both TSC

	 * and apic timer calibration.

 Is there an APIC at all or is it disabled? */

	/*

	 * If interrupt delivery mode is legacy PIC or virtual wire without

	 * configuration, the local APIC timer wont be set up. Make sure

	 * that the PIT is initialized.

 Virt guests may lack ARAT, but still have DEADLINE */

 Deadline timer is based on TSC so no further PIT action required */

 APIC timer disabled? */

	/*

	 * The APIC timer frequency is known already, no PIT calibration

	 * required. If unknown, let the PIT be initialized.

	/*

	 * Check if lapic timer has already been calibrated by platform

	 * specific routine, such as tsc calibration code. If so just fill

	 * in the clockevent structure and return.

		/*

		 * Direct calibration methods must have an always running

		 * local APIC timer, no need for broadcast timer.

	/*

	 * There are platforms w/o global clockevent devices. Instead of

	 * making the calibration conditional on that, use a polling based

	 * approach everywhere.

	/*

	 * Setup the APIC counter to maximum. There is no way the lapic

	 * can underflow in the 100ms detection time frame

	/*

	 * Methods to terminate the calibration loop:

	 *  1) Global clockevent if available (jiffies)

	 *  2) TSC if available and frequency is known

	/*

	 * Enable interrupts so the tick can fire, if a global

	 * clockevent device is available

 Wait for a tick to elapse */

 Invoke the calibration routine */

 Build delta t1-t2 as apic timer counts down */

 we trust the PM based calibration if possible */

	/*

	 * Do a sanity check on the APIC calibration result

	/*

	 * PM timer calibration failed or not turned on so lets try APIC

	 * timer based calibration, if a global clockevent device is

	 * available.

		/*

		 * Setup the apic timer manually

 Let the interrupts run */

 Stop the lapic timer */

 Jiffies delta */

 Check, if the jiffies result is consistent */

/*

 * Setup the boot APIC

 *

 * Calibrate and verify the result.

	/*

	 * The local apic timer can be disabled via the kernel

	 * commandline or from the CPU detection code. Register the lapic

	 * timer as a dummy clock event source on SMP systems, so the

	 * broadcast mechanism is used. On UP systems simply ignore it.

 No broadcast on UP ! */

 No broadcast on UP ! */

	/*

	 * If nmi_watchdog is set to IO_APIC, we need the

	 * PIT/HPET going.  Otherwise register lapic as a dummy

	 * device.

 Setup the lapic or request the broadcast */

/*

 * The guts of the apic timer interrupt

	/*

	 * Normally we should not be here till LAPIC has been initialized but

	 * in some cases like kdump, its possible that there is a pending LAPIC

	 * timer interrupt from previous kernel's context and is delivered in

	 * new kernel the moment interrupts are enabled.

	 *

	 * Interrupts are enabled early and LAPIC is setup much later, hence

	 * its possible that when we get here evt->event_handler is NULL.

	 * Check for event_handler being NULL and discard the interrupt as

	 * spurious.

 Switch it off */

	/*

	 * the NMI deadlock-detector uses this.

/*

 * Local APIC timer interrupt. This is the most natural way for doing

 * local interrupts, but local timer interrupts can be emulated by

 * broadcast interrupts too. [in case the hw doesn't support APIC timers]

 *

 * [ if a single-CPU system runs an SMP kernel then we call the local

 *   interrupt as well. Thus we cannot inline the local irq ... ]

/*

 * Local APIC start and shutdown

/**

 * clear_local_APIC - shutdown the local APIC

 *

 * This is called, when a CPU is disabled and before rebooting, so the state of

 * the local APIC has no dangling leftovers. Also used to cleanout any BIOS

 * leftovers during boot.

 APIC hasn't been mapped yet */

	/*

	 * Masking an LVT entry can trigger a local APIC error

	 * if the vector is zero. Mask LVTERR first to prevent this.

 any non-zero vector will do */

	/*

	 * Careful: we have to set masks only first to deassert

	 * any level-triggered sources.

 lets not touch this if we didn't frob it */

	/*

	 * Clean APIC state for other OSs:

 Integrated APIC (!82489DX) ? */

 Clear ESR due to Pentium errata 3AP and 11AP */

/**

 * apic_soft_disable - Clears and software disables the local APIC on hotplug

 *

 * Contrary to disable_local_APIC() this does not touch the enable bit in

 * MSR_IA32_APICBASE. Clearing that bit on systems based on the 3 wire APIC

 * bus would require a hardware reset as the APIC would lose track of bus

 * arbitration. On systems with FSB delivery APICBASE could be disabled,

 * but it has to be guaranteed that no interrupt is sent to the APIC while

 * in that state and it's not clear from the SDM whether it still responds

 * to INIT/SIPI messages. Stay on the safe side and use software disable.

 Soft disable APIC (implies clearing of registers for 82489DX!). */

/**

 * disable_local_APIC - clear and disable the local APIC

 APIC hasn't been mapped yet */

	/*

	 * When LAPIC was disabled by the BIOS and enabled by the kernel,

	 * restore the disabled state.

/*

 * If Linux enabled the LAPIC against the BIOS default disable it down before

 * re-entering the BIOS on shutdown.  Otherwise the BIOS may get confused and

 * not power-off.  Additionally clear all LVT entries before disable_local_APIC

 * for the case where Linux didn't enable the LAPIC.

/**

 * sync_Arb_IDs - synchronize APIC bus arbitration IDs

	/*

	 * Unsupported on P4 - see Intel Dev. Manual Vol. 3, Ch. 8.6.1 And not

	 * needed on AMD.

	/*

	 * Wait for idle.

 Check kernel option */

 Check BIOS */

 On 64-bit, the APIC must be integrated, Check local APIC only */

 On 32-bit, the APIC may be integrated APIC or 82489DX */

 Neither 82489DX nor integrated APIC ? */

 If the BIOS pretends there is an integrated APIC ? */

 Check MP table or ACPI MADT configuration */

 If SMP should be disabled, then really disable it! */

 Or can we switch back to PIC here? */

 Select the interrupt delivery mode for the BSP */

/*

 * An initial setup of the virtual wire mode.

	/*

	 * Don't do the setup now if we have a SMP BIOS as the

	 * through-I/O-APIC virtual wire mode might be active.

	/*

	 * Do not trust the local APIC being empty at bootup.

	/*

	 * Enable APIC.

 This bit is reserved on P4/Xeon and should be cleared */

	/*

	 * Set up the virtual wire mode.

 82489DX */

 Init the interrupt delivery mode for the BSP */

		/*

		 * Something untraceable is creating bad interrupts on

		 * secondary quads ... for the moment, just leave the

		 * ESR disabled - we can't do anything useful with the

		 * errors anyway - mbligh

 Due to the Pentium erratum 3AP. */

 enables sending errors */

	/*

	 * spec says clear errors after enabling vector.

 Read the IRRs */

 Read the ISRs */

	/*

	 * If the ISR map is not empty. ACK the APIC and run another round

	 * to verify whether a pending IRR has been unblocked and turned

	 * into a ISR.

		/*

		 * There can be multiple ISR bits set when a high priority

		 * interrupt preempted a lower priority one. Issue an ACK

		 * per set bit.

/*

 * After a crash, we no longer service the interrupts and a pending

 * interrupt from previous kernel might still have ISR bit set.

 *

 * Most probably by now the CPU has serviced that pending interrupt and it

 * might not have done the ack_APIC_irq() because it thought, interrupt

 * came from i8259 as ExtInt. LAPIC did not get EOI so it does not clear

 * the ISR bit and cpu thinks it has already serviced the interrupt. Hence

 * a vector might get locked. It was noticed for timer irq (vector

 * 0x31). Issue an extra EOI to clear ISR.

 *

 * If there are pending IRR bits they turn into ISR bits after a higher

 * priority ISR bit has been acked.

 512 loops are way oversized and give the APIC a chance to obey. */

 Dump the IRR/ISR content if that failed */

/**

 * setup_local_APIC - setup the local APIC

 *

 * Used to setup local APIC while initializing BSP or bringing up APs.

 * Always called with preemption disabled.

	/*

	 * If this comes from kexec/kcrash the APIC might be enabled in

	 * SPIV. Soft disable it before doing further initialization.

 Pound the ESR really hard over the head with a big hammer - mbligh */

	/*

	 * Double-check whether this APIC is really registered.

	 * This is meaningless in clustered apic mode, so we skip it.

	/*

	 * Intel recommends to set DFR, LDR and TPR before enabling

	 * an APIC.  See e.g. "AP-388 82489DX User's Manual" (Intel

	 * document number 292116).  So here it goes...

		/*

		 * APIC LDR is initialized.  If logical_apicid mapping was

		 * initialized during get_smp_config(), make sure it matches

		 * the actual value.

 Always use the value from LDR. */

	/*

	 * Set Task Priority to 'accept all except vectors 0-31'.  An APIC

	 * vector in the 16-31 range could be delivered if TPR == 0, but we

	 * would think it's an exception and terrible things will happen.  We

	 * never change this later on.

 Clear eventually stale ISR/IRR bits */

	/*

	 * Now that we are all set up, enable the APIC

	/*

	 * Enable APIC

	/*

	 * Some unknown Intel IO/APIC (or APIC) errata is biting us with

	 * certain networking cards. If high frequency interrupts are

	 * happening on a particular IOAPIC pin, plus the IOAPIC routing

	 * entry is masked/unmasked at a high rate as well then sooner or

	 * later IOAPIC line gets 'stuck', no more interrupts are received

	 * from the device. If focus CPU is disabled then the hang goes

	 * away, oh well :-(

	 *

	 * [ This bug can be reproduced easily with a level-triggered

	 *   PCI Ne2000 networking cards and PII/PIII processors, dual

	 *   BX chipset. ]

	/*

	 * Actually disabling the focus CPU check just makes the hang less

	 * frequent as it makes the interrupt distribution model be more

	 * like LRU than MRU (the short-term load is more even across CPUs).

	/*

	 * - enable focus processor (bit==0)

	 * - 64bit mode always use processor focus

	 *   so no need to set it

	/*

	 * Set spurious IRQ vector

	/*

	 * Set up LVT0, LVT1:

	 *

	 * set up through-local-APIC on the boot CPU's LINT0. This is not

	 * strictly necessary in pure symmetric-IO mode, but sometimes

	 * we delegate interrupts to the 8259A.

	/*

	 * TODO: set up through-local-APIC from through-I/O-APIC? --macro

	/*

	 * Only the BSP sees the LINT1 NMI signal by default. This can be

	 * modified by apic_extnmi= boot option.

 Is 82489DX ? */

 Recheck CMCI information after local APIC is up on CPU #0 */

 Disable the local apic timer */

/*

 * APIC setup function for application processors. Called from smpboot.c

 Disable xapic and x2apic first and then reenable xapic mode */

 Called from cpu_init() to enable x2apic on (secondary) cpus */

	/*

	 * If x2apic is not in ON state, disable it if already enabled

	 * from BIOS.

		/*

		 * Using X2APIC without IR is not architecturally supported

		 * on bare metal but may be supported in guests.

		/*

		 * If the hypervisor supports extended destination ID in

		 * MSI, that increases the maximum APIC ID that can be

		 * used for non-remapped IRQ domains.

		/*

		 * Without IR, all CPUs can be addressed by IOAPIC/MSI only

		 * in physical mode, and CPUs with an APIC ID that cannot

		 * be addressed must not be brought online.

 CONFIG_X86_X2APIC */

	/*

	 * Checkme: Can we simply turn off x2apic here instead of panic?

 !CONFIG_X86_X2APIC */

 If irq_remapping_prepare() succeeded, try to enable it */

 ir_stat contains the remap mode or an error code */

/*

 * Detect and enable local APICs on non-SMP boards.

 * Original code written by Keir Fraser.

 * On AMD64 we trust the BIOS - if it says no APIC it is likely

 * not correctly set up (usually the APIC timer won't work etc.)

	/*

	 * The APIC feature bit should now be enabled

	 * in `cpuid'

 The BIOS may have set up the APIC at some other address */

	/*

	 * Some BIOSes disable the local APIC in the APIC_BASE

	 * MSR. This can only be done in software for Intel P6 or later

	 * and AMD K7 (Model > 1) or later.

/*

 * Detect and initialize APIC

 Disabled by kernel option? */

		/*

		 * Over-ride BIOS and try to enable the local APIC only if

		 * "lapic" specified.

/**

 * init_apic_mappings - initialize APIC mappings

 If no local APIC can be found return early */

 lets NOP'ify apic operations */

		/*

		 * If the system has ACPI MADT tables or MP info, the LAPIC

		 * address is already registered.

	/*

	 * Fetch the APIC ID of the BSP in case we have a

	 * default configuration (or the MP table is broken).

		/*

		 * yeah -- we lie about apic_version

		 * in case if apic was disabled via boot option

		 * but it's not a problem for SMP compiled kernel

		 * since apic_intr_mode_select is prepared for such

		 * a case and disable smp mode

/*

 * Local APIC interrupts

/*

 * Common handling code for spurious_interrupt and spurious_vector entry

 * points below. No point in allowing the compiler to inline it twice.

	/*

	 * If this is a spurious interrupt then do not acknowledge

 See SDM vol 3 */

	/*

	 * If it is a vectored one, verify it's set in the ISR. If set,

	 * acknowledge it.

/**

 * spurious_interrupt - Catch all for interrupts raised on unused vectors

 * @regs:	Pointer to pt_regs on stack

 * @vector:	The vector number

 *

 * This is invoked from ASM entry code to catch all interrupts which

 * trigger on an entry which is routed to the common_spurious idtentry

 * point.

/*

 * This interrupt should never happen with our APIC/SMP architecture

 APIC Error Bit 0 */

 APIC Error Bit 1 */

 APIC Error Bit 2 */

 APIC Error Bit 3 */

 APIC Error Bit 4 */

 APIC Error Bit 5 */

 APIC Error Bit 6 */

 APIC Error Bit 7 */

 First tickle the hardware, only then report what went on. -- REW */

 Due to the Pentium erratum 3AP. */

/**

 * connect_bsp_APIC - attach the APIC to the interrupt system

		/*

		 * Do not trust the local APIC being empty at bootup.

		/*

		 * PIC mode, enable APIC mode in the IMCR, i.e.  connect BSP's

		 * local APIC to INT and NMI lines.

/**

 * disconnect_bsp_APIC - detach the APIC from the interrupt system

 * @virt_wire_setup:	indicates, whether virtual wire mode is selected

 *

 * Virtual wire mode is necessary to deliver legacy interrupts even when the

 * APIC is disabled.

		/*

		 * Put the board back into PIC mode (has an effect only on

		 * certain older boards).  Note that APIC interrupts, including

		 * IPIs, won't work beyond this point!  The only exception are

		 * INIT IPIs.

 Go back to Virtual Wire compatibility mode */

 For the spurious interrupt use vector F, and enable it */

		/*

		 * For LVT0 make it edge triggered, active high,

		 * external and enabled

 Disable LVT0 */

	/*

	 * For LVT1 make it edge triggered, active high,

	 * nmi and enabled

/*

 * The number of allocated logical CPU IDs. Since logical CPU IDs are allocated

 * contiguously, it equals to current allocated max logical CPU ID plus 1.

 * All allocated CPU IDs should be in the [0, nr_logical_cpuids) range,

 * so the maximum of nr_logical_cpuids is nr_cpu_ids.

 *

 * NOTE: Reserve 0 for BSP.

/*

 * Used to store mapping between logical CPU IDs and APIC IDs.

/**

 * apic_id_is_primary_thread - Check whether APIC ID belongs to a primary thread

 * @apicid: APIC ID to check

 Isolate the SMT bit(s) in the APICID and check for 0 */

/*

 * Should use this API to allocate logical CPU IDs to keep nr_logical_cpuids

 * and cpuid_to_apicid[] synchronized.

	/*

	 * cpuid <-> apicid mapping is persistent, so when a cpu is up,

	 * check if the kernel has allocated a cpuid for it.

 Allocate a new cpuid. */

	/*

	 * boot_cpu_physical_apicid is designed to have the apicid

	 * returned by read_apic_id(), i.e, the apicid of the

	 * currently booting-up processor. However, on some platforms,

	 * it is temporarily modified by the apicid reported as BSP

	 * through MP table. Concretely:

	 *

	 * - arch/x86/kernel/mpparse.c: MP_processor_info()

	 * - arch/x86/mm/amdtopology.c: amd_numa_init()

	 *

	 * This function is executed with the modified

	 * boot_cpu_physical_apicid. So, disabled_cpu_apicid kernel

	 * parameter doesn't work to disable APs on kdump 2nd kernel.

	 *

	 * Since fixing handling of boot_cpu_physical_apicid requires

	 * another discussion and tests on each platform, we leave it

	 * for now and here we use read_apic_id() directly in this

	 * function, generic_processor_info().

	/*

	 * If boot cpu has not been detected yet, then only allow upto

	 * nr_cpu_ids - 1 processors and keep one slot free for boot cpu

		/*

		 * x86_bios_cpu_apicid is required to have processors listed

		 * in same order as logical cpu numbers. Hence the first

		 * entry is BSP, and so on.

		 * boot_cpu_init() already hold bit 0 in cpu_present_mask

		 * for BSP.

 Logical cpuid 0 is reserved for BSP. */

	/*

	 * Validate version

	/*

	 * Only the IOMMU itself can use the trick of putting destination

	 * APIC ID into the high bits of the address. Anything else would

	 * just be writing to memory if it tried that, and needs IR to

	 * address APICs which can't be addressed in the normal 32-bit

	 * address range at 0xFFExxxxx. That is typically just 8 bits, but

	 * some hypervisors allow the extended destination ID field in bits

	 * 5-11 to be used, giving support for 15 bits of APIC IDs in total.

/*

 * Override the generic EOI implementation with an optimized version.

 * Only called during early boot when only one CPU is active and with

 * interrupts disabled, so we know this does not race with actual APIC driver

 * use.

 Should happen once for each apic */

	/*

	 * Hack: In case of kdump, after a crash, kernel might be booting

	 * on a cpu with non-zero lapic id. But boot_cpu_physical_apicid

	 * might be zero if read from MP tables. Get it from LAPIC.

/**

 * apic_bsp_setup - Setup function for local apic and io-apic

 * @upmode:		Force UP mode (for APIC_init_uniprocessor)

 Setup local timer */

/*

 * Power management

	/*

	 * 'active' is true if the local APIC was enabled by us and

	 * not the BIOS; this signifies that we are also responsible

	 * for disabling it before entering apm/acpi suspend

 r/w apic fields */

	/*

	 * Mask IOAPIC before disabling the local APIC to prevent stale IRR

	 * entries on some implementations.

	/*

	 * IO-APIC and PIC have their own resume routines.

	 * We just mask them here to make sure the interrupt

	 * subsystem is completely quiet while we enable x2apic

	 * and interrupt-remapping.

		/*

		 * Make sure the APICBASE points to the right address

		 *

		 * FIXME! This will be wrong if we ever support suspend on

		 * SMP! We'll need to do this as part of the CPU restore!

/*

 * This device has no shutdown method - fully functioning local APICs

 * are needed on every CPU up until machine_halt/restart/poweroff.

 XXX: remove suspend/resume procs if !apic_pm_state.active? */

 local apic needs to resume before other devices access its registers. */

 CONFIG_PM */

 CONFIG_PM */

/*

 * apic_is_clustered_box() -- Check if we can expect good TSC

 *

 * Thus far, the major user of this is IBM's Summit2 series:

 * Clustered boxes may have unsynced TSC problems if they are

 * multi-chassis.

 * Use DMI to check them

/*

 * APIC command line parameters

 same as disableapic, for compatibility */

 Put local APIC into the resource map. */

/*

 * need call insert after e820__reserve_resources()

 * that is using request_resource

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Numascale NumaConnect-Specific APIC Code

 *

 * Copyright (C) 2011 Numascale AS. All rights reserved.

 *

 * Send feedback to <support@numascale.com>

 *

 Trust what bootloader passes in MADT */

 Send via local APIC where non-local part matches */

 Account for nodes per socket in multi-core-module processors */

 Map the LCSR area and set up the apic_icr_write function */

 APIC IPIs are queued */

 APIC NMI IPIs are queued */

 REMRD not supported */

 REMRD not supported */

 SPDX-License-Identifier: GPL-2.0

/*

 * __x2apic_send_IPI_mask() possibly needs to read

 * x86_cpu_to_logical_apicid for all online cpus in a sequential way.

 * Using per cpu variable would cost one cache line per cpu.

 x2apic MSRs are special and need a special fence: */

 x2apic MSRs are special and need a special fence: */

 If IPI should not be sent to self, clear current CPU */

 Collapse cpus in a cluster so a single IPI per cluster is sent */

 Remove cluster CPUs from tmpmask */

 Matching cluster found. Link and update it. */

	/*

	 * If a hotplug spare mask exists, check whether it's on the right

	 * node. If not, free it and allocate a new one.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2004 James Cleverdon, IBM.

 *

 * Generic APIC sub-arch probe layer.

 *

 * Hacked for x86-64 by James Cleverdon from i386 architecture code by

 * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and

 * James Cleverdon.

/*

 * Check the APIC IDs in bios_cpu_apicid and choose the APIC mode.

 SPDX-License-Identifier: GPL-2.0

/*

 *  HW NMI watchdog support

 *

 *  started by Don Zickus, Copyright (C) 2010 Red Hat, Inc.

 *

 *  Arch specific calls to support NMI watchdog

 *

 *  Bits copied from original nmi.c file

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Support of MSI, HPET and DMAR interrupts.

 *

 * Copyright (C) 1997, 1998, 1999, 2000, 2009 Ingo Molnar, Hajnalka Szabo

 *	Moved from arch/x86/kernel/apic/io_apic.c.

 * Jiang Liu <jiang.liu@linux.intel.com>

 *	Convert to hierarchical irqdomain

 Save the current configuration */

 Allocate a new target vector */

	/*

	 * For non-maskable and non-remapped MSI interrupts the migration

	 * to a different destination CPU and a different vector has to be

	 * done careful to handle the possible stray interrupt which can be

	 * caused by the non-atomic update of the address/data pair.

	 *

	 * Direct update is possible when:

	 * - The MSI is maskable (remapped MSI does not use this code path)).

	 *   The quirk bit is not set in this case.

	 * - The new vector is the same as the old vector

	 * - The old vector is MANAGED_IRQ_SHUTDOWN_VECTOR (interrupt starts up)

	 * - The interrupt is not yet started up

	 * - The new destination CPU is the same as the old destination CPU

	/*

	 * Paranoia: Validate that the interrupt target is the local

	 * CPU.

	/*

	 * Redirect the interrupt to the new vector on the current CPU

	 * first. This might cause a spurious interrupt on this vector if

	 * the device raises an interrupt right between this update and the

	 * update to the final destination CPU.

	 *

	 * If the vector is in use then the installed device handler will

	 * denote it as spurious which is no harm as this is a rare event

	 * and interrupt handlers have to cope with spurious interrupts

	 * anyway. If the vector is unused, then it is marked so it won't

	 * trigger the 'No irq handler for vector' warning in

	 * common_interrupt().

	 *

	 * This requires to hold vector lock to prevent concurrent updates to

	 * the affected vector.

	/*

	 * Mark the new target vector on the local CPU if it is currently

	 * unused. Reuse the VECTOR_RETRIGGERED state which is also used in

	 * the CPU hotplug path for a similar purpose. This cannot be

	 * undone here as the current CPU has interrupts disabled and

	 * cannot handle the interrupt before the whole set_affinity()

	 * section is done. In the CPU unplug case, the current CPU is

	 * about to vanish and will not handle any interrupts anymore. The

	 * vector is cleaned up when the CPU comes online again.

 Redirect it to the new vector on the local CPU temporarily */

 Now transition it to the target CPU */

	/*

	 * All interrupts after this point are now targeted at the new

	 * vector/CPU.

	 *

	 * Drop vector lock before testing whether the temporary assignment

	 * to the local CPU was hit by an interrupt raised in the device,

	 * because the retrigger function acquires vector lock again.

	/*

	 * Check whether the transition raced with a device interrupt and

	 * is pending in the local APICs IRR. It is safe to do this outside

	 * of vector lock as the irq_desc::lock of this interrupt is still

	 * held and interrupts are disabled: The check is not accessing the

	 * underlying vector store. It's just checking the local APIC's

	 * IRR.

/*

 * IRQ Chip for MSI PCI/PCI-X/PCI-Express Devices,

 * which implement the MSI or MSI-X Capability Structure.

/*

 * The Intel IOMMU (ab)uses the high bits of the MSI address to contain the

 * high bits of the destination APIC ID. This can't be done in the general

 * case for MSIs as it would be targeting real memory above 4GiB not the

 * APIC.

 SPDX-License-Identifier: GPL-2.0

/*

 * APIC driver for "bigsmp" xAPIC machines with more than 8 virtual CPUs.

 *

 * Drives the local APIC in "clustered mode".

 on bigsmp, logical apicid is the same as physical */

/*

 * bigsmp enables physical destination mode

 * and doesn't use LDR and DFR

 For clustered we don't have a good way to do this yet - hack */

 can be set by dmi scanners */

 NULL entry stops DMI scanning */

 SPDX-License-Identifier: GPL-2.0

/*

 *	Intel IO-APIC support for multi-Pentium hosts.

 *

 *	Copyright (C) 1997, 1998, 1999, 2000, 2009 Ingo Molnar, Hajnalka Szabo

 *

 *	Many thanks to Stig Venaas for trying out countless experimental

 *	patches and reporting/debugging problems patiently!

 *

 *	(c) 1999, Multiple IO-APIC support, developed by

 *	Ken-ichi Yaku <yaku@css1.kbnes.nec.co.jp> and

 *      Hidemi Kishimoto <kisimoto@css1.kbnes.nec.co.jp>,

 *	further tested and cleaned up by Zach Brown <zab@redhat.com>

 *	and Ingo Molnar <mingo@redhat.com>

 *

 *	Fixes

 *	Maciej W. Rozycki	:	Bits for genuine 82489DX APICs;

 *					thanks to Eric Gilmore

 *					and Rolf G. Tews

 *					for testing these extensively

 *	Paul Diefenbaugh	:	Added full ACPI support

 *

 * Historical information which is worth to be preserved:

 *

 * - SiS APIC rmw bug:

 *

 *	We used to have a workaround for a bug in SiS chips which

 *	required to rewrite the index register for a read-modify-write

 *	operation as the chip lost the index information which was

 *	setup for the read already. We cache the data now, so that

 *	workaround has been removed.

 time_after() */

	/*

	 * # of IRQ routing registers

	/*

	 * Saved state during suspend/resume, or while enabling intr-remap.

 I/O APIC config */

 IO APIC gsi routing info */

 The one past the highest gsi number used */

 MP IRQ source entries */

 # of MP IRQ source entries */

/**

 * disable_ioapic_support() - disables ioapic support at runtime

 disable IO-APIC */

 Will be called in mpparse/ACPI codes for saving IRQ info */

/*

 * When we write a new IO APIC routing entry, we need to write the high

 * word first! If the mask bit in the low word is clear, we will enable

 * the interrupt, and we need to make sure the entry is fully populated

 * before that happens.

/*

 * When we mask an IO APIC routing entry, we need to write the low

 * word first, in order to set the mask bit before we change the

 * high bits!

/*

 * The common case is 1:1 IRQ<->pin mappings. Sometimes there are

 * shared ISA-space IRQs, so we have to support them. We are super

 * fast in the common case, and fast for shared ISA-space IRQs.

 don't allow duplicates */

/*

 * Reroute an IRQ to a different pin.

 every one is different, right? */

 old apic/pin didn't exist, so just add new ones */

	/*

	 * Synchronize the IO-APIC and the CPU by doing

	 * a dummy read from the IO-APIC

/*

 * IO-APIC versions below 0x20 don't support EOI register.

 * For the record, here is the information about various versions:

 *     0Xh     82489DX

 *     1Xh     I/OAPIC or I/O(x)APIC which are not PCI 2.2 Compliant

 *     2Xh     I/O(x)APIC which is PCI 2.2 Compliant

 *     30h-FFh Reserved

 *

 * Some of the Intel ICH Specs (ICH2 to ICH5) documents the io-apic

 * version as 0x2. This is an error with documentation and these ICH chips

 * use io-apic's of version 0x20.

 *

 * For IO-APIC's with EOI register, we use that to do an explicit EOI.

 * Otherwise, we simulate the EOI message manually by changing the trigger

 * mode to edge and then back to level, with RTE being masked during this.

		/*

		 * Mask the entry and change the trigger mode to edge.

		/*

		 * Restore the previous level triggered entry.

 Check delivery_mode to be sure we're not clearing an SMI pin */

	/*

	 * Make sure the entry is masked and re-read the contents to check

	 * if it is a level triggered pin and if the remote-IRR is set.

		/*

		 * Make sure the trigger mode is set to level. Explicit EOI

		 * doesn't clear the remote-IRR if the trigger mode is not

		 * set to level.

	/*

	 * Clear the rest of the bits in the IO-APIC RTE except for the mask

	 * bit.

/*

 * support for broken MP BIOSs, enables hand-redirection of PIRQ0-7 to

 * specific CPU-side IRQs.

		/*

		 * PIRQs are mapped upside down, usually.

 CONFIG_X86_32 */

/*

 * Saves all the IO-APIC RTE's

/*

 * Mask all IO APIC entries.

/*

 * Restore IO APIC entries which was saved in the ioapic structure.

/*

 * Find the IRQ entry number of a certain pin.

/*

 * Find the pin to which IRQ[irq] (ISA) is connected

	/*

	 * Determine IRQ line polarity (high active or low active):

		/*

		 * Conforms to spec, ie. bus-type dependent polarity.  PCI

		 * defaults to low active. [E]ISA defaults to high active.

 Pointless default required due to do gcc stupidity */

/*

 * EISA Edge/Level control register, ELCR

/*

 * EISA interrupts are always active high and can be edge or level

 * triggered depending on the ELCR value.  If an interrupt is listed as

 * EISA conforming in the MP table, that means its trigger type must be

 * read in from the ELCR.

	/*

	 * Determine IRQ trigger mode (edge or level sensitive):

		/*

		 * Conforms to spec, ie. bus-type dependent trigger

		 * mode. PCI defaults to level, ISA to edge.

 Take EISA into account */

 Pointless default required due to do gcc stupidity */

			/*

			 * PCI interrupts are always active low level

			 * triggered.

	/*

	 * setup_IO_APIC_irqs() programs all legacy IRQs with default trigger

	 * and polarity attributes. So allow the first user to reprogram the

	 * pin with real trigger and polarity attributes.

		/*

		 * Dynamically allocate IRQ number for non-ISA IRQs in the first

		 * 16 GSIs on some weird platforms.

/*

 * Need special handling for ISA IRQs because there may be multiple IOAPIC pins

 * sharing the same ISA IRQ number and irqdomain only supports 1:1 mapping

 * between IOAPIC pin and IRQ number. A typical IOAPIC has 24 pins, pin 0-15 are

 * used for legacy IRQs and pin 16-23 are used for PCI IRQs (PIRQ A-H).

 * When ACPI is disabled, only legacy IRQ numbers (IRQ0-15) are available, and

 * some BIOSes may use MP Interrupt Source records to override IRQ numbers for

 * PIRQs instead of reprogramming the interrupt routing logic. Thus there may be

 * multiple pins sharing the same legacy IRQ number when ACPI is disabled.

	/*

	 * Legacy ISA IRQ has already been allocated, just add pin to

	 * the pin list associated with this IRQ and program the IOAPIC

	 * entry. The IOAPIC entry

		/*

		 * IRQ2 is unusable for historical reasons on systems which

		 * have a legacy PIC. See the comment vs. IRQ2 further down.

		 *

		 * If this gets removed at some point then the related code

		 * in lapic_assign_system_vectors() needs to be adjusted as

		 * well.

	/*

	 * Debugging check, we are in big trouble if this message pops up!

	/*

	 * PCI IRQ command line redirection. Yes, limits are hardcoded.

/*

 * Find a specific PCI IRQ entry.

 * Not an __init, possibly needed by modules

 Skip ISA IRQs */

		/*

		 * Use the first all-but-pin matching entry as a

		 * best-guess fuzzy result for broken mptables.

	/*

	 * Some Intel chipsets with IO APIC VERSION of 0x1? don't have reg_02,

	 * but the value of reg_02 is read as the previous read register

	 * value, so ignore it if reg_02 == reg_01.

	/*

	 * Some Intel chipsets with IO APIC VERSION of 0x2? don't have reg_02

	 * or reg_03, but the value of reg_0[23] is read as the previous read

	 * register value, so ignore it if reg_03 == reg_0[12].

	/*

	 * We are a bit conservative about what we expect.  We have to

	 * know about every hardware change ASAP.

 Where if anywhere is the i8259 connect in external int mode */

 See if any of the pins is in ExtINT mode */

		/* If the interrupt line is enabled and in ExtInt mode

		 * I have found the pin where the i8259 is connected.

 Look to see what if the MP table has reported the ExtINT */

	/* If we could not find the appropriate pin by looking at the ioapic

	 * the i8259 probably is not connected the ioapic but give the

	 * mptable a chance anyway.

 Trust the MP table if nothing is setup in the hardware */

 Complain if the MP table and the hardware disagree */

	/*

	 * Do not trust the IO-APIC being empty at bootup

	/*

	 * If the i8259 is routed through an IOAPIC

	 * Put that IOAPIC in virtual wire mode

	 * so legacy interrupts can be delivered.

		/*

		 * Add it to the IO-APIC irq-routing table:

/*

 * function to set the IO-APIC physical IDs based on the

 * values stored in the MPC table.

 *

 * by Matt Domsch <Matt_Domsch@dell.com>  Tue Dec 21 12:25:05 CST 1999

	/*

	 * This is broken; anything with a real cpu count has to

	 * circumvent this idiocy regardless.

	/*

	 * Set the IOAPIC ID to the value stored in the MPC table.

 Read the register 0 value */

		/*

		 * Sanity check, is the ID really free? Every APIC in a

		 * system must have a unique ID or we get lots of nice

		 * 'stuck on smp_invalidate_needed IPI wait' messages.

		/*

		 * We need to adjust the IRQ routing table

		 * if the ID changed.

		/*

		 * Update the ID register according to the right value

		 * from the MPC table if they are different.

		/*

		 * Sanity check

	/*

	 * Don't check I/O APIC IDs for xAPIC systems.  They have

	 * no meaning without the serial APIC bus.

	/*

	 * We don't know the TSC frequency yet, but waiting for

	 * 40000000000/HZ TSC cycles is safe:

	 * 4 GHz == 10 jiffies

	 * 1 GHz == 40 jiffies

	/*

	 * We don't know any frequency yet, but waiting for

	 * 40940000000/HZ cycles is safe:

	 * 4 GHz == 10 jiffies

	 * 1 GHz == 40 jiffies

	 * 1 << 1 + 1 << 2 +...+ 1 << 11 = 4094

/*

 * There is a nasty bug in some older SMP boards, their mptable lies

 * about the timer IRQ. We do the following to work around the situation:

 *

 *	- timer IRQ defaults to IO-APIC IRQ

 *	- if this function detects that timer IRQs are defunct, then we fall

 *	  back to ISA timer IRQs

	/*

	 * Expect a few ticks at least, to be sure some possible

	 * glue logic does not lock up after one or two first

	 * ticks in a non-ExtINT mode.  Also the local APIC

	 * might have cached one ExtINT interrupt.  Finally, at

	 * least one tick may be lost due to delays.

 Did jiffies advance? */

/*

 * In the SMP+IOAPIC case it might happen that there are an unspecified

 * number of pending IRQ events unhandled. These cases are very rare,

 * so we 'resend' these IRQs via IPIs, to the same CPU. It's much

 * better to do it this way as thus we do not have to be aware of

 * 'pending' interrupts in the IRQ path, except at this point.

/*

 * Edge triggered needs to resend any interrupt

 * that was delayed but this is now handled in the device

 * independent code.

/*

 * Starting up a edge-triggered IO-APIC interrupt is

 * nasty - we need to make sure that we get the edge.

 * If it is already asserted for some reason, we need

 * return 1 to indicate that is was pending.

 *

 * This is not complete - we should be able to fake

 * an edge even if it isn't on the 8259A...

 Is the remote IRR bit set? */

 If we are moving the IRQ we need to mask it */

		/* Only migrate the irq if the ack has been received.

		 *

		 * On rare occasions the broadcast level triggered ack gets

		 * delayed going to ioapics, and if we reprogram the

		 * vector while Remote IRR is still set the irq will never

		 * fire again.

		 *

		 * To prevent this scenario we read the Remote IRR bit

		 * of the ioapic.  This has two effects.

		 * - On any sane system the read of the ioapic will

		 *   flush writes (and acks) going to the ioapic from

		 *   this cpu.

		 * - We get to see if the ACK has actually been delivered.

		 *

		 * Based on failed experiments of reprogramming the

		 * ioapic entry from outside of irq context starting

		 * with masking the ioapic entry and then polling until

		 * Remote IRR was clear before reprogramming the

		 * ioapic I don't trust the Remote IRR bit to be

		 * completely accurate.

		 *

		 * However there appears to be no other way to plug

		 * this race, so if the Remote IRR bit is not

		 * accurate and is causing problems then it is a hardware bug

		 * and you can go talk to the chipset vendor about it.

 If the IRQ is masked in the core, leave it: */

	/*

	 * It appears there is an erratum which affects at least version 0x11

	 * of I/O APIC (that's the 82093AA and cores integrated into various

	 * chipsets).  Under certain conditions a level-triggered interrupt is

	 * erroneously delivered as edge-triggered one but the respective IRR

	 * bit gets set nevertheless.  As a result the I/O unit expects an EOI

	 * message but it will never arrive and further interrupts are blocked

	 * from the source.  The exact reason is so far unknown, but the

	 * phenomenon was observed when two consecutive interrupt requests

	 * from a given source get delivered to the same CPU and the source is

	 * temporarily disabled in between.

	 *

	 * A workaround is to simulate an EOI message manually.  We achieve it

	 * by setting the trigger mode to edge and then to level when the edge

	 * trigger mode gets detected in the TMR of a local APIC for a

	 * level-triggered interrupt.  We mask the source for the time of the

	 * operation to prevent an edge-triggered interrupt escaping meanwhile.

	 * The idea is from Manfred Spraul.  --macro

	 *

	 * Also in the case when cpu goes offline, fixup_irqs() will forward

	 * any unhandled interrupt on the offlined cpu to the new cpu

	 * destination that is handling the corresponding interrupt. This

	 * interrupt forwarding is done via IPI's. Hence, in this case also

	 * level-triggered io-apic interrupt will be seen as an edge

	 * interrupt in the IRR. And we can't rely on the cpu's EOI

	 * to be broadcasted to the IO-APIC's which will clear the remoteIRR

	 * corresponding to the level-triggered interrupt. Hence on IO-APIC's

	 * supporting EOI register, we do an explicit EOI to clear the

	 * remote IRR and on IO-APIC's which don't have an EOI register,

	 * we use the above logic (mask+edge followed by unmask+level) from

	 * Manfred Spraul to clear the remote IRR.

	/*

	 * We must acknowledge the irq before we move it or the acknowledge will

	 * not propagate properly.

	/*

	 * Tail end of clearing remote IRR bit (either by delivering the EOI

	 * message via io-apic EOI register write or simulating it using

	 * mask+edge followed by unmask+level logic) manually when the

	 * level triggered interrupt is seen as the edge triggered interrupt

	 * at the cpu.

	/*

	 * Intr-remapping uses pin number as the virtual vector

	 * in the RTE. Actual vector is programmed in

	 * intr-remapping table entry. Hence for the io-apic

	 * EOI we use the pin number.

/*

 * The I/OAPIC is just a device for generating MSI messages from legacy

 * interrupt pins. Various fields of the RTE translate into bits of the

 * resulting MSI which had a historical meaning.

 *

 * With interrupt remapping, many of those bits have different meanings

 * in the underlying MSI, but the way that the I/OAPIC transforms them

 * from its RTE to the MSI message is the same. This function allows

 * the parent IRQ domain to compose the MSI message, then takes the

 * relevant bits to put them in the appropriate places in the RTE in

 * order to generate that message when the IRQ happens.

 *

 * The setup here relies on a preconfigured route entry (is_level,

 * active_low, masked) because the parent domain is merely composing the

 * generic message routing information which is used for the MSI.

 Let the parent domain compose the MSI message */

	/*

	 * - Real vector

	 * - DMAR/IR: 8bit subhandle (ioapic.pin)

	 * - AMD/IR:  8bit IRTE index

 Delivery mode (for DMAR/IR all 0) */

 Destination mode or DMAR/IR index bit 15 */

 DMAR/IR: 1, 0 for all other modes */

	/*

	 * - DMAR/IR: index bit 0-14.

	 *

	 * - Virt: If the host supports x2apic without a virtualized IR

	 *	   unit then bit 0-6 of dmar_index_0_14 are providing bit

	 *	   8-14 of the destination id.

	 *

	 * All other modes have bit 0-6 of dmar_index_0_14 cleared and the

	 * topmost 8 bits are destination id bit 0-7 (entry::destid_0_7).

/*

 * Interrupt shutdown masks the ioapic pin, but the interrupt might already

 * be in flight, but not yet serviced by the target CPU. That means

 * __synchronize_hardirq() would return and claim that everything is calmed

 * down. So free_irq() would proceed and deactivate the interrupt and free

 * resources.

 *

 * Once the target CPU comes around to service it it will find a cleared

 * vector and complain. While the spurious interrupt is harmless, the full

 * release of resources might prevent the interrupt from being acknowledged

 * which keeps the hardware in a weird state.

 *

 * Verify that the corresponding Remote-IRR bits are clear.

		/*

		 * The remote IRR is only valid in level trigger mode. It's

		 * meaning is undefined for edge triggered interrupts and

		 * irrelevant because the IO-APIC treats them as fire and

		 * forget.

			/*

			 * Hmm.. We don't have an entry for this,

			 * so default to an old-fashioned 8259

			 * interrupt if we can..

 Strange. Oh, well.. */

/*

 * The local APIC irq-chip implementation:

/*

 * This looks a bit hackish but it's about the only one way of sending

 * a few INTA cycles to 8259As and any associated glue logic.  ICR does

 * not support the ExtINT mode, unfortunately.  We need to send these

 * cycles as some i82489DX-based boards have glue logic that keeps the

 * 8259A interrupt line asserted until INTA.  --macro

 Actually the next is obsolete, but keep it for paranoid reasons -AK */

/*

 * This code may look a bit paranoid, but it's supposed to cooperate with

 * a wide range of boards and BIOS bugs.  Fortunately only the timer IRQ

 * is so screwy.  Thanks to Brian Perkins for testing/hacking this beast

 * fanatically on his truly buggy board.

 *

 * FIXME: really need to revamp this for all platforms.

	/*

	 * get/set the timer IRQ vector:

	/*

	 * As IRQ0 is to be enabled in the 8259A, the virtual

	 * wire has to be disabled in the local APIC.  Also

	 * timer interrupts need to be acknowledged manually in

	 * the 8259A for the i82489DX when using the NMI

	 * watchdog as that APIC treats NMIs as level-triggered.

	 * The AEOI mode will finish them in the 8259A

	 * automatically.

	/*

	 * Some BIOS writers are clueless and report the ExtINTA

	 * I/O APIC input from the cascaded 8259A as the timer

	 * interrupt input.  So just in case, if only one pin

	 * was found above, try it both directly and through the

	 * 8259A.

 Ok, does IRQ0 through the IOAPIC work? */

			/*

			 * for edge trigger, it's already unmasked,

			 * so only need to unmask if it is level-trigger

			 * do we really have level trigger timer?

		/*

		 * legacy devices should be connected to IO APIC #0

		/*

		 * Cleanup, just in case ...

 Fixed mode */

/*

 * Traditionally ISA IRQ2 is the cascade IRQ, and is not available

 * to devices.  However there may be an I/O APIC pin available for

 * this interrupt regardless.  The pin may be left unconnected, but

 * typically it will be reused as an ExtINT cascade interrupt for

 * the master 8259A.  In the MPS case such a pin will normally be

 * reported as an ExtINT interrupt in the MP table.  With ACPI

 * there is no provision for ExtINT interrupts, and in the absence

 * of an override it would be treated as an ordinary ISA I/O APIC

 * interrupt, that is edge-triggered and unmasked by default.  We

 * used to do this, but it caused problems on some systems because

 * of the NMI watchdog and sometimes IRQ0 of the 8254 timer using

 * the same ExtINT cascade interrupt to drive the local APIC of the

 * bootstrap processor.  Therefore we refrain from routing IRQ2 to

 * the I/O APIC in all cases now.  No actual device should request

 * it anyway.  --macro

 Handle device tree enumerated APICs proper */

 Release fw handle if it was allocated above */

	/*

         * Set up IO-APIC IRQ routing.

	/* The register returns the maximum index redir index

	 * supported, which is one less than the total number of redir

	 * entries.

	/*

	 * dmar_alloc_hwirq() may be called before setup_IO_APIC(), so use

	 * gsi_top if ioapic_dynirq_base hasn't been initialized yet.

	/*

	 * For DT enabled machines ioapic_dynirq_base is irrelevant and not

	 * updated. So simply return @from if ioapic_dynirq_base == 0.

	/*

	 * The P4 platform supports up to 256 APIC IDs on two separate APIC

	 * buses (one for LAPICs, one for IOAPICs), where predecessors only

	 * supports up to 16 on one shared APIC bus.

	 *

	 * TBD: Expand LAPIC/IOAPIC support on P4-class systems to take full

	 *      advantage of new APIC bus architecture.

	/*

	 * Every APIC in a system must have a unique ID or we get lots of nice

	 * 'stuck on smp_invalidate_needed IPI wait' messages.

 Sanity check */

 Hand out the requested id if available */

	/*

	 * Read the current id from the ioapic and keep it if

	 * available.

	/*

	 * Get the next free id and write it to the ioapic.

 Sanity check */

/*

 * This function updates target affinity of IOAPIC interrupts to include

 * the CPUs which came online during SMP bringup.

 Find the IOAPIC that manages this GSI. */

/**

 * mp_register_ioapic - Register an IOAPIC device

 * @id:		hardware IOAPIC ID

 * @address:	physical address of IOAPIC register area

 * @gsi_base:	base of GSI associated with the IOAPIC

 * @cfg:	configuration information for the IOAPIC

	/*

	 * Build basic GSI lookup table to facilitate gsi->io_apic lookups

	 * and to prevent reprogramming of IOAPIC pins (PCI GSIs).

	/*

	 * If mp_register_ioapic() is called during early boot stage when

	 * walking ACPI/DT tables, it's too early to create irqdomain,

	 * we are still using bootmem allocator. So delay it to setup_IO_APIC().

 Set nr_registers to mark entry present */

 Mark entry not present */

 PCI interrupts are always active low level triggered. */

/*

 * Configure the I/O-APIC specific fields in the routing entry.

 *

 * This is important to setup the I/O-APIC specific bits (is_level,

 * active_low, masked) because the underlying parent domain will only

 * provide the routing information and is oblivious of the I/O-APIC

 * specific bits.

 *

 * The entry is just preconfigured at this point and not written into the

 * RTE. This happens later during activation which will fill in the actual

 * routing information.

	/*

	 * Mask level triggered irqs. Edge triggered irqs are masked

	 * by the irq core code in case they fire.

 It won't be called for IRQ with multiple IOAPIC pins associated */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Do not switch to broadcast mode if:

	 * - Disabled on the command line

	 * - Only a single CPU is online

	 * - Not all present CPUs have been at least booted once

	 *

	 * The latter is important as the local APIC might be in some

	 * random state and a broadcast might cause havoc. That's

	 * especially true for NMI broadcasting.

/*

 * Send a 'reschedule' IPI to another CPU. It goes straight through and

 * wastes no time serializing anything. Worst case is that we lose a

 * reschedule ...

 CONFIG_SMP */

	/*

	 * Subtle. In the case of the 'never do double writes' workaround

	 * we have to lock out interrupts to be safe.  As we don't care

	 * of the value read we use an atomic rmw access to avoid costly

	 * cli/sti.  Otherwise we use an even cheaper single atomic write

	 * to the APIC.

	/*

	 * Wait for idle.

	/*

	 * No need to touch the target chip field. Also the destination

	 * mode is ignored when a shorthand is used.

	/*

	 * Send the IPI. The write to APIC_ICR fires this off.

/*

 * This is used to send an IPI with no shorthand notation (the destination is

 * specified in bits 56 to 63 of the ICR).

	/*

	 * Wait for idle.

	/*

	 * prepare target chip field

	/*

	 * program the ICR

	/*

	 * Send the IPI. The write to APIC_ICR fires this off.

	/*

	 * Hack. The clustered APIC addressing mode doesn't allow us to send

	 * to an arbitrary mask, so I do a unicast to each CPU instead.

	 * - mbligh

 See Hack comment above */

/*

 * Helper function for APICs which insist on cpumasks

	/*

	 * Hack. The clustered APIC addressing mode doesn't allow us to send

	 * to an arbitrary mask, so I do a unicasts to each CPU instead. This

	 * should be modified to do 1 message per cluster ID - mbligh

 See Hack comment above */

/*

 * This is only used on smaller machines.

 must come after the send_IPI functions above for inlining */

 SPDX-License-Identifier: GPL-2.0

 x2apic MSRs are special and need a special fence: */

 x2apic MSRs are special and need a special fence: */

 Common x2apic functions, also used by x2apic_cluster */

 x2apic MSRs are special and need a special fence: */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Default generic APIC driver. This handles up to 8 CPUs.

 *

 * Copyright 2003 Andi Kleen, SuSE Labs.

 *

 * Generic x86 APIC driver probe layer.

/*

 * Set up the logical destination ID.  Intel recommends to set DFR, LDR and

 * TPR before enabling an APIC.  See e.g. "AP-388 82489DX User's Manual"

 * (Intel document number 292116).

 should be called last. */

 Parsed again by __setup for debug/verbose */

 P4 and above */

	/*

	 * This is used to switch to bigsmp mode when

	 * - There is no apic= option specified by the user

	 * - generic_apic_probe() has chosen apic_default as the sub_arch

	 * - we find more than 8 CPUs in acpi LAPIC listing with xAPIC support

 Not visible without early console */

 This function can switch the APIC even after the initial ->probe() */

/*

 * Common functions shared between the various APIC flavours

 *

 * SPDX-License-Identifier: GPL-2.0

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * SGI UV APIC functions (note: not an Intel compatible APIC)

 *

 * (C) Copyright 2020 Hewlett Packard Enterprise Development LP

 * Copyright (C) 2007-2014 Silicon Graphics, Inc. All rights reserved.

 Unpack AT/OEM/TABLE ID's to be NULL terminated strings */

 Information derived from CPUID and some UV MMRs */

 aka pnode_shift for UV2/3 */

 Set this to use hardware error handler instead of kernel panic: */

 Cause a machine fault: */

 min partition is 4 sockets */

 Default unless changed */

 Running on a UV Hubbed system, determine which UV Hub Type it is */

	/*

	 * The NODE_ID MMR is always at offset 0.

	 * Contains the chip part # + revision.

	 * Node_id field started with 15 bits,

	 * ... now 7 but upper 8 are masked to 0.

	 * All blades/nodes have the same part # and hub revision.

 UV4/4A only have a revision difference */

 Different returns from different UV BIOS versions */

 Check if TSC is valid for all sockets */

 If BIOS state unknown, don't do anything */

 Otherwise, BIOS indicates problem with TSC */

 Selector for (4|4A|5) structs */

 [Copied from arch/x86/kernel/cpu/topology.c:detect_extended_topology()] */

 Leaf 0xb SMT level */

 Leaf 0xb sub-leaf types */

 Relies on 'to' being NULL chars so result will be NULL terminated */

 Trim trailing spaces */

 Find UV arch type entry in UVsystab */

 Validate UV arch type field in UVsystab */

 Determine if UV arch type entry might exist in UVsystab */

 UV system found, check which APIC MODE BIOS already selected */

 Save OEM_ID passed from ACPI MADT */

 Check if BIOS sent us a UVarchtype */

 If not use OEM ID for UVarchtype */

 Check if not hubbed */

 (Not hubbed), check if not hubless */

 (Not hubless), not a UV */

 Is UV hubless system */

 UV5 Hubless */

 UV4 Hubless: CH */

 UV3 Hubless: UV300/MC990X w/o hub */

 Copy OEM Table ID */

 Set hubbed type if true */

 Get UV hub chip part number & revision */

 Other UV setup functions */

 Called early to probe for the correct APIC driver */

 Set up early hub info fields for Node 0 */

 If not UV, return. */

 Save for display of the OEM Table ID */

 The following values are used for the per node hub info struct */

 Default UV memory block size is 2GB */

 Kernel parameter to specify UV mem block size */

 Size will be rounded down by set_block_size() below */

 adjust for ffs return of 1..64 */

 bad or zero value, default to 1UL << 31 (2GB) */

 Build GAM range lookup table: */

 Mark hole between RAM/non-RAM: */

 New range: */

 Update range: */

 .. if contiguous: */

 Non-contiguous RAM range: */

 Non-contiguous/non-RAM: */

 base is this entry */

 Shorten table if possible */

 Display resultant GAM range table: */

 Arch specific ENUM cases */

 Calculate and Map MMIOH Regions */

 One (UV2) mapping */

 small and large MMIOH mappings */

 enum values chosen so (index mod 2) is MMIOH 0/1 (low/high) */

 Invalid NASID check */

 Last entry check: */

 Check if we have a cached (or last) redirect to print: */

 UVY flavor */

 UVX flavor */

 UV2 flavor */

 BIOS gives wrong value for clock frequency, so guess: */

 Direct Legacy VGA I/O traffic to designated IOH */

/*

 * Called on each CPU to initialize the per_cpu UV data area.

 * FIXME: hotplug not supported yet

 CPU 0 initialization will be done via uv_system_init. */

 Initialize caller's MN struct and fill in values */

 Show system specific info: */

 adjust max block size to current range start */

 update to next range start */

 Walk through UVsystab decoding the fields */

 Get mapped UVsystab pointer */

 If UVsystab is version 1, there is no extended UVsystab */

 point to payload */

 already processed in early startup */

 Set up physical blade translations from UVH_NODE_PRESENT_TABLE */

 Build socket id -> node id, pnode */

 Fill in pnode/node/addr conversion list values: */

 Duplicate: */

 Set socket -> node values: */

 Set up physical blade to pnode translation from GAM Range Table: */

	/*

	 * If socket id == pnode or socket id == node for all nodes,

	 *   system runs faster by removing corresponding conversion table.

 Check which reboot to use */

 If EFI reboot not available, use ACPI reboot */

/*

 * User proc fs file handling now deprecated.

 * Recommend using /sys/firmware/sgi_uv/... instead.

 Initialize UV hubless systems */

 Setup PCH NMI handler */

 Init kernel/BIOS interface */

 Process UVsystab */

 Set section block size for current node memory */

 Create user access node */

 Get uv_systab for decoding, setup UV BIOS calls */

 If there's an UVsystab problem then abort UV init: */

 uv_num_possible_blades() is really the hub count: */

 Allocate new per hub info list */

 Use information from GAM table if available: */

 Or fill in during CPU loop: */

 Initialize per CPU info: */

 Init memoryless node: */

 Add pnode info for pre-GAM list nodes without CPUs: */

 Register Legacy VGA I/O redirection handler: */

/*

 * There is a different code path needed to initialize a UV system that does

 * not have a "UV HUB" (referred to as "hubless").

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Local APIC related interfaces to support IOAPIC, MSI, etc.

 *

 * Copyright (C) 1997, 1998, 1999, 2000, 2009 Ingo Molnar, Hajnalka Szabo

 *	Moved from arch/x86/kernel/apic/io_apic.c.

 * Jiang Liu <jiang.liu@linux.intel.com>

 *	Enable support of hierarchical irqdomains

	/* Used to the online set of cpus does not change

	 * during assign_irq_vector.

	/*

	 * If there is no vector associated or if the associated vector is

	 * the shutdown vector, which is associated to make PCI/MSI

	 * shutdown mode work, then there is nothing to release. Clear out

	 * prev_vector for this and the offlined target case.

	/*

	 * If the target CPU of the previous vector is online, then mark

	 * the vector as move in progress and store it for cleanup when the

	 * first interrupt on the new vector arrives. If the target CPU is

	 * offline then the regular release mechanism via the cleanup

	 * vector is not possible and the vector can be immediately freed

	 * in the underlying matrix allocator.

	/*

	 * If the current target CPU is online and in the new requested

	 * affinity mask, there is no point in moving the interrupt from

	 * one CPU to another.

	/*

	 * Careful here. @apicd might either have move_in_progress set or

	 * be enqueued for cleanup. Assigning a new vector would either

	 * leave a stale vector on some CPU around or in case of a pending

	 * cleanup corrupt the hlist.

 Get the affinity mask - either irq_default_affinity or (user) set */

 Try the intersection of @affmsk and node mask */

 Try the full affinity mask */

 Try the node mask */

 Try the full online mask */

	/*

	 * Make only a global reservation with no guarantee. A real vector

	 * is associated at activation time.

 set_affinity might call here for nothing */

 Clean up move in progress */

 Regular fixed assigned interrupt */

 If the interrupt has a global reservation, nothing to do */

		/*

		 * Core might have disabled reservation mode after

		 * allocating the irq descriptor. Ideally this should

		 * happen before allocation time, but that would require

		 * completely convoluted ways of transporting that

		 * information.

	/*

	 * Check to ensure that the effective affinity mask is a subset

	 * the user supplied affinity mask, and warn the user if it is not

 Something in the core code broke! Survive gracefully */

	/*

	 * This should not happen. The vector reservation got buggered.  Handle

	 * it gracefully.

	/*

	 * If the interrupt is activated, then it must stay at this vector

	 * position. That's usually the timer interrupt (0).

 Release the vector */

 Currently vector allocator can't guarantee contiguous allocations */

	/*

	 * Catch any attempt to touch the cascade interrupt on a PIC

	 * equipped system.

		/*

		 * Prevent that any of these interrupts is invoked in

		 * non interrupt context via e.g. generic_handle_irq()

		 * as that can corrupt the affinity move state.

 Don't invoke affinity setter on deactivated interrupts */

		/*

		 * Legacy vectors are already assigned when the IOAPIC

		 * takes them over. They stay on the same vector. This is

		 * required for check_timer() to work correctly as it might

		 * switch back to legacy mode. Only update the hardware

		 * config.

	/*

	 * HPET and I/OAPIC cannot be parented in the vector domain

	 * if IRQ remapping is enabled. APIC IDs above 15 bits are

	 * only permitted if IRQ remapping is enabled, so check that.

	/*

	 * for MSI and HT dyn irq

	/*

	 * We don't know if PIC is present at this point so we need to do

	 * probe() to get the right number of legacy IRQs.

	/*

	 * Use assign system here so it wont get accounted as allocated

	 * and moveable in the cpu hotplug check and it prevents managed

	 * irq reservation from touching it.

	/*

	 * If the IO/APIC is disabled via config, kernel command line or

	 * lack of enumeration then all legacy interrupts are routed

	 * through the PIC. Make sure that they are marked as legacy

	 * vectors. PIC_CASCADE_IRQ has already been marked in

	 * lapic_assign_system_vectors().

 System vectors are reserved, online it */

 Mark the preallocated legacy interrupts */

		/*

		 * Don't touch the cascade interrupt. It's unusable

		 * on PIC equipped machines. See the large comment

		 * in the IO/APIC code.

	/*

	 * Allocate the vector matrix allocator data structure and limit the

	 * search area.

 Check whether the irq is in the legacy space */

 Check whether the irq is handled by the IOAPIC */

 Online the local APIC infrastructure and initialize the vectors */

 Online the vector matrix array for this CPU */

	/*

	 * The interrupt affinity logic never targets interrupts to offline

	 * CPUs. The exception are the legacy PIC interrupts. In general

	 * they are only targeted to CPU0, but depending on the platform

	 * they can be distributed to any online CPU in hardware. The

	 * kernel has no influence on that. So all active legacy vectors

	 * must be installed on all CPUs. All non legacy interrupts can be

	 * cleared.

	/*

	 * Managed interrupts are usually not migrated away

	 * from an online CPU, but CPU isolation 'managed_irq'

	 * can make that happen.

	 * 1) Activation does not take the isolation into account

	 *    to keep the code simple

	 * 2) Migration away from an isolated CPU can happen when

	 *    a non-isolated CPU which is in the calculated

	 *    affinity mask comes online.

 Prevent vectors vanishing under us */

		/*

		 * Paranoia: Check if the vector that needs to be cleaned

		 * up is registered at the APICs IRR. If so, then this is

		 * not the best time to clean it up. Clean it up in the

		 * next attempt by sending another IRQ_MOVE_CLEANUP_VECTOR

		 * to this CPU. IRQ_MOVE_CLEANUP_VECTOR is the lowest

		 * priority external vector, so on return from this

		 * interrupt the device interrupt will happen first.

	/*

	 * If the interrupt arrived on the new target CPU, cleanup the

	 * vector on the old target CPU. A vector check is not required

	 * because an interrupt can never move from one vector to another

	 * on the same CPU.

/*

 * Called from fixup_irqs() with @desc->lock held and interrupts disabled.

	/*

	 * The function is called for all descriptors regardless of which

	 * irqdomain they belong to. For example if an IRQ is provided by

	 * an irq_chip as part of a GPIO driver, the chip data for that

	 * descriptor is specific to the irq_chip in question.

	 *

	 * Check first that the chip_data is what we expect

	 * (apic_chip_data) before touching it any further.

	/*

	 * If prev_vector is empty, no action required.

	/*

	 * This is tricky. If the cleanup of the old vector has not been

	 * done yet, then the following setaffinity call will fail with

	 * -EBUSY. This can leave the interrupt in a stale state.

	 *

	 * All CPUs are stuck in stop machine with interrupts disabled so

	 * calling __irq_complete_move() would be completely pointless.

	 *

	 * 1) The interrupt is in move_in_progress state. That means that we

	 *    have not seen an interrupt since the io_apic was reprogrammed to

	 *    the new vector.

	 *

	 * 2) The interrupt has fired on the new vector, but the cleanup IPIs

	 *    have not been processed yet.

		/*

		 * In theory there is a race:

		 *

		 * set_ioapic(new_vector) <-- Interrupt is raised before update

		 *			      is effective, i.e. it's raised on

		 *			      the old vector.

		 *

		 * So if the target cpu cannot handle that interrupt before

		 * the old vector is cleaned up, we get a spurious interrupt

		 * and in the worst case the ioapic irq line becomes stale.

		 *

		 * But in case of cpu hotplug this should be a non issue

		 * because if the affinity update happens right before all

		 * cpus rendezvous in stop machine, there is no way that the

		 * interrupt can be blocked on the target cpu because all cpus

		 * loops first with interrupts enabled in stop machine, so the

		 * old vector is not yet cleaned up when the interrupt fires.

		 *

		 * So the only way to run into this issue is if the delivery

		 * of the interrupt on the apic/system bus would be delayed

		 * beyond the point where the target cpu disables interrupts

		 * in stop machine. I doubt that it can happen, but at least

		 * there is a theoretical chance. Virtualization might be

		 * able to expose this, but AFAICT the IOAPIC emulation is not

		 * as stupid as the real hardware.

		 *

		 * Anyway, there is nothing we can do about that at this point

		 * w/o refactoring the whole fixup_irq() business completely.

		 * We print at least the irq number and the old vector number,

		 * so we have the necessary information when a problem in that

		 * area arises.

/*

 * Note, this is not accurate accounting, but at least good enough to

 * prevent that the actual interrupt move will run out of vectors.

 HOTPLUG_CPU */

 SMP */

 !82489DX */

	/*

	 * Remote read supported only in the 82489DX and local APIC for

	 * Pentium processors.

 !82489DX */

 Due to the Pentium erratum 3AP. */

 PC is LVT#4. */

 ERR is LVT#3. */

 don't print out if apic is not there */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2004 James Cleverdon, IBM.

 *

 * Flat APIC subarch code.

 *

 * Hacked for x86-64 by James Cleverdon from i386 architecture code by

 * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and

 * James Cleverdon.

/*

 * Set up the logical destination ID.

 *

 * Intel recommends to set DFR, LDR and TPR before enabling

 * an APIC.  See e.g. "AP-388 82489DX User's Manual" (Intel

 * document number 292116).  So here it goes...

/*

 * Physflat mode is used when there are more than 8 CPUs on a system.

 * We cannot use logical delivery in this case because the mask

 * overflows, so use physical mode.

	/*

	 * Quirk: some x86_64 machines can only use physical APIC mode

	 * regardless of how many processors are present (x86_64 ES7000

	 * is an example).

	/*

	 * LDR and DFR are not involved in physflat mode, rather:

	 * "In physical destination mode, the destination processor is

	 * specified by its local APIC ID [...]." (Intel SDM, 10.6.2.1)

/*

 * We need to check for physflat first, so this order is important.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Dynamic Ftrace based Kprobes Optimization

 *

 * Copyright (C) Hitachi Ltd., 2012

 Ftrace callback handler for kprobes -- called under preempt disabled */

 Kprobe handler expects regs->ip = ip + 1 as breakpoint hit */

			/*

			 * Emulate singlestep (and also recover regs->ip)

			 * as if there is a 5byte nop

		/*

		 * If pre_handler returns !0, it changes regs->ip. We have to

		 * skip emulating post_handler.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Kernel Probes Jump Optimization (Optprobes)

 *

 * Copyright (C) IBM Corporation, 2002, 2004

 * Copyright (C) Hitachi Ltd., 2012

 This function only handles jump-optimized kprobe */

 If op->list is not empty, op is under optimizing */

	/*

	 * If the kprobe can be optimized, original bytes which can be

	 * overwritten by jump destination address. In this case, original

	 * bytes must be recovered from op->optinsn.copied_insn buffer.

	/*

	 * Can't be static_cpu_has() due to how objtool treats this feature bit.

	 * This isn't a fast path anyway.

 Replace the NOP3 with CLAC */

 Insert a move instruction which sets a pointer to eax/rdi (1st arg). */

 We don't bother saving the ss register */

 Move flags to rsp */

 Skip flags entry */

 CONFIG_X86_32 */

 Move flags into esp */

 Skip flags entry */

 Optimized kprobe call back function: called from optinsn */

 This is possible if op is under delayed unoptimizing */

 Save skipped registers */

 Check whether the address range is reserved */

 Check whether insn is indirect jump */

 Jump */

 Segment based jump */

 Check whether insn jumps into specified address range */

 loopne */

 loope */

 loop */

 jcxz */

 near relative jump */

 short relative jump */

 jcc near */

 jcc short */

	/*

	 * Jump to x86_indirect_thunk_* is treated as an indirect jump.

	 * Note that even with CONFIG_RETPOLINE=y, the kernel compiled with

	 * older gcc may use indirect jump. So we add this check instead of

	 * replace indirect-jump check.

 Decode whole function to ensure any instructions don't jump into target */

 Lookup symbol including addr */

	/*

	 * Do not optimize in the entry code due to the unstable

	 * stack handling and registers setup.

 Check there is enough space for a relative jump. */

 Decode instructions */

 Decode until function end */

			/*

			 * Since some fixup code will jumps into this function,

			 * we can't optimize kprobe in this function.

		/*

		 * In the case of detecting unknown breakpoint, this could be

		 * a padding INT3 between functions. Let's check that all the

		 * rest of the bytes are also INT3.

 Recover address */

 Check any instructions don't jump into target */

 Check optimized_kprobe can actually be optimized. */

 Check the addr is within the optimized instructions. */

 Free optimized instruction slot */

 Record the perf event before freeing the slot */

/*

 * Copy replacing target instructions

 * Target instructions MUST be relocatable (checked inside)

 * This is called when new aggr(opt)probe is allocated or reused.

	/*

	 * Verify if the address gap is in 2GB range, because this uses

	 * a relative jump.

 Copy arch-dep-instance from template */

 Copy instructions into the out-of-line buffer */

 Set probe information */

 Set probe function call */

 Set returning jmp instruction at the tail of out-of-line buffer */

	/*

	 * Note	len = TMPL_END_IDX + op->optinsn.size + JMP32_INSN_SIZE is also

	 * used in __arch_remove_optimized_kprobe().

 We have to use text_poke() for instruction buffer because it is RO */

/*

 * Replace breakpoints (INT3) with relative jumps (JMP.d32).

 * Caller must call with locking kprobe_mutex and text_mutex.

 *

 * The caller will have installed a regular kprobe and after that issued

 * syncrhonize_rcu_tasks(), this ensures that the instruction(s) that live in

 * the 4 bytes after the INT3 are unused and can now be overwritten.

 Backup instructions which will be replaced by jump address */

/*

 * Replace a relative jump (JMP.d32) with a breakpoint (INT3).

 *

 * After that, we can restore the 4 bytes after the INT3 to undo what

 * arch_optimize_kprobes() scribbled. This is safe since those bytes will be

 * unused once the INT3 lands.

/*

 * Recover original instructions and breakpoints from relative jumps.

 * Caller must call with locking kprobe_mutex.

 This kprobe is really able to run optimized path. */

 Detour through copied instructions */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Kernel Probes (KProbes)

 *

 * Copyright (C) IBM Corporation, 2002, 2004

 *

 * 2002-Oct	Created by Vamsi Krishna S <vamsi_krishna@in.ibm.com> Kernel

 *		Probes initial implementation ( includes contributions from

 *		Rusty Russell).

 * 2004-July	Suparna Bhattacharya <suparna@in.ibm.com> added jumper probes

 *		interface to access function arguments.

 * 2004-Oct	Jim Keniston <jkenisto@us.ibm.com> and Prasanna S Panchamukhi

 *		<prasanna@in.ibm.com> adapted for x86_64 from i386.

 * 2005-Mar	Roland McGrath <roland@redhat.com>

 *		Fixed to handle %rip-relative addressing mode correctly.

 * 2005-May	Hien Nguyen <hien@us.ibm.com>, Jim Keniston

 *		<jkenisto@us.ibm.com> and Prasanna S Panchamukhi

 *		<prasanna@in.ibm.com> added function-return probes.

 * 2005-May	Rusty Lynch <rusty.lynch@intel.com>

 *		Added function return probes functionality

 * 2006-Feb	Masami Hiramatsu <hiramatu@sdl.hitachi.co.jp> added

 *		kprobe-booster and kretprobe-booster for i386.

 * 2007-Dec	Masami Hiramatsu <mhiramat@redhat.com> added kprobe-booster

 *		and kretprobe-booster for x86-64

 * 2007-Dec	Masami Hiramatsu <mhiramat@redhat.com>, Arjan van de Ven

 *		<arjan@infradead.org> and Jim Keniston <jkenisto@us.ibm.com>

 *		unified x86 kprobes code.

	/*

	 * Undefined/reserved opcodes, conditional jump, Opcode Extension

	 * Groups, and some special opcodes can not boost.

	 * This is non-const and volatile to keep gcc from statically

	 * optimizing it out, as variable_test_bit makes gcc think only

	 * *(unsigned long*) is used.

      0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f          */

      ----------------------------------------------          */

 00 */

 10 */

 20 */

 30 */

 40 */

 50 */

 60 */

 70 */

 80 */

 90 */

 a0 */

 b0 */

 c0 */

 d0 */

 e0 */

 f0 */

      -----------------------------------------------         */

      0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f          */

	{"__switch_to", }, /* This function switches only current task, but

 Terminator */

 Insert a jump instruction at address 'from', which jumps to address 'to'.*/

 Insert a call instruction at address 'from', which calls address 'to'.*/

/*

 * Returns non-zero if INSN is boostable.

 * RIP relative instructions are adjusted at copying time in 64 bits mode

 Page fault may occur on this address. */

 2nd-byte opcode */

 Can't boost Address-size override prefix and CS override prefix */

 bound */

 Conditional jumps */

 Call far */

 Grp2 */

 software exceptions */

 Grp2 */

 (UD) */

 ESC */

 LOOP*, JCXZ */

 near Call, JMP */

 Short JMP */

 LOCK/REP, HLT */

 Grp3 */

 Grp4 */

 ... are not boostable */

 Grp5 */

 Only indirect jmp is boostable */

	/*

	 * Addresses inside the ftrace location are refused by

	 * arch_check_ftrace_location(). Something went terribly wrong

	 * if such an address is checked here.

	/*

	 * Use the current code if it is not modified by Kprobe

	 * and it cannot be modified by ftrace.

	/*

	 * Basically, kp->ainsn.insn has an original instruction.

	 * However, RIP-relative instruction can not do single-stepping

	 * at different place, __copy_instruction() tweaks the displacement of

	 * that instruction. In that case, we can't recover the instruction

	 * from the kp->ainsn.insn.

	 *

	 * On the other hand, in case on normal Kprobe, kp->opcode has a copy

	 * of the first byte of the probed instruction, which is overwritten

	 * by int3. And the instruction at kp->addr is not modified by kprobes

	 * except for the first byte, we can recover the original instruction

	 * from it and kp->opcode.

	 *

	 * In case of Kprobes using ftrace, we do not have a copy of

	 * the original instruction. In fact, the ftrace location might

	 * be modified at anytime and even could be in an inconsistent state.

	 * Fortunately, we know that the original code is the ideal 5-byte

	 * long NOP.

/*

 * Recover the probed instruction at addr for further analysis.

 * Caller must lock kprobes by kprobe_mutex, or disable preemption

 * for preventing to release referencing kprobes.

 * Returns zero if the instruction can not get recovered (or access failed).

 Check if paddr is at an instruction boundary */

 Decode instructions */

		/*

		 * Check if the instruction has been modified by another

		 * kprobe, in which case we replace the breakpoint by the

		 * original instruction in our buffer.

		 * Also, jump optimization will change the breakpoint to

		 * relative-jump. Since the relative-jump itself is

		 * normally used, we just go through if there is no kprobe.

		/*

		 * Another debugging subsystem might insert this breakpoint.

		 * In that case, we can't recover it.

/*

 * Copy an instruction with recovering modified instruction by kprobes

 * and adjust the displacement if the instruction uses the %rip-relative

 * addressing mode. Note that since @real will be the final place of copied

 * instruction, displacement must be adjust by @real, not @dest.

 * This returns the length of copied instruction, or 0 if it has an error.

 This can access kernel text if given address is not recovered */

 We can not probe force emulate prefixed instruction */

 Another subsystem puts a breakpoint, failed to recover */

 We should not singlestep on the exception masking instructions */

 Only x86_64 has RIP relative instructions */

		/*

		 * The copied instruction uses the %rip-relative addressing

		 * mode.  Adjust the displacement for the difference between

		 * the original location of this instruction and the location

		 * of the copy that will actually be run.  The tricky bit here

		 * is making sure that the sign extension happens correctly in

		 * this calculation, since we need a signed 32-bit result to

		 * be sign-extended to 64 bits when it's added to the %rip

		 * value and yield the same 64-bit result that the sign-

		 * extension of the original signed 32-bit displacement would

		 * have given.

 Prepare reljump or int3 right after instruction */

		/*

		 * These instructions can be executed directly if it

		 * jumps back to correct address.

 Otherwise, put an int3 for trapping singlestep */

 Make page to RO mode when allocate it */

	/*

	 * First make the page read-only, and only then make it executable to

	 * prevent it from being W+X in between.

	/*

	 * TODO: Once additional kernel code protection mechanisms are set, ensure

	 * that the page was not maliciously altered and it is still zeroed.

 Kprobe x86 instruction emulation - only regs->ip or IF flag modifiers */

 cli */

 sti */

 pushf */

 popf */

 LOOP* */

 JCXZ */

 LOOPNE */

 LOOPE */

 cli */

 sti */

 pushfl */

 popf/popfd */

		/*

		 * IF modifiers must be emulated since it will enable interrupt while

		 * int3 single stepping.

 ret/lret */

 far call absolute -- segment is not supported */

 far jmp absolute -- segment is not supported */

 int3 */

 iret -- in-kernel IRET is not supported */

 near call relative */

 short jump relative */

 near jump relative */

 1 byte conditional jump */

 2 bytes Conditional Jump */

 VM extensions - not supported */

 Loop NZ */

 Loop */

 Loop */

 J*CXZ */

		/*

		 * Since the 0xff is an extended group opcode, the instruction

		 * is determined by the MOD/RM byte.

 far call */

 call absolute, indirect */

 far jmp */

 jmp near absolute indirect */

 Don't support different size */

 TODO: support memory addressing */

 Copy an instruction with recovering if other optprobe modifies it.*/

 Analyze the opcode and setup emulate functions */

 Add int3 for single-step or booster jmp */

 Also, displacement change doesn't affect the first byte */

 OK, write back the instruction(s) into ROX insn buffer */

 insn: must be on special executable page on x86. */

 Record the perf event before freeing the slot */

 Replace the return addr with trampoline addr */

 Restore back the original saved kprobes variables and continue. */

 Boost up -- we can execute copied instructions directly */

		/*

		 * Reentering boosted probe doesn't reset current_kprobe,

		 * nor set current_kprobe, because it doesn't use single

		 * stepping.

 Disable interrupt, and set ip register on trampoline */

/*

 * Called after single-stepping.  p->addr is the address of the

 * instruction whose first byte has been replaced by the "int3"

 * instruction.  To avoid the SMP problems that can occur when we

 * temporarily put back the original opcode to single-step, we

 * single-stepped a copy of the instruction.  The address of this

 * copy is p->ainsn.insn. We also doesn't use trap, but "int3" again

 * right after the copied instruction.

 * Different from the trap single-step, "int3" single-step can not

 * handle the instruction which changes the ip register, e.g. jmp,

 * call, conditional jmp, and the instructions which changes the IF

 * flags because interrupt must be disabled around the single-stepping.

 * Such instructions are software emulated, but others are single-stepped

 * using "int3".

 *

 * When the 2nd "int3" handled, the regs->ip and regs->flags needs to

 * be adjusted, so that we can resume execution on correct code.

 Restore saved interrupt flag and ip register */

 Note that regs->ip is executed int3 so must be a step back */

/*

 * We have reentered the kprobe_handler(), since another probe was hit while

 * within the handler. We save the original kprobes variables and just single

 * step on the instruction of the new probe without calling any user handlers.

		/* A probe has been hit in the codepath leading up to, or just

		 * after, single-stepping of a probed instruction. This entire

		 * codepath should strictly reside in .kprobes.text section.

		 * Raise a BUG or we'll continue in an endless reentering loop

		 * and eventually a stack overflow.

 impossible cases */

/*

 * Interrupts are disabled on entry as trap3 is an interrupt gate and they

 * remain disabled throughout this function.

	/*

	 * We don't want to be preempted for the entire duration of kprobe

	 * processing. Since int3 and debug trap disables irqs and we clear

	 * IF while singlestepping, it must be no preemptible.

			/*

			 * If we have no pre-handler or it returned 0, we

			 * continue with normal processing.  If we have a

			 * pre-handler and it returned non-zero, that means

			 * user handler setup registers to exit to another

			 * instruction, we must skip the single stepping.

 Most provably this is the second int3 for singlestep */

		/*

		 * The breakpoint instruction was removed right

		 * after we hit it.  Another cpu has removed

		 * either a probepoint or a debugger breakpoint

		 * at this address.  In either case, no further

		 * handling of this interrupt is appropriate.

		 * Back up over the (now missing) int3 and run

		 * the original instruction.

 else: not a kprobe fault; let the kernel handle it */

/*

 * When a retprobed function returns, this code saves registers and

 * calls trampoline_handler() runs, which calls the kretprobe's handler.

 Push a fake return address to tell the unwinder it's a kretprobe. */

 Save the 'sp - 8', this will be fixed later. */

 In trampoline_handler(), 'regs->flags' is copied to 'regs->sp'. */

 Push a fake return address to tell the unwinder it's a kretprobe. */

 Save the 'sp - 4', this will be fixed later. */

 In trampoline_handler(), 'regs->flags' is copied to 'regs->sp'. */

/*

 * __kretprobe_trampoline() skips updating frame pointer. The frame pointer

 * saved in trampoline_handler() points to the real caller function's

 * frame pointer. Thus the __kretprobe_trampoline() doesn't have a

 * standard stack frame with CONFIG_FRAME_POINTER=y.

 * Let's mark it non-standard function. Anyway, FP unwinder can correctly

 * unwind without the hint.

 This is called from kretprobe_trampoline_handler(). */

 Replace fake return address with real one. */

/*

 * Called from __kretprobe_trampoline

 fixup registers */

	/*

	 * The return address at 'frame_pointer' is recovered by the

	 * arch_kretprobe_fixup_return() which called from the

	 * kretprobe_trampoline_handler().

	/*

	 * Copy FLAGS to 'pt_regs::sp' so that __kretprobe_trapmoline()

	 * can do RET right after POPF.

 This must happen on single-stepping */

		/*

		 * We are here because the instruction being single

		 * stepped caused a page fault. We reset the current

		 * kprobe and the ip points back to the probe address

		 * and allow the page fault handler to continue as a

		 * normal page fault.

		/*

		 * If the IF flag was set before the kprobe hit,

		 * don't touch it:

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2005 Intel Corporation

 * 	Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>

 * 	- Added _PDC for SMP C-states on Intel CPUs

/*

 * Initialize bm_flags based on the CPU cache properties

 * On SMP it depends on cache configuration

 * - When cache is not shared among all CPUs, we flush cache

 *   before entering C3.

 * - When cache is shared among all CPUs, we use bm_check

 *   mechanism as in UP case

 *

 * This routine is called only after all the CPUs are online

		/*

		 * Today all MP CPUs that support C3 share cache.

		 * And caches should not be flushed by software while

		 * entering C3 type state.

	/*

	 * On all recent Intel platforms, ARB_DISABLE is a nop.

	 * So, set bm_control to zero to indicate that ARB_DISABLE

	 * is not required while entering C3 type state on

	 * P4, Core and beyond CPUs

	/*

	 * For all recent Centaur CPUs, the ucode will make sure that each

	 * core can keep cache coherence with each other while entering C3

	 * type state. So, set bm_check to 1 to indicate that the kernel

	 * doesn't need to execute a cache flush operation (WBINVD) when

	 * entering C3 type state.

		/*

		 * All Zhaoxin CPUs that support C3 share cache.

		 * And caches should not be flushed by software while

		 * entering C3 type state.

		/*

		 * On all recent Zhaoxin platforms, ARB_DISABLE is a nop.

		 * So, set bm_control to zero to indicate that ARB_DISABLE

		 * is not required while entering C3 type state.

		/*

		 * For all AMD Zen or newer CPUs that support C3, caches

		 * should not be flushed by software while entering C3

		 * type state. Set bm->check to 1 so that kernel doesn't

		 * need to execute cache flush operation.

		/*

		 * In current AMD C state implementation ARB_DIS is no longer

		 * used. So set bm_control to zero to indicate ARB_DIS is not

		 * required while entering C3 type state.

 The code below handles cstate entry with monitor-mwait pair on Intel*/

 per CPU ptr */

 C-state type and not ACPI C-state type */

 Check whether this particular cx_type (in CST) is supported or not */

 If the HW does not support any sub-states in this C-state */

 mwait ecx extensions INTERRUPT_BREAK should be supported for C2/C3 */

 Make sure we are running on right CPU */

 Use the hint in CST */

	/*

	 * For _CST FFH on Intel, if GAS.access_size bit 1 is cleared,

	 * then we should skip checking BM_STS for this C-state.

	 * ref: "Intel Processor Vendor-Specific ACPI Interface Specification"

 SPDX-License-Identifier: GPL-2.0

/*

 * sleep.c - x86-specific ACPI sleep support.

 *

 *  Copyright (C) 2001-2003 Patrick Mochel

 *  Copyright (C) 2001-2003 Pavel Machek <pavel@ucw.cz>

/**

 * acpi_get_wakeup_address - provide physical address for S3 wakeup

 *

 * Returns the physical address where the kernel should be resumed after the

 * system awakes from S3, e.g. for programming into the firmware waking vector.

/**

 * x86_acpi_enter_sleep_state - enter sleep state

 * @state: Sleep state to enter.

 *

 * Wrapper around acpi_enter_sleep_state() to be called by assembly.

/**

 * x86_acpi_suspend_lowlevel - save kernel state

 *

 * Create an identity mapped page table and copy the wakeup routine to

 * low memory.

	/*

	 * We have to check that we can write back the value, and not

	 * just read it.  At least on 90 nm Pentium M (Family 6, Model

	 * 13), reading an invalid MSR is not guaranteed to trap, see

	 * Erratum X4 in "Intel Pentium M Processor on 90 nm Process

	 * with 2-MB L2 Cache and Intel® Processor A100 and A110 on 90

	 * nm process with 512-KB L2 Cache Specification Update".

 !CONFIG_64BIT */

 CONFIG_64BIT */

 CONFIG_64BIT */

	/*

	 * Pause/unpause graph tracing around do_suspend_lowlevel as it has

	 * inconsistent call/return info after it jumps to the wakeup vector.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Arch-specific APEI-related functions.

	/*

	 * We expect HEST to provide a list of MC banks that report errors

	 * in firmware first mode. Otherwise, return non-zero value to

	 * indicate that we are done parsing HEST.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * cppc_msr.c:  MSR Interface for CPPC

 * Copyright (c) 2016, Intel Corporation.

 Refer to drivers/acpi/cppc_acpi.c for the description of functions */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  boot.c - Architecture-Specific Low-Level ACPI Boot Support

 *

 *  Copyright (C) 2001, 2002 Paul Diefenbaugh <paul.s.diefenbaugh@intel.com>

 *  Copyright (C) 2001 Jun Nakajima <jun.nakajima@intel.com>

 To include x86_acpi_suspend_lowlevel */

 X86 */

 skip ACPI IRQ initialization */

 skip ACPI BGRT */

 skip ACPI PCI scan and IRQ initialization */

 ACPI SCI override configuration */

/*

 * Locks related to IOAPIC hotplug

 * Hotplug side:

 *	->device_hotplug_lock

 *		->acpi_ioapic_lock

 *			->ioapic_lock

 * Interrupt mapping side:

 *	->acpi_ioapic_lock

 *		->ioapic_mutex

 *			->ioapic_lock

/* --------------------------------------------------------------------------

                              Boot-time Configuration

/*

 * The default interrupt routing model is PIC (8259).  This gets

 * overridden if IOAPICs are enumerated (below).

/*

 * ISA irqs by default are the first 16 gsis but can be

 * any gsi as specified by an interrupt source override.

/*

 * This is just a simple wrapper around early_memremap(),

 * with sanity checks for phys == 0 and size == 0.

/**

 * acpi_register_lapic - register a local apic and generates a logic cpu number

 * @id: local apic id to register

 * @acpiid: ACPI id to register

 * @enabled: this cpu is enabled or not

 *

 * Returns the logic cpu number which maps to the local apic

 Ignore invalid ID */

	/*

	 * We need to register disabled CPU as well to permit

	 * counting disabled CPUs. This allows us to size

	 * cpus_possible_map more accurately, to permit

	 * to not preallocating memory for all NR_CPUS

	 * when we use CPU hotplug.

 Ignore invalid ID */

 don't register processors that can not be onlined */

	/*

	 * We need to register disabled CPU as well to permit

	 * counting disabled CPUs. This allows us to size

	 * cpus_possible_map more accurately, to permit

	 * to not preallocating memory for all NR_CPUS

	 * when we use CPU hotplug.

 APIC ID */

 ACPI ID */

 APIC ID */

 ACPI ID */

CONFIG_X86_LOCAL_APIC */

	/*

	 * Check bus_irq boundary.

	/*

	 * TBD: This check is for faulty timer entries, where the override

	 *      erroneously sets the trigger to level, resulting in a HUGE

	 *      increase of timer interrupts!

	/*

	 * Reset default identity mapping if gsi is also an legacy IRQ,

	 * otherwise there will be more than one entry with the same GSI

	 * and acpi_isa_irq_to_gsi() may give wrong result.

 print the entry should happen on mptable identically */

 Convert 'gsi' to 'ioapic.pin'(INTIN#) */

 Statically assign IRQ numbers for IOAPICs hosting legacy IRQs */

/*

 * Parse Interrupt Source Override for the ACPI SCI

 compatible SCI trigger is level */

 compatible SCI polarity is low */

 Command-line over-ride via acpi_sci= */

	/*

	 * stash over-ride to indicate we've been here

	 * and for later update of acpi_gbl_FADT

 TBD: Support nimsrc entries? */

 CONFIG_X86_IO_APIC */

/*

 * acpi_pic_sci_set_trigger()

 *

 * use ELCR to set PIC-mode trigger type for SCI

 *

 * If a PIC-mode SCI is not recognized or gives spurious IRQ7's

 * it may require Edge Trigger -- use "acpi_sci=edge"

 *

 * Port 0x4d0-4d1 are ELCR1 and ELCR2, the Edge/Level Control Registers

 * for the 8259 PIC.  bit[n] = 1 means irq[n] is Level, otherwise Edge.

 * ELCR1 is IRQs 0-7 (IRQ 0, 1, 2 must be 0)

 * ELCR2 is IRQs 8-15 (IRQ 8, 13 must be 0)

 Real old ELCR mask */

	/*

	 * If we use ACPI to set PCI IRQs, then we should clear ELCR

	 * since we will set it correctly as we enable the PCI irq

	 * routing.

	/*

	 * Update SCI information in the ELCR, it isn't in the PCI

	 * routing tables..

 Edge - clear */

 Level - set */

	/*

	 * Make sure all (legacy) PCI IRQs are set as level-triggered.

 Don't set up the ACPI SCI because it's already set up */

/*

 * success: return IRQ number (>=0)

 * failure: return < 0

/*

 *  ACPI based hotplug support for CPU

 CONFIG_ACPI_HOTPLUG_CPU */

/**

 * acpi_ioapic_registered - Check whether IOAPIC associated with @gsi_base

 *			    has been registered

 * @handle:	ACPI handle of the IOAPIC device

 * @gsi_base:	GSI base associated with the IOAPIC

 *

 * Assume caller holds some type of lock to serialize acpi_ioapic_registered()

 * with acpi_register_ioapic()/acpi_unregister_ioapic().

 Save CMOS port */

	/*

	 * Some broken BIOSes advertise HPET at 0x0. We really do not

	 * want to allocate a resource there.

	/*

	 * Some even more broken BIOSes advertise HPET at

	 * 0xfed0000000000000 instead of 0xfed00000. Fix it up and add

	 * some noise:

	/*

	 * Allocate and initialize the HPET firmware resource for adding into

	 * the resource tree during the lateinit timeframe.

/*

 * hpet_insert_resource inserts the HPET resources used into the resource

 * tree.

 detect the location of the ACPI PM Timer */

 FADT rev. 2 */

		/*

		 * "X" fields are optional extensions to the original V1.0

		 * fields, so we must selectively expand V1.0 fields if the

		 * corresponding X field is zero.

 FADT rev. 1 */

/*

 * Parse LAPIC entries in MADT

 * returns 0 on success, < 0 on error

	/*

	 * Note that the LAPIC address is obtained from the MADT (32-bit value)

	 * and (optionally) overridden by a LAPIC_ADDR_OVR entry (64-bit value).

 TBD: Cleanup to allow fallback to MPS */

 TBD: Cleanup to allow fallback to MPS */

 TBD: Cleanup to allow fallback to MPS */

 CONFIG_X86_LOCAL_APIC */

	/*

	 * Fabricate the legacy ISA bus (bus #31).

	/*

	 * Use the default configuration for the IRQs 0-15.  Unless

	 * overridden by (MADT) interrupt source override entries.

 Locate the gsi that irq i maps to. */

		/*

		 * Locate the IOAPIC that manages the ISA IRQ.

 Do we already have a mapping for this ISA IRQ? */

 Do we already have a mapping for this IOAPIC pin */

 IRQ already used */

 Conforming */

 Identity mapped */

/*

 * Parse IOAPIC related entries in MADT

 * returns 0 on success, < 0 on error

	/*

	 * ACPI interpreter is required to complete interrupt setup,

	 * so if it is off, don't enumerate the io-apics with ACPI.

	 * If MPS is present, it will handle them,

	 * otherwise the system will stay in PIC mode

	/*

	 * if "noapic" boot option, don't look for IO-APICs

 TBD: Cleanup to allow fallback to MPS */

	/*

	 * If BIOS did not supply an INT_SRC_OVR for the SCI

	 * pretend we got one so we can set the SCI flags.

	 * But ignore setting up SCI on hardware reduced platforms.

 Fill in identity legacy mappings where no override */

 TBD: Cleanup to allow fallback to MPS */

 !CONFIG_X86_IO_APIC */

		/*

		 * Parse MADT LAPIC entries

			/*

			 * Dell Precision Workstation 410, 610 come here.

		/*

		 * Parse MADT LAPIC entries

			/*

			 * Parse MADT IO-APIC entries

			/*

			 * Dell Precision Workstation 410, 610 come here.

		/*

 		 * ACPI found no MADT, and so ACPI wants UP PIC mode.

 		 * In the event an MPS table was found, forget it.

 		 * Boot with "acpi=off" to use MPS on such a system.

	/*

	 * ACPI supports both logical (e.g. Hyper-Threading) and physical

	 * processors, where MPS only supports physical.

/*

 * Force ignoring BIOS IRQ0 override

/*

 * ACPI offers an alternative platform interface model that removes

 * ACPI hardware requirements for platforms that do not implement

 * the PC Architecture.

 *

 * We initialize the Hardware-reduced ACPI model here:

	/*

	 * Override x86_init functions and bypass legacy PIC in

	 * hardware reduced ACPI mode.

/*

 * If your system is blacklisted here, but you find that acpi=force

 * works for you, please contact linux-acpi@vger.kernel.org

	/*

	 * Boxes that need ACPI disabled

	/*

	 * Boxes that need ACPI PCI IRQ routing disabled

 newer BIOS, Revision 1011, does work */

		/*

		 * Latest BIOS for IBM 600E (1.16) has bad pcinum

		 * for LPC bridge, which is needed for the PCI

		 * interrupt links to work. DSDT fix is in bug 5966.

		 * 2645, 2646 model numbers are shared with 600/600E/600X

	/*

	 * Boxes that need ACPI PCI IRQ routing and PCI scan disabled

 _BBN 0 bug */

 second table for DMI checks that should run after early-quirks */

	/*

	 * HP laptops which use a DSDT reporting as HP/SB400/10000,

	 * which includes some code which overrides all temperature

	 * trip points to 16C if the INTIN2 input of the I/O APIC

	 * is enabled.  This input is incorrectly designated the

	 * ISA IRQ 0 via an interrupt source override even though

	 * it is wired to the output of the master 8259A and INTIN0

	 * is not connected at all.  Force ignoring BIOS IRQ0

	 * override in that cases.

/*

 * acpi_boot_table_init() and acpi_boot_init()

 *  called from setup_arch(), always.

 *	1. checksums all tables

 *	2. enumerates lapics

 *	3. enumerates io-apics

 *

 * acpi_table_init() is separate to allow reading SRAT without

 * other side effects.

 *

 * side effects of acpi_boot_init:

 *	acpi_lapic = 1 if LAPIC found

 *	acpi_ioapic = 1 if IOAPIC found

 *	if (acpi_lapic && acpi_ioapic) smp_found_config = 1;

 *	if acpi_blacklisted() acpi_disabled = 1;

 *	acpi_irq_model=...

 *	...

	/*

	 * If acpi_disabled, bail out

	/*

	 * Initialize the ACPI boot-time table parser.

	/*

	 * blacklist may disable ACPI entirely

	/*

	 * Process the Multiple APIC Description Table (MADT), if present

	/*

	 * Hardware-reduced ACPI mode initialization:

 those are executed after early-quirks are executed */

	/*

	 * If acpi_disabled, bail out

	/*

	 * set sci_int and PM timer address

	/*

	 * Process the Multiple APIC Description Table (MADT), if present

 Do not enable ACPI SPCR console by default */

 "acpi=off" disables both ACPI table parsing and interpreter */

 acpi=force to over-ride black-list */

 acpi=strict disables out-of-spec workarounds */

 acpi=rsdt use RSDT instead of XSDT */

 "acpi=noirq" disables ACPI interrupt routing */

 "acpi=copy_dsdt" copies DSDT */

 "acpi=nocmcff" disables FF mode for corrected errors */

 Core will printk when we return error. */

 FIXME: Using pci= for an ACPI parameter is a travesty. */

 mptable code is not built-in*/

 CONFIG_X86_IO_APIC */

 SPDX-License-Identifier: GPL-2.0

/*

 * Strings for the various x86 power flags

 *

 * This file must not contain any executable code.

 temperature sensor */

 frequency id control */

 voltage id control */

 thermal trip */

 hardware thermal control */

 software thermal control */

 100 MHz multiplier control */

 hardware P-state control */

 tsc invariant mapped to constant_tsc */

 core performance boost */

 Readonly aperf/mperf */

 processor feedback interface */

 accumulated power mechanism */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This file is part of the Linux kernel.

 *

 * Copyright (c) 2011, Intel Corporation

 * Authors: Fenghua Yu <fenghua.yu@intel.com>,

 *          H. Peter Anvin <hpa@linux.intel.com>

/*

 * RDRAND has Built-In-Self-Test (BIST) that runs on every invocation.

 * Run the instruction a few times as a sanity check.

 * If it fails, it is simple to disable RDRAND here.

	/*

	 * Stupid sanity-check whether RDRAND does *actually* generate

	 * some at least random-looking data.

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel PCONFIG instruction support.

 *

 * Copyright (C) 2017 Intel Corporation

 *

 * Author:

 *	Kirill A. Shutemov <kirill.shutemov@linux.intel.com>

 Subleaf type (EAX) for PCONFIG CPUID leaf (0x1B) */

 Bitmask of supported targets */

	/*

	 * We would need to re-think the implementation once we get > 64

	 * PCONFIG targets. Spec allows up to 2^32 targets.

	/*

	 * Scan subleafs of PCONFIG CPUID leaf.

	 *

	 * Subleafs of the same type need not to be consecutive.

	 *

	 * Stop on the first invalid subleaf type. All subleafs after the first

	 * invalid are invalid too.

 Stop on the first invalid subleaf */

 Mark supported PCONFIG targets */

 Unknown CPUID.PCONFIG subleaf: ignore */

 SPDX-License-Identifier: GPL-2.0

/*

 *	Routines to identify caches on Intel CPU.

 *

 *	Changes:

 *	Venkatesh Pallipadi	: Adding cache identification through cpuid(4)

 *	Ashok Raj <ashok.raj@intel.com>: Work with CPU hotplug infrastructure.

 *	Andi Kleen / Andreas Herrmann	: CPUID4 emulation on AMD.

/* All the cache descriptor types we care about (no TLB or

 4-way set assoc, 32 byte line size */

 4-way set assoc, 32 byte line size */

 4-way set assoc, 64 byte line size */

 2 way set assoc, 32 byte line size */

 4-way set assoc, 32 byte line size */

 4-way set assoc, 64 byte line size */

 6-way set assoc, 64 byte line size */

 8-way set assoc, 64 byte line size */

 4-way set assoc, sectored cache, 64 byte line size */

 8-way set assoc, sectored cache, 64 byte line size */

 8-way set assoc, sectored cache, 64 byte line size */

 8-way set assoc, sectored cache, 64 byte line size */

 8-way set assoc, 64 byte line size */

 8-way set assoc, 64 byte line size */

 4-way set assoc, sectored cache, 64 byte line size */

 6-way set assoc, sectored cache, 64 byte line size */

 2-way set assoc, sectored cache, 64 byte line size */

 4-way set assoc, sectored cache, 64 byte line size */

 6-way set assoc, sectored cache, 64 byte line size */

 4-way set assoc, sectored cache, 64 byte line size */

 2-way set assoc, 64 byte line size */

 4-way set assoc, 32 byte line size */

 4-way set assoc, 32 byte line size */

 4-way set assoc, 32 byte line size */

 4-way set assoc, 32 byte line size */

 4-way set assoc, 32 byte line size */

 4-way set assoc, 64 byte line size */

 8-way set assoc, 64 byte line size */

 12-way set assoc, 64 byte line size */

 16-way set assoc, 64 byte line size */

 12-way set assoc, 64 byte line size */

 16-way set assoc, 64 byte line size */

 12-way set assoc, 64 byte line size */

 16-way set assoc, 64 byte line size */

 24-way set assoc, 64 byte line size */

 8-way set assoc, sectored cache, 64 byte line size */

 4-way set assoc, sectored cache, 64 byte line size */

 4-way set assoc, sectored cache, 64 byte line size */

 4-way set assoc, sectored cache, 64 byte line size */

 8-way set assoc */

 8-way set assoc */

 8-way set assoc */

 8-way set assoc */

 4-way set assoc, 64 byte line size */

 8-way set assoc, sectored cache, 64 byte line size */

 8-way set assoc, sectored cache, 64 byte line size */

 8-way set assoc, sectored cache, 64 byte line size */

 8-way set assoc, sectored cache, 64 byte line size */

 8-way set assoc, 64 byte line size */

 2-way set assoc, 64 byte line size */

 8-way set assoc, 64 byte line size */

 8-way set assoc, 32 byte line size */

 8-way set assoc, 32 byte line size */

 8-way set assoc, 32 byte line size */

 8-way set assoc, 32 byte line size */

 4-way set assoc, 64 byte line size */

 8-way set assoc, 64 byte line size */

 4-way set assoc, 64 byte line size */

 4-way set assoc, 64 byte line size */

 4-way set assoc, 64 byte line size */

 8-way set assoc, 64 byte line size */

 8-way set assoc, 64 byte line size */

 12-way set assoc, 64 byte line size */

 12-way set assoc, 64 byte line size */

 12-way set assoc, 64 byte line size */

 12-way set assoc, 64 byte line size */

 16-way set assoc, 64 byte line size */

 16-way set assoc, 64 byte line size */

 16-way set assoc, 64 byte line size */

 24-way set assoc, 64 byte line size */

 24-way set assoc, 64 byte line size */

 24-way set assoc, 64 byte line size */

/* AMD doesn't have CPUID4. Emulate it here to report the same

   information to the user.  This makes some assumptions about the machine:

   L2 not shared, no SMT etc. that is currently true on AMD CPUs.



   In theory the TLBs could be reported as fake type (they are in "dummy").

 fully associative - no way to show this currently */

 cpu_data has errata corrections for K7 applied */

/*

 * L3 cache descriptors

 calculate subcache sizes */

/*

 * check whether a slot used for disabling an L3 index is occupied.

 * @l3: L3 cache descriptor

 * @slot: slot number (0..1)

 *

 * @returns: the disabled index if used or negative value if slot free.

 check whether this slot is activated already */

	/*

	 *  disable index in all 4 subcaches

		/*

		 * We need to WBINVD on a core on the node containing the L3

		 * cache which indices we disable therefore a simple wbinvd()

		 * is not sufficient.

/*

 * disable a L3 cache index by using a disable-slot

 *

 * @l3:    L3 cache descriptor

 * @cpu:   A CPU on the node containing the L3 cache

 * @slot:  slot number (0..1)

 * @index: index to disable

 *

 * @return: 0 on success, error status on failure

  check if @slot is already used or the index is already disabled */

 check whether the other slot has disabled the same index already */

 already initialized */

 only for L3, and not in virtualized environments */

 CONFIG_AMD_NB && CONFIG_SYSFS */

 better error ? */

 Do cpuid(op) loop to find out num_cache_leaves */

	/*

	 * We may have multiple LLCs if L3 caches exist, so check if we

	 * have an L3 cache by looking at the L3 cache CPUID leaf.

 LLC is at the node level. */

		/*

		 * LLC is at the core complex level.

		 * Core complex ID is ApicId[3] for these processors.

		/*

		 * LLC ID is calculated from the number of threads sharing the

		 * cache.

	/*

	 * We may have multiple LLCs if L3 caches exist, so check if we

	 * have an L3 cache by looking at the L3 cache CPUID leaf.

	/*

	 * LLC is at the core complex level.

	 * Core complex ID is ApicId[3] for these processors.

 Cache sizes */

 Cache sizes from cpuid(4) */

 Cache sizes from cpuid(4) */

 Init num_cache_leaves from boot CPU */

		/*

		 * Whenever possible use cpuid(4), deterministic cache

		 * parameters cpuid leaf to find the cache details

	/*

	 * Don't use cpuid2 if cpuid4 is supported. For P4, we use cpuid2 for

	 * trace cache

 supports eax=2  call */

 Number of times to iterate */

 If bit 31 is set, this is an unknown format */

 Byte 0 is level count, not a descriptor */

 look up this descriptor in the table */

	/*

	 * If cpu_llc_id is not yet set, this means cpuid_level < 4 which in

	 * turns means that the only possibility is SMT (as indicated in

	 * cpuid1). Since cpuid2 doesn't specify shared caches, and we know

	 * that SMT shares all caches, we can unconditionally set cpu_llc_id to

	 * c->phys_proc_id.

	/*

	 * For L3, always use the pre-calculated cpu_llc_shared_mask

	 * to derive shared_cpu_map.

 skip if itself or no cacheinfo */

/*

 * The max shared threads number comes from CPUID.4:EAX[25-14] with input

 * ECX as cache index. Then right shift apicid by the number's order to get

 * cache id for this cache node.

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel Performance and Energy Bias Hint support.

 *

 * Copyright (C) 2019 Intel Corporation

 *

 * Author:

 *	Rafael J. Wysocki <rafael.j.wysocki@intel.com>

/**

 * DOC: overview

 *

 * The Performance and Energy Bias Hint (EPB) allows software to specify its

 * preference with respect to the power-performance tradeoffs present in the

 * processor.  Generally, the EPB is expected to be set by user space (directly

 * via sysfs or with the help of the x86_energy_perf_policy tool), but there are

 * two reasons for the kernel to update it.

 *

 * First, there are systems where the platform firmware resets the EPB during

 * system-wide transitions from sleep states back into the working state

 * effectively causing the previous EPB updates by user space to be lost.

 * Thus the kernel needs to save the current EPB values for all CPUs during

 * system-wide transitions to sleep states and restore them on the way back to

 * the working state.  That can be achieved by saving EPB for secondary CPUs

 * when they are taken offline during transitions into system sleep states and

 * for the boot CPU in a syscore suspend operation, so that it can be restored

 * for the boot CPU in a syscore resume operation and for the other CPUs when

 * they are brought back online.  However, CPUs that are already offline when

 * a system-wide PM transition is started are not taken offline again, but their

 * EPB values may still be reset by the platform firmware during the transition,

 * so in fact it is necessary to save the EPB of any CPU taken offline and to

 * restore it when the given CPU goes back online at all times.

 *

 * Second, on many systems the initial EPB value coming from the platform

 * firmware is 0 ('performance') and at least on some of them that is because

 * the platform firmware does not initialize EPB at all with the assumption that

 * the OS will do that anyway.  That sometimes is problematic, as it may cause

 * the system battery to drain too fast, for example, so it is better to adjust

 * it on CPU bring-up and if the initial EPB value for a given CPU is 0, the

 * kernel changes it to 6 ('normal').

	/*

	 * Ensure that saved_epb will always be nonzero after this write even if

	 * the EPB value read from the MSR is 0.

		/*

		 * Because intel_epb_save() has not run for the current CPU yet,

		 * it is going online for the first time, so if its EPB value is

		 * 0 ('performance') at this point, assume that it has not been

		 * initialized by the platform firmware and set it to 6

		 * ('normal').

 SPDX-License-Identifier: GPL-2.0

/*

 * No special init required for Vortex processors.

				/*

				 * Both the Vortex86EX and the Vortex86EX2

				 * have the same family and model id.

				 *

				 * However, the -EX2 supports the product name

				 * CPUID call, so this name will only be used

				 * for the -EX, which does not.

/*

 *	Routines to identify additional cpu features that are scattered in

 *	cpuid space.

/*

 * Please keep the leaf sorted by cpuid_bit.level for faster search.

 * X86_FEATURE_MBA is supported by both Intel and AMD. But the CPUID

 * levels are different and there is a separate entry for each.

 Verify that the level is valid */

 SPDX-License-Identifier: GPL-2.0

/*

 * local apic based NMI watchdog for various CPUs.

 *

 * This file also handles reservation of performance counters for coordination

 * with other users.

 *

 * Note that these events normally don't tick when the CPU idles. This means

 * the frequency varies with CPU load.

 *

 * Original code for K7/P6 written by Keith Owens

 *

/*

 * this number is calculated from Intel's MSR_P4_CRU_ESCR5 register and it's

 * offset from MSR_P4_BSU_ESCR0.

 *

 * It will be the max for all platforms (for now)

/*

 * perfctr_nmi_owner tracks the ownership of the perfctr registers:

 * evtsel_nmi_owner tracks the ownership of the event selection

 * - different performance counters/ event selection may be reserved for

 *   different subsystems this reservation system just tries to coordinate

 *   things a little

 converts an msr to an appropriate reservation bit */

 returns the bit offset of the performance counter register */

/*

 * converts an msr to an appropriate reservation bit

 * returns the bit offset of the event selection register

 returns the bit offset of the event selection register */

 register not managed by the allocator? */

 register not managed by the allocator? */

 register not managed by the allocator? */

 register not managed by the allocator? */

 SPDX-License-Identifier: GPL-2.0

 Transmeta-defined flags: level 0x80860001 */

 Print CMS and CPU revision */

 Unhide possibly hidden capability flags */

 All Transmeta CPUs have a constant TSC */

	/*

	 * randomize_va_space slows us down enormously;

	 * it probably triggers retranslation of x86->native bytecode

 SPDX-License-Identifier: GPL-2.0

	/*

	 * The high bits contain the allowed-1 settings, i.e. features that can

	 * be turned on.  The low bits contain the allowed-0 settings, i.e.

	 * features that can be turned off.  Ignore the allowed-0 settings,

	 * if a feature can be turned on then it's supported.

	 *

	 * Use raw rdmsr() for primary processor controls and pin controls MSRs

	 * as they exist on any CPU that supports VMX, i.e. we want the WARN if

	 * the RDMSR faults.

	/*

	 * Except for EPT+VPID, which enumerates support for both in a single

	 * MSR, low for EPT, high for VPID.

 Pin, EPT, VPID and VM-Func are merged into a single word. */

 EPT bits are full on scattered and must be manually handled. */

 Synthetic APIC features that are aggregates of multiple features. */

 Set the synthetic cpufeatures to preserve /proc/cpuinfo's ABI. */

 CONFIG_X86_VMX_FEATURE_NAMES */

		/*

		 * Separate out SGX driver enabling from KVM.  This allows KVM

		 * guests to use SGX even if the kernel SGX driver refuses to

		 * use it.  This happens if flexible Launch Control is not

		 * available.

	/*

	 * Ignore whatever value BIOS left in the MSR to avoid enabling random

	 * features or faulting on the WRMSR.

	/*

	 * Enable VMX if and only if the kernel may do VMXON at some point,

	 * i.e. KVM is enabled, to avoid unnecessarily adding an attack vector

	 * for the kernel, e.g. using VMX to hide malicious code.

	/*

	 * VMX feature bit may be cleared due to being disabled in BIOS,

	 * in which case SGX virtualization cannot be supported either.

 SPDX-License-Identifier: GPL-2.0

/*

 * Check for extended topology enumeration cpuid leaf 0xb and if it

 * exists, use it for populating initial_apicid and cpu topology

 * detection.

 leaf 0xb SMT level */

 extended topology sub-leaf types */

/*

 * Check if given CPUID extended topology "leaf" is implemented

/*

 * Return best CPUID Extended Topology Leaf supported

	/*

	 * initial apic id, which also represents 32-bit extended x2apic id.

/*

 * Check for extended topology enumeration cpuid leaf, and if it

 * exists, use it for populating initial_apicid and cpu topology

 * detection.

	/*

	 * Populate HT related information from sub-leaf level 0.

		/*

		 * Check for the Core type in the implemented sub leaves.

	/*

	 * Reinit the apicid, now that we have extended initial_apicid.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * HyperV  Detection code.

 *

 * Copyright (C) 2010, Novell, Inc.

 * Author : K. Y. Srinivasan <ksrinivasan@novell.com>

 Is Linux running as the root partition? */

 We have no way to deallocate the interrupt gate */

/*

 * Routines to do per-architecture handling of stimer0

 * interrupts when in Direct Mode

 For x86/x64, override weak placeholders in hyperv_timer.c */

 We have no way to deallocate the interrupt gate */

	/*

	 * Call hv_cpu_die() on all the CPUs, otherwise later the hypervisor

	 * corrupts the old VP Assist Pages and can crash the kexec kernel.

 The function calls stop_other_cpus(). */

 Disable the hypercall page when there is only 1 active CPU. */

 The function calls crash_smp_send_stop(). */

 Disable the hypercall page when there is only 1 active CPU. */

 CONFIG_KEXEC_CORE */

 CONFIG_HYPERV */

 HYPERCALL and VP_INDEX MSRs are mandatory for all features. */

/*

 * Prior to WS2016 Debug-VM sends NMIs to all CPUs which makes

 * it difficult to process CHANNELMSG_UNLOAD in case of crash. Handle

 * unknown NMI on the first CPU which gets it.

	/*

	 * Extract the features and hints

	/*

	 * Check CPU management privilege.

	 *

	 * To mirror what Windows does we should extract CPU management

	 * features and use the ReservedIdentityBit to detect if Linux is the

	 * root partition. But that requires negotiating CPU management

	 * interface (a process to be finalized).

	 *

	 * For now, use the privilege flag as the indicator for running as

	 * root.

	/*

	 * Extract host information.

		/*

		 * Get the APIC frequency.

		/*

		 * Writing to synthetic MSR 0x40000118 updates/changes the

		 * guest visible CPUIDs. Setting bit 0 of this MSR  enables

		 * guests to report invariant TSC feature through CPUID

		 * instruction, CPUID 0x800000007/EDX, bit 8. See code in

		 * early_init_intel() where this bit is examined. The

		 * setting of this MSR bit should happen before init_intel()

		 * is called.

	/*

	 * Generation 2 instances don't support reading the NMI status from

	 * 0x61 port.

	/*

	 * Hyper-V VMs have a PIT emulation quirk such that zeroing the

	 * counter register during PIT shutdown restarts the PIT. So it

	 * continues to interrupt @18.2 HZ. Setting i8253_clear_counter

	 * to false tells pit_shutdown() not to zero the counter so that

	 * the PIT really is shutdown. Generation 2 VMs don't have a PIT,

	 * and setting this value has no effect.

	/*

	 * Setup the hook to get control post apic initialization.

 Setup the IDT for hypervisor callback */

 Setup the IDT for reenlightenment notifications */

 Setup the IDT for stimer0 */

	/*

	 * Hyper-V doesn't provide irq remapping for IO-APIC. To enable x2apic,

	 * set x2apic destination mode to physical mode when x2apic is available

	 * and Hyper-V IOMMU driver makes sure cpus assigned with IO-APIC irqs

	 * have 8-bit APIC id.

 Register Hyper-V specific clocksource */

	/*

	 * TSC should be marked as unstable only after Hyper-V

	 * clocksource has been initialized. This ensures that the

	 * stability of the sched_clock is not altered.

/*

 * If ms_hyperv_msi_ext_dest_id() returns true, hyperv_prepare_irq_remapping()

 * returns -ENODEV and the Hyper-V IOMMU driver is not used; instead, the

 * generic support of the 15-bit APIC ID is used: see __irq_msi_compose_msg().

 *

 * Note: for a VM on Hyper-V, the I/O-APIC is the only device which

 * (logically) generates MSIs directly to the system APIC irq domain.

 * There is no HPET, and PCI MSI/MSI-X interrupts are remapped by the

 * pci-hyperv host bridge.

 SPDX-License-Identifier: GPL-2.0

/**

 * x86_match_cpu - match current CPU again an array of x86_cpu_ids

 * @match: Pointer to array of x86_cpu_ids. Last entry terminated with

 *         {}.

 *

 * Return the entry if the current CPU matches the entries in the

 * passed x86_cpu_id match table. Otherwise NULL.  The match table

 * contains vendor (X86_VENDOR_*), family, model and feature bits or

 * respective wildcard entries.

 *

 * A typical table entry would be to match a specific CPU

 *

 * X86_MATCH_VENDOR_FAM_MODEL_FEATURE(INTEL, 6, INTEL_FAM6_BROADWELL,

 *				      X86_FEATURE_ANY, NULL);

 *

 * Fields can be wildcarded with %X86_VENDOR_ANY, %X86_FAMILY_ANY,

 * %X86_MODEL_ANY, %X86_FEATURE_ANY (except for vendor)

 *

 * asm/cpu_device_id.h contains a set of useful macros which are shortcuts

 * for various common selections. The above can be shortened to:

 *

 * X86_MATCH_INTEL_FAM6_MODEL(BROADWELL, NULL);

 *

 * Arrays used to match for this should also be declared using

 * MODULE_DEVICE_TABLE(x86cpu, ...)

 *

 * This always matches against the boot cpu, assuming models and features are

 * consistent over all CPUs.

/*

 * VMware Detection code.

 *

 * Copyright (C) 2008, VMware, Inc.

 * Author : Alok N Kataria <akataria@vmware.com>

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License as published by

 * the Free Software Foundation; either version 2 of the License, or

 * (at your option) any later version.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or

 * NON INFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this program; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.

 *

 stolen time counter in units of vtsc */

 only for little-endian */

 steal time accounting */

/**

 * vmware_steal_clock() - read the per-cpu steal clock

 * @cpu:            the cpu number whose steal clock we want to read

 *

 * The function reads the steal clock if we are on a 64-bit system, otherwise

 * reads it in parts, checking that the high part didn't change in the

 * meantime.

 *

 * Return:

 *      The steal clock reading in ns.

 Do not reorder initial_high and high readings */

 Keep low reading in between */

 We use reboot notifier only to disable steal clock */

/*

 * VMware hypervisor takes care of exporting a reliable TSC to the guest.

 * Still, due to timing difference when running on virtual cpus, the TSC can

 * be marked as unstable in some cases. For example, the TSC sync check at

 * bootup can fail due to a marginal offset between vcpus' TSCs (though the

 * TSCs do not drift from each other).  Also, the ACPI PM timer clocksource

 * is not suitable as a watchdog when running on a hypervisor because the

 * kernel may miss a wrap of the counter if the vcpu is descheduled for a

 * long time. To skip these checks at runtime we set these capability bits,

 * so that the kernel could just trust the hypervisor with providing a

 * reliable virtual TSC that is suitable for timekeeping.

 Skip lapic calibration since we know the bus frequency. */

/*

 * While checking the dmi string information, just checking the product

 * serial key should be enough, as this will always have a VMware

 * specific string when running under VMware hypervisor.

 * If !boot_cpu_has(X86_FEATURE_HYPERVISOR), vmware_hypercall_mode

 * intentionally defaults to 0.

 Checks if hypervisor supports x2apic without VT-D interrupt remapping. */

 Copy VMWARE specific Hypercall parameters to the GHCB */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Hygon Processor Support for Linux

 *

 * Copyright (C) 2018 Chengdu Haiguang IC Design Co., Ltd.

 *

 * Author: Pu Wen <puwen@hygon.cn>

/*

 * nodes_per_socket: Stores the number of nodes per socket.

 * Refer to CPUID Fn8000_001E_ECX Node Identifiers[10:8]

/*

 * To workaround broken NUMA config.  Read the comment in

 * srat_detect_node().

 Shouldn't happen */

/*

 * Fixup core topology information for

 * (1) Hygon multi-node processors

 *     Assumption: Number of cores in each internal node is the same.

 * (2) Hygon processors supporting compute units

 get information required for multi-node processors */

		/*

		 * In case leaf B is available, use it to derive

		 * topology information.

 Socket ID is ApicId[6] for these processors. */

/*

 * On Hygon setup the lower bits of the APIC id distinguish the cores.

 * Assumes number of cores is a power of two.

 Low order bits define the core id (index of core in socket) */

 Convert the initial APIC ID into the socket ID */

 use socket ID also for last level cache */

	/*

	 * On multi-fabric platform (e.g. Numascale NumaChip) a

	 * platform-specific handler needs to be called to fixup some

	 * IDs of the CPU.

		/*

		 * Two possibilities here:

		 *

		 * - The CPU is missing memory and no node was created.  In

		 *   that case try picking one from a nearby CPU.

		 *

		 * - The APIC IDs differ from the HyperTransport node IDs.

		 *   Assume they are all increased by a constant offset, but

		 *   in the same order as the HT nodeids.  If that doesn't

		 *   result in a usable node fall back to the path for the

		 *   previous case.

		 *

		 * This workaround operates directly on the mapping between

		 * APIC ID and NUMA node, assuming certain relationship

		 * between APIC ID, HT node ID and NUMA topology.  As going

		 * through CPU mapping may alter the outcome, directly

		 * access __apicid_to_node[].

 Pick a nearby node */

 Multi core CPU? */

 CPU telling us the core id bits shift? */

 Otherwise recompute */

		/*

		 * Try to cache the base value so further operations can

		 * avoid RMW. If that faults, do not enable SSBD.

	/*

	 * c->x86_power is 8000_0007 edx. Bit 8 is TSC runs at constant rate

	 * with P/T states and does not stop in deep C-states

 Bit 12 of 8000_0007 edx is accumulated power mechanism. */

 Bit 14 indicates the Runtime Average Power Limit interface. */

	/*

	 * ApicID can always be treated as an 8-bit value for Hygon APIC So, we

	 * can safely set X86_FEATURE_EXTD_APICID unconditionally.

	/*

	 * This is only needed to tell the kernel whether to use VMCALL

	 * and VMMCALL.  VMMCALL is never executed except under virt, so

	 * we can set it unconditionally.

	/*

	 * Bit 31 in normal CPUID used for nonstandard 3DNow ID;

	 * 3DNow is IDd by bit 31 in extended CPUID (1*32+31) anyway

 get apicid instead of initial apic id from cpuid */

		/*

		 * Use LFENCE for execution serialization.  On families which

		 * don't have that MSR, LFENCE is already serializing.

		 * msr_set_bit() uses the safe accessors, too, even if the MSR

		 * is not present.

 A serializing LFENCE stops RDTSC speculation */

	/*

	 * Hygon processors have APIC timer running in deep C states.

 Hygon CPUs don't reset SS attributes on SYSRET, Xen does. */

 Handle DTLB 2M and 4M sizes, fall back to L1 if L2 is disabled */

 a 4M entry uses two 2M entries */

 Handle ITLB 2M and 4M sizes, fall back to L1 if L2 is disabled */

 SPDX-License-Identifier: GPL-2.0

 MSR_VIA_FCR */

 MSR_VIA_RNG */

 Test for Centaur Extended Feature Flags presence */

 enable ACE unit, if present and disabled */

 enable ACE unit */

 enable RNG unit, if present and disabled */

 enable RNG unit */

		/* store Centaur Extended Feature Flags as

		 * word 5 of the CPU capability bit array

 Cyrix III family needs CX8 & PGE explicitly enabled. */

 Before Nehemiah, the C3's had 3dNOW! */

 Emulate MTRRs using Centaur's MCR. */

	/*

	 * Bit 31 in normal CPUID used for nonstandard 3DNow ID;

	 * 3DNow is IDd by bit 31 in extended CPUID (1*32+31) anyway

		/*

		 * Check for version and the number of counters

		 * Version(eax[7:0]) can't be 0;

		 * Counters(eax[15:8]) should be greater than 1;

 Emulate MTRRs using Centaur's MCR. */

 Report CX8 */

 Set 3DNow! on Winchip 2 and above. */

 See if we can find out some more. */

 Yes, we can. */

 Add L1 data and code cache sizes. */

 VIA C3 CPUs (670-68F) need further shifting. */

	/*

	 * There's also an erratum in Nehemiah stepping 1, which

	 * returns '65KB' instead of '64KB'

	 *  - Note, it seems this may only be in engineering samples.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * nodes_per_socket: Stores the number of nodes per socket.

 * Refer to Fam15h Models 00-0fh BKDG - CPUID Fn8000_001E_ECX

 * Node Identifiers[10:8]

/*

 *	B step AMD K6 before B 9730xxxx have hardware bugs that can cause

 *	misexecution of code under Linux. Owners of such processors should

 *	contact AMD for precise details and a CPU swap.

 *

 *	See	http://www.multimania.com/poulot/k6bug.html

 *	and	section 2.6.2 of "AMD-K6 Processor Revision Guide - Model 6"

 *		(Publication # 21266  Issue Date: August 1998)

 *

 *	The following test is erm.. interesting. AMD neglected to up

 *	the chip setting when fixing the bug but they also tweaked some

 *	performance at the same time..

/*

 * General Systems BIOSen alias the cpu frequency registers

 * of the Elan at 0x000df000. Unfortunately, one of the Linux

 * drivers subsequently pokes it, and changes the CPU speed.

 * Workaround : Remove the unneeded alias.

 Configuration Base Address  (32-bit) */

 Based on AMD doc 20734R - June 2000 */

		/*

		 * It looks like AMD fixed the 2.6.2 bug and improved indirect

		 * calls at the same time.

 K6 with old style WHCR */

 We can only write allocate on the low 508Mb */

 The more serious chips .. */

 AMD Geode LX is model 10 */

 placeholder for any needed mods */

	/*

	 * Bit 15 of Athlon specific MSR 15, needs to be 0

	 * to enable SSE on Palomino/Morgan/Barton CPU's.

	 * If the BIOS didn't enable it already, enable it here.

	/*

	 * It's been determined by AMD that Athlons since model 8 stepping 1

	 * are more robust with CLK_CTL set to 200xxxxx instead of 600xxxxx

	 * As per AMD technical note 27212 0.2

 calling is from identify_secondary_cpu() ? */

	/*

	 * Certain Athlons might work (for various values of 'work') in SMP

	 * but they are not certified as MP capable.

 Athlon 660/661 is valid. */

 Duron 670 is valid */

	/*

	 * Athlon 662, Duron 671, and Athlon >model 7 have capability

	 * bit. It's worth noting that the A5 stepping (662) of some

	 * Athlon XP's have the MP bit set.

	 * See http://www.heise.de/newsticker/data/jow-18.10.01-000 for

	 * more.

 If we get here, not a certified SMP capable AMD system. */

	/*

	 * Don't taint if we are running SMP kernel on a single non-MP

	 * approved Athlon

/*

 * To workaround broken NUMA config.  Read the comment in

 * srat_detect_node().

 Shouldn't happen */

/*

 * Fix up cpu_core_id for pre-F17h systems to be in the

 * [0 .. cores_per_node - 1] range. Not really needed but

 * kept so as not to break existing setups.

/*

 * Fixup core topology information for

 * (1) AMD multi-node processors

 *     Assumption: Number of cores in each internal node is the same.

 * (2) AMD processors supporting compute units

 get information required for multi-node processors */

		/*

		 * In case leaf B is available, use it to derive

		 * topology information.

/*

 * On a AMD dual core setup the lower bits of the APIC id distinguish the cores.

 * Assumes number of cores is a power of two.

 Low order bits define the core id (index of core in socket) */

 Convert the initial APIC ID into the socket ID */

 use socket ID also for last level cache */

 When PPIN is defined in CPUID, still need to check PPIN_CTL MSR */

 PPIN is locked in disabled mode, clear feature bit */

 If PPIN is disabled, try to enable it */

 If PPIN_EN bit is 1, return from here; otherwise fall through */

	/*

	 * On multi-fabric platform (e.g. Numascale NumaChip) a

	 * platform-specific handler needs to be called to fixup some

	 * IDs of the CPU.

		/*

		 * Two possibilities here:

		 *

		 * - The CPU is missing memory and no node was created.  In

		 *   that case try picking one from a nearby CPU.

		 *

		 * - The APIC IDs differ from the HyperTransport node IDs

		 *   which the K8 northbridge parsing fills in.  Assume

		 *   they are all increased by a constant offset, but in

		 *   the same order as the HT nodeids.  If that doesn't

		 *   result in a usable node fall back to the path for the

		 *   previous case.

		 *

		 * This workaround operates directly on the mapping between

		 * APIC ID and NUMA node, assuming certain relationship

		 * between APIC ID, HT node ID and NUMA topology.  As going

		 * through CPU mapping may alter the outcome, directly

		 * access __apicid_to_node[].

 Pick a nearby node */

 Multi core CPU? */

 CPU telling us the core id bits shift? */

 Otherwise recompute */

 A random value per boot for bit slice [12:upper_bit) */

		/*

		 * Try to cache the base value so further operations can

		 * avoid RMW. If that faults, do not enable SSBD.

	/*

	 * BIOS support is required for SME and SEV.

	 *   For SME: If BIOS has enabled SME then adjust x86_phys_bits by

	 *	      the SME physical address space reduction value.

	 *	      If BIOS has not enabled SME then don't advertise the

	 *	      SME feature (set in scattered.c).

	 *   For SEV: If BIOS has not enabled SEV then don't advertise the

	 *            SEV and SEV_ES feature (set in scattered.c).

	 *

	 *   In all cases, since support for SME and SEV requires long mode,

	 *   don't advertise the feature under CONFIG_X86_32.

 Check if memory encryption is enabled */

		/*

		 * Always adjust physical address bits. Even though this

		 * will be a value above 32-bits this is still done for

		 * CONFIG_X86_32 so that accurate values are reported.

	/*

	 * c->x86_power is 8000_0007 edx. Bit 8 is TSC runs at constant rate

	 * with P/T states and does not stop in deep C-states

 Bit 12 of 8000_0007 edx is accumulated power mechanism. */

 Bit 14 indicates the Runtime Average Power Limit interface. */

  Set MTRR capability flag if appropriate */

	/*

	 * ApicID can always be treated as an 8-bit value for AMD APIC versions

	 * >= 0x10, but even old K8s came out of reset with version 0x10. So, we

	 * can safely set X86_FEATURE_EXTD_APICID unconditionally for families

	 * after 16h.

 check CPU config space for extended APIC ID */

	/*

	 * This is only needed to tell the kernel whether to use VMCALL

	 * and VMMCALL.  VMMCALL is never executed except under virt, so

	 * we can set it unconditionally.

 F16h erratum 793, CVE-2013-6885 */

	/*

	 * Check whether the machine is affected by erratum 400. This is

	 * used to select the proper idle routine and to enable the check

	 * whether the machine is affected in arch_post_acpi_init(), which

	 * sets the X86_BUG_AMD_APIC_C1E bug depending on the MSR check.

 Re-enable TopologyExtensions if switched off by BIOS */

 On C+ stepping K8 rep microcode works well for copy/memset */

	/*

	 * Some BIOSes incorrectly force this feature, but only K8 revision D

	 * (model = 0x14) and later actually support it.

	 * (AMD Erratum #110, docId: 25759).

	/*

	 * Disable TLB flush filter by setting HWCR.FFDIS on K8

	 * bit 6 of msr C001_0015

	 *

	 * Errata 63 for SH-B3 steppings

	 * Errata 122 for all steppings (F+ have it disabled by default)

 do this for boot cpu */

	/*

	 * Disable GART TLB Walk Errors on Fam10h. We do this here because this

	 * is always needed when GART is enabled, even in a kernel which has no

	 * MCE support built in. BIOS should disable GartTlbWlk Errors already.

	 * If it doesn't, we do it here as suggested by the BKDG.

	 *

	 * Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=33012

	/*

	 * On family 10h BIOS may not have properly enabled WC+ support, causing

	 * it to be converted to CD memtype. This may result in performance

	 * degradation for certain nested-paging guests. Prevent this conversion

	 * by clearing bit 24 in MSR_AMD64_BU_CFG2.

	 *

	 * NOTE: we want to use the _safe accessors so as not to #GP kvm

	 * guests on older kvm hosts.

	/*

	 * Apply erratum 665 fix unconditionally so machines without a BIOS

	 * fix work.

	/*

	 * Saving of the MSR used to hide the RDRAND support during

	 * suspend/resume is done by arch/x86/power/cpu.c, which is

	 * dependent on CONFIG_PM_SLEEP.

	/*

	 * The nordrand option can clear X86_FEATURE_RDRAND, so check for

	 * RDRAND support using the CPUID function directly.

	/*

	 * Verify that the CPUID change has occurred in case the kernel is

	 * running virtualized and the hypervisor doesn't support the MSR.

	/*

	 * Some BIOS implementations do not restore proper RDRAND support

	 * across suspend and resume. Check on whether to hide the RDRAND

	 * instruction support via CPUID.

	/*

	 * The way access filter has a performance penalty on some workloads.

	 * Disable it on the affected CPUs.

	/*

	 * Some BIOS implementations do not restore proper RDRAND support

	 * across suspend and resume. Check on whether to hide the RDRAND

	 * instruction support via CPUID.

	/*

	 * Fix erratum 1076: CPB feature bit not being set in CPUID.

	 * Always set it, except when running under a hypervisor.

	/*

	 * Bit 31 in normal CPUID used for nonstandard 3DNow ID;

	 * 3DNow is IDd by bit 31 in extended CPUID (1*32+31) anyway

 get apicid instead of initial apic id from cpuid */

 K6s reports MCEs but don't actually have all the MSRs */

	/*

	 * Enable workaround for FXSAVE leak on CPUs

	 * without a XSaveErPtr feature

		/*

		 * Use LFENCE for execution serialization.  On families which

		 * don't have that MSR, LFENCE is already serializing.

		 * msr_set_bit() uses the safe accessors, too, even if the MSR

		 * is not present.

 A serializing LFENCE stops RDTSC speculation */

	/*

	 * Family 0x12 and above processors have APIC timer

	 * running in deep C states.

 3DNow or LM implies PREFETCHW */

 AMD CPUs don't reset SS attributes on SYSRET, Xen does. */

	/*

	 * Turn on the Instructions Retired free counter on machines not

	 * susceptible to erratum #1054 "Instructions Retired Performance

	 * Counter May Be Inaccurate".

 AMD errata T13 (order #21922) */

 Duron Rev A0 */

 Tbird rev A1/A2 */

	/*

	 * K8 doesn't have 2M/4M entries in the L2 TLB so read out the L1 TLB

	 * characteristics from the CPUID function 0x80000005 instead.

 Handle DTLB 2M and 4M sizes, fall back to L1 if L2 is disabled */

 a 4M entry uses two 2M entries */

 Handle ITLB 2M and 4M sizes, fall back to L1 if L2 is disabled */

 Erratum 658 */

/*

 * AMD errata checking

 *

 * Errata are defined as arrays of ints using the AMD_LEGACY_ERRATUM() or

 * AMD_OSVW_ERRATUM() macros. The latter is intended for newer errata that

 * have an OSVW id assigned, which it takes as first argument. Both take a

 * variable number of family-specific model-stepping ranges created by

 * AMD_MODEL_RANGE().

 *

 * Example:

 *

 * const int amd_erratum_319[] =

 *	AMD_LEGACY_ERRATUM(AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0x4, 0x2),

 *			   AMD_MODEL_RANGE(0x10, 0x8, 0x0, 0x8, 0x0),

 *			   AMD_MODEL_RANGE(0x10, 0x9, 0x0, 0x9, 0x0));

 #1054: Instructions Retired Performance Counter May Be Inaccurate */

 OSVW unavailable or ID unknown, match family-model-stepping range */

 SPDX-License-Identifier: GPL-2.0

 MSR_ZHAOXIN_FCR */

 MSR_ZHAOXIN_RNG */

 Test for Extended Feature Flags presence */

 Enable ACE unit, if present and disabled */

 Enable ACE unit */

 Enable RNG unit, if present and disabled */

 Enable RNG unit */

		/*

		 * Store Extended Feature Flags as word 5 of the CPU

		 * capability bit array

		/*

		 * If HTT (EDX[28]) is set EBX[16:23] contain the number of

		 * apicids which are reserved per package. Store the resulting

		 * shift value for the package management code.

		/*

		 * Check for version and the number of counters

		 * Version(eax[7:0]) can't be 0;

		 * Counters(eax[15:8]) should be greater than 1;

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1994  Linus Torvalds

 *

 *  Cyrix stuff, June 1998 by:

 *	- Rafael R. Reilova (moved everything from head.S),

 *        <rreilova@ececs.uc.edu>

 *	- Channing Corn (tests & fixes),

 *	- Andrew D. Balsa (code cleanup).

 The base value of the SPEC_CTRL MSR that always has to be preserved. */

/*

 * The vendor and possibly platform specific bits which can be modified in

 * x86_spec_ctrl_base.

/*

 * AMD specific MSR info for Speculative Store Bypass control.

 * x86_amd_ls_cfg_ssbd_mask is initialized in identify_boot_cpu().

 Control conditional STIBP in switch_to() */

 Control conditional IBPB in switch_mm() */

 Control unconditional IBPB in switch_mm() */

 Control MDS CPU buffer clear before returning to user space */

 Control MDS CPU buffer clear before idling (halt, mwait) */

/*

 * Controls whether l1d flush based mitigations are enabled,

 * based on hw features and admin setting via boot parameter

 * defaults to false

	/*

	 * identify_boot_cpu() initialized SMT support information, let the

	 * core code know.

	/*

	 * Read the SPEC_CTRL MSR to account for reserved bits which may

	 * have unknown values. AMD64_LS_CFG MSR is cached in the early AMD

	 * init code as it is not enumerated and depends on the family.

 Allow STIBP in MSR_SPEC_CTRL if supported */

 Select the proper CPU mitigations before patching alternatives: */

	/*

	 * As MDS and TAA mitigations are inter-related, print MDS

	 * mitigation until after TAA mitigation selection is done.

	/*

	 * Check whether we are able to run this kernel safely on SMP.

	 *

	 * - i386 is no longer supported.

	 * - In order to run on anything without a TSC, we need to be

	 *   compiled for a i486.

 CONFIG_X86_64 */

	/*

	 * Make sure the first 2MB area is not mapped by huge pages

	 * There are typically fixed size MTRRs in there and overlapping

	 * MTRRs into large pages causes slow downs.

	 *

	 * Right now we don't do that with gbpages because there seems

	 * very little benefit for that case.

 Is MSR_SPEC_CTRL implemented ? */

		/*

		 * Restrict guest_spec_ctrl to supported values. Clear the

		 * modifiable bits in the host base value and or the

		 * modifiable bits from the guest value.

 SSBD controlled in MSR_SPEC_CTRL */

 Conditional STIBP enabled? */

	/*

	 * If SSBD is not handled in MSR_SPEC_CTRL on AMD, update

	 * MSR_AMD64_L2_CFG or MSR_VIRT_SPEC_CTRL if supported.

	/*

	 * If the host has SSBD mitigation enabled, force it in the host's

	 * virtual MSR value. If its not permanently enabled, evaluate

	 * current's TIF_SSBD thread flag.

 Sanitize the guest value */

 Default mitigation for MDS-affected CPUs */

 Default mitigation for TAA-affected CPUs */

 TSX previously disabled by tsx=off */

	/*

	 * TAA mitigation via VERW is turned off if both

	 * tsx_async_abort=off and mds=off are specified.

	/*

	 * VERW doesn't clear the CPU buffers when MD_CLEAR=1 and MDS_NO=1.

	 * A microcode update fixes this behavior to clear CPU buffers. It also

	 * adds support for MSR_IA32_TSX_CTRL which is enumerated by the

	 * ARCH_CAP_TSX_CTRL_MSR bit.

	 *

	 * On MDS_NO=1 CPUs if ARCH_CAP_TSX_CTRL_MSR is not set, microcode

	 * update is required.

	/*

	 * TSX is enabled, select alternate mitigation for TAA which is

	 * the same as MDS. Enable MDS static branch to clear CPU buffers.

	 *

	 * For guests that can't determine whether the correct microcode is

	 * present on host, enable the mitigation for UCODE_NEEDED as well.

	/*

	 * Update MDS mitigation, if necessary, as the mds_user_clear is

	 * now enabled for TAA mitigation.

	/*

	 * Check to see if this is one of the MDS_NO systems supporting

	 * TSX that are only exposed to SRBDS when TSX is enabled.

/*

 * Does SMAP provide full mitigation against speculative kernel access to

 * userspace?

	/*

	 * On CPUs which are vulnerable to Meltdown, SMAP does not

	 * prevent speculative access to user data in the L1 cache.

	 * Consider SMAP to be non-functional as a mitigation on these

	 * CPUs.

		/*

		 * With Spectre v1, a user can speculatively control either

		 * path of a conditional swapgs with a user-controlled GS

		 * value.  The mitigation is to add lfences to both code paths.

		 *

		 * If FSGSBASE is enabled, the user can put a kernel address in

		 * GS, in which case SMAP provides no protection.

		 *

		 * If FSGSBASE is disabled, the user can only put a user space

		 * address in GS.  That makes an attack harder, but still

		 * possible if there's no SMAP protection.

			/*

			 * Mitigation can be provided from SWAPGS itself or

			 * PTI as the CR3 write in the Meltdown mitigation

			 * is serializing.

			 *

			 * If neither is there, mitigate with an LFENCE to

			 * stop speculation through swapgs.

			/*

			 * Enable lfences in the kernel entry (non-swapgs)

			 * paths, to prevent user entry from speculatively

			 * skipping swapgs.

 The kernel command line selection for spectre v2 */

 Initialize Indirect Branch Prediction Barrier */

	/*

	 * If no STIBP, enhanced IBRS is enabled or SMT impossible, STIBP is not

	 * required.

	/*

	 * At this point, an STIBP mode other than "off" has been set.

	 * If STIBP support is not being forced, check if STIBP always-on

	 * is preferred.

	/*

	 * If the CPU is not affected and the command line mode is NONE or AUTO

	 * then nothing to do.

 Force it so VMEXIT will restore correctly */

	/*

	 * If spectre v2 protection has been enabled, unconditionally fill

	 * RSB during a context switch; this protects against two independent

	 * issues:

	 *

	 *	- RSB underflow (and switch to BTB) on Skylake+

	 *	- SpectreRSB variant of spectre v2 on X86_BUG_SPECTRE_V2 CPUs

	/*

	 * Retpoline means the kernel is safe because it has no indirect

	 * branches. Enhanced IBRS protects firmware too, so, enable restricted

	 * speculation around firmware calls only when Enhanced IBRS isn't

	 * supported.

	 *

	 * Use "mode" to check Enhanced IBRS instead of boot_cpu_has(), because

	 * the user might select retpoline on the kernel command line and if

	 * the CPU supports Enhanced IBRS, kernel might un-intentionally not

	 * enable IBRS around firmware calls.

 Set up IBPB and STIBP depending on the general spectre V2 command */

 Update x86_spec_ctrl_base in case SMT state changed. */

 Update the static key controlling the evaluation of TIF_SPEC_IB */

 Update the static key controlling the MDS CPU buffer clear in idle */

	/*

	 * Enable the idle clearing if SMT is active on CPUs which are

	 * affected only by MSBDS and not any other MDS variant.

	 *

	 * The other variants cannot be mitigated when SMT is enabled, so

	 * clearing the buffers on idle just to prevent the Store Buffer

	 * repartitioning leak would be a window dressing exercise.

www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.\n"

www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html for more details.\n"

 The kernel command line selection */

 Platform decides */

 Disable Speculative Store Bypass */

 Don't touch Speculative Store Bypass */

 Disable Speculative Store Bypass via prctl */

 Disable Speculative Store Bypass via prctl and seccomp */

		/*

		 * Choose prctl+seccomp as the default mode if seccomp is

		 * enabled.

	/*

	 * If SSBD is controlled by the SPEC_CTRL MSR, then set the proper

	 * bit in the mask to allow guests to use the mitigation even in the

	 * case where the host does not enable it.

	/*

	 * We have three CPU feature flags that are in play here:

	 *  - X86_BUG_SPEC_STORE_BYPASS - CPU is susceptible.

	 *  - X86_FEATURE_SSBD - CPU is able to turn off speculative store bypass

	 *  - X86_FEATURE_SPEC_STORE_BYPASS_DISABLE - engage the mitigation

		/*

		 * Intel uses the SPEC CTRL MSR Bit(2) for this, while AMD may

		 * use a completely different MSR and bit dependent on family.

 Force the update of the real TIF bits */

	/*

	 * Immediately update the speculation control MSRs for the current

	 * task, but for a non-current task delay setting the CPU

	 * mitigation until it is scheduled next.

	 *

	 * This can only happen for SECCOMP mitigation. For PRCTL it's

	 * always the current task.

 If speculation is force disabled, enable is not allowed */

		/*

		 * With strict mode for both IBPB and STIBP, the instruction

		 * code paths avoid checking this task flag and instead,

		 * unconditionally run the instruction. However, STIBP and IBPB

		 * are independent and either can be set to conditionally

		 * enabled regardless of the mode of the other.

		 *

		 * If either is set to conditional, allow the task flag to be

		 * updated, unless it was force-disabled by a previous prctl

		 * call. Currently, this is possible on an AMD CPU which has the

		 * feature X86_FEATURE_AMD_STIBP_ALWAYS_ON. In this case, if the

		 * kernel is booted with 'spectre_v2_user=seccomp', then

		 * spectre_v2_user_ibpb == SPECTRE_V2_USER_SECCOMP and

		 * spectre_v2_user_stibp == SPECTRE_V2_USER_STRICT_PREFERRED.

		/*

		 * Indirect branch speculation is always allowed when

		 * mitigation is force disabled.

 Default mitigation for L1TF-affected CPUs */

/*

 * These CPUs all support 44bits physical address space internally in the

 * cache but CPUID can report a smaller number of physical address bits.

 *

 * The L1TF mitigation uses the top most address bit for the inversion of

 * non present PTEs. When the installed memory reaches into the top most

 * address bit due to memory holes, which has been observed on machines

 * which report 36bits physical address bits and have 32G RAM installed,

 * then the mitigation range check in l1tf_select_mitigation() triggers.

 * This is a false positive because the mitigation is still possible due to

 * the fact that the cache uses 44bit internally. Use the cache bits

 * instead of the reported physical bits and adjust them on the affected

 * machines to 44bit if the reported bits are less than 44.

www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html might help you decide.\n");

 SPDX-License-Identifier: GPL-2.0

/*

 * Cache IA32_UMWAIT_CONTROL MSR. This is a systemwide control. By default,

 * umwait max time is 100000 in TSC-quanta and C0.2 is enabled

/*

 * Cache the original IA32_UMWAIT_CONTROL MSR value which is configured by

 * hardware or BIOS before kernel boot.

/*

 * Serialize access to umwait_control_cached and IA32_UMWAIT_CONTROL MSR in

 * the sysfs write functions.

/*

 * The CPU hotplug callback sets the control MSR to the global control

 * value.

 *

 * Disable interrupts so the read of umwait_control_cached and the WRMSR

 * are protected against a concurrent sysfs write. Otherwise the sysfs

 * write could update the cached value after it had been read on this CPU

 * and issue the IPI before the old value had been written. The IPI would

 * interrupt, write the new value and after return from IPI the previous

 * value would be written by this CPU.

 *

 * With interrupts disabled the upcoming CPU either sees the new control

 * value or the IPI is updating this CPU to the new control value after

 * interrupts have been reenabled.

/*

 * The CPU hotplug callback sets the control MSR to the original control

 * value.

	/*

	 * This code is protected by the CPU hotplug already and

	 * orig_umwait_control_cached is never changed after it caches

	 * the original control MSR value in umwait_init(). So there

	 * is no race condition here.

/*

 * On resume, restore IA32_UMWAIT_CONTROL MSR on the boot processor which

 * is the only active CPU at this time. The MSR is set up on the APs via the

 * CPU hotplug callback.

 *

 * This function is invoked on resume from suspend and hibernation. On

 * resume from suspend the restore should be not required, but we neither

 * trust the firmware nor does it matter if the same value is written

 * again.

 sysfs interface */

/*

 * When bit 0 in IA32_UMWAIT_CONTROL MSR is 1, C0.2 is disabled.

 * Otherwise, C0.2 is enabled.

 Propagate to all CPUs */

 bits[1:0] must be zero */

	/*

	 * Cache the original control MSR value before the control MSR is

	 * changed. This is the only place where orig_umwait_control_cached

	 * is modified.

		/*

		 * On failure, the control MSR on all CPUs has the

		 * original control value.

	/*

	 * Add umwait control interface. Ignore failure, so at least the

	 * default values are set up in case the machine manages to boot.

 Declare dependencies between CPUIDs */

/*

 * Table of CPUID features that depend on others.

 *

 * This only includes dependencies that can be usefully disabled, not

 * features part of the base set (like FPU).

 *

 * Note this all is not __init / __initdata because it can be

 * called from cpu hotplug. It shouldn't do anything in this case,

 * but it's difficult to tell that to the init reference checker.

	/*

	 * Note: This could use the non atomic __*_bit() variants, but the

	 * rest of the cpufeature code uses atomics as well, so keep it for

	 * consistency. Cleanup all of it separately.

 Take the capabilities and the BUG bits into account */

 Collect all features to disable, handling dependencies */

 Loop until we get a stable state. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Read NSC/Cyrix DEVID registers (DIR) to get more detailed info. about the CPU

 we test for DEVID by checking whether CCR3 is writable */

 dummy to change bus */

 no DEVID regs. */

 dummy */

 old Cx486SLC/DLC */

 Cx486S A step */

 restore CCR3 */

 read DIR0 and DIR1 CPU registers */

/*

 * Cx86_dir0_msb is a HACK needed by check_cx686_cpuid/slop in bugs.h in

 * order to identify the Cyrix CPU model after we're out of setup.c

 *

 * Actually since bugs.h doesn't even reference this perhaps someone should

 * fix the documentation ???

/*

 * Reset the slow-loop (SLOP) bit on the 686(L) which is set by some old

 * BIOSes for compatibility with DOS games.  This makes the udelay loop

 * work correctly, and improves performance.

 *

 * FIXME: our newer udelay uses the tsc. We don't need to frob with SLOP

 enable MAPEN */

 reset SLOP */

 disable MAPEN */

 possible wrong calibration done */

 enable MAPEN */

 Load/Store Serialize to mem access disable (=reorder it) */

 set load/store serialize from 1GB to 4GB */

 CCR2 bit 2: unlock NW bit */

 set 'Not Write-through' */

 CCR2 bit 2: lock NW bit and set WT1 */

/*

 *	Configure later MediaGX and/or Geode processor.

 Suspend on halt power saving and enable #SUSP pin */

 enable MAPEN */

 FPU fast, DTE cache, Mem bypass */

 disable MAPEN */

 identifies CPU "family"   */

 6x86/6x86L */

 Emulate MTRRs using Cyrix's ARRs. */

 6x86MX/M II */

 Emulate MTRRs using Cyrix's ARRs. */

	/*

	 * Bit 31 in normal CPUID used for nonstandard 3DNow ID;

	 * 3DNow is IDd by bit 31 in extended CPUID (1*32+31) anyway

 Cyrix used bit 24 in extended (AMD) CPUID for Cyrix MMX extensions */

 identifies CPU "family"   */

 model or clock multiplier */

 common case step number/rev -- exceptions handled below */

	/* Now cook; the original recipe is by Channing Corn, from Cyrix.

	 * We do the same thing for each generation: we work out

	 * the model, multiplier and stepping.  Black magic included,

	 * to make the silicon step/rev numbers match the printed ones.

 Cx486SLC/DLC/SRx/DRx */

 Cx486S/DX/DX2/DX4 */

 5x86 */

 6x86/6x86L */

 686L */

 686 */

 Emulate MTRRs using Cyrix's ARRs. */

 6x86's contain this bug */

 MediaGX/GXm or Geode GXM/GXLV/GX1 */

 GX1 with inverted Device ID */

		/*

		 * It isn't really a PCI quirk directly, but the cure is the

		 * same. The MediaGX has deep magic SMM stuff that handles the

		 * SB emulation. It throws away the fifo on disable_dma() which

		 * is wrong and ruins the audio.

		 *

		 *  Bug2: VSA1 has a wrap bug so that using maximum sized DMA

		 *  causes bad things. According to NatSemi VSA2 has another

		 *  bug to do with 'hlt'. I've not seen any boards using VSA2

		 *  and X doesn't seem to support it either so who cares 8).

		 *  VSA1 we work around however.

		/* We do this before the PCI layer is running. However we

		   are safe here as we know the bridge must be a Cyrix

		/*

		 *  The 5510/5520 companion chips have a funky PIT.

 Yep 16K integrated cache that's it */

 GXm supports extended cpuid levels 'ala' AMD */

 Enable cxMMX extensions (GX1 Datasheet 54) */

			/*

			 * GXm : 0x30 ... 0x5f GXm  datasheet 51

			 * GXlv: 0x6x          GXlv datasheet 54

			 *  ?  : 0x7x

			 * GX1 : 0x8x          GX1  datasheet 56

 MediaGX */

 6x86MX/M II */

 M II */

 Enable MMX extensions (App note 108) */

 A 6x86MX - it has the bug. */

 Emulate MTRRs using Cyrix's ARRs. */

 Cyrix 486 without DEVID registers */

 either a 486SLC or DLC w/o DEVID */

 a 486S A step */

 unknown (shouldn't happen, we know everyone ;-) */

/*

 * Handle National Semiconductor branded processors

	/*

	 * There may be GX1 processors in the wild that are branded

	 * NSC and not Cyrix.

	 *

	 * This function only handles the GX processor, and kicks every

	 * thing else to the Cyrix init function above - that should

	 * cover any processors that might have been branded differently

	 * after NSC acquired Cyrix.

	 *

	 * If this breaks your GX1 horribly, please e-mail

	 * info-linux@ldcmail.amd.com to tell us.

 Handle the GX (Formally known as the GX2) */

/*

 * Cyrix CPUs without cpuid or with cpuid not yet enabled can be detected

 * by the fact that they preserve the flags across the division of 5/2.

 * PII and PPro exhibit this behavior too, but they have cpuid available.

/*

 * Perform the Cyrix 5/2 test. A Cyrix won't change

 * the flags, while other 486 chips will.

 clear flags (%eax = 0x0005) */

 divide 5 by 2 */

 store flags into %ah */

 AH is 0x02 on Cyrix after the divide.. */

 Detect Cyrix with disabled CPUID */

 Actually enable cpuid on the older cyrix */

 Retrieve CPU revisions */

 Check it is an affected model */

 enable MAPEN  */

 enable cpuid  */

 disable MAPEN */

 SPDX-License-Identifier: GPL-2.0

/*

 * UMC chips appear to be only either 386 or 486,

 * so no special init takes place.

/*

 * Common hypervisor code

 *

 * Copyright (C) 2008, VMware, Inc.

 * Author : Alok N Kataria <akataria@vmware.com>

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License as published by

 * the Free Software Foundation; either version 2 of the License, or

 * (at your option) any later version.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or

 * NON INFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this program; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel Transactional Synchronization Extensions (TSX) control.

 *

 * Copyright (C) 2019-2021 Intel Corporation

 *

 * Author:

 *	Pawan Gupta <pawan.kumar.gupta@linux.intel.com>

 Force all transactions to immediately abort */

	/*

	 * Ensure TSX support is not enumerated in CPUID.

	 * This is visible to userspace and will ensure they

	 * do not waste resources trying TSX transactions that

	 * will always abort.

 Enable the RTM feature in the cpu */

	/*

	 * Ensure TSX support is enumerated in CPUID.

	 * This is visible to userspace and will ensure they

	 * can enumerate and use the TSX feature.

	/*

	 * TSX is controlled via MSR_IA32_TSX_CTRL.  However, support for this

	 * MSR is enumerated by ARCH_CAP_TSX_MSR bit in MSR_IA32_ARCH_CAPABILITIES.

	 *

	 * TSX control (aka MSR_IA32_TSX_CTRL) is only available after a

	 * microcode update on CPUs that have their MSR_IA32_ARCH_CAPABILITIES

	 * bit MDS_NO=1. CPUs with MDS_NO=0 are not planned to get

	 * MSR_IA32_TSX_CTRL support even after a microcode update. Thus,

	 * tsx= cmdline requests will do nothing on CPUs without

	 * MSR_IA32_TSX_CTRL support.

	/*

	 * MSR_TFA_TSX_CPUID_CLEAR bit is only present when both CPUID

	 * bits RTM_ALWAYS_ABORT and TSX_FORCE_ABORT are present.

	/*

	 * Hardware will always abort a TSX transaction if both CPUID bits

	 * RTM_ALWAYS_ABORT and TSX_FORCE_ABORT are set. In this case, it is

	 * better not to enumerate CPUID.RTM and CPUID.HLE bits. Clear them

	 * here.

 tsx= not provided */

		/*

		 * tsx_disable() will change the state of the RTM and HLE CPUID

		 * bits. Clear them here since they are now expected to be not

		 * set.

		/*

		 * HW defaults TSX to be enabled at bootup.

		 * We may still need the TSX enable support

		 * during init for special cases like

		 * kexec after TSX is disabled.

		/*

		 * tsx_enable() will change the state of the RTM and HLE CPUID

		 * bits. Force them here since they are now expected to be set.

 SPDX-License-Identifier: GPL-2.0

/*

 *	Get CPU information for use by the procfs.

 Cache size */

 SPDX-License-Identifier: GPL-2.0-only

 cpu_feature_enabled() cannot be used this early */

 all of these masks are initialized in setup_cpu_local_masks() */

 representing cpus for which sibling maps can be computed */

 Number of siblings per CPU package */

 Last level cache ID of each logical CPU */

 L2 cache ID of each logical CPU */

 correctly size the local cpu masks */

 Not much we can do here... */

 Check if at least it has cpuid */

 No cpuid. It must be an ancient CPU */

	/*

	 * We need valid kernel segments for data and code in long mode too

	 * IRET will check the segment types  kkeil 2000/10/28

	 * Also sysret mandates a special GDT layout

	 *

	 * TLS descriptors are currently at a different place compared to i386.

	 * Hopefully nobody expects them at a fixed place (Wine?)

	/*

	 * Segments used for calling PnP BIOS have byte granularity.

	 * They code segments and data segments have fixed 64k limits,

	 * the transfer segment sizes are set at run time.

 32-bit code */

 16-bit code */

 16-bit data */

 16-bit data */

 16-bit data */

	/*

	 * The APM segments have byte granularity and their bases

	 * are set at run time.  All have 64k limits.

 32-bit code */

 16-bit code */

 data */

 nopcid doesn't accept parameters */

 do not emit a message if the feature is not present */

 noinvpcid doesn't accept parameters */

 do not emit a message if the feature is not present */

 Standard macro to see if a specific flag is changeable */

	/*

	 * Cyrix and IDT cpus allow disabling of CPUID

	 * so the code below may return different results

	 * when it is executed before and after enabling

	 * the CPUID. Add "volatile" to not allow gcc to

	 * optimize the subsequent calls to this function.

 Probe for the CPUID instruction */

 Disable processor serial number: */

 Disabling the serial number may affect the cpuid level */

 This should have been cleared long ago */

 Check the boot processor, plus build option for UMIP. */

 Check the current processor's cpuid bits. */

	/*

	 * Make sure UMIP is disabled in case it was enabled in a

	 * previous boot (e.g., via kexec).

 These bits should not change their value after CPU init is finished. */

 Warn after we've set the missing bits. */

 Warn after we've corrected the changed bits. */

 Read the CR4 shadow. */

 Initialize cr4 shadow for this CPU. */

/*

 * Once CPU feature detection is finished (and boot params have been

 * parsed), record any of the sensitive CR bits that are set, and

 * enable CR pinning.

 Require an exact match without trailing characters. */

 Do not emit a message if the feature is not present. */

/*

 * Protection Keys are not available in 32-bit mode.

		/*

		 * Setting CR4.PKE will cause the X86_FEATURE_OSPKE cpuid

		 * bit to be set.  Enforce it.

 Load the default PKRU value */

	/*

	 * Do not clear the X86_FEATURE_PKU bit.  All of the

	 * runtime checks are against OSPKE so clearing the

	 * bit does nothing.

	 *

	 * This way, we will see "pku" in cpuinfo, but not

	 * "ospke", which is exactly what we want.  It shows

	 * that the CPU has PKU, but the OS has not enabled it.

	 * This happens to be exactly how a system would look

	 * if we disabled the config option.

 CONFIG_X86_64 */

/*

 * Some CPU features depend on higher CPUID levels, which may not always

 * be available due to CPUID level capping or broken virtualization

 * software.  Add those features to this table to auto-disable them.

		/*

		 * Note: cpuid_level is set to -1 if unavailable, but

		 * extended_extended_level is set to 0 if unavailable

		 * and the legitimate extended levels are all negative

		 * when signed; hence the weird messing around with

		 * signs here...

/*

 * Naming convention should be: <Name> [(<Codename>)]

 * This table only is used unless init_<vendor>() below doesn't set it;

 * in particular, if CPUID levels 0x80000002..4 are supported, this

 * isn't used

 Look up CPU names by table lookup. */

 Range check */

 Not found */

 Aligned to unsigned long to avoid split lock in atomic bitmap ops */

 The 32-bit entry code needs to find cpu_entry_area. */

 Load the original GDT from the per-cpu structure */

 Load a fixmap remapping of the per-cpu GDT */

/*

 * Current gdt points %fs at the "master" per-cpu area: after this,

 * it's on the real one.

 Load the original GDT */

 Reload the per-cpu base */

 Trim whitespace */

 Note the last non-whitespace index */

 On K8 L1 TLB is inclusive, so don't count it */

 Some chips just has a large L1. */

 do processor-specific cache resizing */

 Allow user to override all this if necessary. */

 Again, no L2 cache is possible */

 Get vendor name */

 Intel-defined flags: level 0x00000001 */

	/*

	 * The Intel SPEC_CTRL CPUID bit implies IBRS and IBPB support,

	 * and they also have a different bit for STIBP support. Also,

	 * a hypervisor might have set the individual AMD bits even on

	 * Intel CPUs, for finer-grained selection of what's available.

 Intel-defined flags: level 0x00000001 */

 Thermal and Power Management Leaf: level 0x00000006 (eax) */

 Additional Intel-defined flags: level 0x00000007 */

 Check valid sub-leaf index before accessing it */

 Extended state features: level 0x0000000d */

 AMD-defined flags: level 0x80000001 */

	/*

	 * Clear/Set all flags overridden by options, after probe.

	 * This needs to happen each time we re-probe, which may happen

	 * several times during CPU initialization.

	/*

	 * First of all, decide if this is a 486 or higher

	 * It's a 486 if we can modify the AC flag

 Intel Family 6 */

	/*

	 * Technically, swapgs isn't serializing on AMD (despite it previously

	 * being documented as such in the APM).  But according to AMD, %gs is

	 * updated non-speculatively, and the issuing of %gs-relative memory

	 * operands will be blocked until the %gs update completes, which is

	 * good enough for our purposes.

 AMD Family 0xf - 0x12 */

 FAMILY_ANY must be last, otherwise 0x0f - 0x12 matches won't work */

 Zhaoxin Family 7 */

 Set ITLB_MULTIHIT bug if cpu is not in the whitelist and not mitigated */

	/*

	 * When the CPU is not mitigated for TAA (TAA_NO=0) set TAA bug when:

	 *	- TSX is supported or

	 *	- TSX_CTRL is present

	 *

	 * TSX_CTRL check is needed for cases when TSX could be disabled before

	 * the kernel boot e.g. kexec.

	 * TSX_CTRL check alone is not sufficient for cases when the microcode

	 * update is not present or running as guest that don't get TSX_CTRL.

	/*

	 * SRBDS affects CPUs which support RDRAND or RDSEED and are listed

	 * in the vulnerability blacklist.

 Rogue Data Cache Load? No! */

/*

 * The NOPL instruction is supposed to exist on all CPUs of family >= 6;

 * unfortunately, that's not true in practice because of early VIA

 * chips and (more importantly) broken virtualizers that are not easy

 * to detect. In the latter case it doesn't even *fail* reliably, so

 * probing for it doesn't even work. Disable it completely on 32-bit

 * unless we can find a reliable way to detect all the broken cases.

 * Enable it explicitly on 64-bit for non-constant inputs of cpu_has().

/*

 * We parse cpu parameters early because fpu__init_system() is executed

 * before parse_early_param().

 If the argument was too long, the last bit may be cut off */

/*

 * Do minimum CPU detection early.

 * Fields really needed: vendor, cpuid_level, family, model, mask,

 * cache alignment.

 * The others are not touched to avoid unwanted side effects.

 *

 * WARNING: this function is only called on the boot CPU.  Don't add code

 * here that is supposed to run on all CPUs.

 cyrix could have cpuid enabled via c_identify()*/

	/*

	 * Regardless of whether PCID is enumerated, the SDM says

	 * that it can't be enabled in 32-bit mode.

	/*

	 * Later in the boot process pgtable_l5_enabled() relies on

	 * cpu_feature_enabled(X86_FEATURE_LA57). If 5-level paging is not

	 * enabled by this point we need to clear the feature bit to avoid

	 * false-positives at the later stage.

	 *

	 * pgtable_l5_enabled() can be false here for several reasons:

	 *  - 5-level paging is disabled compile-time;

	 *  - it's 32-bit kernel;

	 *  - machine doesn't support 5-level paging;

	 *  - user specified 'no5lvl' in kernel command line.

	/*

	 * Empirically, writing zero to a segment selector on AMD does

	 * not clear the base, whereas writing zero to a segment

	 * selector on Intel does clear the base.  Intel's behavior

	 * allows slightly faster context switches in the common case

	 * where GS is unused by the prev and next threads.

	 *

	 * Since neither vendor documents this anywhere that I can see,

	 * detect it directly instead of hard-coding the choice by

	 * vendor.

	 *

	 * I've designated AMD's behavior as the "bug" because it's

	 * counterintuitive and less friendly.

 BUG_NULL_SEG is only relevant with 64bit userspace */

 Zen3 CPUs advertise Null Selector Clears Base in CPUID. */

	/*

	 * CPUID bit above wasn't set. If this kernel is still running

	 * as a HV guest, then the HV has decided not to advertize

	 * that CPUID bit for whatever reason.	For example, one

	 * member of the migration pool might be vulnerable.  Which

	 * means, the bug is present: set the BUG flag and return.

	/*

	 * Zen2 CPUs also have this behaviour, but no CPUID bit.

	 * 0x18 is the respective family for Hygon.

 All the remaining ones are affected */

 cyrix could have cpuid enabled via c_identify()*/

 Default name */

	/*

	 * ESPFIX is a strange bug.  All real CPUs have it.  Paravirt

	 * systems that run Linux at CPL > 0 may or may not have the

	 * issue, but, even if they have the issue, there's absolutely

	 * nothing we can do about it because we can't use the real IRET

	 * instruction.

	 *

	 * NB: For the time being, only 32-bit kernels support

	 * X86_BUG_ESPFIX as such.  64-bit kernels directly choose

	 * whether to apply espfix using paravirt hooks.  If any

	 * non-paravirt system ever shows up that does *not* have the

	 * ESPFIX issue, we can change this.

/*

 * Validate that ACPI/mptables have the same information about the

 * effective APIC id and update the package map.

/*

 * This does the hard work of actually picking apart the CPU stuff...

 So far unknown... */

 Unset */

 Unset */

 CPUID not detected */

 Clear/Set all flags overridden by options, after probe */

	/*

	 * Vendor-specific initialization.  In this section we

	 * canonicalize the feature flags, meaning if there are

	 * features a certain CPU supports which CPUID doesn't

	 * tell us, CPUID claiming incorrect flags, or other bugs,

	 * we handle them here.

	 *

	 * At the end of this section, c->x86_capability better

	 * indicate the features this CPU genuinely supports!

 Disable the PN if appropriate */

 Set up SMEP/SMAP/UMIP */

 Enable FSGSBASE instructions if available. */

	/*

	 * The vendor-specific functions might have changed features.

	 * Now we do "generic changes."

 Filter out anything that depends on CPUID levels we don't have */

 If the model name is still unset, do table lookup. */

 Last resort... */

	/*

	 * Clear/Set all flags overridden by options, need do it

	 * before following smp all cpus cap AND.

	/*

	 * On SMP, boot_cpu_data holds the common feature set between

	 * all CPUs; so make sure that we indicate which features are

	 * common between the CPUs.  The first time this routine gets

	 * executed, c == &boot_cpu_data.

 AND the already accumulated flags with these */

 OR, i.e. replicate the bug flags */

 Init Machine Check Exception if available. */

/*

 * Set up the CPU state needed to execute SYSENTER/SYSEXIT instructions

 * on 32-bit kernels:

	/*

	 * We cache MSR_IA32_SYSENTER_CS's value in the TSS's ss1 field --

	 * see the big comment in struct x86_hw_tss's definition.

/*

 * clearcpuid= was already parsed in cpu_parse_early_param().  This dummy

 * function prevents it from becoming an environment variable for init.

/*

 * The following percpu variables are hot.  Align current_task to

 * cacheline size such that they fall in the same cacheline.

 May not be marked __init: used by software suspend */

	/*

	 * This only works on Intel CPUs.

	 * On AMD CPUs these MSRs are 32-bit, CPU truncates MSR_IA32_SYSENTER_EIP.

	 * This does not cause SYSENTER to jump to the wrong location, because

	 * AMD doesn't allow SYSENTER in long mode (either 32- or 64-bit).

	/*

	 * Flags to clear on syscall; clear as much as possible

	 * to minimize user space-kernel interference.

 CONFIG_X86_64 */

/*

 * On x86_32, vm86 modifies tss.sp0, so sp0 isn't a reliable way to find

 * the top of the kernel stack.  Use an extra percpu variable to track the

 * top of the kernel stack directly.

 CONFIG_X86_64 */

/*

 * Clear all 6 debug registers:

 Ignore db4, db5 */

/*

 * Restore debug regs if using kgdbwait and you have a kernel debugger

 * connection established.

 ! CONFIG_KGDB */

 ! CONFIG_KGDB */

	/*

	 * wait for ACK from master CPU before continuing

	 * with AP initialization

 Store CPU and node number in limit. */

 RO data, expand down, accessed */

 Visible to user code */

 Not a system segment */

 Present */

 32-bit */

 Set up the per-CPU TSS IST stacks */

 Only mapped when SEV-ES is active */

 CONFIG_X86_64 */

 !CONFIG_X86_64 */

	/*

	 * Invalidate the extra array entry past the end of the all

	 * permission bitmap as required by the hardware.

/*

 * Setup everything needed to handle exceptions from the IDT, including the IST

 * exceptions which use paranoid_entry().

 paranoid_entry() gets the CPU number from the GDT */

 IST vectors need TSS to be set up. */

 Finally load the IDT */

/*

 * cpu_init() initializes state that is per-CPU. Some data is already

 * initialized (naturally) in the bootstrap process, such as the GDT.  We

 * reload it nevertheless, this function acts as a 'CPU state barrier',

 * nothing should get across.

	/*

	 * Initialize the per-CPU GDT with the boot GDT,

	 * and set up the GDT descriptor:

	/*

	 * sp0 points to the entry trampoline stack regardless of what task

	 * is running.

	/*

	 * Relies on the BP having set-up the IDT tables, which are loaded

	 * on this CPU in cpu_init_exception_handling().

/*

 * The microcode loader calls this upon late microcode load to recheck features,

 * only when microcode has been updated. Caller holds microcode_mutex and CPU

 * hotplug lock.

 Reload CPUID max function as it might've changed. */

	/*

	 * Copy all capability leafs to pick up the synthetic ones so that

	 * memcmp() below doesn't fail on that. The ones coming from CPUID will

	 * get overwritten in get_cpu_cap().

/*

 * Invoked from core CPU hotplug code after hotplug operations

 Handle the speculative execution misfeatures */

 Check whether IPI broadcasting can be enabled */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * x86 APERF/MPERF KHz calculation for

 * /sys/.../cpufreq/scaling_cur_freq

 *

 * Copyright (C) 2017 Intel Corp.

 * Author: Len Brown <len.brown@intel.com>

/*

 * aperfmperf_snapshot_khz()

 * On the current CPU, snapshot APERF, MPERF, and jiffies

 * unless we already did it within 10ms

 * calculate kHz, save snapshot

	/*

	 * There is no architectural guarantee that MPERF

	 * increments faster than we can read it.

 Don't bother re-computing within the cache threshold time. */

 Return false if the previous iteration was too long ago. */

 Idle CPUs are completely uninteresting. */

 Idle CPUs are completely uninteresting. */

 ->scfpending before smp_call_function_single(). */

 SPDX-License-Identifier: GPL-2.0

/*

 * ACRN detection support

 *

 * Copyright (C) 2019 Intel Corporation. All rights reserved.

 *

 * Jason Chen CJ <jason.cj.chen@intel.com>

 * Zhao Yakui <yakui.zhao@intel.com>

 *

 Setup the IDT for ACRN hypervisor callback */

	/*

	 * The hypervisor requires that the APIC EOI should be acked.

	 * If the APIC EOI is not acked, the APIC ISR bit for the

	 * HYPERVISOR_CALLBACK_VECTOR will not be cleared and then it

	 * will block the interrupt whose vector is lower than

	 * HYPERVISOR_CALLBACK_VECTOR.

 SPDX-License-Identifier: GPL-2.0

/*

 * Default to sld_off because most systems do not support split lock detection.

 * sld_state_setup() will switch this to sld_warn on systems that support

 * split lock/bus lock detect, unless there is a command line override.

/*

 * With a name like MSR_TEST_CTL it should go without saying, but don't touch

 * MSR_TEST_CTL unless the CPU is one of the whitelisted models.  Writing it

 * on CPUs that do not support SLD can cause fireworks, even when writing '0'.

/*

 * Processors which have self-snooping capability can handle conflicting

 * memory type across CPUs by snooping its own cache. However, there exists

 * CPU models in which having conflicting memory types still leads to

 * unpredictable behavior, machine check errors, or hangs. Clear this

 * feature to prevent its use on machines with known erratas.

	/*

	 * Ring 3 MONITOR/MWAIT feature cannot be detected without

	 * cpu model and family comparison.

/*

 * Early microcode releases for the Spectre v2 mitigation were broken.

 * Information taken from;

 * - https://newsroom.intel.com/wp-content/uploads/sites/11/2018/03/microcode-update-guidance.pdf

 * - https://kb.vmware.com/s/article/52345

 * - Microcode revisions observed in the wild

 * - Release note from 20180108 microcode release

 Observed in the wild */

	/*

	 * We know that the hypervisor lie to us on the microcode version so

	 * we may as well hope that it is running the correct version.

 Unmask CPUID levels if masked: */

 Now if any of them are set, check the blacklist and clear the lot */

	/*

	 * Atom erratum AAE44/AAF40/AAG38/AAH41:

	 *

	 * A race condition between speculative fetches and invalidating

	 * a large page.  This is worked around in microcode, but we

	 * need the microcode to have already been loaded... so if it is

	 * not, recommend a BIOS update and disable large pages.

 Netburst reports 64 bytes clflush size, but does IO in 128 bytes */

 CPUID workaround for 0F33/0F34 CPU */

	/*

	 * c->x86_power is 8000_0007 edx. Bit 8 is TSC runs at constant rate

	 * with P/T states and does not stop in deep C-states.

	 *

	 * It is also reliable across cores and sockets. (but not across

	 * cabinets - we turn it off in that case explicitly.)

 Penwell and Cloverview have the TSC which doesn't sleep on S3 */

	/*

	 * There is a known erratum on Pentium III and Core Solo

	 * and Core Duo CPUs.

	 * " Page with PAT set to WC while associated MTRR is UC

	 *   may consolidate to UC "

	 * Because of this erratum, it is better to stick with

	 * setting WC in MTRR rather than using PAT on these CPUs.

	 *

	 * Enable PAT WC only on P4, Core 2 or later CPUs.

	/*

	 * If fast string is not enabled in IA32_MISC_ENABLE for any reason,

	 * clear the fast string and enhanced fast string CPU capabilities.

	/*

	 * Intel Quark Core DevMan_001.pdf section 6.4.11

	 * "The operating system also is required to invalidate (i.e., flush)

	 *  the TLB when any changes are made to any of the page table entries.

	 *  The operating system must reload CR3 to cause the TLB to be flushed"

	 *

	 * As a result, boot_cpu_has(X86_FEATURE_PGE) in arch/x86/include/asm/tlbflush.h

	 * should be false so that __flush_tlb_all() causes CR3 instead of CR4.PGE

	 * to be modified.

		/*

		 * If HTT (EDX[28]) is set EBX[16:23] contain the number of

		 * apicids which are reserved per package. Store the resulting

		 * shift value for the package management code.

	/*

	 * Get the number of SMT siblings early from the extended topology

	 * leaf, if available. Otherwise try the legacy SMT detection.

/*

 *	Early probe support logic for ppro memory erratum #50

 *

 *	This is called before we do cpu ident work

 Uses data from early_cpu_detect now */

 calling is from identify_secondary_cpu() ? */

	/*

	 * Mask B, Pentium, but not Pentium MMX

		/*

		 * Remember we have B step Pentia with bugs

	/*

	 * All models of Pentium and Pentium with MMX technology CPUs

	 * have the F0 0F bug, which lets nonprivileged users lock up the

	 * system. Announce that the fault handler will be checking for it.

	 * The Quark is also family 5, but does not have the same bug.

	/*

	 * SEP CPUID bug: Pentium Pro reports SEP but doesn't have it until

	 * model 3 mask 3

	/*

	 * PAE CPUID issue: many Pentium M report no PAE but may have a

	 * functionally usable PAE implementation.

	 * Forcefully enable PAE if kernel parameter "forcepae" is present.

	/*

	 * P4 Xeon erratum 037 workaround.

	 * Hardware prefetcher may cause stale data to be loaded into the cache.

	/*

	 * See if we have a good local APIC by checking for buggy Pentia,

	 * i.e. all B steppings and the C2 stepping of P54C when using their

	 * integrated APIC (see 11AP erratum in "Pentium Processor

	 * Specification Update").

	/*

	 * Set up the preferred alignment for movsl bulk memory moves

 486: untested */

 Old Pentia: untested */

 PII/PIII only like movsl with 8-byte alignment */

 P4 is OK down to 8-byte alignment */

	/* Don't do the funky fallback heuristics the AMD version employs

 reuse the value from init_cpu_to_node() */

 Helpers to access TME_ACTIVATE MSR */

 Bits 7:4 */

 Bits 35:32 */

 Bits 63:48 */

 Values for mktme_status (SW only construct) */

 Broken BIOS? */

 Proceed. We may need to exclude bits from x86_phys_bits. */

 MKTME is usable */

	/*

	 * KeyID bits effectively lower the number of physical address

	 * bits.  Update cpuinfo_x86::x86_phys_bits accordingly.

 Clear all MISC features */

 Check features and update capabilities and shadow control bits */

	/*

	 * Detect the extended topology information if available. This

	 * will reinitialise the initial_apicid which will be used

	 * in init_intel_cacheinfo()

		/*

		 * let's use the legacy cpuid vector 0x1 and 0x4 for topology

		 * detection.

 Check for version and the number of counters */

	/*

	 * Names for the Pentium II/Celeron processors

	 * detectable only by also checking the cache size.

	 * Dixon is NOT a Celeron.

 Work around errata */

	/*

	 * Intel PIII Tualatin. This comes in two flavours.

	 * One has 256kb of cache, the other 512. We have no way

	 * to determine which, so we use a boottime override

	 * for the 512kb model, and assume 256 otherwise.

	/*

	 * Intel Quark SoC X1000 contains a 4-way set associative

	 * 16K cache with a 16 byte cache line and 256 lines per tag

 look up this descriptor in the table */

 Number of times to iterate */

 If bit 31 is set, this is an unknown format */

 Byte 0 is level count, not a descriptor */

	/*

	 * Min ratelimit is 1 bus lock/sec.

	 * Max ratelimit is 1000 bus locks/sec.

 Restore the MSR to its cached value. */

/*

 * MSR_TEST_CTRL is per core, but we treat it like a per CPU MSR. Locking

 * is not implemented as one thread could undo the setting of the other

 * thread immediately after dropping the lock anyway.

	/*

	 * #DB for bus lock handles ratelimit and #AC for split lock is

	 * disabled.

	/*

	 * Disable the split lock detection for this task so it can make

	 * progress and set TIF_SLD so the detection is re-enabled via

	 * switch_to_sld() when the task is scheduled out.

	/*

	 * Warn and fatal are handled by #AC for split lock if #AC for

	 * split lock is supported.

	/*

	 * Enable #DB for bus lock. All bus locks are handled in #DB except

	 * split locks are handled in #AC in the fatal case.

 Enforce no more than bld_ratelimit bus locks/sec. */

 Warn on the bus lock. */

/*

 * This function is called only when switching between tasks with

 * different split-lock detection modes. It sets the MSR for the

 * mode of the new task. This is right most of the time, but since

 * the MSR is shared by hyperthreads on a physical core there can

 * be glitches when the two threads need different modes.

/*

 * Bits in the IA32_CORE_CAPABILITIES are not architectural, so they should

 * only be trusted if it is confirmed that a CPU model implements a

 * specific feature at a particular bit position.

 *

 * The possible driver data field values:

 *

 * - 0: CPU models that are known to have the per-core split-lock detection

 *	feature even though they do not enumerate IA32_CORE_CAPABILITIES.

 *

 * - 1: CPU models which may enumerate IA32_CORE_CAPABILITIES and if so use

 *      bit 5 to enumerate the per-core split-lock detection feature.

/**

 * get_this_hybrid_cpu_type() - Get the type of this hybrid CPU

 *

 * Returns the CPU type [31:24] (i.e., Atom or Core) of a CPU in

 * a hybrid processor. If the processor is not hybrid, returns 0.

 SPDX-License-Identifier: GPL-2.0

/*

 * Resource Director Technology (RDT)

 *

 * Pseudo-locking support built on top of Cache Allocation Technology (CAT)

 *

 * Copyright (C) 2018 Intel Corporation

 *

 * Author: Reinette Chatre <reinette.chatre@intel.com>

 For X86_CONFIG() */

/*

 * The bits needed to disable hardware prefetching varies based on the

 * platform. During initialization we will discover which bits to use.

/*

 * Major number assigned to and shared by all devices exposing

 * pseudo-locked regions.

/**

 * get_prefetch_disable_bits - prefetch disable bits of supported platforms

 * @void: It takes no parameters.

 *

 * Capture the list of platforms that have been validated to support

 * pseudo-locking. This includes testing to ensure pseudo-locked regions

 * with low cache miss rates can be created under variety of load conditions

 * as well as that these pseudo-locked regions can maintain their low cache

 * miss rates under variety of load conditions for significant lengths of time.

 *

 * After a platform has been validated to support pseudo-locking its

 * hardware prefetch disable bits are included here as they are documented

 * in the SDM.

 *

 * When adding a platform here also add support for its cache events to

 * measure_cycles_perf_fn()

 *

 * Return:

 * If platform is supported, the bits to disable hardware prefetchers, 0

 * if platform is not supported.

		/*

		 * SDM defines bits of MSR_MISC_FEATURE_CONTROL register

		 * as:

		 * 0    L2 Hardware Prefetcher Disable (R/W)

		 * 1    L2 Adjacent Cache Line Prefetcher Disable (R/W)

		 * 2    DCU Hardware Prefetcher Disable (R/W)

		 * 3    DCU IP Prefetcher Disable (R/W)

		 * 63:4 Reserved

		/*

		 * SDM defines bits of MSR_MISC_FEATURE_CONTROL register

		 * as:

		 * 0     L2 Hardware Prefetcher Disable (R/W)

		 * 1     Reserved

		 * 2     DCU Hardware Prefetcher Disable (R/W)

		 * 63:3  Reserved

/**

 * pseudo_lock_minor_get - Obtain available minor number

 * @minor: Pointer to where new minor number will be stored

 *

 * A bitmask is used to track available minor numbers. Here the next free

 * minor number is marked as unavailable and returned.

 *

 * Return: 0 on success, <0 on failure.

/**

 * pseudo_lock_minor_release - Return minor number to available

 * @minor: The minor number made available

/**

 * region_find_by_minor - Locate a pseudo-lock region by inode minor number

 * @minor: The minor number of the device representing pseudo-locked region

 *

 * When the character device is accessed we need to determine which

 * pseudo-locked region it belongs to. This is done by matching the minor

 * number of the device to the pseudo-locked region it belongs.

 *

 * Minor numbers are assigned at the time a pseudo-locked region is associated

 * with a cache instance.

 *

 * Return: On success return pointer to resource group owning the pseudo-locked

 *         region, NULL on failure.

/**

 * struct pseudo_lock_pm_req - A power management QoS request list entry

 * @list:	Entry within the @pm_reqs list for a pseudo-locked region

 * @req:	PM QoS request

/**

 * pseudo_lock_cstates_constrain - Restrict cores from entering C6

 * @plr: Pseudo-locked region

 *

 * To prevent the cache from being affected by power management entering

 * C6 has to be avoided. This is accomplished by requesting a latency

 * requirement lower than lowest C6 exit latency of all supported

 * platforms as found in the cpuidle state tables in the intel_idle driver.

 * At this time it is possible to do so with a single latency requirement

 * for all supported platforms.

 *

 * Since Goldmont is supported, which is affected by X86_BUG_MONITOR,

 * the ACPI latencies need to be considered while keeping in mind that C2

 * may be set to map to deeper sleep states. In this case the latency

 * requirement needs to prevent entering C2 also.

 *

 * Return: 0 on success, <0 on failure

/**

 * pseudo_lock_region_clear - Reset pseudo-lock region data

 * @plr: pseudo-lock region

 *

 * All content of the pseudo-locked region is reset - any memory allocated

 * freed.

 *

 * Return: void

/**

 * pseudo_lock_region_init - Initialize pseudo-lock region information

 * @plr: pseudo-lock region

 *

 * Called after user provided a schemata to be pseudo-locked. From the

 * schemata the &struct pseudo_lock_region is on entry already initialized

 * with the resource, domain, and capacity bitmask. Here the information

 * required for pseudo-locking is deduced from this data and &struct

 * pseudo_lock_region initialized further. This information includes:

 * - size in bytes of the region to be pseudo-locked

 * - cache line size to know the stride with which data needs to be accessed

 *   to be pseudo-locked

 * - a cpu associated with the cache instance on which the pseudo-locking

 *   flow can be executed

 *

 * Return: 0 on success, <0 on failure. Descriptive error will be written

 * to last_cmd_status buffer.

 Pick the first cpu we find that is associated with the cache. */

/**

 * pseudo_lock_init - Initialize a pseudo-lock region

 * @rdtgrp: resource group to which new pseudo-locked region will belong

 *

 * A pseudo-locked region is associated with a resource group. When this

 * association is created the pseudo-locked region is initialized. The

 * details of the pseudo-locked region are not known at this time so only

 * allocation is done and association established.

 *

 * Return: 0 on success, <0 on failure

/**

 * pseudo_lock_region_alloc - Allocate kernel memory that will be pseudo-locked

 * @plr: pseudo-lock region

 *

 * Initialize the details required to set up the pseudo-locked region and

 * allocate the contiguous memory that will be pseudo-locked to the cache.

 *

 * Return: 0 on success, <0 on failure.  Descriptive error will be written

 * to last_cmd_status buffer.

	/*

	 * We do not yet support contiguous regions larger than

	 * KMALLOC_MAX_SIZE.

/**

 * pseudo_lock_free - Free a pseudo-locked region

 * @rdtgrp: resource group to which pseudo-locked region belonged

 *

 * The pseudo-locked region's resources have already been released, or not

 * yet created at this point. Now it can be freed and disassociated from the

 * resource group.

 *

 * Return: void

/**

 * pseudo_lock_fn - Load kernel memory into cache

 * @_rdtgrp: resource group to which pseudo-lock region belongs

 *

 * This is the core pseudo-locking flow.

 *

 * First we ensure that the kernel memory cannot be found in the cache.

 * Then, while taking care that there will be as little interference as

 * possible, the memory to be loaded is accessed while core is running

 * with class of service set to the bitmask of the pseudo-locked region.

 * After this is complete no future CAT allocations will be allowed to

 * overlap with this bitmask.

 *

 * Local register variables are utilized to ensure that the memory region

 * to be locked is the only memory access made during the critical locking

 * loop.

 *

 * Return: 0. Waiter on waitqueue will be woken on completion.

	/*

	 * The registers used for local register variables are also used

	 * when KASAN is active. When KASAN is active we use a regular

	 * variable to ensure we always use a valid pointer, but the cost

	 * is that this variable will enter the cache through evicting the

	 * memory we are trying to lock into the cache. Thus expect lower

	 * pseudo-locking success rate when KASAN is active.

 CONFIG_KASAN */

	/*

	 * Make sure none of the allocated memory is cached. If it is we

	 * will get a cache hit in below loop from outside of pseudo-locked

	 * region.

	 * wbinvd (as opposed to clflush/clflushopt) is required to

	 * increase likelihood that allocated cache portion will be filled

	 * with associated memory.

	/*

	 * Always called with interrupts enabled. By disabling interrupts

	 * ensure that we will not be preempted during this critical section.

	/*

	 * Call wrmsr and rdmsr as directly as possible to avoid tracing

	 * clobbering local register variables or affecting cache accesses.

	 *

	 * Disable the hardware prefetcher so that when the end of the memory

	 * being pseudo-locked is reached the hardware will not read beyond

	 * the buffer and evict pseudo-locked memory read earlier from the

	 * cache.

	/*

	 * Critical section begin: start by writing the closid associated

	 * with the capacity bitmask of the cache region being

	 * pseudo-locked followed by reading of kernel memory to load it

	 * into the cache.

	/*

	 * Cache was flushed earlier. Now access kernel memory to read it

	 * into cache region associated with just activated plr->closid.

	 * Loop over data twice:

	 * - In first loop the cache region is shared with the page walker

	 *   as it populates the paging structure caches (including TLB).

	 * - In the second loop the paging structure caches are used and

	 *   cache region is populated with the memory being referenced.

		/*

		 * Add a barrier to prevent speculative execution of this

		 * loop reading beyond the end of the buffer.

		/*

		 * Add a barrier to prevent speculative execution of this

		 * loop reading beyond the end of the buffer.

	/*

	 * Critical section end: restore closid with capacity bitmask that

	 * does not overlap with pseudo-locked region.

 Re-enable the hardware prefetcher(s) */

/**

 * rdtgroup_monitor_in_progress - Test if monitoring in progress

 * @rdtgrp: resource group being queried

 *

 * Return: 1 if monitor groups have been created for this resource

 * group, 0 otherwise.

/**

 * rdtgroup_locksetup_user_restrict - Restrict user access to group

 * @rdtgrp: resource group needing access restricted

 *

 * A resource group used for cache pseudo-locking cannot have cpus or tasks

 * assigned to it. This is communicated to the user by restricting access

 * to all the files that can be used to make such changes.

 *

 * Permissions restored with rdtgroup_locksetup_user_restore()

 *

 * Return: 0 on success, <0 on failure. If a failure occurs during the

 * restriction of access an attempt will be made to restore permissions but

 * the state of the mode of these files will be uncertain when a failure

 * occurs.

/**

 * rdtgroup_locksetup_user_restore - Restore user access to group

 * @rdtgrp: resource group needing access restored

 *

 * Restore all file access previously removed using

 * rdtgroup_locksetup_user_restrict()

 *

 * Return: 0 on success, <0 on failure.  If a failure occurs during the

 * restoration of access an attempt will be made to restrict permissions

 * again but the state of the mode of these files will be uncertain when

 * a failure occurs.

/**

 * rdtgroup_locksetup_enter - Resource group enters locksetup mode

 * @rdtgrp: resource group requested to enter locksetup mode

 *

 * A resource group enters locksetup mode to reflect that it would be used

 * to represent a pseudo-locked region and is in the process of being set

 * up to do so. A resource group used for a pseudo-locked region would

 * lose the closid associated with it so we cannot allow it to have any

 * tasks or cpus assigned nor permit tasks or cpus to be assigned in the

 * future. Monitoring of a pseudo-locked region is not allowed either.

 *

 * The above and more restrictions on a pseudo-locked region are checked

 * for and enforced before the resource group enters the locksetup mode.

 *

 * Returns: 0 if the resource group successfully entered locksetup mode, <0

 * on failure. On failure the last_cmd_status buffer is updated with text to

 * communicate details of failure to the user.

	/*

	 * The default resource group can neither be removed nor lose the

	 * default closid associated with it.

	/*

	 * Cache Pseudo-locking not supported when CDP is enabled.

	 *

	 * Some things to consider if you would like to enable this

	 * support (using L3 CDP as example):

	 * - When CDP is enabled two separate resources are exposed,

	 *   L3DATA and L3CODE, but they are actually on the same cache.

	 *   The implication for pseudo-locking is that if a

	 *   pseudo-locked region is created on a domain of one

	 *   resource (eg. L3CODE), then a pseudo-locked region cannot

	 *   be created on that same domain of the other resource

	 *   (eg. L3DATA). This is because the creation of a

	 *   pseudo-locked region involves a call to wbinvd that will

	 *   affect all cache allocations on particular domain.

	 * - Considering the previous, it may be possible to only

	 *   expose one of the CDP resources to pseudo-locking and

	 *   hide the other. For example, we could consider to only

	 *   expose L3DATA and since the L3 cache is unified it is

	 *   still possible to place instructions there are execute it.

	 * - If only one region is exposed to pseudo-locking we should

	 *   still keep in mind that availability of a portion of cache

	 *   for pseudo-locking should take into account both resources.

	 *   Similarly, if a pseudo-locked region is created in one

	 *   resource, the portion of cache used by it should be made

	 *   unavailable to all future allocations from both resources.

	/*

	 * Not knowing the bits to disable prefetching implies that this

	 * platform does not support Cache Pseudo-Locking.

	/*

	 * If this system is capable of monitoring a rmid would have been

	 * allocated when the control group was created. This is not needed

	 * anymore when this group would be used for pseudo-locking. This

	 * is safe to call on platforms not capable of monitoring.

/**

 * rdtgroup_locksetup_exit - resource group exist locksetup mode

 * @rdtgrp: resource group

 *

 * When a resource group exits locksetup mode the earlier restrictions are

 * lifted.

 *

 * Return: 0 on success, <0 on failure

/**

 * rdtgroup_cbm_overlaps_pseudo_locked - Test if CBM or portion is pseudo-locked

 * @d: RDT domain

 * @cbm: CBM to test

 *

 * @d represents a cache instance and @cbm a capacity bitmask that is

 * considered for it. Determine if @cbm overlaps with any existing

 * pseudo-locked region on @d.

 *

 * @cbm is unsigned long, even if only 32 bits are used, to make the

 * bitmap functions work correctly.

 *

 * Return: true if @cbm overlaps with pseudo-locked region on @d, false

 * otherwise.

/**

 * rdtgroup_pseudo_locked_in_hierarchy - Pseudo-locked region in cache hierarchy

 * @d: RDT domain under test

 *

 * The setup of a pseudo-locked region affects all cache instances within

 * the hierarchy of the region. It is thus essential to know if any

 * pseudo-locked regions exist within a cache hierarchy to prevent any

 * attempts to create new pseudo-locked regions in the same hierarchy.

 *

 * Return: true if a pseudo-locked region exists in the hierarchy of @d or

 *         if it is not possible to test due to memory allocation issue,

 *         false otherwise.

	/*

	 * First determine which cpus have pseudo-locked regions

	 * associated with them.

	/*

	 * Next test if new pseudo-locked region would intersect with

	 * existing region.

/**

 * measure_cycles_lat_fn - Measure cycle latency to read pseudo-locked memory

 * @_plr: pseudo-lock region to measure

 *

 * There is no deterministic way to test if a memory region is cached. One

 * way is to measure how long it takes to read the memory, the speed of

 * access is a good way to learn how close to the cpu the data was. Even

 * more, if the prefetcher is disabled and the memory is read at a stride

 * of half the cache line, then a cache miss will be easy to spot since the

 * read of the first half would be significantly slower than the read of

 * the second half.

 *

 * Return: 0. Waiter on waitqueue will be woken on completion.

	/*

	 * Disable hardware prefetchers.

	/*

	 * Dummy execute of the time measurement to load the needed

	 * instructions into the L1 instruction cache.

/*

 * Create a perf_event_attr for the hit and miss perf events that will

 * be used during the performance measurement. A perf_event maintains

 * a pointer to its perf_event_attr so a unique attribute structure is

 * created for each perf_event.

 *

 * The actual configuration of the event is set right before use in order

 * to use the X86_CONFIG macro.

	/*

	 * Check any possible error state of events used by performing

	 * one local read.

	/*

	 * Disable hardware prefetchers.

 Initialize rest of local variables */

	/*

	 * Performance event has been validated right before this with

	 * interrupts disabled - it is thus safe to read the counter index.

	/*

	 * Read counter variables twice - first to load the instructions

	 * used in L1 cache, second to capture accurate value that does not

	 * include cache misses incurred because of instruction loads.

	/*

	 * From SDM: Performing back-to-back fast reads are not guaranteed

	 * to be monotonic.

	 * Use LFENCE to ensure all previous instructions are retired

	 * before proceeding.

	/*

	 * Use LFENCE to ensure all previous instructions are retired

	 * before proceeding.

		/*

		 * Add a barrier to prevent speculative execution of this

		 * loop reading beyond the end of the buffer.

	/*

	 * Use LFENCE to ensure all previous instructions are retired

	 * before proceeding.

	/*

	 * Use LFENCE to ensure all previous instructions are retired

	 * before proceeding.

 Re-enable hardware prefetchers */

	/*

	 * All counts will be zero on failure.

	/*

	 * Non-architectural event for the Goldmont Microarchitecture

	 * from Intel x86 Architecture Software Developer Manual (SDM):

	 * MEM_LOAD_UOPS_RETIRED D1H (event number)

	 * Umask values:

	 *     L2_HIT   02H

	 *     L2_MISS  10H

	/*

	 * If a failure prevented the measurements from succeeding

	 * tracepoints will still be written and all counts will be zero.

	/*

	 * On Broadwell Microarchitecture the MEM_LOAD_UOPS_RETIRED event

	 * has two "no fix" errata associated with it: BDM35 and BDM100. On

	 * this platform the following events are used instead:

	 * LONGEST_LAT_CACHE 2EH (Documented in SDM)

	 *       REFERENCE 4FH

	 *       MISS      41H

 On BDW the hit event counts references, not hits */

	/*

	 * If a failure prevented the measurements from succeeding

	 * tracepoints will still be written and all counts will be zero.

		/*

		 * On BDW references and misses are counted, need to adjust.

		 * Sometimes the "hits" counter is a bit more than the

		 * references, for example, x references but x + 1 hits.

		 * To not report invalid hit values in this case we treat

		 * that as misses equal to references.

 First compute the number of cache references measured */

 Next convert references to cache hits */

/**

 * pseudo_lock_measure_cycles - Trigger latency measure to pseudo-locked region

 * @rdtgrp: Resource group to which the pseudo-locked region belongs.

 * @sel: Selector of which measurement to perform on a pseudo-locked region.

 *

 * The measurement of latency to access a pseudo-locked region should be

 * done from a cpu that is associated with that pseudo-locked region.

 * Determine which cpu is associated with this region and start a thread on

 * that cpu to perform the measurement, wait for that thread to complete.

 *

 * Return: 0 on success, <0 on failure

/**

 * rdtgroup_pseudo_lock_create - Create a pseudo-locked region

 * @rdtgrp: resource group to which pseudo-lock region belongs

 *

 * Called when a resource group in the pseudo-locksetup mode receives a

 * valid schemata that should be pseudo-locked. Since the resource group is

 * in pseudo-locksetup mode the &struct pseudo_lock_region has already been

 * allocated and initialized with the essential information. If a failure

 * occurs the resource group remains in the pseudo-locksetup mode with the

 * &struct pseudo_lock_region associated with it, but cleared from all

 * information and ready for the user to re-attempt pseudo-locking by

 * writing the schemata again.

 *

 * Return: 0 if the pseudo-locked region was successfully pseudo-locked, <0

 * on failure. Descriptive error will be written to last_cmd_status buffer.

		/*

		 * If the thread does not get on the CPU for whatever

		 * reason and the process which sets up the region is

		 * interrupted then this will leave the thread in runnable

		 * state and once it gets on the CPU it will dereference

		 * the cleared, but not freed, plr struct resulting in an

		 * empty pseudo-locking loop.

	/*

	 * Unlock access but do not release the reference. The

	 * pseudo-locked region will still be here on return.

	 *

	 * The mutex has to be released temporarily to avoid a potential

	 * deadlock with the mm->mmap_lock which is obtained in the

	 * device_create() and debugfs_create_dir() callpath below as well as

	 * before the mmap() callback is called.

 We released the mutex - check if group was removed while we did so */

/**

 * rdtgroup_pseudo_lock_remove - Remove a pseudo-locked region

 * @rdtgrp: resource group to which the pseudo-locked region belongs

 *

 * The removal of a pseudo-locked region can be initiated when the resource

 * group is removed from user space via a "rmdir" from userspace or the

 * unmount of the resctrl filesystem. On removal the resource group does

 * not go back to pseudo-locksetup mode before it is removed, instead it is

 * removed directly. There is thus asymmetry with the creation where the

 * &struct pseudo_lock_region is removed here while it was not created in

 * rdtgroup_pseudo_lock_create().

 *

 * Return: void

		/*

		 * Default group cannot be a pseudo-locked region so we can

		 * free closid here.

 Perform a non-seekable open - llseek is not supported */

 Not supported */

	/*

	 * Task is required to run with affinity to the cpus associated

	 * with the pseudo-locked region. If this is not the case the task

	 * may be scheduled elsewhere and invalidate entries in the

	 * pseudo-locked region.

	/*

	 * Ensure changes are carried directly to the memory being mapped,

	 * do not allow copy-on-write mapping.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Resource Director Technology(RDT)

 * - Cache Allocation code.

 *

 * Copyright (C) 2016 Intel Corporation

 *

 * Authors:

 *    Fenghua Yu <fenghua.yu@intel.com>

 *    Tony Luck <tony.luck@intel.com>

 *

 * More information about RDT be found in the Intel (R) x86 Architecture

 * Software Developer Manual June 2016, volume 3, section 17.17.

/*

 * Check whether MBA bandwidth percentage value is correct. The value is

 * checked against the minimum and max bandwidth values specified by the

 * hardware. The allocated bandwidth percentage is rounded to the next

 * control step available on the hardware.

	/*

	 * Only linear delay values is supported for current Intel SKUs.

/*

 * Check whether a cache bit mask is valid.

 * For Intel the SDM says:

 *	Please note that all (and only) contiguous '1' combinations

 *	are allowed (e.g. FFFFH, 0FF0H, 003CH, etc.).

 * Additionally Haswell requires at least two bits set.

 * AMD allows non-contiguous bitmasks.

 Are non-contiguous bitmaps allowed? */

/*

 * Read one cache bit mask (hex). Check that it is valid for the current

 * resource type.

	/*

	 * Cannot set up more than one pseudo-locked region in a cache

	 * hierarchy.

	/*

	 * The CBM may not overlap with the CBM of another closid if

	 * either is exclusive.

/*

 * For each domain in this resource we expect to find a series of:

 *	id=mask

 * separated by ";". The "id" is in decimal, and must match one of

 * the "id"s for this resource.

				/*

				 * In pseudo-locking setup mode and just

				 * parsed a valid CBM that should be

				 * pseudo-locked. Only one locked region per

				 * resource group and domain so just do

				 * the required initialization for single

				 * region and return.

	/*

	 * Avoid writing the control msr with control values when

	 * MBA software controller is enabled

 Update resource control msr on this CPU if it's in cpu_mask. */

 Update resource control msr on other CPUs. */

 Valid input requires a trailing newline */

	/*

	 * No changes to pseudo-locked region allowed. It has to be removed

	 * and re-created instead.

		/*

		 * If pseudo-locking fails we keep the resource group in

		 * mode RDT_MODE_PSEUDO_LOCKSETUP with its class of service

		 * active and updated for just the domain the pseudo-locked

		 * region was requested for.

	/*

	 * setup the parameters to send to the IPI to read the data.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * User interface for Resource Allocation in Resource Director Technology(RDT)

 *

 * Copyright (C) 2016 Intel Corporation

 *

 * Author: Fenghua Yu <fenghua.yu@intel.com>

 *

 * More information about RDT be found in the Intel (R) x86 Architecture

 * Software Developer Manual.

 list of entries for the schemata file */

 Kernel fs node for "info" directory under root */

 Kernel fs node for "mon_groups" directory under root */

 Kernel fs node for "mon_data" directory under root */

/*

 * Trivial allocator for CLOSIDs. Since h/w only supports a small number,

 * we can keep a bitmap of free CLOSIDs in a single integer.

 *

 * Using a global CLOSID across all resources has some advantages and

 * some drawbacks:

 * + We can simply set "current->closid" to assign a task to a resource

 *   group.

 * + Context switch code can avoid extra memory references deciding which

 *   CLOSID to load into the PQR_ASSOC MSR

 * - We give up some options in configuring resource groups across multi-socket

 *   systems.

 * - Our choices on how to configure each resource become progressively more

 *   limited as the number of resources grows.

 Compute rdt_min_closid across all resources */

 CLOSID 0 is always reserved for the default group */

/**

 * closid_allocated - test if provided closid is in use

 * @closid: closid to be tested

 *

 * Return: true if @closid is currently associated with a resource group,

 * false if @closid is free

/**

 * rdtgroup_mode_by_closid - Return mode of resource group with closid

 * @closid: closid if the resource group

 *

 * Each resource group is associated with a @closid. Here the mode

 * of a resource group can be queried by searching for it using its closid.

 *

 * Return: mode as &enum rdtgrp_mode of resource group with closid @closid

/**

 * rdtgroup_mode_str - Return the string representation of mode

 * @mode: the resource group mode as &enum rdtgroup_mode

 *

 * Return: string representation of valid mode, "unknown" otherwise

 set uid and gid of rdtgroup dirs and files to that of the creator */

/*

 * This is safe against resctrl_sched_in() called from __switch_to()

 * because __switch_to() is executed with interrupts disabled. A local call

 * from update_closid_rmid() is protected against __switch_to() because

 * preemption is disabled.

	/*

	 * We cannot unconditionally write the MSR because the current

	 * executing task might have its own closid selected. Just reuse

	 * the context switch code.

/*

 * Update the PGR_ASSOC MSR on all cpus in @cpu_mask,

 *

 * Per task closids/rmids must have been set up before calling this function.

 Check whether cpus belong to parent ctrl group */

 Check whether cpus are dropped from this group */

 Give any dropped cpus to parent rdtgroup */

	/*

	 * If we added cpus, remove them from previous group that owned them

	 * and update per-cpu rmid

 Done pushing/pulling - update this group with new mask */

 update the child mon group masks as well*/

 Check whether cpus are dropped from this group */

 Can't drop from default group */

 Give any dropped cpus to rdtgroup_default */

	/*

	 * If we added cpus, remove them from previous group and

	 * the prev group's child groups that owned them

	 * and update per-cpu closid/rmid.

 Done pushing/pulling - update this group with new mask */

	/*

	 * Clear child mon group masks since there is a new parent mask

	 * now and update the rmid for the cpus the child lost.

 check that user didn't specify any offline cpus */

/**

 * rdtgroup_remove - the helper to remove resource group safely

 * @rdtgrp: resource group to remove

 *

 * On resource group creation via a mkdir, an extra kernfs_node reference is

 * taken to ensure that the rdtgroup structure remains accessible for the

 * rdtgroup_kn_unlock() calls where it is removed.

 *

 * Drop the extra reference here, then free the rdtgroup structure.

 *

 * Return: void

	/*

	 * If the task is still current on this CPU, update PQR_ASSOC MSR.

	 * Otherwise, the MSR is updated when the task is scheduled in.

 If the task is already in rdtgrp, no need to move the task. */

	/*

	 * Set the task's closid/rmid before the PQR_ASSOC MSR can be

	 * updated by them.

	 *

	 * For ctrl_mon groups, move both closid and rmid.

	 * For monitor groups, can move the tasks only from

	 * their parent CTRL group.

	/*

	 * Ensure the task's closid and rmid are written before determining if

	 * the task is current that will decide if it will be interrupted.

	/*

	 * By now, the task's closid and rmid are set. If the task is current

	 * on a CPU, the PQR_ASSOC MSR needs to be updated to make the resource

	 * group go into effect. If the task is not current, the MSR will be

	 * updated when the task is scheduled in.

/**

 * rdtgroup_tasks_assigned - Test if tasks have been assigned to resource group

 * @r: Resource group

 *

 * Return: 1 if tasks have been assigned to @r, 0 otherwise

	/*

	 * Even if we're attaching all tasks in the thread group, we only

	 * need to check permissions on one of them.

/*

 * A task can only be part of one resctrl control group and of one monitor

 * group which is associated to that control group.

 *

 * 1)   res:

 *      mon:

 *

 *    resctrl is not available.

 *

 * 2)   res:/

 *      mon:

 *

 *    Task is part of the root resctrl control group, and it is not associated

 *    to any monitor group.

 *

 * 3)  res:/

 *     mon:mon0

 *

 *    Task is part of the root resctrl control group and monitor group mon0.

 *

 * 4)  res:group0

 *     mon:

 *

 *    Task is part of resctrl control group group0, and it is not associated

 *    to any monitor group.

 *

 * 5) res:group0

 *    mon:mon1

 *

 *    Task is part of resctrl control group group0 and monitor group mon1.

 Return empty if resctrl has not been mounted. */

		/*

		 * Task information is only relevant for shareable

		 * and exclusive groups.

	/*

	 * The above search should succeed. Otherwise return

	 * with an error.

/**

 * rdt_bit_usage_show - Display current usage of resources

 *

 * A domain is a shared resource that can now be allocated differently. Here

 * we display the current regions of the domain as an annotated bitmask.

 * For each domain of this resource its allocation bitmask

 * is annotated as below to indicate the current usage of the corresponding bit:

 *   0 - currently unused

 *   X - currently available for sharing and used by software and hardware

 *   H - currently used by hardware only but available for software use

 *   S - currently used and shareable by software only

 *   E - currently used exclusively by one resource group

 *   P - currently pseudo-locked by one resource group

	/*

	 * Use unsigned long even though only 32 bits are used to ensure

	 * test_bit() is used safely.

			/*

			 * RDT_MODE_PSEUDO_LOCKSETUP is possible

			 * here but not included since the CBM

			 * associated with this CLOSID in this mode

			 * is not initialized and no task or cpu can be

			 * assigned this CLOSID.

 Unused bits remain */

/*

 * rdtgroup_mode_show - Display mode of this resource group

/**

 * __rdtgroup_cbm_overlaps - Does CBM for intended closid overlap with other

 * @r: Resource to which domain instance @d belongs.

 * @d: The domain instance for which @closid is being tested.

 * @cbm: Capacity bitmask being tested.

 * @closid: Intended closid for @cbm.

 * @exclusive: Only check if overlaps with exclusive resource groups

 *

 * Checks if provided @cbm intended to be used for @closid on domain

 * @d overlaps with any other closids or other hardware usage associated

 * with this domain. If @exclusive is true then only overlaps with

 * resource groups in exclusive mode will be considered. If @exclusive

 * is false then overlaps with any resource group or hardware entities

 * will be considered.

 *

 * @cbm is unsigned long, even if only 32 bits are used, to make the

 * bitmap functions work correctly.

 *

 * Return: false if CBM does not overlap, true if it does.

 Check for any overlap with regions used by hardware directly */

 Check for overlap with other resource groups */

/**

 * rdtgroup_cbm_overlaps - Does CBM overlap with other use of hardware

 * @s: Schema for the resource to which domain instance @d belongs.

 * @d: The domain instance for which @closid is being tested.

 * @cbm: Capacity bitmask being tested.

 * @closid: Intended closid for @cbm.

 * @exclusive: Only check if overlaps with exclusive resource groups

 *

 * Resources that can be allocated using a CBM can use the CBM to control

 * the overlap of these allocations. rdtgroup_cmb_overlaps() is the test

 * for overlap. Overlap test is not limited to the specific resource for

 * which the CBM is intended though - when dealing with CDP resources that

 * share the underlying hardware the overlap check should be performed on

 * the CDP resource sharing the hardware also.

 *

 * Refer to description of __rdtgroup_cbm_overlaps() for the details of the

 * overlap test.

 *

 * Return: true if CBM overlap detected, false if there is no overlap

/**

 * rdtgroup_mode_test_exclusive - Test if this resource group can be exclusive

 *

 * An exclusive resource group implies that there should be no sharing of

 * its allocated resources. At the time this group is considered to be

 * exclusive this test can determine if its current schemata supports this

 * setting by testing for overlap with all other resource groups.

 *

 * Return: true if resource group can be exclusive, false if there is overlap

 * with allocations of other resource groups and thus this resource group

 * cannot be exclusive.

/**

 * rdtgroup_mode_write - Modify the resource group's mode

 *

 Valid input requires a trailing newline */

/**

 * rdtgroup_cbm_to_size - Translate CBM to size in bytes

 * @r: RDT resource to which @d belongs.

 * @d: RDT domain instance.

 * @cbm: bitmask for which the size should be computed.

 *

 * The bitmask provided associated with the RDT domain instance @d will be

 * translated into how many bytes it represents. The size in bytes is

 * computed by first dividing the total cache size by the CBM length to

 * determine how many bytes each bit in the bitmask represents. The result

 * is multiplied with the number of bits set in the bitmask.

 *

 * @cbm is unsigned long, even if only 32 bits are used to make the

 * bitmap functions work correctly.

/**

 * rdtgroup_size_show - Display size in bytes of allocated regions

 *

 * The "size" file mirrors the layout of the "schemata" file, printing the

 * size in bytes of each region instead of the capacity bitmask.

 *

 rdtgroup information files for one cache resource. */

	/*

	 * Platform specific which (if any) capabilities are provided by

	 * thread_throttle_mode. Defer "fflags" initialization to platform

	 * discovery.

/**

 * rdtgroup_kn_mode_restrict - Restrict user access to named resctrl file

 * @r: The resource group with which the file is associated.

 * @name: Name of the file

 *

 * The permissions of named resctrl file, directory, or link are modified

 * to not allow read, write, or execute by any user.

 *

 * WARNING: This function is intended to communicate to the user that the

 * resctrl file has been locked down - that it is not relevant to the

 * particular state the system finds itself in. It should not be relied

 * on to protect from user access because after the file's permissions

 * are restricted the user can still change the permissions using chmod

 * from the command line.

 *

 * Return: 0 on success, <0 on failure.

/**

 * rdtgroup_kn_mode_restore - Restore user access to named resctrl file

 * @r: The resource group with which the file is associated.

 * @name: Name of the file

 * @mask: Mask of permissions that should be restored

 *

 * Restore the permissions of the named file. If @name is a directory the

 * permissions of its parent will be used.

 *

 * Return: 0 on success, <0 on failure.

 create the directory */

 loop over enabled controls, these are all alloc_enabled */

 create the directory */

 Pick all the CPUs in the domain instance */

 Pick one CPU from each domain instance to update MSR */

 Update QOS_CFG MSR on this cpu if it's in cpu_mask. */

 Update QOS_CFG MSR on all other cpus in cpu_mask. */

 Restore the qos cfg state when a domain comes online */

/*

 * Enable or disable the MBA software controller

 * which helps user specify bandwidth in MBps.

 * MBA software controller is supported only if

 * MBM is supported and MBA is in linear scale.

/*

 * We don't allow rdtgroup directories to be created anywhere

 * except the root directory. Thus when looking for the rdtgroup

 * structure for a kernfs node we are either looking at a directory,

 * in which case the rdtgroup structure is pointed at by the "priv"

 * field, otherwise we have a file, and need only look to the parent

 * to find the rdtgroup.

		/*

		 * All the resource directories use "kn->priv"

		 * to point to the "struct rdtgroup" for the

		 * resource. "info" and its subdirectories don't

		 * have rdtgroup structures, so return NULL here.

 Was this group deleted while we waited? */

	/*

	 * If CDP is supported by this resource, but not enabled,

	 * include the suffix. This ensures the tabular format of the

	 * schemata file does not change between mounts of the filesystem.

	/*

	 * resctrl file system can only be mounted once.

	/*

	 * Disable resource control for this resource by setting all

	 * CBMs in all domains to the maximum mask value. Pick one CPU

	 * from each domain to update the MSRs below.

 Update CBM on this cpu if it's in cpu_mask. */

 Update CBM on all other cpus in cpu_mask. */

/*

 * Move tasks from one to the other group. If @from is NULL, then all tasks

 * in the systems are moved unconditionally (used for teardown).

 *

 * If @mask is not NULL the cpus on which moved tasks are running are set

 * in that mask so the update smp function call is restricted to affected

 * cpus.

			/*

			 * If the task is on a CPU, set the CPU in the mask.

			 * The detection is inaccurate as tasks might move or

			 * schedule before the smp function call takes place.

			 * In such a case the function call is pointless, but

			 * there is no other side effect.

/*

 * Forcibly remove all of subdirectories under root.

 Move all tasks to the default resource group */

 Free any child rmids */

 Remove each rdtgroup other than root */

		/*

		 * Give any CPUs back to the default group. We cannot copy

		 * cpu_online_mask because a CPU might have executed the

		 * offline callback already, but is still marked online.

 Notify online CPUs to update per cpu storage and PQR_ASSOC MSR */

Put everything back to default values. */

/*

 * Remove all subdirectories of mon_data of ctrl_mon groups

 * and monitor groups with given domain id.

 create the directory */

/*

 * Add all subdirectories of mon_data for "ctrl_mon" groups

 * and "monitor" groups with given domain id.

/*

 * This creates a directory mon_data which contains the monitored data.

 *

 * mon_data has one directory for each domain which are named

 * in the format mon_<domain_name>_<domain_id>. For ex: A mon_data

 * with L3 domain looks as below:

 * ./mon_data:

 * mon_L3_00

 * mon_L3_01

 * mon_L3_02

 * ...

 *

 * Each domain directory has one file per event:

 * ./mon_L3_00/:

 * llc_occupancy

 *

	/*

	 * Create the mon_data directory first.

	/*

	 * Create the subdirectories for each domain. Note that all events

	 * in a domain like L3 are grouped into a resource whose domain is L3

/**

 * cbm_ensure_valid - Enforce validity on provided CBM

 * @_val:	Candidate CBM

 * @r:		RDT resource to which the CBM belongs

 *

 * The provided CBM represents all cache portions available for use. This

 * may be represented by a bitmap that does not consist of contiguous ones

 * and thus be an invalid CBM.

 * Here the provided CBM is forced to be a valid CBM by only considering

 * the first set of contiguous bits as valid and clearing all bits.

 * The intention here is to provide a valid default CBM with which a new

 * resource group is initialized. The user can follow this with a

 * modification to the CBM if the default does not satisfy the

 * requirements.

 Clear any remaining bits to ensure contiguous region */

/*

 * Initialize cache resources per RDT domain

 *

 * Set the RDT domain up to start off with all usable allocations. That is,

 * all shareable and unused bits. All-zero CBM is invalid.

				/*

				 * ctrl values for locksetup aren't relevant

				 * until the schemata is written, and the mode

				 * becomes RDT_MODE_PSEUDO_LOCKED.

			/*

			 * If CDP is active include peer domain's

			 * usage to ensure there is no overlap

			 * with an exclusive group.

	/*

	 * Force the initial CBM to be valid, user can

	 * modify the CBM based on system availability.

	/*

	 * Assign the u32 CBM to an unsigned long to ensure that

	 * bitmap_weight() does not access out-of-bound memory.

/*

 * Initialize cache resources with default values.

 *

 * A new RDT group is being created on an allocation capable (CAT)

 * supporting system. Set this group up to start off with all usable

 * allocations.

 *

 * If there are no more shareable bits available on any domain then

 * the entire allocation will fail.

 Initialize MBA resource with default values. */

 Initialize the RDT group's allocations. */

 allocate the rdtgroup. */

 kernfs creates the directory for rdtgrp */

	/*

	 * kernfs_remove() will drop the reference count on "kn" which

	 * will free it. But we still need it to stick around for the

	 * rdtgroup_kn_unlock(kn) call. Take one extra reference here,

	 * which will be dropped by kernfs_put() in rdtgroup_remove().

	/*

	 * The caller unlocks the parent_kn upon success.

/*

 * Create a monitor group under "mon_groups" directory of a control

 * and monitor group(ctrl_mon). This is a resource group

 * to monitor a subset of tasks and cpus in its parent ctrl_mon group.

	/*

	 * Add the rdtgrp to the list of rdtgrps the parent

	 * ctrl_mon group has to track.

/*

 * These are rdtgroups created under the root directory. Can be used

 * to allocate and monitor resources.

		/*

		 * Create an empty mon_groups directory to hold the subset

		 * of tasks and cpus to monitor.

/*

 * We allow creating mon groups only with in a directory called "mon_groups"

 * which is present in every ctrl_mon group. Check if this is a valid

 * "mon_groups" directory.

 *

 * 1. The directory should be named "mon_groups".

 * 2. The mon group itself should "not" be named "mon_groups".

 *   This makes sure "mon_groups" directory always has a ctrl_mon group

 *   as parent.

 Do not accept '\n' to avoid unparsable situation. */

	/*

	 * If the parent directory is the root directory and RDT

	 * allocation is supported, add a control and monitoring

	 * subdirectory

	/*

	 * If RDT monitoring is supported and the parent directory is a valid

	 * "mon_groups" directory, add a monitoring subdirectory.

 Give any tasks back to the parent group */

 Update per cpu rmid of the moved CPUs first */

	/*

	 * Update the MSR on moved CPUs and CPUs which have moved

	 * task running on them.

	/*

	 * Remove the rdtgrp from the parent ctrl_mon group's list

 Give any tasks back to the default group */

 Give any CPUs back to the default group */

 Update per cpu closid and rmid of the moved CPUs first */

	/*

	 * Update the MSR on moved CPUs and CPUs which have moved

	 * task running on them.

	/*

	 * Free all the child monitor group rmids.

	/*

	 * If the rdtgroup is a ctrl_mon group and parent directory

	 * is the root directory, remove the ctrl_mon group.

	 *

	 * If the rdtgroup is a mon group and parent directory

	 * is a valid "mon_groups" directory, remove the mon group.

/*

 * rdtgroup_init - rdtgroup initialization

 *

 * Setup resctrl file system including set up root, create mount point,

 * register rdtgroup filesystem, and initialize files under root directory.

 *

 * Return: 0 on success or -errno

	/*

	 * Adding the resctrl debugfs directory here may not be ideal since

	 * it would let the resctrl debugfs directory appear on the debugfs

	 * filesystem before the resctrl filesystem is mounted.

	 * It may also be ok since that would enable debugging of RDT before

	 * resctrl is mounted.

	 * The reason why the debugfs directory is created here and not in

	 * rdt_get_tree() is because rdt_get_tree() takes rdtgroup_mutex and

	 * during the debugfs directory creation also &sb->s_type->i_mutex_key

	 * (the lockdep class of inode->i_rwsem). Other filesystem

	 * interactions (eg. SyS_getdents) have the lock ordering:

	 * &sb->s_type->i_mutex_key --> &mm->mmap_lock

	 * During mmap(), called with &mm->mmap_lock, the rdtgroup_mutex

	 * is taken, thus creating dependency:

	 * &mm->mmap_lock --> rdtgroup_mutex for the latter that can cause

	 * issues considering the other two lock dependencies.

	 * By creating the debugfs directory here we avoid a dependency

	 * that may cause deadlock (even though file operations cannot

	 * occur until the filesystem is mounted, but I do not know how to

	 * tell lockdep that).

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Resource Director Technology(RDT)

 * - Monitoring code

 *

 * Copyright (C) 2017 Intel Corporation

 *

 * Author:

 *    Vikas Shivappa <vikas.shivappa@intel.com>

 *

 * This replaces the cqm.c based on perf but we reuse a lot of

 * code and datastructures originally from Peter Zijlstra and Matt Fleming.

 *

 * More information about RDT be found in the Intel (R) x86 Architecture

 * Software Developer Manual June 2016, volume 3, section 17.17.

/**

 * @rmid_free_lru    A least recently used list of free RMIDs

 *     These RMIDs are guaranteed to have an occupancy less than the

 *     threshold occupancy

/**

 * @rmid_limbo_count     count of currently unused but (potentially)

 *     dirty RMIDs.

 *     This counts RMIDs that no one is currently using but that

 *     may have a occupancy value > intel_cqm_threshold. User can change

 *     the threshold occupancy value.

/**

 * @rmid_entry - The entry in the limbo and free lists.

/*

 * Global boolean for rdt_monitor which is true if any

 * resource monitoring is enabled.

/*

 * Global to indicate which monitoring events are enabled.

/*

 * This is the threshold cache occupancy at which we will consider an

 * RMID available for re-allocation.

/*

 * The correction factor table is documented in Documentation/x86/resctrl.rst.

 * If rmid > rmid threshold, MBM total and local values should be multiplied

 * by the correction factor.

 *

 * The original table is modified for better code:

 *

 * 1. The threshold 0 is changed to rmid count - 1 so don't do correction

 *    for the case.

 * 2. MBM total and local correction table indexed by core counter which is

 *    equal to (x86_cache_max_rmid + 1) / 8 - 1 and is from 0 up to 27.

 * 3. The correction factor is normalized to 2^20 (1048576) so it's faster

 *    to calculate corrected value by shifting:

 *    corrected_value = (original_value * correction_factor) >> 20

 Correct MBM value. */

	/*

	 * As per the SDM, when IA32_QM_EVTSEL.EvtID (bits 7:0) is configured

	 * with a valid event code for supported resource type and the bits

	 * IA32_QM_EVTSEL.RMID (bits 41:32) are configured with valid RMID,

	 * IA32_QM_CTR.data (bits 61:0) reports the monitored data.

	 * IA32_QM_CTR.Error (bit 63) and IA32_QM_CTR.Unavailable (bit 62)

	 * are error bits.

/*

 * Check the RMIDs that are marked as busy for this domain. If the

 * reported LLC occupancy is below the threshold clear the busy bit and

 * decrement the count. If the busy count gets to zero on an RMID, we

 * free the RMID

	/*

	 * Skip RMID 0 and start from RMID 1 and check all the RMIDs that

	 * are marked as busy for occupancy < threshold. If the occupancy

	 * is less than the threshold decrement the busy counter of the

	 * RMID and move it to the free list when the counter reaches 0.

/*

 * As of now the RMIDs allocation is global.

 * However we keep track of which packages the RMIDs

 * are used to optimize the limbo list management.

		/*

		 * For the first limbo RMID in the domain,

		 * setup up the limbo worker.

		/*

		 * Code would never reach here because an invalid

		 * event id would fail the __rmid_read.

/*

 * Supporting function to calculate the memory bandwidth

 * and delta bandwidth in MBps.

/*

 * This is called via IPI to read the CQM/MBM counters

 * on a domain.

	/*

	 * For Ctrl groups read data from child monitor groups and

	 * add them together. Count events which are read successfully.

	 * Discard the rmid_read's reporting errors.

 Report error if none of rmid_reads are successful */

/*

 * Feedback loop for MBA software controller (mba_sc)

 *

 * mba_sc is a feedback loop where we periodically read MBM counters and

 * adjust the bandwidth percentage values via the IA32_MBA_THRTL_MSRs so

 * that:

 *

 *   current bandwidth(cur_bw) < user specified bandwidth(user_bw)

 *

 * This uses the MBM counters to measure the bandwidth and MBA throttle

 * MSRs to control the bandwidth for a particular rdtgrp. It builds on the

 * fact that resctrl rdtgroups have both monitoring and control.

 *

 * The frequency of the checks is 1s and we just tag along the MBM overflow

 * timer. Having 1s interval makes the calculation of bandwidth simpler.

 *

 * Although MBA's goal is to restrict the bandwidth to a maximum, there may

 * be a need to increase the bandwidth to avoid unnecessarily restricting

 * the L2 <-> L3 traffic.

 *

 * Since MBA controls the L2 external bandwidth where as MBM measures the

 * L3 external bandwidth the following sequence could lead to such a

 * situation.

 *

 * Consider an rdtgroup which had high L3 <-> memory traffic in initial

 * phases -> mba_sc kicks in and reduced bandwidth percentage values -> but

 * after some time rdtgroup has mostly L2 <-> L3 traffic.

 *

 * In this case we may restrict the rdtgroup's L2 <-> L3 traffic as its

 * throttle MSRs already have low percentage values.  To avoid

 * unnecessarily restricting such rdtgroups, we also increase the bandwidth.

	/*

	 * resctrl_arch_get_config() chooses the mbps/ctrl value to return

	 * based on is_mba_sc(). For now, reach into the hw_dom.

	/*

	 * For Ctrl groups read data from child monitor groups.

	/*

	 * Scale up/down the bandwidth linearly for the ctrl group.  The

	 * bandwidth step is the bandwidth granularity specified by the

	 * hardware.

	 *

	 * The delta_bw is used when increasing the bandwidth so that we

	 * dont alternately increase and decrease the control values

	 * continuously.

	 *

	 * For ex: consider cur_bw = 90MBps, user_bw = 100MBps and if

	 * bandwidth step is 20MBps(> user_bw - cur_bw), we would keep

	 * switching between 90 and 110 continuously if we only check

	 * cur_bw < user_bw.

	/*

	 * Delta values are updated dynamically package wise for each

	 * rdtgrp every time the throttle MSR changes value.

	 *

	 * This is because (1)the increase in bandwidth is not perfectly

	 * linear and only "approximately" linear even when the hardware

	 * says it is linear.(2)Also since MBA is a core specific

	 * mechanism, the delta values vary based on number of cores used

	 * by the rdtgrp.

	/*

	 * This is protected from concurrent reads from user

	 * as both the user and we hold the global mutex.

		/*

		 * Call the MBA software controller only for the

		 * control groups and when user has enabled

		 * the software controller explicitly.

/*

 * Handler to scan the limbo list and move the RMIDs

 * to free list whose occupancy < threshold_occupancy.

	/*

	 * RMID 0 is special and is always allocated. It's used for all

	 * tasks that are not monitored.

/*

 * Initialize the event list for the resource.

 *

 * Note that MBM events are also part of RDT_RESOURCE_L3 resource

 * because as per the SDM the total and local memory bandwidth

 * are enumerated as part of L3 monitoring.

	/*

	 * A reasonable upper limit on the max threshold is the number

	 * of lines tagged per RMID if all RMIDs have the same number of

	 * lines tagged in the LLC.

	 *

	 * For a 35MB LLC and 56 RMIDs, this is ~1.8% of the LLC.

 h/w works in units of "boot_cpu_data.x86_cache_occ_scale" */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Resource Director Technology(RDT)

 * - Cache Allocation code.

 *

 * Copyright (C) 2016 Intel Corporation

 *

 * Authors:

 *    Fenghua Yu <fenghua.yu@intel.com>

 *    Tony Luck <tony.luck@intel.com>

 *    Vikas Shivappa <vikas.shivappa@intel.com>

 *

 * More information about RDT be found in the Intel (R) x86 Architecture

 * Software Developer Manual June 2016, volume 3, section 17.17.

 Mutex to protect rdtgroup access. */

/*

 * The cached resctrl_pqr_state is strictly per CPU and can never be

 * updated from a remote CPU. Functions which modify the state

 * are called with interrupts disabled and no preemption, which

 * is sufficient for the protection.

/*

 * Used to store the max resource name width and max resource data width

 * to display the schemata in a tabular format

/*

 * Global boolean for rdt_alloc which is true if any

 * resource allocation is enabled.

/*

 * cache_alloc_hsw_probe() - Have to probe for Intel haswell server CPUs

 * as they do not have CPUID enumeration support for Cache allocation.

 * The check for Vendor/Family/Model is not enough to guarantee that

 * the MSRs won't #GP fault because only the following SKUs support

 * CAT:

 *	Intel(R) Xeon(R)  CPU E5-2658  v3  @  2.20GHz

 *	Intel(R) Xeon(R)  CPU E5-2648L v3  @  1.80GHz

 *	Intel(R) Xeon(R)  CPU E5-2628L v3  @  2.00GHz

 *	Intel(R) Xeon(R)  CPU E5-2618L v3  @  2.30GHz

 *	Intel(R) Xeon(R)  CPU E5-2608L v3  @  2.00GHz

 *	Intel(R) Xeon(R)  CPU E5-2658A v3  @  2.20GHz

 *

 * Probe by trying to write the first of the L3 cache mask registers

 * and checking that the bits stick. Max CLOSids is always 4 and max cbm length

 * is always 20 on hsw server parts. The minimum cache bitmask length

 * allowed for HSW server is always 2 bits. Hardcode all of them.

 If all the bits were set in MSR, return success */

/*

 * rdt_get_mb_table() - get a mapping of bandwidth(b/w) percentage values

 * exposed to user interface and the h/w understandable delay values.

 *

 * The non-linear delay values have the granularity of power of two

 * and also the h/w does not guarantee a curve for configured delay

 * values vs. actual b/w enforced.

 * Hence we need a mapping that is pre calibrated so the user can

 * express the memory b/w as a percentage value.

	/*

	 * There are no Intel SKUs as of now to support non-linear delay.

 AMD does not use delay */

	/*

	 * AMD does not use memory delay throttle model to control

	 * the allocation like Intel does.

 Max value is 2048, Data width should be 4 in decimal */

	/*

	 * By default, CDP is disabled. CDP can be enabled by mount parameter

	 * "cdp" during resctrl file system mount time.

/*

 * Map the memory b/w percentage value to delay values

 * that can be written to QOS_MSRs.

 * There are currently no SKUs which support non linear delay values.

  Write the delay values for mba. */

 Find the domain that contains this CPU */

/*

 * rdt_find_domain - Find a domain in a resource that matches input resource id

 *

 * Search resource r's domain list to find the resource id. If the resource

 * id is found in a domain, return the domain. Otherwise, if requested by

 * caller, return the first domain whose id is bigger than the input id.

 * The domain list is sorted by id in ascending order.

 When id is found, return its domain. */

 Stop searching when finding id's position in sorted list. */

	/*

	 * Initialize the Control MSRs to having no control.

	 * For Cache Allocation: Set all bits in cbm

	 * For Memory Allocation: Set b/w requested to 100%

	 * and the bandwidth in MBps to U32_MAX

/*

 * domain_add_cpu - Add a cpu to a resource's domain list.

 *

 * If an existing domain in the resource r's domain list matches the cpu's

 * resource id, add the cpu in the domain.

 *

 * Otherwise, a new domain is allocated and inserted into the right position

 * in the domain list sorted by id in ascending order.

 *

 * The order in the domain list is visible to users when we print entries

 * in the schemata file and schemata input is validated to have the same order

 * as this list.

	/*

	 * If resctrl is mounted, add

	 * per domain monitor data directories.

		/*

		 * If resctrl is mounted, remove all the

		 * per domain monitor data directories.

			/*

			 * When a package is going down, forcefully

			 * decrement rmid->ebusy. There is no way to know

			 * that the L3 was flushed and hence may lead to

			 * incorrect counts in rare scenarios, but leaving

			 * the RMID as busy creates RMID leaks if the

			 * package never comes back.

		/*

		 * rdt_domain "d" is going to be freed below, so clear

		 * its pointer from pseudo_lock_region struct.

 The cpu is set in default rdtgroup after online. */

/*

 * Choose a width for the resource name and resource data based on the

 * resource that has widest name and cbm.

 CPUID 0x10.2 fields are same format at 0x10.1 */

 Runs once on the BSP during boot. */

 will be overridden if occupancy monitoring exists */

 QoS sub-leaf, EAX=0Fh, ECX=1 */

	/*

	 * Initialize functions(or definitions) that are different

	 * between vendors here.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  AMD CPU Microcode Update Driver for Linux

 *

 *  This driver allows to upgrade microcode on F10h AMD

 *  CPUs and later.

 *

 *  Copyright (C) 2008-2011 Advanced Micro Devices Inc.

 *	          2013-2018 Borislav Petkov <bp@alien8.de>

 *

 *  Author: Peter Oruba <peter.oruba@amd.com>

 *

 *  Based on work by:

 *  Tigran Aivazian <aivazian.tigran@gmail.com>

 *

 *  early loader:

 *  Copyright (C) 2013 Advanced Micro Devices, Inc.

 *

 *  Author: Jacob Shin <jacob.shin@amd.com>

 *  Fixes: Borislav Petkov <bp@suse.de>

/*

 * This points to the current valid container of microcode patches which we will

 * save from the initrd/builtin before jettisoning its contents. @mc is the

 * microcode patch we found to match.

/*

 * Microcode patch container file is prepended to the initrd in cpio

 * format. See Documentation/x86/microcode.rst

/*

 * Check whether there is a valid microcode container file at the beginning

 * of @buf of size @buf_size. Set @early to use this function in the early path.

/*

 * Check whether there is a valid, non-truncated CPU equivalence table at the

 * beginning of @buf of size @buf_size. Set @early to use this function in the

 * early path.

/*

 * Check whether there is a valid, non-truncated microcode patch section at the

 * beginning of @buf of size @buf_size. Set @early to use this function in the

 * early path.

 *

 * On success, @sh_psize returns the patch size according to the section header,

 * to the caller.

/*

 * Check whether the passed remaining file @buf_size is large enough to contain

 * a patch of the indicated @sh_psize (and also whether this size does not

 * exceed the per-family maximum). @sh_psize is the size read from the section

 * header.

/*

 * Verify the patch in @buf.

 *

 * Returns:

 * negative: on error

 * positive: patch is not for this family, skip it

 * 0: success

	/*

	 * The section header length is not included in this indicated size

	 * but is present in the leftover file length so we need to subtract

	 * it before passing this value to the function below.

	/*

	 * Check if the remaining buffer is big enough to contain a patch of

	 * size sh_psize, as the section claims.

/*

 * This scans the ucode blob for the proper container as we can have multiple

 * containers glued together. Returns the equivalence ID from the equivalence

 * table or 0 if none found.

 * Returns the amount of bytes consumed while scanning. @desc contains all the

 * data we're going to use in later stages of the application.

	/*

	 * Find the equivalence ID of our CPU in this table. Even if this table

	 * doesn't contain a patch for the CPU, scan through the whole container

	 * so that it can be skipped in case there are other containers appended.

	/*

	 * Scan through the rest of the container to find where it ends. We do

	 * some basic sanity-checking too.

			/*

			 * Patch verification failed, skip to the next

			 * container, if there's one:

 Skip patch section header too: */

	/*

	 * If we have found a patch (desc->mc), it means we're looking at the

	 * container which has a patch for this CPU so return 0 to mean, @ucode

	 * already points to the proper container. Otherwise, we return the size

	 * we scanned so that we can advance to the next container in the

	 * buffer.

/*

 * Scan the ucode blob for the proper container as we can have multiple

 * containers glued together.

 catch wraparound */

 verify patch application was successful */

/*

 * Early load occurs before we can vmalloc(). So we look for the microcode

 * patch container file in initrd, traverse equivalent cpu table, look for a

 * matching microcode patch, and update, all in initrd memory in place.

 * When vmalloc() is available for use later -- on 64-bit during first AP load,

 * and on 32-bit during save_microcode_in_initrd_amd() -- we can call

 * load_microcode_amd() to save equivalent cpu table and microcode patches in

 * kernel heap memory.

 *

 * Returns true if container found (sets @desc), false otherwise.

 Needed in load_microcode_amd() */

 Check whether we have saved a new patch already: */

/*

 * a small, trivial cache of per-family ucode patches

 we already have the latest patch */

 no patch found, add it */

	/*

	 * a patch could have been loaded early, set uci->mc so that

	 * mc_bp_resume() can call apply_microcode()

 need to apply patch? */

 Update boot_cpu_data's revision too, if we're on the BSP: */

 add header length */

/*

 * Return a non-negative value even if some of the checks failed so that

 * we can skip over the next patch. If we return a negative value, we

 * signal a grave error like a memory allocation has failed and the

 * driver cannot continue functioning normally. In such cases, we tear

 * down everything we've used up so far and exit.

 ... and add to cache. */

 free old equiv table */

 save BSP's matching patch for early load */

/*

 * AMD microcode firmware naming convention, up to family 15h they are in

 * the legacy file:

 *

 *    amd-ucode/microcode_amd.bin

 *

 * This legacy file is always smaller than 2K in size.

 *

 * Beginning with family 15h, they are in family-specific firmware files:

 *

 *    amd-ucode/microcode_amd_fam15h.bin

 *    amd-ucode/microcode_amd_fam16h.bin

 *    ...

 *

 * These might be larger than 2K.

 reload ucode container only on the boot cpu */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * CPU Microcode Update Driver for Linux

 *

 * Copyright (C) 2000-2006 Tigran Aivazian <aivazian.tigran@gmail.com>

 *	      2006	Shaohua Li <shaohua.li@intel.com>

 *	      2013-2016	Borislav Petkov <bp@alien8.de>

 *

 * X86 CPU microcode early update for Linux:

 *

 *	Copyright (C) 2012 Fenghua Yu <fenghua.yu@intel.com>

 *			   H Peter Anvin" <hpa@zytor.com>

 *		  (C) 2015 Borislav Petkov <bp@alien8.de>

 *

 * This driver allows to upgrade microcode on x86 processors.

/*

 * Synchronization.

 *

 * All non cpu-hotplug-callback call sites use:

 *

 * - microcode_mutex to synchronize with each other;

 * - cpus_read_lock/unlock() to synchronize with

 *   the cpu-hotplug-callback call sites.

 *

 * We guarantee that only a single cpu is being

 * updated at any particular moment of time.

/*

 * Those patch levels cannot be updated to newer ones and thus should be final.

 T-101 terminator */

/*

 * Check the current patch level on this CPU.

 *

 * Returns:

 *  - true: if update should stop

 *  - false: otherwise

 CONFIG_X86_64 */

	/*

	 * CPUID(1).ECX[31]: reserved for hypervisor use. This is still not

	 * completely accurate as xen pv guests don't see that CPUID bit set but

	 * that's good enough as they don't land on the BSP path anyway.

	/*

	 * Set start only if we have an initrd image. We cannot use initrd_start

	 * because it is not set that early yet.

 CONFIG_X86_64 */

	/*

	 * Fixup the start address: after reserve_initrd() runs, initrd_start

	 * has the virtual address of the beginning of the initrd. It also

	 * possibly relocates the ramdisk. In either case, initrd_start contains

	 * the updated address so use that instead.

	 *

	 * initrd_gone is for the hotplug case where we've thrown out initrd

	 * already.

		/*

		 * The picture with physical addresses is a bit different: we

		 * need to get the *physical* address to which the ramdisk was

		 * relocated, i.e., relocated_ramdisk (not initrd_start) and

		 * since we're running from physical addresses, we need to access

		 * relocated_ramdisk through its *physical* address too.

 !CONFIG_BLK_DEV_INITRD */

 fake device for request_firmware */

/*

 * Late loading dance. Why the heavy-handed stomp_machine effort?

 *

 * - HT siblings must be idle and not execute other code while the other sibling

 *   is loading microcode in order to avoid any negative interactions caused by

 *   the loading.

 *

 * - In addition, microcode update on the cores must be serialized until this

 *   requirement can be relaxed in the future. Right now, this is conservative

 *   and good.

 100 nsec */

	/*

	 * Make sure all CPUs are online.  It's fine for SMT to be disabled if

	 * all the primary threads are still online.

/*

 * Returns:

 * < 0 - on error

 *   0 - success (no update done or microcode was updated)

	/*

	 * Wait for all CPUs to arrive. A load will not be attempted unless all

	 * CPUs show up.

	/*

	 * On an SMT system, it suffices to load the microcode on one sibling of

	 * the core because the microcode engine is shared between the threads.

	 * Synchronization still needs to take place so that no concurrent

	 * loading attempts happen on multiple threads of an SMT core. See

	 * below.

	/*

	 * At least one thread has completed update on each core.

	 * For others, simply call the update to make sure the

	 * per-cpu cpuinfo can be updated with right microcode

	 * revision.

/*

 * Reload microcode late on all CPUs. Wait for a sec until they

 * all gather together.

 --dimm. Trigger a delayed update? */

 Refresh CPU microcode revision after resume. */

/**

 * mc_bp_resume - Update boot CPU microcode during resume.

 Suspend is in progress, only remove the interface */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Intel CPU Microcode Update Driver for Linux

 *

 * Copyright (C) 2000-2006 Tigran Aivazian <aivazian.tigran@gmail.com>

 *		 2006 Shaohua Li <shaohua.li@intel.com>

 *

 * Intel CPU microcode early update for Linux

 *

 * Copyright (C) 2012 Fenghua Yu <fenghua.yu@intel.com>

 *		      H Peter Anvin" <hpa@zytor.com>

/*

 * This needs to be before all headers so that pr_debug in printk.h doesn't turn

 * printk calls into no_printk().

 *

 *#define DEBUG

 Current microcode patch used in early patching on the APs. */

 last level cache size per core */

 Processor flags are either both 0 ... */

 ... or they intersect. */

/*

 * Returns 1 if update has been found, 0 otherwise.

 Look for ext. headers: */

/*

 * Returns 1 if update has been found, 0 otherwise.

	/*

	 * There weren't any previous patches found in the list cache; save the

	 * newly found.

	/*

	 * Save for early loading. On 32-bit, that needs to be a physical

	 * address as the APs are running from physical addresses, before

	 * paging has been enabled.

		/*

		 * Check extended table checksum: the sum of all dwords that

		 * comprise a valid table must be 0.

	/*

	 * Calculate the checksum of update data and header. The checksum of

	 * valid update data and header including the extended signature table

	 * must be 0.

	/*

	 * Check extended signature checksum: 0 => valid.

/*

 * Get microcode matching with BSP's model. Only CPUs with the same model as

 * BSP can stay in the platform.

 We have a newer patch, save it. */

 get processor flags from MSR 0x17 */

 Look for ext. headers: */

/*

 * Save this microcode patch. It will be loaded early when a CPU is

 * hot-added or resumes.

 Synchronization during CPU hotplug. */

/*

 * Print ucode update info.

/*

 * Print early updated ucode info after printk works. This is delayed info dump.

/*

 * At this point, we can not call printk() yet. Delay printing microcode info in

 * show_ucode_info_early() until printk() works.

	/*

	 * Save us the MSR write below - which is a particular expensive

	 * operation - when the other hyperthread has updated the microcode

	 * already.

	/*

	 * Writeback and invalidate caches before updating microcode to avoid

	 * internal issues depending on what the microcode is updating.

 write microcode via MSR 0x79 */

	/*

	 * initrd is going away, clear patch ptr. We will scan the microcode one

	 * last time before jettisoning and save a patch, if found. Then we will

	 * update that pointer too, with a stable patch address to use when

	 * resuming the cores.

/*

 * @res_patch, output: a pointer to the patch we found.

 try built-in microcode first */

 Mixed-silicon system? Try to refetch the proper patch: */

 get processor flags from MSR 0x17 */

 No extra locking on prev, races are harmless. */

 We should bind the task to the CPU */

 Look for a newer patch in our cache: */

	/*

	 * Save us the MSR write below - which is a particular expensive

	 * operation - when the other hyperthread has updated the microcode

	 * already.

	/*

	 * Writeback and invalidate caches before updating microcode to avoid

	 * internal issues depending on what the microcode is updating.

 write microcode via MSR 0x79 */

 Update boot_cpu_data's revision too, if we're on the BSP: */

 For performance reasons, reuse mc area when possible */

 trigger new vmalloc */

	/*

	 * If early loading microcode is supported, save this mc into

	 * permanent memory. So it will be loaded early when a CPU is hot added

	 * or resumes.

	/*

	 * Late loading on model 79 with microcode revision less than 0x0b000021

	 * and LLC size per core bigger than 2.5MB may result in a system hang.

	 * This behavior is documented in item BDF90, #334165 (Intel Xeon

	 * Processor E7-8800/4800 v4 Product Family).

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This only handles 32bit MTRR on 32bit hosts. This is strictly wrong

 * because MTRRs can span up to 40 bits (36bits on most modern x86)

 start address of an MTRR block */

 number of MTRRs in this block  */

 one   64k MTRR  */

 two   16k MTRRs */

 eight  4k MTRRs */

/*

 * BIOS is expected to clear MtrrFixDramModEn bit, see for example

 * "BIOS and Kernel Developer's Guide for the AMD Athlon 64 and AMD

 * Opteron Processors" (26094 Rev. 3.30 February 2006), section

 * "13.2.1.2 SYSCFG Register": "The MtrrFixDramModEn bit should be set

 * to 1 during BIOS initialization of the fixed MTRRs, then cleared to

 * 0 for operation."

 Get the size of contiguous MTRR range */

/*

 * Check and return the effective type for MTRR-MTRR type overlap.

 * Returns 1 if the effective type is UNCACHEABLE, else returns 0

/**

 * mtrr_type_lookup_fixed - look up memory type in MTRR fixed entries

 *

 * Return the MTRR fixed memory type of 'start'.

 *

 * MTRR fixed entries are divided into the following ways:

 *  0x00000 - 0x7FFFF : This range is divided into eight 64KB sub-ranges

 *  0x80000 - 0xBFFFF : This range is divided into sixteen 16KB sub-ranges

 *  0xC0000 - 0xFFFFF : This range is divided into sixty-four 4KB sub-ranges

 *

 * Return Values:

 * MTRR_TYPE_(type)  - Matched memory type

 * MTRR_TYPE_INVALID - Unmatched

 0x0 - 0x7FFFF */

 0x80000 - 0xBFFFF */

 0xC0000 - 0xFFFFF */

/**

 * mtrr_type_lookup_variable - look up memory type in MTRR variable entries

 *

 * Return Value:

 * MTRR_TYPE_(type) - Matched memory type or default memory type (unmatched)

 *

 * Output Arguments:

 * repeat - Set to 1 when [start:end] spanned across MTRR range and type

 *	    returned corresponds only to [start:*partial_end].  Caller has

 *	    to lookup again for [*partial_end:end].

 *

 * uniform - Set to 1 when an MTRR covers the region uniformly, i.e. the

 *	     region is fully covered by a single MTRR entry or the default

 *	     type.

			/*

			 * We have start:end spanning across an MTRR.

			 * We split the region into either

			 *

			 * - start_state:1

			 * (start:mtrr_end)(mtrr_end:end)

			 * - end_state:1

			 * (start:mtrr_start)(mtrr_start:end)

			 * - inclusive:1

			 * (start:mtrr_start)(mtrr_start:mtrr_end)(mtrr_end:end)

			 *

			 * depending on kind of overlap.

			 *

			 * Return the type of the first region and a pointer

			 * to the start of next region so that caller will be

			 * advised to lookup again after having adjusted start

			 * and end.

			 *

			 * Note: This way we handle overlaps with multiple

			 * entries and the default type properly.

 end is inclusive */

/**

 * mtrr_type_lookup - look up memory type in MTRR

 *

 * Return Values:

 * MTRR_TYPE_(type)  - The effective MTRR type for the region

 * MTRR_TYPE_INVALID - MTRR is disabled

 *

 * Output Argument:

 * uniform - Set to 1 when an MTRR covers the region uniformly, i.e. the

 *	     region is fully covered by a single MTRR entry or the default

 *	     type.

 Make end inclusive instead of exclusive */

	/*

	 * Look up the fixed ranges first, which take priority over

	 * the variable ranges.

	/*

	 * Look up the variable ranges.  Look of multiple ranges matching

	 * this address and pick type as per MTRR precedence.

	/*

	 * Common path is with repeat = 0.

	 * However, we can have cases where [start:end] spans across some

	 * MTRR ranges and/or the default type.  Do repeated lookups for

	 * that case here.

 Get the MSR pair relating to a var range */

 Fill the MSR pair relating to a var range */

 new segments: gap or different type */

 tail */

 PAT setup for BP. We need to go through sync steps here */

 Grab all of the MTRR state for this CPU into *state */

 TOP_MEM2 */

 Some BIOS's are messed up and don't set all MTRRs the same! */

/*

 * Doesn't attempt to pass an error out to MTRR users

 * because it's quite complicated in some cases and probably not

 * worth it because the best error handling is to ignore it.

/**

 * set_fixed_range - checks & updates a fixed-range MTRR if it

 *		     differs from the value it should have

 * @msr: MSR address of the MTTR which should be checked and updated

 * @changed: pointer which indicates whether the MTRR needed to be changed

 * @msrwords: pointer to the MSR values which the MSR should have

/**

 * generic_get_free_region - Get a free MTRR.

 * @base: The starting (base) address of the region.

 * @size: The size (in bytes) of the region.

 * @replace_reg: mtrr index to be replaced; set to invalid value if none.

 *

 * Returns: The index of the region on success, else negative on error.

	/*

	 * get_mtrr doesn't need to update mtrr_state, also it could be called

	 * from any cpu, so try to print it out directly.

  Invalid (i.e. free) range */

 Work out the shifted address mask: */

 Expand tmp with high bits to all 1s: */

	/*

	 * This works correctly if size is a power of two, i.e. a

	 * contiguous range:

/**

 * set_fixed_ranges - checks & updates the fixed-range MTRRs if they

 *		      differ from the saved set

 * @frs: pointer to fixed-range MTRR values, saved by get_fixed_ranges()

/*

 * Set the MSR pair relating to a var range.

 * Returns true if changes are made.

/**

 * set_mtrr_state - Set the MTRR state for this CPU.

 *

 * NOTE: The CPU must already be in a safe state for MTRR changes.

 * RETURNS: 0 if no changes made, else a mask indicating what was changed.

	/*

	 * Set_mtrr_restore restores the old value of MTRRdefType,

	 * so to set it we fiddle with the saved value:

/*

 * Since we are disabling the cache don't allow any interrupts,

 * they would run extremely slow and would only increase the pain.

 *

 * The caller must ensure that local interrupts are disabled and

 * are reenabled after post_set() has been called.

	/*

	 * Note that this is not ideal

	 * since the cache is only flushed/disabled for this CPU while the

	 * MTRRs are changed, but changing this requires more invasive

	 * changes to the way the kernel boots

 Enter the no-fill (CD=1, NW=0) cache mode and flush caches. */

	/*

	 * Cache flushing is the most time-consuming step when programming

	 * the MTRRs. Fortunately, as per the Intel Software Development

	 * Manual, we can skip it if the processor supports cache self-

	 * snooping.

 Save value of CR4 and clear Page Global Enable (bit 7) */

 Flush all TLBs via a mov %cr3, %reg; mov %reg, %cr3 */

 Save MTRR state */

 Disable MTRRs, and set the default type to uncached */

 Again, only flush caches if we have to. */

 Flush TLBs (no need to flush caches - they are disabled) */

 Intel (P6) standard MTRRs */

 Enable caches */

 Restore value of CR4 */

 Actually set the state */

 also set PAT */

 Use the atomic bitops to update the global mask */

/**

 * generic_set_mtrr - set variable MTRR register on the local CPU.

 *

 * @reg: The register to set.

 * @base: The base address of the region.

 * @size: The size of the region. If this is 0 the region is disabled.

 * @type: The type of the region.

 *

 * Returns nothing.

		/*

		 * The invalid bit is kept in the mask, so we simply

		 * clear the relevant mask register to disable a range.

	/*

	 * For Intel PPro stepping <= 7

	 * must be 4 MiB aligned and not touch 0x70000000 -> 0x7003FFFF

	/*

	 * Check upper bits of base and last are equal and lower bits are 0

	 * for base and 1 for last

/*

 * Generic structure...

 SPDX-License-Identifier: GPL-2.0

 0 for winchip, 1 for winchip2 */

/**

 * centaur_get_free_region - Get a free MTRR.

 *

 * @base: The starting (base) address of the region.

 * @size: The size (in bytes) of the region.

 *

 * Returns: the index of the region on success, else -1 on error.

/*

 * Report boot time MCR setups

 write-combining  */

 Disable */

 Only support write-combining... */

 NC */

 WWO, WC */

	/*

	 * FIXME: Winchip2 supports uncached

 SPDX-License-Identifier: GPL-2.0

 Upper dword is region 1, lower is region 0 */

 The base masks off on the right alignment */

	/*

	 * This needs a little explaining. The size is stored as an

	 * inverted mask of bits of 128K granularity 15 bits long offset

	 * 2 bits.

	 *

	 * So to get a size we do invert the mask and add 1 to the lowest

	 * mask bit (4 as its 2 bits in). This gives us a size we then shift

	 * to turn into 128K blocks.

	 *

	 * eg              111 1111 1111 1100      is 512K

	 *

	 * invert          000 0000 0000 0011

	 * +1              000 0000 0000 0100

	 * *128K   ...

/**

 * amd_set_mtrr - Set variable MTRR register on the local CPU.

 *

 * @reg The register to set.

 * @base The base address of the region.

 * @size The size of the region. If this is 0 the region is disabled.

 * @type The type of the region.

 *

 * Returns nothing.

	/*

	 * Low is MTRR0, High MTRR 1

	/*

	 * Blank to disable

		/*

		 * Set the register to the base, the type (off by one) and an

		 * inverted bitmask of the size The size is the only odd

		 * bit. We are fed say 512K We invert this and we get 111 1111

		 * 1111 1011 but if you subtract one and invert you get the

		 * desired 111 1111 1111 1100 mask

		 *

		 *  But ~(x - 1) == ~x + 1 == -x. Two's complement rocks!

	/*

	 * The writeback rule is quite specific. See the manual. Its

	 * disable local interrupts, write back the cache, set the mtrr

	/*

	 * Apply the K6 block alignment and size rules

	 * In order

	 * o Uncached or gathering only

	 * o 128K or bigger block

	 * o Power of 2 block

	 * o base suitably aligned to the power

 SPDX-License-Identifier: GPL-2.0

 avoid multiplication by 3 */

 enable MAPEN */

 disable MAPEN */

	/*

	 * Power of two, at least 4K on ARR0-ARR6, 256K on ARR7

	 * Note: shift==0xf means 4G, this is unsupported.

 Bit 0 is Cache Enable on ARR7, Cache Disable on ARR0-ARR6 */

/*

 * cyrix_get_free_region - get a free ARR.

 *

 * @base: the starting (base) address of the region.

 * @size: the size (in bytes) of the region.

 *

 * Returns: the index of the region on success, else -1 on error.

 If we are to set up a region >32M then look at ARR7 immediately */

 Else try ARR0-ARR6 first  */

		/*

		 * ARR0-ARR6 isn't free

		 * try ARR7 but its size must be at least 256K

  Save value of CR4 and clear Page Global Enable (bit 7)  */

	/*

	 * Disable and flush caches.

	 * Note that wbinvd flushes the TLBs as a side-effect

 Cyrix ARRs - everything else was excluded at the top */

 Cyrix ARRs - everything else was excluded at the top */

 Flush caches and TLBs */

 Cyrix ARRs - everything else was excluded at the top */

 Enable caches */

 Restore value of CR4 */

 avoid multiplication by 3 */

 count down from 32M (ARR0-ARR6) or from 2G (ARR7) */

 make sure arr_size <= 14 */

 the CCRs are not contiguous */

/*

 * MTRR (Memory Type Range Register) cleanup

 *

 *  Copyright (C) 2009 Yinghai Lu

 *

 * This library is free software; you can redistribute it and/or

 * modify it under the terms of the GNU Library General Public

 * License as published by the Free Software Foundation; either

 * version 2 of the License, or (at your option) any later version.

 *

 * This library is distributed in the hope that it will be useful,

 * but WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU

 * Library General Public License for more details.

 *

 * You should have received a copy of the GNU Library General Public

 * License along with this library; if not, write to the Free

 * Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.

 Should be related to MTRR_VAR_RANGES nums */

 Take out UC ranges: */

 Var MTRR contains UC entry below 1M? Skip it: */

 sort the ranges */

 Not MB-aligned: */

 Compute the maximum size with which we can make a range: */

 Align with gran size, prevent small block used up MTRRs: */

 Try to append some small hole: */

 No increase: */

 Only cut back when it is not the last: */

 One hole in the middle: */

 One hole in middle or at the end: */

 Hole size should be less than half of range0 size: */

 Need to handle left over range: */

 See if I can merge with the last range: */

 Write the range mtrrs: */

 Allocate an msr: */

 Minimum size of mtrr block that can take hole: */

 Granularity of mtrr of block: */

 Write the range: */

 Write the last range: */

 Clear out the extra MTRR's: */

/*

 * gran_size: 64K, 128K, 256K, 512K, 1M, 2M, ..., 2G

 * chunk size: gran_size, ..., 2G

 * so we need (1+16)*8

 Extra one for all 0: */

 Check entries number: */

 Check if we got UC entries: */

 Check if we only had WB and UC */

	/*

	 * range_new should really be an automatic variable, but

	 * putting 4096 bytes on the stack is frowned upon, to put it

	 * mildly. It is safe to make it a static __initdata variable,

	 * since mtrr_calc_range_state is only called during init and

	 * there's no way it will call itself recursively.

 Convert ranges to var ranges state: */

 We got new setting in range_state, check it: */

 Double check it: */

 Get it and store it aside: */

 Check if we need handle it and can handle it: */

 Print original var MTRRs at first, for debugging: */

	/*

	 * [0, 1M) should always be covered by var mtrr with WB

	 * and fixed mtrrs should take effect before var mtrr for it:

 add from var mtrr at last */

 Try to find the optimal index: */

 Convert ranges to var ranges state: */

 print out all */

/*

 * Newer AMD K8s and later CPUs have a special magic MSR way to force WB

 * for memory >4GB. Check for that here.

 * Note this won't check if the MTRRs < 4GB where the magic bit doesn't

 * apply to are wrong, but so far we don't know of any such case in the wild.

 In case some hypervisor doesn't pass SYSCFG through: */

	/*

	 * Memory between 4GB and top of mem is forced WB by this magic bit.

	 * Reserved before K8RevF, but should be zero there.

/**

 * mtrr_trim_uncached_memory - trim RAM not covered by MTRRs

 * @end_pfn: ending page frame number

 *

 * Some buggy BIOSes don't setup the MTRRs properly for systems with certain

 * memory configurations.  This routine checks that the highest MTRR matches

 * the end of memory, to make sure the MTRRs having a write back type cover

 * all of the memory the kernel is intending to use.  If not, it'll trim any

 * memory off the end by adjusting end_pfn, removing it from the kernel's

 * allocation pools, warning the user with an obnoxious message.

 extra one for all 0 */

	/*

	 * Make sure we only trim uncachable memory on machines that

	 * support the Intel MTRR architecture:

 Get it and store it aside: */

 Find highest cached pfn: */

 kvm/qemu doesn't have mtrr set right, don't trim them all: */

 Check entries number: */

 No entry for WB? */

 Check if we only had WB and UC: */

 Check the head: */

 Check the holes: */

 Check the top: */

 SPDX-License-Identifier: GPL-2.0

 0 */

 1 */

 2 */

 3 */

 4 */

 5 */

 6 */

/*

 * seq_file can seek but we ignore it.

 *

 * Format of control line:

 *    "base=%Lx size=%Lx type=%s" or "disable=%d"

 Hide entries that go above 4GB */

 Hide entries that would overflow */

 less than 1MB */

 Base can be > 32bit */

  CONFIG_PROC_FS  */

/*  Generic MTRR (Memory Type Range Register) driver.



    Copyright (C) 1997-2000  Richard Gooch

    Copyright (c) 2002	     Patrick Mochel



    This library is free software; you can redistribute it and/or

    modify it under the terms of the GNU Library General Public

    License as published by the Free Software Foundation; either

    version 2 of the License, or (at your option) any later version.



    This library is distributed in the hope that it will be useful,

    but WITHOUT ANY WARRANTY; without even the implied warranty of

    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU

    Library General Public License for more details.



    You should have received a copy of the GNU Library General Public

    License along with this library; if not, write to the Free

    Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.



    Richard Gooch may be reached by email at  rgooch@atnf.csiro.au

    The postal address is:

      Richard Gooch, c/o ATNF, P. O. Box 76, Epping, N.S.W., 2121, Australia.



    Source: "Pentium Pro Family Developer's Manual, Volume 3:

    Operating System Writer's Guide" (Intel document number 242692),

    section 11.11.7



    This was cleaned and made readable by Patrick Mochel <mochel@osdl.org>

    on 6-7 March 2002.

    Source: Intel Architecture Software Developers Manual, Volume 3:

    System Programming Guide; Section 9.11. (1997 edition - PPro).

 FIXME: kvm_para.h needs this */

 arch_phys_wc_add returns an MTRR register index plus this offset. */

  Returns non-zero if we have the write-combining memory type  */

		/*

		 * ServerWorks LE chipsets < rev 6 have problems with

		 * write-combining. Don't allow it and leave room for other

		 * chipsets to be tagged

		/*

		 * Intel 450NX errata # 23. Non ascending cacheline evictions to

		 * write combining memory may resulting in data corruption

  This function returns the number of variable MTRRs  */

/**

 * mtrr_rendezvous_handler - Work done in the synchronization handler. Executed

 * by all the CPUs.

 * @info: pointer to mtrr configuration data

 *

 * Returns nothing.

	/*

	 * We use this same function to initialize the mtrrs during boot,

	 * resume, runtime cpu online and on an explicit request to set a

	 * specific MTRR.

	 *

	 * During boot or suspend, the state of the boot cpu's mtrrs has been

	 * saved, and we want to replicate that across all the cpus that come

	 * online (either at the end of boot or resume or during a runtime cpu

	 * online). If we're doing that, @reg is set to something special and on

	 * all the cpu's we do mtrr_if->set_all() (On the logical cpu that

	 * started the boot/resume sequence, this might be a duplicate

	 * set_all()).

/**

 * set_mtrr - update mtrrs on all processors

 * @reg:	mtrr in question

 * @base:	mtrr base

 * @size:	mtrr size

 * @type:	mtrr type

 *

 * This is kinda tricky, but fortunately, Intel spelled it out for us cleanly:

 *

 * 1. Queue work to do the following on all processors:

 * 2. Disable Interrupts

 * 3. Wait for all procs to do so

 * 4. Enter no-fill cache mode

 * 5. Flush caches

 * 6. Clear PGE bit

 * 7. Flush all TLBs

 * 8. Disable all range registers

 * 9. Update the MTRRs

 * 10. Enable all range registers

 * 11. Flush all TLBs and caches again

 * 12. Enter normal cache mode and reenable caching

 * 13. Set PGE

 * 14. Wait for buddies to catch up

 * 15. Enable interrupts.

 *

 * What does that mean for us? Well, stop_machine() will ensure that

 * the rendezvous handler is started on each CPU. And in lockstep they

 * do the state transition of disabling interrupts, updating MTRR's

 * (the CPU vendors may each do it differently, so we call mtrr_if->set()

 * callback and let them take care of it.) and enabling interrupts.

 *

 * Note that the mechanism is the same for UP systems, too; all the SMP stuff

 * becomes nops.

/**

 * mtrr_add_page - Add a memory type region

 * @base: Physical base address of region in pages (in units of 4 kB!)

 * @size: Physical size of region in pages (4 kB)

 * @type: Type of MTRR desired

 * @increment: If this is true do usage counting on the region

 *

 * Memory type region registers control the caching on newer Intel and

 * non Intel processors. This function allows drivers to request an

 * MTRR is added. The details and hardware specifics of each processor's

 * implementation are hidden from the caller, but nevertheless the

 * caller should expect to need to provide a power of two size on an

 * equivalent power of two boundary.

 *

 * If the region cannot be added either because all regions are in use

 * or the CPU cannot support it a negative value is returned. On success

 * the register number for this entry is returned, but should be treated

 * as a cookie only.

 *

 * On a multiprocessor machine the changes are made to all processors.

 * This is required on x86 by the Intel processors.

 *

 * The available types are

 *

 * %MTRR_TYPE_UNCACHABLE - No caching

 *

 * %MTRR_TYPE_WRBACK - Write data back in bursts whenever

 *

 * %MTRR_TYPE_WRCOMB - Write data back soon but allow bursts

 *

 * %MTRR_TYPE_WRTHROUGH - Cache reads but not writes

 *

 * BUGS: Needs a quiet flag for the cases where drivers do not mind

 * failures and do not wish system log messages to be sent.

 If the type is WC, check that this processor supports it */

 No CPU hotplug when we change MTRR entries */

 Search for existing MTRR  */

		/*

		 * At this point we know there is some kind of

		 * overlap/enclosure

  New region encloses an existing region  */

 New region is enclosed by an existing region */

 Search for an empty MTRR */

/**

 * mtrr_add - Add a memory type region

 * @base: Physical base address of region

 * @size: Physical size of region

 * @type: Type of MTRR desired

 * @increment: If this is true do usage counting on the region

 *

 * Memory type region registers control the caching on newer Intel and

 * non Intel processors. This function allows drivers to request an

 * MTRR is added. The details and hardware specifics of each processor's

 * implementation are hidden from the caller, but nevertheless the

 * caller should expect to need to provide a power of two size on an

 * equivalent power of two boundary.

 *

 * If the region cannot be added either because all regions are in use

 * or the CPU cannot support it a negative value is returned. On success

 * the register number for this entry is returned, but should be treated

 * as a cookie only.

 *

 * On a multiprocessor machine the changes are made to all processors.

 * This is required on x86 by the Intel processors.

 *

 * The available types are

 *

 * %MTRR_TYPE_UNCACHABLE - No caching

 *

 * %MTRR_TYPE_WRBACK - Write data back in bursts whenever

 *

 * %MTRR_TYPE_WRCOMB - Write data back soon but allow bursts

 *

 * %MTRR_TYPE_WRTHROUGH - Cache reads but not writes

 *

 * BUGS: Needs a quiet flag for the cases where drivers do not mind

 * failures and do not wish system log messages to be sent.

/**

 * mtrr_del_page - delete a memory type region

 * @reg: Register returned by mtrr_add

 * @base: Physical base address

 * @size: Size of region

 *

 * If register is supplied then base and size are ignored. This is

 * how drivers should call it.

 *

 * Releases an MTRR region. If the usage count drops to zero the

 * register is freed and the region returns to default state.

 * On success the register is returned, on failure a negative error

 * code.

 No CPU hotplug when we change MTRR entries */

  Search for existing MTRR  */

/**

 * mtrr_del - delete a memory type region

 * @reg: Register returned by mtrr_add

 * @base: Physical base address

 * @size: Size of region

 *

 * If register is supplied then base and size are ignored. This is

 * how drivers should call it.

 *

 * Releases an MTRR region. If the usage count drops to zero the

 * register is freed and the region returns to default state.

 * On success the register is returned, on failure a negative error

 * code.

/**

 * arch_phys_wc_add - add a WC MTRR and handle errors if PAT is unavailable

 * @base: Physical base address

 * @size: Size of region

 *

 * If PAT is available, this does nothing.  If PAT is unavailable, it

 * attempts to add a WC MTRR covering size bytes starting at base and

 * logs an error if this fails.

 *

 * The called should provide a power of two size on an equivalent

 * power of two boundary.

 *

 * Drivers must store the return value to pass to mtrr_del_wc_if_needed,

 * but drivers should not try to interpret that return value.

 Success!  (We don't need to do anything.) */

/*

 * arch_phys_wc_del - undoes arch_phys_wc_add

 * @handle: Return value from arch_phys_wc_add

 *

 * This cleans up after mtrr_add_wc_if_needed.

 *

 * The API guarantees that mtrr_del_wc_if_needed(error code) and

 * mtrr_del_wc_if_needed(0) do nothing.

/*

 * arch_phys_wc_index - translates arch_phys_wc_add's return value

 * @handle: Return value from arch_phys_wc_add

 *

 * This will turn the return value from arch_phys_wc_add into an mtrr

 * index suitable for debugging.

 *

 * Note: There is no legitimate use for this function, except possibly

 * in printk line.  Alas there is an illegitimate use in some ancient

 * drm ioctls.

/*

 * HACK ALERT!

 * These should be called implicitly, but we can't yet until all the initcall

 * stuff is done...

/* The suspend/resume methods are only for CPU without MTRR. CPU using generic

 * MTRR driver doesn't require this

/**

 * mtrr_bp_init - initialize mtrrs on the boot CPU

 *

 * This needs to be called early; before any of the other CPUs are

 * initialized (i.e. before smp_init()).

 *

		/*

		 * This is an AMD specific MSR, but we assume(hope?) that

		 * Intel will implement it too when they extend the address

		 * bus of the Xeon.

 CPUID workaround for Intel 0F33/0F34 CPU */

			/*

			 * VIA C* family have Intel style MTRRs,

			 * but don't support PAE

 Pre-Athlon (K6) AMD CPU MTRRs */

 BIOS may override */

		/*

		 * PAT initialization relies on MTRR's rendezvous handler.

		 * Skip PAT init until the handler can initialize both

		 * features independently.

	/*

	 * Ideally we should hold mtrr_mutex here to avoid mtrr entries

	 * changed, but this routine will be called in cpu boot time,

	 * holding the lock breaks it.

	 *

	 * This routine is called in two cases:

	 *

	 *   1. very early time of software resume, when there absolutely

	 *      isn't mtrr entry changes;

	 *

	 *   2. cpu hotadd time. We let mtrr_add/del_page hold cpuhotplug

	 *      lock to prevent mtrr entry changes

/**

 * mtrr_save_state - Save current fixed-range MTRR state of the first

 *	cpu in cpu_online_mask.

/*

 * Delayed MTRR initialization for all AP's

	/*

	 * Check if someone has requested the delay of AP MTRR initialization,

	 * by doing set_mtrr_aps_delayed_init(), prior to this point. If not,

	 * then we are done.

	/*

	 * The CPU has no MTRR and seems to not support SMP. They have

	 * specific drivers, we use a tricky method to support

	 * suspend/resume for them.

	 *

	 * TBD: is there any system with such CPU which supports

	 * suspend/resume? If no, we should remove the code.

 SPDX-License-Identifier: GPL-2.0

  Copyright(c) 2016-20 Intel Corporation. */

 else the tail page of the VA page list had free slots. */

 The extra page goes to SECS. */

 Set only after completion, as encl->lock has not been taken. */

/**

 * sgx_ioc_enclave_create() - handler for %SGX_IOC_ENCLAVE_CREATE

 * @encl:	An enclave pointer.

 * @arg:	The ioctl argument.

 *

 * Allocate kernel data structures for the enclave and invoke ECREATE.

 *

 * Return:

 * - 0:		Success.

 * - -EIO:	ECREATE failed.

 * - -errno:	POSIX error.

	/*

	 * TCS pages must always RW set for CPU access while the SECINFO

	 * permissions are *always* zero - the CPU ignores the user provided

	 * values and silently overwrites them with zero permissions.

 Calculate maximum of the VM flags for the page. */

	/*

	 * CPU will silently overwrite the permissions as zero, which means

	 * that we need to validate it ourselves.

 Deny noexec. */

/*

 * If the caller requires measurement of the page as a proof for the content,

 * use EEXTEND to add a measurement for 256 bytes of the page. Repeat this

 * operation until the entire page is measured."

	/*

	 * Adding to encl->va_pages must be done under encl->lock.  Ditto for

	 * deleting (via sgx_encl_shrink()) in the error path.

	/*

	 * Insert prior to EADD in case of OOM.  EADD modifies MRENCLAVE, i.e.

	 * can't be gracefully unwound, while failure on EADD/EXTEND is limited

	 * to userspace errors (or kernel/hardware bugs).

	/*

	 * Complete the "add" before doing the "extend" so that the "add"

	 * isn't in a half-baked state in the extremely unlikely scenario

	 * the enclave will be destroyed in response to EEXTEND failure.

/**

 * sgx_ioc_enclave_add_pages() - The handler for %SGX_IOC_ENCLAVE_ADD_PAGES

 * @encl:       an enclave pointer

 * @arg:	a user pointer to a struct sgx_enclave_add_pages instance

 *

 * Add one or more pages to an uninitialized enclave, and optionally extend the

 * measurement with the contents of the page. The SECINFO and measurement mask

 * are applied to all pages.

 *

 * A SECINFO for a TCS is required to always contain zero permissions because

 * CPU silently zeros them. Allowing anything else would cause a mismatch in

 * the measurement.

 *

 * mmap()'s protection bits are capped by the page permissions. For each page

 * address, the maximum protection bits are computed with the following

 * heuristics:

 *

 * 1. A regular page: PROT_R, PROT_W and PROT_X match the SECINFO permissions.

 * 2. A TCS page: PROT_R | PROT_W.

 *

 * mmap() is not allowed to surpass the minimum of the maximum protection bits

 * within the given address range.

 *

 * The function deinitializes kernel data structures for enclave and returns

 * -EIO in any of the following conditions:

 *

 * - Enclave Page Cache (EPC), the physical memory holding enclaves, has

 *   been invalidated. This will cause EADD and EEXTEND to fail.

 * - If the source address is corrupted somehow when executing EADD.

 *

 * Return:

 * - 0:		Success.

 * - -EACCES:	The source page is located in a noexec partition.

 * - -ENOMEM:	Out of EPC pages.

 * - -EINTR:	The call was interrupted before data was processed.

 * - -EIO:	Either EADD or EEXTEND failed because invalid source address

 *		or power cycle.

 * - -errno:	POSIX error.

	/*

	 * Deny initializing enclaves with attributes (namely provisioning)

	 * that have not been explicitly allowed.

	/*

	 * Attributes should not be enforced *only* against what's available on

	 * platform (done in sgx_encl_create) but checked and enforced against

	 * the mask for enforcement in sigstruct. For example an enclave could

	 * opt to sign with AVX bit in xfrm, but still be loadable on a platform

	 * without it if the sigstruct->body.attributes_mask does not turn that

	 * bit on.

	/*

	 * ENCLS[EINIT] is interruptible because it has such a high latency,

	 * e.g. 50k+ cycles on success. If an IRQ/NMI/SMI becomes pending,

	 * EINIT may fail with SGX_UNMASKED_EVENT so that the event can be

	 * serviced.

/**

 * sgx_ioc_enclave_init() - handler for %SGX_IOC_ENCLAVE_INIT

 * @encl:	an enclave pointer

 * @arg:	userspace pointer to a struct sgx_enclave_init instance

 *

 * Flush any outstanding enqueued EADD operations and perform EINIT.  The

 * Launch Enclave Public Key Hash MSRs are rewritten as necessary to match

 * the enclave's MRSIGNER, which is caculated from the provided sigstruct.

 *

 * Return:

 * - 0:		Success.

 * - -EPERM:	Invalid SIGSTRUCT.

 * - -EIO:	EINIT failed because of a power cycle.

 * - -errno:	POSIX error.

	/*

	 * 'sigstruct' must be on a page boundary and 'token' on a 512 byte

	 * boundary.  kmalloc() will give this alignment when allocating

	 * PAGE_SIZE bytes.

	/*

	 * A legacy field used with Intel signed enclaves. These used to mean

	 * regular and architectural enclaves. The CPU only accepts these values

	 * but they do not have any other meaning.

	 *

	 * Thus, reject any other values.

/**

 * sgx_ioc_enclave_provision() - handler for %SGX_IOC_ENCLAVE_PROVISION

 * @encl:	an enclave pointer

 * @arg:	userspace pointer to a struct sgx_enclave_provision instance

 *

 * Allow ATTRIBUTE.PROVISION_KEY for an enclave by providing a file handle to

 * /dev/sgx_provision.

 *

 * Return:

 * - 0:		Success.

 * - -errno:	Otherwise.

 SPDX-License-Identifier: GPL-2.0

  Copyright(c) 2016-20 Intel Corporation. */

	/*

	 * Drain the remaining mm_list entries. At this point the list contains

	 * entries for processes, which have closed the enclave file but have

	 * not exited yet. The processes, which have exited, are gone from the

	 * list by sgx_mmu_notifier_release().

 The enclave is no longer mapped by any mm. */

 'encl_mm' is gone, put encl_mm->encl reference: */

 SPDX-License-Identifier: GPL-2.0

  Copyright(c) 2016-20 Intel Corporation. */

/*

 * ELDU: Load an EPC page as unblocked. For more info, see "OS Management of EPC

 * Pages" in the SDM.

	/*

	 * Verify that the faulted page has equal or higher build time

	 * permissions than the VMA permissions (i.e. the subset of {VM_READ,

	 * VM_WRITE, VM_EXECUTE} in vma->vm_flags).

 Entry successfully located. */

	/*

	 * It's very unlikely but possible that allocating memory for the

	 * mm_list entry of a forked process failed in sgx_vma_open(). When

	 * this happens, vm_private_data is set to NULL.

	/*

	 * It's possible but unlikely that vm_private_data is NULL. This can

	 * happen in a grandchild of a process, when sgx_encl_mm_add() had

	 * failed to allocate memory in this callback.

/**

 * sgx_encl_may_map() - Check if a requested VMA mapping is allowed

 * @encl:		an enclave pointer

 * @start:		lower bound of the address range, inclusive

 * @end:		upper bound of the address range, exclusive

 * @vm_flags:		VMA flags

 *

 * Iterate through the enclave pages contained within [@start, @end) to verify

 * that the permissions requested by a subset of {VM_READ, VM_WRITE, VM_EXEC}

 * do not contain any permissions that are not contained in the build time

 * permissions of any of the enclave pages within the given address range.

 *

 * An enclave creator must declare the strongest permissions that will be

 * needed for each enclave page. This ensures that mappings have the identical

 * or weaker permissions than the earlier declared permissions.

 *

 * Return: 0 on success, -EACCES otherwise

	/*

	 * Disallow READ_IMPLIES_EXEC tasks as their VMA permissions might

	 * conflict with the enclave page permissions.

 Reschedule on every XA_CHECK_SCHED iteration. */

/*

 * Load an enclave page to EPC if required, and take encl->lock.

	/*

	 * If process was forked, VMA is still there but vm_private_data is set

	 * to NULL.

/**

 * sgx_encl_release - Destroy an enclave instance

 * @ref:	address of a kref inside &sgx_encl

 *

 * Used together with kref_put(). Frees all the resources associated with the

 * enclave and the instance itself.

			/*

			 * The page and its radix tree entry cannot be freed

			 * if the page is being held by the reclaimer.

 Detect EPC page leak's. */

/*

 * 'mm' is exiting and no longer needs mmu notifications.

	/*

	 * The enclave itself can remove encl_mm.  Note, objects can't be moved

	 * off an RCU protected list, but deletion is ok.

 'encl_mm' is going away, put encl_mm->encl reference: */

	/*

	 * Even though a single enclave may be mapped into an mm more than once,

	 * each 'mm' only appears once on encl->mm_list. This is guaranteed by

	 * holding the mm's mmap lock for write before an mm can be added or

	 * remove to an encl->mm_list.

	/*

	 * It's possible that an entry already exists in the mm_list, because it

	 * is removed only on VFS release or process exit.

 Grab a refcount for the encl_mm->encl reference: */

 Pairs with smp_rmb() in sgx_reclaimer_block(). */

/**

 * sgx_encl_get_backing() - Pin the backing storage

 * @encl:	an enclave pointer

 * @page_index:	enclave page index

 * @backing:	data for accessing backing storage for the page

 *

 * Pin the backing storage pages for storing the encrypted contents and Paging

 * Crypto MetaData (PCMD) of an enclave page.

 *

 * Return:

 *   0 on success,

 *   -errno otherwise.

/**

 * sgx_encl_put_backing() - Unpin the backing storage

 * @backing:	data for accessing backing storage for the page

 * @do_write:	mark pages dirty

/**

 * sgx_encl_test_and_clear_young() - Test and reset the accessed bit

 * @mm:		mm_struct that is checked

 * @page:	enclave page to be tested for recent access

 *

 * Checks the Access (A) bit from the PTE corresponding to the enclave page and

 * clears it.

 *

 * Return: 1 if the page has been recently accessed and 0 if not.

/**

 * sgx_alloc_va_page() - Allocate a Version Array (VA) page

 *

 * Allocate a free EPC page and convert it to a Version Array (VA) page.

 *

 * Return:

 *   a VA page,

 *   -errno otherwise

/**

 * sgx_alloc_va_slot - allocate a VA slot

 * @va_page:	a &struct sgx_va_page instance

 *

 * Allocates a slot from a &struct sgx_va_page instance.

 *

 * Return: offset of the slot inside the VA page

/**

 * sgx_free_va_slot - free a VA slot

 * @va_page:	a &struct sgx_va_page instance

 * @offset:	offset of the slot inside the VA page

 *

 * Frees a slot from a &struct sgx_va_page instance.

/**

 * sgx_va_page_full - is the VA page full?

 * @va_page:	a &struct sgx_va_page instance

 *

 * Return: true if all slots have been taken

/**

 * sgx_encl_free_epc_page - free an EPC page assigned to an enclave

 * @page:	EPC page to be freed

 *

 * Free an EPC page assigned to an enclave. It does EREMOVE for the page, and

 * only upon success, it puts the page back to free page list.  Otherwise, it

 * gives a WARNING to indicate page is leaked.

 SPDX-License-Identifier: GPL-2.0

/*

 * Device driver to expose SGX enclave memory to KVM guests.

 *

 * Copyright(c) 2021 Intel Corporation.

/*

 * Temporary SECS pages that cannot be EREMOVE'd due to having child in other

 * virtual EPC instances, and the lock to protect it.

 Calculate index of EPC page in virtual EPC's page_array */

 Don't copy VMA in fork() */

	/*

	 * Take a previously guest-owned EPC page and return it to the

	 * general EPC page pool.

	 *

	 * Guests can not be trusted to have left this page in a good

	 * state, so run EREMOVE on the page unconditionally.  In the

	 * case that a guest properly EREMOVE'd this page, a superfluous

	 * EREMOVE is harmless.

		/*

		 * Only SGX_CHILD_PRESENT is expected, which is because of

		 * EREMOVE'ing an SECS still with child, in which case it can

		 * be handled by EREMOVE'ing the SECS again after all pages in

		 * virtual EPC have been EREMOVE'd. See comments in below in

		 * sgx_vepc_release().

		 *

		 * The user of virtual EPC (KVM) needs to guarantee there's no

		 * logical processor is still running in the enclave in guest,

		 * otherwise EREMOVE will get SGX_ENCLAVE_ACT which cannot be

		 * handled here.

 The page is a SECS, userspace will retry.  */

				/*

				 * Report errors due to #GP or SGX_ENCLAVE_ACT; do not

				 * WARN, as userspace can induce said failures by

				 * calling the ioctl concurrently on multiple vEPCs or

				 * while one or more CPUs is running the enclave.  Only

				 * a #PF on EREMOVE indicates a kernel/hardware issue.

	/*

	 * Return the number of SECS pages that failed to be removed, so

	 * userspace knows that it has to retry.

		/*

		 * Remove all normal, child pages.  sgx_vepc_free_page()

		 * will fail if EREMOVE fails, but this is OK and expected on

		 * SECS pages.  Those can only be EREMOVE'd *after* all their

		 * child pages. Retries below will clean them up.

	/*

	 * Retry EREMOVE'ing pages.  This will clean up any SECS pages that

	 * only had children in this 'epc' area.

		/*

		 * An EREMOVE failure here means that the SECS page still

		 * has children.  But, since all children in this 'sgx_vepc'

		 * have been removed, the SECS page must have a child on

		 * another instance.

	/*

	 * SECS pages are "pinned" by child pages, and "unpinned" once all

	 * children have been EREMOVE'd.  A child page in this instance

	 * may have pinned an SECS page encountered in an earlier release(),

	 * creating a zombie.  Since some children were EREMOVE'd above,

	 * try to EREMOVE all zombies in the hopes that one was unpinned.

		/*

		 * Speculatively remove the page from the list of zombies,

		 * if the page is successfully EREMOVE'd it will be added to

		 * the list of free pages.  If EREMOVE fails, throw the page

		 * on the local list, which will be spliced on at the end.

 SGX virtualization requires KVM to work */

/**

 * sgx_virt_ecreate() - Run ECREATE on behalf of guest

 * @pageinfo:	Pointer to PAGEINFO structure

 * @secs:	Userspace pointer to SECS page

 * @trapnr:	trap number injected to guest in case of ECREATE error

 *

 * Run ECREATE on behalf of guest after KVM traps ECREATE for the purpose

 * of enforcing policies of guest's enclaves, and return the trap number

 * which should be injected to guest in case of any ECREATE error.

 *

 * Return:

 * -  0:	ECREATE was successful.

 * - <0:	on error.

	/*

	 * @secs is an untrusted, userspace-provided address.  It comes from

	 * KVM and is assumed to be a valid pointer which points somewhere in

	 * userspace.  This can fault and call SGX or other fault handlers when

	 * userspace mapping @secs doesn't exist.

	 *

	 * Add a WARN() to make sure @secs is already valid userspace pointer

	 * from caller (KVM), who should already have handled invalid pointer

	 * case (for instance, made by malicious guest).  All other checks,

	 * such as alignment of @secs, are deferred to ENCLS itself.

 ECREATE doesn't return an error code, it faults or succeeds. */

	/*

	 * Make sure all userspace pointers from caller (KVM) are valid.

	 * All other checks deferred to ENCLS itself.  Also see comment

	 * for @secs in sgx_virt_ecreate().

/**

 * sgx_virt_einit() - Run EINIT on behalf of guest

 * @sigstruct:		Userspace pointer to SIGSTRUCT structure

 * @token:		Userspace pointer to EINITTOKEN structure

 * @secs:		Userspace pointer to SECS page

 * @lepubkeyhash:	Pointer to guest's *virtual* SGX_LEPUBKEYHASH MSR values

 * @trapnr:		trap number injected to guest in case of EINIT error

 *

 * Run EINIT on behalf of guest after KVM traps EINIT. If SGX_LC is available

 * in host, SGX driver may rewrite the hardware values at wish, therefore KVM

 * needs to update hardware values to guest's virtual MSR values in order to

 * ensure EINIT is executed with expected hardware values.

 *

 * Return:

 * -  0:	EINIT was successful.

 * - <0:	on error.

 Propagate up the error from the WARN_ON_ONCE in __sgx_virt_einit() */

 SPDX-License-Identifier: GPL-2.0

  Copyright(c) 2016-20 Intel Corporation. */

/*

 * These variables are part of the state of the reclaimer, and must be accessed

 * with sgx_reclaimer_lock acquired.

 Nodes with one or more EPC sections. */

/*

 * Array with one list_head for each possible NUMA node.  Each

 * list contains all the sgx_epc_section's which are on that

 * node.

/*

 * Reset post-kexec EPC pages to the uninitialized state. The pages are removed

 * from the input list, and made available for the page allocator. SECS pages

 * prepending their children in the input list are left intact.

 dirty_page_list is thread-local, no need for a lock: */

			/*

			 * page is now sanitized.  Make it available via the SGX

			 * page allocator:

 The page is not yet clean - move to the dirty list. */

 Pairs with smp_rmb() in sgx_encl_mm_add(). */

	/*

	 * Can race with sgx_encl_mm_add(), but ETRACK has already been

	 * executed, which means that the CPUs running in the new mm will enter

	 * into the enclave with a fresh epoch.

/*

 * Swap page to the regular memory transformed to the blocked state by using

 * EBLOCK, which means that it can no longer be referenced (no new TLB entries).

 *

 * The first trial just tries to write the page assuming that some other thread

 * has reset the count for threads inside the enclave by using ETRACK, and

 * previous thread count has been zeroed out. The second trial calls ETRACK

 * before EWB. If that fails we kick all the HW threads out, and then do EWB,

 * which should be guaranteed the succeed.

			/*

			 * Slow path, send IPIs to kick cpus out of the

			 * enclave.  Note, it's imperative that the cpu

			 * mask is generated *after* ETRACK, else we'll

			 * miss cpus that entered the enclave between

			 * generating the mask and incrementing epoch.

/*

 * Take a fixed number of pages from the head of the active page pool and

 * reclaim them to the enclave's private shmem files. Skip the pages, which have

 * been accessed since the last scan. Move those pages to the tail of active

 * page pool so that the pages get scanned in LRU like fashion.

 *

 * Batch process a chunk of pages (at the moment 16) in order to degrade amount

 * of IPI's and ETRACK's potentially required. sgx_encl_ewb() does degrade a bit

 * among the HW threads with three stage EWB pipeline (EWB, ETRACK + EWB and IPI

 * + EWB) but not sufficiently. Reclaiming one page at a time would also be

 * problematic as it would increase the lock contention too much, which would

 * halt forward progress.

			/* The owner is freeing the page. No need to add the

			 * page back to the list of reclaimable pages.

	/*

	 * Sanitize pages in order to recover from kexec(). The 2nd pass is

	 * required for SECS pages, whose child pages blocked EREMOVE.

 sanity check: */

/**

 * __sgx_alloc_epc_page() - Allocate an EPC page

 *

 * Iterate through NUMA nodes and reserve ia free EPC page to the caller. Start

 * from the NUMA node, where the caller is executing.

 *

 * Return:

 * - an EPC page:	A borrowed EPC pages were available.

 * - NULL:		Out of EPC pages.

 Fall back to the non-local NUMA nodes: */

/**

 * sgx_mark_page_reclaimable() - Mark a page as reclaimable

 * @page:	EPC page

 *

 * Mark a page as reclaimable and add it to the active page list. Pages

 * are automatically removed from the active list when freed.

/**

 * sgx_unmark_page_reclaimable() - Remove a page from the reclaim list

 * @page:	EPC page

 *

 * Clear the reclaimable flag and remove the page from the active page list.

 *

 * Return:

 *   0 on success,

 *   -EBUSY if the page is in the process of being reclaimed

 The page is being reclaimed. */

/**

 * sgx_alloc_epc_page() - Allocate an EPC page

 * @owner:	the owner of the EPC page

 * @reclaim:	reclaim pages if necessary

 *

 * Iterate through EPC sections and borrow a free EPC page to the caller. When a

 * page is no longer needed it must be released with sgx_free_epc_page(). If

 * @reclaim is set to true, directly reclaim pages when we are out of pages. No

 * mm's can be locked when @reclaim is set to true.

 *

 * Finally, wake up ksgxd when the number of pages goes below the watermark

 * before returning back to the caller.

 *

 * Return:

 *   an EPC page,

 *   -errno on error

/**

 * sgx_free_epc_page() - Free an EPC page

 * @page:	an EPC page

 *

 * Put the EPC page back to the list of free pages. It's the caller's

 * responsibility to make sure that the page is in uninitialized state. In other

 * words, do EREMOVE, EWB or whatever operation is necessary before calling

 * this function.

/**

 * A section metric is concatenated in a way that @low bits 12-31 define the

 * bits 12-31 of the metric and @high bits 0-19 define the bits 32-51 of the

 * metric.

 The physical address is already printed above. */

/*

 * Update the SGX_LEPUBKEYHASH MSRs to the values specified by caller.

 * Bare-metal driver requires to update them to hash of enclave's signer

 * before EINIT. KVM needs to update them to guest's virtual MSR values

 * before doing EINIT from guest.

/**

 * sgx_set_attribute() - Update allowed attributes given file descriptor

 * @allowed_attributes:		Pointer to allowed enclave attributes

 * @attribute_fd:		File descriptor for specific attribute

 *

 * Append enclave attribute indicated by file descriptor to allowed

 * attributes. Currently only SGX_ATTR_PROVISIONKEY indicated by

 * /dev/sgx_provision is supported.

 *

 * Return:

 * -0:		SGX_ATTR_PROVISIONKEY is appended to allowed_attributes

 * -EINVAL:	Invalid, or not supported file descriptor

	/*

	 * Always try to initialize the native *and* KVM drivers.

	 * The KVM driver is less picky than the native one and

	 * can function if the native one is not supported on the

	 * current system or fails to initialize.

	 *

	 * Error out only if both fail to initialize.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Machine check injection support.

 * Copyright 2008 Intel Corporation.

 *

 * Authors:

 * Andi Kleen

 * Ying Huang

 *

 * The AMD part (from mce_amd_inj.c): a simple MCE injection facility

 * for testing different aspects of the RAS code. This driver should be

 * built as module so that it can be loaded on production kernels for

 * testing purposes.

 *

 * Copyright (c) 2010-17:  Borislav Petkov <bp@alien8.de>

 *			   Advanced Micro Devices Inc.

/*

 * Collect all the MCi_XXX settings

 SW injection, simply decode the error */

 Trigger a #MC */

 Trigger Deferred error interrupt */

 Trigger threshold interrupt */

 Set default injection to SW_INJ */

 Update fake mce registers on current CPU. */

 Make sure no one reads partially written injectm */

 First set the fields after finished */

 Now write record in order, finished last (except above) */

 Finally activate it */

 do_machine_check() expects interrupts disabled -- at least */

 Inject mce on current CPU */

			/*

			 * Could do more to fake interrupts like

			 * calling irq_enter, but the necessary

			 * machinery isn't exported currently.

				/*

				 * don't wait because mce_irq_ipi is necessary

				 * to be sync with following raise_local

/*

 * Caller needs to be make sure this cpu doesn't disappear

 * from under us, i.e.: get_cpu/put_cpu.

 strip whitespace */

/*

 * On which CPU to inject?

 prep MCE global settings for the injection */

	/*

	 * Ensure necessary status bits for deferred errors:

	 * - MCx_STATUS[Deferred]: make sure it is a deferred error

	 * - MCx_STATUS[UC] cleared: deferred errors are _not_ UC

	/*

	 * For multi node CPUs, logging and reporting of bank 4 errors happens

	 * only on the node base core. Refer to D18F3x44[NbMcaToMstCpuEn] for

	 * Fam10h and later BKDGs.

/*

 * This denotes into which bank we're injecting and triggers

 * the injection, at the same time.

 Get bank count on target CPU so we can handle non-uniform values. */

 Reset injection struct */

 SPDX-License-Identifier: GPL-2.0

/*

 * Common corrected MCE threshold handler code:

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MCE grading rules.

 * Copyright 2008, 2009 Intel Corporation.

 *

 * Author: Andi Kleen

/*

 * Grade an mce by severity. In general the most severe ones are processed

 * first. Since there are quite a lot of combinations test the bits in a

 * table-driven way. The rules are simply processed in order, first

 * match wins.

 *

 * Note this is only used for machine check exceptions, the corrected

 * errors use much simpler rules. The exceptions still check for the corrected

 * errors, but only to leave them alone for the CMCI handler (except for

 * panic situations)

 When MCIP is not set something is very confused */

 Neither return not error IP -- no chance to recover -> PANIC */

	/*

	 * known AO MCACODs reported via MCE or CMC:

	 *

	 * SRAO could be signaled either via a machine check exception or

	 * CMCI with the corresponding bit S 1 or 0. So we don't need to

	 * check bit S for SRAO.

	/*

	 * Quirk for Skylake/Cascade Lake. Patrol scrubber may be configured

	 * to report uncorrected errors using CMCI with a special signature.

	 * UC=0, MSCOD=0x0010, MCACOD=binary(000X 0000 1100 XXXX) reported

	 * in one of the memory controller banks.

	 * Set severity to "AO" for same action as normal patrol scrub error.

 ignore OVER for UCNA */

 known AR MCACODs: */

 always matches. keep at end */

 MOV mem,reg */

 MOVZ mem,reg */

 REP MOVS */

/*

 * If mcgstatus indicated that ip/cs on the stack were

 * no good, then "m->cs" will be zero and we will have

 * to assume the worst case (IN_KERNEL) as we actually

 * have no idea what we were executing when the machine

 * check hit.

 * If we do have a good "m->cs" (or a faked one in the

 * case we were executing in VM86 mode) we can use it to

 * distinguish an exception taken in user from from one

 * taken in the kernel.

	/*

	 * We need to look at the following bits:

	 * - "succor" bit (data poisoning support), and

	 * - TCC bit (Task Context Corrupt)

	 * in MCi_STATUS to determine error severity.

 TCC (Task context corrupt). If set and if IN_KERNEL, panic. */

 ...otherwise invoke hwpoison handler. */

/*

 * See AMD Error Scope Hierarchy table in a newer BKDG. For example

 * 49125_15h_Models_30h-3Fh_BKDG.pdf, section "RAS Features"

 Processor Context Corrupt, no need to fumble too much, die! */

		/*

		 * On older systems where overflow_recov flag is not present, we

		 * should simply panic if an error overflow occurs. If

		 * overflow_recov flag is present and set, then software can try

		 * to at least kill process to prolong system operation.

 kill current process */

 at least one error was not logged */

		/*

		 * For any other case, return MCE_UC_SEVERITY so that we log the

		 * error and exit #MC handler.

	/*

	 * deferred error: poll handler catches these and adds to mce_ring so

	 * memory-failure can take recovery actions.

	/*

	 * corrected error: poll handler catches these and passes responsibility

	 * of decoding the error to EDAC

 CONFIG_DEBUG_FS */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * /dev/mcelog driver

 *

 * K8 parts Copyright 2002,2003 Andi Kleen, SuSE Labs.

 * Rest from unknown author(s).

 * 2004 Andi Kleen. Rewrote most of it.

 * Copyright 2008 Intel Corporation

 * Author: Andi Kleen

/*

 * Lockless MCE logging infrastructure.

 * This avoids deadlocks on printk locks without having to break locks. Also

 * separate MCEs from kernel messages to avoid bogus bug reports.

	/*

	 * When the buffer fills up discard new entries. Assume that the

	 * earlier errors are the more interesting ones:

 wake processes polling /dev/mcelog */

/*

 * mce_chrdev: Character device /dev/mcelog to read and clear the MCE log.

 #times opened */

 already open exclusive? */

 Collect MCE record of previous boot in persistent storage via APEI ERST. */

 Error or no more MCE record */

		/*

		 * When ERST is disabled, mce_chrdev_read() should return

		 * "no record" instead of "no device."

	/*

	 * In fact, we should have cleared the record after that has

	 * been flushed to the disk or sent to network in

	 * /sbin/mcelog, but we have no interface to support that now,

	 * so just clear it to avoid duplication.

 Only supports full reads right now */

	/*

	 * There are some cases where real MSR reads could slip

	 * through.

	/*

	 * Need to give user space some time to set everything up,

	 * so do it a jiffie or two later everywhere.

 register character device /dev/mcelog */

 Xen dom0 might have registered the device already. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Bridge between MCE and APEI

 *

 * On some machine, corrected memory errors are reported via APEI

 * generic hardware error source (GHES) instead of corrected Machine

 * Check. These corrected memory errors can be reported to user space

 * through /dev/mcelog via faking a corrected Machine Check, so that

 * the error memory page can be offlined by /sbin/mcelog if the error

 * count for one page is beyond the threshold.

 *

 * For fatal MCE, save MCE record into persistent storage via ERST, so

 * that the MCE record can be logged after reboot via ERST.

 *

 * Copyright 2010 Intel Corp.

 *   Author: Huang Ying <ying.huang@intel.com>

 Fake a memory read error with unknown channel */

	/*

	 * The starting address of the register array extracted from BERT must

	 * match with the first expected register in the register layout of

	 * SMCA address space. This address corresponds to banks's MCA_STATUS

	 * register.

	 *

	 * Match any MCi_STATUS register by turning off bank numbers.

	/*

	 * The register array size must be large enough to include all the

	 * SMCA registers which need to be extracted.

	 *

	 * The number of registers in the register array is determined by

	 * Register Array Size/8 as defined in UEFI spec v2.8, sec N.2.4.2.2.

	 * The register layout is fixed and currently the raw data in the

	 * register array includes 6 SMCA registers which the kernel can

	 * extract.

 Skipping MCA_CONFIG */

/*

 * CPER specification (in UEFI specification 2.3 appendix N) requires

 * byte-packed.

 timestamp, platform_id, partition_id are all invalid */

 fru_id and fru_text is invalid */

 no more record */

 someone else has cleared the record, try next one */

 try to skip other type records in storage */

 Check whether there is record in ERST */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MCE event pool management in MCE context

 *

 * Copyright (C) 2015 Intel Corp.

 * Author: Chen, Gong <gong.chen@linux.intel.com>

/*

 * printk() is not safe in MCE context. This is a lock-less memory allocator

 * used to save error information organized in a lock-less list.

 *

 * This memory pool is only to be used to save MCE records in MCE context.

 * MCE events are rare, so a fixed size memory pool should be enough. Use

 * 2 pages to save MCE events for now (~80 MCE records at most).

/*

 * Compare the record "t" with each of the records on list "l" to see if

 * an equivalent one is present in the list.

/*

 * The system has panicked - we'd like to peruse the list of MCE records

 * that have been queued, but not seen by anyone yet.  The list is in

 * reverse time order, so we need to reverse it. While doing that we can

 * also drop duplicate records (these were logged because some banks are

 * shared between cores or by all threads on a socket).

 squeeze out duplicates while reversing order */

 Just init mce_gen_pool once. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  (c) 2005-2016 Advanced Micro Devices, Inc.

 *

 *  Written by Jacob Shin - AMD, Inc.

 *  Maintained by: Borislav Petkov <bp@alien8.de>

 *

 *  All MC4_MISCi registers are shared between cores on a node.

 Deferred error settings */

 Scalable MCA: */

 Threshold LVT offset is at MSR0xC0000410[15:12] */

 Short name for sysfs */

 Long name for pretty-printing */

 UMC v2 is separate because both of them can exist in a single system. */

 { bank_type, hwid_mcatype } */

 Reserved type */

 ZN Core (HWID=0xB0) MCA types */

 HWID 0xB0 MCATYPE 0x4 is Reserved */

 Data Fabric MCA types */

 Unified Memory Controller MCA type */

 Parameter Block MCA type */

 Platform Security Processor MCA type */

 System Management Unit MCA type */

 Microprocessor 5 Unit MCA type */

 Northbridge IO Unit MCA type */

 PCI Express Unit MCA type */

 xGMI PCS MCA type */

 xGMI PHY MCA type */

 WAFL PHY MCA type */

/*

 * In SMCA enabled processors, we can have multiple banks for a given IP type.

 * So to define a unique name for each bank, we use a temp c-string to append

 * the MCA_IPID[InstanceId] to type's name in get_name().

 *

 * InstanceId is 32 bits which is 8 characters. Make sure MAX_MCATYPE_NAME_LEN

 * is greater than 8 plus 1 (for underscore) plus length of longest type name.

/*

 * A list of the banks enabled on each logical CPU. Controls which respective

 * descriptors to initialize later in mce_threshold_create_device().

 Map of banks that have more than MCA_MISC0 available. */

	/*

	 * For SMCA enabled processors, BLKPTR field of the first MISC register

	 * (MCx_MISC0) indicates presence of additional MISC regs set (MISC1-4).

 Set appropriate bits in MCA_CONFIG */

		/*

		 * OS is required to set the MCAX bit to acknowledge that it is

		 * now using the new MSR ranges and new registers under each

		 * bank. It also means that the OS will configure deferred

		 * errors in the new MCx_CONFIG register. If the bit is not set,

		 * uncorrectable errors will cause a system panic.

		 *

		 * MCA_CONFIG[MCAX] is bit 32 (0 in the high portion of the MSR.)

		/*

		 * SMCA sets the Deferred Error Interrupt type per bank.

		 *

		 * MCA_CONFIG[DeferredIntTypeSupported] is bit 5, and tells us

		 * if the DeferredIntType bit field is available.

		 *

		 * MCA_CONFIG[DeferredIntType] is bits [38:37] ([6:5] in the

		 * high portion of the MSR). OS should set this to 0x1 to enable

		 * APIC based interrupt. First, check that no interrupt has been

		 * set.

 Return early if this bank was already initialized. */

	/*

	 * Scalable MCA provides for only one core to have access to the MSRs of

	 * a shared bank.

 Bank 4 is for northbridge reporting and is thus shared */

 MSR4_MISC0 */

	/*

	 * bank 4 supports APIC LVT interrupts implicitly since forever.

	/*

	 * IntP: interrupt present; if this bit is set, the thresholding

	 * bank can generate APIC LVT interrupts

		/*

		 * On SMCA CPUs, LVT offset is programmed at a different MSR, and

		 * the BIOS provides the value. The original field where LVT offset

		 * was set is reserved. Return early here:

 Reprogram MCx_MISC MSR behind this threshold bank. */

 sysfs write might race against an offline operation */

 limit cannot be lower than err count */

 reset err count and overflow bit */

 change limit w/o reset */

 clear IntType */

 set new lvt offset */

 Fall back to method we used for older processors: */

 Gather LVT offset for thresholding: */

 See Family 17h Models 10h-2Fh Erratum #1114. */

 NB GART TLB error reporting is disabled by default. */

/*

 * Turn off thresholding banks for the following conditions:

 * - MC4_MISC thresholding is not supported on Family 0x15.

 * - Prevent possible spurious interrupts from the IF bank on Family 0x17

 *   Models 0x10-0x2F due to Erratum #1114.

 MC4_MISC0 */

 MC4_MISC1 */

 McStatusWrEn has to be set */

 Clear CntP bit safely */

 restore old settings */

 cpu init entry point, called from mce.c with preempt off */

 We start from the normalized address */

 Read D18F0x1B4 (DramOffset), check if base 1 is used. */

 Remove HiAddrOffset from normalized address, if enabled: */

 Read D18F0x110 (DramBaseAddress). */

 Check if address range is valid. */

 {0, 1, 2, 3} map to address bits {8, 9, 10, 11} respectively */

 Read D18F0x114 (DramLimitAddress). */

 Re-use intlv_num_chan by setting it equal to log2(#channels) */

 Add a bit if sockets are interleaved. */

 Assert num_intlv_bits <= 4 */

		/*

		 * Read FabricBlockInstanceInformation3_CS[BlockFabricID].

		 * This is the fabric id for this coherent slave. Use

		 * umc/channel# as instance id of the coherent slave

		 * for FICAA.

 If interleaved over more than 1 channel: */

 Read D18F1x208 (SystemFabricIdMask). */

 If interleaved over more than 1 die. */

 If interleaved over more than 1 socket. */

		/*

		 * The pre-interleaved address consists of XXXXXXIIIYYYYY

		 * where III is the ID for this CS, and XXXXXXYYYYY are the

		 * address bits from the post-interleaved address.

		 * "num_intlv_bits" has been calculated to tell us how many "I"

		 * bits there are. "intlv_addr_bit" tells us how many "Y" bits

		 * there are (where "I" starts).

 Add dram base address */

 If legacy MMIO hole enabled */

 Save some parentheses and grab ls-bit at the end. */

 Is calculated system address is above DRAM limit address? */

 ErrCodeExt[20:16] */

		/*

		 * Extract [55:<lsb>] where lsb is the least significant

		 * *valid* bit of the address bits.

/*

 * Returns true if the logged error is deferred. False, otherwise.

/*

 * We have three scenarios for checking for Deferred errors:

 *

 * 1) Non-SMCA systems check MCA_STATUS and log error if found.

 * 2) SMCA systems check MCA_STATUS. If error is found then log it and also

 *    clear MCA_DESTAT.

 * 3) SMCA systems check MCA_DESTAT, if error was not found in MCA_STATUS, and

 *    log it.

 Clear MCA_DESTAT if we logged the deferred error from MCA_STATUS. */

	/*

	 * Only deferred errors are logged in MCA_DE{STAT,ADDR} so just check

	 * for a valid error.

 APIC interrupt handler for deferred errors */

 Log the MCE which caused the threshold event. */

 Reset threshold block after logging error. */

/*

 * Threshold interrupt handler will service THRESHOLD_APIC_VECTOR. The interrupt

 * goes off when error_count reaches threshold_limit.

	/*

	 * Validate that the threshold bank has been initialized already. The

	 * handler is installed at boot time, but on a hotplug event the

	 * interrupt might fire before the data has been initialized.

		/*

		 * The first block is also the head of the list. Check it first

		 * before iterating over the rest.

/*

 * Sysfs Interface

 CPU might be offline by now */

 possibly interrupt_enable if supported, see below */

 This is safe as @tb is not visible yet */

 threshold descriptor already initialized on this node? */

 yes, use it */

 Associate the bank with the per-CPU MCE device */

 nb is already initialized, see above */

		/*

		 * The last CPU on this node using the shared bank is going

		 * away, remove that bank now.

	/*

	 * Clear the pointer before cleaning up, so that the interrupt won't

	 * touch anything of this.

/**

 * mce_threshold_create_device - Create the per-CPU MCE threshold device

 * @cpu:	The plugged in CPU

 *

 * Create directories and files for all valid threshold banks.

 *

 * This is invoked from the CPU hotplug callback which was installed in

 * mcheck_init_device(). The invocation happens in context of the hotplug

 * thread running on @cpu.  The callback is invoked on all CPUs which are

 * online when the callback is installed or during a real hotplug event.

 SPDX-License-Identifier: GPL-2.0

/*

 * IDT Winchip specific Machine Check Exception Reporting

 * (C) Copyright 2002 Alan Cox <alan@lxorguk.ukuu.org.uk>

 Machine check handler for WinChip C6: */

 Set up machine check reporting on the Winchip C6 series */

 Enable EIERRINT (int 18 MCE) */

 Enable MCE */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Machine check handler.

 *

 * K8 parts Copyright 2002,2003 Andi Kleen, SuSE Labs.

 * Rest from unknown author(s).

 * 2004 Andi Kleen. Rewrote most of it.

 * Copyright 2008 Intel Corporation

 * Author: Andi Kleen

 sysfs synchronization */

 100ns */

 subevents to enable */

 initialise bank? */

 One object for each MCE bank, shared by all CPUs */

 device attribute */

 attribute name */

 bank number */

	/*

	 * Tolerant levels:

	 * 0: always panic on uncorrected errors, log corrected errors

	 * 1: panic or SIGBUS on uncorrected errors, log corrected errors

	 * 2: SIGBUS or log uncorrected errors (if possible), log corr. errors

	 * 3: never panic or SIGBUS, log all errors (for testing only)

/*

 * MCA banks polled by the period polling timer for corrected events.

 * With Intel CMCI, this only has MCA banks which do not support CMCI (if any).

/*

 * MCA banks controlled through firmware first for corrected errors.

 * This is a global list of banks for which we won't enable CMCI and we

 * won't poll. Firmware controls these banks and is responsible for

 * reporting corrected errors through GHES. Uncorrected/recoverable

 * errors are still notified through a machine check.

/*

 * CPU/chipset specific EDAC code can register a notifier call here to print

 * MCE errors in a human-readable form.

 Do initial initialization of a struct mce */

 need the internal __ version to avoid deadlocks */

	/*

	 * Note this output is parsed by external tools and old fields

	 * should not be changed.

 5 seconds */

 Panic in progress. Enable interrupts and wait for final IPI */

		/*

		 * Make sure only one CPU runs in machine check panic

 Don't log too much for fake panic */

 First print corrected ones that are still unlogged */

 Now print uncorrected but with the final one last */

 Support code for software error injection */

 MSR access wrappers used for error injection */

	/*

	 * RDMSR on MCA MSRs should not fault. If they do, this is very much an

	 * architectural violation and needs to be reported to hw vendor. Panic

	 * the box to not allow any further progress.

 See comment in mce_rdmsrl() */

/*

 * Collect all global (w.r.t. this processor) status about this machine

 * check into our "mce" struct so that we can use it later to assess

 * the severity of the problem as we read per-bank specific details.

		/*

		 * Get the address of the instruction at the time of

		 * the machine check error.

			/*

			 * When in VM86 mode make the cs look like ring 3

			 * always. This is a lie, but it's better than passing

			 * the additional vm86 bit around everywhere.

 Use accurate RIP reporting if available. */

/*

 * Check if the address reported by the CPU is in a format we can parse.

 * It would be possible to add code for most other cases, but all would

 * be somewhat complicated (e.g. segment offset would require an instruction

 * parser). So only support physical addresses up to page granularity for now.

 Checks after this one are Intel/Zhaoxin-specific: */

		/*

		 * Intel SDM Volume 3B - 15.9.2 Compound Error Codes

		 *

		 * Bit 7 of the MCACOD field of IA32_MCi_STATUS is used for

		 * indicating a memory error. Bit 8 is used for indicating a

		 * cache hierarchy error. The combination of bit 2 and bit 3

		 * is used for indicating a `generic' cache hierarchy error

		 * But we can't just blindly check the above bits, because if

		 * bit 11 is set, then it is a bus/interconnect error - and

		 * either way the above bits just gives more detail on what

		 * bus/interconnect error happened. Note that bit 12 can be

		 * ignored, as it's the "filter" bit.

 Emit the trace record: */

 lowest prio, we want it to run last. */

/*

 * Read ADDR and MISC registers.

		/*

		 * Mask the reported address by the reported granularity.

		/*

		 * Extract [55:<lsb>] where lsb is the least significant

		 * *valid* bit of the address bits.

/*

 * Poll for corrected events or events that happened before reset.

 * Those are just logged through /dev/mcelog.

 *

 * This is executed in standard interrupt context.

 *

 * Note: spec recommends to panic for fatal unsignalled

 * errors here. However this would be quite problematic --

 * we would need to reimplement the Monarch handling and

 * it would mess up the exclusion between exception handler

 * and poll handler -- * so we skip this for now.

 * These cases should not happen anyways, or only when the CPU

 * is already totally * confused. In this case it's likely it will

 * not fully execute the machine check handler either.

 If this entry is not valid, ignore it */

		/*

		 * If we are logging everything (at CPU online) or this

		 * is a corrected error, then we must log it.

		/*

		 * Newer Intel systems that support software error

		 * recovery need to make additional checks. Other

		 * CPUs should skip over uncorrected errors, but log

		 * everything else.

 Log "not enabled" (speculative) errors */

		/*

		 * Log UCNA (SDM: 15.6.3 "UCR Error Classification")

		 * UC == 1 && PCC == 0 && S == 0

		/*

		 * Skip anything else. Presumption is that our read of this

		 * bank is racing with a machine check. Leave the log alone

		 * for do_machine_check() to deal with it.

		/*

		 * Don't get the IP here because it's unlikely to

		 * have anything to do with the actual error location.

		/*

		 * Clear state for this bank.

	/*

	 * Don't clear MCG_STATUS here because it's only defined for

	 * exceptions.

/*

 * During IFU recovery Sandy Bridge -EP4S processors set the RIPV and

 * EIPV bits in MCG_STATUS to zero on the affected logical processor (SDM

 * Vol 3B Table 15-20). But this confuses both the code that determines

 * whether the machine check occurred in kernel or user mode, and also

 * the severity assessment code. Pretend that EIPV was set, and take the

 * ip/cs values from the pt_regs that mce_gather_info() ignored earlier.

/*

 * Do a quick check if any of the events requires a panic.

 * This decides if we keep the events around or clear them.

/*

 * Variable to establish order between CPUs while scanning.

 * Each CPU spins initially until executing is equal its number.

/*

 * Defines order of CPUs on entry. First CPU becomes Monarch.

/*

 * Track which CPUs entered the MCA broadcast synchronization and which not in

 * order to print holdouts.

/*

 * Check if a timeout waiting for other CPUs happened.

	/*

	 * The others already did panic for some reason.

	 * Bail out like in a timeout.

	 * rmb() to tell the compiler that system_state

	 * might have been modified by someone else.

/*

 * The Monarch's reign.  The Monarch is the CPU who entered

 * the machine check handler first. It waits for the others to

 * raise the exception too and then grades them. When any

 * error is fatal panic. Only then let the others continue.

 *

 * The other CPUs entering the MCE handler will be controlled by the

 * Monarch. They are called Subjects.

 *

 * This way we prevent any potential data corruption in a unrecoverable case

 * and also makes sure always all CPU's errors are examined.

 *

 * Also this detects the case of a machine check event coming from outer

 * space (not detected by any CPUs) In this case some external agent wants

 * us to shut down, so panic too.

 *

 * The other CPUs might still decide to panic if the handler happens

 * in a unrecoverable place, but in this case the system is in a semi-stable

 * state and won't corrupt anything by itself. It's ok to let the others

 * continue for a bit first.

 *

 * All the spin loops have timeouts; when a timeout happens a CPU

 * typically elects itself to be Monarch.

	/*

	 * This CPU is the Monarch and the other CPUs have run

	 * through their handlers.

	 * Grade the severity of the errors of all the CPUs.

	/*

	 * Cannot recover? Panic here then.

	 * This dumps all the mces in the log buffer and stops the

	 * other CPUs.

 call mce_severity() to get "msg" for panic */

	/*

	 * For UC somewhere we let the CPU who detects it handle it.

	 * Also must let continue the others, otherwise the handling

	 * CPU could deadlock on a lock.

	/*

	 * No machine check event found. Must be some external

	 * source or one CPU is hung. Panic.

	/*

	 * Now clear all the mces_seen so that they don't reappear on

	 * the next mce.

/*

 * Start of Monarch synchronization. This waits until all CPUs have

 * entered the exception handler and then determines if any of them

 * saw a fatal event that requires panic. Then it executes them

 * in the entry order.

 * TBD double check parallel CPU hotunplug

	/*

	 * Rely on the implied barrier below, such that global_nwo

	 * is updated before mce_callin.

	/*

	 * Wait for everyone.

	/*

	 * mce_callin should be read before global_nwo

		/*

		 * Monarch: Starts executing now, the others wait.

		/*

		 * Subject: Now start the scanning loop one by one in

		 * the original callin order.

		 * This way when there are any shared banks it will be

		 * only seen by one CPU before cleared, avoiding duplicates.

	/*

	 * Cache the global no_way_out state.

/*

 * Synchronize between CPUs after main scanning loop.

 * This invokes the bulk of the Monarch processing.

	/*

	 * Allow others to run.

 CHECKME: Can this race with a parallel hotplug? */

		/*

		 * Monarch: Wait for everyone to go through their scanning

		 * loops.

		/*

		 * Subject: Wait for Monarch to finish.

		/*

		 * Don't reset anything. That's done by the Monarch.

	/*

	 * Reset all global state.

	/*

	 * Let others run again.

/*

 * Cases where we avoid rendezvous handler timeout:

 * 1) If this CPU is offline.

 *

 * 2) If crashing_cpu was set, e.g. we're entering kdump and we need to

 *  skip those CPUs which remain looping in the 1st kernel - see

 *  crash_nmi_callback().

 *

 * Note: there still is a small window between kexec-ing and the new,

 * kdump kernel establishing a new #MC handler where a broadcasted MCE

 * might not get handled properly.

		/*

		 * Corrected or non-signaled errors are handled by

		 * machine_check_poll(). Leave them alone, unless this panics.

 Set taint even when machine check was not enabled. */

		/*

		 * When machine check was for corrected/deferred handler don't

		 * touch, unless we're panicking.

 Machine check event was not enabled. Clear, but ignore. */

 assuming valid severity level != 0 */

 mce_clear_state will clear *final, save locally for use later */

	/*

	 * -EHWPOISON from memory_failure() means that it already sent SIGBUS

	 * to the current process with the proper error info, so no need to

	 * send SIGBUS here again.

 First call, save all the details */

 Ten is likely overkill. Don't expect more than two faults before task_work() */

 Second or later call, make sure page address matches the one from first call */

 Do not call task_work_add() more than once */

 Handle unconfigured int18 (should never happen) */

/*

 * The actual machine check handler. This only handles real

 * exceptions when something got corrupted coming in through int 18.

 *

 * This is executed in NMI context not subject to normal locking rules. This

 * implies that most kernel services cannot be safely used. Don't even

 * think about putting a printk in there!

 *

 * On Intel systems this is entered on all CPUs in parallel through

 * MCE broadcast. However some CPUs might be broken beyond repair,

 * so be always careful when synchronizing with others.

 *

 * Tracing and kprobes are disabled: if we interrupted a kernel context

 * with IF=1, we need to minimize stack usage.  There are also recursion

 * issues: if the machine check was due to a failure of the memory

 * backing the user stack, tracing that reads the user stack will cause

 * potentially infinite recursion.

	/*

	 * Establish sequential order between the CPUs entering the machine

	 * check handler.

	/*

	 * If no_way_out gets set, there is no safe way to recover from this

	 * MCE.  If mca_cfg.tolerant is cranked up, we'll try anyway.

	/*

	 * If kill_current_task is not set, there might be a way to recover from this

	 * error.

	/*

	 * MCEs are always local on AMD. Same is determined by MCG_STATUS_LMCES

	 * on Intel.

	/*

	 * When no restart IP might need to kill or panic.

	 * Assume the worst for now, but if we find the

	 * severity is MCE_AR_SEVERITY we have other options.

	/*

	 * Check if this MCE is signaled to only this logical processor,

	 * on Intel, Zhaoxin only.

	/*

	 * Local machine check may already know that we have to panic.

	 * Broadcast machine check begins rendezvous in mce_start()

	 * Go through all banks in exclusion of the other CPUs. This way we

	 * don't report duplicated events on shared banks because the first one

	 * to see it will clear it.

	/*

	 * Do most of the synchronization with other CPUs.

	 * When there's any problem use only local no_way_out state.

		/*

		 * If there was a fatal machine check we should have

		 * already called mce_panic earlier in this function.

		 * Since we re-read the banks, we might have found

		 * something new. Check again to see if we found a

		 * fatal error. We call "mce_severity()" again to

		 * make sure we have the right "msg".

 Fault was in user mode and we need to take some action */

 If this triggers there is no way to recover. Die hard. */

		/*

		 * Handle an MCE which has happened in kernel space but from

		 * which the kernel can recover: ex_has_fault_handler() has

		 * already verified that the rIP at which the error happened is

		 * a rIP from which the kernel can recover (by jumping to

		 * recovery code specified in _ASM_EXTABLE_FAULT()) and the

		 * corresponding exception handler which would do that is the

		 * proper one.

 mce_severity() should not hand us an ACTION_REQUIRED error */

/*

 * Periodic polling timer for "silent" machine check errors.  If the

 * poller finds an MCE, poll 2x faster.  When the poller finds no more

 * errors, poll 2x slower (up to check_interval seconds).

 in jiffies */

	/*

	 * Alert userspace if needed. If we logged an MCE, reduce the polling

	 * interval, otherwise increase the polling interval.

/*

 * Ensure that the timer is firing in @interval from now.

 Must not be called in IRQ context where del_timer_sync() can deadlock */

/*

 * Notify the user(s) about new machine check events.

 * Can be called from interrupt context, but not from machine check/NMI

 * context.

 Not more than two messages every minute */

		/*

		 * Init them all, __mcheck_cpu_apply_quirks() is going to apply

		 * the required vendor quirks before

		 * __mcheck_cpu_init_clear_banks() does the final bank setup.

/*

 * Initialize Machine Checks for a CPU.

 Use accurate RIP reporting if available. */

	/*

	 * Log the machine checks left over from the previous reset. Log them

	 * only, do not start processing them. That will happen in mcheck_late_init()

	 * when all consumers have been registered on the notifier chain.

/*

 * Do a final check to see if there are any unused/RAZ banks.

 *

 * This must be done after the banks have been initialized and any quirks have

 * been applied.

 *

 * Do not call this from any user-initiated flows, e.g. CPU hotplug or sysfs.

 * Otherwise, a user who disables a bank will not be able to re-enable it

 * without a system reboot.

 Add per CPU specific workarounds here */

 This should be disabled by the BIOS, but isn't always */

			/*

			 * disable GART TBL walk error reporting, which

			 * trips off incorrectly with the IOMMU & 3ware

			 * & Cerberus:

			/*

			 * Lots of broken BIOS around that don't clear them

			 * by default and leave crap in there. Don't log:

		/*

		 * Various K7s with broken bank 0 around. Always disable

		 * by default.

		/*

		 * overflow_recov is supported for F15h Models 00h-0fh

		 * even though we don't have a CPUID bit for it.

		/*

		 * SDM documents that on family 6 bank 0 should not be written

		 * because it aliases to another special BIOS controlled

		 * register.

		 * But it's not aliased anymore on model 0x1a+

		 * Don't ignore bank 0 completely because there could be a

		 * valid event later, merely don't write CTL0.

		/*

		 * All newer Intel systems support MCE broadcasting. Enable

		 * synchronization with a one second timeout.

		/*

		 * There are also broken BIOSes on some Pentium M and

		 * earlier systems:

		/*

		 * All newer Zhaoxin CPUs support MCE broadcasting. Enable

		 * synchronization with a one second timeout.

/*

 * Init basic CPU features needed for early decoding of MCEs.

	 /*

	  * All newer Centaur CPUs support MCE broadcasting. Enable

	  * synchronization with a one second timeout.

	/*

	 * These CPUs have MCA bank 8 which reports only one error type called

	 * SVAD (System View Address Decoder). The reporting of that error is

	 * controlled by IA32_MC8.CTL.0.

	 *

	 * If enabled, prefetching on these CPUs will cause SVAD MCE when

	 * virtual machines start and result in a system  panic. Always disable

	 * bank 8 SVAD error by default.

	/*

	 * Only required when from kernel mode. See

	 * mce_check_crashing_cpu() for details.

 MCE hit kernel mode */

 The user mode variant. */

 32bit unified entry point */

/*

 * Called for each booted CPU to set up machine checks.

 * Must be called with preempt off:

/*

 * Called for each booted CPU to clear some machine checks opt-ins

	/*

	 * Possibly to clear general settings generic to x86

	 * __mcheck_cpu_clear_generic(c);

/*

 * mce=off Disables machine check

 * mce=no_cmci Disables CMCI

 * mce=no_lmce Disables LMCE

 * mce=dont_log_ce Clears corrected events silently, no log created for CEs.

 * mce=print_all Print all machine check logs to console

 * mce=ignore_ce Disables polling and CMCI, corrected events are not cleared.

 * mce=TOLERANCELEVEL[,monarchtimeout] (number, see above)

 *	monarchtimeout is how long to wait for other CPUs on machine

 *	check, or 0 to not wait

 * mce=bootlog Log MCEs from before booting. Disabled by default on AMD Fam10h

	and older.

 * mce=nobootlog Don't log MCEs from before booting.

 * mce=bios_cmci_threshold Don't program the CMCI threshold

 * mce=recovery force enable copy_mc_fragile()

/*

 * mce_syscore: PM support

/*

 * Disable machine checks on suspend and shutdown. We can't really handle

 * them later.

	/*

	 * Don't clear on Intel or AMD or Hygon or Zhaoxin CPUs. Some of these

	 * MSRs are socket-wide. Disabling them for just a single offlined CPU

	 * is bad, since it will inhibit reporting for all shared resources on

	 * the socket like the last level cache (LLC), the integrated memory

	 * controller (iMC), etc.

/*

 * On resume clear all MCE state. Don't want to see leftovers from the BIOS.

 * Only one CPU is active at this time, the others get re-added later using

 * CPU hotplug:

/*

 * mce_device: Sysfs support

 Reinit MCEs after user configuration changes */

 Toggle features for corrected errors */

 disable ce features */

 enable ce features */

 disable cmci */

 enable cmci */

 Per CPU device init. All of the CPUs still share the same bank device: */

 Make sure there are no machine checks on offlined CPUs. */

 intentionally ignoring frozen here */

/*

 * When running on XEN, this initcall is ordered against the XEN mcelog

 * initcall:

 *

 *   device_initcall(xen_late_init_mcelog);

 *   device_initcall_sync(mcheck_init_device);

	/*

	 * Check if we have a spare virtual bit. This will only become

	 * a problem if/when we move beyond 5-level page tables.

	/*

	 * Invokes mce_cpu_online() on all CPUs which are online when

	 * the state is installed.

/*

 * Old style boot options parsing. Only for compatibility.

	/*

	 * Flush out everything that has been logged during early boot, now that

	 * everything has been initialized (workqueues, decoders, ...).

 SPDX-License-Identifier: GPL-2.0

/*

 * P5 specific Machine Check Exception Reporting

 * (C) Copyright 2002 Alan Cox <alan@lxorguk.ukuu.org.uk>

 By default disabled */

 Machine check handler for Pentium class Intel CPUs: */

 Set up machine check reporting for processors with Intel style MCE: */

 Default P5 to off as its often misconnected: */

 Check for MCE support: */

 Read registers before enabling: */

 Enable MCE: */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel specific MCE features.

 * Copyright 2004 Zwane Mwaikambo <zwane@linuxpower.ca>

 * Copyright (C) 2008, 2009 Intel Corporation

 * Author: Andi Kleen

/*

 * Support for Intel Correct Machine Check Interrupts. This allows

 * the CPU to raise an interrupt when a corrected machine check happened.

 * Normally we pick those up using a regular polling timer.

 * Also supports reliable discovery of shared banks.

/*

 * CMCI can be delivered to multiple cpus that share a machine check bank

 * so we need to designate a single cpu to process errors logged in each bank

 * in the interrupt handler (otherwise we would have many races and potential

 * double reporting of the same error).

 * Note that this can change when a cpu is offlined or brought online since

 * some MCA banks are shared across cpus. When a cpu is offlined, cmci_clear()

 * disables CMCI on all banks owned by the cpu and clears this bitfield. At

 * this point, cmci_rediscover() kicks in and a different cpu may end up

 * taking ownership of some of the shared MCA banks that were previously

 * owned by the offlined cpu.

/*

 * CMCI storm detection backoff counter

 *

 * During storm, we reset this counter to INITIAL_CHECK_INTERVAL in case we've

 * encountered an error. If not, we decrement it by one. We signal the end of

 * the CMCI storm when it reaches 0.

/*

 * cmci_discover_lock protects against parallel discovery attempts

 * which could race against each other.

	/*

	 * Vendor check is not strictly needed, but the initial

	 * initialization is vendor keyed and this

	 * makes sure none of the backdoors are entered otherwise.

	/*

	 * LMCE depends on recovery support in the processor. Hence both

	 * MCG_SER_P and MCG_LMCE_P should be present in MCG_CAP.

	/*

	 * BIOS should indicate support for LMCE by setting bit 20 in

	 * IA32_FEAT_CTL without which touching MCG_EXT_CTL will generate a #GP

	 * fault.  The MSR must also be locked for LMCE_ENABLED to take effect.

	 * WARN if the MSR isn't locked as init_ia32_feat_ctl() unconditionally

	 * locks the MSR in the event that it wasn't already locked by BIOS.

	/*

	 * Reset the counter if we've logged an error in the last poll

	 * during the storm.

		/*

		 * We switch back to interrupt mode once the poll timer has

		 * silenced itself. That means no events recorded and the timer

		 * interval is back to our poll interval.

		/*

		 * We wait for all CPUs to go back to SUBSIDED state. When that

		 * happens we switch back to interrupt mode.

 We have shiny weather. Let the poll do whatever it thinks. */

/*

 * The interrupt handler. This is called on every event.

 * Just call the poller directly to log any events.

 * This could in theory increase the threshold under high load,

 * but doesn't for now.

/*

 * Enable CMCI (Corrected Machine Check Interrupt) for available MCE banks

 * on this CPU. Use the algorithm recommended in the SDM to discover shared

 * banks.

 Skip banks in firmware first mode */

 Already owned by someone else? */

			/*

			 * If bios_cmci_threshold boot option was specified

			 * but the threshold is zero, we'll try to initialize

			 * it to 1.

 Did the enable bit stick? -- the bank supports CMCI */

			/*

			 * We are able to set thresholds for some banks that

			 * had a threshold of 0. This means the BIOS has not

			 * set the thresholds properly or does not work with

			 * this boot option. Note down now and report later.

/*

 * Just in case we missed an event during initialization check

 * all the CMCI owned banks.

 Caller must hold the lock on cmci_discover_lock */

/*

 * Disable CMCI on this CPU for all banks it owns when it goes down.

 * This allows other CPUs to claim the banks on rediscovery.

 Recheck banks in case CPUs don't all have the same */

 After a CPU went down cycle through all the others and rediscover */

/*

 * Reenable CMCI on this CPU in case a CPU down failed.

	/*

	 * For CPU #0 this runs with still disabled APIC, but that's

	 * ok because only the vector is set up. We still do another

	 * check for the banks later for CPU #0 just to make sure

	 * to not miss any events.

	/*

	 * Even if testing the presence of the MSR would be enough, we don't

	 * want to risk the situation where other models reuse this MSR for

	 * other purposes.

 PPIN locked in disabled mode */

 If PPIN is disabled, try to enable */

 Is the enable bit set? */

/*

 * Enable additional error logs from the integrated

 * memory controller on processors that support this.

 MCE errata HSD131, HSM142, HSW131, BDM48, HSM142 and SKX37 */

 SPDX-License-Identifier: GPL-2.0

	/* This is drawn from a dump from vgacon:startup in

 SPDX-License-Identifier: GPL-2.0

 fallback to return an emulated IO_APIC values */

	/* Shouldn't need this as APIC is turned off for PV, and we only

 Warn to see if there's any stray references */

 Warn to see if there's any stray references */

 .delivery_mode and .dest_mode_logical not used by XENPV */

 Used on 32-bit */

 setup_local_APIC calls it */

 Used on 32-bit */

 Used on 32-bit */

 smp_sanity_check needs it */

 detect_ht */

 Can be NULL on 32-bit. */

 .wait_for_init_deassert- used  by AP bootup - smp_callin which we don't use */

	/* On PV guests the APIC CPUID bit is disabled so none of the

 SPDX-License-Identifier: GPL-2.0

/*

 * Machine specific setup for xen

 *

 * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007

 Amount of extra memory space we add to the e820 ranges */

 Number of pages released from the initial allocation. */

 E820 map used during setting up memory. */

/*

 * Buffer used to remap identity mapped pages. We only need the virtual space.

 * The physical page behind this address is remapped as needed to different

 * buffer pages.

/*

 * The maximum amount of extra memory compared to the base size.  The

 * main scaling factor is the size of struct page.  At extreme ratios

 * of base:extra, all the base memory can be filled with page

 * structures for the extra memory, leaving no space for anything

 * else.

 *

 * 10x seems like a reasonable balance between scaling flexibility and

 * leaving a practically usable system.

	/*

	 * No need to check for zero size, should happen rarely and will only

	 * write a new entry regarded to be unused due to zero size.

 Add new region. */

 Append to existing region. */

 Start of region. */

 End of region. */

 Mid of region. */

 Calling memblock_reserve() again is okay. */

/*

 * Called during boot before the p2m list can take entries beyond the

 * hypervisor supplied p2m list. Entries in extra mem are to be regarded as

 * invalid.

/*

 * Mark all pfns of extra mem as invalid in p2m list.

/*

 * Finds the next RAM pfn available in the E820 map after min_pfn.

 * This function updates min_pfn with the pfn found and returns

 * the size of that range or zero if not found.

 We only care about E820 after this */

		/* If min_pfn falls within the E820 entry, we want to start

		 * at the min_pfn PFN.

/*

 * This releases a chunk of memory and then does the identity map. It's used

 * as a fallback if the remapping fails.

 Release pages first. */

 Make sure pfn exists to start with */

/*

 * Helper function to update the p2m and m2p tables and kernel mapping.

 Update p2m */

 Update m2p */

/*

 * This function updates the p2m and m2p tables with an identity map from

 * start_pfn to start_pfn+size and prepares remapping the underlying RAM of the

 * original allocation at remap_pfn. The information needed for remapping is

 * saved in the memory itself to avoid the need for allocating buffers. The

 * complete remap information is contained in a list of MFNs each containing

 * up to REMAP_SIZE MFNs and the start target PFN for doing the remap.

 * This enables us to preserve the original mfn sequence while doing the

 * remapping at a time when the memory management is capable of allocating

 * virtual and physical memory in arbitrary amounts, see 'xen_remap_memory' and

 * its callers.

 Map first pfn to xen_remap_buf */

 Save mapping information in page */

 Put remap buf into list. */

 Set identity map */

 Restore old xen_remap_buf mapping */

/*

 * This function takes a contiguous pfn range that needs to be identity mapped

 * and:

 *

 *  1) Finds a new range of pfns to use to remap based on E820 and remap_pfn.

 *  2) Calls the do_ function to actually do the mapping/remapping work.

 *

 * The goal is to not allocate additional memory but to remap the existing

 * pages. In the case of an error the underlying memory is simply released back

 * to Xen and not remapped.

 Do not remap pages beyond the current allocation */

 Identity map remaining pages */

 Adjust size to fit in current e820 RAM region */

 Update variables to reflect new mappings. */

	/*

	 * If the PFNs are currently mapped, their VA mappings need to be

	 * zapped.

	/*

	 * Combine non-RAM regions and gaps until a RAM region (or the

	 * end of the map) is reached, then call the provided function

	 * to perform its duty on the non-RAM region.

	 *

	 * The combined non-RAM regions are rounded to a whole number

	 * of pages so any partial pages are accessible via the 1:1

	 * mapping.  This is needed for some BIOSes that put (for

	 * example) the DMI tables in a reserved region that begins on

	 * a non-page boundary.

/*

 * Remap the memory prepared in xen_do_set_identity_and_remap_chunk().

 * The remap information (which mfn remap to which pfn) is contained in the

 * to be remapped memory itself in a linked list anchored at xen_remap_mfn.

 * This scheme allows to remap the different chunks in arbitrary order while

 * the resulting mapping will be independent from the order.

 Map the remap information */

	/*

	 * For the initial domain we use the maximum reservation as

	 * the maximum page.

	 *

	 * For guest domains the current maximum reservation reflects

	 * the current maximum rather than the static maximum. In this

	 * case the e820 map provided to us will cover the static

	 * maximum region.

 Align RAM regions to page boundaries. */

		/*

		 * Don't allow adding memory not in E820 map while booting the

		 * system. Once the balloon driver is up it will remove that

		 * restriction again.

/*

 * Find a free area in physical memory not yet reserved and compliant with

 * E820 map.

 * Used to relocate pre-allocated areas like initrd or p2m list which are in

 * conflict with the to be used E820 map.

 * In case no area is found, return 0. Otherwise return the physical address

 * of the area which is already reserved for convenience.

/*

 * Like memcpy, but with physical addresses for dest and src.

/*

 * Reserve Xen mfn_list.

/**

 * machine_specific_memory_setup - Hook for machine specific memory setup.

 8MB slack (to balance backend allocations). */

	/*

	 * Xen won't allow a 1:1 mapping to be created to UNUSABLE

	 * regions, so if we're using the machine memory map leave the

	 * region as RAM as it is in the pseudo-physical map.

	 *

	 * UNUSABLE regions in domUs are not handled and will need

	 * a patch in the future.

 Make sure the Xen-supplied memory map is well-ordered. */

 How many extra pages do we need due to remapping? */

	/*

	 * Clamp the amount of extra memory to a EXTRA_MEM_RATIO

	 * factor the base size.

	 *

	 * Make sure we have no memory above max_pages, as this area

	 * isn't handled by the p2m management.

	/*

	 * Set the rest as identity mapped, in case PCI BARs are

	 * located here.

	/*

	 * In domU, the ISA region is normal, usable memory, but we

	 * reserve ISA memory anyway because too many things poke

	 * about in there.

	/*

	 * Check whether the kernel itself conflicts with the target E820 map.

	 * Failing now is better than running into weird problems later due

	 * to relocating (and even reusing) pages with kernel text or data.

	/*

	 * Check for a conflict of the hypervisor supplied page tables with

	 * the target E820 map.

 Check for a conflict of the initrd with the target E820 map. */

	/*

	 * Set identity map on non-RAM pages and prepare remapping the

	 * underlying RAM.

		/* Pretty fatal; 64-bit userspace has no other

 This function is not called for HVM domains */

 Set up idle, making sure it calls safe_halt() pvop */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2014 Oracle Co., Daniel Kiper

 Initialized later. */

 Ignored by Linux Kernel. */

 Ignored by Linux Kernel. */

 Initialized later. */

 Initialized later. */

 Not used under Xen. */

 Not used under Xen. */

 Not used under Xen. */

 Not used under Xen. */

 Not used under Xen. */

 Not used under Xen. */

 Not used under Xen. */

 Not used under Xen. */

 Initialized later. */

 Initialized later. */

 Here we know that Xen runs on EFI platform. */

/*

 * Determine whether we're in secure boot mode.

 See if a user has put the shim into insecure mode. */

 If it fails, we don't care why. Default to secure. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Split spinlock implementation out into its own file, so it can be

 * compiled in a FTRACE-compatible way.

 Don't kick if the target's kicker interrupt is not initialized. */

/*

 * Halt the current CPU & release it back to the host

 If kicker interrupts not initialized yet, just spin */

 Detect reentry. */

 If irq pending already and no nested call clear it. */

 Block until irq becomes pending (or a spurious wakeup) */

 make sure it's never delivered */

	/*

	 * When booting the kernel with 'mitigations=auto,nosmt', the secondary

	 * CPUs are not activated, and lock_kicker_irq is not initialized.

/*

 * Our init of PV spinlocks is split in two init functions due to us

 * using paravirt patching and jump labels patching and having to do

 * all of this before SMP code is invoked.

 *

 * The paravirt patching needs to be done _before_ the alternative asm code

 * is started, otherwise we would not patch the core kernel code.

  Don't need to use pvqspinlock code if there is only 1 vCPU. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Xen time implementation.

 *

 * This is implemented in terms of a clocksource driver which uses

 * the hypervisor clock as a nanosecond timebase, and a clockevent

 * driver which uses the hypervisor's timer mechanism.

 *

 * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007

 Minimum amount of time until next clock event fires */

 Get the TSC speed from Xen */

 Protected by the calling core code serialization */

	/*

	 * We only take the expensive HV call when the clock was set

	 * or when the 11 minutes RTC synchronization time elapsed.

	/*

	 * Move the next drift compensation time 11 minutes

	 * ahead. That's emulating the sync_cmos_clock() update for

	 * the hardware RTC.

/*

   Xen clockevent implementation



   Xen has two clockevent implementations:



   The old timer_op one works with all released versions of Xen prior

   to version 3.0.4.  This version of the hypervisor provides a

   single-shot timer with nanosecond resolution.  However, sharing the

   same event channel is a 100Hz tick which is delivered while the

   vcpu is running.  We don't care about or use this tick, but it will

   cause the core time code to think the timer fired too soon, and

   will end up resetting it each time.  It could be filtered, but

   doing so has complications when the ktime clocksource is not yet

   the xen clocksource (ie, at boot time).



   The new vcpu_op-based timer interface allows the tick timer period

   to be changed or turned off.  The tick timer is not useful as a

   periodic timer because events are only delivered to running vcpus.

   The one-shot timer can report when a timeout is in the past, so

   set_next_event is capable of returning -ETIME when appropriate.

   This interface is used when available.

/*

  Get a hypervisor absolute time.  In theory we could maintain an

  offset between the kernel's time and the hypervisor's time, and

  apply that to a kernel's absolute timeout.  Unfortunately the

  hypervisor and kernel times can drift even if the kernel is using

  the Xen clocksource, because ntp can warp the kernel's clocksource.

 cancel timeout */

	/* We may have missed the deadline, but there's no real way of

	   knowing for sure.  If the event was in the past, then we'll

 Get an event anyway, even if the timeout is already expired */

	/*

	 * We don't disable VDSO_CLOCKMODE_PVCLOCK entirely if it fails to

	 * register the secondary time info with Xen or if we migrated to a

	 * host without the necessary flags. On both of these cases what

	 * happens is either process seeing a zeroed out pvti or seeing no

	 * PVCLOCK_TSC_STABLE_BIT bit set. Userspace checks the latter and

	 * if 0, it discards the data in pvti and fallbacks to a system

	 * call for a reliable timestamp.

 Need pvclock_resume() before using xen_clocksource_read(). */

	/*

	 * If primary time info had this bit set, secondary should too since

	 * it's the same data on both just different memory regions. But we

	 * still check it in case hypervisor is buggy.

 As Dom0 is never moved, no penalty on using TSC there */

		/* Successfully turned off 100Hz tick, so we have the

 Set initial system time with full resolution */

	/*

	 * We check ahead on the primary time info if this

	 * bit is supported hence speeding up Xen clocksource.

 Dom0 uses the native method to set the hardware RTC. */

	/*

	 * xen_setup_timer(cpu) - snprintf is bad in atomic context. Hence

	 * doing it xen_hvm_cpu_notify (which gets called by smp_init during

	 * early bootup and also during CPU hotplug events).

	/*

	 * vector callback is needed otherwise we cannot receive interrupts

	 * on cpu > 0 and at this point we don't know how many cpus are

	 * available.

 Kernel parameter to specify Xen timer slop */

 SPDX-License-Identifier: GPL-2.0

 Glue code to lib/swiotlb-xen.c */

/*

 * pci_xen_swiotlb_detect - set xen_swiotlb to 1 if necessary

 *

 * This returns non-zero if we are forced to use xen_swiotlb (by the boot

 * option).

	/* If running as PV guest, either iommu=soft, or swiotlb=force will

	 * activate this IOMMU. If running as PV privileged, activate it

	 * irregardless.

	/* If we are running under Xen, we MUST disable the native SWIOTLB.

	 * Don't worry about swiotlb_force flag activating the native, as

	/* pci_swiotlb_detect_4gb turns on native SWIOTLB if no_iommu == 0

	 * (so no iommu=X command line over-writes).

	 * Considering that PV guests do not want the *native SWIOTLB* but

	 * only Xen SWIOTLB it is not useful to us so set no_iommu=1 here.

 Make sure ACS will be enabled */

 Make sure ACS will be enabled */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0 OR MIT

/******************************************************************************

 * grant_table.c

 * x86 specific part

 *

 * Granting foreign access to our memory reservation.

 *

 * Copyright (c) 2005-2006, Christopher Clark

 * Copyright (c) 2004-2005, K A Fraser

 * Copyright (c) 2008 Isaku Yamahata <yamahata at valinux co jp>

 *                    VA Linux Systems Japan. Split out x86 specific part.

	/*

	 * Always allocate the space for the status frames in case

	 * we're migrated to a host with V2 support.

/* Call it _before_ __gnttab_init as we need to initialize the

 SPDX-License-Identifier: GPL-2.0

/*

 * Pointer to the xen_vcpu_info structure or

 * &HYPERVISOR_shared_info->vcpu_info[cpu]. See xen_hvm_init_shared_info

 * and xen_vcpu_setup for details. By default it points to share_info->vcpu_info

 * but during boot it is switched to point to xen_vcpu_info.

 * The pointer is used in __xen_evtchn_do_upcall to acknowledge pending events.

 Linux <-> Xen vCPU id mapping */

/*

 * NB: These need to live in .data or alike because they're used by

 * xen_prepare_pvh() which runs before clearing the bss.

/*

 * Point at some empty memory to start with. We map the real shared_info

 * page as soon as fixmap is up and running.

 Any per_cpu(xen_vcpu) is stale, so reset it */

	/*

	 * For PVH and PVHVM, setup online VCPUs only. The rest will

	 * be handled by hotplug.

/*

 * On restore, set the vcpu placement up again.

 * If it fails, then we're in a bad state, since

 * we can't back out from using it...

 Only Xen 4.5 and higher support this. */

 Set to NULL so that if somebody accesses it we get an OOPS */

	/*

	 * This path is called on PVHVM at bootup (xen_hvm_smp_prepare_boot_cpu)

	 * and at restore (xen_vcpu_restore). Also called for hotplugged

	 * VCPUs (cpu_init -> xen_hvm_cpu_prepare_hvm).

	 * However, the hypercall can only be done once (see below) so if a VCPU

	 * is offlined and comes back online then let's not redo the hypercall.

	 *

	 * For PV it is called during restore (xen_vcpu_restore) and bootup

	 * (xen_setup_vcpu_info_placement). The hotplug mechanism does not

	 * use this function.

	/*

	 * N.B. This hypercall can _only_ be called once per CPU.

	 * Subsequent calls will error out with -EINVAL. This is due to

	 * the fact that hypervisor has no unregister variant and this

	 * hypercall does not allow to over-write info.mfn and

	 * info.offset.

 Check if running on Xen version (major, minor) or later */

		/*

		 * If panic_timeout==0 then we are supposed to wait forever.

		 * However, to preserve original dom0 behavior we have to drop

		 * into hypervisor. (domU behavior is controlled by its

		 * config file)

 Ignore errors when removing override. */

 SPDX-License-Identifier: GPL-2.0

 x86_pmu.handle_irq definition */

 Shared page between hypervisor and domain */

 Macro for computing address of a PMU MSR bank */

 AMD PMU */

 Intel PMU */

 Number of general pmu registers (CPUID.EAX[0xa].EAX[8..15]) */

 Number of fixed pmu registers (CPUID.EDX[0xa].EDX[0..4]) */

 Alias registers (0x4c1) for full-width writes to PMCs */

 perf callbacks */

 Convert registers from Xen's format to Linux' */

 Write out cached context to HW */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Xen mmu operations

 *

 * This file contains the various mmu fetch and update operations.

 * The most important job they must perform is the mapping between the

 * domain's pfn and the overall machine mfns.

 *

 * Xen allows guests to directly update the pagetable, in a controlled

 * fashion.  In other words, the guest modifies the same pagetable

 * that the CPU actually uses, which eliminates the overhead of having

 * a separate shadow pagetable.

 *

 * In order to allow this, it falls on the guest domain to map its

 * notion of a "physical" pfn - which is just a domain-local linear

 * address - into a real "machine address" which the CPU's MMU can

 * use.

 *

 * A pgd_t/pmd_t/pte_t will typically contain an mfn, and so can be

 * inserted directly into the pagetable.  When creating a new

 * pte/pmd/pgd, it converts the passed pfn into an mfn.  Conversely,

 * when reading the content back with __(pgd|pmd|pte)_val, it converts

 * the mfn back into a pfn.

 *

 * The other constraint is that all pages which make up a pagetable

 * must be mapped read-only in the guest.  This prevents uncontrolled

 * guest updates to the pagetable.  Xen strictly enforces this, and

 * will disallow any pagetable update which will end up mapping a

 * pagetable page RW, and will disallow using any writable page as a

 * pagetable.

 *

 * Naively, when loading %cr3 with the base of a new pagetable, Xen

 * would need to validate the whole pagetable before going on.

 * Naturally, this is quite slow.  The solution is to "pin" a

 * pagetable, which enforces all the constraints on the pagetable even

 * when it is not actively in use.  This menas that Xen can be assured

 * that it is still valid when you do load it into %cr3, and doesn't

 * need to revalidate it.

 *

 * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007

 l3 pud for userspace vsyscall mapping */

/*

 * Protects atomic reservation decrease/increase against concurrent increases.

 * Also protects non-atomic updates of current_pages and balloon lists.

/*

 * Note about cr3 (pagetable base) values:

 *

 * xen_cr3 contains the current logical cr3 value; it contains the

 * last set cr3.  This may not be the current effective cr3, because

 * its update may be being lazily deferred.  However, a vcpu looking

 * at its own cr3 can use this value knowing that it everything will

 * be self-consistent.

 *

 * xen_current_cr3 contains the actual vcpu cr3; it is set once the

 * hypercall to set the vcpu cr3 is complete (so it may be a little

 * out of date, but it will never be set early).  If one vcpu is

 * looking at another vcpu's cr3 value, it should use this variable.

 cr3 stored as physaddr */

 actual vcpu cr3 */

/*

 * Just beyond the highest usermode address.  STACK_TOP_MAX has a

 * redzone above it, so round it up to a PGD boundary.

 vaddr missing */

 vaddr missing */

/*

 * During early boot all page table pages are pinned, but we do not have struct

 * pages, so return true until struct pages are ready.

 ptr may be ioremapped for 64-bit pagetable setup */

	/* If page is not pinned, we can just update the entry

/*

 * Associate a virtual page frame with a given physical page frame

 * and protection flags for that frame.

		/*

		 * Could call native_set_pte() here and trap and

		 * emulate the PTE write, but a hypercall is much cheaper.

 Just return the pte as-is.  We preserve the bits on commit */

 Assume pteval_t is equivalent to all the other *val_t types. */

		/*

		 * If there's no mfn for the pfn, then just create an

		 * empty non-present pte.  Unfortunately this loses

		 * information about the original pfn, so

		 * pte_mfn_to_pfn is asymmetric.

 ptr may be ioremapped for 64-bit pagetable setup */

	/* If page is not pinned, we can just update the entry

/*

 * Raw hypercall-based set_p4d, intended for in early boot before

 * there's a page structure.  This implies:

 *  1. The only existing pagetable is the kernel's

 *  2. It is always pinned

 *  3. It has no user pagetable attached to it

	/* If page is not pinned, we can just update the entry

	/* If it's pinned, then we can at least batch the kernel and

 CONFIG_PGTABLE_LEVELS >= 5 */

/*

 * (Yet another) pagetable walker.  This one is intended for pinning a

 * pagetable.  This means that it walks a pagetable and calls the

 * callback function on each page it finds making up the page table,

 * at every level.  It walks the entire pagetable, but it only bothers

 * pinning pte pages which are below limit.  In the normal case this

 * will be STACK_TOP_MAX, but at boot we need to pin up to

 * FIXADDR_TOP.

 *

 * We must skip the Xen hole in the middle of the address space, just after

 * the big x86-64 virtual hole.

 The limit is the last byte to be touched */

	/*

	 * 64-bit has a great big hole in the middle of the address

	 * space, which contains the Xen mappings.

	/* Do the top level last, so that the callbacks can use it as

/* If we're using split pte locks, then take the page's lock and

		/*

		 * We need to hold the pagetable lock between the time

		 * we make the pagetable RO and when we actually pin

		 * it.  If we don't, then other users may come in and

		 * attempt to update the pagetable by writing it,

		 * which will fail because the memory is RO but not

		 * pinned, so Xen won't do the trap'n'emulate.

		 *

		 * If we're using split pte locks, we can't hold the

		 * entire pagetable's worth of locks during the

		 * traverse, because we may wrap the preempt count (8

		 * bits).  The solution is to mark RO and pin each PTE

		 * page while holding the lock.  This means the number

		 * of locks we end up holding is never more than a

		 * batch size (~32 entries, at present).

		 *

		 * If we're not using split pte locks, we needn't pin

		 * the PTE pages independently, because we're

		 * protected by the overall pagetable lock.

			/* Queue a deferred unlock for when this batch

/* This is called just after a mm has been created, but it has not

   been used yet.  We need to make sure that its pagetable is all

/*

 * On save, we need to pin all pagetables to make sure they get their

 * mfns turned into pfns.  Search the list for any unpinned pgds and pin

 * them (unpinned pgds are not currently in use, probably because the

 * process is under construction or destruction).

 *

 * Expected to be called in stop_machine() ("equivalent to taking

 * every spinlock in the system"), so the locking doesn't really

 * matter all that much.

/*

 * The init_mm pagetable is really pinned as soon as its created, but

 * that's before we have page structures to store the bits.  So do all

 * the book-keeping now once struct pages for allocated pages are

 * initialized. This happens only after memblock_free_all() is called.

		/*

		 * Do the converse to pin_page.  If we're using split

		 * pte locks, we must be holding the lock for while

		 * the pte page is unpinned but still RO to prevent

		 * concurrent updates from seeing it in this

		 * partially-pinned state.

 unlock when batch completed */

 Release a pagetables pages back as normal RW */

/*

 * On resume, undo any pinning done at save, so that the rest of the

 * kernel doesn't see any unexpected pinned pagetables.

	/*

	 * If this cpu still has a stale cr3 reference, then make sure

	 * it has been flushed.

/*

 * Another cpu may still have their %cr3 pointing at the pagetable, so

 * we need to repoint it somewhere else before we can unpin it.

 Get the "official" set of cpus referring to our pagetable. */

	/*

	 * It's possible that a vcpu may have a stale reference to our

	 * cr3, because its in lazy mode, and it hasn't yet flushed

	 * its set of pending hypercalls yet.  In this case, we can

	 * look at its actual current cr3 value, and force it to flush

	 * if needed.

/*

 * While a process runs, Xen pins its pagetables, which means that the

 * hypervisor forces it to be read-only, and it controls all updates

 * to it.  This means that all pagetable updates have to go via the

 * hypervisor, which is moderately expensive.

 *

 * Since we're pulling the pagetable down, we switch to use init_mm,

 * unpin old process pagetable and mark it all read-write, which

 * allows further operations on it to be simple memory accesses.

 *

 * The only subtle point is that another CPU may be still using the

 * pagetable because of lazy tlb flushing.  This means we need need to

 * switch all CPUs off this pagetable before we can unpin it.

 make sure we don't move around */

 pgd may not be pinned in the error exit path of execve */

	/* NOTE: The loop is more greedy than the cleanup_highmap variant.

	/* In case we did something silly, we should crash in this function

/*

 * Make a page range writeable and free it.

/*

 * Since it is well isolated we can (and since it is perhaps large we should)

 * also free the page tables mapping the initial P->M table.

 No memory or already called. */

 using __ka address and sticking INVALID_P2M_ENTRY! */

	/*

	 * We could be in __ka space.

	 * We roundup to the PMD, which means that if anybody at this stage is

	 * using the __ka address of xen_start_info or

	 * xen_start_info->shared_info they are in going to crash. Fortunately

	 * we have already revectored in xen_setup_kernel_pagetable.

	/* At this stage, cleanup_highmap has already cleaned __ka space

	 * from _brk_limit way up to the max_pfn_mapped (which is the end of

	 * the ramdisk). We continue on, erasing PMD entries that point to page

	 * tables - do note that they are accessible at this stage via __va.

	 * As Xen is aligning the memory end to a 4MB boundary, for good

	 * measure we also round up to PMD_SIZE * 2 - which means that if

	 * anybody is using __ka address to the initial boot-stack - and try

	 * to use it - they are going to crash. The xen_start_info has been

 And revector! Bye bye old array */

	/*

	 * The majority of further PTE writes is to pagetables already

	 * announced as such to Xen. Hence it is more efficient to use

	 * hypercalls for these updates.

 Allocate and initialize top and mid mfn levels for p2m structure */

 Remap memory freed due to conflicts with E820 map */

 nothing to do */

 Remove any offline CPUs */

		/* Update xen_current_cr3 once the batch has actually

 disables interrupts */

	/* Update while interrupts are disabled, so its atomic with

 interrupts restored */

/*

 * At the start of the day - when Xen launches a guest, it has already

 * built pagetables for the guest. We diligently look over them

 * in xen_setup_kernel_pagetable and graft as appropriate them in the

 * init_top_pgt and its friends. Then when we are happy we load

 * the new init_top_pgt - and continue on.

 *

 * The generic code starts (start_kernel) and 'init_mem_mapping' sets

 * up the rest of the pagetables. When it has completed it loads the cr3.

 * N.B. that baremetal would start at 'start_kernel' (and the early

 * #PF handler would create bootstrap pagetables) - so we are running

 * with the same assumptions as what to do when write_cr3 is executed

 * at this point.

 *

 * Since there are no user-page tables at all, we have two variants

 * of xen_write_cr3 - the early bootup (this one), and the late one

 * (xen_write_cr3). The reason we have to do that is that in 64-bit

 * the Linux kernel and user-space are both in ring 3 while the

 * hypervisor is in ring 0.

 disables interrupts */

	/* Update while interrupts are disabled, so its atomic with

 interrupts restored */

/*

 * Init-time set_pte while constructing initial pagetables, which

 * doesn't allow RO page table pages to be remapped RW.

 *

 * If there is no MFN for this PFN then this page is initially

 * ballooned out so clear the PTE (as in decrease_reservation() in

 * drivers/xen/balloon.c).

 *

 * Many of these PTE updates are done on unpinned and writable pages

 * and doing a hypercall for these is unnecessary and expensive.  At

 * this point it is rarely possible to tell if a page is pinned, so

 * mostly write the PTE directly and rely on Xen trapping and

 * emulating any updates as necessary.

	/*

	 * Pages belonging to the initial p2m list mapped outside the default

	 * address range must be mapped read-only. This region contains the

	 * page tables for mapping the p2m list, too, and page tables MUST be

	 * mapped read-only.

/* Early in boot, while setting up the initial pagetable, assume

 should only be used early */

 Used for pmd and pud */

 should only be used early */

/* Early release_pte assumes that all pts are pinned, since there's

/* This needs to make sure the new pte page is pinned iff its being

 This should never happen until we're OK to use struct page */

/*

 * Like __va(), but returns address in the kernel mapping (which is

 * all we have until the physical memory mapping has been set up.

 Convert a machine address to physical address */

 Convert a machine address to kernel virtual */

 Set the page permissions on an identity-mapped pages */

	/* All levels are converted the same way, so just treat them

/*

 * Set up the initial kernel pagetable.

 *

 * We can construct this by grafting the Xen provided pagetable into

 * head_64.S's preconstructed pagetables.  We copy the Xen L2's into

 * level2_ident_pgt, and level2_kernel_pgt.  This means that only the

 * kernel has a physical mapping to start with - but that's enough to

 * get __va working.  We need to fill in the rest of the physical

 * mapping once some sort of allocator has been set up.

	/* max_pfn_mapped is the last pfn mapped in the initial memory

	 * mappings. Considering that on Xen after the kernel mappings we

	 * have the mappings of some pages that don't exist in pfn space, we

 Zap identity mapping */

 Pre-constructed entries are in pfn, so convert to mfn */

 L4[273] -> level3_ident_pgt  */

 L4[511] -> level3_kernel_pgt */

 L3_i[0] -> level2_ident_pgt */

 L3_k[510] -> level2_kernel_pgt */

 L3_k[511] -> level2_fixmap_pgt */

 L3_k[511][508-FIXMAP_PMD_NUM ... 507] -> level1_fixmap_pgt */

 We get [511][511] and have Xen's version of level2_kernel_pgt */

	/* Graft it onto L4[273][0]. Note that we creating an aliasing problem:

	 * Both L4[273][0] and L4[511][510] have entries that point to the same

	 * L2 (PMD) tables. Meaning that if you modify it in __va space

	 * it will be also modified in the __ka space! (But if you just

	 * modify the PMD table to point to other PTE's or none, then you

 Graft it onto L4[511][510] */

	/*

	 * Zap execute permission from the ident map. Due to the sharing of

	 * L1 entries we need to do this in the L2.

 Copy the initial P->M table mappings if necessary. */

 Make pagetable pieces RO */

 Pin down new L4 */

 Unpin Xen-provided one */

 Pin user vsyscall L3 */

	/*

	 * At this stage there can be no user pgd, and no page structure to

	 * attach it to, so make sure we just set kernel pgd.

	/* We can't that easily rip out L3 and L2, as the Xen pagetables are

	 * set out this way: [L4], [L1], [L2], [L3], [L1], [L1] ...  for

	 * the initial domain. For guests using the toolstack, they are in:

	 * [L4], [L3], [L2], [L1], [L1], order .. So for dom0 we can only

	 * rip out the [L4] (pgd), but for guests we shave off three pages.

 Our (by three pages) smaller Xen pagetable that we are using */

 Revector the xen_start_info */

/*

 * Read a value from a physical address.

/*

 * Translate a virtual address to a physical one without relying on mapped

 * page tables. Don't rely on big pages being aligned in (guest) physical

 * space!

/*

 * Find a new area for the hypervisor supplied p2m list and relocate the p2m to

 * this area.

	/*

	 * Setup the page tables for addressing the new p2m list.

	 * We have asked the hypervisor to map the p2m list at the user address

	 * PUD_SIZE. It may have done so, or it may have used a kernel space

	 * address depending on the Xen version.

	 * To avoid any possible virtual address collision, just use

	 * 2 * PUD_SIZE for the new area.

 Now copy the old p2m info to the new area. */

 Release the old p2m list and set new list info. */

 All local page mappings */

 maps dummy local APIC */

		/*

		 * We just don't map the IO APIC - all access is via

		 * hypercalls.  Keep the address in the pte for reference.

		/* This is an MFN, but it isn't an IO mapping from the

 By default, set_fixmap is used for hardware mappings */

	/* Replicate changes to map the vsyscall page into the user

	/* This will work as long as patching hasn't happened yet

 Protected by xen_reservation_lock. */

 2MB */

/*

 * Update the pfn-to-mfn mappings for a virtual address range, either to

 * point to an array of mfns, or contiguously from a single starting

 * mfn.

/*

 * Perform the hypercall to exchange a region of our pfns to point to

 * memory with the required contiguous alignment.  Takes the pfns as

 * input, and populates mfns as output.

 *

 * Returns a success code indicating whether the hypervisor was able to

 * satisfy the request or not.

	/*

	 * Currently an auto-translated guest will not perform I/O, nor will

	 * it require PAE page directories below 4GB. Therefore any calls to

	 * this function are redundant and can be ignored.

 1. Zap current PTEs, remembering MFNs. */

 2. Get a new contiguous memory extent. */

 3. Map the new extent in place of old pages. */

 1. Find start MFN of contiguous extent. */

 2. Zap current PTEs. */

 3. Do the exchange for non-contiguous MFNs. */

 4. Map new pages in place of old pages. */

	/*

	 * If we have a contiguous range, just update the pfn itself,

	 * else update pointer to be "next pfn".

	/*

	 * We use the err_ptr to indicate if there we are doing a contiguous

	 * mapping or a discontiguous mapping.

		/*

		 * We record the error for each page that gives an error, but

		 * continue mapping until the whole set is done

			/*

			 * @err_ptr may be the same buffer as @gfn, so

			 * only clear it after each chunk of @gfn is

			 * used.

 Skip failed frame. */

 CONFIG_KEXEC_CORE */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Search for a free page starting at 4kB physical address.

	 * Low memory is preferred to avoid an EPT large page split up

	 * by the mapping.

	 * Starting below X86_RESERVE_LOW (usually 64kB) is fine as

	 * the BIOS used for HVM guests is well behaved and won't

	 * clobber memory other than the first 4kB.

	/*

	 * The virtual address of the shared_info page has changed, so

	 * the vcpu_info pointer for VCPU 0 is now stale.

	 *

	 * The prepare_boot_cpu callback will re-initialize it via

	 * xen_vcpu_setup, but we can't rely on that to be called for

	 * old Xen versions (xen_have_vector_callback == 0).

	 *

	 * It is, in any case, bad to have a stale vcpu_info pointer

	 * so reset it now.

 PVH set up hypercall page in xen_prepare_pvh(). */

	/*

	 * This can happen if CPU was offlined earlier and

	 * offlining timed out in common_cpu_die().

	/*

	 * xen_vcpu is a pointer to the vcpu_info struct in the shared_info

	 * page, we use it in the event channel upcall and in some pvclock

	 * related functions.

 Test for PVH domain (PVH boot path taken overrides ACPI flags). */

 PVH detected. */

 Make sure we don't fall back to (default) ACPI_IRQ_MODEL_PIC. */

 Guest booting via the Xen-PVH boot entry goes here */

		/*

		 * Guest booting via normal boot entry (like via grub2) goes

		 * here.

		 *

		 * Use interface functions for bare hardware if nopv,

		 * xen_hvm_guest_late_init is an exception as we need to

		 * detect PVH and panic there.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Xen leaves the responsibility for maintaining p2m mappings to the

 * guests themselves, but it must also access and update the p2m array

 * during suspend/resume when all the pages are reallocated.

 *

 * The logical flat p2m table is mapped to a linear kernel memory area.

 * For accesses by Xen a three-level tree linked via mfns only is set up to

 * allow the address space to be sparse.

 *

 *               Xen

 *                |

 *          p2m_top_mfn

 *              /   \

 * p2m_mid_mfn p2m_mid_mfn

 *         /           /

 *  p2m p2m p2m ...

 *

 * The p2m_mid_mfn pages are mapped by p2m_top_mfn_p.

 *

 * The p2m_top_mfn level is limited to 1 page, so the maximum representable

 * pseudo-physical address space is:

 *  P2M_TOP_PER_PAGE * P2M_MID_PER_PAGE * P2M_PER_PAGE pages

 *

 * P2M_PER_PAGE depends on the architecture, as a mfn is always

 * unsigned long (8 bytes on 64-bit, 4 bytes on 32), leading to

 * 512 and 1024 entries respectively.

 *

 * In short, these structures contain the Machine Frame Number (MFN) of the PFN.

 *

 * However not all entries are filled with MFNs. Specifically for all other

 * leaf entries, or for the top  root, or middle one, for which there is a void

 * entry, we assume it is  "missing". So (for example)

 *  pfn_to_mfn(0x90909090)=INVALID_P2M_ENTRY.

 * We have a dedicated page p2m_missing with all entries being

 * INVALID_P2M_ENTRY. This page may be referenced multiple times in the p2m

 * list/tree in case there are multiple areas with P2M_PER_PAGE invalid pfns.

 *

 * We also have the possibility of setting 1-1 mappings on certain regions, so

 * that:

 *  pfn_to_mfn(0xc0000)=0xc0000

 *

 * The benefit of this is, that we can assume for non-RAM regions (think

 * PCI BARs, or ACPI spaces), we can create mappings easily because we

 * get the PFN value to match the MFN.

 *

 * For this to work efficiently we have one new page p2m_identity. All entries

 * in p2m_identity are set to INVALID_P2M_ENTRY type (Xen toolstack only

 * recognizes that and MFNs, no other fancy value).

 *

 * On lookup we spot that the entry points to p2m_identity and return the

 * identity value instead of dereferencing and returning INVALID_P2M_ENTRY.

 * If the entry points to an allocated page, we just proceed as before and

 * return the PFN. If the PFN has IDENTITY_FRAME_BIT set we unmask that in

 * appropriate functions (pfn_to_mfn).

 *

 * The reason for having the IDENTITY_FRAME_BIT instead of just returning the

 * PFN is that we could find ourselves where pfn_to_mfn(pfn)==pfn for a

 * non-identity pfn. To protect ourselves against we elect to set (and get) the

 * IDENTITY_FRAME_BIT on all identity mapped PFNs.

/*

 * Hint at last populated PFN.

 *

 * Used to set HYPERVISOR_shared_info->arch.max_pfn so the toolstack

 * can avoid scanning the whole P2M (which may be sized to account for

 * hotplugged memory).

/*

 * Build the parallel p2m_top_mfn and p2m_mid_mfn structures

 *

 * This is called both at boot time, and after resuming from suspend:

 * - At boot time we're called rather early, and must use alloc_bootmem*()

 *   to allocate memory.

 *

 * - After resume we're called from within stop_machine, but the mfn

 *   tree should already be completely allocated.

 Pre-initialize p2m_top_mfn to be completely missing */

 Reinitialise, mfn's all change after migration */

		/* Don't bother allocating any mfn mid levels if

		 * they're just missing, just update the stored mfn,

		 * since all could have changed over a migrate.

 Set up p2m_top to point to the domain-builder provided p2m pages */

		/*

		 * Try to map missing/identity PMDs or p2m-pages if possible.

		 * We have to respect the structure of the mfn_list_list

		 * which will be built just afterwards.

		 * Chunk size to test is one p2m page if we are in the middle

		 * of a mfn_list_list mid page and the complete mid page area

		 * if we are at index 0 of the mid page. Please note that a

		 * mid page might cover more than one PMD, e.g. on 32 bit PAE

		 * kernels.

 Reset to minimal chunk size. */

 Use initial p2m page contents. */

 Map complete missing or identity p2m-page. */

 Complete missing or identity PMD(s) can be mapped. */

	/*

	 * The INVALID_P2M_ENTRY is filled in both p2m_*identity

	 * and in p2m_*missing, so returning the INVALID_P2M_ENTRY

	 * would be wrong.

/*

 * Allocate new pmd(s). It is checked whether the old pmd is still in place.

 * If not, nothing is changed. This is okay as the only reason for allocating

 * a new pmd is to replace p2m_missing_pte or p2m_identity_pte by a individual

 * pmd.

 Do all allocations first to bail out in error case. */

 Tools are synchronizing via p2m_generation. */

 Tools are synchronizing via p2m_generation. */

/*

 * Fully allocate the p2m structure for a given pfn.  We need to check

 * that both the top and mid levels are allocated, and make sure the

 * parallel mfn tree is kept in sync.  We may race with other cpus, so

 * the new pages are installed with cmpxchg; if we lose the race then

 * simply free the page we allocated and use the one that's there.

 PMD level is missing, allocate a new one */

 Separately check the mid mfn level */

 p2m leaf page is missing */

 Tools are synchronizing via p2m_generation. */

 Tools are synchronizing via p2m_generation. */

 Expanded the p2m? */

 Only invalid entries allowed above the highest p2m covered frame. */

	/*

	 * The interface requires atomic updates on p2m elements.

	 * xen_safe_write_ulong() is using an atomic store via asm().

 Do not add to override if the map failed. */

		/*

		 * Signal an error for this slot. This in turn requires

		 * immediate unmapping.

		/*

		 * Pre-populate both status fields, to be recognizable in

		 * the log message below.

 CONFIG_XEN_DEBUG_FS */

 SPDX-License-Identifier: GPL-2.0

/*

 * PVH variables.

 *

 * The variable xen_pvh needs to live in a data segment since it is used

 * after startup_{32|64} is invoked, which will clear the .bss segment.

 SPDX-License-Identifier: GPL-2.0

/*

 * Reschedule call back.

 Make sure other vcpus get a chance to run if they need to. */

 Some use that instead of NMI_VECTOR */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Setup vcpu_info for boot CPU. Secondary CPUs get their vcpu_info

	 * in xen_cpu_up_prepare_hvm().

	/*

	 * The alternative logic (which patches the unlock/lock) runs before

	 * the smp bootup up code is activated. Hence we need to set this up

	 * the core kernel is being patched. Otherwise we will have only

	 * modules patched but not core code.

 Set default vcpu_id to make sure that we don't use cpu-0's */

 SPDX-License-Identifier: GPL-2.0

/******************************************************************************

 * platform-pci-unplug.c

 *

 * Xen platform PCI device driver

 * Copyright (c) 2010, Citrix

 store the value of xen_emul_unplug after the unplug is done */

 PV and PVH domains always have them. */

	/* And user has xen_platform_pci=0 set in guest config as

	/* This is an odd one - we are going to run legacy

	/* And the caller has to follow with xen_pv_{disk,nic}_devices

 HVM domains might or might not */

/*

 * This one is odd - it determines whether you want to run PV _and_

 * legacy (IDE) drivers together. This combination is only possible

 * under HVM.

 N.B. This is only ever used in HVM mode */

 PVH guests don't have emulated devices. */

 user explicitly requested no unplug */

 check the version of the xen platform PCI device */

	/* If the version matches enable the Xen platform PCI driver.

	 * Also enable the Xen platform PCI driver if the host does

	 * not support the unplug protocol (XEN_PLATFORM_ERR_MAGIC)

	/* Set the default value of xen_emul_unplug depending on whether or

	 * not the Xen PV frontends and the Xen platform PCI driver have

 Now unplug the emulated devices */

 SPDX-License-Identifier: GPL-2.0

/*

 * Xen hypercall batching.

 *

 * Xen allows multiple hypercalls to be issued at once, using the

 * multicall interface.  This allows the cost of trapping into the

 * hypervisor to be amortized over several calls.

 *

 * This file implements a simple interface for multicalls.  There's a

 * per-cpu buffer of outstanding multicalls.  When you want to queue a

 * multicall for issuing, you can allocate a multicall slot for the

 * call and its arguments, along with storage for space which is

 * pointed to by the arguments (for passing pointers to structures,

 * etc).  When the multicall is actually issued, all the space for the

 * commands and allocated memory is freed for reuse.

 *

 * Multicalls are flushed whenever any of the buffers get full, or

 * when explicitly requested.  There's no way to get per-multicall

 * return results back.  It will BUG if any of the multicalls fail.

 *

 * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007

	/* Disable interrupts in case someone comes in and queues

 no-op */

		/* Singleton multicall - bypass multicall machinery

 SPDX-License-Identifier: GPL-2.0

/*

 * Core of Xen paravirt_ops implementation.

 *

 * This file contains the xen_paravirt_ops structure itself, and the

 * implementations for:

 * - privileged instructions

 * - interrupt flags

 * - segment operations

 * - booting and setup

 *

 * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007

 get_cpu_cap() */

/*

 * Updating the 3 TLS descriptors in the GDT on every task switch is

 * surprisingly expensive so we avoid updating them if they haven't

 * changed.  Since Xen writes different descriptors than the one

 * passed in the update_descriptor hypercall we keep shadow copies to

 * compare against.

 xen clock uses per-cpu vcpu_info, need to init it for boot cpu */

 pvclock is in shared info area */

 Setup shared vcpu info for non-smp configurations */

	/*

	 * Mask out inconvenient features, to try and disable as many

	 * unsupported kernel subsystems as possible.

 Synthesize the values.. */

 Suppress extended topology stuff */

 XEN_EMULATE_PREFIX */

	/* We need to determine whether it is OK to expose the MWAIT

	 * capability to the kernel to harvest deeper than C3 states from ACPI

	 * _CST using the processor_harvest_xen.c module. For this to work, we

	 * need to gather the MWAIT_LEAF values (which the cstate.c code

	 * checks against). The hypervisor won't expose the MWAIT flag because

	 * it would break backwards compatibility; so we will find out directly

	 * from the hardware and hypercall.

	/*

	 * When running under platform earlier than Xen4.2, do not expose

	 * mwait, to avoid the risk of loading native acpi pad driver

	/* We need to emulate the MWAIT_LEAF and for that we need both

	 * ecx and edx. The hypercall provides only partial information.

	/* Ask the Hypervisor whether to clear ACPI_PDC_C_C2C3_FFH. If so,

	 * don't expose MWAIT_LEAF and let ACPI pick the IOPORT version of C3.

 Xen will set CR4.OSXSAVE if supported and not disabled by force */

	/*

	 * Xen PV would need some work to support PCID: CR3 handling as well

	 * as xen_flush_tlb_others() would need updating.

/*

 * Set the page permissions for a particular virtual address.  If the

 * address is a vmalloc mapping (or other non-linear mapping), then

 * find the linear mapping of the page and also set its protections to

 * match.

	/*

	 * Careful: update_va_mapping() will fail if the virtual address

	 * we're poking isn't populated in the page tables.  We don't

	 * need to worry about the direct map (that's always in the page

	 * tables), but we need to be careful about vmap space.  In

	 * particular, the top level page table can lazily propagate

	 * entries between processes, so if we've switched mms since we

	 * vmapped the target in the first place, we might not have the

	 * top-level page table entry populated.

	 *

	 * We disable preemption because we want the same mm active when

	 * we probe the target and when we issue the hypercall.  We'll

	 * have the same nominal mm, but if we're a kernel thread, lazy

	 * mm dropping could change our pgd.

	 *

	 * Out of an abundance of caution, this uses __get_user() to fault

	 * in the target address just in case there's some obscure case

	 * in which the target address isn't readable.

	/*

	 * We need to mark the all aliases of the LDT pages RO.  We

	 * don't need to call vm_flush_aliases(), though, since that's

	 * only responsible for flushing aliases out the TLBs, not the

	 * page tables, and Xen will flush the TLB for us if needed.

	 *

	 * To avoid confusing future readers: none of this is necessary

	 * to load the LDT.  The hypervisor only checks this when the

	 * LDT is faulted in due to subsequent descriptor access.

 @size should be at most GDT_SIZE which is smaller than PAGE_SIZE. */

	/*

	 * The GDT is per-cpu and is in the percpu data area.

	 * That can be virtually mapped, so we need to do a

	 * page-walk to get the underlying MFN for the

	 * hypercall.  The page can also be in the kernel's

	 * linear range, so we need to RO that mapping too.

/*

 * load_gdt for early boot, when the gdt is only mapped once

 @size should be at most GDT_SIZE which is smaller than PAGE_SIZE. */

	/*

	 * In lazy mode we need to zero %fs, otherwise we may get an

	 * exception between the new %fs descriptor being loaded and

	 * %fs being effectively cleared at __switch_to().

 On Xen PV, NMI doesn't use IST.  The C part is the same as native. */

 On Xen PV, DF doesn't use IST.  The C part is the same as native. */

	/*

	 * There's no IST on Xen PV, but we still need to dispatch

	 * to the correct handler.

 This should never happen and there is no way to handle it. */

	/*

	 * There's no IST on Xen PV, but we still need to dispatch

	 * to the correct handler.

	/*

	 * Replace trap handler addresses by Xen specific ones.

	 * Check for known traps using IST and whitelist them.

	 * The debugger ones are the only ones we care about.

	 * Xen will handle faults like double_fault, so we should never see

	 * them.  Warn if there's an unexpected IST-using fault handler.

 interrupt gates clear IF */

 Locations of each CPU's IDT */

/* Set an IDT entry.  If the entry is part of the current IDT, then

/* Load a new IDT into Xen.  In principle this can be per-CPU, so we

   hold a spinlock to protect the static traps[] array (static because

/* Write a GDT descriptor entry.  Ignore LDT descriptors, since

 ignore */

/*

 * Version of write_gdt_entry for use at early boot-time needed to

 * update an entry as simply as possible.

 ignore */

	/* Only pay attention to cr0.TS; everything else is

		/* Fast syscall setup is all done in hypercalls, so

		   these are all ignored.  Stub them out here to stop

	/*

	 * This will silently swallow a #GP from RDMSR.  It may be worth

	 * changing that.

	/*

	 * This will silently swallow a #GP from WRMSR.  It may be worth

	 * changing that.

 This is called once we have the cpu_possible_mask */

 Set up direct vCPU id mapping for PV guests. */

 Construct a value which looks like it came from port 0x61. */

/*

 * Set up the GDT and segment registers for -fstack-protector.  Until

 * we do this, we have to be careful not to call any stack-protected

 * function, which is most of the kernel.

 First C function to be called on Xen boot */

 Install Xen paravirt ops */

	/*

	 * Setup xen_vcpu early because it is needed for

	 * local_irq_disable(), irqs_disabled(), e.g. in printk().

	 *

	 * Don't do the full vcpu_info placement stuff until we have

	 * the cpu_possible_mask and a non-dummy shared_info.

	/*

	 * Set up some pagetable state before starting to set any ptes.

 Prevent unwanted bits from being set in PTEs. */

 Get mfn list */

 Work out if we support NX */

	/*

	 * Set up kernel GDT and segment registers, mainly so that

	 * -fstack-protector code can be executed.

 Determine virtual and physical address sizes */

 Let's presume PV guests always boot on vCPU with id 0. */

	/*

	 * set up the basic apic ops.

	/*

	 * The only reliable way to retain the initial address of the

	 * percpu gdt_page is to remember it here, so we can go and

	 * mark it RW later, when the initial percpu area is freed.

	/*

	 * The pages we from Xen are not related to machine pages, so

	 * any NUMA information the kernel tries to get from ACPI will

	 * be meaningless.  Prevent it from trying.

	/*

	 * We used to do this in xen_arch_setup, but that is too late

	 * on AMD were early_cpu_init (run before ->arch_setup()) calls

	 * early_amd_init which pokes 0xcf8 port.

 Poke various useful things into boot_params */

 Make sure ACS will be enabled */

 Avoid searching for BIOS MP tables */

		/*

		 * Disable selecting "Firmware First mode" for correctable

		 * memory errors, as this is the duty of the hypervisor to

		 * decide.

 PCI BIOS service won't work from a PV guest. */

 We need this for printk timestamps */

 Start the world */

 32b kernel does this in i386_start_kernel() */

 SPDX-License-Identifier: GPL-2.0

/*

 * Xen SMP support

 *

 * This file implements the Xen versions of smp_ops.  SMP under Xen is

 * very straightforward.  Bringing a CPU up is simply a matter of

 * loading its initial context and setting it running.

 *

 * IPIs are handled through the Xen event mechanism.

 *

 * Because virtual CPUs can be scheduled onto any real CPU, there's no

 * useful topology information for the kernel to make use of.  As a

 * result, all CPUs are treated as if they're single-core and

 * single-threaded.

 PVH runs in ring 0 and allows us to do native syscalls. Yay! */

 Implies full memory barrier. */

 We can take interrupts now: we're officially "up". */

	/* This is akin to using 'nr_cpus' on the Linux command line.

	 * Which is OK as when we use 'dom0_max_vcpus=X' we can only

	 * have up to X, while nr_cpu_ids is greater than X. This

	 * normally is not a problem, except when CPU hotplugging

	 * is involved and then there might be more than X CPUs

	 * in the guest - which will not work as there is no

	 * hypercall to expand the max number of VCPUs an already

		/* We've switched to the "real" per-cpu gdt, so make

	/*

	 * The alternative logic (which patches the unlock/lock) runs before

	 * the smp bootup up code is activated. Hence we need to set this up

	 * the core kernel is being patched. Otherwise we will have only

	 * modules patched but not core code.

 Restrict the possible_map according to max_cpus. */

 used to tell cpu_init() that it can proceed with initialization */

	/*

	 * Bring up the CPU in cpu_bringup_and_idle() with the stack

	 * pointing just below where pt_regs would be if it were a normal

	 * kernel entry.

 IOPL_RING1 */

	/*

	 * Set SS:SP that Xen will use when entering guest kernel mode

	 * from guest user mode.  Subsequent calls to load_sp0() can

	 * change this value.

	/*

	 * PV VCPUs are always successfully taken down (see 'while' loop

	 * in xen_cpu_die()), so -EBUSY is an error.

 make sure interrupts start blocked */

 used only with HOTPLUG_CPU */

	/*

	 * commit 4b0c0f294 (tick: Cleanup NOHZ per cpu data on cpu down)

	 * clears certain data that the cpu_idle loop (which called us

	 * and that we return from) expects. The only way to get that

	 * data back is to call:

 !CONFIG_HOTPLUG_CPU */

 make sure we're not pinning something down */

 should set up a minimal gdt */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * if the PFN is in the linear mapped vaddr range, we can just use

	 * the (quick) virt_to_machine() p2m lookup

 otherwise we have to do a (slower) full page-table walk */

 Returns: 0 success */

 SPDX-License-Identifier: GPL-2.0

/*

 * Force a proper event-channel callback from Xen after clearing the

 * callback mask. We do this in a very simple manner, by making a call

 * down into Xen. The pending flag will be checked by Xen on return.

 Blocking includes an implicit local_irq_enable(). */

 Initial interrupt flag handling only called while interrupts off. */

 SPDX-License-Identifier: GPL-2.0

/*

 * The kdump kernel has to check whether a pfn of the crashed kernel

 * was a ballooned page. vmcore is using this function to decide

 * whether to access a pfn of the crashed kernel.

 * Returns "false" if the pfn is not backed by a RAM page, the caller may

 * handle the pfn special in this case.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 Boot processor notified via generic timekeeping_resume() */

 SPDX-License-Identifier: GPL-2.0

/*

 * User address space access functions.

 *

 *  For licencing details see kernel-base/COPYING

/**

 * copy_from_user_nmi - NMI safe copy from user

 * @to:		Pointer to the destination buffer

 * @from:	Pointer to a user space address of the current task

 * @n:		Number of bytes to copy

 *

 * Returns: The number of not copied bytes. 0 is success, i.e. all bytes copied

 *

 * Contrary to other copy_from_user() variants this function can be called

 * from NMI context. Despite the name it is not restricted to be called

 * from NMI context. It is safe to be called from any other context as

 * well. It disables pagefaults across the copy which means a fault will

 * abort the copy.

 *

 * For NMI context invocations this relies on the nested NMI work to allow

 * atomic faults from the NMI path; the nested NMI paths are careful to

 * preserve CR2.

	/*

	 * Even though this function is typically called from NMI/IRQ context

	 * disable pagefaults so that its behaviour is consistent even when

	 * called from other contexts.

 SPDX-License-Identifier: GPL-2.0

/**

 * Read an MSR with error handling

 *

 * @msr: MSR to read

 * @m: value to read into

 *

 * It returns read data only on success, otherwise it doesn't change the output

 * argument @m.

 *

/**

 * Write an MSR with error handling

 *

 * @msr: MSR to write

 * @m: value to write

/**

 * Set @bit in a MSR @msr.

 *

 * Retval:

 * < 0: An error was encountered.

 * = 0: Bit was already set.

 * > 0: Hardware accepted the MSR write.

/**

 * Clear @bit in a MSR @msr.

 *

 * Retval:

 * < 0: An error was encountered.

 * = 0: Bit was already cleared.

 * > 0: Hardware accepted the MSR write.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2002, 2003 Andi Kleen, SuSE Labs.

 *

 * Wrappers of assembly checksum functions for x86-64.

/**

 * csum_and_copy_from_user - Copy and checksum from user space.

 * @src: source address (user space)

 * @dst: destination address

 * @len: number of bytes to be copied.

 * @isum: initial sum that is added into the result (32bit unfolded)

 * @errp: set to -EFAULT for an bad source address.

 *

 * Returns an 32bit unfolded checksum of the buffer.

 * src and dst are best aligned to 64bits.

/**

 * csum_and_copy_to_user - Copy and checksum to user space.

 * @src: source address

 * @dst: destination address (user space)

 * @len: number of bytes to be copied.

 * @isum: initial sum that is added into the result (32bit unfolded)

 * @errp: set to -EFAULT for an bad destination address.

 *

 * Returns an 32bit unfolded checksum of the buffer.

 * src and dst are best aligned to 64bits.

/**

 * csum_partial_copy_nocheck - Copy and checksum.

 * @src: source address

 * @dst: destination address

 * @len: number of bytes to be copied.

 * @sum: initial sum that is added into the result (32bit unfolded)

 *

 * Returns an 32bit unfolded checksum of the buffer.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/x86_64/lib/csum-partial.c

 *

 * This file contains network checksum routines that are better done

 * in an architecture-specific manner due to speed.

/*

 * Do a 64-bit checksum on an arbitrary memory area.

 * Returns a 32bit checksum.

 *

 * This isn't as time critical as it used to be because many NICs

 * do hardware checksumming these days.

 * 

 * Things tried and found to not make it faster:

 * Manual Prefetching

 * Unrolling to an 128 bytes inner loop.

 * Using interleaving with more registers to break the carry chains.

 nr of 16-bit words.. */

 nr of 32-bit words.. */

 nr of 64-bit words.. */

 main loop using 64byte blocks */

 last up to 7 8byte blocks */

/*

 * computes the checksum of a memory block at buff, length len,

 * and adds in "sum" (32-bit)

 *

 * returns a 32-bit number suitable for feeding into itself

 * or csum_tcpudp_magic

 *

 * this function must be called with even lengths, except

 * for the last fragment, which may be odd

 *

 * it's best to have buff aligned on a 64-bit boundary

/*

 * this routine is used for miscellaneous IP-like checksums, mainly

 * in icmp.c

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/* 

 * User address space access functions.

 *

 * Copyright 1997 Andi Kleen <ak@muc.de>

 * Copyright 1997 Linus Torvalds

 * Copyright 2002 Andi Kleen <ak@suse.de>

/*

 * Zero Userspace

	/* no memory constraint because it doesn't change any memory gcc knows

/**

 * clean_cache_range - write back a cache range with CLWB

 * @vaddr:	virtual start address

 * @size:	number of bytes to write back

 *

 * Write back a cache range using the CLWB (cache line write back)

 * instruction. Note that @size is internally rounded up to be cache

 * line size aligned.

	/*

	 * __copy_user_nocache() uses non-temporal stores for the bulk

	 * of the transfer, but we need to manually flush if the

	 * transfer is unaligned. A cached memory copy is used when

	 * destination or size is not naturally aligned. That is:

	 *   - Require 8-byte alignment when size is 8 bytes or larger.

	 *   - Require 4-byte alignment when size is 4 bytes.

 cache copy and flush to align dest */

 4x8 movnti loop */

 1x8 movnti loop */

 1x4 movnti loop */

 cache copy for remaining bytes */

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Misc librarized functions for cmdline poking.

 Close enough approximation */

/**

 * Find a boolean option (like quiet,noapic,nosmp....)

 *

 * @cmdline: the cmdline string

 * @option: option string to look for

 *

 * Returns the position of that @option (starts counting with 1)

 * or 0 on not found.  @option will only be found if it is found

 * as an entire word in @cmdline.  For instance, if @option="car"

 * then a cmdline which contains "cart" will not match.

 Start of word/after whitespace */

 Comparing this word */

 Miscompare, skip */

 No command line */

	/*

	 * This 'pos' check ensures we do not overrun

	 * a non-NULL-terminated 'cmdline'

				/*

				 * We matched all the way to the end of the

				 * option we were looking for.  If the

				 * command-line has a space _or_ ends, then

				 * we matched!

				/*

				 * We hit the end of the option, but _not_

				 * the end of a word on the cmdline.  Not

				 * a match.

				/*

				 * Hit the NULL terminator on the end of

				 * cmdline.

				/*

				 * We are currently matching, so continue

				 * to the next character on the cmdline.

 Buffer overrun */

/*

 * Find a non-boolean option (i.e. option=argument). In accordance with

 * standard Linux practice, if this option is repeated, this returns the

 * last instance on the command line.

 *

 * @cmdline: the cmdline string

 * @max_cmdline_size: the maximum size of cmdline

 * @option: option string to look for

 * @buffer: memory buffer to return the option argument

 * @bufsize: size of the supplied memory buffer

 *

 * Returns the length of the argument (regardless of if it was

 * truncated to fit in the buffer), or -1 on not found.

 Start of word/after whitespace */

 Comparing this word */

 Miscompare, skip */

 Copying this to buffer */

 No command line */

	/*

	 * This 'pos' check ensures we do not overrun

	 * a non-NULL-terminated 'cmdline'

				/*

				 * We matched all the way to the end of the

				 * option we were looking for, prepare to

				 * copy the argument.

				/*

				 * We are currently matching, so continue

				 * to the next character on the cmdline.

				/*

				 * Increment len, but don't overrun the

				 * supplied buffer and leave room for the

				 * NULL terminator.

 SPDX-License-Identifier: GPL-2.0

/*

 * Count the digits of @val including a possible sign.

 *

 * (Typed on and submitted from hpa's mobile phone.)

/*

 * Utility functions for x86 operand and address decoding

 *

 * Copyright (C) Intel Corporation 2017

/**

 * is_string_insn() - Determine if instruction is a string instruction

 * @insn:	Instruction containing the opcode to inspect

 *

 * Returns:

 *

 * true if the instruction, determined by the opcode, is any of the

 * string instructions as defined in the Intel Software Development manual.

 * False otherwise.

 All string instructions have a 1-byte opcode. */

 INS, OUTS */

 MOVS, CMPS */

 STOS, LODS, SCAS */

/**

 * insn_has_rep_prefix() - Determine if instruction has a REP prefix

 * @insn:	Instruction containing the prefix to inspect

 *

 * Returns:

 *

 * true if the instruction has a REP prefix, false if not.

/**

 * get_seg_reg_override_idx() - obtain segment register override index

 * @insn:	Valid instruction with segment override prefixes

 *

 * Inspect the instruction prefixes in @insn and find segment overrides, if any.

 *

 * Returns:

 *

 * A constant identifying the segment register to use, among CS, SS, DS,

 * ES, FS, or GS. INAT_SEG_REG_DEFAULT is returned if no segment override

 * prefixes were found.

 *

 * -EINVAL in case of error.

 Look for any segment override prefixes. */

 No default action needed. */

 More than one segment override prefix leads to undefined behavior. */

/**

 * check_seg_overrides() - check if segment override prefixes are allowed

 * @insn:	Valid instruction with segment override prefixes

 * @regoff:	Operand offset, in pt_regs, for which the check is performed

 *

 * For a particular register used in register-indirect addressing, determine if

 * segment override prefixes can be used. Specifically, no overrides are allowed

 * for rDI if used with a string instruction.

 *

 * Returns:

 *

 * True if segment override prefixes can be used with the register indicated

 * in @regoff. False if otherwise.

/**

 * resolve_default_seg() - resolve default segment register index for an operand

 * @insn:	Instruction with opcode and address size. Must be valid.

 * @regs:	Register values as seen when entering kernel mode

 * @off:	Operand offset, in pt_regs, for which resolution is needed

 *

 * Resolve the default segment register index associated with the instruction

 * operand register indicated by @off. Such index is resolved based on defaults

 * described in the Intel Software Development Manual.

 *

 * Returns:

 *

 * If in protected mode, a constant identifying the segment register to use,

 * among CS, SS, ES or DS. If in long mode, INAT_SEG_REG_IGNORE.

 *

 * -EINVAL in case of error.

	/*

	 * Resolve the default segment register as described in Section 3.7.4

	 * of the Intel Software Development Manual Vol. 1:

	 *

	 *  + DS for all references involving r[ABCD]X, and rSI.

	 *  + If used in a string instruction, ES for rDI. Otherwise, DS.

	 *  + AX, CX and DX are not valid register operands in 16-bit address

	 *    encodings but are valid for 32-bit and 64-bit encodings.

	 *  + -EDOM is reserved to identify for cases in which no register

	 *    is used (i.e., displacement-only addressing). Use DS.

	 *  + SS for rSP or rBP.

	 *  + CS for rIP.

 Need insn to verify address size. */

/**

 * resolve_seg_reg() - obtain segment register index

 * @insn:	Instruction with operands

 * @regs:	Register values as seen when entering kernel mode

 * @regoff:	Operand offset, in pt_regs, used to determine segment register

 *

 * Determine the segment register associated with the operands and, if

 * applicable, prefixes and the instruction pointed by @insn.

 *

 * The segment register associated to an operand used in register-indirect

 * addressing depends on:

 *

 * a) Whether running in long mode (in such a case segments are ignored, except

 * if FS or GS are used).

 *

 * b) Whether segment override prefixes can be used. Certain instructions and

 *    registers do not allow override prefixes.

 *

 * c) Whether segment overrides prefixes are found in the instruction prefixes.

 *

 * d) If there are not segment override prefixes or they cannot be used, the

 *    default segment register associated with the operand register is used.

 *

 * The function checks first if segment override prefixes can be used with the

 * operand indicated by @regoff. If allowed, obtain such overridden segment

 * register index. Lastly, if not prefixes were found or cannot be used, resolve

 * the segment register index to use based on the defaults described in the

 * Intel documentation. In long mode, all segment register indexes will be

 * ignored, except if overrides were found for FS or GS. All these operations

 * are done using helper functions.

 *

 * The operand register, @regoff, is represented as the offset from the base of

 * pt_regs.

 *

 * As stated, the main use of this function is to determine the segment register

 * index based on the instruction, its operands and prefixes. Hence, @insn

 * must be valid. However, if @regoff indicates rIP, we don't need to inspect

 * @insn at all as in this case CS is used in all cases. This case is checked

 * before proceeding further.

 *

 * Please note that this function does not return the value in the segment

 * register (i.e., the segment selector) but our defined index. The segment

 * selector needs to be obtained using get_segment_selector() and passing the

 * segment register index resolved by this function.

 *

 * Returns:

 *

 * An index identifying the segment register to use, among CS, SS, DS,

 * ES, FS, or GS. INAT_SEG_REG_IGNORE is returned if running in long mode.

 *

 * -EINVAL in case of error.

	/*

	 * In the unlikely event of having to resolve the segment register

	 * index for rIP, do it first. Segment override prefixes should not

	 * be used. Hence, it is not necessary to inspect the instruction,

	 * which may be invalid at this point.

	/*

	 * In long mode, segment override prefixes are ignored, except for

	 * overrides for FS and GS.

/**

 * get_segment_selector() - obtain segment selector

 * @regs:		Register values as seen when entering kernel mode

 * @seg_reg_idx:	Segment register index to use

 *

 * Obtain the segment selector from any of the CS, SS, DS, ES, FS, GS segment

 * registers. In CONFIG_X86_32, the segment is obtained from either pt_regs or

 * kernel_vm86_regs as applicable. In CONFIG_X86_64, CS and SS are obtained

 * from pt_regs. DS, ES, FS and GS are obtained by reading the actual CPU

 * registers. This done for only for completeness as in CONFIG_X86_64 segment

 * registers are ignored.

 *

 * Returns:

 *

 * Value of the segment selector, including null when running in

 * long mode.

 *

 * -EINVAL on error.

 CONFIG_X86_32 */

 CONFIG_X86_64 */

	/*

	 * Don't possibly decode a 32-bit instructions as

	 * reading a 64-bit-only register.

		/*

		 * ModRM.mod == 0 and ModRM.rm == 5 means a 32-bit displacement

		 * follows the ModRM byte.

		/*

		 * If ModRM.mod != 3 and SIB.index = 4 the scale*index

		 * portion of the address computation is null. This is

		 * true only if REX.X is 0. In such a case, the SIB index

		 * is used in the address computation.

		/*

		 * If ModRM.mod is 0 and SIB.base == 5, the base of the

		 * register-indirect addressing is 0. In this case, a

		 * 32-bit displacement follows the SIB byte.

/**

 * get_reg_offset_16() - Obtain offset of register indicated by instruction

 * @insn:	Instruction containing ModRM byte

 * @regs:	Register values as seen when entering kernel mode

 * @offs1:	Offset of the first operand register

 * @offs2:	Offset of the second operand register, if applicable

 *

 * Obtain the offset, in pt_regs, of the registers indicated by the ModRM byte

 * in @insn. This function is to be used with 16-bit address encodings. The

 * @offs1 and @offs2 will be written with the offset of the two registers

 * indicated by the instruction. In cases where any of the registers is not

 * referenced by the instruction, the value will be set to -EDOM.

 *

 * Returns:

 *

 * 0 on success, -EINVAL on error.

	/*

	 * 16-bit addressing can use one or two registers. Specifics of

	 * encodings are given in Table 2-1. "16-Bit Addressing Forms with the

	 * ModR/M Byte" of the Intel Software Development Manual.

 Operand is a register, use the generic function. */

	/*

	 * If ModRM.mod is 0 and ModRM.rm is 110b, then we use displacement-

	 * only addressing. This means that no registers are involved in

	 * computing the effective address. Thus, ensure that the first

	 * register offset is invalid. The second register offset is already

	 * invalid under the aforementioned conditions.

/**

 * get_desc() - Obtain contents of a segment descriptor

 * @out:	Segment descriptor contents on success

 * @sel:	Segment selector

 *

 * Given a segment selector, obtain a pointer to the segment descriptor.

 * Both global and local descriptor tables are supported.

 *

 * Returns:

 *

 * True on success, false on failure.

 *

 * NULL on error.

 Bits [15:3] contain the index of the desired entry. */

	/*

	 * Segment descriptors have a size of 8 bytes. Thus, the index is

	 * multiplied by 8 to obtain the memory offset of the desired descriptor

	 * from the base of the GDT. As bits [15:3] of the segment selector

	 * contain the index, it can be regarded as multiplied by 8 already.

	 * All that remains is to clear bits [2:0].

/**

 * insn_get_seg_base() - Obtain base address of segment descriptor.

 * @regs:		Register values as seen when entering kernel mode

 * @seg_reg_idx:	Index of the segment register pointing to seg descriptor

 *

 * Obtain the base address of the segment as indicated by the segment descriptor

 * pointed by the segment selector. The segment selector is obtained from the

 * input segment register index @seg_reg_idx.

 *

 * Returns:

 *

 * In protected mode, base address of the segment. Zero in long mode,

 * except when FS or GS are used. In virtual-8086 mode, the segment

 * selector shifted 4 bits to the right.

 *

 * -1L in case of error.

		/*

		 * Base is simply the segment selector shifted 4

		 * bits to the right.

		/*

		 * Only FS or GS will have a base address, the rest of

		 * the segments' bases are forced to 0.

			/*

			 * swapgs was called at the kernel entry point. Thus,

			 * MSR_KERNEL_GS_BASE will have the user-space GS base.

 In protected mode the segment selector cannot be null. */

/**

 * get_seg_limit() - Obtain the limit of a segment descriptor

 * @regs:		Register values as seen when entering kernel mode

 * @seg_reg_idx:	Index of the segment register pointing to seg descriptor

 *

 * Obtain the limit of the segment as indicated by the segment descriptor

 * pointed by the segment selector. The segment selector is obtained from the

 * input segment register index @seg_reg_idx.

 *

 * Returns:

 *

 * In protected mode, the limit of the segment descriptor in bytes.

 * In long mode and virtual-8086 mode, segment limits are not enforced. Thus,

 * limit is returned as -1L to imply a limit-less segment.

 *

 * Zero is returned on error.

	/*

	 * If the granularity bit is set, the limit is given in multiples

	 * of 4096. This also means that the 12 least significant bits are

	 * not tested when checking the segment limits. In practice,

	 * this means that the segment ends in (limit << 12) + 0xfff.

/**

 * insn_get_code_seg_params() - Obtain code segment parameters

 * @regs:	Structure with register values as seen when entering kernel mode

 *

 * Obtain address and operand sizes of the code segment. It is obtained from the

 * selector contained in the CS register in regs. In protected mode, the default

 * address is determined by inspecting the L and D bits of the segment

 * descriptor. In virtual-8086 mode, the default is always two bytes for both

 * address and operand sizes.

 *

 * Returns:

 *

 * An int containing ORed-in default parameters on success.

 *

 * -EINVAL on error.

 Address and operand size are both 16-bit. */

	/*

	 * The most significant byte of the Type field of the segment descriptor

	 * determines whether a segment contains data or code. If this is a data

	 * segment, return error.

	case 0: /*

		 * Legacy mode. CS.L=0, CS.D=0. Address and operand size are

		 * both 16-bit.

	case 1: /*

		 * Legacy mode. CS.L=0, CS.D=1. Address and operand size are

		 * both 32-bit.

	case 2: /*

		 * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;

		 * operand size is 32-bit.

 Invalid setting. CS.L=1, CS.D=1 */

/**

 * insn_get_modrm_rm_off() - Obtain register in r/m part of the ModRM byte

 * @insn:	Instruction containing the ModRM byte

 * @regs:	Register values as seen when entering kernel mode

 *

 * Returns:

 *

 * The register indicated by the r/m part of the ModRM byte. The

 * register is obtained as an offset from the base of pt_regs. In specific

 * cases, the returned value can be -EDOM to indicate that the particular value

 * of ModRM does not refer to a register and shall be ignored.

/**

 * insn_get_modrm_reg_off() - Obtain register in reg part of the ModRM byte

 * @insn:	Instruction containing the ModRM byte

 * @regs:	Register values as seen when entering kernel mode

 *

 * Returns:

 *

 * The register indicated by the reg part of the ModRM byte. The

 * register is obtained as an offset from the base of pt_regs.

/**

 * get_seg_base_limit() - obtain base address and limit of a segment

 * @insn:	Instruction. Must be valid.

 * @regs:	Register values as seen when entering kernel mode

 * @regoff:	Operand offset, in pt_regs, used to resolve segment descriptor

 * @base:	Obtained segment base

 * @limit:	Obtained segment limit

 *

 * Obtain the base address and limit of the segment associated with the operand

 * @regoff and, if any or allowed, override prefixes in @insn. This function is

 * different from insn_get_seg_base() as the latter does not resolve the segment

 * associated with the instruction operand. If a limit is not needed (e.g.,

 * when running in long mode), @limit can be NULL.

 *

 * Returns:

 *

 * 0 on success. @base and @limit will contain the base address and of the

 * resolved segment, respectively.

 *

 * -EINVAL on error.

/**

 * get_eff_addr_reg() - Obtain effective address from register operand

 * @insn:	Instruction. Must be valid.

 * @regs:	Register values as seen when entering kernel mode

 * @regoff:	Obtained operand offset, in pt_regs, with the effective address

 * @eff_addr:	Obtained effective address

 *

 * Obtain the effective address stored in the register operand as indicated by

 * the ModRM byte. This function is to be used only with register addressing

 * (i.e.,  ModRM.mod is 3). The effective address is saved in @eff_addr. The

 * register operand, as an offset from the base of pt_regs, is saved in @regoff;

 * such offset can then be used to resolve the segment associated with the

 * operand. This function can be used with any of the supported address sizes

 * in x86.

 *

 * Returns:

 *

 * 0 on success. @eff_addr will have the effective address stored in the

 * operand indicated by ModRM. @regoff will have such operand as an offset from

 * the base of pt_regs.

 *

 * -EINVAL on error.

 Ignore bytes that are outside the address size. */

 64-bit address */

/**

 * get_eff_addr_modrm() - Obtain referenced effective address via ModRM

 * @insn:	Instruction. Must be valid.

 * @regs:	Register values as seen when entering kernel mode

 * @regoff:	Obtained operand offset, in pt_regs, associated with segment

 * @eff_addr:	Obtained effective address

 *

 * Obtain the effective address referenced by the ModRM byte of @insn. After

 * identifying the registers involved in the register-indirect memory reference,

 * its value is obtained from the operands in @regs. The computed address is

 * stored @eff_addr. Also, the register operand that indicates the associated

 * segment is stored in @regoff, this parameter can later be used to determine

 * such segment.

 *

 * Returns:

 *

 * 0 on success. @eff_addr will have the referenced effective address. @regoff

 * will have a register, as an offset from the base of pt_regs, that can be used

 * to resolve the associated segment.

 *

 * -EINVAL on error.

	/*

	 * -EDOM means that we must ignore the address_offset. In such a case,

	 * in 64-bit mode the effective address relative to the rIP of the

	 * following instruction.

/**

 * get_eff_addr_modrm_16() - Obtain referenced effective address via ModRM

 * @insn:	Instruction. Must be valid.

 * @regs:	Register values as seen when entering kernel mode

 * @regoff:	Obtained operand offset, in pt_regs, associated with segment

 * @eff_addr:	Obtained effective address

 *

 * Obtain the 16-bit effective address referenced by the ModRM byte of @insn.

 * After identifying the registers involved in the register-indirect memory

 * reference, its value is obtained from the operands in @regs. The computed

 * address is stored @eff_addr. Also, the register operand that indicates

 * the associated segment is stored in @regoff, this parameter can later be used

 * to determine such segment.

 *

 * Returns:

 *

 * 0 on success. @eff_addr will have the referenced effective address. @regoff

 * will have a register, as an offset from the base of pt_regs, that can be used

 * to resolve the associated segment.

 *

 * -EINVAL on error.

	/*

	 * Don't fail on invalid offset values. They might be invalid because

	 * they cannot be used for this particular value of ModRM. Instead, use

	 * them in the computation only if they contain a valid value.

	/*

	 * The first operand register could indicate to use of either SS or DS

	 * registers to obtain the segment selector.  The second operand

	 * register can only indicate the use of DS. Thus, the first operand

	 * will be used to obtain the segment selector.

/**

 * get_eff_addr_sib() - Obtain referenced effective address via SIB

 * @insn:	Instruction. Must be valid.

 * @regs:	Register values as seen when entering kernel mode

 * @regoff:	Obtained operand offset, in pt_regs, associated with segment

 * @eff_addr:	Obtained effective address

 *

 * Obtain the effective address referenced by the SIB byte of @insn. After

 * identifying the registers involved in the indexed, register-indirect memory

 * reference, its value is obtained from the operands in @regs. The computed

 * address is stored @eff_addr. Also, the register operand that indicates the

 * associated segment is stored in @regoff, this parameter can later be used to

 * determine such segment.

 *

 * Returns:

 *

 * 0 on success. @eff_addr will have the referenced effective address.

 * @base_offset will have a register, as an offset from the base of pt_regs,

 * that can be used to resolve the associated segment.

 *

 * Negative value on error.

	/*

	 * Negative values in the base and index offset means an error when

	 * decoding the SIB byte. Except -EDOM, which means that the registers

	 * should not be used in the address computation.

/**

 * get_addr_ref_16() - Obtain the 16-bit address referred by instruction

 * @insn:	Instruction containing ModRM byte and displacement

 * @regs:	Register values as seen when entering kernel mode

 *

 * This function is to be used with 16-bit address encodings. Obtain the memory

 * address referred by the instruction's ModRM and displacement bytes. Also, the

 * segment used as base is determined by either any segment override prefixes in

 * @insn or the default segment of the registers involved in the address

 * computation. In protected mode, segment limits are enforced.

 *

 * Returns:

 *

 * Linear address referenced by the instruction operands on success.

 *

 * -1L on error.

	/*

	 * Before computing the linear address, make sure the effective address

	 * is within the limits of the segment. In virtual-8086 mode, segment

	 * limits are not enforced. In such a case, the segment limit is -1L to

	 * reflect this fact.

 Limit linear address to 20 bits */

/**

 * get_addr_ref_32() - Obtain a 32-bit linear address

 * @insn:	Instruction with ModRM, SIB bytes and displacement

 * @regs:	Register values as seen when entering kernel mode

 *

 * This function is to be used with 32-bit address encodings to obtain the

 * linear memory address referred by the instruction's ModRM, SIB,

 * displacement bytes and segment base address, as applicable. If in protected

 * mode, segment limits are enforced.

 *

 * Returns:

 *

 * Linear address referenced by instruction and registers on success.

 *

 * -1L on error.

	/*

	 * In protected mode, before computing the linear address, make sure

	 * the effective address is within the limits of the segment.

	 * 32-bit addresses can be used in long and virtual-8086 modes if an

	 * address override prefix is used. In such cases, segment limits are

	 * not enforced. When in virtual-8086 mode, the segment limit is -1L

	 * to reflect this situation.

	 *

	 * After computed, the effective address is treated as an unsigned

	 * quantity.

	/*

	 * Even though 32-bit address encodings are allowed in virtual-8086

	 * mode, the address range is still limited to [0x-0xffff].

	/*

	 * Data type long could be 64 bits in size. Ensure that our 32-bit

	 * effective address is not sign-extended when computing the linear

	 * address.

 Limit linear address to 20 bits */

/**

 * get_addr_ref_64() - Obtain a 64-bit linear address

 * @insn:	Instruction struct with ModRM and SIB bytes and displacement

 * @regs:	Structure with register values as seen when entering kernel mode

 *

 * This function is to be used with 64-bit address encodings to obtain the

 * linear memory address referred by the instruction's ModRM, SIB,

 * displacement bytes and segment base address, as applicable.

 *

 * Returns:

 *

 * Linear address referenced by instruction and registers on success.

 *

 * -1L on error.

 CONFIG_X86_64 */

/**

 * insn_get_addr_ref() - Obtain the linear address referred by instruction

 * @insn:	Instruction structure containing ModRM byte and displacement

 * @regs:	Structure with register values as seen when entering kernel mode

 *

 * Obtain the linear address referred by the instruction's ModRM, SIB and

 * displacement bytes, and segment base, as applicable. In protected mode,

 * segment limits are enforced.

 *

 * Returns:

 *

 * Linear address referenced by instruction and registers on success.

 *

 * -1L on error.

	/*

	 * If not in user-space long mode, a custom code segment could be in

	 * use. This is true in protected mode (if the process defined a local

	 * descriptor table), or virtual-8086 mode. In most of the cases

	 * seg_base will be zero as in USER_CS.

/**

 * insn_fetch_from_user() - Copy instruction bytes from user-space memory

 * @regs:	Structure with register values as seen when entering kernel mode

 * @buf:	Array to store the fetched instruction

 *

 * Gets the linear address of the instruction and copies the instruction bytes

 * to the buf.

 *

 * Returns:

 *

 * - number of instruction bytes copied.

 * - 0 if nothing was copied.

 * - -EINVAL if the linear address of the instruction could not be calculated

/**

 * insn_fetch_from_user_inatomic() - Copy instruction bytes from user-space memory

 *                                   while in atomic code

 * @regs:	Structure with register values as seen when entering kernel mode

 * @buf:	Array to store the fetched instruction

 *

 * Gets the linear address of the instruction and copies the instruction bytes

 * to the buf. This function must be used in atomic context.

 *

 * Returns:

 *

 *  - number of instruction bytes copied.

 *  - 0 if nothing was copied.

 *  - -EINVAL if the linear address of the instruction could not be calculated.

/**

 * insn_decode_from_regs() - Decode an instruction

 * @insn:	Structure to store decoded instruction

 * @regs:	Structure with register values as seen when entering kernel mode

 * @buf:	Buffer containing the instruction bytes

 * @buf_size:   Number of instruction bytes available in buf

 *

 * Decodes the instruction provided in buf and stores the decoding results in

 * insn. Also determines the correct address and operand sizes.

 *

 * Returns:

 *

 * True if instruction was decoded, False otherwise.

	/*

	 * Override the default operand and address sizes with what is specified

	 * in the code segment descriptor. The instruction decoder only sets

	 * the address size it to either 4 or 8 address bytes and does nothing

	 * for the operand bytes. This OK for most of the cases, but we could

	 * have special cases where, for instance, a 16-bit code segment

	 * descriptor is used.

	 * If there is an address override prefix, the instruction decoder

	 * correctly updates these values, even for 16-bit defaults.

 SPDX-License-Identifier: GPL-2.0

/*

 * User address space access functions.

 * The non inlined parts of asm-i386/uaccess.h are here.

 *

 * Copyright 1997 Andi Kleen <ak@muc.de>

 * Copyright 1997 Linus Torvalds

/*

 * Alignment at which movsl is preferred for bulk memory copies.

/*

 * Zero Userspace

/**

 * clear_user - Zero a block of memory in user space.

 * @to:   Destination address, in user space.

 * @n:    Number of bytes to zero.

 *

 * Zero a block of memory in user space.

 *

 * Return: number of bytes that could not be cleared.

 * On success, this will be zero.

/**

 * __clear_user - Zero a block of memory in user space, with less checking.

 * @to:   Destination address, in user space.

 * @n:    Number of bytes to zero.

 *

 * Zero a block of memory in user space.  Caller must check

 * the specified block with access_ok() before calling this function.

 *

 * Return: number of bytes that could not be cleared.

 * On success, this will be zero.

/*

 * Leave these declared but undefined.  They should not be any references to

 * them

 CONFIG_X86_INTEL_USERCOPY */

 Generic arbitrary sized copy.  */

 SPDX-License-Identifier: GPL-2.0

/*

 * Support for the configuration register space at port I/O locations

 * 0x22 and 0x23 variously used by PC architectures, e.g. the MP Spec,

 * Cyrix CPUs, numerous chipsets.  As the space is indirectly addressed

 * it may have to be protected with a spinlock, depending on the context.

 SPDX-License-Identifier: GPL-2.0

/* rdmsr on a bunch of CPUs

 *

 * @mask:       which CPUs

 * @msr_no:     which MSR

 * @msrs:       array of MSR values

 *

/*

 * wrmsr on a bunch of CPUs

 *

 * @mask:       which CPUs

 * @msr_no:     which MSR

 * @msrs:       array of MSR values

 *

/* These "safe" variants are slower and should be used when the target MSR

/*

 * These variants are significantly slower, but allows control over

 * the entire 32-bit GPR set.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Most of the string-functions are rather heavily hand-optimized,

 * see especially strsep,strstr,str[c]spn. They should work, but are not

 * very easy to understand. Everything is done entirely within the register

 * set, making the functions fast and clean. String instructions have been

 * used through-out, making for "slightly" unclear code :-)

 *

 * AK: On P4 and K7 using non string instruction implementations might be faster

 * for large memory blocks. But most of them are unlikely to be used on large

 * strings.

 SPDX-License-Identifier: GPL-2.0

/*

 * Entropy functions used on early boot for KASLR base and memory

 * randomization. The base randomization is done in the compressed

 * kernel and memory randomization is done early when the regular

 * kernel starts. This file is included in the compressed kernel and

 * normally linked in the regular.

/*

 * When built for the regular kernel, several functions need to be stubbed out

 * or changed to their regular kernel equivalent.

 Circular multiply for better bit diffusion */

 SPDX-License-Identifier: GPL-2.0

 NOTE! This also sets Z if searchstring='' */

 also works for empty string, see above */

 SPDX-License-Identifier: GPL-2.0

/*

 *	Precise Delay Loops for i386

 *

 *	Copyright (C) 1993 Linus Torvalds

 *	Copyright (C) 1997 Martin Mares <mj@atrey.karlin.mff.cuni.cz>

 *	Copyright (C) 2008 Jiri Hladky <hladky _dot_ jiri _at_ gmail _dot_ com>

 *

 *	The __delay function must _NOT_ be inlined as its execution time

 *	depends wildly on alignment on many x86 processors. The additional

 *	jump magic is needed to get the timing stable on all the CPU's

 *	we have to worry about.

/*

 * Calibration and selection of the delay mechanism happens only once

 * during boot.

 simple loop based delay: */

 we don't need output */

 TSC based delay: */

 Allow RT tasks to run */

		/*

		 * It is possible that we moved to another CPU, and

		 * since TSC's are per-cpu we need to calculate

		 * that. The delay must guarantee that we wait "at

		 * least" the amount of time. Being moved to another

		 * CPU could make the wait longer but we just need to

		 * make sure we waited long enough. Rebalance the

		 * counter for this CPU.

/*

 * On Intel the TPAUSE instruction waits until any of:

 * 1) the TSC counter exceeds the value provided in EDX:EAX

 * 2) global timeout in IA32_UMWAIT_CONTROL is exceeded

 * 3) an external interrupt occurs

	/*

	 * Hard code the deeper (C0.2) sleep state because exit latency is

	 * small compared to the "microseconds" that usleep() will delay.

/*

 * On some AMD platforms, MWAITX has a configurable 32-bit timer, that

 * counts with TSC frequency. The input value is the number of TSC cycles

 * to wait. MWAITX will also exit when the timer expires.

	/*

	 * Use cpu_tss_rw as a cacheline-aligned, seldomly accessed per-cpu

	 * variable as the monitor target.

	/*

	 * AMD, like Intel, supports the EAX hint and EAX=0xf means, do not

	 * enter any deep C-state and we use it here in delay() to minimize

	 * wakeup latency.

/*

 * Call a vendor specific function to delay for a given amount of time. Because

 * these functions may return earlier than requested, check for actual elapsed

 * time and call again until done.

	/*

	 * Timer value of 0 causes MWAITX to wait indefinitely, unless there

	 * is a store on the memory monitored by MONITORX.

 2**32 / 1000000 (rounded up) */

 2**32 / 1000000000 (rounded up) */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * x86 instruction attribute tables

 *

 * Written by Masami Hiramatsu <mhiramat@redhat.com>

 __ignore_sync_check__ */

 Attribute tables are generated from opcode map */

 Attribute search APIs */

 At first, this checks the master table */

 If this is not a group, get attribute directly */

 SPDX-License-Identifier: GPL-2.0

 Copyright(c) 2016-2020 Intel Corporation. All rights reserved. */

/*

 * Similar to copy_user_handle_tail, probe for the write fault point, or

 * source exception point.

/*

 * No point in doing careful copying, or consulting a static key when

 * there is no #MC handler in the CONFIG_X86_MCE=n case.

/**

 * copy_mc_to_kernel - memory copy that handles source exceptions

 *

 * @dst:	destination address

 * @src:	source address

 * @len:	number of bytes to copy

 *

 * Call into the 'fragile' version on systems that benefit from avoiding

 * corner case poison consumption scenarios, For example, accessing

 * poison across 2 cachelines with a single instruction. Almost all

 * other uses case can use copy_mc_enhanced_fast_string() for a fast

 * recoverable copy, or fallback to plain memcpy.

 *

 * Return 0 for success, or number of bytes not copied if there was an

 * exception.

 SPDX-License-Identifier: GPL-2.0

/*

 *	MMX 3DNow! library helper functions

 *

 *	To do:

 *	We can use MMX just for prefetch in IRQ's. This may be a win.

 *		(reported so on K6-III)

 *	We should use a better code neutral filler for the short jump

 *		leal ebx. [ebx] is apparently best for K6-2, but Cyrix ??

 *	We also want to clobber the filler register so we don't get any

 *		register forwarding stalls on the filler.

 *

 *	Add *user handling. Checksums are not a win with MMX on any CPU

 *	tested so far for any MMX solution figured.

 *

 *	22/09/2000 - Arjan van de Ven

 *		Improved for non-engineering-sample Athlons

 *

/*

 * Use KFPU_387.  MMX instructions are not affected by MXCSR,

 * but both AMD and Intel documentation states that even integer MMX

 * operations will result in #MF if an exception is pending in FCW.

 *

 * EMMS is not needed afterwards because, after calling kernel_fpu_end(),

 * any subsequent user of the 387 stack will reinitialize it using

 * KFPU_387.

 len/64 */

 This set is 28 bytes */

 jmp on 26 bytes */

 jmp on 5 bytes */

	/*

	 * Now do the tail of the block:

/*

 *	The K7 has streaming cache bypass load/store. The Cyrix III, K6 and

 *	other MMX using processors do not.

	/*

	 * Since movntq is weakly-ordered, a "sfence" is needed to become

	 * ordered again:

	/*

	 * maybe the prefetch stuff can go before the expensive fnsave...

	 * but that is for later. -AV

 jmp on 26 bytes */

 jmp on 5 bytes */

	/*

	 * Since movntq is weakly-ordered, a "sfence" is needed to become

	 * ordered again:

 CONFIG_MK7 */

/*

 *	Generic MMX implementation without K7 specific streaming

 jmp on 26 bytes */

 jmp on 5 bytes */

 !CONFIG_MK7 */

/*

 * Favour MMX for page clear and copy:

 Originally from i386/string.h */

 Align any unaligned source IO */

 Align any unaligned destination IO */

	/*

	 * TODO: memset can mangle the IO patterns quite a bit.

	 * perhaps it would be better to use a dumb one:

 SPDX-License-Identifier: GPL-2.0

 Handle more 16 bytes in loop */

 Decide forward/backward copy mode */

		/*

		 * movs instruction have many startup latency

		 * so we handle small size by general register.

		/*

		 * movs instruction is only good for aligned case.

		/*

		 * We gobble 16 bytes forward in each loop.

		/*

		 * Handle data forward by movs.

		/*

		 * Handle data backward by movs.

		/*

		 * Start to prepare for backward copy.

		/*

		 * Calculate copy position to tail.

		/*

		 * We gobble 16 bytes backward in each loop.

		/*

		 * Calculate copy position to head.

		/*

		 * Move data from 8 bytes to 15 bytes.

		/*

		 * Move data from 4 bytes to 7 bytes.

		/*

		 * Move data from 2 bytes to 3 bytes.

		/*

		 * Move data for 1 byte.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * x86 instruction analysis

 *

 * Copyright (C) IBM Corporation, 2002, 2004, 2009

__ignore_sync_check__ */

 __ignore_sync_check__ */

 __ignore_sync_check__ */

 __ignore_sync_check__ */

 Verify next sizeof(t) bytes can be on the same instruction */

/**

 * insn_init() - initialize struct insn

 * @insn:	&struct insn to be initialized

 * @kaddr:	address (in kernel memory) of instruction (or copy thereof)

 * @buf_len:	length of the insn buffer at @kaddr

 * @x86_64:	!0 for 64-bit kernel or 64-bit app

	/*

	 * Instructions longer than MAX_INSN_SIZE (15 bytes) are invalid

	 * even if the input buffer is long enough to hold them.

/**

 * insn_get_prefixes - scan x86 instruction prefix bytes

 * @insn:	&struct insn containing instruction

 *

 * Populates the @insn->prefixes bitmap, and updates @insn->next_byte

 * to point to the (first) opcode.  No effect if @insn->prefixes.got

 * is already set.

 *

 * * Returns:

 * 0:  on success

 * < 0: on error

 Skip if same prefix */

 Invalid instruction */

 address size switches 2/4 or 4/8 */

 oprand size switches 2/4 */

 Set the last prefix */

 Swap the last prefix */

 Decode REX prefix */

 REX.W overrides opnd_size */

 Decode VEX prefix */

			/*

			 * In 32-bits mode, if the [7:6] bits (mod bits of

			 * ModRM) on the second byte are not 11b, it is

			 * LDS or LES or BOUND.

 VEX.W overrides opnd_size */

 VEX.W overrides opnd_size */

			/*

			 * For VEX2, fake VEX3-like byte#2.

			 * Makes it easier to decode vex.W, vex.vvvv,

			 * vex.L and vex.pp. Masking with 0x7f sets vex.W == 0.

/**

 * insn_get_opcode - collect opcode(s)

 * @insn:	&struct insn containing instruction

 *

 * Populates @insn->opcode, updates @insn->next_byte to point past the

 * opcode byte(s), and set @insn->attr (except for groups).

 * If necessary, first collects any preceding (prefix) bytes.

 * Sets @insn->opcode.value = opcode1.  No effect if @insn->opcode.got

 * is already 1.

 *

 * Returns:

 * 0:  on success

 * < 0: on error

 Get first opcode */

 Check if there is VEX prefix or not */

 This instruction is bad */

 VEX has only 1 byte for opcode */

 Get escaped opcode */

 This instruction is bad */

/**

 * insn_get_modrm - collect ModRM byte, if any

 * @insn:	&struct insn containing instruction

 *

 * Populates @insn->modrm and updates @insn->next_byte to point past the

 * ModRM byte, if any.  If necessary, first collects the preceding bytes

 * (prefixes and opcode(s)).  No effect if @insn->modrm.got is already 1.

 *

 * Returns:

 * 0:  on success

 * < 0: on error

 Bad insn */

/**

 * insn_rip_relative() - Does instruction use RIP-relative addressing mode?

 * @insn:	&struct insn containing instruction

 *

 * If necessary, first collects the instruction up to and including the

 * ModRM byte.  No effect if @insn->x86_64 is 0.

	/*

	 * For rip-relative instructions, the mod field (top 2 bits)

	 * is zero and the r/m field (bottom 3 bits) is 0x5.

/**

 * insn_get_sib() - Get the SIB byte of instruction

 * @insn:	&struct insn containing instruction

 *

 * If necessary, first collects the instruction up to and including the

 * ModRM byte.

 *

 * Returns:

 * 0: if decoding succeeded

 * < 0: otherwise.

/**

 * insn_get_displacement() - Get the displacement of instruction

 * @insn:	&struct insn containing instruction

 *

 * If necessary, first collects the instruction up to and including the

 * SIB byte.

 * Displacement value is sign-expanded.

 *

 * * Returns:

 * 0: if decoding succeeded

 * < 0: otherwise.

		/*

		 * Interpreting the modrm byte:

		 * mod = 00 - no displacement fields (exceptions below)

		 * mod = 01 - 1-byte displacement field

		 * mod = 10 - displacement field is 4 bytes, or 2 bytes if

		 * 	address size = 2 (0x67 prefix in 32-bit mode)

		 * mod = 11 - no memory operand

		 *

		 * If address size = 2...

		 * mod = 00, r/m = 110 - displacement field is 2 bytes

		 *

		 * If address size != 2...

		 * mod != 11, r/m = 100 - SIB byte exists

		 * mod = 00, SIB base = 101 - displacement field is 4 bytes

		 * mod = 00, r/m = 101 - rip-relative addressing, displacement

		 * 	field is 4 bytes

 Decode moffset16/32/64. Return 0 if failed */

 opnd_bytes must be modified manually */

 Decode imm v32(Iz). Return 0 if failed */

 opnd_bytes must be modified manually */

 Decode imm v64(Iv/Ov), Return 0 if failed */

 opnd_bytes must be modified manually */

 Decode ptr16:16/32(Ap) */

 ptr16:64 is not exist (no segment) */

 opnd_bytes must be modified manually */

/**

 * insn_get_immediate() - Get the immediate in an instruction

 * @insn:	&struct insn containing instruction

 *

 * If necessary, first collects the instruction up to and including the

 * displacement bytes.

 * Basically, most of immediates are sign-expanded. Unsigned-value can be

 * computed by bit masking with ((1 << (nbytes * 8)) - 1)

 *

 * Returns:

 * 0:  on success

 * < 0: on error

 no immediates */

 Here, insn must have an immediate, but failed */

/**

 * insn_get_length() - Get the length of instruction

 * @insn:	&struct insn containing instruction

 *

 * If necessary, first collects the instruction up to and including the

 * immediates bytes.

 *

 * Returns:

 *  - 0 on success

 *  - < 0 on error

 Ensure this instruction is decoded completely */

/**

 * insn_decode() - Decode an x86 instruction

 * @insn:	&struct insn to be initialized

 * @kaddr:	address (in kernel memory) of instruction (or copy thereof)

 * @buf_len:	length of the insn buffer at @kaddr

 * @m:		insn mode, see enum insn_mode

 *

 * Returns:

 * 0: if decoding succeeded

 * < 0: otherwise.

 #define INSN_MODE_KERN	-1 __ignore_sync_check__ mode is only valid in the kernel */

/*

 * Copyright (C) 2007 Antonino Daplas <adaplas@gmail.com>

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file COPYING in the main directory of this archive

 * for more details.

 *

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

	/*

	 * If CLONE_SETTLS is set, we need to save the thread id

	 * so it can be set during context switches.

/*

 * Copyright (C) 2003 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Copyright 2003 PathScale, Inc.

 *

 * Licensed under the GPL

 XXX This should get the constants from libc */

	/*

	 * With ARCH_SET_FS (and ARCH_SET_GS is treated similarly to

	 * be safe), we need to call arch_prctl on the host because

	 * setting %fs may result in something else happening (like a

	 * GDT or thread.fs being set instead).  So, we let the host

	 * fiddle the registers and thread struct and restore the

	 * registers afterwards.

	 *

	 * So, the saved registers are stored to the process (this

	 * needed because a stub may have been the last thing to run),

	 * arch_prctl is run on the host, then the registers are read

	 * back.

		/*

		 * With these two, we read to a local pointer and

		 * put_user it to the userspace pointer that we were

		 * given.  If addr isn't valid (because it hasn't been

		 * faulted in or is just bogus), we want put_user to

		 * fault it in (or return -EFAULT) instead of having

		 * the host return -EFAULT.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Richard Weinberger <richrd@nod.at>

 SPDX-License-Identifier: GPL-2.0

/*

 * System call table for UML/i386, copied from arch/x86/kernel/syscall_*.c

 * with some changes for UML.

/*

 * Below you can see, in terms of #define's, the differences between the x86-64

 * and the UML syscall table.

 Not going to be implemented by UML, since we have no hardware. */

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Licensed under the GPL

		/* access_process_vm() grants access to vsyscall and stub,

		 * while copy_from_user doesn't. Maybe access_process_vm is

		 * slow, but that doesn't matter, since it will be called only

		 * in case of singlestepping, if copy_from_user failed.

 int 0x80 or sysenter */

 determines which flags the user has access to. */

 1 = access 0 = no access */

 Update the syscall number. */

 read the word at location addr in the USER area. */

 Default return condition */

 Get the child FPU state. */

 Set the child FPU state. */

 Get the child FPU state. */

 Set the child FPU state. */

/*

 * Copyright 2003 PathScale, Inc.

 *

 * Licensed under the GPL

 SPDX-License-Identifier: GPL-2.0

/*

 * System call table for UML/x86-64, copied from arch/x86/kernel/syscall_*.c

 * with some changes for UML.

/*

 * Below you can see, in terms of #define's, the differences between the x86-64

 * and the UML syscall table.

 Not going to be implemented by UML, since we have no hardware. */

/*

 * The UML TLS problem. Note that x86_64 does not implement this, so the below

 * is needed only for the ia32 compatibility.

 On UML we call it this way ("old" means it's not mmap2) */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 match other core phdrs */

/*

 * Copyright (C) 2005 Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>

 * Licensed under the GPL

/*

 * If needed we can detect when it's uninitialized.

 *

 * These are initialized in an initcall and unchanged thereafter.

/*

 * sys_get_thread_area: get a yet unused TLS descriptor index.

 * XXX: Consider leaving one free slot for glibc usage at first place. This must

 * be done here (and by changing GDT_ENTRY_TLS_* macros) and nowhere else.

 *

 * Also, this must be tested when compiling in SKAS mode with dynamic linking

 * and running against NPTL.

 Postcondition: LDT_empty(info) returns true. */

	/*

	 * Check the LDT_empty or the i386 sys_get_thread_area code - we obtain

	 * indeed an empty user_desc.

		/*

		 * Actually, now if it wasn't flushed it gets cleared and

		 * flushed to the host, which will clear it.

/*

 * Verify if we need to do a flush for the new process, i.e. if there are any

 * present desc's, only if they haven't been flushed.

		/*

		 * Can't test curr->present, we may need to clear a descriptor

		 * which had a value.

/*

 * On a newly forked process, the TLS descriptors haven't yet been flushed. So

 * we mark them as such and the first switch_to will do the job.

		/*

		 * Still correct to do this, if it wasn't present on the host it

		 * will remain as flushed as it was.

/*

 * In SKAS0 mode, currently, multiple guest threads sharing the same ->mm have a

 * common host process. So this is needed in SKAS0 too.

 *

 * However, if each thread had a different host process (and this was discussed

 * for SMP support) this won't be needed.

 *

 * And this will not need be used when (and if) we'll add support to the host

 * SKAS patch.

	/*

	 * We have no need whatsoever to switch TLS for kernel threads; beyond

	 * that, that would also result in us calling os_set_thread_area with

	 * userspace_pid[cpu] == 0, which gives an error.

 XXX: use do_get_thread_area to read the host value? I'm not at all sure! */

	/*

	 * Temporary debugging check, to make sure that things have been

	 * flushed. This could be triggered if load_TLS() failed.

	/*

	 * When the TLS entry has not been set, the values read to user in the

	 * tls_array are 0 (because it's cleared at boot, see

	 * arch/i386/kernel/head.S:cpu_gdt_table). Emulate that.

 Tell the user which slot we chose for him.*/

/*

 * Perform set_thread_area on behalf of the traced child.

 * Note: error handling is not done on the deferred load, and this differ from

 * i386. However the only possible error are caused by bugs.

/*

 * Perform get_thread_area on behalf of the traced child.

/*

 * This code is really i386-only, but it detects and logs x86_64 GDT indexes

 * if a 32-bit UML is running on a 64-bit host.

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Licensed under the GPL

/*

 * Copyright 2003 PathScale, Inc.

 * Copyright (C) 2003 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 *

 * Licensed under the GPL

/*

 * determines which flags the user has access to.

 * 1 = access 0 = no access

 Update the syscall number. */

 read the word at location addr in the USER area. */

 Default return condition */

 XXX Mostly copied from sys-i386 */

		/*

		 * access_process_vm() grants access to vsyscall and stub,

		 * while copy_from_user doesn't. Maybe access_process_vm is

		 * slow, but that doesn't matter, since it will be called only

		 * in case of singlestepping, if copy_from_user failed.

 sysenter */

 Get the child FPU state. */

 Set the child FPU state. */

 XXX Calls ptrace on the host - needs some SMP thinking */

/*

 * Copyright 2003 PathScale, Inc.

 *

 * Licensed under the GPL

/* 

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Licensed under the GPL

 These two are from asm-um/uaccess.h and linux/module.h, check them. */

 Compare this to arch/i386/mm/extable.c:fixup_exception() */

/*

 * Copyright (C) 2003 PathScale, Inc.

 * Copyright (C) 2003 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Licensed under the GPL

/*

 * FPU tag word conversions.

 to avoid 16 bit prefixes in the code */

 Transform each pair of bits into 01 (valid) or 00 (empty) */

 0V0V0V0V0V0V0V0V */

 and move the valid bits to the lower byte. */

 00VV00VV00VV00VV */

 0000VVVV0000VVVV */

 00000000VVVVVVVV */

 Special */

 Zero */

 Special */

 Valid */

 Special */

 Empty */

 Always make any pending restarted system calls return -EINTR */

 This is the same calculation as i386 - ((sp + 4) & 15) == 0 */

	/*

	 * This is popl %eax ; movl $,%eax ; int $0x80

	 *

	 * WE DO NOT USE IT ANY MORE! It's only left here for historical

	 * reasons and because gdb uses it as a signature to notice

	 * signal handler stack frames.

	/*

	 * This is movl $,%eax ; int $0x80

	 *

	 * WE DO NOT USE IT ANY MORE! It's only left here for historical

	 * reasons and because gdb uses it as a signature to notice

	 * signal handler stack frames.

 Avoid ERESTART handling */

 Subtract 128 for a red zone and 8 for proper alignment */

 Create the ucontext.  */

	/*

	 * Set up to return from userspace.  If provided, use a stub

	 * already in userspace.

 x86-64 should always use SA_RESTORER. */

 could use a vstub here */

 In case the signal handler was declared without prototypes */

	/*

	 * This also works for non SA_SIGINFO handlers because they expect the

	 * next argument after the signal number on the stack.

 Avoid ERESTART handling */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2003 Jeff Dike (jdike@addtoit.com)

 * Licensed under the GPL

 This is declared by <linux/sched.h> */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Richard Weinberger <richrd@nod.at>

 * Mostly copied from arch/x86/lib/delay.c

 we don't need output */

 2**32 / 1000000 (rounded up) */

 2**32 / 1000000000 (rounded up) */

/*

 * Copyright (C) 2004 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Licensed under the GPL

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Licensed under the GPL

 Set during early boot */

 Make sure that SIGILL is enabled after the handler longjmps back */

	/*

	 * This is testing for a cmov (0x0f 0x4x) instruction causing a

	 * SIGILL in init.

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Licensed under the GPL

/*

 * In skas mode, we hold our own ldt data in UML.

 * Thus, the code implementing sys_modify_ldt_skas

 * is very similar to (and mostly stolen from) sys_modify_ldt

 * for arch/i386/kernel/ldt.c

 * The routines copied and modified in part are:

 * - read_ldt

 * - read_default_ldt

 * - write_ldt

 * - sys_modify_ldt_skas

	/*

	 * UML doesn't support lcall7 and lcall27.

	 * So, we don't really have a default ldt, but emulate

	 * an empty ldt of common host default ldt size.

 Undo the change in host */

 default_ldt is active, simply write an empty entry 0 */

		/*

		 * Now we try to retrieve info about the ldt, we

		 * inherited from the host. All ldt-entries found

		 * will be reset in the following loop

	/*

	 * Our local LDT is used to supply the data for

	 * modify_ldt(READLDT), if PTRACE_LDT isn't available,

	 * i.e., we have to use the stub for modify_ldt, which

	 * can't handle the big read buffer of up to 64kB.

 See non-um modify_ldt() for why we do this cast */

 SPDX-License-Identifier: GPL-2.0

/* Checks whether host supports TLS, and sets *tls_min according to the value

 * valid on the host.

 Values for x86 and x86_64.*/

 SPDX-License-Identifier: GPL-2.0

 sic */

/*

 * Copyright (C) 2007 Jeff Dike (jdike@{addtoit.com,linux.intel.com})

 * Licensed under the GPL

/*

 * Copyright (C) 2004 PathScale, Inc

 * Copyright (C) 2004 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Licensed under the GPL

 SPDX-License-Identifier: GPL-2.0

	/*

	 * First see if the page is readable.  If it is, it may still

	 * be a VDSO, so we go on to see if it's writable.  If not

	 * then try mapping memory there.  If that fails, then we're

	 * still in the kernel area.  As a sanity check, we'll fail if

	 * the mmap succeeds, but gives us an address different from

	 * what we wanted.

	/*

	 * Now, is it writeable?  If so, then we're in user address

	 * space.  If not, then try mprotecting it and try the write

	 * again.

	/*

	 * A 32-bit UML on a 64-bit host gets confused about the VDSO at

	 * 0xffffe000.  It is mapped, is readable, can be reprotected writeable

	 * and written.  However, exec discovers later that it can't be

	 * unmapped.  So, just set the highest address to be checked to just

	 * below it.  This might waste some address space on 4G/4G 32-bit

	 * hosts, but shouldn't hurt otherwise.

	/*

	 * We're going to be longjmping out of the signal handler, so

	 * SA_DEFER needs to be set.

	/* Manually scan the address space, bottom-up, until we find

	 * the first valid page (or run out of them).

 If we've got this far, we ran out of pages. */

 This could happen with a 4G/4G split */

 Restore the old SIGSEGV handling */

 The old value of CONFIG_TOP_ADDR */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Richard Weinberger <richrd@nod.at>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Richard Weinberger <richrd@nod.at>

 *

 * This vDSO turns all calls into a syscall so that UML can trap them.

 Disable profiling for userspace code */

	/*

	 * UML does not support SMP, we can cheat here. :)

 SPDX-License-Identifier: GPL-2.0-only

/*

 * bpf_jit_comp.c: BPF JIT compiler

 *

 * Copyright (C) 2011-2013 Eric Dumazet (eric.dumazet@gmail.com)

 * Internal BPF Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com

 mov dst, src */

 imm32 */

/*

 * List of x86 cond jumps opcodes (. + s8)

 * Add 0x10 (and an extra 0x0f) to generate far jumps (. + s32)

 Pick a register outside of BPF range for JIT internal work */

/*

 * The following table maps BPF registers to x86-64 registers.

 *

 * x86-64 register R12 is unused, since if used as base address

 * register in load/store instructions, it always needs an

 * extra byte of encoding and is callee saved.

 *

 * x86-64 register R9 is not used by BPF programs, but can be used by BPF

 * trampoline. x86-64 register R10 is used for blinding (if enabled).

 RAX */

 RDI */

 RSI */

 RDX */

 RCX */

 R8  */

 RBX callee saved */

 R13 callee saved */

 R14 callee saved */

 R15 callee saved */

 RBP readonly */

 R10 temp register */

 R11 temp register */

 R9 register, 6th function argument */

/*

 * is_ereg() == true if BPF register 'reg' maps to x86-64 r8..r15

 * which need extra byte of encoding.

 * rax,rcx,...,rbp have simpler encoding

/*

 * is_ereg_8l() == true if BPF register 'reg' is mapped to access x86-64

 * lower 8-bit registers dil,sil,bpl,spl,r8b..r15b, which need extra byte

 * of encoding. al,cl,dl,bl have simpler encoding.

 Add modifiers if 'reg' maps to x86-64 registers R8..R15 */

 Encode 'dst_reg' register into x86-64 opcode 'byte' */

 Encode 'dst_reg' and 'src_reg' registers into x86-64 opcode 'byte' */

 Some 1-byte opcodes for binary ALU operations */

 Fill whole space with INT3 instructions */

 Epilogue code offset */

	/*

	 * Program specific offsets of labels in the code; these rely on the

	 * JIT doing at least 2 passes, recording the position on the first

	 * pass, only to generate the correct offset on the second pass.

 Maximum number of bytes emitted while JITing one eBPF insn */

 Number of bytes emit_patch() needs to generate instructions */

 Number of bytes that will be skipped on tailcall */

 push rbx */

 push r13 */

 push r14 */

 push r15 */

 pop r15 */

 pop r14 */

 pop r13 */

 pop rbx */

/*

 * Emit x86-64 prologue code for BPF program.

 * bpf_tail_call helper will skip the first X86_TAIL_CALL_OFFSET bytes

 * while jumping to another program

	/* BPF trampoline can be made to work without these nops,

	 * but let's waste 5 bytes for now and optimize later

 xor eax, eax */

 nop2 */

 push rbp */

 mov rbp, rsp */

 sub rsp, rounded_stack_depth */

 push rax */

 BPF poking in modules is not supported */

/*

 * Generate the following code:

 *

 * ... bpf_tail_call(void *ctx, struct bpf_array *array, u64 index) ...

 *   if (index >= array->map.max_entries)

 *     goto out;

 *   if (++tail_call_cnt > MAX_TAIL_CALL_CNT)

 *     goto out;

 *   prog = array->ptrs[index];

 *   if (prog == NULL)

 *     goto out;

 *   goto *(prog->bpf_func + prologue_size);

 * out:

	/*

	 * rdi - pointer to ctx

	 * rsi - pointer to bpf_array

	 * rdx - index in bpf_array

	/*

	 * if (index >= array->map.max_entries)

	 *	goto out;

 mov edx, edx */

 cmp dword ptr [rsi + 16], edx */

 jbe out */

	/*

	 * if (tail_call_cnt > MAX_TAIL_CALL_CNT)

	 *	goto out;

 mov eax, dword ptr [rbp - tcc_off] */

 cmp eax, MAX_TAIL_CALL_CNT */

 ja out */

 add eax, 1 */

 mov dword ptr [rbp - tcc_off], eax */

 prog = array->ptrs[index]; */

 mov rcx, [rsi + rdx * 8 + offsetof(...)] */

	/*

	 * if (prog == NULL)

	 *	goto out;

 test rcx,rcx */

 je out */

 pop rax */

 add rsp, sd */

 goto *(prog->bpf_func + X86_TAIL_CALL_OFFSET); */

 mov rcx, qword ptr [rcx + 32] */

 add rcx, X86_TAIL_CALL_OFFSET */

	/*

	 * Now we're ready to jump into next BPF program

	 * rdi == ctx (1st arg)

	 * rcx == prog->bpf_func + X86_TAIL_CALL_OFFSET

 rcx */, ip + (prog - start));

 out: */

	/*

	 * if (tail_call_cnt > MAX_TAIL_CALL_CNT)

	 *	goto out;

 mov eax, dword ptr [rbp - tcc_off] */

 cmp eax, MAX_TAIL_CALL_CNT */

 ja out */

 add eax, 1 */

 mov dword ptr [rbp - tcc_off], eax */

 pop rax */

 out: */

			/* Plain memcpy is used when image is not live yet

			 * and still not locked as read-only. Once poke

			 * location is active (poke->tailcall_target_stable),

			 * any parallel bpf_arch_text_poke() might occur

			 * still on the read-write image until we finally

			 * locked it as read-only. Both modifications on

			 * the given image are under text_mutex to avoid

			 * interference.

	/*

	 * Optimization: if imm32 is positive, use 'mov %eax, imm32'

	 * (which zero-extends imm32) to save 2 bytes.

 'mov %rax, imm32' sign extends imm32 */

	/*

	 * Optimization: if imm32 is zero, use 'xor %eax, %eax'

	 * to save 3 bytes.

 xor */

 mov %eax, imm32 */

		/*

		 * For emitting plain u32, where sign bit must not be

		 * propagated LLVM tends to load imm64 over mov32

		 * directly, so save couple of bytes by just doing

		 * 'mov %eax, imm32' instead.

 movabsq %rax, imm64 */

 mov dst, src */

 mov32 dst, src */

 Emit the suffix (ModR/M etc) for addressing *(ptr_reg + off) and val_reg */

		/* 1-byte signed displacement.

		 *

		 * If off == 0 we could skip this and save one extra byte, but

		 * special case of x86 R13 which always needs an offset is not

		 * worth the hassle

 4-byte signed displacement */

/*

 * Emit a REX byte if it will be necessary to address these registers

/*

 * Similar version of maybe_emit_mod() for a single register

 LDX: dst_reg = *(u8*)(src_reg + off) */

 Emit 'movzx rax, byte ptr [rax + off]' */

 Emit 'movzx rax, word ptr [rax + off]' */

 Emit 'mov eax, dword ptr [rax+0x14]' */

 Emit 'mov rax, qword ptr [rax+0x14]' */

 STX: *(u8*)(dst_reg + off) = src_reg */

 Emit 'mov byte ptr [rax + off], al' */

 Add extra byte for eregs or SIL,DIL,BPL in src_reg */

 lock prefix */

 emit opcode */

 lock *(u32/u64*)(dst_reg + off) <op>= src_reg */

 src_reg = atomic_fetch_add(dst_reg + off, src_reg); */

 src_reg = atomic_xchg(dst_reg + off, src_reg); */

 r0 = atomic_cmpxchg(dst_reg + off, r0, src_reg); */

 jump over faulting load and clear dest register */

 tail call's presence in current prog implies it is reachable */

 ALU */

 neg dst */

			/*

			 * b3 holds 'normal' opcode, b2 short form only valid

			 * in case dst is eax/rax.

 dst %= src, dst /= src, dst %= imm32, dst /= imm32 */

 push rax */

 push rdx */

 mov r11, src_reg */

 mov r11, imm32 */

 mov rax, dst_reg */

			/*

			 * xor edx, edx

			 * equivalent to 'xor rdx, rdx', but one byte less

 div src_reg */

 mov dst_reg, rdx */

 mov dst_reg, rax */

 pop rdx */

 pop rax */

 imul dst_reg, dst_reg, imm8 */

 imul dst_reg, dst_reg, imm32 */

 imul dst_reg, src_reg */

 Shifts */

 Check for bad case when dst_reg == rcx */

 mov r11, dst_reg */

 common case */

 push rcx */

 mov rcx, src_reg */

 shl %rax, %cl | shr %rax, %cl | sar %rax, %cl */

 pop rcx */

 mov dst_reg, r11 */

 Emit 'ror %ax, 8' to swap lower 2 bytes */

 Emit 'movzwl eax, ax' */

 Emit 'bswap eax' to swap lower 4 bytes */

 Emit 'bswap rax' to swap 8 bytes */

				/*

				 * Emit 'movzwl eax, ax' to zero extend 16-bit

				 * into 64 bit

 Emit 'mov eax, eax' to clear upper 32-bits */

 nop */

 speculation barrier */

 ST: *(u8*)(dst_reg + off) = imm */

 STX: *(u8*)(dst_reg + off) = src_reg */

 LDX: dst_reg = *(u8*)(src_reg + off) */

 test src_reg, src_reg */

 always 1 byte */

 jne start_of_ldx */

 xor dst_reg, dst_reg */

 jmp byte_after_ldx */

 populate jmp_offset for JNE above */

 sizeof(test + jne) */;

 populate jmp_offset for JMP above */

				/*

				 * Compute size of x86 insn and its target dest x86 register.

				 * ex_handler_bpf() will use lower 8 bits to adjust

				 * pt_regs->ip to jump over this x86 instruction

				 * and upper bits to figure out which pt_regs to zero out.

				 * End result: x86 insn "mov rbx, qword ptr [rax+0x14]"

				 * of 4 bytes will be ignored and rbx will be zero inited.

				/*

				 * Can't be implemented with a single x86 insn.

				 * Need to do a CMPXCHG loop.

 Will need RAX as a CMPXCHG operand so save R0 */

 Load old value */

				/*

				 * Perform the (commutative) operation locally,

				 * put the result in the AUX_REG.

 Attempt to swap in new value */

				/*

				 * ZF tells us whether we won the race. If it's

				 * cleared we need to try again.

 Return the pre-modification value */

 Restore R0 after clobbering RAX */

 call */

 cond jump */

 cmp dst_reg, src_reg */

 test dst_reg, src_reg */

 test dst_reg, imm32 */

 test dst_reg, dst_reg to save one extra byte */

 cmp dst_reg, imm8/32 */

 Convert BPF opcode to x86 */

 GT is unsigned '>', JA in x86 */

 LT is unsigned '<', JB in x86 */

 GE is unsigned '>=', JAE in x86 */

 LE is unsigned '<=', JBE in x86 */

 Signed '>', GT in x86 */

 Signed '<', LT in x86 */

 Signed '>=', GE in x86 */

 Signed '<=', LE in x86 */

 to silence GCC warning */

					/* To keep the jmp_offset valid, the extra bytes are

					 * padded before the jump insn, so we subtract the

					 * 2 bytes of jmp_cond insn from INSN_SZ_DIFF.

					 *

					 * If the previous pass already emits an imm8

					 * jmp_cond, then this BPF insn won't shrink, so

					 * "nops" is 0.

					 *

					 * On the other hand, if the previous pass emits an

					 * imm32 jmp_cond, the extra 4 bytes(*) is padded to

					 * keep the image from shrinking further.

					 *

					 * (*) imm32 jmp_cond is 6 bytes, and imm8 jmp_cond

					 *     is 2 bytes, so the size difference is 4 bytes.

				/* -1 jmp instructions will always jump

				 * backwards two bytes. Explicitly handling

				 * this case avoids wasting too many passes

				 * when there are long sequences of replaced

				 * dead code.

				/*

				 * If jmp_padding is enabled, the extra nops will

				 * be inserted. Otherwise, optimize out nop jumps.

					/* There are 3 possible conditions.

					 * (1) This BPF_JA is already optimized out in

					 *     the previous run, so there is no need

					 *     to pad any extra byte (0 byte).

					 * (2) The previous pass emits an imm8 jmp,

					 *     so we pad 2 bytes to match the previous

					 *     insn size.

					 * (3) Similarly, the previous pass emits an

					 *     imm32 jmp, and 5 bytes is padded.

					/* To avoid breaking jmp_offset, the extra bytes

					 * are padded before the actual jmp insn, so

					 * 2 bytes is subtracted from INSN_SZ_DIFF.

					 *

					 * If the previous pass already emits an imm8

					 * jmp, there is nothing to pad (0 byte).

					 *

					 * If it emits an imm32 jmp (5 bytes) previously

					 * and now an imm8 jmp (2 bytes), then we pad

					 * (5 - 2 = 3) bytes to stop the image from

					 * shrinking further.

 Update cleanup_addr */

 leave */

 ret */

			/*

			 * By design x86-64 JIT should support all BPF instructions.

			 * This error will be seen if new instruction was added

			 * to the interpreter, but not to the JIT, or if there is

			 * junk in bpf_prog.

			/*

			 * When populating the image, assert that:

			 *

			 *  i) We do not write beyond the allocated space, and

			 * ii) addrs[i] did not change from the prior run, in order

			 *     to validate assumptions made for computing branch

			 *     displacements.

	/* Store function arguments to stack.

	 * For a function that accepts two pointers the sequence will be:

	 * mov QWORD PTR [rbp-0x10],rdi

	 * mov QWORD PTR [rbp-0x8],rsi

	/* Restore function arguments from stack.

	 * For a function that accepts two pointers the sequence will be:

	 * EMIT4(0x48, 0x8B, 0x7D, 0xF0); mov rdi,QWORD PTR [rbp-0x10]

	 * EMIT4(0x48, 0x8B, 0x75, 0xF8); mov rsi,QWORD PTR [rbp-0x8]

 arg1: mov rdi, progs[i] */

 remember prog start time returned by __bpf_prog_enter */

	/* if (__bpf_prog_enter*(prog) == 0)

	 *	goto skip_exec_of_prog;

 test rax,rax */

 emit 2 nops that will be replaced with JE insn */

 arg1: lea rdi, [rbp - stack_size] */

 arg2: progs[i]->insnsi for interpreter */

 call JITed bpf program or interpreter */

	/*

	 * BPF_TRAMP_MODIFY_RETURN trampolines can modify the return

	 * of the previous call which is then passed on the stack to

	 * the next BPF program.

	 *

	 * BPF_TRAMP_FENTRY trampoline may need to return the return

	 * value of BPF_PROG_TYPE_STRUCT_OPS prog.

 replace 2 nops with JE insn, since jmp target is known */

 arg1: mov rdi, progs[i] */

 arg2: mov rsi, rbx <- start time in nsec */

	/* The first fmod_ret program will receive a garbage return value.

	 * Set this to 0 to avoid confusing the program.

		/* mod_ret prog stored return value into [rbp - 8]. Emit:

		 * if (*(u64 *)(rbp - 8) !=  0)

		 *	goto do_fexit;

 cmp QWORD PTR [rbp - 0x8], 0x0 */

		/* Save the location of the branch and Generate 6 nops

		 * (4 bytes for an offset and 2 bytes for the jump) These nops

		 * are replaced with a conditional jump once do_fexit (i.e. the

		 * start of the fexit invocation) is finalized.

	/*

	 * BPF_TRAMP_F_RET_FENTRY_RET is only used by bpf_struct_ops,

	 * and it must be used alone.

/* Example:

 * __be16 eth_type_trans(struct sk_buff *skb, struct net_device *dev);

 * its 'struct btf_func_model' will be nr_args=2

 * The assembly code when eth_type_trans is executing after trampoline:

 *

 * push rbp

 * mov rbp, rsp

 * sub rsp, 16                     // space for skb and dev

 * push rbx                        // temp regs to pass start time

 * mov qword ptr [rbp - 16], rdi   // save skb pointer to stack

 * mov qword ptr [rbp - 8], rsi    // save dev pointer to stack

 * call __bpf_prog_enter           // rcu_read_lock and preempt_disable

 * mov rbx, rax                    // remember start time in bpf stats are enabled

 * lea rdi, [rbp - 16]             // R1==ctx of bpf prog

 * call addr_of_jited_FENTRY_prog

 * movabsq rdi, 64bit_addr_of_struct_bpf_prog  // unused if bpf stats are off

 * mov rsi, rbx                    // prog start time

 * call __bpf_prog_exit            // rcu_read_unlock, preempt_enable and stats math

 * mov rdi, qword ptr [rbp - 16]   // restore skb pointer from stack

 * mov rsi, qword ptr [rbp - 8]    // restore dev pointer from stack

 * pop rbx

 * leave

 * ret

 *

 * eth_type_trans has 5 byte nop at the beginning. These 5 bytes will be

 * replaced with 'call generated_bpf_trampoline'. When it returns

 * eth_type_trans will continue executing with original skb and dev pointers.

 *

 * The assembly code when eth_type_trans is called from trampoline:

 *

 * push rbp

 * mov rbp, rsp

 * sub rsp, 24                     // space for skb, dev, return value

 * push rbx                        // temp regs to pass start time

 * mov qword ptr [rbp - 24], rdi   // save skb pointer to stack

 * mov qword ptr [rbp - 16], rsi   // save dev pointer to stack

 * call __bpf_prog_enter           // rcu_read_lock and preempt_disable

 * mov rbx, rax                    // remember start time if bpf stats are enabled

 * lea rdi, [rbp - 24]             // R1==ctx of bpf prog

 * call addr_of_jited_FENTRY_prog  // bpf prog can access skb and dev

 * movabsq rdi, 64bit_addr_of_struct_bpf_prog  // unused if bpf stats are off

 * mov rsi, rbx                    // prog start time

 * call __bpf_prog_exit            // rcu_read_unlock, preempt_enable and stats math

 * mov rdi, qword ptr [rbp - 24]   // restore skb pointer from stack

 * mov rsi, qword ptr [rbp - 16]   // restore dev pointer from stack

 * call eth_type_trans+5           // execute body of eth_type_trans

 * mov qword ptr [rbp - 8], rax    // save return value

 * call __bpf_prog_enter           // rcu_read_lock and preempt_disable

 * mov rbx, rax                    // remember start time in bpf stats are enabled

 * lea rdi, [rbp - 24]             // R1==ctx of bpf prog

 * call addr_of_jited_FEXIT_prog   // bpf prog can access skb, dev, return value

 * movabsq rdi, 64bit_addr_of_struct_bpf_prog  // unused if bpf stats are off

 * mov rsi, rbx                    // prog start time

 * call __bpf_prog_exit            // rcu_read_unlock, preempt_enable and stats math

 * mov rax, qword ptr [rbp - 8]    // restore eth_type_trans's return value

 * pop rbx

 * leave

 * add rsp, 8                      // skip eth_type_trans's frame

 * ret                             // return to its caller

 x86-64 supports up to 6 arguments. 7+ can be added in the future */

 room for return value of orig_call or fentry prog */

 room for IP address argument */

		/* skip patched call instruction and point orig_call to actual

		 * body of the kernel function.

 push rbp */

 mov rbp, rsp */

 sub rsp, stack_size */

 push rbx */

		/* Store IP address of the traced function:

		 * mov rax, QWORD PTR [rbp + 8]

		 * sub rax, X86_PATCH_SIZE

		 * mov QWORD PTR [rbp - stack_size], rax

		/* Continue with stack_size for regs storage, stack will

		 * be correctly restored with 'leave' instruction.

 arg1: mov rdi, im */

 call original function */

 remember return value in a stack for bpf prog to access */

		/* From Intel 64 and IA-32 Architectures Optimization

		 * Reference Manual, 3.4.1.4 Code Alignment, Assembly/Compiler

		 * Coding Rule 11: All branch targets should be 16-byte

		 * aligned.

		/* Update the branches saved in invoke_bpf_mod_ret with the

		 * aligned address of do_fexit.

	/* This needs to be done regardless. If there were fmod_ret programs,

	 * the return value is only updated on the stack and still needs to be

	 * restored to R0.

 arg1: mov rdi, im */

 restore return value of orig_call or fentry prog back into RAX */

 pop rbx */

 leave */

 skip our return address and return to parent */

 add rsp, 8 */

 ret */

 Make sure the trampoline generation logic doesn't overflow */

		/* Leaf node of recursion, i.e. not a range of indices

		 * anymore.

 cmp rdx,func */

 je func */

 rdx */, prog);

	/* Not a leaf node, so we pivot, and recursively descend into

	 * the lower and upper ranges.

 cmp rdx,func */

 jg upper_part */

 Require near jump. */

 emit lower_part */

	/* From Intel 64 and IA-32 Architectures Optimization

	 * Reference Manual, 3.4.1.4 Code Alignment, Assembly/Compiler

	 * Coding Rule 11: All branch targets should be 16-byte

	 * aligned.

 emit upper_part */

	/*

	 * If blinding was requested and we failed during blinding,

	 * we must fall back to the interpreter.

	/*

	 * Before first pass, make a rough estimation of addrs[]

	 * each BPF instruction is translated to less than 64 bytes

	/*

	 * JITed image shrinks with every pass and the loop iterates

	 * until the image stops shrinking. Very large BPF programs

	 * may converge on the last pass. In such case do one more

	 * pass to emit the final image.

			/*

			 * The number of entries in extable is the number of BPF_LDX

			 * insns that access kernel memory via "pointer to BTF type".

			 * The verifier changed their opcode from LDX|MEM|size

			 * to LDX|PROBE_MEM|size to make JITing easier.

 allocate module memory for x86 insns and extable */

 SPDX-License-Identifier: GPL-2.0

/*

 * Just-In-Time compiler for eBPF filters on IA32 (32bit x86)

 *

 * Author: Wang YanQing (udknight@gmail.com)

 * The code based on code and ideas from:

 * Eric Dumazet (eric.dumazet@gmail.com)

 * and from:

 * Shubham Bansal <illusionist.neo@gmail.com>

/*

 * eBPF prog stack layout:

 *

 *                         high

 * original ESP =>        +-----+

 *                        |     | callee saved registers

 *                        +-----+

 *                        | ... | eBPF JIT scratch space

 * BPF_FP,IA32_EBP  =>    +-----+

 *                        | ... | eBPF prog stack

 *                        +-----+

 *                        |RSVD | JIT scratchpad

 * current ESP =>         +-----+

 *                        |     |

 *                        | ... | Function call stack

 *                        |     |

 *                        +-----+

 *                          low

 *

 * The callee saved registers:

 *

 *                                high

 * original ESP =>        +------------------+ \

 *                        |        ebp       | |

 * current EBP =>         +------------------+ } callee saved registers

 *                        |    ebx,esi,edi   | |

 *                        +------------------+ /

 *                                low

 Tail Call Count */

/*

 * List of x86 cond jumps opcodes (. + s8)

 * Add 0x10 (and an extra 0x0f) to generate far jumps (. + s32)

/*

 * Map eBPF registers to IA32 32bit registers or stack scratch space.

 *

 * 1. All the registers, R0-R10, are mapped to scratch space on stack.

 * 2. We need two 64 bit temp registers to do complex operations on eBPF

 *    registers.

 * 3. For performance reason, the BPF_REG_AX for blinding constant, is

 *    mapped to real hardware register pair, IA32_ESI and IA32_EDI.

 *

 * As the eBPF registers are all 64 bit registers and IA32 has only 32 bit

 * registers, we have to map each eBPF registers with two IA32 32 bit regs

 * or scratch memory space and we have to build eBPF 64 bit register from those.

 *

 * We use IA32_EAX, IA32_EDX, IA32_ECX, IA32_EBX as temporary registers.

 Return value from in-kernel function, and exit value from eBPF */

 The arguments from eBPF program to in-kernel function */

 Stored on stack scratch space */

 Callee saved registers that in-kernel function will preserve */

 Stored on stack scratch space */

 Read only Frame Pointer to access Stack */

 Temporary register for blinding constants. */

 Tail call count. Stored on stack scratch space. */

/*

 * Stack space for BPF_REG_1, BPF_REG_2, BPF_REG_3, BPF_REG_4,

 * BPF_REG_5, BPF_REG_6, BPF_REG_7, BPF_REG_8, BPF_REG_9,

 * BPF_REG_FP, BPF_REG_AX and Tail call counts.

 Total stack size used in JITed code */

 Get the offset of eBPF REGISTERs stored on scratch space. */

 Encode 'dst_reg' register into IA32 opcode 'byte' */

 Encode 'dst_reg' and 'src_reg' registers into IA32 opcode 'byte' */

 Fill whole space with int3 instructions */

 xor eax,eax */

 mov dword ptr [ebp+off],eax */

 dst = imm (4 bytes)*/

 mov eax,dword ptr [ebp+off] */

 mov dword ptr [ebp+off],eax */

 mov dst,sreg */

 dst = src */

 complete 8 byte move */

 zero out high 4 bytes */

 Sign extended move */

/*

 * ALU operation (32 bit)

 * dst = dst * src

 mov ecx,dword ptr [ebp+off] */

 mov eax,dword ptr [ebp+off] */

 mov eax,dst */

 mov dword ptr [ebp+off],eax */

 mov dst,eax */

		/*

		 * Emit 'movzwl eax,ax' to zero extend 16-bit

		 * into 64 bit

 xor dreg_hi,dreg_hi */

 xor dreg_hi,dreg_hi */

 nop */

 mov dword ptr [ebp+off],dreg_lo */

 mov dword ptr [ebp+off],dreg_hi */

 Emit 'ror %ax, 8' to swap lower 2 bytes */

 xor dreg_hi,dreg_hi */

 Emit 'bswap eax' to swap lower 4 bytes */

 xor dreg_hi,dreg_hi */

 Emit 'bswap eax' to swap lower 4 bytes */

 Emit 'bswap edx' to swap lower 4 bytes */

 mov ecx,dreg_hi */

 mov dreg_hi,dreg_lo */

 mov dreg_lo,ecx */

 mov dword ptr [ebp+off],dreg_lo */

 mov dword ptr [ebp+off],dreg_hi */

/*

 * ALU operation (32 bit)

 * dst = dst (div|mod) src

 mov ecx,dword ptr [ebp+off] */

 mov ecx,src */

 mov eax,dword ptr [ebp+off] */

 mov eax,dst */

 xor edx,edx */

 div ecx */

/*

 * ALU operation (32 bit)

 * dst = dst (shift) src

 mov eax,dword ptr [ebp+off] */

 mov ecx,dword ptr [ebp+off] */

 mov ecx,src */

 mov dword ptr [ebp+off],dreg */

/*

 * ALU operation (32 bit)

 * dst = dst (op) src

 mov eax,dword ptr [ebp+off] */

 mov eax,dword ptr [ebp+off] */

 dst = dst + src */

 dst = dst - src */

 dst = dst | src */

 dst = dst & src */

 dst = dst ^ src */

 mov dword ptr [ebp+off],dreg */

 ALU operation (64 bit) */

/*

 * ALU operation (32 bit)

 * dst = dst (op) val

 mov eax,dword ptr [ebp+off] */

 mov edx,imm32*/

 dst = dst + val */

 dst = dst - val */

 dst = dst | val */

 dst = dst & val */

 dst = dst ^ val */

 mov dword ptr [ebp+off],dreg */

 ALU operation (64 bit) */

 dst = ~dst (64 bit) */

 neg dreg_lo */

 adc dreg_hi,0x0 */

 neg dreg_hi */

 mov dword ptr [ebp+off],dreg_lo */

 mov dword ptr [ebp+off],dreg_hi */

 dst = dst << src */

 mov ecx,dword ptr [ebp+off] */

 mov ecx,src_lo */

 shld dreg_hi,dreg_lo,cl */

 shl dreg_lo,cl */

 if ecx >= 32, mov dreg_lo into dreg_hi and clear dreg_lo */

 cmp ecx,32 */

 skip the next two instructions (4 bytes) when < 32 */

 mov dreg_hi,dreg_lo */

 xor dreg_lo,dreg_lo */

 mov dword ptr [ebp+off],dreg_lo */

 mov dword ptr [ebp+off],dreg_hi */

 out: */

 dst = dst >> src (signed)*/

 mov ecx,dword ptr [ebp+off] */

 mov ecx,src_lo */

 shrd dreg_lo,dreg_hi,cl */

 sar dreg_hi,cl */

 if ecx >= 32, mov dreg_hi to dreg_lo and set/clear dreg_hi depending on sign */

 cmp ecx,32 */

 skip the next two instructions (5 bytes) when < 32 */

 mov dreg_lo,dreg_hi */

 sar dreg_hi,31 */

 mov dword ptr [ebp+off],dreg_lo */

 mov dword ptr [ebp+off],dreg_hi */

 out: */

 dst = dst >> src */

 mov ecx,dword ptr [ebp+off] */

 mov ecx,src_lo */

 shrd dreg_lo,dreg_hi,cl */

 shr dreg_hi,cl */

 if ecx >= 32, mov dreg_hi to dreg_lo and clear dreg_hi */

 cmp ecx,32 */

 skip the next two instructions (4 bytes) when < 32 */

 mov dreg_lo,dreg_hi */

 xor dreg_hi,dreg_hi */

 mov dword ptr [ebp+off],dreg_lo */

 mov dword ptr [ebp+off],dreg_hi */

 out: */

 dst = dst << val */

 Do LSH operation */

 shld dreg_hi,dreg_lo,imm8 */

 shl dreg_lo,imm8 */

 shl dreg_lo,imm8 */

 mov dreg_hi,dreg_lo */

 xor dreg_lo,dreg_lo */

 xor dreg_lo,dreg_lo */

 xor dreg_hi,dreg_hi */

 mov dword ptr [ebp+off],dreg_lo */

 mov dword ptr [ebp+off],dreg_hi */

 dst = dst >> val */

 Do RSH operation */

 shrd dreg_lo,dreg_hi,imm8 */

 shr dreg_hi,imm8 */

 shr dreg_hi,imm8 */

 mov dreg_lo,dreg_hi */

 xor dreg_hi,dreg_hi */

 xor dreg_lo,dreg_lo */

 xor dreg_hi,dreg_hi */

 mov dword ptr [ebp+off],dreg_lo */

 mov dword ptr [ebp+off],dreg_hi */

 dst = dst >> val (signed) */

 Do RSH operation */

 shrd dreg_lo,dreg_hi,imm8 */

 ashr dreg_hi,imm8 */

 ashr dreg_hi,imm8 */

 mov dreg_lo,dreg_hi */

 ashr dreg_hi,imm8 */

 ashr dreg_hi,imm8 */

 mov dreg_lo,dreg_hi */

 mov dword ptr [ebp+off],dreg_lo */

 mov dword ptr [ebp+off],dreg_hi */

 mov eax,dword ptr [ebp+off] */

 mov eax,dst_hi */

 mul dword ptr [ebp+off] */

 mul src_lo */

 mov ecx,eax */

 mov eax,dword ptr [ebp+off] */

 mov eax,dst_lo */

 mul dword ptr [ebp+off] */

 mul src_hi */

 add eax,eax */

 mov eax,dword ptr [ebp+off] */

 mov eax,dst_lo */

 mul dword ptr [ebp+off] */

 mul src_lo */

 add ecx,edx */

 mov dword ptr [ebp+off],eax */

 mov dword ptr [ebp+off],ecx */

 mov dst_lo,eax */

 mov dst_hi,ecx */

 movl eax,imm32 */

 mul dword ptr [ebp+off] */

 mul dst_hi */

 mov ecx,eax */

 movl eax,imm32 */

 mul dword ptr [ebp+off] */

 mul dst_lo */

 add ecx,eax */

 movl eax,imm32 */

 mul dword ptr [ebp+off] */

 mul dst_lo */

 add ecx,edx */

 mov dword ptr [ebp+off],eax */

 mov dword ptr [ebp+off],ecx */

 mov dword ptr [ebp+off],eax */

 mov dword ptr [ebp+off],ecx */

 imm32 */

 Epilogue code offset */

 Maximum number of bytes emitted while JITing one eBPF insn */

/*

 * Emit prologue code for BPF program and check it's size.

 * bpf_tail_call helper will skip it while jumping into another program.

 push ebp */

 mov ebp,esp */

 push edi */

 push esi */

 push ebx */

 sub esp,STACK_SIZE */

 sub ebp,SCRATCH_SIZE+12*/

 xor ebx,ebx */

 Set up BPF prog stack base register */

 Move BPF_CTX (EAX) to BPF_REG_R1 */

 mov dword ptr [ebp+off],eax */

 Initialize Tail Count */

 Emit epilogue code for BPF program */

 mov eax,dword ptr [ebp+off]*/

 mov edx,dword ptr [ebp+off]*/

 add ebp,SCRATCH_SIZE+12*/

 mov ebx,dword ptr [ebp-12]*/

 mov esi,dword ptr [ebp-8]*/

 mov edi,dword ptr [ebp-4]*/

 leave */

 ret */

/*

 * Generate the following code:

 * ... bpf_tail_call(void *ctx, struct bpf_array *array, u64 index) ...

 *   if (index >= array->map.max_entries)

 *     goto out;

 *   if (++tail_call_cnt > MAX_TAIL_CALL_CNT)

 *     goto out;

 *   prog = array->ptrs[index];

 *   if (prog == NULL)

 *     goto out;

 *   goto *(prog->bpf_func + prologue_size);

 * out:

	/*

	 * if (index >= array->map.max_entries)

	 *     goto out;

 mov eax,dword ptr [ebp+off] */

 mov edx,dword ptr [ebp+off] */

 cmp dword ptr [eax+off],edx */

 jbe out */

	/*

	 * if (tail_call_cnt > MAX_TAIL_CALL_CNT)

	 *     goto out;

 cmp edx,hi */

 cmp ecx,lo */

 ja out */

 add eax,0x1 */

 adc ebx,0x0 */

 mov dword ptr [ebp+off],eax */

 mov dword ptr [ebp+off],edx */

 prog = array->ptrs[index]; */

 mov edx, [eax + edx * 4 + offsetof(...)] */

	/*

	 * if (prog == NULL)

	 *     goto out;

 test edx,edx */

 je out */

 goto *(prog->bpf_func + prologue_size); */

 mov edx, dword ptr [edx + 32] */

 add edx,prologue_size */

 mov eax,dword ptr [ebp+off] */

	/*

	 * Now we're ready to jump into next BPF program:

	 * eax == ctx (1st arg)

	 * edx == prog->bpf_func + prologue_size

 out: */

 Push the scratch stack register on top of the stack. */

 mov ecx,dword ptr [ebp+off] */

 push ecx */

 mov ecx,dword ptr [ebp+off] */

 push ecx */

 mov ecx,dword ptr [ebp+off] */

 push ecx */

 Convert BPF opcode to x86 */

 GT is unsigned '>', JA in x86 */

 LT is unsigned '<', JB in x86 */

 GE is unsigned '>=', JAE in x86 */

 LE is unsigned '<=', JBE in x86 */

 Signed '>', GT in x86 */

 GT is unsigned '>', JA in x86 */

 Signed '<', LT in x86 */

 LT is unsigned '<', JB in x86 */

 Signed '>=', GE in x86 */

 GE is unsigned '>=', JAE in x86 */

 Signed '<=', LE in x86 */

 LE is unsigned '<=', JBE in x86 */

 to silence GCC warning */

/* i386 kernel compiles with "-mregparm=3".  From gcc document:

 *

 * ==== snippet ====

 * regparm (number)

 *	On x86-32 targets, the regparm attribute causes the compiler

 *	to pass arguments number one to (number) if they are of integral

 *	type in registers EAX, EDX, and ECX instead of on the stack.

 *	Functions that take a variable number of arguments continue

 *	to be passed all of their arguments on the stack.

 * ==== snippet ====

 *

 * The first three args of a function will be considered for

 * putting into the 32bit register EAX, EDX, and ECX.

 *

 * Two 32bit registers are used to pass a 64bit arg.

 *

 * For example,

 * void foo(u32 a, u32 b, u32 c, u32 d):

 *	u32 a: EAX

 *	u32 b: EDX

 *	u32 c: ECX

 *	u32 d: stack

 *

 * void foo(u64 a, u32 b, u32 c):

 *	u64 a: EAX (lo32) EDX (hi32)

 *	u32 b: ECX

 *	u32 c: stack

 *

 * void foo(u32 a, u64 b, u32 c):

 *	u32 a: EAX

 *	u64 b: EDX (lo32) ECX (hi32)

 *	u32 c: stack

 *

 * void foo(u32 a, u32 b, u64 c):

 *	u32 a: EAX

 *	u32 b: EDX

 *	u64 c: stack

 *

 * The return value will be stored in the EAX (and EDX for 64bit value).

 *

 * For example,

 * u32 foo(u32 a, u32 b, u32 c):

 *	return value: EAX

 *

 * u64 foo(u32 a, u32 b, u32 c):

 *	return value: EAX (lo32) EDX (hi32)

 *

 * Notes:

 *	The verifier only accepts function having integer and pointers

 *	as its args and return value, so it does not have

 *	struct-by-value.

 *

 * emit_kfunc_call() finds out the btf_func_model by calling

 * bpf_jit_find_kfunc_model().  A btf_func_model

 * has the details about the number of args, size of each arg,

 * and the size of the return value.

 *

 * It first decides how many args can be passed by EAX, EDX, and ECX.

 * That will decide what args should be pushed to the stack:

 * [first_stack_regno, last_stack_regno] are the bpf regnos

 * that should be pushed to the stack.

 *

 * It will first push all args to the stack because the push

 * will need to use ECX.  Then, it moves

 * [BPF_REG_1, first_stack_regno) to EAX, EDX, and ECX.

 *

 * When emitting a call (0xE8), it needs to figure out

 * the jmp_offset relative to the jit-insn address immediately

 * following the call (0xE8) instruction.  At this point, it knows

 * the end of the jit-insn address after completely translated the

 * current (BPF_JMP | BPF_CALL) bpf-insn.  It is passed as "end_addr"

 * to the emit_kfunc_call().  Thus, it can learn the "immediate-follow-call"

 * address by figuring out how many jit-insn is generated between

 * the call (0xE8) and the end_addr:

 *	- 0-1 jit-insn (3 bytes each) to restore the esp pointer if there

 *	  is arg pushed to the stack.

 *	- 0-2 jit-insns (3 bytes each) to handle the return value.

 Push the args to the stack */

 mov e[adc]x,dword ptr [ebp+off] */

 mov e[adc]x,dword ptr [ebp+off] */

 add esp,"bytes_in_stack" */

 mov dword ptr [ebp+off],edx */

 mov dword ptr [ebp+off],eax */

 mov dword ptr [ebp+off],eax */

 mov dword ptr [ebp+off],edx */

 add esp,"bytes_in_stack" */

 ALU operations */

 dst = src */

 Special mov32 for zext. */

 Sign-extend immediate value to dst reg */

 dst = dst + src/imm */

 dst = dst - src/imm */

 dst = dst | src/imm */

 dst = dst & src/imm */

 dst = dst ^ src/imm */

 dst = dst * src/imm */

 dst = dst << src */

 dst = dst >> src */

 mov ecx,imm32*/

 mov ecx,imm32*/

 dst = dst / src(imm) */

 dst = dst % src(imm) */

 mov ecx,imm32*/

 dst = dst >> imm */

 dst = dst << imm */

 mov ecx,imm32*/

 dst = dst << imm */

 dst = dst >> imm */

 dst = dst << src */

 dst = dst >> src */

 dst = dst >> src (signed) */

 dst = dst >> imm (signed) */

 dst = ~dst */

 dst = ~dst (64 bit) */

 dst = dst * src/imm */

 dst = htole(dst) */

 dst = htobe(dst) */

 dst = imm64 */

 speculation barrier */

 Emit 'lfence' */

 ST: *(u8*)(dst_reg + off) = imm */

 mov eax,dword ptr [ebp+off] */

 mov eax,dst_lo */

 STX: *(u8*)(dst_reg + off) = src_reg */

 mov eax,dword ptr [ebp+off] */

 mov eax,dst_lo */

 mov edx,dword ptr [ebp+off] */

 mov edx,src_lo */

 mov edi,dword ptr [ebp+off] */

 mov edi,src_hi */

 LDX: dst_reg = *(u8*)(src_reg + off) */

 mov eax,dword ptr [ebp+off] */

 mov eax,dword ptr [ebp+off] */

 mov dword ptr [ebp+off],edx */

 mov dst_lo,edx */

 xor dst_hi,dst_hi */

 call */

 mov eax,dword ptr [ebp+off] */

 mov edx,dword ptr [ebp+off] */

 mov dword ptr [ebp+off],eax */

 mov dword ptr [ebp+off],edx */

 add esp,32 */

 cond jump */

 cmp dreg_hi,sreg_hi */

 cmp dreg_lo,sreg_lo */

 cmp dreg_hi,sreg_hi */

 cmp dreg_lo,sreg_lo */

 mov dreg_lo,dst_lo */

 mov dreg_hi,dst_hi */

 and dreg_lo,sreg_lo */

 and dreg_hi,sreg_hi */

 or dreg_lo,dreg_hi */

 mov dreg_lo,dst_lo */

 mov dreg_hi,dst_hi */

 mov ecx,imm32 */

 and dreg_lo,sreg_lo */

 mov ebx,imm32 */

 and dreg_hi,sreg_hi */

 or dreg_lo,dreg_hi */

 mov ecx,imm32 */

 mov ebx,imm32 */

 cmp dreg_hi,sreg_hi */

 cmp dreg_lo,sreg_lo */

 mov ecx,imm32 */

 mov ebx,imm32 */

 cmp dreg_hi,sreg_hi */

 cmp dreg_lo,sreg_lo */

			/*

			 * For simplicity of branch offset computation,

			 * let's use fixed jump coding here.

 Check the condition for low 32-bit comparison */

 Check the condition for high 32-bit comparison */

				/* -1 jmp instructions will always jump

				 * backwards two bytes. Explicitly handling

				 * this case avoids wasting too many passes

				 * when there are long sequences of replaced

				 * dead code.

 Optimize out nop jumps */

 Update cleanup_addr */

			/*

			 * This error will be seen if new instruction was added

			 * to interpreter, but not to JIT or if there is junk in

			 * bpf_prog

			/*

			 * When populating the image, assert that:

			 *

			 *  i) We do not write beyond the allocated space, and

			 * ii) addrs[i] did not change from the prior run, in order

			 *     to validate assumptions made for computing branch

			 *     displacements.

	/*

	 * If blinding was requested and we failed during blinding,

	 * we must fall back to the interpreter.

	/*

	 * Before first pass, make a rough estimation of addrs[]

	 * each BPF instruction is translated to less than 64 bytes

	/*

	 * JITed image shrinks with every pass and the loop iterates

	 * until the image stops shrinking. Very large BPF programs

	 * may converge on the last pass. In such case do one more

	 * pass to emit the final image.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * purgatory: Runs between two kernels

 *

 * Copyright (C) 2014 Red Hat Inc.

 *

 * Author:

 *       Vivek Goyal <vgoyal@redhat.com>

 loop forever */

/*

 * Defined in order to reuse memcpy() and memset() from

 * arch/x86/boot/compressed/string.c

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel CE4100  platform specific setup code

 *

 * (C) Copyright 2010 Intel Corporation

/*

 * The CE4100 platform has an internal 8051 Microcontroller which is

 * responsible for signaling to the external Power Management Unit the

 * intention to reset, reboot or power off the system. This 8051 device has

 * its command register mapped at I/O port 0xcf9 and the value 0x4 is used

 * to power off the system.

/*

 * The UART Tx interrupts are not set under some conditions and therefore serial

 * transmission hangs. This is a silicon issue and has not been root caused. The

 * workaround for this silicon issue checks UART_LSR_THRE bit and UART_LSR_TEMT

 * bit of LSR register in interrupt handler to see whether at least one of these

 * two bits is set, if so then process the transmit request. If this workaround

 * is not applied, then the serial transmission may hang. This workaround is for

 * errata number 9 in Errata - B step.

 see if the TX interrupt should have really set */

 see if the UART's XMIT interrupt is enabled */

				/* now check to see if the UART should be

	/*

	 * Over ride the legacy port configuration that comes from

	 * asm/serial.h. Using the ioport driver then switching to the

	 * PCI memmaped driver hangs the IOAPIC

/*

 * CE4100 specific x86_init function overrides and early setup

 * calls.

	/*

	 * By default, the reboot method is ACPI which is supported by the

	 * CE4100 bootloader CEFDK using FADT.ResetReg Address and ResetValue

	 * the bootloader will however issue a system power off instead of

	 * reboot. By using BOOT_KBD we ensure proper system reboot as

	 * expected.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Eurobraille/Iris power off support.

 *

 * Eurobraille's Iris machine is a PC with no APM or ACPI support.

 * It is shutdown by a special I/O sequence which this module provides.

 *

 *  Copyright (C) Shérab <Sebastien.Hinderer@ens-lyon.org>

 First byte to send */

 Second byte to send */

 Likely not an Iris */

/*

 * Before installing the power_off handler, try to make sure the OS is

 * running on an Iris.  Since Iris does not support DMI, this is done

 * by reading its input port and seeing whether the read value is

 * meaningful.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * System Specific setup for Traverse Technologies GEOS.

 * At the moment this means setup of GPIO control of LEDs.

 *

 * Copyright (C) 2008 Constantin Baranov <const@mimas.ru>

 * Copyright (C) 2011 Ed Wildgoose <kernel@wildgooses.com>

 *                and Philip Prindeville <philipp@redfish-solutions.com>

 *

 * TODO: There are large similarities with leds-net5501.c

 * by Alessandro Zummo <a.zummo@towertech.it>

 * In the future leds-net5501.c should be migrated over to platform

 The Geode GPIOs should be on the CS5535 companion chip */

 Setup LED control through leds-gpio driver */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * System Specific setup for PCEngines ALIX.

 * At the moment this means setup of GPIO control of LEDs

 * on Alix.2/3/6 boards.

 *

 * Copyright (C) 2008 Constantin Baranov <const@mimas.ru>

 * Copyright (C) 2011 Ed Wildgoose <kernel@wildgooses.com>

 *                and Philip Prindeville <philipp@redfish-solutions.com>

 *

 * TODO: There are large similarities with leds-net5501.c

 * by Alessandro Zummo <a.zummo@towertech.it>

 * In the future leds-net5501.c should be migrated over to platform

/*

 * This driver is not modular, but to keep back compatibility

 * with existing use cases, continuing with module_param is

 * the easiest way forward.

 FIXME: Award bios is not automatically detected as Alix platform */

 The Geode GPIOs should be on the CS5535 companion chip */

 Setup LED control through leds-gpio driver */

 remove the first \0 character from string */

 cut the string at a newline */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * System Specific setup for Soekris net5501

 * At the moment this means setup of GPIO control of LEDs and buttons

 * on net5501 boards.

 *

 * Copyright (C) 2008-2009 Tower Technologies

 * Written by Alessandro Zummo <a.zummo@towertech.it>

 *

 * Copyright (C) 2008 Constantin Baranov <const@mimas.ru>

 * Copyright (C) 2011 Ed Wildgoose <kernel@wildgooses.com>

 *                and Philip Prindeville <philipp@redfish-solutions.com>

 The Geode GPIOs should be on the CS5535 companion chip */

 Setup LED control through leds-gpio driver */

 net5501 v1.33/1.33c */

 net5501 v1.32i */

 null terminated */

 SPDX-License-Identifier: GPL-2.0

/*

 * Common EFI (Extensible Firmware Interface) support functions

 * Based on Extensible Firmware Interface Specification version 1.0

 *

 * Copyright (C) 1999 VA Linux Systems

 * Copyright (C) 1999 Walt Drummond <drummond@valinux.com>

 * Copyright (C) 1999-2002 Hewlett-Packard Co.

 *	David Mosberger-Tang <davidm@hpl.hp.com>

 *	Stephane Eranian <eranian@hpl.hp.com>

 * Copyright (C) 2005-2008 Intel Co.

 *	Fenghua Yu <fenghua.yu@intel.com>

 *	Bibo Mao <bibo.mao@intel.com>

 *	Chandramouli Narayanan <mouli@linux.intel.com>

 *	Huang Ying <ying.huang@intel.com>

 * Copyright (C) 2013 SuSE Labs

 *	Borislav Petkov <bp@suse.de> - runtime services VA mapping

 *

 * Copied from efi_32.c to eliminate the duplicated code between EFI

 * 32/64 support code. --ying 2007-10-26

 *

 * All EFI Runtime Services are not implemented yet as EFI only

 * supports physical mode addressing on SoftSDV. This is to be fixed

 * in a future version.  --drummond 1999-07-20

 *

 * Implemented EFI runtime services and virtual mode calls.  --davidm

 *

 * Goutham Rao: <goutham.rao@intel.com>

 *	Skip non-WB memory and ignore empty memory ranges.

 efi setup_data physical address */

/*

 * Tell the kernel about the EFI memory map.  This might include

 * more than the max 128 entries that can fit in the passed in e820

 * legacy (zeropage) memory map, but the kernel's e820 table can hold

 * E820_MAX_ENTRIES.

			/*

			 * EFI_RESERVED_TYPE EFI_RUNTIME_SERVICES_CODE

			 * EFI_RUNTIME_SERVICES_DATA EFI_MEMORY_MAPPED_IO

			 * EFI_MEMORY_MAPPED_IO_PORT_SPACE EFI_PAL_CODE

/*

 * Given add_efi_memmap defaults to 0 and there there is no alternative

 * e820 mechanism for soft-reserved memory, import the full EFI memory

 * map if soft reservations are present and enabled. Otherwise, the

 * mechanism to disable the kernel's consideration of EFI_MEMORY_SP is

 * the efi=nosoftreserve option.

 Can't handle firmware tables above 4GB on i386 */

	/*

	 * Let's see what config tables the firmware passed to us.

	/*

	 * Note: We currently don't support runtime services on an EFI

	 * that doesn't match the kernel 32/64-bit mode.

 Parse the EFI Properties table if it exists */

 Merge contiguous regions of the same type and attribute */

	/*

	 * A first-time allocation doesn't have anything to copy.

/*

 * Iterate the EFI memory map in reverse order because the regions

 * will be mapped top-down. The end result is the same as if we had

 * mapped things forward, but doesn't require us to change the

 * existing implementation of efi_map_region().

 Initial call */

/*

 * efi_map_next_entry - Return the next EFI memory map descriptor

 * @entry: Previous EFI memory map descriptor

 *

 * This is a helper function to iterate over the EFI memory map, which

 * we do in different orders depending on the current configuration.

 *

 * To begin traversing the memory map @entry must be %NULL.

 *

 * Returns %NULL when we reach the end of the memory map.

		/*

		 * Starting in UEFI v2.5 the EFI_PROPERTIES_TABLE

		 * config table feature requires us to map all entries

		 * in the same order as they appear in the EFI memory

		 * map. That is to say, entry N must have a lower

		 * virtual address than entry N+1. This is because the

		 * firmware toolchain leaves relative references in

		 * the code/data sections, which are split and become

		 * separate EFI memory regions. Mapping things

		 * out-of-order leads to the firmware accessing

		 * unmapped addresses.

		 *

		 * Since we need to map things this way whether or not

		 * the kernel actually makes use of

		 * EFI_PROPERTIES_TABLE, let's just switch to this

		 * scheme by default for 64-bit.

 Initial call */

	/*

	 * Runtime regions always require runtime mappings (obviously).

	/*

	 * 32-bit EFI doesn't suffer from the bug that requires us to

	 * reserve boot services regions, and mixed mode support

	 * doesn't exist for 32-bit kernels.

	/*

	 * EFI specific purpose memory may be reserved by default

	 * depending on kernel config and boot options.

	/*

	 * Map all of RAM so that we can access arguments in the 1:1

	 * mapping when making EFI runtime calls.

	/*

	 * Map boot services regions as a workaround for buggy

	 * firmware that accesses them even when they shouldn't.

	 *

	 * See efi_{reserve,free}_boot_services().

/*

 * Map the efi memory ranges of the runtime services and update new_mmap with

 * virtual addresses.

	/*

	 * We don't do virtual mode, since we don't do runtime services, on

	 * non-native EFI.

	/*

	* Map efi regions which were passed via setup_data. The virt_addr is a

	* fixed addr which was used in first kernel of a kexec boot.

 FIXME: add error handling */

	/*

	 * Unregister the early EFI memmap from efi_init() and install

	 * the new EFI memory map.

/*

 * This function will switch the EFI runtime services to virtual mode.

 * Essentially, we look through the EFI memmap and map every region that

 * has the runtime attribute bit set in its memory descriptor into the

 * efi_pgd page table.

 *

 * The new method does a pagetable switch in a preemption-safe manner

 * so that we're in a different address space when calling a runtime

 * function. For function arguments passing we do copy the PUDs of the

 * kernel page table into efi_pgd prior to each call.

 *

 * Specially for kexec boot, efi runtime maps in previous kernel should

 * be passed in via setup_data. In that case runtime ranges will be mapped

 * to the same virtual addresses as the first kernel, see

 * kexec_enter_virtual_mode().

	/*

	 * Unregister the early EFI memmap from efi_init() and install

	 * the new EFI memory map that we are about to pass to the

	 * firmware via SetVirtualAddressMap().

	/*

	 * Apply more restrictive page table mapping attributes now that

	 * SVAM() has been called and the firmware has performed all

	 * necessary relocation fixups for the new virtual addresses.

 clean DUMMY object */

 SPDX-License-Identifier: GPL-2.0

/*

 * Extensible Firmware Interface

 *

 * Based on Extensible Firmware Interface Specification version 1.0

 *

 * Copyright (C) 1999 VA Linux Systems

 * Copyright (C) 1999 Walt Drummond <drummond@valinux.com>

 * Copyright (C) 1999-2002 Hewlett-Packard Co.

 *	David Mosberger-Tang <davidm@hpl.hp.com>

 *	Stephane Eranian <eranian@hpl.hp.com>

 *

 * All EFI Runtime Services are not implemented yet as EFI only

 * supports physical mode addressing on SoftSDV. This is to be fixed

 * in a future version.  --drummond 1999-07-20

 *

 * Implemented EFI runtime services and virtual mode calls.  --davidm

 *

 * Goutham Rao: <goutham.rao@intel.com>

 *	Skip non-WB memory and ignore empty memory ranges.

/*

 * To make EFI call EFI runtime service in physical addressing mode we need

 * prolog/epilog before/after the invocation to claim the EFI runtime service

 * handler exclusively and to duplicate a memory mapping in low memory space,

 * say 0 - 3G.

 Current pgd is swapper_pg_dir, we'll restore it later: */

 Disable interrupts around EFI calls: */

 Make EFI runtime service code area executable */

 SPDX-License-Identifier: GPL-2.0-only

 _CSH */

/*

 * Header prepended to the standard EFI capsule on Quark systems the are based

 * on Intel firmware BSP.

 * @csh_signature:	Unique identifier to sanity check signed module

 * 			presence ("_CSH").

 * @version:		Current version of CSH used. Should be one for Quark A0.

 * @modulesize:		Size of the entire module including the module header

 * 			and payload.

 * @security_version_number_index: Index of SVN to use for validation of signed

 * 			module.

 * @security_version_number: Used to prevent against roll back of modules.

 * @rsvd_module_id:	Currently unused for Clanton (Quark).

 * @rsvd_module_vendor:	Vendor Identifier. For Intel products value is

 * 			0x00008086.

 * @rsvd_date:		BCD representation of build date as yyyymmdd, where

 * 			yyyy=4 digit year, mm=1-12, dd=1-31.

 * @headersize:		Total length of the header including including any

 * 			padding optionally added by the signing tool.

 * @hash_algo:		What Hash is used in the module signing.

 * @cryp_algo:		What Crypto is used in the module signing.

 * @keysize:		Total length of the key data including including any

 * 			padding optionally added by the signing tool.

 * @signaturesize:	Total length of the signature including including any

 * 			padding optionally added by the signing tool.

 * @rsvd_next_header:	32-bit pointer to the next Secure Boot Module in the

 * 			chain, if there is a next header.

 * @rsvd:		Reserved, padding structure to required size.

 *

 * See also QuartSecurityHeader_t in

 * Quark_EDKII_v1.2.1.1/QuarkPlatformPkg/Include/QuarkBootRom.h

 * from https://downloadcenter.intel.com/download/23197/Intel-Quark-SoC-X1000-Board-Support-Package-BSP

/*

 * Some firmware implementations refuse to boot if there's insufficient

 * space in the variable store. The implementation of garbage collection

 * in some FW versions causes stale (deleted) variables to take up space

 * longer than intended and space is only freed once the store becomes

 * almost completely full.

 *

 * Enabling this option disables the space checks in

 * efi_query_variable_store() and forces garbage collection.

 *

 * Only enable this option if deleting EFI variables does not free up

 * space in your variable store, e.g. if despite deleting variables

 * you're unable to create new ones.

/*

 * Deleting the dummy variable which kicks off garbage collection

/*

 * In the nonblocking case we do not attempt to perform garbage

 * collection if we do not have enough free space. Rather, we do the

 * bare minimum check and give up immediately if the available space

 * is below EFI_MIN_RESERVE.

 *

 * This function is intended to be small and simple because it is

 * invoked from crash handler paths.

/*

 * Some firmware implementations refuse to boot if there's insufficient space

 * in the variable store. Ensure that we never use more than a safe limit.

 *

 * Return EFI_SUCCESS if it is safe to write 'size' bytes to the variable

 * store.

	/*

	 * We account for that by refusing the write if permitting it would

	 * reduce the available space to under 5KB. This figure was provided by

	 * Samsung, so should be safe.

		/*

		 * Triggering garbage collection may require that the firmware

		 * generate a real EFI_OUT_OF_RESOURCES error. We can force

		 * that by attempting to use more space than is available.

			/*

			 * This should have failed, so if it didn't make sure

			 * that we delete it...

		/*

		 * The runtime code may now have triggered a garbage collection

		 * run, so check the variable info again

		/*

		 * There still isn't enough room, so return an error

/*

 * The UEFI specification makes it clear that the operating system is

 * free to do whatever it wants with boot services code after

 * ExitBootServices() has been called. Ignoring this recommendation a

 * significant bunch of EFI implementations continue calling into boot

 * services code (SetVirtualAddressMap). In order to work around such

 * buggy implementations we reserve boot services region during EFI

 * init and make sure it stays executable. Then, after

 * SetVirtualAddressMap(), it is discarded.

 *

 * However, some boot services regions contain data that is required

 * by drivers, so we need to track which memory ranges can never be

 * freed. This is done by tagging those regions with the

 * EFI_MEMORY_RUNTIME attribute.

 *

 * Any driver that wants to mark a region as reserved must use

 * efi_mem_reserve() which will insert a new EFI memory descriptor

 * into efi.memmap (splitting existing regions if necessary) and tag

 * it with EFI_MEMORY_RUNTIME.

/*

 * Helper function for efi_reserve_boot_services() to figure out if we

 * can free regions in efi_free_boot_services().

 *

 * Use this function to ensure we do not free regions owned by somebody

 * else. We must only reserve (and then free) regions:

 *

 * - Not within any part of the kernel

 * - Not the BIOS reserved area (E820_TYPE_RESERVED, E820_TYPE_NVS, etc)

		/*

		 * Because the following memblock_reserve() is paired

		 * with memblock_free_late() for this region in

		 * efi_free_boot_services(), we must be extremely

		 * careful not to reserve, and subsequently free,

		 * critical regions of memory (like the kernel image) or

		 * those regions that somebody else has already

		 * reserved.

		 *

		 * A good example of a critical region that must not be

		 * freed is page zero (first 4Kb of memory), which may

		 * contain boot services code/data but is marked

		 * E820_TYPE_RESERVED by trim_bios_range().

			/*

			 * If we are the first to reserve the region, no

			 * one else cares about it. We own it and can

			 * free it later.

		/*

		 * We don't own the region. We must not free it.

		 *

		 * Setting this bit for a boot services region really

		 * doesn't make sense as far as the firmware is

		 * concerned, but it does provide us with a way to tag

		 * those regions that must not be paired with

		 * memblock_free_late().

/*

 * Apart from having VA mappings for EFI boot services code/data regions,

 * (duplicate) 1:1 mappings were also created as a quirk for buggy firmware. So,

 * unmap both 1:1 and VA mappings.

	/*

	 * EFI mixed mode has all RAM mapped to access arguments while making

	 * EFI runtime calls, hence don't unmap EFI boot services code/data

	 * regions.

 Keep all regions for /sys/kernel/debug/efi */

 Do not free, someone else owns it: */

		/*

		 * Before calling set_virtual_address_map(), EFI boot services

		 * code/data regions were mapped as a quirk for buggy firmware.

		 * Unmap them from efi_pgd before freeing them up.

		/*

		 * Nasty quirk: if all sub-1MB memory is used for boot

		 * services, we can get here without having allocated the

		 * real mode trampoline.  It's too late to hand boot services

		 * memory back to the memblock allocator, so instead

		 * try to manually allocate the trampoline if needed.

		 *

		 * I've seen this on a Dell XPS 13 9350 with firmware

		 * 1.4.4 with SGX enabled booting Linux via Fedora 24's

		 * grub2-efi on a hard disk.  (And no, I don't know why

		 * this happened, but Linux should still try to boot rather

		 * panicking early.)

		/*

		 * Don't free memory under 1M for two reasons:

		 * - BIOS might clobber it

		 * - Crash kernel needs it to be reserved

	/*

	 * Build a new EFI memmap that excludes any boot services

	 * regions that are not tagged EFI_MEMORY_RUNTIME, since those

	 * regions have now been freed.

/*

 * A number of config table entries get remapped to virtual addresses

 * after entering EFI virtual mode. However, the kexec kernel requires

 * their physical addresses therefore we pass them via setup_data and

 * correct those entries to their respective physical addresses here.

 *

 * Currently only handles smbios which is necessary for some firmware

 * implementation.

	/*

	 * Once setup is done earlier, unmap the EFI memory map on mismatched

	 * firmware/kernel architectures since there is no support for runtime

	 * services.

/*

 * For most modern platforms the preferred method of powering off is via

 * ACPI. However, there are some that are known to require the use of

 * EFI runtime services and for which ACPI does not work at all.

 *

 * Using EFI is a last resort, to be used only if no other option

 * exists.

 Only process data block that is larger than the security header */

 Only process data block if EFI header is included */

	/*

	 * Update the first page pointer to skip over the CSH header.

	/*

	 * cap_info->capsule should point at a virtual mapping of the entire

	 * capsule, starting at the capsule header. Our image has the Quark

	 * security header prepended, so we cannot rely on the default vmap()

	 * mapping created by the generic capsule code.

	 * Given that the Quark firmware does not appear to care about the

	 * virtual mapping, let's just point cap_info->capsule at our copy

	 * of the capsule header.

		/*

		 * The quirk handler is supposed to return

		 *  - a value > 0 if the setup should continue, after advancing

		 *    kbuff as needed

		 *  - 0 if not enough hdr_bytes are available yet

		 *  - a negative error code otherwise

/*

 * If any access by any efi runtime service causes a page fault, then,

 * 1. If it's efi_reset_system(), reboot through BIOS.

 * 2. If any other efi runtime service, then

 *    a. Return error status to the efi caller process.

 *    b. Disable EFI Runtime Services forever and

 *    c. Freeze efi_rts_wq and schedule new process.

 *

 * @return: Returns, if the page fault is not handled. This function

 * will never return if the page fault is handled successfully.

	/*

	 * If we get an interrupt/NMI while processing an EFI runtime service

	 * then this is a regular OOPS, not an EFI failure.

	/*

	 * Make sure that an efi runtime service caused the page fault.

	 * READ_ONCE() because we might be OOPSing in a different thread,

	 * and we don't want to trip KTSAN while trying to OOPS.

	/*

	 * Address range 0x0000 - 0x0fff is always mapped in the efi_pgd, so

	 * page faulting on these addresses isn't expected.

	/*

	 * Print stack trace as it might be useful to know which EFI Runtime

	 * Service is buggy.

	/*

	 * Buggy efi_reset_system() is handled differently from other EFI

	 * Runtime Services as it doesn't use efi_rts_wq. Although,

	 * native_machine_emergency_restart() says that machine_real_restart()

	 * could fail, it's better not to complicate this fault handler

	 * because this case occurs *very* rarely and hence could be improved

	 * on a need by basis.

	/*

	 * Before calling EFI Runtime Service, the kernel has switched the

	 * calling process to efi_mm. Hence, switch back to task_mm.

 Signal error status to the efi caller process */

	/*

	 * Call schedule() in an infinite loop, so that any spurious wake ups

	 * will never run efi_rts_wq again.

 SPDX-License-Identifier: GPL-2.0

/*

 * x86_64 specific EFI support functions

 * Based on Extensible Firmware Interface Specification version 1.0

 *

 * Copyright (C) 2005-2008 Intel Co.

 *	Fenghua Yu <fenghua.yu@intel.com>

 *	Bibo Mao <bibo.mao@intel.com>

 *	Chandramouli Narayanan <mouli@linux.intel.com>

 *	Huang Ying <ying.huang@intel.com>

 *

 * Code to convert EFI to E820 map has been implemented in elilo bootloader

 * based on a EFI patch by Edgar Hucek. Based on the E820 map, the page table

 * is setup appropriately for EFI runtime code.

 * - mouli 06/14/2007.

 *

/*

 * We allocate runtime services regions top-down, starting from -4G, i.e.

 * 0xffff_ffff_0000_0000 and limit EFI VA mapping space to 64G.

/*

 * We need our own copy of the higher levels of the page tables

 * because we want to avoid inserting EFI region mappings (EFI_VA_END

 * to EFI_VA_START) into the standard kernel page tables. Everything

 * else can be shared, see efi_sync_low_kernel_mappings().

 *

 * We don't want the pgd on the pgd_list and cannot use pgd_alloc() for the

 * allocation.

/*

 * Add low kernel mappings for passing arguments to EFI functions.

	/*

	 * We share all the PUD entries apart from those that map the

	 * EFI regions. Copy around them.

/*

 * Wrapper for slow_virt_to_phys() that handles NULL addresses.

 check if the object crosses a page boundary */

	/*

	 * It can happen that the physical address of new_memmap lands in memory

	 * which is not mapped in the EFI page table. Therefore we need to go

	 * and ident-map those pages containing the map before calling

	 * phys_efi_set_virtual_address_map().

	/*

	 * Certain firmware versions are way too sentimental and still believe

	 * they are exclusive and unquestionable owners of the first physical page,

	 * even though they explicitly mark it as EFI_CONVENTIONAL_MEMORY

	 * (but then write-access it later during SetVirtualAddressMap()).

	 *

	 * Create a 1:1 mapping for this page, to avoid triple faults during early

	 * boot with such firmware. We are free to hand this page to the BIOS,

	 * as trim_bios_range() will reserve the first page and isolate it away

	 * from memory allocators anyway.

	/*

	 * When SEV-ES is active, the GHCB as set by the kernel will be used

	 * by firmware. Create a 1:1 unencrypted mapping for each GHCB.

	/*

	 * When making calls to the firmware everything needs to be 1:1

	 * mapped and addressable with 32-bit pointers. Map the kernel

	 * text and allocate a new stack because we can't rely on the

	 * stack pointer being < 4GB.

 stack grows down */

	/*

	 * EFI_RUNTIME_SERVICES_CODE regions typically cover PE/COFF

	 * executable images in memory that consist of both R-X and

	 * RW- sections, so we cannot apply read-only or non-exec

	 * permissions just yet. However, modern EFI systems provide

	 * a memory attributes table that describes those sections

	 * with the appropriate restricted permissions, which are

	 * applied in efi_runtime_update_mappings() below. All other

	 * regions can be mapped non-executable at this point, with

	 * the exception of boot services code regions, but those will

	 * be unmapped again entirely in efi_free_boot_services().

	/*

	 * Make sure the 1:1 mappings are present as a catch-all for b0rked

	 * firmware which doesn't update all internal pointers after switching

	 * to virtual mode and would otherwise crap on us.

	/*

	 * Enforce the 1:1 mapping as the default virtual address when

	 * booting in EFI mixed mode, because even though we may be

	 * running a 64-bit kernel, the firmware may only be 32-bit.

 Is PA 2M-aligned? */

 get us the same offset within this 2M page */

 Do the VA map */

/*

 * kexec kernel will use efi_map_region_fixed to map efi runtime memory ranges.

 * md->virt_addr is the original virtual address which had been mapped in kexec

 * 1st kernel.

 Update the 1:1 mapping */

	/*

	 * Use the EFI Memory Attribute Table for mapping permissions if it

	 * exists, since it is intended to supersede EFI_PROPERTIES_TABLE.

	/*

	 * EFI_MEMORY_ATTRIBUTES_TABLE is intended to replace

	 * EFI_PROPERTIES_TABLE. So, use EFI_PROPERTIES_TABLE to update

	 * permissions only if EFI_MEMORY_ATTRIBUTES_TABLE is not

	 * published by the firmware. Even if we find a buggy implementation of

	 * EFI_MEMORY_ATTRIBUTES_TABLE, don't fall back to

	 * EFI_PROPERTIES_TABLE, because of the same reason.

/*

 * Makes the calling thread switch to/from efi_mm context. Can be used

 * in a kernel thread and user context. Preemption needs to remain disabled

 * while the EFI-mm is borrowed. mmgrab()/mmdrop() is not used because the mm

 * can not change under us.

 * It should be ensured that there are no concurrent calls to this function.

/*

 * DS and ES contain user values.  We need to save them.

 * The 32-bit EFI code needs a valid DS, ES, and SS.  There's no

 * need to save the old SS: __KERNEL_DS is always acceptable.

/*

 * Switch to the EFI page tables early so that we can access the 1:1

 * runtime services mappings which are not mapped in any other page

 * tables.

 *

 * Also, disable interrupts because the IDT points to 64-bit handlers,

 * which aren't going to function correctly when we switch to 32-bit.

	/*

	 * To properly support this function we would need to repackage

	 * 'capsules' because the firmware doesn't understand 64-bit

	 * pointers.

	/*

	 * To properly support this function we would need to repackage

	 * 'capsules' because the firmware doesn't understand 64-bit

	 * pointers.

 Disable interrupts around EFI calls: */

 grab the virtually remapped EFI runtime services table pointer */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * SGI NMI support routines

 *

 * (C) Copyright 2020 Hewlett Packard Enterprise Development LP

 * Copyright (C) 2007-2017 Silicon Graphics, Inc. All rights reserved.

 * Copyright (c) Mike Travis

/*

 * UV handler for NMI

 *

 * Handle system-wide NMI events generated by the global 'power nmi' command.

 *

 * Basic operation is to field the NMI interrupt on each CPU and wait

 * until all CPU's have arrived into the nmi handler.  If some CPU's do not

 * make it into the handler, try and force them in with the IPI(NMI) signal.

 *

 * We also have to lessen UV Hub MMR accesses as much as possible as this

 * disrupts the UV Hub's primary mission of directing NumaLink traffic and

 * can cause system problems to occur.

 *

 * To do this we register our primary NMI notifier on the NMI_UNKNOWN

 * chain.  This reduces the number of false NMI calls when the perf

 * tools are running which generate an enormous number of NMIs per

 * second (~4M/s for 1024 CPU threads).  Our secondary NMI handler is

 * very short as it only checks that if it has been "pinged" with the

 * IPI(NMI) signal as mentioned above, and does not read the UV Hub's MMR.

 *

 Newer SMM NMI handler, not present in all systems */

 UVH_EVENT_OCCURRED0/1 */

 UVH_EVENT_OCCURRED0/1_ALIAS */

 UVH_EVENT_OCCURRED0/1_EXTIO_INT0_SHFT */

 "EXTIO_INT0" */

 Non-zero indicates newer SMM NMI handler present */

 UVH_EXTIO_INT0_BROADCAST */

 Indicates to BIOS that we want to use the newer SMM NMI handler */

 UVH_BIOS_KERNEL_MMR_ALIAS_2 */

 62 */

 UV hubless values */

 Values for uv_nmi_slave_continue */

/*

 * Default is all stack dumps go to the console and buffer.

 * Lower level to send to log buffer only.

/*

 * The following values show statistics on how perf events are affecting

 * this system.

 Clear on any write */

/*

 * Following values allow tuning for large systems under heavy loading

 Valid NMI Actions */

 (remove possible '\n') */

 Setup which NMI support is present in system */

 First determine arch specific MMRs to handshake with BIOS */

 Then find out if new NMI is supported */

 Read NMI MMR and check if NMI flag was set by BMC. */

/*

 * UV hubless NMI handler functions

 (from arch/x86/include/asm/mach_traps.h) */

 dummy read */

 dummy read */

 OR in new data */

 clear status bit */

 flush write data */

 HOSTSW_OWN_GPP_D_0 */

 ACPI Mode */

 Clear status: */

 GPI_INT_STS_GPP_D_0 */

 Clear Status */

 GPI_GPE_STS_GPP_D_0 */

 Clear Status */

 GPI_SMI_STS_GPP_D_0 */

 Clear Status */

 GPI_NMI_STS_GPP_D_0 */

 Clear Status */

 Disable interrupts: */

 GPI_INT_EN_GPP_D_0 */

 Disable interrupt generation */

 GPI_GPE_EN_GPP_D_0 */

 Disable interrupt generation */

 GPI_SMI_EN_GPP_D_0 */

 Disable interrupt generation */

 GPI_NMI_EN_GPP_D_0 */

 Disable interrupt generation */

 Setup GPP_D_0 Pad Config: */

 PAD_CFG_DW0_GPP_D_0 */

/*

 *  31:30 Pad Reset Config (PADRSTCFG): = 2h  # PLTRST# (default)

 *

 *  29    RX Pad State Select (RXPADSTSEL): = 0 # Raw RX pad state directly

 *                                                from RX buffer (default)

 *

 *  28    RX Raw Override to '1' (RXRAW1): = 0 # No Override

 *

 *  26:25 RX Level/Edge Configuration (RXEVCFG):

 *      = 0h # Level

 *      = 1h # Edge

 *

 *  23    RX Invert (RXINV): = 0 # No Inversion (signal active high)

 *

 *  20    GPIO Input Route IOxAPIC (GPIROUTIOXAPIC):

 * = 0 # Routing does not cause peripheral IRQ...

 *     # (we want an NMI not an IRQ)

 *

 *  19    GPIO Input Route SCI (GPIROUTSCI): = 0 # Routing does not cause SCI.

 *  18    GPIO Input Route SMI (GPIROUTSMI): = 0 # Routing does not cause SMI.

 *  17    GPIO Input Route NMI (GPIROUTNMI): = 1 # Routing can cause NMI.

 *

 *  11:10 Pad Mode (PMODE1/0): = 0h = GPIO control the Pad.

 *   9    GPIO RX Disable (GPIORXDIS):

 * = 0 # Enable the input buffer (active low enable)

 *

 *   8    GPIO TX Disable (GPIOTXDIS):

 * = 1 # Disable the output buffer; i.e. Hi-Z

 *

 *   1 GPIO RX State (GPIORXSTATE): This is the current internal RX pad state..

 *   0 GPIO TX State (GPIOTXSTATE):

 * = 0 # (Leave at default)

 Pad Config DW1 */

 PAD_CFG_DW1_GPP_D_0 */

 Termination = none (default) */

 Not a UV external NMI */

 Is a UV NMI: clear GPP_D_0 status */

 Flush write */

 Only PCH owner can check status */

/*

 * If first CPU in on this hub, set hub_nmi "in_nmi" and "owner" values and

 * return true.  If first CPU in on the system, set global "in_nmi" flag.

 Check if this is a system NMI event */

 Check flag for UV external NMI */

 A non-PCH node in a hubless system waits for NMI */

 MMR/PCH NMI flag is clear */

 Wait a moment for the HUB NMI locker to set flag */

 Re-check hub in_nmi flag */

		/*

		 * Check if this BMC missed setting the MMR NMI flag (or)

		 * UV hubless system where only PCH owner can check flag

 If we're holding the hub lock, release it now */

 Need to reset the NMI MMR register, but only once per hub. */

 Ping non-responding CPU's attempting to force them into the NMI handler */

 Clean up flags for CPU's that ignored both NMI and ping */

 Loop waiting as CPU's enter NMI handler */

 PCH NMI causes only one CPU to respond */

 all in? */

 abort if no new CPU's coming in */

 Extend delay if waiting only for CPU 0: */

 Wait until all slave CPU's have entered UV NMI handler */

 Indicate this CPU is in: */

 If not the first CPU in (the master), then we are a slave CPU */

 Wait for all other CPU's to gather here */

 If not all made it in, send IPI NMI to them */

 If all CPU's are in, then done */

 Dump Instruction Pointer header */

 Dump Instruction Pointer info */

/*

 * Dump this CPU's state.  If action was set to "kdump" and the crash_kexec

 * failed, then we provide "dump" as an alternate action.  Action "dump" now

 * also includes the show "ips" (instruction pointers) action whereas the

 * action "ips" only displays instruction pointers for the non-idle CPU's.

 * This is an abbreviated form of the "ps" command.

 Trigger a slave CPU to dump it's state */

 Wait until all CPU's ready to exit */

 Current "health" check is to check which CPU's are responsive */

 Walk through CPU list and dump state of each */

 Check if kdump kernel loaded for both main and secondary CPUs */

 Call crash to dump system state */

 secondary */

 If kdump kernel fails, secondaries will exit this loop */

 Once shootdown cpus starts, they do not return */

 !CONFIG_KGDB_KDB */

 Ensure user is expecting to attach gdb remote */

 CONFIG_KGDB_KDB */

/*

 * Call KGDB/KDB from NMI handler

 *

 * Note that if both KGDB and KDB are configured, then the action of 'kgdb' or

 * 'kdb' has no affect on which is used.  See the KGDB documentation for further

 * information.

 Call KGDB NMI handler as MASTER */

 Wait for KGDB signal that it's ready for slaves to enter */

 Call KGDB as slave */

 !CONFIG_KGDB */

 !CONFIG_KGDB */

/*

 * UV NMI handler

 If not a UV System NMI, ignore */

 Indicate we are the first CPU into the NMI handler */

 If NMI action is "kdump", then attempt to do it */

 Unexpected return, revert action to "dump" */

 Pause as all CPU's enter the NMI handler */

 Process actions other than "kdump": */

 Clear per_cpu "in_nmi" flag */

 Clear MMR NMI flag on each hub */

 Clear global flags */

/*

 * NMI handler for pulling in CPU's when perf events are grabbing our NMI

	/*

	 * Unmask NMI on all CPU's

 Setup HUB NMI info */

 Setup for UV Hub systems */

 Setup for UV Hubless systems */

 Ensure NMI enabled in Processor Interface Reg: */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * BIOS run time interface routines.

 *

 * (C) Copyright 2020 Hewlett Packard Enterprise Development LP

 * Copyright (C) 2007-2017 Silicon Graphics, Inc. All rights reserved.

 * Copyright (c) Russ Anderson <rja@sgi.com>

		/*

		 * BIOS does not support UV systab

	/*

	 * bios returns watchlist number or negative error number.

/*

 * uv_bios_set_legacy_vga_target - Set Legacy VGA I/O Target

 * @decode: true to enable target, false to disable target

 * @domain: PCI domain number

 * @bus: PCI bus number

 *

 * Returns:

 *    0: Success

 *    -EINVAL: Invalid domain or bus number

 *    -ENOSYS: Capability not available

 *    -EBUSY: Legacy VGA I/O cannot be retargeted at this time

 Starting with UV4 the UV systab size is variable */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * SGI UV IRQ functions

 *

 * Copyright (C) 2008 Silicon Graphics, Inc. All rights reserved.

 MMR offset and pnode of hub sourcing interrupts for a given irq */

/*

 * Re-target the irq to the specified CPU and enable the specified MMR located

 * on the specified blade to allow the sending of MSIs to the specified CPU.

/*

 * Disable the specified MMR located on the specified blade so that MSIs are

 * longer allowed to be sent.

/*

 * Set up a mapping of an available irq and vector, and enable the specified

 * MMR that defines the MSI that is to be sent to the specified CPU when an

 * interrupt is raised.

/*

 * Tear down a mapping of an irq and vector, and disable the specified MMR that

 * defined the MSI that was to be sent to the specified CPU when an interrupt

 * was raised.

 *

 * Set mmr_blade and mmr_offset to what was passed in on uv_setup_irq().

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * SGI RTC clock/timer routines.

 *

 *  (C) Copyright 2020 Hewlett Packard Enterprise Development LP

 *  Copyright (c) 2009-2013 Silicon Graphics, Inc.  All Rights Reserved.

 *  Copyright (c) Dimitri Sivanich

 There is one of these allocated per node */

 next cpu waiting for timer, local node relative: */

 number of cpus on this node: */

 systemwide logical cpu number */

 next timer expiration for this cpu */

/*

 * Access to uv_rtc_timer_head via blade id.

/*

 * Hardware interface routines

 Send IPIs to another node */

 Check for an RTC interrupt pending */

 Setup interrupt and return non-zero if early expiration occurred. */

 Set configuration */

 Initialize comparator value */

/*

 * Per-cpu timer tracking routines

 Allocate per-node list of cpu timer expiration times. */

 Find and set the next expiring timer.  */

 If we didn't set it up in time, trigger */

/*

 * Set expiration time for current cpu.

 *

 * Returns 1 if we missed the expiration time.

 Will this one be next to go off? */

/*

 * Unset expiration time for current cpu.

 *

 * Returns 1 if this timer was pending.

 Was the hardware setup for this timer? */

/*

 * Kernel interface routines.

/*

 * Read the RTC.

 *

 * Starting with HUB rev 2.0, the UV RTC register is replicated across all

 * cachelines of it's own page.  This allows faster simultaneous reads

 * from a given socket.

/*

 * Program the next event, relative to now

/*

 * Shutdown the RTC timer

 Setup and register clockevents */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel MID Power Management Unit (PWRMU) device driver

 *

 * Copyright (C) 2016, Intel Corporation

 *

 * Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>

 *

 * Intel MID Power Management Unit device driver handles the South Complex PCI

 * devices such as GPDMA, SPI, I2C, PWM, and so on. By default PCI core

 * modifies bits in PMCSR register in the PCI configuration space. This is not

 * enough on some SoCs like Intel Tangier. In such case PCI core sets a new

 * power state of the device in question through a PM hook registered in struct

 * pci_platform_pm_ops (see drivers/pci/pci-mid.c).

 Registers */

 Bits in PM_STS */

 Bits in PM_CMD */

 System states */

 Trigger variants */

 Message to wait for TRIGGER_NC case */

 List of commands */

 Bits in PM_ICS */

 List of interrupts */

 South Complex devices */

 wake state width */

 power state width */

 Supported device IDs */

 Wait 500ms that the latest PWRMU command finished */

 Check if the device is already in desired state */

 Update the power state */

 Send command to SCU */

 Check if the device is already in desired state */

 Find device in cache or first free cell */

 Store the desired state in cache */

 Find the power state we may use */

 We support states between PCI_D0 and PCI_D3hot */

 Send command to SCU */

	/*

	 * Mapping to PWRMU index is kept in the Logical SubSystem ID byte of

	 * Vendor capability.

 Read the Logical SubSystem ID byte */

 Disable interrupts */

	/*

	 * Enable wake events.

	 *

	 * PWRMU supports up to 32 sources for wake up the system. Ungate them

	 * all here.

	/*

	 * Power off South Complex devices.

	 *

	 * There is a map (see a note below) of 64 devices with 2 bits per each

	 * on 32-bit HW registers. The following calls set all devices to one

	 * known initial state, i.e. PCI_D3hot. This is done in conjunction

	 * with PMCSR setting in arch/x86/pci/intel_mid_pci.c.

	 *

	 * NOTE: The actual device mapping is provided by a platform at run

	 * time using vendor capability of PCI configuration space.

 Send command to SCU */

 On Penwell SRAM must stay powered on */

 PM_SSC(0) */

 PM_SSC(1) */

 PM_SSC(2) */

 PM_SSC(3) */

 PM_SSC(0) */

 PM_SSC(1) */

 PM_SSC(2) */

 PM_SSC(3) */

 This table should be in sync with the one in drivers/pci/pci-mid.c */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel MID platform setup code

 *

 * (C) Copyright 2008, 2012, 2021 Intel Corporation

 * Author: Jacob Pan (jacob.jun.pan@intel.com)

 * Author: Sathyanarayanan Kuppuswamy <sathyanarayanan.kuppuswamy@intel.com>

 Only for Tangier */

 Shut down South Complex via PWRMU */

 Only for Tangier, the rest will ignore this command */

 Lapic only, no apbt */

	/*

	 * Intel MID platforms are using explicitly defined regulators.

	 *

	 * Let the regulator core know that we do not have any additional

	 * regulators left. This lets it substitute unprovided regulators with

	 * dummy ones:

/*

 * Moorestown does not have external NMI source nor port 0x61 to report

 * NMI status. The possible NMI sources are from pmu as a result of NMI

 * watchdog or lock debug. Reading io port 0x61 results in 0xff which

 * misled NMI handler.

/*

 * Moorestown specific x86_init function overrides and early setup

 * calls.

	/*

	 * Do nothing for now as everything needed done in

	 * x86_intel_mid_early_setup() below.

 Avoid searching for BIOS MP tables */

 SPDX-License-Identifier: GPL-2.0

/*

 * PVH variables.

 *

 * pvh_bootparams and pvh_start_info need to live in a data segment since

 * they are used after startup_{32|64}, which clear .bss, are invoked.

/*

 * Xen guests are able to obtain the memory map from the hypervisor via the

 * HYPERVISOR_memory_op hypercall.

 * If we are trying to boot a Xen PVH guest, it is expected that the kernel

 * will have been configured to provide an override for this routine to do

 * just that.

 Non-xen guests are not supported by version 0 */

 The first module is always ramdisk. */

	/*

	 * See Documentation/x86/boot.rst.

	 *

	 * Version 2.12 supports Xen entry point but we will use default x86/PC

	 * environment (i.e. hardware_subarch 0).

/*

 * If we are trying to boot a Xen PVH guest, it is expected that the kernel

 * will have been configured to provide the required override for this routine.

/*

 * This routine (and those that it might call) should not use

 * anything that lives in .bss since that segment will be cleared later.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IOSF-SB MailBox Interface Driver

 * Copyright (c) 2013, Intel Corporation.

 *

 * The IOSF-SB is a fabric bus available on Atom based SOC's that uses a

 * mailbox interface (MBI) to communicate with multiple devices. This

 * driver implements access to this interface for those platforms that can

 * enumerate the device using PCI.

*************** Generic iosf_mbi access helpers ****************/

 Access to the GFX unit is handled by GPU code */

 Access to the GFX unit is handled by GPU code */

 Access to the GFX unit is handled by GPU code */

 Read current mdr value */

 Apply mask */

 Write back */

 Mbi isn't hot-pluggable. No remove routine is provided */

/*

 **************** P-Unit/kernel shared I2C bus arbitration ****************

 *

 * Some Bay Trail and Cherry Trail devices have the P-Unit and us (the kernel)

 * share a single I2C bus to the PMIC. Below are helpers to arbitrate the

 * accesses between the kernel and the P-Unit.

 *

 * See arch/x86/include/asm/iosf_mbi.h for kernel-doc text for each function.

 Wait for any I2C PMIC accesses from in kernel drivers to finish. */

	/*

	 * We do not need to do anything to allow the PUNIT to safely access

	 * the PMIC, other then block in kernel accesses to the PMIC.

/*

 * This function blocks P-Unit accesses to the PMIC I2C bus, so that kernel

 * I2C code, such as e.g. a fuel-gauge driver, can access it safely.

 *

 * This function may be called by I2C controller code while an I2C driver has

 * already blocked P-Unit accesses because it wants them blocked over multiple

 * i2c-transfers, for e.g. read-modify-write of an I2C client register.

 *

 * To allow safe PMIC i2c bus accesses this function takes the following steps:

 *

 * 1) Some code sends request to the P-Unit which make it access the PMIC

 *    I2C bus. Testing has shown that the P-Unit does not check its internal

 *    PMIC bus semaphore for these requests. Callers of these requests call

 *    iosf_mbi_punit_acquire()/_release() around their P-Unit accesses, these

 *    functions increase/decrease iosf_mbi_pmic_punit_access_count, so first

 *    we wait for iosf_mbi_pmic_punit_access_count to become 0.

 *

 * 2) Check iosf_mbi_pmic_i2c_access_count, if access has already

 *    been blocked by another caller, we only need to increment

 *    iosf_mbi_pmic_i2c_access_count and we can skip the other steps.

 *

 * 3) Some code makes such P-Unit requests from atomic contexts where it

 *    cannot call iosf_mbi_punit_acquire() as that may sleep.

 *    As the second step we call a notifier chain which allows any code

 *    needing P-Unit resources from atomic context to acquire them before

 *    we take control over the PMIC I2C bus.

 *

 * 4) When CPU cores enter C6 or C7 the P-Unit needs to talk to the PMIC

 *    if this happens while the kernel itself is accessing the PMIC I2C bus

 *    the SoC hangs.

 *    As the third step we call cpu_latency_qos_update_request() to disallow the

 *    CPU to enter C6 or C7.

 *

 * 5) The P-Unit has a PMIC bus semaphore which we can request to stop

 *    autonomous P-Unit tasks from accessing the PMIC I2C bus while we hold it.

 *    As the fourth and final step we request this semaphore and wait for our

 *    request to be acknowledged.

	/*

	 * Disallow the CPU to enter C6 or C7 state, entering these states

	 * requires the P-Unit to talk to the PMIC and if this happens while

	 * we're holding the semaphore, the SoC hangs.

 host driver writes to side band semaphore register */

 host driver waits for bit 0 to be set in semaphore register */

 Wait for the bus to go inactive before registering */

 Wait for the bus to go inactive before unregistering */

*************** iosf_mbi debug code ****************/

 mdr */

 mcrx */

 mcr - initiates mailbox transaction */

 CONFIG_IOSF_MBI_DEBUG */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Technologic Systems TS-5500 Single Board Computer support

 *

 * Copyright (C) 2013-2014 Savoir-faire Linux Inc.

 *	Vivien Didelot <vivien.didelot@savoirfairelinux.com>

 *

 * This driver registers the Technologic Systems TS-5500 Single Board Computer

 * (SBC) and its devices, and exposes information to userspace such as jumpers'

 * state or available options. For further information about sysfs entries, see

 * Documentation/ABI/testing/sysfs-platform-ts5500.

 *

 * This code may be extended to support similar x86-based platforms.

 * Actually, the TS-5500 and TS-5400 are supported.

 Product code register */

 TS-5500 product code */

 TS-5400 product code */

 SRAM/RS-485/ADC options, and RS-485 RTS/Automatic RS-485 flags register */

 SRAM option */

 RS-485 option */

 A/D converter option */

 RTS for RS-485 */

 Automatic RS-485 */

 External Reset/Industrial Temperature Range options register */

 External Reset option */

 Indust. Temp. Range option */

 LED/Jumpers register */

 LED flag */

 Automatic CMOS */

 Enable Serial Console */

 Write Enable Drive A */

 Fast Console (115K baud) */

 User Jumper */

 Console on COM1 (req. JP2) */

 Undocumented (Unused) */

 A/D Converter registers */

 Conversion state register */

 Start conv. / LSB register */

 MSB register */

 usec */

/**

 * struct ts5500_sbc - TS-5500 board description

 * @name:	Board model name.

 * @id:		Board product ID.

 * @sram:	Flag for SRAM option.

 * @rs485:	Flag for RS-485 option.

 * @adc:	Flag for Analog/Digital converter option.

 * @ereset:	Flag for External Reset option.

 * @itr:	Flag for Industrial Temperature Range option.

 * @jumpers:	Bitfield for jumpers' state.

 Board signatures in BIOS shadow RAM */

 Start conversion (ensure the 3 MSB are set to 0) */

	/*

	 * The platform has CPLD logic driving the A/D converter.

	 * The conversion must complete within 11 microseconds,

	 * otherwise we have to re-initiate a conversion.

 Read the raw data */

	/*

	 * There is no DMI available or PCI bridge subvendor info,

	 * only the BIOS provides a 16-bit identification call.

	 * It is safer to find a signature in the BIOS shadow RAM.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * imr.c -- Intel Isolated Memory Region driver

 *

 * Copyright(c) 2013 Intel Corporation.

 * Copyright(c) 2015 Bryan O'Donoghue <pure.logic@nexus-software.ie>

 *

 * IMR registers define an isolated region of memory that can

 * be masked to prohibit certain system agents from accessing memory.

 * When a device behind a masked port performs an access - snooped or

 * not, an IMR may optionally prevent that transaction from changing

 * the state of memory or from getting correct data in response to the

 * operation.

 *

 * Write data will be dropped and reads will return 0xFFFFFFFF, the

 * system will reset and system BIOS will print out an error message to

 * inform the user that an IMR has been violated.

 *

 * This code is based on the Linux MTRR code and reference code from

 * Intel's Quark BSP EFI, Linux and grub code.

 *

 * See quark-x1000-datasheet.pdf for register definitions.

 * http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/quark-x1000-datasheet.pdf

/*

 * IMR read/write mask control registers.

 * See quark-x1000-datasheet.pdf sections 12.7.4.5 and 12.7.4.6 for

 * bit definitions.

 *

 * addr_hi

 * 31		Lock bit

 * 30:24	Reserved

 * 23:2		1 KiB aligned lo address

 * 1:0		Reserved

 *

 * addr_hi

 * 31:24	Reserved

 * 23:2		1 KiB aligned hi address

 * 1:0		Reserved

/**

 * imr_is_enabled - true if an IMR is enabled false otherwise.

 *

 * Determines if an IMR is enabled based on address range and read/write

 * mask. An IMR set with an address range set to zero and a read/write

 * access mask set to all is considered to be disabled. An IMR in any

 * other state - for example set to zero but without read/write access

 * all is considered to be enabled. This definition of disabled is how

 * firmware switches off an IMR and is maintained in kernel for

 * consistency.

 *

 * @imr:	pointer to IMR descriptor.

 * @return:	true if IMR enabled false if disabled.

/**

 * imr_read - read an IMR at a given index.

 *

 * Requires caller to hold imr mutex.

 *

 * @idev:	pointer to imr_device structure.

 * @imr_id:	IMR entry to read.

 * @imr:	IMR structure representing address and access masks.

 * @return:	0 on success or error code passed from mbi_iosf on failure.

/**

 * imr_write - write an IMR at a given index.

 *

 * Requires caller to hold imr mutex.

 * Note lock bits need to be written independently of address bits.

 *

 * @idev:	pointer to imr_device structure.

 * @imr_id:	IMR entry to write.

 * @imr:	IMR structure representing address and access masks.

 * @return:	0 on success or error code passed from mbi_iosf on failure.

	/*

	 * If writing to the IOSF failed then we're in an unknown state,

	 * likely a very bad state. An IMR in an invalid state will almost

	 * certainly lead to a memory access violation.

/**

 * imr_dbgfs_state_show - print state of IMR registers.

 *

 * @s:		pointer to seq_file for output.

 * @unused:	unused parameter.

 * @return:	0 on success or error code passed from mbi_iosf on failure.

		/*

		 * Remember to add IMR_ALIGN bytes to size to indicate the

		 * inherent IMR_ALIGN size bytes contained in the masked away

		 * lower ten bits.

/**

 * imr_debugfs_register - register debugfs hooks.

 *

 * @idev:	pointer to imr_device structure.

/**

 * imr_check_params - check passed address range IMR alignment and non-zero size

 *

 * @base:	base address of intended IMR.

 * @size:	size of intended IMR.

 * @return:	zero on valid range -EINVAL on unaligned base/size.

/**

 * imr_raw_size - account for the IMR_ALIGN bytes that addr_hi appends.

 *

 * IMR addr_hi has a built in offset of plus IMR_ALIGN (0x400) bytes from the

 * value in the register. We need to subtract IMR_ALIGN bytes from input sizes

 * as a result.

 *

 * @size:	input size bytes.

 * @return:	reduced size.

/**

 * imr_address_overlap - detects an address overlap.

 *

 * @addr:	address to check against an existing IMR.

 * @imr:	imr being checked.

 * @return:	true for overlap false for no overlap.

/**

 * imr_add_range - add an Isolated Memory Region.

 *

 * @base:	physical base address of region aligned to 1KiB.

 * @size:	physical size of region in bytes must be aligned to 1KiB.

 * @read_mask:	read access mask.

 * @write_mask:	write access mask.

 * @return:	zero on success or negative value indicating error.

 Tweak the size value. */

	/*

	 * Check for reserved IMR value common to firmware, kernel and grub

	 * indicating a disabled IMR.

	/*

	 * Find a free IMR while checking for an existing overlapping range.

	 * Note there's no restriction in silicon to prevent IMR overlaps.

	 * For the sake of simplicity and ease in defining/debugging an IMR

	 * memory map we exclude IMR overlaps.

 Find overlap @ base or end of requested range. */

 Error out if we have no free IMR entries. */

 Enable IMR at specified range and access mask. */

		/*

		 * In the highly unlikely event iosf_mbi_write failed

		 * attempt to rollback the IMR setup skipping the trapping

		 * of further IOSF write failures.

/**

 * __imr_remove_range - delete an Isolated Memory Region.

 *

 * This function allows you to delete an IMR by its index specified by reg or

 * by address range specified by base and size respectively. If you specify an

 * index on its own the base and size parameters are ignored.

 * imr_remove_range(0, base, size); delete IMR at index 0 base/size ignored.

 * imr_remove_range(-1, base, size); delete IMR from base to base+size.

 *

 * @reg:	imr index to remove.

 * @base:	physical base address of region aligned to 1 KiB.

 * @size:	physical size of region in bytes aligned to 1 KiB.

 * @return:	-EINVAL on invalid range or out or range id

 *		-ENODEV if reg is valid but no IMR exists or is locked

 *		0 on success.

	/*

	 * Validate address range if deleting by address, else we are

	 * deleting by index where base and size will be ignored.

 Tweak the size value. */

 If a specific IMR is given try to use it. */

 Search for match based on address range. */

 Tear down the IMR. */

/**

 * imr_remove_range - delete an Isolated Memory Region by address

 *

 * This function allows you to delete an IMR by an address range specified

 * by base and size respectively.

 * imr_remove_range(base, size); delete IMR from base to base+size.

 *

 * @base:	physical base address of region aligned to 1 KiB.

 * @size:	physical size of region in bytes aligned to 1 KiB.

 * @return:	-EINVAL on invalid range or out or range id

 *		-ENODEV if reg is valid but no IMR exists or is locked

 *		0 on success.

/**

 * imr_clear - delete an Isolated Memory Region by index

 *

 * This function allows you to delete an IMR by an address range specified

 * by the index of the IMR. Useful for initial sanitization of the IMR

 * address map.

 * imr_ge(base, size); delete IMR from base to base+size.

 *

 * @reg:	imr index to remove.

 * @return:	-EINVAL on invalid range or out or range id

 *		-ENODEV if reg is valid but no IMR exists or is locked

 *		0 on success.

/**

 * imr_fixup_memmap - Tear down IMRs used during bootup.

 *

 * BIOS and Grub both setup IMRs around compressed kernel, initrd memory

 * that need to be removed before the kernel hands out one of the IMR

 * encased addresses to a downstream DMA agent such as the SD or Ethernet.

 * IMRs on Galileo are setup to immediately reset the system on violation.

 * As a result if you're running a root filesystem from SD - you'll need

 * the boot-time IMRs torn down or you'll find seemingly random resets when

 * using your filesystem.

 *

 * @idev:	pointer to imr_device structure.

 * @return:

 Tear down all existing unlocked IMRs. */

	/*

	 * Setup an unlocked IMR around the physical extent of the kernel

	 * from the beginning of the .text section to the end of the

	 * .rodata section as one physically contiguous block.

	 *

	 * We don't round up @size since it is already PAGE_SIZE aligned.

	 * See vmlinux.lds.S for details.

/**

 * imr_init - entry point for IMR driver.

 *

 * return: -ENODEV for no IMR support 0 if good to go.

 SPDX-License-Identifier: GPL-2.0

/*

 * imr_selftest.c -- Intel Isolated Memory Region self-test driver

 *

 * Copyright(c) 2013 Intel Corporation.

 * Copyright(c) 2015 Bryan O'Donoghue <pure.logic@nexus-software.ie>

 *

 * IMR self test. The purpose of this module is to run a set of tests on the

 * IMR API to validate it's sanity. We check for overlapping, reserved

 * addresses and setup/teardown sanity.

 *

/**

 * imr_self_test_result - Print result string for self test.

 *

 * @res:	result code - true if test passed false otherwise.

 * @fmt:	format string.

 * ...		variadic argument list.

 Print pass/fail. */

 Print variable string. */

 Optional warning. */

/**

 * imr_self_test

 *

 * Verify IMR self_test with some simple tests to verify overlap,

 * zero sized allocations and 1 KiB sized areas.

 *

 Test zero zero. */

 Test exact overlap. */

 Test overlap with base inside of existing. */

 Test overlap with end inside of existing. */

 Test that a 1 KiB IMR @ zero with read/write all will bomb out. */

 Test that a 1 KiB IMR @ zero with CPU only will work. */

 Test 2 KiB works. */

/**

 * imr_self_test_init - entry point for IMR driver.

 *

 * return: -ENODEV for no IMR support 0 if good to go.

/**

 * imr_self_test_exit - exit point for IMR code.

 *

 * return:

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (c) 2001,2002 Christer Weinigel <wingel@nano-system.com>

 *

 *  National Semiconductor SCx200 support.

 Verify that the configuration block really is there */

 read the current values driven on the GPIO signals */

 find the base of the Configuration Block */

 SPDX-License-Identifier: GPL-2.0-only

 address of OFW callback interface; will be NULL if OFW isn't found */

 page dir entry containing OFW's pgdir table; filled in by head_32.S */

 fetch OFW's PDE */

 install OFW's PDE permanently into the kernel's pgtable */

 implicit optimization barrier here due to uninline function return */

 call into ofw */

 OFW cif _should_ be above this address */

 OFW starts on a 1MB boundary */

 ensure OFW booted us by checking for "OFW " string */

 determine where OFW starts in memory */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support for the OLPC DCON and OLPC EC access

 *

 * Copyright © 2006  Advanced Micro Devices, Inc.

 * Copyright © 2007-2008  Andres Salomon <dilinger@debian.org>

 what the timeout *should* be (in ms) */

 the timeout that bugs in the EC might force us to actually use */

/*

 * These {i,o}bf_status functions return whether the buffers are full or not.

/*

 * This allows the kernel to run Embedded Controller commands.  The EC is

 * documented at <http://wiki.laptop.org/go/Embedded_controller>, and the

 * available EC commands are here:

 * <http://wiki.laptop.org/go/Ec_specification>.  Unfortunately, while

 * OpenFirmware's source is available, the EC's is not.

 Clear OBF */

	/*

	 * Note that if we time out during any IBF checks, that's a failure;

	 * we have to return.  There's no way for the kernel to clear that.

	 *

	 * If we time out during an OBF check, we can restart the command;

	 * reissuing it will clear the OBF flag, and we should be alright.

	 * The OBF flag will sometimes misbehave due to what we believe

	 * is a hardware quirk..

 write data to EC */

 read data from EC */

	/*

	 * Squelch SCIs while suspended.  This is a fix for

	 * <http://dev.laptop.org/ticket/1835>.

 Tell the EC to stop inhibiting SCIs */

	/*

	 * Tell the wireless module to restart USB communication.

	 * Must be done twice.

	/*

	 * XO-1 EC wakeups are available when olpc-xo1-sci driver is

	 * compiled in

	/*

	 * XO-1.5 EC wakeups are available when olpc-xo15-sci driver is

	 * compiled in

 register the XO-1 and 1.5-specific EC handler */

 XO-1 */

 assume B1 and above models always have a DCON */

	/* If the VSA exists let it emulate PCI, if not emulate in kernel.

 XO-1 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * OLPC-specific OFW device tree support code.

 *

 * Paul Mackerras	August 1996.

 * Copyright (C) 1996-2005 Paul Mackerras.

 *

 *  Adapted for 64bit PowerPC by Dave Engebretsen and Peter Bergner.

 *    {engebret|bergner}@us.ibm.com

 *

 *  Adapted for sparc by David S. Miller davem@davemloft.net

 *  Adapted for x86/OLPC by Andres Salomon <dilinger@queued.net>

		/*

		 * To minimize the number of allocations, grab at least

		 * PAGE_SIZE of memory (that's an arbitrary choice that's

		 * fast enough on the platforms we care about while minimizing

		 * wasted bootmem) and hand off chunks of it to callers.

 allocate from the local cache */

/*

 * Extract board revision directly from OFW device tree.

 * We can't use olpc_platform_info because that hasn't been set up yet.

 XO-1.5 */

 Add olpc,xo1.5-battery compatible marker to battery node */

			/*

			 * If we have a olpc,xo1-battery compatible, then we're

			 * running a new enough firmware that already has

			 * the dcon node.

 Add dcon device */

 XO-1 */

			/*

			 * If we have a olpc,xo1-battery compatible, then we're

			 * running a new enough firmware that already has

			 * the dcon and RTC nodes.

 Add dcon device, mark RTC as olpc,xo1-rtc */

 Add olpc,xo1-battery compatible marker to battery node */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support for power management features of the OLPC XO-1 laptop

 *

 * Copyright (C) 2010 Andres Salomon <dilinger@queued.net>

 * Copyright (C) 2010 One Laptop per Child

 * Copyright (C) 2006 Red Hat, Inc.

 * Copyright (C) 2006 Advanced Micro Devices, Inc.

 Set bits in the wakeup mask */

 Clear bits in the wakeup mask */

 Only STR is supported */

	/*

	 * Save SCI mask (this gets lost since PM1_EN is used as a mask for

	 * wakeup events, which is not necessarily the same event set)

 Save CPU state */

 Resume path starts here */

 Restore SCI mask (using dword access to CS5536_PM1_EN) */

 Program wakeup mask (using dword access to CS5536_PM1_EN) */

 Enable all of these controls with 0 delay */

 Clear status bits (possibly unnecessary) */

 Write SLP_EN bit to start the machinery */

 suspend-to-RAM only */

 don't run on non-XOs */

 If we have both addresses, we can override the poweroff hook */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support for OLPC XO-1 Real Time Clock (RTC)

 *

 * Copyright (C) 2011 One Laptop per Child

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support for OLPC XO-1.5 System Control Interrupts (SCI)

 *

 * Copyright (C) 2009-2010 One Laptop per Child

/*

 * The normal ACPI LID wakeup behavior is wake-on-open, but not

 * wake-on-close. This is implemented as standard by the XO-1.5 DSDT.

 *

 * We provide here a sysfs attribute that will additionally enable

 * wake-on-close behavior. This is useful (e.g.) when we opportunistically

 * suspend with the display running; if the lid is then closed, we want to

 * wake up to turn the display off.

 *

 * This is controlled through a custom method in the XO-1.5 DSDT.

 Get GPE bit assignment (EC events). */

 Flush queue, and enable all SCI events */

 Enable wake-on-EC */

 Enable all EC events */

 Power/battery status might have changed */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support for OLPC XO-1 System Control Interrupts (SCI)

 *

 * Copyright (C) 2010 One Laptop per Child

 * Copyright (C) 2006 Red Hat, Inc.

 * Copyright (C) 2006 Advanced Micro Devices, Inc.

 Report current ebook switch state through input layer */

 Nothing new to report. */

 gpio is high; invert so we'll get l->h event interrupt */

	/*

	 * the edge detector hookup on the gpio inputs on the geode is

	 * odd, to say the least.  See http://dev.laptop.org/ticket/5703

	 * for details, but in a nutshell:  we don't use the edge

	 * detectors.  instead, we make use of an anomaly:  with the both

	 * edge detectors turned off, we still get an edge event on a

	 * positive edge transition.  to take advantage of this, we use the

	 * front-end inverter to ensure that that's the edge we're always

	 * going to see next.

 x ^^ y */

 Report current lid switch state through input layer */

 Nothing new to report. */

/*

 * Process all items in the EC's SCI queue.

 *

 * This is handled in a workqueue because olpc_ec_cmd can be slow (and

 * can even timeout).

 *

 * If propagate_events is false, the queue is drained without events being

 * generated for the interrupts.

			/* Only report power button input when it was pressed

			 * during regular operation (as opposed to when it

 Report the wakeup event in all cases. */

		/* When the system is woken by the RTC alarm, report the

 EC GPIO */

 we may have just caused an event */

	/*

	 * We don't know what may have happened while we were asleep.

	 * Reestablish our lid setup so we're sure to catch all transitions.

 Enable all EC events */

 Power/battery status might have changed too */

 Zero means masked */

 Select level triggered in PIC */

 Enable interesting SCI events, and clear pending interrupts */

 Clear pending EC SCI events */

	/*

	 * Enable EC SCI events, and map them to both a PME and the SCI

	 * interrupt.

	 *

	 * Ordinarily, in addition to functioning as GPIOs, Geode GPIOs can

	 * be mapped to regular interrupts *or* Geode-specific Power

	 * Management Events (PMEs) - events that bring the system out of

	 * suspend. In this case, we want both of those things - the system

	 * wakeup, *and* the ability to get an interrupt when an event occurs.

	 *

	 * To achieve this, we map the GPIO to a PME, and then we use one

	 * of the many generic knobs on the CS5535 PIC to additionally map the

	 * PME to the regular SCI interrupt line.

 Set the SCI to cause a PME event on group 7 */

 And have group 7 also fire the SCI interrupt */

 Clear edge detection and event enable for now */

 Set the LID to cause an PME event on group 6 */

 Set PME group 6 to fire the SCI interrupt */

 Enable the event */

 don't run on non-XOs */

 Enable PME generation for EC-generated events */

 Clear pending events */

 Initial sync */

 Enable all EC events */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel SOC Punit device state debug driver

 * Punit controls power management for North Complex devices (Graphics

 * blocks, Image Signal Processing, video processing, display, DSP etc.)

 *

 * Copyright (c) 2015, Intel Corporation.

 Subsystem config/status Video processor */

 Subsystem config/status ISP (Image Signal Processor) */

 Subsystem config/status Input/output controller */

 Shift bits for getting status for video, isp and i/o */

 Power gate status reg */

 Shift bits for getting status for graphics rendering */

 Shift bits for getting status for media control */

 Shift bits for getting status for Valley View/Baytrail display */

 Subsystem config/status display for Cherry Trail SOC */

 Shift bits for getting status for display */

 SPDX-License-Identifier: GPL-2.0

/*

 * Hyper-V Isolation VM interface with paravisor and hypervisor

 *

 * Author:

 *  Tianyu Lan <Tianyu.Lan@microsoft.com>

 Check size of union hv_ghcb here. */

/*

 * hv_is_isolation_supported - Check system runs in the Hyper-V

 * isolation VM.

/*

 * hv_isolation_type_snp - Check system runs in the AMD SEV-SNP based

 * isolation VM.

/*

 * hv_mark_gpa_visibility - Set pages visible to host via hvcall.

 *

 * In Isolation VM, all guest memory is encrypted from host and guest

 * needs to set memory visible to host via hvcall before sharing memory

 * with host.

 no-op if partition isolation is not enabled */

/*

 * hv_set_mem_host_visibility - Set specified memory visible to host.

 *

 * In Isolation VM, all guest memory is encrypted from host and guest

 * needs to set memory visible to host via hvcall before sharing memory

 * with host. This function works as wrap of hv_mark_gpa_visibility()

 * with memory base and size.

 SPDX-License-Identifier: GPL-2.0

/*

 * Hyper-V specific spinlock code.

 *

 * Copyright (C) 2018, Intel, Inc.

 *

 * Author : Yi Sun <yi.y.sun@intel.com>

	/*

	 * Reading HV_X64_MSR_GUEST_IDLE MSR tells the hypervisor that the

	 * vCPU can be put into 'idle' state. This 'idle' state is

	 * terminated by an IPI, usually from hv_qlock_kick(), even if

	 * interrupts are disabled on the vCPU.

	 *

	 * To prevent a race against the unlock path it is required to

	 * disable interrupts before accessing the HV_X64_MSR_GUEST_IDLE

	 * MSR. Otherwise, if the IPI from hv_qlock_kick() arrives between

	 * the lock value check and the rdmsrl() then the vCPU might be put

	 * into 'idle' state by the hypervisor and kept in that state for

	 * an unspecified amount of time.

	/*

	 * Only issue the rdmsrl() when the lock state has not changed.

/*

 * Hyper-V does not support this so far.

 SPDX-License-Identifier: GPL-2.0

/*

 * Hyper-V nested virtualization code.

 *

 * Copyright (C) 2018, Microsoft, Inc.

 *

 * Author : Lan Tianyu <Tianyu.Lan@microsoft.com>

		/*

		 * If flush requests exceed max flush count, go back to

		 * flush tlbs without range.

 SPDX-License-Identifier: GPL-2.0

/*

 * Irqdomain for Linux to run as the root partition on Microsoft Hypervisor.

 *

 * Authors:

 *  Sunil Muthuswamy <sunilmut@microsoft.com>

 *  Wei Liu <wei.liu@kernel.org>

	/*

	 * var-sized hypercall, var-size starts after vp_mask (thus

	 * vp_set.format does not count, but vp_set.valid_bank_mask

	 * does).

		/*

		 * Microsoft Hypervisor requires a bus range when the bridge is

		 * running in PCI-X mode.

		 *

		 * To distinguish conventional vs PCI-X bridge, we can check

		 * the bridge's PCI-X Secondary Status Register, Secondary Bus

		 * Mode and Frequency bits. See PCI Express to PCI/PCI-X Bridge

		 * Specification Revision 1.0 5.2.2.1.3.

		 *

		 * Value zero means it is in conventional mode, otherwise it is

		 * in PCI-X mode.

 Non-zero, PCI-X mode */

 High address is always 0 */

		/*

		 * This interrupt is already mapped. Let's unmap first.

		 *

		 * We don't use retarget interrupt hypercalls here because

		 * Microsoft Hypervisor doens't allow root to change the vector

		 * or specify VPs outside of the set that is initially used

		 * during mapping.

/*

 * IRQ Chip for MSI PCI/PCI-X/PCI-Express Devices,

 * which implement the MSI or MSI-X Capability Structure.

 No point in going further if we can't get an irq domain */

 CONFIG_PCI_MSI */

 SPDX-License-Identifier: GPL-2.0

/*

 * See struct hv_deposit_memory. The first u64 is partition ID, the rest

 * are GPAs.

 Deposits exact number of pages. Must be called with interrupts enabled.  */

 One buffer for page pointers and counts */

 Allocate all the pages before disabling interrupts */

 Find highest order we can actually allocate */

 Populate gpa_page_list - these will fit on the input page */

	/*

	 * When adding a logical processor, the hypervisor may return

	 * HV_STATUS_INSUFFICIENT_MEMORY. When that happens, we deposit more

	 * pages and retry.

 We don't do anything with the output right now */

 Root VPs don't seem to need pages deposited */

 The value 90 is empirically determined. It may change. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * X86 specific Hyper-V initialization code.

 *

 * Copyright (C) 2016, Microsoft, Inc.

 *

 * Author : K. Y. Srinivasan <kys@microsoft.com>

 Storage to save the hypercall page temporarily for hibernation */

	/*

	 * GHCB page is allocated by paravisor. The address

	 * returned by MSR_AMD64_SEV_ES_GHCB is above shared

	 * memory boundary and map it here.

			/*

			 * For root partition we get the hypervisor provided VP assist

			 * page, instead of allocating a new page.

			/*

			 * The VP assist page is an "overlay" page (see Hyper-V TLFS's

			 * Section 5.2.1 "GPA Overlay Pages"). Here it must be zeroed

			 * out to make sure we always write the EOI MSR in

			 * hv_apic_eoi_write() *after* the EOI optimization is disabled

			 * in hv_cpu_die(), otherwise a CPU may not be stopped in the

			 * case of CPU offlining and the VM will hang.

 Don't issue the callback if TSC accesses are not emulated */

	/*

	 * Check for required features and privileges to make TSC frequency

	 * change notifications work.

 Make sure callback is registered before we write to MSRs */

			/*

			 * For root partition the VP assist page is mapped to

			 * hypervisor provided page, and thus we unmap the

			 * page here and nullify it, so that in future we have

			 * correct page address mapped in hv_cpu_init.

		/*

		 * Reassign reenlightenment notifications to some other online

		 * CPU or just disable the feature if there are no online CPUs

		 * left (happens on hibernation).

	/*

	 * For Generation-2 VM, we exit from pci_arch_init() by returning 0.

	 * The purpose is to suppress the harmless warning:

	 * "PCI: Fatal: No config space access function found"

 For Generation-1 VM, we'll proceed in pci_arch_init().  */

	/*

	 * Reset the hypercall page as it is going to be invalidated

	 * across hibernation. Setting hv_hypercall_pg to NULL ensures

	 * that any subsequent hypercall operation fails safely instead of

	 * crashing due to an access of an invalid page. The hypercall page

	 * pointer is restored on resume.

 Disable the hypercall page in the hypervisor */

 Re-enable the hypercall page */

	/*

	 * Reenlightenment notifications are disabled by hv_cpu_die(0),

	 * reenable them here if hv_reenlightenment_cb was previously set.

 Note: when the ops are called, only CPU0 is online and IRQs are disabled. */

	/*

	 * Ignore any errors in setting up stimer clockevents

	 * as we can run with the LAPIC timer as a fallback.

	/*

	 * Still register the LAPIC timer, because the direct-mode STIMER is

	 * not supported by old versions of Hyper-V. This also allows users

	 * to switch to LAPIC timer via /sys, if they want to.

 No point in proceeding if this failed */

/*

 * This function is to be invoked early in the boot sequence after the

 * hypervisor has been detected.

 *

 * 1. Setup the hypercall page.

 * 2. Register Hyper-V specific clocksource.

 * 3. Setup Hyper-V specific APIC entry points.

	/*

	 * Setup the hypercall page and enable hypercalls.

	 * 1. Register the guest ID

	 * 2. Enable the hypercall and register the hypercall page

 Hyper-V requires to write guest os id via ghcb in SNP IVM. */

		/*

		 * For the root partition, the hypervisor will set up its

		 * hypercall page. The hypervisor guarantees it will not show

		 * up in the root's address space. The root can't change the

		 * location of the hypercall page.

		 *

		 * Order is important here. We must enable the hypercall page

		 * so it is populated with code, then copy the code to an

		 * executable page.

	/*

	 * hyperv_init() is called before LAPIC is initialized: see

	 * apic_intr_mode_init() -> x86_platform.apic_post_init() and

	 * apic_bsp_setup() -> setup_local_APIC(). The direct-mode STIMER

	 * depends on LAPIC, so hv_stimer_alloc() should be called from

	 * x86_init.timers.setup_percpu_clockev.

	/*

	 * If we're running as root, we want to create our own PCI MSI domain.

	 * We can't set this in hv_pci_init because that would be too late.

 Query the VMs extended capability once, so that it can be cached. */

/*

 * This routine is called before kexec/kdump, it does the required cleanup.

 Reset our OS id */

	/*

	 * Reset hypercall page reference before reset the page,

	 * let hypercall operations fail safely rather than

	 * panic the kernel for using invalid hypercall page

 Reset the hypercall page */

 Reset the TSC page */

	/*

	 * We prefer to report panic on 'die' chain as we have proper

	 * registers to report, but if we miss it (e.g. on BUG()) we need

	 * to report it on 'panic'.

	/*

	 * Let Hyper-V know there is crash data available

	/*

	 * Ensure that we're really on Hyper-V, and not a KVM or Xen

	 * emulation of Hyper-V

	/*

	 * Verify that earlier initialization succeeded by checking

	 * that the hypercall page is setup

 Each gva in gva_list encodes up to 4096 pages to flush */

/*

 * Fills in gva_list starting from offset. Returns the number of items added.

		/*

		 * Lower 12 bits encode the number of additional

		 * pages to flush (in addition to the 'cur' page).

	/*

	 * Only check the mask _after_ interrupt has been disabled to avoid the

	 * mask changing under our feet.

		/*

		 * AddressSpace argument must match the CR3 with PCID bits

		 * stripped out.

		/*

		 * From the supplied CPU set we need to figure out if we can get

		 * away with cheaper HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE}

		 * hypercalls. This is possible when the highest VP number in

		 * the set is < 64. As VP numbers are usually in ascending order

		 * and match Linux CPU ids, here is an optimization: we check

		 * the VP number for the highest bit in the supplied set first

		 * so we can quickly find out if using *_EX hypercalls is a

		 * must. We will also check all VP numbers when walking the

		 * supplied CPU set to remain correct in all cases.

	/*

	 * We can flush not more than max_gvas with one hypercall. Flush the

	 * whole address space if we were asked to do more.

		/*

		 * AddressSpace argument must match the CR3 with PCID bits

		 * stripped out.

	/*

	 * We can flush not more than max_gvas with one hypercall. Flush the

	 * whole address space if we were asked to do more.

 SPDX-License-Identifier: GPL-2.0

/*

 * Hyper-V specific APIC code.

 *

 * Copyright (C) 2018, Microsoft, Inc.

 *

 * Author : K. Y. Srinivasan <kys@microsoft.com>

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of the GNU General Public License version 2 as published

 * by the Free Software Foundation.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or

 * NON INFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

/*

 * IPI implementation on Hyper-V.

	/*

	 * Use HV_GENERIC_SET_ALL and avoid converting cpumask to VP_SET

	 * when the IPI is sent to all currently present CPUs.

		/*

		 * 'nr_bank <= 0' means some CPUs in cpumask can't be

		 * represented in VP_SET. Return an error and fall back to

		 * native (architectural) method of sending IPIs.

	/*

	 * Do nothing if

	 *   1. the mask is empty

	 *   2. the mask only contains self when exclude_self is true

	/*

	 * From the supplied CPU set we need to figure out if we can get away

	 * with cheaper HVCALL_SEND_IPI hypercall. This is possible when the

	 * highest VP number in the set is < 64. As VP numbers are usually in

	 * ascending order and match Linux CPU ids, here is an optimization:

	 * we check the VP number for the highest bit in the supplied set first

	 * so we can quickly find out if using HVCALL_SEND_IPI_EX hypercall is

	 * a must. We will also check all VP numbers when walking the supplied

	 * CPU set to remain correct in all cases.

		/*

		 * This particular version of the IPI hypercall can

		 * only target upto 64 CPUs.

		/*

		 * Set the IPI entry points.

		/*

		 * When in x2apic mode, don't use the Hyper-V specific APIC

		 * accessors since the field layout in the ICR register is

		 * different in x2apic mode. Furthermore, the architectural

		 * x2apic MSRs function just as well as the Hyper-V

		 * synthetic APIC MSRs, so there's no benefit in having

		 * separate Hyper-V accessors for x2apic mode. The only

		 * exception is hv_apic_eoi_write, because it benefits from

		 * lazy EOI when available, but the same accessor works for

		 * both xapic and x2apic because the field layout is the same.

 SPDX-License-Identifier: GPL-2.0

 unsupported modes and filters */

 no sampling */

 Careful, an NMI might modify the previous event value: */

 If valid, extract digital readout, otherwise set to -1: */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Support Intel/AMD RAPL energy consumption counters

 * Copyright (C) 2013 Google, Inc., Stephane Eranian

 *

 * Intel RAPL interface is specified in the IA-32 Manual Vol3b

 * section 14.7.1 (September 2013)

 *

 * AMD RAPL interface for Fam17h is described in the public PPR:

 * https://bugzilla.kernel.org/show_bug.cgi?id=206537

 *

 * RAPL provides more controls than just reporting energy consumption

 * however here we only expose the 3 energy consumption free running

 * counters (pp0, pkg, dram).

 *

 * Each of those counters increments in a power unit defined by the

 * RAPL_POWER_UNIT MSR. On SandyBridge, this unit is 1/(2^16) Joules

 * but it can vary.

 *

 * Counter to rapl events mappings:

 *

 *  pp0 counter: consumption of all physical cores (power plane 0)

 * 	  event: rapl_energy_cores

 *    perf code: 0x1

 *

 *  pkg counter: consumption of the whole processor package

 *	  event: rapl_energy_pkg

 *    perf code: 0x2

 *

 * dram counter: consumption of the dram domain (servers only)

 *	  event: rapl_energy_dram

 *    perf code: 0x3

 *

 * gpu counter: consumption of the builtin-gpu domain (client only)

 *	  event: rapl_energy_gpu

 *    perf code: 0x4

 *

 *  psys counter: consumption of the builtin-psys domain (client only)

 *	  event: rapl_energy_psys

 *    perf code: 0x5

 *

 * We manage those counters as free running (read-only). They may be

 * use simultaneously by other tools, such as turbostat.

 *

 * The events only support system-wide mode counting. There is no

 * sampling support because it does not make sense and is not

 * supported by the RAPL hardware.

 *

 * Because we want to avoid floating-point operations in the kernel,

 * the events are all reported in fixed point arithmetic (32.32).

 * Tools must adjust the counts to convert them to Watts using

 * the duration of the measurement. Tools may use a function such as

 * ldexp(raw_count, -32);

/*

 * RAPL energy status counters

 all cores */

 entire package */

 DRAM */

 gpu */

 psys */

/*

 * event code: LSB 8 bits, passed in attr->config

 * any other bit is reserved

 1/2^hw_unit Joule */

	/*

	 * The unsigned check also catches the '-1' return value for non

	 * existent mappings in the topology map.

	/*

	 * scale delta to smallest unit (1/2^32)

	 * users must then scale back: count * 1/(1e9*2^32) to get Joules

	 * or use ldexp(count, -32).

	 * Watts = Joules/Time delta

	/*

	 * Now we have the new raw value and have updated the prev

	 * timestamp already. We can now calculate the elapsed delta

	 * (event-)time and add that to the generic event.

	 *

	 * Careful, not all hw sign-extends above the physical width

	 * of the count.

 mark event as deactivated and stopped */

 check if update of sw counter is necessary */

		/*

		 * Drain the remaining delta count out of a event

		 * that we are disabling:

 only look at RAPL events */

 check only supported bits are set */

 check event supported */

 unsupported modes and filters */

 no sampling */

 must be done before validate_group */

/*

 * we compute in 0.23 nJ increments regardless of MSR

/*

 * There are no default events, but we need to create

 * "events" group (with empty attrs) before updating

 * it with detected events.

 Only lower 32bits of the MSR represents the energy counter */

/*

 * Force to PERF_RAPL_MAX size due to:

 * - perf_msr_probe(PERF_RAPL_MAX)

 * - want to use same event codes across both architectures

 Check if exiting cpu is used for collecting rapl events */

 Find a new cpu to collect rapl events */

 Migrate rapl events to the new target */

	/*

	 * Check if there is an online cpu in the package which collects rapl

	 * events already.

 protect rdmsrl() to handle virtualization */

	/*

	 * DRAM domain on HSW server and KNL has fixed energy unit which can be

	 * different than the unit from power unit MSR. See

	 * "Intel Xeon Processor E5-1600 and E5-2600 v3 Product Families, V2

	 * of 2. Datasheet, September 2014, Reference Number: 330784-001 "

	/*

	 * SPR shares the same DRAM domain energy unit as HSW, plus it

	 * also has a fixed energy unit for Psys domain.

	/*

	 * Calculate the timer rate:

	 * Use reference of 200W for scaling the timeout to avoid counter

	 * overflows. 200W = 200 Joules/sec

	 * Divide interval by 2 to avoid lockstep (2 * 100)

	 * if hw unit is 32, then we use 2 ms 1/200/2

	/*

	 * Install callbacks. Core will call them for each online cpu.

/*

 * Performance events x86 architecture code

 *

 *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>

 *  Copyright (C) 2008-2009 Red Hat, Inc., Ingo Molnar

 *  Copyright (C) 2009 Jaswinder Singh Rajput

 *  Copyright (C) 2009 Advanced Micro Devices, Inc., Robert Richter

 *  Copyright (C) 2008-2009 Red Hat, Inc., Peter Zijlstra

 *  Copyright (C) 2009 Intel Corporation, <markus.t.metzger@intel.com>

 *  Copyright (C) 2009 Google, Inc., Stephane Eranian

 *

 *  For licencing details see kernel-base/COPYING

/*

 * This here uses DEFINE_STATIC_CALL_NULL() to get a static_call defined

 * from just a typename, as opposed to an actual function.

/*

 * This one is magic, it will get called even when PMU init fails (because

 * there is no PMU), in which case it should simply return NULL.

/*

 * Propagate event elapsed time into the generic event.

 * Can only be executed on the CPU where the event is active.

 * Returns the delta events processed.

	/*

	 * Careful: an NMI might modify the previous event value.

	 *

	 * Our tactic to handle this is to first atomically read and

	 * exchange a new raw count - then add that new-prev delta

	 * count to the generic event atomically:

	/*

	 * Now we have the new raw value and have updated the prev

	 * timestamp already. We can now calculate the elapsed delta

	 * (event-)time and add that to the generic event.

	 *

	 * Careful, not all hw sign-extends above the physical width

	 * of the count.

/*

 * Find and validate any extra registers to set up.

 Check if the extra msrs can be safely accessed*/

	/*

	 * Check to see if the BIOS enabled any of the counters, if so

	 * complain and bail.

	/*

	 * If all the counters are enabled, the below test will always

	 * fail.  The tools will also become useless in this scenario.

	 * Just fail and disable the hardware counters.

	/*

	 * Read the current value, change it and read it back to see if it

	 * matches, this is needed to detect certain hardware emulators

	 * (qemu/kvm) that don't trap on the MSR access and always return 0s.

	/*

	 * We still allow the PMU driver to operate:

 undo the lbr/bts event accounting */

/*

 * Check if we can create event of a certain type (that no conflicting events

 * are present).

	/*

	 * When lbr_pt_coexist we allow PT to coexist with either LBR or BTS.

	 * LBR and BTS are still mutually exclusive.

	/*

	 * See the comment in x86_add_exclusive().

	/*

	 * The generic map:

/*

 * check that branch_sample_type is compatible with

 * settings needed for precise_ip > 1 which implies

 * using the LBR to capture ALL taken branches at the

 * priv levels of the measurement

 must capture all branches */

	/*

	 * ignore PERF_SAMPLE_BRANCH_HV, not supported on x86

 Support for constant skid */

 Support for IP fixup */

 There's no sense in having PEBS for non sampling events: */

	/*

	 * check that PEBS LBR correction does not conflict with

	 * whatever the user is asking with attr->branch_sample_type

 branch_sample_type is compatible */

			/*

			 * user did not specify  branch_sample_type

			 *

			 * For PEBS fixups, we capture all

			 * the branches at the priv level of the

			 * event.

	/*

	 * Generate PMC IRQs:

	 * (keep 'enabled' bit clear for now)

	/*

	 * Count user and OS events unless requested not to

 sample_regs_user never support XMM registers */

	/*

	 * Besides the general purpose registers, XMM registers may

	 * be collected in PEBS on some platforms, e.g. Icelake

/*

 * Setup the hardware configuration for a given attr_type

 mark unused */

/*

 * There may be PMI landing after enabled=0. The PMI hitting could be before or

 * after disable_all.

 *

 * If PMI hits before disable_all, the PMU will be disabled in the NMI handler.

 * It will not be re-enabled in the NMI handler again, because enabled=0. After

 * handling the NMI, disable_all will be called, which will not change the

 * state either. If PMI hits after disable_all, the PMU is already disabled

 * before entering NMI handler. The NMI handler will not change the state

 * either.

 *

 * So either situation is harmless.

	/*

	 * All CPUs of the hybrid type have been offline.

	 * The x86_get_pmu() should not be invoked.

/*

 * Event scheduler state:

 *

 * Assign events iterating over all events and counters, beginning

 * with events with least weights first. Keep the current iterator

 * state in struct sched_state.

 event index */

 counter index */

 number of events to be assigned left */

 number of GP counters used */

 Total max is X86_PMC_IDX_MAX, but we are O(n!) limited */

/*

 * Initialize iterator that runs through all events and counters.

 start with min weight */

 this assignment didn't work out */

 XXX broken vs EVENT_PAIR */

 try the next one */

/*

 * Select a counter for the current event to schedule. Return true on

 * success.

 Prefer fixed purpose counters */

 Grab the first unused counter starting with idx */

/*

 * Go through all unassigned events and find the next one to schedule.

 * Take events with the least weight first. Return true on success.

 next event */

 next weight */

 start with first counter */

/*

 * Assign a counter for each event.

 failed */

	/*

	 * Compute the number of events already present; see x86_pmu_add(),

	 * validate_group() and x86_pmu_commit_txn(). For the former two

	 * cpuc->n_events hasn't been updated yet, while for the latter

	 * cpuc->n_txn contains the number of events added in the current

	 * transaction.

		/*

		 * Previously scheduled events should have a cached constraint,

		 * while new events should not have one.

		/*

		 * Request constraints for new events; or for those events that

		 * have a dynamic constraint -- for those the constraint can

		 * change due to external factors (sibling state, allow_tfa).

	/*

	 * fastpath, try to reuse previous register

 never assigned */

 constraint still honored */

 not already used */

 slow path */

		/*

		 * Do not allow scheduling of more than half the available

		 * generic counters.

		 *

		 * This helps avoid counter starvation of sibling thread by

		 * ensuring at most half the counters cannot be in exclusive

		 * mode. There is no designated counters for the limits. Any

		 * N/2 counters can be used. This helps with events with

		 * specific counter constraints.

		/*

		 * Reduce the amount of available counters to allow fitting

		 * the extra Merge events needed by large increment events.

	/*

	 * In case of success (unsched = 0), mark events as committed,

	 * so we do not put_constraint() in case new events are added

	 * and fail to be scheduled

	 *

	 * We invoke the lower level commit callback to lock the resource

	 *

	 * We do not need to do all of this in case we are called to

	 * validate an event group (assign == NULL)

			/*

			 * release events that failed scheduling

/*

 * dogrp: true if must collect siblings events (group)

 * returns total number of events and error code

 current number of events already accepted */

		/*

		 * For PEBS->PT, if !aux_event, the group leader (PT) went

		 * away, the group was broken down and this singleton event

		 * can't schedule any more.

		/*

		 * pebs_output: 0: no PEBS so far, 1: PT, 2: DS

 All the metric events are mapped onto the fixed counter 3. */

/**

 * x86_perf_rdpmc_index - Return PMC counter used for event

 * @event: the perf_event to which the PMC counter was assigned

 *

 * The counter assigned to this performance event may change if interrupts

 * are enabled. This counter should thus never be used while interrupts are

 * enabled. Before this function is used to obtain the assigned counter the

 * event should be checked for validity using, for example,

 * perf_event_read_local(), within the same interrupt disabled section in

 * which this counter is planned to be used.

 *

 * Return: The index of the performance monitoring counter assigned to

 * @perf_event.

		/*

		 * apply assignment obtained either from

		 * hw_perf_group_sched_in() or x86_pmu_enable()

		 *

		 * step1: save events moving to new counters

			/*

			 * we can avoid reprogramming counter if:

			 * - assigned same counter as last time

			 * - running on same CPU as last time

			 * - no other event has used the counter since

			/*

			 * Ensure we don't accidentally enable a stopped

			 * counter simply because we rescheduled.

		/*

		 * step2: reprogram moved events into new counters

/*

 * Set the next IRQ period, based on the hwc->period_left value.

 * To be called with the event disabled in hw:

	/*

	 * If we are way outside a reasonable range then just skip forward:

	/*

	 * Quirk: certain CPUs dont like it if just 1 hw_event is left:

	/*

	 * The hw event starts counting from this event offset,

	 * mark it to be able to extra future deltas:

	/*

	 * Sign extend the Merge event counter's upper 16 bits since

	 * we currently declare a 48-bit counter width

	/*

	 * Due to erratum on certan cpu we need

	 * a second write to be sure the register

	 * is updated properly

/*

 * Add a single event to the PMU.

 *

 * The event is added to the group of enabled events

 * but only if it can be scheduled with existing events.

	/*

	 * If group events scheduling transaction was started,

	 * skip the schedulability test here, it will be performed

	 * at commit time (->commit_txn) as a whole.

	 *

	 * If commit fails, we'll call ->del() on all events

	 * for which ->add() was called.

	/*

	 * copy new assignment, now we know it is possible

	 * will be used by hw_perf_enable()

	/*

	 * Commit the collect_events() state. See x86_pmu_del() and

	 * x86_pmu_*_txn().

	/*

	 * This is before x86_pmu_enable() will call x86_pmu_start(),

	 * so we enable LBRs before an event needs them etc..

		/*

		 * Drain the remaining delta count out of a event

		 * that we are disabling:

	/*

	 * If we're called during a txn, we only need to undo x86_pmu.add.

	 * The events never got scheduled and ->cancel_txn will truncate

	 * the event_list.

	 *

	 * XXX assumes any ->del() called during a TXN will only be on

	 * an event added during that same TXN.

	/*

	 * Not a TXN, therefore cleanup properly.

 called ->del() without ->add() ? */

 If we have a newly added event; make sure to decrease n_added. */

 Delete the array entry. */

	/*

	 * This is after x86_pmu_stop(); so we disable LBRs after any

	 * event can need them etc..

	/*

	 * Some chipsets need to unmask the LVTPC in a particular spot

	 * inside the nmi handler.  As a result, the unmasking was pushed

	 * into all the nmi handlers.

	 *

	 * This generic handler doesn't seem to have any issues where the

	 * unmasking occurs so it was left at the top.

		/*

		 * event overflow

	/*

	 * Always use NMI for PMU

	/*

	 * All PMUs/events that share this PMI handler should make sure to

	 * increment active_events for their events.

	/*

	 * If we have a PMU initialized but no APIC

	 * interrupts, we cannot sample hardware

	 * events (user-space has to fall back and

	 * sample via a hrtimer based software event):

 string trumps id */

	/*

	 * Report conditional events depending on Hyper-Threading.

	 *

	 * This is overly conservative as usually the HT special

	 * handling is not needed if the other CPU thread is idle.

	 *

	 * Note this does not (and cannot) handle the case when thread

	 * siblings are invisible, for example with virtualization

	 * if they are owned by some other guest.  The user tool

	 * has to re-read when a thread sibling gets onlined later.

	/*

	 * Hybrid PMUs may support the same event name, but with different

	 * event encoding, e.g., the mem-loads event on an Atom PMU has

	 * different event encoding from a Core PMU.

	 *

	 * The event_str includes all event encodings. Each event encoding

	 * is divided by ";". The order of the event encodings must follow

	 * the order of the hybrid PMU index.

/*

 * Remove all undefined events (x86_pmu.event_map(id) == 0)

 * out of events_attr attributes.

 str trumps id */

	/*

	* We have whole page size to spend and just little data

	* to write, so we can safely use sprintf.

/*

 * The generic code is not hybrid friendly. The hybrid_pmu->pmu

 * of the first registered PMU is unconditionally assigned to

 * each possible cpuctx->ctx.pmu.

 * Update the correct hybrid PMU to the cpuctx->ctx.pmu.

 sanity check that the hardware exists or is emulated */

 enable userspace RDPMC usage by default */

	/*

	 * Install callbacks. Core will call them for each online

	 * cpu.

/*

 * Start group events scheduling transaction

 * Set the flag to make pmu::enable() not perform the

 * schedulability test, it will be performed at commit time

 *

 * We only support PERF_PMU_TXN_ADD transactions. Save the

 * transaction flags but otherwise ignore non-PERF_PMU_TXN_ADD

 * transactions.

 txn already in flight */

/*

 * Stop group events scheduling transaction

 * Clear the flag and pmu::enable() will perform the

 * schedulability test.

 no txn in flight */

	/*

	 * Truncate collected array by the number of events added in this

	 * transaction. See x86_pmu_add() and x86_pmu_*_txn().

/*

 * Commit group events scheduling transaction

 * Perform the group schedulability test as a whole

 * Return 0 if success

 *

 * Does not cancel the transaction on failure; expects the caller to do this.

 no txn in flight */

	/*

	 * copy new assignment, now we know it is possible

	 * will be used by hw_perf_enable()

/*

 * a fake_cpuc is used to validate event groups. Due to

 * the extra reg logic, we need to also allocate a fake

 * per_core and per_cpu structure. Otherwise, group events

 * using extra reg may conflict without the kernel being

 * able to catch this when the last event gets added to

 * the group.

/*

 * validate that we can schedule this event

/*

 * validate a single event group

 *

 * validation include:

 *	- check events are compatible which each other

 *	- events do not compete for the same counter

 *	- number of events <= number of counters

 *

 * validation ensures the group can be loaded onto the

 * PMU if it was the only group available.

	/*

	 * Reject events from different hybrid PMUs.

	/*

	 * the event is not yet connected with its

	 * siblings therefore we must first collect

	 * existing siblings, then add the new event

	 * before we can simulate the scheduling

 Don't need to clear the assigned counter. */

 Metrics and fake events don't have corresponding HW counters. */

	/*

	 * This function relies on not being called concurrently in two

	 * tasks in the same mm.  Otherwise one task could observe

	 * perf_rdpmc_allowed > 1 and return all the way back to

	 * userspace with CR4.PCE clear while another task is still

	 * doing on_each_cpu_mask() to propagate CR4.PCE.

	 *

	 * For now, this can't happen because all callers hold mmap_lock

	 * for write.  If this changes, we'll need a different solution.

		/*

		 * Changing into or out of never available or always available,

		 * aka perf-event-bypassing mode. This path is extremely slow,

		 * but only root can trigger it, so it's okay.

	/*

	 * Internal timekeeping for enabled/running/stopped times

	 * is always in the local_clock domain.

	/*

	 * cap_user_time_zero doesn't make sense when we're using a different

	 * time base for the records.

/*

 * Determine whether the regs were taken from an irq/exception handler rather

 * than from perf_arch_fetch_caller_regs().

 TODO: We don't support guest os callchain now */

 IRQs are off, so this synchronizes with smp_store_release */

 32-bit process in 64-bit kernel. */

 TODO: We don't support guest os callchain now */

	/*

	 * We don't know what to do with VM86 stacks.. ignore them for now.

/*

 * Deal with code segment offsets for the various execution modes:

 *

 *   VM86 - the good olde 16 bit days, where the linear address is

 *          20 bits and we use regs->ip + 0x10 * regs->cs.

 *

 *   IA32 - Where we need to look at GDT/LDT segment descriptor tables

 *          to figure out what the 32bit base address is.

 *

 *    X32 - has TIF_X32 set, but is running in x86_64

 *

 * X86_64 - CS,DS,SS,ES are all zero based.

	/*

	 * For IA32 we look at the GDT/LDT segment base to convert the

	 * effective IP to a linear address.

	/*

	 * If we are in VM86 mode, add the segment offset to convert to a

	 * linear address.

	/*

	 * KVM doesn't support the hybrid PMU yet.

	 * Return the common value in global x86_pmu,

	 * which available for all cores.

 SPDX-License-Identifier: GPL-2.0

/*

 * Accepts msr[] array with non populated entries as long as either

 * msr[i].msr is 0 or msr[i].grp is NULL. Note that the default sysfs

 * visibility is visible when group->is_visible callback is set.

 skip entry with no group */

 skip unpopulated entry */

 Virt sucks; you cannot tell if a R/O MSR is present :/ */

 Disable zero counters if requested. */

 SPDX-License-Identifier: GPL-2.0

 Nehalem/SandBridge/Haswell/Broadwell/Skylake uncore support */

 Uncore IMC PCI IDs */

 SNB event control */

 SNB global control register */

 SNB uncore global control */

 SNB Cbo register */

 SNB ARB register */

 NHM global control register */

 NHM uncore global control */

 NHM uncore register */

 SKL uncore global control */

 ICL Cbo register */

 ICL ARB register */

 ADL uncore global control */

 ADL Cbo register */

 ADL ARB register */

 Sandy Bridge uncore support */

 end: all zeroes */ },

 The 8th CBOX has different MSR space */

 end: all zeroes */ },

 end: all zeroes */ },

 page size multiple covering all config regs */

 BW break down- legacy counters */

/*

 * Keep the custom event_init() function compatible with old event

 * encoding for free running counters.

 no device found for this pmu */

 Sampling not supported yet */

 unsupported modes and filters */

 no sampling */

	/*

	 * Place all uncore events for a particular physical package

	 * onto a single cpu

 check only supported bits are set */

	/*

	 * check event is known (whitelist, determines counter)

 must be done before validate_group */

 Convert to standard encoding format for freerunning counters */

 no group validation needed, we have free running counters */

 IMC */

 end: all zeroes */ },

 IMC */

 IMC */

 end: all zeroes */ },

 IMC */

 IMC */

 end: all zeroes */ },

 IMC */

 end: all zeroes */ },

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 end: all zeroes */ },

 IMC */

 IMC */

 IMC */

 IMC */

 end: all zeroes */ },

 3rd Gen Core processor */

 Xeon E3-1200 v2/3rd Gen Core processor */

 4th Gen Core Processor */

 4th Gen Core ULT Mobile Processor */

 5th Gen Core U */

 6th Gen Core Y */

 6th Gen Core U */

 6th Gen Core H Dual Core */

 6th Gen Core H Quad Core */

 6th Gen Core S Dual Core */

 6th Gen Core S Quad Core */

 Xeon E3 V5 Gen Core processor */

 7th Gen Core Y */

 7th Gen Core U */

 7th Gen Core U Quad Core */

 7th Gen Core S Dual Core */

 7th Gen Core S Quad Core */

 7th Gen Core H Quad Core */

 7th Gen Core S 4 cores Work Station */

 8th Gen Core U 2 Cores */

 8th Gen Core U 4 Cores */

 8th Gen Core H 4 Cores */

 8th Gen Core H 6 Cores */

 8th Gen Core S 2 Cores Desktop */

 8th Gen Core S 4 Cores Desktop */

 8th Gen Core S 6 Cores Desktop */

 8th Gen Core S 8 Cores Desktop */

 8th Gen Core S 4 Cores Work Station */

 8th Gen Core S 6 Cores Work Station */

 8th Gen Core S 8 Cores Work Station */

 8th Gen Core S 4 Cores Server */

 8th Gen Core S 6 Cores Server */

 8th Gen Core S 8 Cores Server */

 8th Gen Core Y Mobile Dual Core */

 8th Gen Core Y Mobile Quad Core */

 8th Gen Core U Mobile Quad Core */

 8th Gen Core U Mobile Quad Core */

 8th Gen Core U Mobile Dual Core */

 10th Gen Core Mobile */

 10th Gen Core Mobile */

 end marker */ }

 end of Sandy Bridge uncore support */

 Nehalem uncore support */

 end: all zeroes */ },

 end of Nehalem uncore support */

 Tiger Lake MMIO uncore support */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 IMC */

 end: all zeroes */ }

 end: all zeroes */ }

 MCHBAR is disabled */

 end of Tiger Lake MMIO uncore support */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * BTS PMU driver for perf

 * Copyright (c) 2013-2014, Intel Corporation.

 BTS context states: */

 no ongoing AUX transactions */

 AUX transaction is on, BTS tracing is disabled */

 AUX transaction is on, BTS tracing is running */

 multiple of BTS_RECORD_SIZE */

 count all the high order buffers */

	/*

	 * to avoid interrupts in overwrite mode, only allow one physical

		/*

		 * old and head are always in the same physical buffer, so we

		 * can subtract them to get the data size.

	/*

	 * Since BTS is coherent, just add compiler barrier to ensure

	 * BTS updating is ordered against bts::handle::event.

/*

 * Ordering PMU callbacks wrt themselves and the PMI is done by means

 * of bts::state, which:

 *  - is set when bts::handle::event is valid, that is, between

 *    perf_aux_output_begin() and perf_aux_output_end();

 *  - is zero otherwise;

 *  - is ordered against bts::handle::event with a compiler barrier.

	/*

	 * local barrier to make sure that ds configuration made it

	 * before we enable BTS and bts::state goes ACTIVE

 INACTIVE/STOPPED -> ACTIVE */

 ACTIVE -> INACTIVE(PMI)/STOPPED(->stop()) */

	/*

	 * No extra synchronization is mandated by the documentation to have

	 * BTS data stores globally visible.

	/*

	 * Here we transition from INACTIVE to ACTIVE;

	 * if we instead are STOPPED from the interrupt handler,

	 * stay that way. Can't be ACTIVE here though.

	/*

	 * Here we transition from ACTIVE to INACTIVE;

	 * do nothing for STOPPED or INACTIVE.

 See if next phys buffer has more space */

 Advance to next phys buffer */

				/*

				 * After this, cur_buf and head won't match ds

				 * anymore, so we must not be racing with

				 * bts_update().

 Don't go far beyond wakeup watermark */

	/*

	 * If we have no space, the lost notification would have been sent when

	 * we hit absolute_maximum - see bts_update()

	/*

	 * The only surefire way of knowing if this NMI is ours is by checking

	 * the write ptr against the PMI threshold.

	/*

	 * this is wrapped in intel_bts_enable_local/intel_bts_disable_local,

	 * so we can only be INACTIVE or STOPPED

	/*

	 * Skip snapshot counters: they don't use the interrupt, but

	 * there's no other way of telling, because the pointer will

	 * keep moving

 no new data */

			/*

			 * BTS_STATE_STOPPED should be visible before

			 * cleared handle::event

	/*

	 * BTS leaks kernel addresses even when CPL0 tracing is

	 * disabled, so disallow intel_bts driver for unprivileged

	 * users on paranoid systems since it provides trace data

	 * to the user in a zero-copy fashion.

	 *

	 * Note that the default paranoia setting permits unprivileged

	 * users to profile the kernel.

		/*

		 * BTS hardware writes through a virtual memory map we must

		 * either use the kernel physical map, or the user mapping of

		 * the AUX buffer.

		 *

		 * However, since this driver supports per-CPU and per-task inherit

		 * we cannot use the user mapping since it will not be available

		 * if we're not running the owning process.

		 *

		 * With PTI we can't use the kernel map either, because its not

		 * there when we run userspace.

		 *

		 * For now, disable this driver when using PTI.

 SPDX-License-Identifier: GPL-2.0-only */

/*

 * Support Intel uncore PerfMon discovery mechanism.

 * Copyright(c) 2021 Intel Corporation.

 A discovery table device has the unique capability ID. */

	/*

	 * If the NUMA info is not available, assume that the logical die id is

	 * continuous in the order in which the discovery table devices are

	 * detected.

	/*

	 * All CPUs of a node may be offlined. For this case,

	 * the PCI and MMIO type of uncore blocks which are

	 * enumerated by the device will be unavailable.

 Store the first box of each die */

 Store generic information for the first box */

 Read Global Discovery State */

 Parsing Unit Discovery State */

	/*

	 * Start a new search and iterates through the list of

	 * the discovery table devices.

 None of the discovery tables are available */

/*

 * Support cstate residency counters

 *

 * Copyright (C) 2015, Intel Corp.

 * Author: Kan Liang (kan.liang@intel.com)

 *

 * This library is free software; you can redistribute it and/or

 * modify it under the terms of the GNU Library General Public

 * License as published by the Free Software Foundation; either

 * version 2 of the License, or (at your option) any later version.

 *

 * This library is distributed in the hope that it will be useful,

 * but WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU

 * Library General Public License for more details.

 *

/*

 * This file export cstate related free running (read-only) counters

 * for perf. These counters may be use simultaneously by other tools,

 * such as turbostat. However, it still make sense to implement them

 * in perf. Because we can conveniently collect them together with

 * other events, and allow to use them from tools without special MSR

 * access code.

 *

 * The events only support system-wide mode counting. There is no

 * sampling support because it is not supported by the hardware.

 *

 * According to counters' scope and category, two PMUs are registered

 * with the perf_event core subsystem.

 *  - 'cstate_core': The counter is available for each physical core.

 *    The counters include CORE_C*_RESIDENCY.

 *  - 'cstate_pkg': The counter is available for each physical package.

 *    The counters include PKG_C*_RESIDENCY.

 *

 * All of these counters are specified in the Intel® 64 and IA-32

 * Architectures Software Developer.s Manual Vol3b.

 *

 * Model specific counters:

 *	MSR_CORE_C1_RES: CORE C1 Residency Counter

 *			 perf code: 0x00

 *			 Available model: SLM,AMT,GLM,CNL,ICX,TNT,ADL

 *			 Scope: Core (each processor core has a MSR)

 *	MSR_CORE_C3_RESIDENCY: CORE C3 Residency Counter

 *			       perf code: 0x01

 *			       Available model: NHM,WSM,SNB,IVB,HSW,BDW,SKL,GLM,

 *						CNL,KBL,CML,TNT

 *			       Scope: Core

 *	MSR_CORE_C6_RESIDENCY: CORE C6 Residency Counter

 *			       perf code: 0x02

 *			       Available model: SLM,AMT,NHM,WSM,SNB,IVB,HSW,BDW,

 *						SKL,KNL,GLM,CNL,KBL,CML,ICL,ICX,

 *						TGL,TNT,RKL,ADL

 *			       Scope: Core

 *	MSR_CORE_C7_RESIDENCY: CORE C7 Residency Counter

 *			       perf code: 0x03

 *			       Available model: SNB,IVB,HSW,BDW,SKL,CNL,KBL,CML,

 *						ICL,TGL,RKL,ADL

 *			       Scope: Core

 *	MSR_PKG_C2_RESIDENCY:  Package C2 Residency Counter.

 *			       perf code: 0x00

 *			       Available model: SNB,IVB,HSW,BDW,SKL,KNL,GLM,CNL,

 *						KBL,CML,ICL,ICX,TGL,TNT,RKL,ADL

 *			       Scope: Package (physical package)

 *	MSR_PKG_C3_RESIDENCY:  Package C3 Residency Counter.

 *			       perf code: 0x01

 *			       Available model: NHM,WSM,SNB,IVB,HSW,BDW,SKL,KNL,

 *						GLM,CNL,KBL,CML,ICL,TGL,TNT,RKL,

 *						ADL

 *			       Scope: Package (physical package)

 *	MSR_PKG_C6_RESIDENCY:  Package C6 Residency Counter.

 *			       perf code: 0x02

 *			       Available model: SLM,AMT,NHM,WSM,SNB,IVB,HSW,BDW,

 *						SKL,KNL,GLM,CNL,KBL,CML,ICL,ICX,

 *						TGL,TNT,RKL,ADL

 *			       Scope: Package (physical package)

 *	MSR_PKG_C7_RESIDENCY:  Package C7 Residency Counter.

 *			       perf code: 0x03

 *			       Available model: NHM,WSM,SNB,IVB,HSW,BDW,SKL,CNL,

 *						KBL,CML,ICL,TGL,RKL,ADL

 *			       Scope: Package (physical package)

 *	MSR_PKG_C8_RESIDENCY:  Package C8 Residency Counter.

 *			       perf code: 0x04

 *			       Available model: HSW ULT,KBL,CNL,CML,ICL,TGL,RKL,

 *						ADL

 *			       Scope: Package (physical package)

 *	MSR_PKG_C9_RESIDENCY:  Package C9 Residency Counter.

 *			       perf code: 0x05

 *			       Available model: HSW ULT,KBL,CNL,CML,ICL,TGL,RKL,

 *						ADL

 *			       Scope: Package (physical package)

 *	MSR_PKG_C10_RESIDENCY: Package C10 Residency Counter.

 *			       perf code: 0x06

 *			       Available model: HSW ULT,KBL,GLM,CNL,CML,ICL,TGL,

 *						TNT,RKL,ADL

 *			       Scope: Package (physical package)

 *

 Model -> events mapping */

 Quirk flags */

 cstate_core PMU */

/*

 * There are no default events, but we need to create

 * "events" group (with empty attrs) before updating

 * it with detected events.

 cstate_pkg PMU */

 unsupported modes and filters */

 no sampling */

/*

 * Check if exiting cpu is the designated reader. If so migrate the

 * events when there is a valid target available

 Migrate events if there is a valid target */

 Migrate events if there is a valid target */

	/*

	 * If this is the first online thread of that core, set it in

	 * the core cpu mask as the designated reader.

	/*

	 * If this is the first online thread of that package, set it

	 * in the package cpu mask as the designated reader.

 SLM has different MSR for PKG C6 */

 KNL has different MSR for CORE C6 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel(R) Processor Trace PMU driver for perf

 * Copyright (c) 2013-2014, Intel Corporation.

 *

 * Intel PT is specified in the Intel Architecture Instruction Set Extensions

 * Programming Reference:

 * http://software.intel.com/en-us/intel-isa-extensions

/*

 * Capabilities of Intel PT hardware, such as number of address bits or

 * supported output schemes, are cached and exported to userspace as "caps"

 * attribute group of pt pmu device

 * (/sys/bus/event_source/devices/intel_pt/caps/) so that userspace can store

 * relevant bits together with intel_pt traces.

 *

 * These are necessary for both trace decoding (payloads_lip, contains address

 * width encoded in IP-related packets), and event configuration (bitmasks with

 * permitted values for certain bit fields).

	/*

	 * if available, read in TSC to core crystal clock ratio,

	 * otherwise, zero for numerator stands for "not enumerated"

	 * as per SDM

 model-specific quirks */

 not setting BRANCH_EN will #GP, erratum BDM106 */

		/*

		 * Intel SDM, 36.5 "Tracing post-VMXON" says that

		 * "IA32_VMX_MISC[bit 14]" being 1 means PT can trace

		 * post-VMXON.

/*

 * Bit 0 (TraceEn) in the attr.config is meaningless as the

 * corresponding bit in the RTIT_CTL can only be controlled

 * by the driver; therefore, repurpose it to mean: pass

 * through the bit that was previously assumed to be always

 * on for PT, thereby allowing the user to *not* set it if

 * they so wish. See also pt_event_valid() and pt_config().

		/*

		 * In the unlikely case that CPUID lists valid mtc periods,

		 * but not the mtc capability, drop out here.

		 *

		 * Spec says that setting mtc period bits while mtc bit in

		 * CPUID is 0 will #GP, so better safe than sorry.

 FUPonPTW without PTW doesn't make sense */

	/*

	 * Setting bit 0 (TraceEn in RTIT_CTL MSR) in the attr.config

	 * clears the assumption that BranchEn must always be enabled,

	 * as was the case with the first implementation of PT.

	 * If this bit is not set, the legacy behavior is preserved

	 * for compatibility with the older userspace.

	 *

	 * Re-using bit 0 for this purpose is fine because it is never

	 * directly set by the user; previous attempts at setting it in

	 * the attr.config resulted in -EINVAL.

		/*

		 * Disallow not setting BRANCH_EN where BRANCH_EN is

		 * always required.

		/*

		 * Disallow BRANCH_EN without the PASSTHROUGH.

/*

 * PT configuration helpers

 * These all are cpu affine and operate on a local PT

 Address ranges and their corresponding msr configuration registers */

		/*

		 * Note, if the range has zero start/end addresses due

		 * to its dynamic object not being loaded yet, we just

		 * go ahead and program zeroed range, which will simply

		 * produce no data. Note^2: if executable code at 0x0

		 * is a concern, we can set up an "invalid" configuration

		 * such as msr_b < msr_a.

 avoid redundant msr writes */

 First round: clear STATUS, in particular the PSB byte counter. */

	/*

	 * Previously, we had BRANCH_EN on by default, but now that PT has

	 * grown features outside of branch tracing, it is useful to allow

	 * the user to disable it. Setting bit 0 in the event's attr.config

	 * allows BRANCH_EN to pass through instead of being always on. See

	 * also the comment in pt_event_valid().

 may be already stopped by a PMI */

	/*

	 * A wrmsr that disables trace generation serializes other PT

	 * registers and causes all data packets to be written to memory,

	 * but a fence is required for the data to become globally visible.

	 *

	 * The below WMB, separating data store and aux_head store matches

	 * the consumer's RMB that separates aux_head load and data load.

/**

 * struct topa - ToPA metadata

 * @list:	linkage to struct pt_buffer's list of tables

 * @offset:	offset of the first entry in this table in the buffer

 * @size:	total size of all entries in this table

 * @last:	index of the last initialized entry in this table

 * @z_count:	how many times the first entry repeats

/*

 * Keep ToPA table-related metadata on the same page as the actual table,

 * taking up a few words from the top

/**

 * struct topa_page - page-sized ToPA table with metadata at the top

 * @table:	actual ToPA table entries, as understood by PT hardware

 * @topa:	metadata

 make -1 stand for the last table entry */

/**

 * topa_alloc() - allocate page-sized ToPA table

 * @cpu:	CPU on which to allocate.

 * @gfp:	Allocation flags.

 *

 * Return:	On success, return the pointer to ToPA table page.

	/*

	 * In case of singe-entry ToPA, always put the self-referencing END

	 * link as the 2nd entry in the table

/**

 * topa_free() - free a page-sized ToPA table

 * @topa:	Table to deallocate.

/**

 * topa_insert_table() - insert a ToPA table into a buffer

 * @buf:	 PT buffer that's being extended.

 * @topa:	 New topa table to be inserted.

 *

 * If it's the first table in this buffer, set up buffer's pointers

 * accordingly; otherwise, add a END=1 link entry to @topa to the current

 * "last" table and adjust the last table pointer to @topa.

/**

 * topa_table_full() - check if a ToPA table is filled up

 * @topa:	ToPA table.

 single-entry ToPA is a special case */

/**

 * topa_insert_pages() - create a list of ToPA tables

 * @buf:	PT buffer being initialized.

 * @gfp:	Allocation flags.

 *

 * This initializes a list of ToPA tables with entries from

 * the data_pages provided by rb_alloc_aux().

 *

 * Return:	0 on success or error code.

/**

 * pt_topa_dump() - print ToPA tables and their entries

 * @buf:	PT buffer.

/**

 * pt_buffer_advance() - advance to the next output region

 * @buf:	PT buffer.

 *

 * Advance the current pointers in the buffer to the next ToPA entry.

/**

 * pt_update_head() - calculate current offsets and sizes

 * @pt:		Per-cpu pt context.

 *

 * Update buffer's current write pointer position and data size.

 offset of the first region in this table from the beginning of buf */

 offset of the current output region within this table */

/**

 * pt_buffer_region() - obtain current output region's address

 * @buf:	PT buffer.

/**

 * pt_buffer_region_size() - obtain current output region's size

 * @buf:	PT buffer.

/**

 * pt_handle_status() - take care of possible status conditions

 * @pt:		Per-cpu pt context.

		/*

		 * On systems that only do single-entry ToPA, hitting STOP

		 * means we are already losing data; need to let the decoder

		 * know.

	/*

	 * Also on single-entry ToPA implementations, interrupt will come

	 * before the output reaches its output region's boundary.

 everything within this margin needs to be zeroed out */

/**

 * pt_read_offset() - translate registers into buffer pointers

 * @buf:	PT buffer.

 *

 * Set buffer's output pointers from MSR values.

 offset within current output region */

 index of current output region within this table */

	/*

	 * Indicates a bug in the caller.

	/*

	 * First, find the ToPA table where @pg fits. With high

	 * order allocations, there shouldn't be many of these.

	/*

	 * Hitting this means we have a problem in the ToPA

	 * allocation code.

	/*

	 * Indicates a problem in the ToPA allocation code.

	/*

	 * Multiple entries at the beginning of the table have the same size,

	 * ideally all of them; if @pg falls there, the search is done.

	/*

	 * Otherwise, slow path: iterate through the remaining entries.

	/*

	 * Means we couldn't find a ToPA entry in the table that does match.

/**

 * pt_buffer_reset_markers() - place interrupt and stop bits in the buffer

 * @buf:	PT buffer.

 * @handle:	Current output handle.

 *

 * Place INT and STOP marks to prevent overwriting old data that the consumer

 * hasn't yet collected and waking up the consumer after a certain fraction of

 * the buffer has filled up. Only needed and sensible for non-snapshot counters.

 *

 * This obviously relies on buf::head to figure out buffer markers, so it has

 * to be called after pt_buffer_reset_offsets() and before the hardware tracing

 * is enabled.

 can't stop in the middle of an output region */

 single entry ToPA is handled by marking all regions STOP=1 INT=1 */

 clear STOP and INT from current entry */

 how many pages till the STOP marker */

 if it's on a page boundary, fill up one more page */

 in the worst case, wake up the consumer one page before hard stop */

/**

 * pt_buffer_reset_offsets() - adjust buffer's write pointers from aux_head

 * @buf:	PT buffer.

 * @head:	Write pointer (aux_head) from AUX buffer.

 *

 * Find the ToPA table and entry corresponding to given @head and set buffer's

 * "current" pointers accordingly. This is done after we have obtained the

 * current aux_head position from a successful call to perf_aux_output_begin()

 * to make sure the hardware is writing to the right place.

 *

 * This function modifies buf::{cur,cur_idx,output_off} that will be programmed

 * into PT msrs when the tracing is enabled and buf::head and buf::data_size,

 * which are used to determine INT and STOP markers' locations by a subsequent

 * call to pt_buffer_reset_markers().

/**

 * pt_buffer_fini_topa() - deallocate ToPA structure of a buffer

 * @buf:	PT buffer.

		/*

		 * right now, this is in free_aux() path only, so

		 * no need to unlink this table from the list

/**

 * pt_buffer_init_topa() - initialize ToPA table for pt buffer

 * @buf:	PT buffer.

 * @size:	Total size of all regions within this ToPA.

 * @gfp:	Allocation flags.

 link last table to the first one, unless we're double buffering */

	/*

	 * We can use single range output mode

	 * + in snapshot mode, where we don't need interrupts;

	 * + if the hardware supports it;

	 * + if the entire buffer is one contiguous allocation.

/**

 * pt_buffer_setup_aux() - set up topa tables for a PT buffer

 * @cpu:	Cpu on which to allocate, -1 means current.

 * @pages:	Array of pointers to buffer pages passed from perf core.

 * @nr_pages:	Number of pages in the buffer.

 * @snapshot:	If this is a snapshot/overwrite counter.

 *

 * This is a pmu::setup_aux callback that sets up ToPA tables and all the

 * bookkeeping for an AUX buffer.

 *

 * Return:	Our private PT buffer structure.

	/*

	 * Only support AUX sampling in snapshot mode, where we don't

	 * generate NMIs.

/**

 * pt_buffer_free_aux() - perf AUX deallocation path callback

 * @data:	PT buffer.

		/*

		 * PT doesn't support single address triggers and

		 * 'start' filters.

 apply the offset */

/**

 * intel_pt_interrupt() - PT PMI handler

	/*

	 * There may be a dangling PT bit in the interrupt status register

	 * after PT has been disabled by pt_event_stop(). Make sure we don't

	 * do anything (particularly, re-enable) for this event here.

 snapshot counters don't use PMI, so it's safe */

 PT plays nice with VMX, do nothing */

	/*

	 * VMXON will clear RTIT_CTL.TraceEn; we need to make

	 * sure to not try to set it while VMX is on. Disable

	 * interrupts to avoid racing with pmu callbacks;

	 * concurrent PMI should be handled fine.

	/*

	 * If an AUX transaction is in progress, it will contain

	 * gap(s), so flag it PARTIAL to inform the user.

 Turn PTs back on */

/*

 * PMU callbacks

	/*

	 * Protect against the PMI racing with disabling wrmsr,

	 * see comment in intel_pt_interrupt().

	/*

	 * Sampling is only allowed on snapshot events;

	 * see pt_buffer_setup_aux().

	/*

	 * Here, handle_nmi tells us if the tracing is on

	/*

	 * If the tracing was on when we turned up, restart it.

	 * Compiler barrier not needed as we couldn't have been

	 * preempted by anything that touches pt->handle_nmi.

 SPDX-License-Identifier: GPL-2.0

 Driver for Intel Xeon Phi "Knights Corner" PMU */

 On Xeon Phi event "0" is a valid DATA_READ          */

   (L1 Data Cache Reads) Instruction.                */

 We code this as ARCH_PERFMON_EVENTSEL_INT as this   */

 bit will always be set in x86_pmu_hw_config().      */

 DATA_READ           */

 DATA_READ_MISS      */

 DATA_WRITE          */

 DATA_WRITE_MISS     */

 L1_DATA_PF1         */

 L1_DATA_PF1_MISS    */

 CODE_READ          */

 CODE_CACHE_MISS    */

 L2_READ_MISS */

 L2_WRITE_HIT */

 L2_DATA_PF2      */

 L2_DATA_PF2_MISS */

 DATA_READ */

 see note on L1 OP_READ */

 DATA_PAGE_WALK */

 DATA_WRITE */

 DATA_PAGE_WALK */

 CODE_READ */

 CODE_PAGE_WALK */

 BRANCHES */

 BRANCHES_MISPREDICTED */

 HWP_L2HIT */

 HWP_L2MISS */

 L2_READ_HIT_E */

 L2_READ_HIT_M */

 L2_READ_HIT_S */

 L2_READ_MISS */

 L2_WRITE_HIT */

 L2_STRONGLY_ORDERED_STREAMING_VSTORES_MISS */

 L2_WEAKLY_ORDERED_STREAMING_VSTORE_MISS */

 L2_VICTIM_REQ_WITH_DATA */

 SNP_HITM_BUNIT */

 SNP_HIT_L2 */

 SNP_HITM_L2 */

 L2_DATA_READ_MISS_CACHE_FILL */

 L2_DATA_WRITE_MISS_CACHE_FILL */

 L2_DATA_READ_MISS_MEM_FILL */

 L2_DATA_WRITE_MISS_MEM_FILL */

 L2_DATA_PF2 */

 L2_DATA_PF2_DROP */

 L2_DATA_PF2_MISS */

 L2_DATA_HIT_INFLIGHT_PF2 */

	/*

	 * Repeat if there is more work to be done:

 Only restore PMU state when it's active. See x86_pmu_disable(). */

 SPDX-License-Identifier: GPL-2.0

 SandyBridge-EP/IvyTown uncore support */

 SNB-EP pci bus to socket mapping */

 SNB-EP Box level control */

 SNB-EP event control */

 SNB-EP Ubox event control */

 SNB-EP PCU event control */

 SNB-EP pci control register */

 SNB-EP pci counter register */

 SNB-EP home agent register */

 SNB-EP memory controller register */

 SNB-EP QPI register */

 SNB-EP Ubox register */

 SNB-EP Cbo register */

 SNB-EP PCU register */

 IVBEP event control */

 IVBEP Ubox */

 IVBEP Cbo */

 IVBEP home agent */

 IVBEP PCU */

 IVBEP QPI */

 Haswell-EP Ubox */

 Haswell-EP CBo */

 Haswell-EP Sbox */

 Haswell-EP PCU */

 KNL Ubox */

 KNL CHA */

 KNL EDC/MC UCLK */

 KNL EDC */

 KNL MC */

 KNL IRP */

 KNL PCU */

 SKX pci bus to socket mapping */

/*

 * The CPU_BUS_NUMBER MSR returns the values of the respective CPUBUSNO CSR

 * that BIOS programmed. MSR has package scope.

 * |  Bit  |  Default  |  Description

 * | [63]  |    00h    | VALID - When set, indicates the CPU bus

 *                       numbers have been initialized. (RO)

 * |[62:48]|    ---    | Reserved

 * |[47:40]|    00h    | BUS_NUM_5 - Return the bus number BIOS assigned

 *                       CPUBUSNO(5). (RO)

 * |[39:32]|    00h    | BUS_NUM_4 - Return the bus number BIOS assigned

 *                       CPUBUSNO(4). (RO)

 * |[31:24]|    00h    | BUS_NUM_3 - Return the bus number BIOS assigned

 *                       CPUBUSNO(3). (RO)

 * |[23:16]|    00h    | BUS_NUM_2 - Return the bus number BIOS assigned

 *                       CPUBUSNO(2). (RO)

 * |[15:8] |    00h    | BUS_NUM_1 - Return the bus number BIOS assigned

 *                       CPUBUSNO(1). (RO)

 * | [7:0] |    00h    | BUS_NUM_0 - Return the bus number BIOS assigned

 *                       CPUBUSNO(0). (RO)

 SKX CHA */

 SKX IIO */

 SKX IRP */

 SKX UPI */

 SKX M2M */

 Memory Map registers device ID */

 Getting I/O stack id in SAD_COTROL_CFG notation */

 SNR Ubox */

 SNR CHA */

 SNR IIO */

 SNR IRP */

 SNR M2PCIE */

 SNR PCU */

 SNR M2M */

 SNR PCIE3 */

 SNR IMC */

 ICX CHA */

 ICX IIO */

 ICX IRP */

 ICX M2PCIE */

 ICX UPI */

 ICX M3UPI*/

 ICX IMC */

 SPR */

 SPR CHA */

 end: all zeroes */ },

 end: all zeroes */ },

 Home Agent */

 MC Channel 0 */

 MC Channel 1 */

 MC Channel 2 */

 MC Channel 3 */

 QPI Port 0 */

 QPI Port 1 */

 R2PCIe */

 R3QPI Link 0 */

 R3QPI Link 1 */

 QPI Port 0 filter  */

 QPI Port 0 filter  */

 end: all zeroes */ }

/*

 * build pci bus to socket mapping

 find the UBOX device */

		/*

		 * The nodeid and idmap registers only contain enough

		 * information to handle 8 nodes.  On systems with more

		 * than 8 nodes, we need to rely on NUMA information,

		 * filled in from BIOS supplied information, to determine

		 * the topology.

 get the Node ID of the local register */

 get the Node ID mapping */

			/*

			 * every three bits in the Node ID mapping register maps

			 * to a particular node.

		/*

		 * For PCI bus with no UBOX device, find the next bus

		 * that has UBOX device and use its mapping.

 end of Sandy Bridge-EP uncore support */

 IvyTown uncore support */

 registers in IRP boxes are not properly aligned */

 Home Agent 0 */

 Home Agent 1 */

 MC0 Channel 0 */

 MC0 Channel 1 */

 MC0 Channel 3 */

 MC0 Channel 4 */

 MC1 Channel 0 */

 MC1 Channel 1 */

 MC1 Channel 3 */

 MC1 Channel 4 */

 IRP */

 QPI0 Port 0 */

 QPI0 Port 1 */

 QPI1 Port 2 */

 R2PCIe */

 R3QPI0 Link 0 */

 R3QPI0 Link 1 */

 R3QPI1 Link 2 */

 QPI Port 0 filter  */

 QPI Port 0 filter  */

 end: all zeroes */ }

 end of IvyTown uncore support */

 KNL uncore support */

/*

 * KNL uses a common PCI device ID for multiple instances of an Uncore PMU

 * device type. prior to KNL, each instance of a PMU device type had a unique

 * device ID.

 *

 *	PCI Device ID	Uncore PMU Devices

 *	----------------------------------

 *	0x7841		MC0 UClk, MC1 UClk

 *	0x7843		MC0 DClk CH 0, MC0 DClk CH 1, MC0 DClk CH 2,

 *			MC1 DClk CH 0, MC1 DClk CH 1, MC1 DClk CH 2

 *	0x7833		EDC0 UClk, EDC1 UClk, EDC2 UClk, EDC3 UClk,

 *			EDC4 UClk, EDC5 UClk, EDC6 UClk, EDC7 UClk

 *	0x7835		EDC0 EClk, EDC1 EClk, EDC2 EClk, EDC3 EClk,

 *			EDC4 EClk, EDC5 EClk, EDC6 EClk, EDC7 EClk

 *	0x7817		M2PCIe

 *	0x7814		IRP

 MC0 UClk */

 MC1 UClk */

 MC0 DClk CH 0 */

 MC0 DClk CH 1 */

 MC0 DClk CH 2 */

 MC1 DClk CH 0 */

 MC1 DClk CH 1 */

 MC1 DClk CH 2 */

 EDC0 UClk */

 EDC1 UClk */

 EDC2 UClk */

 EDC3 UClk */

 EDC4 UClk */

 EDC5 UClk */

 EDC6 UClk */

 EDC7 UClk */

 EDC0 EClk */

 EDC1 EClk */

 EDC2 EClk */

 EDC3 EClk */

 EDC4 EClk */

 EDC5 EClk */

 EDC6 EClk */

 EDC7 EClk */

 M2PCIe */

 IRP */

 end: all zeroes */ }

 All KNL PCI based PMON units are on the same PCI bus except IRP */

 IRP */

 M2PCIe */

 end of KNL uncore support */

 Haswell-EP uncore support */

/*

 * Write SBOX Initialization register bit by bit to avoid spurious #GPs

 Detect 6-8 core systems with only two SBOXes */

 end: all zeroes */ },

 Home Agent 0 */

 Home Agent 1 */

 MC0 Channel 0 */

 MC0 Channel 1 */

 MC0 Channel 2 */

 MC0 Channel 3 */

 MC1 Channel 0 */

 MC1 Channel 1 */

 MC1 Channel 2 */

 MC1 Channel 3 */

 IRP */

 QPI0 Port 0 */

 QPI0 Port 1 */

 QPI1 Port 2 */

 R2PCIe */

 R3QPI0 Link 0 */

 R3QPI0 Link 1 */

 R3QPI1 Link 2 */

 QPI Port 0 filter  */

 QPI Port 1 filter  */

 end: all zeroes */ }

 end of Haswell-EP uncore support */

 BDX uncore support */

 Bit 7 'Use Occupancy' is not available for counter 0 on BDX */

 Detect systems with no SBOXes */

 Home Agent 0 */

 Home Agent 1 */

 MC0 Channel 0 */

 MC0 Channel 1 */

 MC0 Channel 2 */

 MC0 Channel 3 */

 MC1 Channel 0 */

 MC1 Channel 1 */

 MC1 Channel 2 */

 MC1 Channel 3 */

 IRP */

 QPI0 Port 0 */

 QPI0 Port 1 */

 QPI1 Port 2 */

 R2PCIe */

 R3QPI0 Link 0 */

 R3QPI0 Link 1 */

 R3QPI1 Link 2 */

 QPI Port 0 filter  */

 QPI Port 1 filter  */

 QPI Port 2 filter  */

 end: all zeroes */ }

 end of BDX uncore support */

 SKX uncore support */

 Any of the CHA events may be filtered by Thread/Core-ID.*/

 There is no frz_en for chabox ctl */

 Root bus 0x00 is valid only for pmu_idx = 0. */

	/*

	 * Using cpus_read_lock() to ensure cpu is not going down between

	 * looking at cpu_online_mask.

 One more for NULL. */

 Free-Running IO CLOCKS Counter */

 Free-Running IIO BANDWIDTH Counters */

 Free-running IIO UTILIZATION Counters */

 end: all zeroes */ },

/*

 * To determine the number of CHAs, it should read bits 27:0 in the CAPID6

 * register which located at Device 30, Function 3, Offset 0x9C. PCI ID 0x2083.

 MC0 Channel 0 */

 MC0 Channel 1 */

 MC0 Channel 2 */

 MC1 Channel 0 */

 MC1 Channel 1 */

 MC1 Channel 2 */

 M2M0 */

 M2M1 */

 UPI0 Link 0 */

 UPI0 Link 1 */

 UPI1 Link 2 */

 M2PCIe 0 */

 M2PCIe 1 */

 M2PCIe 2 */

 M2PCIe 3 */

 M3UPI0 Link 0 */

 M3UPI0 Link 1 */

 M3UPI1 Link 2 */

 end: all zeroes */ }

 need to double check pci address */

 end of SKX uncore support */

 SNR uncore support */

 Root bus 0x00 is valid only for pmu_idx = 1. */

 Convert stack id from SAD_CONTROL to PMON notation. */

/*

 * SNR has a static mapping of stack IDs from SAD_CONTROL_CFG notation to PMON

 Free-Running IIO CLOCKS Counter */

 Free-Running IIO BANDWIDTH IN Counters */

 end: all zeroes */ },

 M2M */

 end: all zeroes */ }

 PCIe3 RP */

 end: all zeroes */ }

 SNR UBOX DID */

 end: all zeroes */ },

 end: all zeroes */ },

 end of SNR uncore support */

 ICX uncore support */

 Root bus 0x00 is valid only for pmu_idx = 5. */

/*

 * ICX has a static mapping of stack IDs from SAD_CONTROL_CFG notation to PMON

 Free-Running IIO CLOCKS Counter */

 Free-Running IIO BANDWIDTH IN Counters */

 end: all zeroes */ },

/*

 * To determine the number of CHAs, it should read CAPID6(Low) and CAPID7 (High)

 * registers which located at Device 30, Function 3

 M2M 0 */

 M2M 1 */

 M2M 2 */

 M2M 3 */

 UPI Link 0 */

 UPI Link 1 */

 UPI Link 2 */

 M3UPI Link 0 */

 M3UPI Link 1 */

 M3UPI Link 2 */

 end: all zeroes */ }

 ICX UBOX DID */

 end: all zeroes */ },

 end of ICX uncore support */

 SPR uncore support */

 Free-Running IIO CLOCKS Counter */

 Free-Running IIO BANDWIDTH IN Counters */

 Free-Running IIO BANDWIDTH OUT Counters */

 end: all zeroes */ },

 end: all zeroes */ },

 Only copy the customized features */

 end of SPR uncore support */

 SPDX-License-Identifier: GPL-2.0-only

 The PCI driver for the device which the uncore doesn't own. */

 pci bus to socket mapping */

 mask of cpus that collect uncore events */

 constraint for the fixed counter */

 Find first pci bus which attributes to specified die. */

	/*

	 * The unsigned check also catches the '-1' return value for non

	 * existent mappings in the topology map.

/*

 * generic get constraint function for shared match/mask registers.

	/*

	 * reg->alloc can be set due to existing state, so for fake box we

	 * need to ignore this, otherwise we might fail to allocate proper

	 * fake state for this extra reg constraint.

	/*

	 * Only put constraint if extra reg was actually allocated. Also

	 * takes care of event which do not use an extra shared reg.

	 *

	 * Also, if this is a fake box we shouldn't touch any event state

	 * (reg->alloc) and we don't care about leaving inconsistent box

	 * state either since it will be thrown out.

 the hrtimer might modify the previous event value */

/*

 * The overflow interrupt is unavailable for SandyBridge-EP, is broken

 * for SandyBridge. So we use hrtimer to periodically poll the counter

 * to avoid overflow.

	/*

	 * disable local interrupt to prevent uncore_pmu_event_start/stop

	 * to interrupt the update process

	/*

	 * handle boxes with an active event list as opposed to active

	 * counters

 set default hrtimer timeout */

/*

 * Using uncore_pmu_event_init pmu event_init callback

 * as a detection point for uncore events.

 fastpath, try to reuse previous register */

 never assigned */

 constraint still honored */

 not already used */

 slow path */

	/*

	 * Free running counter is read-only and always active.

	 * Use the current counter value as start point.

	 * There is no overflow interrupt for free running counter.

	 * Use hrtimer to periodically poll the counter to avoid overflow.

 Cannot disable free running counter which is read-only */

		/*

		 * Drain the remaining delta count out of a event

		 * that we are disabling:

	/*

	 * The free funning counter is assigned in event_init().

	 * The free running counter event and free running counter

	 * are 1:1 mapped. It doesn't need to be tracked in event_list.

 save events moving to new counters */

		/*

		 * Ensure we don't accidentally enable a stopped

		 * counter simply because we rescheduled.

 reprogram moved events into new counters */

	/*

	 * The event for free running counter is not tracked by event_list.

	 * It doesn't need to force event->hw.idx = -1 to reassign the counter.

	 * Because the event and the free running counter are 1:1 mapped.

/*

 * validation ensures the group can be loaded onto the

 * PMU if it was the only group available.

 The free running counter is always active. */

	/*

	 * the event is not yet connected with its

	 * siblings therefore we must first collect

	 * existing siblings, then add the new event

	 * before we can simulate the scheduling

 no device found for this pmu */

 Sampling not supported yet */

	/*

	 * Place all uncore events for a particular physical package

	 * onto a single cpu

 no fixed counter */

		/*

		 * if there is only one fixed counter, only the first pmu

		 * can access the fixed counter

 fixed counters have event field hardcoded to zero */

		/*

		 * The free running counter event and free running counter

		 * are always 1:1 mapped.

		 * The free running counter is always active.

		 * Assign the free running counter here.

	/*

	 * No uncore block name in discovery table.

	 * Use uncore_type_&typeid_&boxid as name.

		/*

		 * Use the box ID from the discovery table if applicable.

/*

 * Get the die information of a PCI device.

 * @pdev: The PCI device.

 * @die: The die id which the device maps to.

/*

 * Find the PMU of a PCI device.

 * @pdev: The PCI device.

 * @ids: The ID table of the available PCI devices with a PMU.

 *       If NULL, search the whole uncore_pci_uncores.

/*

 * Register the PMU for a PCI device

 * @pdev: The PCI device.

 * @type: The corresponding PMU type of the device.

 * @pmu: The corresponding PMU of the device.

 * @die: The die id which the device maps to.

 First active box registers the pmu */

/*

 * add a pci uncore device

	/*

	 * Some platforms, e.g.  Knights Landing, use a common PCI device ID

	 * for multiple instances of an uncore PMU device type. We should check

	 * PCI slot and func to indicate the uncore box.

		/*

		 * for performance monitoring unit with multiple boxes,

		 * each box has a different function id.

/*

 * Unregister the PMU of a PCI device

 * @pmu: The corresponding PMU is unregistered.

 * @die: The die id which the device maps to.

 Unregister the PMU when the device is going to be deleted. */

		/*

		 * Search the available device, and register the

		 * corresponding PMU.

 Check if exiting cpu is used for collecting uncore events */

 Find a new cpu to collect uncore events */

 Migrate uncore events to the new target */

 Clear the references */

 Try to allocate all required boxes */

 Install them in the pmus */

	/*

	 * Check if there is an online cpu in the package

	 * which collects uncore events already.

 Install hotplug callbacks to setup the targets for each package */

 SPDX-License-Identifier: GPL-2.0

/*

 * Not sure about some of these

 CPU_CLK_UNHALTED */

 INST_RETIRED     */

 L2_RQSTS:M:E:S:I */

 L2_RQSTS:I       */

 BR_INST_RETIRED  */

 BR_MISS_PRED_RETIRED */

 BUS_DRDY_CLOCKS  */

 RESOURCE_STALLS  */

 DATA_MEM_REFS       */

 DCU_LINES_IN        */

 L2_LD:M:E:S:I       */

 IFU_IFETCH         */

 L2_IFETCH:M:E:S:I  */

 L2_M_LINES_INM     */

 DATA_MEM_REFS      */

 IFU_IFETCH         */

 ITLB_MISS          */

 BR_INST_RETIRED      */

 BR_MISS_PRED_RETIRED */

/*

 * Event setting that is specified not to count anything.

 * We use this to effectively disable a counter.

 *

 * L2_RQSTS with 0 MESI unit mask.

 FLOPS */

 FP_COMP_OPS_EXE */

 FP_ASSIST */

 MUL */

 DIV */

 CYCLES_DIV_BUSY */

 p6 only has one enable register */

 p6 only has one enable register */

	/*

	 * p6 only has a global event enable, set on PerfEvtSel0

	 * We "disable" events by programming P6_NOP_EVENT

	 * and we rely on p6_pmu_enable_all() being called

	 * to actually enable the events.

	/*

	 * Events have 40 bits implemented. However they are designed such

	 * that bits [32-39] are sign extensions of bit 31. As such the

	 * effective width of a event for P6-like PMU is 32 bits only.

	 *

	 * See IA-32 Intel Architecture Software developer manual Vol 3B

		/*

		 * PPro erratum 26; fixed in stepping 9 and above.

 Pentium Pro */

 Pentium II - Klamath */

 Pentium II - Deschutes */

 Pentium II - Mendocino */

 Pentium III - Katmai */

 Pentium III - Coppermine */

 Pentium III Xeon */

 Pentium III - Tualatin */

 Pentium M - Banias */

 Pentium M - Dothan */

/*

 * Netburst Performance Events (P4, old Xeon)

 *

 *  Copyright (C) 2010 Parallels, Inc., Cyrill Gorcunov <gorcunov@openvz.org>

 *  Copyright (C) 2010 Intel Corporation, Lin Ming <ming.m.lin@intel.com>

 *

 *  For licencing details see kernel-base/COPYING

/*

 * array indices: 0,1 - HT threads, used with HT enabled cpu

 Event code and ESCR selector */

 ESCR MSR for this event */

 valid ESCR EventMask bits */

 event is shared across threads */

 counter index (offset), -1 on absence */

 it sets P4_PEBS_ENABLE_UOP_TAG as well */

/*

 * note we have P4_PEBS_ENABLE_UOP_TAG always set here

 *

 * it's needed for mapping P4_PEBS_CONFIG_METRIC_MASK bits of

 * event configuration to find out which values are to be

 * written into MSR_IA32_PEBS_ENABLE and MSR_P4_PEBS_MATRIX_VERT

 * registers

/*

 * Note that we don't use CCCR1 here, there is an

 * exception for P4_BSQ_ALLOCATION but we just have

 * no workaround

 *

 * consider this binding as resources which particular

 * event may borrow, it doesn't contain EventMask,

 * Tags and friends -- they are left to a caller

 shared ESCR */

 shared ESCR, broken CCCR1 */

 shared ESCR */

/*

 * Because of Netburst being quite restricted in how many

 * identical events may run simultaneously, we introduce event aliases,

 * ie the different events which have the same functionality but

 * utilize non-intersected resources (ESCR/CCCR/counter registers).

 *

 * This allow us to relax restrictions a bit and run two or more

 * identical events together.

 *

 * Never set any custom internal bits such as P4_CONFIG_HT,

 * P4_CONFIG_ALIASABLE or bits for P4_PEBS_METRIC, they are

 * either up to date automatically or not applicable at all.

		/*

		 * Non-halted cycles can be substituted with non-sleeping cycles (see

		 * Intel SDM Vol3b for details). We need this alias to be able

		 * to run nmi-watchdog and 'perf top' (or any other user space tool

		 * which is interested in running PERF_COUNT_HW_CPU_CYCLES)

		 * simultaneously.

	/*

	 * Only event with special mark is allowed,

	 * we're to be sure it didn't come as malformed

	 * RAW event.

 non-halted CPU clocks */

  /*

   * retired instructions

   * in a sake of simplicity we don't use the FSB tagging

 cache hits */

 cache misses */

 branch instructions retired */

 mispredicted branches retired */

 bus ready clocks (cpu is driving #DRDY_DRV\#DRDY_OWN):  */

 check cpu model specifics */

 INSTR_COMPLETED event only exist for model 3, 4, 6 (Prescott) */

	/*

	 * For info

	 * - IQ_ESCR0, IQ_ESCR1 only for models 1 and 2

 User data may have out-of-bound event index */

 It may be unsupported: */

	/*

	 * NOTE: P4_CCCR_THREAD_ANY has not the same meaning as

	 * in Architectural Performance Monitoring, it means not

	 * on _which_ logical cpu to count but rather _when_, ie it

	 * depends on logical cpu state -- count event if one cpu active,

	 * none, both or any, so we just allow user to pass any value

	 * desired.

	 *

	 * In turn we always set Tx_OS/Tx_USR bits bound to logical

	 * cpu without their propagation to another cpu

	/*

	 * if an event is shared across the logical threads

	 * the user needs special permissions to be able to use it

 ESCR EventMask bits may be invalid */

	/*

	 * it may have some invalid PEBS bits

	/*

	 * the reason we use cpu that early is that: if we get scheduled

	 * first time on the same cpu -- we will not need swap thread

	 * specific flags in config (and will save some cpu cycles)

		/*

		 * Clear bits we reserve to be managed by kernel itself

		 * and never allowed from a user space

		/*

		 * Note that for RAW events we allow user to use P4_CCCR_RESERVED

		 * bits since we keep additional info here (for cache events and etc)

 an official way for overflow indication */

	/*

	 * In some circumstances the overflow might issue an NMI but did

	 * not set P4_CCCR_OVF bit. Because a counter holds a negative value

	 * we simply check for high bit being set, if it's cleared it means

	 * the counter has reached zero value and continued counting before

	 * real NMI signal was received:

	/*

	 * FIXME

	 *

	 * It's still allowed that two threads setup same cache

	 * events so we can't simply clear metrics until we knew

	 * no one is depending on us, so we need kind of counter

	 * for "ReplayEvent" users.

	 *

	 * What is more complex -- RAW events, if user (for some

	 * reason) will pass some cache event metric with improper

	 * event opcode -- it's fine from hardware point of view

	 * but completely nonsense from "meaning" of such action.

	 *

	 * So at moment let leave metrics turned on forever -- it's

	 * ok for now but need to be revisited!

	 *

	 * (void)wrmsrl_safe(MSR_IA32_PEBS_ENABLE, 0);

	 * (void)wrmsrl_safe(MSR_P4_PEBS_MATRIX_VERT, 0);

	/*

	 * If event gets disabled while counter is in overflowed

	 * state we need to clear P4_CCCR_OVF, otherwise interrupt get

	 * asserted again and again

 configuration must be valid */

	/*

	 * - we dont support cascaded counters yet

	 * - and counter 1 is broken (erratum)

 we need a real Event value */

	/*

	 * it could be Cache event so we need to write metrics

	 * into additional MSRs

 catch in-flight IRQs */

 it might be unflagged overflow */

 event overflow for sure */

	/*

	 * When dealing with the unmasking of the LVTPC on P4 perf hw, it has

	 * been observed that the OVF bit flag has to be cleared first _before_

	 * the LVTPC can be unmasked.

	 *

	 * The reason is the NMI line will continue to be asserted while the OVF

	 * bit is set.  This causes a second NMI to generate if the LVTPC is

	 * unmasked before the OVF bit is cleared, leading to unknown NMI

	 * messages.

/*

 * swap thread specific fields according to a thread

 * we are going to run on

	/*

	 * we either lucky and continue on same cpu or no HT support

	/*

	 * the event is migrated from an another logical

	 * cpu, so we need to swap thread specific flags

/*

 * ESCR address hashing is tricky, ESCRs are not sequential

 * in memory but all starts from MSR_P4_BSU_ESCR0 (0x03a0) and

 * the metric between any ESCRs is laid in range [0xa0,0xe1]

 *

 * so we make ~70% filled hashtable

		/*

		 * It's possible to hit a circular lock

		 * between original and alternative events

		 * if both are scheduled already.

			/*

			 * Check whether an event alias is still available.

		/*

		 * Perf does test runs to see if a whole group can be assigned

		 * together successfully.  There can be multiple rounds of this.

		 * Unfortunately, p4_pmu_swap_config_ts touches the hwc->config

		 * bits, such that the next round of group assignments will

		 * cause the above p4_should_swap_ts to pass instead of fail.

		 * This leads to counters exclusive to thread0 being used by

		 * thread1.

		 *

		 * Solve this with a cheap hack, reset the idx back to -1 to

		 * force a new lookup (p4_next_cntr) to get the right counter

		 * for the right thread.

		 *

		 * This probably doesn't comply with the general spirit of how

		 * perf wants to work, but P4 is special. :-(

	/*

	 * IF HT disabled we may need to use all

	 * ARCH_P4_MAX_CCCR counters simultaneously

	 * though leave it restricted at moment assuming

	 * HT is on

	/*

	 * This handles erratum N15 in intel doc 249199-029,

	 * the counter may not be updated correctly on write

	 * so we need a second write operation to do the trick

	 * (the official workaround didn't work)

	 *

	 * the former idea is taken from OProfile code

 If we get stripped -- indexing fails */

	/*

	 * Even though the counters are configured to interrupt a particular

	 * logical processor when an overflow happens, testing has shown that

	 * on kdump kernels (which uses a single cpu), thread1's counter

	 * continues to run and will report an NMI on thread0.  Due to the

	 * overflow bug, this leads to a stream of unknown NMIs.

	 *

	 * Solve this by zero'ing out the registers to mimic a reset.

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel LBR_SELECT bits

 * Intel Vol3a, April 2011, Section 16.7 Table 16-10

 *

 * Hardware branch filter (not available on all CPUs)

 do not capture at ring0 */

 do not capture at ring > 0 */

 do not capture conditional branches */

 do not capture relative calls */

 do not capture indirect calls */

 do not capture near returns */

 do not capture indirect jumps */

 do not capture relative jumps */

 do not capture far branches */

 enable call stack */

/*

 * Following bit only exists in Linux; we mask it out before writing it to

 * the actual MSR. But it helps the constraint perf code to understand

 * that this is a separate configuration.

 don't read LBR_INFO. */

 valid bits in LBR_SELECT */

 LBR filter not supported */

 ignored */

/*

 * x86control flow change classification

 * x86control flow changes include branches, interrupts, traps, faults

 unknown */

 branch target is user */

 branch target is kernel */

 call */

 return */

 syscall */

 syscall return */

 sw interrupt */

 return from interrupt */

 conditional */

 jump */

 hw interrupt or trap or fault */

 indirect calls */

 transaction abort */

 in transaction */

 not in transaction */

 zero length call */

 call stack */

 indirect jump */

 indicate to save branch type */

/*

 * Intel LBR_CTL bits

 *

 * Hardware branch filter for Arch LBR

 capture at ring0 */

 capture at ring > 0 */

 enable call stack */

 capture conditional branches */

 capture relative jumps */

 capture indirect jumps */

 capture relative calls */

 capture indirect calls */

 capture near returns */

 capture other branches */

/*

 * We only support LBR implementations that have FREEZE_LBRS_ON_PMI

 * otherwise it becomes near impossible to get a reliable stack.

	/*

	 * No need to unfreeze manually, as v4 can do that as part

	 * of the GLOBAL_STATUS ack.

	/*

	 * No need to reprogram LBR_SELECT in a PMI, as it

	 * did not change.

	/*

	 * LBR callstack does not work well with FREEZE_LBRS_ON_PMI.

	 * If FREEZE_LBRS_ON_PMI is set, PMI near call/return instructions

	 * may cause superfluous increase/decrease of LBR_TOS.

 Write to ARCH_LBR_DEPTH MSR, all LBR entries are reset to 0 */

/*

 * TOS = most recently recorded branch

/*

 * For formats with LBR_TSX flags (e.g. LBR_FORMAT_EIP_FLAGS2), bits 61:62 in

 * MSR_LAST_BRANCH_FROM_x are the TSX flags when TSX is supported, but when

 * TSX is not supported they have no consistent behavior:

 *

 *   - For wrmsr(), bits 61:62 are considered part of the sign extension.

 *   - For HW updates (branch captures) bits 61:62 are always OFF and are not

 *     part of the sign extension.

 *

 * Therefore, if:

 *

 *   1) LBR has TSX format

 *   2) CPU has no TSX support enabled

 *

 * ... then any value passed to wrmsr() must be sign extended to 63 bits and any

 * value from rdmsr() must be converted to have a 61 bits sign extension,

 * ignoring the TSX flags.

 If quirk is enabled, ensure sign extension is 63 bits: */

		/*

		 * Sign extend into bits 61:62 while preserving bit 63.

		 *

		 * Quirk is enabled when TSX is disabled. Therefore TSX bits

		 * in val are always OFF and must be changed to be sign

		 * extension bits. Since bits 59:60 are guaranteed to be

		 * part of the sign extension bits, we can just copy them

		 * to 61:62.

/*

 * If quirk is needed, ensure sign extension is 61 bits:

		/*

		 * Quirk is on when TSX is not enabled. Therefore TSX

		 * flags must be read as OFF.

 Don't read invalid entry */

 Fast reset the LBRs before restore if the call stack is not full. */

/*

 * Restore the Architecture LBR state from the xsave area in the perf

 * context data for the task via the XRSTORS instruction.

	/*

	 * Does not restore the LBR registers, if

	 * - No one else touched them, and

	 * - Was not cleared in Cstate

 LBR call stack is not full. Reset is required in restore. */

/*

 * Save the Architecture LBR state to the xsave area in the perf

 * context data for the task via the XSAVES instruction.

	/*

	 * Architecture specific synchronization makes sense in

	 * case both prev->task_ctx_data and next->task_ctx_data

	 * pointers are allocated.

	/*

	 * If LBR callstack feature is enabled and the stack was saved when

	 * the task was scheduled out, restore the stack. Otherwise flush

	 * the LBR stack.

	/*

	 * Since a context switch can flip the address space and LBR entries

	 * are not tagged with an identifier, we need to wipe the LBR, even for

	 * per-cpu events. You simply cannot resolve the branches from the old

	 * address space.

	/*

	 * Request pmu::sched_task() callback, which will fire inside the

	 * regular perf event scheduling, so that call will:

	 *

	 *  - restore or wipe; when LBR-callstack,

	 *  - wipe; otherwise,

	 *

	 * when this is from __perf_event_task_sched_in().

	 *

	 * However, if this is from perf_install_in_context(), no such callback

	 * will follow and we'll need to reset the LBR here if this is the

	 * first LBR event.

	 *

	 * The problem is, we cannot tell these cases apart... but we can

	 * exclude the biggest chunk of cases by looking at

	 * event->total_time_running. An event that has accrued runtime cannot

	 * be 'new'. Conversely, a new event can get installed through the

	 * context switch path for the first time.

/*

 * Due to lack of segmentation in Linux the effective address (offset)

 * is the same as the linear address, allowing us to merge the LIP and EIP

 * LBR formats.

		/*

		 * Read LBR call stack entries

		 * until invalid entry (0s) is detected.

		/*

		 * Some CPUs report duplicated abort records,

		 * with the second entry not having an abort bit set.

		 * Skip them here. This loop runs backwards,

		 * so we need to undo the previous record.

		 * If the abort just happened outside the window

		 * the extra entry cannot be removed.

		/*

		 * Read LBR entries until invalid entry (0s) is detected.

	/*

	 * Don't read when all LBRs users are using adaptive PEBS.

	 *

	 * This could be smarter and actually check the event,

	 * but this simple approach seems to work for now.

/*

 * SW filter is used:

 * - in case there is no HW filter

 * - in case the HW filter has errata or limitations

 we ignore BRANCH_HV here */

	/*

	 * stash actual user request into reg, it may

	 * be used by fixup code for some CPU

/*

 * setup the HW LBR filter

 * Used only when available, may not be enough to disambiguate

 * all branches, may need the help of the SW filter

	/*

	 * The first 9 bits (LBR_SEL_MASK) in LBR_SELECT operate

	 * in suppress mode. So LBR_SELECT should be set to

	 * (~mask & LBR_SEL_MASK) | (mask & ~LBR_SEL_MASK)

	 * But the 10th bit LBR_CALL_STACK does not operate

	 * in suppress mode.

	/*

	 * no LBR on this PMU

	/*

	 * setup SW LBR filter

	/*

	 * setup HW LBR filter, if any

/*

 * return the type of control flow change at address "from"

 * instruction is not necessarily a branch (in case of interrupt).

 *

 * The branch type returned also includes the priv level of the

 * target of the control flow change (X86_BR_USER, X86_BR_KERNEL).

 *

 * If a branch type is unknown OR the instruction cannot be

 * decoded (e.g., text page not present), then X86_BR_NONE is

 * returned.

	/*

	 * maybe zero if lbr did not fill up after a reset by the time

	 * we get a PMU interrupt

		/*

		 * can happen if measuring at the user level only

		 * and we interrupt in a kernel thread, e.g., idle.

 may fail if text not present */

		/*

		 * The LBR logs any address in the IP, even if the IP just

		 * faulted. This means userspace can control the from address.

		 * Ensure we don't blindly read any address by validating it is

		 * a known text address.

			/*

			 * Assume we can get the maximum possible size

			 * when grabbing kernel data.  This is not

			 * _strictly_ true since we could possibly be

			 * executing up next to a memory hole, but

			 * it is very unlikely to be a problem.

	/*

	 * decoder needs to know the ABI especially

	 * on 64-bit systems running 32-bit apps

 syscall */

 sysenter */

 sysret */

 sysexit */

 conditional */

 conditional */

 near ret */

 near ret */

 far ret */

 far ret */

 iret */

 int */

 call near rel */

 zero length call */

 call far absolute */

 loop jmp */

 jmp */

 call near absolute, call far absolute ind */

 near ind call */

 far ind call */

	/*

	 * interrupts, traps, faults (and thus ring transition) may

	 * occur on any instructions. Thus, to classify them correctly,

	 * we need to first look at the from and to priv levels. If they

	 * are different and to is in the kernel, then it indicates

	 * a ring transition. If the from instruction is not a ring

	 * transition instr (syscall, systenter, int), then it means

	 * it was a irq, trap or fault.

	 *

	 * we have no way of detecting kernel to kernel faults.

	/*

	 * branch priv level determined by target as

	 * is done by HW when LBR_SELECT is implemented

 X86_BR_CALL */

 X86_BR_RET */

 X86_BR_SYSCALL */

 X86_BR_SYSRET */

 X86_BR_INT */

 X86_BR_IRET */

 X86_BR_JCC */

 X86_BR_JMP */

 X86_BR_IRQ */

 X86_BR_IND_CALL */

 X86_BR_ABORT */

 X86_BR_IN_TX */

 X86_BR_NO_TX */

 X86_BR_ZERO_CALL */

 X86_BR_CALL_STACK */

 X86_BR_IND_JMP */

 skip X86_BR_USER and X86_BR_KERNEL */

/*

 * implement actual branch filter based on user demand.

 * Hardware may not exactly satisfy that request, thus

 * we need to inspect opcodes. Mismatched branches are

 * discarded. Therefore, the number of branches returned

 * in PERF_SAMPLE_BRANCH_STACK sample may vary.

 if sampling all branches, then nothing to filter */

		/*

		 * Parse the branch type recorded in LBR_x_INFO MSR.

		 * Doesn't support OTHER_BRANCH decoding for now.

		 * OTHER_BRANCH branch type still rely on software decoding.

 if type does not correspond, then discard */

 remove all entries with from=0 */

 Cannot get TOS for large PEBS and Arch LBR */

/*

 * Map interface branch filters onto LBR filters

	/*

	 * NHM/WSM erratum: must include REL_JMP+IND_JMP to get CALL branches

	/*

	 * NHM/WSM erratum: must include IND_JMP to capture IND_CALL

 core */

	/*

	 * SW branch filter usage:

	 * - compensate for lack of HW filter

 nehalem/westmere */

	/*

	 * SW branch filter usage:

	 * - workaround LBR_SEL errata (see above)

	 * - support syscall, sysret capture.

	 *   That requires LBR_FAR but that means far

	 *   jmp need to be filtered out

 sandy bridge */

	/*

	 * SW branch filter usage:

	 * - support syscall, sysret capture.

	 *   That requires LBR_FAR but that means far

	 *   jmp need to be filtered out

 haswell */

 skylake */

	/*

	 * SW branch filter usage:

	 * - support syscall, sysret capture.

	 *   That requires LBR_FAR but that means far

	 *   jmp need to be filtered out

 atom */

	/*

	 * only models starting at stepping 10 seems

	 * to have an operational LBR which can freeze

	 * on PMU interrupt

	/*

	 * SW branch filter usage:

	 * - compensate for lack of HW filter

 slm */

	/*

	 * SW branch filter usage:

	 * - compensate for lack of HW filter

 Knights Landing */

 Knights Landing does have MISPREDICT bit */

/*

 * LBR state size is variable based on the max number of registers.

 * This calculates the expected state size, which should match

 * what the hardware enumerates for the size of XFEATURE_LBR.

	/*

	 * Check the LBR state with the corresponding software structure.

	 * Disable LBR XSAVES support if the size doesn't match.

 Arch LBR Capabilities */

 Apply the max depth of Arch LBR */

 LBR callstack requires both CPL and Branch Filtering support */

/**

 * x86_perf_get_lbr - get the LBR records information

 *

 * @lbr: the caller's memory to store the LBR records information

 *

 * Returns: 0 indicates the LBR info has been successfully obtained

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Per core/cpu state

 *

 * Used to coordinate shared registers between HT threads or

 * among events on a single PMU.

/*

 * Intel PerfMon, used on Core and later.

 pseudo-encoding */

 FP_ASSIST */

 MUL */

 DIV */

 CYCLES_DIV_BUSY */

 DELAYED_BYPASS */

 FP_COMP_INSTR_RET */

 INST_RETIRED.ANY */

 CPU_CLK_UNHALTED.CORE */

 CPU_CLK_UNHALTED.REF */

 FP_COMP_OPS_EXE */

 FP_ASSIST */

 MUL */

 DIV */

 CYCLES_DIV_BUSY */

 IDLE_DURING_DIV */

 DELAYED_BYPASS */

 RS_UOPS_DISPATCH_CYCLES */

 ITLB_MISS_RETIRED (T30-9) */

 MEM_LOAD_RETIRED */

 INST_RETIRED.ANY */

 CPU_CLK_UNHALTED.CORE */

 CPU_CLK_UNHALTED.REF */

 L1D_CACHE_LD */

 L1D_CACHE_ST */

 L1D_CACHE_LOCK */

 L1D_ALL_REF */

 L1D_PEND_MISS */

 L1D_PREFETCH */

 L1D */

 CACHE_LOCK_CYCLES */

 must define OFFCORE_RSP_X first, see intel_fixup_er() */

 INST_RETIRED.ANY */

 CPU_CLK_UNHALTED.CORE */

 CPU_CLK_UNHALTED.REF */

 L1D */

 OFFCORE_REQUESTS_OUTSTANDING */

 CACHE_LOCK_CYCLES */

 SNOOPQ_REQUEST_OUTSTANDING */

 INST_RETIRED.ANY */

 CPU_CLK_UNHALTED.CORE */

 CPU_CLK_UNHALTED.REF */

 CYCLE_ACTIVITY.CYCLES_NO_DISPATCH */

 CYCLE_ACTIVITY.STALLS_L2_PENDING */

 CYCLE_ACTIVITY.CYCLES_L1D_PENDING */

 CYCLE_ACTIVITY.STALLS_L1D_PENDING */

 L1D_PEND_MISS.PENDING */

 INST_RETIRED.PREC_DIST */

 MEM_TRANS_RETIRED.LOAD_LATENCY */

 CYCLE_ACTIVITY.CYCLES_NO_DISPATCH */

 CYCLE_ACTIVITY.CYCLES_L1D_PENDING */

	/*

	 * When HT is off these events can only run on the bottom 4 counters

	 * When HT is on, they are impacted by the HT bug and require EXCL access

 MEM_UOPS_RETIRED.* */

 MEM_LOAD_UOPS_RETIRED.* */

 MEM_LOAD_UOPS_LLC_HIT_RETIRED.* */

 MEM_LOAD_UOPS_LLC_MISS_RETIRED.* */

 INST_RETIRED.ANY */

 CPU_CLK_UNHALTED.CORE */

 CPU_CLK_UNHALTED.REF */

 L1D_PEND_MISS.PENDING */

 IDQ.EMPTY */

 IDQ_UOPS_NOT_DELIVERED.CORE */

 CYCLE_ACTIVITY.CYCLES_LDM_PENDING */

 CYCLE_ACTIVITY.CYCLES_NO_EXECUTE */

 CYCLE_ACTIVITY.STALLS_L2_PENDING */

 CYCLE_ACTIVITY.STALLS_LDM_PENDING */

 CYCLE_ACTIVITY.CYCLES_L1D_PENDING */

 CYCLE_ACTIVITY.STALLS_L1D_PENDING */

 INST_RETIRED.PREC_DIST */

	/*

	 * When HT is off these events can only run on the bottom 4 counters

	 * When HT is on, they are impacted by the HT bug and require EXCL access

 MEM_UOPS_RETIRED.* */

 MEM_LOAD_UOPS_RETIRED.* */

 MEM_LOAD_UOPS_LLC_HIT_RETIRED.* */

 MEM_LOAD_UOPS_LLC_MISS_RETIRED.* */

 must define OFFCORE_RSP_X first, see intel_fixup_er() */

 INST_RETIRED.ANY */

 CPU_CLK_UNHALTED.CORE */

 CPU_CLK_UNHALTED.REF */

 INST_RETIRED.ANY */

 CPU_CLK_UNHALTED.CORE */

 pseudo CPU_CLK_UNHALTED.REF */

 INST_RETIRED.ANY */

 CPU_CLK_UNHALTED.CORE */

 CPU_CLK_UNHALTED.REF */

 INST_RETIRED.PREC_DIST */

	/*

	 * when HT is off, these can only run on the bottom 4 counters

 MEM_INST_RETIRED.* */

 MEM_LOAD_RETIRED.* */

 MEM_LOAD_L3_HIT_RETIRED.* */

 MEM_TRANS_RETIRED.* */

 FRONTEND_RETIRED.* */

 must define OFFCORE_RSP_X first, see intel_fixup_er() */

 must define OFFCORE_RSP_X first, see intel_fixup_er() */

	/*

	 * Note the low 8 bits eventsel code is not a continuous field, containing

	 * some #GPing bits. These are masked out.

 INST_RETIRED.ANY */

 old INST_RETIRED.PREC_DIST */

 INST_RETIRED.PREC_DIST */

 CPU_CLK_UNHALTED.CORE */

 CPU_CLK_UNHALTED.REF */

 SLOTS */

 SW_PREFETCH_ACCESS.* */

 CYCLE_ACTIVITY.STALLS_TOTAL */

 CYCLE_ACTIVITY.CYCLES_MEM_ANY */

 CYCLE_ACTIVITY.STALLS_MEM_ANY */

 CYCLE_ACTIVITY.* */

 INST_RETIRED.ANY */

 INST_RETIRED.PREC_DIST */

 CPU_CLK_UNHALTED.CORE */

 CPU_CLK_UNHALTED.REF */

 SLOTS */

	/*

	 * Generally event codes < 0x90 are restricted to counters 0-3.

	 * The 0x2E and 0x3C are exception, which has no restriction.

	/*

	 * Generally event codes >= 0x90 are likely to have no restrictions.

	 * The exception are defined as above.

/*

 * topdown events for Intel Core CPUs.

 *

 * The events are all in slots, which is a free slot in a 4 wide

 * pipeline. Some events are already reported in slots, for cycle

 * events we multiply by the pipeline width (4).

 *

 * With Hyper Threading on, topdown metrics are either summed or averaged

 * between the threads of a core: (count_t0 + count_t1).

 *

 * For the average case the metric is always scaled to pipeline width,

 * so we use factor 2 ((count_t0 + count_t1) / 2 * 4)

 cpu_clk_unhalted.thread */

 cpu_clk_unhalted.thread_any */

 uops_issued.any */

 uops_retired.retire_slots */

 idq_uops_not_delivered_core */

 int_misc.recovery_cycles */

 int_misc.recovery_cycles_any */

 INST_RETIRED.ANY */

 CPU_CLK_UNHALTED.CORE */

 CPU_CLK_UNHALTED.REF */

 L1D_PEND_MISS.PENDING */

 INST_RETIRED.PREC_DIST */

 MEM_TRANS_RETIRED.LOAD_LATENCY */

 CYCLE_ACTIVITY.CYCLES_L1D_PENDING */

 CYCLE_ACTIVITY.STALLS_L1D_PENDING */

 CYCLE_ACTIVITY.CYCLES_NO_EXECUTE */

	/*

	 * When HT is off these events can only run on the bottom 4 counters

	 * When HT is on, they are impacted by the HT bug and require EXCL access

 MEM_UOPS_RETIRED.* */

 MEM_LOAD_UOPS_RETIRED.* */

 MEM_LOAD_UOPS_LLC_HIT_RETIRED.* */

 MEM_LOAD_UOPS_LLC_MISS_RETIRED.* */

 INST_RETIRED.ANY */

 CPU_CLK_UNHALTED.CORE */

 CPU_CLK_UNHALTED.REF */

 L1D_PEND_MISS.PENDING */

 CYCLE_ACTIVITY.CYCLES_L1D_MISS */

	/*

	 * when HT is off, these can only run on the bottom 4 counters

 MEM_INST_RETIRED.* */

 MEM_LOAD_RETIRED.* */

 MEM_LOAD_L3_HIT_RETIRED.* */

 MEM_TRANS_RETIRED.* */

/*

 * Notes on the events:

 * - data reads do not include code reads (comparable to earlier tables)

 * - data counts include speculative execution (except L1 write, dtlb, bpu)

 * - remote node access includes remote memory, remote cache, remote mmio.

 * - prefetches are not included in the counts.

 * - icache miss does not include decoded icache

 MEM_INST_RETIRED.ALL_LOADS */

 L1D.REPLACEMENT */

 MEM_INST_RETIRED.ALL_STORES */

 ICACHE_64B.MISS */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 MEM_INST_RETIRED.ALL_LOADS */

 DTLB_LOAD_MISSES.WALK_COMPLETED */

 MEM_INST_RETIRED.ALL_STORES */

 DTLB_STORE_MISSES.WALK_COMPLETED */

 ITLB_MISSES.STLB_HIT */

 ITLB_MISSES.WALK_COMPLETED */

 BR_INST_RETIRED.ALL_BRANCHES */

 BR_MISP_RETIRED.ALL_BRANCHES */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 MEM_UOP_RETIRED.LOADS        */

 L1D.REPLACEMENT              */

 MEM_UOP_RETIRED.STORES       */

 L1D.ALL_M_REPLACEMENT        */

 HW_PRE_REQ.DL1_MISS          */

 ICACHE.MISSES */

 OFFCORE_RESPONSE.ANY_DATA.LOCAL_CACHE */

 OFFCORE_RESPONSE.ANY_DATA.ANY_LLC_MISS */

 OFFCORE_RESPONSE.ANY_RFO.LOCAL_CACHE */

 OFFCORE_RESPONSE.ANY_RFO.ANY_LLC_MISS */

 OFFCORE_RESPONSE.PREFETCH.LOCAL_CACHE */

 OFFCORE_RESPONSE.PREFETCH.ANY_LLC_MISS */

 MEM_UOP_RETIRED.ALL_LOADS */

 DTLB_LOAD_MISSES.CAUSES_A_WALK */

 MEM_UOP_RETIRED.ALL_STORES */

 DTLB_STORE_MISSES.MISS_CAUSES_A_WALK */

 ITLB_MISSES.STLB_HIT         */

 ITLB_MISSES.CAUSES_A_WALK    */

 BR_INST_RETIRED.ALL_BRANCHES */

 BR_MISP_RETIRED.ALL_BRANCHES */

/*

 * Notes on the events:

 * - data reads do not include code reads (comparable to earlier tables)

 * - data counts include speculative execution (except L1 write, dtlb, bpu)

 * - remote node access includes remote memory, remote cache, remote mmio.

 * - prefetches are not included in the counts because they are not

 *   reliably counted.

 MEM_UOPS_RETIRED.ALL_LOADS */

 L1D.REPLACEMENT */

 MEM_UOPS_RETIRED.ALL_STORES */

 ICACHE.MISSES */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 MEM_UOPS_RETIRED.ALL_LOADS */

 DTLB_LOAD_MISSES.MISS_CAUSES_A_WALK */

 MEM_UOPS_RETIRED.ALL_STORES */

 DTLB_STORE_MISSES.MISS_CAUSES_A_WALK */

 ITLB_MISSES.STLB_HIT */

 ITLB_MISSES.MISS_CAUSES_A_WALK */

 BR_INST_RETIRED.ALL_BRANCHES */

 BR_MISP_RETIRED.ALL_BRANCHES */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 MEM_INST_RETIRED.LOADS       */

 L1D.REPL                     */

 MEM_INST_RETURED.STORES      */

 L1D.M_REPL                   */

 L1D_PREFETCH.REQUESTS        */

 L1D_PREFETCH.MISS            */

 L1I.READS                    */

 L1I.MISSES                   */

 OFFCORE_RESPONSE.ANY_DATA.LOCAL_CACHE */

 OFFCORE_RESPONSE.ANY_DATA.ANY_LLC_MISS */

	/*

	 * Use RFO, not WRITEBACK, because a write miss would typically occur

	 * on RFO.

 OFFCORE_RESPONSE.ANY_RFO.LOCAL_CACHE */

 OFFCORE_RESPONSE.ANY_RFO.ANY_LLC_MISS */

 OFFCORE_RESPONSE.PREFETCH.LOCAL_CACHE */

 OFFCORE_RESPONSE.PREFETCH.ANY_LLC_MISS */

 MEM_INST_RETIRED.LOADS       */

 DTLB_LOAD_MISSES.ANY         */

 MEM_INST_RETURED.STORES      */

 MEM_STORE_RETIRED.DTLB_MISS  */

 INST_RETIRED.ANY_P           */

 ITLB_MISSES.ANY              */

 BR_INST_RETIRED.ALL_BRANCHES */

 BPU_CLEARS.ANY               */

/*

 * Nehalem/Westmere MSR_OFFCORE_RESPONSE bits;

 * See IA32 SDM Vol 3B 30.6.1.3

 reserved */

 MEM_INST_RETIRED.LOADS       */

 L1D.REPL                     */

 MEM_INST_RETURED.STORES      */

 L1D.M_REPL                   */

 L1D_PREFETCH.REQUESTS        */

 L1D_PREFETCH.MISS            */

 L1I.READS                    */

 L1I.MISSES                   */

 OFFCORE_RESPONSE.ANY_DATA.LOCAL_CACHE */

 OFFCORE_RESPONSE.ANY_DATA.ANY_LLC_MISS */

	/*

	 * Use RFO, not WRITEBACK, because a write miss would typically occur

	 * on RFO.

 OFFCORE_RESPONSE.ANY_RFO.LOCAL_CACHE */

 OFFCORE_RESPONSE.ANY_RFO.ANY_LLC_MISS */

 OFFCORE_RESPONSE.PREFETCH.LOCAL_CACHE */

 OFFCORE_RESPONSE.PREFETCH.ANY_LLC_MISS */

 L1D_CACHE_LD.MESI   (alias)  */

 DTLB_LOAD_MISSES.ANY         */

 L1D_CACHE_ST.MESI   (alias)  */

 MEM_STORE_RETIRED.DTLB_MISS  */

 INST_RETIRED.ANY_P           */

 ITLB_MISS_RETIRED            */

 BR_INST_RETIRED.ALL_BRANCHES */

 BPU_CLEARS.ANY               */

 L1D_CACHE_LD.MESI          */

 L1D_CACHE_LD.I_STATE       */

 L1D_CACHE_ST.MESI          */

 L1D_CACHE_ST.I_STATE       */

 L1D_PREFETCH.REQUESTS      */

 L1I.READS                  */

 L1I.MISSES                 */

 L2_LD.MESI                 */

 L2_LD.ISTATE               */

 L2_ST.MESI                 */

 L2_ST.ISTATE               */

 L1D_CACHE_LD.MESI  (alias) */

 DTLB_MISSES.MISS_LD        */

 L1D_CACHE_ST.MESI  (alias) */

 DTLB_MISSES.MISS_ST        */

 INST_RETIRED.ANY_P         */

 ITLBMISSES                 */

 BR_INST_RETIRED.ANY        */

 BP_INST_RETIRED.MISPRED    */

 L1D_CACHE.LD               */

 L1D_CACHE.ST               */

 L1I.READS                  */

 L1I.MISSES                 */

 L2_LD.MESI                 */

 L2_LD.ISTATE               */

 L2_ST.MESI                 */

 L2_ST.ISTATE               */

 L1D_CACHE_LD.MESI  (alias) */

 DTLB_MISSES.MISS_LD        */

 L1D_CACHE_ST.MESI  (alias) */

 DTLB_MISSES.MISS_ST        */

 INST_RETIRED.ANY_P         */

 ITLB.MISSES                */

 BR_INST_RETIRED.ANY        */

 BP_INST_RETIRED.MISPRED    */

 no_alloc_cycles.not_delivered */

 uops_retired.all */

 uops_retired.all */

 must define OFFCORE_RSP_X first, see intel_fixup_er() */

 LD_DCU_MISS */

 ICACHE.ACCESSES */

 ICACGE.MISSES */

 OFFCORE_RESPONSE.ANY_DATA.LOCAL_CACHE */

 OFFCORE_RESPONSE.ANY_RFO.LOCAL_CACHE */

 OFFCORE_RESPONSE.ANY_RFO.ANY_LLC_MISS */

 OFFCORE_RESPONSE.PREFETCH.LOCAL_CACHE */

 OFFCORE_RESPONSE.PREFETCH.ANY_LLC_MISS */

 LD_DTLB_MISS */

 INST_RETIRED.ANY_P */

 PAGE_WALKS.I_SIDE_WALKS */

 BR_INST_RETIRED.ANY */

 BP_INST_RETIRED.MISPRED */

 UOPS_NOT_DELIVERED.ANY */

 ISSUE_SLOTS_NOT_CONSUMED.RECOVERY */

 UOPS_RETIRED.ANY */

 UOPS_ISSUED.ANY */

 must define OFFCORE_RSP_X first, see intel_fixup_er() */

 MEM_UOPS_RETIRED.ALL_LOADS */

 MEM_UOPS_RETIRED.ALL_STORES */

 ICACHE.ACCESSES */

 ICACHE.MISSES */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 MEM_UOPS_RETIRED.ALL_LOADS */

 MEM_UOPS_RETIRED.ALL_STORES */

 INST_RETIRED.ANY_P */

 ITLB.MISS */

 BR_INST_RETIRED.ALL_BRANCHES */

 BR_MISP_RETIRED.ALL_BRANCHES */

 MEM_UOPS_RETIRED.ALL_LOADS */

 MEM_UOPS_RETIRED.ALL_STORES */

 ICACHE.ACCESSES */

 ICACHE.MISSES */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 OFFCORE_RESPONSE */

 MEM_UOPS_RETIRED.ALL_LOADS */

 DTLB_LOAD_MISSES.WALK_COMPLETED */

 MEM_UOPS_RETIRED.ALL_STORES */

 DTLB_STORE_MISSES.WALK_COMPLETED */

 INST_RETIRED.ANY_P */

 ITLB.MISS */

 BR_INST_RETIRED.ALL_BRANCHES */

 BR_MISP_RETIRED.ALL_BRANCHES */

 must define OFFCORE_RSP_X first, see intel_fixup_er() */

 must define OFFCORE_RSP_X first, see intel_fixup_er() */

 Other Tile L2 Hit */

 Other Tile L2 Hit */

/*

 * Used from PMIs where the LBRs are already disabled.

 *

 * This function could be called consecutively. It is required to remain in

 * disabled state if called consecutively.

 *

 * During consecutive calls, the same disable value will be written to related

 * registers, so the PMU state remains unchanged.

 *

 * intel_bts events don't coexist with intel PMU's BTS events because of

 * x86_add_exclusive(x86_lbr_exclusive_lbr); there's no need to keep them

 * disabled around intel PMU's event batching etc, only inside the PMI handler.

 *

 * Avoid PEBS_ENABLE MSR access in PMIs.

 * The GLOBAL_CTRL has been disabled. All the counters do not count anymore.

 * It doesn't matter if the PEBS is enabled or not.

 * Usually, the PEBS status are not changed in PMIs. It's unnecessary to

 * access PEBS_ENABLE MSR in disable_all()/enable_all().

 * However, there are some cases which may change PEBS status, e.g. PMI

 * throttle. The PEBS_ENABLE should be updated where the status changes.

 must not have branches... */

 we don't care about BTS */

            ... until here */

 must not have branches... */

 we don't care about BTS */

            ... until here */

/*

 * Workaround for:

 *   Intel Errata AAK100 (model 26)

 *   Intel Errata AAP53  (model 30)

 *   Intel Errata BD53   (model 44)

 *

 * The official story:

 *   These chips need to be 'reset' when adding counters by programming the

 *   magic three (non-counting) events 0x4300B5, 0x4300D2, and 0x4300B1 either

 *   in sequence on the same PMC or on different PMCs.

 *

 * In practice it appears some of these events do in fact count, and

 * we need to program all 4 events.

	/*

	 * The Errata requires below steps:

	 * 1) Clear MSR_IA32_PEBS_ENABLE and MSR_CORE_PERF_GLOBAL_CTRL;

	 * 2) Configure 4 PERFEVTSELx with the magic events and clear

	 *    the corresponding PMCx;

	 * 3) set bit0~bit3 of MSR_CORE_PERF_GLOBAL_CTRL;

	 * 4) Clear MSR_CORE_PERF_GLOBAL_CTRL;

	 * 5) Clear 4 pairs of ERFEVTSELx and PMCx;

	/*

	 * The real steps we choose are a little different from above.

	 * A) To reduce MSR operations, we don't run step 1) as they

	 *    are already cleared before this function is called;

	 * B) Call x86_perf_event_update to save PMCx before configuring

	 *    PERFEVTSELx with magic number;

	 * C) With step 5), we do clear only when the PERFEVTSELx is

	 *    not used currently.

	 * D) Call x86_perf_event_set_period to restore PMCx;

 We always operate 4 pairs of PERF Counters */

	/*

	 * We're going to use PMC3, make sure TFA is set before we touch it.

	/*

	 * If we find PMC3 is no longer used when we enable the PMU, we can

	 * clear TFA.

		/*

		 * When there are other active TopDown events,

		 * don't disable the fixed counter 3.

	/*

	 * Needs to be called after x86_pmu_disable_event,

	 * so we don't trigger the event without PEBS bit set.

	/*

	 * The values in PERF_METRICS MSR are derived from fixed counter 3.

	 * Software should start both registers, PERF_METRICS and fixed

	 * counter 3, from zero.

	 * Clear PERF_METRICS and Fixed counter 3 in initialization.

	 * After that, both MSRs will be cleared for each read.

	 * Don't need to clear them again.

	/*

	 * The metric is reported as an 8bit integer fraction

	 * summing up to 0xff.

	 * slots-in-metric = (Metric / 0xff) * slots

	/*

	 * The 8bit integer fraction of metric may be not accurate,

	 * especially when the changes is very small.

	 * For example, if only a few bad_spec happens, the fraction

	 * may be reduced from 1 to 0. If so, the bad_spec event value

	 * will be 0 which is definitely less than the last value.

	 * Avoid update event->count for this case.

/*

 * Update all active Topdown events.

 *

 * The PERF_METRICS and Fixed counter 3 are read separately. The values may be

 * modify by a NMI. PMU has to be disabled before calling this function.

 read Fixed counter 3 */

 read PERF_METRICS */

	/*

	 * Check and update this event, which may have been cleared

	 * in active_mask e.g. x86_pmu_stop()

		/*

		 * In x86_pmu_stop(), the event is cleared in active_mask first,

		 * then drain the delta, which indicates context switch for

		 * counting.

		 * Save metric and slots for context switch.

		 * Don't need to reset the PERF_METRICS and Fixed counter 3.

		 * Because the values will be restored in next schedule in.

 The fixed counter 3 has to be written before the PERF_METRICS. */

 Only need to call update_topdown_event() once for group read. */

		/*

		 * When there are other active TopDown events,

		 * don't enable the fixed counter 3 again.

	/*

	 * Enable IRQ generation (0x8), if not PEBS,

	 * and enable ring-3 counting (0x2) and ring-0 counting (0x1)

	 * if requested:

	/*

	 * ANY bit is supported in v3 and up

/*

 * Save and restart an expired event. Called by NMI contexts,

 * so it has to be careful about preempting normal event ops:

	/*

	 * For a checkpointed counter always reset back to 0.  This

	 * avoids a situation where the counter overflows, aborts the

	 * transaction and is then set back to shortly before the

	 * overflow, and overflows and aborts again.

 No race with NMIs because the counter should not be armed */

 Ack all overflows and disable fixed counters */

 Reset LBRs and LBR freezing */

	/*

	 * Ignore a range of extra bits in status that do not indicate

	 * overflow by themselves.

	/*

	 * In case multiple PEBS events are sampled at the same time,

	 * it is possible to have GLOBAL_STATUS bit 62 set indicating

	 * PEBS buffer overflow and also seeing at most 3 PEBS counters

	 * having their bits set in the status register. This is a sign

	 * that there was at least one PEBS record pending at the time

	 * of the PMU interrupt. PEBS counters must only be processed

	 * via the drain_pebs() calls and not via the regular sample

	 * processing loop coming after that the function, otherwise

	 * phony regular samples may be generated in the sampling buffer

	 * not marked with the EXACT tag. Another possibility is to have

	 * one PEBS event and at least one non-PEBS event which overflows

	 * while PEBS has armed. In this case, bit 62 of GLOBAL_STATUS will

	 * not be set, yet the overflow status bit for the PEBS counter will

	 * be on Skylake.

	 *

	 * To avoid this problem, we systematically ignore the PEBS-enabled

	 * counters from the GLOBAL_STATUS mask and we always process PEBS

	 * events via drain_pebs().

	/*

	 * PEBS overflow sets bit 62 in the global status register

		/*

		 * PMI throttle may be triggered, which stops the PEBS event.

		 * Although cpuc->pebs_enabled is updated accordingly, the

		 * MSR_IA32_PEBS_ENABLE is not updated. Because the

		 * cpuc->enabled has been forced to 0 in PMI.

		 * Update the MSR if pebs_enabled is changed.

	/*

	 * Intel PT

	/*

	 * Intel Perf metrics

	/*

	 * Checkpointed counters can lead to 'spurious' PMIs because the

	 * rollback caused by the PMI will have cleared the overflow status

	 * bit. Therefore always force probe these counters.

/*

 * This handler is triggered by the local APIC, so the APIC IRQ handling

 * rules apply:

	/*

	 * Save the PMU state.

	 * It needs to be restored when leaving the handler.

	/*

	 * In general, the early ACK is only applied for old platforms.

	 * For the big core starts from Haswell, the late ACK should be

	 * applied.

	 * For the small core after Tremont, we have to do the ACK right

	 * before re-enabling counters, which is in the middle of the

	 * NMI handler.

	/*

	 * Repeat if there is more work to be done:

 Only restore PMU state when it's active. See x86_pmu_disable(). */

	/*

	 * Only unmask the NMI after the overflow counters

	 * have been reset. This avoids spurious NMIs on

	 * Haswell CPUs.

/*

 * Note: matches a fake event, like Fixed2.

/*

 * manage allocation of shared extra msr for certain events

 *

 * sharing can be:

 * per-cpu: to be shared between the various events on a single PMU

 * per-core: per-cpu + shared by HT threads

	/*

	 * reg->alloc can be set due to existing state, so for fake cpuc we

	 * need to ignore this, otherwise we might fail to allocate proper fake

	 * state for this extra reg constraint. Also see the comment below.

 call x86_get_event_constraint() */

	/*

	 * we use spin_lock_irqsave() to avoid lockdep issues when

	 * passing a fake cpuc

		/*

		 * If its a fake cpuc -- as per validate_{group,event}() we

		 * shouldn't touch event state and we can avoid doing so

		 * since both will only call get_event_constraints() once

		 * on each event, this avoids the need for reg->alloc.

		 *

		 * Not doing the ER fixup will only result in era->reg being

		 * wrong, but since we won't actually try and program hardware

		 * this isn't a problem either.

			/*

			 * x86_schedule_events() can call get_event_constraints()

			 * multiple times on events in the case of incremental

			 * scheduling(). reg->alloc ensures we only do the ER

			 * allocation once.

 lock in msr value */

 one more user */

		/*

		 * need to call x86_get_event_constraint()

		 * to check if associated event has constraints

	/*

	 * Only put constraint if extra reg was actually allocated. Also takes

	 * care of event which do not use an extra shared reg.

	 *

	 * Also, if this is a fake cpuc we shouldn't touch any event state

	 * (reg->alloc) and we don't care about leaving inconsistent cpuc state

	 * either since it'll be thrown out.

 one fewer user */

 allocate again next time */

	/*

	 * nothing needed if in group validation mode

	/*

	 * no exclusion needed

	/*

	 * lock shared state until we are done scheduling

	 * in stop_event_scheduling()

	 * makes scheduling appear as a transaction

	/*

	 * nothing needed if in group validation mode

	/*

	 * no exclusion needed

	/*

	 * release shared state lock (acquired in intel_start_scheduling())

		/*

		 * grab pre-allocated constraint entry

		/*

		 * initialize dynamic constraint

		 * with static constraint

		/*

		 * mark constraint as dynamic

	/*

	 * validating a group does not require

	 * enforcing cross-thread  exclusion

	/*

	 * no exclusion needed

	/*

	 * because we modify the constraint, we need

	 * to make a copy. Static constraints come

	 * from static const tables.

	 *

	 * only needed when constraint has not yet

	 * been cloned (marked dynamic)

	/*

	 * From here on, the constraint is dynamic.

	 * Either it was just allocated above, or it

	 * was allocated during a earlier invocation

	 * of this function

	/*

	 * state of sibling HT

	/*

	 * event requires exclusive counter access

	 * across HT threads

	/*

	 * Modify static constraint with current dynamic

	 * state of thread

	 *

	 * EXCLUSIVE: sibling counter measuring exclusive event

	 * SHARED   : sibling counter measuring non-exclusive event

	 * UNUSED   : sibling counter unused

		/*

		 * exclusive event in sibling counter

		 * our corresponding counter cannot be used

		 * regardless of our event

		/*

		 * if measuring an exclusive event, sibling

		 * measuring non-exclusive, then counter cannot

		 * be used

	/*

	 * if we return an empty mask, then switch

	 * back to static empty constraint to avoid

	 * the cost of freeing later on

	/*

	 * first time only

	 * - static constraint: no change across incremental scheduling calls

	 * - dynamic constraint: handled by intel_get_excl_constraints()

	/*

	 * nothing needed if in group validation mode

	/*

	 * If event was actually assigned, then mark the counter state as

	 * unused now.

		/*

		 * put_constraint may be called from x86_schedule_events()

		 * which already has the lock held so here make locking

		 * conditional.

	/*

	 * is PMU has exclusive counter restrictions, then

	 * all events are subject to and must call the

	 * put_excl_constraints() routine

		/*

		 * Use an alternative encoding for CPU_CLK_UNHALTED.THREAD_P

		 * (0x003c) so that we can use it with PEBS.

		 *

		 * The regular CPU_CLK_UNHALTED.THREAD_P event (0x003c) isn't

		 * PEBS capable. However we can use INST_RETIRED.ANY_P

		 * (0x00c0), which is a PEBS capable event, to get the same

		 * count.

		 *

		 * INST_RETIRED.ANY_P counts the number of cycles that retires

		 * CNTMASK instructions. By setting CNTMASK to a value (16)

		 * larger than the maximum number of instructions that can be

		 * retired per cycle (4) and then inverting the condition, we

		 * count all cycles that retire 16 or less instructions, which

		 * is every cycle.

		 *

		 * Thereby we gain a PEBS capable cycle counter.

		/*

		 * Use an alternative encoding for CPU_CLK_UNHALTED.THREAD_P

		 * (0x003c) so that we can use it with PEBS.

		 *

		 * The regular CPU_CLK_UNHALTED.THREAD_P event (0x003c) isn't

		 * PEBS capable. However we can use UOPS_RETIRED.ALL

		 * (0x01c2), which is a PEBS capable event, to get the same

		 * count.

		 *

		 * UOPS_RETIRED.ALL counts the number of cycles that retires

		 * CNTMASK micro-ops. By setting CNTMASK to a value (16)

		 * larger than the maximum number of micro-ops that can be

		 * retired per cycle (4) and then inverting the condition, we

		 * count all cycles that retire 16 or less micro-ops, which

		 * is every cycle.

		 *

		 * Thereby we gain a PEBS capable cycle counter.

		/*

		 * Use an alternative encoding for CPU_CLK_UNHALTED.THREAD_P

		 * (0x003c) so that we can use it with PEBS.

		 *

		 * The regular CPU_CLK_UNHALTED.THREAD_P event (0x003c) isn't

		 * PEBS capable. However we can use INST_RETIRED.PREC_DIST

		 * (0x01c0), which is a PEBS capable event, to get the same

		 * count.

		 *

		 * The PREC_DIST event has special support to minimize sample

		 * shadowing effects. One drawback is that it can be

		 * only programmed on counter 1, but that seems like an

		 * acceptable trade off.

 BTS is not supported by this architecture. */

 BTS is currently only allowed for user-mode. */

 BTS is not allowed for precise events. */

 disallow bts if conflicting events are present */

		/*

		 * BTS is set up earlier in this path, so don't account twice

 disallow lbr if conflicting events are present */

	/*

	 * Config Topdown slots and metric events

	 *

	 * The slots event on Fixed Counter 3 can support sampling,

	 * which will be handled normally in x86_perf_event_update().

	 *

	 * Metric events don't support sampling and require being paired

	 * with a slots event as group leader. When the slots event

	 * is used in a metrics group, it too cannot support sampling.

		/*

		 * The TopDown metrics events and slots event don't

		 * support any filters.

 The metric events don't support sampling. */

 The metric events require a slots group leader. */

			/*

			 * The leader/SLOTS must not be a sampling event for

			 * metric use; hardware requires it starts at 0 when used

			 * in conjunction with MSR_PERF_METRICS.

			/*

			 * Only once we have a METRICs sibling do we

			 * need TopDown magic.

	/*

	 * The load latency event X86_CONFIG(.event=0xcd, .umask=0x01) on SPR

	 * doesn't function quite right. As a work-around it needs to always be

	 * co-scheduled with a auxiliary event X86_CONFIG(.event=0x03, .umask=0x82).

	 * The actual count of this second event is irrelevant it just needs

	 * to be active to make the first event function correctly.

	 *

	 * In a group, the auxiliary event must be in front of the load latency

	 * event. The rule is to simplify the implementation of the check.

	 * That's because perf cannot have a complete group at the moment.

		/*

		 * If PMU counter has PEBS enabled it is not enough to

		 * disable counter on a guest entry since PEBS memory

		 * write can overshoot guest entry and corrupt guest

		 * memory. Disabling PEBS solves the problem.

		 *

		 * Don't do this if the CPU already enforces it.

	/*

	 * IN_TX/IN_TX-CP filters are not supported by the Haswell PMU with

	 * PEBS or in ANY thread mode. Since the results are non-sensical forbid

	 * this combination.

		/*

		 * Sampling of checkpointed events can cause situations where

		 * the CPU constantly aborts because of a overflow, which is

		 * then checkpointed back and ignored. Forbid checkpointing

		 * for sampling.

		 *

		 * But still allow a long sampling period, so that perf stat

		 * from KVM works.

 Handle special quirk on in_tx_checkpointed only in counter 2 */

	/*

	 * Fixed counter 0 has less skid.

	 * Force instruction:ppp in Fixed counter 0

	/*

	 * The :ppp indicates the Precise Distribution (PDist) facility, which

	 * is only supported on the GP counter 0. If a :ppp event which is not

	 * available on the GP counter 0, error out.

	 * Exception: Instruction PDIR is only available on the fixed counter 0.

 :ppp means to do reduced skid PEBS which is PMC0 only. */

	/*

	 * :ppp means to do reduced skid PEBS,

	 * which is available on PMC0 and fixed counter 0.

 Force instruction:ppp on PMC0 and Fixed counter 0 */

	/*

	 * Without TFA we must not use PMC3.

/*

 * Broadwell:

 *

 * The INST_RETIRED.ALL period always needs to have lowest 6 bits cleared

 * (BDM55) and it must not use a period smaller than 100 (BDM11). We combine

 * the two to enforce a minimum period of 128 (the smallest value that has bits

 * 0-5 cleared and >= 100).

 *

 * Because of how the code in x86_perf_event_set_period() works, the truncation

 * of the lower 6 bits is 'harmless' as we'll occasionally add a longer period

 * to make up for the 'lost' events due to carrying the 'error' in period_left.

 *

 * Therefore the effective (average) period matches the requested period,

 * despite coarser hardware granularity.

 v3 + */

		/*

		 * initialize the locks to keep lockdep happy

 Only check and dump the PMU information for the first CPU */

	/*

	 * Deal with CPUs that don't clear their LBRs on power-up.

	/*

	 * Disable perf metrics if any added CPU doesn't support it.

	 *

	 * Turn off the check for a hybrid architecture, because the

	 * architecture MSR, MSR_IA32_PERF_CAPABILITIES, only indicate

	 * the architecture features. The perf metrics is a model-specific

	 * feature for now. The corresponding bit should always be 0 on

	 * a hybrid platform, e.g., Alder Lake.

 Refer also intel_pmu_aux_output_match() */

 intel_pmu_assign_event() is needed, refer intel_aux_output_init() */

	/*

	 * Intel PMCs cannot be accessed sanely above 32-bit width,

	 * so we install an artificial 1<<31 period regardless of

	 * the generic event period:

	/*

	 * Virtual (or funny metal) CPU can define x86_pmu.extra_regs

	 * together with PMU version 1 and thus be using core_pmu with

	 * shared_regs. We need following callbacks here to allocate

	 * it properly.

	/*

	 * Intel PMCs cannot be accessed sanely above 32 bit width,

	 * so we install an artificial 1<<31 period regardless of

	 * the generic event period:

	/*

	 * PEBS is unreliable due to:

	 *

	 *   AJ67  - PEBS may experience CPL leaks

	 *   AJ68  - PEBS PMI may be delayed by one event

	 *   AJ69  - GLOBAL_STATUS[62] will only be set when DEBUGCTL[12]

	 *   AJ106 - FREEZE_LBRS_ON_PMI doesn't work in combination with PEBS

	 *

	 * AJ67 could be worked around by restricting the OS/USR flags.

	 * AJ69 could be worked around by setting PMU_FREEZE_ON_PMI.

	 *

	 * AJ106 could possibly be worked around by not allowing LBR

	 *       usage from PEBS, including the fixup.

	 * AJ68  could possibly be worked around by always programming

	 *	 a pebs_event_reset[0] value and coping with the lost events.

	 *

	 * But taken together it might just make sense to not enable PEBS on

	 * these chips.

	/*

	 * Serialized by the microcode lock..

/*

 * Under certain circumstances, access certain MSR may cause #GP.

 * The function tests if the input MSR can be safely accessed.

	/*

	 * Disable the check for real HW, so we don't

	 * mess with potentially enabled registers:

	/*

	 * Read the current value, change it and read it back to see if it

	 * matches, this is needed to detect certain hardware emulators

	 * (qemu/kvm) that don't trap on the MSR access and always return 0s.

	/*

	 * Only change the bits which can be updated by wrmsrl.

	/*

	 * Quirk only affects validation in wrmsr(), so wrmsrl()'s value

	 * should equal rdmsrl()'s even with the quirk.

	/* Here it's sure that the MSR can be safely accessed.

	 * Restore the old value and return.

 disable event that reported as not present by cpuid */

		/*

		 * Erratum AAJ80 detected, we work it around by using

		 * the BR_MISP_EXEC.ANY event. This will over-count

		 * branch-misses, but it's still much better than the

		 * architectural event which is often completely bogus:

/*

 * enable software workaround for errata:

 * SNB: BJ122

 * IVB: BV98

 * HSW: HSD29

 *

 * Only needed when HT is enabled. However detecting

 * if HT is enabled is difficult (model specific). So instead,

 * we enable the workaround in the early boot, and verify if

 * it is needed in a later initcall phase once we have valid

 * topology information to check if HT is actually enabled

 Haswell special events */

	/*

	 * check if PMC3 is used

	 * and if so force schedule out for all event types all contexts

 no change */

 Must be in IDX order */

	/*

	 * event on fixed counter2 (REF_CYCLES) only works on this

	 * counter, so do not extend mask to generic counters

		/*

		 * Don't extend the topdown slots and metrics

		 * events to the generic counters.

			/*

			 * Disable topdown slots and metrics events,

			 * if slots event is not in CPUID.

 Disabled fixed counters which are not in CPUID */

	/*

	 * Access extra MSR may cause #GP under certain circumstances.

	 * E.g. KVM doesn't support offcore event

	 * Check all extra_regs here.

 Disable LBR select mapping */

	/*

	 * Check whether the Architectural PerfMon supports

	 * Branch Misses Retired hw_event or not.

	/*

	 * Quirk: v2 perfmon does not report fixed-purpose events, so

	 * assume at least 3 events, when not running in a hypervisor:

 Install first, so it runs last */

	/*

	 * Install the hw-cache-events table:

 UOPS_ISSUED.STALLED_CYCLES */

 UOPS_EXECUTED.CORE_ACTIVE_CYCLES,c=1,i=1 */

		/*

		 * It's recommended to use CPU_CLK_UNHALTED.CORE_P + NPEBS

		 * for precise cycles.

		 * :pp is identical to :ppp

		/*

		 * It's recommended to use CPU_CLK_UNHALTED.CORE_P + NPEBS

		 * for precise cycles.

 Goldmont Plus has 4-wide pipeline */

		/*

		 * It's recommended to use CPU_CLK_UNHALTED.CORE_P + NPEBS

		 * for precise cycles.

 UOPS_ISSUED.STALLED_CYCLES */

 UOPS_EXECUTED.CORE_ACTIVE_CYCLES,c=1,i=1 */

 all extra regs are per-cpu when HT is on */

 UOPS_ISSUED.ANY,c=1,i=1 to count stall cycles */

 UOPS_DISPATCHED.THREAD,c=1,i=1 to count stall cycles*/

 dTLB-load-misses on IVB is different than SNB */

 DTLB_LOAD_MISSES.DEMAND_LD_MISS_CAUSES_A_WALK */

 all extra regs are per-cpu when HT is on */

 UOPS_ISSUED.ANY,c=1,i=1 to count stall cycles */

 all extra regs are per-cpu when HT is on */

 L3_MISS_LOCAL_DRAM is BIT(26) in Broadwell */

 all extra regs are per-cpu when HT is on */

 all extra regs are per-cpu when HT is on */

 INT_MISC.RECOVERY_CYCLES has umask 1 in Skylake */

 all extra regs are per-cpu when HT is on */

		/*

		 * Processors with CPUID.RTM_ALWAYS_ABORT have TSX deprecated by default.

		 * TSX force abort hooks are not required on these systems. Only deploy

		 * workaround when microcode has not enabled X86_FEATURE_RTM_ALWAYS_ABORT.

		/*

		 * Alder Lake has 2 types of CPU, core and atom.

		 *

		 * Initialize the common PerfMon capabilities here.

		/*

		 * The rtm_abort_event is used to check whether to enable GPRs

		 * for the RTM abort event. Atom doesn't have the RTM abort

		 * event. There is no harmful to set it in the common

		 * x86_pmu.rtm_abort_event.

 Initialize big core specific PerfMon capabilities.*/

 Initialize Atom core specific PerfMon capabilities.*/

			/*

			 * default constraints for v2 and up

 AnyThread may be deprecated on arch perfmon v5 or later */

	/*

	 * Access LBR MSR may cause #GP under certain circumstances.

	 * E.g. KVM doesn't support LBR MSR

	 * Check all LBT MSR here.

	 * Disable LBR access if any LBR MSRs can not be accessed.

 only support branch_stack snapshot for perfmon >= v2 */

 Support full width counters using alternative MSR range */

/*

 * HT bug: phase 2 init

 * Called once we have valid topology information to check

 * whether or not HT is enabled

 * If HT is off, then we disable the workaround

	/*

	 * problem not present on this CPU model, nothing to do

 SPDX-License-Identifier: GPL-2.0

 Waste a full page so it can be mapped into the cpu_entry_area */

 The size of a BTS record in bytes: */

/*

 * pebs_record_32 for p4 and core not supported



struct pebs_record_32 {

	u32 flags, ip;

	u32 ax, bc, cx, dx;

	u32 si, di, bp, sp;

};



/*

 * Map PEBS Load Latency Data Source encodings to generic

 * memory data source information

 Version for Sandy Bridge and later */

 0x00:ukn L3 */

 0x01: L1 local */

 0x02: LFB hit */

 0x03: L2 hit */

 0x04: L3 hit */

 0x05: L3 hit, snoop miss */

 0x06: L3 hit, snoop hit */

 0x07: L3 hit, snoop hitm */

 0x08: L3 miss snoop hit */

 0x09: L3 miss snoop hitm*/

 0x0a: L3 miss, shared */

 0x0b: L3 miss, shared */

 0x0c: L3 miss, excl */

 0x0d: L3 miss, excl */

 0x0e: I/O */

 0x0f: uncached */

 Patch up minor differences in the bits */

	/*

	 * bit 4: TLB access

	 * 1 = stored missed 2nd level TLB

	 *

	 * so it either hit the walker or the OS

	 * otherwise hit 2nd level TLB

	/*

	 * bit 0: hit L1 data cache

	 * if not set, then all we know is that

	 * it missed L1D

	/*

	 * bit 5: Locked prefix

	/*

	 * L1 info only valid for following events:

	 *

	 * MEM_UOPS_RETIRED.STLB_MISS_STORES

	 * MEM_UOPS_RETIRED.LOCK_STORES

	 * MEM_UOPS_RETIRED.SPLIT_STORES

	 * MEM_UOPS_RETIRED.ALL_STORES

	/*

	 * use the mapping table for bit 0-3

	/*

	 * Nehalem models do not support TLB, Lock infos

	/*

	 * bit 4: TLB access

	 * 0 = did not miss 2nd level TLB

	 * 1 = missed 2nd level TLB

	/*

	 * bit 5: locked prefix

	/*

	 * Ice Lake and earlier models do not support block infos.

	/*

	 * bit 6: load was blocked since its data could not be forwarded

	 *        from a preceding store

	/*

	 * bit 7: load was blocked due to potential address conflict with

	 *        a preceding store

	/*

	 * use the mapping table for bit 0-3

	/*

	 * bit 4: TLB access

	 * 0 = did not miss 2nd level TLB

	 * 1 = missed 2nd level TLB

	/*

	 * bit 5: locked prefix

/*

 * Same as pebs_record_nhm, with two additional fields.

 Same as HSW, plus TSC */

	/*

	 * This is a cross-CPU update of the cpu_entry_area, we must shoot down

	 * all TLB entries for it.

	/*

	 * HSW+ already provides us the eventing ip; no need to allocate this

	 * buffer then.

 Update the cpu entry area mapping */

 Clear the fixmap */

 Update the fixmap */

 Clear the fixmap */

		/*

		 * Again, ignore errors from offline CPUs, they will no longer

		 * observe cpu_hw_events.ds and not program the DS_AREA when

		 * they come up.

			/*

			 * Ignores wrmsr_on_cpu() errors for offline CPUs they

			 * will get this call through intel_pmu_cpu_starting().

/*

 * BTS

	/*

	 * BTS leaks kernel addresses in branches across the cpl boundary,

	 * such as traps or system calls, so unless the user is asking for

	 * kernel tracing (and right now it's not possible), we'd need to

	 * filter them out. But first we need to count how many of those we

	 * have in the current batch. This is an extra O(n) pass, however,

	 * it's much faster than the other one especially considering that

	 * n <= 2560 (BTS_BUFFER_SIZE / BTS_RECORD_SIZE * 15/16; see the

	 * alloc_bts_buffer()).

		/*

		 * Note that right now *this* BTS code only works if

		 * attr::exclude_kernel is set, but let's keep this extra

		 * check here in case that changes.

	/*

	 * Prepare a generic sample, i.e. fill in the invariant fields.

	 * We will overwrite the from and to address before we output

	 * the sample.

 Filter out any records that contain kernel addresses. */

 There's new data available. */

/*

 * PEBS

 INST_RETIRED.ANY */

 X87_OPS_RETIRED.ANY */

 BR_INST_RETIRED.MISPRED */

 SIMD_INST_RETURED.ANY */

 MEM_LOAD_RETIRED.* */

 INST_RETIRED.ANY_P, inv=1, cmask=16 (cycles:p). */

 INST_RETIRED.ANY */

 MISPREDICTED_BRANCH_RETIRED */

 MEM_LOAD_RETIRED.* */

 INST_RETIRED.ANY_P, inv=1, cmask=16 (cycles:p). */

 Allow all events as PEBS with no flags */

 INST_RETIRED.ANY_P, inv=1, cmask=16 (cycles:p). */

 Allow all events as PEBS with no flags */

 Allow all events as PEBS with no flags */

 Allow all events as PEBS with no flags */

 MEM_INST_RETIRED.* */

 MEM_UNCORE_RETIRED.* */

 MEM_STORE_RETIRED.DTLB_MISS */

 INST_RETIRED.ANY */

 UOPS_RETIRED.* */

 BR_INST_RETIRED.* */

 BR_MISP_RETIRED.NEAR_CALL */

 SSEX_UOPS_RETIRED.* */

 ITLB_MISS_RETIRED */

 MEM_LOAD_RETIRED.* */

 FP_ASSIST.* */

 INST_RETIRED.ANY_P, inv=1, cmask=16 (cycles:p). */

 MEM_INST_RETIRED.* */

 MEM_UNCORE_RETIRED.* */

 MEM_STORE_RETIRED.DTLB_MISS */

 INSTR_RETIRED.* */

 UOPS_RETIRED.* */

 BR_INST_RETIRED.* */

 BR_MISP_RETIRED.* */

 SSEX_UOPS_RETIRED.* */

 ITLB_MISS_RETIRED */

 MEM_LOAD_RETIRED.* */

 FP_ASSIST.* */

 INST_RETIRED.ANY_P, inv=1, cmask=16 (cycles:p). */

 INST_RETIRED.PRECDIST */

 MEM_TRANS_RETIRED.LAT_ABOVE_THR */

 MEM_TRANS_RETIRED.PRECISE_STORES */

 UOPS_RETIRED.ALL, inv=1, cmask=16 (cycles:p). */

 MEM_UOP_RETIRED.* */

 MEM_LOAD_UOPS_RETIRED.* */

 MEM_LOAD_UOPS_LLC_HIT_RETIRED.* */

 MEM_LOAD_UOPS_LLC_MISS_RETIRED.* */

 Allow all events as PEBS with no flags */

 INST_RETIRED.PRECDIST */

 MEM_TRANS_RETIRED.LAT_ABOVE_THR */

 MEM_TRANS_RETIRED.PRECISE_STORES */

 UOPS_RETIRED.ALL, inv=1, cmask=16 (cycles:p). */

 INST_RETIRED.PREC_DIST, inv=1, cmask=16 (cycles:ppp). */

 MEM_UOP_RETIRED.* */

 MEM_LOAD_UOPS_RETIRED.* */

 MEM_LOAD_UOPS_LLC_HIT_RETIRED.* */

 MEM_LOAD_UOPS_LLC_MISS_RETIRED.* */

 Allow all events as PEBS with no flags */

 INST_RETIRED.PRECDIST */

 MEM_TRANS_RETIRED.* */

 UOPS_RETIRED.ALL, inv=1, cmask=16 (cycles:p). */

 INST_RETIRED.PREC_DIST, inv=1, cmask=16 (cycles:ppp). */

 UOPS_RETIRED.ALL */

 MEM_UOPS_RETIRED.STLB_MISS_LOADS */

 MEM_UOPS_RETIRED.LOCK_LOADS */

 MEM_UOPS_RETIRED.SPLIT_LOADS */

 MEM_UOPS_RETIRED.ALL_LOADS */

 MEM_UOPS_RETIRED.STLB_MISS_STORES */

 MEM_UOPS_RETIRED.SPLIT_STORES */

 MEM_UOPS_RETIRED.ALL_STORES */

 MEM_LOAD_UOPS_RETIRED.* */

 MEM_LOAD_UOPS_L3_HIT_RETIRED.* */

 MEM_LOAD_UOPS_L3_MISS_RETIRED.* */

 Allow all events as PEBS with no flags */

 INST_RETIRED.PRECDIST */

 MEM_TRANS_RETIRED.* */

 UOPS_RETIRED.ALL, inv=1, cmask=16 (cycles:p). */

 INST_RETIRED.PREC_DIST, inv=1, cmask=16 (cycles:ppp). */

 UOPS_RETIRED.ALL */

 MEM_UOPS_RETIRED.STLB_MISS_LOADS */

 MEM_UOPS_RETIRED.LOCK_LOADS */

 MEM_UOPS_RETIRED.SPLIT_LOADS */

 MEM_UOPS_RETIRED.ALL_LOADS */

 MEM_UOPS_RETIRED.STLB_MISS_STORES */

 MEM_UOPS_RETIRED.SPLIT_STORES */

 MEM_UOPS_RETIRED.ALL_STORES */

 MEM_LOAD_UOPS_RETIRED.* */

 MEM_LOAD_UOPS_L3_HIT_RETIRED.* */

 MEM_LOAD_UOPS_L3_MISS_RETIRED.* */

 Allow all events as PEBS with no flags */

 INST_RETIRED.PREC_DIST */

 INST_RETIRED.PREC_DIST, inv=1, cmask=16 (cycles:ppp). */

 INST_RETIRED.TOTAL_CYCLES_PS (inv=1, cmask=16) (cycles:p). */

 MEM_TRANS_RETIRED.* */

 MEM_INST_RETIRED.STLB_MISS_LOADS */

 MEM_INST_RETIRED.STLB_MISS_STORES */

 MEM_INST_RETIRED.LOCK_LOADS */

 MEM_INST_RETIRED.LOCK_STORES */

 MEM_INST_RETIRED.SPLIT_LOADS */

 MEM_INST_RETIRED.SPLIT_STORES */

 MEM_INST_RETIRED.ALL_LOADS */

 MEM_INST_RETIRED.ALL_STORES */

 MEM_LOAD_RETIRED.* */

 MEM_LOAD_L3_HIT_RETIRED.* */

 MEM_LOAD_L3_MISS_RETIRED.* */

 Allow all events as PEBS with no flags */

 old INST_RETIRED.PREC_DIST */

 INST_RETIRED.PREC_DIST */

 SLOTS */

 MEM_TRANS_RETIRED.LOAD_LATENCY */

 MEM_INST_RETIRED.LOAD */

 MEM_INST_RETIRED.STORE */

 MEM_LOAD_*_RETIRED.* */

 MEM_INST_RETIRED.* */

	/*

	 * Everything else is handled by PMU_FL_PEBS_ALL, because we

	 * need the full constraints from the main table.

 INST_RETIRED.PREC_DIST */

	/*

	 * Everything else is handled by PMU_FL_PEBS_ALL, because we

	 * need the full constraints from the main table.

	/*

	 * Extended PEBS support

	 * Makes the PEBS code search the normal constraints.

/*

 * We need the sched_task callback even for per-cpu events when we use

 * the large interrupt threshold, such that we can provide PID and TID

 * to PEBS samples.

	/*

	 * We need GPRs when:

	 * + user requested them

	 * + precise_ip < 2 for the non event IP

	 * + For RTM TSX weight we need GPRs for the abort code.

		/*

		 * For now always log all LBRs. Could configure this

		 * later.

	/*

	 * Make sure we get updated with the first PEBS

	 * event. It will trigger also during removal, but

	 * that does not hurt:

	/*

	 * The PEBS record doesn't shrink on pmu::del(). Doing so would require

	 * iterating all remaining PEBS events to reconstruct the config.

 Clear pebs_data_cfg and pebs_record_size for first PEBS. */

 Update pebs_record_size if new event requires more data. */

	/*

	 * Use auto-reload if possible to save a MSR write in the PMI.

	 * This must be done in pmu::start(), because PERF_EVENT_IOC_PERIOD.

	/*

	 * We don't need to fixup if the PEBS assist is fault like

	/*

	 * No LBR entry, no basic block, no rewinding

	/*

	 * Basic blocks should never cross user/kernel boundaries

	/*

	 * unsigned math, either ip is before the start (impossible) or

	 * the basic block is larger than 1 page (sanity)

	/*

	 * We sampled a branch insn, rewind using the LBR stack

 'size' must fit our buffer, see above */

		/*

		 * Make sure there was not a problem decoding the instruction.

		 * This is doubly important because we have an infinite loop if

		 * insn.length=0.

	/*

	 * Even though we decoded the basic block, the instruction stream

	 * never matched the given IP, either the TO or the IP got corrupted.

 For RTM XABORTs also log the abort code from AX */

	/*

	 * We cast to the biggest pebs_record but are careful not to

	 * unconditionally access the 'extra' entries.

	/*

	 * Use latency for weight (only avail with PEBS-LL)

	/*

	 * data.data_src encodes the data source

	/*

	 * We must however always use iregs for the unwinder to stay sane; the

	 * record BP,SP,IP can point into thin air when the record is from a

	 * previous PMI context or an (I)RET happened between the record and

	 * PMI.

	/*

	 * We use the interrupt regs as a base because the PEBS record does not

	 * contain a full regs set, specifically it seems to lack segment

	 * descriptors, which get used by things like user_mode().

	 *

	 * In the simple case fix up only the IP for PERF_SAMPLE_IP.

	/*

	 * Initialize regs_>flags from PEBS,

	 * Clear exact bit (which uses x86 EFLAGS Reserved bit 3),

	 * i.e., do not rely on it being zero:

		/*

		 * Haswell and later processors have an 'eventing IP'

		 * (real IP) which fixes the off-by-1 skid in hardware.

		 * Use it when precise_ip >= 2 :

 Otherwise, use PEBS off-by-1 IP: */

			/*

			 * With precise_ip >= 2, try to fix up the off-by-1 IP

			 * using the LBR. If successful, the fixup function

			 * corrects regs->ip and calls set_linear_ip() on regs:

		/*

		 * When precise_ip == 1, return the PEBS off-by-1 IP,

		 * no fixup attempted:

 Only set the TSX weight when no memory weight. */

	/*

	 * v3 supplies an accurate time stamp, so we use that

	 * for the time stamp.

	 *

	 * We can only do this for the default trace clock.

/*

 * With adaptive PEBS the layout depends on what fields are configured.

	/*

	 * We must however always use iregs for the unwinder to stay sane; the

	 * record BP,SP,IP can point into thin air when the record is from a

	 * previous PMI context or an (I)RET happened between the record and

	 * PMI.

 The ip in basic is EventingIP */

	/*

	 * The record for MEMINFO is in front of GP

	 * But PERF_SAMPLE_TRANSACTION needs gprs->ax.

	 * Save the pointer here but process later.

			/*

			 * Although meminfo::latency is defined as a u64,

			 * only the lower 32 bits include the valid data

			 * in practice on Ice Lake and earlier platforms.

	/*

	 * fmt0 does not have a status bitfield (does not use

	 * perf_record_nhm format)

 PEBS v3 has accurate status bits */

 clear non-PEBS bit and re-check */

/*

 * Special variant of intel_pmu_save_and_restart() for auto-reload.

	/*

	 * drain_pebs() only happens when the PMU is disabled.

	/*

	 * Since the counter increments a negative counter value and

	 * overflows on the sign switch, giving the interval:

	 *

	 *   [-period, 0]

	 *

	 * the difference between two consecutive reads is:

	 *

	 *   A) value2 - value1;

	 *      when no overflows have happened in between,

	 *

	 *   B) (0 - value1) + (value2 - (-period));

	 *      when one overflow happened in between,

	 *

	 *   C) (0 - value1) + (n - 1) * (period) + (value2 - (-period));

	 *      when @n overflows happened in between.

	 *

	 * Here A) is the obvious difference, B) is the extension to the

	 * discrete interval, where the first term is to the top of the

	 * interval and the second term is from the bottom of the next

	 * interval and C) the extension to multiple intervals, where the

	 * middle term is the whole intervals covered.

	 *

	 * An equivalent of C, by reduction, is:

	 *

	 *   value2 - value1 + n * period

		/*

		 * Now, auto-reload is only enabled in fixed period mode.

		 * The reload value is always hwc->sample_period.

		 * May need to change it, if auto-reload is enabled in

		 * freq mode later.

		/*

		 * The PEBS records may be drained in the non-overflow context,

		 * e.g., large PEBS + context switch. Perf should treat the

		 * last record the same as other PEBS records, and doesn't

		 * invoke the generic overflow handler.

		/*

		 * All but the last records are processed.

		 * The last one is left to be able to call the overflow handler.

 PMC0 only */

	/*

	 * Whatever else happens, drain the thing

	/*

	 * The drain_pebs() could be called twice in a short period

	 * for auto-reload event in pmu::read(). There are no

	 * overflows have happened in between.

	 * It needs to call intel_pmu_save_and_restart_reload() to

	 * update the event->count for this case.

 PEBS v3 has more accurate status bits */

		/*

		 * On some CPUs the PEBS status can be zero when PEBS is

		 * racing with clearing of GLOBAL_STATUS.

		 *

		 * Normally we would drop that record, but in the

		 * case when there is only a single active PEBS event

		 * we can assume it's for that event.

		/*

		 * The PEBS hardware does not deal well with the situation

		 * when events happen near to each other and multiple bits

		 * are set. But it should happen rarely.

		 *

		 * If these events include one PEBS and multiple non-PEBS

		 * events, it doesn't impact PEBS record. The record will

		 * be handled normally. (slow path)

		 *

		 * If these events include two or more PEBS events, the

		 * records for the events can be collapsed into a single

		 * one, and it's not possible to reconstruct all events

		 * that caused the PEBS record. It's called collision.

		 * If collision happened, the record will be dropped.

 log dropped samples number */

/*

 * BTS, PEBS probe and setup

	/*

	 * No support for 32bit formats

			/*

			 * Using >PAGE_SIZE buffers makes the WRMSR to

			 * PERF_GLOBAL_CTRL in intel_pmu_enable_all()

			 * mysteriously hang on Core2.

			 *

			 * As a workaround, we don't do this.

 Only basic record supported */

 SPDX-License-Identifier: GPL-2.0

 Nehalem-EX/Westmere-EX uncore support */

 NHM-EX event control */

 NHM-EX Ubox */

 NHM-EX Cbox */

 NHM-EX Bbox */

 NHM-EX Sbox */

 NHM-EX Mbox */

/*

 * use the 9~13 bits to select event If the 7th bit is not set,

 * otherwise use the 19~21 bits to select event.

 NHM-EX Rbox */

 NHM-EX Wbox */

 WBox has a fixed counter */

 WBox has a fixed counter */

 msr offset for each instance of cbox */

 end: all zeroes */ },

 events that do not use the match/mask registers */

/*

 * The Bbox has 4 counters, but each counter monitors different events.

 * Use bits 6-7 in the event config to select counter.

 only TO_R_PROG_EV event uses the match/mask register */

 event 0xa uses two extra registers */

 events 0xd ~ 0x10 use the same extra register */

 Nehalem-EX or Westmere-EX ? */

	/*

	 * The ZDP_CTL_FVC MSR has 4 fields which are used to control

	 * events 0xd ~ 0x10. Besides these 4 fields, there are additional

	 * fields which are shared.

 mask of the shared fields */

 add mask of the non-shared field if it's in use */

 get the non-shared control bits and shift them */

 add the shared control bits back */

 adjust the main event selector */

 for the match/mask registers */

	/*

	 * If it's a fake box -- as per validate_{group,event}() we

	 * shouldn't touch event state and we can avoid doing so

	 * since both will only call get_event_constraints() once

	 * on each event, this avoids the need for reg->alloc.

		/*

		 * events 0xd ~ 0x10 are functional identical, but are

		 * controlled by different fields in the ZDP_CTL_FVC

		 * register. If we failed to take one field, try the

		 * rest 3 choices.

	/*

	 * The mbox events may require 2 extra MSRs at the most. But only

	 * the lower 32 bits in these MSRs are significant, so we can use

	 * config1 to pass two MSRs' config.

 always use the 32~63 bits to pass the PLD config */

	/*

	 * The mbox only provides ability to perform address matching

	 * for the PLD events.

 end: all zeroes */ },

 end: all zeroes */ },

 adjust the main event selector and extra register index */

 adjust extra register config */

 shift the 8~15 bits to the 0~7 bits */

 shift the 0~7 bits to the 8~15 bits */

/*

 * Each rbox has 4 event set which monitor PQI port 0~3 or 4~7.

 * An event set consists of 6 events, the 3rd and 4th events in

 * an event set use the same extra register. So an event set uses

 * 5 extra registers.

 the 3rd and 4th events use the same extra register */

		/*

		 * these two events use different fields in a extra register,

		 * the 0~7 bits and the 8~15 bits respectively.

		/*

		 * The Rbox events are always in pairs. The paired

		 * events are functional identical, but use different

		 * extra registers. If we failed to take an extra

		 * register, try the alternative.

 end: all zeroes */ },

 end of Nehalem-EX uncore support */

/*

 * Performance events - AMD IBS

 *

 *  Copyright (C) 2011 Advanced Micro Devices, Inc., Robert Richter

 *

 *  For licencing details see kernel-base/COPYING

/*

 * IBS states:

 *

 * ENABLED; tracks the pmu::add(), pmu::del() state, when set the counter is taken

 * and any further add()s must fail.

 *

 * STARTED/STOPPING/STOPPED; deal with pmu::start(), pmu::stop() state but are

 * complicated by the fact that the IBS hardware can send late NMIs (ie. after

 * we've cleared the EN bit).

 *

 * In order to consume these late NMIs we have the STOPPED state, any NMI that

 * happens after we've cleared the EN state will clear this bit and report the

 * NMI handled (this is fundamentally racy in the face or multiple NMI sources,

 * someone else can consume our BIT and our NMI will go unhandled).

 *

 * And since we cannot set/clear this separate bit together with the EN bit,

 * there are races; if we cleared STARTED early, an NMI could land in

 * between clearing STARTED and clearing the EN bit (in fact multiple NMIs

 * could happen if the period is small enough), and consume our STOPPED bit

 * and trigger streams of unhandled NMIs.

 *

 * If, however, we clear STARTED late, an NMI can hit between clearing the

 * EN bit and clearing STARTED, still see STARTED set and process the event.

 * If this event will have the VALID bit clear, we bail properly, but this

 * is not a given. With VALID set we can end up calling pmu::stop() again

 * (the throttle logic) and trigger the WARNs in there.

 *

 * So what we do is set STOPPING before clearing EN to avoid the pmu::stop()

 * nesting, and clear STARTED late, so that we have a well defined state over

 * the clearing of the EN bit.

 *

 * XXX: we could probably be using !atomic bitops for all this.

	/*

	 * If we are way outside a reasonable range then just skip forward:

	/*

	 * If the hw period that triggers the sw overflow is too short

	 * we might hit the irq handler. This biases the results.

	 * Thus we shorten the next-to-last period and set the last

	 * period to the max period.

	/*

	 * Careful: an NMI might modify the previous event value.

	 *

	 * Our tactic to handle this is to first atomically read and

	 * exchange a new raw count - then add that new-prev delta

	 * count to the generic event atomically:

	/*

	 * Now we have the new raw value and have updated the prev

	 * timestamp already. We can now calculate the elapsed delta

	 * (event-)time and add that to the generic event.

	 *

	 * Careful, not all hw sign-extends above the physical width

	 * of the count.

/*

 * Use IBS for precise event sampling:

 *

 *  perf record -a -e cpu-cycles:p ...    # use ibs op counting cycle count

 *  perf record -a -e r076:p ...          # same as -e cpu-cycles:p

 *  perf record -a -e r0C1:p ...          # use ibs op counting micro-ops

 *

 * IbsOpCntCtl (bit 19) of IBS Execution Control Register (IbsOpCtl,

 * MSRC001_1033) is used to select either cycle or micro-ops counting

 * mode.

 *

 * The rip of IBS samples has skid 0. Thus, IBS supports precise

 * levels 1 and 2 and the PERF_EFLAGS_EXACT is set. In rare cases the

 * rip is invalid when IBS was not able to record the rip correctly.

 * We clear PERF_EFLAGS_EXACT and take the rip from pt_regs then.

 *

 raw max_cnt may not be set */

			/*

			 * lower 4 bits can not be set in ibs max cnt,

			 * but allowing it in case we adjust the

			 * sample period to set a frequency.

	/*

	 * If we modify hwc->sample_period, we also need to update

	 * hwc->last_period and hwc->period_left.

 ignore lower 4 bits in min count: */

	/*

	 * If the internal 27-bit counter rolled over, the count is MaxCnt

	 * and the lower 7 bits of CurCnt are randomized.

	 * Otherwise CurCnt has the full 27-bit current counter value.

	/*

	 * Set width to 64 since we do not overflow on max width but

	 * instead on max count. In perf_ibs_set_period() we clear

	 * prev count manually on overflow.

/*

 * Erratum #420 Instruction-Based Sampling Engine May Generate

 * Interrupt that Cannot Be Cleared:

 *

 * Must clear counter mask first, then clear the enable bit. See

 * Revision Guide for AMD Family 10h Processors, Publication #41322.

/*

 * We cannot restore the ibs pmu state, so we always needs to update

 * the event while stopping it and then reset the state when starting

 * again. Thus, ignoring PERF_EF_RELOAD and PERF_EF_UPDATE flags in

 * perf_ibs_start()/perf_ibs_stop() and instead always do it.

	/*

	 * Set STARTED before enabling the hardware, such that a subsequent NMI

	 * must observe it.

		/*

		 * Set STOPPED before disabling the hardware, such that it

		 * must be visible to NMIs the moment we clear the EN bit,

		 * at which point we can generate an !VALID sample which

		 * we need to consume.

		/*

		 * Clear STARTED after disabling the hardware; if it were

		 * cleared before an NMI hitting after the clear but before

		 * clearing the EN bit might think it a spurious NMI and not

		 * handle it.

		 *

		 * Clearing it after, however, creates the problem of the NMI

		 * handler seeing STARTED but not having a valid sample.

	/*

	 * Clear valid bit to not count rollovers on update, rollovers

	 * are only updated in the irq handler.

 &format_attr_cnt_ctl.attr if IBS_CAPS_OPCNT */

		/*

		 * Catch spurious interrupts after stopping IBS: After

		 * disabling IBS there could be still incoming NMIs

		 * with samples that even have the valid bit cleared.

		 * Mark all this NMIs as handled.

 no sw counter overflow */

	/*

	 * Read IbsBrTarget, IbsOpData4, and IbsExtdCtl separately

	 * depending on their availability.

	 * Can't add to offset_max as they are staggered

 Workaround for erratum #1197 */

 register attributes */

	/*

	 * Some chips fail to reset the fetch count when it is written; instead

	 * they need a 0-1 transition of IbsFetchEn.

 defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_AMD) */

 IBS - apic initialization, for perf and oprofile */

 check IBS cpuid feature flags */

 cpuid flags not valid */

/*

 * Check and reserve APIC extended interrupt LVT offset for IBS if available.

/*

 * This runs only on the current cpu. We try to find an LVT offset and

 * setup the local APIC. For this we must disable preemption. On

 * success we initialize all nodes with this offset. This updates then

 * the offset in the IBS_CTL per-node msr. The per-core APIC setup of

 * the IBS interrupt vector is handled by perf_ibs_cpu_notifier that

 * is using the new offset.

 find the next free available EILVT entry, skip offset 0 */

	/*

	 * Force LVT offset assignment for family 10h: The offsets are

	 * not assigned by the BIOS for this family, so the OS is

	 * responsible for doing it. If the OS assignment fails, fall

	 * back to BIOS settings and try to setup this.

 ibs not supported by the cpu */

 make ibs_caps visible to other cpus: */

	/*

	 * x86_pmu_amd_ibs_starting_cpu will be called from core on

	 * all online cpus.

 Since we need the pci subsystem to init ibs we can't do this earlier: */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2013 Advanced Micro Devices, Inc.

 *

 * Author: Steven Kinney <Steven.Kinney@amd.com>

 * Author: Suravee Suthikulpanit <Suraveee.Suthikulpanit@amd.com>

 *

 * Perf: amd_iommu - AMD IOMMU Performance Counter PMU implementation

 iommu pmu conf masks */

 iommu pmu conf1 masks */

/*---------------------------------------------

 * sysfs format attributes

/*---------------------------------------------

 * sysfs events attributes

 end: all zeroes */ },

/*---------------------------------------------

 * sysfs cpumask attributes

---------------------------------------------*/

 test the event attr type check for PMU enumeration */

	/*

	 * IOMMU counters are shared across all cores.

	 * Therefore, it does not support per-process mode.

	 * Also, it does not support event sampling mode.

 update the hw_perf_event struct with the iommu config data */

	/*

	 * To account for power-gating, which prevents write to

	 * the counter, we need to enable the counter

	 * before setting up counter register.

		/*

		 * Since the IOMMU PMU only support counting mode,

		 * the counter always start with value zero.

 IOMMU pc counter register is only 48 bits */

	/*

	 * Since the counter always start with value zero,

	 * simply just accumulate the count for the event.

	/*

	 * To account for power-gating, in which reading the counter would

	 * return zero, we need to read the register before disabling.

 request an iommu bank/counter */

 clear the assigned iommu bank/counter */

 Make sure the IOMMU PC resource is available */

	/*

	 * An IOMMU PMU is specific to an IOMMU, and can function independently.

	 * So we go through all IOMMUs and ignore the one that fails init

	 * unless all IOMMU are failing.

 Init cpumask attributes to only core 0 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2013 Advanced Micro Devices, Inc.

 *

 * Author: Jacob Shin <jacob.shin@amd.com>

	/*

	 * since we do not enable counter overflow interrupts,

	 * we do not have to worry about prev_count changing on us

 are we already assigned? */

 if not, take the first available counter */

/*

 * Return a full thread and slice mask unless user

 * has provided them

	/*

	 * If the user doesn't specify a threadmask, they're not trying to

	 * count core 0, so we enable all cores & threads.

	 * We'll also assume that they want to count slice 0 if they specify

	 * a threadmask and leave sliceid and enallslices unpopulated.

	/*

	 * NB and Last level cache counters (MSRs) are shared across all cores

	 * that share the same NB / Last level cache.  On family 16h and below,

	 * Interrupts can be directed to a single target core, however, event

	 * counts generated by processes running on other cores cannot be masked

	 * out. So we do not support sampling and per-thread events via

	 * CAP_NO_INTERRUPT, and we do not enable counter overflow interrupts:

	/*

	 * SliceMask and ThreadMask need to be set for certain L3 events.

	 * For other events, the two fields do not affect the count.

	/*

	 * since request can come in to any of the shared cores, we will remap

	 * to a single common cpu.

 F17h+ DF */

 F17h+ L3 */

 F19h L3 */

 F17h L3 */

 F17h L3 */

 F19h L3 */

 F19h L3 */

 F19h L3 */

 F19h L3 */

 event14 if F17h+ */

 event8 if F17h+ */

 slicemask if F17h,	coreid if F19h */

 threadmask8 if F17h,	enallslices if F19h */

			enallcores if F19h */

			sliceid if F19h */

			threadmask2 if F19h */

 this cpu is going down, migrate to a shared sibling if possible */

		/*

		 * For F17h and above, the Northbridge counters are

		 * repurposed as Data Fabric counters. Also, L3

		 * counters are supported too. The PMUs are exported

		 * based on family as either L2 or L3 and NB or DF.

	/*

	 * Install callbacks. Core will call them for each online cpu.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Performance events - AMD Processor Power Reporting Mechanism

 *

 * Copyright (C) 2016 Advanced Micro Devices, Inc.

 *

 * Author: Huang Rui <ray.huang@amd.com>

 Event code: LSB 8 bits, passed in attr->config any other bit is reserved. */

/*

 * Accumulated power status counters.

/*

 * The ratio of compute unit power accumulator sample period to the

 * PTSC period.

 Maximum accumulated power of a compute unit. */

/*

 * Accumulated power represents the sum of each compute unit's (CU) power

 * consumption. On any core of each CU we read the total accumulated power from

 * MSR_F15H_CU_PWR_ACCUMULATOR. cpu_mask represents CPU bit map of all cores

 * which are picked to measure the power for the CUs they belong to.

	/*

	 * Calculate the CU power consumption over a time period, the unit of

	 * final value (delta) is micro-Watts. Then add it to the event count.

 Mark event as deactivated and stopped. */

 Check if software counter update is necessary. */

		/*

		 * Drain the remaining delta count out of an event

		 * that we are disabling:

 Only look at AMD power events. */

 Unsupported modes and filters. */

/*

 * Currently it only supports to report the power of each

 * processor/package.

 Convert the count from micro-Watts to milli-Watts. */

 system-wide only */

	/*

	 * Find a new CPU on the same compute unit, if was set in cpumask

	 * and still some CPUs on compute unit. Then migrate event and

	 * context to new CPU.

	/*

	 * 1) If any CPU is set at cpu_mask in the same compute unit, do

	 * nothing.

	 * 2) If no CPU is set at cpu_mask in the same compute unit,

	 * set current ONLINE CPU.

	 *

	 * Note: if there is a CPU aside of the new one already in the

	 * sibling mask, then it is also in cpu_mask.

 SPDX-License-Identifier: GPL-2.0-only

 AMD Event 0xFFF: Merge.  Used with Large Increment per Cycle events */

 Data Cache Accesses        */

 Data Cache Misses          */

 Data Prefetcher :attempts  */

 Data Prefetcher :cancelled */

 Instruction cache fetches  */

 Instruction cache misses   */

 Prefetch Instructions :Load */

 Requests to L2 Cache :IC+DC */

 L2 Cache Misses : IC+DC     */

 L2 Fill/Writeback           */

 Data Cache Accesses        */

 L1_DTLB_AND_L2_DLTB_MISS.ALL */

 Instruction fecthes        */

 L1_ITLB_AND_L2_ITLB_MISS.ALL */

 Retired Branch Instr.      */

 Retired Mispredicted BI    */

 CPU Request to Memory, l+r */

 CPU Request to Memory, r   */

 Data Cache Accesses */

 L2$ access from DC Miss */

 h/w prefetch DC Fills */

 Instruction cache fetches  */

 Instruction cache misses   */

 All L2 DTLB accesses */

 L2 DTLB misses (PT walks) */

 L1 ITLB misses, L2 ITLB hits */

 L1 ITLB misses, L2 misses */

 Retired Branch Instr.      */

 Retired Mispredicted BI    */

/*

 * AMD Performance Monitor K7 and later, up to and including Family 16h:

 "Decoder empty" event */

 "Dispatch stalls" event */

/*

 * AMD Performance Monitor Family 17h and later:

/*

 * Previously calculated offsets

/*

 * Legacy CPUs:

 *   4 counters starting at 0xc0010000 each offset by 1

 *

 * CPUs with core performance counter extensions:

 *   6 counters starting at 0xc0010200 each offset by 2

/*

 * AMD64 events are detected based on their event codes.

 Retired SSE/AVX FLOPs */

		/*

		 * When HO == GO == 1 the hardware treats that as GO == HO == 0

		 * and will count in both modes. We don't want to count in that

		 * case so we emulate no-counting by setting US = OS = 0.

 pass precise event sampling to ibs: */

	/*

	 * need to scan whole list because event may not have

	 * been assigned during scheduling

	 *

	 * no race condition possible because event can only

	 * be removed on one CPU at a time AND PMU is disabled

	 * when we come here

 /*

  * AMD64 NorthBridge events need special treatment because

  * counter access needs to be synchronized across all cores

  * of a package. Refer to BKDG section 3.12

  *

  * NB events are events measuring L3 cache, Hypertransport

  * traffic. They are identified by an event code >= 0xe00.

  * They measure events on the NorthBride which is shared

  * by all cores on a package. NB events are counted on a

  * shared set of counters. When a NB event is programmed

  * in a counter, the data actually comes from a shared

  * counter. Thus, access to those counters needs to be

  * synchronized.

  *

  * We implement the synchronization such that no two cores

  * can be measuring NB events using the same counters. Thus,

  * we maintain a per-NB allocation table. The available slot

  * is propagated using the event_constraint structure.

  *

  * We provide only one choice for each NB event based on

  * the fact that only NB events have restrictions. Consequently,

  * if a counter is available, there is a guarantee the NB event

  * will be assigned to it. If no slot is available, an empty

  * constraint is returned and scheduling will eventually fail

  * for this event.

  *

  * Note that all cores attached the same NB compete for the same

  * counters to host NB events, this is why we use atomic ops. Some

  * multi-chip CPUs may have more than one NB.

  *

  * Given that resources are allocated (cmpxchg), they must be

  * eventually freed for others to use. This is accomplished by

  * calling __amd_put_nb_event_constraints()

  *

  * Non NB events are not impacted by this restriction.

	/*

	 * detect if already present, if so reuse

	 *

	 * cannot merge with actual allocation

	 * because of possible holes

	 *

	 * event can already be present yet not assigned (in hwc->idx)

	 * because of successive calls to x86_schedule_events() from

	 * hw_perf_group_sched_in() without hw_perf_enable()

 assign free slot, prefer hwc->idx */

 event already present */

 reassign to this slot */

 already present, reuse */

	/*

	 * initialize all possible NB constraints

/*

 * When a PMC counter overflows, an NMI is used to process the event and

 * reset the counter. NMI latency can result in the counter being updated

 * before the NMI can run, which can result in what appear to be spurious

 * NMIs. This function is intended to wait for the NMI to run and reset

 * the counter to avoid possible unhandled NMI messages.

	/*

	 * Wait for the counter to be reset if it has overflowed. This loop

	 * should exit very, very quickly, but just in case, don't wait

	 * forever...

 Might be in IRQ context, so can't sleep */

	/*

	 * This shouldn't be called from NMI context, but add a safeguard here

	 * to return, since if we're in NMI context we can't wait for an NMI

	 * to reset an overflowed counter value.

	/*

	 * Check each counter for overflow and wait for it to be reset by the

	 * NMI if it has overflowed. This relies on the fact that all active

	 * counters are always enabled when this function is called and

	 * ARCH_PERFMON_EVENTSEL_INT is always set.

	/*

	 * This can be called from NMI context (via x86_pmu_stop). The counter

	 * may have overflowed, but either way, we'll never see it get reset

	 * by the NMI if we're already in the NMI. And the NMI latency support

	 * below will take care of any pending NMI that might have been

	 * generated by the overflow.

/*

 * Because of NMI latency, if multiple PMC counters are active or other sources

 * of NMIs are received, the perf NMI handler can handle one or more overflowed

 * PMC counters outside of the NMI associated with the PMC overflow. If the NMI

 * doesn't arrive at the LAPIC in time to become a pending NMI, then the kernel

 * back-to-back NMI support won't be active. This PMC handler needs to take into

 * account that this can occur, otherwise this could result in unknown NMI

 * messages being issued. Examples of this is PMC overflow while in the NMI

 * handler when multiple PMCs are active or PMC overflow while handling some

 * other source of an NMI.

 *

 * Attempt to mitigate this by creating an NMI window in which un-handled NMIs

 * received during this window will be claimed. This prevents extending the

 * window past when it is possible that latent NMIs should be received. The

 * per-CPU perf_nmi_tstamp will be set to the window end time whenever perf has

 * handled a counter. When an un-handled NMI is received, it will be claimed

 * only if arriving within that window.

 Process any counter overflows */

	/*

	 * If a counter was handled, record a timestamp such that un-handled

	 * NMIs will be claimed if arriving within that window.

	/*

	 * if not NB event or no NB, then no constraints

 AMD Family 15h */

/*

 * AMD family 15h event code/PMC mappings:

 *

 * type = event_code & 0x0F0:

 *

 * 0x000	FP	PERF_CTL[5:3]

 * 0x010	FP	PERF_CTL[5:3]

 * 0x020	LS	PERF_CTL[5:0]

 * 0x030	LS	PERF_CTL[5:0]

 * 0x040	DC	PERF_CTL[5:0]

 * 0x050	DC	PERF_CTL[5:0]

 * 0x060	CU	PERF_CTL[2:0]

 * 0x070	CU	PERF_CTL[2:0]

 * 0x080	IC/DE	PERF_CTL[2:0]

 * 0x090	IC/DE	PERF_CTL[2:0]

 * 0x0A0	---

 * 0x0B0	---

 * 0x0C0	EX/LS	PERF_CTL[5:0]

 * 0x0D0	DE	PERF_CTL[2:0]

 * 0x0E0	NB	NB_PERF_CTL[3:0]

 * 0x0F0	NB	NB_PERF_CTL[3:0]

 *

 * Exceptions:

 *

 * 0x000	FP	PERF_CTL[3], PERF_CTL[5:3] (*)

 * 0x003	FP	PERF_CTL[3]

 * 0x004	FP	PERF_CTL[3], PERF_CTL[5:3] (*)

 * 0x00B	FP	PERF_CTL[3]

 * 0x00D	FP	PERF_CTL[3]

 * 0x023	DE	PERF_CTL[2:0]

 * 0x02D	LS	PERF_CTL[3]

 * 0x02E	LS	PERF_CTL[3,0]

 * 0x031	LS	PERF_CTL[2:0] (**)

 * 0x043	CU	PERF_CTL[2:0]

 * 0x045	CU	PERF_CTL[2:0]

 * 0x046	CU	PERF_CTL[2:0]

 * 0x054	CU	PERF_CTL[2:0]

 * 0x055	CU	PERF_CTL[2:0]

 * 0x08F	IC	PERF_CTL[0]

 * 0x187	DE	PERF_CTL[0]

 * 0x188	DE	PERF_CTL[0]

 * 0x0DB	EX	PERF_CTL[5:0]

 * 0x0DC	LS	PERF_CTL[5:0]

 * 0x0DD	LS	PERF_CTL[5:0]

 * 0x0DE	LS	PERF_CTL[5:0]

 * 0x0DF	LS	PERF_CTL[5:0]

 * 0x1C0	EX	PERF_CTL[5:3]

 * 0x1D6	EX	PERF_CTL[5:0]

 * 0x1D8	EX	PERF_CTL[5:0]

 *

 * (*)  depending on the umask all FPU counters may be used

 * (**) only one unitmask enabled at a time

 moved to uncore.c */

 use highest bit to detect overflow */

 Avoid calculating the value each time in the NMI handler */

	/*

	 * If core performance counter extensions exists, we must use

	 * MSR_F15H_PERF_CTL/MSR_F15H_PERF_CTR msrs. See also

	 * amd_pmu_addr_offset().

	/*

	 * AMD Core perfctr has separate MSRs for the NB events, see

	 * the amd/uncore.c driver.

		/*

		 * Family 17h and compatibles have constraints for Large

		 * Increment per Cycle events: they may only be assigned an

		 * even numbered counter that has a consecutive adjacent odd

		 * numbered counter following it.

 Performance-monitoring supported from K7 and later: */

		/*

		 * No point in allocating data structures to serialize

		 * against other CPUs, when there is only the one CPU.

 Reload all events */

	/*

	 * We only mask out the Host-only bit so that host-only counting works

	 * when SVM is disabled. If someone sets up a guest-only counter when

	 * SVM is disabled the Guest-only bits still gets set and the counter

	 * will not count anything.

 Reload all events */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Zhaoxin PMU; like Intel Architectural PerfMon-v2

/*

 * Zhaoxin PerfMon, used on zxc and later.

 unhalted core clock cycles */

 retired instructions */

 unhalted core clock cycles */

 unhalted bus clock cycles */

	/*

	 * ZXC needs global control enabled in order to clear status bits.

	/*

	 * Enable IRQ generation (0x8),

	 * and enable ring-3 counting (0x2) and ring-0 counting (0x1)

	 * if requested:

/*

 * This handler is triggered by the local APIC, so the APIC IRQ handling

 * rules apply:

	/*

	 * CondChgd bit 63 doesn't mean any overflow status. Ignore

	 * and clear the bit.

	/*

	 * Repeat if there is more work to be done:

	/*

	 * For zxd/zxe, read/write operation for PMCx MSR is 48 bits.

 disable event that reported as not present by cpuid */

	/*

	 * Check whether the Architectural PerfMon supports

	 * hw_event or not.

 Clearing status works only if the global control is enable on zxc. */

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *

/*

 * Prepare the machine for transition to protected mode.

/*

 * Invoke the realmode switch hook if present; otherwise

 * disable all interrupts.

 Disable NMI */

/*

 * Disable all interrupts at the legacy PIC.

 Mask all interrupts on the secondary PIC */

 Mask all but cascade on the primary PIC */

/*

 * Reset IGNNE# if asserted in the FPU.

/*

 * Set up the GDT

	/* There are machines which are known to not boot with the GDT

 CS: code, read/execute, 4 GB, base 0 */

 DS: data, read/write, 4 GB, base 0 */

 TSS: 32-bit tss, 104 bytes, base 4096 */

		/* We only have a TSS here to keep Intel VT happy;

	/* Xen HVM incorrectly stores a pointer to the gdt_ptr, instead

	   of the gdt_ptr contents.  Thus, make it static so it will

	   stay in memory, at least long enough that we switch to the

/*

 * Set up the IDT

/*

 * Actual invocation sequence

 Hook before leaving real mode, also disables interrupts */

 Enable the A20 gate */

 Reset coprocessor (IGNNE#) */

 Mask all interrupts in the PIC */

 Actual transition to protected mode... */

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *   Copyright 2009 Intel Corporation; author H. Peter Anvin

 *

/*

 * Common all-VGA modes

 Set basic 80x25 mode */

 Query current mode */

 Set the mode */

 AH=0: set mode */

 Set 8x8 font - 80x43 on EGA, 80x50 on VGA */

 Set 8x8 font */

 ireg.bl = 0; */

 Use alternate print screen */

 Turn off cursor emulation */

 Cursor is scan lines 6-7 */

 Set 9x14 font - 80x28 on VGA */

 Set 9x14 font */

 ireg.bl = 0; */

 Turn off cursor emulation */

 Cursor is scan lines 11-12 */

 Set 80x43 mode on VGA (not EGA) */

 Set 350 scans */

 Reset video mode */

 I/O address of the VGA CRTC */

 CRTC base address */

 CRTC miscellaneous output register */

 Vertical sync end, unlock CR0-7 */

 Vertical total */

 Vertical overflow */

 Vertical sync start */

 Vertical display end */

 Vertical blank start */

 Vertical blank end */

 CRTC base address */

 CRTC overflow register */

 Vertical overflow */

 Vertical display end */

 Set the basic mode */

 Override a possibly broken BIOS */

/*

 * Note: this probe includes basic information required by all

 * systems.  It should be executed first, by making sure

 * video-vga.c is listed first in the Makefile.

 Check EGA/VGA */

 If we have MDA/CGA/HGC then BL will be unchanged at 0x10 */

 EGA/VGA */

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007-2008 rPath, Inc. - All Rights Reserved

 *

/*

 * arch/i386/boot/video-mode.c

 *

 * Set the video mode.  This is separated out into a different

 * file in order to be shared with the ACPI wakeup code.

/*

 * Common variables

 0=CGA/MDA/HGC, 1=EGA, 2=VGA+ */

 Don't query the BIOS for cols/rows */

 Screen contents changed during mode flip */

 Graphic mode with linear frame buffer */

 Probe the video drivers and have them generate their mode lists. */

 Test if a mode is defined */

 Set mode (without recalc) */

 Drop the recalc bit if set */

 Scan for mode based on fixed ID, position, or resolution */

 Nothing found?  Is it an "exceptional" (unprobed) mode? */

 Otherwise, failure... */

/*

 * Recalculate the vertical video cutoff (hack!)

 BIOS: font size (pixels) */

 Text rows */

 Visible scan lines */

 ... minus one */

 Unlock CR0-7 */

 Lower height register */

 Overflow register */

 Set mode (with recalc if specified) */

 Very special mode numbers... */

 Nothing to do... */

	/* Save the canonical mode number for the kernel, not

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *

/*

 * Oh, it's a waste of space, but oh-so-yummy for debugging.  This

 * version of printf() does not include 64-bit support.  "Live with

 * it."

 *

 pad with zero */

 unsigned/signed long */

 show plus */

 space if plus */

 left justified */

 Must be 32 == 0x20 */

 0x */

 we are called with base 8, 10 or 16, only, thus don't need "G..."  */

 "GHIJKLMNOPQRSTUVWXYZ"; */

	/* locase = 0 or 0x20. ORing digits or letters with 'locase'

 flags to number() */

 width of output field */

	int precision;		/* min. # of digits for integers; max

 'h', 'l', or 'L' for integer fields */

 process flags */

 this also skips first '%' */

 get field width */

 it's the next argument */

 get the precision */

 it's the next argument */

 get the conversion qualifier */

 default base */

 integer number formats - set up the flags and "break" */

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *   Copyright 2009 Intel Corporation; author H. Peter Anvin

 *

/*

 * VESA text modes

 VESA information */

 _WAKEUP */

 _WAKEUP */

 Not present */

 Heap full, can't save mode info */

 Just in case... */

			/* Text Mode, TTY BIOS supported,

 text */

			/* Graphics mode, color, linear frame buffer

			   supported.  Only register the mode if

			   if framebuffer is configured, however,

 Just in case... */

 It's a supported text mode */

 It's a graphics mode with linear frame buffer */

 Request linear frame buffer */

 Invalid mode */

 Text mode */

 Graphics mode */

 Switch DAC to 8-bit mode */

 If possible, switch the DAC to 8-bit mode */

 Set the color sizes to the DAC size, and offsets to 0 */

 Save the VESA protected mode info */

/*

 * Save video mode parameters for graphics mode

 Tell the kernel we're in VESA graphics mode */

 Mode parameters */

 General parameters */

/*

 * Save EDID information for the kernel; this is invoked, separately,

 * after mode-setting.

 Apparently used as a nonsense token... */

 EDID requires VBE 2.0+ */

 VBE DDC */

 ireg.bx = 0x0000; */		
 ireg.cx = 0;	*/		
 ES:DI must be 0 by spec */

 No EDID */

 BH = time in seconds to transfer EDD information */

 BL = DDC level supported */

 VBE DDC */

 Read EDID */

 ireg.cx = 0; */		
 ireg.dx = 0;	*/		
 (ES:)Pointer to block */

 CONFIG_FIRMWARE_EDID */

 not _WAKEUP */

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007-2008 rPath, Inc. - All Rights Reserved

 *

/*

 * arch/x86/boot/cpu.c

 *

 * Check for obligatory CPU features and abort if the features are not

 * present.

 Skip to the next string */

 SPDX-License-Identifier: GPL-2.0-or-later

/* -----------------------------------------------------------------------

 *

 *   Copyright 2009 Intel Corporation; author H. Peter Anvin

 *

/*

 * Simple helper function for initializing a register set.

 *

 * Note that this sets EFLAGS_CF in the input register set; this

 * makes it easier to catch functions which do nothing but don't

 * explicitly set CF.

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *

/*

 * Simple command-line parser for early boot.

 Close enough approximation */

/*

 * Find a non-boolean option, that is, "option=argument".  In accordance

 * with standard Linux practice, if this option is repeated, this returns

 * the last instance on the command line.

 *

 * Returns the length of the argument (regardless of if it was

 * truncated to fit in the buffer), or -1 on not found.

 Start of word/after whitespace */

 Comparing this word */

 Miscompare, skip */

 Copying this to buffer */

 No command line */

 else */

/*

 * Find a boolean option (like quiet,noapic,nosmp....)

 *

 * Returns the position of that option (starts counting with 1)

 * or 0 on not found

 Start of word/after whitespace */

 Comparing this word */

 Miscompare, skip */

 No command line */

 Buffer overrun */

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *

/*

 * Very basic string functions

/*

 * Undef these macros so that the functions that we provide

 * here will have the correct names regardless of how string.h

 * may have chosen to #define them.

/*

 * Clang may lower `memcmp == 0` to `bcmp == 0`.

 Works only for digits and letters, but small and fast */

/**

 * simple_strtoull - convert a string to an unsigned long long

 * @cp: The start of the string

 * @endp: A pointer to the end of the parsed string will be placed here

 * @base: The number base to use

/**

 * strlen - Find the length of a string

 * @s: The string to be sized

 nothing */;

/**

 * strstr - Find the first substring in a %NUL terminated string

 * @s1: The string to be searched

 * @s2: The string to search for

/**

 * strchr - Find the first occurrence of the character c in the string s.

 * @s: the string to be searched

 * @c: the character to search for

/*

 * Convert non-negative integer string representation in explicitly given radix

 * to an integer.

 * Return number of characters consumed maybe or-ed with overflow bit.

 * If overflow occurs, result integer (incorrect) is still returned.

 *

 * Don't you dare use this function.

 don't tolower() this line */

		/*

		 * Check for overflow only if we are within range of

		 * it in the max base we support (16)

/**

 * kstrtoull - convert a string to an unsigned long long

 * @s: The start of the string. The string must be null-terminated, and may also

 *  include a single newline before its terminating null. The first character

 *  may also be a plus sign, but not a minus sign.

 * @base: The number base to use. The maximum supported base is 16. If base is

 *  given as 0, then the base of the string is automatically detected with the

 *  conventional semantics - If it begins with 0x the number will be parsed as a

 *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be

 *  parsed as an octal number. Otherwise it will be parsed as a decimal.

 * @res: Where to write the result of the conversion on success.

 *

 * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.

 * Used as a replacement for the obsolete simple_strtoull. Return code must

 * be checked.

/**

 * kstrtoul - convert a string to an unsigned long

 * @s: The start of the string. The string must be null-terminated, and may also

 *  include a single newline before its terminating null. The first character

 *  may also be a plus sign, but not a minus sign.

 * @base: The number base to use. The maximum supported base is 16. If base is

 *  given as 0, then the base of the string is automatically detected with the

 *  conventional semantics - If it begins with 0x the number will be parsed as a

 *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be

 *  parsed as an octal number. Otherwise it will be parsed as a decimal.

 * @res: Where to write the result of the conversion on success.

 *

 * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.

 * Used as a replacement for the simple_strtoull.

	/*

	 * We want to shortcut function call, but

	 * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *   Copyright 2009 Intel Corporation; author H. Peter Anvin

 *

/*

 * Select video mode

	/* N.B.: the saving of the video page here is a bit silly,

 Not all BIOSes are clean with respect to the top bit */

/*

 * Store the video mode parameters for later usage by the kernel.

 * This is done by asking the BIOS except for the rows/columns

 * parameters in the default 80x25 mode -- these are set directly,

 * because some very obscure BIOSes supply insane values.

	/* For graphics mode, it is up to the mode-setting driver

 MDA, HGC, or VGA in monochrome mode */

 CGA, EGA, VGA and so forth */

 Font size, BIOS area */

 Default */

 Hidden mode */

 Out of keys... */

 Default */

 Beep! */

 Save screen content to the heap */

 Should be called after store_mode_params() */

 Not enough heap to save the screen */

 Should be called after store_mode_params() */

 Can't restore onto a graphic mode */

 No saved screen contents */

 Restore screen contents */

		/* Writes "npad" blank characters to

 Restore cursor position */

 Set cursor position */

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *

/*

 * Check for obligatory CPU features and abort if the features are not

 * present.  This code should be compilable as 16-, 32- or 64-bit

 * code, so be very careful with types and inline assembly.

 *

 * This code should not contain any messages; that requires an

 * additional wrapper.

 *

 * As written, this code is not safe for inclusion into the kernel

 * proper (after FPU initialization, in particular).

 REQUIRED_MASK2 not implemented in this file */

 REQUIRED_MASK3 not implemented in this file */

 REQUIRED_MASK5 not implemented in this file */

 REQUIRED_MASK7 not implemented in this file */

 REQUIRED_MASK8 not implemented in this file */

 REQUIRED_MASK9 not implemented in this file */

 REQUIRED_MASK10 not implemented in this file */

 REQUIRED_MASK11 not implemented in this file */

 REQUIRED_MASK12 not implemented in this file */

 REQUIRED_MASK13 not implemented in this file */

 REQUIRED_MASK14 not implemented in this file */

 REQUIRED_MASK15 not implemented in this file */

 Returns a bitmask of which words we have error bits in */

/*

 * Returns -1 on error.

 *

 * *cpu_level is set to the current CPU level; *req_level to the required

 * level.  x86-64 is considered level 64 for this purpose.

 *

 * *err_flags_ptr is set to the flags error array if there are flags missing.

		/* If this is an AMD and we're only missing SSE+SSE2, try to

 Make sure it really did something */

		/* If this is a VIA C3, we might have to enable CX8

 Transmeta might have masked feature bits in word 0 */

 PAE is disabled on this Pentium M but can be forced */

	/*

	 * First check for the affected model/family:

	/*

	 * This erratum affects the Accessed/Dirty bits, and can

	 * cause stray bits to be set in !Present PTEs.  We have

	 * enough bits in our 64-bit PTEs (which we have on real

	 * 64-bit mode or PAE) to avoid using these troublesome

	 * bits.  But, we do not have enough space in our 32-bit

	 * PTEs.  So, refuse to run on 32-bit non-PAE kernels.

 SPDX-License-Identifier: GPL-2.0

/*

 * Serial port routines for use during early boot reporting. This code is

 * included from both the compressed kernel and the regular kernel.

 ttyS0 */

  Transmit register (WRITE) */

  Receive register  (READ)  */

  Interrupt Enable          */

  Interrupt ID              */

  FIFO control              */

  Line control              */

  Modem control             */

  Line Status               */

  Modem Status              */

  Divisor Latch Low         */

  Divisor latch High        */

 8n1 */

 no interrupt */

 no fifo */

 DTR + RTS */

		/*

		 * make sure we have

		 *	"serial,0x3f8,115200"

		 *	"serial,ttyS0,115200"

		 *	"ttyS0,115200"

 += strlen("ttyS"); */

	/*

	 * console=uart8250,io,0x3f8,115200n8

	 * need to make sure it is last one console !

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007-2008 rPath, Inc. - All Rights Reserved

 *   Copyright 2009 Intel Corporation; author H. Peter Anvin

 *

/*

 * Enable A20 gate (return -1 on failure)

 FF is a plausible, but very unlikely status */

 Assume no KBC present */

 Read and discard input data */

 Buffers empty, finished! */

/* Returns nonzero if the A20 line is enabled.  The memory address

 2^21 */

 Serialize and make delay constant */

 Quick test to see if A20 is already enabled */

/* Longer test that actually waits for A20 to come on line; this

 Command write */

 A20 on */

 Null command, but UHCI wants it */

 Configuration port A */

 Enable A20 */

 Do not reset machine */

/*

 * Actual routine to enable A20; return 0 on ok, -1 on failure

 Number of times to try */

	       /* First, check to see if A20 is already enabled

 Next, try the BIOS (INT 0x15, AX=0x2401) */

 Try enabling A20 through the keyboard controller */

 BIOS worked, but with delayed reaction */

 Finally, try enabling the "fast A20 gate" */

 SPDX-License-Identifier: GPL-2.0

/*

 * For building the 16-bit code we want to explicitly specify 32-bit

 * push/pop operations, rather than just saying 'pushf' or 'popf' and

 * letting the compiler choose. But this is also included from the

 * compressed/ directory where it may be 64-bit code, and thus needs

 * to be 'pushfq' or 'popfq' in that case.

 Handle x86_32 PIC using ebx. */

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *   Copyright 2009 Intel Corporation; author H. Peter Anvin

 *

/*

 * Standard video BIOS modes

 *

 * We have two options for this; silent and scanned.

 Set a conventional BIOS mode */

 AH=0x00 Set Video Mode */

 Get Current Video Mode */

 Assume video contents were lost */

 Not all BIOSes are clean with the top bit */

 Mode change OK */

		/* Mode setting failed, but we didn't end up where we

		   started.  That's bad.  Try to revert to the original

 Try to verify that it's a text mode. */

 Attribute Controller: make graphics controller disabled */

 Graphics Controller: verify Alpha addressing enabled */

 CRTC cursor location low should be zero(?) */

 text */

 SPDX-License-Identifier: GPL-2.0-or-later

/* ----------------------------------------------------------------------- *

 *

 *   Copyright 2008 rPath, Inc. - All Rights Reserved

 *

/*

 * This is a host program to preprocess the CPU strings into a

 * compact format suitable for the setup code.

				/* The last entry must be unconditional; this

				   also consumes the compiler-added null

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *   Copyright 2009 Intel Corporation; author H. Peter Anvin

 *

/*

 * Get EDD BIOS disk information

/*

 * Read the MBR (first sector) from a specific device.

 Legacy Read, one sector */

 Sector 0-0-1 */

 0 or -1 */

 Best available guess */

 Produce a naturally aligned buffer on the heap */

 Make sure we actually have space on the heap... */

 check for valid MBR magic */

 Check Extensions Present */

 No extended information */

 EDD version number */

 EDD functionality subsets */

 Extended Get Device Parameters */

 Get legacy CHS parameters */

 Ralf Brown recommends setting ES:DI to 0:0 */

	/* Bugs in OnBoard or AddOnCards Bios may hang the EDD probe,

	 * so give a hint if this happens.

		/*

		 * Scan the BIOS-supported hard disks and query EDD

		 * information...

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *   Copyright 2009 Intel Corporation; author H. Peter Anvin

 *

 *   Original APM BIOS checking by Stephen Rothwell, May 1994

 *   (sfr@canb.auug.org.au)

 *

/*

 * Get APM BIOS information

 APM BIOS installation check */

 No APM BIOS */

 "PM" signature */

 32 bits supported? */

 Disconnect first, just in case */

 32-bit connect */

	/* Redo the installation check as the 32-bit connect;

 Failure with 32-bit connect, try to disconnect and ignore */

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *   Copyright 2009 Intel Corporation; author H. Peter Anvin

 *

/*

 * Very simple screen and serial I/O

  Transmit register (WRITE) */

  Line Status               */

/*

 * These functions are in .inittext so they can be used to signal

 * error during initialization.

 \n -> \r\n */

/*

 * Read the CMOS clock through the BIOS, and return the

 * seconds in BCD.

/*

 * Read from the keyboard

 ireg.ah = 0x00; */

 Timeout! */

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *

/*

 * Kernel version string

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *   Copyright 2009 Intel Corporation; author H. Peter Anvin

 *

/*

 * Memory detection code

 ASCII "SMAP" */

 static so it is zeroed */

	/*

	 * Note: at least one BIOS is known which assumes that the

	 * buffer pointed to by one e820 call is the same one as

	 * the previous call, and only changes modified fields.  Therefore,

	 * we use a temporary buffer and copy the results entry by entry.

	 *

	 * This routine deliberately does not try to account for

	 * ACPI 3+ extended attributes.  This is because there are

	 * BIOSes in the field which report zero for the valid bit for

	 * all ranges, and we don't currently make any use of the

	 * other attribute bits.  Revisit this if we see the extended

	 * attribute bits deployed in a meaningful way in the future.

 for next iteration... */

		/* BIOSes which terminate the chain with CF = 1 as opposed

		   to %ebx = 0 don't always report the SMAP signature on

		/* Some BIOSes stop returning SMAP in the middle of

		   the search loop.  We don't know exactly how the BIOS

		   screwed up the map at that point, we might have a

		   partial map, the full map, or complete garbage, so

 Do we really need to do this? */

 Bogus! */

		/*

		 * This ignores memory above 16MB if we have a memory

		 * hole there.  If someone actually finds a machine

		 * with a memory hole at 16MB and no support for

		 * 0E820h they should probably generate a fake e820

		 * map.

 SPDX-License-Identifier: GPL-2.0-only

/* -*- linux-c -*- ------------------------------------------------------- *

 *

 *   Copyright (C) 1991, 1992 Linus Torvalds

 *   Copyright 2007 rPath, Inc. - All Rights Reserved

 *   Copyright 2009 Intel Corporation; author H. Peter Anvin

 *

/*

 * Main module for the real-mode kernel code

 Default end of heap = no heap */

/*

 * Copy the header into the boot parameter block.  Since this

 * screws up the old-style command line protocol, adjust by

 * filling in the new-style command line pointer instead.

 Old-style command line protocol. */

		/* Figure out if the command line falls in the region

		   of memory that an old kernel would have copied up

/*

 * Query the keyboard lock status as given by the BIOS, and

 * set the keyboard repeat rate to maximum.  Unclear why the latter

 * is done here; this might be possible to kill off as stale code.

 Get keyboard status */

 Set keyboard repeat rate */

/*

 * Get Intel SpeedStep (IST) information.

	/* Some older BIOSes apparently crash on this call, so filter

 IST Support */

 Request value */

/*

 * Tell the BIOS what CPU mode we intend to run in.

 Boot protocol 2.00 only, no heap available */

 First, copy the boot header into the "zeropage" */

 Initialize the early-boot console */

 End of heap check */

 Make sure we have all the proper CPU support */

 Tell the BIOS what CPU mode we intend to run in. */

 Detect memory layout */

 Set keyboard repeat rate (why?) and query the lock flags */

 Query Intel SpeedStep (IST) information */

 Query APM information */

 Query EDD information */

 Set the video mode */

 Do the last things and invoke protected mode */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *  Copyright (C) 1997 Martin Mares

 *  Copyright (C) 2007 H. Peter Anvin

/*

 * This file builds a disk-image from three different files:

 *

 * - setup: 8086 machine code, sets up system parm

 * - system: 80386 code for actual system

 * - zoffset.h: header with ZO_* defines

 *

 * It does some checking that all files are of the correct type, and writes

 * the result to the specified destination, removing headers and padding to

 * the right amount. It also writes some system data to stdout.

/*

 * Changes by tytso to allow root device specification

 * High loaded stuff by Hans Lermen & Werner Almesberger, Feb. 1996

 * Cross compiling fixes by Gertjan van Wingerde, July 1996

 * Rewritten by Martin Mares, April 1997

 * Substantially overhauled by H. Peter Anvin, April 2007

 Minimal number of setup sectors */

 This must be large enough to hold the entire setup */

----------------------------------------------------------------------*/

 section header size field */

 section header vma field */

 section header 'size of initialised data' field */

 section header 'file offset' field */

	/*

	 * Modify .reloc section contents with a single entry. The

	 * relocation is applied to offset 10 of the relocation section.

	/*

	 * Put the IA-32 machine type (0x14c) and the associated entry point

	 * address in the .compat section, so loaders can figure out which other

	 * execution modes this image supports.

	/*

	 * The PE/COFF loader may load the image at an address which is

	 * misaligned with respect to the kernel_alignment field in the setup

	 * header.

	 *

	 * In order to avoid relocating the kernel to correct the misalignment,

	 * add slack to allow the buffer to be aligned within the declared size

	 * of the image.

	/*

	 * Size of code: Subtract the size of the first sector (512 bytes)

	 * which includes the header.

 Size of image */

	/*

	 * Address of entry point for PE/COFF executable

 Reserve 0x20 bytes for .reloc section */

 Defaults for old kernel */

 Yes, this is really how we defined it :( */

 CONFIG_EFI_STUB */

 Reserve 0x20 bytes for .compat section */

/*

 * Parse zoffset.h and find the entry points. We could just #include zoffset.h

 * but that would mean tools/build would have to be rebuilt every time. It's

 * not as if parsing it is hard...

 Copy the setup code */

 Pad unused space with zeros */

 Set the default root device */

 Open and stat the kernel file */

 Number of 16-byte paragraphs, including space for a 4-byte CRC */

	/*

	 * COFF requires minimum 32-byte alignment of sections, and

	 * adding a signature is problematic without that alignment.

 Patch the setup code with the appropriate size parameters */

	/*

	 * The decompression buffer will start at ImageBase. When relocating

	 * the compressed kernel to its end, we must ensure that the head

	 * section does not get overwritten.  The head section occupies

	 * [i, i + _ehead), and the destination is [init_sz - _end, init_sz).

	 *

	 * At present these should never overlap, because 'i' is at most 32k

	 * because of SETUP_SECT_MAX, '_ehead' is less than 1k, and the

	 * calculation of INIT_SIZE in boot/header.S ensures that

	 * 'init_sz - _end' is at least 64k.

	 *

	 * For future-proofing, increase init_sz if necessary.

 Update kernel_info offset. */

 Copy the kernel code */

 Add padding leaving 4 bytes for the checksum */

 Write the CRC */

 Catch any delayed write failures */

 Everything is OK */

 SPDX-License-Identifier: GPL-2.0

/*

 * AMD Encrypted Register State Support

 *

 * Author: Joerg Roedel <jroedel@suse.de>

/*

 * misc.h needs to be first because it knows how to include the other kernel

 * headers in the pre-decompression code in a way that does not break

 * compilation.

/*

 * Copy a version of this function here - insn-eval.c can't be used in

 * pre-decompression code.

/*

 * Only a dummy for insn_get_seg_base() - Early boot-code is 64bit only and

 * doesn't use segments.

 Basic instruction decoding support needed */

 Include code for early handlers */

 Page is now mapped decrypted, clear it */

 Initialize lookup tables for the instruction decoder */

	/*

	 * GHCB Page must be flushed from the cache and mapped encrypted again.

	 * Otherwise the running kernel will see strange cache effects when

	 * trying to use that page.

	/*

	 * GHCB page is mapped encrypted again and flushed from the cache.

	 * Mark it non-present now to catch bugs when #VC exceptions trigger

	 * after this point.

 Check whether the fault was on the GHCB page */

 SPDX-License-Identifier: GPL-2.0

/*

 * This code is used on x86_64 to create page table identity mappings on

 * demand by building up a new set of page tables (or appending to the

 * existing ones), and then switching over to them when ready.

 *

 * Copyright (C) 2015-2016  Yinghai Lu

 * Copyright (C)      2016  Kees Cook

/*

 * Since we're dealing with identity mappings, physical and virtual

 * addresses are the same, so override these defines which are ultimately

 * used by the headers in misc.h.

 No PAGE_TABLE_ISOLATION support needed either: */

 These actually do the work of building the kernel identity maps. */

 Use the static base for this part of the boot process */

 For COMMAND_LINE_SIZE */

 Used by PAGE_KERN* macros: */

 Used to track our page table allocation area. */

/*

 * Allocates space for a page table entry, using struct alloc_pgt_data

 * above. Besides the local callers, this is used as the allocation

 * callback in mapping_info below.

 Validate there is space available for a new page. */

 Used to track our allocated page tables. */

 The top level page table entry pointer. */

/*

 * Mapping information structure passed to kernel_ident_mapping_init().

 * Due to relocation, pointers must be assigned at run time not build time.

/*

 * Adds the specified range to the identity mappings.

 Align boundary to 2M. */

 Build the mapping. */

 Locates and clears a region for a new top level page table. */

 Exclude the encryption mask from __PHYSICAL_MASK */

 Init mapping_info with run-time function/buffer pointers. */

	/*

	 * It should be impossible for this not to already be true,

	 * but since calling this a second time would rewind the other

	 * counters, let's just make sure this is reset too.

	/*

	 * If we came here via startup_32(), cr3 will be _pgtable already

	 * and we must append to the existing area instead of entirely

	 * overwriting it.

	 *

	 * With 5-level paging, we use '_pgtable' to allocate the p4d page table,

	 * the top-level page table is allocated separately.

	 *

	 * p4d_offset(top_level_pgt, 0) would cover both the 4- and 5-level

	 * cases. On 4-level paging it's equal to 'top_level_pgt'.

	/*

	 * New page-table is set up - map the kernel image, boot_params and the

	 * command line. The uncompressed kernel requires boot_params and the

	 * command line to be mapped in the identity mapping. Map them

	 * explicitly here in case the compressed kernel does not touch them,

	 * or does not touch all the pages covering them.

 Load the new page-table. */

 No large page - clear PSE flag */

 Populate the PTEs */

	/*

	 * Ideally we need to clear the large PMD first and do a TLB

	 * flush before we write the new PMD. But the 2M range of the

	 * PMD might contain the code we execute and/or the stack

	 * we are on, so we can't do that. But that should be safe here

	 * because we are going from large to small mappings and we are

	 * also the only user of the page-table, so there is no chance

	 * of a TLB multihit.

 Flush TLB to establish the new PMD */

	/*

	 * Hardcode cl-size to 64 - CPUID can't be used here because that might

	 * cause another #VC exception and the GHCB is not ready to use yet.

	/*

	 * First make sure there are no pending writes on the cache-lines to

	 * flush.

	/*

	 * First make sure there is a PMD mapping for 'address'.

	 * It should already exist, but keep things generic.

	 *

	 * To map the page just read from it and fault it in if there is no

	 * mapping yet. add_identity_map() can't be called here because that

	 * would unconditionally map the address on PMD level, destroying any

	 * PTE-level mappings that might already exist. Use assembly here so

	 * the access won't be optimized away.

	/*

	 * The page is mapped at least with PMD size - so skip checks and walk

	 * directly to the PMD.

	/*

	 * Changing encryption attributes of a page requires to flush it from

	 * the caches.

 Update PTE */

 Flush TLB after changing encryption attribute */

	/*

	 * Check for unexpected error codes. Unexpected are:

	 *	- Faults on present pages

	 *	- User faults

	 *	- Reserved bits set

	/*

	 * Error code is sane - now identity map the 2M region around

	 * the faulting address.

 SPDX-License-Identifier: GPL-2.0

 128K, less than this is insane */

 640K, absolute maximum */

 __pgtable_l5_enabled needs to be in .data to avoid being cleared along with .bss */

 Buffer to preserve trampoline memory */

/*

 * Trampoline address will be printed by extract_kernel() for debugging

 * purposes.

 *

 * Avoid putting the pointer into .bss as it will be cleared between

 * paging_prepare() and extract_kernel().

	/*

	 * Find a suitable spot for the trampoline.

	 * This code is based on reserve_bios_regions().

	/*

	 * EFI systems may not provide legacy ROM. The memory may not be mapped

	 * at all.

	 *

	 * Only look for values in the legacy ROM for non-EFI system.

 Find the first usable memory region under bios_start. */

 Skip all entries above bios_start. */

 Skip non-RAM entries. */

 Adjust bios_start to the end of the entry if needed. */

 Keep bios_start page-aligned. */

 Skip the entry if it's too small. */

 Protect against underflow. */

 Place the trampoline just below the end of low memory */

 Initialize boot_params. Required for cmdline_find_option_bool(). */

	/*

	 * Check if LA57 is desired and supported.

	 *

	 * There are several parts to the check:

	 *   - if the kernel supports 5-level paging: CONFIG_X86_5LEVEL=y

	 *   - if user asked to disable 5-level paging: no5lvl in cmdline

	 *   - if the machine supports 5-level paging:

	 *     + CPUID leaf 7 is supported

	 *     + the leaf has the feature bit set

	 *

	 * That's substitute for boot_cpu_has() in early boot code.

 Preserve trampoline memory */

 Clear trampoline memory first */

 Copy trampoline code in place */

	/*

	 * The code below prepares page table in trampoline memory.

	 *

	 * The new page table will be used by trampoline code for switching

	 * from 4- to 5-level paging or vice versa.

	 *

	 * If switching is not required, the page table is unused: trampoline

	 * code wouldn't touch CR3.

	/*

	 * We are not going to use the page table in trampoline memory if we

	 * are already in the desired paging mode.

		/*

		 * For 4- to 5-level paging transition, set up current CR3 as

		 * the first and the only entry in a new top-level page table.

		/*

		 * For 5- to 4-level paging transition, copy page table pointed

		 * by first entry in the current top-level page table as our

		 * new top-level page table.

		 *

		 * We cannot just point to the page table from trampoline as it

		 * may be above 4G.

	/*

	 * Move the top level page table out of trampoline memory,

	 * if it's there.

 Restore trampoline memory */

 Initialize variables for 5-level paging */

 SPDX-License-Identifier: GPL-2.0-only

/* ----------------------------------------------------------------------- *

 *

 *  Copyright (C) 2009 Intel Corporation. All rights reserved.

 *

 *  H. Peter Anvin <hpa@linux.intel.com>

 *

 * -----------------------------------------------------------------------

 *

 * Outputs a small assembly wrapper with the appropriate symbols defined.

 Get the information for the compressed kernel image first */

 SPDX-License-Identifier: GPL-2.0

 shift it back */

 SPDX-License-Identifier: GPL-2.0

/*

 * misc.c

 *

 * This is a collection of several routines used to extract the kernel

 * which includes KASLR relocation, decompression, ELF parsing, and

 * relocation processing. Additionally included are the screen and serial

 * output functions and related debugging support functions.

 *

 * malloc by Hannu Savolainen 1993 and Matthias Urlichs 1994

 * puts by Nick Holloway 1993, better puts by Martin Mares 1995

 * High loaded stuff by Hans Lermen & Werner Almesberger, Feb. 1996

/*

 * WARNING!!

 * This code is compiled with -fPIC and it is relocated dynamically at

 * run time, but no relocation processing is performed. This means that

 * it is not safe to place pointers in static structures.

 Macros used by the included decompressor code below. */

 Define an externally visible malloc()/free(). */

/*

 * Provide definitions of memzero and memmove as some of the decompressors will

 * try to define their own functions if these are not defined as macros.

 Functions used by the included decompressor code below. */

/*

 * This is set up by the setup-routine at boot-time

/*

 * NOTE: When adding a new decompressor, please update the analysis in

 * ../header.S.

  Transmit register (WRITE) */

  Line Status               */

 Update cursor position */

	/*

	 * Calculate the delta between where vmlinux was linked to load

	 * and where it was actually loaded.

	/*

	 * The kernel contains a table of relocation addresses. Those

	 * addresses have the final load address of the kernel in virtual

	 * memory. We are currently working in the self map. So we need to

	 * create an adjustment for kernel memory addresses to the self map.

	 * This will involve subtracting out the base address of the kernel.

	/*

	 * 32-bit always performs relocations. 64-bit relocations are only

	 * needed if KASLR has chosen a different starting address offset

	 * from __START_KERNEL_map.

	/*

	 * Process relocations: 32 bit relocations first then 64 bit after.

	 * Three sets of binary relocations are added to the end of the kernel

	 * before compression. Each relocation table entry is the kernel

	 * address of the location which needs to be updated stored as a

	 * 32-bit value which is sign extended to 64 bits.

	 *

	 * Format is:

	 *

	 * kernel bits...

	 * 0 - zero terminator for 64 bit relocations

	 * 64 bit relocation repeated

	 * 0 - zero terminator for inverse 32 bit relocations

	 * 32 bit inverse relocation repeated

	 * 0 - zero terminator for 32 bit relocations

	 * 32 bit relocation repeated

	 *

	 * So we work backwards from the end of the decompressed image.

 Ignore other PT_* */ break;

/*

 * The compressed kernel image (ZO), has been moved so that its position

 * is against the end of the buffer used to hold the uncompressed kernel

 * image (VO) and the execution environment (.bss, .brk), which makes sure

 * there is room to do the in-place decompression. (See header.S for the

 * calculations.)

 *

 *                             |-----compressed kernel image------|

 *                             V                                  V

 * 0                       extract_offset                      +INIT_SIZE

 * |-----------|---------------|-------------------------|--------|

 *             |               |                         |        |

 *           VO__text      startup_32 of ZO          VO__end    ZO__end

 *             ^                                         ^

 *             |-------uncompressed kernel image---------|

 *

 Retain x86 boot parameters pointer passed from startup_32/64. */

 Clear flags intended for solely in-kernel use. */

	/*

	 * Save RSDP address for later use. Have this after console_init()

	 * so that early debugging output from the RSDP parsing code can be

	 * collected.

 Heap */

	/*

	 * The memory hole needed for the kernel is the larger of either

	 * the entire decompressed kernel plus relocation table, or the

	 * entire decompressed kernel plus .bss and .brk sections.

	 *

	 * On X86_64, the memory is mapped with PMD pages. Round the

	 * size up so that the full extent of PMD pages mapped is

	 * included in the check against the valid memory table

	 * entries. This ensures the full mapped area is usable RAM

	 * and doesn't include any reserved areas.

 Report initial kernel position details. */

 Report address of 32-bit trampoline */

 Validate memory location choices. */

 Disable exception handling before booting the kernel */

 SPDX-License-Identifier: GPL-2.0

/*

 * This provides an optimized implementation of memcpy, and a simplified

 * implementation of memset and memmove. These are used here because the

 * standard kernel runtime versions are not yet available and we don't

 * trust the gcc built-in implementations as they may do unexpected things

 * (e.g. FPU ops) in the minimal decompression stub execution environment.

 Detect and warn about potential overlaps, but handle them with memmove. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Callers outside of misc.c need access to the error reporting routines,

 * but the *_putstr() functions need to stay in misc.c because of how

 * memcpy() and memmove() are defined for the compressed boot environment.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * kaslr.c

 *

 * This contains the routines needed to generate a reasonable level of

 * entropy to choose a randomized kernel base address offset in support

 * of Kernel Address Space Layout Randomization (KASLR). Additionally

 * handles walking the physical memory maps (and tracking memory regions

 * to avoid) in order to select a physical memory location that can

 * contain the entire properly aligned running kernel image.

 *

/*

 * isspace() in linux/ctype.h is expected by next_args() to filter

 * out "space/lf/tab". While boot/ctype.h conflicts with linux/ctype.h,

 * since isdigit() is implemented in both of them. Hence disable it

 * here.

 For COMMAND_LINE_SIZE */

 Simplified build-specific string for starting entropy. */

 Rotate by odd number of bits and XOR. */

 Attempt to create a simple but unpredictable starting entropy. */

 Only supporting at most 4 unusable memmap regions with kaslr */

/*

 * Store memory limit: MAXMEM on 64-bit and KERNEL_IMAGE_SIZE on 32-bit.

 * It may be reduced by "mem=nn[KMG]" or "memmap=nn[KMG]" command line options.

 Number of immovable memory regions */

 Item one is entirely before item two. */

 Item one is entirely after item two. */

 We don't care about this option here */

			/*

			 * memmap=nn@ss specifies usable region, should

			 * be skipped

			/*

			 * efi_fake_mem=nn@ss:attr the attr specifies

			 * flags that might imply a soft-reservation.

		/*

		 * If w/o offset, only size specified, memmap=nn[KMG] has the

		 * same behaviour as mem=nn[KMG]. It limits the max address

		 * system can use. Region above the limit should be avoided.

 Store the specified memory limit if size > 0 */

 More than 4 memmaps, fail kaslr */

 Store the number of 1GB huge pages which users specified: */

 Chew leading spaces */

 Stop at -- */

/*

 * In theory, KASLR can put the kernel anywhere in the range of [16M, MAXMEM)

 * on 64-bit, and [16M, KERNEL_IMAGE_SIZE) on 32-bit.

 *

 * The mem_avoid array is used to store the ranges that need to be avoided

 * when KASLR searches for an appropriate random address. We must avoid any

 * regions that are unsafe to overlap with during decompression, and other

 * things like the initrd, cmdline and boot_params. This comment seeks to

 * explain mem_avoid as clearly as possible since incorrect mem_avoid

 * memory ranges lead to really hard to debug boot failures.

 *

 * The initrd, cmdline, and boot_params are trivial to identify for

 * avoiding. They are MEM_AVOID_INITRD, MEM_AVOID_CMDLINE, and

 * MEM_AVOID_BOOTPARAMS respectively below.

 *

 * What is not obvious how to avoid is the range of memory that is used

 * during decompression (MEM_AVOID_ZO_RANGE below). This range must cover

 * the compressed kernel (ZO) and its run space, which is used to extract

 * the uncompressed kernel (VO) and relocs.

 *

 * ZO's full run size sits against the end of the decompression buffer, so

 * we can calculate where text, data, bss, etc of ZO are positioned more

 * easily.

 *

 * For additional background, the decompression calculations can be found

 * in header.S, and the memory diagram is based on the one found in misc.c.

 *

 * The following conditions are already enforced by the image layouts and

 * associated code:

 *  - input + input_size >= output + output_size

 *  - kernel_total_size <= init_size

 *  - kernel_total_size <= output_size (see Note below)

 *  - output + init_size >= output + output_size

 *

 * (Note that kernel_total_size and output_size have no fundamental

 * relationship, but output_size is passed to choose_random_location

 * as a maximum of the two. The diagram is showing a case where

 * kernel_total_size is larger than output_size, but this case is

 * handled by bumping output_size.)

 *

 * The above conditions can be illustrated by a diagram:

 *

 * 0   output            input            input+input_size    output+init_size

 * |     |                 |                             |             |

 * |     |                 |                             |             |

 * |-----|--------|--------|--------------|-----------|--|-------------|

 *                |                       |           |

 *                |                       |           |

 * output+init_size-ZO_INIT_SIZE  output+output_size  output+kernel_total_size

 *

 * [output, output+init_size) is the entire memory range used for

 * extracting the compressed image.

 *

 * [output, output+kernel_total_size) is the range needed for the

 * uncompressed kernel (VO) and its run size (bss, brk, etc).

 *

 * [output, output+output_size) is VO plus relocs (i.e. the entire

 * uncompressed payload contained by ZO). This is the area of the buffer

 * written to during decompression.

 *

 * [output+init_size-ZO_INIT_SIZE, output+init_size) is the worst-case

 * range of the copied ZO and decompression code. (i.e. the range

 * covered backwards of size ZO_INIT_SIZE, starting from output+init_size.)

 *

 * [input, input+input_size) is the original copied compressed image (ZO)

 * (i.e. it does not include its run size). This range must be avoided

 * because it contains the data used for decompression.

 *

 * [input+input_size, output+init_size) is [_text, _end) for ZO. This

 * range includes ZO's heap and stack, and must be avoided since it

 * performs the decompression.

 *

 * Since the above two ranges need to be avoided and they are adjacent,

 * they can be merged, resulting in: [input, output+init_size) which

 * becomes the MEM_AVOID_ZO_RANGE below.

	/*

	 * Avoid the region that is unsafe to overlap during

	 * decompression.

 Avoid initrd. */

 No need to set mapping for initrd, it will be handled in VO. */

 Avoid kernel command line. */

 Calculate size of cmd_line. */

 Avoid boot parameters. */

 We don't need to set a mapping for setup_data. */

 Mark the memmap regions we need to avoid */

 Enumerate the immovable memory regions */

/*

 * Does this memory vector overlap a known avoided area? If so, record the

 * overlap region with the lowest address.

 Avoid all entries in the setup_data linked list. */

/*

 * Skip as many 1GB huge pages as possible in the passed region

 * according to the number which users specified:

 Are there any 1GB pages in the region? */

 No good 1GB huge pages found: */

 Check if the head part of the region is usable. */

 Skip the good 1GB pages. */

 Check if the tail part of the region is usable. */

 Handle case of no slots stored. */

 Enforce minimum and memory limit. */

 Give up if slot area array is full. */

 Potentially raise address to meet alignment needs. */

 Did we raise the address above the passed in memory entry? */

 Reduce size by any delta from the original address. */

 Return if region can't contain decompressed kernel */

 If nothing overlaps, store the region and return. */

 Store beginning of region if holds at least image_size. */

 Clip off the overlapping region and start over. */

	/*

	 * If no immovable memory found, or MEMORY_HOTREMOVE disabled,

	 * use @region directly.

	/*

	 * If immovable memory found, filter the intersection between

	 * immovable memory and @region.

/*

 * Returns true if we processed the EFI memmap, which we prefer over the E820

 * table if it is available.

 Can't handle data above 4GB at this time */

		/*

		 * Here we are more conservative in picking free memory than

		 * the EFI spec allows:

		 *

		 * According to the spec, EFI_BOOT_SERVICES_{CODE|DATA} are also

		 * free memory and thus available to place the kernel image into,

		 * but in practice there's firmware where using that memory leads

		 * to crashes.

		 *

		 * Only EFI_CONVENTIONAL_MEMORY is guaranteed to be free.

 Verify potential e820 positions, appending to slots list. */

 Skip non-RAM entries. */

 Bail out early if it's impossible to succeed. */

 Check if we had too many memmaps. */

 Perform a final check to make sure the address is in range. */

	/*

	 * There are how many CONFIG_PHYSICAL_ALIGN-sized slots

	 * that can hold image_size within the range of minimum to

	 * KERNEL_IMAGE_SIZE?

/*

 * Since this function examines addresses much more numerically,

 * it takes the input and output pointers as 'unsigned long'.

 Record the various known unsafe memory ranges. */

	/*

	 * Low end of the randomization range should be the

	 * smaller of 512M or the initial kernel image

	 * location:

 Make sure minimum is aligned. */

 Walk available memory entries to find a random address. */

 Update the new physical address location. */

 Pick random virtual address starting from LOAD_PHYSICAL_ADDR. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Longest parameter of 'acpi=' is 'copy_dsdt', plus an extra '\0'

 * for termination.

/*

 * Immovable memory regions representation. Max amount of memory regions is

 * MAX_NUMNODES*2.

/*

 * Search EFI system tables for RSDP.  If both ACPI_20_TABLE_GUID and

 * ACPI_TABLE_GUID are found, take the former, which has more features.

 Get EFI tables from systab. */

 EFI/kexec support is 64-bit only. */

 Get systab from boot params. */

 CONFIG_X86_64 */

 Get systab from boot params. */

 Handle EFI bitness properly */

 Search a block of memory for the RSDP signature. */

 Search from given start address for the requested length */

		/*

		 * Both RSDP signature and checksum must be correct.

		 * Note: Sometimes there exists more than one RSDP in memory;

		 * the valid RSDP has a valid checksum, all others have an

		 * invalid checksum.

 BAD Signature */

 Check the standard checksum */

 Check extended checksum if table version >= 2 */

 Signature and checksum valid, we have found a real RSDP */

 Search RSDP address in EBDA. */

 Get the location of the Extended BIOS Data Area (EBDA) */

	/*

	 * Search EBDA paragraphs (EBDA is required to be a minimum of

	 * 1K length)

 Search upper memory: 16-byte boundaries in E0000h-FFFFFh */

 Return RSDP address on success, otherwise 0. */

	/*

	 * Try to get EFI data from setup_data. This can happen when we're a

	 * kexec'ed kernel and kexec(1) has passed all the required EFI info to

	 * us.

/*

 * Max length of 64-bit hex address string is 19, prefix "0x" + 16 hex

 * digits, and '\0' for termination.

 Compute SRAT address from RSDP. */

	/*

	 * Check whether we were given an RSDP on the command line. We don't

	 * stash this in boot params because the kernel itself may have

	 * different ideas about whether to trust a command-line parameter.

 Get ACPI root table from RSDP.*/

/**

 * count_immovable_mem_regions - Parse SRAT and cache the immovable

 * memory regions into the immovable_mem array.

 *

 * Return the number of immovable memory regions on success, 0 on failure:

 *

 * - Too many immovable memory regions

 * - ACPI off or no SRAT found

 * - No immovable memory region found.

 CONFIG_RANDOMIZE_BASE && CONFIG_MEMORY_HOTREMOVE */

 SPDX-License-Identifier: GPL-2.0-only

 Have this here so we don't need to include <asm/desc.h> */

 Setup IDT before kernel jumping to  .Lrelocated */

 Setup IDT after kernel jumping to  .Lrelocated */

	/*

	 * Flush GHCB from cache and map it encrypted again when running as

	 * SEV-ES guest.

 Set a null-idt, disabling #PF and #VC handling */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Hibernation support for x86-64

 *

 * Copyright (c) 2007 Rafael J. Wysocki <rjw@sisk.pl>

 * Copyright (c) 2002 Pavel Machek <pavel@ucw.cz>

 * Copyright (c) 2001 Patrick Mochel <mochel@osdl.org>

 Filter out unsupported __PAGE_KERNEL* bits: */

	/*

	 * The new mapping only has to cover the page containing the image

	 * kernel's entry point (jump_address_phys), because the switch over to

	 * it is carried out by relocated code running from a page allocated

	 * specifically for this purpose and covered by the identity mapping, so

	 * the temporary kernel text mapping is only needed for the final jump.

	 * Moreover, in that mapping the virtual address of the image kernel's

	 * entry point must be the same as its virtual address in the image

	 * kernel (restore_jump_address), so the image kernel's

	 * restore_registers() code doesn't find itself in a different area of

	 * the virtual address space after switching over to the original page

	 * tables used by the image kernel.

 No p4d for 4-level paging: point the pgd to the pud page table */

 Prepare a temporary mapping for the kernel text */

 Set up the direct mapping from scratch */

 We have got enough memory and from now on we cannot recover */

 SPDX-License-Identifier: GPL-2.0

/*

 * Hibernation support for x86

 *

 * Copyright (c) 2007 Rafael J. Wysocki <rjw@sisk.pl>

 * Copyright (c) 2002 Pavel Machek <pavel@ucw.cz>

 * Copyright (c) 2001 Patrick Mochel <mochel@osdl.org>

/*

 * Address to jump to in the last phase of restore in order to get to the image

 * kernel's text (this value is passed in the image header).

/*

 * Value of the cr3 register from before the hibernation (this value is passed

 * in the image header).

/**

 *	pfn_is_nosave - check if given pfn is in the 'nosave' section

/**

 * compute_e820_crc32 - calculate crc32 of a given e820 table

 *

 * @table: the e820 table to be calculated

 *

 * Return: the resulting checksum

/**

 *	arch_hibernation_header_save - populate the architecture specific part

 *		of a hibernation image header

 *	@addr: address to save the data at

	/*

	 * The restore code fixes up CR3 and CR4 in the following sequence:

	 *

	 * [in hibernation asm]

	 * 1. CR3 <= temporary page tables

	 * 2. CR4 <= mmu_cr4_features (from the kernel that restores us)

	 * 3. CR3 <= rdr->cr3

	 * 4. CR4 <= mmu_cr4_features (from us, i.e. the image kernel)

	 * [in restore_processor_state()]

	 * 5. CR4 <= saved CR4

	 * 6. CR3 <= saved CR3

	 *

	 * Our mmu_cr4_features has CR4.PCIDE=0, and toggling

	 * CR4.PCIDE while CR3's PCID bits are nonzero is illegal, so

	 * rdr->cr3 needs to point to valid page tables but must not

	 * have any of the PCID bits set.

/**

 *	arch_hibernation_header_restore - read the architecture specific data

 *		from the hibernation image header

 *	@addr: address to read the data from

 Make the page containing the relocated code executable */

	/*

	 * We reached this while coming out of hibernation. This means

	 * that SMT siblings are sleeping in hlt, as mwait is not safe

	 * against control transition during resume (see comment in

	 * hibernate_resume_nonboot_cpu_disable()).

	 *

	 * If the resumed kernel has SMT disabled, we have to take all the

	 * SMT siblings out of hlt, and offline them again so that they

	 * end up in mwait proper.

	 *

	 * Called with hotplug disabled.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Suspend support specific for i386/x86-64.

 *

 * Copyright (c) 2007 Rafael J. Wysocki <rjw@sisk.pl>

 * Copyright (c) 2002 Pavel Machek <pavel@ucw.cz>

 * Copyright (c) 2001 Patrick Mochel <mochel@osdl.org>

/**

 * __save_processor_state() - Save CPU registers before creating a

 *                             hibernation image and before restoring

 *                             the memory state from it

 * @ctxt: Structure to store the registers contents in.

 *

 * NOTE: If there is a CPU register the modification of which by the

 * boot kernel (ie. the kernel used for loading the hibernation image)

 * might affect the operations of the restored target kernel (ie. the one

 * saved in the hibernation image), then its contents must be saved by this

 * function.  In other words, if kernel A is hibernated and different

 * kernel B is used for loading the hibernation image into memory, the

 * kernel A's __save_processor_state() function must save all registers

 * needed by kernel A, so that it can operate correctly after the resume

 * regardless of what kernel B does in the meantime.

	/*

	 * descriptor tables

	/*

	 * We save it here, but restore it only in the hibernate case.

	 * For ACPI S3 resume, this is loaded via 'early_gdt_desc' in 64-bit

	 * mode in "secondary_startup_64". In 32-bit mode it is done via

	 * 'pmode_gdt' in wakeup_start.

 XMM0..XMM15 should be handled by kernel_fpu_begin(). */

	/*

	 * segment registers

	/*

	 * control registers

 Needed by apm.c */

	/*

	 * Restore FPU regs if necessary.

	/*

	 * We need to reload TR, which requires that we change the

	 * GDT entry to indicate "available" first.

	 *

	 * XXX: This could probably all be replaced by a call to

	 * force_reload_TR().

 The available 64-bit TSS (see AMD vol 2, pg 91 */

 This sets MSR_*STAR and related */

 This does ltr */

 This does lldt */

 The processor is back on the direct GDT, load back the fixmap */

/**

 * __restore_processor_state() - Restore the contents of CPU registers saved

 *                               by __save_processor_state()

 * @ctxt: Structure to load the registers contents from.

 *

 * The asm code that gets us here will have restored a usable GDT, although

 * it will be pointing to the wrong alias.

	/*

	 * control registers

 cr4 was introduced in the Pentium CPU */

 CONFIG X86_64 */

 Restore the IDT. */

	/*

	 * Just in case the asm code got us here with the SS, DS, or ES

	 * out of sync with the GDT, update them.

	/*

	 * Restore percpu access.  Percpu access can happen in exception

	 * handlers or in complicated helpers like load_gs_index().

 Restore the TSS, RO GDT, LDT, and usermode-relevant MSRs. */

	/*

	 * Now that we have descriptor tables fully restored and working

	 * exception handling, restore the usermode segments.

	/*

	 * Restore FSBASE and GSBASE after restoring the selectors, since

	 * restoring the selectors clobbers the bases.  Keep in mind

	 * that MSR_KERNEL_GS_BASE is horribly misnamed.

 Needed by apm.c */

	/*

	 * Ensure that MONITOR/MWAIT will not be used in the "play dead" loop

	 * during hibernate image restoration, because it is likely that the

	 * monitored address will be actually written to at that time and then

	 * the "dead" CPU will attempt to execute instructions again, but the

	 * address in its instruction pointer may not be possible to resolve

	 * any more at that point (the page tables used by it previously may

	 * have been overwritten by hibernate image data).

	 *

	 * First, make sure that we wake up all the potentially disabled SMT

	 * threads which have been initially brought up and then put into

	 * mwait/cpuidle sleep.

	 * Those will be put to proper (not interfering with hibernation

	 * resume) sleep afterwards, and the resumed kernel will decide itself

	 * what to do with them.

/*

 * When bsp_check() is called in hibernate and suspend, cpu hotplug

 * is disabled already. So it's unnecessary to handle race condition between

 * cpumask query and cpu hotplug.

		/*

		 * When system resumes from hibernation, online CPU0 because

		 * 1. it's required for resume and

		 * 2. the CPU was online before hibernation

		/*

		 * When a resume really happens, this code won't be called.

		 *

		 * This code is called only when user space hibernation software

		 * prepares for snapshot device during boot time. So we just

		 * call _debug_hotplug_cpu() to restore to CPU0's state prior to

		 * preparing the snapshot device.

		 *

		 * This works for normal boot case in our CPU0 hotplug debug

		 * mode, i.e. CPU0 is offline and user mode hibernation

		 * software initializes during boot time.

		 *

		 * If CPU0 is online and user application accesses snapshot

		 * device after boot time, this will offline CPU0 and user may

		 * see different CPU0 state before and after accessing

		 * the snapshot device. But hopefully this is not a case when

		 * user debugging CPU0 hotplug. Even if users hit this case,

		 * they can easily online CPU0 back.

		 *

		 * To simplify this debug code, we only consider normal boot

		 * case. Otherwise we need to remember CPU0's state and restore

		 * to that state and resolve racy conditions etc.

	/*

	 * Set this bsp_pm_callback as lower priority than

	 * cpu_hotplug_pm_callback. So cpu_hotplug_pm_callback will be called

	 * earlier to disable cpu hotplug before bsp online check.

		/*

		 * Multiple callbacks can invoke this function, so copy any

		 * MSR save requests from previous invocations.

/*

 * The following sections are a quirk framework for problematic BIOSen:

 * Sometimes MSRs are modified by the BIOSen after suspended to

 * RAM, this might cause unexpected behavior after wakeup.

 * Thus we save/restore these specified MSRs across suspend/resume

 * in order to work around it.

 *

 * For any further problematic BIOSen/platforms,

 * please add your own function similar to msr_initialize_bdw.

 Add any extra MSR ids into this array. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Hibernation support specific for i386 - temporary page tables

 *

 * Copyright (c) 2006 Rafael J. Wysocki <rjw@sisk.pl>

 Pointer to the temporary resume page tables */

/* The following three functions are based on the analogous code in

 * arch/x86/mm/init_32.c

/*

 * Create a middle page table on a resume-safe page and put a pointer to it in

 * the given global directory entry.  This only returns the gd entry

 * in non-PAE compilation mode, since the middle layer is folded.

/*

 * Create a page table on a resume-safe page and place a pointer to it in

 * a middle page directory entry.

/*

 * This maps the physical memory to kernel virtual address space, a total

 * of max_low_pfn pages, by creating page tables starting from address

 * PAGE_OFFSET.  The page tables are allocated out of resume-safe pages.

			/* Map with big pages if possible, otherwise create

			 * normal page tables.

			 * NOTE: We can mark everything as executable here

 Init entries of the first-level page table to the zero page */

 We have got enough memory and from now on we cannot recover */

 SPDX-License-Identifier: GPL-2.0

 Hold the pgd entry used on booting additional CPUs */

 Has to be under 1M so we can execute real-mode AP code. */

	/*

	 * Unconditionally reserve the entire fisrt 1M, see comment in

	 * setup_arch().

		/*

		 * Skip the call to verify_cpu() in secondary_startup_64 as it

		 * will cause #VC exceptions when the AP can't handle them yet.

	/*

	 * If SME is active, the trampoline area will need to be in

	 * decrypted memory in order to bring up other processors

	 * successfully. This is not needed for SEV.

 16-bit segment relocations. */

 32-bit linear relocations. */

 Must be performed *after* relocation. */

	/*

	 * Some AMD processors will #GP(0) if EFER.LMA is set in WRMSR

	 * so we need to mask it out.

/*

 * reserve_real_mode() gets called very early, to guarantee the

 * availability of low memory. This is before the proper kernel page

 * tables are set up, so we cannot set page permissions in that

 * function. Also trampoline code will be executed by APs so we

 * need to mark it executable at do_pre_smp_initcalls() at least,

 * thus run it as a early_initcall().

 SPDX-License-Identifier: GPL-2.0

 Approximately 1 us */

 Turn off speaker */

 Ctr 2, squarewave, load, binary */

 LSB of counter */

 MSB of counter */

 Turn on speaker */

 Dummy read of System Control Port B */

 Enable timer 2 output to speaker */

 Okay, this is totally silly, but it's kind of fun. */

 Assume it's a space */

 Kill machine if structures are wrong */

 Need to call BIOS */

/*

 * Cryptographic API.

 *

 * Glue code for the SHA256 Secure Hash Algorithm assembler

 * implementation using supplemental SSE3 / AVX / AVX2 instructions.

 *

 * This file is based on sha256_generic.c

 *

 * Copyright (C) 2013 Intel Corporation.

 *

 * Author:

 *     Tim Chen <tim.c.chen@linux.intel.com>

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of the GNU General Public License as published by the Free

 * Software Foundation; either version 2 of the License, or (at your option)

 * any later version.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

	/*

	 * Make sure struct sha256_state begins directly with the SHA256

	 * 256-bit internal state, as this is what the asm functions expect.

 Add padding and return the message digest. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Glue Code for assembler optimized version of 3DES

 *

 * Copyright © 2014 Jussi Kivilinna <jussi.kivilinna@mbnet.fi>

 *

 * CBC & ECB parts based on code (crypto/cbc.c,ecb.c) by:

 *   Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>

 regular block cipher functions */

 3-way parallel cipher functions */

 Process four block batch */

 Handle leftovers */

 Start of the last block. */

 Process four block batch */

 Handle leftovers */

	/* Fix encryption context for this implementation and form decryption

		/*

		 * On Pentium 4, des3_ede-x86_64 is slower than generic C

		 * implementation because use of 64bit rotates (which are really

		 * slow on P4). Therefore blacklist P4s.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Glue Code for the AVX assembler implementation of the Cast5 Cipher

 *

 * Copyright (C) 2012 Johannes Goetzfried

 *     <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>

 SPDX-License-Identifier: GPL-2.0-or-later */

/*

 * SM4 Cipher Algorithm, AES-NI/AVX2 optimized.

 * as specified in

 * https://tools.ietf.org/id/draft-ribose-cfrg-sm4-10.html

 *

 * Copyright (c) 2021, Alibaba Group.

 * Copyright (c) 2021 Tianjia Zhang <tianjia.zhang@linux.alibaba.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Glue Code for assembler optimized version of Blowfish

 *

 * Copyright (c) 2011 Jussi Kivilinna <jussi.kivilinna@mbnet.fi>

 *

 * CBC & ECB parts based on code (crypto/cbc.c,ecb.c) by:

 *   Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>

 regular block cipher functions */

 4-way parallel cipher functions */

 Process four block batch */

 Handle leftovers */

 Start of the last block. */

 Process four block batch */

 Handle leftovers */

		/*

		 * On Pentium 4, blowfish-x86_64 is slower than generic C

		 * implementation because use of 64bit rotates (which are really

		 * slow on P4). Therefore blacklist P4s.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Cryptographic API.

 *

 * Glue code for the SHA1 Secure Hash Algorithm assembler implementation using

 * Supplemental SSE3 instructions.

 *

 * This file is based on sha1_generic.c

 *

 * Copyright (c) Alan Smithee.

 * Copyright (c) Andrew McDonald <andrew@mcdonald.org.uk>

 * Copyright (c) Jean-Francois Dive <jef@linuxbe.org>

 * Copyright (c) Mathias Krause <minipli@googlemail.com>

 * Copyright (c) Chandramouli Narayanan <mouli@linux.intel.com>

	/*

	 * Make sure struct sha1_state begins directly with the SHA1

	 * 160-bit internal state, as this is what the asm functions expect.

 Add padding and return the message digest. */

 optimal 4*64 bytes of SHA1 blocks */

 Select the optimal transform based on data block size */

/*

 * Cryptographic API.

 *

 * T10 Data Integrity Field CRC16 Crypto Transform using PCLMULQDQ Instructions

 *

 * Copyright (C) 2013 Intel Corporation

 * Author: Tim Chen <tim.c.chen@linux.intel.com>

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of the GNU General Public License as published by the Free

 * Software Foundation; either version 2 of the License, or (at your option)

 * any later version.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

/*

 * Cryptographic API.

 *

 * Glue code for the SHA512 Secure Hash Algorithm assembler

 * implementation using supplemental SSE3 / AVX / AVX2 instructions.

 *

 * This file is based on sha512_generic.c

 *

 * Copyright (C) 2013 Intel Corporation

 * Author: Tim Chen <tim.c.chen@linux.intel.com>

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of the GNU General Public License as published by the Free

 * Software Foundation; either version 2 of the License, or (at your option)

 * any later version.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

 *

	/*

	 * Make sure struct sha512_state begins directly with the SHA512

	 * 512-bit internal state, as this is what the asm functions expect.

 Add padding and return the message digest. */

 Add padding and return the message digest. */

 Add padding and return the message digest. */

 SPDX-License-Identifier: GPL-2.0 OR MIT

/*

 * Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.

 SIMD disables preemption, so relax after processing each page. */

 SPDX-License-Identifier: GPL-2.0-or-later */

/*

 * SM4 Cipher Algorithm, AES-NI/AVX optimized.

 * as specified in

 * https://tools.ietf.org/id/draft-ribose-cfrg-sm4-10.html

 *

 * Copyright (c) 2021, Alibaba Group.

 * Copyright (c) 2021 Tianjia Zhang <tianjia.zhang@linux.alibaba.com>

 tail */

 tail */

 tail */

 SPDX-License-Identifier: GPL-2.0 OR MIT

/*

 * Copyright (C) 2015-2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.

/* The AVX code uses base 2^26, while the scalar code uses base 2^64. If we hit

 * the unfortunate situation of using AVX and then having to go back to scalar

 * -- because the user is silly and has called the update function from two

 * separate contexts -- then we need to convert back to the original base before

 * proceeding. It is possible to reason that the initial reduction below is

 * sufficient given the implementation invariants. However, for an avoidance of

 * doubt and because this is not performance critical, we do the full reduction

 * anyway. Z3 proof of below function: https://xn--4db.cc/ltPtHCKN/py

 SIMD disables preemption, so relax after processing each page. */

 Skylake downclocks unacceptably much when using zmm, but later generations are fast. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Glue Code for AVX assembler version of Twofish Cipher

 *

 * Copyright (C) 2012 Johannes Goetzfried

 *     <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>

 *

 * Copyright © 2013 Jussi Kivilinna <jussi.kivilinna@iki.fi>

 8-way parallel cipher functions */

/*

 * Glue Code for assembler optimized version of TWOFISH

 *

 * Originally Twofish for GPG

 * By Matthew Skala <mskala@ansuz.sooke.bc.ca>, July 26, 1998

 * 256-bit key length added March 20, 1999

 * Some modifications to reduce the text size by Werner Koch, April, 1998

 * Ported to the kerneli patch by Marc Mutz <Marc@Mutz.com>

 * Ported to CryptoAPI by Colin Slater <hoho@tacomeat.net>

 *

 * The original author has disclaimed all copyright interest in this

 * code and thus put it in the public domain. The subsequent authors

 * have put this under the GNU General Public License.

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License as published by

 * the Free Software Foundation; either version 2 of the License, or

 * (at your option) any later version.

 *

 * This program is distributed in the hope that it will be useful,

 * but WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this program; if not, write to the Free Software

 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307

 * USA

 *

 * This code is a "clean room" implementation, written from the paper

 * _Twofish: A 128-Bit Block Cipher_ by Bruce Schneier, John Kelsey,

 * Doug Whiting, David Wagner, Chris Hall, and Niels Ferguson, available

 * through http://www.counterpane.com/twofish.html

 *

 * For background information on multiplication in finite fields, used for

 * the matrix operations in the key schedule, see the book _Contemporary

 * Abstract Algebra_ by Joseph A. Gallian, especially chapter 22 in the

 * Third Edition.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Glue Code for assembler optimized version of Camellia

 *

 * Copyright (c) 2012 Jussi Kivilinna <jussi.kivilinna@mbnet.fi>

 *

 * Camellia parts based on code by:

 *  Copyright (C) 2006 NTT (Nippon Telegraph and Telephone Corporation)

 regular block cipher functions */

 2-way parallel cipher functions */

 camellia sboxes */

 key constants */

 macros */

 absorb kw2 to other subkeys */

 round 2 */

 round 4 */

 round 6 */

 modified for FLinv(kl2) */

 round 8 */

 round 10 */

 round 12 */

 modified for FLinv(kl4) */

 round 14 */

 round 16 */

 round 18 */

 kw3 */

 absorb kw4 to other subkeys */

 modified for FLinv(kl6) */

 round 20 */

 round 22 */

 round 24 */

 kw3 */

 absorb kw4 to other subkeys */

 round 23 */

 round 21 */

 round 19 */

 modified for FL(kl5) */

 round 17 */

 round 15 */

 round 13 */

 modified for FL(kl3) */

 round 11 */

 round 9 */

 round 7 */

 modified for FL(kl1) */

 round 5 */

 round 3 */

 round 1 */

 kw1 */

 key XOR is end of F-function */

 kw1 */

 round 1 */

 round 2 */

 round 3 */

 round 4 */

 round 5 */

 FL(kl1) */

 round 6 */

 FL(kl1) */

 FLinv(kl2) */

 FLinv(kl2) */

 round 7 */

 round 8 */

 round 9 */

 round 10 */

 round 11 */

 FL(kl3) */

 round 12 */

 FL(kl3) */

 FLinv(kl4) */

 FLinv(kl4) */

 round 13 */

 round 14 */

 round 15 */

 round 16 */

 round 17 */

 round 18 */

 kw3 */

 FL(kl5) */

 round 18 */

 FL(kl5) */

 FLinv(kl6) */

 FLinv(kl6) */

 round 19 */

 round 20 */

 round 21 */

 round 22 */

 round 23 */

 round 24 */

 kw3 */

	/**

	 *  k == kl || kr (|| is concatenation)

 generate KL dependent subkeys */

 kw1 */

 kw2 */

 rotation left shift 15bit */

 k3 */

 k4 */

 rotation left shift 15+30bit */

 k7 */

 k8 */

 rotation left shift 15+30+15bit */

 k10 */

 rotation left shift 15+30+15+17 bit */

 kl3 */

 kl4 */

 rotation left shift 15+30+15+17+17 bit */

 k13 */

 k14 */

 rotation left shift 15+30+15+17+17+17 bit */

 k17 */

 k18 */

 generate KA */

 current status == (kll, klr, w0, w1) */

 generate KA dependent subkeys */

 k1, k2 */

 k5,k6 */

 kl1, kl2 */

 k9 */

 k11, k12 */

 k15, k16 */

 kw3, kw4 */

 left half of key */

 right half of key */

 temporary variables */

	/**

	 *  key = (kl || kr || krl || krr) (|| is concatenation)

 generate KL dependent subkeys */

 kw1 */

 kw2 */

 k9 */

 k10 */

 kl3 */

 kl4 */

 k17 */

 k18 */

 k23 */

 k24 */

 generate KR dependent subkeys */

 k3 */

 k4 */

 kl1 */

 kl2 */

 k13 */

 k14 */

 k19 */

 k20 */

 generate KA */

 generate KB */

 generate KA dependent subkeys */

 k5 */

 k6 */

 k11 */

 k12 */

 rotation left shift 32bit */

 kl5 */

 kl6 */

 rotation left shift 17 from k11,k12 -> k21,k22 */

 k21 */

 k22 */

 generate KB dependent subkeys */

 k1 */

 k2 */

 k7 */

 k8 */

 k15 */

 k16 */

 kw3 */

 kw4 */

		/*

		 * On Pentium 4, camellia-asm is slower than original assembler

		 * implementation because excessive uses of 64bit rotate and

		 * left-shifts (which are really slow on P4) needed to store and

		 * handle 128bit block in two 64bit registers.

/* GPL HEADER START

 *

 * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License version 2 only,

 * as published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU

 * General Public License version 2 for more details (a copy is included

 * in the LICENSE file that accompanied this code).

 *

 * You should have received a copy of the GNU General Public License

 * version 2 along with this program; If not, see http://www.gnu.org/licenses

 *

 * Please  visit http://www.xyratex.com/contact if you need additional

 * information or have any questions.

 *

 * GPL HEADER END

/*

 * Copyright 2012 Xyratex Technology Limited

 *

 * Wrappers for kernel crypto shash api to pclmulqdq crc32 implementation.

#define PCLMUL_MIN_LEN		64L     /* minimum size of buffer

 size of xmm register */

 align p to 16 byte */

 No final XOR 0xFFFFFFFF, like crc32_le */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Glue Code for the AVX assembler implementation of the Cast6 Cipher

 *

 * Copyright (C) 2012 Johannes Goetzfried

 *     <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>

 *

 * Copyright © 2013 Jussi Kivilinna <jussi.kivilinna@iki.fi>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * x64 SIMD accelerated ChaCha and XChaCha stream ciphers,

 * including ChaCha20 (RFC7539)

 *

 * Copyright (C) 2015 Martin Willi

 kmovq */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Accelerated GHASH implementation with Intel PCLMULQDQ-NI

 * instructions. This file contains glue code.

 *

 * Copyright (c) 2009 Intel Corp.

 *   Author: Huang Ying <ying.huang@intel.com>

 perform multiplication by 'x' in GF(2^128) */

 Pickle-Mickle-Duck */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Glue Code for 3-way parallel assembler optimized version of Twofish

 *

 * Copyright (c) 2011 Jussi Kivilinna <jussi.kivilinna@mbnet.fi>

		/*

		 * On Atom, twofish-3way is slower than original assembler

		 * implementation. Twofish-3way trades off some performance in

		 * storing blocks in 64bit registers to allow three blocks to

		 * be processed parallel. Parallel operation then allows gaining

		 * more performance than was trade off, on out-of-order CPUs.

		 * However Atom does not benefit from this parallelism and

		 * should be blacklisted.

		/*

		 * On Pentium 4, twofish-3way is slower than original assembler

		 * implementation because excessive uses of 64bit rotate and

		 * left-shifts (which are really slow on P4) needed to store and

		 * handle 128bit block in two 64bit registers.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support for Intel AES-NI instructions. This file contains glue

 * code, the real AES implementation is in intel-aes_asm.S.

 *

 * Copyright (C) 2008, Intel Corp.

 *    Author: Huang Ying <ying.huang@intel.com>

 *

 * Added RFC4106 AES-GCM support for 128-bit keys under the AEAD

 * interface for 64-bit kernels.

 *    Authors: Adrian Hoban <adrian.hoban@intel.com>

 *             Gabriele Paoloni <gabriele.paoloni@intel.com>

 *             Tadeusz Struk (tadeusz.struk@intel.com)

 *             Aidan O'Mahony (aidan.o.mahony@intel.com)

 *    Copyright (c) 2010, Intel Corporation.

/* This data is stored at the end of the crypto_tfm struct.

 * It's a type of per "session" data storage location.

 * This needs to be 16 byte aligned.

 init, update and finalize context data */

 Scatter / Gather routines, with args similar to above */

/*

 * asmlinkage void aesni_gcm_init_avx_gen2()

 * gcm_data *my_ctx_data, context data

 * u8 *hash_subkey,  the Hash sub key input. Data starts on a 16-byte boundary.

/*

 * asmlinkage void aesni_gcm_init_avx_gen4()

 * gcm_data *my_ctx_data, context data

 * u8 *hash_subkey,  the Hash sub key input. Data starts on a 16-byte boundary.

 handle ciphertext stealing */

 handle ciphertext stealing */

	/*

	 * based on key length, override with the by8 version

	 * of ctr mode encryption/decryption for improved performance

	 * aes_set_key_common() ensures that key length is one of

	 * {128,192,256}

 Clear the data in the hash sub key container to zero.*/

 We want to cipher all zeros to create the hash sub key. */

Account for 4 byte nonce at the end.*/

/* This is the Integrity Check Value (aka the authentication tag) length and can

 Linearize assoc, if not already linear */

 assoc can be any length, so must be on heap */

 Copy out original auth_tag */

 Compare generated tag with passed in tag. */

 Assuming we are supporting rfc4106 64-bit extended */

 sequence numbers We need to have the AAD length equal */

 to 16 or 20 bytes */

 IV below built */

 Assuming we are supporting rfc4106 64-bit extended */

 sequence numbers We need to have the AAD length */

 equal to 16 or 20 bytes */

 IV below built */

 first half of xts-key is for crypt */

 second half of xts-key is for tweak */

 calculate first value of T */

 optimize performance of ctr mode encryption transform */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Glue Code for x86_64/AVX/AES-NI assembler optimized version of Camellia

 *

 * Copyright © 2012-2013 Jussi Kivilinna <jussi.kivilinna@iki.fi>

 16-way parallel cipher functions (avx/aes-ni) */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Glue Code for AVX assembler versions of Serpent Cipher

 *

 * Copyright (C) 2012 Johannes Goetzfried

 *     <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>

 *

 * Copyright © 2011-2013 Jussi Kivilinna <jussi.kivilinna@iki.fi>

 8-way parallel cipher functions */

 SPDX-License-Identifier: GPL-2.0 OR MIT

/*

 * Copyright (C) 2020 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved.

 * Copyright (c) 2016-2020 INRIA, CMU and Microsoft Corporation

/* Computes the addition of four-element f1 with value in f2

 Clear registers to propagate the carry bit */

 Begin addition chain */

 Return the carry bit in a register */

 Computes the field addition of two field elements */

 Compute the raw addition of f1 + f2 */

 Wrap the result back into the field */

 Step 1: Compute carry*38 */

 Step 2: Add carry*38 to the original sum */

 Step 3: Fold the carry bit back in; guaranteed not to carry at this point */

 Computes the field subtraction of two field elements */

 Compute the raw subtraction of f1-f2 */

 Wrap the result back into the field */

 Step 1: Compute carry*38 */

 Step 2: Subtract carry*38 from the original difference */

 Step 3: Fold the carry bit back in; guaranteed not to carry at this point */

 Store the result */

/* Computes a field multiplication: out <- f1 * f2

 Compute the raw multiplication: tmp <- src1 * src2 */

 Compute src1[0] * src2 */

 Compute src1[1] * src2 */

 Compute src1[2] * src2 */

 Compute src1[3] * src2 */

 Line up pointers */

 Wrap the result back into the field */

 Step 1: Compute dst + carry == tmp_hi * 38 + tmp_lo */

 Step 2: Fold the carry back into dst */

 Step 3: Fold the carry bit back in; guaranteed not to carry at this point */

/* Computes two field multiplications:

 * out[0] <- f1[0] * f2[0]

 * out[1] <- f1[1] * f2[1]

 Compute the raw multiplication tmp[0] <- f1[0] * f2[0] */

 Compute src1[0] * src2 */

 Compute src1[1] * src2 */

 Compute src1[2] * src2 */

 Compute src1[3] * src2 */

 Compute the raw multiplication tmp[1] <- f1[1] * f2[1] */

 Compute src1[0] * src2 */

 Compute src1[1] * src2 */

 Compute src1[2] * src2 */

 Compute src1[3] * src2 */

 Line up pointers */

 Wrap the results back into the field */

 Step 1: Compute dst + carry == tmp_hi * 38 + tmp_lo */

 Step 2: Fold the carry back into dst */

 Step 3: Fold the carry bit back in; guaranteed not to carry at this point */

 Step 1: Compute dst + carry == tmp_hi * 38 + tmp_lo */

 Step 2: Fold the carry back into dst */

 Step 3: Fold the carry bit back in; guaranteed not to carry at this point */

 Computes the field multiplication of four-element f1 with value in f2 */

 Compute the raw multiplication of f1*f2 */

 f1[0]*f2 */

 f1[1]*f2 */

 f1[2]*f2 */

 f1[3]*f2 */

 Wrap the result back into the field */

 Step 1: Compute carry*38 */

 Step 2: Fold the carry back into dst */

 Step 3: Fold the carry bit back in; guaranteed not to carry at this point */

 Computes p1 <- bit ? p2 : p1 in constant time */

 Invert the polarity of bit to match cmov expectations */

 cswap p1[0], p2[0] */

 cswap p1[1], p2[1] */

 cswap p1[2], p2[2] */

 cswap p1[3], p2[3] */

 cswap p1[4], p2[4] */

 cswap p1[5], p2[5] */

 cswap p1[6], p2[6] */

 cswap p1[7], p2[7] */

/* Computes the square of a field element: out <- f * f

 Compute the raw multiplication: tmp <- f * f */

 Step 1: Compute all partial products */

 f[0] */

 f[1]*f[0] */

 f[2]*f[0] */

 f[3]*f[0] */

 f[3] */

 f[1]*f[3] */

 f[2]*f[3] */

 f1 */

 f[2]*f[1] */

 Step 2: Compute two parallel carry chains */

 Step 3: Compute intermediate squares */

 f[0]^2 */

 f[1]^2 */

 f[2]^2 */

 f[3]^2 */

 Line up pointers */

 Wrap the result back into the field */

 Step 1: Compute dst + carry == tmp_hi * 38 + tmp_lo */

 Step 2: Fold the carry back into dst */

 Step 3: Fold the carry bit back in; guaranteed not to carry at this point */

/* Computes two field squarings:

 * out[0] <- f[0] * f[0]

 * out[1] <- f[1] * f[1]

 Step 1: Compute all partial products */

 f[0] */

 f[1]*f[0] */

 f[2]*f[0] */

 f[3]*f[0] */

 f[3] */

 f[1]*f[3] */

 f[2]*f[3] */

 f1 */

 f[2]*f[1] */

 Step 2: Compute two parallel carry chains */

 Step 3: Compute intermediate squares */

 f[0]^2 */

 f[1]^2 */

 f[2]^2 */

 f[3]^2 */

 Step 1: Compute all partial products */

 f[0] */

 f[1]*f[0] */

 f[2]*f[0] */

 f[3]*f[0] */

 f[3] */

 f[1]*f[3] */

 f[2]*f[3] */

 f1 */

 f[2]*f[1] */

 Step 2: Compute two parallel carry chains */

 Step 3: Compute intermediate squares */

 f[0]^2 */

 f[1]^2 */

 f[2]^2 */

 f[3]^2 */

 Line up pointers */

 Step 1: Compute dst + carry == tmp_hi * 38 + tmp_lo */

 Step 2: Fold the carry back into dst */

 Step 3: Fold the carry bit back in; guaranteed not to carry at this point */

 Step 1: Compute dst + carry == tmp_hi * 38 + tmp_lo */

 Step 2: Fold the carry back into dst */

 Step 3: Fold the carry bit back in; guaranteed not to carry at this point */

/* The below constants were generated using this sage script:

 *

 * #!/usr/bin/env sage

 * import sys

 * from sage.all import *

 * def limbs(n):

 * 	n = int(n)

 * 	l = ((n >> 0) % 2^64, (n >> 64) % 2^64, (n >> 128) % 2^64, (n >> 192) % 2^64)

 * 	return "0x%016xULL, 0x%016xULL, 0x%016xULL, 0x%016xULL" % l

 * ec = EllipticCurve(GF(2^255 - 19), [0, 486662, 0, 1, 0])

 * p_minus_s = (ec.lift_x(9) - ec.lift_x(1))[0]

 * print("static const u64 p_minus_s[] = { %s };\n" % limbs(p_minus_s))

 * print("static const u64 table_ladder[] = {")

 * p = ec.lift_x(9)

 * for i in range(252):

 * 	l = (p[0] + p[2]) / (p[0] - p[2])

 * 	print(("\t%s" + ("," if i != 251 else "")) % limbs(l))

 * 	p = p * 2

 * print("};")

 *

 might want less than we've got */

 might want less than we've got */

 SPDX-License-Identifier: GPL-2.0

/*

 * NHPoly1305 - ε-almost-∆-universal hash function for Adiantum

 * (AVX2 accelerated version)

 *

 * Copyright 2018 Google LLC

 wrapper to avoid indirect call to assembly, which doesn't work with CFI */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * The AEGIS-128 Authenticated-Encryption Algorithm

 *   Glue for AES-NI + SSE2 implementation

 *

 * Copyright (c) 2017-2018 Ondrej Mosnacek <omosnacek@gmail.com>

 * Copyright (C) 2017-2018 Red Hat, Inc. All rights reserved.

 SPDX-License-Identifier: GPL-2.0

/*

 * NHPoly1305 - ε-almost-∆-universal hash function for Adiantum

 * (SSE2 accelerated version)

 *

 * Copyright 2018 Google LLC

 wrapper to avoid indirect call to assembly, which doesn't work with CFI */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Glue Code for x86_64/AVX2 assembler optimized version of Serpent

 *

 * Copyright © 2012-2013 Jussi Kivilinna <jussi.kivilinna@mbnet.fi>

 16-way AVX2 parallel cipher functions */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Glue Code for SSE2 assembler versions of Serpent Cipher

 *

 * Copyright (c) 2011 Jussi Kivilinna <jussi.kivilinna@mbnet.fi>

 *

 * Glue code based on aesni-intel_glue.c by:

 *  Copyright (C) 2008, Intel Corp.

 *    Author: Huang Ying <ying.huang@intel.com>

 *

 * CBC & ECB parts based on code (crypto/cbc.c,ecb.c) by:

 *   Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Using hardware provided CRC32 instruction to accelerate the CRC32 disposal.

 * CRC32C polynomial:0x1EDC6F41(BE)/0x82F63B78(LE)

 * CRC32 is a new instruction in Intel SSE4.2, the reference can be found at:

 * http://www.intel.com/products/processor/manuals/

 * Intel(R) 64 and IA-32 Architectures Software Developer's Manual

 * Volume 2A: Instruction Set Reference, A-M

 *

 * Copyright (C) 2008 Intel Corporation

 * Authors: Austin Zhang <austin_zhang@linux.intel.com>

 *          Kent Liu <kent.liu@intel.com>

/*

 * use carryless multiply version of crc32c when buffer

 * size is >= 512 to account

 * for fpu state save/restore overhead.

 CONFIG_X86_64 */

/*

 * Setting the seed allows arbitrary accumulators and flexible XOR policy

 * If your algorithm starts with ~0, then XOR with ~0 before you set

 * the seed.

	/*

	 * use faster PCL version if datasize is large enough to

	 * overcome kernel fpu state save/restore overhead

 CONFIG_X86_64 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Glue Code for x86_64/AVX2/AES-NI assembler optimized version of Camellia

 *

 * Copyright © 2013 Jussi Kivilinna <jussi.kivilinna@mbnet.fi>

 32-way AVX2/AES-NI parallel cipher functions */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Xen PCI - handle PCI (INTx) and MSI infrastructure calls for PV, HVM and

 * initial domain support. We also handle the DSDT _PRT callbacks for GSI's

 * used in HVM and initial domain mode (PV does not parse ACPI, so it has no

 * concept of GSIs). Under PV we hook under the pnbbios API for IRQs and

 * 0xcf8 PCI configuration read/write.

 *

 *   Author: Ryan Wilson <hap9@epoch.ncsc.mil>

 *           Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

 *           Stefano Stabellini <stefano.stabellini@eu.citrix.com>

 In PV DomU the Xen PCI backend puts the PIRQ in the interrupt line.*/

 no mapping of GSI to PIRQ */);

	/*

	 * We set vector == 0 to tell the hypervisor we don't care about

	 * it, but we want a pirq setup instead.  We use the dest_id fields

	 * to pass the pirq that we want.

		/* N.B. Casting int's -ENODEV to uint16_t results in 0xFFED,

			/*

			 * If MAP_PIRQ_TYPE_MULTI_MSI is not available

			 * there's nothing else we can do in this case.

			 * Just set ret > 0 so driver can retry with

			 * single MSI.

 CONFIG_XEN_PV_DOM0 */

 !CONFIG_XEN_PV_DOM0 */

/*

 * This irq domain is a blatant violation of the irq domain design, but

 * distangling XEN into real irq domains is not a job for mere mortals with

 * limited XENology. But it's the least dangerous way for a mere mortal to

 * get rid of the arch_*_msi_irqs() hackery in order to store the irq

 * domain pointer in struct device. This irq domain wrappery allows to do

 * that without breaking XEN terminally.

 FIXME: No idea how to survive if this fails */

	/*

	 * Override the PCI/MSI irq domain init function. No point

	 * in allocating the native domain and never use it.

 CONFIG_PCI_MSI */

 CONFIG_PCI_MSI */

 Keep ACPI out of the picture */

		/*

		 * If hardware supports (x2)APIC virtualization (as indicated

		 * by hypervisor's leaf 4) then we don't need to use pirqs/

		 * event channels for MSI handling and instead use regular

		 * APIC processing

	/*

	 * We don't want to change the actual ACPI delivery model,

	 * just how GSIs get registered.

	/*

	 * We need to wait until after x2apic is initialized

	 * before we can set MSI IRQ ops.

	/*

	 * Pre-allocate the legacy IRQs.  Use NR_LEGACY_IRQS here

	 * because we don't have a PIC and thus nr_legacy_irqs() is zero.

 Map GSI to PIRQ */);

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Low-level PCI config space access for OLPC systems who lack the VSA

 * PCI virtualization software.

 *

 * Copyright © 2006  Advanced Micro Devices, Inc.

 *

 * The AMD Geode chipset (ie: GX2 processor, cs5536 I/O companion device)

 * has some I/O functions (display, southbridge, sound, USB HCIs, etc)

 * that more or less behave like PCI devices, but the hardware doesn't

 * directly implement the PCI configuration space headers.  AMD provides

 * "VSA" (Virtual System Architecture) software that emulates PCI config

 * space for these devices, by trapping I/O accesses to PCI config register

 * (CF8/CFC) and running some code in System Management Mode interrupt state.

 * On the OLPC platform, we don't want to use that VSA code because

 * (a) it slows down suspend/resume, and (b) recompiling it requires special

 * compilers that are hard to get.  So instead of letting the complex VSA

 * code simulate the PCI config registers for the on-chip devices, we

 * just simulate them the easy way, by inserting the code into the

 * pci_write_config and pci_read_config path.  Most of the config registers

 * are read-only anyway, so the bulk of the simulation is just table lookup.

/*

 * In the tables below, the first two line (8 longwords) are the

 * size masks that are used when the higher level PCI code determines

 * the size of the region by writing ~0 to a base address register

 * and reading back the result.

 *

 * The following lines are the values that are read during normal

 * PCI config access cycles, i.e. not after just having written

 * ~0 to a base address register.

 dev 1 function 0 - devfn = 8 */

 AMD Vendor ID */

 No virtual registers, hence no BAR */

 dev 1 function 0 - devfn = 8 */

 NSC Vendor ID */

 I/O BAR - base of virtual registers */

 dev 1 function 1 - devfn = 9 */

 AMD Vendor ID */

 FB, GP, VG, DF */

 VIP */

 INTA, IRQ14 for graphics accel */

 VG IO, VG IO, EGA FB, MONO FB */

 dev 1 function 1 - devfn = 9 */

 NSC Vendor ID */

 FB, GP, VG, DF */

 VG IO, VG IO, EGA FB, MONO FB */

 dev 1 function 2 - devfn = 0xa */

 NSC Vendor ID */

 AES registers */

 dev f function 0 - devfn = 78 */

 SMB-8   GPIO-256 MFGPT-64  IRQ-32 */

 PMS-128 ACPI-64 */

 IRQ steering */

 dev f function 3 - devfn = 7b */

 I/O BAR-128 */

 IntB, IRQ5 */

 dev f function 4 - devfn = 7c */

 MEMBAR-1000 */

 CapPtr INT-D, IRQA */

	0xc8020001, 0x0, 0x0,	0x0,	/* Capabilities - 40 is R/O,

 dev f function 4 - devfn = 7d */

 MEMBAR-1000 */

 CapPtr INT-D, IRQA */

	0xc8020001, 0x0, 0x0,	0x0,	/* Capabilities - 40 is R/O, 44 is

 EECP - see EHCI spec section 2.1.7 */

 EECP - see EHCI spec section 2.1.7 */

	0x2020,	0x0,	0x0,	0x0,	/* (EHCI page 8) 60 SBRN (R/O),

 Set after a write of ~0 to a BAR */

 Northbridge - GX chip - Device 1 */

 Southbridge - CS5536 chip - Device F */

	/*

	 * This is a little bit tricky.  The header maps consist of

	 * 0x20 bytes of size masks, followed by 0x70 bytes of header data.

	 * In the normal case, when not probing a BAR's size, we want

	 * to access the header data, so we add 0x20 to the reg offset,

	 * thus skipping the size mask area.

	 * In the BAR probing case, we want to access the size mask for

	 * the BAR, so we subtract 0x10 (the config header offset for

	 * BAR0), and don't skip the size mask area.

 Use the hardware mechanism for non-simulated devices */

	/*

	 * No device has config registers past 0x70, so we save table space

	 * by not storing entries for the nonexistent registers

 Use the hardware mechanism for non-simulated devices */

 XXX we may want to extend this to simulate EHCI power management */

	/*

	 * Mostly we just discard writes, but if the write is a size probe

	 * (i.e. writing ~0 to a BAR), we remember it and arrange to return

	 * the appropriate size mask on the next read.  This is cheating

	 * to some extent, because it depends on the fact that the next

	 * access after such a write will always be a read to the same BAR.

 write is to a BAR */

		/*

		 * No warning on writes to ROM BAR, CMD, LATENCY_TIMER,

		 * CACHE_LINE_SIZE, or PM registers.

 SPDX-License-Identifier: GPL-2.0

/*

 *	Low-Level PCI Access for i386 machines

 *

 * Copyright 1993, 1994 Drew Eckhardt

 *      Visionary Computing

 *      (Unix and Linux consulting and custom programming)

 *      Drew@Colorado.EDU

 *      +1 (303) 786-7975

 *

 * Drew's work was sponsored by:

 *	iX Multiuser Multitasking Magazine

 *	Hannover, Germany

 *	hm@ix.de

 *

 * Copyright 1997--2000 Martin Mares <mj@ucw.cz>

 *

 * For more information, please consult the following manuals (look at

 * http://www.pcisig.com/ for how to get them):

 *

 * PCI BIOS Specification

 * PCI Local Bus Specification

 * PCI to PCI Bridge Specification

 * PCI System Design Guide

 *

/*

 * This list of dynamic mappings is for temporarily maintaining

 * original BIOS BAR addresses for possible reinstatement.

 Must be called with 'pcibios_fwaddrmap_lock' lock held. */

/*

 * We need to avoid collisions with `mirrored' VGA ports

 * and other strange ISA hardware, so we always want the

 * addresses to be allocated in the 0x000-0x0ff region

 * modulo 0x400.

 *

 * Why? Because some silly external IO cards only decode

 * the low 10 bits of the IO address. The 0x00-0xff region

 * is reserved for motherboard devices that decode all 16

 * bits, so it's ok to allocate at, say, 0x2800-0x28ff,

 * but we want to try to avoid allocating at 0x2900-0x2bff

 * which might have be mirrored at 0x0100-0x03ff..

 The low 1MB range is reserved for ISA cards */

/*

 *  Handle resources of PCI devices.  If the world were perfect, we could

 *  just allocate all the resource regions and do nothing more.  It isn't.

 *  On the other hand, we cannot just re-allocate all devices, as it would

 *  require us to know lots of host bridge internals.  So we attempt to

 *  keep as much of the original configuration as possible, but tweak it

 *  when it's found to be wrong.

 *

 *  Known BIOS problems we have to work around:

 *	- I/O or memory regions not configured

 *	- regions configured, but not enabled in the command register

 *	- bogus I/O addresses above 64K used

 *	- expansion ROMs left enabled (this may sound harmless, but given

 *	  the fact the PCI specs explicitly allow address decoders to be

 *	  shared between expansion ROMs and other resource regions, it's

 *	  at least dangerous)

 *	- bad resource sizes or overlaps with other regions

 *

 *  Our solution:

 *	(1) Allocate resources for all buses behind PCI-to-PCI bridges.

 *	    This gives us fixed barriers on where we can allocate.

 *	(2) Allocate resources for all enabled devices.  If there is

 *	    a collision, just mark the resource as unallocated. Also

 *	    disable expansion ROMs during this step.

 *	(3) Try to allocate resources for disabled devices.  If the

 *	    resources were assigned correctly, everything goes well,

 *	    if they weren't, they won't disturb allocation of other

 *	    resources.

 *	(4) Assign new addresses to resources which were either

 *	    not configured at all or misconfigured.  If explicitly

 *	    requested by the user, configure expansion ROM address

 *	    as well.

 Already allocated */

			/*

			 * Something is wrong with the region.

			 * Invalidate the resource to prevent

			 * child resource allocations in this

			 * range.

 Depth-First Search on bus tree */

 Already allocated */

 Address not assigned at all */

 We'll assign a new address later */

			/* Turn the ROM off, leave the resource region,

	/*

	 * Try to use BIOS settings for ROMs, otherwise let

	 * pci_assign_unassigned_resources() allocate the new

	 * addresses.

 Already allocated */

/*

 * This is an fs_initcall (one below subsys_initcall) in order to reserve

 * resources properly.

	/*

	 * Insert the IO APIC resources after PCI initialization has

	 * occurred to handle IO APICS that are mapped in on a BAR in

	 * PCI space, but before trying to assign unassigned pci res.

 SPDX-License-Identifier: GPL-2.0

/*

 * IMPORTANT NOTE:

 * hb_probes[] and early_root_info_init() is in maintenance mode.

 * It only supports K8, Fam10h, Fam11h, and Fam15h_00h-0fh .

 * Future processor will rely on information in ACPI.

 K8 */

 Family10h */

 Family10h */

 Family11h */

 Family15h */

 find the position */

/**

 * early_root_info_init()

 * called before pcibios_scan_root and pci_scan_bus

 * fills the mp_bus_to_cpumask array based according

 * to the LDT Bus Number Registers found in the northbridge.

	/*

	 * We should learn topology and routing information from _PXM and

	 * _CRS methods in the ACPI namespace.  We extract node numbers

	 * here to work around BIOSes that don't supply _PXM.

 Check if that register is enabled for bus range */

	/*

	 * The following code extracts routing information for use on old

	 * systems where Linux doesn't automatically use host bridge _CRS

	 * methods (or when the user specifies "pci=nocrs").

	 *

	 * We only do this through Fam11h, because _CRS should be enough on

	 * newer systems.

 get the default node and link for left over res */

 io port resource */

 not found */

 kernel only handle 16 bit only */

 add left over io port range to def node/link, [0, 0xffff] */

 find the position */

 0xfd00000000-0xffffffffff for HT */

 need to take out [0, TOM) for RAM*/

 get mmconfig */

 need to take out mmconf range */

 mmio resource */

 39:16 on 31:8*/

		/*

		 * some sick allocation would have range overlap with fam10h

		 * mmconf range, so need to update start and end.

 we got a hole */

 need to take out [4G, TOM2) for RAM*/

 SYS_CFG */

 TOP_MEM2 is enabled? */

 TOP_MEM2 */

	/*

	 * add left over mmio range to def node/link ?

	 * that is tricky, just record range in from start_min to 4G

 assume all cpus from fam10h have IO ECS */

 Try the PCI method first. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * legacy.c - traditional, old school PCI bus probing

/*

 * Discover remaining PCI buses in case there are peer host bridges.

 * We use the number of last PCI bus provided by the PCI BIOS.

	/*

	 * The init function returns an non zero value when

	 * pci_legacy_init should be invoked.

 SPDX-License-Identifier: GPL-2.0

 already added by acpi ? */

	/*

	 * We don't have any host bridge aperture information from the

	 * "native host bridge drivers," e.g., amd_bus or broadcom_bus,

	 * so fall back to the defaults historically used by pci_create_bus().

 try to merge it with old one */

 need to add that */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright(c) 2010 Intel Corporation. All rights reserved.

 *

 *  Contact Information:

 *    Intel Corporation

 *    2200 Mission College Blvd.

 *    Santa Clara, CA  97052

 *

 * This provides access methods for PCI registers that mis-behave on

 * the CE4100. Each register can be assigned a private init, read and

 * write routine. The exception to this is the bridge device.  The

 * bridge device is the only device on bus zero (0) that requires any

 * fixup so it is a special case ATM

/*

 * All read/write functions are called with pci_config_lock held.

 force interrupt pin value to 0 */

 Make BARs appear to not request any memory. */

		/* Since subordinate bus number register is hardwired

		 * to zero and read only, so do the simulation.

 Get the A/V bridge base address. */

		/* Make prefetchable memory limit smaller than prefetchable

		 * memory base, so not claim prefetchable memory space.

 Make IO limit smaller than IO base, so not claim IO space. */

 Discard writes to A/V bridge BAR. */

 Indicate caller that it should invoke pci_legacy_init() */

 SPDX-License-Identifier: GPL-2.0

/* arch_initcall has too random ordering, so call the initializers

	/*

	 * Must happen after x86_init.pci.arch_init(). Xen sets up the

	 * x86_init.irqs.create_pci_msi_domain there.

	/*

	 * don't check for raw_pci_ops here because we want pcbios as last

	 * fallback, yet it's needed to run first to set pcibios_last_bus

	 * in case legacy PCI probing is used. otherwise detecting peer busses

	 * fails.

 SPDX-License-Identifier: GPL-2.0

/*

 * Exceptions for specific devices. Usually work-arounds for fatal design flaws.

	/*

	 * i450NX -- Find and scan all secondary buses on all PXB's.

 Bus A */

 Bus B */

	/*

	 * i450GX and i450KX -- Find and scan all secondary buses.

	 * (called separately for each PCI bridge found)

	/*

	 * UM8886BF IDE controller sets region type bits incorrectly,

	 * therefore they look like memory despite of them being I/O.

	/*

	 *  SiS 5597 and 5598 chipsets require latency timer set to

	 *  at most 32 to avoid lockups.

	/*

	 * PIIX4 ACPI device: hardwired IRQ9

/*

 * Addresses issues with problems in the memory write queue timer in

 * certain VIA Northbridges.  This bugfix is per VIA's specifications,

 * except for the KL133/KM133: clearing bit 5 on those Northbridges seems

 * to trigger a bug in its integrated ProSavage video card, which

 * causes screen corruption.  We only clear bits 6 and 7 for that chipset,

 * until VIA can provide us with definitive information on why screen

 * corruption occurs, and what exactly those bits do.

 *

 * VIA 8363,8622,8361 Northbridges:

 *  - bits  5, 6, 7 at offset 0x55 need to be turned off

 * VIA 8367 (KT266x) Northbridges:

 *  - bits  5, 6, 7 at offset 0x95 need to be turned off

 * VIA 8363 rev 0x81/0x84 (KL133/KM133) Northbridges:

 *  - bits     6, 7 at offset 0x55 need to be turned off

 clear bits 5, 6, 7 by default */

		/* fix pci bus latency issues resulted by NB bios error

		   it appears on bug free^Wreduced kt266x's bios forces

		where = 0x95; /* the memory write queue timer register is

			mask = 0x3f; /* clear only bits 6 and 7; clearing bit 5

/*

 * For some reasons Intel decided that certain parts of their

 * 815, 845 and some other chipsets must look like PCI-to-PCI bridges

 * while they are obviously not. The 82801 family (AA, AB, BAM/CAM,

 * BA/CA/DB and E) PCI bridges are actually HUB-to-PCI ones, according

 * to Intel terminology. These devices do forward all addresses from

 * system to PCI bus no matter what are their window settings, so they are

 * "transparent" (or subtractive decoding) from programmers point of view.

/*

 * Fixup for C1 Halt Disconnect problem on nForce2 systems.

 *

 * From information provided by "Allen Martin" <AMartin@nvidia.com>:

 *

 * A hang is caused when the CPU generates a very fast CONNECT/HALT cycle

 * sequence.  Workaround is to set the SYSTEM_IDLE_TIMEOUT to 80 ns.

 * This allows the state-machine and timer to return to a proper state within

 * 80 ns of the CONNECT and probe appearing together.  Since the CPU will not

 * issue another HALT within 80 ns of the initial HALT, the failure condition

 * is avoided.

	/*

	 * Chip  Old value   New value

	 * C17   0x1F0FFF01  0x1F01FF01

	 * C18D  0x9F0FFF01  0x9F01FF01

	 *

	 * Northbridge chip version may be determined by

	 * reading the PCI revision ID (0xC1 or greater is C18D).

	/*

	 * Apply fixup if needed, but don't touch disconnect state

 Max PCI Express root ports */

/*

 * Replace the original pci bus ops for write with a new one that will filter

 * the request to insure ASPM cannot be enabled.

/*

 * Prevents PCI Express ASPM (Active State Power Management) being enabled.

 *

 * Save the register offset, where the ASPM control bits are located,

 * for each PCI Express device that is in the device list of

 * the root port in an array for fast indexing. Replace the bus ops

 * with the modified one.

	/*

	 * Check if the DID of pdev matches one of the six root ports. This

	 * check is needed in the case this function is called directly by the

	 * hot-plug driver.

		/*

		 * If no device is attached to the root port at power-up or

		 * after hot-remove, the pbus->devices is empty and this code

		 * will set the offsets to zero and the bus ops to parent's bus

		 * ops, which is unmodified.

		/*

		 * If devices are attached to the root port at power-up or

		 * after hot-add, the code loops through the device list of

		 * each root port to save the register offsets and replace the

		 * bus ops.

 There are 0 to 8 devices attached to this bus */

/*

 * Fixup to mark boot BIOS video selected by BIOS before it changes

 *

 * From information provided by "Jon Smirl" <jonsmirl@gmail.com>

 *

 * The standard boot ROM sequence for an x86 machine uses the BIOS

 * to select an initial video card for boot display. This boot video

 * card will have its BIOS copied to 0xC0000 in system RAM.

 * IORESOURCE_ROM_SHADOW is used to associate the boot video

 * card with this copy. On laptops this copy has to be used since

 * the main ROM may be compressed or combined with another image.

 * See pci_map_rom() for use of this flag. Before marking the device

 * with IORESOURCE_ROM_SHADOW check if a vga_default_device is already set

 * by either arch code or vga-arbitration; if so only apply the fixup to this

 * already-determined primary video card.

 Is VGA routed to us? */

		/*

		 * From information provided by

		 * "David Miller" <davem@davemloft.net>

		 * The bridge control register is valid for PCI header

		 * type BRIDGE, or CARDBUS. Host to PCI controllers use

		 * PCI header type NORMAL.

/*

 * The AMD-Athlon64 board MSI "K8T Neo2-FIR" disables the onboard sound

 * card if a PCI-soundcard is added.

 *

 * The BIOS only gives options "DISABLED" and "AUTO". This code sets

 * the corresponding register-value to enable the soundcard.

 *

 * The soundcard is only enabled, if the mainboard is identified

 * via DMI-tables and the soundcard is detected to be off.

 only applies to MSI K8T Neo2-FIR */

 verify the change for status output */

/*

 * Some Toshiba laptops need extra code to enable their TI TSB43AB22/A.

 *

 * We pretend to bring them out of full D3 state, and restore the proper

 * IRQ, PCI cache line size, and BARs, otherwise the device won't function

 * properly.  In some cases, the device will generate an interrupt on

 * the wrong IRQ line, causing any devices sharing the line it's

 * *supposed* to use to be disabled by the kernel's IRQ debug code.

 only applies to certain Toshibas (so far) */

 only applies to certain Toshibas (so far) */

 Restore config space on Toshiba laptops */

/*

 * Prevent the BIOS trapping accesses to the Cyrix CS5530A video device

 * configuration space.

 clear 'F4 Video Configuration Trap' bit */

/*

 * Siemens Nixdorf AG FSC Multiprocessor Interrupt Controller:

 * prevent update of the BAR0, which doesn't look like a normal BAR.

/*

 * SB600: Disable BAR1 on device 14.0 to avoid HPET resources from

 * confusing the PCI engine:

	/*

	 * The SB600 and SB700 both share the same device

	 * ID, but the PM register 0x55 does something different

	 * for the SB700, so make sure we are dealing with the

	 * SB600 before touching the bit:

 Set bit 7 in PM register 0x55 */

/*

 * Twinhead H12Y needs us to block out a region otherwise we map devices

 * there and any access kills the box.

 *

 *   See: https://bugzilla.kernel.org/show_bug.cgi?id=10231

 *

 * Match off the LPC and svid/sdid (older kernels lose the bridge subvendor)

/*

 * Device [8086:2fc0]

 * Erratum HSE43

 * CONFIG_TDP_NOMINAL CSR Implemented at Incorrect Offset

 * https://www.intel.com/content/www/us/en/processors/xeon/xeon-e5-v3-spec-update.html

 *

 * Devices [8086:6f60,6fa0,6fc0]

 * Erratum BDF2

 * PCI BARs in the Home Agent Will Return Non-Zero Values During Enumeration

 * https://www.intel.com/content/www/us/en/processors/xeon/xeon-e5-v4-spec-update.html

/*

 * Device [1022:7808]

 * 23. USB Wake on Connect/Disconnect with Low Speed Devices

 * https://support.amd.com/TechDocs/46837.pdf

 * Appendix A2

 * https://support.amd.com/TechDocs/42413.pdf

/*

 * Device [1022:7914]

 * When in D0, PME# doesn't get asserted when plugging USB 2.0 device.

/*

 * Apple MacBook Pro: Avoid [mem 0x7fa00000-0x7fbfffff]

 *

 * Using the [mem 0x7fa00000-0x7fbfffff] region, e.g., by assigning it to

 * the 00:1c.0 Root Port, causes a conflict with [io 0x1804], which is used

 * for soft poweroff and suspend-to-RAM.

 *

 * As far as we know, this is related to the address space, not to the Root

 * Port itself.  Attaching the quirk to the Root Port is a convenience, but

 * it could probably also be a standalone DMI quirk.

 *

 * https://bugzilla.kernel.org/show_bug.cgi?id=103211

/*

 * VMD-enabled root ports will change the source ID for all messages

 * to the VMD device. Rather than doing device matching with the source

 * ID, the AER driver should traverse the child device tree, reading

 * AER registers to find the faulting device.

 VMD Domain */

	/*

	 * Denverton reports 2k of RTIT_BAR (intel_th resource 4), which

	 * appears to be 4 MB in reality.

/*

 * The PCI Firmware Spec, rev 3.2, notes that ACPI should optionally allow

 * configuring host bridge windows using the _PRS and _SRS methods.

 *

 * But this is rarely implemented, so we manually enable a large 64bit BAR for

 * PCIe device on AMD Family 15h (Models 00h-1fh, 30h-3fh, 60h-7fh) Processors

 * here.

 Check that we are the only device of that type */

 This is a multi-socket system, don't touch it for now */

 Is this slot free? */

 Abort if a slot already configures a 64bit BAR. */

	/*

	 * Allocate a 256GB window directly below the 0xfd00000000 hardware

	 * limit (see AMD Family 15h Models 30h-3Fh BKDG, sec 2.4.6).

 We are resuming from suspend; just reenable the window */

/*

 * Some BIOS implementations support RAM above 4GB, but do not configure the

 * PCI host to respond to bus master accesses for these addresses. These

 * implementations set the TOP_OF_DRAM_SLOT1 register correctly, so PCI DMA

 * works as expected for addresses below 4GB.

 *

 * Reference: "AMD RS690 ASIC Family Register Reference Guide" (pg. 2-57)

 * https://www.amd.com/system/files/TechDocs/43372_rs690_rrg_3.00o.pdf

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Read address ranges from a Broadcom CNB20LE Host Bridge

 *

 * Copyright (c) 2010 Ira W. Snyder <iws@ovro.caltech.edu>

 read the PCI bus numbers */

	/*

	 * Add the legacy IDE ports on bus 0

	 *

	 * These do not exist anywhere in the bridge registers, AFAICT. I do

	 * not have the datasheet, so this is the best I can do.

 read the non-prefetchable memory window */

 read the prefetchable memory window */

 read the IO port window */

 print information about this host bridge */

	/*

	 * We should get host bridge information from ACPI unless the BIOS

	 * doesn't support it.

 SPDX-License-Identifier: GPL-2.0

/*

 * direct.c - Low-level direct PCI config space access

/*

 * Functions for accessing PCI base (first 256 bytes) and extended

 * (4096 bytes per PCI function) configuration space with type 1

 * accesses.

/*

 * Functions for accessing PCI configuration space with type 2 accesses

/*

 * Before we decide to use direct hardware access mechanisms, we try to do some

 * trivial checks to ensure it at least _seems_ to be working -- we just test

 * whether bus 00 contains a host bridge (this is similar to checking

 * techniques used in XFree86, but ours should be more reliable since we

 * attempt to make use of direct access hints provided by the PCI BIOS).

 *

 * This should be close to trivial, but it isn't, because there are buggy

 * chipsets (yes, you guessed it, by Intel and Compaq) that have no class ID.

	/* Assume Type 1 works for newer systems.

 SPDX-License-Identifier: GPL-2.0

/*

 * mmconfig.c - Low-level direct PCI config space access via MMCONFIG

 *

 * This is an 64bit optimized version that always keeps the full mmconfig

 * space mapped. This allows lockless config space operation.

 Why do we have this when nobody checks it. How about a BUG()!? -AK */

 Why do we have this when nobody checks it. How about a BUG()!? -AK */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel MID PCI support

 *   Copyright (c) 2008 Intel Corporation

 *     Jesse Barnes <jesse.barnes@intel.com>

 *

 * Moorestown has an interesting PCI implementation:

 *   - configuration space is memory mapped (as defined by MCFG)

 *   - Lincroft devices also have a real, type 1 configuration space

 *   - Early Lincroft silicon has a type 1 access bug that will cause

 *     a hang if non-existent devices are accessed

 *   - some devices have the "fixed BAR" capability, which means

 *     they can't be relocated or modified; check for that during

 *     BAR sizing

 *

 * So, we use the MCFG space for all reads and writes, but also send

 * Lincroft writes to type 1 space.  But only read/write if the device

 * actually exists, otherwise return all 1s for reads and bit bucket

 * the writes.

 Quirks for the listed devices */

 Fixed BAR fields */

 Fixed BAR (TBD) */

/**

 * fixed_bar_cap - return the offset of the fixed BAR cap if found

 * @bus: PCI bus

 * @devfn: device in question

 *

 * Look for the fixed BAR cap on @bus and @devfn, returning its offset

 * if found or 0 otherwise.

 Turn the size into a decode pattern for the sizing code */

		/*

		 * If val is all ones, the core code is trying to size the reg,

		 * so update the mmconfig space with the real size.

		 *

		 * Note: this assumes the fixed size we got is a power of two.

 This is some other kind of BAR write, so just do it. */

/**

 * type1_access_ok - check whether to use type 1

 * @bus: bus number

 * @devfn: device & function in question

 * @reg: configuration register offset

 *

 * If the bus is on a Lincroft chip and it exists, or is not on a Lincroft at

 * all, the we can go ahead with any reads & writes.  If it's on a Lincroft,

 * but doesn't exist, avoid the access altogether to keep the chip from

 * hanging.

	/*

	 * This is a workaround for A0 LNC bug where PCI status register does

	 * not have new CAP bit set. can not be written by SW either.

	 *

	 * PCI header type in real LNC indicates a single function device, this

	 * will prevent probing other devices under the same function in PCI

	 * shim. Therefore, use the header type in shim instead.

 Langwell on others */

	/*

	 * On MRST, there is no PCI ROM BAR, this will cause a subsequent read

	 * to ROM BAR return 0 then being ignored.

	/*

	 * Devices with fixed BARs need special handling:

	 *   - BAR sizing code will save, write ~0, read size, restore

	 *   - so writes to fixed BARs need special handling

	 *   - other writes to fixed BAR devices should go through mmconfig

	/*

	 * On Moorestown update both real & mmconfig space

	 * Note: early Lincroft silicon can't handle type 1 accesses to

	 *       non-existent devices, so just eat the write in that case.

 Special treatment for IRQ0 */

			/*

			 * Skip HS UART common registers device since it has

			 * IRQ0 assigned and not used by the kernel.

			/*

			 * TNG has IRQ0 assigned to eMMC controller. But there

			 * are also other devices with bogus PCI configuration

			 * that have IRQ0 assigned. This check ensures that

			 * eMMC gets it. The rest of devices still could be

			 * enabled without interrupt line being allocated.

	/*

	 * MRST only have IOAPIC, the PCI irq lines are 1:1 mapped to

	 * IOAPIC RTE entries, so we just enable RTE for the device.

/**

 * intel_mid_pci_init - installs intel_mid_pci_ops

 *

 * Moorestown has an interesting PCI implementation (see above).

 * Called when the early platform detection installs it.

 Continue with standard init */

/*

 * Langwell devices are not true PCI devices; they are not subject to 10 ms

 * d3 to d0 delay required by PCI spec.

	/*

	 * PCI fixups are effectively decided compile time. If we have a dual

	 * SoC/non-SoC kernel we don't want to mangle d3 on non-SoC devices.

	/*

	 * True PCI devices in Lincroft should allow type 1 access, the rest

	 * are Langwell fake PCI devices.

	/*

	 * Update current state first, otherwise PCI core enforces PCI_D0 in

	 * pci_set_power_state() for devices which status was PCI_UNKNOWN.

	/*

	 * This sets only PMCSR bits. The actual power off will happen in

	 * arch/x86/platform/intel-mid/pwr.c.

/*

 * Langwell devices reside at fixed offsets, don't try to move them.

 Must have extended configuration space */

 Fixup the BAR sizes for fixed BAR devices and make them unmoveable */

 SPDX-License-Identifier: GPL-2.0

/*

 * BIOS32 and PCI BIOS handling.

 BIOS32 signature: "_32_" */

 PCI signature: "PCI " */

 PCI service signature: "$PCI" */

 PCI BIOS hardware mechanism flags */

/* According to the BIOS specification at:

 * http://members.datafast.net.au/dft0802/specs/bios21.pdf, we could

 * restrict the x zone to some pages and make it ro. But this may be

 * broken on some bios, complex to handle with static_protections.

 * We could make the 0xe0000-0x100000 range rox, but this can break

 * some ISA mapping.

 *

 * So we let's an rw and x hole when pcibios is used. This shouldn't

 * happen for modern system with mmconfig, and if you don't want it

 * you could disable pcibios...

/*

 * This is the standard structure used to identify the entry point

 * to the BIOS32 Service Directory, as documented in

 * 	Standard BIOS 32-bit Service Directory Proposal

 * 	Revision 0.4 May 24, 1993

 * 	Phoenix Technologies Ltd.

 *	Norwood, MA

 * and the PCI BIOS specification.

 _32_ */

 32 bit physical address */

 Revision level, 0 */

 Length in paragraphs should be 01 */

 All bytes must add up to zero */

 Must be zero */

/*

 * Physical address of the service directory.  I don't know if we're

 * allowed to have more than one of these or not, so just in case

 * we'll make pcibios_present() take a memory start parameter and store

 * the array there.

/*

 * Returns the entry point for the given service, NULL on error

 %al */

 %ebx */

 %ecx */

 %edx */

 Not present */

 Shouldn't happen */

	/*

	 * Zero-extend the result beyond 8 or 16 bits, do not trust the

	 * BIOS having done it:

/*

 * Function table for BIOS32 access

/*

 * Try to find PCI BIOS.

	/*

	 * Follow the standard procedure for locating the BIOS32 Service

	 * directory by scanning the permissible address range from

	 * 0xe0000 through 0xfffff for a valid BIOS32 structure.

 Hopefully more than one BIOS32 cannot happen... */

/*

 *  BIOS Functions for IRQ Routing

 SPDX-License-Identifier: GPL-2.0

/* Direct PCI access. This is used for PCI accesses in early boot before

 SPDX-License-Identifier: GPL-2.0

bugzilla.kernel.org/show_bug.cgi?id=14183 */

bugzilla.kernel.org/show_bug.cgi?id=16007 */

 2006 AMD HT/VIA system with two host bridges */

bugzilla.kernel.org/show_bug.cgi?id=30552 */

 2006 AMD HT/VIA system with two host bridges */

bugzilla.kernel.org/show_bug.cgi?id=42619 */

bugs.launchpad.net/ubuntu/+source/alsa-driver/+bug/931368 */

bugs.launchpad.net/ubuntu/+source/alsa-driver/+bug/1033299 */

 Now for the blacklist.. */

bugzilla.redhat.com/show_bug.cgi?id=769657 */

bugzilla.redhat.com/show_bug.cgi?id=769657 */

bugzilla.kernel.org/show_bug.cgi?id=42606 */

bugzilla.kernel.org/show_bug.cgi?id=15362 */

	/*

	 * If the user specifies "pci=use_crs" or "pci=nocrs" explicitly, that

	 * takes precedence over anything we figured out above.

	/*

	 * Failure in adding MMCFG information is not fatal,

	 * just can't access extended configuration space of

	 * devices under this host bridge.

 return success if MMCFG is not in use */

 enable MMCFG if it hasn't been enabled yet */

/*

 * An IO port or MMIO resource assigned to a PCI host bridge may be

 * consumed by the host bridge itself or available to its child

 * bus/devices. The ACPI specification defines a bit (Producer/Consumer)

 * to tell whether the resource is consumed by the host bridge itself,

 * but firmware hasn't used that bit consistently, so we can't rely on it.

 *

 * On x86 and IA64 platforms, all IO port and MMIO resources are assumed

 * to be available to child bus/devices except one special case:

 *     IO port [0xCF8-0xCFF] is consumed by the host bridge itself

 *     to access PCI configuration space.

 *

 * So explicitly filter out PCI CFG IO ports[0xCF8-0xCFF].

		/*

		 * If the desired bus has been scanned already, replace

		 * its bus->sysdata.

	/* After the PCI-E bus has been walked and all devices discovered,

	 * configure any settings of the fabric that might be necessary.

	/*

	 * We pass NULL as parent to pci_create_root_bus(), so if it is not NULL

	 * here, pci_create_root_bus() has been called by someone else and

	 * sysdata is likely to be different from what we expect.  Let it go in

	 * that case.

		/*

		 * PCI IRQ routing is set up by pci_enable_device(), but we

		 * also do it here in case there are still broken drivers that

		 * don't use pci_enable_device().

 SPDX-License-Identifier: GPL-2.0-only

/*

 *	Low-Level PCI Support for PC

 *

 *	(c) 1999--2000 Martin Mares <mj@ucw.cz>

/*

 * This interrupt-safe spinlock protects all accesses to PCI configuration

 * space, except for the mmconfig (ECAM) based operations.

/*

 * Systems where PCI IO resource ISA alignment can be skipped

 * when the ISA enable bit in the bridge control is not set

		/*

		* If the BIOS did not assign the BAR, zero out the

		* resource so the kernel doesn't attempt to assign

		* it later on in pci_assign_unassigned_resources

 we deal with BIOS assigned ROM later */

/*

 *  Called after each bus is probed, but before its children

 *  are examined.

/*

 * Only use DMI information to set this if nothing was passed

 * on the kernel command line (which was parsed earlier).

/*

 * Enable renumbering of PCI bus# ranges to reach all PCI busses (Cardbus)

/*

 * Laptops which need pci=assign-busses to see Cardbus cards

 __i386__ */

	/*

	 * Set PCI cacheline size to that of the CPU if the CPU has reported it.

	 * (For older CPUs that don't support cpuid, we se it to 32 bytes

	 * It's also good for 386/486s (which actually have 16)

	 * as quite a few PCI devices do not support smaller values.

	/*

	 * Setup the initial MSI domain of the device. If the underlying

	 * bus has a PCI/MSI irqdomain associated use the bus domain,

	 * otherwise set the default domain. This ensures that special irq

	 * domains e.g. VMD are preserved. The default ensures initial

	 * operation if irq remapping is not active. If irq remapping is

	 * active it will overwrite the domain pointer when the device is

	 * associated to a remapping domain.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2004 Matthew Wilcox <matthew@wil.cx>

 * Copyright (C) 2004 Intel Corp.

/*

 * mmconfig.c - Low-level direct PCI config space access via MMCONFIG

 Assume systems with more busses have correct MCFG */

 The base address of the last MMCONFIG device accessed */

/*

 * Functions for accessing PCI configuration space with MMCONFIG accesses

/*

 * This is always called under pci_config_lock

 Invalidate the cached mmcfg map entry. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * DMA translation between STA2x11 AMBA memory mapping and the x86 memory mapping

 *

 * ST Microelectronics ConneXt (STA2X11/STA2X10)

 *

 * Copyright (c) 2010-2011 Wind River Systems, Inc.

/*

 * We build a list of bus numbers that are under the ConneXt. The

 * main bridge hosts 4 busses, which are the 4 endpoints, in order.

 0..3 included */

 0..7 included */

 saved during suspend */

 At probe time, record new instances of this bridge (likely one only) */

 This has a subordinate bridge, with 4 more-subordinate ones */

 First instance: register your own swiotlb area */

/*

 * Utility functions used in this file from below

 This is exported, as some devices need to access the MFD registers */

 At setup time, we use our own ops if the device is a ConneXt one */

 either a sta2x11 bridge or another ST device */

 We must enable all devices as master, for audio DMA to work */

/*

 * At boot we must set up the mappings for the pcie-to-amba bridge.

 * It involves device access, and the same happens at suspend/resume time

 Relax Order Ena */

 No Snoop Enable */

 At probe time, enable mapping for each endpoint, using the pdev */

 Configure AHB mapping */

 Disable all the other windows */

 Some register values must be saved and restored */

 Save all window configs */

 Restore all window configs */

 Like at boot, enable master on all devices */

 CONFIG_PM */

 SPDX-License-Identifier: GPL-2.0

/*

 * mmconfig-shared.c - Low-level direct PCI config space access via

 *                     MMCONFIG - common code between i386 and x86-64.

 *

 * This code does:

 * - known chipset handling

 * - ACPI decoding and validation

 *

 * Per-architecture code takes care of the mappings and accesses

 * themselves.

 Indicate if the mmcfg resources have been placed into the resource table. */

 keep list sorted by segment and starting bus number */

 Enable bit */

 Size bits */

 Errata #2, things break when not aligned on a 256Mb boundary */

 Can only happen in 64M/128M mode */

 Don't hit the APIC registers and their friends */

 mmconfig is not enable */

	/*

	 * only handle bus 0 ?

	 * need to skip it

	/*

	 * do check if amd fam10h already took over

 base could > 4G */

 Fixup overlaps */

 Don't access the list head ! */

 some end_bus_number is crazy, fix it */

 update end_bus */

	/*

	 * e820__mapped_all() is marked as __init.

	 * All entries from ACPI MCFG table have been checked at boot time.

	 * For MCFG information constructed from hotpluggable host bridge's

	 * _CBA method, just assume it's reserved.

	/* Don't try to do this check unless configuration

 how many config structures do we have */

 MMCONFIG disabled */

 MMCONFIG hasn't been enabled yet, try again */

 If we are not using MMCONFIG, don't insert the resources. */

	/*

	 * Attempt to insert the mmcfg resources but not with the busy flag

	 * marked so it won't cause request errors when __request_region is

	 * called.

/*

 * Perform MMCONFIG resource insertion after PCI initialization to allow for

 * misprogrammed MCFG tables that state larger sizes but actually conflict

 * with other system resources.

 Add MMCFG information for host bridges */

 Insert resource if it's not in boot stage */

 Delete MMCFG information for host bridges */

 SPDX-License-Identifier: GPL-2.0

/*

 *	Low-Level PCI Support for PC -- Routing of Interrupts

 *

 *	(c) 1999--2000 Martin Mares <mj@ucw.cz>

/*

 * Never use: 0, 1, 2 (timer, keyboard, and cascade)

 * Avoid using: 13, 14 and 15 (FP error and IDE).

 * Penalize: 3, 4, 6, 7, 12 (known ISA uses: serial, floppy, parallel and mouse)

/*

 *  Check passed address for the PCI IRQ Routing Table signature

 *  and perform checksum verification.

/*

 *  Search 0xf0000 -- 0xfffff for the PCI IRQ Routing Table.

/*

 *  If we have a IRQ routing table, use it to search for peer host

 *  bridges.  It's a gross hack, but since there are no other known

 *  ways how to get a list of buses, we have to go this way.

/*

 *  Code for querying and setting of IRQ routes on various interrupt routers.

 *  PIC Edge/Level Control Registers (ELCR) 0x4d0 & 0x4d1.

/*

 *	PIRQ routing for the M1487 ISA Bus Controller (IBC) ASIC used

 *	with the ALi FinALi 486 chipset.  The IBC is not decoded in the

 *	PCI configuration space, so we identify it by the accompanying

 *	M1489 Cache-Memory PCI Controller (CMP) ASIC.

 *

 *	There are four 4-bit mappings provided, spread across two PCI

 *	INTx Routing Table Mapping Registers, available in the port I/O

 *	space accessible indirectly via the index/data register pair at

 *	0x22/0x23, located at indices 0x42 and 0x43 for the INT1/INT2

 *	and INT3/INT4 lines respectively.  The INT1/INT3 and INT2/INT4

 *	lines are mapped in the low and the high 4-bit nibble of the

 *	corresponding register as follows:

 *

 *	0000 : Disabled

 *	0001 : IRQ9

 *	0010 : IRQ3

 *	0011 : IRQ10

 *	0100 : IRQ4

 *	0101 : IRQ5

 *	0110 : IRQ7

 *	0111 : IRQ6

 *	1000 : Reserved

 *	1001 : IRQ11

 *	1010 : Reserved

 *	1011 : IRQ12

 *	1100 : Reserved

 *	1101 : IRQ14

 *	1110 : Reserved

 *	1111 : IRQ15

 *

 *	In addition to the usual ELCR register pair there is a separate

 *	PCI INTx Sensitivity Register at index 0x44 in the same port I/O

 *	space, whose bits 3:0 select the trigger mode for INT[4:1] lines

 *	respectively.  Any bit set to 1 causes interrupts coming on the

 *	corresponding line to be passed to ISA as edge-triggered and

 *	otherwise they are passed as level-triggered.  Manufacturer's

 *	documentation says this register has to be set consistently with

 *	the relevant ELCR register.

 *

 *	Accesses to the port I/O space concerned here need to be unlocked

 *	by writing the value of 0xc5 to the Lock Register at index 0x03

 *	beforehand.  Any other value written to said register prevents

 *	further accesses from reaching the register file, except for the

 *	Lock Register being written with 0xc5 again.

 *

 *	References:

 *

 *	"M1489/M1487: 486 PCI Chip Set", Version 1.2, Acer Laboratories

 *	Inc., July 1997

/*

 * Common IRQ routing practice: nibbles in config space,

 * offset by some magic constant.

/*

 * ALI pirq entries are damn ugly, and completely undocumented.

 * This has been figured out from pirq tables, and it's not a pretty

 * picture.

/*

 *	PIRQ routing for the 82374EB/82374SB EISA System Component (ESC)

 *	ASIC used with the Intel 82420 and 82430 PCIsets.  The ESC is not

 *	decoded in the PCI configuration space, so we identify it by the

 *	accompanying 82375EB/82375SB PCI-EISA Bridge (PCEB) ASIC.

 *

 *	There are four PIRQ Route Control registers, available in the

 *	port I/O space accessible indirectly via the index/data register

 *	pair at 0x22/0x23, located at indices 0x60/0x61/0x62/0x63 for the

 *	PIRQ0/1/2/3# lines respectively.  The semantics is the same as

 *	with the PIIX router.

 *

 *	Accesses to the port I/O space concerned here need to be unlocked

 *	by writing the value of 0x0f to the ESC ID Register at index 0x02

 *	beforehand.  Any other value written to said register prevents

 *	further accesses from reaching the register file, except for the

 *	ESC ID Register being written with 0x0f again.

 *

 *	References:

 *

 *	"82374EB/82374SB EISA System Component (ESC)", Intel Corporation,

 *	Order Number: 290476-004, March 1996

 *

 *	"82375EB/82375SB PCI-EISA Bridge (PCEB)", Intel Corporation, Order

 *	Number: 290477-004, March 1996

/*

 * The Intel PIIX4 pirq rules are fairly simple: "pirq" is

 * just a pointer to the config space.

/*

 *	PIRQ routing for the 82426EX ISA Bridge (IB) ASIC used with the

 *	Intel 82420EX PCIset.

 *

 *	There are only two PIRQ Route Control registers, available in the

 *	combined 82425EX/82426EX PCI configuration space, at 0x66 and 0x67

 *	for the PIRQ0# and PIRQ1# lines respectively.  The semantics is

 *	the same as with the PIIX router.

 *

 *	References:

 *

 *	"82420EX PCIset Data Sheet, 82425EX PCI System Controller (PSC)

 *	and 82426EX ISA Bridge (IB)", Intel Corporation, Order Number:

 *	290488-004, December 1995

/*

 * The VIA pirq rules are nibble-based, like ALI,

 * but without the ugly irq number munging.

 * However, PIRQD is in the upper instead of lower 4 bits.

/*

 * The VIA pirq rules are nibble-based, like ALI,

 * but without the ugly irq number munging.

 * However, for 82C586, nibble map is different .

/*

 * ITE 8330G pirq rules are nibble-based

 * FIXME: pirqmap may be { 1, 0, 3, 2 },

 * 	  2+3 are both mapped to irq 9 on my system

/*

 * OPTI: high four bits are nibble pointer..

 * I wonder what the low bits do?

/*

 * Cyrix: nibble offset 0x5C

 * 0x5C bits 7:4 is INTB bits 3:0 is INTA

 * 0x5D bits 7:4 is INTD bits 3:0 is INTC

/*

 *	PIRQ routing for SiS 85C503 router used in several SiS chipsets.

 *	We have to deal with the following issues here:

 *	- vendors have different ideas about the meaning of link values

 *	- some onboard devices (integrated in the chipset) have special

 *	  links and are thus routed differently (i.e. not via PCI INTA-INTD)

 *	- different revision of the router have a different layout for

 *	  the routing registers, particularly for the onchip devices

 *

 *	For all routing registers the common thing is we have one byte

 *	per routeable link which is defined as:

 *		 bit 7      IRQ mapping enabled (0) or disabled (1)

 *		 bits [6:4] reserved (sometimes used for onchip devices)

 *		 bits [3:0] IRQ to map to

 *		     allowed: 3-7, 9-12, 14-15

 *		     reserved: 0, 1, 2, 8, 13

 *

 *	The config-space registers located at 0x41/0x42/0x43/0x44 are

 *	always used to route the normal PCI INT A/B/C/D respectively.

 *	Apparently there are systems implementing PCI routing table using

 *	link values 0x01-0x04 and others using 0x41-0x44 for PCI INTA..D.

 *	We try our best to handle both link mappings.

 *

 *	Currently (2003-05-21) it appears most SiS chipsets follow the

 *	definition of routing registers from the SiS-5595 southbridge.

 *	According to the SiS 5595 datasheets the revision id's of the

 *	router (ISA-bridge) should be 0x01 or 0xb0.

 *

 *	Furthermore we've also seen lspci dumps with revision 0x00 and 0xb1.

 *	Looks like these are used in a number of SiS 5xx/6xx/7xx chipsets.

 *	They seem to work with the current routing code. However there is

 *	some concern because of the two USB-OHCI HCs (original SiS 5595

 *	had only one). YMMV.

 *

 *	Onchip routing for router rev-id 0x01/0xb0 and probably 0x00/0xb1:

 *

 *	0x61:	IDEIRQ:

 *		bits [6:5] must be written 01

 *		bit 4 channel-select primary (0), secondary (1)

 *

 *	0x62:	USBIRQ:

 *		bit 6 OHCI function disabled (0), enabled (1)

 *

 *	0x6a:	ACPI/SCI IRQ: bits 4-6 reserved

 *

 *	0x7e:	Data Acq. Module IRQ - bits 4-6 reserved

 *

 *	We support USBIRQ (in addition to INTA-INTD) and keep the

 *	IDE, ACPI and DAQ routing untouched as set by the BIOS.

 *

 *	Currently the only reported exception is the new SiS 65x chipset

 *	which includes the SiS 69x southbridge. Here we have the 85C503

 *	router revision 0x04 and there are changes in the register layout

 *	mostly related to the different USB HCs with USB 2.0 support.

 *

 *	Onchip routing for router rev-id 0x04 (try-and-error observation)

 *

 *	0x60/0x61/0x62/0x63:	1xEHCI and 3xOHCI (companion) USB-HCs

 *				bit 6-4 are probably unused, not like 5595

/*

 * VLSI: nibble offset 0x74 - educated guess due to routing table and

 *       config space of VLSI 82C534 PCI-bridge/router (1004:0102)

 *       Tested on HP OmniBook 800 covering PIRQ 1, 2, 4, 8 for onboard

 *       devices, PIRQ 3 for non-pci(!) soundchip and (untested) PIRQ 6

 *       for the busbridge to the docking station.

/*

 * ServerWorks: PCI interrupts mapped to system IRQ lines through Index

 * and Redirect I/O registers (0x0c00 and 0x0c01).  The Index register

 * format is (PCIIRQ## | 0x10), e.g.: PCIIRQ10=0x1a.  The Redirect

 * register is a straight binary coding of desired PIC IRQ (low nibble).

 *

 * The 'link' value in the PIRQ table is already in the correct format

 * for the Index register.  There are some special index values:

 * 0x00 for ACPI (SCI), 0x01 for USB, 0x02 for IDE0, 0x04 for IDE1,

 * and 0x03 for SMBus.

/* Support for AMD756 PCI IRQ Routing

 * Jhon H. Caicedo <jhcaiced@osso.org.co>

 * Jun/21/2001 0.2.0 Release, fixed to use "nybble" functions... (jhcaiced)

 * Jun/19/2001 Alpha Release 0.1.0 (jhcaiced)

 * The AMD756 pirq rules are nibble-based

 * offset 0x56 0-3 PIRQA  4-7  PIRQB

 * offset 0x57 0-3 PIRQC  4-7  PIRQD

/*

 * PicoPower PT86C523

 440GX has a proprietary PIRQ router -- don't use it */

 FIXME: We should move some of the quirk fixup stuff here */

	/*

	 * workarounds for some buggy BIOSes

			/*

			 * Asus k7m bios wrongly reports 82C686A

			 * as 586-compatible

			/**

			 * Asus a7v-x bios wrongly reports 8235

			 * as 586-compatible

			/**

			 * Asus a7v600 bios wrongly reports 8237

			 * as 586-compatible

 FIXME: add new ones for 8233/5 */

 Someone with docs needs to add the ATI Radeon IGP */

/*

 *	FIXME: should we have an option to say "generic for

 *	chipset" ?

 Default unless a driver reloads it */

 First look for a router match */

 Fall back to a device match */

 The device remains referenced for the kernel lifetime */

 Find IRQ pin */

 Find IRQ routing entry */

	/* Work around broken HP Pavilion Notebooks which assign USB to

 same for Acer Travelmate 360, but with CB and irq 11 -> 10 */

	/*

	 * Find the best IRQ to assign: use the one

	 * reported by the device if possible.

 Check if it is hardcoded */

 Update IRQ for all devices with the same pirq value */

			/*

			 * We refuse to override the dev->irq

			 * information. Give a warning!

		/*

		 * If the BIOS has set an out of range IRQ number, just

		 * ignore it.  Also keep track of which IRQ's are

		 * already in use.

		/*

		 * If the IRQ is already assigned to a PCI device,

		 * ignore its ISA use penalty

		/*

		 * Still no IRQ? Try to lookup one...

/*

 * Work around broken HP Pavilion Notebooks which assign USB to

 * IRQ 9 even though it is actually wired to IRQ 11

/*

 * Work around broken Acer TravelMate 360 Notebooks which assign

 * Cardbus to IRQ 11 even though it is actually wired to IRQ 10

		/*

		 * If we're using the I/O APIC, avoid using the PCI IRQ

		 * routing table

		/*

		 * PCI IRQ routing is set up by pci_enable_device(), but we

		 * also do it here in case there are still broken drivers that

		 * don't use pci_enable_device().

	/*

	 *  If any ISAPnP device reports an IRQ in its list of possible

	 *  IRQ's, we try to avoid assigning it to PCI devices.

			/*

			 * Busses behind bridges are typically not listed in the MP-table.

			 * In this case we have to look up the IRQ based on the parent bus,

			 * parent slot, and pin number. The SMP code detects such bridged

			 * busses itself so we should get into this branch reliably.

 go back to the bridge */

		/*

		 * With IDE legacy devices the IRQ lookup failure is not

		 * a problem..

 SPDX-License-Identifier: GPL-2.0

/*

 * Numascale NumaConnect-specific PCI code

 *

 * Copyright (C) 2012 Numascale AS. All rights reserved.

 *

 * Send feedback to <support@numascale.com>

 *

 * PCI accessor functions derived from mmconfig_64.c

 *

 Why do we have this when nobody checks it. How about a BUG()!? -AK */

 Ensure AMD Northbridges don't decode reads to other devices */

 Why do we have this when nobody checks it. How about a BUG()!? -AK */

 Ensure AMD Northbridges don't decode writes to other devices */

	/* For remote I/O, restrict bus 0 access to the actual number of AMD

 HyperTransport fabric size in bits 6:4 */

 Use NumaChip PCI accessors for non-extended and extended access */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2019 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2019 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2019 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2019 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2019 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2019 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2019 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2019 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2019 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

/*

 * Emulate a single FPU arithmetic instruction.

 resulting format */

	/*

	 * If an exception is required, generate a tidy SIGFPE exception.

 a real fpu computation instruction */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2019 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2019 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2019 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

/*

 * We need to ensure that shared mappings are correctly aligned to

 * avoid aliasing issues with VIPT caches.  We need to ensure that

 * a specific page of an object is always mapped at a multiple of

 * SHMLBA bytes.

 *

 * We unconditionally provide this function for all cases, however

 * in the VIVT case, we optimise out the alignment rules.

	/*

	 * We only need to do colour alignment if either the I or D

	 * caches alias.

	/*

	 * We enforce the MAP_FIXED case.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

/*

 * need to get a page for level 1

/*

 * In order to soft-boot, we need to insert a 1:1 mapping in place of

 * the user-mode pages.  This will then ensure that we have predictable

 * results when turning the mmu off

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 1995-2005 Russell King

 Copyright (C) 2012 ARM Ltd.

 Copyright (C) 2013-2017 Andes Technology Corporation

/*

 * empty_zero_page is a special page that is used for

 * zero-initialized data and COW.

/*

 * Map all physical memory under high_memory into kernel's address space.

 *

 * This is explicitly coded for two-level page tables, so if you need

 * something else then this needs to change.

	/* These mark extents of read-only kernel pages...

	 * ...from vmlinux.lds.S

 Alloc one page for holding PTE's... */

 Fill the newly allocated page with PTE'S */

 Create mapping between p and v. */

 TODO: more fine grant for page access permission */

 CONFIG_HIGHMEM */

	/*

	 * Fixed mappings:

	/*

	 * Permanent kmaps:

 CONFIG_HIGHMEM */

/*

 * paging_init() sets up the page tables, initialises the zone memory

 * maps, and sets up the zero page, bad page and bad page tables.

 clear out the init_mm.pgd that will contain the kernel's mappings */

 allocate space for empty_zero_page */

/*

 * mem_init() marks the free areas in the mem_map and tells us how much

 * memory is free.  This is done after various parts of the system have

 * claimed their memory after the kernel image.

 this will put all low memory onto the freelists */

	/*

	 * Check boundaries twice: Some fundamental inconsistencies can

	 * be detected at build time already.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 this should be consistent with ptrace.h */

 R0-R25 */

 FP, GP, LP */

 SP */

 cause a segfault */

 LHI333    */

 LWI333    */

 LWI333.bi */

 LWI450    */

 SHI333    */

 SWI333    */

 SWI333.bi */

 SWI450    */

 LHI       */

 LHI.bi    */

 LHSI      */

 LHSI.bi   */

 LWI       */

 LWI.bi    */

 SHI       */

 SHI.bi    */

 SWI       */

 SWI.bi    */

 LH        */

 LH.bi     */

 LHS       */

 LHS.bi    */

 LW        */

 LW.bi     */

 SH        */

 SH.bi     */

 SW        */

 SW.bi     */

/*

 * Initialize nds32 alignment-correction interface

 CONFIG_PROC_FS */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

/*

 * This is useful to dump out the page tables associated with

 * 'addr' in mm 'mm'.

 We must not map this if we have highmem enabled */

	/*

	 * We fault-in kernel-space virtual memory on-demand. The

	 * 'reference' page table is init_mm.pgd.

	 *

	 * NOTE! We MUST NOT take any locks for this case. We may

	 * be in an interrupt or a critical region, and should

	 * only copy the information from the master page table,

	 * nothing more.

 Send a signal to the task for handling the unalignment access. */

	/*

	 * If we're in an interrupt or have no user

	 * context, we must not take the fault..

	/*

	 * As per x86, we may deadlock here. However, since the kernel only

	 * validly references user space from well defined areas of the code,

	 * we can bug out early if this is from code which shouldn't.

		/*

		 * The above down_read_trylock() might have succeeded in which

		 * case, we'll have missed the might_sleep() from down_read().

	/*

	 * Ok, we have a good vm_area for this memory access, so

	 * we can handle it..

 first do some preliminary protection checks */

	/*

	 * If for any reason at all we couldn't handle the fault,

	 * make sure we exit gracefully rather than endlessly redo

	 * the fault.

	/*

	 * If we need to retry but a fatal signal is pending, handle the

	 * signal first. We do not need to release the mmap_lock because it

	 * would already be released in __lock_page_or_retry in mm/filemap.c.

			/* No need to mmap_read_unlock(mm) as we would

			 * have already released it in __lock_page_or_retry

			 * in mm/filemap.c.

	/*

	 * Something tried to access memory that isn't in our memory map..

	 * Fix it, but check if it's kernel or user first..

 User mode accesses just cause a SIGSEGV */

	/* Are we prepared to handle this kernel fault?

	 *

	 * (The kernel has valid exception-points in the source

	 *  when it acesses user-memory. When it fails in one

	 *  of those points, we find it in a table and do a jump

	 *  to some fixup code that loads an appropriate error

	 *  code)

 Adjust the instruction pointer in the stackframe */

	/*

	 * Oops. The kernel tried to access some bad page. We'll have to

	 * terminate things with extreme prejudice.

	/*

	 * We ran out of memory, or some other thing happened to us that made

	 * us unable to handle the page fault gracefully.

 Kernel mode? Handle exceptions or die */

	/*

	 * Send a sigbus

		/*

		 * Synchronize this task's top level page-table

		 * with the 'reference' page table.

		 *

		 * Use current_pgd instead of tsk->active_mm->pgd

		 * since the latter might be unavailable if this

		 * code is executed in a misfortunately run irq

		 * (like inside schedule() between switch_mm and

		 *  switch_to...).

		/*

		 * Since the vmalloc area is global, we don't

		 * need to copy individual PTE's, it is enough to

		 * copy the pgd pointer into the pte page of the

		 * root task. If that is there, we'll find our pte if

		 * it exists.

		/* Make sure the actual PTE exists as well to

		 * catch kernel vmalloc-area accesses to non-mapped

		 * addres. If we don't do this, this will just

		 * silently loop forever.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 user mode */

		/* superuser mode is always readable, so we can only

 user mode */

 superuser mode */

/*

 * All

 Section 1: Ensure the section 2 & 3 program code execution after */

 Section 2: Confirm the writeback all level is done in CPU and L2C */

 Section 3: Writeback whole L2 cache */

/*

 * Page

/*

 * Range

 TODO Can Use PAGE Mode to optimize if range large than PAGE_SIZE */

/*

 * DMA

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 Use cpu handler, jump to 0 */

	/*

	 * Clean and disable cache, and turn off interrupts

	/*

	 * Tell the mm system that we are going to reboot -

	 * we may need it to insert some 1:1 mappings so that

	 * soft boot works.

 Execute kernel restart handler call chain */

	/*

	 * Now call the architecture specific reboot code.

	/*

	 * Whoops - the architecture was unable to reboot.

	 * Tell the user!

 kernel thread fn */

 kernel thread argument */

 child get zero as ret. */

 cpu context switching  */

/*

 * fill in the fpe structure for a core dump...

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2008-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

/*

 * Dump out the contents of some memory nicely...

/*

 * This function is protected against re-entrancy.

 Check platform support. */

	/*

	 * 0x800 = 128 vectors * 16byte.

	 * It should be enough to flush a page.

 trap_signal */

 kernel_trap */

/*

 * 2:DEF dispatch for TLB MISC exception handler

 Permission exceptions */

 Alignment check */

 Reserved instruction */

 Coprocessor */

 trap, used on v3 EDM target debugging workaround */

		/*

		 * DIPC(OIPC) is passed as parameter before

		 * interrupt is enabled, so the DIPC will not be corrupted

		 * even though interrupts are coming in

		/*

		 * 1. update ipc

		 * 2. update pt_regs ipc with oipc

		 * 3. update pt_regs ipsw (clear DEX)

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

/*

 * The following string table, must sync with HWCAP_xx bitmask,

 * which is defined above

 Disable fpu and enable when it is used. */

 Find main memory where is the kernel */

 free_ram_start_pfn is first page after kernel */

 it could update max_pfn */

 high_memory is related with VMALLOC */

	/*

	 * initialize the boot-time allocator (with low memory only).

	 *

	 * This makes the memory from the end of the kernel to the end of

	 * RAM usable.

 setup bootmem allocator */

 paging_init() sets up the MMU and marks all pages as reserved */

 invalidate all TLB entries because the new mapping is created */

 use generic way to parse */

 dump out the processor features */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 mem functions */

 user mem (segment) */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 Only 1 level and I/D cache seperate. */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2018 Andes Technology Corporation

 no output */

 no output */

 no output */

 no output */

 no output */

 no output */

 Enable to use FPU. */

Lazy FPU is used

 Other processes fpu state, save away */

 First time FPU user.  */

 Coprocessor disabled exception */

 Coprocessor exception such as underflow and overflow */

 SPDX-License-Identifier: GPL-2.0

 avoid to optimize as pure function */

 save all state by the compiler prologue */

 restore all state by the compiler epilogue */

 CONFIG_DYNAMIC_FTRACE */

 avoid to optimize as pure function */

 avoid to optimize as pure function */

 save all state needed by the compiler prologue */

	/*

	 * prepare arguments for real tracing function

	 * first  arg : __builtin_return_address(0) - MCOUNT_INSN_SIZE

	 * second arg : parent_ip

 a placeholder for the call to a real tracing function */

 a placeholder for the call to ftrace_graph_caller */

 restore all state needed by the compiler epilogue */

 sethi $r15, imm20u       */

 ori   $r15, $r15, imm15u */

 jral  $lp,  $r15         */

 CONFIG_DYNAMIC_FTRACE */

 save state needed by the ABI     */

 get original return address      */

 restore state needed by the ABI  */

 CONFIG_DYNAMIC_FTRACE */

 CONFIG_FUNCTION_GRAPH_TRACER */

 CONFIG_TRACE_IRQFLAGS */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

	/*

	 * Avoid sys_rt_sigreturn() restarting.

 Always make any pending restarted system calls return -EINTR */

	/*

	 * Since we stacked the signal on a 64-bit boundary,

	 * then 'sp' should be two-word aligned here.  If it's

	 * not, then the user is trying to mess with us.

 Default to using normal stack */

	/*

	 * If we are on the alternate signal stack and would overflow it, don't.

	 * Return an always-bogus address instead so we will die with SIGSEGV.

 This is the X/Open sanctioned signal stack switching. */

	/*

	 * nds32 mandates 8-byte alignment

/*

 * OK, we're invoking a handler

 Avoid additional syscall restarting via ret_slow_syscall. */

	/*

	 * Set up the stack frame

/*

 * Note that 'init' is a special process: it doesn't get signals it doesn't

 * want to handle. Thus you cannot kill init even with a SIGKILL even by

 * mistake.

 *

 * Note that we go through the signals twice: once to check the signals that

 * the kernel can handle, and then we build all the user-level signal handling

 * stack-frames in one go after that.

	/*

	 * If we were from a system call, check for system call restarting...

 Restart the system call - no handlers present */

 Avoid additional syscall restarting via ret_slow_syscall. */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

/* do_ptrace()

 *

 * Provide ptrace defined service.

/* sys_trace()

 *

 * syscall trace handler.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2012 ARM Limited

 Copyright (C) 2005-2017 Andes Technology Corporation

/*

 * The vDSO data page.

 Creat a timer io mapping to get clock cycles counter */

 Allocate the vDSO pagelist */

 Round the lowest possible end address up to a PMD boundary. */

 Be sure to map the data page */

Map vdata to user space */

Map timer to user space */

Map vdso to user space */

 Pairs with smp_rmb in vdso_read_retry */

 Pairs with smp_rmb in vdso_read_begin */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2008-2017 Andes Technology Corporation

 *

 * Reference ARMv7: Jean Pihet <jpihet@mvista.com>

 * 2010 (c) MontaVista Software, LLC.

 Set at runtime when we know what CPU type we are. */

 Read overflow status */

 Write overflow bit to clear status, and others keep it 0 */

/*

 * Set the next IRQ period, based on the hwc->period_left value.

 * To be called with the event disabled in hw:

 The period may have been changed by PERF_EVENT_IOC_PERIOD */

	/*

	 * The hw event starts counting from this event offset,

	 * mark it to be able to extract future "deltas":

	/*

	 * Get and reset the IRQ flags

	/*

	 * Did an overflow occur?

	/*

	 * Handle the counter(s) overflow(s)

 Ignore if we don't have an event. */

		/*

		 * We have a single interrupt for all counters. Check that

		 * each counter has overflowed before we process it.

	/*

	 * Handle the pending perf events.

	 *

	 * Note: this call *must* be run with interrupts disabled. For

	 * platforms that can have the PMU interrupts raised as an NMI, this

	 * will not work.

/*

 * Add an event filter to a given event.

 If index is -1, do not do anything */

	/*

	 * Default: enable both kernel and user mode tracing.

	/*

	 * Install the filter into config_base as this is used to

	 * construct the event type.

 Clear previous mode selection, and write new one */

 Clear previous event selection */

 undo the linear mapping */

 Other modes NDS32 does not support */

	/*

	 * Enable counter and interrupt, and set the counter to count

	 * the event that we're interested in.

	/*

	 * Disable counter

	/*

	 * Check whether we need to exclude the counter from certain modes.

 Write event */

	/*

	 * Enable interrupt for this counter

	/*

	 * Enable counter

	/*

	 * Disable counter and interrupt

	/*

	 * Disable counter

	/*

	 * Disable interrupt for this counter

	/*

	 * Current implementation maps cycles, instruction count and cache-miss

	 * to specific counter.

	 * However, multiple of the 3 counters are able to count these events.

	 *

	 *

	 * SOFTWARE_EVENT_MASK mask for getting event num ,

	 * This is defined by Jia-Rung, you can change the polocies.

	 * However, do not exceed 8 bits. This is hardware specific.

	 * The last number is SPAv3_2_SEL_LAST.

	/*

	 * Try to get the counter for correpsonding event

 Enable all counters , NDS PFM has 3 counters */

 Disable all counters , NDS PFM has 3 counters */

 Maximum counts */

 NDS32 SPAv3 PMU support 3 counter */

	/*

	 * This name should be devive-specific name, whatever you like :)

	 * I think "PMU" will be a good generic name.

/*

 * CPU PMU identification and probing.

	/*

	 * If ther are various CPU types with its own PMU, initialize with

	 *

	 * the corresponding one

	/*

	 * Initialize the fake PMU. We only need to populate the

	 * used_mask for the purposes of validation.

	/*

	 * We don't assign an index until we actually place the event onto

	 * hardware. Use -1 to signify that we haven't decided where to put it

	 * yet. For SMP systems, each core has it's own PMU so we can't do any

	 * clever allocation or constraints checking at this point.

	/*

	 * Check whether we need to exclude the counter from certain modes.

	/*

	 * Store the event encoding into the config_base field.

		/*

		 * For non-sampling runs, limit the sample_period to half

		 * of the counter width. That way, the new counter value

		 * is far less likely to overtake the previous one unless

		 * you have some serious IRQ latency issues.

 does not support taken branch sampling */

 Register irq handler */

	/*

	 * NDS pmu always has to reprogram the period, so ignore

	 * PERF_EF_RELOAD, see the comment below.

 Set the period for the event. */

 If we don't have a space for the counter then finish early. */

	/*

	 * If there is an event in the counter we are going to use then make

	 * sure it is disabled.

 Propagate our changes to the userspace mapping. */

	/*

	 * Whether overflow or not, "unsigned substraction"

	 * will always get their delta

	/*

	 * NDS pmu always has to update the counter, so ignore

	 * PERF_EF_UPDATE, see comments in nds32_start().

 Please refer to SPAv3 for more hardware specific details */

 Ensure the PMU has sane values out of reset. */

/*

 * References: arch/nds32/kernel/traps.c:__dump()

 * You will need to know the NDS ABI first.

 0x3 means misalignment */

		/*

		 *	The array index is based on the ABI, the below graph

		 *	illustrate the reasons.

		 *	Function call procedure: "smw" and "lmw" will always

		 *	update SP and FP for you automatically.

		 *

		 *	Stack                                 Relative Address

		 *	|  |                                          0

		 *	----

		 *	|LP| <-- SP(before smw)  <-- FP(after smw)   -1

		 *	----

		 *	|FP|                                         -2

		 *	----

		 *	|  | <-- SP(after smw)                       -3

 make sure CONFIG_FUNCTION_GRAPH_TRACER is turned on */

	/*

	 * You can refer to arch/nds32/kernel/traps.c:__dump()

	 * Treat "sp" as "fp", but the "sp" is one frame ahead of "fp".

	 * And, the "sp" is not always correct.

	 *

	 *   Stack                                 Relative Address

	 *   |  |                                          0

	 *   ----

	 *   |LP| <-- SP(before smw)                      -1

	 *   ----

	 *   |  | <-- SP(after smw)                       -2

	 *   ----

		/* TODO: How to deal with the value in first

		 * "sp" is not correct?

/*

 * Gets called by walk_stackframe() for every stackframe. This will be called

 * whist unwinding the stackframe and is like a subroutine return so we use

 * the PC.

/*

 * Get the return address for a single stackframe and return a pointer to the

 * next frame tail.

 Check accessibility of one struct frame_tail beyond */

	/*

	 * Refer to unwind_frame_kernel() for more illurstration

 ((unsigned long *)fp)[-1] */

 ((unsigned long *)fp)[FP_OFFSET] */

 Check accessibility of one struct frame_tail beyond */

	/*

	 * Refer to unwind_frame_kernel() for more illurstration

 ((unsigned long *)fp)[-1] */

 ((unsigned long *)fp)[FP_OFFSET] */

/*

 * This will be called when the target is in user mode

 * This function will only be called when we use

 * "PERF_SAMPLE_CALLCHAIN" in

 * kernel/events/core.c:perf_prepare_sample()

 *

 * How to trigger perf_callchain_[user/kernel] :

 * $ perf record -e cpu-clock --call-graph fp ./program

 * $ perf report --call-graph

 We don't support guest os callchain now */

			/*

			 * Maybe this is non leaf function

			 * with optimize for size,

			 * or maybe this is the function

			 * with optimize for size

				/* non leaf function with optimize

				 * for size condition

				/* this is the function

				 * without optimize for size

 this is leaf function */

 previous function callcahin  */

 This will be called when the target is in kernel mode */

 We don't support guest os callchain now */

 However, NDS32 does not support virtualization */

 However, NDS32 does not support virtualization */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2005-2017 Andes Technology Corporation

 Pairs with smp_wmb in vdso_write_end */

 Pairs with smp_wmb in vdso_write_begin */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 *  linux/arch/math-emu/driver.c.c

 *

 *	decodes and dispatches unimplemented FPU instructions

 *

 *  Copyright (C) 1999, 2000  Philipp Rumpf <prumpf@tux.org>

 *  Copyright (C) 2001	      Hewlett-Packard <bame@debian.org>

 Format of the floating-point exception registers. */

/* Macros for grabbing bits of the instruction format from the 'ei'

 Major opcode 0c and 0e */

 Class 1 subopcode */

 Major opcode 0c, uid 2 (performance monitoring) */

 Major opcode 2e (fused operations).   */

 Major opcode 26 (FMPYSUB) */

 Major opcode 06 (FMPYADD) */

 Flags and enable bits of the status word. */

/* Handle a floating point exception.  Return zero if the faulting

	/* need an intermediate copy of float regs because FPU emulation

	 * code expects an artificial last entry which contains zero

	 *

	 * also, the passed in fr registers contain one word that defines

	 * the fpu type. the fpu type information is constructed 

	 * inside the emulation code

 Status word = FR0L. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/fp/decode_exc.c		$ Revision: $

 *

 *  Purpose:

 *	<<please update with a synopsis of the functionality provided by this file>>

 *

 *  External Interfaces:

 *	<<the following list was autogenerated, please review>>

 *	decode_fpu(Fpu_register, trap_counts)

 *

 *  Internal Interfaces:

 *	<<please update>>

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

 #include "types.h" */

 #include <machine/sys/mdep_private.h> */

 General definitions */

 bit position 2 */

 bits 21 & 22 */

 bits 19 & 20 */

 bits 17 & 18 */

 mask low-order 2 bits */

 mask low-order 5 bits */

 number of excpeption registers to check */

 Exception register definitions */

 Miscellaneous definitions */

 Single precision floating-point definitions */

 Double precision floating-point definitions */

 assumes zero fill. 32 bit */	\

    /* Keep stats on how many floating point exceptions (based on type)

     * that happen.  Want to keep this overhead low, but still provide

     * some information to the customer.  All exits from this routine

     * need to restore Fpu_register[0]

    /* exception_index is used to index the exception register queue.  It

     *   always points at the last register that contains a valid exception.  A

     *   zero value implies no exceptions (also the initialized value).  Setting

     *   the T-bit resets the exception_index to zero.

    /*

     * Check for reserved-op exception.  A reserved-op exception does not 

     * set any exception registers nor does it set the T-bit.  If the T-bit

     * is not set then a reserved-op exception occurred.

     *

     * At some point, we may want to report reserved op exceptions as

     * illegal instructions.

    /* 

     * Is a coprocessor op. 

     *

     * Now we need to determine what type of exception occurred.

	  /*

	   * On PA89: there are 5 different unimplemented exception

	   * codes: 0x1, 0x9, 0xb, 0x3, and 0x23.  PA-RISC 2.0 adds

	   * another, 0x2b.  Only these have the low order bit set.

		/*

		 * Clear T-bit and exception register so that

		 * we can tell if a trap really occurs while 

		 * emulating the instruction.

		/*

		 * Now emulate this instruction.  If a trap occurs,

		 * fpudispatch will return a non-zero number 

 accumulate the status flags, don't lose them as in hpux */

			/*

			 * We now need to make sure that the T-bit and the

			 * exception register contain the correct values

			 * before continuing.

			/*

			 * Set t-bit since it might still be needed for a

			 * subsequent real trap (I don't understand fully -PB)

			/* some of the following code uses

				/*

			 	 * it is really unimplemented, so restore the

			 	 * TIMEX extended unimplemented exception code

			/* some of the following code uses excptype, so

		/* handle exceptions other than the real UNIMPLIMENTED the

 For now use 'break', should technically be 'continue' */

	  /*

	   * In PA89, the underflow exception has been extended to encode

	   * additional information.  The exception looks like pp01x0,

	   * where x is 1 if inexact and pp represent the inexact bit (I)

	   * and the round away bit (RA)

 check for underflow trap enabled */

		    /*

		     * Isn't a real trap; we need to 

		     * return the default value.

		        /*

		         * If ra (round-away) is set, will 

		         * want to undo the rounding done

		         * by the hardware.

 now denormalize */

		    	/*

		    	 * If ra (round-away) is set, will 

		    	 * want to undo the rounding done

		    	 * by the hardware.

 now denormalize */

		    /* 

		     * Underflow can generate an inexact

		     * exception.  If inexact trap is enabled,

		     * want to do an inexact trap, otherwise 

		     * set inexact flag.

		    	/*

		    	 * Set exception field of exception register

		    	 * to inexact, parm field to zero.

			 * Underflow bit should be cleared.

		    	/*

		    	 * Exception register needs to be cleared.  

			 * Inexact flag needs to be set if inexact.

 check for overflow trap enabled */

			/*

			 * Isn't a real trap; we need to 

			 * return the default value.

			/* 

			 * Overflow always generates an inexact

			 * exception.  If inexact trap is enabled,

			 * want to do an inexact trap, otherwise 

			 * set inexact flag.

				/*

				 * Set exception field of exception

				 * register to inexact.  Overflow

				 * bit should be cleared.

				/*

				 * Exception register needs to be cleared.  

				 * Inexact flag needs to be set.

 no exception */

		/*

		 * Clear exception register in case 

		 * other fields are non-zero.

    /*

     * No real exceptions occurred.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/dfcmp.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	dbl_cmp: compare two values

 *

 *  External Interfaces:

 *	dbl_fcmp(leftptr, rightptr, cond, status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 * dbl_cmp: compare two values

 The predicate to be tested */

 Create local copies of the numbers */

    /*

     * Test for NaN

	/* Check if a NaN is involved.  Signal an invalid exception when 

	 * comparing a signaling NaN or when comparing quiet NaNs and the

	/* All the exceptional conditions are handled, now special case

 NaNs always compare unordered. */

 infinities will drop down to the normal compare mechanisms */

    /* First compare for unequal signs => less or greater or

        /* left negative => less, left positive => greater.

    /* Signs are the same.  Treat negative numbers separately

 Positive compare */

	    /* Equal first parts.  Now we must use unsigned compares to

        /* Negative compare.  Signed or unsigned compares

         * both work the same.  That distinction is only

	    /* Equal first parts.  Now we must use unsigned compares to

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/dfmpy.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Double Precision Floating-point Multiply

 *

 *  External Interfaces:

 *	dbl_fmpy(srcptr1,srcptr2,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Double Precision Floating-point Multiply

	/* 

	 * set sign bit of result 

	/*

	 * check first operand for NaN's or infinity

					/* 

					 * invalid since operands are infinity 

					 * and zero 

				/*

			 	 * return infinity

                	/*

                 	 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                	/*

                 	 * return quiet NaN

	/*

	 * check second operand for NaN's or infinity

 invalid since operands are zero & infinity */

			/*

			 * return infinity

                /*

                 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                /*

                 * return quiet NaN

	/*

	 * Generate exponent 

	/*

	 * Generate mantissa

 set hidden bit */

 check for zero */

 is denormalized, adjust exponent */

 opnd2 needs to have hidden bit set with msb in hidden bit */

 check for zero */

 is denormalized; want to normalize */

 Multiply two source mantissas together */

 make room for guard bits */

        /* 

         * Four bits at a time are inspected in each loop, and a 

         * simple shift and add multiply algorithm is used. 

 Twoword_add should be an ADDC followed by an ADD. */

 result mantissa >= 2. */

 check for denormalized result */

	/*

	 * check for guard, sticky and inexact bits 

 align result mantissa */

	/* 

	 * round result 

        /* 

         * Test for overflow

 trap if OVERFLOWTRAP enabled */

                        /*

                         * Adjust bias of result

 set result to infinity or largest number */

        /* 

         * Test for underflow

 trap if UNDERFLOWTRAP enabled */

                        /*

                         * Adjust bias of result

 Determine if should set underflow flag */

		/*

		 * denormalize result or set to signed zero

 return zero or smallest number */

 check for inexact */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/fcnvxf.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Single Fixed-point to Single Floating-point

 *	Single Fixed-point to Double Floating-point 

 *	Double Fixed-point to Single Floating-point 

 *	Double Fixed-point to Double Floating-point 

 *

 *  External Interfaces:

 *	dbl_to_dbl_fcnvxf(srcptr,nullptr,dstptr,status)

 *	dbl_to_sgl_fcnvxf(srcptr,nullptr,dstptr,status)

 *	sgl_to_dbl_fcnvxf(srcptr,nullptr,dstptr,status)

 *	sgl_to_sgl_fcnvxf(srcptr,nullptr,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Convert single fixed-point to single floating-point format

	/* 

	 * set sign bit of result and get magnitude of source 

 Check for zero */ 

	/*

	 * Generate exponent and normalized mantissa

 initialize for normalization */

	/*

	 * Check word for most significant bit set.  Returns

	 * a value in dst_exponent indicating the bit position,

	 * between -1 and 30.

  left justify source, with msb at bit position 1  */

 check for inexact */

/*

 *  Single Fixed-point to Double Floating-point 

	/* 

	 * set sign bit of result and get magnitude of source 

 Check for zero */

	/*

	 * Generate exponent and normalized mantissa

 initialize for normalization */

	/*

	 * Check word for most significant bit set.  Returns

	 * a value in dst_exponent indicating the bit position,

	 * between -1 and 30.

  left justify source, with msb at bit position 1  */

/*

 *  Double Fixed-point to Single Floating-point 

	/* 

	 * set sign bit of result and get magnitude of source 

 Check for zero */

	/*

	 * Generate exponent and normalized mantissa

 initialize for normalization */

		/*

		 * Check word for most significant bit set.  Returns

		 * a value in dst_exponent indicating the bit position,

		 * between -1 and 30.

  left justify source, with msb at bit position 1  */

		/*

		 *  since msb set is in second word, need to 

		 *  adjust bit position count

		/*

		 * Check word for most significant bit set.  Returns

		 * a value in dst_exponent indicating the bit position,

		 * between -1 and 30.

		 *

  left justify source, with msb at bit position 1  */

		/*

		 * If dst_exponent = 0, we don't need to shift anything.

		 * If dst_exponent = -1, src = - 2**63 so we won't need to 

		 * shift srcp2.

 check for inexact */

/*

 *  Double Fixed-point to Double Floating-point 

	/* 

	 * set sign bit of result and get magnitude of source 

 Check for zero */

	/*

	 * Generate exponent and normalized mantissa

 initialize for normalization */

		/*

		 * Check word for most significant bit set.  Returns

		 * a value in dst_exponent indicating the bit position,

		 * between -1 and 30.

  left justify source, with msb at bit position 1  */

		/*

		 *  since msb set is in second word, need to 

		 *  adjust bit position count

		/*

		 * Check word for most significant bit set.  Returns

		 * a value in dst_exponent indicating the bit position,

		 * between -1 and 30.

  left justify source, with msb at bit position 1  */

		/*

		 * If dst_exponent = 0, we don't need to shift anything.

		 * If dst_exponent = -1, src = - 2**63 so we won't need to 

		 * shift srcp2.

 check for inexact */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/sfsub.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Single_subtract: subtract two single precision values.

 *

 *  External Interfaces:

 *	sgl_fsub(leftptr, rightptr, dstptr, status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 * Single_subtract: subtract two single precision values.

 Create local copies of the numbers */

    /* A zero "save" helps discover equal operands (for later),  *

to*/save);

    /*

     * check first operand for NaN's or infinity

		    /* 

		     * invalid since operands are same signed infinity's

		/*

	 	 * return infinity

            /*

             * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	    /* 

	     * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	    /*

 	     * return quiet NaN

 End left NaN or Infinity processing */

    /*

     * check second operand for NaN's or infinity

 return infinity */

        /*

         * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	/*

	 * return quiet NaN

 End right NaN or Infinity processing */

 Invariant: Must be dealing with finite numbers */

 Compare operands by removing the sign */

 sign difference selects sub or add operation. */

	/* Set the left operand to the larger one by XOR swap *

to*/right);

to*/left);

 Invariant:  left is not smaller than right. */ 

 Denormalized operands.  First look for zeroes */

 right is zero */

 Both operands are zeros */

with*/right);

with*/right);

		/* Left is not a zero and must be the result.  Trapped

		 * underflows are signaled if left is denormalized.  Result

 need to normalize results mantissa */

using*/sign_save);

 inexact = FALSE */

 Neither are zeroes */

 Exponent is already cleared */

	    /* Both operands are denormalized.  The result must be exact

	     * and is simply calculated.  A sum could become normalized and a

signed*/int) save >= 0 )

minus*/right,
into*/result);

 need to normalize result */

using*/sign_save);

 inexact = FALSE */

	right_exponent = 1;	/* Set exponent to reflect different bias

    /* 

     * Special case alignment of operands that would force alignment 

     * beyond the extent of the extension.  A further optimization

     * could special case this but only reduces the path length for this

     * infrequent case.

 Align right operand by shifting to right */

operand*/right,
and lower to*/extent);

 Treat sum and difference of the operands separately. */

signed*/int) save >= 0 )

	/*

	 * Difference of the two operands.  Their can be no overflow.  A

	 * borrow can occur out of the hidden bit and force a post

	 * normalization phase.

minus*/right,into*/result);

 Handle normalization */

	    /* A straightforward algorithm would now shift the result

	     * and extension left until the hidden bit becomes one.  Not

	     * all of the extension bits need participate in the shift.

	     * Only the two most significant bits (round and guard) are

	     * needed.  If only a single shift is needed then the guard

	     * bit becomes a significant low order bit and the extension

	     * must participate in the rounding.  If more than a single 

	     * shift is needed, then all bits to the right of the guard 

            /* Need to check for a zero result.  The sign and exponent

	     * fields have already been zeroed.  The more efficient test

	     * of the full object can be used.

 Must have been "x-x" or "x+(-x)". */

 Look to see if normalization is finished. */

		    /* Denormalized, exponent should be zero.  Left operand *

 No further normalization is needed. */

using*/sign_save);

	    /* Check for denormalized, exponent should be zero.  Left    *

	    /* Shift extension to complete one bit of normalization and

	    /* Discover first one bit to determine shift amount.  Use a

	     * modified binary search.  We have already shifted the result

	     * one position right and still not found a one so the remainder

 Scan bytes */

 Now narrow it down to the nibble */

 The lower nibble contains the normalizing one */

	    /* Select case were first bit is set (already normalized)

 Already normalized */

using*/sign_save);

using*/result_exponent);

using*/sign_save);

using*/result_exponent);

 Sign bit is already set */

 Fixup potential underflows */

 inexact = FALSE */

	    /*

	     * Since we cannot get an inexact denormalized result,

	     * we can now return.

by*/(1-result_exponent),extent);

 end if(hidden...)... */

 Fall through and round */

 end if(save >= 0)... */

 Add magnitudes */

to*/result);

 Prenormalization required. */

 end if hiddenoverflow... */

 end else ...sub magnitudes... */

    /* Round the result.  If the extension is all zeros,then the result is

     * exact.  Otherwise round in the correct direction.  No underflow is

     * possible. If a postnormalization is necessary, then the mantissa is

 The default. */

 at least 1/2 ulp */

 either exactly half way and odd or more than 1/2ulp */

 Round up positive results */

 Round down negative results */

 truncate is simple */

 end switch... */

 Overflow */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/sfsqrt.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Single Floating-point Square Root

 *

 *  External Interfaces:

 *	sgl_fsqrt(srcptr,nullptr,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Single Floating-point Square Root

ARGSUSED*/

        /*

         * check source operand for NaN or infinity

                /*

                 * is signaling NaN?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                /*

                 * Return quiet NaN or positive infinity.

		 *  Fall through to negative test if negative infinity.

        /*

         * check for zero source operand

        /*

         * check for negative source operand 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	/*

	 * Generate result

 normalize operand */

 exponent is even */

 Add comment here.  Explain why odd exponent needs correction */

	/*

	 * Add comment here.  Explain following algorithm.

	 * 

	 * Trust me, it works.

	 *

 update result */

 correct exponent for pre-shift */

 check for inexact */

  now round result  */

		     /* stickybit is always true, so guardbit 

 increment result exponent by 1 if mantissa overflowed */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/fcnvuf.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Fixed point to Floating-point Converts

 *

 *  External Interfaces:

 *	dbl_to_dbl_fcnvuf(srcptr,nullptr,dstptr,status)

 *	dbl_to_sgl_fcnvuf(srcptr,nullptr,dstptr,status)

 *	sgl_to_dbl_fcnvuf(srcptr,nullptr,dstptr,status)

 *	sgl_to_sgl_fcnvuf(srcptr,nullptr,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/************************************************************************

 *  Fixed point to Floating-point Converts				*

/*

 *  Convert Single Unsigned Fixed to Single Floating-point format

 Check for zero */ 

	/*

	 * Generate exponent and normalized mantissa

 initialize for normalization */

	/*

	 * Check word for most significant bit set.  Returns

	 * a value in dst_exponent indicating the bit position,

	 * between -1 and 30.

  left justify source, with msb at bit position 0  */

 check for inexact */

 never negative */

/*

 *  Single Unsigned Fixed to Double Floating-point 

 Check for zero */

	/*

	 * Generate exponent and normalized mantissa

 initialize for normalization */

	/*

	 * Check word for most significant bit set.  Returns

	 * a value in dst_exponent indicating the bit position,

	 * between -1 and 30.

  left justify source, with msb at bit position 0  */

/*

 *  Double Unsigned Fixed to Single Floating-point 

 Check for zero */

	/*

	 * Generate exponent and normalized mantissa

 initialize for normalization */

		/*

		 * Check word for most significant bit set.  Returns

		 * a value in dst_exponent indicating the bit position,

		 * between -1 and 30.

  left justify source, with msb at bit position 0  */

		/*

		 *  since msb set is in second word, need to 

		 *  adjust bit position count

		/*

		 * Check word for most significant bit set.  Returns

		 * a value in dst_exponent indicating the bit position,

		 * between -1 and 30.

		 *

  left justify source, with msb at bit position 0  */

 check for inexact */

 never negative */

/*

 *  Double Unsigned Fixed to Double Floating-point 

 Check for zero */

	/*

	 * Generate exponent and normalized mantissa

 initialize for normalization */

		/*

		 * Check word for most significant bit set.  Returns

		 * a value in dst_exponent indicating the bit position,

		 * between -1 and 30.

  left justify source, with msb at bit position 0  */

		/*

		 *  since msb set is in second word, need to 

		 *  adjust bit position count

		/*

		 * Check word for most significant bit set.  Returns

		 * a value in dst_exponent indicating the bit position,

		 * between -1 and 30.

  left justify source, with msb at bit position 0  */

 check for inexact */

 never negative */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/fp/fpudispatch.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	<<please update with a synopsis of the functionality provided by this file>>

 *

 *  External Interfaces:

 *	<<the following list was autogenerated, please review>>

 *	emfpudispatch(ir, dummy1, dummy2, fpregs)

 *	fpudispatch(ir, excp_code, holder, fpregs)

 *

 *  Internal Interfaces:

 *	<<the following list was autogenerated, please review>>

 *	static u_int decode_06(u_int, u_int *)

 *	static u_int decode_0c(u_int, u_int, u_int, u_int *)

 *	static u_int decode_0e(u_int, u_int, u_int, u_int *)

 *	static u_int decode_26(u_int, u_int *)

 *	static u_int decode_2e(u_int, u_int *)

 *	static void update_status_cbit(u_int *, u_int, u_int, u_int)

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

 #include <sys/debug.h> */

 #include <machine/sys/mdep_private.h> */

/*

 * definition of extru macro.  If pos and len are constants, the compiler

 * will generate an extru instruction when optimized

 definitions of bit field locations in the instruction */

/*

 * the following are the extra bits for the 0E major op

/*

 * the following are for the multi-ops

/*

 * the following are for the fused FP instructions

 fprm1pos 10 */

 fprm2pos 15 */

 fpfmtpos 20 */

 fpxtpos 25 */

 fptpos	31 */

/*

 * offset to constant zero in the FP emulation registers

/*

 * extract the major opcode from the instruction

/*

 * extract the two bit class field from the FP instruction. The class is at bit

 * positions 21-22

/*

 * extract the 3 bit subop field.  For all but class 1 instructions, it is

 * located at bit positions 16-18

/*

 * extract the 2 or 3 bit subop field from class 1 instructions.  It is located

 * at bit positions 15-16 (PA1.1) or 14-16 (PA2.0)

 PA89 (1.1) fmt */

 PA 2.0 fmt */

 definitions of unimplemented exceptions */

/*

 * Special Defines for TIMEX specific code

/*

 * Static function definitions

 !_PROTOTYPES&&!_lint */

 _PROTOTYPES&&!_lint */

	/* on pa-linux the fpu type is not filled in by the

	 * caller; it is constructed here  

/*

 * this routine will decode the excepting floating point instruction and

 * call the appropriate emulation routine.

 * It is called by decode_fpu with the following parameters:

 * fpudispatch(current_ir, unimplemented_code, 0, &Fpu_register)

 * where current_ir is the instruction to be emulated,

 * unimplemented_code is the exception_code that the hardware generated

 * and &Fpu_register is the address of emulated FP reg 0.

 All FP emulation code assumes that ints are 4-bytes in length */

 get fpu type flags */

			/* "crashme Night Gallery painting nr 2. (asm_crash.s).

			 * This was fixed for multi-user kernels, but

			 * workstation kernels had a panic here.  This allowed

			 * any arbitrary user to panic the kernel by executing

			 * setting the FP exception registers to strange values

			 * and generating an emulation trap.  The emulation and

			 * exception code must never be able to panic the

			 * kernel.

/*

 * this routine is called by $emulation_trap to emulate a coprocessor

 * instruction if one doesn't exist

 All FP emulation code assumes that ints are 4-bytes in length */

 get fpu type flags */

 operand register offsets */ 

 also sf for class 1 conversions */

 for class 1 conversions */

 fp status register */

 and local copy */

 map fr0 source to constant zero */

 don't allow fr0 as a dest */

 get fmt completer */

 COPR 0,0 emulated above*/

 FCPY */

 illegal */

 quad */

 force to even reg #s */

 double */

 single */

 FABS */

 illegal */

 quad */

 force to even reg #s */

 double */

 single */

 copy and clear sign bit */

 FNEG */

 illegal */

 quad */

 force to even reg #s */

 double */

 single */

 copy and invert sign bit */

 FNEGABS */

 illegal */

 quad */

 force to even reg #s */

 double */

 single */

 copy and set sign bit */

 FSQRT */

 quad not implemented */

 FRND */

 quad not implemented */

 end of switch (subop) */

 class 1 */

 get dest format */

			/*

			 * fmt's 2 and 3 are illegal of not implemented

			 * quad conversions

		/*

		 * encode source and dest formats into 2 bits.

		 * high bit is source, low bit is dest.

		 * bit = 1 --> double precision

 FCNVFF */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 FCNVXF */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 FCNVFX */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 FCNVFXT */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 FCNVUF (PA2.0 only) */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 FCNVFU (PA2.0 only) */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 FCNVFUT (PA2.0 only) */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 undefined */

 end of switch subop */

 class 2 */

 FTEST if nullify bit set, otherwise FCMP */

 FTEST */

					/*

					 * arg0 is not used

					 * second param is the t field used for

					 * ftest,acc and ftest,rej

					 * third param is the subop (y-field)

					/* Unsupported

					 * return(ftest(0L,extru(ir,fptpos,5),

					 *	 &fpregs[0],subop));

 FCMP */

 illegal */

 quad not implemented */

 end of if for PA2.0 */

 PA1.0 & PA1.1 */

 FCMP */

 illegal */

 quad not implemented */

 FTEST */

					/*

					 * arg0 is not used

					 * second param is the t field used for

					 * ftest,acc and ftest,rej

					 * third param is the subop (y-field)

					/* unsupported

					 * return(ftest(0L,extru(ir,fptpos,5),

					 *     &fpregs[0],subop));

 end of switch subop */

 end of else for PA1.0 & PA1.1 */

 class 3 */

 FADD */

 illegal */

 quad not implemented */

 FSUB */

 illegal */

 quad not implemented */

 FMPY */

 illegal */

 quad not implemented */

 FDIV */

 illegal */

 quad not implemented */

 FREM */

 illegal */

 quad not implemented */

 end of class 3 switch */

 end of switch(class) */

 If we get here, something is really wrong! */

 operand register offsets */

 also sf for class 1 conversions */

 dest format for class 1 conversions */

 class 0 or 1 has 2 bit fmt */

 class 2 and 3 have 1 bit fmt */

	/*

	 * An undefined combination, double precision accessing the

	 * right half of a FPR, can get us into trouble.  

	 * Let's just force proper alignment on it.

 unimplemented */

 FCPY */

 double */

 single */

 FABS */

 double */

 single */

 FNEG */

 double */

 single */

 FNEGABS */

 double */

 single */

 FSQRT */

 FRMD */

 end of switch (subop */

 class 1 */

 get dest format */

		/*

		 * Fix Crashme problem (writing to 31R in double precision)

		 * here too.

 FCNVFF */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 FCNVXF */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 FCNVFX */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 FCNVFXT */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 FCNVUF (PA2.0 only) */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 FCNVFU (PA2.0 only) */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 FCNVFUT (PA2.0 only) */

 sgl/sgl */

 sgl/dbl */

 dbl/sgl */

 dbl/dbl */

 undefined */

 end of switch subop */

 class 2 */

		/*

		 * Be careful out there.

		 * Crashme can generate cases where FR31R is specified

		 * as the source or target of a double precision operation.

		 * Since we just pass the address of the floating-point

		 * register to the emulation routines, this can cause

		 * corruption of fpzeroreg.

 FTEST if nullify bit set, otherwise FCMP */

 FTEST */

 not legal */

 FCMP */

				    /*

				     * fmt is only 1 bit long

 end of if for PA2.0 */

 PA1.0 & PA1.1 */

 FCMP */

				    /*

				     * fmt is only 1 bit long

 end of switch subop */

 end of else for PA1.0 & PA1.1 */

 class 3 */

		/*

		 * Be careful out there.

		 * Crashme can generate cases where FR31R is specified

		 * as the source or target of a double precision operation.

		 * Since we just pass the address of the floating-point

		 * register to the emulation routines, this can cause

		 * corruption of fpzeroreg.

			/*

 FADD */

 FSUB */

 FMPY or XMPYU */

				/*

				 * check for integer multiply (x bit set)

				    /*

				     * emulate XMPYU

					    /*

					     * bad instruction if t specifies

					     * the right half of a register

					    /* unsupported

					     * impyu(&fpregs[r1],&fpregs[r2],

						 * &fpregs[t]);

 FMPY */

 FDIV */

 FREM */

 end of class 3 switch */

 end of switch(class) */

 If we get here, something is really wrong! */

/*

 * routine to decode the 06 (FMPYADD and FMPYCFXT) instruction

 operands */

 use a local copy of status reg */

 get fpu type flags */

 get sgl/dbl flag */

 DBL */

 special case FMPYCFXT, see sgl case below */

 copy results */

 SGL */

		/*

		 * calculate offsets for single precision numbers

		 * See table 6-14 in PA-89 architecture for mapping

 get offset */

 add right word offset */

 get offset */

 add right word offset */

 get offset */

 add right word offset */

 get offset */

 add right word offset */

 get offset */

 add right word offset */

			/* special case FMPYCFXT (really 0)

			  * This instruction is only present on the Timex and

			  * Rolex fpu's in so if it is the special case and

			  * one of these fpu's we run the FMPYCFXT instruction

 copy results */

/*

 * routine to decode the 26 (FMPYSUB) instruction

 operands */

 get sgl/dbl flag */

 DBL */

 copy results */

 SGL */

		/*

		 * calculate offsets for single precision numbers

		 * See table 6-14 in PA-89 architecture for mapping

 get offset */

 add right word offset */

 get offset */

 add right word offset */

 get offset */

 add right word offset */

 get offset */

 add right word offset */

 get offset */

 add right word offset */

 copy results */

/*

 * routine to decode the 2E (FMPYFADD,FMPYNFADD) instructions

 operands */

 get fmt completer */

 DBL */

 fmpyfadd or fmpynfadd? */

 end DBL */

 SGL */

 fmpyfadd or fmpynfadd? */

 end SGL */

/*

 * update_status_cbit

 *

 *	This routine returns the correct FP status register value in

 *	*status, based on the C-bit & V-bit returned by the FCMP

 *	emulation routine in new_status.  The architecture type

 *	(PA83, PA89 or PA2.0) is available in fpu_type.  The y_field

 *	and the architecture type are used to determine what flavor

 *	of FCMP is being emulated.

	/*

	 * For PA89 FPU's which implement the Compare Queue and

	 * for PA2.0 FPU's, update the Compare Queue if the y-field = 0,

	 * otherwise update the specified bit in the Compare Array.

	 * Note that the y-field will always be 0 for non-PA2.0 FPU's.

 old Cbit */

 old CQ   */

 all other bits*/

 old Cbit */

 other bits */

 if PA83, just update the C-bit */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/fcnvfxt.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Single Floating-point to Single Fixed-point /w truncated result

 *	Single Floating-point to Double Fixed-point /w truncated result

 *	Double Floating-point to Single Fixed-point /w truncated result

 *	Double Floating-point to Double Fixed-point /w truncated result

 *

 *  External Interfaces:

 *	dbl_to_dbl_fcnvfxt(srcptr,nullptr,dstptr,status)

 *	dbl_to_sgl_fcnvfxt(srcptr,nullptr,dstptr,status)

 *	sgl_to_dbl_fcnvfxt(srcptr,nullptr,dstptr,status)

 *	sgl_to_sgl_fcnvfxt(srcptr,nullptr,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Convert single floating-point to single fixed-point format

 *  with truncated result

ARGSUSED*/

	/* 

	 * Test for overflow

 check for MININT */

	/*

	 * Generate result

 check for inexact */

 check for inexact */

/*

 *  Single Floating-point to Double Fixed-point 

ARGSUSED*/

	/* 

	 * Test for overflow

 check for MININT */

	/*

	 * Generate result

 check for inexact */

 check for inexact */

/*

 *  Double Floating-point to Single Fixed-point 

ARGSUSED*/

	/* 

	 * Test for overflow

 check for MININT */

	/*

	 * Generate result

 check for inexact */

 check for inexact */

/*

 *  Double Floating-point to Double Fixed-point 

ARGSUSED*/

	/* 

	 * Test for overflow

 check for MININT */

	/*

	 * Generate result

 check for inexact */

 check for inexact */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/dfsqrt.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Double Floating-point Square Root

 *

 *  External Interfaces:

 *	dbl_fsqrt(srcptr,nullptr,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Double Floating-point Square Root

ARGSUSED*/

        /*

         * check source operand for NaN or infinity

                /*

                 * is signaling NaN?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                /*

                 * Return quiet NaN or positive infinity.

		 *  Fall through to negative test if negative infinity.

        /*

         * check for zero source operand

        /*

         * check for negative source operand 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	/*

	 * Generate result

 normalize operand */

 exponent is even */

 Add comment here.  Explain why odd exponent needs correction */

	/*

	 * Add comment here.  Explain following algorithm.

	 * 

	 * Trust me, it works.

	 *

 update result */

 correct exponent for pre-shift */

 check for inexact */

  now round result  */

		     /* stickybit is always true, so guardbit 

 increment result exponent by 1 if mantissa overflowed */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/fcnvfut.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Floating-point to Unsigned Fixed-point Converts with Truncation

 *

 *  External Interfaces:

 *	dbl_to_dbl_fcnvfut(srcptr,nullptr,dstptr,status)

 *	dbl_to_sgl_fcnvfut(srcptr,nullptr,dstptr,status)

 *	sgl_to_dbl_fcnvfut(srcptr,nullptr,dstptr,status)

 *	sgl_to_sgl_fcnvfut(srcptr,nullptr,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/************************************************************************

 *  Floating-point to Unsigned Fixed-point Converts with Truncation	*

/*

 *  Convert single floating-point to single fixed-point format

 *  with truncated result

ARGSUSED*/

	/* 

	 * Test for overflow

	/*

	 * Generate result

		/* 

		 * Check sign.

		 * If negative, trap unimplemented.

 check for inexact */

 check for inexact */

/*

 *  Single Floating-point to Double Unsigned Fixed 

ARGSUSED*/

	/* 

	 * Test for overflow

	/*

	 * Generate result

		/* 

		 * Check sign.

		 * If negative, trap unimplemented.

 check for inexact */

 check for inexact */

/*

 *  Double Floating-point to Single Unsigned Fixed 

ARGSUSED*/

	/* 

	 * Test for overflow

	/*

	 * Generate result

		/* 

		 * Check sign.

		 * If negative, trap unimplemented.

 check for inexact */

 check for inexact */

/*

 *  Double Floating-point to Double Unsigned Fixed 

ARGSUSED*/

	/* 

	 * Test for overflow

	/*

	 * Generate result

		/* 

		 * Check sign.

		 * If negative, trap unimplemented.

 check for inexact */

 check for inexact */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  Purpose:

 *	Single Floating-point Round to Integer

 *	Double Floating-point Round to Integer

 *	Quad Floating-point Round to Integer (returns unimplemented)

 *

 *  External Interfaces:

 *	dbl_frnd(srcptr,nullptr,dstptr,status)

 *	sgl_frnd(srcptr,nullptr,dstptr,status)

 *

 * END_DESC

/*

 *  Single Floating-point Round to Integer

ARGSUSED*/

        /*

         * check source operand for NaN or infinity

                /*

                 * is signaling NaN?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                /*

                 * return quiet NaN or infinity

	/* 

	 * Need to round?

	/*

	 * Generate result

 check for inexact */

  round result  */

 set sign */

 check for inexact */

  round result  */

/*

 *  Double Floating-point Round to Integer

ARGSUSED*/

        /*

         * check source operand for NaN or infinity

                /*

                 * is signaling NaN?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                /*

                 * return quiet NaN or infinity

	/* 

	 * Need to round?

	/*

	 * Generate result

 check for inexact */

  round result  */

 set sign */

 check for inexact */

  round result  */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/fp/denormal.c		$ Revision: $

 *

 *  Purpose:

 *	<<please update with a synopsis of the functionality provided by this file>>

 *

 *  External Interfaces:

 *	<<the following list was autogenerated, please review>>

 *	dbl_denormalize(dbl_opndp1,dbl_opndp2,inexactflag,rmode)

 *	sgl_denormalize(sgl_opnd,inexactflag,rmode)

 *

 *  Internal Interfaces:

 *	<<please update>>

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

 #include <machine/sys/mdep_private.h> */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/fcnvfx.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Single Floating-point to Single Fixed-point

 *	Single Floating-point to Double Fixed-point 

 *	Double Floating-point to Single Fixed-point 

 *	Double Floating-point to Double Fixed-point 

 *

 *  External Interfaces:

 *	dbl_to_dbl_fcnvfx(srcptr,nullptr,dstptr,status)

 *	dbl_to_sgl_fcnvfx(srcptr,nullptr,dstptr,status)

 *	sgl_to_dbl_fcnvfx(srcptr,nullptr,dstptr,status)

 *	sgl_to_sgl_fcnvfx(srcptr,nullptr,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Single Floating-point to Single Fixed-point 

ARGSUSED*/

	/* 

	 * Test for overflow

 check for MININT */

	/*

	 * Generate result

 check for inexact */

  round result  */

 check for inexact */

  round result  */

/*

 *  Single Floating-point to Double Fixed-point 

ARGSUSED*/

	/* 

	 * Test for overflow

 check for MININT */

	/*

	 * Generate result

 check for inexact */

  round result  */

 check for inexact */

  round result  */

/*

 *  Double Floating-point to Single Fixed-point 

ARGSUSED*/

	/* 

	 * Test for overflow

 check for MININT */

	/*

	 * Generate result

 check for inexact */

  round result  */

 check for overflow */

 check for inexact */

  round result  */

/*

 *  Double Floating-point to Double Fixed-point 

ARGSUSED*/

	/* 

	 * Test for overflow

 check for MININT */

	/*

	 * Generate result

 check for inexact */

  round result  */

 check for inexact */

  round result  */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/dfadd.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Double_add: add two double precision values.

 *

 *  External Interfaces:

 *	dbl_fadd(leftptr, rightptr, dstptr, status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 * Double_add: add two double precision values.

 Create local copies of the numbers */

    /* A zero "save" helps discover equal operands (for later),  *

to*/save);

    /*

     * check first operand for NaN's or infinity

		    /* 

		     * invalid since operands are opposite signed infinity's

		/*

	 	 * return infinity

            /*

             * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	    /* 

	     * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	    /*

 	     * return quiet NaN

 End left NaN or Infinity processing */

    /*

     * check second operand for NaN's or infinity

 return infinity */

        /*

         * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	/*

	 * return quiet NaN

 End right NaN or Infinity processing */

 Invariant: Must be dealing with finite numbers */

 Compare operands by removing the sign */

 sign difference selects add or sub operation. */

	/* Set the left operand to the larger one by XOR swap *

to*/rightp1);

to*/leftp1);

 Invariant:  left is not smaller than right. */ 

 Denormalized operands.  First look for zeroes */

 right is zero */

 Both operands are zeros */

with*/rightp1);

with*/rightp1);

		/* Left is not a zero and must be the result.  Trapped

		 * underflows are signaled if left is denormalized.  Result

 need to normalize results mantissa */

using*/sign_save);

 inexact = FALSE */

 Neither are zeroes */

 Exponent is already cleared */

	    /* Both operands are denormalized.  The result must be exact

	     * and is simply calculated.  A sum could become normalized and a

signed*/int) save < 0 )

minus*/rightp1,rightp2,

into*/resultp1,resultp2);

into*/resultp1,resultp2);

 need to normalize result */

using*/sign_save);

 inexact = FALSE */

	right_exponent = 1;	/* Set exponent to reflect different bias

    /* 

     * Special case alignment of operands that would force alignment 

     * beyond the extent of the extension.  A further optimization

     * could special case this but only reduces the path length for this

     * infrequent case.

 Align right operand by shifting to right */

operand*/rightp1,rightp2,
and lower to*/extent);

 Treat sum and difference of the operands separately. */

signed*/int) save < 0 )

	/*

	 * Difference of the two operands.  Their can be no overflow.  A

	 * borrow can occur out of the hidden bit and force a post

	 * normalization phase.

minus*/rightp1,rightp2,

with*/extent,
 Handle normalization */

	    /* A straight forward algorithm would now shift the result

	     * and extension left until the hidden bit becomes one.  Not

	     * all of the extension bits need participate in the shift.

	     * Only the two most significant bits (round and guard) are

	     * needed.  If only a single shift is needed then the guard

	     * bit becomes a significant low order bit and the extension

	     * must participate in the rounding.  If more than a single 

	     * shift is needed, then all bits to the right of the guard 

            /* Need to check for a zero result.  The sign and exponent

	     * fields have already been zeroed.  The more efficient test

	     * of the full object can be used.

 Must have been "x-x" or "x+(-x)". */

 Look to see if normalization is finished. */

		    /* Denormalized, exponent should be zero.  Left operand *

 No further normalization is needed. */

using*/sign_save);

	    /* Check for denormalized, exponent should be zero.  Left    *

	    /* Shift extension to complete one bit of normalization and

	    /* Discover first one bit to determine shift amount.  Use a

	     * modified binary search.  We have already shifted the result

	     * one position right and still not found a one so the remainder

 Scan bytes */

 Now narrow it down to the nibble */

 The lower nibble contains the normalizing one */

	    /* Select case were first bit is set (already normalized)

 Already normalized */

using*/sign_save);

using*/result_exponent);

using*/sign_save);

using*/result_exponent);

 Sign bit is already set */

 Fixup potential underflows */

 inexact = FALSE */

	    /* 

	     * Since we cannot get an inexact denormalized result,

	     * we can now return.

 end if(hidden...)... */

 Fall through and round */

 end if(save < 0)... */

 Add magnitudes */

to*/resultp1,resultp2);

 Prenormalization required. */

 end if hiddenoverflow... */

 end else ...add magnitudes... */

    /* Round the result.  If the extension is all zeros,then the result is

     * exact.  Otherwise round in the correct direction.  No underflow is

     * possible. If a postnormalization is necessary, then the mantissa is

 The default. */

 at least 1/2 ulp */

 either exactly half way and odd or more than 1/2ulp */

 Round up positive results */

 Round down negative results */

 truncate is simple */

 end switch... */

 Overflow */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/sfdiv.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Single Precision Floating-point Divide

 *

 *  External Interfaces:

 *	sgl_fdiv(srcptr1,srcptr2,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Single Precision Floating-point Divide

	/* 

	 * set sign bit of result 

	/*

	 * check first operand for NaN's or infinity

					/* 

					 * invalid since both operands 

					 * are infinity 

				/*

			 	 * return infinity

                	/*

                 	 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                	/*

                 	 * return quiet NaN

	/*

	 * check second operand for NaN's or infinity

			/*

			 * return zero

                /*

                 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                /*

                 * return quiet NaN

	/*

	 * check for division by zero

 invalid since both operands are zero */

	/*

	 * Generate exponent 

	/*

	 * Generate mantissa

 set hidden bit */

 check for zero */

 is denormalized; want to normalize */

 opnd2 needs to have hidden bit set with msb in hidden bit */

 is denormalized; want to normalize */

 Divide the source mantissas */

	/*

	 * A non_restoring divide algorithm is used.

 need to get one more bit of result */

	/* 

	 * round result 

        /* 

         * Test for overflow

 trap if OVERFLOWTRAP enabled */

                        /*

                         * Adjust bias of result

 set result to infinity or largest number */

        /* 

         * Test for underflow

 trap if UNDERFLOWTRAP enabled */

                        /*

                         * Adjust bias of result

 Determine if should set underflow flag */

                /*

                 * denormalize result or set to signed zero

 return rounded number */ 

 check for inexact */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/fcnvff.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Single Floating-point to Double Floating-point

 *	Double Floating-point to Single Floating-point

 *

 *  External Interfaces:

 *	dbl_to_sgl_fcnvff(srcptr,nullptr,dstptr,status)

 *	sgl_to_dbl_fcnvff(srcptr,nullptr,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Single Floating-point to Double Floating-point 

ARGSUSED*/

 set sign of result */

	/* 

 	 * Test for NaN or infinity

		/*

		 * determine if NaN or infinity

			/*

			 * is infinity; want to return double infinity

			/* 

			 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * NaN is quiet, return as double NaN 

	/* 

 	 * Test for zero or denormalized

		/*

		 * determine if zero or denormalized

			/*

			 * is denormalized; want to normalize

	/*

	 * No special cases, just complete the conversion

/*

 *  Double Floating-point to Single Floating-point 

ARGSUSED*/

 set sign of result */

        /* 

         * Test for NaN or infinity

                /*

                 * determine if NaN or infinity

                        /*

                         * is infinity; want to return single infinity

                /* 

                 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                /* 

                 * NaN is quiet, return as single NaN 

        /*

         * Generate result

			/* compute result, determine inexact info,

			 * and set Underflowflag if appropriate

        /* 

         * Now round result if not exact

        /*

         * check for mantissa overflow after rounding

        /* 

         * Test for overflow

 trap if OVERFLOWTRAP enabled */

                        /* 

                         * Check for gross overflow

                        /*

                         * Adjust bias of result

 set result to infinity or largest number */

        /* 

         * Test for underflow

 trap if UNDERFLOWTRAP enabled */

                        /* 

                         * Check for gross underflow

                        /*

                         * Adjust bias of result

                 /* 

                  * result is denormalized or signed zero

        /* 

         * Trap if inexact trap is enabled

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/sfrem.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Single Precision Floating-point Remainder

 *

 *  External Interfaces:

 *	sgl_frem(srcptr1,srcptr2,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Single Precision Floating-point Remainder

	/*

	 * check first operand for NaN's or infinity

 invalid since first operand is infinity */

                	/*

                 	 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                	/*

                 	 * return quiet NaN

	/*

	 * check second operand for NaN's or infinity

			/*

			 * return first operand

                /*

                 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                /*

                 * return quiet NaN

	/*

	 * check second operand for zero

 invalid since second operand is zero */

	/* 

	 * get sign of result

	/* 

	 * check for denormalized operands

 check for zero */

 normalize, then continue */

 normalize, then continue */

 find result exponent and divide step loop count */

	/*

	 * check for opnd1/opnd2 < 1

		/*

		 * check for opnd1/opnd2 > 1/2

		 *

		 * In this case n will round to 1, so 

		 *    r = opnd1 - opnd2 

 set sign */

 align opnd2 with opnd1 */

 now normalize */

		/*

		 * opnd1/opnd2 <= 1/2

		 *

		 * In this case n will round to zero, so 

		 *    r = opnd1

	/*

	 * Generate result

	 *

	 * Do iterative subtract until remainder is less than operand 2.

	/*

	 * Do last subtract, then determine which way to round if remainder 

	 * is exactly 1/2 of opnd2 

 division is exact, remainder is zero */

	/* 

	 * Check for cases where opnd1/opnd2 < n 

	 *

	 * In this case the result's sign will be opposite that of

	 * opnd1.  The mantissa also needs some correction.

 check for remainder being exactly 1/2 of opnd2 */

 normalize result's mantissa */

        /* 

         * Test for underflow

 trap if UNDERFLOWTRAP enabled */

                        /*

                         * Adjust bias of result

 frem is always exact */

                /*

                 * denormalize result or set to signed zero

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/dfdiv.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Double Precision Floating-point Divide

 *

 *  External Interfaces:

 *	dbl_fdiv(srcptr1,srcptr2,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Double Precision Floating-point Divide

	/* 

	 * set sign bit of result 

	/*

	 * check first operand for NaN's or infinity

					/* 

					 * invalid since both operands 

					 * are infinity 

				/*

			 	 * return infinity

                	/*

                 	 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                	/*

                 	 * return quiet NaN

	/*

	 * check second operand for NaN's or infinity

			/*

			 * return zero

                /*

                 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                /*

                 * return quiet NaN

        /*

         * check for division by zero

 invalid since both operands are zero */

	/*

	 * Generate exponent 

	/*

	 * Generate mantissa

 set hidden bit */

 check for zero */

 is denormalized, want to normalize */

 opnd2 needs to have hidden bit set with msb in hidden bit */

 is denormalized; want to normalize */

 Divide the source mantissas */

	/* 

	 * A non-restoring divide algorithm is used.

 need to get one more bit of result */

	/* 

	 * round result 

        /* 

         * Test for overflow

 trap if OVERFLOWTRAP enabled */

                        /*

                         * Adjust bias of result

 set result to infinity or largest number */

        /* 

         * Test for underflow

 trap if UNDERFLOWTRAP enabled */

                        /*

                         * Adjust bias of result

 Determine if should set underflow flag */

                /*

                 * denormalize result or set to signed zero

 return rounded number */ 

 check for inexact */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/dfrem.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Double Precision Floating-point Remainder

 *

 *  External Interfaces:

 *	dbl_frem(srcptr1,srcptr2,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Double Precision Floating-point Remainder

	/*

	 * check first operand for NaN's or infinity

 invalid since first operand is infinity */

                	/*

                 	 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                	/*

                 	 * return quiet NaN

	/*

	 * check second operand for NaN's or infinity

			/*

			 * return first operand

                /*

                 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                /*

                 * return quiet NaN

	/*

	 * check second operand for zero

 invalid since second operand is zero */

	/* 

	 * get sign of result

	/* 

	 * check for denormalized operands

 check for zero */

 normalize, then continue */

 normalize, then continue */

 find result exponent and divide step loop count */

	/*

	 * check for opnd1/opnd2 < 1

		/*

		 * check for opnd1/opnd2 > 1/2

		 *

		 * In this case n will round to 1, so 

		 *    r = opnd1 - opnd2 

 set sign */

 align opnd2 with opnd1 */

 now normalize */

		/*

		 * opnd1/opnd2 <= 1/2

		 *

		 * In this case n will round to zero, so 

		 *    r = opnd1

	/*

	 * Generate result

	 *

	 * Do iterative subtract until remainder is less than operand 2.

	/*

	 * Do last subtract, then determine which way to round if remainder 

	 * is exactly 1/2 of opnd2 

 division is exact, remainder is zero */

	/* 

	 * Check for cases where opnd1/opnd2 < n 

	 *

	 * In this case the result's sign will be opposite that of

	 * opnd1.  The mantissa also needs some correction.

 check for remainder being exactly 1/2 of opnd2 */

 normalize result's mantissa */

        /* 

         * Test for underflow

 trap if UNDERFLOWTRAP enabled */

                        /*

                         * Adjust bias of result

 frem is always exact */

                /*

                 * denormalize result or set to signed zero

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/fcnvfu.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Floating-point to Unsigned Fixed-point Converts

 *

 *  External Interfaces:

 *	dbl_to_dbl_fcnvfu(srcptr,nullptr,dstptr,status)

 *	dbl_to_sgl_fcnvfu(srcptr,nullptr,dstptr,status)

 *	sgl_to_dbl_fcnvfu(srcptr,nullptr,dstptr,status)

 *	sgl_to_sgl_fcnvfu(srcptr,nullptr,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/************************************************************************

 *  Floating-point to Unsigned Fixed-point Converts			*

/*

 *  Single Floating-point to Single Unsigned Fixed 

ARGSUSED*/

	/* 

	 * Test for overflow

	/*

	 * Generate result

		/* 

		 * Check sign.

		 * If negative, trap unimplemented.

 check for inexact */

  round result  */

 never negative */

 check for inexact */

  round result  */

/*

 *  Single Floating-point to Double Unsigned Fixed 

ARGSUSED*/

	/* 

	 * Test for overflow

	/*

	 * Generate result

		/* 

		 * Check sign.

		 * If negative, trap unimplemented.

 check for inexact */

  round result  */

 never negative */

 check for inexact */

  round result  */

/*

 *  Double Floating-point to Single Unsigned Fixed 

ARGSUSED*/

	/* 

	 * Test for overflow

	/*

	 * Generate result

		/* 

		 * Check sign.

		 * If negative, trap unimplemented.

 check for inexact */

  round result  */

 never negative */

 check for overflow */

 check for inexact */

  round result  */

/*

 *  Double Floating-point to Double Unsigned Fixed 

ARGSUSED*/

	/* 

	 * Test for overflow

	/*

	 * Generate result

		/* 

		 * Check sign.

		 * If negative, trap unimplemented.

 check for inexact */

  round result  */

 never negative */

 check for inexact */

  round result  */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/sfmpy.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Single Precision Floating-point Multiply

 *

 *  External Interfaces:

 *	sgl_fmpy(srcptr1,srcptr2,dstptr,status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Single Precision Floating-point Multiply

	/* 

	 * set sign bit of result 

	/*

	 * check first operand for NaN's or infinity

					/* 

					 * invalid since operands are infinity 

					 * and zero 

				/*

			 	 * return infinity

                	/*

                 	 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                	/*

                 	 * return quiet NaN

	/*

	 * check second operand for NaN's or infinity

 invalid since operands are zero & infinity */

			/*

			 * return infinity

                /*

                 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

                /*

                 * return quiet NaN

	/*

	 * Generate exponent 

	/*

	 * Generate mantissa

 set hidden bit */

 check for zero */

 is denormalized, adjust exponent */

 opnd2 needs to have hidden bit set with msb in hidden bit */

 check for zero */

 is denormalized; want to normalize */

 Multiply two source mantissas together */

 make room for guard bits */

	/*

	 * Four bits at a time are inspected in each loop, and a

	 * simple shift and add multiply algorithm is used.

 make sure result is left-justified */

 result mantissa >= 2. */

 check for denormalized result */

	/*

	 * check for guard, sticky and inexact bits

 re-align mantissa */

	/* 

	 * round result 

        /* 

         * Test for overflow

 trap if OVERFLOWTRAP enabled */

                        /*

                         * Adjust bias of result

 set result to infinity or largest number */

        /* 

         * Test for underflow

 trap if UNDERFLOWTRAP enabled */

                        /*

                         * Adjust bias of result

 Determine if should set underflow flag */

                /*

                 * denormalize result or set to signed zero

 return zero or smallest number */

 check for inexact */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/dfsub.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Double_subtract: subtract two double precision values.

 *

 *  External Interfaces:

 *	dbl_fsub(leftptr, rightptr, dstptr, status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 * Double_subtract: subtract two double precision values.

 Create local copies of the numbers */

    /* A zero "save" helps discover equal operands (for later),  *

to*/save);

    /*

     * check first operand for NaN's or infinity

		    /* 

		     * invalid since operands are same signed infinity's

		/*

	 	 * return infinity

            /*

             * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	    /* 

	     * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	    /*

 	     * return quiet NaN

 End left NaN or Infinity processing */

    /*

     * check second operand for NaN's or infinity

 return infinity */

        /*

         * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	/*

	 * return quiet NaN

 End right NaN or Infinity processing */

 Invariant: Must be dealing with finite numbers */

 Compare operands by removing the sign */

 sign difference selects add or sub operation. */

	/* Set the left operand to the larger one by XOR swap *

to*/rightp1);

to*/leftp1);

 Invariant:  left is not smaller than right. */ 

 Denormalized operands.  First look for zeroes */

 right is zero */

 Both operands are zeros */

with*/rightp1);

with*/rightp1);

		/* Left is not a zero and must be the result.  Trapped

		 * underflows are signaled if left is denormalized.  Result

 need to normalize results mantissa */

using*/sign_save);

 inexact = FALSE */

 Neither are zeroes */

 Exponent is already cleared */

	    /* Both operands are denormalized.  The result must be exact

	     * and is simply calculated.  A sum could become normalized and a

signed*/int) save >= 0 )

minus*/rightp1,rightp2,

into*/resultp1,resultp2);

into*/resultp1,resultp2);

 need to normalize result */

using*/sign_save);

 inexact = FALSE */

	right_exponent = 1;	/* Set exponent to reflect different bias

    /* 

     * Special case alignment of operands that would force alignment 

     * beyond the extent of the extension.  A further optimization

     * could special case this but only reduces the path length for this

     * infrequent case.

 Align right operand by shifting to right */

operand*/rightp1,rightp2,
and lower to*/extent);

 Treat sum and difference of the operands separately. */

signed*/int) save >= 0 )

	/*

	 * Difference of the two operands.  Their can be no overflow.  A

	 * borrow can occur out of the hidden bit and force a post

	 * normalization phase.

minus*/rightp1,rightp2,

with*/extent,
 Handle normalization */

	    /* A straight forward algorithm would now shift the result

	     * and extension left until the hidden bit becomes one.  Not

	     * all of the extension bits need participate in the shift.

	     * Only the two most significant bits (round and guard) are

	     * needed.  If only a single shift is needed then the guard

	     * bit becomes a significant low order bit and the extension

	     * must participate in the rounding.  If more than a single 

	     * shift is needed, then all bits to the right of the guard 

            /* Need to check for a zero result.  The sign and exponent

	     * fields have already been zeroed.  The more efficient test

	     * of the full object can be used.

 Must have been "x-x" or "x+(-x)". */

 Look to see if normalization is finished. */

		    /* Denormalized, exponent should be zero.  Left operand *

 No further normalization is needed. */

using*/sign_save);

	    /* Check for denormalized, exponent should be zero.  Left    *

	    /* Shift extension to complete one bit of normalization and

	    /* Discover first one bit to determine shift amount.  Use a

	     * modified binary search.  We have already shifted the result

	     * one position right and still not found a one so the remainder

 Scan bytes */

 Now narrow it down to the nibble */

 The lower nibble contains the normalizing one */

	    /* Select case were first bit is set (already normalized)

 Already normalized */

using*/sign_save);

using*/result_exponent);

using*/sign_save);

using*/result_exponent);

 Sign bit is already set */

 Fixup potential underflows */

 inexact = FALSE */

	    /* 

	     * Since we cannot get an inexact denormalized result,

	     * we can now return.

 end if(hidden...)... */

 Fall through and round */

 end if(save >= 0)... */

 Subtract magnitudes */

to*/resultp1,resultp2);

 Prenormalization required. */

 end if hiddenoverflow... */

 end else ...subtract magnitudes... */

    /* Round the result.  If the extension is all zeros,then the result is

     * exact.  Otherwise round in the correct direction.  No underflow is

     * possible. If a postnormalization is necessary, then the mantissa is

 The default. */

 at least 1/2 ulp */

 either exactly half way and odd or more than 1/2ulp */

 Round up positive results */

 Round down negative results */

 truncate is simple */

 end switch... */

 Overflow */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/fmpyfadd.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Double Floating-point Multiply Fused Add

 *	Double Floating-point Multiply Negate Fused Add

 *	Single Floating-point Multiply Fused Add

 *	Single Floating-point Multiply Negate Fused Add

 *

 *  External Interfaces:

 *	dbl_fmpyfadd(src1ptr,src2ptr,src3ptr,status,dstptr)

 *	dbl_fmpynfadd(src1ptr,src2ptr,src3ptr,status,dstptr)

 *	sgl_fmpyfadd(src1ptr,src2ptr,src3ptr,status,dstptr)

 *	sgl_fmpynfadd(src1ptr,src2ptr,src3ptr,status,dstptr)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 *  Double Floating-point Multiply Fused Add

	/* 

	 * set sign bit of result of multiply

	/*

	 * Generate multiply exponent 

	/*

	 * check first operand for NaN's or infinity

					/* 

					 * invalid since operands are infinity 

					 * and zero 

				/*

				 * Check third operand for infinity with a

				 *  sign opposite of the multiply result

					/* 

					 * invalid since attempting a magnitude

					 * subtraction of infinities

				/*

			 	 * return infinity

			/*

		 	 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is third operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/*

		 	 * return quiet NaN

	/*

	 * check second operand for NaN's or infinity

					/* 

					 * invalid since multiply operands are

					 * zero & infinity

				/*

				 * Check third operand for infinity with a

				 *  sign opposite of the multiply result

					/* 

					 * invalid since attempting a magnitude

					 * subtraction of infinities

				/*

				 * return infinity

			/*

			 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is third operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/*

			 * return quiet NaN

	/*

	 * check third operand for NaN's or infinity

 return infinity */

			/*

			 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/*

			 * return quiet NaN

	/*

	 * Generate multiply mantissa

 set hidden bit */

 check for zero */

			/*

			 * Perform the add opnd3 with zero here.

			/*

			 * Now let's check for trapped underflow case.

 need to normalize results mantissa */

using*/sign_save);

 inexact = FALSE */

 is denormalized, adjust exponent */

 opnd2 needs to have hidden bit set with msb in hidden bit */

 check for zero */

			/*

			 * Perform the add opnd3 with zero here.

			/*

			 * Now let's check for trapped underflow case.

 need to normalize results mantissa */

using*/sign_save);

 inexact = FALSE */

 is denormalized; want to normalize */

 Multiply the first two source mantissas together */

	/* 

	 * The intermediate result will be kept in tmpres,

	 * which needs enough room for 106 bits of mantissa,

	 * so lets call it a Double extended.

	/* 

	 * Four bits at a time are inspected in each loop, and a 

	 * simple shift and add multiply algorithm is used. 

 Fourword_add should be an ADD followed by 3 ADDC's */

 result mantissa >= 2 (mantissa overflow) */

	/*

	 * Restore the sign of the mpy result which was saved in resultp1.

	 * The exponent will continue to be kept in mpy_exponent.

	/* 

	 * No rounding is required, since the result of the multiply

	 * is exact in the extended format.

	/*

	 * Now we are ready to perform the add portion of the operation.

	 *

	 * The exponents need to be kept as integers for now, since the

	 * multiply result might not fit into the exponent field.  We

	 * can't overflow or underflow because of this yet, since the

	 * add could bring the final result back into range.

	/*

	 * Check for denormalized or zero add operand.

 check for zero */

 right is zero */

			/* Left can't be zero and must be result.

			 *

			 * The final result is now in tmpres and mpy_exponent,

			 * and needs to be rounded and squeezed back into

			 * double precision format from double extended.

save sign*/

		/* 

		 * Neither are zeroes.  

		 * Adjust exponent and normalize add operand.

 save sign */

 restore sign */

	/*

	 * Copy opnd3 to the double extended variable called right.

	/*

	 * A zero "save" helps discover equal operands (for later),

	 * and is used in swapping operands (if needed).

to*/save);

	/*

	 * Compare magnitude of operands.

		/*

		 * Set the left operand to the larger one by XOR swap.

		 * First finish the first word "save".

to*/rightp1);

to*/tmpresp1);

 also setup exponents used in rest of routine */

 also setup exponents used in rest of routine */

 Invariant: left is not smaller than right. */

	/*

	 * Special case alignment of operands that would force alignment

	 * beyond the extent of the extension.  A further optimization

	 * could special case this but only reduces the path length for

	 * this infrequent case.

 Align right operand by shifting it to the right */

shifted by*/diff_exponent);

 Treat sum and difference of the operands separately. */

		/*

		 * Difference of the two operands.  Overflow can occur if the

		 * multiply overflowed.  A borrow can occur out of the hidden

		 * bit and force a post normalization phase.

 Handle normalization */

		/* A straightforward algorithm would now shift the

		 * result and extension left until the hidden bit

		 * becomes one.  Not all of the extension bits need

		 * participate in the shift.  Only the two most 

		 * significant bits (round and guard) are needed.

		 * If only a single shift is needed then the guard

		 * bit becomes a significant low order bit and the

		 * extension must participate in the rounding.

		 * If more than a single shift is needed, then all

		 * bits to the right of the guard bit are zeros, 

			/* Need to check for a zero result.  The sign and

			 * exponent fields have already been zeroed.  The more

			 * efficient test of the full object can be used.

 Must have been "x-x" or "x+(-x)". */

 Look to see if normalization is finished. */

 No further normalization is needed */

			/* Discover first one bit to determine shift amount.

			 * Use a modified binary search.  We have already

			 * shifted the result one position right and still

			 * not found a one so the remainder of the extension

 Scan bytes */

 Now narrow it down to the nibble */

				/* The lower nibble contains the

			/* Select case where first bit is set (already

 end if (hidden...)... */

 Fall through and round */

 end if (save < 0)... */

 Add magnitudes */

to*/resultp1,resultp2,resultp3,resultp4);

 Prenormalization required. */

 end if hiddenoverflow... */

 end else ...add magnitudes... */

	/* Round the result.  If the extension and lower two words are

	 * all zeros, then the result is exact.  Otherwise round in the

	 * correct direction.  Underflow is possible. If a postnormalization

	 * is necessary, then the mantissa is all zeros so no shift is needed.

using*/sign_save);

 The default. */

 at least 1/2 ulp */

					/* either exactly half way and odd or

 Round up positive results */

 Round down negative results */

 truncate is simple */

 end switch... */

 trap if OVERFLOWTRAP enabled */

                        /*

                         * Adjust bias of result

 set result to infinity or largest number */

 underflow case */

                        /*

                         * Adjust bias of result

/*

 *  Double Floating-point Multiply Negate Fused Add

	/* 

	 * set sign bit of result of multiply

	/*

	 * Generate multiply exponent 

	/*

	 * check first operand for NaN's or infinity

					/* 

					 * invalid since operands are infinity 

					 * and zero 

				/*

				 * Check third operand for infinity with a

				 *  sign opposite of the multiply result

					/* 

					 * invalid since attempting a magnitude

					 * subtraction of infinities

				/*

			 	 * return infinity

			/*

		 	 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is third operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/*

		 	 * return quiet NaN

	/*

	 * check second operand for NaN's or infinity

					/* 

					 * invalid since multiply operands are

					 * zero & infinity

				/*

				 * Check third operand for infinity with a

				 *  sign opposite of the multiply result

					/* 

					 * invalid since attempting a magnitude

					 * subtraction of infinities

				/*

				 * return infinity

			/*

			 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is third operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/*

			 * return quiet NaN

	/*

	 * check third operand for NaN's or infinity

 return infinity */

			/*

			 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/*

			 * return quiet NaN

	/*

	 * Generate multiply mantissa

 set hidden bit */

 check for zero */

			/*

			 * Perform the add opnd3 with zero here.

			/*

			 * Now let's check for trapped underflow case.

 need to normalize results mantissa */

using*/sign_save);

 inexact = FALSE */

 is denormalized, adjust exponent */

 opnd2 needs to have hidden bit set with msb in hidden bit */

 check for zero */

			/*

			 * Perform the add opnd3 with zero here.

			/*

			 * Now let's check for trapped underflow case.

 need to normalize results mantissa */

using*/sign_save);

 inexact = FALSE */

 is denormalized; want to normalize */

 Multiply the first two source mantissas together */

	/* 

	 * The intermediate result will be kept in tmpres,

	 * which needs enough room for 106 bits of mantissa,

	 * so lets call it a Double extended.

	/* 

	 * Four bits at a time are inspected in each loop, and a 

	 * simple shift and add multiply algorithm is used. 

 Fourword_add should be an ADD followed by 3 ADDC's */

 result mantissa >= 2 (mantissa overflow) */

	/*

	 * Restore the sign of the mpy result which was saved in resultp1.

	 * The exponent will continue to be kept in mpy_exponent.

	/* 

	 * No rounding is required, since the result of the multiply

	 * is exact in the extended format.

	/*

	 * Now we are ready to perform the add portion of the operation.

	 *

	 * The exponents need to be kept as integers for now, since the

	 * multiply result might not fit into the exponent field.  We

	 * can't overflow or underflow because of this yet, since the

	 * add could bring the final result back into range.

	/*

	 * Check for denormalized or zero add operand.

 check for zero */

 right is zero */

			/* Left can't be zero and must be result.

			 *

			 * The final result is now in tmpres and mpy_exponent,

			 * and needs to be rounded and squeezed back into

			 * double precision format from double extended.

save sign*/

		/* 

		 * Neither are zeroes.  

		 * Adjust exponent and normalize add operand.

 save sign */

 restore sign */

	/*

	 * Copy opnd3 to the double extended variable called right.

	/*

	 * A zero "save" helps discover equal operands (for later),

	 * and is used in swapping operands (if needed).

to*/save);

	/*

	 * Compare magnitude of operands.

		/*

		 * Set the left operand to the larger one by XOR swap.

		 * First finish the first word "save".

to*/rightp1);

to*/tmpresp1);

 also setup exponents used in rest of routine */

 also setup exponents used in rest of routine */

 Invariant: left is not smaller than right. */

	/*

	 * Special case alignment of operands that would force alignment

	 * beyond the extent of the extension.  A further optimization

	 * could special case this but only reduces the path length for

	 * this infrequent case.

 Align right operand by shifting it to the right */

shifted by*/diff_exponent);

 Treat sum and difference of the operands separately. */

		/*

		 * Difference of the two operands.  Overflow can occur if the

		 * multiply overflowed.  A borrow can occur out of the hidden

		 * bit and force a post normalization phase.

 Handle normalization */

		/* A straightforward algorithm would now shift the

		 * result and extension left until the hidden bit

		 * becomes one.  Not all of the extension bits need

		 * participate in the shift.  Only the two most 

		 * significant bits (round and guard) are needed.

		 * If only a single shift is needed then the guard

		 * bit becomes a significant low order bit and the

		 * extension must participate in the rounding.

		 * If more than a single shift is needed, then all

		 * bits to the right of the guard bit are zeros, 

			/* Need to check for a zero result.  The sign and

			 * exponent fields have already been zeroed.  The more

			 * efficient test of the full object can be used.

 Must have been "x-x" or "x+(-x)". */

 Look to see if normalization is finished. */

 No further normalization is needed */

			/* Discover first one bit to determine shift amount.

			 * Use a modified binary search.  We have already

			 * shifted the result one position right and still

			 * not found a one so the remainder of the extension

 Scan bytes */

 Now narrow it down to the nibble */

				/* The lower nibble contains the

			/* Select case where first bit is set (already

 end if (hidden...)... */

 Fall through and round */

 end if (save < 0)... */

 Add magnitudes */

to*/resultp1,resultp2,resultp3,resultp4);

 Prenormalization required. */

 end if hiddenoverflow... */

 end else ...add magnitudes... */

	/* Round the result.  If the extension and lower two words are

	 * all zeros, then the result is exact.  Otherwise round in the

	 * correct direction.  Underflow is possible. If a postnormalization

	 * is necessary, then the mantissa is all zeros so no shift is needed.

using*/sign_save);

 The default. */

 at least 1/2 ulp */

					/* either exactly half way and odd or

 Round up positive results */

 Round down negative results */

 truncate is simple */

 end switch... */

 Overflow */

                        /*

                         * Adjust bias of result

 underflow case */

                        /*

                         * Adjust bias of result

/*

 *  Single Floating-point Multiply Fused Add

	/* 

	 * set sign bit of result of multiply

	/*

	 * Generate multiply exponent 

	/*

	 * check first operand for NaN's or infinity

					/* 

					 * invalid since operands are infinity 

					 * and zero 

				/*

				 * Check third operand for infinity with a

				 *  sign opposite of the multiply result

					/* 

					 * invalid since attempting a magnitude

					 * subtraction of infinities

				/*

			 	 * return infinity

			/*

		 	 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is third operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/*

		 	 * return quiet NaN

	/*

	 * check second operand for NaN's or infinity

					/* 

					 * invalid since multiply operands are

					 * zero & infinity

				/*

				 * Check third operand for infinity with a

				 *  sign opposite of the multiply result

					/* 

					 * invalid since attempting a magnitude

					 * subtraction of infinities

				/*

				 * return infinity

			/*

			 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is third operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/*

			 * return quiet NaN

	/*

	 * check third operand for NaN's or infinity

 return infinity */

			/*

			 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/*

			 * return quiet NaN

	/*

	 * Generate multiply mantissa

 set hidden bit */

 check for zero */

			/*

			 * Perform the add opnd3 with zero here.

			/*

			 * Now let's check for trapped underflow case.

 need to normalize results mantissa */

using*/sign_save);

 inexact = FALSE */

 is denormalized, adjust exponent */

 opnd2 needs to have hidden bit set with msb in hidden bit */

 check for zero */

			/*

			 * Perform the add opnd3 with zero here.

			/*

			 * Now let's check for trapped underflow case.

 need to normalize results mantissa */

using*/sign_save);

 inexact = FALSE */

 is denormalized; want to normalize */

 Multiply the first two source mantissas together */

	/* 

	 * The intermediate result will be kept in tmpres,

	 * which needs enough room for 106 bits of mantissa,

	 * so lets call it a Double extended.

	/* 

	 * Four bits at a time are inspected in each loop, and a 

	 * simple shift and add multiply algorithm is used. 

 Twoword_add should be an ADD followed by 2 ADDC's */

 result mantissa >= 2 (mantissa overflow) */

	/*

	 * Restore the sign of the mpy result which was saved in resultp1.

	 * The exponent will continue to be kept in mpy_exponent.

	/* 

	 * No rounding is required, since the result of the multiply

	 * is exact in the extended format.

	/*

	 * Now we are ready to perform the add portion of the operation.

	 *

	 * The exponents need to be kept as integers for now, since the

	 * multiply result might not fit into the exponent field.  We

	 * can't overflow or underflow because of this yet, since the

	 * add could bring the final result back into range.

	/*

	 * Check for denormalized or zero add operand.

 check for zero */

 right is zero */

			/* Left can't be zero and must be result.

			 *

			 * The final result is now in tmpres and mpy_exponent,

			 * and needs to be rounded and squeezed back into

			 * double precision format from double extended.

save sign*/

		/* 

		 * Neither are zeroes.  

		 * Adjust exponent and normalize add operand.

 save sign */

 restore sign */

	/*

	 * Copy opnd3 to the double extended variable called right.

	/*

	 * A zero "save" helps discover equal operands (for later),

	 * and is used in swapping operands (if needed).

to*/save);

	/*

	 * Compare magnitude of operands.

		/*

		 * Set the left operand to the larger one by XOR swap.

		 * First finish the first word "save".

to*/rightp1);

to*/tmpresp1);

 also setup exponents used in rest of routine */

 also setup exponents used in rest of routine */

 Invariant: left is not smaller than right. */

	/*

	 * Special case alignment of operands that would force alignment

	 * beyond the extent of the extension.  A further optimization

	 * could special case this but only reduces the path length for

	 * this infrequent case.

 Align right operand by shifting it to the right */

shifted by*/diff_exponent);

 Treat sum and difference of the operands separately. */

		/*

		 * Difference of the two operands.  Overflow can occur if the

		 * multiply overflowed.  A borrow can occur out of the hidden

		 * bit and force a post normalization phase.

 Handle normalization */

		/* A straightforward algorithm would now shift the

		 * result and extension left until the hidden bit

		 * becomes one.  Not all of the extension bits need

		 * participate in the shift.  Only the two most 

		 * significant bits (round and guard) are needed.

		 * If only a single shift is needed then the guard

		 * bit becomes a significant low order bit and the

		 * extension must participate in the rounding.

		 * If more than a single shift is needed, then all

		 * bits to the right of the guard bit are zeros, 

			/* Need to check for a zero result.  The sign and

			 * exponent fields have already been zeroed.  The more

			 * efficient test of the full object can be used.

 Must have been "x-x" or "x+(-x)". */

 Look to see if normalization is finished. */

 No further normalization is needed */

			/* Discover first one bit to determine shift amount.

			 * Use a modified binary search.  We have already

			 * shifted the result one position right and still

			 * not found a one so the remainder of the extension

 Scan bytes */

 Now narrow it down to the nibble */

				/* The lower nibble contains the

			/* Select case where first bit is set (already

 end if (hidden...)... */

 Fall through and round */

 end if (save < 0)... */

 Add magnitudes */

to*/resultp1,resultp2);

 Prenormalization required. */

 end if hiddenoverflow... */

 end else ...add magnitudes... */

	/* Round the result.  If the extension and lower two words are

	 * all zeros, then the result is exact.  Otherwise round in the

	 * correct direction.  Underflow is possible. If a postnormalization

	 * is necessary, then the mantissa is all zeros so no shift is needed.

using*/sign_save);

 The default. */

 at least 1/2 ulp */

					/* either exactly half way and odd or

 Round up positive results */

 Round down negative results */

 truncate is simple */

 end switch... */

 Overflow */

                        /*

                         * Adjust bias of result

 underflow case */

                        /*

                         * Adjust bias of result

/*

 *  Single Floating-point Multiply Negate Fused Add

	/* 

	 * set sign bit of result of multiply

	/*

	 * Generate multiply exponent 

	/*

	 * check first operand for NaN's or infinity

					/* 

					 * invalid since operands are infinity 

					 * and zero 

				/*

				 * Check third operand for infinity with a

				 *  sign opposite of the multiply result

					/* 

					 * invalid since attempting a magnitude

					 * subtraction of infinities

				/*

			 	 * return infinity

			/*

		 	 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is third operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/*

		 	 * return quiet NaN

	/*

	 * check second operand for NaN's or infinity

					/* 

					 * invalid since multiply operands are

					 * zero & infinity

				/*

				 * Check third operand for infinity with a

				 *  sign opposite of the multiply result

					/* 

					 * invalid since attempting a magnitude

					 * subtraction of infinities

				/*

				 * return infinity

			/*

			 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/* 

			 * is third operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/*

			 * return quiet NaN

	/*

	 * check third operand for NaN's or infinity

 return infinity */

			/*

			 * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

			/*

			 * return quiet NaN

	/*

	 * Generate multiply mantissa

 set hidden bit */

 check for zero */

			/*

			 * Perform the add opnd3 with zero here.

			/*

			 * Now let's check for trapped underflow case.

 need to normalize results mantissa */

using*/sign_save);

 inexact = FALSE */

 is denormalized, adjust exponent */

 opnd2 needs to have hidden bit set with msb in hidden bit */

 check for zero */

			/*

			 * Perform the add opnd3 with zero here.

			/*

			 * Now let's check for trapped underflow case.

 need to normalize results mantissa */

using*/sign_save);

 inexact = FALSE */

 is denormalized; want to normalize */

 Multiply the first two source mantissas together */

	/* 

	 * The intermediate result will be kept in tmpres,

	 * which needs enough room for 106 bits of mantissa,

	 * so lets call it a Double extended.

	/* 

	 * Four bits at a time are inspected in each loop, and a 

	 * simple shift and add multiply algorithm is used. 

 Twoword_add should be an ADD followed by 2 ADDC's */

 result mantissa >= 2 (mantissa overflow) */

	/*

	 * Restore the sign of the mpy result which was saved in resultp1.

	 * The exponent will continue to be kept in mpy_exponent.

	/* 

	 * No rounding is required, since the result of the multiply

	 * is exact in the extended format.

	/*

	 * Now we are ready to perform the add portion of the operation.

	 *

	 * The exponents need to be kept as integers for now, since the

	 * multiply result might not fit into the exponent field.  We

	 * can't overflow or underflow because of this yet, since the

	 * add could bring the final result back into range.

	/*

	 * Check for denormalized or zero add operand.

 check for zero */

 right is zero */

			/* Left can't be zero and must be result.

			 *

			 * The final result is now in tmpres and mpy_exponent,

			 * and needs to be rounded and squeezed back into

			 * double precision format from double extended.

save sign*/

		/* 

		 * Neither are zeroes.  

		 * Adjust exponent and normalize add operand.

 save sign */

 restore sign */

	/*

	 * Copy opnd3 to the double extended variable called right.

	/*

	 * A zero "save" helps discover equal operands (for later),

	 * and is used in swapping operands (if needed).

to*/save);

	/*

	 * Compare magnitude of operands.

		/*

		 * Set the left operand to the larger one by XOR swap.

		 * First finish the first word "save".

to*/rightp1);

to*/tmpresp1);

 also setup exponents used in rest of routine */

 also setup exponents used in rest of routine */

 Invariant: left is not smaller than right. */

	/*

	 * Special case alignment of operands that would force alignment

	 * beyond the extent of the extension.  A further optimization

	 * could special case this but only reduces the path length for

	 * this infrequent case.

 Align right operand by shifting it to the right */

shifted by*/diff_exponent);

 Treat sum and difference of the operands separately. */

		/*

		 * Difference of the two operands.  Overflow can occur if the

		 * multiply overflowed.  A borrow can occur out of the hidden

		 * bit and force a post normalization phase.

 Handle normalization */

		/* A straightforward algorithm would now shift the

		 * result and extension left until the hidden bit

		 * becomes one.  Not all of the extension bits need

		 * participate in the shift.  Only the two most 

		 * significant bits (round and guard) are needed.

		 * If only a single shift is needed then the guard

		 * bit becomes a significant low order bit and the

		 * extension must participate in the rounding.

		 * If more than a single shift is needed, then all

		 * bits to the right of the guard bit are zeros, 

			/* Need to check for a zero result.  The sign and

			 * exponent fields have already been zeroed.  The more

			 * efficient test of the full object can be used.

 Must have been "x-x" or "x+(-x)". */

 Look to see if normalization is finished. */

 No further normalization is needed */

			/* Discover first one bit to determine shift amount.

			 * Use a modified binary search.  We have already

			 * shifted the result one position right and still

			 * not found a one so the remainder of the extension

 Scan bytes */

 Now narrow it down to the nibble */

				/* The lower nibble contains the

			/* Select case where first bit is set (already

 end if (hidden...)... */

 Fall through and round */

 end if (save < 0)... */

 Add magnitudes */

to*/resultp1,resultp2);

 Prenormalization required. */

 end if hiddenoverflow... */

 end else ...add magnitudes... */

	/* Round the result.  If the extension and lower two words are

	 * all zeros, then the result is exact.  Otherwise round in the

	 * correct direction.  Underflow is possible. If a postnormalization

	 * is necessary, then the mantissa is all zeros so no shift is needed.

using*/sign_save);

 The default. */

 at least 1/2 ulp */

					/* either exactly half way and odd or

 Round up positive results */

 Round down negative results */

 truncate is simple */

 end switch... */

 Overflow */

                        /*

                         * Adjust bias of result

 underflow case */

                        /*

                         * Adjust bias of result

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/sfcmp.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	sgl_cmp: compare two values

 *

 *  External Interfaces:

 *	sgl_fcmp(leftptr, rightptr, cond, status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 * sgl_cmp: compare two values

 The predicate to be tested */

 Create local copies of the numbers */

    /*

     * Test for NaN

	/* Check if a NaN is involved.  Signal an invalid exception when 

	 * comparing a signaling NaN or when comparing quiet NaNs and the

	/* All the exceptional conditions are handled, now special case

 NaNs always compare unordered. */

 infinities will drop down to the normal compare mechanisms */

    /* First compare for unequal signs => less or greater or

        /* left negative => less, left positive => greater.

    /* Signs are the same.  Treat negative numbers separately

 Positive compare */

        /* Negative compare.  Signed or unsigned compares

         * both work the same.  That distinction is only

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/PA-RISC Project (http://www.parisc-linux.org/)

 *

 * Floating-point emulation code

 *  Copyright (C) 2001 Hewlett-Packard (Paul Bame) <bame@debian.org>

/*

 * BEGIN_DESC

 *

 *  File:

 *	@(#)	pa/spmath/sfadd.c		$Revision: 1.1 $

 *

 *  Purpose:

 *	Single_add: add two single precision values.

 *

 *  External Interfaces:

 *	sgl_fadd(leftptr, rightptr, dstptr, status)

 *

 *  Internal Interfaces:

 *

 *  Theory:

 *	<<please update with a overview of the operation of this file>>

 *

 * END_DESC

/*

 * Single_add: add two single precision values.

 Create local copies of the numbers */

    /* A zero "save" helps discover equal operands (for later),  *

to*/save);

    /*

     * check first operand for NaN's or infinity

		    /* 

		     * invalid since operands are opposite signed infinity's

		/*

	 	 * return infinity

            /*

             * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	    /* 

	     * is second operand a signaling NaN? 

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	    /*

 	     * return quiet NaN

 End left NaN or Infinity processing */

    /*

     * check second operand for NaN's or infinity

 return infinity */

        /*

         * is NaN; signaling or quiet?

 trap if INVALIDTRAP enabled */

 make NaN quiet */

	/*

	 * return quiet NaN

 End right NaN or Infinity processing */

 Invariant: Must be dealing with finite numbers */

 Compare operands by removing the sign */

 sign difference selects add or sub operation. */

	/* Set the left operand to the larger one by XOR swap *

to*/right);

to*/left);

 Invariant:  left is not smaller than right. */ 

 Denormalized operands.  First look for zeroes */

 right is zero */

 Both operands are zeros */

with*/right);

with*/right);

		/* Left is not a zero and must be the result.  Trapped

		 * underflows are signaled if left is denormalized.  Result

 need to normalize results mantissa */

using*/sign_save);

 Neither are zeroes */

 Exponent is already cleared */

	    /* Both operands are denormalized.  The result must be exact

	     * and is simply calculated.  A sum could become normalized and a

signed*/int) save < 0 )

minus*/right,
into*/result);

 need to normalize result */

using*/sign_save);

	right_exponent = 1;	/* Set exponent to reflect different bias

    /* 

     * Special case alignment of operands that would force alignment 

     * beyond the extent of the extension.  A further optimization

     * could special case this but only reduces the path length for this

     * infrequent case.

 Align right operand by shifting to right */

operand*/right,
and lower to*/extent);

 Treat sum and difference of the operands separately. */

signed*/int) save < 0 )

	/*

	 * Difference of the two operands.  Their can be no overflow.  A

	 * borrow can occur out of the hidden bit and force a post

	 * normalization phase.

minus*/right,into*/result);

 Handle normalization */

	    /* A straightforward algorithm would now shift the result

	     * and extension left until the hidden bit becomes one.  Not

	     * all of the extension bits need participate in the shift.

	     * Only the two most significant bits (round and guard) are

	     * needed.  If only a single shift is needed then the guard

	     * bit becomes a significant low order bit and the extension

	     * must participate in the rounding.  If more than a single 

	     * shift is needed, then all bits to the right of the guard 

            /* Need to check for a zero result.  The sign and exponent

	     * fields have already been zeroed.  The more efficient test

	     * of the full object can be used.

 Must have been "x-x" or "x+(-x)". */

 Look to see if normalization is finished. */

		    /* Denormalized, exponent should be zero.  Left operand *

 No further normalization is needed. */

using*/sign_save);

	    /* Check for denormalized, exponent should be zero.  Left    * 

	    /* Shift extension to complete one bit of normalization and

	    /* Discover first one bit to determine shift amount.  Use a

	     * modified binary search.  We have already shifted the result

	     * one position right and still not found a one so the remainder

 Scan bytes */

 Now narrow it down to the nibble */

 The lower nibble contains the normalizing one */

	    /* Select case were first bit is set (already normalized)

 Already normalized */

using*/sign_save);

using*/result_exponent);

using*/sign_save);

using*/result_exponent);

 Sign bit is already set */

 Fixup potential underflows */

 inexact = FALSE; */

	    /* 

	     * Since we cannot get an inexact denormalized result,

	     * we can now return.

by*/(1-result_exponent),extent);

 end if(hidden...)... */

 Fall through and round */

 end if(save < 0)... */

 Add magnitudes */

to*/result);

 Prenormalization required. */

 end if hiddenoverflow... */

 end else ...add magnitudes... */

    /* Round the result.  If the extension is all zeros,then the result is

     * exact.  Otherwise round in the correct direction.  No underflow is

     * possible. If a postnormalization is necessary, then the mantissa is

 The default. */

 at least 1/2 ulp */

 either exactly half way and odd or more than 1/2ulp */

 Round up positive results */

 Round down negative results */

 truncate is simple */

 end switch... */

 Overflow */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/parisc/mm/init.c

 *

 *  Copyright (C) 1995	Linus Torvalds

 *  Copyright 1999 SuSE GmbH

 *    changed by Philipp Rumpf

 *  Copyright 1999 Philipp Rumpf (prumpf@tux.org)

 *  Copyright 2004 Randolph Chung (tausq@debian.org)

 *  Copyright 2006-2007 Helge Deller (deller@gmx.de)

 *

 for node_online_map */

 for release_pages */

 Kernel entry point in head.S */

/* The following array is initialized from the firmware specific

 * information retrieved in kernel/inventory.c.

 !CONFIG_64BIT */

 !CONFIG_64BIT */

 We need this before __setup() functions are called */

 Turn off space register hashing */

	/*

	 * Sort the ranges. Since the number of ranges is typically

	 * small, and performance is not an issue here, just do

	 * a simple insertion sort.

	/*

	 * Throw out ranges that are too far apart (controlled by

	 * MAX_GAP).

 Print the memory ranges */

 request memory resource */

	/*

	 * For 32 bit kernels we limit the amount of memory we can

	 * support, in order to preserve enough kernel address space

	 * for other purposes. For 64 bit kernels we don't normally

	 * limit the memory, but this mechanism can be used to

	 * artificially limit the amount of memory (and it is written

	 * to work with multiple memory ranges).

 check for "mem=" argument */

 Merge the ranges, keeping track of the holes */

	/*

	 * Initialize and free the full range of memory in each range.

 add system RAM memblock */

	/*

	 * We can't use memblock top-down allocations because we only

	 * created the initial mapping up to KERNEL_INITIAL_SIZE in

	 * the assembly bootup code.

	/* IOMMU is always used to access "high mem" on those boxes

	 * that can support enough mem that a PCI device couldn't

	 * directly DMA to any physical addresses.

	 * ISA DMA support will need to revisit this.

 reserve PAGE0 pdc memory, kernel text/data/bss & bootmap */

 reserve the holes */

	/* We don't know which region the kernel will be in, so try

	 * all of them.

 Initialize Page Deallocation Table (PDT) and check for bad memory. */

 for 2-level configuration PTRS_PER_PMD is 0 so start_pmd will be 0 */

 outside kernel memory */

 still initializing, allow writing to RO memory */

 Code (ro) and Data areas */

 force the kernel to see the new page table entries */

 Remap kernel text and data, but do not touch init section yet. */

	/* The init text pages are marked R-X.  We have to

	 * flush the icache and mark them RW-

	 *

	 * This is tricky, because map_pages is in the init section.

	 * Do a dummy remap of the data section first (the data

	 * section is already PAGE_KERNEL) to pull in the TLB entries

	/* now remap at PAGE_KERNEL since the TLB is pre-primed to execute

 force the kernel to see the new TLB entries */

	/* finally dump all the instructions which were cached, since the

 set up a new led state on systems shipped LED State panel */

	/* rodata memory was already mapped with KERNEL_RO access rights by

/*

 * Just an arbitrary offset to serve as a "hole" between mapping areas

 * (between top of physical memory and a potential pcxl dma mapping

 * area, and below the vmalloc mapping area).

 *

 * The current 32K value just means that there will be a 32K "hole"

 * between mapping areas. That means that  any out-of-bounds memory

 * accesses will hopefully be caught. The vmalloc() routines leaves

 * a hole of 4kB between each vmalloced area for the same reason.

 Leave room for gateway page expansion */

 Do sanity checks on IPC (compat) structures */

 Do sanity checks on page table constants */

	/*

	 * Do not expose the virtual kernel memory layout to userspace.

	 * But keep code for debugging purposes.

/*

 * pagetable_init() sets up the page tables

 *

 * Note that gateway_init() places the Linux gateway page at page 0.

 * Since gateway pages cannot be dereferenced this has the desirable

 * side effect of trapping those pesky NULL-reference errors in the

 * kernel.

 Map each physical memory range to its kernel vaddr */

	/* FIXME: This is 'const' in order to trick the compiler

	/*

	 * Setup Linux Gateway page.

	 *

	 * The Linux gateway page will reside in kernel space (on virtual

	 * page 0), so it doesn't need to be aliased into user space.

 start with known state */

/*

 * Currently, all PA20 chips have 18 bit protection IDs, which is the

 * limiting factor (space ids are 32 bits).

/*

 * Currently we have a one-to-one relationship between space IDs and

 * protection IDs. Older parisc chips (PCXS, PCXT, PCXL, PCXL2) only

 * support 15 bit protection IDs, so that is the limiting factor.

 * PCXT' has 18 bit protection IDs, but only 16 bit spaceids, so it's

 * probably not worth the effort for a special case here.

 !CONFIG_PA20 */

 disallow space 0 */

 flush_tlb_all() calls recycle_sids() */

 attempt to free space id twice */

 NOTE: sid_lock must be held upon entry */

 NOTE: sid_lock must be held upon entry */

 CONFIG_SMP */

 NOTE: sid_lock must be held upon entry */

/*

 * flush_tlb_all() calls recycle_sids(), since whenever the entire tlb is

 * purged, we can safely reuse the space ids that were released but

 * not flushed from the tlb.

 FIXME: Use a semaphore/wait queue here */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 *

 * Copyright (C) 1995, 1996, 1997, 1998 by Ralf Baechle

 * Copyright 1999 SuSE GmbH (Philipp Rumpf, prumpf@tux.org)

 * Copyright 1999 Hewlett Packard Co.

 *

 Various important other fields */

 extended opcode is 0x6a */

 for identifying LDCW */

/*

 * parisc_acctyp(unsigned int inst) --

 *    Given a PA-RISC memory access instruction, determine if the

 *    the instruction would perform a memory read or memory write

 *    operation.

 *

 *    This function assumes that the given instruction is a memory access

 *    instruction (i.e. you should really only call it if you know that

 *    the instruction has generated some sort of a memory access fault).

 *

 * Returns:

 *   VM_READ  if read operation

 *   VM_WRITE if write operation

 *   VM_EXEC  if execute operation

 load */

 new load */

 store */

 new store */

 coproc */

 coproc2 */

 indexed/memory management */

			/*

			 * Check for the 'Graphics Flush Read' instruction.

			 * It resembles an FDC instruction, except for bits

			 * 20 and 21. Any combination other than zero will

			 * utilize the block mover functionality on some

			 * older PA-RISC platforms.  The case where a block

			 * move is performed from VM to graphics IO space

			 * should be treated as a READ.

			 *

			 * The significance of bits 20,21 in the FDC

			 * instruction is:

			 *

			 *   00  Flush data cache (normal instruction behavior)

			 *   01  Graphics flush write  (IO space -> VM)

			 *   10  Graphics flush read   (VM -> IO space)

			 *   11  Graphics flush read/write (VM <-> IO space)

			/*

			 * Check for LDCWX and LDCWS (semaphore instructions).

			 * If bits 23 through 25 are all 1's it is one of

			 * the above two instructions and is a write.

			 *

			 * Note: With the limited bits we are looking at,

			 * this will also catch PROBEW and PROBEWI. However,

			 * these should never get in here because they don't

			 * generate exceptions of the type:

			 *   Data TLB miss fault/data page fault

			 *   Data memory protection trap

 Default */

 Default */

/* This is the treewalk to find a vma which is the highest that has

 * a start < addr.  We're using find_vma_prev instead right now, but

 * we might want to use this at some point in the future.  Probably

 * not, but I want it committed to CVS so I don't lose it :-)

		/*

		 * Fix up get_user() and put_user().

		 * ASM_EXCEPTIONTABLE_ENTRY_EFAULT() sets the least-significant

		 * bit in the relative address of the fixup routine to indicate

		 * that %r8 should be loaded with -EFAULT to report a userspace

		 * access error.

 zero target register for get_user() */

		/*

		 * NOTE: In some cases the faulting instruction

		 * may be in the delay slot of a branch. We

		 * don't want to take the branch, so we don't

		 * increment iaoq[1], instead we set it to be

		 * iaoq[0]+4, and clear the B bit in the PSW

 IPSW in gr[0] */

/*

 * parisc hardware trap list

 *

 * Documented in section 3 "Addressing and Access Control" of the

 * "PA-RISC 1.1 Architecture and Instruction Set Reference Manual"

 * https://parisc.wiki.kernel.org/index.php/File:Pa11_acd.pdf

 *

 * For implementation see handle_interruption() in traps.c

/*

 * Print out info about fatal segfaults, if the show_unhandled_signals

 * sysctl is set:

/*

 * Ok, we have a good vm_area for this memory access. We still need to

 * check the access permissions.

	/*

	 * If for any reason at all we couldn't handle the fault, make

	 * sure we exit gracefully rather than endlessly redo the

	 * fault.

		/*

		 * We hit a shared mapping outside of the file, or some

		 * other thing happened to us that made us unable to

		 * handle the page fault gracefully.

			/*

			 * No need to mmap_read_unlock(mm) as we would

			 * have already released it in __lock_page_or_retry

			 * in mm/filemap.c.

/*

 * Something tried to access memory that isn't in our memory map..

 Data TLB miss fault/Data page fault */

 send SIGSEGV when outside of vma */

 send SIGSEGV for wrong permissions */

 probably address is outside of mapped file */

 NA data TLB miss / page fault */

 Unaligned access - PCXS only */

 Non-access instruction TLB miss fault */

 PCXL: Data memory access rights trap */

			/*

			 * Either small page or large page may be poisoned.

			 * In other words, VM_FAULT_HWPOISON_LARGE and

			 * VM_FAULT_HWPOISON are mutually exclusive.

 SPDX-License-Identifier: GPL-2.0

/*

 * PARISC64 Huge TLB page support.

 *

 * This parisc implementation is heavily based on the SPARC and x86 code.

 *

 * Copyright (C) 2015 Helge Deller <deller@gmx.de>

 we need to make sure the colouring is OK */

	/* We must align the address, because our caller will run

	 * set_huge_pte_at() on whatever we return, which writes out

	 * all of the sub-ptes for the hugepage range.  So we have

	 * to give it the first such sub-pte.

/* Purge data and instruction TLB entries.  Must be called holding

 * the pa_tlb_lock.  The TLB purge instructions are slow on SMP

 * machines since the purge must be broadcast to all CPUs.

	/* We may use multiple physical huge pages (e.g. 2x1 MB) to emulate

 __set_huge_pte_at() must be called holding the pa_tlb_lock. */

 SPDX-License-Identifier: GPL-2.0

/*

 * fixmaps for parisc

 *

 * Copyright (c) 2019 Sven Schnelle <svens@stackframe.org>

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/parisc/mm/ioremap.c

 *

 * (C) Copyright 1995 1996 Linus Torvalds

 * (C) Copyright 2001-2019 Helge Deller <deller@gmx.de>

 * (C) Copyright 2005 Kyle McMartin <kyle@parisc-linux.org>

/*

 * Generic mapping function (not visible outside):

/*

 * Remap an arbitrary physical address space into the kernel virtual

 * address space.

 *

 * NOTE! We need to allow non-page-aligned mappings too: we will obviously

 * have to convert them into an offset in a page-aligned mapping, but the

 * caller shouldn't need to know that small detail.

 Support EISA addresses */

 Don't allow wraparound or zero size */

	/*

	 * Don't allow anybody to remap normal RAM that we're using..

	/*

	 * Mappings have to be page-aligned

	/*

	 * Ok, go for it..

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *    PARISC Architecture-dependent parts of process handling

 *    based on the work for i386

 *

 *    Copyright (C) 1999-2003 Matthew Wilcox <willy at parisc-linux.org>

 *    Copyright (C) 2000 Martin K Petersen <mkp at mkp.net>

 *    Copyright (C) 2000 John Marvin <jsm at parisc-linux.org>

 *    Copyright (C) 2000 David Huggins-Daines <dhd with pobox.org>

 *    Copyright (C) 2000-2003 Paul Bame <bame at parisc-linux.org>

 *    Copyright (C) 2000 Philipp Rumpf <prumpf with tux.org>

 *    Copyright (C) 2000 David Kennedy <dkennedy with linuxcare.com>

 *    Copyright (C) 2000 Richard Hirst <rhirst with parisc-linux.org>

 *    Copyright (C) 2000 Grant Grundler <grundler with parisc-linux.org>

 *    Copyright (C) 2001 Alan Modra <amodra at parisc-linux.org>

 *    Copyright (C) 2001-2002 Ryan Bradetich <rbrad at parisc-linux.org>

 *    Copyright (C) 2001-2014 Helge Deller <deller@gmx.de>

 *    Copyright (C) 2002 Randolph Chung <tausq with parisc-linux.org>

 reset any module */

/*

** The Wright Brothers and Gecko systems have a H/W problem

** (Lasi...'nuf said) may cause a broadcast reset to lockup

** the system. An HVERSION dependent PDC call was developed

** to perform a "safe", platform specific broadcast reset instead

** of kludging up all the code.

**

** Older machines which do not implement PDC_BROADCAST_RESET will

** return (with an error) and the regular broadcast reset can be

** issued. Obviously, if the PDC does implement PDC_BROADCAST_RESET

** the PDC call will not return (the system will be reset).

	/*

	 ** If user has modified the Firmware Selftest Bitmap,

	 ** run the tests specified in the bitmap after the

	 ** system is rebooted w/PDC_DO_RESET.

	 **

	 ** ftc_bitmap = 0x1AUL "Skip destructive memory tests"

	 **

	 ** Using "directed resets" at each processor with the MEM_TOC

	 ** vector cleared will also avoid running destructive

	 ** memory self tests. (Not implemented yet)

 set up a new led state on systems shipped with a LED State panel */

 "Normal" system reset */

 Nope...box should reset with just CMD_RESET now */

 Wait for RESET to lay us to rest. */

/*

 * This routine is called from sys_reboot to actually turn off the

 * machine 

 If there is a registered power off handler, call it. */

	/* Put the soft power button back under hardware control.

	 * If the user had already pressed the power button, the

 ipmi_poweroff may have been installed. */

	/* It seems we have no way to power the system off via

 prevent soft lockup/stalled CPU messages for endless loop. */

	/* Only needs to handle fpu stuff or perf monitors.

	** REVISIT: several arches implement a "lazy fpu state".

/*

 * Idle thread support

 *

 * Detect when running on QEMU with SeaBIOS PDC Firmware and let

 * QEMU idle the host too.

 nop on real hardware, qemu will offline CPU. */

 nop on real hardware, qemu will idle sleep. */

/*

 * Copy architecture-specific thread state

	/* We have to use void * instead of a function pointer, because

	 * function pointers aren't a pointer to the function on 64-bit.

 kernel thread */

 idle thread */

		/* Must exit via ret_from_kernel_thread in order

		 * to call schedule_tail()

		/*

		 * Copy function and argument to be called from

		 * ret_from_kernel_thread.

 user thread */

		/* usp must be word aligned.  This also prevents users from

		 * passing in the value 1 (which is the signal for a special

 Setup thread TLS area */

	/*

	 * These bracket the sleeping functions..

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Parisc performance counters

 *  Copyright (C) 2001 Randolph Chung <tausq@debian.org>

 *

 *  This code is derived, with permission, from HP/UX sources.

/*

 *  Edited comment from original sources:

 *

 *  This driver programs the PCX-U/PCX-W performance counters

 *  on the PA-RISC 2.0 chips.  The driver keeps all images now

 *  internally to the kernel to hopefully eliminate the possibility

 *  of a bad image halting the CPU.  Also, there are different

 *  images for the PCX-W and later chips vs the PCX-U chips.

 *

 *  Only 1 process is allowed to access the driver at any time,

 *  so the only protection that is needed is at open and close.

 *  A variable "perf_enabled" is used to hold the state of the

 *  driver.  The spinlock "perf_lock" is used to protect the

 *  modification of the state during open/close operations so

 *  multiple processes don't get into the driver simultaneously.

 *

 *  This driver accesses the processor directly vs going through

 *  the PDC INTRIGUE calls.  This is done to eliminate bugs introduced

 *  in various PDC revisions.  The code is much more maintainable

 *  and reliable this way vs having to debug on every version of PDC

 *  on every box.

 for __raw_read() */

 derived from hpux's PI v2 interface */

 definition of RDR regs */

 RDRs to write for PCX-W */

 RDRs to write for PCX-U */

 RDR register descriptions for PCX-W */

 RDR 0 */

 RDR 1 */

 RDR 2 */

 RDR 3 */

 RDR 4 */

 RDR 5 */

 RDR 6 */

 RDR 7 */

 RDR 8 */

 RDR 9 */

 RDR 10 */

 RDR 11 */

 RDR 12 */

 RDR 13 */

 RDR 14 */

 RDR 15 */

 RDR 16 */

 RDR 17 */

 RDR 18 */

 RDR 19 */

 RDR 20 */

 RDR 21 */

 RDR 22 */

 RDR 23 */

 RDR 24 */

 RDR 25 */

 RDR 26 */

 RDR 27 */

 RDR 28 */

 RDR 29 */

 RDR 30 */

 RDR 31 */

 RDR register descriptions for PCX-U */

 RDR 0 */

 RDR 1 */

 RDR 2 */

 RDR 3 */

 RDR 4 */

 RDR 5 */

 RDR 6 */

 RDR 7 */

 RDR 8 */

 RDR 9 */

 RDR 10 */

 RDR 11 */

 RDR 12 */

 RDR 13 */

 RDR 14 */

 RDR 15 */

 RDR 16 */

 RDR 17 */

 RDR 18 */

 RDR 19 */

 RDR 20 */

 RDR 21 */

 RDR 22 */

 RDR 23 */

 RDR 24 */

 RDR 25 */

 RDR 26 */

 RDR 27 */

 RDR 28 */

 RDR 29 */

 RDR 30 */

 RDR 31 */

/*

 * A non-zero write_control in the above tables is a byte offset into

 * this array.

 first dbl word must be zero */

 RDR0 bitmask */

 RDR1 bitmask */

 RDR20-RDR21 bitmask (152 bits) */

 RDR22-RDR23 bitmask (233 bits) */

/*

 * Write control bitmasks for Pa-8700 processor given

 * some things have changed slightly.

 first dbl word must be zero */

 RDR0 bitmask */

 RDR1 bitmask */

 RDR20-RDR21 bitmask (158 bits) */

 RDR22-RDR23 bitmask (210 bits) */

 array of bitmasks to use */

/******************************************************************************

 * Function Prototypes

 External Assembly Routines */

/******************************************************************************

 * Function Definitions

/*

 * configure:

 *

 * Configure the cpu with a given data image.  First turn off the counters,

 * then download the image, then turn the counters back on.

 Stop the counters*/

 Write the image to the chip */

 Start the counters */

/*

 * Open the device and initialize all of its memory.  The device is only

 * opened once, but can be "queried" by multiple processes that know its

 * file descriptor.

/*

 * Close the device.

/*

 * Read does nothing for this driver

/*

 * write:

 *

 * This routine downloads the image to the chip.  It must be

 * called on the processor that the download should happen

 * on.

 Get the interface type and test type */

 Make sure everything makes sense */

	/* First check the machine type is correct for

	/* Next check to make sure the requested image

 Copy the image into the processor */

/*

 * Patch the images that need to know the IVA addresses.

 FIXME!! */

/*

 * NOTE:  this routine is VERY specific to the current TLB image.

 * If the image is changed, this routine might also need to be changed.

	/*

	 * We can only use the lower 32-bits, the upper 32-bits should be 0

	 * anyway given this is in the kernel

 clear last 2 bytes */

 set 2 bytes */

 clear last 2 bytes */

 set 2 bytes */

 clear last 2 bytes */

 set 2 bytes */

 clear last 2 bytes */

 set 2 bytes */

 Cuda interface */

 Unknown type */

/*

 * ioctl routine

 * All routines effect the processor that they are executed on.  Thus you

 * must be running on the processor that you wish to change.

 Start the counters */

 copy out the Counters */

 Return the version # */

/*

 * Initialize the module

 Determine correct processor interface to use */

 Patch the images to match the system */

 TODO: this only lets us access the first cpu.. what to do for SMP? */

/*

 * perf_start_counters(void)

 *

 * Start the counters.

 Enable performance monitor counters */

/*

 * perf_stop_counters

 *

 * Stop the performance counters and save counts

 * in a per_processor array.

 Disable performance counters */

		/*

		 * Read the counters

 Counter0 is bits 1398 to 1429 */

 OR sticky0 (bit 1430) to counter0 bit 32 */

 Counter1 is bits 1431 to 1462 */

 OR sticky1 (bit 1463) to counter1 bit 32 */

 Counter2 is bits 1464 to 1495 */

 OR sticky2 (bit 1496) to counter2 bit 32 */

 Counter3 is bits 1497 to 1528 */

 OR sticky3 (bit 1529) to counter3 bit 32 */

		/*

		 * Zero out the counters

		/*

		 * The counters and sticky-bits comprise the last 132 bits

		 * (1398 - 1529) of RDR16 on a U chip.  We'll zero these

		 * out the easy way: zero out last 10 bits of dword 21,

		 * all of dword 22 and 58 bits (plus 6 don't care bits) of

		 * dword 23.

 0 to last 10 bits */

		/*

		 * Write back the zeroed bytes + the image given

		 * the read was destructive.

		/*

		 * Read RDR-15 which contains the counters and sticky bits

		/*

		 * Clear out the counters

		/*

		 * Copy the counters 

/*

 * perf_rdr_get_entry

 *

 * Retrieve a pointer to the description of what this

 * RDR contains.

/*

 * perf_rdr_read_ubuf

 *

 * Read the RDR value into the buffer specified.

 Clear out buffer */

 Check for bits an even number of 64 */

 Grab all of the data */

/*

 * perf_rdr_clear

 *

 * Zero out the given RDR register

/*

 * perf_write_image

 *

 * Write the given image out to the processor

 Clear out counters */

 Toggle performance monitor */

 Write all RDRs */

	/*

	 * Now copy out the Runway stuff which is not in RDRs

 Merge intrigue bits into Runway STATUS 0 */

 Write RUNWAY DEBUG registers */

/*

 * perf_rdr_write

 *

 * Write the given RDR register with the contents

 * of the given buffer.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Stack trace management functions

 *

 *  Copyright (C) 2009-2021 Helge Deller <deller@gmx.de>

 *  based on arch/x86/kernel/stacktrace.c by Ingo Molnar <mingo@redhat.com>

 *  and parisc unwind functions by Randolph Chung <tausq@debian.org>

 *

 *  TODO: Userspace stacktrace (CONFIG_USER_STACKTRACE_SUPPORT)

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/parisc/traps.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *  Copyright (C) 1999, 2000  Philipp Rumpf <prumpf@tux.org>

/*

 * 'Traps.c' handles hardware traps and faults after we have saved some

 * state in 'asm.s'.

 for handle_fpe() */

 fpregs are 64-bit always */

	/* FR are 64bit everywhere. Need to use asm to get the content

	 * of fpsr/fper1, and we assume that we won't have a FP Identify

	 * in our way, otherwise we're screwed.

	 * The fldd is used to restore the T-bit if there was one, as the

	 * store clears it anyway.

 here we'll print fr0 again, tho it'll be meaningless */

 STFU */

 Amuse the user in a SPARC fashion */

 unlock the pdc lock if necessary */

	/* maybe the kernel hasn't booted very far yet and hasn't been able 

	 * to initialize the serial or STI console. In that case we should 

	 * re-enable the pdc console, so that the user will be able to 

 Wot's wrong wif bein' racy? */

 gdb uses break 4,8 */

 check if a BUG() or WARN() trapped here.  */

 return to next instruction when WARN_ON().  */

 send standard GDB signal */

	/*

	 * Note: The following code will probably generate a

	 * bunch of truncation error warnings from the compiler.

	 * Could be handled with an ifdef, but perhaps there

	 * is a better way.

    /*

     * The following fields only have meaning if we came through

     * another path. So just zero them here.

/*

 * This routine is called as a last resort when everything else

 * has gone clearly wrong. We get called for faults in kernel space,

 * and HPMC's.

 unlock the pdc lock if necessary */

 restart pdc console if necessary */

 Not all paths will gutter the processor... */

 show_stack(NULL, (unsigned long *)regs->gr[30]); */

	/* put soft power button back under hardware control;

	 * if the user had pressed it once at any time, the 

	/* Call kernel panic() so reboot timeouts work properly 

	 * FIXME: This function should be on the list of

	 * panic notifiers, and we should call panic

	 * directly from the location that we wish. 

	 * e.g. We should not call panic from

	 * parisc_terminate, but rather the oter way around.

	 * This hack works, prints the panic message twice,

	 * and it enables reboot timers!

 switch back to pdc if HPMC */

	/* Security check:

	 * If the priority level is still user, and the

	 * faulting space is not equal to the active space

	 * then the user is attempting something in a space

	 * that does not belong to them. Kill the process.

	 *

	 * This is normally the situation when the user

	 * attempts to jump into the kernel space at the

	 * wrong offset, be it at the gateway page or a

	 * random location.

	 *

	 * We cannot normally signal the process because it

	 * could *be* on the gateway page, and processes

	 * executing on the gateway page can't have signals

	 * delivered.

	 * 

	 * We merely readjust the address into the users

	 * space, at a destination address of zero, and

	 * allow processing to continue.

 Kill the user process later */

 High-priority machine check (HPMC) */

 set up a new led state on systems shipped with a LED State panel */

 NOT REACHED */

 Power failure interrupt */

 Recovery counter trap */

 else this must be the start of a syscall - just let it run */

 Low-priority machine check */

 Instruction TLB miss fault/Instruction page fault */

 Illegal instruction trap */

 Break instruction trap */

 Privileged operation trap */

 Privileged register trap */

			/* This is a MFCTL cr26/cr27 to gr instruction.

			 * PCXS traps on this, so we need to emulate it.

 Overflow Trap, let the userland signal handler do the cleanup */

		/* Conditional Trap

		   The condition succeeds in an instruction which traps

			/* Let userspace app figure it out from the insn pointed

			 * to by si_addr.

 The kernel doesn't want to handle condition codes */

 Assist Exception Trap, i.e. floating point exception. */

 quiet */

 Data TLB miss fault/Data page fault */

 Non-access instruction TLB miss fault */

		/* The instruction TLB entry needed for the target address of the FIC

 Non-access data TLB miss fault/Non-access data page fault */

		/* FIXME: 

			 Still need to add slow path emulation code here!

			 If the insn used a non-shadow register, then the tlb

			 handlers could not have their side-effect (e.g. probe

			 writing to a target register) emulated since rfir would

			 erase the changes to said register. Instead we have to

			 setup everything, call this function we are in, and emulate

			 by hand. Technically we need to emulate:

			 fdc,fdce,pdc,"fic,4f",prober,probeir,probew, probeiw

 PCXS only -- later cpu's split this into types 26,27 & 28 */

 Check for unaligned access */

 PCXL: Data memory access rights trap */

 Data memory break trap */

 So we can single-step over the trap */

 Page reference trap */

 Taken branch trap */

		/* else this must be the start of a syscall - just let it

		 * run.

 Instruction access rights */

 PCXL: Instruction memory protection trap */

		/*

		 * This could be caused by either: 1) a process attempting

		 * to execute within a vma that does not have execute

		 * permission, or 2) an access rights violation caused by a

		 * flush only translation set up by ptep_get_and_clear().

		 * So we check the vma permissions to differentiate the two.

		 * If the vma indicates we have execute permission, then

		 * the cause is the latter one. In this case, we need to

		 * call do_page_fault() to fix the problem.

 call do_page_fault() */

 Data memory protection ID trap */

 Unaligned data reference trap */

 SIGBUS, for lack of a better one. */

 NOT REACHED */

	    /*

	     * The kernel should never fault on its own address space,

	     * unless pagefault_disable() was called before.

 Clean up and return if in exception table. */

 Clean up and return if handled by kfence. */

	/*

	 * Use PDC_INSTR firmware function to get instruction that invokes

	 * PDCE_CHECK in HPMC handler.  See programming note at page 1-31 of

	 * the PA 1.1 Firmware Architecture document.

	/*

	 * Rules for the checksum of the HPMC handler:

	 * 1. The IVA does not point to PDC/PDH space (ie: the OS has installed

	 *    its own IVA).

	 * 2. The word at IVA + 32 is nonzero.

	 * 3. If Length (IVA + 60) is not zero, then Length (IVA + 60) and

	 *    Address (IVA + 56) are word-aligned.

	 * 4. The checksum of the 8 words starting at IVA + 32 plus the sum of

	 *    the Length/4 words starting at Address is zero.

 Setup IVA and compute checksum for HPMC handler */

/* early_trap_init() is called before we set up kernel mappings and

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *    Initial setup-routines for HP 9000 based hardware.

 *

 *    Copyright (C) 1991, 1992, 1995  Linus Torvalds

 *    Modifications for PA-RISC (C) 1999 Helge Deller <deller@gmx.de>

 *    Modifications copyright 1999 SuSE GmbH (Philipp Rumpf)

 *    Modifications copyright 2000 Martin K. Petersen <mkp@mkp.net>

 *    Modifications copyright 2000 Philipp Rumpf <prumpf@tux.org>

 *    Modifications copyright 2001 Ryan Bradetich <rbradetich@uswest.net>

 *

 *    Initial PA-RISC Version: 04-23-1999 by Helge Deller

 for pa7300lc_init() proto */

 Intended for ccio/sba/cpu statistics under /proc/bus/{runway|gsc} */

 Collect stuff passed in from the boot loader */

 boot_args[0] is free-mem start, boot_args[1] is ptr to command line */

 called from hpux boot loader */

 did palo pass us a ramdisk? */

		/*

		 * We've got way too many dependencies on 1.1 semantics

		 * to support 1.0 boxes at this point.

 Set Modes & Enable FP */

	/*

	 * Check if initial kernel page mappings are sufficient.

	 * panic early if not, else we may access kernel functions

	 * and variables which can't be reached.

 probe for physical memory */

 initialize the LCD/LED after boot_cpu_data is available ! */

 LCD/LED initialization */

/*

 * Display CPU info for all CPUs.

 * for parisc this is in processor.c

    	/* Looks like the caller will call repeatedly until we return

	 * 0, signaling EOF perhaps.  This could be used to sequence

	 * through CPUs for example.  Since we print all cpu info in our

	 * show_cpuinfo() disregarding 'pos' (which I assume is 'v' above)

	/*

	** Can't call proc_mkdir() until after proc_root_init() has been

	** called by start_kernel(). In other words, this code can't

	** live in arch/.../setup.c because start_parisc() calls

	** start_kernel().

		/* FIXME: this was added to prevent the compiler 

		 * complaining about missing pcx, pcxs and pcxt

 probe for hardware */

 set up a new led state on systems shipped LED State panel */

 tell PDC we're Linux. Nevermind failure. */

 start with known state */

 Don't serialize TLB flushes if we run on one CPU only. */

 These are in a non-obvious order, will fix when we have an iotree */

 CCIO before any potential subdevices */

	/*

	 * Need to register Asp & Wax before the EISA adapters for the IRQ

	 * regions.  EISA must come before PCI to be sure it gets IRQ region

	 * 0.

 register LED port info in procfs */

 check QEMU/SeaBIOS marker in PAGE0 */

 initialize checksum of fault_vector */

 not reached

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *    PARISC specific syscalls

 *

 *    Copyright (C) 1999-2003 Matthew Wilcox <willy at parisc-linux.org>

 *    Copyright (C) 2000-2003 Paul Bame <bame at parisc-linux.org>

 *    Copyright (C) 2001 Thomas Bogendoerfer <tsbogend at parisc-linux.org>

 *    Copyright (C) 1999-2020 Helge Deller <deller@gmx.de>

/* we construct an artificial offset for the mapping based on the physical

 nothing */ }

 1 GB */		\

/*

 * Top of mmap area (just below the process stack).

/*

 * When called from arch_get_unmapped_area(), rlim_stack will be NULL,

 * indicating that "current" should be used instead of a passed-in

 * value from the exec bprm as done with arch_pick_mmap_layout().

 Limit stack size - see setup_arg_pages() in fs/exec.c */

 Add space for stack randomization. */

 requested length too big for entire address space */

 requesting a specific address */

	/*

	 * A failed mmap() very likely causes application failure,

	 * so fall back to the bottom-up function here. This scenario

	 * can happen with large stack limits and large mmap()

	 * allocations.

	/* parisc stack always grows up - so a unlimited stack should

	 * not be an indicator to use the legacy memory layout.

	 * if (rlimit(RLIMIT_STACK) == RLIM_INFINITY)

	 *	return 1;

/*

 * This function, called very early during the creation of a new

 * process VM image, sets up which VM layout function to use:

	/* Make sure the shift for mmap2 is constant (12), no matter what PAGE_SIZE

 Fucking broken ABI */

/* stubs for the benefit of the syscall_table since truncate64 and truncate 

/*

 * Up to kernel v5.9 we defined O_NONBLOCK as 000200004,

 * since then O_NONBLOCK is defined as 000200000.

 *

 * The following wrapper functions mask out the old

 * O_NDELAY bit from calls which use O_NONBLOCK.

 *

 * XXX: Remove those in year 2022 (or later)?

 SPDX-License-Identifier: GPL-2.0

/*

 * sys_parisc32.c: Conversion between 32bit and 64bit native syscalls.

 *

 * Copyright (C) 2000-2001 Hewlett Packard Company

 * Copyright (C) 2000 John Marvin

 * Copyright (C) 2001 Matthew Wilcox

 * Copyright (C) 2014 Helge Deller <deller@gmx.de>

 *

 * These routines maintain argument size conversion between 32bit and 64bit

 * environment. Based heavily on sys_ia32.c and sys_sparc32.c.

 SPDX-License-Identifier: GPL-2.0-only

/* 

 *    interfaces to Chassis Codes via PDC (firmware)

 *

 *    Copyright (C) 2002 Laurent Canet <canetl@esiee.fr>

 *    Copyright (C) 2002-2006 Thibaut VARENE <varenet@parisc-linux.org>

 *

 *    TODO: poll chassis warns, trigger (configurable) machine shutdown when

 *    		needed.

 *    	    Find out how to get Chassis warnings out of PAT boxes?

/**

 * pdc_chassis_setup() - Enable/disable pdc_chassis code at boot time.

 * @str configuration param: 0 to disable chassis log

 * @return 1

panic_timeout = simple_strtoul(str, NULL, 0);*/

/** 

 * pdc_chassis_checkold() - Checks for old PDC_CHASSIS compatibility

 * @pdc_chassis_old: 1 if old pdc chassis style

 * 

 * Currently, only E class and A180 are known to work with this.

 * Inspired by Christoph Plattner

 E25 */

 E35 */

 E45 */

 E55 */

 A180 */

/**

 * pdc_chassis_panic_event() - Called by the panic handler.

 *

 * As soon as a panic occurs, we should inform the PDC.

/**

 * parisc_reboot_event() - Called by the reboot handler.

 *

 * As soon as a reboot occurs, we should inform the PDC.

 CONFIG_PDC_CHASSIS */

/**

 * parisc_pdc_chassis_init() - Called at boot time.

 Let see if we have something to handle... */

 initialize panic notifier chain */

 initialize reboot notifier chain */

 CONFIG_PDC_CHASSIS */

/** 

 * pdc_chassis_send_status() - Sends a predefined message to the chassis,

 * and changes the front panel LEDs according to the new system state

 * @retval: PDC call return value.

 *

 * Only machines with 64 bits PDC PAT and those reported in

 * pdc_chassis_checkold() are supported atm.

 * 

 * returns 0 if no error, -1 if no supported PDC is present or invalid message,

 * else returns the appropriate PDC error code.

 * 

 * For a list of predefined messages, see asm-parisc/pdc_chassis.h

 Maybe we should do that in an other way ? */

 CONFIG_64BIT */

 if (pdc_chassis_enabled) */

 CONFIG_PDC_CHASSIS */

 seems that some boxes (eg L1000) do not implement this */

 CONFIG_PROC_FS */

 CONFIG_PDC_CHASSIS_WARN */

 SPDX-License-Identifier: GPL-2.0

/*

 *   linux/arch/parisc/kernel/pa7300lc.c

 *	- PA7300LC-specific functions	

 *

 CPU register indices */

 this returns the HPA of the CPU it was called on */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * inventory.c

 *

 * Copyright (c) 1999 The Puffin Group (David Kennedy and Alex deVries)

 * Copyright (c) 2001 Matthew Wilcox for Hewlett-Packard

 *

 * These are the routines to discover what hardware exists in this box.

 * This task is complicated by there being 3 different ways of

 * performing an inventory, depending largely on the age of the box.

 * The recommended way to do this is to check to see whether the machine

 * is a `Snake' first, then try System Map, then try PAT.  We try System

 * Map before checking for a Snake -- this probably doesn't cause any

 * problems, but...

/*

** Debug options

** DEBUG_PAT Dump details which PDC PAT provides about ranges/devices.

 cell number and location (PAT firmware only) */

 Determine the pdc "type" used on this machine */

	/*

	 * If the machine doesn't support PDC_SYSTEM_MAP then either it

	 * is a pdc pat box, or it is an older box. All 64 bit capable

	 * machines are either pdc pat boxes or they support PDC_SYSTEM_MAP.

	/*

	 * TODO: We should test for 64 bit capability and give a

	 * clearer message.

 Check the CPU's bus ID.  There's probably a better test.  */

 720, 730, 750, 735, 755 */

 705, 710 */

 715, 725 */

 745, 747, 742 */

 712 and similar */

 715/64, at least */

 Everything else */

 pdc pages are always 4k */

	/* Rather than aligning and potentially throwing away

	 * memory, we'll assume that any ranges are already

	 * nicely aligned with any reasonable page size, and

	 * panic if they are not (it's more likely that the

	 * pdc info is bad in this case).

	/* Use the 32 bit information from page zero to create a single

	 * entry in the pmem_ranges[] table.

	 *

	 * We currently don't support machines with contiguous memory

	 * >= 4 Gb, who report that memory using 64 bit only fields

	 * on page zero. It's not worth doing until it can be tested,

	 * and it is not clear we can support those machines for other

	 * reasons.

	 *

	 * If that support is done in the future, this is where it

	 * should be done.

 All of the PDC PAT specific code is 64-bit only */

/*

**  The module object is filled via PDC_PAT_CELL[Return Cell Module].

**  If a module is found, register module will get the IODC bytes via

**  pdc_iodc_read() using the PA view of conf_base_addr for the hpa parameter.

**

**  The IO view can be used by PDC_PAT_CELL[Return Cell Module]

**  only for SBAs and LBAs.  This view will cause an invalid

**  argument error for all other cell module types.

**

 64-bit scratch value */

 PDC return value status */

 return cell module (PA or Processor view) */

 no more cell modules or error */

 alloc_pa_dev sets dev->hpa */

	/*

	** save parameters in the parisc_device

	** (The idea being the device driver will call pdc_pat_cell_module()

	** and store the results in its own data structure.)

 save generic info returned from the call */

 REVISIT: who is the consumer of this? not sure yet... */

 pass to PAT_GET_ENTITY() */

 advertise device */

 dump what we see so far... */

 type */

 start */

 finish (ie end) */

 type */

 start */

 finish (ie end) */

 DEBUG_PAT */

/* pat pdc can return information about a variety of different

 * types of memory (e.g. firmware,i/o, etc) but we only care about

 * the usable physical ram right now. Since the firmware specific

 * information is allocated on the stack, we'll be generous, in

 * case there is a lot of other information we don't care about.

		/* The above pdc call shouldn't fail, but, just in

		 * case, just use the PAGE0 info.

	/* Copy information into the firmware independent pmem_ranges

	 * array, skipping types we don't care about. Notice we said

	 * "may" above. We'll use all the entries that were returned.

 Global firmware independent table */

	/*

	** Note:  Prelude (and it's successors: Lclass, A400/500) only

	**        implement PDC_PAT_CELL sub-options 0 and 2.

 We only look for extended memory ranges on a 64 bit capable box */

		/* The above pdc call only works on boxes with sprockets

		 * firmware (newer B,C,J class). Other non PAT PDC machines

		 * do support more than 3.75 Gb of memory, but we don't

		 * support them yet.

 Global firmware independent table */

 !CONFIG_64BIT */

 !CONFIG_64BIT */

 Code to support Snake machines (7[2350], 7[235]5, 715/Scorpio) */

/**

 * snake_inventory

 *

 * Before PDC_SYSTEM_MAP was invented, the PDC_MEM_MAP call was used.

 * To use it, we initialise the mod_path.bc to 0xff and try all values of

 * mod to get the HPA for the top-level devices.  Bus adapters may have

 * sub-devices which are discovered by setting bc[5] to 0 and bc[4] to the

 * module, then trying all possible functions.

 CONFIG_PA20 */

 CONFIG_PA20 */

 Common 32/64 bit based code goes here */

/**

 * add_system_map_addresses - Add additional addresses to the parisc device.

 * @dev: The parisc device.

 * @num_addrs: Then number of addresses to add;

 * @module_instance: The system_map module instance.

 *

 * This function adds any additional addresses reported by the system_map

 * firmware to the parisc device.

/**

 * system_map_inventory - Retrieve firmware devices via SYSTEM_MAP.

 *

 * This function attempts to retrieve and register all the devices firmware

 * knows about via the SYSTEM_MAP PDC call.

 if available, get the additional addresses for a module */

 SeaBIOS stored it here */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/parisc/kernel/time.c

 *

 *  Copyright (C) 1991, 1992, 1995  Linus Torvalds

 *  Modifications for ARM (C) 1994, 1995, 1996,1997 Russell King

 *  Copyright (C) 1999 SuSE GmbH, (Philipp Rumpf, prumpf@tux.org)

 *

 * 1994-07-02  Alan Modra

 *             fixed set_rtc_mmss, fixed time.year for >= 2000, new mktime

 * 1998-12-20  Updated NTP code according to technical memorandum Jan '96

 *             "A Kernel Model for Precision Timekeeping" by Dave Mills

 timer cycles per tick */

/*

 * We keep time on PA-RISC Linux by using the Interval Timer which is

 * a pair of registers; one is read-only and one is write-only; both

 * accessed through CR16.  The read-only register is 32 or 64 bits wide,

 * and increments by 1 every CPU clock tick.  The architecture only

 * guarantees us a rate between 0.5 and 2, but all implementations use a

 * rate of 1.  The write-only register is 32-bits wide.  When the lowest

 * 32 bits of the read-only register compare equal to the write-only

 * register, it raises a maskable external interrupt.  Each processor has

 * an Interval Timer of its own and they are not synchronised.  

 *

 * We want to generate an interrupt every 1/HZ seconds.  So we program

 * CR16 to interrupt every @clocktick cycles.  The it_value in cpu_data

 * is programmed with the intended time of the next tick.  We can be

 * held off for an arbitrarily long period of time by interrupts being

 * disabled, so we may miss one or more ticks.

 gcc can optimize for "read-only" case with a local clocktick */

 Initialize next_tick to the old expected tick time. */

 Calculate how many ticks have elapsed. */

 Store (in CR16 cycles) up to when we are accounting right now. */

 Go do system house keeping. */

	/* Skip clockticks on purpose if we know we would miss those.

	 * The new CR16 must be "later" than current CR16 otherwise

	 * itimer would not fire until CR16 wrapped - e.g 4 seconds

	 * later on a 1Ghz processor. We'll account for the missed

	 * ticks on the next timer interrupt.

	 * We want IT to fire modulo clocktick even if we miss/skip some.

	 * But those interrupts don't in fact get delivered that regularly.

	 *

	 * "next_tick - now" will always give the difference regardless

	 * if one or the other wrapped. If "now" is "bigger" we'll end up

	 * with a very large unsigned number.

	/* Program the IT when to deliver the next interrupt.

	 * Only bottom 32-bits of next_tick are writable in CR16!

	 * Timer interrupt will be delivered at least a few hundred cycles

	 * after the IT fires, so if we are too close (<= 8000 cycles) to the

	 * next cycle, simply skip it.

 clock source code */

 kick off Interval Timer (CR16) */

 we treat tod_sec as unsigned, so this can work until year 2106 */

 hppa has Y2K38 problem: pdc_tod_set() takes an u32 value! */

/*

 * timer interrupt and sched_clock() initialization

 get CPU 0 started */

 Hz */

 register as sched_clock source */

	/*

	 * The cr16 interval timers are not syncronized across CPUs on

	 * different sockets, so mark them unstable and lower rating on

	 * multi-socket SMP systems.

 mark sched_clock unstable */

 register at clocksource framework */

 SPDX-License-Identifier: GPL-2.0

 give other CPUs time to show their backtrace */

 should never reach this */

/*

 * arch/parisc/kernel/topology.c

 *

 * Copyright (C) 2017 Helge Deller <deller@gmx.de>

 *

 * based on arch/arm/kernel/topology.c

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 /*

  * cpu topology table

 update core and thread sibling masks */

/*

 * store_cpu_topology is called at boot when only one cpu is running

 * and with the mutex cpu_hotplug.lock locked, when several cpus have booted,

 * which prevents simultaneous write access to cpu_topology array

 If the cpu topology has been already set, just return */

 create cpu topology mapping */

 ignore current cpu */

/*

 * init_cpu_topology is called at boot when only one cpu is running

 * which prevent simultaneous write access to cpu_topology array

 init core mask and capacity */

 Set scheduler topology descriptor */

 SPDX-License-Identifier: GPL-2.0-or-later

/* 

 *    PDC Console support - ie use firmware to dump text via boot console

 *

 *    Copyright (C) 1999-2003 Matthew Wilcox <willy at parisc-linux.org>

 *    Copyright (C) 2000 Martin K Petersen <mkp at mkp.net>

 *    Copyright (C) 2000 John Marvin <jsm at parisc-linux.org>

 *    Copyright (C) 2000-2003 Paul Bame <bame at parisc-linux.org>

 *    Copyright (C) 2000 Philipp Rumpf <prumpf with tux.org>

 *    Copyright (C) 2000 Michael Ang <mang with subcarrier.org>

 *    Copyright (C) 2000 Grant Grundler <grundler with parisc-linux.org>

 *    Copyright (C) 2001-2002 Ryan Bradetich <rbrad at parisc-linux.org>

 *    Copyright (C) 2001 Helge Deller <deller at parisc-linux.org>

 *    Copyright (C) 2001 Thomas Bogendoerfer <tsbogend at parisc-linux.org>

 *    Copyright (C) 2002 Randolph Chung <tausq with parisc-linux.org>

 *    Copyright (C) 2010 Guy Martin <gmsoft at tuxicoman.be>

/*

 *  The PDC console is a simple console, which can be used for debugging 

 *  boot related problems on HP PA-RISC machines. It is also useful when no

 *  other console works.

 *

 *  This code uses the ROM (=PDC) based functions to read and write characters

 *  from and to PDC's boot path.

/* Define EARLY_BOOTUP_DEBUG to debug kernel related boot problems. 

 for PAGE0 */

 for iodc_call() proto and friends */

 no limit, no buffer used */

	/* Check if the console driver is still registered.

	 * It is unregistered if the pdc console was not selected as the

 If the console is duplex then copy the COUT parameters to CIN. */

 register the pdc console */

/*

 * Used for emergencies. Currently only used if an HPMC occurs. If an

 * HPMC occurs, it is possible that the current console may not be

 * properly initialised after the PDC IO reset. This routine unregisters

 * all of the current consoles, reinitializes the pdc console and

 * registers it.

 If we've already seen the output, don't bother to print it again */

 force registering the pdc console */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2019 Helge Deller <deller@gmx.de>

 *

 * Based on arch/arm64/kernel/jump_label.c

		/*

		 * Encode the PA1.1 "b,n" instruction with a 17-bit

		 * displacement.  In case we hit the BUG(), we could use

		 * another branch instruction with a 22-bit displacement on

		 * 64-bit CPUs instead. But this seems sufficient for now.

	/*

	 * We use the architected NOP in arch_static_branch, so there's no

	 * need to patch an identical NOP over the top of it here. The core

	 * will call arch_jump_label_transform from a module notifier if the

	 * NOP needs to be replaced by a branch.

 SPDX-License-Identifier: GPL-2.0

/*

 * Code for tracing calls in Linux kernel.

 * Copyright (C) 2009-2016 Helge Deller <deller@gmx.de>

 *

 * based on code for x86 which is:

 * Copyright (C) 2007-2008 Steven Rostedt <srostedt@redhat.com>

 *

 * future possible enhancements:

 *	- add CONFIG_STACK_TRACER

/*

 * Hook the return address and push it in the stack of return addrs

 * in current thread info.

 activate parisc_return_to_handler() as return point */

 CONFIG_FUNCTION_GRAPH_TRACER */

 calculate pointer to %rp in stack */

 sanity check: parent_rp should hold parent */

 std,ma r1,100(sp) */

 ldd -10(r1),r1 */

 bve,n (r1) */

 b,l,n .-14,r1 */

 ldo 100(sp),sp */

 std r1,-100(sp) */

 ldo -4(r1),r1 */

 ldd -20(r1),r1 */

 bve,n (r1) */

 b,l,n .-20,r1 */

 stw,ma r1,40(sp) */

 ldw -18(r1),r1 */

 bv,n r0(r1) */

 b,l,n .-c,r1 */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

** PARISC 1.1 Dynamic DMA mapping support.

** This implementation is for PA-RISC platforms that do not support

** I/O TLBs (aka DMA address translation hardware).

** See Documentation/core-api/dma-api-howto.rst for interface definitions.

**

**      (c) Copyright 1999,2000 Hewlett-Packard Company

**      (c) Copyright 2000 Grant Grundler

**	(c) Copyright 2000 Philipp Rumpf <prumpf@tux.org>

**      (c) Copyright 2000 John Marvin

**

** "leveraged" from 2.3.47: arch/ia64/kernel/pci-dma.c.

** (I assume it's from David Mosberger-Tang but there was no Copyright)

**

** AFAIK, all PA7100LC and PA7300LC platforms can use this code.

**

** - ggg

 for DMA_CHUNK_SIZE */

 get_order */

 for purge_tlb_*() macros */

 Start of pcxl dma mapping area */

/*

** Dump a hex representation of the resource map.

	/* 

	** return the corresponding vaddr in the pcxl dma map

 BUG_ON((*res_ptr & m) != m); */ \

/*

** clear bits in the pcxl resource map

 8 bits per byte */

/* This probably isn't needed to support EISA cards.

** ISA cards will certainly only support 24-bit DMA addressing.

** Not clear if we can, want, or need to support ISA.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * drivers.c

 *

 * Copyright (c) 1999 The Puffin Group

 * Copyright (c) 2001 Matthew Wilcox for Hewlett Packard

 * Copyright (c) 2001 Helge Deller <deller@gmx.de>

 * Copyright (c) 2001,2002 Ryan Bradetich 

 * Copyright (c) 2004-2005 Thibaut VARENE <varenet@parisc-linux.org>

 * 

 * The file handles registering devices and drivers, then matching them.

 * It's the closest we get to a dating agency.

 *

 * If you're thinking about modifying this file, here are some gotchas to

 * bear in mind:

 *  - 715/Mirage device paths have a dummy device between Lasi and its children

 *  - The EISA adapter may show up as a sibling or child of Wax

 *  - Dino has an optionally functional serial port.  If firmware enables it,

 *    it shows up as a child of Dino.  If firmware disables it, the buswalk

 *    finds it and it shows up as a child of Cujo

 *  - Dino has both parisc and pci devices as children

 *  - parisc devices are discovered in a random order, including children

 *    before parents in some cases.

 See comments in include/asm-parisc/pci.h */

/**

 *	for_each_padev - Iterate over all devices in the tree

 *	@fn:	Function to call for each device.

 *	@data:	Data to pass to the called function.

 *

 *	This performs a depth-first traversal of the tree, calling the

 *	function passed for each node.  It calls the function for parents

 *	before children.

/**

 * match_device - Report whether this driver can handle this device

 * @driver: the PA-RISC driver to try

 * @dev: the PA-RISC device to try

/**

 * register_parisc_driver - Register this driver if it can handle a device

 * @driver: the PA-RISC driver to try

	/* FIXME: we need this because apparently the sti

 We install our own probe and remove routines */

/**

 * count_parisc_driver - count # of devices this driver would match

 * @driver: the PA-RISC driver to try

 *

 * Use by IOMMU support to "guess" the right size IOPdir.

 * Formula is something like memsize/(num_iommu * entry_size).

/**

 * unregister_parisc_driver - Unregister this driver from the list of drivers

 * @driver: the PA-RISC driver to unregister

/**

 * find_pa_parent_type - Find a parent of a specific type

 * @dev: The device to start searching from

 * @type: The device type to search for.

 *

 * Walks up the device tree looking for a device of the specified type.

 * If it finds it, it returns it.  If not, it returns NULL.

/*

 * get_node_path fills in @path with the firmware path to the device.

 * Note that if @node is a parisc device, we don't fill in the 'mod' field.

 * This is because both callers pass the parent and fill in the mod

 * themselves.  If @node is a PCI device, we do fill it in, even though this

 * is inconsistent.

/**

 * print_pa_hwpath - Returns hardware path for PA devices

 * dev: The device to return the path for

 * output: Pointer to a previously-allocated array to place the path in.

 *

 * This function fills in the output array with a human-readable path

 * to a PA device.  This string is compatible with that used by PDC, and

 * may be printed on the outside of the box.

/**

 * get_pci_node_path - Determines the hardware path for a PCI device

 * @pdev: The device to return the path for

 * @path: Pointer to a previously-allocated array to place the path in.

 *

 * This function fills in the hardware_path structure with the route to

 * the specified PCI device.  This structure is suitable for passing to

 * PDC calls.

/**

 * print_pci_hwpath - Returns hardware path for PCI devices

 * dev: The device to return the path for

 * output: Pointer to a previously-allocated array to place the path in.

 *

 * This function fills in the output array with a human-readable path

 * to a PCI device.  This string is compatible with that used by PDC, and

 * may be printed on the outside of the box.

 defined(CONFIG_PCI) || defined(CONFIG_ISA) */

 PARISC devices are 32-bit */

 make the generic dma mask a pointer to the parisc one */

/**

 * alloc_tree_node - returns a device entry in the iotree

 * @parent: the parent node in the tree

 * @id: the element of the module path for this entry

 *

 * Checks all the children of @parent for a matching @id.  If none

 * found, it allocates a new device and returns it.

 Check to make sure this device has not already been added - Ryan */

	/* This is awkward.  The STI spec says that gfx devices may occupy

	 * 32MB or 64MB.  Unfortunately, we don't know how to tell whether

	 * it's the former or the latter.  Assumptions either way can hurt us.

	/* Silently fail things like mouse ports which are subsumed within

	 * the keyboard controller

/**

 * register_parisc_device - Locate a driver to manage this device.

 * @dev: The parisc device.

 *

 * Search the driver list for a driver that is willing to manage

 * this device.

/**

 * match_pci_device - Matches a pci device against a given hardware path

 * entry.

 * @dev: the generic device (known to be contained by a pci_dev).

 * @index: the current BC index

 * @modpath: the hardware path.

 * @return: true if the device matches the hardware path.

 we are at the end of the path, and on the actual device */

 index might be out of bounds for bc[] */

/**

 * match_parisc_device - Matches a parisc device against a given hardware

 * path entry.

 * @dev: the generic device (known to be contained by a parisc_device).

 * @index: the current BC index

 * @modpath: the hardware path.

 * @return: true if the device matches the hardware path.

 we are on a bus bridge */

/**

 * parse_tree_node - returns a device entry in the iotree

 * @parent: the parent node in the tree

 * @index: the current BC index

 * @modpath: the hardware_path struct to match a device against

 * @return: The corresponding device if found, NULL otherwise.

 *

 * Checks all the children of @parent for a matching @id.  If none

 * found, it returns NULL.

 nothing */;

/**

 * hwpath_to_device - Finds the generic device corresponding to a given hardware path.

 * @modpath: the hardware path.

 * @return: The target device, NULL if not found.

 pci devices already parse MOD */

/**

 * device_to_hwpath - Populates the hwpath corresponding to the given device.

 * @param dev the target device

 * @param path pointer to a previously allocated hwpath struct to be filled in

/**

 * walk_native_bus -- Probe a bus for devices

 * @io_io_low: Base address of this bus.

 * @io_io_high: Last address of this bus.

 * @parent: The parent bus device.

 * 

 * A native bus (eg Runway or GSC) may have up to 64 devices on it,

 * spaced at intervals of 0x1000 bytes.  PDC may not inform us of these

 * devices, so we have to probe for them.  Unfortunately, we may find

 * devices which are not physically connected (such as extra serial &

 * keyboard ports).  This problem is not yet solved.

 Was the device already added by Firmware? */

/**

 * walk_central_bus - Find devices attached to the central bus

 *

 * PDC doesn't tell us about all devices in the system.  This routine

 * finds devices connected to the central bus.

/**

 * init_parisc_bus - Some preparation to be done before inventory

 AUTO-GENERATED HEADER FILE FOR SEABIOS FIRMWARE */\n");

 generated with Linux kernel */\n");

 search for PARISC_QEMU_MACHINE_HEADER in Linux */\n\n");

 print iodc data of the various hpa modules for qemu inclusion */

 pad: 0x%04x, 0x%04x */\n",

/**

 * print_parisc_devices - Print out a list of devices found in this system

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1997, 1998 Ralf Baechle

 * Copyright (C) 1999 SuSE GmbH

 * Copyright (C) 1999-2001 Hewlett-Packard Company

 * Copyright (C) 1999-2001 Grant Grundler

 parisc_pci_hba used by pci_port->in/out() ops to lookup bus data.  */

/********************************************************************

**

** I/O port space support

**

/* EISA port numbers and PCI port numbers share the same interface.  Some

 * machines have both EISA and PCI adapters installed.  Rather than turn

 * pci_port into an array, we reserve bus 0 for EISA and call the EISA

 * routines if the access is to a port on bus 0.  We don't want to fix

 * EISA and ISA drivers which assume port space is <= 0xffff.

/*

 * BIOS32 replacement.

 Set the CLS for PCI as early as possible. */

 Called from pci_do_scan_bus() *after* walking a bus but before walking PPBs. */

/*

 * Called by pci_set_master() - a driver interface.

 *

 * Legacy PDC guarantees to set:

 *	Map Memory BAR's into PA IO space.

 *	Map Expansion ROM BAR into one common PA IO space per bus.

 *	Map IO BAR's into PCI IO space.

 *	Command (see below)

 *	Cache Line Size

 *	Latency Timer

 *	Interrupt Line

 *	PPB: secondary latency timer, io/mmio base/limit,

 *		bus numbers, bridge control

 *

 If someone already mucked with this, don't touch it. */

	/*

	** HP generally has fewer devices on the bus than other architectures.

	** upper byte is PCI_LATENCY_TIMER.

/*

 * pcibios_init_bridge() initializes cache line and default latency

 * for pci controllers and pci-pci bridges

 We deal only with pci controllers and pci-pci bridges. */

	/* PCI-PCI bridge - set the cache line and default latency

	 * (32) for primary and secondary buses.

/*

 * pcibios align resources() is called every time generic PCI code

 * wants to generate a new address. The process of looking for

 * an available address, each candidate is first "aligned" and

 * then checked if the resource is available until a match is found.

 *

 * Since we are just checking candidates, don't use any fields other

 * than res->start.

 If it's not IO, then it's gotta be MEM */

 Align to largest of MIN or input size */

/*

 * A driver is enabling the device.  We make sure that all the appropriate

 * bits are set to allow the device to operate as the driver is expecting.

 * We enable the port IO and memory IO bits if the device has any BARs of

 * that type, and we enable the PERR and SERR bits unconditionally.

 * Drivers that do not need parity (eg graphics and possibly networking)

 * can clear these bits if they want.

 If bridge/bus controller has FBB enabled, child must too. */

 PA-RISC specific */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *    Initial setup-routines for HP 9000 based hardware.

 *

 *    Copyright (C) 1991, 1992, 1995  Linus Torvalds

 *    Modifications for PA-RISC (C) 1999-2008 Helge Deller <deller@gmx.de>

 *    Modifications copyright 1999 SuSE GmbH (Philipp Rumpf)

 *    Modifications copyright 2000 Martin K. Petersen <mkp@mkp.net>

 *    Modifications copyright 2000 Philipp Rumpf <prumpf@tux.org>

 *    Modifications copyright 2001 Ryan Bradetich <rbradetich@uswest.net>

 *

 *    Initial PA-RISC Version: 04-23-1999 by Helge Deller

 for register_parisc_driver() stuff */

 for struct irq_region */

/*

**  	PARISC CPU driver - claim "device" and initialize CPU data structures.

**

** Consolidate per CPU initialization into (mostly) one module.

** Monarch CPU will initialize boot_cpu_data which shouldn't

** change once the system has booted.

**

** The callback *should* do per-instance initialization of

** everything including the monarch. "Per CPU" init code in

** setup.c:start_parisc() has migrated here and start_parisc()

** will call register_parisc_driver(&cpu_driver) before calling do_inventory().

**

** The goal of consolidating CPU initialization into one place is

** to make sure all CPUs get initialized the same way.

** The code path not shared is how PDC hands control of the CPU to the OS.

** The initialization of OS data structures is the same (done below).

/**

 * init_cpu_profiler - enable/setup per cpu profiling hooks.

 * @cpunum: The processor instance.

 *

 * FIXME: doesn't do much yet...

/**

 * processor_probe - Determine if processor driver should claim this device.

 * @dev: The device which has been found.

 *

 * Determine if processor driver should claim this chip (return 0) or not 

 * (return 1).  If so, initialize the chip and tell other partners in crime 

 * they have work to do.

	/* logical CPU ID and update global counter

	 * May get overwritten by PAT code.

 for legacy PDC */

 verify it's the same as what do_pat_inventory() found */

 id_eid for IO sapic */

 get the cpu number */

/* We need contiguous numbers for cpuid. Firmware's notion

 * of cpuid is for physical CPUs and we just don't care yet.

 * We'll care when we need to query PAT PDC about a CPU *after*

 * boot time (ie shutdown a CPU from an OS perspective).

 Ignore CPU since it will only crash */

 initialize counters - CPU 0 gets it_value set in time_init() */

 Save IODC data in case we need it */

 save CPU hpa */

 save CPU id */

 save CPU IRQ address */

	/*

	** FIXME: review if any other initialization is clobbered

	**	  for boot_cpu by the above memset().

	/*

	** CONFIG_SMP: init_smp_config() will attempt to get CPUs into

	** OS control. RENDEZVOUS is the default state - see mem_set above.

	**	p->state = STATE_RENDEZVOUS;

 CPU 0 IRQ table is statically allocated/initialized */

		/*

		** itimer and ipi IRQ handlers are statically initialized in

		** arch/parisc/kernel/irq.c. ie Don't need to register them.

 not getting it's own table, share with monarch */

	/* 

	 * Bring this CPU up now! (ignore bootstrap cpuid == 0)

/**

 * collect_boot_cpu_data - Fill the boot_cpu_data structure.

 *

 * This function collects and stores the generic processor information

 * in the boot_cpu_data structure.

 Hz of this PARISC */

 get CPU-Model Information... */

/**

 * init_per_cpu - Handle individual processor initializations.

 * @cpunum: logical processor number.

 *

 * This function handles initialization for *every* CPU

 * in the system:

 *

 * o Set "default" CPU width for trap handlers

 *

 * o Enable FP coprocessor

 *   REVISIT: this could be done in the "code 22" trap handler.

 *	(frowands idea - that way we know which processes need FP

 *	registers saved on the interrupt stack.)

 *   NEWS FLASH: wide kernels need FP coprocessor enabled to handle

 *	formatted printing of %lx for example (double divides I think)

 *

 * o Enable CPU profiling hooks.

 10 == Coprocessor Control Reg */

		/* FWIW, FP rev/model is a more accurate way to determine

		** CPU type. CPU rev/model has some ambiguous cases.

		/*

		** store status register to stack (hopefully aligned)

		** and clear the T-bit.

 previous chars get pushed to console */

 FUTURE: Enable Performance Monitor : ccr bit 0x20 */

/*

 * Display CPU info for all CPUs.

 cpu MHz */

 print cachesize info */

/**

 * processor_init - Processor initialization procedure.

 *

 * Register this driver.

 SPDX-License-Identifier: GPL-2.0-or-later

/*    Signal support for 32-bit kernel builds

 *

 *    Copyright (C) 2001 Matthew Wilcox <willy at parisc-linux.org>

 *    Copyright (C) 2006 Kyle McMartin <kyle at parisc-linux.org>

 *

 *    Code was mostly borrowed from kernel/signal.c.

 *    See kernel/signal.c for additional Copyrights.

	/* When loading 32-bit values into 64-bit registers make

 Load upper half */

 XXX: BE WARNED FR's are 64-BIT! */

	/* Better safe than sorry, pass __get_user two things of

	   the same size and let gcc do the upward conversion to 

 Load upper half */

 Load upper half */

 Load the upper half for iasq */

 Load the upper half for iasq */

 Load the upper half for sar */

/*

 * Set up the sigcontext structure for this process.

 * This is not an easy task if the kernel is 64-bit, it will require

 * that we examine the process personality to determine if we need to

 * truncate for a 32-bit userspace.

 Truncate gr31 */

 regs->iaoq is undefined in the syscall return path */

 Store upper half */

 Store upper half */

 Truncate sr3 */

 Store upper half */

 Store upper half */

 Store upper half */

 Store upper half */

 Store upper half */

 Print out the IAOQ for debugging */		

 Truncate a general register */

 Store upper half */

 DEBUG: Write out the "upper / lower" register data */

	/* Copy the floating point registers (same size)

 Store upper half */

 SPDX-License-Identifier: GPL-2.0

/*

 * Load ELF vmlinux file for the kexec_file_load syscall.

 *

 * Copyright (c) 2019 Sven Schnelle <svens@stackframe.org>

 *

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * arch/parisc/kernel/firmware.c  - safe PDC access routines

 *

 *	PDC == Processor Dependent Code

 *

 * See PDC documentation at

 * https://parisc.wiki.kernel.org/index.php/Technical_Documentation

 * for documentation describing the entry points and calling

 * conventions defined below.

 *

 * Copyright 1999 SuSE GmbH Nuernberg (Philipp Rumpf, prumpf@tux.org)

 * Copyright 1999 The Puffin Group, (Alex deVries, David Kennedy)

 * Copyright 2003 Grant Grundler <grundler parisc-linux org>

 * Copyright 2003,2004 Ryan Bradetich <rbrad@parisc-linux.org>

 * Copyright 2004,2006 Thibaut VARENE <varenet@parisc-linux.org>

/*	I think it would be in everyone's best interest to follow this

 *	guidelines when writing PDC wrappers:

 *

 *	 - the name of the pdc wrapper should match one of the macros

 *	   used for the first two arguments

 *	 - don't use caps for random parts of the name

 *	 - use the static PDC result buffers and "copyout" to structs

 *	   supplied by the caller to encapsulate alignment restrictions

 *	 - hold pdc_lock while in PDC or using static result buffers

 *	 - use __pa() to convert virtual (kernel) pointers to physical

 *	   ones.

 *	 - the name of the struct used for pdc return values should equal

 *	   one of the macros used for the first two arguments to the

 *	   corresponding PDC call

 *	 - keep the order of arguments

 *	 - don't be smart (setting trailing NUL bytes for strings, return

 *	   something useful even if the call failed) unless you are sure

 *	   it's not going to affect functionality or performance

 *

 *	Example:

 *	int pdc_cache_info(struct pdc_cache_info *cache_info )

 *	{

 *		int retval;

 *

 *		spin_lock_irq(&pdc_lock);

 *		retval = mem_pdc_call(PDC_CACHE,PDC_CACHE_INFO,__pa(cache_info),0);

 *		convert_to_wide(pdc_result);

 *		memcpy(cache_info, pdc_result, sizeof(*cache_info));

 *		spin_unlock_irq(&pdc_lock);

 *

 *		return retval;

 *	}

 *					prumpf	991016	

 for boot_cpu_data */

/* Firmware needs to be initially set to narrow to determine the 

/* On most currently-supported platforms, IODC I/O calls are 32-bit calls

 * and MEM_PDC calls are always the same width as the OS.

 * Some PAT boxes may have 64-bit IODC I/O.

 *

 * Ryan Bradetich added the now obsolete CONFIG_PDC_NARROW to allow

 * 64-bit kernels to run on systems with 32-bit MEM_PDC calls.

 * This allowed wide kernels to run on Cxxx boxes.

 * We now detect 32-bit-only PDC and dynamically switch to 32-bit mode

 * when running a 64-bit kernel on such boxes (e.g. C200 or C360).

/**

 * f_extend - Convert PDC addresses to kernel addresses.

 * @address: Address returned from PDC.

 *

 * This function is used to convert PDC addresses into kernel addresses

 * when the PDC address size and kernel address size are different.

/**

 * convert_to_wide - Convert the return buffer addresses into kernel addresses.

 * @address: The return buffer from PDC.

 *

 * This function is used to convert the return buffer addresses retrieved from PDC

 * into kernel addresses when the PDC address size and kernel address size are

 * different.

/**

 * set_firmware_width - Determine if the firmware is wide or narrow.

 * 

 * This function must be called before any pdc_* function that uses the

 * convert_to_wide function.

CONFIG_64BIT*/

/**

 * pdc_emergency_unlock - Unlock the linux pdc lock

 *

 * This call unlocks the linux pdc lock in case we need some PDC functions

 * (like pdc_add_valid) during kernel stack dump.

 Spinlock DEBUG code freaks out if we unconditionally unlock */

/**

 * pdc_add_valid - Verify address can be accessed without causing a HPMC.

 * @address: Address to be verified.

 *

 * This PDC call attempts to read from the specified address and verifies

 * if the address is valid.

 * 

 * The return value is PDC_OK (0) in case accessing this address is valid.

/**

 * pdc_instr - Get instruction that invokes PDCE_CHECK in HPMC handler.

 * @instr: Pointer to variable which will get instruction opcode.

 *

 * The return value is PDC_OK (0) in case call succeeded.

/**

 * pdc_chassis_info - Return chassis information.

 * @result: The return buffer.

 * @chassis_info: The memory buffer address.

 * @len: The size of the memory buffer address.

 *

 * An HVERSION dependent call for returning the chassis information.

/**

 * pdc_pat_chassis_send_log - Sends a PDC PAT CHASSIS log message.

 * @retval: -1 on error, 0 on success. Other value are PDC errors

 * 

 * Must be correctly formatted or expect system crash

/**

 * pdc_chassis_disp - Updates chassis code

 * @retval: -1 on error, 0 on success

/**

 * pdc_cpu_rendenzvous - Stop currently executing CPU

 * @retval: -1 on error, 0 on success

/**

 * pdc_chassis_warn - Fetches chassis warnings

 * @retval: -1 on error, 0 on success

/**

 * pdc_coproc_cfg - To identify coprocessors attached to the processor.

 * @pdc_coproc_info: Return buffer address.

 *

 * This PDC call returns the presence and status of all the coprocessors

 * attached to the processor.

/**

 * pdc_iodc_read - Read data from the modules IODC.

 * @actcnt: The actual number of bytes.

 * @hpa: The HPA of the module for the iodc read.

 * @index: The iodc entry point.

 * @iodc_data: A buffer memory for the iodc options.

 * @iodc_data_size: Size of the memory buffer.

 *

 * This PDC call reads from the IODC of the module specified by the hpa

 * argument.

/**

 * pdc_system_map_find_mods - Locate unarchitected modules.

 * @pdc_mod_info: Return buffer address.

 * @mod_path: pointer to dev path structure.

 * @mod_index: fixed address module index.

 *

 * To locate and identify modules which reside at fixed I/O addresses, which

 * do not self-identify via architected bus walks.

/**

 * pdc_system_map_find_addrs - Retrieve additional address ranges.

 * @pdc_addr_info: Return buffer address.

 * @mod_index: Fixed address module index.

 * @addr_index: Address range index.

 * 

 * Retrieve additional information about subsequent address ranges for modules

 * with multiple address ranges.  

/**

 * pdc_model_info - Return model information about the processor.

 * @model: The return buffer.

 *

 * Returns the version numbers, identifiers, and capabilities from the processor module.

/**

 * pdc_model_sysmodel - Get the system model name.

 * @name: A char array of at least 81 characters.

 *

 * Get system model name from PDC ROM (e.g. 9000/715 or 9000/778/B160L).

 * Using OS_ID_HPUX will return the equivalent of the 'modelname' command

 * on HP/UX.

 add trailing '\0' */

/**

 * pdc_model_versions - Identify the version number of each processor.

 * @cpu_id: The return buffer.

 * @id: The id of the processor to check.

 *

 * Returns the version number for each processor component.

 *

 * This comment was here before, but I do not know what it means :( -RB

 * id: 0 = cpu revision, 1 = boot-rom-version

/**

 * pdc_model_cpuid - Returns the CPU_ID.

 * @cpu_id: The return buffer.

 *

 * Returns the CPU_ID value which uniquely identifies the cpu portion of

 * the processor module.

 preset zero (call may not be implemented!) */

/**

 * pdc_model_capabilities - Returns the platform capabilities.

 * @capabilities: The return buffer.

 *

 * Returns information about platform support for 32- and/or 64-bit

 * OSes, IO-PDIR coherency, and virtual aliasing.

 preset zero (call may not be implemented!) */

/**

 * pdc_model_platform_info - Returns machine product and serial number.

 * @orig_prod_num: Return buffer for original product number.

 * @current_prod_num: Return buffer for current product number.

 * @serial_no: Return buffer for serial number.

 *

 * Returns strings containing the original and current product numbers and the

 * serial number of the system.

/**

 * pdc_cache_info - Return cache and TLB information.

 * @cache_info: The return buffer.

 *

 * Returns information about the processor's cache and TLB.

/**

 * pdc_spaceid_bits - Return whether Space ID hashing is turned on.

 * @space_bits: Should be 0, if not, bad mojo!

 *

 * Returns information about Space ID hashing.

/**

 * pdc_btlb_info - Return block TLB information.

 * @btlb: The return buffer.

 *

 * Returns information about the hardware Block TLB.

/**

 * pdc_mem_map_hpa - Find fixed module information.  

 * @address: The return buffer

 * @mod_path: pointer to dev path structure.

 *

 * This call was developed for S700 workstations to allow the kernel to find

 * the I/O devices (Core I/O). In the future (Kittyhawk and beyond) this

 * call will be replaced (on workstations) by the architected PDC_SYSTEM_MAP

 * call.

 *

 * This call is supported by all existing S700 workstations (up to  Gecko).

 !CONFIG_PA20 */

/**

 * pdc_lan_station_id - Get the LAN address.

 * @lan_addr: The return buffer.

 * @hpa: The network device HPA.

 *

 * Get the LAN station address when it is not directly available from the LAN hardware.

 FIXME: else read MAC from NVRAM */

/**

 * pdc_stable_read - Read data from Stable Storage.

 * @staddr: Stable Storage address to access.

 * @memaddr: The memory address where Stable Storage data shall be copied.

 * @count: number of bytes to transfer. count is multiple of 4.

 *

 * This PDC call reads from the Stable Storage address supplied in staddr

 * and copies count bytes to the memory address memaddr.

 * The call will fail if staddr+count > PDC_STABLE size.

/**

 * pdc_stable_write - Write data to Stable Storage.

 * @staddr: Stable Storage address to access.

 * @memaddr: The memory address where Stable Storage data shall be read from.

 * @count: number of bytes to transfer. count is multiple of 4.

 *

 * This PDC call reads count bytes from the supplied memaddr address,

 * and copies count bytes to the Stable Storage address staddr.

 * The call will fail if staddr+count > PDC_STABLE size.

/**

 * pdc_stable_get_size - Get Stable Storage size in bytes.

 * @size: pointer where the size will be stored.

 *

 * This PDC call returns the number of bytes in the processor's Stable

 * Storage, which is the number of contiguous bytes implemented in Stable

 * Storage starting from staddr=0. size in an unsigned 64-bit integer

 * which is a multiple of four.

/**

 * pdc_stable_verify_contents - Checks that Stable Storage contents are valid.

 *

 * This PDC call is meant to be used to check the integrity of the current

 * contents of Stable Storage.

/**

 * pdc_stable_initialize - Sets Stable Storage contents to zero and initialize

 * the validity indicator.

 *

 * This PDC call will erase all contents of Stable Storage. Use with care!

/**

 * pdc_get_initiator - Get the SCSI Interface Card params (SCSI ID, SDTR, SE or LVD)

 * @hwpath: fully bc.mod style path to the device.

 * @initiator: the array to return the result into

 *

 * Get the SCSI operational parameters from PDC.

 * Needed since HPUX never used BIOS or symbios card NVRAM.

 * Most ncr/sym cards won't have an entry and just use whatever

 * capabilities of the card are (eg Ultra, LVD). But there are

 * several cases where it's useful:

 *    o set SCSI id for Multi-initiator clusters,

 *    o cable too long (ie SE scsi 10Mhz won't support 6m length),

 *    o bus width exported is less than what the interface chip supports.

 BCJ-XXXX series boxes. E.G. "9000/785/C3000" */

	/*

	 * Sprockets and Piranha return 20 or 40 (MT/s).  Prelude returns

	 * 1, 2, 5 or 10 for 5, 10, 20 or 40 MT/s, respectively

/**

 * pdc_pci_irt_size - Get the number of entries in the interrupt routing table.

 * @num_entries: The return value.

 * @hpa: The HPA for the device.

 *

 * This PDC function returns the number of entries in the specified cell's

 * interrupt table.

 * Similar to PDC_PAT stuff - but added for Forte/Allegro boxes

/** 

 * pdc_pci_irt - Get the PCI interrupt routing table.

 * @num_entries: The number of entries in the table.

 * @hpa: The Hard Physical Address of the device.

 * @tbl: 

 *

 * Get the PCI interrupt routing table for the device at the given HPA.

 * Similar to PDC_PAT stuff - but added for Forte/Allegro boxes

 UNTEST CODE - left here in case someone needs it */

/** 

 * pdc_pci_config_read - read PCI config space.

 * @hpa		token from PDC to indicate which PCI device

 * @pci_addr	configuration space address to read from

 *

 * Read PCI Configuration space *before* linux PCI subsystem is running.

/** 

 * pdc_pci_config_write - read PCI config space.

 * @hpa		token from PDC to indicate which PCI device

 * @pci_addr	configuration space address to write

 * @val		value we want in the 32-bit register

 *

 * Write PCI Configuration space *before* linux PCI subsystem is running.

 UNTESTED CODE */

/**

 * pdc_tod_read - Read the Time-Of-Day clock.

 * @tod: The return buffer:

 *

 * Read the Time-Of-Day clock

	/*

	 * 64-bit kernels should not call this PDT function in narrow mode.

	 * The pdt_entries_ptr array above will now contain 32-bit values

/**

 * pdc_pim_toc11 - Fetch TOC PIM 1.1 data from firmware.

 * @ret: pointer to return buffer

/**

 * pdc_pim_toc20 - Fetch TOC PIM 2.0 data from firmware.

 * @ret: pointer to return buffer

/**

 * pdc_tod_set - Set the Time-Of-Day clock.

 * @sec: The number of seconds since epoch.

 * @usec: The number of micro seconds.

 *

 * Set the Time-Of-Day clock.

 CONFIG_64BIT */

/* FIXME: Is this pdc used?  I could not find type reference to ftc_bitmap

 * so I guessed at unsigned long.  Someone who knows what this does, can fix

 * it later. :)

/*

 * pdc_do_reset - Reset the system.

 *

 * Reset the system.

/*

 * pdc_soft_power_info - Enable soft power switch.

 * @power_reg: address of soft power register

 *

 * Return the absolute address of the soft power switch register

/*

 * pdc_soft_power_button - Control the soft power button behaviour

 * @sw_control: 0 for hardware control, 1 for software control 

 *

 *

 * This PDC function places the soft power button under software or

 * hardware control.

 * Under software control the OS may control to when to allow to shut 

 * down the system. Under hardware control pressing the power button 

 * powers off the system immediately.

/*

 * pdc_io_reset - Hack to avoid overlapping range registers of Bridges devices.

 * Primarily a problem on T600 (which parisc-linux doesn't support) but

 * who knows what other platform firmware might do with this OS "hook".

/*

 * pdc_io_reset_devices - Hack to Stop USB controller

 *

 * If PDC used the usb controller, the usb controller

 * is still running and will crash the machines during iommu 

 * setup, because of still running DMA. This PDC call

 * stops the USB controller.

 * Normally called after calling pdc_io_reset().

 defined(BOOTLOADER) */

 locked by pdc_console_lock */

/**

 * pdc_iodc_print - Console print using IODC.

 * @str: the string to output.

 * @count: length of str

 *

 * Note that only these special chars are architected for console IODC io:

 * BEL, BS, CR, and LF. Others are passed through.

 * Since the HP console requires CR+LF to perform a 'newline', we translate

 * "\n" to "\r\n".

/**

 * pdc_iodc_getc - Read a character (non-blocking) from the PDC console.

 *

 * Read a character (non-blocking) from the PDC console, returns -1 if

 * key is not present.

 Bail if no console input device. */

 wait for a keyboard (rs232)-input */

/**

 * pdc_pat_cell_get_number - Returns the cell number.

 * @cell_info: The return buffer.

 *

 * This PDC call returns the cell number of the cell from which the call

 * is made.

/**

 * pdc_pat_cell_module - Retrieve the cell's module information.

 * @actcnt: The number of bytes written to mem_addr.

 * @ploc: The physical location.

 * @mod: The module index.

 * @view_type: The view of the address type.

 * @mem_addr: The return buffer.

 *

 * This PDC call returns information about each module attached to the cell

 * at the specified location.

/**

 * pdc_pat_cell_info - Retrieve the cell's information.

 * @info: The pointer to a struct pdc_pat_cell_info_rtn_block.

 * @actcnt: The number of bytes which should be written to info.

 * @offset: offset of the structure.

 * @cell_number: The cell number which should be asked, or -1 for current cell.

 *

 * This PDC call returns information about the given cell (or all cells).

/**

 * pdc_pat_cpu_get_number - Retrieve the cpu number.

 * @cpu_info: The return buffer.

 * @hpa: The Hard Physical Address of the CPU.

 *

 * Retrieve the cpu number for the cpu at the specified HPA.

/**

 * pdc_pat_get_irt_size - Retrieve the number of entries in the cell's interrupt table.

 * @num_entries: The return value.

 * @cell_num: The target cell.

 *

 * This PDC function returns the number of entries in the specified cell's

 * interrupt table.

/**

 * pdc_pat_get_irt - Retrieve the cell's interrupt table.

 * @r_addr: The return buffer.

 * @cell_num: The target cell.

 *

 * This PDC function returns the actual interrupt table for the specified cell.

/**

 * pdc_pat_pd_get_addr_map - Retrieve information about memory address ranges.

 * @actlen: The return buffer.

 * @mem_addr: Pointer to the memory buffer.

 * @count: The number of bytes to read from the buffer.

 * @offset: The offset with respect to the beginning of the buffer.

 *

/**

 * pdc_pat_pd_get_PDC_interface_revisions - Retrieve PDC interface revisions.

 * @legacy_rev: The legacy revision.

 * @pat_rev: The PAT revision.

 * @pdc_cap: The PDC capabilities.

 *

/**

 * pdc_pat_io_pci_cfg_read - Read PCI configuration space.

 * @pci_addr: PCI configuration space address for which the read request is being made.

 * @pci_size: Size of read in bytes. Valid values are 1, 2, and 4. 

 * @mem_addr: Pointer to return memory buffer.

 *

/**

 * pdc_pat_io_pci_cfg_write - Retrieve information about memory address ranges.

 * @pci_addr: PCI configuration space address for which the write  request is being made.

 * @pci_size: Size of write in bytes. Valid values are 1, 2, and 4. 

 * @value: Pointer to 1, 2, or 4 byte value in low order end of argument to be 

 *         written to PCI Config space.

 *

/**

 * pdc_pat_mem_pdc_info - Retrieve information about page deallocation table

 * @rinfo: memory pdt information

 *

/**

 * pdc_pat_mem_pdt_cell_info - Retrieve information about page deallocation

 *				table of a cell

 * @rinfo: memory pdt information

 * @cell: cell number

 *

/**

 * pdc_pat_mem_read_cell_pdt - Read PDT entries from (old) PAT firmware

 * @pret: array of PDT entries

 * @pdt_entries_ptr: ptr to hold number of PDT entries

 * @max_entries: maximum number of entries to be read

 *

 PDC_PAT_MEM_CELL_READ is available on early PAT machines only */

 build up return value as for PDC_PAT_MEM_PD_READ */

/**

 * pdc_pat_mem_read_pd_pdt - Read PDT entries from (newer) PAT firmware

 * @pret: array of PDT entries

 * @pdt_entries_ptr: ptr to hold number of PDT entries

 * @count: number of bytes to read

 * @offset: offset to start (in bytes)

 *

/**

 * pdc_pat_mem_get_dimm_phys_location - Get physical DIMM slot via PAT firmware

 * @pret: ptr to hold returned information

 * @phys_addr: physical address to examine

 *

 CONFIG_64BIT */

 defined(BOOTLOADER) */

**************** 32-bit real-mode calls ***********/

/* The struct below is used

 * to overlay real_stack (real2.S), preparing a 32-bit call frame.

 * real32_call_asm() then uses this stack in narrow real mode

 use int, not long which is 64 bits */

 in reality, there's nearly 8k of stack after this */

**************** 64-bit real-mode calls ***********/

 rp, previous sp */

 in reality, there's nearly 8k of stack after this */

 CONFIG_64BIT */

 SPDX-License-Identifier: GPL-2.0

/*

 *    Page Deallocation Table (PDT) support

 *

 *    The Page Deallocation Table (PDT) is maintained by firmware and holds a

 *    list of memory addresses in which memory errors were detected.

 *    The list contains both single-bit (correctable) and double-bit

 *    (uncorrectable) errors.

 *

 *    Copyright 2017 by Helge Deller <deller@gmx.de>

 *

 *    possible future enhancements:

 *    - add userspace interface via procfs or sysfs to clear PDT

 PDT poll interval: 1 minute if errors, 5 minutes if everything OK. */

 global PDT status information */

/*

 * Constants for the pdt_entry format:

 * A pdt_entry holds the physical address in bits 0-57, bits 58-61 are

 * reserved, bit 62 is the perm bit and bit 63 is the error_type bit.

 * The perm bit indicates whether the error have been verified as a permanent

 * error (value of 1) or has not been verified, and may be transient (value

 * of 0). The error_type bit indicates whether the error is a single bit error

 * (value of 1) or a multiple bit error.

 * On non-PAT machines phys_addr is encoded in bits 0-59 and error_type in bit

 * 63. Those machines don't provide the perm bit.

 report PDT entries via /proc/meminfo */

 newer PAT machines like C8000 report info for all cells */

 older PAT machines like rp5470 report cell info only */

 show DIMM slot description on PAT machines */

/*

 * pdc_pdt_init()

 *

 * Initialize kernel PDT structures, read initial PDT table from firmware,

 * report all current PDT entries and mark bad memory with memblock_reserve()

 * to avoid that the kernel will use broken memory areas.

 *

 non-PAT machines provide the standard PDC call */

 mark memory page bad */

/*

 * This is the PDT kernel thread main loop.

 Do we have new PDT entries? */

 if no new PDT entries, just wait again */

 decrease poll interval in case we found memory errors */

 limit entries to get */

 get new entries */

 report and mark memory broken */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *    Architecture-specific kernel symbols

 *

 *    Copyright (C) 2000-2001 Richard Hirst <rhirst with parisc-linux.org>

 *    Copyright (C) 2001 Dave Kennedy

 *    Copyright (C) 2001 Paul Bame <bame at parisc-linux.org>

 *    Copyright (C) 2001-2003 Grant Grundler <grundler with parisc-linux.org>

 *    Copyright (C) 2002-2003 Matthew Wilcox <willy at parisc-linux.org>

 *    Copyright (C) 2002 Randolph Chung <tausq at parisc-linux.org>

 *    Copyright (C) 2002-2007 Helge Deller <deller with parisc-linux.org>

 Needed so insmod can set dp value */

 from pacache.S -- needed for clear/copy_page */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

** SMP Support

**

** Copyright (C) 1999 Walt Drummond <drummond@valinux.com>

** Copyright (C) 1999 David Mosberger-Tang <davidm@hpl.hp.com>

** Copyright (C) 2001,2004 Grant Grundler <grundler@parisc-linux.org>

** 

** Lots of stuff stolen from arch/alpha/kernel/smp.c

** ...and then parisc stole from arch/ia64/kernel/smp.c. Thanks David! :^)

**

** Thanks to John Curry and Ullas Ponnadi. I learned a lot from their work.

** -grant (1/12/2001)

**

 for CPU_IRQ_REGION and friends */

 DEBUG_SMP */

 track which CPU is booting */

********* SMP inter processor interrupt and communication routines */

/* XXX REVISIT Ignore for now.

**    *May* need this "hook" to register IPI handler

**    once we have perCPU ExtIntr switch tables.

/*

** Yoink this CPU from the runnable list... 

**

 REVISIT : redirect I/O Interrupts to another CPU? */

 REVISIT : does PM *know* this CPU isn't available? */

 Order bit clearing and data access. */

 Switch */

 before doing more, let in any pending interrupts */

 while (ops) */

/*

 * Called by secondaries to update state and initialize CPU registers.

 arch/parisc/kernel/irq.c */

 arch/parisc/kernel/time.c */

 Set modes and Enable floating point coprocessor */

 Well, support 2.4 linux scheme as well. */

 arch/parisc.../process.c */

 Initialise the idle task for this CPU */

 make sure no IRQs are enabled or pending */

/*

 * Slaves start using C here. Indirectly called from smp_slave_stext.

 * Do what start_kernel() and main() do for boot strap processor (aka monarch)

 start with known state */

 Interrupts have been off until now */

 NOTREACHED */

/*

 * Bring one cpu online.

	/* Let _start know what logical CPU we're booting

	** (offset into init_tasks[],cpu_data[])

	/* 

	** boot strap code needs to know the task address since

	** it also contains the process stack.

	/*

	** This gets PDC to release the CPU from a very tight loop.

	**

	** From the PA-RISC 2.0 Firmware Architecture Reference Specification:

	** "The MEM_RENDEZ vector specifies the location of OS_RENDEZ which 

	** is executed after receiving the rendezvous signal (an interrupt to 

	** EIR{0}). MEM_RENDEZ is valid only when it is nonzero and the 

	** contents of memory are valid."

	/* 

	 * OK, wait a bit for that CPU to finish staggering about. 

	 * Slave will set a bit when it reaches smp_cpu_init().

	 * Once the "monarch CPU" sees the bit change, it can move on.

 Which implies Slave has started up */

 Remember the Slave data */

 Setup BSP mappings */

/*

** inventory.c:do_inventory() hasn't yet been run and thus we

** don't 'discover' the additional CPUs until later.

 SPDX-License-Identifier: GPL-2.0

/*

 * Kernel unwinding support

 *

 * (c) 2002-2004 Randolph Chung <tausq@debian.org>

 *

 * Derived partially from the IA64 implementation. The PA-RISC

 * Runtime Architecture Document is also a useful reference to

 * understand what is happening here

 #define DEBUG 1 */

/*

 * the kernel unwind block is not dynamically allocated so that

 * we can call unwind_init as early in the bootup process as 

 * possible (before the slab allocator is initialized)

 Move-to-front to exploit common traces */

 Called from setup_arch to import the kernel unwind info */

	/*

	 * We have to use void * instead of a function pointer, because

	 * function pointers aren't a pointer to the function on 64-bit.

	 * Make them const so the compiler knows they live in .text

	 * Note: We could use dereference_kernel_function_descriptor()

	 * instead but we want to keep it simple here.

 CONFIG_IRQSTACKS */

		/* Since we are doing the unwinding blind, we don't know if

		   we are adjusting the stack correctly or extracting the rp

		   correctly. The rp is checked to see if it belongs to the

		   kernel text section, if not we assume we don't have a 

		   correct stack frame and we continue to unwind the stack.

		   This is not quite correct, and will fail for loadable

 Check if stack is inside kernel stack area */

 ldo X(sp), sp, or stwm X,D(sp) */

 std,ma X,D(sp) */

 stw rp,-20(sp) */

 std rp,-16(sr0,sp) */

 initialize unwind info */

 unwind stack */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1999-2006 Helge Deller <deller@gmx.de> (07-13-1999)

 * Copyright (C) 1999 SuSE GmbH Nuernberg

 * Copyright (C) 2000 Philipp Rumpf (prumpf@tux.org)

 *

 * Cache and TLB management

 *

/* On some machines (i.e., ones with the Merced bus), there can be

 * only a single PxTLB broadcast at a time; this must be guaranteed

 * by software. We need a spinlock around all TLB flushes to ensure

 * this.

 Swapper page setup lock. */

 Virtual address of pfn.  */

	/* We don't have pte special.  As a result, we can be called with

	   an invalid pfn and we don't need to flush the kernel dcache page.

 BTLB - Block TLB */

	/* "New and Improved" version from Jim Hull 

	 *	(1 << (cc_block-1)) * (cc_line << (4 + cnf.cc_shift))

	 * The following CAFL_STRIDE is an optimized version, see

	 * http://lists.parisc-linux.org/pipermail/parisc-linux/2004-June/023625.html

	 * http://lists.parisc-linux.org/pipermail/parisc-linux/2004-June/023671.html

 We shouldn't get this far.  setup.c should prevent it. */

 pcxl2 doesn't support space register hashing */

 Currently all PA2.0 machines use the same ins. sequence */

 If this procedure isn't implemented, don't panic. */

	/* We have carefully arranged in arch_get_unmapped_area() that

	 * *any* mappings of a file are always congruently mapped (whether

	 * declared as MAP_PRIVATE or MAP_SHARED), so we only need

		/* The TLB is the engine of coherence on parisc: The

		 * CPU is entitled to speculate any page with a TLB

		 * mapping, so here we kill the mapping then flush the

		 * page along a special flush only alias mapping.

		 * This guarantees that the page is no-longer in the

		 * cache for any process and nor may it be

		 * speculatively read in (until the user or kernel

 Defined in arch/parisc/kernel/pacache.S */

 0.5MB */

 16 KiB minimum TLB threshold */

 calculate TLB flush threshold */

	/* On SMP machines, skip the TLB measure of kernel text which

       /* Copy using kernel mapping.  No coherency is needed (all in

	  kunmap) for the `to' page.  However, the `from' page needs to

	  be flushed through a mapping equivalent to the user mapping

/* __flush_tlb_range()

 *

 * returns 1 if all TLBs were flushed.

	/* Purge TLB entries for small ranges using the pdtlb and

	   pitlb instructions.  These instructions execute locally

	/* Flushing the whole cache on each cpu takes forever on

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/parisc/kernel/signal.c: Architecture-specific signal

 *  handling support.

 *

 *  Copyright (C) 2000 David Huggins-Daines <dhd@debian.org>

 *  Copyright (C) 2000 Linuxcare, Inc.

 *

 *  Based on the ia64, i386, and alpha versions.

 *

 *  Like the IA-64, we are a recent enough port (we are *starting*

 *  with glibc2.2) that we do not need to support the old non-realtime

 *  Linux signals.  Therefore we don't.

/* gcc will complain if a pointer is cast to an integer of different

 * size.  If you really need to do this (and we do for an ELF32 user

 * application in an ELF64 kernel) then you have to do a cast to an

 * integer of the same size first.  The A() macro accomplishes

/*

 * Do a signal return - restore sigcontext.

 Trampoline for calling rt_sigreturn() */

 ldi  0,%r25 (in_syscall=0) */

 ldi  1,%r25 (in_syscall=1) */

 ldi  __NR_rt_sigreturn,%r20 */

 be,l 0x100(%sr2,%r0),%sr0,%r31 */

 For debugging */

 stw %r0,0x666(%sr0,%r0) */

 Unwind the user stack to get the rt_sigframe structure. */

 no restarts for sigreturn */

 Good thing we saved the old gr[30], eh? */

 FIXME: Load upper half from register file

	/* If we are on the syscall path IAOQ will not be restored, and

	 * if we are on the interrupt path we must not corrupt gr31.

/*

 * Set up a signal frame.

	/*FIXME: ELF32 vs. ELF64 has different frame_size, but since we

	/* Align alternate stack and reserve 64 bytes for the signal

 Stacks grow up! */

 Stacks grow up.  Fun. */

 regs->iaoq is undefined in the syscall return path */

 The gcc alloca implementation leaves garbage in the upper 32 bits of sp */

 FIXME: Should probably be converted as well for the compat case */

	/* Set up to return from userspace.  If provided, use a stub

	   already in userspace. The first words of tramp are used to

	   save the previous sigrestartblock trampoline that might be

	   on the stack. We start the sigreturn trampoline at 

	/* TRAMP Words 0-4, Length 5 = SIGRESTARTBLOCK_TRAMP

	 * TRAMP Words 5-9, Length 4 = SIGRETURN_TRAMP

	 * So the SIGRETURN_TRAMP is at the end of SIGRESTARTBLOCK_TRAMP

 The sa_handler may be a pointer to a function descriptor */

	/* The syscall return path will create IAOQ values from r31.

		/* If we are singlestepping, arrange a trap to be delivered

		   when we return to userspace. Note the semantics -- we

		   should trap before the first insn in the handler is

		   executed. Ref:

			http://sources.redhat.com/ml/gdb/2004-11/msg00245.html

 userland return pointer */

 signal number */

 siginfo pointer */

 ucontext pointer */

 siginfo pointer */

 ucontext pointer */

 Raise the user stack pointer to make a proper call frame. */

/*

 * OK, we're invoking a handler.

 Set up the stack frame */

/*

 * Check how the syscall number gets loaded into %r20 within

 * the delay branch in userspace and adjust as needed.

	/* Usually we don't have to restore %r20 (the system call number)

	 * because it gets loaded in the delay slot of the branch external

	 * instruction via the ldi instruction.

	 * In some cases a register-to-register copy instruction might have

	 * been used instead, in which case we need to copy the syscall

	 * number into the source register before returning to userspace.

	/* A syscall is just a branch, so all we have to do is fiddle the

	 * return pointer so that the ble instruction gets executed again.

 delayed branching */

 Get assembler opcode of code in delay branch */

 Check if delay branch uses "ldi int,%r20" */

 everything ok, just return */

 Check if delay branch uses "nop" */

 Check if delay branch uses "copy %rX,%r20" */

 no more restarts */

 Check the return code */

 no more restarts */

 Restart the system call - no handlers present */

 check that we don't exceed the stack */

		/* Setup a trampoline to restart the syscall

		 * with __NR_restart_syscall

		 *

		 *  0: <return address (orig r31)>

		 *  4: <2nd half for 64-bit>

		 *  8: ldw 0(%sp), %r31

		 * 12: be 0x100(%sr2, %r0)

		 * 16: ldi __NR_restart_syscall, %r20

 flush data/instruction cache for new insns */

/*

 * We need to be able to restore the syscall arguments (r21-r26) to

 * restart syscalls.  Thus, the syscall path should save them in the

 * pt_regs structure (it's okay to do so since they are caller-save

 * registers).  As noted below, the syscall number gets restored for

 * us due to the magic of delayed branching.

 Restart a system call if necessary. */

 Did we come from a system call? */

 SPDX-License-Identifier: GPL-2.0

/*

 * PA-RISC KGDB support

 *

 * Copyright (c) 2019 Sven Schnelle <svens@stackframe.org>

 *

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Kernel support for the ptrace() and syscall tracing interfaces.

 *

 * Copyright (C) 2000 Hewlett-Packard Co, Linuxcare Inc.

 * Copyright (C) 2000 Matthew Wilcox <matthew@wil.cx>

 * Copyright (C) 2000 David Huggins-Daines <dhd@debian.org>

 * Copyright (C) 2008-2016 Helge Deller <deller@gmx.de>

 PSW bits we allow the debugger to modify */

/*

 * These are our native regset flavors.

/*

 * Called by kernel/ptrace.c when detaching..

 *

 * Make sure single step bits etc are not set.

 make sure the trap bits are not set */

/*

 * The following functions are called by ptrace_resume() when

 * enabling or disabling single/block tracing.

 Nullified, just crank over the queue. */

		/* Don't wake up the task, but let the

 notify_parent(task, SIGCHLD); */

	/* Enable recovery counter traps.  The recovery counter

	 * itself will be set to zero on a task switch.  If the

	 * task is suspended on a syscall then the syscall return

	 * path will overwrite the recovery counter with a suitable

	 * value such that it traps once back in user space.  We

	 * disable interrupts in the tasks PSW here also, to avoid

	 * interrupts while the recovery counter is decrementing.

 Enable taken branch trap. */

	/* Read the word at location addr in the USER area.  For ptraced

	/* Write the word at location addr in the USER area.  This will need

	   to change when the kernel no longer saves all regs on a syscall.

	   FIXME.  There is a problem at the moment in that r3-r18 are only

	   saved if the process is ptraced on syscall entry, and even then

	   those values are overwritten by actual register values on syscall

		/* Some register values written here may be ignored in

		 * entry.S:syscall_restore_rfi; e.g. iaoq is written with

		 * r31/r31+4, and not with the values in pt_regs.

			/* Allow writing to Nullify, Divide-step-correction,

			 * and carry/borrow bits.

			 * BEWARE, if you set N, and then single step, it won't

			 * stop on the nullified instruction.

 ensure userspace privilege */

 Get all gp regs from the child. */

 Set all gp regs in the child. */

 Get the child FPU state. */

 Set the child FPU state. */

/* This function is needed to translate 32 bit pt_regs offsets in to

 * 64 bit pt_regs offsets.  For example, a 32 bit gdb under a 64 bit kernel

 * will request offset 12 if it wants gr3, but the lower 32 bits of

 * the 64 bit kernels view of gr3 will be at offset 28 (3*8 + 4).

 * This code relies on a 32 bit pt_regs being comprised of 32 bit values

 * except for the fp registers which (a) are 64 bits, and (b) follow

 * the gr registers at the start of pt_regs.  The 32 bit pt_regs should

 * be half the size of the 64 bit pt_regs, plus 32*4 to allow for fr[]

 * being 64 bit in both cases.

 gr[0..31] */

 fr[0] ... fr[31] */

 sr[0] ... ipsw */

	/* Write the word at location addr in the USER area.  This will need

	   to change when the kernel no longer saves all regs on a syscall.

	   FIXME.  There is a problem at the moment in that r3-r18 are only

	   saved if the process is ptraced on syscall entry, and even then

	   those values are overwritten by actual register values on syscall

		/* Some register values written here may be ignored in

		 * entry.S:syscall_restore_rfi; e.g. iaoq is written with

		 * r31/r31+4, and not with the values in pt_regs.

			/* Since PT_PSW==0, it is valid for 32 bit processes

			 * under 64 bit kernels as well.

 ensure userspace privilege */

 Special case, fp regs are 64 bits anyway */

 Zero the top 32 bits */

		/*

		 * As tracesys_next does not set %r28 to -ENOSYS

		 * when %r20 is set to -1, initialize it here.

			/*

			 * A nonzero return code from

			 * tracehook_report_syscall_entry() tells us

			 * to prevent the syscall execution.  Skip

			 * the syscall call and the syscall restart handling.

			 *

			 * Note that the tracer may also just change

			 * regs->gr[20] to an invalid syscall number,

			 * that is handled by tracesys_next.

 Do the secure computing check after ptrace. */

	/*

	 * Sign extend the syscall number to 64bit since it may have been

	 * modified by a compat ptrace call

/*

 * regset functions.

	case RI(gr[0]): /*

			 * PSW is in gr[0].

			 * Allow writing to Nullify, Divide-step-correction,

			 * and carry/borrow bits.

			 * BEWARE, if you set N, and then single step, it won't

			 * stop on the nullified instruction.

 set 2 lowest bits to ensure userspace privilege: */

 do not allow to change any of the following registers (yet) */

/*

 * These are the regset flavors matching the 32bit native set.

 CONFIG_64BIT */

 HAVE_REGS_AND_STACK_ACCESS_API feature */

/**

 * regs_query_register_offset() - query register offset from its name

 * @name:	the name of a register

 *

 * regs_query_register_offset() returns the offset of a register in struct

 * pt_regs from its name. If the name is invalid, this returns -EINVAL;

/**

 * regs_query_register_name() - query register name from its offset

 * @offset:	the offset of a register in struct pt_regs.

 *

 * regs_query_register_name() returns the name of a register from its

 * offset in struct pt_regs. If the @offset is invalid, this returns NULL;

/**

 * regs_within_kernel_stack() - check the address in the stack

 * @regs:      pt_regs which contains kernel stack pointer.

 * @addr:      address which is checked.

 *

 * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).

 * If @addr is within the kernel stack, it returns true. If not, returns false.

/**

 * regs_get_kernel_stack_nth() - get Nth entry of the stack

 * @regs:	pt_regs which contains kernel stack pointer.

 * @n:		stack entry number.

 *

 * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which

 * is specified by @regs. If the @n th entry is NOT in the kernel stack,

 * this returns 0.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *    Hardware descriptions for HP 9000 based hardware, including

 *    system types, SCSI controllers, DMA controllers, HPPB controllers

 *    and lots more.

 * 

 *    Based on the document "PA-RISC 1.1 I/O Firmware Architecture 

 *    Reference Specification", March 7, 1999, version 0.96.  This

 *    is available at

 *    https://parisc.wiki.kernel.org/index.php/Technical_Documentation

 *

 *    Copyright 1999 by Alex deVries <alex@onefishtwo.ca>

 *    and copyright 1999 The Puffin Group Inc.

/*

 *	HP PARISC Hardware Database

 *	Access to this database is only possible during bootup

 *	so don't reference this table after starting the init process

 *

 *	NOTE: Product names which are listed here and ends with a '?'

 *	are guessed. If you know the correct name, please let us know.

 Special Marker for last entry */

 0x0000 - 0x000f */

 0x0040 - 0x004f */

 0x0080 - 0x008f */

 0x0100 - 0x010f */

 0x0182 - 0x0183 */

 0x0182 - 0x0183 */

 0x0184 - 0x0184 */

 0x0200 - 0x0201 */

 0x0202 - 0x0202 */

 0x0203 - 0x0203 */

 0x0204 - 0x0207 */

 0x0280 - 0x0283 */

 0x0284 - 0x0287 */

 0x0288 - 0x0288 */

 0x0300 - 0x0303 */

 0x0310 - 0x031f */

 0x0320 - 0x032f */

 0x0400 - 0x040f */

 0x0480 - 0x048f */

 0x0500 - 0x050f */

 0x0510 - 0x051f */

 0x0580 - 0x0587 */

 0x0588 - 0x058b */

 0x058c - 0x058d */

 0x058e - 0x058e */

 0x058f - 0x058f */

 0x0590 - 0x0591 */

 0x0592 - 0x0592 */

 0x0593 - 0x0593 */

 0x0594 - 0x0597 */

 0x0598 - 0x0599 */

 0x059a - 0x059b */

 0x059c - 0x059c */

 0x059d - 0x059d */

 0x059e - 0x059e */

 0x059f - 0x059f */

 0x05a0 - 0x05a1 */

 0x05a2 - 0x05a3 */

 0x05a4 - 0x05a7 */

 0x05a8 - 0x05ab */

 0x05ad - 0x05ad */

 0x05ae - 0x05af */

 0x05b0 - 0x05b1 */

 0x05b2 - 0x05b2 */

 0x05b3 - 0x05b3 */

 0x05b4 - 0x05b4 */

 0x05b5 - 0x05b5 */

 0x05b6 - 0x05b7 */

 0x05b8 - 0x05b9 */

 0x05ba - 0x05ba */

 0x05bb - 0x05bb */

 0x05bc - 0x05bf */

 0x05c0 - 0x05c3 */

 0x05c4 - 0x05c5 */

 0x05c6 - 0x05c6 */

 0x05c7 - 0x05c7 */

 0x05c8 - 0x05cb */

 0x05cc - 0x05cd */

 0x05ce - 0x05cf */

 0x05d0 - 0x05d3 */

 0x05d4 - 0x05d5 */

 0x05d6 - 0x05d6 */

 0x05d7 - 0x05d7 */

 0x05d8 - 0x05db */

 0x05dc - 0x05dd */

 0x05de - 0x05de */

 0x05df - 0x05df */

 0x05e0 - 0x05e3 */

 0x05e4 - 0x05e4 */

 0x05e5 - 0x05e5 */

 0x05e6 - 0x05e7 */

 0x05e8 - 0x05ef */

 0x05f0 - 0x05ff */

 0x0600 - 0x061f */

 0x0880 - 0x088f */

 0x0890 - 0x089f */

 terminate table */

	/*

	 * ok, the above hardware table isn't complete, and we haven't found

	 * our device in this table. So let's now try to find a generic name

	 * to describe the given hardware...

 Interpret hversion (ret[0]) from PDC_MODEL(4)/PDC_MODEL_INFO(0) */

 not reached: */

 SPDX-License-Identifier: GPL-2.0

 /*

  * functions to patch RO kernel text during runtime

  *

  * Copyright (c) 2019 Sven Schnelle <svens@stackframe.org>

 Make sure we don't have any aliases in cache */

			/*

			 * We're crossing a page boundary, so

			 * need to remap

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/parisc/kernel/kprobes.c

 *

 * PA-RISC kprobes implementation

 *

 * Copyright (c) 2019 Sven Schnelle <svens@stackframe.org>

		/*

		 * We have reentered the kprobe_handler, since another kprobe

		 * was hit while within the handler, we save the original

		 * kprobes and single step on the instruction of the new probe

		 * without calling any user handlers to avoid recursive

		 * kprobes.

	/* If we have no pre-handler or it returned 0, we continue with

	 * normal processing. If we have a pre-handler and it returned

	 * non-zero - which means user handler setup registers to exit

	 * to another instruction, we must skip the single stepping.

 restore back original saved kprobe variables and continue */

	/* for absolute branch instructions we can copy iaoq_b. for relative

	 * branch instructions we need to calculate the new address based on the

	 * difference between iaoq_f and iaoq_b. We cannot use iaoq_b without

	 * modificationt because it's based on our ainsn.insn address.

 BE */

 BE,L */

 BV */

 BVE */

		/* for absolute branches, regs->iaoq[1] has already the right

		 * address

 Replace the return addr with trampoline addr. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*    Kernel dynamically loadable module help for PARISC.

 *

 *    The best reference for this stuff is probably the Processor-

 *    Specific ELF Supplement for PA-RISC:

 *        https://parisc.wiki.kernel.org/index.php/File:Elf-pa-hp.pdf

 *

 *    Linux/PA-RISC Project

 *    Copyright (C) 2003 Randolph Chung <tausq at debian . org>

 *    Copyright (C) 2008 Helge Deller <deller@gmx.de>

 *

 *    Notes:

 *    - PLT stub handling

 *      On 32bit (and sometimes 64bit) and with big kernel modules like xfs or

 *      ipv6 the relocation types R_PARISC_PCREL17F and R_PARISC_PCREL22F may

 *      fail to reach their PLT stub if we only create one big stub array for

 *      all sections at the beginning of the core or init section.

 *      Instead we now insert individual PLT stub entries directly in front of

 *      of the code sections where the stubs are actually called.

 *      This reduces the distance between the PCREL location and the stub entry

 *      so that the relocations can be fulfilled.

 *      While calculating the final layout of the kernel module in memory, the

 *      kernel module loader calls arch_mod_section_prepend() to request the

 *      to be reserved amount of memory in front of each individual section.

 *

 *    - SEGREL32 handling

 *      We are not doing SEGREL32 handling correctly. According to the ABI, we

 *      should do a value offset, like this:

 *			if (in_init(me, (void *)val))

 *				val -= (uint32_t)me->init_layout.base;

 *			else

 *				val -= (uint32_t)me->core_layout.base;

 *	However, SEGREL32 is used only for PARISC unwind entries, and we want

 *	those entries to have an absolute address, and not just an offset.

 *

 *	The unwind table mechanism has the ability to specify an offset for

 *	the unwind table; however, because we split off the init functions into

 *	a different piece of memory, it is not possible to do this using a

 *	single offset. Instead, we use the above hack for now.

/* Maximum number of GOT entries. We use a long displacement ldd from

 * the bottom of the table, which has a maximum signed displacement of

 * 0x3fff; however, since we're only going forward, this becomes

 * 0x1fff, and thus, since each GOT entry is 8 bytes long we can have

 * at most 1023 entries.

 * To overcome this 14bit displacement with some kernel modules, we'll

 * use instead the unusal 16bit displacement method (see reassemble_16a)

 * which gives us a maximum positive displacement of 0x7fff, and as such

/* three functions to determine where in the module core

 each stub entry has two insns */

 each stub entry has four insns */

 Field selection types defined by hppa */

 fsel: full 32 bits */

 lsel: select left 21 bits */

 rsel: select right 11 bits */

 lrsel with rounding of addend to nearest 8k */

 rrsel with rounding of addend to nearest 8k */

/* The reassemble_* functions prepare an immediate value for

   insertion into an opcode. pa-risc uses all sorts of weird bitfields

 Unusual 16-bit encoding, for wide mode only.  */

	/* using RWX means less protection for modules, but it's

	 * easier than trying to map the text, data, init_text and

 Additional bytes needed in front of individual sections */

	/* size needed for all stubs of this section (including

		/* some of these are not relevant for 32-bit/64-bit

		 * we leave them here to make the code common. the

		 * compiler will do its thing and optimize out the

		 * stuff we don't need

		/* XXX: By sorting the relocs and finding duplicate entries

		 *  we could reduce the number of necessary stubs and save

 so we need relocation stubs. reserve necessary memory. */

 sh_info gives the section for which we need to add stubs. */

 each code section should only have one relocation section */

 store number of stubs we need for this section */

 align things a bit */

 CONFIG_64BIT */

 Look for existing fdesc entry. */

 Create new one */

 CONFIG_64BIT */

 initialize stub_offset to point in front of the section */

 get correct alignment for the stubs */

 get address of stub entry */

 do not write outside available stub area */

/* for 32-bit the stub looks like this:

 * 	ldil L'XXX,%r1

 * 	be,n R'XXX(%sr4,%r1)

value = *(unsigned long *)((value + addend) & ~3); /* why? */

 ldil L'XXX,%r1	*/

 be,n R'XXX(%sr4,%r1)	*/

/* for 64-bit we have three kinds of stubs:

 * for normal function calls:

 * 	ldd 0(%dp),%dp

 * 	ldd 10(%dp), %r1

 * 	bve (%r1)

 * 	ldd 18(%dp), %dp

 *

 * for millicode:

 * 	ldil 0, %r1

 * 	ldo 0(%r1), %r1

 * 	ldd 10(%r1), %r1

 * 	bve,n (%r1)

 *

 * for direct branches (jumps between different section of the

 * same module):

 *	ldil 0, %r1

 *	ldo 0(%r1), %r1

 *	bve,n (%r1)

 Format 5 */

 ldd 0(%dp),%dp	*/

 Format 3 */

 ldd 0(%dp),%dp	*/

 ldd 10(%dp),%r1	*/

 bve (%r1)		*/

 ldd 18(%dp),%dp	*/

 ldil 0,%r1		*/

 ldo 0(%r1), %r1	*/

 ldd 10(%r1),%r1	*/

 bve,n (%r1)		*/

 ldil 0,%r1           */

 ldo 0(%r1), %r1      */

 bve,n (%r1)          */

unsigned long dp = (unsigned long)$global$;

 This is where to make the change */

 This is the start of the target section */

 This is the symbol it is referring to */

dot = (sechdrs[relsec].sh_addr + rel->r_offset) & ~0x03;

 32-bit function address */

 no function descriptors... */

 direct 32-bit ref */

 left 21 bits of effective address */

 right 14 bits of effective address */

 32-bit segment relative address */

			/* See note about special handling of SEGREL32 at

			 * the beginning of this file.

 32-bit section relative address. */

 left 21 bit of relative address */

 right 14 bit of relative address */

 17-bit PC relative address */

 calculate direct call offset */

				/* direct distance too far, create

 22-bit PC relative address; only defined for pa20 */

 calculate direct call offset */

				/* direct distance too far, create

 32-bit PC relative address */

 This is where to make the change */

 This is the start of the target section */

 This is the symbol it is referring to */

dot = (sechdrs[relsec].sh_addr + rel->r_offset) & ~0x03;

 LT-relative; left 21 bits */

 L(ltoff(val+addend)) */

 LT-relative; right 14 bits */

 PC-relative; 22 bits */

 can we reach it locally? */

				/* this is the case where the symbol is local

				 * to the module, but in a different section,

				 * so stub the jump in case it's more than 22

					/* direct distance too far, create

 Ok, we can reach it directly. */

 32-bit PC relative address */

 64-bit PC relative address */

 64-bit effective address */

 32-bit segment relative address */

			/* See note about special handling of SEGREL32 at

			 * the beginning of this file.

 32-bit section relative address. */

 64-bit function address */

				/* if the symbol is not local to this

				 * module then val+addend is a pointer

	/* haven't filled in me->symtab yet, so have to find it

			/* FIXME: AWFUL HACK

			 * The cast is to drop the const from

 no symbol table */

 we start counting at 1 */

 note, count starts at 1 so preincrement */

 find .altinstructions section */

 patch .altinstructions */

		/* For 32 bit kernels we're compiling modules with

		 * -ffunction-sections so we must relocate the addresses in the

		 *  ftrace callsite section.

 SPDX-License-Identifier: GPL-2.0

/*

 *    Alternative live-patching for parisc.

 *    Copyright (C) 2018 Helge Deller <deller@gmx.de>

 *

		/*

		 * If the PDC_MODEL capabilities has Non-coherent IO-PDIR bit

		 * set (bit #61, big endian), we have to flush and sync every

		 * time IO-PDIR is changed in Ike/Astro.

 Bounce out if none of the conditions are true. */

 Want to replace pdtlb by a pdtlb,l instruction? */

 >= pa2.0 ? */

 set el bit */

		/*

		 * Replace instruction with NOPs?

		 * For long distance insert a branch instruction instead.

 "b,n .+8" */

 Replace multiple instruction by new code */

 Replace by one instruction */

 SPDX-License-Identifier: GPL-2.0-or-later

/* 

 * Code to handle x86 style IRQs plus some generic interrupt stuff.

 *

 * Copyright (C) 1992 Linus Torvalds

 * Copyright (C) 1994, 1995, 1996, 1997, 1998 Ralf Baechle

 * Copyright (C) 1999 SuSE GmbH (Philipp Rumpf, prumpf@tux.org)

 * Copyright (C) 1999-2000 Grant Grundler

 * Copyright (c) 2005 Matthew Wilcox

/* Bits in EIEM correlate with cpu_irq_action[].

** Numbered *Big Endian*! (ie bit 0 is MSB)

/*

** local ACK bitmap ... habitually set to 1, but reset to zero

** between ->ack() and ->end() of the interrupt to prevent

** re-interruption of a processing interrupt.

	/* Do nothing on the other CPUs.  If they get this interrupt,

	 * The & cpu_eiem in the do_cpu_irq_mask() ensures they won't

	 * handle it, and the set_eiem() at the bottom will ensure it

	/* This is just a simple NOP IPI.  But what it does is cause

	 * all the other CPUs to do a set_eiem(cpu_eiem) at the end

 Clear in EIEM so we can no longer process */

 disable the interrupt */

 and now ack it */

 set it in the eiems---it's no longer in process */

 enable the interrupt */

 timer and ipi have to always be received on all CPUs */

 whatever mask they set, we just allow one CPU */

	/* XXX: Needs to be written.  We managed without it so far, but

	 * we really ought to write it.

/*

 * /proc/interrupts printing for arch specific interrupts

/*

** The following form a "set": Virtual IRQ, Transaction Address, Trans Data.

** Respectively, these map to IRQ region+EIRR, Processor HPA, EIRR bit.

**

** To use txn_XXX() interfaces, get a Virtual IRQ first.

** Then use that to get the Transaction address and data.

 for iosapic interrupts */

/*

 * The bits_wide parameter accommodates the limitations of the HW/SW which

 * use these bits:

 * Legacy PA I/O (GSC/NIO): 5 bits (architected EIM register)

 * V-class (EPIC):          6 bits

 * N/L/A-class (iosapic):   8 bits

 * PCI 2.2 MSI:            16 bits

 * Some PCI devices:       32 bits (Symbios SCSI/ATM/HyperFabric)

 *

 * On the service provider side:

 * o PA 1.1 (and PA2.0 narrow mode)     5-bits (width of EIR register)

 * o PA 2.0 wide mode                   6-bits (per processor)

 * o IA64                               8-bits (0-256 total)

 *

 * So a Legacy PA I/O device on a PA 2.0 box can't use all the bits supported

 * by the processor...and the N/L-class I/O subsystem supports more bits than

 * PA2.0 has. The first case is the problem.

 never return irq 0 cause that's the interval timer */

 unlikely, but be prepared */

 assign to "next" CPU we want this bugger on */

 validate entry */

 nothing else, assign monarch */

/*

 * IRQ STACK - used for irq handler

 64k irq stack size */

 32k irq stack size */

	/* if sr7 != 0, we interrupted a userspace process which we do not want

 exit if already in panic */

 calculate kernel stack usage */

 found kernel stack */

 check irq stack usage */

 check kernel stack usage */

 disable further checks */

 in entry.S: */

 align for stack frame usage */

	/* We may be called recursive. If we are already using the irq stack,

	 * just continue to use it. Use spinlocks to serialize

	 * the irq stack usage.

		/* We are using the IRQ stack already.

 This is where we switch to the IRQ stack. */

 free up irq stack usage. */

 CONFIG_IRQSTACKS */

 ONLY called from entry.S:intr_extint() */

 Filter out spurious interrupts, mostly from serial port at bootup */

 CONFIG_IRQSTACKS */

 PARANOID - should already be disabled */

 EIRR : clear all pending external intr */

 EIEM : enable all external intr */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *    Unaligned memory access handler

 *

 *    Copyright (C) 2001 Randolph Chung <tausq@debian.org>

 *    Significantly tweaked by LaMont Jones <lamont@debian.org>

 #define DEBUG_UNALIGNED 1 */

 If you use FIXUP_BRANCH, then you must list this clobber */

 1111 1100 0000 0000 0001 0011 1100 0000 */

 skip LDB - never unaligned (index) */

 skip LDB - never unaligned (short) */

 skip STB - never unaligned */

 skip STBY - never unaligned */

 skip STDBY - never unaligned */

 r19=(ofs&3)*8 */

 r19=(ofs&7)*8 */

 r19=(ofs&3)*8 */

 true if this is a flop */

 log a message with pacing */

 handle modification - OK, it's ugly, see the instruction manual */

 short loads */

 scaled indexed */

 simple indexed */

 TODO: make this cleaner... */

 "undefined", but lets kill them. */

		/*

		 * The unaligned handler failed.

		 * If we were called by __get_user() or __put_user() jump

		 * to it's exception fixup handler instead of crashing.

 couldn't handle it ... */

 else we handled it, let life go on. */

/*

 * NB: check_unaligned() is only used for PCXS processors right

 * now, so we only check for PA1.1 encodings at this point.

 Get alignment mask */

 SPDX-License-Identifier: GPL-2.0-or-later

/* 

 * Generate definitions needed by assembly language modules.

 * This code generates raw asm output which is post-processed to extract

 * and format the required data.

 *

 *    Copyright (C) 2000-2001 John Marvin <jsm at parisc-linux.org>

 *    Copyright (C) 2000 David Huggins-Daines <dhd with pobox.org>

 *    Copyright (C) 2000 Sam Creasey <sammy@sammy.net>

 *    Copyright (C) 2000 Grant Grundler <grundler with parisc-linux.org>

 *    Copyright (C) 2001 Paul Bame <bame at parisc-linux.org>

 *    Copyright (C) 2001 Richard Hirst <rhirst at parisc-linux.org>

 *    Copyright (C) 2002 Randolph Chung <tausq with parisc-linux.org>

 *    Copyright (C) 2003 James Bottomley <jejb at parisc-linux.org>

/* Add FRAME_SIZE to the size x and align it to y. All definitions

 * that use align_frame will include space for a frame.

 PT_SZ_ALGN includes space for a stack frame. */

	/* HUGEPAGE_SIZE is only used in vmlinux.lds.S to align kernel text

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *    Optimized memory copy routines.

 *

 *    Copyright (C) 2004 Randolph Chung <tausq@debian.org>

 *    Copyright (C) 2013-2017 Helge Deller <deller@gmx.de>

 *

 *    Portions derived from the GNU C Library

 *    Copyright (C) 1991, 1997, 2003 Free Software Foundation, Inc.

 Returns 0 for success, otherwise, returns number of bytes not transferred. */

 check for I/O space F_EXTEND(0xfff00000) access as well? */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/parisc/lib/io.c

 *

 * Copyright (c) Matthew Wilcox 2001 for Hewlett-Packard

 * Copyright (c) Randolph Chung 2001 <tausq@debian.org>

 *

 * IO accessing functions which shouldn't be inlined because they're too big

/* Copies a block of memory to a device in an efficient manner.

 * Assumes the device can cope with 32-bit transfers.  If it can't,

 * don't use this function.

/*

** Copies a block of memory from a device in an efficient manner.

** Assumes the device can cope with 32-bit transfers.  If it can't,

** don't use this function.

**

** CR16 counts on C3000 reading 256 bytes from Symbios 896 RAM:

**	27341/64    = 427 cyc per int

**	61311/128   = 478 cyc per short

**	122637/256  = 479 cyc per byte

** Ergo bus latencies dominant (not transfer size).

**      Minimize total number of transfers at cost of CPU cycles.

**	TODO: only look at src alignment and adjust the stores to dest.

 first compare alignment of src/dst */ 

 Then check for misaligned start address */

/* Sets a block of memory on a device to a given value.

 * Assumes the device can cope with 32-bit transfers.  If it can't,

 * don't use this function.

/*

 * Read COUNT 8-bit bytes from port PORT into memory starting at

 * SRC.

/*

 * Read COUNT 16-bit words from port PORT into memory starting at

 * SRC.  SRC must be at least short aligned.  This is used by the

 * IDE driver to read disk sectors.  Performance is important, but

 * the interfaces seems to be slow: just using the inlined version

 * of the inw() breaks things.

 Buffer 32-bit aligned */

 Buffer 16-bit aligned */

 Buffer 8-bit aligned */

		/* I don't bother with 32bit transfers

/*

 * Read COUNT 32-bit words from port PORT into memory starting at

 * SRC. Now works with any alignment in SRC. Performance is important,

 * but the interfaces seems to be slow: just using the inlined version

 * of the inl() breaks things.

 Buffer 32-bit aligned */

 Buffer 16-bit aligned */

 Buffer 8-bit aligned */

 Buffer 8-bit aligned */

/*

 * Like insb but in the opposite direction.

 * Don't worry as much about doing aligned memory transfers:

 * doing byte reads the "slow" way isn't nearly as slow as

 * doing byte writes the slow way (no r-m-w cycle).

/*

 * Like insw but in the opposite direction.  This is used by the IDE

 * driver to write disk sectors.  Performance is important, but the

 * interfaces seems to be slow: just using the inlined version of the

 * outw() breaks things.

 Buffer 32-bit aligned */

 Buffer 16-bit aligned */

 Buffer 8-bit aligned */	

		/* I don't bother with 32bit transfers

/*

 * Like insl but in the opposite direction.  This is used by the IDE

 * driver to write disk sectors.  Works with any alignment in SRC.

 *  Performance is important, but the interfaces seems to be slow:

 * just using the inlined version of the outl() breaks things.

 Buffer 32-bit aligned */

 Buffer 16-bit aligned */

 Buffer 8-bit aligned */

 Buffer 8-bit aligned */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * INET		An implementation of the TCP/IP protocol suite for the LINUX

 *		operating system.  INET is implemented using the  BSD Socket

 *		interface as the means of communication with the user level.

 *

 *		MIPS specific IP/TCP/UDP checksumming routines

 *

 * Authors:	Ralf Baechle, <ralf@waldorf-gmbh.de>

 *		Lots of code moved from tcp.c and ip.c; see those files

 *		for more names.

 32 bits --> 16 bits + carry */

 16 bits + carry --> 16 bits including carry */

 nr of 16-bit words.. */

 nr of 32-bit words.. */

/*

 * computes a partial checksum, e.g. for TCP/UDP fragments

/*

 * why bother folding?

 SPDX-License-Identifier: GPL-2.0

/*

 * iomap.c - Implement iomap interface for PA-RISC

 * Copyright (c) 2004 Matthew Wilcox

/*

 * The iomap space on 32-bit PA-RISC is intended to look like this:

 * 00000000-7fffffff virtual mapped IO

 * 80000000-8fffffff ISA/EISA port space that can't be virtually mapped

 * 90000000-9fffffff Dino port space

 * a0000000-afffffff Astro port space

 * b0000000-bfffffff PAT port space

 * c0000000-cfffffff non-swapped memory IO

 * f0000000-ffffffff legacy IO memory pointers

 *

 * For the moment, here's what it looks like:

 * 80000000-8fffffff All ISA/EISA port space

 * f0000000-ffffffff legacy IO memory pointers

 *

 * On 64-bit, everything is extended, so:

 * 8000000000000000-8fffffffffffffff All ISA/EISA port space

 * f000000000000000-ffffffffffffffff legacy IO memory pointers

/*

 * Technically, this should be 'if (VMALLOC_START < addr < VMALLOC_END),

 * but that's slow and we know it'll be within the first 2GB.

 Generic ioport ops.  To be replaced later by specific dino/elroy/wax code */

 Legacy I/O memory ops */

 Repeating interfaces */

 Mapping interfaces */

 SPDX-License-Identifier: GPL-2.0-or-later */

 Do the shift in two steps to avoid warning if long has 32 bits.  */

      /* There are at least some bytes to set.

 Write 8 `op_t' per iteration until less than 8 `op_t' remain.  */

 Write 1 `op_t' per iteration until less than OPSIZ bytes remain.  */

 Write the last few bytes.  */

 SPDX-License-Identifier: GPL-2.0

/*

 * bitops.c: atomic operations which got too long to be inlined all over

 *      the place.

 * 

 * Copyright 1999 Philipp Rumpf (prumpf@tux.org)

 * Copyright 2000 Grant Grundler (grundler@cup.hp.com)

 XXX - sign extension wanted? */

 XXX - sign extension wanted? */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 *	Precise Delay Loops for parisc

 *

 *	based on code by:

 *	Copyright (C) 1993 Linus Torvalds

 *	Copyright (C) 1997 Martin Mares <mj@atrey.karlin.mff.cuni.cz>

 *	Copyright (C) 2008 Jiri Hladky <hladky _dot_ jiri _at_ gmail _dot_ com>

 *

 *	parisc implementation:

 *	Copyright (C) 2013 Helge Deller <deller@gmx.de>

 for mfctl() */

 for boot_cpu_data */

 CR16 based delay: */

	/*

	 * Note: Due to unsigned math, cr16 rollovers shouldn't be

	 * a problem here. However, on 32 bit, we need to make sure

	 * we don't pass in too big a value. The current default

	 * value of MAX_UDELAY_MS should help prevent this.

 Allow RT tasks to run */

		/*

		 * It is possible that we moved to another CPU, and

		 * since CR16's are per-cpu we need to calculate

		 * that. The delay must guarantee that we wait "at

		 * least" the amount of time. Being moved to another

		 * CPU could make the wait longer but we just need to

		 * make sure we waited long enough. Rebalance the

		 * counter for this CPU.

/*

 * Definitions and wrapper functions for kernel decompressor

 *

 *   (C) 2017 Helge Deller <deller@gmx.de>

/*

 * gzip declarations

 Symbols defined by linker scripts */

 output_len is inserted by the linker possibly at an unaligned address */

 wait forever */

 helper functions for libgcc */

 if you get this D and no more, string storage */

 in $GLOBAL$ is wrong or %dp is wrong */

 where the final bits are stored */

	/*

	 * Calculate addr to where the vmlinux ELF file shall be decompressed.

	 * Assembly code in head.S positioned the stack directly behind bss, so

	 * leave 2 MB for the stack.

	/*

	 * Initialize free_mem_ptr and free_mem_end_ptr.

 Limit memory for bootoader to 1GB */

 if we have ramdisk this is at end of memory */

/*

===============================================================================



This C source file is part of the SoftFloat IEC/IEEE Floating-point

Arithmetic Package, Release 2.



Written by John R. Hauser.  This work was made possible in part by the

International Computer Science Institute, located at Suite 600, 1947 Center

Street, Berkeley, California 94704.  Funding was partially provided by the

National Science Foundation under grant MIP-9311980.  The original version

of this code was written as part of a project to build a fixed-point vector

processor in collaboration with the University of California at Berkeley,

overseen by Profs. Nelson Morgan and John Wawrzynek.  More information

is available through the web page

http://www.jhauser.us/arithmetic/SoftFloat-2b/SoftFloat-source.txt



THIS SOFTWARE IS DISTRIBUTED AS IS, FOR FREE.  Although reasonable effort

has been made to avoid it, THIS SOFTWARE MAY CONTAIN FAULTS THAT WILL AT

TIMES RESULT IN INCORRECT BEHAVIOR.  USE OF THIS SOFTWARE IS RESTRICTED TO

PERSONS AND ORGANIZATIONS WHO CAN AND WILL TAKE FULL RESPONSIBILITY FOR ANY

AND ALL LOSSES, COSTS, OR OTHER PROBLEMS ARISING FROM ITS USE.



Derivative works are acceptable, even for commercial purposes, so long as

(1) they include prominent notice that the work is derivative, and (2) they

include prominent notice akin to these three paragraphs for those parts of

this code that are retained.



===============================================================================

#include "milieu.h"

#include "softfloat.h"

/*

-------------------------------------------------------------------------------

Primitive arithmetic functions, including multi-word arithmetic, and

division and square root approximations.  (Can be specialized to target if

desired.)

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Functions and definitions to determine:  (1) whether tininess for underflow

is detected before or after rounding by default, (2) what (if anything)

happens when exceptions are raised, (3) how signaling NaNs are distinguished

from quiet NaNs, (4) the default generated quiet NaNs, and (5) how NaNs

are propagated from function inputs to output.  These details are target-

specific.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Takes a 64-bit fixed-point value `absZ' with binary point between bits 6

and 7, and returns the properly rounded 32-bit integer corresponding to the

input.  If `zSign' is nonzero, the input is negated before being converted

to an integer.  Bit 63 of `absZ' must be zero.  Ordinarily, the fixed-point

input is simply rounded to an integer, with the inexact exception raised if

the input cannot be represented exactly as an integer.  If the fixed-point

input is too large, however, the invalid exception is raised and the largest

positive or negative integer is returned.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the fraction bits of the single-precision floating-point value `a'.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the exponent bits of the single-precision floating-point value `a'.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the sign bit of the single-precision floating-point value `a'.

-------------------------------------------------------------------------------

 in softfloat.h */

/*

-------------------------------------------------------------------------------

Normalizes the subnormal single-precision floating-point value represented

by the denormalized significand `aSig'.  The normalized exponent and

significand are stored at the locations pointed to by `zExpPtr' and

`zSigPtr', respectively.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Packs the sign `zSign', exponent `zExp', and significand `zSig' into a

single-precision floating-point value, returning the result.  After being

shifted into the proper positions, the three fields are simply added

together to form the result.  This means that any integer portion of `zSig'

will be added into the exponent.  Since a properly normalized significand

will have an integer portion equal to 1, the `zExp' input should be 1 less

than the desired result exponent whenever `zSig' is a complete, normalized

significand.

-------------------------------------------------------------------------------

 no outputs */

/*

-------------------------------------------------------------------------------

Takes an abstract floating-point value having sign `zSign', exponent `zExp',

and significand `zSig', and returns the proper single-precision floating-

point value corresponding to the abstract input.  Ordinarily, the abstract

value is simply rounded and packed into the single-precision format, with

the inexact exception raised if the abstract input cannot be represented

exactly.  If the abstract value is too large, however, the overflow and

inexact exceptions are raised and an infinity or maximal finite value is

returned.  If the abstract value is too small, the input value is rounded to

a subnormal number, and the underflow and inexact exceptions are raised if

the abstract input cannot be represented exactly as a subnormal single-

precision floating-point number.

    The input significand `zSig' has its binary point between bits 30

and 29, which is 7 bits to the left of the usual location.  This shifted

significand must be normalized or smaller.  If `zSig' is not normalized,

`zExp' must be 0; in that case, the result returned is a subnormal number,

and it must not require rounding.  In the usual case that `zSig' is

normalized, `zExp' must be 1 less than the ``true'' floating-point exponent.

The handling of underflow and overflow follows the IEC/IEEE Standard for

Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Takes an abstract floating-point value having sign `zSign', exponent `zExp',

and significand `zSig', and returns the proper single-precision floating-

point value corresponding to the abstract input.  This routine is just like

`roundAndPackFloat32' except that `zSig' does not have to be normalized in

any way.  In all cases, `zExp' must be 1 less than the ``true'' floating-

point exponent.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the fraction bits of the double-precision floating-point value `a'.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the exponent bits of the double-precision floating-point value `a'.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the sign bit of the double-precision floating-point value `a'.

-------------------------------------------------------------------------------

 in softfloat.h */

/*

-------------------------------------------------------------------------------

Normalizes the subnormal double-precision floating-point value represented

by the denormalized significand `aSig'.  The normalized exponent and

significand are stored at the locations pointed to by `zExpPtr' and

`zSigPtr', respectively.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Packs the sign `zSign', exponent `zExp', and significand `zSig' into a

double-precision floating-point value, returning the result.  After being

shifted into the proper positions, the three fields are simply added

together to form the result.  This means that any integer portion of `zSig'

will be added into the exponent.  Since a properly normalized significand

will have an integer portion equal to 1, the `zExp' input should be 1 less

than the desired result exponent whenever `zSig' is a complete, normalized

significand.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Takes an abstract floating-point value having sign `zSign', exponent `zExp',

and significand `zSig', and returns the proper double-precision floating-

point value corresponding to the abstract input.  Ordinarily, the abstract

value is simply rounded and packed into the double-precision format, with

the inexact exception raised if the abstract input cannot be represented

exactly.  If the abstract value is too large, however, the overflow and

inexact exceptions are raised and an infinity or maximal finite value is

returned.  If the abstract value is too small, the input value is rounded to

a subnormal number, and the underflow and inexact exceptions are raised if

the abstract input cannot be represented exactly as a subnormal double-

precision floating-point number.

    The input significand `zSig' has its binary point between bits 62

and 61, which is 10 bits to the left of the usual location.  This shifted

significand must be normalized or smaller.  If `zSig' is not normalized,

`zExp' must be 0; in that case, the result returned is a subnormal number,

and it must not require rounding.  In the usual case that `zSig' is

normalized, `zExp' must be 1 less than the ``true'' floating-point exponent.

The handling of underflow and overflow follows the IEC/IEEE Standard for

Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

register int lr = __builtin_return_address(0);

printk("roundAndPackFloat64 called from 0x%08x\n",lr);

/*

-------------------------------------------------------------------------------

Takes an abstract floating-point value having sign `zSign', exponent `zExp',

and significand `zSig', and returns the proper double-precision floating-

point value corresponding to the abstract input.  This routine is just like

`roundAndPackFloat64' except that `zSig' does not have to be normalized in

any way.  In all cases, `zExp' must be 1 less than the ``true'' floating-

point exponent.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the fraction bits of the extended double-precision floating-point

value `a'.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the exponent bits of the extended double-precision floating-point

value `a'.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the sign bit of the extended double-precision floating-point value

`a'.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Normalizes the subnormal extended double-precision floating-point value

represented by the denormalized significand `aSig'.  The normalized exponent

and significand are stored at the locations pointed to by `zExpPtr' and

`zSigPtr', respectively.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Packs the sign `zSign', exponent `zExp', and significand `zSig' into an

extended double-precision floating-point value, returning the result.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Takes an abstract floating-point value having sign `zSign', exponent `zExp',

and extended significand formed by the concatenation of `zSig0' and `zSig1',

and returns the proper extended double-precision floating-point value

corresponding to the abstract input.  Ordinarily, the abstract value is

rounded and packed into the extended double-precision format, with the

inexact exception raised if the abstract input cannot be represented

exactly.  If the abstract value is too large, however, the overflow and

inexact exceptions are raised and an infinity or maximal finite value is

returned.  If the abstract value is too small, the input value is rounded to

a subnormal number, and the underflow and inexact exceptions are raised if

the abstract input cannot be represented exactly as a subnormal extended

double-precision floating-point number.

    If `roundingPrecision' is 32 or 64, the result is rounded to the same

number of bits as single or double precision, respectively.  Otherwise, the

result is rounded to the full precision of the extended double-precision

format.

    The input significand must be normalized or smaller.  If the input

significand is not normalized, `zExp' must be 0; in that case, the result

returned is a subnormal number, and it must not require rounding.  The

handling of underflow and overflow follows the IEC/IEEE Standard for Binary

Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Takes an abstract floating-point value having sign `zSign', exponent

`zExp', and significand formed by the concatenation of `zSig0' and `zSig1',

and returns the proper extended double-precision floating-point value

corresponding to the abstract input.  This routine is just like

`roundAndPackFloatx80' except that the input significand does not have to be

normalized.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the 32-bit two's complement integer `a' to

the single-precision floating-point format.  The conversion is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the 32-bit two's complement integer `a' to

the double-precision floating-point format.  The conversion is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the 32-bit two's complement integer `a'

to the extended double-precision floating-point format.  The conversion

is performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the single-precision floating-point value

`a' to the 32-bit two's complement integer format.  The conversion is

performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic---which means in particular that the conversion is rounded

according to the current rounding mode.  If `a' is a NaN, the largest

positive integer is returned.  Otherwise, if the conversion overflows, the

largest integer with the same sign as `a' is returned.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the single-precision floating-point value

`a' to the 32-bit two's complement integer format.  The conversion is

performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic, except that the conversion is always rounded toward zero.  If

`a' is a NaN, the largest positive integer is returned.  Otherwise, if the

conversion overflows, the largest integer with the same sign as `a' is

returned.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the single-precision floating-point value

`a' to the double-precision floating-point format.  The conversion is

performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the single-precision floating-point value

`a' to the extended double-precision floating-point format.  The conversion

is performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Rounds the single-precision floating-point value `a' to an integer, and

returns the result as a single-precision floating-point value.  The

operation is performed according to the IEC/IEEE Standard for Binary

Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of adding the absolute values of the single-precision

floating-point values `a' and `b'.  If `zSign' is true, the sum is negated

before being returned.  `zSign' is ignored if the result is a NaN.  The

addition is performed according to the IEC/IEEE Standard for Binary

Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of subtracting the absolute values of the single-

precision floating-point values `a' and `b'.  If `zSign' is true, the

difference is negated before being returned.  `zSign' is ignored if the

result is a NaN.  The subtraction is performed according to the IEC/IEEE

Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of adding the single-precision floating-point values `a'

and `b'.  The operation is performed according to the IEC/IEEE Standard for

Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of subtracting the single-precision floating-point values

`a' and `b'.  The operation is performed according to the IEC/IEEE Standard

for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of multiplying the single-precision floating-point values

`a' and `b'.  The operation is performed according to the IEC/IEEE Standard

for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of dividing the single-precision floating-point value `a'

by the corresponding value `b'.  The operation is performed according to the

IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the remainder of the single-precision floating-point value `a'

with respect to the corresponding value `b'.  The operation is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the square root of the single-precision floating-point value `a'.

The operation is performed according to the IEC/IEEE Standard for Binary

Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the single-precision floating-point value `a' is equal to the

corresponding value `b', and 0 otherwise.  The comparison is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the single-precision floating-point value `a' is less than or

equal to the corresponding value `b', and 0 otherwise.  The comparison is

performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the single-precision floating-point value `a' is less than

the corresponding value `b', and 0 otherwise.  The comparison is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the single-precision floating-point value `a' is equal to the

corresponding value `b', and 0 otherwise.  The invalid exception is raised

if either operand is a NaN.  Otherwise, the comparison is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the single-precision floating-point value `a' is less than or

equal to the corresponding value `b', and 0 otherwise.  Quiet NaNs do not

cause an exception.  Otherwise, the comparison is performed according to the

IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

int16 aExp, bExp;

 Do nothing, even if NaN as we're quiet */

/*

-------------------------------------------------------------------------------

Returns 1 if the single-precision floating-point value `a' is less than

the corresponding value `b', and 0 otherwise.  Quiet NaNs do not cause an

exception.  Otherwise, the comparison is performed according to the IEC/IEEE

Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

 Do nothing, even if NaN as we're quiet */

/*

-------------------------------------------------------------------------------

Returns the result of converting the double-precision floating-point value

`a' to the 32-bit two's complement integer format.  The conversion is

performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic---which means in particular that the conversion is rounded

according to the current rounding mode.  If `a' is a NaN, the largest

positive integer is returned.  Otherwise, if the conversion overflows, the

largest integer with the same sign as `a' is returned.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the double-precision floating-point value

`a' to the 32-bit two's complement integer format.  The conversion is

performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic, except that the conversion is always rounded toward zero.  If

`a' is a NaN, the largest positive integer is returned.  Otherwise, if the

conversion overflows, the largest integer with the same sign as `a' is

returned.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the double-precision floating-point value

`a' to the 32-bit two's complement unsigned integer format.  The conversion

is performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic---which means in particular that the conversion is rounded

according to the current rounding mode.  If `a' is a NaN, the largest

positive integer is returned.  Otherwise, if the conversion overflows, the

largest positive integer is returned.

-------------------------------------------------------------------------------

extractFloat64Sign( a );

if ( ( aExp == 0x7FF ) && aSig ) aSign = 0;

/*

-------------------------------------------------------------------------------

Returns the result of converting the double-precision floating-point value

`a' to the 32-bit two's complement integer format.  The conversion is

performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic, except that the conversion is always rounded toward zero.  If

`a' is a NaN, the largest positive integer is returned.  Otherwise, if the

conversion overflows, the largest positive integer is returned.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the double-precision floating-point value

`a' to the single-precision floating-point format.  The conversion is

performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the double-precision floating-point value

`a' to the extended double-precision floating-point format.  The conversion

is performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Rounds the double-precision floating-point value `a' to an integer, and

returns the result as a double-precision floating-point value.  The

operation is performed according to the IEC/IEEE Standard for Binary

Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of adding the absolute values of the double-precision

floating-point values `a' and `b'.  If `zSign' is true, the sum is negated

before being returned.  `zSign' is ignored if the result is a NaN.  The

addition is performed according to the IEC/IEEE Standard for Binary

Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of subtracting the absolute values of the double-

precision floating-point values `a' and `b'.  If `zSign' is true, the

difference is negated before being returned.  `zSign' is ignored if the

result is a NaN.  The subtraction is performed according to the IEC/IEEE

Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of adding the double-precision floating-point values `a'

and `b'.  The operation is performed according to the IEC/IEEE Standard for

Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of subtracting the double-precision floating-point values

`a' and `b'.  The operation is performed according to the IEC/IEEE Standard

for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of multiplying the double-precision floating-point values

`a' and `b'.  The operation is performed according to the IEC/IEEE Standard

for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of dividing the double-precision floating-point value `a'

by the corresponding value `b'.  The operation is performed according to

the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the remainder of the double-precision floating-point value `a'

with respect to the corresponding value `b'.  The operation is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the square root of the double-precision floating-point value `a'.

The operation is performed according to the IEC/IEEE Standard for Binary

Floating-point Arithmetic.

-------------------------------------------------------------------------------

, shiftedRem;

float64 z;

/*

-------------------------------------------------------------------------------

Returns 1 if the double-precision floating-point value `a' is equal to the

corresponding value `b', and 0 otherwise.  The comparison is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the double-precision floating-point value `a' is less than or

equal to the corresponding value `b', and 0 otherwise.  The comparison is

performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the double-precision floating-point value `a' is less than

the corresponding value `b', and 0 otherwise.  The comparison is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the double-precision floating-point value `a' is equal to the

corresponding value `b', and 0 otherwise.  The invalid exception is raised

if either operand is a NaN.  Otherwise, the comparison is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the double-precision floating-point value `a' is less than or

equal to the corresponding value `b', and 0 otherwise.  Quiet NaNs do not

cause an exception.  Otherwise, the comparison is performed according to the

IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

int16 aExp, bExp;

 Do nothing, even if NaN as we're quiet */

/*

-------------------------------------------------------------------------------

Returns 1 if the double-precision floating-point value `a' is less than

the corresponding value `b', and 0 otherwise.  Quiet NaNs do not cause an

exception.  Otherwise, the comparison is performed according to the IEC/IEEE

Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

 Do nothing, even if NaN as we're quiet */

/*

-------------------------------------------------------------------------------

Returns the result of converting the extended double-precision floating-

point value `a' to the 32-bit two's complement integer format.  The

conversion is performed according to the IEC/IEEE Standard for Binary

Floating-point Arithmetic---which means in particular that the conversion

is rounded according to the current rounding mode.  If `a' is a NaN, the

largest positive integer is returned.  Otherwise, if the conversion

overflows, the largest integer with the same sign as `a' is returned.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the extended double-precision floating-

point value `a' to the 32-bit two's complement integer format.  The

conversion is performed according to the IEC/IEEE Standard for Binary

Floating-point Arithmetic, except that the conversion is always rounded

toward zero.  If `a' is a NaN, the largest positive integer is returned.

Otherwise, if the conversion overflows, the largest integer with the same

sign as `a' is returned.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the extended double-precision floating-

point value `a' to the single-precision floating-point format.  The

conversion is performed according to the IEC/IEEE Standard for Binary

Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of converting the extended double-precision floating-

point value `a' to the double-precision floating-point format.  The

conversion is performed according to the IEC/IEEE Standard for Binary

Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Rounds the extended double-precision floating-point value `a' to an integer,

and returns the result as an extended quadruple-precision floating-point

value.  The operation is performed according to the IEC/IEEE Standard for

Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of adding the absolute values of the extended double-

precision floating-point values `a' and `b'.  If `zSign' is true, the sum is

negated before being returned.  `zSign' is ignored if the result is a NaN.

The addition is performed according to the IEC/IEEE Standard for Binary

Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of subtracting the absolute values of the extended

double-precision floating-point values `a' and `b'.  If `zSign' is true,

the difference is negated before being returned.  `zSign' is ignored if the

result is a NaN.  The subtraction is performed according to the IEC/IEEE

Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of adding the extended double-precision floating-point

values `a' and `b'.  The operation is performed according to the IEC/IEEE

Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of subtracting the extended double-precision floating-

point values `a' and `b'.  The operation is performed according to the

IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of multiplying the extended double-precision floating-

point values `a' and `b'.  The operation is performed according to the

IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the result of dividing the extended double-precision floating-point

value `a' by the corresponding value `b'.  The operation is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the remainder of the extended double-precision floating-point value

`a' with respect to the corresponding value `b'.  The operation is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns the square root of the extended double-precision floating-point

value `a'.  The operation is performed according to the IEC/IEEE Standard

for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the extended double-precision floating-point value `a' is

equal to the corresponding value `b', and 0 otherwise.  The comparison is

performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the extended double-precision floating-point value `a' is

less than or equal to the corresponding value `b', and 0 otherwise.  The

comparison is performed according to the IEC/IEEE Standard for Binary

Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the extended double-precision floating-point value `a' is

less than the corresponding value `b', and 0 otherwise.  The comparison

is performed according to the IEC/IEEE Standard for Binary Floating-point

Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the extended double-precision floating-point value `a' is equal

to the corresponding value `b', and 0 otherwise.  The invalid exception is

raised if either operand is a NaN.  Otherwise, the comparison is performed

according to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

/*

-------------------------------------------------------------------------------

Returns 1 if the extended double-precision floating-point value `a' is less

than or equal to the corresponding value `b', and 0 otherwise.  Quiet NaNs

do not cause an exception.  Otherwise, the comparison is performed according

to the IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

 Do nothing, even if NaN as we're quiet */

/*

-------------------------------------------------------------------------------

Returns 1 if the extended double-precision floating-point value `a' is less

than the corresponding value `b', and 0 otherwise.  Quiet NaNs do not cause

an exception.  Otherwise, the comparison is performed according to the

IEC/IEEE Standard for Binary Floating-point Arithmetic.

-------------------------------------------------------------------------------

 Do nothing, even if NaN as we're quiet */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

    NetWinder Floating Point Emulator

    (c) Rebel.com, 1998-1999

    (c) Philip Blundell, 1998, 2001



    Direct questions, comments to Scott Bambrough <scottb@netwinder.org>



 sign & exponent */

 sign & exponent */

 sign & exponent */

 ms bits */

 ls bits */

 ls bits */

 ms bits */

 Single */

 double msw */

 empty */

 msw */

 msw */

 lsw */

 msw */

 lsw */

 sign & exp */

 msw */

 msw */

 single */

 double msw */

 msw */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

    NetWinder Floating Point Emulator

    (c) Rebel.COM, 1998,1999

    (c) Philip Blundell, 2001



    Direct questions, comments to Scott Bambrough <scottb@netwinder.org>



 SPDX-License-Identifier: GPL-2.0-or-later

/*

    NetWinder Floating Point Emulator

    (c) Rebel.COM, 1998,1999



    Direct questions, comments to Scott Bambrough <scottb@netwinder.org>



 extended 0.0 */

 extended 1.0 */

 extended 2.0 */

 extended 3.0 */

 extended 4.0 */

 extended 5.0 */

 extended 0.5 */

 extended 10.0 */

 double 0.0 */

 double 1.0 */

 double 2.0 */

 double 3.0 */

 double 4.0 */

 double 5.0 */

 double 0.5 */

 double 10.0 */

 single 0.0 */

 single 1.0 */

 single 2.0 */

 single 3.0 */

 single 4.0 */

 single 5.0 */

 single 0.5 */

 single 10.0 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

    NetWinder Floating Point Emulator

    (c) Rebel.COM, 1998,1999



    Direct questions, comments to Scott Bambrough <scottb@netwinder.org>



 strictly, these opcodes should not be implemented */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

    NetWinder Floating Point Emulator

    (c) Rebel.COM, 1998,1999



    Direct questions, comments to Scott Bambrough <scottb@netwinder.org>



 strictly, these opcodes should not be implemented */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

    NetWinder Floating Point Emulator

    (c) Rebel.com, 1998-1999

    (c) Philip Blundell, 1998-1999



    Direct questions, comments to Scott Bambrough <scottb@netwinder.org>



 XXX */

 kernel symbols required for signal handling */

 kernel function prototypes required */

 external declarations for saved kernel symbols */

 Original value of fp_enter from kernel before patched by fpe_init. */

 forward declarations */

 Display title, version and copyright information. */

 Save pointer to the old FP handler and then patch ourselves in */

 Restore the values we saved earlier. */

/*

ScottB:  November 4, 1998



Moved this function out of softfloat-specialize into fpmodule.c.

This effectively isolates all the changes required for integrating with the

Linux kernel into fpmodule.c.  Porting to NetBSD should only require modifying

fpmodule.c to integrate with the NetBSD kernel (I hope!).



[1/1/99: Not quite true any more unfortunately.  There is Linux-specific

code to access data in user space in some other source files at the 

moment (grep for get_user / put_user calls).  --philb]



This function is called by the SoftFloat routines to raise a floating

point exception.  We check the trap enable byte in the FPSR, and raise

a SIGFPE exception if necessary.  If not the relevant bits in the 

cumulative exceptions flag byte are set and we return.

 By default, ignore inexact errors as there are far too many of them to log */

 Read fpsr and initialize the cumulativeTraps.  */

	/* For each type of exception, the cumulative trap exception bit is only

 Set the cumulative exceptions flags.  */

 Raise an exception if necessary.  */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

    NetWinder Floating Point Emulator

    (c) Rebel.COM, 1998,1999

    (c) Philip Blundell, 2001



    Direct questions, comments to Scott Bambrough <scottb@netwinder.org>



 Reset the FPA11 chip.  Called to initialize and reset the emulator. */

 initialize the register type array */

 FPSR: set system id to FP_EMULATOR, set AC, clear all other bits */

 Emulate the instruction in the opcode. */

 For coprocessor 1 or 2 (FPA11) */

 Emulate conversion opcodes. */

 Emulate register transfer opcodes. */

 Emulate comparison opcodes. */

 Emulate monadic arithmetic opcodes. */

 Emulate dyadic arithmetic opcodes. */

 Emulate load/store opcodes. */

 Emulate load/store multiple opcodes. */

 Invalid instruction detected.  Return FALSE. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

    NetWinder Floating Point Emulator

    (c) Rebel.COM, 1998,1999

    (c) Philip Blundell, 1999, 2001



    Direct questions, comments to Scott Bambrough <scottb@netwinder.org>



		/* This is some variant of a comparison (PerformComparison

		   will sort out which one).  Since most of the other CPRT

		   instructions are oddball cases of some sort or other it

 Hint to GCC that we'd like a jump table rather than a load of CMPs */

 This instruction sets the flags N, Z, C, V in the FPSR. */

 1 if CxFE */

 1 if CNxx */

	/* Check for unordered condition and convert all operands to 80-bit

	   format.

	   ?? Might be some mileage in avoiding this conversion if possible.

	   Eg, if both operands are 32-bit, detect this and do a 32-bit

printk("single.\n");

printk("double.\n");

printk("extended.\n");

printk("Fm is a constant: #%d.\n",Fm);

printk("Fm = r%d which contains a ",Fm);

printk("single.\n");

printk("double.\n");

printk("extended.\n");

 test for less than condition */

 test for equal condition */

 test for greater than or equal condition */

		/* Fm is a constant.  Do the comparison in whatever precision

 test for less than condition */

 test for equal condition */

 test for greater than or equal condition */

 test for less than condition */

 test for equal condition */

 test for greater than or equal condition */

 Both operands are in registers.  */

 test for less than condition */

 test for equal condition */

 test for greater than or equal condition */

 Promote 32-bit operand to 64 bits.  */

 test for less than condition */

 test for equal condition */

 test for greater than or equal condition */

	/* ?? The FPA data sheet is pretty vague about this, in particular

	   about whether the non-E comparisons can ever raise exceptions.

	   This implementation is based on a combination of what it says in

	   the data sheet, observation of how the Acorn emulator actually

 SPDX-License-Identifier: GPL-2.0-or-later

/*

    NetWinder Floating Point Emulator

    (c) Rebel.COM, 1998,1999

    (c) Philip Blundell, 2001



    Direct questions, comments to Scott Bambrough <scottb@netwinder.org>



	/* Get the destination size.  If not valid let Linux perform

	/* Compare the size of the operands in Fn and Fm.

	   Choose the largest size and perform operations in that size,

	   in order to make use of all the precision of the operands.

	   If Fm is a constant, we just grab a constant of a size

	/* The CPDO functions used to always set the destination type

		/* If the operation succeeded, check to see if the result in the

		   destination register is the correct size.  If not force it

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  arch/arm/common/dmabounce.c

 *

 *  Special dma_{map/unmap/dma_sync}_* routines for systems that have

 *  limited DMA windows. These functions utilize bounce buffers to

 *  copy data to/from buffers located outside the DMA region. This

 *  only works for systems in which DMA memory is at the bottom of

 *  RAM, the remainder of memory is at the top and the DMA memory

 *  can be marked as ZONE_DMA. Anything beyond that such as discontiguous

 *  DMA windows will require custom implementations that reserve memory

 *  areas at early bootup.

 *

 *  Original version by Brad Parker (brad@heeltoe.com)

 *  Re-written by Christopher Hoover <ch@murgatroid.com>

 *  Made generic by Deepak Saxena <dsaxena@plexity.net>

 *

 *  Copyright (C) 2002 Hewlett Packard Company.

 *  Copyright (C) 2004 MontaVista Software, Inc.

 ************************************************** */

 original request */

 safe buffer info */

 allocate a 'safe' buffer and keep track of it */

 determine if a buffer is from our "safe" pool */

 ************************************************** */

 Figure out if we need to bounce from the DMA mask. */

		/*

		 * Since we may have written to a page cache page,

		 * we need to ensure that the data will be coherent

		 * with user mappings.

 ************************************************** */

/*

 * see if a buffer address is in an 'unsafe' range.  if it is

 * allocate a 'safe' buffer and copy the unsafe buffer into it.

 * substitute the safe buffer for the unsafe one.

 * (basically move the buffer from an unsafe area to a safe one)

/*

 * see if a mapped address was really a "safe" buffer and if so, copy

 * the data from the safe buffer back to the unsafe buffer and free up

 * the safe buffer.  (basically return things back to the way they

 * should be)

 byte alignment */,

 no page-crossing issues */);

 SPDX-License-Identifier: GPL-2.0

 Copyright (c) 2018, The Linux Foundation. All rights reserved.

	/*

	 * Select the L2 window by poking l2cpselr, then write to the window

	 * via l2cpdr.

	/*

	 * Select the L2 window by poking l2cpselr, then read from the window

	 * via l2cpdr.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm/common/bL_switcher_dummy_if.c -- b.L switcher dummy interface

 *

 * Created by:	Nicolas Pitre, November 2012

 * Copyright:	(C) 2012-2013  Linaro Limited

 *

 * Dummy interface to user space for debugging purpose only.

 format: <cpu#>,<cluster#> */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/arch/arm/common/sa1111.c

 *

 * SA1111 support

 *

 * Original code by John Dorsey

 *

 * This file contains all generic SA1111 support.

 *

 * All initialization functions provided here are intended to be called

 * from machine specific code with proper arguments when required.

 SA1111 IRQs */

/*

 * We keep the following data for the overall SA1111.  Note that the

 * struct device and struct resource are "fake"; they should be supplied

 * by the bus above us.  However, in the interests of getting all SA1111

 * drivers converted over to the device model, we provide this as an

 * anchor point for all the other drivers.

 base for cascaded on-chip IRQs */

/*

 * We _really_ need to eliminate this.  Its only users

 * are the PWM and DMA checking code.

/*

 * SA1111 interrupt support.  Since clearing an IRQ while there are

 * active IRQs causes the interrupt output to pulse, the upper levels

 * will call us again if there are more interrupts to process.

 For level-based interrupts */

/*

 * Attempt to re-trigger the interrupt.  The SA1111 contains a register

 * (INTSET) which claims to do this.  However, in practice no amount of

 * manipulation of INTEN and INTSET guarantees that the interrupt will

 * be triggered.  In fact, its very difficult, if not impossible to get

 * INTSET to re-trigger the interrupt.

 Disallow unavailable interrupts */

	/*

	 * We're guaranteed that this region hasn't been taken.

 disable all IRQs */

	/*

	 * detect on rising edge.  Note: Feb 2001 Errata for SA1111

	 * specifies that S0ReadyInt and S1ReadyInt should be '1'.

 clear all IRQs */

	/*

	 * Register SA1111 interrupt

 disable all IRQs */

/*

 * Bring the SA1111 out of reset.  This requires a set procedure:

 *  1. nRESET asserted (by hardware)

 *  2. CLK turned on from SA1110

 *  3. nRESET deasserted

 *  4. VCO turned on, PLL_BYPASS turned off

 *  5. Wait lock time, then assert RCLKEn

 *  7. PCR set to allow clocking of individual functions

 *

 * Until we've done this, the only registers we can access are:

 *   SBI_SKCR

 *   SBI_SMCR

 *   SBI_SKID

	/*

	 * Turn VCO on, and disable PLL Bypass.

	/*

	 * Wait lock time.  SA1111 manual _doesn't_

	 * specify a figure for this!  We choose 100us.

	/*

	 * Enable RCLK.  We also ensure that RDYEN is set.

	/*

	 * Wait 14 RCLK cycles for the chip to finish coming out

	 * of reset. (RCLK=24MHz).  This is 590ns.

	/*

	 * Ensure all clocks are initially off.

/*

 * Configure the SA1111 shared memory controller.

	/*

	 * Now clear the bits in the DMA mask to work around the SA1111

	 * DMA erratum (Intel StrongARM SA-1111 Microprocessor Companion

	 * Chip Specification Update, June 2000, Erratum #7).

	/*

	 * If the parent device has a DMA mask associated with it, and

	 * this child supports DMA, propagate it down to the children.

/**

 *	sa1111_probe - probe for a single SA1111 chip.

 *	@phys_addr: physical address of device.

 *

 *	Probe for a SA1111 chip.  This must be called

 *	before any other SA1111-specific code.

 *

 *	Returns:

 *	%-ENODEV	device not found.

 *	%-EBUSY		physical address already marked in-use.

 *	%-EINVAL	no platform data passed

 *	%0		successful.

	/*

	 * Map the whole region.  This also maps the

	 * registers for our children.

	/*

	 * Probe for the chip.  Only touch the SBI registers.

	/*

	 * We found it.  Wake the chip up, and initialise.

	/*

	 * The interrupt controller must be initialised before any

	 * other device to ensure that the interrupts are available.

 Setup the GPIOs - should really be done after the IRQ setup */

	/*

	 * The SDRAM configuration of the SA1110 and the SA1111 must

	 * match.  This is very important to ensure that SA1111 accesses

	 * don't corrupt the SDRAM.  Note that this ungates the SA1111's

	 * MBGNT signal, so we must have called sa1110_mb_disable()

	 * beforehand.

	/*

	 * We only need to turn on DCLK whenever we want to use the

	 * DMA.  It can otherwise be held firmly in the off position.

	 * (currently, we always enable it.)

	/*

	 * Enable the SA1110 memory bus request and grant signals.

	/*

	 * Interrupt controller

	/*

	 * Save state.

	/*

	 * Disable.

/*

 *	sa1111_resume - Restore the SA1111 device state.

 *	@dev: device to restore

 *

 *	Restore the general state of the SA1111; clock control and

 *	interrupt controller.  Other parts of the SA1111 must be

 *	restored by their respective drivers, and must be called

 *	via LDM after this function.

	/*

	 * Ensure that the SA1111 is still here.

	 * FIXME: shouldn't do this here.

	/*

	 * First of all, wake up the chip.

 Enable the memory bus request/grant signals */

	/*

	 * Only lock for write ops. Also, sa1111_wake must be called with

	 * released spinlock!

/*

 *	Not sure if this should be on the system bus or not yet.

 *	We really want some way to register a system device at

 *	the per-machine level, and then have this driver pick

 *	up the registered devices.

 *

 *	We also need to handle the SDRAM configuration for

 *	PXA250/SA1110 machine classes.

/*

 *	Get the parent device driver (us) structure

 *	from a child function device

/*

 * The bits in the opdiv field are non-linear.

/**

 *	sa1111_pll_clock - return the current PLL clock frequency.

 *	@sadev: SA1111 function block

 *

 *	BUG: we should look at SKCR.  We also blindly believe that

 *	the chip is being fed with the 3.6864MHz clock.

 *

 *	Returns the PLL clock in Hz.

/**

 *	sa1111_select_audio_mode - select I2S or AC link mode

 *	@sadev: SA1111 function block

 *	@mode: One of %SA1111_AUDIO_ACLINK or %SA1111_AUDIO_I2S

 *

 *	Frob the SKCR to select AC Link mode or I2S mode for

 *	the audio block.

/**

 *	sa1111_set_audio_rate - set the audio sample rate

 *	@sadev: SA1111 SAC function block

 *	@rate: sample rate to select

/**

 *	sa1111_get_audio_rate - get the audio sample rate

 *	@sadev: SA1111 SAC function block device

/*

 * Individual device operations.

/**

 *	sa1111_enable_device - enable an on-chip SA1111 function block

 *	@sadev: SA1111 function block device to enable

/**

 *	sa1111_disable_device - disable an on-chip SA1111 function block

 *	@sadev: SA1111 function block device to disable

/*

 *	SA1111 "Register Access Bus."

 *

 *	We model this as a regular bus type, and hang devices directly

 *	off this.

/*

 * According to the "Intel StrongARM SA-1111 Microprocessor Companion

 * Chip Specification Update" (June 2000), erratum #7, there is a

 * significant bug in the SA1111 SDRAM shared memory controller.  If

 * an access to a region of memory above 1MB relative to the bank base,

 * it is important that address bit 10 _NOT_ be asserted. Depending

 * on the configuration of the RAM, bit 10 may correspond to one

 * of several different (processor-relative) address bits.

 *

 * This routine only identifies whether or not a given DMA address

 * is susceptible to the bug.

 *

 * This should only get called for sa1111_device types due to the

 * way we configure our device dma_masks.

	/*

	 * Section 4.6 of the "Intel StrongARM SA-1111 Development Module

	 * User's Guide" mentions that jumpers R51 and R52 control the

	 * target of SA-1111 DMA (either SDRAM bank 0 on Assabet, or

	 * SDRAM bank 1 on Neponset). The default configuration selects

	 * Assabet, so any address in bank 1 is necessarily invalid.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/arch/arm/mach-vexpress/mcpm_platsmp.c

 *

 * Created by:  Nicolas Pitre, November 2012

 * Copyright:   (C) 2012-2013  Linaro Limited

 *

 * Code to handle secondary CPU bringup and hotplug for the cluster power API.

 We assume all CPUs may be shut down. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Samsung Electronics.

 * Kyungmin Park <kyungmin.park@samsung.com>

 * Tomasz Figa <t.figa@samsung.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/arch/arm/common/locomo.c

 *

 * Sharp LoCoMo support

 *

 * This file contains all generic LoCoMo support.

 *

 * All initialization functions provided here are intended to be called

 * from machine specific code with proper arguments when required.

 *

 * Based on sa1111.c

 LoCoMo Interrupts */

 M62332 output channel selection */

 M62332 volume channel number  */

   0 : CH.1 , 1 : CH. 2        */

 DAC send data */

 Slave address  */

 W bit (0 only) */

 Sub address    */

 A bit (0 only) */

 DAC setup and hold times (expressed in us) */

   4.7 us */

   4.7 us */

   4.0 us */

   4.7 us */

   4.7 us */

   4.0 us */

   250 ns */

   300 ns */

   300 ns */

  1000 ns */

 the following is the overall data for the locomo chip */

/* All the locomo devices.  If offset is non-zero, the mapbase for the

 * locomo_dev will be set to the chip base plus offset.  If offset is

 * zero, then the mapbase for the locomo_dev will be set to zero.  An

 * offset of zero means the device only uses GPIOs or other helper

 Acknowledge the parent IRQ */

 check why this interrupt was generated */

 generate the next interrupt(s) */

	/*

	 * Install handler for IRQ_LOCOMO_HW.

 Install handlers for IRQ_LOCOMO_* */

	/*

	 * If the parent device has a DMA mask associated with it,

	 * propagate it down to the children.

 GPIO */

 SPI */

 GPIO */

 ADSTART */

 SPI */

 CLK32 off */

 18MHz already enabled, so no wait */

 CLK32 on */

 18MHz clock off*/

 22MHz/24MHz clock off */

 FL */

/**

 *	locomo_probe - probe for a single LoCoMo chip.

 *	@phys_addr: physical address of device.

 *

 *	Probe for a LoCoMo chip.  This must be called

 *	before any other locomo-specific code.

 *

 *	Returns:

 *	%-ENODEV	device not found.

 *	%-EBUSY		physical address already marked in-use.

 *	%0		successful.

	/*

	 * Map the whole region.  This also maps the

	 * registers for our children.

 locomo initialize */

 KEYBOARD */

 GPIO */

 Frontlight */

 Longtime timer */

 SPI */

 XON */

 CLK9MEN */

 init DAC */

	/*

	 * The interrupt controller must be initialised before any

	 * other device to ensure that the interrupts are available.

/*

 *	Not sure if this should be on the system bus or not yet.

 *	We really want some way to register a system device at

 *	the per-machine level, and then have this driver pick

 *	up the registered devices.

/*

 *	Get the parent device driver (us) structure

 *	from a child function device

 300 nsec */

 300 nsec */

 300 nsec */

 4.7 usec */

 1000 nsec */

 300 nsec */

 250 nsec */

 1000 nsec */

  4.0 usec */

 Start */

 5.0 usec */

 1000 nsec */

 4.0 usec */

 5.0 usec */

 300 nsec */

 Send slave address and W bit (LSB is W bit) */

 Check A bit */

 300 nsec */

 4.7 usec */

 300 nsec */

 1000 nsec */

 4.7 usec */

 High is error */

 Send Sub address (LSB is channel select) */

    channel = 0 : ch1 select              */

            = 1 : ch2 select              */

 Check A bit */

 300 nsec */

 4.7 usec */

 300 nsec */

 1000 nsec */

 4.7 usec */

 High is error */

 Send DAC data */

 Check A bit */

 300 nsec */

 4.7 usec */

 300 nsec */

 1000 nsec */

 4.7 usec */

 High is error */

 stop */

 300 nsec */

 4.7 usec */

 1000 nsec */

 4 usec */

 1000 nsec */

 4 usec */

 1000 nsec */

 4.7 usec */

/*

 *	Frontlight control

/*

 *	LoCoMo "Register Access Bus."

 *

 *	We model this as a regular bus type, and hang devices directly

 *	off this.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Support code for the SCOOP interface found on various Sharp PDAs

 *

 * Copyright (c) 2004 Richard Purdie

 *

 *	Based on code written by Sharp/Lineo for 2.4 kernels

/* PCMCIA to Scoop linkage



   There is no easy way to link multiple scoop devices into one

   single entity for the pxa2xx_pcmcia device so this structure

   is used which is setup by the platform code.



   This file is never modular so this symbol is always

   accessile to the board support files.

 00 */

 04 */

 10 */

 18 */

 14 */

 1C */

 XXX: I'm unsure, but it seems so */

 PA11 = 0, PA12 = 1, etc. up to PA22 = 11 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Hardware parameter area specific to Sharp SL series devices

 *

 * Copyright (c) 2005 Richard Purdie

 *

 * Based on Sharp's 2.4 kernel patches

/*

 * Certain hardware parameters determined at the time of device manufacture,

 * typically including LCD parameters are loaded by the bootloader at the

 * address PARAM_BASE. As the kernel will overwrite them, we need to store

 * them early in the boot process, then pass them to the appropriate drivers.

 * Not all devices use all parameters but the format is common to all.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm/common/bL_switcher.c -- big.LITTLE cluster switcher core driver

 *

 * Created by:	Nicolas Pitre, March 2012

 * Copyright:	(C) 2012-2013  Linaro Limited

/*

 * Use our own MPIDR accessors as the generic ones in asm/cputype.h have

 * __attribute_const__ and we don't want the compiler to assume any

 * constness here as the value _does_ change along some code paths.

/*

 * bL switcher core code.

 Advertise our handshake location */

	/*

	 * Our state has been saved at this point.  Let's release our

	 * inbound CPU.

	/*

	 * From this point, we must assume that our counterpart CPU might

	 * have taken over in its parallel world already, as if execution

	 * just returned from cpu_suspend().  It is therefore important to

	 * be very careful not to make any change the other guy is not

	 * expecting.  This is why we need stack isolation.

	 *

	 * Fancy under cover tasks could be performed here.  For now

	 * we have none.

	/*

	 * Let's wait until our inbound is alive.

 Let's put ourself down. */

 should never get here */

/*

 * Stack isolation.  To ensure 'current' remains valid, we just use another

 * piece of our thread's stack space which should be fairly lightly used.

 * The selected area starts just above the thread_info structure located

 * at the very bottom of the stack, aligned to a cache line, and indexed

 * with the cluster number.

/*

 * Generic switcher interface

/*

 * bL_switch_to - Switch to a specific cluster for the current CPU

 * @new_cluster_id: the ID of the cluster to switch to.

 *

 * This function must be called on the CPU to be switched.

 * Returns 0 on success, else a negative status code.

 Close the gate for our entry vectors */

 Install our "inbound alive" notifier. */

	/*

	 * Let's wake up the inbound CPU now in case it requires some delay

	 * to come online, but leave it gated in our entry vector code.

	/*

	 * Raise a SGI on the inbound CPU to make sure it doesn't stall

	 * in a possible WFI, such as in bL_power_down().

	/*

	 * Wait for the inbound to come up.  This allows for other

	 * tasks to be scheduled in the mean time.

	/*

	 * From this point we are entering the switch critical zone

	 * and can't take any interrupts anymore.

 redirect GIC's SGIs to our counterpart */

 we can not tolerate errors at this point */

 Swap the physical CPUs in the logical map for this logical CPU. */

 Let's do the actual CPU switch. */

 We are executing on the inbound CPU at this point */

/*

 * bL_switch_request_cb - Switch to a specific cluster for the given CPU,

 *      with completion notification via a callback

 *

 * @cpu: the CPU to switch

 * @new_cluster_id: the ID of the cluster to switch to.

 * @completer: switch completion callback.  if non-NULL,

 *	@completer(@completer_cookie) will be called on completion of

 *	the switch, in non-atomic context.

 * @completer_cookie: opaque context argument for @completer.

 *

 * This function causes a cluster switch on the given CPU by waking up

 * the appropriate switcher thread.  This function may or may not return

 * before the switch has occurred.

 *

 * If a @completer callback function is supplied, it will be called when

 * the switch is complete.  This can be used to determine asynchronously

 * when the switch is complete, regardless of when bL_switch_request()

 * returns.  When @completer is supplied, no new switch request is permitted

 * for the affected CPU until after the switch is complete, and @completer

 * has returned.

/*

 * Activation and configuration code.

 First pass to validate what we have */

	/*

	 * Now let's do the pairing.  We match each CPU with another CPU

	 * from a different cluster.  To get a uniform scheduling behavior

	 * without fiddling with CPU topology and compute capacity data,

	 * we'll use logical CPUs initially belonging to the same cluster.

			/*

			 * Let's remember the last match to create "odd"

			 * pairings on purpose in order for other code not

			 * to assume any relation between physical and

			 * logical CPU numbers.

	/*

	 * Now we disable the unwanted CPUs i.e. everything that has no

	 * pairing information (that includes the pairing counterparts).

 Let's take note of the GIC ID for this CPU */

 Determine the logical CPU a given physical CPU is grouped on. */

	/*

	 * To deactivate the switcher, we must shut down the switcher

	 * threads to prevent any other requests from being accepted.

	 * Then, if the final cluster for given logical CPU is not the

	 * same as the original one, we'll recreate a switcher thread

	 * just for the purpose of switching the CPU back without any

	 * possibility for interference from external requests.

 no more switch may happen on this CPU at this point */

 If execution gets here, we're in trouble. */

 CONFIG_SYSFS */

/*

 * Veto any CPU hotplug operation on those CPUs we've removed

 * while the switcher is active.

 * We're just not ready to deal with that given the trickery involved.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm/common/mcpm_entry.c -- entry point for multi-cluster PM

 *

 * Created by:  Nicolas Pitre, March 2012

 * Copyright:   (C) 2012-2013  Linaro Limited

/*

 * The public API for this code is documented in arch/arm/include/asm/mcpm.h.

 * For a comprehensive description of the main algorithm used here, please

 * see Documentation/arm/cluster-pm-race-avoidance.rst.

/*

 * __mcpm_cpu_going_down: Indicates that the cpu is being torn down.

 *    This must be called at the point of committing to teardown of a CPU.

 *    The CPU cache (SCTRL.C bit) is expected to still be active.

/*

 * __mcpm_cpu_down: Indicates that cpu teardown is complete and that the

 *    cluster can be torn down without disrupting this CPU.

 *    To avoid deadlocks, this must be called before a CPU is powered down.

 *    The CPU cache (SCTRL.C bit) is expected to be off.

 *    However L2 cache might or might not be active.

/*

 * __mcpm_outbound_leave_critical: Leave the cluster teardown critical section.

 * @state: the final state of the cluster:

 *     CLUSTER_UP: no destructive teardown was done and the cluster has been

 *         restored to the previous state (CPU cache still active); or

 *     CLUSTER_DOWN: the cluster has been torn-down, ready for power-off

 *         (CPU cache disabled, L2 cache either enabled or disabled).

/*

 * __mcpm_outbound_enter_critical: Enter the cluster teardown critical section.

 * This function should be called by the last man, after local CPU teardown

 * is complete.  CPU cache expected to be active.

 *

 * Returns:

 *     false: the critical section was not entered because an inbound CPU was

 *         observed, or the cluster is already being set up;

 *     true: the critical section was entered: it is now safe to tear down the

 *         cluster.

 Warn inbound CPUs that the cluster is being torn down: */

 Back out if the inbound cluster is already in the critical region: */

	/*

	 * Wait for all CPUs to get out of the GOING_DOWN state, so that local

	 * teardown is complete on each CPU before tearing down the cluster.

	 *

	 * If any CPU has been woken up again from the DOWN state, then we

	 * shouldn't be taking the cluster down at all: abort in that case.

/*

 * We can't use regular spinlocks. In the switcher case, it is possible

 * for an outbound CPU to call power_down() after its inbound counterpart

 * is already live using the same logical CPU number which trips lockdep

 * debugging.

 try not to shadow power_up errors */

	/*

	 * Since this is called with IRQs enabled, and no arch_spin_lock_irq

	 * variant exists, we need to disable IRQs manually here.

	/*

	 * The only possible values are:

	 * 0 = CPU down

	 * 1 = CPU (still) up

	 * 2 = CPU requested to be up before it had a chance

	 *     to actually make itself down.

	 * Any other value is a bug.

		/*

		 * If cpu_going_down is false here, that means a power_up

		 * request raced ahead of us.  Even if we do not want to

		 * shut this CPU down, the caller still expects execution

		 * to return through the system resume entry path, like

		 * when the WFI is aborted due to a new IRQ or the like..

		 * So let's continue with cache cleaning in all cases.

 Now we are prepared for power-down, do it: */

	/*

	 * It is possible for a power_up request to happen concurrently

	 * with a power_down request for the same CPU. In this case the

	 * CPU might not be able to actually enter a powered down state

	 * with the WFI instruction if the power_up request has removed

	 * the required reset condition.  We must perform a re-entry in

	 * the kernel as if the power_up method just had deasserted reset

	 * on the CPU.

 should never get here */

 Some platforms might have to enable special resume modes, etc. */

	/*

	 * We're going to soft-restart the current CPU through the

	 * low-level MCPM code by leveraging the suspend/resume

	 * infrastructure. Let's play it safe by using cpu_pm_enter()

	 * in case the CPU init code path resets the VFP or similar.

	/*

	 * Set initial CPU and cluster states.

	 * Only one cluster is assumed to be active at this point.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2010 Freescale Semiconductor, Inc.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2012 Freescale Semiconductor, Inc.

 * Copyright 2012 Linaro Ltd.

 MXS DIGCTL SAIF CLKMUX */

 offset 0x150 */

 offset 0x160 */

	/*

	 * clk_enable(hbus_clk) for ocotp can be skipped

	 * as it must be on when system is running.

 try to clear ERROR bit */

 check both BUSY and ERROR cleared */

 open OCOTP banks for read */

 approximately wait 32 hclk cycles */

 poll BUSY bit becoming cleared */

 close banks for power saving */

		/*

		 * OCOTP only stores the last 4 octets for each mac address,

		 * so hard-code OUI here.

/*

 * Reset the system. It is called by machine_restart().

 reset the chip */

 Delay to allow the serial port to show the message */

 We'll take a jump through zero as a poor second */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/arm/mach-footbridge/ebsa285.c

 *

 * EBSA285 machine fixup

 LEDs */

/*

 * The triggers lines up below will only be used if the

 * LED triggers are compiled in.

 3 LEDS all off */

/*

 * Since we may have triggers on any subsystem, defer registration

 * until after subsystem_init.

 Maintainer: Russell King */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/arm/mach-footbridge/dc21285-timer.c

 *

 *  Copyright (C) 1998 Russell King.

 *  Copyright (C) 1998 Phil Blundell

 Stop the timer if in one-shot mode */

/*

 * Set up timer interrupt.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/arm/kernel/dma-ebsa285.c

 *

 *  Copyright (C) 1998 Phil Blundell

 *

 * DMA functions specific to EBSA-285/CATS architectures

 *

 *  Changelog:

 *   09-Nov-1998 RMK	Split out ISA DMA functions to dma-isa.c

 *   17-Mar-1999 RMK	Allow any EBSA285-like architecture to have

 *			ISA DMA controllers.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/arch/arm/mach-footbridge/irq.c

 *

 *  Copyright (C) 1996-2000 Russell King

 *

 *  Changelog:

 *   22-Aug-1998 RMK	Restructured IRQ routines

 *   03-Sep-1998 PJB	Merged CATS support

 *   20-Jan-1998 RMK	Started merge of EBSA286, CATS and NetWinder

 *   26-Jan-1999 PJB	Don't use IACK on CATS

 *   16-Mar-1999 RMK	Added autodetect of ISA PICs

	/*

	 * Setup, and then probe for an ISA PIC

	 * If the PIC is not there, then we

	 * ignore the PIC.

 IRQ number		*/

 Slave on Ch2		*/

 x86			*/

 pattern: 11110101	*/

 IRQ number		*/

 Slave on Ch1		*/

 x86			*/

 pattern: 11111010	*/

 mask all IRQs	*/

 mask all IRQs	*/

		/*

		 * On the NetWinder, don't automatically

		 * enable ISA IRQ11 when it is requested.

		 * There appears to be a missing pull-up

		 * resistor on this line.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/arm/mach-footbridge/isa-timer.c

 *

 *  Copyright (C) 1998 Russell King.

 *  Copyright (C) 1998 Phil Blundell

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/arm/mach-footbridge/ebsa285-pci.c

 *

 * PCI bios-type initialisation for PCI machines

 *

 * Bits taken from various places.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/arm/mach-footbridge/netwinder-hw.c

 *

 * Netwinder machine fixup

 *

 * Copyright (C) 1998, 1999 Russell King, Phil Blundell

/*

 * Winbond WB83977F accessibility stuff

/*

 * This is a lock for accessing ports GP1_IO_BASE and GP2_IO_BASE

 Open up the SuperIO chip */

 Close up the EFER gate */

/*

 * Initialise the Winbond W83977F global registers

	/*

	 * Enable R/W config registers

	/*

	 * Power down FDC (not used)

	/*

	 * GP12, GP11, CIRRX, IRRXH, GP10

	/*

	 * GP23, GP22, GP21, GP20, GP13

	/*

	 * GP17, GP16, GP15, GP14

/*

 * Initialise the Winbond W83977F printer port

	/*

	 * mode 1 == EPP

/*

 * Initialise the Winbond W83977F keyboard controller

	/*

	 * Keyboard controller address

	/*

	 * Keyboard IRQ 1, active high, edge trigger

	/*

	 * Mouse IRQ 5, active high, edge trigger

	/*

	 * KBC 8MHz

	/*

	 * Enable device

/*

 * Initialise the Winbond W83977F Infra-Red device

	/*

	 * IR base address

	/*

	 * IRDA IRQ 6, active high, edge trigger

	/*

	 * RX DMA - ISA DMA 0

	/*

	 * TX DMA - Disable Tx DMA

	/*

	 * Append CRC, Enable bank selection

	/*

	 * Enable device

/*

 * Initialise Winbond W83977F general purpose IO

	/*

	 * Set up initial I/O definitions

	/*

	 * Group1 base address

	/*

	 * GP10 (Orage button) IRQ 10, active high, edge trigger

	/*

	 * GP10: Debounce filter enabled, IRQ, input

	/*

	 * Enable Group1

	/*

	 * Group2 base address

	/*

	 * Clear watchdog timer regs

	 *  - timer disable

	/*

	 *  - disable LED, no mouse nor keyboard IRQ

	/*

	 *  - timer counting, disable power LED, disable timeouot

	/*

	 * Enable group2

	/*

	 * Set Group1/Group2 outputs

/*

 * Initialise the Winbond W83977F chip.

	/*

	 * Open up the SuperIO chip

	/*

	 * Initialise the global registers

	/*

	 * Initialise the various devices in

	 * the multi-IO chip.

	/*

	 * Close up the EFER gate

 Assign a card no = 2

 disable the modem section of the chip */

 disable the cdrom section of the chip */

 disable the MPU-401 section of the chip */

 turn on OPL3 */

/*

 * Initialise any other hardware after we've got the PCI bus

 * initialised.  We may need the PCI bus to talk to this other

 * hardware.

/*

 * Older NeTTroms either do not provide a parameters

 * page, or they don't supply correct information in

 * the parameter page.

	/*

	 * We must not use the kernels ISAPnP code

	 * on the NetWinder - it will reset the settings

	 * for the WaveArtist chip and render it inoperable.

 Jump into the ROM */

 open up the SuperIO chip */

 aux function group 1 (logical device 7) */

 set GP16 for WD-TIMER output */

 set a RED LED and toggle WD_TIMER for rebooting */

 LEDs */

/*

 * The triggers lines up below will only be used if the

 * LED triggers are compiled in.

/*

 * The LED control in Netwinder is reversed:

 *  - setting bit means turn off LED

 *  - clearing bit means turn on LED

/*

 * Since we may have triggers on any subsystem, defer registration

 * until after subsystem_init.

 Maintainer: Russell King/Rebel.com */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/arm/mach-footbridge/cats-pci.c

 *

 * PCI bios-type initialisation for PCI machines

 *

 * Bits taken from various places.

 cats host-specific stuff */

 not a valid interrupt. */

/*

 * why not the standard PCI swizzle?  does this prevent 4-port tulip

 * cards being used (ie, pci-pci bridge based cards)?

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/arm/mach-footbridge/netwinder-pci.c

 *

 * PCI bios-type initialisation for PCI machines

 *

 * Bits taken from various places.

/*

 * We now use the slot ID instead of the device identifiers to select

 * which interrupt is routed where.

 host bridge */

 CyberPro */

 DC21143 */

 Winbond 553 */

 Winbond 89C940F */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/arm/mach-footbridge/cats-hw.c

 *

 * CATS machine fixup

 *

 * Copyright (C) 1998, 1999 Russell King, Phil Blundell

 Set Aladdin to CONFIGURE mode */

 Select logical device 3 */

		/* Set parallel port to DMA channel 3, ECP+EPP1.9, 

 Select logical device 4 */

 UART1 high speed mode */

 Select logical device 5 */

 UART2 high speed mode */

 Set Aladdin to RUN mode */

/*

 * CATS uses soft-reboot by default, since

 * hard reboots fail on early boards.

 Maintainer: Philip Blundell */

 SPDX-License-Identifier: GPL-2.0

/*

 *  arch/arm/mach-footbridge/isa-rtc.c

 *

 *  Copyright (C) 1998 Russell King.

 *  Copyright (C) 1998 Phil Blundell

 *

 * CATS has a real-time clock, though the evaluation board doesn't.

 *

 * Changelog:

 *  21-Mar-1998	RMK	Created

 *  27-Aug-1998	PJB	CATS support

 *  28-Dec-1998	APH	Made leds optional

 *  20-Jan-1999	RMK	Started merge of EBSA285, CATS and NetWinder

 *  16-Mar-1999	RMK	More support for EBSA285-like machines with RTCs in

	/*

	 * Probe for the RTC.

	/*

	 * make sure the divider is set

	/*

	 * Set control reg B

	 *   (24 hour mode, update enabled)

		/*

		 * We have a RTC.  Check the battery

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/arch/arm/mach-footbridge/common.c

 *

 *  Copyright (C) 1998-2000 Russell King, Dave Gilbert.

/*

 * Footbridge IRQ translation table

 *  Converts from our IRQ numbers into FootBridge masks

  0 */

  1 */

  2 */

  3 */

  4 */

  5 */

  6 */

  7 */

  8 */

  9 */

 10 */

 11 */

 12 */

 13 */

 14 */

 15 */

 16 */

 17 */

 18 */

 19 */

	/*

	 * setup DC21285 IRQs

		/* The following is dependent on which slot

		 * you plug the Southbridge card into.  We

		 * currently assume that you plug it into

		 * the right-hand most slot.

/*

 * Common mapping for all systems.  Note that the outbound write flush is

 * commented out since there is a "No Fix" problem with it.  Not mapping

 * it means that we have extra bullet protection on our feet.

/*

 * The mapping when the footbridge is in host mode.  We don't map any of

 * this when we are in add-in mode.

	/*

	 * Set up the common mapping first; we need this to

	 * determine whether we're in host mode or not.

	/*

	 * Now, work out what we've got to map in addition on this

	 * platform.

 Jump into the ROM */

		/*

		 * Force the watchdog to do a CPU reset.

		 *

		 * After making sure that the watchdog is disabled

		 * (so we can change the timer registers) we first

		 * enable the timer to autoreload itself.  Next, the

		 * timer interval is set really short and any

		 * current interrupt request is cleared (so we can

		 * see an edge transition).  Finally, TIMER4 is

		 * enabled as the watchdog.

/*

 * These two functions convert virtual addresses to PCI addresses and PCI

 * addresses to virtual addresses.  Note that it is only legal to use these

 * on memory obtained via get_zeroed_page or kmalloc.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/arch/arm/kernel/dec21285.c: PCI functions for DC21285

 *

 *  Copyright (C) 1998-2001 Russell King

 *  Copyright (C) 1998-2000 Phil Blundell

			/*

			 * For devfn 0, point at the 21285

/*

 * Warn on PCI errors.

	/*

	 * back off this interrupt

	/*

	 * back off this interrupt

	/*

	 * These registers need to be set up whether we're the

	 * central function or not.

		/*

		 * Clear any existing errors - we aren't

		 * interested in historical data...

	/*

	 * We don't care if these fail.

		/*

		 * Map our SDRAM at a known address in PCI space, just in case

		 * the firmware had other ideas.  Using a nonzero base is

		 * necessary, since some VGA cards forcefully use PCI addresses

		 * in the range 0x000a0000 to 0x000c0000. (eg, S3 cards).

		/*

		 * If we are not compiled to accept "add-in" mode, then

		 * we are using a constant virt_to_bus translation which

		 * can not hope to cater for the way the host BIOS  has

		 * set up the machine.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/arch/arm/mach-footbridge/isa.c

 *

 *  Copyright (C) 2004 Russell King.

 Personal server doesn't have RTC */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 Linus Walleij

 The syscon contains the magic SMP start address registers */

 Put the boot address in this magic register */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2014 Linaro Ltd.

 *

 * Author: Linus Walleij <linus.walleij@linaro.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm/probes/decode-thumb.c

 *

 * Copyright (C) 2011 Jon Medhurst <tixy@yxit.co.uk>.

 Load/store multiple instructions */

 Rn is PC		1110 100x x0xx 1111 xxxx xxxx xxxx xxxx */

 SRS			1110 1000 00x0 xxxx xxxx xxxx xxxx xxxx */

 RFE			1110 1000 00x1 xxxx xxxx xxxx xxxx xxxx */

 SRS			1110 1001 10x0 xxxx xxxx xxxx xxxx xxxx */

 RFE			1110 1001 10x1 xxxx xxxx xxxx xxxx xxxx */

 STM Rn, {...pc}	1110 100x x0x0 xxxx 1xxx xxxx xxxx xxxx */

 LDM Rn, {...lr,pc}	1110 100x x0x1 xxxx 11xx xxxx xxxx xxxx */

 LDM/STM Rn, {...sp}	1110 100x x0xx xxxx xx1x xxxx xxxx xxxx */

 STMIA		1110 1000 10x0 xxxx xxxx xxxx xxxx xxxx */

 LDMIA		1110 1000 10x1 xxxx xxxx xxxx xxxx xxxx */

 STMDB		1110 1001 00x0 xxxx xxxx xxxx xxxx xxxx */

 LDMDB		1110 1001 00x1 xxxx xxxx xxxx xxxx xxxx */

 Load/store dual, load/store exclusive, table branch */

 STRD (immediate)	1110 1000 x110 xxxx xxxx xxxx xxxx xxxx */

 LDRD (immediate)	1110 1000 x111 xxxx xxxx xxxx xxxx xxxx */

 STRD (immediate)	1110 1001 x1x0 xxxx xxxx xxxx xxxx xxxx */

 LDRD (immediate)	1110 1001 x1x1 xxxx xxxx xxxx xxxx xxxx */

 TBB			1110 1000 1101 xxxx xxxx xxxx 0000 xxxx */

 TBH			1110 1000 1101 xxxx xxxx xxxx 0001 xxxx */

 STREX		1110 1000 0100 xxxx xxxx xxxx xxxx xxxx */

 LDREX		1110 1000 0101 xxxx xxxx xxxx xxxx xxxx */

 STREXB		1110 1000 1100 xxxx xxxx xxxx 0100 xxxx */

 STREXH		1110 1000 1100 xxxx xxxx xxxx 0101 xxxx */

 STREXD		1110 1000 1100 xxxx xxxx xxxx 0111 xxxx */

 LDREXB		1110 1000 1101 xxxx xxxx xxxx 0100 xxxx */

 LDREXH		1110 1000 1101 xxxx xxxx xxxx 0101 xxxx */

 LDREXD		1110 1000 1101 xxxx xxxx xxxx 0111 xxxx */

 And unallocated instructions...				*/

 Data-processing (shifted register)				*/

 TST			1110 1010 0001 xxxx xxxx 1111 xxxx xxxx */

 TEQ			1110 1010 1001 xxxx xxxx 1111 xxxx xxxx */

 CMN			1110 1011 0001 xxxx xxxx 1111 xxxx xxxx */

 CMP			1110 1011 1011 xxxx xxxx 1111 xxxx xxxx */

 MOV			1110 1010 010x 1111 xxxx xxxx xxxx xxxx */

 MVN			1110 1010 011x 1111 xxxx xxxx xxxx xxxx */

 ???			1110 1010 101x xxxx xxxx xxxx xxxx xxxx */

 ???			1110 1010 111x xxxx xxxx xxxx xxxx xxxx */

 ???			1110 1011 001x xxxx xxxx xxxx xxxx xxxx */

 ???			1110 1011 100x xxxx xxxx xxxx xxxx xxxx */

 ???			1110 1011 111x xxxx xxxx xxxx xxxx xxxx */

 ADD/SUB SP, SP, Rm, LSL #0..3				*/

			1110 1011 x0xx 1101 x000 1101 xx00 xxxx */

 ADD/SUB SP, SP, Rm, shift					*/

			1110 1011 x0xx 1101 xxxx 1101 xxxx xxxx */

 ADD/SUB Rd, SP, Rm, shift					*/

			1110 1011 x0xx 1101 xxxx xxxx xxxx xxxx */

 AND			1110 1010 000x xxxx xxxx xxxx xxxx xxxx */

 BIC			1110 1010 001x xxxx xxxx xxxx xxxx xxxx */

 ORR			1110 1010 010x xxxx xxxx xxxx xxxx xxxx */

 ORN			1110 1010 011x xxxx xxxx xxxx xxxx xxxx */

 EOR			1110 1010 100x xxxx xxxx xxxx xxxx xxxx */

 PKH			1110 1010 110x xxxx xxxx xxxx xxxx xxxx */

 ADD			1110 1011 000x xxxx xxxx xxxx xxxx xxxx */

 ADC			1110 1011 010x xxxx xxxx xxxx xxxx xxxx */

 SBC			1110 1011 011x xxxx xxxx xxxx xxxx xxxx */

 SUB			1110 1011 101x xxxx xxxx xxxx xxxx xxxx */

 RSB			1110 1011 110x xxxx xxxx xxxx xxxx xxxx */

 Data-processing (modified immediate)				*/

 TST			1111 0x00 0001 xxxx 0xxx 1111 xxxx xxxx */

 TEQ			1111 0x00 1001 xxxx 0xxx 1111 xxxx xxxx */

 CMN			1111 0x01 0001 xxxx 0xxx 1111 xxxx xxxx */

 CMP			1111 0x01 1011 xxxx 0xxx 1111 xxxx xxxx */

 MOV			1111 0x00 010x 1111 0xxx xxxx xxxx xxxx */

 MVN			1111 0x00 011x 1111 0xxx xxxx xxxx xxxx */

 ???			1111 0x00 101x xxxx 0xxx xxxx xxxx xxxx */

 ???			1111 0x00 110x xxxx 0xxx xxxx xxxx xxxx */

 ???			1111 0x00 111x xxxx 0xxx xxxx xxxx xxxx */

 ???			1111 0x01 001x xxxx 0xxx xxxx xxxx xxxx */

 ???			1111 0x01 100x xxxx 0xxx xxxx xxxx xxxx */

 ???			1111 0x01 111x xxxx 0xxx xxxx xxxx xxxx */

 ADD Rd, SP, #imm	1111 0x01 000x 1101 0xxx xxxx xxxx xxxx */

 SUB Rd, SP, #imm	1111 0x01 101x 1101 0xxx xxxx xxxx xxxx */

 AND			1111 0x00 000x xxxx 0xxx xxxx xxxx xxxx */

 BIC			1111 0x00 001x xxxx 0xxx xxxx xxxx xxxx */

 ORR			1111 0x00 010x xxxx 0xxx xxxx xxxx xxxx */

 ORN			1111 0x00 011x xxxx 0xxx xxxx xxxx xxxx */

 EOR			1111 0x00 100x xxxx 0xxx xxxx xxxx xxxx */

 ADD			1111 0x01 000x xxxx 0xxx xxxx xxxx xxxx */

 ADC			1111 0x01 010x xxxx 0xxx xxxx xxxx xxxx */

 SBC			1111 0x01 011x xxxx 0xxx xxxx xxxx xxxx */

 SUB			1111 0x01 101x xxxx 0xxx xxxx xxxx xxxx */

 RSB			1111 0x01 110x xxxx 0xxx xxxx xxxx xxxx */

 Data-processing (plain binary immediate)			*/

 ADDW Rd, PC, #imm	1111 0x10 0000 1111 0xxx xxxx xxxx xxxx */

 SUBW	Rd, PC, #imm	1111 0x10 1010 1111 0xxx xxxx xxxx xxxx */

 ADDW SP, SP, #imm	1111 0x10 0000 1101 0xxx 1101 xxxx xxxx */

 SUBW	SP, SP, #imm	1111 0x10 1010 1101 0xxx 1101 xxxx xxxx */

 ADDW			1111 0x10 0000 xxxx 0xxx xxxx xxxx xxxx */

 SUBW			1111 0x10 1010 xxxx 0xxx xxxx xxxx xxxx */

 MOVW			1111 0x10 0100 xxxx 0xxx xxxx xxxx xxxx */

 MOVT			1111 0x10 1100 xxxx 0xxx xxxx xxxx xxxx */

 SSAT16		1111 0x11 0010 xxxx 0000 xxxx 00xx xxxx */

 SSAT			1111 0x11 00x0 xxxx 0xxx xxxx xxxx xxxx */

 USAT16		1111 0x11 1010 xxxx 0000 xxxx 00xx xxxx */

 USAT			1111 0x11 10x0 xxxx 0xxx xxxx xxxx xxxx */

 SFBX			1111 0x11 0100 xxxx 0xxx xxxx xxxx xxxx */

 UFBX			1111 0x11 1100 xxxx 0xxx xxxx xxxx xxxx */

 BFC			1111 0x11 0110 1111 0xxx xxxx xxxx xxxx */

 BFI			1111 0x11 0110 xxxx 0xxx xxxx xxxx xxxx */

 Branches and miscellaneous control				*/

 YIELD		1111 0011 1010 xxxx 10x0 x000 0000 0001 */

 SEV			1111 0011 1010 xxxx 10x0 x000 0000 0100 */

 NOP			1111 0011 1010 xxxx 10x0 x000 0000 0000 */

 WFE			1111 0011 1010 xxxx 10x0 x000 0000 0010 */

 WFI			1111 0011 1010 xxxx 10x0 x000 0000 0011 */

 MRS Rd, CPSR		1111 0011 1110 xxxx 10x0 xxxx xxxx xxxx */

	/*

	 * Unsupported instructions

	 *			1111 0x11 1xxx xxxx 10x0 xxxx xxxx xxxx

	 *

	 * MSR			1111 0011 100x xxxx 10x0 xxxx xxxx xxxx

	 * DBG hint		1111 0011 1010 xxxx 10x0 x000 1111 xxxx

	 * Unallocated hints	1111 0011 1010 xxxx 10x0 x000 xxxx xxxx

	 * CPS			1111 0011 1010 xxxx 10x0 xxxx xxxx xxxx

	 * CLREX/DSB/DMB/ISB	1111 0011 1011 xxxx 10x0 xxxx xxxx xxxx

	 * BXJ			1111 0011 1100 xxxx 10x0 xxxx xxxx xxxx

	 * SUBS PC,LR,#<imm8>	1111 0011 1101 xxxx 10x0 xxxx xxxx xxxx

	 * MRS Rd, SPSR		1111 0011 1111 xxxx 10x0 xxxx xxxx xxxx

	 * SMC			1111 0111 1111 xxxx 1000 xxxx xxxx xxxx

	 * UNDEFINED		1111 0111 1111 xxxx 1010 xxxx xxxx xxxx

	 * ???			1111 0111 1xxx xxxx 1010 xxxx xxxx xxxx

 Bcc			1111 0xxx xxxx xxxx 10x0 xxxx xxxx xxxx */

 BLX			1111 0xxx xxxx xxxx 11x0 xxxx xxxx xxx0 */

 B			1111 0xxx xxxx xxxx 10x1 xxxx xxxx xxxx */

 BL			1111 0xxx xxxx xxxx 11x1 xxxx xxxx xxxx */

 Memory hints							*/

 PLD (literal)	1111 1000 x001 1111 1111 xxxx xxxx xxxx */

 PLI (literal)	1111 1001 x001 1111 1111 xxxx xxxx xxxx */

 PLD{W} (immediate)	1111 1000 10x1 xxxx 1111 xxxx xxxx xxxx */

 PLD{W} (immediate)	1111 1000 00x1 xxxx 1111 1100 xxxx xxxx */

 PLI (immediate)	1111 1001 1001 xxxx 1111 xxxx xxxx xxxx */

 PLI (immediate)	1111 1001 0001 xxxx 1111 1100 xxxx xxxx */

 PLD{W} (register)	1111 1000 00x1 xxxx 1111 0000 00xx xxxx */

 PLI (register)	1111 1001 0001 xxxx 1111 0000 00xx xxxx */

 Other unallocated instructions...				*/

 Store/Load single data item					*/

 ???			1111 100x x11x xxxx xxxx xxxx xxxx xxxx */

 ???			1111 1001 0101 xxxx xxxx xxxx xxxx xxxx */

 ???			1111 100x 0xxx xxxx xxxx 10x0 xxxx xxxx */

 STRBT		1111 1000 0000 xxxx xxxx 1110 xxxx xxxx */

 STRHT		1111 1000 0010 xxxx xxxx 1110 xxxx xxxx */

 STRT			1111 1000 0100 xxxx xxxx 1110 xxxx xxxx */

 LDRBT		1111 1000 0001 xxxx xxxx 1110 xxxx xxxx */

 LDRSBT		1111 1001 0001 xxxx xxxx 1110 xxxx xxxx */

 LDRHT		1111 1000 0011 xxxx xxxx 1110 xxxx xxxx */

 LDRSHT		1111 1001 0011 xxxx xxxx 1110 xxxx xxxx */

 LDRT			1111 1000 0101 xxxx xxxx 1110 xxxx xxxx */

 STR{,B,H} Rn,[PC...]	1111 1000 xxx0 1111 xxxx xxxx xxxx xxxx */

 STR{,B,H} PC,[Rn...]	1111 1000 xxx0 xxxx 1111 xxxx xxxx xxxx */

 LDR (literal)	1111 1000 x101 1111 xxxx xxxx xxxx xxxx */

 STR (immediate)	1111 1000 0100 xxxx xxxx 1xxx xxxx xxxx */

 LDR (immediate)	1111 1000 0101 xxxx xxxx 1xxx xxxx xxxx */

 STR (immediate)	1111 1000 1100 xxxx xxxx xxxx xxxx xxxx */

 LDR (immediate)	1111 1000 1101 xxxx xxxx xxxx xxxx xxxx */

 STR (register)	1111 1000 0100 xxxx xxxx 0000 00xx xxxx */

 LDR (register)	1111 1000 0101 xxxx xxxx 0000 00xx xxxx */

 LDRB (literal)	1111 1000 x001 1111 xxxx xxxx xxxx xxxx */

 LDRSB (literal)	1111 1001 x001 1111 xxxx xxxx xxxx xxxx */

 LDRH (literal)	1111 1000 x011 1111 xxxx xxxx xxxx xxxx */

 LDRSH (literal)	1111 1001 x011 1111 xxxx xxxx xxxx xxxx */

 STRB (immediate)	1111 1000 0000 xxxx xxxx 1xxx xxxx xxxx */

 STRH (immediate)	1111 1000 0010 xxxx xxxx 1xxx xxxx xxxx */

 LDRB (immediate)	1111 1000 0001 xxxx xxxx 1xxx xxxx xxxx */

 LDRSB (immediate)	1111 1001 0001 xxxx xxxx 1xxx xxxx xxxx */

 LDRH (immediate)	1111 1000 0011 xxxx xxxx 1xxx xxxx xxxx */

 LDRSH (immediate)	1111 1001 0011 xxxx xxxx 1xxx xxxx xxxx */

 STRB (immediate)	1111 1000 1000 xxxx xxxx xxxx xxxx xxxx */

 STRH (immediate)	1111 1000 1010 xxxx xxxx xxxx xxxx xxxx */

 LDRB (immediate)	1111 1000 1001 xxxx xxxx xxxx xxxx xxxx */

 LDRSB (immediate)	1111 1001 1001 xxxx xxxx xxxx xxxx xxxx */

 LDRH (immediate)	1111 1000 1011 xxxx xxxx xxxx xxxx xxxx */

 LDRSH (immediate)	1111 1001 1011 xxxx xxxx xxxx xxxx xxxx */

 STRB (register)	1111 1000 0000 xxxx xxxx 0000 00xx xxxx */

 STRH (register)	1111 1000 0010 xxxx xxxx 0000 00xx xxxx */

 LDRB (register)	1111 1000 0001 xxxx xxxx 0000 00xx xxxx */

 LDRSB (register)	1111 1001 0001 xxxx xxxx 0000 00xx xxxx */

 LDRH (register)	1111 1000 0011 xxxx xxxx 0000 00xx xxxx */

 LDRSH (register)	1111 1001 0011 xxxx xxxx 0000 00xx xxxx */

 Other unallocated instructions...				*/

 Data-processing (register)					*/

 ???			1111 1010 011x xxxx 1111 xxxx 1xxx xxxx */

 SXTH			1111 1010 0000 1111 1111 xxxx 1xxx xxxx */

 UXTH			1111 1010 0001 1111 1111 xxxx 1xxx xxxx */

 SXTB16		1111 1010 0010 1111 1111 xxxx 1xxx xxxx */

 UXTB16		1111 1010 0011 1111 1111 xxxx 1xxx xxxx */

 SXTB			1111 1010 0100 1111 1111 xxxx 1xxx xxxx */

 UXTB			1111 1010 0101 1111 1111 xxxx 1xxx xxxx */

 ???			1111 1010 1xxx xxxx 1111 xxxx 0x11 xxxx */

 ???			1111 1010 1x11 xxxx 1111 xxxx 0xxx xxxx */

 SADD16		1111 1010 1001 xxxx 1111 xxxx 0000 xxxx */

 SASX			1111 1010 1010 xxxx 1111 xxxx 0000 xxxx */

 SSAX			1111 1010 1110 xxxx 1111 xxxx 0000 xxxx */

 SSUB16		1111 1010 1101 xxxx 1111 xxxx 0000 xxxx */

 SADD8		1111 1010 1000 xxxx 1111 xxxx 0000 xxxx */

 SSUB8		1111 1010 1100 xxxx 1111 xxxx 0000 xxxx */

 QADD16		1111 1010 1001 xxxx 1111 xxxx 0001 xxxx */

 QASX			1111 1010 1010 xxxx 1111 xxxx 0001 xxxx */

 QSAX			1111 1010 1110 xxxx 1111 xxxx 0001 xxxx */

 QSUB16		1111 1010 1101 xxxx 1111 xxxx 0001 xxxx */

 QADD8		1111 1010 1000 xxxx 1111 xxxx 0001 xxxx */

 QSUB8		1111 1010 1100 xxxx 1111 xxxx 0001 xxxx */

 SHADD16		1111 1010 1001 xxxx 1111 xxxx 0010 xxxx */

 SHASX		1111 1010 1010 xxxx 1111 xxxx 0010 xxxx */

 SHSAX		1111 1010 1110 xxxx 1111 xxxx 0010 xxxx */

 SHSUB16		1111 1010 1101 xxxx 1111 xxxx 0010 xxxx */

 SHADD8		1111 1010 1000 xxxx 1111 xxxx 0010 xxxx */

 SHSUB8		1111 1010 1100 xxxx 1111 xxxx 0010 xxxx */

 UADD16		1111 1010 1001 xxxx 1111 xxxx 0100 xxxx */

 UASX			1111 1010 1010 xxxx 1111 xxxx 0100 xxxx */

 USAX			1111 1010 1110 xxxx 1111 xxxx 0100 xxxx */

 USUB16		1111 1010 1101 xxxx 1111 xxxx 0100 xxxx */

 UADD8		1111 1010 1000 xxxx 1111 xxxx 0100 xxxx */

 USUB8		1111 1010 1100 xxxx 1111 xxxx 0100 xxxx */

 UQADD16		1111 1010 1001 xxxx 1111 xxxx 0101 xxxx */

 UQASX		1111 1010 1010 xxxx 1111 xxxx 0101 xxxx */

 UQSAX		1111 1010 1110 xxxx 1111 xxxx 0101 xxxx */

 UQSUB16		1111 1010 1101 xxxx 1111 xxxx 0101 xxxx */

 UQADD8		1111 1010 1000 xxxx 1111 xxxx 0101 xxxx */

 UQSUB8		1111 1010 1100 xxxx 1111 xxxx 0101 xxxx */

 UHADD16		1111 1010 1001 xxxx 1111 xxxx 0110 xxxx */

 UHASX		1111 1010 1010 xxxx 1111 xxxx 0110 xxxx */

 UHSAX		1111 1010 1110 xxxx 1111 xxxx 0110 xxxx */

 UHSUB16		1111 1010 1101 xxxx 1111 xxxx 0110 xxxx */

 UHADD8		1111 1010 1000 xxxx 1111 xxxx 0110 xxxx */

 UHSUB8		1111 1010 1100 xxxx 1111 xxxx 0110 xxxx */

 SXTAH		1111 1010 0000 xxxx 1111 xxxx 1xxx xxxx */

 UXTAH		1111 1010 0001 xxxx 1111 xxxx 1xxx xxxx */

 SXTAB16		1111 1010 0010 xxxx 1111 xxxx 1xxx xxxx */

 UXTAB16		1111 1010 0011 xxxx 1111 xxxx 1xxx xxxx */

 SXTAB		1111 1010 0100 xxxx 1111 xxxx 1xxx xxxx */

 UXTAB		1111 1010 0101 xxxx 1111 xxxx 1xxx xxxx */

 QADD			1111 1010 1000 xxxx 1111 xxxx 1000 xxxx */

 QDADD		1111 1010 1000 xxxx 1111 xxxx 1001 xxxx */

 QSUB			1111 1010 1000 xxxx 1111 xxxx 1010 xxxx */

 QDSUB		1111 1010 1000 xxxx 1111 xxxx 1011 xxxx */

 SEL			1111 1010 1010 xxxx 1111 xxxx 1000 xxxx */

 LSL			1111 1010 000x xxxx 1111 xxxx 0000 xxxx */

 LSR			1111 1010 001x xxxx 1111 xxxx 0000 xxxx */

 ASR			1111 1010 010x xxxx 1111 xxxx 0000 xxxx */

 ROR			1111 1010 011x xxxx 1111 xxxx 0000 xxxx */

 CLZ			1111 1010 1010 xxxx 1111 xxxx 1000 xxxx */

 REV			1111 1010 1001 xxxx 1111 xxxx 1000 xxxx */

 REV16		1111 1010 1001 xxxx 1111 xxxx 1001 xxxx */

 RBIT			1111 1010 1001 xxxx 1111 xxxx 1010 xxxx */

 REVSH		1111 1010 1001 xxxx 1111 xxxx 1011 xxxx */

 Other unallocated instructions...				*/

 Multiply, multiply accumulate, and absolute difference	*/

 ???			1111 1011 0000 xxxx 1111 xxxx 0001 xxxx */

 ???			1111 1011 0111 xxxx 1111 xxxx 0001 xxxx */

 SMULxy		1111 1011 0001 xxxx 1111 xxxx 00xx xxxx */

 MUL			1111 1011 0000 xxxx 1111 xxxx 0000 xxxx */

 SMUAD{X}		1111 1011 0010 xxxx 1111 xxxx 000x xxxx */

 SMULWy		1111 1011 0011 xxxx 1111 xxxx 000x xxxx */

 SMUSD{X}		1111 1011 0100 xxxx 1111 xxxx 000x xxxx */

 SMMUL{R}		1111 1011 0101 xxxx 1111 xxxx 000x xxxx */

 USAD8		1111 1011 0111 xxxx 1111 xxxx 0000 xxxx */

 ???			1111 1011 0111 xxxx xxxx xxxx 0001 xxxx */

 SMLAxy		1111 1011 0001 xxxx xxxx xxxx 00xx xxxx */

 MLA			1111 1011 0000 xxxx xxxx xxxx 0000 xxxx */

 MLS			1111 1011 0000 xxxx xxxx xxxx 0001 xxxx */

 SMLAD{X}		1111 1011 0010 xxxx xxxx xxxx 000x xxxx */

 SMLAWy		1111 1011 0011 xxxx xxxx xxxx 000x xxxx */

 SMLSD{X}		1111 1011 0100 xxxx xxxx xxxx 000x xxxx */

 SMMLA{R}		1111 1011 0101 xxxx xxxx xxxx 000x xxxx */

 SMMLS{R}		1111 1011 0110 xxxx xxxx xxxx 000x xxxx */

 USADA8		1111 1011 0111 xxxx xxxx xxxx 0000 xxxx */

 Other unallocated instructions...				*/

 Long multiply, long multiply accumulate, and divide		*/

 UMAAL		1111 1011 1110 xxxx xxxx xxxx 0110 xxxx */

 SMLALxy		1111 1011 1100 xxxx xxxx xxxx 10xx xxxx */

 SMLALD{X}		1111 1011 1100 xxxx xxxx xxxx 110x xxxx */

 SMLSLD{X}		1111 1011 1101 xxxx xxxx xxxx 110x xxxx */

 SMULL		1111 1011 1000 xxxx xxxx xxxx 0000 xxxx */

 UMULL		1111 1011 1010 xxxx xxxx xxxx 0000 xxxx */

 SMLAL		1111 1011 1100 xxxx xxxx xxxx 0000 xxxx */

 UMLAL		1111 1011 1110 xxxx xxxx xxxx 0000 xxxx */

 SDIV			1111 1011 1001 xxxx xxxx xxxx 1111 xxxx */

 UDIV			1111 1011 1011 xxxx xxxx xxxx 1111 xxxx */

 Other unallocated instructions...				*/

	/*

	 * Load/store multiple instructions

	 *			1110 100x x0xx xxxx xxxx xxxx xxxx xxxx

	/*

	 * Load/store dual, load/store exclusive, table branch

	 *			1110 100x x1xx xxxx xxxx xxxx xxxx xxxx

	/*

	 * Data-processing (shifted register)

	 *			1110 101x xxxx xxxx xxxx xxxx xxxx xxxx

	/*

	 * Coprocessor instructions

	 *			1110 11xx xxxx xxxx xxxx xxxx xxxx xxxx

	/*

	 * Data-processing (modified immediate)

	 *			1111 0x0x xxxx xxxx 0xxx xxxx xxxx xxxx

	/*

	 * Data-processing (plain binary immediate)

	 *			1111 0x1x xxxx xxxx 0xxx xxxx xxxx xxxx

	/*

	 * Branches and miscellaneous control

	 *			1111 0xxx xxxx xxxx 1xxx xxxx xxxx xxxx

	/*

	 * Advanced SIMD element or structure load/store instructions

	 *			1111 1001 xxx0 xxxx xxxx xxxx xxxx xxxx

	/*

	 * Memory hints

	 *			1111 100x x0x1 xxxx 1111 xxxx xxxx xxxx

	/*

	 * Store single data item

	 *			1111 1000 xxx0 xxxx xxxx xxxx xxxx xxxx

	 * Load single data items

	 *			1111 100x xxx1 xxxx xxxx xxxx xxxx xxxx

	/*

	 * Data-processing (register)

	 *			1111 1010 xxxx xxxx 1111 xxxx xxxx xxxx

	/*

	 * Multiply, multiply accumulate, and absolute difference

	 *			1111 1011 0xxx xxxx xxxx xxxx xxxx xxxx

	/*

	 * Long multiply, long multiply accumulate, and divide

	 *			1111 1011 1xxx xxxx xxxx xxxx xxxx xxxx

	/*

	 * Coprocessor instructions

	 *			1111 11xx xxxx xxxx xxxx xxxx xxxx xxxx

 Miscellaneous 16-bit instructions		    */

 ADD (SP plus immediate)	1011 0000 0xxx xxxx */

 SUB (SP minus immediate)	1011 0000 1xxx xxxx */

 CBZ				1011 00x1 xxxx xxxx */

 CBNZ				1011 10x1 xxxx xxxx */

 SXTH				1011 0010 00xx xxxx */

 SXTB				1011 0010 01xx xxxx */

 UXTH				1011 0010 10xx xxxx */

 UXTB				1011 0010 11xx xxxx */

 REV				1011 1010 00xx xxxx */

 REV16			1011 1010 01xx xxxx */

 ???				1011 1010 10xx xxxx */

 REVSH			1011 1010 11xx xxxx */

 PUSH				1011 010x xxxx xxxx */

 POP				1011 110x xxxx xxxx */

	/*

	 * If-Then, and hints

	 *				1011 1111 xxxx xxxx

 YIELD			1011 1111 0001 0000 */

 SEV				1011 1111 0100 0000 */

 NOP				1011 1111 0000 0000 */

 WFE				1011 1111 0010 0000 */

 WFI				1011 1111 0011 0000 */

 Unassigned hints		1011 1111 xxxx 0000 */

 IT				1011 1111 xxxx xxxx */

 SETEND			1011 0110 010x xxxx */

 CPS				1011 0110 011x xxxx */

 BKPT				1011 1110 xxxx xxxx */

 And unallocated instructions...		    */

	/*

	 * Shift (immediate), add, subtract, move, and compare

	 *				00xx xxxx xxxx xxxx

 CMP (immediate)		0010 1xxx xxxx xxxx */

 ADD (register)		0001 100x xxxx xxxx */

 SUB (register)		0001 101x xxxx xxxx */

 LSL (immediate)		0000 0xxx xxxx xxxx */

 LSR (immediate)		0000 1xxx xxxx xxxx */

 ASR (immediate)		0001 0xxx xxxx xxxx */

 ADD (immediate, Thumb)	0001 110x xxxx xxxx */

 SUB (immediate, Thumb)	0001 111x xxxx xxxx */

 MOV (immediate)		0010 0xxx xxxx xxxx */

 ADD (immediate, Thumb)	0011 0xxx xxxx xxxx */

 SUB (immediate, Thumb)	0011 1xxx xxxx xxxx */

	/*

	 * 16-bit Thumb data-processing instructions

	 *				0100 00xx xxxx xxxx

 TST (register)		0100 0010 00xx xxxx */

 CMP (register)		0100 0010 10xx xxxx */

 CMN (register)		0100 0010 11xx xxxx */

 AND (register)		0100 0000 00xx xxxx */

 EOR (register)		0100 0000 01xx xxxx */

 LSL (register)		0100 0000 10xx xxxx */

 LSR (register)		0100 0000 11xx xxxx */

 ASR (register)		0100 0001 00xx xxxx */

 ADC (register)		0100 0001 01xx xxxx */

 SBC (register)		0100 0001 10xx xxxx */

 ROR (register)		0100 0001 11xx xxxx */

 RSB (immediate)		0100 0010 01xx xxxx */

 ORR (register)		0100 0011 00xx xxxx */

 MUL				0100 0011 00xx xxxx */

 BIC (register)		0100 0011 10xx xxxx */

 MVN (register)		0100 0011 10xx xxxx */

	/*

	 * Special data instructions and branch and exchange

	 *				0100 01xx xxxx xxxx

 BLX pc			0100 0111 1111 1xxx */

 BX (register)		0100 0111 0xxx xxxx */

 BLX (register)		0100 0111 1xxx xxxx */

 ADD pc, pc			0100 0100 1111 1111 */

 ADD (register)		0100 0100 xxxx xxxx */

 CMP (register)		0100 0101 xxxx xxxx */

 MOV (register)		0100 0110 xxxx xxxx */

	/*

	 * Load from Literal Pool

	 * LDR (literal)		0100 1xxx xxxx xxxx

	/*

	 * 16-bit Thumb Load/store instructions

	 *				0101 xxxx xxxx xxxx

	 *				011x xxxx xxxx xxxx

	 *				100x xxxx xxxx xxxx

 STR (register)		0101 000x xxxx xxxx */

 STRH (register)		0101 001x xxxx xxxx */

 STRB (register)		0101 010x xxxx xxxx */

 LDRSB (register)		0101 011x xxxx xxxx */

 LDR (register)		0101 100x xxxx xxxx */

 LDRH (register)		0101 101x xxxx xxxx */

 LDRB (register)		0101 110x xxxx xxxx */

 LDRSH (register)		0101 111x xxxx xxxx */

 STR (immediate, Thumb)	0110 0xxx xxxx xxxx */

 LDR (immediate, Thumb)	0110 1xxx xxxx xxxx */

 STRB (immediate, Thumb)	0111 0xxx xxxx xxxx */

 LDRB (immediate, Thumb)	0111 1xxx xxxx xxxx */

 STRH (immediate, Thumb)	1000 0xxx xxxx xxxx */

 LDRH (immediate, Thumb)	1000 1xxx xxxx xxxx */

 STR (immediate, Thumb)	1001 0xxx xxxx xxxx */

 LDR (immediate, Thumb)	1001 1xxx xxxx xxxx */

	/*

	 * Generate PC-/SP-relative address

	 * ADR (literal)		1010 0xxx xxxx xxxx

	 * ADD (SP plus immediate)	1010 1xxx xxxx xxxx

	/*

	 * Miscellaneous 16-bit instructions

	 *				1011 xxxx xxxx xxxx

 STM				1100 0xxx xxxx xxxx */

 LDM				1100 1xxx xxxx xxxx */

	/*

	 * Conditional branch, and Supervisor Call

 Permanently UNDEFINED	1101 1110 xxxx xxxx */

 SVC				1101 1111 xxxx xxxx */

 Conditional branch		1101 xxxx xxxx xxxx */

	/*

	 * Unconditional branch

	 * B				1110 0xxx xxxx xxxx

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm/probes/decode.c

 *

 * Copyright (C) 2011 Jon Medhurst <tixy@yxit.co.uk>.

 *

 * Some contents moved here from arch/arm/include/asm/kprobes-arm.c which is

 * Copyright (C) 2006, 2007 Motorola Inc.

/*

 * For STR and STM instructions, an ARM core may choose to use either

 * a +8 or a +12 displacement from the current instruction's address.

 * Whichever value is chosen for a given core, it must be the same for

 * both instructions and may not change.  This function measures it.

 !find_str_pc_offset */

 !test_load_write_pc_interworking */

 !test_alu_write_pc_interworking */

 PSR_C_BIT &= ~PSR_Z_BIT */

 PSR_C_BIT &= ~PSR_Z_BIT */

 PSR_N_BIT ^= PSR_V_BIT */

 PSR_N_BIT ^= PSR_V_BIT */

 PSR_N_BIT ^= PSR_V_BIT */

 PSR_N_BIT |= PSR_Z_BIT */

 PSR_N_BIT ^= PSR_V_BIT */

 PSR_N_BIT |= PSR_Z_BIT */

/*

 * Prepare an instruction slot to receive an instruction for emulating.

 * This is done by placing a subroutine return after the location where the

 * instruction will be placed. We also modify ARM instructions to be

 * unconditional as the condition code will already be checked before any

 * emulation handler is called.

 Thumb bx lr */

 ARM bx lr */

 mov pc, lr */

 Make an ARM instruction unconditional */

/*

 * Write a (probably modified) instruction into the slot previously prepared by

 * prepare_emulated_insn

/*

 * When we modify the register numbers encoded in an instruction to be emulated,

 * the new values come from this define. For ARM and 32-bit Thumb instructions

 * this gives...

 *

 *	bit position	  16  12   8   4   0

 *	---------------+---+---+---+---+---+

 *	register	 r2  r0  r1  --  r3

 Each nibble has same value as that at INSN_NEW_BITS bit 16 */

/*

 * Validate and modify each of the registers encoded in an instruction.

 *

 * Each nibble in regs contains a value from enum decode_reg_type. For each

 * non-zero value, the corresponding nibble in pinsn is validated and modified

 * according to the type.

 Start at least significant nibble */

 Nibble not a register, skip to next */

 Any register is allowed */

 Replace register with same as at bit position 16 */

 Only allow SP (R13) */

 Only allow PC (R15) */

 Reject SP (R13) */

 Reject SP and PC (R13 and R15) */

 No writeback, so any register is OK */

 Reject PC (R15) */

 Replace value of nibble with new register number... */

/*

 * probes_decode_insn operates on data tables in order to decode an ARM

 * architecture instruction onto which a kprobe has been placed.

 *

 * These instruction decoding tables are a concatenation of entries each

 * of which consist of one of the following structs:

 *

 *	decode_table

 *	decode_custom

 *	decode_simulate

 *	decode_emulate

 *	decode_or

 *	decode_reject

 *

 * Each of these starts with a struct decode_header which has the following

 * fields:

 *

 *	type_regs

 *	mask

 *	value

 *

 * The least significant DECODE_TYPE_BITS of type_regs contains a value

 * from enum decode_type, this indicates which of the decode_* structs

 * the entry contains. The value DECODE_TYPE_END indicates the end of the

 * table.

 *

 * When the table is parsed, each entry is checked in turn to see if it

 * matches the instruction to be decoded using the test:

 *

 *	(insn & mask) == value

 *

 * If no match is found before the end of the table is reached then decoding

 * fails with INSN_REJECTED.

 *

 * When a match is found, decode_regs() is called to validate and modify each

 * of the registers encoded in the instruction; the data it uses to do this

 * is (type_regs >> DECODE_TYPE_BITS). A validation failure will cause decoding

 * to fail with INSN_REJECTED.

 *

 * Once the instruction has passed the above tests, further processing

 * depends on the type of the table entry's decode struct.

 *

	/*

	 * @insn can be modified by decode_regs. Save its original

	 * value for checkers.

	/*

	 * stack_space is initialized to 0 here. Checker functions

	 * should update is value if they find this is a stack store

	 * instruction: positive value means bytes of stack usage,

	 * negitive value means unable to determine stack usage

	 * statically. For instruction doesn't store to stack, checker

	 * do nothing with it.

	/*

	 * Similarly to stack_space, register_usage_flags is filled by

	 * checkers. Its default value is set to ~0, which is 'all

	 * registers are used', to prevent any potential optimization.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * arch/arm/probes/decode-arm.c

 *

 * Some code moved here from arch/arm/kernel/kprobes-arm.c

 *

 * Copyright (C) 2006, 2007 Motorola Inc.

/*

 * To avoid the complications of mimicing single-stepping on a

 * processor without a Next-PC or a single-step mode, and to

 * avoid having to deal with the side-effects of boosting, we

 * simulate or emulate (almost) all ARM instructions.

 *

 * "Simulation" is where the instruction's behavior is duplicated in

 * C code.  "Emulation" is where the original instruction is rewritten

 * and executed, often by altering its registers.

 *

 * By having all behavior of the kprobe'd instruction completed before

 * returning from the kprobe_handler(), all locks (scheduler and

 * interrupt) can safely be released.  There is no need for secondary

 * breakpoints, no race with MP or preemptable kernels, nor having to

 * clean up resources counts at a later time impacting overall system

 * performance.  By rewriting the instruction, only the minimum registers

 * need to be loaded and saved back optimizing performance.

 *

 * Calling the insnslot_*_rwflags version of a function doesn't hurt

 * anything even when the CPSR flags aren't updated by the

 * instruction.  It's just a little slower in return for saving

 * a little space by not having a duplicate function that doesn't

 * update the flags.  (The same optimization can be said for

 * instructions that do or don't perform register writeback)

 * Also, instructions can either read the flags, only write the

 * flags, or read and write the flags.  To save combinations

 * rather than for sheer performance, flag functions just assume

 * read and write of flags.

 Mask out execution state */

/*

 * For the instruction masking and comparisons in all the "space_*"

 * functions below, Do _not_ rearrange the order of tests unless

 * you're very, very sure of what you are doing.  For the sake of

 * efficiency, the masks for some tests sometimes assume other test

 * have been done prior to them so the number of patterns to test

 * for an instruction set can be as broad as possible to reduce the

 * number of tests needed.

 Unconditional instructions					*/

 memory hint		1111 0100 x001 xxxx xxxx xxxx xxxx xxxx */

 PLDI (immediate)	1111 0100 x101 xxxx xxxx xxxx xxxx xxxx */

 PLDW (immediate)	1111 0101 x001 xxxx xxxx xxxx xxxx xxxx */

 PLD (immediate)	1111 0101 x101 xxxx xxxx xxxx xxxx xxxx */

 memory hint		1111 0110 x001 xxxx xxxx xxxx xxx0 xxxx */

 PLDI (register)	1111 0110 x101 xxxx xxxx xxxx xxx0 xxxx */

 PLDW (register)	1111 0111 x001 xxxx xxxx xxxx xxx0 xxxx */

 PLD (register)	1111 0111 x101 xxxx xxxx xxxx xxx0 xxxx */

 BLX (immediate)	1111 101x xxxx xxxx xxxx xxxx xxxx xxxx */

 CPS			1111 0001 0000 xxx0 xxxx xxxx xx0x xxxx */

 SETEND		1111 0001 0000 0001 xxxx xxxx 0000 xxxx */

 SRS			1111 100x x1x0 xxxx xxxx xxxx xxxx xxxx */

 RFE			1111 100x x0x1 xxxx xxxx xxxx xxxx xxxx */

 Coprocessor instructions... */

 MCRR2		1111 1100 0100 xxxx xxxx xxxx xxxx xxxx */

 MRRC2		1111 1100 0101 xxxx xxxx xxxx xxxx xxxx */

 LDC2			1111 110x xxx1 xxxx xxxx xxxx xxxx xxxx */

 STC2			1111 110x xxx0 xxxx xxxx xxxx xxxx xxxx */

 CDP2			1111 1110 xxxx xxxx xxxx xxxx xxx0 xxxx */

 MCR2			1111 1110 xxx0 xxxx xxxx xxxx xxx1 xxxx */

 MRC2			1111 1110 xxx1 xxxx xxxx xxxx xxx1 xxxx */

 Other unallocated instructions...				*/

 Miscellaneous instructions					*/

 MRS cpsr		cccc 0001 0000 xxxx xxxx xxxx 0000 xxxx */

 BX			cccc 0001 0010 xxxx xxxx xxxx 0001 xxxx */

 BLX (register)	cccc 0001 0010 xxxx xxxx xxxx 0011 xxxx */

 CLZ			cccc 0001 0110 xxxx xxxx xxxx 0001 xxxx */

 QADD			cccc 0001 0000 xxxx xxxx xxxx 0101 xxxx */

 QSUB			cccc 0001 0010 xxxx xxxx xxxx 0101 xxxx */

 QDADD		cccc 0001 0100 xxxx xxxx xxxx 0101 xxxx */

 QDSUB		cccc 0001 0110 xxxx xxxx xxxx 0101 xxxx */

 BXJ			cccc 0001 0010 xxxx xxxx xxxx 0010 xxxx */

 MSR			cccc 0001 0x10 xxxx xxxx xxxx 0000 xxxx */

 MRS spsr		cccc 0001 0100 xxxx xxxx xxxx 0000 xxxx */

 BKPT			1110 0001 0010 xxxx xxxx xxxx 0111 xxxx */

 SMC			cccc 0001 0110 xxxx xxxx xxxx 0111 xxxx */

 And unallocated instructions...				*/

 Halfword multiply and multiply-accumulate			*/

 SMLALxy		cccc 0001 0100 xxxx xxxx xxxx 1xx0 xxxx */

 SMULWy		cccc 0001 0010 xxxx xxxx xxxx 1x10 xxxx */

 SMULxy		cccc 0001 0110 xxxx xxxx xxxx 1xx0 xxxx */

 SMLAxy		cccc 0001 0000 xxxx xxxx xxxx 1xx0 xxxx */

 SMLAWy		cccc 0001 0010 xxxx xxxx xxxx 1x00 xxxx */

 Multiply and multiply-accumulate				*/

 MUL			cccc 0000 0000 xxxx xxxx xxxx 1001 xxxx */

 MULS			cccc 0000 0001 xxxx xxxx xxxx 1001 xxxx */

 MLA			cccc 0000 0010 xxxx xxxx xxxx 1001 xxxx */

 MLAS			cccc 0000 0011 xxxx xxxx xxxx 1001 xxxx */

 MLS			cccc 0000 0110 xxxx xxxx xxxx 1001 xxxx */

 UMAAL		cccc 0000 0100 xxxx xxxx xxxx 1001 xxxx */

 UMULL		cccc 0000 1000 xxxx xxxx xxxx 1001 xxxx */

 UMULLS		cccc 0000 1001 xxxx xxxx xxxx 1001 xxxx */

 UMLAL		cccc 0000 1010 xxxx xxxx xxxx 1001 xxxx */

 UMLALS		cccc 0000 1011 xxxx xxxx xxxx 1001 xxxx */

 SMULL		cccc 0000 1100 xxxx xxxx xxxx 1001 xxxx */

 SMULLS		cccc 0000 1101 xxxx xxxx xxxx 1001 xxxx */

 SMLAL		cccc 0000 1110 xxxx xxxx xxxx 1001 xxxx */

 SMLALS		cccc 0000 1111 xxxx xxxx xxxx 1001 xxxx */

 Synchronization primitives					*/

 Deprecated on ARMv6 and may be UNDEFINED on v7		*/

 SMP/SWPB		cccc 0001 0x00 xxxx xxxx xxxx 1001 xxxx */

 LDREX/STREX{,D,B,H}	cccc 0001 1xxx xxxx xxxx xxxx 1001 xxxx */

 And unallocated instructions...				*/

 Extra load/store instructions				*/

 STRHT		cccc 0000 xx10 xxxx xxxx xxxx 1011 xxxx */

 ???			cccc 0000 xx10 xxxx xxxx xxxx 11x1 xxxx */

 LDRHT		cccc 0000 xx11 xxxx xxxx xxxx 1011 xxxx */

 LDRSBT		cccc 0000 xx11 xxxx xxxx xxxx 1101 xxxx */

 LDRSHT		cccc 0000 xx11 xxxx xxxx xxxx 1111 xxxx */

 LDRD/STRD lr,pc,{...	cccc 000x x0x0 xxxx 111x xxxx 1101 xxxx */

 LDRD (register)	cccc 000x x0x0 xxxx xxxx xxxx 1101 xxxx */

 STRD (register)	cccc 000x x0x0 xxxx xxxx xxxx 1111 xxxx */

 LDRD (immediate)	cccc 000x x1x0 xxxx xxxx xxxx 1101 xxxx */

 STRD (immediate)	cccc 000x x1x0 xxxx xxxx xxxx 1111 xxxx */

 STRH (register)	cccc 000x x0x0 xxxx xxxx xxxx 1011 xxxx */

 LDRH (register)	cccc 000x x0x1 xxxx xxxx xxxx 1011 xxxx */

 LDRSB (register)	cccc 000x x0x1 xxxx xxxx xxxx 1101 xxxx */

 LDRSH (register)	cccc 000x x0x1 xxxx xxxx xxxx 1111 xxxx */

 STRH (immediate)	cccc 000x x1x0 xxxx xxxx xxxx 1011 xxxx */

 LDRH (immediate)	cccc 000x x1x1 xxxx xxxx xxxx 1011 xxxx */

 LDRSB (immediate)	cccc 000x x1x1 xxxx xxxx xxxx 1101 xxxx */

 LDRSH (immediate)	cccc 000x x1x1 xxxx xxxx xxxx 1111 xxxx */

 Data-processing (register)					*/

 <op>S PC, ...	cccc 000x xxx1 xxxx 1111 xxxx xxxx xxxx */

 MOV IP, SP		1110 0001 1010 0000 1100 0000 0000 1101 */

 TST (register)	cccc 0001 0001 xxxx xxxx xxxx xxx0 xxxx */

 TEQ (register)	cccc 0001 0011 xxxx xxxx xxxx xxx0 xxxx */

 CMP (register)	cccc 0001 0101 xxxx xxxx xxxx xxx0 xxxx */

 CMN (register)	cccc 0001 0111 xxxx xxxx xxxx xxx0 xxxx */

 MOV (register)	cccc 0001 101x xxxx xxxx xxxx xxx0 xxxx */

 MVN (register)	cccc 0001 111x xxxx xxxx xxxx xxx0 xxxx */

 AND (register)	cccc 0000 000x xxxx xxxx xxxx xxx0 xxxx */

 EOR (register)	cccc 0000 001x xxxx xxxx xxxx xxx0 xxxx */

 SUB (register)	cccc 0000 010x xxxx xxxx xxxx xxx0 xxxx */

 RSB (register)	cccc 0000 011x xxxx xxxx xxxx xxx0 xxxx */

 ADD (register)	cccc 0000 100x xxxx xxxx xxxx xxx0 xxxx */

 ADC (register)	cccc 0000 101x xxxx xxxx xxxx xxx0 xxxx */

 SBC (register)	cccc 0000 110x xxxx xxxx xxxx xxx0 xxxx */

 RSC (register)	cccc 0000 111x xxxx xxxx xxxx xxx0 xxxx */

 ORR (register)	cccc 0001 100x xxxx xxxx xxxx xxx0 xxxx */

 BIC (register)	cccc 0001 110x xxxx xxxx xxxx xxx0 xxxx */

 TST (reg-shift reg)	cccc 0001 0001 xxxx xxxx xxxx 0xx1 xxxx */

 TEQ (reg-shift reg)	cccc 0001 0011 xxxx xxxx xxxx 0xx1 xxxx */

 CMP (reg-shift reg)	cccc 0001 0101 xxxx xxxx xxxx 0xx1 xxxx */

 CMN (reg-shift reg)	cccc 0001 0111 xxxx xxxx xxxx 0xx1 xxxx */

 MOV (reg-shift reg)	cccc 0001 101x xxxx xxxx xxxx 0xx1 xxxx */

 MVN (reg-shift reg)	cccc 0001 111x xxxx xxxx xxxx 0xx1 xxxx */

 AND (reg-shift reg)	cccc 0000 000x xxxx xxxx xxxx 0xx1 xxxx */

 EOR (reg-shift reg)	cccc 0000 001x xxxx xxxx xxxx 0xx1 xxxx */

 SUB (reg-shift reg)	cccc 0000 010x xxxx xxxx xxxx 0xx1 xxxx */

 RSB (reg-shift reg)	cccc 0000 011x xxxx xxxx xxxx 0xx1 xxxx */

 ADD (reg-shift reg)	cccc 0000 100x xxxx xxxx xxxx 0xx1 xxxx */

 ADC (reg-shift reg)	cccc 0000 101x xxxx xxxx xxxx 0xx1 xxxx */

 SBC (reg-shift reg)	cccc 0000 110x xxxx xxxx xxxx 0xx1 xxxx */

 RSC (reg-shift reg)	cccc 0000 111x xxxx xxxx xxxx 0xx1 xxxx */

 ORR (reg-shift reg)	cccc 0001 100x xxxx xxxx xxxx 0xx1 xxxx */

 BIC (reg-shift reg)	cccc 0001 110x xxxx xxxx xxxx 0xx1 xxxx */

 Data-processing (immediate)					*/

 MOVW			cccc 0011 0000 xxxx xxxx xxxx xxxx xxxx */

 MOVT			cccc 0011 0100 xxxx xxxx xxxx xxxx xxxx */

 YIELD		cccc 0011 0010 0000 xxxx xxxx 0000 0001 */

 SEV			cccc 0011 0010 0000 xxxx xxxx 0000 0100 */

 NOP			cccc 0011 0010 0000 xxxx xxxx 0000 0000 */

 WFE			cccc 0011 0010 0000 xxxx xxxx 0000 0010 */

 WFI			cccc 0011 0010 0000 xxxx xxxx 0000 0011 */

 DBG			cccc 0011 0010 0000 xxxx xxxx ffff xxxx */

 unallocated hints	cccc 0011 0010 0000 xxxx xxxx xxxx xxxx */

 MSR (immediate)	cccc 0011 0x10 xxxx xxxx xxxx xxxx xxxx */

 <op>S PC, ...	cccc 001x xxx1 xxxx 1111 xxxx xxxx xxxx */

 TST (immediate)	cccc 0011 0001 xxxx xxxx xxxx xxxx xxxx */

 TEQ (immediate)	cccc 0011 0011 xxxx xxxx xxxx xxxx xxxx */

 CMP (immediate)	cccc 0011 0101 xxxx xxxx xxxx xxxx xxxx */

 CMN (immediate)	cccc 0011 0111 xxxx xxxx xxxx xxxx xxxx */

 MOV (immediate)	cccc 0011 101x xxxx xxxx xxxx xxxx xxxx */

 MVN (immediate)	cccc 0011 111x xxxx xxxx xxxx xxxx xxxx */

 AND (immediate)	cccc 0010 000x xxxx xxxx xxxx xxxx xxxx */

 EOR (immediate)	cccc 0010 001x xxxx xxxx xxxx xxxx xxxx */

 SUB (immediate)	cccc 0010 010x xxxx xxxx xxxx xxxx xxxx */

 RSB (immediate)	cccc 0010 011x xxxx xxxx xxxx xxxx xxxx */

 ADD (immediate)	cccc 0010 100x xxxx xxxx xxxx xxxx xxxx */

 ADC (immediate)	cccc 0010 101x xxxx xxxx xxxx xxxx xxxx */

 SBC (immediate)	cccc 0010 110x xxxx xxxx xxxx xxxx xxxx */

 RSC (immediate)	cccc 0010 111x xxxx xxxx xxxx xxxx xxxx */

 ORR (immediate)	cccc 0011 100x xxxx xxxx xxxx xxxx xxxx */

 BIC (immediate)	cccc 0011 110x xxxx xxxx xxxx xxxx xxxx */

 Media instructions						*/

 SEL			cccc 0110 1000 xxxx xxxx xxxx 1011 xxxx */

 SSAT			cccc 0110 101x xxxx xxxx xxxx xx01 xxxx */

 USAT			cccc 0110 111x xxxx xxxx xxxx xx01 xxxx */

 SSAT16		cccc 0110 1010 xxxx xxxx xxxx 0011 xxxx */

 USAT16		cccc 0110 1110 xxxx xxxx xxxx 0011 xxxx */

 REV			cccc 0110 1011 xxxx xxxx xxxx 0011 xxxx */

 REV16		cccc 0110 1011 xxxx xxxx xxxx 1011 xxxx */

 RBIT			cccc 0110 1111 xxxx xxxx xxxx 0011 xxxx */

 REVSH		cccc 0110 1111 xxxx xxxx xxxx 1011 xxxx */

 ???			cccc 0110 0x00 xxxx xxxx xxxx xxx1 xxxx */

 ???			cccc 0110 0xxx xxxx xxxx xxxx 1011 xxxx */

 ???			cccc 0110 0xxx xxxx xxxx xxxx 1101 xxxx */

 SADD16		cccc 0110 0001 xxxx xxxx xxxx 0001 xxxx */

 SADDSUBX		cccc 0110 0001 xxxx xxxx xxxx 0011 xxxx */

 SSUBADDX		cccc 0110 0001 xxxx xxxx xxxx 0101 xxxx */

 SSUB16		cccc 0110 0001 xxxx xxxx xxxx 0111 xxxx */

 SADD8		cccc 0110 0001 xxxx xxxx xxxx 1001 xxxx */

 SSUB8		cccc 0110 0001 xxxx xxxx xxxx 1111 xxxx */

 QADD16		cccc 0110 0010 xxxx xxxx xxxx 0001 xxxx */

 QADDSUBX		cccc 0110 0010 xxxx xxxx xxxx 0011 xxxx */

 QSUBADDX		cccc 0110 0010 xxxx xxxx xxxx 0101 xxxx */

 QSUB16		cccc 0110 0010 xxxx xxxx xxxx 0111 xxxx */

 QADD8		cccc 0110 0010 xxxx xxxx xxxx 1001 xxxx */

 QSUB8		cccc 0110 0010 xxxx xxxx xxxx 1111 xxxx */

 SHADD16		cccc 0110 0011 xxxx xxxx xxxx 0001 xxxx */

 SHADDSUBX		cccc 0110 0011 xxxx xxxx xxxx 0011 xxxx */

 SHSUBADDX		cccc 0110 0011 xxxx xxxx xxxx 0101 xxxx */

 SHSUB16		cccc 0110 0011 xxxx xxxx xxxx 0111 xxxx */

 SHADD8		cccc 0110 0011 xxxx xxxx xxxx 1001 xxxx */

 SHSUB8		cccc 0110 0011 xxxx xxxx xxxx 1111 xxxx */

 UADD16		cccc 0110 0101 xxxx xxxx xxxx 0001 xxxx */

 UADDSUBX		cccc 0110 0101 xxxx xxxx xxxx 0011 xxxx */

 USUBADDX		cccc 0110 0101 xxxx xxxx xxxx 0101 xxxx */

 USUB16		cccc 0110 0101 xxxx xxxx xxxx 0111 xxxx */

 UADD8		cccc 0110 0101 xxxx xxxx xxxx 1001 xxxx */

 USUB8		cccc 0110 0101 xxxx xxxx xxxx 1111 xxxx */

 UQADD16		cccc 0110 0110 xxxx xxxx xxxx 0001 xxxx */

 UQADDSUBX		cccc 0110 0110 xxxx xxxx xxxx 0011 xxxx */

 UQSUBADDX		cccc 0110 0110 xxxx xxxx xxxx 0101 xxxx */

 UQSUB16		cccc 0110 0110 xxxx xxxx xxxx 0111 xxxx */

 UQADD8		cccc 0110 0110 xxxx xxxx xxxx 1001 xxxx */

 UQSUB8		cccc 0110 0110 xxxx xxxx xxxx 1111 xxxx */

 UHADD16		cccc 0110 0111 xxxx xxxx xxxx 0001 xxxx */

 UHADDSUBX		cccc 0110 0111 xxxx xxxx xxxx 0011 xxxx */

 UHSUBADDX		cccc 0110 0111 xxxx xxxx xxxx 0101 xxxx */

 UHSUB16		cccc 0110 0111 xxxx xxxx xxxx 0111 xxxx */

 UHADD8		cccc 0110 0111 xxxx xxxx xxxx 1001 xxxx */

 UHSUB8		cccc 0110 0111 xxxx xxxx xxxx 1111 xxxx */

 PKHBT		cccc 0110 1000 xxxx xxxx xxxx x001 xxxx */

 PKHTB		cccc 0110 1000 xxxx xxxx xxxx x101 xxxx */

 ???			cccc 0110 1001 xxxx xxxx xxxx 0111 xxxx */

 ???			cccc 0110 1101 xxxx xxxx xxxx 0111 xxxx */

 SXTB16		cccc 0110 1000 1111 xxxx xxxx 0111 xxxx */

 SXTB			cccc 0110 1010 1111 xxxx xxxx 0111 xxxx */

 SXTH			cccc 0110 1011 1111 xxxx xxxx 0111 xxxx */

 UXTB16		cccc 0110 1100 1111 xxxx xxxx 0111 xxxx */

 UXTB			cccc 0110 1110 1111 xxxx xxxx 0111 xxxx */

 UXTH			cccc 0110 1111 1111 xxxx xxxx 0111 xxxx */

 SXTAB16		cccc 0110 1000 xxxx xxxx xxxx 0111 xxxx */

 SXTAB		cccc 0110 1010 xxxx xxxx xxxx 0111 xxxx */

 SXTAH		cccc 0110 1011 xxxx xxxx xxxx 0111 xxxx */

 UXTAB16		cccc 0110 1100 xxxx xxxx xxxx 0111 xxxx */

 UXTAB		cccc 0110 1110 xxxx xxxx xxxx 0111 xxxx */

 UXTAH		cccc 0110 1111 xxxx xxxx xxxx 0111 xxxx */

 Media instructions						*/

 UNDEFINED		cccc 0111 1111 xxxx xxxx xxxx 1111 xxxx */

 SMLALD		cccc 0111 0100 xxxx xxxx xxxx 00x1 xxxx */

 SMLSLD		cccc 0111 0100 xxxx xxxx xxxx 01x1 xxxx */

 SMUAD		cccc 0111 0000 xxxx 1111 xxxx 00x1 xxxx */

 SMUSD		cccc 0111 0000 xxxx 1111 xxxx 01x1 xxxx */

 SMMUL		cccc 0111 0101 xxxx 1111 xxxx 00x1 xxxx */

 USAD8		cccc 0111 1000 xxxx 1111 xxxx 0001 xxxx */

 SMLAD		cccc 0111 0000 xxxx xxxx xxxx 00x1 xxxx */

 SMLSD		cccc 0111 0000 xxxx xxxx xxxx 01x1 xxxx */

 SMMLA		cccc 0111 0101 xxxx xxxx xxxx 00x1 xxxx */

 USADA8		cccc 0111 1000 xxxx xxxx xxxx 0001 xxxx */

 SMMLS		cccc 0111 0101 xxxx xxxx xxxx 11x1 xxxx */

 SBFX			cccc 0111 101x xxxx xxxx xxxx x101 xxxx */

 UBFX			cccc 0111 111x xxxx xxxx xxxx x101 xxxx */

 BFC			cccc 0111 110x xxxx xxxx xxxx x001 1111 */

 BFI			cccc 0111 110x xxxx xxxx xxxx x001 xxxx */

 Load/store word and unsigned byte				*/

 LDRB/STRB pc,[...]	cccc 01xx x0xx xxxx xxxx xxxx xxxx xxxx */

 STRT			cccc 01x0 x010 xxxx xxxx xxxx xxxx xxxx */

 LDRT			cccc 01x0 x011 xxxx xxxx xxxx xxxx xxxx */

 STRBT		cccc 01x0 x110 xxxx xxxx xxxx xxxx xxxx */

 LDRBT		cccc 01x0 x111 xxxx xxxx xxxx xxxx xxxx */

 STR (immediate)	cccc 010x x0x0 xxxx xxxx xxxx xxxx xxxx */

 STRB (immediate)	cccc 010x x1x0 xxxx xxxx xxxx xxxx xxxx */

 LDR (immediate)	cccc 010x x0x1 xxxx xxxx xxxx xxxx xxxx */

 LDRB (immediate)	cccc 010x x1x1 xxxx xxxx xxxx xxxx xxxx */

 STR (register)	cccc 011x x0x0 xxxx xxxx xxxx xxxx xxxx */

 STRB (register)	cccc 011x x1x0 xxxx xxxx xxxx xxxx xxxx */

 LDR (register)	cccc 011x x0x1 xxxx xxxx xxxx xxxx xxxx */

 LDRB (register)	cccc 011x x1x1 xxxx xxxx xxxx xxxx xxxx */

 Block data transfer instructions				*/

 LDM			cccc 100x x0x1 xxxx xxxx xxxx xxxx xxxx */

 STM			cccc 100x x0x0 xxxx xxxx xxxx xxxx xxxx */

 STM (user registers)	cccc 100x x1x0 xxxx xxxx xxxx xxxx xxxx */

 LDM (user registers)	cccc 100x x1x1 xxxx 0xxx xxxx xxxx xxxx */

 LDM (exception ret)	cccc 100x x1x1 xxxx 1xxx xxxx xxxx xxxx */

	/*

	 * Unconditional instructions

	 *			1111 xxxx xxxx xxxx xxxx xxxx xxxx xxxx

	/*

	 * Miscellaneous instructions

	 *			cccc 0001 0xx0 xxxx xxxx xxxx 0xxx xxxx

	/*

	 * Halfword multiply and multiply-accumulate

	 *			cccc 0001 0xx0 xxxx xxxx xxxx 1xx0 xxxx

	/*

	 * Multiply and multiply-accumulate

	 *			cccc 0000 xxxx xxxx xxxx xxxx 1001 xxxx

	/*

	 * Synchronization primitives

	 *			cccc 0001 xxxx xxxx xxxx xxxx 1001 xxxx

	/*

	 * Extra load/store instructions

	 *			cccc 000x xxxx xxxx xxxx xxxx 1xx1 xxxx

	/*

	 * Data-processing (register)

	 *			cccc 000x xxxx xxxx xxxx xxxx xxx0 xxxx

	 * Data-processing (register-shifted register)

	 *			cccc 000x xxxx xxxx xxxx xxxx 0xx1 xxxx

	/*

	 * Data-processing (immediate)

	 *			cccc 001x xxxx xxxx xxxx xxxx xxxx xxxx

	/*

	 * Media instructions

	 *			cccc 011x xxxx xxxx xxxx xxxx xxx1 xxxx

	/*

	 * Load/store word and unsigned byte

	 *			cccc 01xx xxxx xxxx xxxx xxxx xxxx xxxx

	/*

	 * Block data transfer instructions

	 *			cccc 100x xxxx xxxx xxxx xxxx xxxx xxxx

 B			cccc 1010 xxxx xxxx xxxx xxxx xxxx xxxx */

 BL			cccc 1011 xxxx xxxx xxxx xxxx xxxx xxxx */

	/*

	 * Supervisor Call, and coprocessor instructions

 MCRR			cccc 1100 0100 xxxx xxxx xxxx xxxx xxxx */

 MRRC			cccc 1100 0101 xxxx xxxx xxxx xxxx xxxx */

 LDC			cccc 110x xxx1 xxxx xxxx xxxx xxxx xxxx */

 STC			cccc 110x xxx0 xxxx xxxx xxxx xxxx xxxx */

 CDP			cccc 1110 xxxx xxxx xxxx xxxx xxx0 xxxx */

 MCR			cccc 1110 xxx0 xxxx xxxx xxxx xxx1 xxxx */

 MRC			cccc 1110 xxx1 xxxx xxxx xxxx xxx1 xxxx */

 SVC			cccc 1111 xxxx xxxx xxxx xxxx xxxx xxxx */

/* Return:

 *   INSN_REJECTED     If instruction is one not allowed to kprobe,

 *   INSN_GOOD         If instruction is supported and uses instruction slot,

 *   INSN_GOOD_NO_SLOT If instruction is supported but doesn't use its slot.

 *

 * For instructions we don't want to kprobe (INSN_REJECTED return result):

 *   These are generally ones that modify the processor state making

 *   them "hard" to simulate such as switches processor modes or

 *   make accesses in alternate modes.  Any of these could be simulated

 *   if the work was put into it, but low return considering they

 *   should also be very rare.

